{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def true_2D_1(xt): #True function for 2D_1 Klein Gordon Equation x \\in [-50,50] , t \\in [0,10]\n",
    "#     y = xt[:,0]*np.cos(xt[:,1])\n",
    "#     return y.reshape(-1,1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_thresh = 25000\n",
    "label = \"ES_stan\"\n",
    "\n",
    "x = np.linspace(0,1,500).reshape(-1,1)\n",
    "y = np.linspace(0,1,500).reshape(-1,1)\n",
    "\n",
    "X,Y = np.meshgrid(x,y)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "Y = Y.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xy = np.hstack((X,Y))\n",
    "\n",
    "# bound_pts_1 = (X == 0).reshape(-1,)\n",
    "# bound_pts_2 = np.logical_and(Y == 0,X != 0).reshape(-1,)\n",
    "# bound_pts_3 = np.logical_and(X == 1,Y != 0).reshape(-1,) \n",
    "# bound_pts_4 = np.logical_and(Y == 1,X != 1).reshape(-1,) \n",
    "\n",
    "# xy_bound_1 = xy[bound_pts_1,:]\n",
    "# xy_bound_2 = xy[bound_pts_2,:]\n",
    "# xy_bound_3 = xy[bound_pts_3,:]\n",
    "# xy_bound_4 = xy[bound_pts_4,:]\n",
    "\n",
    "# u_bound_1 = 1000*np.ones((np.shape(xy_bound_1)[0],1))\n",
    "# u_bound_2 = 800*np.ones((np.shape(xy_bound_2)[0],1))\n",
    "# u_bound_3 = 500*np.ones((np.shape(xy_bound_3)[0],1))\n",
    "# u_bound_4 = np.zeros((np.shape(xy_bound_4)[0],1))\n",
    "\n",
    "# xy_bound = np.vstack((xy_bound_1,xy_bound_2,xy_bound_3,xy_bound_4))\n",
    "# u_bound = np.vstack((u_bound_1,u_bound_2,u_bound_3,u_bound_4))\n",
    "\n",
    "# xy_test_tensor = torch.from_numpy(xy).float().to(device)\n",
    "\n",
    "lb_xy = xy[0]\n",
    "ub_xy = xy[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_data = scipy.io.loadmat('./../ES_FEA.mat')\n",
    "\n",
    "xy = np.array(fea_data['xy'])\n",
    "u_true = np.array(fea_data['u'])\n",
    "\n",
    "xy_test_tensor = torch.from_numpy(xy).float().to(device)\n",
    "u_true_norm = np.linalg.norm(u_true,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_T,N_f,seed):\n",
    "    '''Boundary Conditions''' \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    N_t = int(N_T/4)\n",
    "    \n",
    "    x_BC1 = np.random.uniform(size = N_t).reshape(-1,1)\n",
    "    y_BC1 = np.zeros((N_t,1))\n",
    "    u_BC1 = 0*np.ones((N_t,1))\n",
    "    \n",
    "    x_BC2 = np.ones((N_t,1))\n",
    "    y_BC2 = np.random.uniform(size = N_t).reshape(-1,1) \n",
    "    u_BC2 = 1000*np.ones((N_t,1))\n",
    "    \n",
    "    x_BC3 = np.random.uniform(size = N_t).reshape(-1,1)\n",
    "    y_BC3 = np.ones((N_t,1)) \n",
    "    u_BC3 = 0*np.ones((N_t,1))\n",
    "    \n",
    "    x_BC4 = np.zeros((N_t,1))\n",
    "    y_BC4 = np.random.uniform(size = N_t).reshape(-1,1) \n",
    "    u_BC4 = 1000*np.ones((N_t,1))\n",
    "    \n",
    "    XY_corners = np.array([[0,0],[1,0],[0,1],[1,1]]).reshape(-1,2)\n",
    "    U_corners = 1000*np.ones((4,1))\n",
    "    \n",
    "    XY_1 = np.hstack((x_BC1,y_BC1))\n",
    "    XY_2 = np.hstack((x_BC2,y_BC2))\n",
    "    XY_3 = np.hstack((x_BC3,y_BC3))\n",
    "    XY_4 = np.hstack((x_BC4,y_BC4))\n",
    "    \n",
    "    xy_BC = np.vstack((XY_1,XY_2,XY_3,XY_4,XY_corners)) #choose indices from  set 'idx' (x,t)\n",
    "    u_BC = np.vstack((u_BC1,u_BC2,u_BC3,u_BC4,U_corners))\n",
    "    \n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    samples = sampling(N_f)\n",
    "    \n",
    "    xy_coll = lb_xy + (ub_xy - lb_xy)*samples\n",
    "    \n",
    "    xy_coll = np.vstack((xy_coll, xy_BC)) # append training points to collocation points \n",
    "\n",
    "    return xy_coll, xy_BC, u_BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,beta_init):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        self.iter = 0\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)   \n",
    "        \n",
    "        self.beta = Parameter(beta_init*torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "        \n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xy):\n",
    "        if torch.is_tensor(xy) != True:         \n",
    "            xy = torch.from_numpy(xy)                \n",
    "        \n",
    "        ubxy = torch.from_numpy(ub_xy).float().to(device)\n",
    "        lbxy = torch.from_numpy(lb_xy).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xy = (xy - lbxy)/(ubxy - lbxy)\n",
    "        \n",
    "        #convert to float\n",
    "        a = xy.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            z1 =self.activation(z)\n",
    "            a = z1 + self.beta[:,i]*z*z1\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC(self,xy,u):\n",
    "                \n",
    "        loss_bc = self.loss_function(self.forward(xy), u)\n",
    "                \n",
    "        return loss_bc\n",
    "    \n",
    "    def loss_PDE(self, xy_coll, f_hat):\n",
    "        \n",
    "        g = xy_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_y = autograd.grad(u,g,torch.ones([xy_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_xx_yy = autograd.grad(u_x_y,g,torch.ones(xy_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        #du_dx = u_x_t[:,[0]]\n",
    "        \n",
    "        d2u_dx2 = u_xx_yy[:,[0]]\n",
    "        d2u_dy2 = u_xx_yy[:,[1]]    \n",
    "        \n",
    "\n",
    "        f = d2u_dx2 + d2u_dy2\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xy_BC,u_BC,xy_coll,f_hat):\n",
    "\n",
    "        loss_BC = self.loss_BC(xy_BC,u_BC)\n",
    "        loss_f = self.loss_PDE(xy_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_BC + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "     \n",
    "    'callable for optimizer'  \n",
    "    def closure(self):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = self.loss(xy_BC, u_BC, xy_coll,f_hat)\n",
    "        self.train_loss.append(loss.cpu().detach().numpy())\n",
    "        \n",
    "        u_pred = self.test(xy_test_tensor)\n",
    "        #self.test_loss.append(np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))) #Commented because no true values yet\n",
    "        self.beta_val.append(self.beta.cpu().detach().numpy())\n",
    "        \n",
    "        #print(self.iter,\"Train Loss\",self.train_loss[-1],\"Test Loss\",self.test_loss[-1])\n",
    "        print(self.iter,\"Train Loss\",self.train_loss[-1])\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        self.iter += 1\n",
    "  \n",
    "\n",
    "        return loss        \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        u_pred = self.forward(xy_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "        \n",
    "        \n",
    "        return test_mse, test_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(xy_BC,u_BC,xy_coll,f_hat,seed):\n",
    "\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(xy_BC,u_BC,xy_coll,f_hat)\n",
    "        loss.backward()\n",
    "        #print(loss.cpu().detach().numpy())\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*9)\n",
    "    start_time = time.time() \n",
    "    thresh_flag = 0\n",
    "\n",
    "    xy_coll_np_array, xy_BC_np_array, u_BC_np_array = trainingdata(N_T,N_f,rep*22)\n",
    "        \n",
    "    xy_coll = torch.from_numpy(xy_coll_np_array).float().to(device)\n",
    "    xy_BC = torch.from_numpy(xy_BC_np_array).float().to(device)\n",
    "    u_BC = torch.from_numpy(u_BC_np_array).float().to(device)\n",
    "        \n",
    "    f_hat = torch.zeros(xy_coll.shape[0],1).to(device)\n",
    "    \n",
    "\n",
    "    for i in range(max_iter):\n",
    "        train_step(xy_BC,u_BC,xy_coll,f_hat,i)\n",
    "        loss_np = PINN.loss(xy_BC,u_BC,xy_coll,f_hat).cpu().detach().numpy()\n",
    "        \n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1          \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        \n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])   \n",
    "        \n",
    "    elapsed_time[rep] = time.time() - start_time  \n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ES_stan\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 248126.16 Test MSE 74382.51381440704 Test RE 0.4771681719110999\n",
      "1 Train Loss 239420.86 Test MSE 70071.56275786147 Test RE 0.4631342970559795\n",
      "2 Train Loss 222422.36 Test MSE 63941.105364545525 Test RE 0.44241119874083185\n",
      "3 Train Loss 197389.17 Test MSE 54548.57806499005 Test RE 0.4086276583591155\n",
      "4 Train Loss 175988.33 Test MSE 46129.22448025694 Test RE 0.3757717423755804\n",
      "5 Train Loss 163107.81 Test MSE 42417.797034226554 Test RE 0.36033802735170534\n",
      "6 Train Loss 151883.69 Test MSE 39783.8186337546 Test RE 0.3489709494383978\n",
      "7 Train Loss 142897.03 Test MSE 36828.679144026435 Test RE 0.3357601215717553\n",
      "8 Train Loss 133209.77 Test MSE 36095.2105378784 Test RE 0.3323998594742205\n",
      "9 Train Loss 125165.57 Test MSE 33350.990768228876 Test RE 0.31951438237051205\n",
      "10 Train Loss 117950.836 Test MSE 32405.248664665596 Test RE 0.31495152898353906\n",
      "11 Train Loss 113219.914 Test MSE 30949.25733671794 Test RE 0.3077947145706672\n",
      "12 Train Loss 107321.3 Test MSE 29730.615942174278 Test RE 0.3016740787955787\n",
      "13 Train Loss 103550.05 Test MSE 27055.72966613772 Test RE 0.2877833523367995\n",
      "14 Train Loss 99182.414 Test MSE 25644.564140644874 Test RE 0.2801777884449464\n",
      "15 Train Loss 94917.45 Test MSE 26038.945055188626 Test RE 0.2823239583371228\n",
      "16 Train Loss 92024.29 Test MSE 26157.009133582724 Test RE 0.2829632818579382\n",
      "17 Train Loss 87697.516 Test MSE 23905.29715507507 Test RE 0.2705098696536005\n",
      "18 Train Loss 84152.53 Test MSE 23032.45167092498 Test RE 0.26552543324003525\n",
      "19 Train Loss 81196.9 Test MSE 21895.03018111018 Test RE 0.25888614937184484\n",
      "20 Train Loss 77994.44 Test MSE 19811.63963958727 Test RE 0.2462613476391267\n",
      "21 Train Loss 75610.85 Test MSE 17980.71623945793 Test RE 0.2346062272039201\n",
      "22 Train Loss 73243.69 Test MSE 17353.472841369094 Test RE 0.23047787550991433\n",
      "23 Train Loss 70625.63 Test MSE 15838.907188528947 Test RE 0.2201905360943655\n",
      "24 Train Loss 68679.664 Test MSE 15113.837940652624 Test RE 0.2150915862604741\n",
      "25 Train Loss 66796.125 Test MSE 14128.85006336413 Test RE 0.207964616732312\n",
      "26 Train Loss 64728.594 Test MSE 13664.273082310432 Test RE 0.20451695041878645\n",
      "27 Train Loss 62048.21 Test MSE 12777.858827993583 Test RE 0.1977721271645967\n",
      "28 Train Loss 60310.62 Test MSE 12259.592418930668 Test RE 0.1937198204297838\n",
      "29 Train Loss 59106.73 Test MSE 12198.088707182755 Test RE 0.19323328432448508\n",
      "30 Train Loss 57449.926 Test MSE 11326.89306246897 Test RE 0.18620504436894172\n",
      "31 Train Loss 56397.758 Test MSE 11161.2914900877 Test RE 0.1848388539175282\n",
      "32 Train Loss 55457.105 Test MSE 10876.793190864382 Test RE 0.18246790147777486\n",
      "33 Train Loss 54500.79 Test MSE 11288.66128805049 Test RE 0.18589052888190336\n",
      "34 Train Loss 53937.895 Test MSE 11082.34725014011 Test RE 0.18418400779682845\n",
      "35 Train Loss 53124.77 Test MSE 10826.136269563542 Test RE 0.18204249808218495\n",
      "36 Train Loss 52555.773 Test MSE 11028.8989436838 Test RE 0.18373932666745163\n",
      "37 Train Loss 51900.254 Test MSE 10615.14451300104 Test RE 0.1802598466733376\n",
      "38 Train Loss 51257.31 Test MSE 10281.512446138146 Test RE 0.1774044643758715\n",
      "39 Train Loss 50777.16 Test MSE 10263.807426158777 Test RE 0.177251651111943\n",
      "40 Train Loss 50298.527 Test MSE 10121.6253655457 Test RE 0.17601965731574676\n",
      "41 Train Loss 49902.35 Test MSE 10063.622876108228 Test RE 0.1755145878910207\n",
      "42 Train Loss 49453.336 Test MSE 10209.951354056646 Test RE 0.1767860035812077\n",
      "43 Train Loss 49058.07 Test MSE 9959.262008795798 Test RE 0.17460216352757058\n",
      "44 Train Loss 48432.504 Test MSE 9729.41065394833 Test RE 0.17257556701106047\n",
      "45 Train Loss 48179.05 Test MSE 9646.370750060869 Test RE 0.17183752808894237\n",
      "46 Train Loss 47976.89 Test MSE 9541.08887495005 Test RE 0.17089722567664306\n",
      "47 Train Loss 47781.86 Test MSE 9407.596501992195 Test RE 0.16969747597915008\n",
      "48 Train Loss 47469.637 Test MSE 9275.190436662686 Test RE 0.16849905108958907\n",
      "49 Train Loss 47247.02 Test MSE 9381.746081705332 Test RE 0.16946416618917154\n",
      "50 Train Loss 46970.95 Test MSE 9155.720389629187 Test RE 0.16741034924001308\n",
      "51 Train Loss 46796.258 Test MSE 9208.137599427291 Test RE 0.16788888392644116\n",
      "52 Train Loss 46538.297 Test MSE 9355.146666226352 Test RE 0.16922376065491224\n",
      "53 Train Loss 46246.133 Test MSE 9254.964680686819 Test RE 0.16831523380577895\n",
      "54 Train Loss 45927.477 Test MSE 9090.381209979712 Test RE 0.1668119233515478\n",
      "55 Train Loss 45666.24 Test MSE 9033.203460179937 Test RE 0.1662864791622069\n",
      "56 Train Loss 45476.652 Test MSE 8810.219564102588 Test RE 0.16422127050985263\n",
      "57 Train Loss 45289.246 Test MSE 8788.070058145031 Test RE 0.16401470874126017\n",
      "58 Train Loss 44995.688 Test MSE 8763.5037597783 Test RE 0.16378530376198855\n",
      "59 Train Loss 44855.25 Test MSE 8865.028040855339 Test RE 0.1647312897325993\n",
      "60 Train Loss 44700.58 Test MSE 8711.668673905213 Test RE 0.16330020003740436\n",
      "61 Train Loss 44471.44 Test MSE 8656.16955587378 Test RE 0.16277920354760733\n",
      "62 Train Loss 44252.445 Test MSE 8614.736815311575 Test RE 0.16238916507044984\n",
      "63 Train Loss 44073.137 Test MSE 8576.81583777942 Test RE 0.16203136264401907\n",
      "64 Train Loss 43880.438 Test MSE 8516.228873341852 Test RE 0.16145805041508057\n",
      "65 Train Loss 43737.855 Test MSE 8418.55773497811 Test RE 0.16052951337018811\n",
      "66 Train Loss 43602.863 Test MSE 8444.461584712062 Test RE 0.16077629783399874\n",
      "67 Train Loss 43394.996 Test MSE 8345.464251849608 Test RE 0.1598311014823112\n",
      "68 Train Loss 43245.75 Test MSE 8285.713377035245 Test RE 0.15925790368623596\n",
      "69 Train Loss 43093.566 Test MSE 8313.742171010204 Test RE 0.15952704395902678\n",
      "70 Train Loss 42930.92 Test MSE 8336.342191262727 Test RE 0.15974372541902626\n",
      "71 Train Loss 42810.715 Test MSE 8268.071357621435 Test RE 0.15908826662130965\n",
      "72 Train Loss 42687.055 Test MSE 8194.35358616371 Test RE 0.15837746660954394\n",
      "73 Train Loss 42485.266 Test MSE 8136.988878710047 Test RE 0.15782213095970896\n",
      "74 Train Loss 42318.93 Test MSE 8106.998953224892 Test RE 0.15753102554206766\n",
      "75 Train Loss 42128.957 Test MSE 8106.194600622553 Test RE 0.1575232104656015\n",
      "76 Train Loss 41985.46 Test MSE 8084.474028677193 Test RE 0.15731202695432267\n",
      "77 Train Loss 41834.895 Test MSE 7947.127753112218 Test RE 0.1559700240447729\n",
      "78 Train Loss 41638.11 Test MSE 7829.782980368494 Test RE 0.15481423969012495\n",
      "79 Train Loss 41450.676 Test MSE 7725.556784095455 Test RE 0.15378038234165656\n",
      "80 Train Loss 41180.055 Test MSE 7618.906272442501 Test RE 0.15271523236566592\n",
      "81 Train Loss 41046.383 Test MSE 7615.929378705816 Test RE 0.15268539465545838\n",
      "82 Train Loss 40893.816 Test MSE 7513.200652288644 Test RE 0.15165213737706562\n",
      "83 Train Loss 40641.375 Test MSE 7429.4965737991515 Test RE 0.150804998001141\n",
      "84 Train Loss 40519.37 Test MSE 7284.3770863490545 Test RE 0.14932490655271094\n",
      "85 Train Loss 40400.355 Test MSE 7268.358037074784 Test RE 0.14916062623477713\n",
      "86 Train Loss 40295.016 Test MSE 7238.0655800906325 Test RE 0.1488494720811528\n",
      "87 Train Loss 40215.19 Test MSE 7242.367492387965 Test RE 0.14889369952356507\n",
      "88 Train Loss 40067.305 Test MSE 7330.245022243847 Test RE 0.1497942999545106\n",
      "89 Train Loss 39933.1 Test MSE 7382.885804520276 Test RE 0.15033119763181435\n",
      "90 Train Loss 39761.76 Test MSE 7255.053416709453 Test RE 0.14902404555943308\n",
      "91 Train Loss 39634.055 Test MSE 7263.897354299463 Test RE 0.14911484833357683\n",
      "92 Train Loss 39511.367 Test MSE 7210.021718787069 Test RE 0.14856083383262395\n",
      "93 Train Loss 39294.832 Test MSE 7105.879170676966 Test RE 0.14748401467124186\n",
      "94 Train Loss 39115.85 Test MSE 7032.483555011404 Test RE 0.1467203669412963\n",
      "95 Train Loss 38963.42 Test MSE 7045.392668988894 Test RE 0.14685496814811225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96 Train Loss 38854.008 Test MSE 7081.302149641181 Test RE 0.14722874313707213\n",
      "97 Train Loss 38723.47 Test MSE 7191.091334871767 Test RE 0.14836567755330116\n",
      "98 Train Loss 38604.566 Test MSE 7252.106528409123 Test RE 0.14899377687670654\n",
      "99 Train Loss 38521.56 Test MSE 7213.894352502449 Test RE 0.14860072583871817\n",
      "100 Train Loss 38449.098 Test MSE 7175.486441097793 Test RE 0.14820461103881183\n",
      "101 Train Loss 38372.465 Test MSE 7149.364109921127 Test RE 0.14793459590525632\n",
      "102 Train Loss 38277.727 Test MSE 7144.11490988881 Test RE 0.14788027772651094\n",
      "103 Train Loss 38171.082 Test MSE 7138.209930553907 Test RE 0.14781914975530358\n",
      "104 Train Loss 38032.664 Test MSE 7108.743554652108 Test RE 0.14751373712177202\n",
      "105 Train Loss 37959.082 Test MSE 7057.566068993769 Test RE 0.1469817852603267\n",
      "106 Train Loss 37801.67 Test MSE 6957.342184809546 Test RE 0.1459344157657104\n",
      "107 Train Loss 37691.01 Test MSE 7023.79547904397 Test RE 0.14662970824081517\n",
      "108 Train Loss 37550.625 Test MSE 7043.194561703146 Test RE 0.1468320575615396\n",
      "109 Train Loss 37373.79 Test MSE 7096.664429807628 Test RE 0.14738835670833836\n",
      "110 Train Loss 37247.32 Test MSE 7093.555131115759 Test RE 0.1473560651552584\n",
      "111 Train Loss 37127.445 Test MSE 6951.260644438266 Test RE 0.14587061984705377\n",
      "112 Train Loss 37019.254 Test MSE 6960.150452684347 Test RE 0.1459638653438508\n",
      "113 Train Loss 36868.684 Test MSE 6857.950550656642 Test RE 0.1448882665371576\n",
      "114 Train Loss 36731.93 Test MSE 6854.871763366764 Test RE 0.1448557400391261\n",
      "115 Train Loss 36582.77 Test MSE 6752.189089130528 Test RE 0.1437667117965404\n",
      "116 Train Loss 36439.56 Test MSE 6669.213853667414 Test RE 0.1428806323661664\n",
      "117 Train Loss 36295.805 Test MSE 6632.811396033522 Test RE 0.14249015733186157\n",
      "118 Train Loss 36206.824 Test MSE 6514.315947044157 Test RE 0.14121162507306873\n",
      "119 Train Loss 35974.754 Test MSE 6554.475650670665 Test RE 0.14164622969385482\n",
      "120 Train Loss 35817.316 Test MSE 6549.771836577725 Test RE 0.1415953944212673\n",
      "121 Train Loss 35725.945 Test MSE 6541.750293396587 Test RE 0.14150866151121996\n",
      "122 Train Loss 35562.734 Test MSE 6483.70832192365 Test RE 0.14087949186007262\n",
      "123 Train Loss 35379.59 Test MSE 6504.749318398567 Test RE 0.14110789848629732\n",
      "124 Train Loss 35200.473 Test MSE 6591.02504649477 Test RE 0.14204060807476054\n",
      "125 Train Loss 34864.855 Test MSE 6702.333066123807 Test RE 0.1432349645100094\n",
      "126 Train Loss 34692.31 Test MSE 6726.802215789256 Test RE 0.14349619031559113\n",
      "127 Train Loss 34512.047 Test MSE 6723.200136602605 Test RE 0.14345776538216737\n",
      "128 Train Loss 34322.605 Test MSE 6675.985101168466 Test RE 0.14295314726074135\n",
      "129 Train Loss 34104.86 Test MSE 6684.709217447448 Test RE 0.14304652169825954\n",
      "130 Train Loss 33823.59 Test MSE 6561.132101858261 Test RE 0.14171813643384293\n",
      "131 Train Loss 33616.926 Test MSE 6472.813059132382 Test RE 0.14076107471561922\n",
      "132 Train Loss 33346.926 Test MSE 6503.014883517618 Test RE 0.14108908463460837\n",
      "133 Train Loss 33078.387 Test MSE 6556.236566219791 Test RE 0.14166525564735685\n",
      "134 Train Loss 32870.66 Test MSE 6433.841716503031 Test RE 0.1403366896813678\n",
      "135 Train Loss 32708.24 Test MSE 6384.237929601974 Test RE 0.1397946573313923\n",
      "136 Train Loss 32511.23 Test MSE 6295.572175430875 Test RE 0.13882051316030286\n",
      "137 Train Loss 32311.557 Test MSE 6214.33639274442 Test RE 0.13792196029252118\n",
      "138 Train Loss 32131.402 Test MSE 6090.07740429888 Test RE 0.13653608570761333\n",
      "139 Train Loss 31938.867 Test MSE 6001.977137240386 Test RE 0.1355449089674273\n",
      "140 Train Loss 31749.309 Test MSE 6128.402208636856 Test RE 0.13696502214480233\n",
      "141 Train Loss 31553.541 Test MSE 6127.9729493070345 Test RE 0.13696022525474957\n",
      "142 Train Loss 31368.188 Test MSE 5955.1590089560095 Test RE 0.135015218281383\n",
      "143 Train Loss 31209.496 Test MSE 5907.282578465241 Test RE 0.1344713964373035\n",
      "144 Train Loss 30976.652 Test MSE 5821.052396691553 Test RE 0.1334863309589164\n",
      "145 Train Loss 30698.598 Test MSE 5830.041034268387 Test RE 0.1335893533448936\n",
      "146 Train Loss 30516.506 Test MSE 5718.899300392565 Test RE 0.13230987726591525\n",
      "147 Train Loss 30303.934 Test MSE 5598.591541691135 Test RE 0.13091078698712838\n",
      "148 Train Loss 30115.887 Test MSE 5532.20223827274 Test RE 0.13013228805918278\n",
      "149 Train Loss 30017.889 Test MSE 5526.019085238642 Test RE 0.13005954552654742\n",
      "150 Train Loss 29858.438 Test MSE 5481.013254073363 Test RE 0.12952883754769084\n",
      "151 Train Loss 29675.88 Test MSE 5405.621421162177 Test RE 0.12863491246212877\n",
      "152 Train Loss 29517.246 Test MSE 5289.6791656978585 Test RE 0.12724792456607445\n",
      "153 Train Loss 29298.725 Test MSE 5234.734405420413 Test RE 0.12658532697354788\n",
      "154 Train Loss 29138.605 Test MSE 5122.700188889734 Test RE 0.12522340578159055\n",
      "155 Train Loss 29041.598 Test MSE 5061.711080466512 Test RE 0.12447574033340424\n",
      "156 Train Loss 28880.67 Test MSE 5060.745092518756 Test RE 0.12446386215620288\n",
      "157 Train Loss 28741.07 Test MSE 4963.798562113941 Test RE 0.12326594692590695\n",
      "158 Train Loss 28620.578 Test MSE 4968.850281110839 Test RE 0.12332865561209903\n",
      "159 Train Loss 28472.508 Test MSE 4991.255904877267 Test RE 0.1236064006914418\n",
      "160 Train Loss 28371.184 Test MSE 4995.870229016662 Test RE 0.12366352341249956\n",
      "161 Train Loss 28219.812 Test MSE 4981.594329594371 Test RE 0.12348671027207908\n",
      "162 Train Loss 28021.516 Test MSE 5008.425059552714 Test RE 0.12381881171176802\n",
      "163 Train Loss 27871.484 Test MSE 4968.083655706575 Test RE 0.12331914128562312\n",
      "164 Train Loss 27733.223 Test MSE 5024.468754045657 Test RE 0.12401697009874996\n",
      "165 Train Loss 27605.049 Test MSE 5021.463962843439 Test RE 0.12397988151794904\n",
      "166 Train Loss 27451.941 Test MSE 4985.746941553716 Test RE 0.12353816825276945\n",
      "167 Train Loss 27261.328 Test MSE 4970.304679564749 Test RE 0.12334670363853852\n",
      "168 Train Loss 27124.082 Test MSE 4861.50216473958 Test RE 0.1219891720086464\n",
      "169 Train Loss 26997.959 Test MSE 4851.078913412921 Test RE 0.12185832705201513\n",
      "170 Train Loss 26803.459 Test MSE 4815.231392867907 Test RE 0.12140725019408173\n",
      "171 Train Loss 26548.227 Test MSE 4861.565440907637 Test RE 0.12198996589723995\n",
      "172 Train Loss 26205.809 Test MSE 4900.563442026692 Test RE 0.12247827181693591\n",
      "173 Train Loss 25974.896 Test MSE 5006.766225414025 Test RE 0.12379830507750698\n",
      "174 Train Loss 25845.676 Test MSE 4999.4979364167475 Test RE 0.12370841385685864\n",
      "175 Train Loss 25668.555 Test MSE 5002.16051912798 Test RE 0.12374135116824836\n",
      "176 Train Loss 25527.027 Test MSE 4980.205498100347 Test RE 0.12346949548339645\n",
      "177 Train Loss 25398.52 Test MSE 5041.161789318483 Test RE 0.12422281305551983\n",
      "178 Train Loss 25225.297 Test MSE 5045.678280278224 Test RE 0.12427844761231867\n",
      "179 Train Loss 25071.305 Test MSE 4924.346279213801 Test RE 0.12277511066712728\n",
      "180 Train Loss 24873.05 Test MSE 4969.754035006348 Test RE 0.1233398708509324\n",
      "181 Train Loss 24767.834 Test MSE 4951.97075570255 Test RE 0.12311899945306574\n",
      "182 Train Loss 24546.473 Test MSE 4946.180068107202 Test RE 0.12304699254472494\n",
      "183 Train Loss 24368.469 Test MSE 4858.661658506983 Test RE 0.12195352853654612\n",
      "184 Train Loss 24210.709 Test MSE 4845.337649937605 Test RE 0.12178619589335411\n",
      "185 Train Loss 24082.51 Test MSE 4740.052230446468 Test RE 0.12045576924690236\n",
      "186 Train Loss 23963.283 Test MSE 4766.63418789947 Test RE 0.12079305177355362\n",
      "187 Train Loss 23855.654 Test MSE 4710.9820077951945 Test RE 0.12008583013685004\n",
      "188 Train Loss 23737.645 Test MSE 4682.3201623932255 Test RE 0.1197199687704911\n",
      "189 Train Loss 23629.268 Test MSE 4711.595283663148 Test RE 0.12009364627209942\n",
      "190 Train Loss 23502.098 Test MSE 4668.532757538019 Test RE 0.11954357709824702\n",
      "191 Train Loss 23391.969 Test MSE 4647.174834789972 Test RE 0.11926981556999573\n",
      "192 Train Loss 23279.73 Test MSE 4712.707169647974 Test RE 0.12010781584281899\n",
      "193 Train Loss 23136.629 Test MSE 4611.088049819326 Test RE 0.11880582911079528\n",
      "194 Train Loss 22987.46 Test MSE 4642.657489806042 Test RE 0.11921183261060475\n",
      "195 Train Loss 22839.79 Test MSE 4736.298899714366 Test RE 0.12040806936375391\n",
      "196 Train Loss 22686.09 Test MSE 4770.538404805031 Test RE 0.12084251075516289\n",
      "197 Train Loss 22526.955 Test MSE 4803.682004523808 Test RE 0.12126156443553991\n",
      "198 Train Loss 22365.72 Test MSE 4815.652985529593 Test RE 0.12141256492158706\n",
      "199 Train Loss 22271.7 Test MSE 4814.074388068999 Test RE 0.1213926634381591\n",
      "Training time: 190.05\n",
      "Training time: 190.05\n",
      "ES_stan\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 247227.25 Test MSE 73905.65630611402 Test RE 0.47563617821751075\n",
      "1 Train Loss 237245.06 Test MSE 69946.53929286299 Test RE 0.4627209445973497\n",
      "2 Train Loss 222221.2 Test MSE 63880.97190806032 Test RE 0.44220311684867747\n",
      "3 Train Loss 201602.2 Test MSE 55046.36582673254 Test RE 0.4104879075306705\n",
      "4 Train Loss 177882.6 Test MSE 47360.860906988826 Test RE 0.3807551936136863\n",
      "5 Train Loss 158396.11 Test MSE 41825.457322267364 Test RE 0.35781322718601494\n",
      "6 Train Loss 151115.14 Test MSE 39360.92826632275 Test RE 0.34711126464627334\n",
      "7 Train Loss 146365.47 Test MSE 37653.55682397235 Test RE 0.3394994266743815\n",
      "8 Train Loss 135070.53 Test MSE 36392.6416790676 Test RE 0.3337665676090924\n",
      "9 Train Loss 130452.766 Test MSE 33487.76806262245 Test RE 0.32016889961251355\n",
      "10 Train Loss 123857.21 Test MSE 30841.601179318197 Test RE 0.30725892041942604\n",
      "11 Train Loss 119034.32 Test MSE 29129.368274104243 Test RE 0.2986080936736739\n",
      "12 Train Loss 116564.914 Test MSE 28209.95785568613 Test RE 0.2938578254663534\n",
      "13 Train Loss 113003.305 Test MSE 27225.71548770996 Test RE 0.2886859798646068\n",
      "14 Train Loss 109590.55 Test MSE 26202.698164065674 Test RE 0.2832103031692114\n",
      "15 Train Loss 105299.6 Test MSE 25696.56511088522 Test RE 0.28046171096307465\n",
      "16 Train Loss 102707.59 Test MSE 24852.814163886917 Test RE 0.27581877674848126\n",
      "17 Train Loss 99796.39 Test MSE 23084.382076290432 Test RE 0.2658245998253481\n",
      "18 Train Loss 97109.97 Test MSE 22271.78554774878 Test RE 0.26110402095748025\n",
      "19 Train Loss 94837.336 Test MSE 21822.322011810924 Test RE 0.2584559423657469\n",
      "20 Train Loss 92018.11 Test MSE 20669.563697752747 Test RE 0.2515368955404511\n",
      "21 Train Loss 89065.66 Test MSE 19033.506725275594 Test RE 0.24137675629294758\n",
      "22 Train Loss 85433.65 Test MSE 17628.062568954305 Test RE 0.23229418244598765\n",
      "23 Train Loss 82257.04 Test MSE 16999.03701678912 Test RE 0.2281120364456182\n",
      "24 Train Loss 79502.09 Test MSE 16554.07409190296 Test RE 0.22510673513578378\n",
      "25 Train Loss 77555.53 Test MSE 16451.423083770664 Test RE 0.22440771184140829\n",
      "26 Train Loss 75493.734 Test MSE 15826.544600131601 Test RE 0.2201045877296787\n",
      "27 Train Loss 74296.35 Test MSE 16248.479893904312 Test RE 0.22301928057424422\n",
      "28 Train Loss 73409.76 Test MSE 16054.6993514454 Test RE 0.22168541970745848\n",
      "29 Train Loss 71648.625 Test MSE 15631.226766866892 Test RE 0.2187421989481247\n",
      "30 Train Loss 69830.14 Test MSE 15223.041569805542 Test RE 0.2158672497784039\n",
      "31 Train Loss 68289.37 Test MSE 14660.700859468141 Test RE 0.21184265367339625\n",
      "32 Train Loss 66845.43 Test MSE 14142.998383765316 Test RE 0.20806871627982773\n",
      "33 Train Loss 65924.83 Test MSE 14137.822166410622 Test RE 0.20803063710336828\n",
      "34 Train Loss 64886.387 Test MSE 13485.76926884014 Test RE 0.20317670098130686\n",
      "35 Train Loss 63582.72 Test MSE 13176.460872659009 Test RE 0.2008331639351466\n",
      "36 Train Loss 62171.688 Test MSE 12535.142015852436 Test RE 0.19588477006979976\n",
      "37 Train Loss 61128.703 Test MSE 12074.591518967258 Test RE 0.19225261937815175\n",
      "38 Train Loss 60073.188 Test MSE 12096.53683769132 Test RE 0.1924272476351255\n",
      "39 Train Loss 59313.97 Test MSE 11531.265547900643 Test RE 0.18787739442625195\n",
      "40 Train Loss 58307.22 Test MSE 10688.902808506544 Test RE 0.18088502153639344\n",
      "41 Train Loss 57409.63 Test MSE 10342.014592519126 Test RE 0.17792567207579738\n",
      "42 Train Loss 56802.957 Test MSE 10184.481082212262 Test RE 0.17656535613916388\n",
      "43 Train Loss 56141.066 Test MSE 9888.563188140082 Test RE 0.17398132674592115\n",
      "44 Train Loss 55007.895 Test MSE 9524.826914417388 Test RE 0.17075152379817465\n",
      "45 Train Loss 53830.76 Test MSE 9738.99187841606 Test RE 0.1726605196576766\n",
      "46 Train Loss 53237.055 Test MSE 9188.208772532633 Test RE 0.16770710771254332\n",
      "47 Train Loss 52479.176 Test MSE 8904.914186138672 Test RE 0.16510145901446646\n",
      "48 Train Loss 51738.805 Test MSE 8479.480630582142 Test RE 0.1611093212863124\n",
      "49 Train Loss 51190.63 Test MSE 8549.658703471076 Test RE 0.16177463588748642\n",
      "50 Train Loss 50607.13 Test MSE 8360.018077196286 Test RE 0.15997040713339838\n",
      "51 Train Loss 49905.707 Test MSE 8057.661807792325 Test RE 0.15705094702675187\n",
      "52 Train Loss 49179.684 Test MSE 8025.927925775679 Test RE 0.1567413812339432\n",
      "53 Train Loss 48384.805 Test MSE 7860.854694518353 Test RE 0.1551211179853016\n",
      "54 Train Loss 48091.367 Test MSE 7897.373181563771 Test RE 0.1554810168062154\n",
      "55 Train Loss 47749.324 Test MSE 7795.396795896479 Test RE 0.1544739155277995\n",
      "56 Train Loss 47381.117 Test MSE 7680.301468535831 Test RE 0.1533293079613467\n",
      "57 Train Loss 47127.06 Test MSE 7775.378809948072 Test RE 0.15427544940257204\n",
      "58 Train Loss 46864.582 Test MSE 7550.849376056173 Test RE 0.15203162775376913\n",
      "59 Train Loss 46587.703 Test MSE 7643.477591522376 Test RE 0.15296129092121835\n",
      "60 Train Loss 46371.21 Test MSE 7685.053789661605 Test RE 0.15337673822355188\n",
      "61 Train Loss 46285.723 Test MSE 7693.759461982902 Test RE 0.15346358665241375\n",
      "62 Train Loss 46108.746 Test MSE 7646.58454747735 Test RE 0.1529923759674548\n",
      "63 Train Loss 45895.43 Test MSE 7641.750617038587 Test RE 0.15294400983639111\n",
      "64 Train Loss 45751.98 Test MSE 7600.980364920892 Test RE 0.15253547091586134\n",
      "65 Train Loss 45582.344 Test MSE 7550.4553021367565 Test RE 0.152027660486105\n",
      "66 Train Loss 45380.406 Test MSE 7457.118009342355 Test RE 0.15108506987985557\n",
      "67 Train Loss 45120.69 Test MSE 7510.197713114621 Test RE 0.1516218275478019\n",
      "68 Train Loss 44898.81 Test MSE 7710.150490721659 Test RE 0.15362697152043417\n",
      "69 Train Loss 44689.5 Test MSE 7826.678880941524 Test RE 0.15478354877362924\n",
      "70 Train Loss 44547.727 Test MSE 7644.110173105748 Test RE 0.1529676204023041\n",
      "71 Train Loss 44403.92 Test MSE 7683.183963631931 Test RE 0.1533580782849355\n",
      "72 Train Loss 44293.105 Test MSE 7661.070620252738 Test RE 0.15313722560743442\n",
      "73 Train Loss 44182.82 Test MSE 7505.733440530032 Test RE 0.15157675671125326\n",
      "74 Train Loss 44070.965 Test MSE 7456.612198563854 Test RE 0.1510799457989003\n",
      "75 Train Loss 43984.613 Test MSE 7361.716732664836 Test RE 0.15011551933862974\n",
      "76 Train Loss 43838.273 Test MSE 7474.908022910419 Test RE 0.15126517996485495\n",
      "77 Train Loss 43650.82 Test MSE 7313.685757229388 Test RE 0.14962500916265356\n",
      "78 Train Loss 43277.344 Test MSE 7104.006491557557 Test RE 0.14746457946644614\n",
      "79 Train Loss 43084.65 Test MSE 6917.777074550629 Test RE 0.14551887321291782\n",
      "80 Train Loss 42705.73 Test MSE 6782.298829879063 Test RE 0.1440869015387292\n",
      "81 Train Loss 42592.547 Test MSE 6820.514567512359 Test RE 0.14449226943296414\n",
      "82 Train Loss 42498.63 Test MSE 6808.6170107354865 Test RE 0.14436618984547378\n",
      "83 Train Loss 42370.312 Test MSE 6731.979385173917 Test RE 0.14355139940109593\n",
      "84 Train Loss 42303.547 Test MSE 6735.924888454403 Test RE 0.1435934598057909\n",
      "85 Train Loss 42229.17 Test MSE 6673.449332996532 Test RE 0.1429259954274123\n",
      "86 Train Loss 42187.582 Test MSE 6686.020559007409 Test RE 0.14306055175133772\n",
      "87 Train Loss 42138.04 Test MSE 6650.6907692039695 Test RE 0.14268207596230958\n",
      "88 Train Loss 42067.465 Test MSE 6674.291322903687 Test RE 0.14293501163806707\n",
      "89 Train Loss 41969.098 Test MSE 6601.211768590947 Test RE 0.14215033073165292\n",
      "90 Train Loss 41851.9 Test MSE 6583.564264201157 Test RE 0.14196019310816288\n",
      "91 Train Loss 41748.52 Test MSE 6570.8979017431975 Test RE 0.14182356613162647\n",
      "92 Train Loss 41659.36 Test MSE 6511.359343783133 Test RE 0.14117957611059975\n",
      "93 Train Loss 41576.914 Test MSE 6514.270094818446 Test RE 0.1412111281000364\n",
      "94 Train Loss 41500.78 Test MSE 6488.507438656302 Test RE 0.1409316203666968\n",
      "95 Train Loss 41418.445 Test MSE 6467.714210076249 Test RE 0.14070562271469325\n",
      "96 Train Loss 41368.67 Test MSE 6424.328768354398 Test RE 0.140232901807957\n",
      "97 Train Loss 41305.527 Test MSE 6448.751416667851 Test RE 0.140499202783778\n",
      "98 Train Loss 41150.99 Test MSE 6346.214963099044 Test RE 0.13937774417699839\n",
      "99 Train Loss 41009.53 Test MSE 6284.350557920209 Test RE 0.13869673684157496\n",
      "100 Train Loss 40892.305 Test MSE 6225.066958380624 Test RE 0.13804098686417104\n",
      "101 Train Loss 40761.758 Test MSE 6178.348429393217 Test RE 0.1375220191627836\n",
      "102 Train Loss 40688.46 Test MSE 6112.9717305574195 Test RE 0.13679248387847998\n",
      "103 Train Loss 40562.125 Test MSE 6056.747606375063 Test RE 0.1361619555251007\n",
      "104 Train Loss 40426.17 Test MSE 6095.998449244394 Test RE 0.13660244281956022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105 Train Loss 40340.695 Test MSE 6046.97485620623 Test RE 0.13605206040762904\n",
      "106 Train Loss 40172.355 Test MSE 6118.638293467566 Test RE 0.1368558706974113\n",
      "107 Train Loss 40059.13 Test MSE 6041.185130565204 Test RE 0.13598691272964564\n",
      "108 Train Loss 39743.016 Test MSE 5821.918698782527 Test RE 0.1334962634571688\n",
      "109 Train Loss 39597.74 Test MSE 5687.114200141181 Test RE 0.13194168207609575\n",
      "110 Train Loss 39418.35 Test MSE 5693.24975645842 Test RE 0.13201283568474886\n",
      "111 Train Loss 39281.89 Test MSE 5636.381626451793 Test RE 0.13135186306535465\n",
      "112 Train Loss 39123.65 Test MSE 5631.360593769387 Test RE 0.1312933442330361\n",
      "113 Train Loss 38976.754 Test MSE 5554.994106662762 Test RE 0.13040007557876754\n",
      "114 Train Loss 38797.375 Test MSE 5601.441153174747 Test RE 0.13094409870691165\n",
      "115 Train Loss 38480.316 Test MSE 5403.676710968704 Test RE 0.12861177172501023\n",
      "116 Train Loss 38240.746 Test MSE 5374.805421104299 Test RE 0.12826773182127518\n",
      "117 Train Loss 38111.09 Test MSE 5304.377544163121 Test RE 0.12742459319050173\n",
      "118 Train Loss 37796.094 Test MSE 5313.048421966816 Test RE 0.12752869889450982\n",
      "119 Train Loss 37561.953 Test MSE 5174.3596278101295 Test RE 0.12585322437479166\n",
      "120 Train Loss 37386.55 Test MSE 5265.551590770534 Test RE 0.12695738777565363\n",
      "121 Train Loss 37279.562 Test MSE 5244.142106928265 Test RE 0.12669902351550755\n",
      "122 Train Loss 37143.25 Test MSE 5283.174140813532 Test RE 0.12716965842461736\n",
      "123 Train Loss 36842.61 Test MSE 5292.900814598474 Test RE 0.1272866684788244\n",
      "124 Train Loss 36605.645 Test MSE 5268.864053131025 Test RE 0.12699731478455137\n",
      "125 Train Loss 36421.348 Test MSE 5236.7269953554205 Test RE 0.12660941689201685\n",
      "126 Train Loss 36042.492 Test MSE 5249.760380030607 Test RE 0.12676687438129985\n",
      "127 Train Loss 35812.75 Test MSE 5516.093999931288 Test RE 0.12994269538552528\n",
      "128 Train Loss 35318.79 Test MSE 5485.55434890382 Test RE 0.12958248464944497\n",
      "129 Train Loss 34943.3 Test MSE 5679.19259003346 Test RE 0.1318497589370417\n",
      "130 Train Loss 34359.25 Test MSE 5709.440319127549 Test RE 0.13220041261977117\n",
      "131 Train Loss 33963.617 Test MSE 5845.943655744389 Test RE 0.1337714253219183\n",
      "132 Train Loss 33836.31 Test MSE 6045.133312597644 Test RE 0.13603134220667526\n",
      "133 Train Loss 33617.633 Test MSE 5773.972421345402 Test RE 0.1329454243042697\n",
      "134 Train Loss 33352.41 Test MSE 5682.769128076303 Test RE 0.1318912693675448\n",
      "135 Train Loss 33123.227 Test MSE 5640.298692704452 Test RE 0.13139749735907158\n",
      "136 Train Loss 32883.516 Test MSE 5549.564708149233 Test RE 0.130336334101805\n",
      "137 Train Loss 32530.086 Test MSE 5327.494294378112 Test RE 0.1277019527983299\n",
      "138 Train Loss 32355.45 Test MSE 5239.520717034617 Test RE 0.12664318457845586\n",
      "139 Train Loss 32138.875 Test MSE 5107.81876682244 Test RE 0.1250413867661006\n",
      "140 Train Loss 31979.305 Test MSE 4929.76058455771 Test RE 0.12284258757472032\n",
      "141 Train Loss 31753.658 Test MSE 5020.40968469872 Test RE 0.12396686577772849\n",
      "142 Train Loss 31554.568 Test MSE 4980.060015171661 Test RE 0.12346769206032507\n",
      "143 Train Loss 31326.047 Test MSE 4894.198195349905 Test RE 0.12239870364689932\n",
      "144 Train Loss 31134.318 Test MSE 4786.507982460195 Test RE 0.12104460445702692\n",
      "145 Train Loss 30948.908 Test MSE 4809.245142150255 Test RE 0.12133176054472328\n",
      "146 Train Loss 30781.768 Test MSE 4856.416585094063 Test RE 0.12192534935063022\n",
      "147 Train Loss 30535.854 Test MSE 4784.978215172473 Test RE 0.12102525999187823\n",
      "148 Train Loss 30237.756 Test MSE 4832.959295829519 Test RE 0.12163053319166539\n",
      "149 Train Loss 30081.799 Test MSE 4767.523003176198 Test RE 0.1208043131480476\n",
      "150 Train Loss 29765.549 Test MSE 4858.481074158941 Test RE 0.1219512621610884\n",
      "151 Train Loss 29505.658 Test MSE 4850.030593044821 Test RE 0.12184515952054221\n",
      "152 Train Loss 29279.453 Test MSE 5231.191116980847 Test RE 0.12654247816627012\n",
      "153 Train Loss 29094.746 Test MSE 5248.479272741766 Test RE 0.12675140587759612\n",
      "154 Train Loss 28757.555 Test MSE 5256.660459576091 Test RE 0.12685015573359376\n",
      "155 Train Loss 28523.346 Test MSE 5051.426657006653 Test RE 0.12434922065206512\n",
      "156 Train Loss 28309.05 Test MSE 5001.662529333457 Test RE 0.12373519148349064\n",
      "157 Train Loss 28129.09 Test MSE 5116.774902850139 Test RE 0.1251509635974719\n",
      "158 Train Loss 27772.84 Test MSE 5149.053458141568 Test RE 0.1255450928501476\n",
      "159 Train Loss 27540.379 Test MSE 5081.1045669460955 Test RE 0.12471397111096554\n",
      "160 Train Loss 27434.896 Test MSE 5148.765387582358 Test RE 0.12554158090846276\n",
      "161 Train Loss 27067.084 Test MSE 5168.620846732602 Test RE 0.12578341433815107\n",
      "162 Train Loss 26923.96 Test MSE 5103.738429379311 Test RE 0.12499143266525886\n",
      "163 Train Loss 26865.055 Test MSE 5073.633607634781 Test RE 0.12462225131511138\n",
      "164 Train Loss 26766.512 Test MSE 4980.3035153256 Test RE 0.12347071050131177\n",
      "165 Train Loss 26463.102 Test MSE 5018.833416968074 Test RE 0.12394740319172504\n",
      "166 Train Loss 26328.812 Test MSE 4996.203369410535 Test RE 0.12366764648077798\n",
      "167 Train Loss 26145.736 Test MSE 5036.573767269819 Test RE 0.12416627184887476\n",
      "168 Train Loss 26067.572 Test MSE 5041.727274726763 Test RE 0.12422978012196824\n",
      "169 Train Loss 25949.018 Test MSE 5056.681028861158 Test RE 0.12441387636925505\n",
      "170 Train Loss 25642.914 Test MSE 5137.748112525694 Test RE 0.12540719268952344\n",
      "171 Train Loss 25435.219 Test MSE 5163.939775995396 Test RE 0.12572644223156942\n",
      "172 Train Loss 25258.652 Test MSE 5367.977225335066 Test RE 0.1281862297519366\n",
      "173 Train Loss 25095.328 Test MSE 5588.019270336798 Test RE 0.13078712388706548\n",
      "174 Train Loss 24915.05 Test MSE 5562.028934366856 Test RE 0.13048261858427218\n",
      "175 Train Loss 24616.521 Test MSE 5802.627047347068 Test RE 0.13327490169443598\n",
      "176 Train Loss 24483.23 Test MSE 5805.23882551645 Test RE 0.13330489201746573\n",
      "177 Train Loss 24389.613 Test MSE 5791.606784691093 Test RE 0.13314828469682558\n",
      "178 Train Loss 24213.262 Test MSE 5778.2820165663215 Test RE 0.13299502915643593\n",
      "179 Train Loss 23833.768 Test MSE 6139.318690240252 Test RE 0.13708695530529327\n",
      "180 Train Loss 23653.94 Test MSE 6184.070569892453 Test RE 0.13758568813557892\n",
      "181 Train Loss 23375.682 Test MSE 6000.834663032046 Test RE 0.1355320078909195\n",
      "182 Train Loss 23193.254 Test MSE 6199.794262653201 Test RE 0.13776049061125614\n",
      "183 Train Loss 22957.64 Test MSE 6086.644429220448 Test RE 0.13649759760458266\n",
      "184 Train Loss 22860.693 Test MSE 5882.70957860023 Test RE 0.13419141921533276\n",
      "185 Train Loss 22752.639 Test MSE 5836.405175032343 Test RE 0.13366224730514287\n",
      "186 Train Loss 22666.73 Test MSE 5747.934094406046 Test RE 0.13264532000067647\n",
      "187 Train Loss 22440.838 Test MSE 5847.964904103028 Test RE 0.13379454921108352\n",
      "188 Train Loss 22360.303 Test MSE 5783.335133141425 Test RE 0.1330531686279646\n",
      "189 Train Loss 22253.496 Test MSE 5750.959024312636 Test RE 0.13268021862797819\n",
      "190 Train Loss 22211.027 Test MSE 5694.466515288134 Test RE 0.13202694179364258\n",
      "191 Train Loss 22032.082 Test MSE 5731.279096928287 Test RE 0.13245300654634898\n",
      "192 Train Loss 21873.297 Test MSE 5843.16594524917 Test RE 0.13373964068085165\n",
      "193 Train Loss 21691.812 Test MSE 5686.2915390793105 Test RE 0.13193213881689275\n",
      "194 Train Loss 21435.367 Test MSE 5875.4660883252745 Test RE 0.13410877756642417\n",
      "195 Train Loss 21294.31 Test MSE 5703.400038157132 Test RE 0.13213046364897843\n",
      "196 Train Loss 21139.24 Test MSE 5768.88062362656 Test RE 0.1328867921821213\n",
      "197 Train Loss 21081.084 Test MSE 5743.120097943768 Test RE 0.1325897619647947\n",
      "198 Train Loss 21015.666 Test MSE 5816.758504728046 Test RE 0.13343708886450678\n",
      "199 Train Loss 20952.525 Test MSE 5915.087385619409 Test RE 0.1345602001189959\n",
      "Training time: 187.18\n",
      "Training time: 187.18\n",
      "ES_stan\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 244512.22 Test MSE 72882.15128727636 Test RE 0.47233119949362823\n",
      "1 Train Loss 221325.4 Test MSE 63647.5268593802 Test RE 0.44139438924028956\n",
      "2 Train Loss 183421.97 Test MSE 51621.73889717503 Test RE 0.39751393421305403\n",
      "3 Train Loss 155774.12 Test MSE 42292.83156526332 Test RE 0.35980684671735735\n",
      "4 Train Loss 139853.56 Test MSE 38793.67267162123 Test RE 0.34460096585548977\n",
      "5 Train Loss 129766.02 Test MSE 37013.917059224244 Test RE 0.3366034520407117\n",
      "6 Train Loss 120336.79 Test MSE 34826.66999579811 Test RE 0.32650663960867843\n",
      "7 Train Loss 110877.086 Test MSE 31380.10548544978 Test RE 0.30992973249561745\n",
      "8 Train Loss 104167.28 Test MSE 27729.79161784279 Test RE 0.29134619116591526\n",
      "9 Train Loss 98127.24 Test MSE 25876.787043922235 Test RE 0.2814434965908649\n",
      "10 Train Loss 91963.11 Test MSE 24607.555901194908 Test RE 0.27445445329265555\n",
      "11 Train Loss 86931.24 Test MSE 22303.066185215546 Test RE 0.2612873164191224\n",
      "12 Train Loss 82460.33 Test MSE 19408.13520018684 Test RE 0.24374063952340547\n",
      "13 Train Loss 78049.73 Test MSE 19297.570856934526 Test RE 0.24304537658350495\n",
      "14 Train Loss 74876.46 Test MSE 16886.849487898282 Test RE 0.22735806170732953\n",
      "15 Train Loss 72606.61 Test MSE 16731.687681819316 Test RE 0.22631113176602324\n",
      "16 Train Loss 70818.836 Test MSE 16718.556385405056 Test RE 0.22622230803498575\n",
      "17 Train Loss 68863.086 Test MSE 16066.837982805266 Test RE 0.2217692097889924\n",
      "18 Train Loss 66708.67 Test MSE 15787.29049356416 Test RE 0.2198314588438579\n",
      "19 Train Loss 65446.145 Test MSE 15254.785954398298 Test RE 0.2160922049653818\n",
      "20 Train Loss 63419.87 Test MSE 14256.759058170608 Test RE 0.20890385141943532\n",
      "21 Train Loss 62275.49 Test MSE 14516.366756445132 Test RE 0.21079728252619573\n",
      "22 Train Loss 61612.953 Test MSE 14399.224264877224 Test RE 0.20994502596130585\n",
      "23 Train Loss 60726.77 Test MSE 13883.085086551659 Test RE 0.2061479567072437\n",
      "24 Train Loss 60005.258 Test MSE 13586.529139717253 Test RE 0.2039343129623335\n",
      "25 Train Loss 59505.973 Test MSE 14081.569564453375 Test RE 0.20761636083936766\n",
      "26 Train Loss 59139.242 Test MSE 13826.767415360602 Test RE 0.2057294052589015\n",
      "27 Train Loss 58443.14 Test MSE 13102.29774091924 Test RE 0.2002671760773171\n",
      "28 Train Loss 57872.82 Test MSE 12621.418811124684 Test RE 0.19655773131326734\n",
      "29 Train Loss 57309.316 Test MSE 12562.222734368697 Test RE 0.19609624906048717\n",
      "30 Train Loss 56908.46 Test MSE 12229.01964814644 Test RE 0.1934781219921181\n",
      "31 Train Loss 56498.22 Test MSE 11852.941285901434 Test RE 0.19047987982523154\n",
      "32 Train Loss 56148.664 Test MSE 11700.906058313889 Test RE 0.189254314100935\n",
      "33 Train Loss 55521.445 Test MSE 11691.132013415607 Test RE 0.1891752532740746\n",
      "34 Train Loss 54989.61 Test MSE 11574.793047014631 Test RE 0.18823165434349615\n",
      "35 Train Loss 54328.105 Test MSE 11718.971015947374 Test RE 0.18940035204762198\n",
      "36 Train Loss 53868.67 Test MSE 11653.769341633195 Test RE 0.18887272700858176\n",
      "37 Train Loss 53454.66 Test MSE 11257.094728222159 Test RE 0.18563044346405286\n",
      "38 Train Loss 53130.66 Test MSE 11208.75021928222 Test RE 0.18523141206576135\n",
      "39 Train Loss 52930.66 Test MSE 10956.766678920829 Test RE 0.18313748619852052\n",
      "40 Train Loss 52771.914 Test MSE 11010.267866719822 Test RE 0.18358406599718766\n",
      "41 Train Loss 52603.27 Test MSE 11070.563512966255 Test RE 0.18408606134507\n",
      "42 Train Loss 52498.184 Test MSE 10989.106871683554 Test RE 0.18340756302997013\n",
      "43 Train Loss 52331.156 Test MSE 11060.771643443735 Test RE 0.184004631639427\n",
      "44 Train Loss 52079.33 Test MSE 10907.392790625465 Test RE 0.18272438899229426\n",
      "45 Train Loss 51879.74 Test MSE 10936.620018431548 Test RE 0.18296903749071897\n",
      "46 Train Loss 51677.227 Test MSE 10900.115105331679 Test RE 0.18266341967914704\n",
      "47 Train Loss 51531.94 Test MSE 10807.857768879023 Test RE 0.181888755808597\n",
      "48 Train Loss 51389.598 Test MSE 10907.901783676594 Test RE 0.1827286523557329\n",
      "49 Train Loss 51280.82 Test MSE 10711.27906484142 Test RE 0.18107425581099892\n",
      "50 Train Loss 51138.445 Test MSE 10497.52022102691 Test RE 0.17925835296406314\n",
      "51 Train Loss 51009.434 Test MSE 10325.73326248376 Test RE 0.17778556360910303\n",
      "52 Train Loss 50827.85 Test MSE 10300.8770878406 Test RE 0.17757145137539698\n",
      "53 Train Loss 50687.363 Test MSE 9867.78948807016 Test RE 0.17379848238288012\n",
      "54 Train Loss 50579.207 Test MSE 9988.788551747886 Test RE 0.17486079628753676\n",
      "55 Train Loss 50460.7 Test MSE 9910.438942972049 Test RE 0.1741736635992208\n",
      "56 Train Loss 50317.496 Test MSE 9618.612194198231 Test RE 0.1715901087110882\n",
      "57 Train Loss 50131.73 Test MSE 9752.123144488554 Test RE 0.17277688116216025\n",
      "58 Train Loss 49933.59 Test MSE 9610.98849620067 Test RE 0.1715220941964047\n",
      "59 Train Loss 49787.26 Test MSE 9625.38606981851 Test RE 0.17165051895847505\n",
      "60 Train Loss 49609.086 Test MSE 9258.391129232195 Test RE 0.16834638844212793\n",
      "61 Train Loss 49446.72 Test MSE 9109.189533780136 Test RE 0.16698440409779022\n",
      "62 Train Loss 49314.18 Test MSE 9008.497156222715 Test RE 0.16605892216721901\n",
      "63 Train Loss 49226.31 Test MSE 9101.38576714382 Test RE 0.1669128616995037\n",
      "64 Train Loss 49168.336 Test MSE 9065.18216247417 Test RE 0.16658055688769255\n",
      "65 Train Loss 48877.066 Test MSE 9055.690105908428 Test RE 0.16649332167445524\n",
      "66 Train Loss 48697.84 Test MSE 8908.066726912994 Test RE 0.16513068125134006\n",
      "67 Train Loss 48538.062 Test MSE 8886.92215721856 Test RE 0.1649345841782408\n",
      "68 Train Loss 48450.11 Test MSE 8891.817732977079 Test RE 0.16498000702360724\n",
      "69 Train Loss 48232.008 Test MSE 8817.294168630553 Test RE 0.16428719210225337\n",
      "70 Train Loss 48072.85 Test MSE 8673.640930735206 Test RE 0.1629433952759394\n",
      "71 Train Loss 47930.836 Test MSE 8709.597730902035 Test RE 0.1632807889739134\n",
      "72 Train Loss 47716.645 Test MSE 8626.597099728524 Test RE 0.16250091075028858\n",
      "73 Train Loss 47534.74 Test MSE 8566.516213104416 Test RE 0.16193404427002686\n",
      "74 Train Loss 47437.008 Test MSE 8502.621680204511 Test RE 0.16132901036721067\n",
      "75 Train Loss 47284.438 Test MSE 8510.781142322534 Test RE 0.16140640074995907\n",
      "76 Train Loss 47189.766 Test MSE 8468.037773799013 Test RE 0.1610005779948748\n",
      "77 Train Loss 47021.535 Test MSE 8613.724247336599 Test RE 0.16237962125596186\n",
      "78 Train Loss 46795.914 Test MSE 8652.812649940326 Test RE 0.16274763719146756\n",
      "79 Train Loss 46574.816 Test MSE 8727.549405279098 Test RE 0.1634489743952338\n",
      "80 Train Loss 46328.95 Test MSE 8517.328665700252 Test RE 0.1614684754870325\n",
      "81 Train Loss 46140.16 Test MSE 8425.210089550046 Test RE 0.16059292615390355\n",
      "82 Train Loss 45922.758 Test MSE 8141.316461555892 Test RE 0.15786409350695374\n",
      "83 Train Loss 45766.45 Test MSE 8112.862974492569 Test RE 0.15758798856275105\n",
      "84 Train Loss 45530.96 Test MSE 8026.612648646466 Test RE 0.15674806719727083\n",
      "85 Train Loss 45432.46 Test MSE 7968.471190726362 Test RE 0.15617932635269677\n",
      "86 Train Loss 45282.805 Test MSE 7575.073906586404 Test RE 0.1522753053521782\n",
      "87 Train Loss 45070.652 Test MSE 7569.133069274208 Test RE 0.15221558182567574\n",
      "88 Train Loss 44974.37 Test MSE 7515.0352039381805 Test RE 0.151670651237561\n",
      "89 Train Loss 44900.95 Test MSE 7530.236694844928 Test RE 0.15182397422160776\n",
      "90 Train Loss 44774.473 Test MSE 7399.778823843289 Test RE 0.15050308819384714\n",
      "91 Train Loss 44640.902 Test MSE 7253.838032267708 Test RE 0.14901156259878962\n",
      "92 Train Loss 44553.562 Test MSE 7224.1406673480005 Test RE 0.14870622153005983\n",
      "93 Train Loss 44297.113 Test MSE 6888.104703593186 Test RE 0.14520645131271598\n",
      "94 Train Loss 44136.312 Test MSE 6794.9226682396775 Test RE 0.14422093309627676\n",
      "95 Train Loss 43945.27 Test MSE 6951.989730444341 Test RE 0.1458782694982835\n",
      "96 Train Loss 43819.15 Test MSE 6836.83868524152 Test RE 0.1446650789149781\n",
      "97 Train Loss 43736.152 Test MSE 6895.543750092343 Test RE 0.14528484051395393\n",
      "98 Train Loss 43589.105 Test MSE 6722.680738970132 Test RE 0.14345222389438395\n",
      "99 Train Loss 43468.438 Test MSE 6637.478826206035 Test RE 0.14254028282630601\n",
      "100 Train Loss 43387.543 Test MSE 6544.863263335195 Test RE 0.14154232679927742\n",
      "101 Train Loss 43274.863 Test MSE 6475.76149466415 Test RE 0.1407931301526507\n",
      "102 Train Loss 43120.273 Test MSE 6380.219895546778 Test RE 0.13975065927658603\n",
      "103 Train Loss 42965.254 Test MSE 6335.324307752708 Test RE 0.13925810067599617\n",
      "104 Train Loss 42848.508 Test MSE 6308.398935134426 Test RE 0.13896185943261688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105 Train Loss 42627.52 Test MSE 6254.717702447945 Test RE 0.1383693492487201\n",
      "106 Train Loss 42539.184 Test MSE 6219.336444891667 Test RE 0.13797743510829705\n",
      "107 Train Loss 42470.832 Test MSE 6154.075452143205 Test RE 0.13725161082323248\n",
      "108 Train Loss 42346.062 Test MSE 6175.562899787509 Test RE 0.13749101453045978\n",
      "109 Train Loss 42221.793 Test MSE 6143.939173287115 Test RE 0.1371385317792626\n",
      "110 Train Loss 42022.004 Test MSE 6101.121548432827 Test RE 0.1366598313587024\n",
      "111 Train Loss 41899.39 Test MSE 6127.331079843907 Test RE 0.1369530521740969\n",
      "112 Train Loss 41773.684 Test MSE 6117.123214999086 Test RE 0.13683892573267378\n",
      "113 Train Loss 41673.23 Test MSE 6142.648603644135 Test RE 0.13712412765543786\n",
      "114 Train Loss 41592.125 Test MSE 6184.691997523777 Test RE 0.1375926008478042\n",
      "115 Train Loss 41510.875 Test MSE 6182.403723056422 Test RE 0.1375671445466869\n",
      "116 Train Loss 41411.37 Test MSE 6200.543606535882 Test RE 0.13776881563443297\n",
      "117 Train Loss 41324.035 Test MSE 6171.252067455421 Test RE 0.13744301856719587\n",
      "118 Train Loss 41253.43 Test MSE 6179.121387500923 Test RE 0.13753062141599315\n",
      "119 Train Loss 41179.582 Test MSE 6196.45980323422 Test RE 0.13772343949898994\n",
      "120 Train Loss 41138.062 Test MSE 6203.489320387229 Test RE 0.1378015369042297\n",
      "121 Train Loss 41088.285 Test MSE 6200.222604582296 Test RE 0.13776524944458313\n",
      "122 Train Loss 41065.668 Test MSE 6177.784367042587 Test RE 0.13751574137148936\n",
      "123 Train Loss 40999.97 Test MSE 6165.61314560815 Test RE 0.13738021060239772\n",
      "124 Train Loss 40964.54 Test MSE 6146.597623855422 Test RE 0.1371681981356878\n",
      "125 Train Loss 40940.742 Test MSE 6134.87414609607 Test RE 0.13703732444489675\n",
      "126 Train Loss 40912.98 Test MSE 6135.248112656658 Test RE 0.13704150110726346\n",
      "127 Train Loss 40884.754 Test MSE 6120.553049728228 Test RE 0.13687728274408228\n",
      "128 Train Loss 40866.67 Test MSE 6119.183903148525 Test RE 0.13686197240009668\n",
      "129 Train Loss 40853.402 Test MSE 6104.3273123435765 Test RE 0.1366957298093823\n",
      "130 Train Loss 40819.266 Test MSE 6079.614173951378 Test RE 0.1364187454372306\n",
      "131 Train Loss 40794.113 Test MSE 6068.779620786989 Test RE 0.13629713449271608\n",
      "132 Train Loss 40767.11 Test MSE 6061.189069251245 Test RE 0.13621187071737761\n",
      "133 Train Loss 40741.188 Test MSE 6061.625871203671 Test RE 0.13621677870972884\n",
      "134 Train Loss 40718.93 Test MSE 6050.157755295005 Test RE 0.1360878620291352\n",
      "135 Train Loss 40699.32 Test MSE 6039.376529137155 Test RE 0.1359665554214562\n",
      "136 Train Loss 40673.246 Test MSE 6029.039318213485 Test RE 0.13585014299852255\n",
      "137 Train Loss 40649.047 Test MSE 6034.923921058319 Test RE 0.13591642463335624\n",
      "138 Train Loss 40621.137 Test MSE 6013.622610836577 Test RE 0.135676342301107\n",
      "139 Train Loss 40597.832 Test MSE 6002.354302886895 Test RE 0.1355491677374072\n",
      "140 Train Loss 40567.703 Test MSE 5989.578605690928 Test RE 0.1354048362389968\n",
      "141 Train Loss 40535.33 Test MSE 5989.520287480531 Test RE 0.13540417704513105\n",
      "142 Train Loss 40510.668 Test MSE 5998.400645387445 Test RE 0.13550451831858615\n",
      "143 Train Loss 40477.336 Test MSE 5970.074981225706 Test RE 0.135184199812782\n",
      "144 Train Loss 40422.22 Test MSE 5981.101254967524 Test RE 0.1353089796845778\n",
      "145 Train Loss 40378.816 Test MSE 5945.349667806637 Test RE 0.13490397388165404\n",
      "146 Train Loss 40343.434 Test MSE 5950.232473194984 Test RE 0.1349593595777308\n",
      "147 Train Loss 40319.953 Test MSE 5925.050452857821 Test RE 0.13467347556076276\n",
      "148 Train Loss 40288.15 Test MSE 5910.163726179241 Test RE 0.13450418517920396\n",
      "149 Train Loss 40252.77 Test MSE 5906.607316828093 Test RE 0.1344637105031041\n",
      "150 Train Loss 40231.266 Test MSE 5902.53300494011 Test RE 0.1344173267186305\n",
      "151 Train Loss 40179.844 Test MSE 5863.972456723999 Test RE 0.1339775410464639\n",
      "152 Train Loss 40129.82 Test MSE 5856.036208832464 Test RE 0.13388684834734135\n",
      "153 Train Loss 40087.96 Test MSE 5837.924542220302 Test RE 0.1336796440426532\n",
      "154 Train Loss 40049.055 Test MSE 5823.693691104856 Test RE 0.13351661214262378\n",
      "155 Train Loss 40002.367 Test MSE 5809.544177070156 Test RE 0.1333543144504143\n",
      "156 Train Loss 39909.94 Test MSE 5767.8831325853225 Test RE 0.13287530302817965\n",
      "157 Train Loss 39795.965 Test MSE 5707.051177291857 Test RE 0.1321727497870325\n",
      "158 Train Loss 39714.367 Test MSE 5712.588029877826 Test RE 0.1322368497570259\n",
      "159 Train Loss 39616.832 Test MSE 5707.918279373693 Test RE 0.13218279025364585\n",
      "160 Train Loss 39530.684 Test MSE 5704.1295718077035 Test RE 0.13213891391869234\n",
      "161 Train Loss 39457.01 Test MSE 5680.799254465767 Test RE 0.13186840800857733\n",
      "162 Train Loss 39368.85 Test MSE 5648.766196372487 Test RE 0.1314960906772943\n",
      "163 Train Loss 39319.26 Test MSE 5614.621057364226 Test RE 0.1310980605734\n",
      "164 Train Loss 39210.023 Test MSE 5603.658717982828 Test RE 0.13097001599171326\n",
      "165 Train Loss 39118.277 Test MSE 5604.197196437005 Test RE 0.13097630856243694\n",
      "166 Train Loss 38974.27 Test MSE 5605.916565339211 Test RE 0.13099639880141617\n",
      "167 Train Loss 38910.387 Test MSE 5613.190125642771 Test RE 0.13108135380677574\n",
      "168 Train Loss 38847.066 Test MSE 5614.267163959759 Test RE 0.13109392890809282\n",
      "169 Train Loss 38801.6 Test MSE 5617.183875283806 Test RE 0.13112797729656298\n",
      "170 Train Loss 38682.85 Test MSE 5596.926466469347 Test RE 0.13089131847299149\n",
      "171 Train Loss 38561.6 Test MSE 5603.196469438338 Test RE 0.13096461399001486\n",
      "172 Train Loss 38358.062 Test MSE 5616.651723261595 Test RE 0.13112176585034793\n",
      "173 Train Loss 38283.164 Test MSE 5650.990634726765 Test RE 0.1315219791763507\n",
      "174 Train Loss 38229.69 Test MSE 5630.113743085845 Test RE 0.13127880846892181\n",
      "175 Train Loss 38165.4 Test MSE 5548.252766085584 Test RE 0.13032092714331012\n",
      "176 Train Loss 38069.293 Test MSE 5532.4412975287505 Test RE 0.1301350996874058\n",
      "177 Train Loss 37750.363 Test MSE 5438.351762510542 Test RE 0.1290237586590627\n",
      "178 Train Loss 37559.2 Test MSE 5428.18557103437 Test RE 0.1289031068637839\n",
      "179 Train Loss 37218.29 Test MSE 5402.556801624363 Test RE 0.128598443671155\n",
      "180 Train Loss 36932.91 Test MSE 5310.477204022169 Test RE 0.12749783678537627\n",
      "181 Train Loss 36656.477 Test MSE 5261.279025050811 Test RE 0.12690586953930336\n",
      "182 Train Loss 36446.934 Test MSE 5298.242881458375 Test RE 0.12735088679985254\n",
      "183 Train Loss 36314.36 Test MSE 5298.467862360554 Test RE 0.12735359064090832\n",
      "184 Train Loss 36091.453 Test MSE 5278.233262579527 Test RE 0.1271101793354917\n",
      "185 Train Loss 35858.027 Test MSE 5241.558765480681 Test RE 0.1266678127711082\n",
      "186 Train Loss 35517.055 Test MSE 5145.107570613929 Test RE 0.125496978981821\n",
      "187 Train Loss 35259.367 Test MSE 5214.782968940266 Test RE 0.1263438658063952\n",
      "188 Train Loss 35057.082 Test MSE 5377.337127891063 Test RE 0.12829793738505116\n",
      "189 Train Loss 34796.09 Test MSE 5259.317651988579 Test RE 0.12688221246322862\n",
      "190 Train Loss 34654.914 Test MSE 5169.038164429537 Test RE 0.12578849215163662\n",
      "191 Train Loss 34468.035 Test MSE 5206.898585402839 Test RE 0.12624831817659782\n",
      "192 Train Loss 34145.016 Test MSE 5248.304890024513 Test RE 0.12674930017830457\n",
      "193 Train Loss 33737.027 Test MSE 5282.384160008574 Test RE 0.1271601503767867\n",
      "194 Train Loss 33182.68 Test MSE 4964.903731334288 Test RE 0.12327966848882416\n",
      "195 Train Loss 32838.168 Test MSE 4925.105112999277 Test RE 0.12278457002556852\n",
      "196 Train Loss 32200.559 Test MSE 4933.435616751101 Test RE 0.12288836731908813\n",
      "197 Train Loss 31938.78 Test MSE 5159.417011693816 Test RE 0.12567137229942432\n",
      "198 Train Loss 31558.719 Test MSE 4860.365296526847 Test RE 0.12197490751629893\n",
      "199 Train Loss 31204.482 Test MSE 4709.60550565926 Test RE 0.12006828491190467\n",
      "Training time: 194.96\n",
      "Training time: 194.96\n",
      "ES_stan\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 254749.31 Test MSE 76171.20110621725 Test RE 0.48287135600444764\n",
      "1 Train Loss 248492.52 Test MSE 74620.0081445395 Test RE 0.4779293348218222\n",
      "2 Train Loss 247589.11 Test MSE 74165.99517408047 Test RE 0.47647317591597527\n",
      "3 Train Loss 245614.61 Test MSE 72914.25746096278 Test RE 0.4724352241246805\n",
      "4 Train Loss 244805.14 Test MSE 72555.27837353203 Test RE 0.4712708179567551\n",
      "5 Train Loss 244318.83 Test MSE 72202.5610232708 Test RE 0.4701239137561013\n",
      "6 Train Loss 243578.86 Test MSE 72183.93557777243 Test RE 0.47006327302552486\n",
      "7 Train Loss 242234.8 Test MSE 71646.02445626666 Test RE 0.4683085537492511\n",
      "8 Train Loss 240592.84 Test MSE 71054.85971542375 Test RE 0.46637250108400674\n",
      "9 Train Loss 238622.7 Test MSE 70063.28619228341 Test RE 0.463106944486505\n",
      "10 Train Loss 238307.89 Test MSE 69948.86256542253 Test RE 0.4627286291658612\n",
      "11 Train Loss 237460.34 Test MSE 69621.18446238784 Test RE 0.4616435219497371\n",
      "12 Train Loss 236884.88 Test MSE 69410.98750317963 Test RE 0.4609461091023459\n",
      "13 Train Loss 236666.8 Test MSE 69240.7763165219 Test RE 0.4603805909809602\n",
      "14 Train Loss 235461.62 Test MSE 68854.6849085465 Test RE 0.45909523949809905\n",
      "15 Train Loss 234707.0 Test MSE 68523.92707441025 Test RE 0.4579912322210239\n",
      "16 Train Loss 233572.72 Test MSE 67805.62754214699 Test RE 0.45558447032690513\n",
      "17 Train Loss 232645.36 Test MSE 67520.31069359278 Test RE 0.45462494116050955\n",
      "18 Train Loss 232405.98 Test MSE 67390.03197538412 Test RE 0.4541861357751815\n",
      "19 Train Loss 232096.03 Test MSE 67437.0679171756 Test RE 0.4543446113363676\n",
      "20 Train Loss 231653.95 Test MSE 67438.14490044404 Test RE 0.45434823930838664\n",
      "21 Train Loss 229551.69 Test MSE 66540.00283742885 Test RE 0.45131259044169486\n",
      "22 Train Loss 225506.39 Test MSE 65609.62605849655 Test RE 0.4481463082135214\n",
      "23 Train Loss 222650.25 Test MSE 64146.43345119209 Test RE 0.44312096623104885\n",
      "24 Train Loss 221151.9 Test MSE 63615.42838334227 Test RE 0.44128307404333766\n",
      "25 Train Loss 219793.17 Test MSE 62719.79391703029 Test RE 0.4381656753716916\n",
      "26 Train Loss 218149.31 Test MSE 62390.431294475515 Test RE 0.4370136837264742\n",
      "27 Train Loss 217455.1 Test MSE 62409.25003926585 Test RE 0.43707958669231767\n",
      "28 Train Loss 215271.3 Test MSE 61865.37501804721 Test RE 0.43517092056497975\n",
      "29 Train Loss 214108.77 Test MSE 61384.09831857652 Test RE 0.4334749271098454\n",
      "30 Train Loss 211430.03 Test MSE 59959.02536132284 Test RE 0.42841367413033593\n",
      "31 Train Loss 210287.73 Test MSE 59725.422675709015 Test RE 0.4275783015397253\n",
      "32 Train Loss 208115.22 Test MSE 57415.85135396839 Test RE 0.41922960728955616\n",
      "33 Train Loss 205807.38 Test MSE 55957.335656464216 Test RE 0.413870580116243\n",
      "34 Train Loss 202373.38 Test MSE 52804.563629741504 Test RE 0.4020423203865114\n",
      "35 Train Loss 198045.98 Test MSE 49312.64072871928 Test RE 0.3885216029070953\n",
      "36 Train Loss 195826.92 Test MSE 50630.823624993835 Test RE 0.39368016854372556\n",
      "37 Train Loss 193225.28 Test MSE 50437.260662529734 Test RE 0.39292692314656136\n",
      "38 Train Loss 192436.97 Test MSE 49812.88838044481 Test RE 0.39048729153887907\n",
      "39 Train Loss 191264.58 Test MSE 49120.61407188096 Test RE 0.3877644007391844\n",
      "40 Train Loss 186960.1 Test MSE 46563.54515740983 Test RE 0.3775366005553666\n",
      "41 Train Loss 183630.05 Test MSE 45966.28176619541 Test RE 0.37510748415148576\n",
      "42 Train Loss 177302.97 Test MSE 42862.48998698592 Test RE 0.36222192993005786\n",
      "43 Train Loss 174881.19 Test MSE 42085.140592015305 Test RE 0.3589222925101881\n",
      "44 Train Loss 172946.81 Test MSE 41551.74626098111 Test RE 0.35664051803229047\n",
      "45 Train Loss 170027.56 Test MSE 41526.169962309694 Test RE 0.35653073987247724\n",
      "46 Train Loss 166796.5 Test MSE 41076.306516122444 Test RE 0.35459428747241495\n",
      "47 Train Loss 163960.88 Test MSE 39226.145834760915 Test RE 0.3465164537292146\n",
      "48 Train Loss 162376.45 Test MSE 39770.05556450322 Test RE 0.34891058159327715\n",
      "49 Train Loss 159546.69 Test MSE 37771.263967924 Test RE 0.3400296597983115\n",
      "50 Train Loss 155169.34 Test MSE 35428.21258861731 Test RE 0.3293143554434596\n",
      "51 Train Loss 153190.5 Test MSE 35344.696250836445 Test RE 0.3289259734927086\n",
      "52 Train Loss 152276.83 Test MSE 34359.9010775856 Test RE 0.3243112351579047\n",
      "53 Train Loss 149119.78 Test MSE 33393.45108134267 Test RE 0.3197177101442902\n",
      "54 Train Loss 140731.84 Test MSE 32934.90953383803 Test RE 0.31751502359790656\n",
      "55 Train Loss 137225.33 Test MSE 30982.697200249295 Test RE 0.3079609517506913\n",
      "56 Train Loss 136566.52 Test MSE 30840.602912770133 Test RE 0.30725394777254394\n",
      "57 Train Loss 135333.45 Test MSE 30701.184920378524 Test RE 0.3065586752072573\n",
      "58 Train Loss 134763.78 Test MSE 30843.599829191186 Test RE 0.3072688760156025\n",
      "59 Train Loss 133877.64 Test MSE 30556.560460167813 Test RE 0.30583576793534795\n",
      "60 Train Loss 130901.91 Test MSE 30094.903343644983 Test RE 0.3035166486868149\n",
      "61 Train Loss 128417.05 Test MSE 28899.449541209757 Test RE 0.29742729898203174\n",
      "62 Train Loss 127886.8 Test MSE 29234.987240112314 Test RE 0.29914895918121737\n",
      "63 Train Loss 127091.75 Test MSE 29330.441035075197 Test RE 0.29963692985880747\n",
      "64 Train Loss 122910.78 Test MSE 28657.26631576186 Test RE 0.29617842660063354\n",
      "65 Train Loss 121606.23 Test MSE 28240.046935979863 Test RE 0.29401449984503975\n",
      "66 Train Loss 121106.28 Test MSE 27228.670550613733 Test RE 0.28870164634145745\n",
      "67 Train Loss 120345.69 Test MSE 26174.285428186686 Test RE 0.2830567128396142\n",
      "68 Train Loss 119667.58 Test MSE 25742.748240803005 Test RE 0.28071362761108254\n",
      "69 Train Loss 119226.86 Test MSE 25398.559134610114 Test RE 0.27883069528944593\n",
      "70 Train Loss 118619.12 Test MSE 25659.875618197842 Test RE 0.28026141817137284\n",
      "71 Train Loss 117827.8 Test MSE 25669.030901925507 Test RE 0.28031141147625893\n",
      "72 Train Loss 116637.92 Test MSE 26069.28935955066 Test RE 0.28248841257488283\n",
      "73 Train Loss 112646.3 Test MSE 26692.997052811 Test RE 0.285847705906018\n",
      "74 Train Loss 107589.45 Test MSE 26404.736233222167 Test RE 0.28430006475007996\n",
      "75 Train Loss 105428.55 Test MSE 26264.61113598474 Test RE 0.28354469709086927\n",
      "76 Train Loss 104870.75 Test MSE 25630.062567613346 Test RE 0.28009855931097805\n",
      "77 Train Loss 103932.65 Test MSE 24623.02159863225 Test RE 0.2745406862129547\n",
      "78 Train Loss 102632.84 Test MSE 25602.97781488862 Test RE 0.27995052210851507\n",
      "79 Train Loss 102288.83 Test MSE 27470.32962391916 Test RE 0.2899799543663993\n",
      "80 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "81 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "82 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "83 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "84 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "85 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "86 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "87 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "88 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "89 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "90 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "91 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "92 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "93 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "94 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "95 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "96 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "97 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "98 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "99 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "100 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "101 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "102 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "103 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "104 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "106 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "107 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "108 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "109 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "110 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "111 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "112 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "113 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "114 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "115 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "116 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "117 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "118 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "119 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "120 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "121 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "122 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "123 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "124 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "125 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "126 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "127 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "128 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "129 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "130 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "131 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "132 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "133 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "134 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "135 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "136 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "137 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "138 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "139 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "140 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "141 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "142 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "143 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "144 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "145 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "146 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "147 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "148 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "149 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "150 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "151 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "152 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "153 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "154 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "155 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "156 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "157 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "158 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "159 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "160 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "161 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "162 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "163 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "164 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "165 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "166 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "167 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "168 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "169 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "170 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "171 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "172 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "173 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "174 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "175 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "176 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "177 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "178 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "179 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "180 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "181 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "182 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "183 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "184 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "185 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "186 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "187 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "188 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "189 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "190 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "191 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "192 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "193 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "194 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "195 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "196 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "197 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "198 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "199 Train Loss 102203.0 Test MSE 27876.435755976916 Test RE 0.29211554197243783\n",
      "Training time: 117.17\n",
      "Training time: 117.17\n",
      "ES_stan\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 250867.11 Test MSE 75396.72392000633 Test RE 0.4804102663311015\n",
      "1 Train Loss 245536.1 Test MSE 73482.19304521481 Test RE 0.47427157500687944\n",
      "2 Train Loss 239023.8 Test MSE 70780.51939224158 Test RE 0.4654713064313761\n",
      "3 Train Loss 225780.02 Test MSE 65716.09731457153 Test RE 0.44850978657067314\n",
      "4 Train Loss 209993.34 Test MSE 59589.35125630392 Test RE 0.42709095159814797\n",
      "5 Train Loss 189816.94 Test MSE 51753.635799112366 Test RE 0.3980214472218589\n",
      "6 Train Loss 175141.12 Test MSE 46456.49607315898 Test RE 0.37710237457080686\n",
      "7 Train Loss 164808.72 Test MSE 43513.51237920587 Test RE 0.3649623901633976\n",
      "8 Train Loss 150669.97 Test MSE 37794.510779117874 Test RE 0.34013428152125275\n",
      "9 Train Loss 140147.64 Test MSE 34602.1743200199 Test RE 0.32545259346533917\n",
      "10 Train Loss 134050.25 Test MSE 32497.27977402734 Test RE 0.3153984440565077\n",
      "11 Train Loss 128632.38 Test MSE 30244.731158095376 Test RE 0.3042712411889786\n",
      "12 Train Loss 124200.3 Test MSE 29034.51258716871 Test RE 0.29812150961813394\n",
      "13 Train Loss 117779.92 Test MSE 27180.867748803365 Test RE 0.288448111927311\n",
      "14 Train Loss 109276.42 Test MSE 26625.95129197192 Test RE 0.2854884932199425\n",
      "15 Train Loss 106723.766 Test MSE 27016.388405570295 Test RE 0.28757404586680274\n",
      "16 Train Loss 103097.55 Test MSE 27818.886428199145 Test RE 0.29181385816758937\n",
      "17 Train Loss 95747.766 Test MSE 25395.924293597454 Test RE 0.2788162319964226\n",
      "18 Train Loss 91104.54 Test MSE 24351.405782343085 Test RE 0.27302226212081177\n",
      "19 Train Loss 86963.08 Test MSE 25947.739435643514 Test RE 0.2818290819312425\n",
      "20 Train Loss 85401.41 Test MSE 24522.80201987365 Test RE 0.2739814046099113\n",
      "21 Train Loss 83544.33 Test MSE 23533.65934202225 Test RE 0.2683989256754033\n",
      "22 Train Loss 81391.53 Test MSE 20462.781661208784 Test RE 0.25027552266560693\n",
      "23 Train Loss 80079.94 Test MSE 18838.56049879858 Test RE 0.24013745241177994\n",
      "24 Train Loss 78329.68 Test MSE 19380.62384102588 Test RE 0.2435678250249813\n",
      "25 Train Loss 77500.62 Test MSE 20210.04427011393 Test RE 0.2487251344298992\n",
      "26 Train Loss 75946.2 Test MSE 18372.404933476322 Test RE 0.2371477706050724\n",
      "27 Train Loss 74296.62 Test MSE 17821.618882094033 Test RE 0.23356599713025436\n",
      "28 Train Loss 73229.086 Test MSE 17427.674642887596 Test RE 0.23097010064586412\n",
      "29 Train Loss 71616.63 Test MSE 17584.939947827486 Test RE 0.23200988382881033\n",
      "30 Train Loss 70451.54 Test MSE 18151.250817173168 Test RE 0.23571614010807962\n",
      "31 Train Loss 69396.46 Test MSE 17084.24061214936 Test RE 0.2286830003168989\n",
      "32 Train Loss 68621.39 Test MSE 17058.81819347137 Test RE 0.22851278966781863\n",
      "33 Train Loss 67584.6 Test MSE 17006.594945138666 Test RE 0.22816274116599125\n",
      "34 Train Loss 66675.85 Test MSE 16269.638926750378 Test RE 0.2231644429852917\n",
      "35 Train Loss 66216.6 Test MSE 15905.552862129394 Test RE 0.22065329978167308\n",
      "36 Train Loss 65691.33 Test MSE 15859.767644180789 Test RE 0.22033548812524942\n",
      "37 Train Loss 64968.336 Test MSE 15122.508401225898 Test RE 0.215153273957987\n",
      "38 Train Loss 64068.78 Test MSE 14890.813906350368 Test RE 0.21349871229159928\n",
      "39 Train Loss 63398.957 Test MSE 15002.585480490694 Test RE 0.214298483044253\n",
      "40 Train Loss 62875.24 Test MSE 15137.28068541531 Test RE 0.2152583335627538\n",
      "41 Train Loss 61785.535 Test MSE 13347.93003257357 Test RE 0.20213569060057887\n",
      "42 Train Loss 61394.15 Test MSE 13284.605198392419 Test RE 0.20165563763485206\n",
      "43 Train Loss 60573.938 Test MSE 13380.239774472566 Test RE 0.2023801856113375\n",
      "44 Train Loss 60125.61 Test MSE 12911.031883582662 Test RE 0.1988000634389248\n",
      "45 Train Loss 59807.023 Test MSE 12786.78215089772 Test RE 0.1978411714613511\n",
      "46 Train Loss 59154.15 Test MSE 12376.438894073957 Test RE 0.1946408053244302\n",
      "47 Train Loss 58708.39 Test MSE 11624.17876396497 Test RE 0.1886327872445248\n",
      "48 Train Loss 58281.387 Test MSE 11965.429327304731 Test RE 0.19138160165917323\n",
      "49 Train Loss 57813.42 Test MSE 11674.218493875578 Test RE 0.18903836416032546\n",
      "50 Train Loss 56985.938 Test MSE 11459.525298354196 Test RE 0.18729205537360066\n",
      "51 Train Loss 56700.81 Test MSE 11490.738459938691 Test RE 0.1875469525734055\n",
      "52 Train Loss 56397.266 Test MSE 11462.476450320679 Test RE 0.18731617032260522\n",
      "53 Train Loss 56289.34 Test MSE 11623.102683022638 Test RE 0.18862405592468343\n",
      "54 Train Loss 56036.312 Test MSE 11645.476164777385 Test RE 0.1888055112585506\n",
      "55 Train Loss 55579.23 Test MSE 11756.932136453315 Test RE 0.18970686515749227\n",
      "56 Train Loss 55463.113 Test MSE 11676.034291476482 Test RE 0.18905306501854258\n",
      "57 Train Loss 55347.16 Test MSE 11649.710099327847 Test RE 0.18883983005844127\n",
      "58 Train Loss 55159.92 Test MSE 11667.096564971482 Test RE 0.18898069335412016\n",
      "59 Train Loss 54932.85 Test MSE 11541.951811392255 Test RE 0.18796442921016152\n",
      "60 Train Loss 54627.766 Test MSE 11749.029753944018 Test RE 0.18964309902402093\n",
      "61 Train Loss 54385.777 Test MSE 11741.616261551162 Test RE 0.18958325835842632\n",
      "62 Train Loss 54160.434 Test MSE 11642.573497856063 Test RE 0.18878197964598778\n",
      "63 Train Loss 54023.715 Test MSE 11595.683704021794 Test RE 0.18840144183888136\n",
      "64 Train Loss 53863.28 Test MSE 11208.302868329438 Test RE 0.1852277156556417\n",
      "65 Train Loss 53742.87 Test MSE 11297.01977183373 Test RE 0.18595933577689425\n",
      "66 Train Loss 53604.402 Test MSE 11467.496974916035 Test RE 0.1873571877454576\n",
      "67 Train Loss 53352.008 Test MSE 11299.273915858157 Test RE 0.18597788749344943\n",
      "68 Train Loss 53118.773 Test MSE 11085.292976401872 Test RE 0.18420848454385758\n",
      "69 Train Loss 52982.42 Test MSE 11071.799307143167 Test RE 0.18409633571520795\n",
      "70 Train Loss 52888.82 Test MSE 11094.542697156301 Test RE 0.18428532156385613\n",
      "71 Train Loss 52696.75 Test MSE 11362.959819553686 Test RE 0.18650126304463008\n",
      "72 Train Loss 52590.55 Test MSE 11122.414521380706 Test RE 0.18451665810319956\n",
      "73 Train Loss 52552.227 Test MSE 11055.198462644194 Test RE 0.18395826867651593\n",
      "74 Train Loss 52449.84 Test MSE 10808.506936108488 Test RE 0.1818942182438554\n",
      "75 Train Loss 52351.508 Test MSE 10843.574197506916 Test RE 0.18218904929051064\n",
      "76 Train Loss 52233.227 Test MSE 10639.59292298582 Test RE 0.18046731120664955\n",
      "77 Train Loss 52156.89 Test MSE 10490.963813430766 Test RE 0.1792023647701052\n",
      "78 Train Loss 52016.246 Test MSE 10372.767187462272 Test RE 0.17819001199343576\n",
      "79 Train Loss 51890.887 Test MSE 10322.675117856239 Test RE 0.17775923452358255\n",
      "80 Train Loss 51803.81 Test MSE 10289.03590178564 Test RE 0.17746936001021127\n",
      "81 Train Loss 51747.445 Test MSE 10218.675926230531 Test RE 0.1768615207329073\n",
      "82 Train Loss 51652.36 Test MSE 10128.56702884812 Test RE 0.17608000630744933\n",
      "83 Train Loss 51502.73 Test MSE 10155.219571461303 Test RE 0.17631152457700358\n",
      "84 Train Loss 51398.363 Test MSE 10000.648986573475 Test RE 0.1749645781325952\n",
      "85 Train Loss 51323.617 Test MSE 9978.058726389996 Test RE 0.17476685446859355\n",
      "86 Train Loss 51254.812 Test MSE 9991.499306044012 Test RE 0.17488452151196507\n",
      "87 Train Loss 51159.39 Test MSE 9914.950789973349 Test RE 0.1742133064195511\n",
      "88 Train Loss 51047.473 Test MSE 9682.618193019742 Test RE 0.1721600758571224\n",
      "89 Train Loss 50987.395 Test MSE 9625.404722676281 Test RE 0.17165068527757857\n",
      "90 Train Loss 50901.52 Test MSE 9502.299513610067 Test RE 0.17054947996272835\n",
      "91 Train Loss 50856.957 Test MSE 9625.298121109556 Test RE 0.17164973475740977\n",
      "92 Train Loss 50799.723 Test MSE 9666.987094207236 Test RE 0.17202105673784726\n",
      "93 Train Loss 50760.86 Test MSE 9706.869605336115 Test RE 0.17237553999501973\n",
      "94 Train Loss 50720.53 Test MSE 9695.105038491445 Test RE 0.17227105016100017\n",
      "95 Train Loss 50670.617 Test MSE 9833.329393260286 Test RE 0.17349474920626856\n",
      "96 Train Loss 50618.387 Test MSE 9741.303648183202 Test RE 0.17268101087946927\n",
      "97 Train Loss 50557.25 Test MSE 9883.158516700796 Test RE 0.17393377482110817\n",
      "98 Train Loss 50508.793 Test MSE 9935.031025410748 Test RE 0.17438962977563563\n",
      "99 Train Loss 50475.79 Test MSE 9806.766049391903 Test RE 0.17326025501415956\n",
      "100 Train Loss 50426.316 Test MSE 9778.494622175773 Test RE 0.1730103331725557\n",
      "101 Train Loss 50396.883 Test MSE 9694.717192104037 Test RE 0.172267604330737\n",
      "102 Train Loss 50370.434 Test MSE 9680.329104080894 Test RE 0.17213972428422364\n",
      "103 Train Loss 50333.203 Test MSE 9627.245632163947 Test RE 0.1716670990428586\n",
      "104 Train Loss 50292.258 Test MSE 9614.786893642671 Test RE 0.17155598481678339\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105 Train Loss 50241.754 Test MSE 9556.340044946022 Test RE 0.17103375841145635\n",
      "106 Train Loss 50210.3 Test MSE 9499.034995535201 Test RE 0.17052018128158572\n",
      "107 Train Loss 50158.125 Test MSE 9595.151489134409 Test RE 0.17138071869882648\n",
      "108 Train Loss 50107.8 Test MSE 9710.317508391681 Test RE 0.17240615137682264\n",
      "109 Train Loss 50076.84 Test MSE 9763.02402543954 Test RE 0.17287341881652288\n",
      "110 Train Loss 50051.18 Test MSE 9766.569932617027 Test RE 0.1729048095744313\n",
      "111 Train Loss 50020.04 Test MSE 9791.75229426877 Test RE 0.17312757705452714\n",
      "112 Train Loss 49988.402 Test MSE 9702.977402605102 Test RE 0.17234097747158383\n",
      "113 Train Loss 49961.492 Test MSE 9675.92020296947 Test RE 0.17210051934352624\n",
      "114 Train Loss 49933.11 Test MSE 9706.858877994866 Test RE 0.17237544474640576\n",
      "115 Train Loss 49909.746 Test MSE 9711.412906820608 Test RE 0.17241587547191342\n",
      "116 Train Loss 49882.85 Test MSE 9683.992263904878 Test RE 0.1721722911356282\n",
      "117 Train Loss 49839.16 Test MSE 9644.502431847906 Test RE 0.17182088645560667\n",
      "118 Train Loss 49806.324 Test MSE 9622.48619750713 Test RE 0.17162466014720237\n",
      "119 Train Loss 49707.363 Test MSE 9573.518520254605 Test RE 0.17118741451754724\n",
      "120 Train Loss 49637.273 Test MSE 9341.848707493475 Test RE 0.1691034455533302\n",
      "121 Train Loss 49588.406 Test MSE 9192.05946714087 Test RE 0.16774224629202794\n",
      "122 Train Loss 49538.637 Test MSE 9262.57791250515 Test RE 0.16838444852015397\n",
      "123 Train Loss 49517.895 Test MSE 9248.334640966474 Test RE 0.16825493446085876\n",
      "124 Train Loss 49478.24 Test MSE 9328.908538813543 Test RE 0.16898628537030272\n",
      "125 Train Loss 49433.54 Test MSE 9148.513617989163 Test RE 0.16734444914608232\n",
      "126 Train Loss 49401.875 Test MSE 9144.825114192892 Test RE 0.1673107107219971\n",
      "127 Train Loss 49360.582 Test MSE 9162.65225440369 Test RE 0.1674737110666719\n",
      "128 Train Loss 49321.395 Test MSE 9148.578917815292 Test RE 0.16734504637664835\n",
      "129 Train Loss 49293.574 Test MSE 9052.428189622204 Test RE 0.16646333300513047\n",
      "130 Train Loss 49263.945 Test MSE 9070.604079881356 Test RE 0.16663036565025177\n",
      "131 Train Loss 49249.97 Test MSE 9066.871680706998 Test RE 0.1665960793433325\n",
      "132 Train Loss 49222.51 Test MSE 9044.983187679192 Test RE 0.16639486658206432\n",
      "133 Train Loss 49188.074 Test MSE 9009.89369682965 Test RE 0.1660717932952635\n",
      "134 Train Loss 49168.83 Test MSE 9052.370145384653 Test RE 0.1664627993222042\n",
      "135 Train Loss 49128.797 Test MSE 9016.798382845616 Test RE 0.1661354152437939\n",
      "136 Train Loss 49090.61 Test MSE 9000.107826930334 Test RE 0.16598158143824832\n",
      "137 Train Loss 49066.695 Test MSE 9012.437405547584 Test RE 0.1660952346626492\n",
      "138 Train Loss 49032.89 Test MSE 8941.16604694862 Test RE 0.16543718128471718\n",
      "139 Train Loss 48996.79 Test MSE 8825.128972079538 Test RE 0.16436016642002327\n",
      "140 Train Loss 48955.297 Test MSE 8829.12788557746 Test RE 0.16439740030357264\n",
      "141 Train Loss 48885.734 Test MSE 8957.994039899218 Test RE 0.16559279113585676\n",
      "142 Train Loss 48827.926 Test MSE 8889.996266290693 Test RE 0.16496310828665778\n",
      "143 Train Loss 48731.22 Test MSE 8794.625118406584 Test RE 0.1640758669865769\n",
      "144 Train Loss 48681.008 Test MSE 8724.954486014638 Test RE 0.16342467384968323\n",
      "145 Train Loss 48650.445 Test MSE 8744.289116129079 Test RE 0.16360564940926642\n",
      "146 Train Loss 48600.402 Test MSE 8818.976266680582 Test RE 0.16430286210155165\n",
      "147 Train Loss 48550.24 Test MSE 8785.379291292678 Test RE 0.16398959747533226\n",
      "148 Train Loss 48520.535 Test MSE 8846.093788848777 Test RE 0.16455527613329568\n",
      "149 Train Loss 48498.21 Test MSE 8848.06484201468 Test RE 0.16457360790437386\n",
      "150 Train Loss 48470.69 Test MSE 8828.611048365103 Test RE 0.16439258850683186\n",
      "151 Train Loss 48412.58 Test MSE 8776.369251210546 Test RE 0.16390548432404195\n",
      "152 Train Loss 48347.45 Test MSE 8763.786065719629 Test RE 0.16378794181597112\n",
      "153 Train Loss 48313.65 Test MSE 8860.639852956778 Test RE 0.16469051370085377\n",
      "154 Train Loss 48298.496 Test MSE 8869.715366450271 Test RE 0.16477483427339817\n",
      "155 Train Loss 48245.285 Test MSE 8818.767410326 Test RE 0.16430091652992512\n",
      "156 Train Loss 48186.69 Test MSE 8796.092276353647 Test RE 0.16408955234382966\n",
      "157 Train Loss 48125.01 Test MSE 8586.712005362031 Test RE 0.16212481383136434\n",
      "158 Train Loss 48082.324 Test MSE 8465.692297400614 Test RE 0.1609782794873559\n",
      "159 Train Loss 48059.617 Test MSE 8485.8073680916 Test RE 0.1611694137832012\n",
      "160 Train Loss 48027.715 Test MSE 8488.047953603185 Test RE 0.16119068989776913\n",
      "161 Train Loss 48001.16 Test MSE 8541.519878824909 Test RE 0.16169761708853564\n",
      "162 Train Loss 47962.2 Test MSE 8525.920517795428 Test RE 0.16154989559110933\n",
      "163 Train Loss 47932.57 Test MSE 8452.01029590502 Test RE 0.16084814271832218\n",
      "164 Train Loss 47887.977 Test MSE 8459.153188428529 Test RE 0.16091609568552823\n",
      "165 Train Loss 47858.438 Test MSE 8494.605517758178 Test RE 0.16125294297533782\n",
      "166 Train Loss 47834.53 Test MSE 8431.3505384933 Test RE 0.16065143704231907\n",
      "167 Train Loss 47788.69 Test MSE 8477.729729935554 Test RE 0.16109268695513201\n",
      "168 Train Loss 47731.414 Test MSE 8500.980406344555 Test RE 0.1613134388248836\n",
      "169 Train Loss 47640.11 Test MSE 8446.171066088864 Test RE 0.16079257064049424\n",
      "170 Train Loss 47611.758 Test MSE 8432.864664712743 Test RE 0.160665861518692\n",
      "171 Train Loss 47592.055 Test MSE 8447.153287040497 Test RE 0.1608019198023319\n",
      "172 Train Loss 47548.74 Test MSE 8382.307243054944 Test RE 0.1601835187227122\n",
      "173 Train Loss 47468.152 Test MSE 8385.450291419278 Test RE 0.16021354729036785\n",
      "174 Train Loss 47360.47 Test MSE 8258.801378103597 Test RE 0.15899905847928425\n",
      "175 Train Loss 47305.812 Test MSE 8175.76159527 Test RE 0.15819769498441252\n",
      "176 Train Loss 47257.23 Test MSE 8107.122348464416 Test RE 0.15753222441385573\n",
      "177 Train Loss 47213.99 Test MSE 8100.392981617673 Test RE 0.15746683054424104\n",
      "178 Train Loss 47176.633 Test MSE 8169.0430242139155 Test RE 0.1581326808026534\n",
      "179 Train Loss 47119.484 Test MSE 8099.8191982563985 Test RE 0.15746125344152656\n",
      "180 Train Loss 47083.438 Test MSE 8029.856045624697 Test RE 0.15677973341077078\n",
      "181 Train Loss 47042.836 Test MSE 7942.993049029456 Test RE 0.15592944499424416\n",
      "182 Train Loss 46980.03 Test MSE 8007.191016551216 Test RE 0.15655831423433336\n",
      "183 Train Loss 46906.668 Test MSE 8115.225176695951 Test RE 0.1576109291449971\n",
      "184 Train Loss 46857.934 Test MSE 8095.798699380205 Test RE 0.15742216915112864\n",
      "185 Train Loss 46809.87 Test MSE 8013.563417429635 Test RE 0.15662059911828538\n",
      "186 Train Loss 46761.367 Test MSE 8023.071456698859 Test RE 0.15671348621938155\n",
      "187 Train Loss 46734.246 Test MSE 8005.71948943035 Test RE 0.156543927766567\n",
      "188 Train Loss 46711.77 Test MSE 7976.356268962931 Test RE 0.1562565796726141\n",
      "189 Train Loss 46689.68 Test MSE 8057.693055332531 Test RE 0.15705125154753843\n",
      "190 Train Loss 46624.07 Test MSE 7939.793807193238 Test RE 0.155898039562898\n",
      "191 Train Loss 46569.58 Test MSE 8021.8370334856345 Test RE 0.15670142985120167\n",
      "192 Train Loss 46538.51 Test MSE 8066.559051783598 Test RE 0.15713763067715059\n",
      "193 Train Loss 46527.293 Test MSE 8091.472172215867 Test RE 0.15738009903937694\n",
      "194 Train Loss 46483.074 Test MSE 8079.317402747399 Test RE 0.15726184875657487\n",
      "195 Train Loss 46427.848 Test MSE 8108.162318568614 Test RE 0.15754232809449617\n",
      "196 Train Loss 46324.043 Test MSE 7858.545614753847 Test RE 0.15509833335460907\n",
      "197 Train Loss 46269.543 Test MSE 7714.2168300806625 Test RE 0.15366747767246228\n",
      "198 Train Loss 46185.3 Test MSE 7561.685254991378 Test RE 0.15214067546671825\n",
      "199 Train Loss 46110.77 Test MSE 7551.127763291658 Test RE 0.1520344303043403\n",
      "Training time: 265.31\n",
      "Training time: 265.31\n",
      "ES_stan\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 252317.06 Test MSE 76264.61068152769 Test RE 0.4831673404971544\n",
      "1 Train Loss 248965.75 Test MSE 74627.1953003775 Test RE 0.47795235056510227\n",
      "2 Train Loss 246193.67 Test MSE 73558.37350922666 Test RE 0.4745173547514162\n",
      "3 Train Loss 239651.61 Test MSE 70984.01547061729 Test RE 0.4661399480443788\n",
      "4 Train Loss 227678.81 Test MSE 66682.64149662563 Test RE 0.4517960602174256\n",
      "5 Train Loss 208597.16 Test MSE 58647.26297168537 Test RE 0.42370141676622475\n",
      "6 Train Loss 184043.08 Test MSE 49610.80512410935 Test RE 0.3896944130173328\n",
      "7 Train Loss 159783.28 Test MSE 38213.50815129841 Test RE 0.34201448255141653\n",
      "8 Train Loss 148455.6 Test MSE 34804.96666357562 Test RE 0.32640488732476747\n",
      "9 Train Loss 136918.5 Test MSE 31976.832147197136 Test RE 0.31286267948611995\n",
      "10 Train Loss 120662.84 Test MSE 27301.031478178764 Test RE 0.28908500802412607\n",
      "11 Train Loss 111102.08 Test MSE 24900.855439032457 Test RE 0.27608523125016843\n",
      "12 Train Loss 98051.7 Test MSE 20998.593383556356 Test RE 0.25353104344032046\n",
      "13 Train Loss 89740.83 Test MSE 20254.34126340314 Test RE 0.2489975669094229\n",
      "14 Train Loss 83428.26 Test MSE 17738.063092611214 Test RE 0.23301782246931407\n",
      "15 Train Loss 78652.89 Test MSE 17360.05697892243 Test RE 0.23052159454347068\n",
      "16 Train Loss 74737.22 Test MSE 17565.212204229196 Test RE 0.2318797066452257\n",
      "17 Train Loss 72384.74 Test MSE 17499.208120181233 Test RE 0.23144363413165248\n",
      "18 Train Loss 69540.766 Test MSE 15783.274641660897 Test RE 0.21980349753122572\n",
      "19 Train Loss 66750.984 Test MSE 14367.232894281326 Test RE 0.2097116743688572\n",
      "20 Train Loss 65207.652 Test MSE 14441.608032538437 Test RE 0.21025378297152855\n",
      "21 Train Loss 63594.234 Test MSE 13450.913315883497 Test RE 0.20291396107461135\n",
      "22 Train Loss 62390.934 Test MSE 13218.687151604703 Test RE 0.20115470907108102\n",
      "23 Train Loss 61547.92 Test MSE 12613.693451413294 Test RE 0.19649756725282158\n",
      "24 Train Loss 59986.246 Test MSE 12051.186912913696 Test RE 0.19206620398845675\n",
      "25 Train Loss 59292.17 Test MSE 11977.72213531497 Test RE 0.19147988535987856\n",
      "26 Train Loss 57693.324 Test MSE 10914.683960885162 Test RE 0.18278545088339762\n",
      "27 Train Loss 56745.35 Test MSE 10921.680154984973 Test RE 0.18284402324511706\n",
      "28 Train Loss 56357.543 Test MSE 10704.506559661308 Test RE 0.18101700213316496\n",
      "29 Train Loss 55944.02 Test MSE 11016.04815235881 Test RE 0.1836322496160518\n",
      "30 Train Loss 55554.13 Test MSE 10745.913046711361 Test RE 0.18136676341869604\n",
      "31 Train Loss 54829.195 Test MSE 9854.348781882549 Test RE 0.17368007844206754\n",
      "32 Train Loss 53927.89 Test MSE 9800.227931277346 Test RE 0.17320248954507678\n",
      "33 Train Loss 53267.184 Test MSE 9972.79843485251 Test RE 0.174720781087664\n",
      "34 Train Loss 52604.812 Test MSE 9518.591864707229 Test RE 0.1706956267967197\n",
      "35 Train Loss 51709.156 Test MSE 8907.222884438881 Test RE 0.16512285982459307\n",
      "36 Train Loss 50269.207 Test MSE 7493.423328238015 Test RE 0.15145240559933823\n",
      "37 Train Loss 49407.633 Test MSE 7279.466777393999 Test RE 0.14927456902731925\n",
      "38 Train Loss 48792.56 Test MSE 7206.540304556722 Test RE 0.14852496263348314\n",
      "39 Train Loss 48165.24 Test MSE 6873.244785120549 Test RE 0.14504973759200968\n",
      "40 Train Loss 47340.6 Test MSE 6603.172588280603 Test RE 0.14217144128541953\n",
      "41 Train Loss 46189.984 Test MSE 6173.992634049439 Test RE 0.13747353343936208\n",
      "42 Train Loss 45450.45 Test MSE 6114.076057600423 Test RE 0.1368048393108273\n",
      "43 Train Loss 45015.598 Test MSE 6139.980070646543 Test RE 0.137094339201541\n",
      "44 Train Loss 44500.555 Test MSE 6142.307596922718 Test RE 0.13712032140660255\n",
      "45 Train Loss 44211.617 Test MSE 6173.669045139824 Test RE 0.13746993078737332\n",
      "46 Train Loss 43906.66 Test MSE 6190.134445043457 Test RE 0.13765312737358018\n",
      "47 Train Loss 43784.977 Test MSE 6309.724712618534 Test RE 0.13897646082575074\n",
      "48 Train Loss 43695.992 Test MSE 6303.275838364192 Test RE 0.13890542200236408\n",
      "49 Train Loss 43536.547 Test MSE 6372.9581692384645 Test RE 0.1396711071623355\n",
      "50 Train Loss 43421.973 Test MSE 6464.623428679701 Test RE 0.14067199860302507\n",
      "51 Train Loss 43238.17 Test MSE 6383.143678585491 Test RE 0.13978267649649292\n",
      "52 Train Loss 43070.816 Test MSE 6329.335581090761 Test RE 0.13919226538159848\n",
      "53 Train Loss 42942.023 Test MSE 6334.457294138461 Test RE 0.1392485713453348\n",
      "54 Train Loss 42801.113 Test MSE 6310.371400821329 Test RE 0.13898358254175816\n",
      "55 Train Loss 42755.055 Test MSE 6291.138727903235 Test RE 0.13877162468479703\n",
      "56 Train Loss 42690.71 Test MSE 6273.120377894543 Test RE 0.13857275539053712\n",
      "57 Train Loss 42610.203 Test MSE 6178.457526063398 Test RE 0.1375232333325822\n",
      "58 Train Loss 42552.004 Test MSE 6179.783254987028 Test RE 0.1375379869137334\n",
      "59 Train Loss 42361.027 Test MSE 6218.727155924206 Test RE 0.13797067633356366\n",
      "60 Train Loss 42279.22 Test MSE 6168.843774605378 Test RE 0.1374161978074409\n",
      "61 Train Loss 42182.734 Test MSE 6123.427802776345 Test RE 0.13690942380953827\n",
      "62 Train Loss 42123.59 Test MSE 6104.954037425432 Test RE 0.1367027468351129\n",
      "63 Train Loss 42037.63 Test MSE 6058.844417111033 Test RE 0.13618552272344617\n",
      "64 Train Loss 41888.19 Test MSE 6070.414402547343 Test RE 0.13631549082463876\n",
      "65 Train Loss 41795.47 Test MSE 6081.124721035567 Test RE 0.136435691754377\n",
      "66 Train Loss 41679.89 Test MSE 6161.614552730584 Test RE 0.13733565569669975\n",
      "67 Train Loss 41575.383 Test MSE 6100.636801496233 Test RE 0.13665440229537631\n",
      "68 Train Loss 41456.137 Test MSE 6097.51803026561 Test RE 0.1366194675569237\n",
      "69 Train Loss 41349.805 Test MSE 6030.257664595441 Test RE 0.13586386858237098\n",
      "70 Train Loss 41280.527 Test MSE 5958.6755966418295 Test RE 0.13505507639275866\n",
      "71 Train Loss 41189.977 Test MSE 5842.3128285708035 Test RE 0.1337298771653231\n",
      "72 Train Loss 41089.047 Test MSE 5818.204464789153 Test RE 0.13345367307646114\n",
      "73 Train Loss 40891.812 Test MSE 5759.013776001784 Test RE 0.13277310159470315\n",
      "74 Train Loss 40724.523 Test MSE 5772.797150671465 Test RE 0.13293189334164182\n",
      "75 Train Loss 40623.594 Test MSE 5724.861086185198 Test RE 0.13237882389673528\n",
      "76 Train Loss 40513.547 Test MSE 5722.021918638353 Test RE 0.1323459940808243\n",
      "77 Train Loss 40398.25 Test MSE 5728.340246332428 Test RE 0.1324190429664944\n",
      "78 Train Loss 40240.207 Test MSE 5667.492816626452 Test RE 0.1317138762753026\n",
      "79 Train Loss 40157.977 Test MSE 5604.165892853345 Test RE 0.13097594276217916\n",
      "80 Train Loss 40023.82 Test MSE 5611.95955106073 Test RE 0.1310669845960203\n",
      "81 Train Loss 39946.613 Test MSE 5520.31322905324 Test RE 0.1299923821026821\n",
      "82 Train Loss 39790.42 Test MSE 5483.400334592983 Test RE 0.12955704055439304\n",
      "83 Train Loss 39669.773 Test MSE 5482.843522716889 Test RE 0.12955046245268248\n",
      "84 Train Loss 39536.32 Test MSE 5497.412162569353 Test RE 0.1297224646204217\n",
      "85 Train Loss 39432.168 Test MSE 5487.812861001207 Test RE 0.12960915775054982\n",
      "86 Train Loss 39374.934 Test MSE 5460.032768881389 Test RE 0.12928069150789637\n",
      "87 Train Loss 39308.707 Test MSE 5453.389223361782 Test RE 0.12920201582796162\n",
      "88 Train Loss 39217.47 Test MSE 5460.710663824153 Test RE 0.12928871673509132\n",
      "89 Train Loss 39126.582 Test MSE 5486.643049584345 Test RE 0.12959534292512598\n",
      "90 Train Loss 38984.17 Test MSE 5493.850021928888 Test RE 0.12968042988403194\n",
      "91 Train Loss 38863.87 Test MSE 5561.08407038362 Test RE 0.13047153507817946\n",
      "92 Train Loss 38761.184 Test MSE 5539.153080621773 Test RE 0.13021401365761154\n",
      "93 Train Loss 38658.453 Test MSE 5504.718758166221 Test RE 0.12980864288307287\n",
      "94 Train Loss 38548.863 Test MSE 5522.155708328775 Test RE 0.13001407365156864\n",
      "95 Train Loss 38424.953 Test MSE 5439.349815088315 Test RE 0.12903559741030293\n",
      "96 Train Loss 38364.918 Test MSE 5398.75564058005 Test RE 0.12855319570570753\n",
      "97 Train Loss 38316.54 Test MSE 5426.015414510532 Test RE 0.1288773369375512\n",
      "98 Train Loss 38210.76 Test MSE 5401.414190983829 Test RE 0.12858484402514928\n",
      "99 Train Loss 38147.977 Test MSE 5471.2791022153915 Test RE 0.12941376633499557\n",
      "100 Train Loss 38104.348 Test MSE 5411.6633273985435 Test RE 0.12870678052038528\n",
      "101 Train Loss 37989.06 Test MSE 5442.056199867052 Test RE 0.1290676946793\n",
      "102 Train Loss 37809.17 Test MSE 5484.66771491233 Test RE 0.12957201197237467\n",
      "103 Train Loss 37749.62 Test MSE 5549.783751457179 Test RE 0.13033890628733313\n",
      "104 Train Loss 37662.02 Test MSE 5672.782909128326 Test RE 0.1317753334392277\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105 Train Loss 37538.562 Test MSE 5708.550406623633 Test RE 0.13219010938702822\n",
      "106 Train Loss 37402.68 Test MSE 5809.758987369398 Test RE 0.13335677984309618\n",
      "107 Train Loss 37187.332 Test MSE 5718.914012537539 Test RE 0.1323100474525842\n",
      "108 Train Loss 36835.906 Test MSE 5811.930681373866 Test RE 0.13338170196643948\n",
      "109 Train Loss 36612.047 Test MSE 5575.834590995923 Test RE 0.1306444553717356\n",
      "110 Train Loss 36349.316 Test MSE 5420.310551824828 Test RE 0.12880956889702186\n",
      "111 Train Loss 36253.164 Test MSE 5274.947623897403 Test RE 0.1270706108742956\n",
      "112 Train Loss 35982.473 Test MSE 5042.231221230696 Test RE 0.12423598866872382\n",
      "113 Train Loss 35233.242 Test MSE 5074.416838031796 Test RE 0.12463187007911897\n",
      "114 Train Loss 35053.1 Test MSE 4886.609799157408 Test RE 0.12230377796896628\n",
      "115 Train Loss 34961.15 Test MSE 4763.969783779546 Test RE 0.12075928722611805\n",
      "116 Train Loss 34679.863 Test MSE 4727.167243208422 Test RE 0.12029193906453849\n",
      "117 Train Loss 34089.242 Test MSE 4636.743016544716 Test RE 0.11913587397083693\n",
      "118 Train Loss 33872.23 Test MSE 4669.34689611181 Test RE 0.11955400015830643\n",
      "119 Train Loss 33496.117 Test MSE 4509.812951249437 Test RE 0.1174938964592377\n",
      "120 Train Loss 33325.258 Test MSE 4506.609041443281 Test RE 0.11745215340542706\n",
      "121 Train Loss 32990.43 Test MSE 4428.263567851222 Test RE 0.11642674955046953\n",
      "122 Train Loss 32907.41 Test MSE 4469.177314621561 Test RE 0.11696335970800355\n",
      "123 Train Loss 32471.057 Test MSE 4245.839604630442 Test RE 0.11400340785989746\n",
      "124 Train Loss 32046.055 Test MSE 4305.76342060439 Test RE 0.11480508482530186\n",
      "125 Train Loss 31964.742 Test MSE 4314.176846866398 Test RE 0.11491719418195107\n",
      "126 Train Loss 31674.121 Test MSE 4292.457029285932 Test RE 0.11462755260798811\n",
      "127 Train Loss 31293.014 Test MSE 4197.248608890466 Test RE 0.11334918159202392\n",
      "128 Train Loss 31005.955 Test MSE 4309.578345388353 Test RE 0.11485593246277855\n",
      "129 Train Loss 30867.19 Test MSE 4299.853758537686 Test RE 0.11472627276053485\n",
      "130 Train Loss 30676.3 Test MSE 4251.765398861383 Test RE 0.1140829357329147\n",
      "131 Train Loss 30467.56 Test MSE 4219.551183895396 Test RE 0.11364992972179948\n",
      "132 Train Loss 30191.176 Test MSE 4083.951172797236 Test RE 0.11180888392743749\n",
      "133 Train Loss 29878.498 Test MSE 4053.420772023534 Test RE 0.1113901749911766\n",
      "134 Train Loss 29774.969 Test MSE 4062.2736527393795 Test RE 0.11151174959918148\n",
      "135 Train Loss 29695.57 Test MSE 4136.620643120973 Test RE 0.11252755672961115\n",
      "136 Train Loss 29610.205 Test MSE 4194.842388155684 Test RE 0.11331668622689887\n",
      "137 Train Loss 29222.115 Test MSE 4104.105316804768 Test RE 0.11208443069152592\n",
      "138 Train Loss 29067.63 Test MSE 4129.222772929355 Test RE 0.11242689040625903\n",
      "139 Train Loss 28956.414 Test MSE 4116.5713845039 Test RE 0.11225452777435449\n",
      "140 Train Loss 28773.04 Test MSE 4248.342478100616 Test RE 0.11403700475631985\n",
      "141 Train Loss 28568.885 Test MSE 4525.75722634783 Test RE 0.11770141084555027\n",
      "142 Train Loss 28485.39 Test MSE 4544.5157916488915 Test RE 0.1179450857002858\n",
      "143 Train Loss 28285.404 Test MSE 4525.566495098066 Test RE 0.11769893064471369\n",
      "144 Train Loss 28192.992 Test MSE 4580.379627325737 Test RE 0.118409563189383\n",
      "145 Train Loss 28111.434 Test MSE 4582.820066633801 Test RE 0.11844110346497448\n",
      "146 Train Loss 28068.64 Test MSE 4630.551980214402 Test RE 0.11905631156134878\n",
      "147 Train Loss 27741.977 Test MSE 4582.854990978019 Test RE 0.11844155476678493\n",
      "148 Train Loss 27464.426 Test MSE 4673.968784370619 Test RE 0.11961315495792733\n",
      "149 Train Loss 27347.266 Test MSE 4855.202765080784 Test RE 0.1219101112964724\n",
      "150 Train Loss 27309.006 Test MSE 4927.532789651136 Test RE 0.12281482770572096\n",
      "151 Train Loss 27193.496 Test MSE 5044.885642109373 Test RE 0.1242686856234754\n",
      "152 Train Loss 26819.396 Test MSE 5117.297879185582 Test RE 0.12515735916120496\n",
      "153 Train Loss 26726.354 Test MSE 5018.0395058202 Test RE 0.12393759940777874\n",
      "154 Train Loss 26565.252 Test MSE 4993.164818140886 Test RE 0.12363003515802973\n",
      "155 Train Loss 26411.531 Test MSE 4835.2659879751245 Test RE 0.1216595558576163\n",
      "156 Train Loss 26346.55 Test MSE 4788.502388111689 Test RE 0.12106981980210745\n",
      "157 Train Loss 26229.768 Test MSE 4769.030756167083 Test RE 0.12082341412204423\n",
      "158 Train Loss 26054.469 Test MSE 4815.725564467892 Test RE 0.12141347985066137\n",
      "159 Train Loss 25718.717 Test MSE 4753.010476243136 Test RE 0.12062030647775449\n",
      "160 Train Loss 25599.625 Test MSE 4615.678310119816 Test RE 0.11886494899972987\n",
      "161 Train Loss 25565.936 Test MSE 4644.467265481555 Test RE 0.11923506560314856\n",
      "162 Train Loss 25513.812 Test MSE 4631.8744151083765 Test RE 0.11907331093657965\n",
      "163 Train Loss 25396.988 Test MSE 4472.506230145793 Test RE 0.11700691231718739\n",
      "164 Train Loss 25043.275 Test MSE 4309.460686570367 Test RE 0.114854364570933\n",
      "165 Train Loss 24714.344 Test MSE 4292.995572831167 Test RE 0.11463474312754356\n",
      "166 Train Loss 24597.383 Test MSE 4366.403657201063 Test RE 0.1156106872548972\n",
      "167 Train Loss 24536.23 Test MSE 4437.750554836457 Test RE 0.11655139751804416\n",
      "168 Train Loss 24468.918 Test MSE 4400.952803873667 Test RE 0.11606717054034853\n",
      "169 Train Loss 24330.148 Test MSE 4298.093182664119 Test RE 0.11470278300959312\n",
      "170 Train Loss 24097.014 Test MSE 4296.798523377881 Test RE 0.11468550648703804\n",
      "171 Train Loss 23783.938 Test MSE 3882.5842748831324 Test RE 0.1090175675108433\n",
      "172 Train Loss 23650.697 Test MSE 3751.1406953089668 Test RE 0.10715630228158655\n",
      "173 Train Loss 23476.896 Test MSE 3817.0307868420605 Test RE 0.10809332428208042\n",
      "174 Train Loss 23304.852 Test MSE 3912.77139734101 Test RE 0.109440553134613\n",
      "175 Train Loss 23216.713 Test MSE 3931.069023615748 Test RE 0.1096961477601865\n",
      "176 Train Loss 23192.896 Test MSE 3995.6101617353156 Test RE 0.11059298910637104\n",
      "177 Train Loss 23162.234 Test MSE 4030.271319474915 Test RE 0.11107163986347687\n",
      "178 Train Loss 23080.014 Test MSE 4085.9271042396513 Test RE 0.11183592881163903\n",
      "179 Train Loss 22987.285 Test MSE 4079.2806061871256 Test RE 0.11174493112676537\n",
      "180 Train Loss 22839.418 Test MSE 4055.515936568255 Test RE 0.1114189593943093\n",
      "181 Train Loss 22611.596 Test MSE 3969.2904050496395 Test RE 0.1102281399657247\n",
      "182 Train Loss 22556.35 Test MSE 3856.1185275060857 Test RE 0.10864537146279478\n",
      "183 Train Loss 22472.895 Test MSE 3834.128016118155 Test RE 0.10833513939662737\n",
      "184 Train Loss 22402.176 Test MSE 3797.5712595426585 Test RE 0.10781743801727556\n",
      "185 Train Loss 22350.133 Test MSE 3691.6914025049496 Test RE 0.10630378716344016\n",
      "186 Train Loss 22294.32 Test MSE 3655.1716223567805 Test RE 0.10577667921114697\n",
      "187 Train Loss 22247.146 Test MSE 3740.7884557288935 Test RE 0.1070083374067709\n",
      "188 Train Loss 22160.201 Test MSE 3863.547685431168 Test RE 0.10874997862568143\n",
      "189 Train Loss 22068.271 Test MSE 3964.842455174921 Test RE 0.110166362340292\n",
      "190 Train Loss 21721.47 Test MSE 4163.737518845096 Test RE 0.11289578137944013\n",
      "191 Train Loss 21319.426 Test MSE 4414.065294040442 Test RE 0.11623995104395371\n",
      "192 Train Loss 21140.16 Test MSE 4330.063030412831 Test RE 0.11512858075511191\n",
      "193 Train Loss 21095.094 Test MSE 4421.14800146727 Test RE 0.11633317163709868\n",
      "194 Train Loss 21067.53 Test MSE 4452.882410202025 Test RE 0.11674993707209681\n",
      "195 Train Loss 21055.498 Test MSE 4501.16185376667 Test RE 0.11738114909130513\n",
      "196 Train Loss 21001.666 Test MSE 4500.076280543584 Test RE 0.11736699346655363\n",
      "197 Train Loss 20783.516 Test MSE 4548.8050390280005 Test RE 0.11800073258979127\n",
      "198 Train Loss 20643.883 Test MSE 4493.405423871368 Test RE 0.11727996952380121\n",
      "199 Train Loss 20450.553 Test MSE 4571.618164643556 Test RE 0.11829626062068294\n",
      "Training time: 208.00\n",
      "Training time: 208.00\n",
      "ES_stan\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 255523.75 Test MSE 77515.02549379657 Test RE 0.48711217990899086\n",
      "1 Train Loss 251334.94 Test MSE 75649.93103499919 Test RE 0.48121627840143305\n",
      "2 Train Loss 249941.08 Test MSE 75137.43858386627 Test RE 0.4795835022018046\n",
      "3 Train Loss 248673.22 Test MSE 74558.02165751961 Test RE 0.4777307867636475\n",
      "4 Train Loss 247051.67 Test MSE 73875.3113109625 Test RE 0.4755385222322947\n",
      "5 Train Loss 246109.0 Test MSE 73652.93141714447 Test RE 0.4748222483738155\n",
      "6 Train Loss 243734.83 Test MSE 72840.129751191 Test RE 0.47219501426883814\n",
      "7 Train Loss 239430.77 Test MSE 70798.10111511081 Test RE 0.4655291138580381\n",
      "8 Train Loss 237377.33 Test MSE 70166.86925854528 Test RE 0.46344915167541956\n",
      "9 Train Loss 234934.44 Test MSE 69708.70926444292 Test RE 0.4619336101407254\n",
      "10 Train Loss 232315.78 Test MSE 68936.68611305159 Test RE 0.45936853361586416\n",
      "11 Train Loss 230217.42 Test MSE 68132.75926859355 Test RE 0.4566821432924196\n",
      "12 Train Loss 225997.95 Test MSE 67070.17777738042 Test RE 0.4531069989471327\n",
      "13 Train Loss 219537.66 Test MSE 64686.48288506928 Test RE 0.4449823764514701\n",
      "14 Train Loss 213601.72 Test MSE 61251.25070874539 Test RE 0.43300560931969473\n",
      "15 Train Loss 210287.73 Test MSE 59513.42633959549 Test RE 0.42681877897100506\n",
      "16 Train Loss 206115.14 Test MSE 57916.40290861157 Test RE 0.42105306413991156\n",
      "17 Train Loss 203109.19 Test MSE 58231.04094419218 Test RE 0.42219522647953045\n",
      "18 Train Loss 199745.7 Test MSE 56029.98859798295 Test RE 0.41413917011377527\n",
      "19 Train Loss 196186.95 Test MSE 54889.55627493755 Test RE 0.40990281593995914\n",
      "20 Train Loss 192717.34 Test MSE 54199.027683610126 Test RE 0.40731629956451676\n",
      "21 Train Loss 191417.36 Test MSE 52895.9763644136 Test RE 0.4023901681240333\n",
      "22 Train Loss 188570.69 Test MSE 51373.03558476311 Test RE 0.3965552063823212\n",
      "23 Train Loss 186302.55 Test MSE 51558.978471155875 Test RE 0.3972722169458803\n",
      "24 Train Loss 183093.73 Test MSE 49313.867524769055 Test RE 0.3885264356821835\n",
      "25 Train Loss 178855.47 Test MSE 46963.66848727795 Test RE 0.3791552280527926\n",
      "26 Train Loss 174066.19 Test MSE 46715.63308980396 Test RE 0.3781526613736662\n",
      "27 Train Loss 171219.61 Test MSE 44903.528044548024 Test RE 0.3707458293105803\n",
      "28 Train Loss 167370.66 Test MSE 44210.41105591479 Test RE 0.36787334244649006\n",
      "29 Train Loss 163485.78 Test MSE 43271.63240860365 Test RE 0.3639466124577257\n",
      "30 Train Loss 159850.4 Test MSE 40909.22948222167 Test RE 0.3538724001648424\n",
      "31 Train Loss 156846.2 Test MSE 40192.41796957215 Test RE 0.35075842303498794\n",
      "32 Train Loss 148945.97 Test MSE 36915.042890856086 Test RE 0.33615357204064095\n",
      "33 Train Loss 143863.25 Test MSE 34449.19176227417 Test RE 0.3247323537081428\n",
      "34 Train Loss 139712.81 Test MSE 36496.78124423273 Test RE 0.33424377225024504\n",
      "35 Train Loss 136525.55 Test MSE 34449.04210201793 Test RE 0.3247316483275873\n",
      "36 Train Loss 131088.03 Test MSE 33583.205633325 Test RE 0.32062480342548694\n",
      "37 Train Loss 124449.945 Test MSE 29371.946493944175 Test RE 0.2998488627721494\n",
      "38 Train Loss 119587.125 Test MSE 29515.762502798985 Test RE 0.3005820523292984\n",
      "39 Train Loss 110671.14 Test MSE 31383.00779056076 Test RE 0.3099440646630061\n",
      "40 Train Loss 102806.54 Test MSE 29620.614843479358 Test RE 0.30111547563491853\n",
      "41 Train Loss 95962.664 Test MSE 25683.63844017614 Test RE 0.28039115888291144\n",
      "42 Train Loss 92448.67 Test MSE 25031.228192774764 Test RE 0.27680703379400395\n",
      "43 Train Loss 88499.516 Test MSE 23443.76118306082 Test RE 0.26788579561502485\n",
      "44 Train Loss 83655.84 Test MSE 18257.1792383618 Test RE 0.23640294444154736\n",
      "45 Train Loss 80694.6 Test MSE 17890.541899444568 Test RE 0.2340172058294025\n",
      "46 Train Loss 73242.13 Test MSE 13764.443381845042 Test RE 0.2052652198403695\n",
      "47 Train Loss 68974.24 Test MSE 10869.307694608751 Test RE 0.18240510273509208\n",
      "48 Train Loss 65697.03 Test MSE 10935.48377408973 Test RE 0.18295953259188774\n",
      "49 Train Loss 61668.27 Test MSE 9734.663836549886 Test RE 0.1726221499265395\n",
      "50 Train Loss 60394.72 Test MSE 9268.106790160025 Test RE 0.1684346957698353\n",
      "51 Train Loss 59017.35 Test MSE 9149.745814042504 Test RE 0.16735571842083297\n",
      "52 Train Loss 55397.484 Test MSE 9146.307895920403 Test RE 0.16732427441545145\n",
      "53 Train Loss 52277.418 Test MSE 8700.347787507893 Test RE 0.1631940605551577\n",
      "54 Train Loss 51719.555 Test MSE 8863.226516509925 Test RE 0.16471455078343214\n",
      "55 Train Loss 50473.594 Test MSE 8900.839383338007 Test RE 0.16506368027495333\n",
      "56 Train Loss 50088.07 Test MSE 8926.728448041285 Test RE 0.16530355885569478\n",
      "57 Train Loss 49598.367 Test MSE 9258.571107453641 Test RE 0.16834802471650379\n",
      "58 Train Loss 49401.965 Test MSE 8981.623525090126 Test RE 0.16581104845064823\n",
      "59 Train Loss 49197.582 Test MSE 8930.925333671032 Test RE 0.16534241287771317\n",
      "60 Train Loss 48400.96 Test MSE 8557.986319166113 Test RE 0.16185340330680775\n",
      "61 Train Loss 47953.32 Test MSE 8041.2096805802685 Test RE 0.15689053185417404\n",
      "62 Train Loss 46466.145 Test MSE 7488.618847864233 Test RE 0.15140384523174943\n",
      "63 Train Loss 45237.723 Test MSE 7090.798497864499 Test RE 0.14732743028224832\n",
      "64 Train Loss 44679.16 Test MSE 7154.89241726288 Test RE 0.1479917807026067\n",
      "65 Train Loss 44320.83 Test MSE 6884.919781388185 Test RE 0.14517287714807042\n",
      "66 Train Loss 43914.957 Test MSE 6685.302751292498 Test RE 0.1430528720917275\n",
      "67 Train Loss 43627.203 Test MSE 6620.528300475369 Test RE 0.14235815966453058\n",
      "68 Train Loss 43460.9 Test MSE 6643.419641287637 Test RE 0.14260405825124411\n",
      "69 Train Loss 43219.87 Test MSE 6819.863660867568 Test RE 0.14448537455565122\n",
      "70 Train Loss 43049.996 Test MSE 6782.265896732306 Test RE 0.1440865517133268\n",
      "71 Train Loss 42854.15 Test MSE 6810.760032481259 Test RE 0.14438890778827318\n",
      "72 Train Loss 42813.344 Test MSE 6810.2973036267795 Test RE 0.14438400275209132\n",
      "73 Train Loss 42752.96 Test MSE 6906.247539962526 Test RE 0.14539755790382314\n",
      "74 Train Loss 42231.438 Test MSE 6880.2841486612715 Test RE 0.14512399629849992\n",
      "75 Train Loss 42190.367 Test MSE 6847.283046882189 Test RE 0.14477553623761785\n",
      "76 Train Loss 42122.09 Test MSE 6819.084585473512 Test RE 0.14447712158897558\n",
      "77 Train Loss 42001.863 Test MSE 6775.519775116159 Test RE 0.14401487454469972\n",
      "78 Train Loss 41914.707 Test MSE 6676.497076263259 Test RE 0.14295862862767642\n",
      "79 Train Loss 41892.098 Test MSE 6704.939556872715 Test RE 0.14326281334383312\n",
      "80 Train Loss 41819.242 Test MSE 6752.400944136291 Test RE 0.1437689671731486\n",
      "81 Train Loss 41721.25 Test MSE 6718.563786793762 Test RE 0.14340829228329588\n",
      "82 Train Loss 41651.445 Test MSE 6661.897530513688 Test RE 0.14280223873859701\n",
      "83 Train Loss 41611.133 Test MSE 6719.7334001295485 Test RE 0.14342077448612095\n",
      "84 Train Loss 41602.88 Test MSE 6703.88180347157 Test RE 0.1432515125182428\n",
      "85 Train Loss 41557.2 Test MSE 6627.307752207862 Test RE 0.14243102872312002\n",
      "86 Train Loss 41432.367 Test MSE 6659.866994347361 Test RE 0.14278047412816422\n",
      "87 Train Loss 41316.766 Test MSE 6561.4089022691805 Test RE 0.1417211257978785\n",
      "88 Train Loss 41301.65 Test MSE 6548.019207121313 Test RE 0.14157644865728425\n",
      "89 Train Loss 41291.617 Test MSE 6541.5064512490235 Test RE 0.14150602413621105\n",
      "90 Train Loss 41191.77 Test MSE 6509.413430954129 Test RE 0.14115847885101926\n",
      "91 Train Loss 41122.91 Test MSE 6472.5800089892045 Test RE 0.14075854067952095\n",
      "92 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "93 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "94 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "95 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "96 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "97 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "98 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "99 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "100 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "101 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "102 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "103 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "104 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "106 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "107 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "108 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "109 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "110 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "111 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "112 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "113 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "114 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "115 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "116 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "117 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "118 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "119 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "120 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "121 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "122 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "123 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "124 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "125 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "126 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "127 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "128 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "129 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "130 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "131 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "132 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "133 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "134 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "135 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "136 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "137 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "138 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "139 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "140 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "141 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "142 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "143 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "144 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "145 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "146 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "147 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "148 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "149 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "150 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "151 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "152 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "153 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "154 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "155 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "156 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "157 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "158 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "159 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "160 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "161 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "162 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "163 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "164 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "165 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "166 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "167 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "168 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "169 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "170 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "171 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "172 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "173 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "174 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "175 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "176 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "177 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "178 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "179 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "180 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "181 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "182 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "183 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "184 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "185 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "186 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "187 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "188 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "189 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "190 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "191 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "192 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "193 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "194 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "195 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "196 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "197 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "198 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "199 Train Loss 41095.66 Test MSE 6473.934666588088 Test RE 0.14077326970956772\n",
      "Training time: 128.55\n",
      "Training time: 128.55\n",
      "ES_stan\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 243645.4 Test MSE 72690.96028375956 Test RE 0.47171126185049694\n",
      "1 Train Loss 222511.61 Test MSE 64357.76727581782 Test RE 0.443850308764445\n",
      "2 Train Loss 177667.52 Test MSE 47836.07626757849 Test RE 0.3826606603819818\n",
      "3 Train Loss 139455.23 Test MSE 37544.15259839815 Test RE 0.3390058519072587\n",
      "4 Train Loss 118355.24 Test MSE 31620.5779754824 Test RE 0.3111149951613779\n",
      "5 Train Loss 107106.18 Test MSE 27432.789546627926 Test RE 0.2897817479439972\n",
      "6 Train Loss 96664.414 Test MSE 26086.5176645685 Test RE 0.2825817406410859\n",
      "7 Train Loss 89097.48 Test MSE 24020.509949543502 Test RE 0.27116095414468694\n",
      "8 Train Loss 84242.02 Test MSE 21951.322168287865 Test RE 0.2592187330858281\n",
      "9 Train Loss 79596.74 Test MSE 19455.589265740517 Test RE 0.24403843791074561\n",
      "10 Train Loss 75045.664 Test MSE 18214.363882994043 Test RE 0.23612558458601812\n",
      "11 Train Loss 71970.055 Test MSE 17509.678817058717 Test RE 0.23151286622834588\n",
      "12 Train Loss 69223.72 Test MSE 16870.063008430785 Test RE 0.22724503025130371\n",
      "13 Train Loss 66832.914 Test MSE 16303.440975867192 Test RE 0.22339614761878285\n",
      "14 Train Loss 65135.07 Test MSE 14570.226855611798 Test RE 0.21118798120919904\n",
      "15 Train Loss 63917.34 Test MSE 13789.637394275573 Test RE 0.20545298951991142\n",
      "16 Train Loss 62551.06 Test MSE 14126.692038358517 Test RE 0.20794873398249453\n",
      "17 Train Loss 61659.68 Test MSE 13145.615006152406 Test RE 0.20059795272034595\n",
      "18 Train Loss 60808.07 Test MSE 13200.696292163097 Test RE 0.20101777496196893\n",
      "19 Train Loss 59960.992 Test MSE 13152.867373969386 Test RE 0.20065327951708783\n",
      "20 Train Loss 59060.445 Test MSE 13067.151881920297 Test RE 0.19999839542116718\n",
      "21 Train Loss 58407.914 Test MSE 12979.70822876139 Test RE 0.1993280907281599\n",
      "22 Train Loss 57858.277 Test MSE 12468.737584371676 Test RE 0.1953652350939\n",
      "23 Train Loss 56897.227 Test MSE 11582.311202846497 Test RE 0.18829277531485608\n",
      "24 Train Loss 56130.74 Test MSE 11366.965873370122 Test RE 0.18653413600505728\n",
      "25 Train Loss 55387.707 Test MSE 10959.692916752092 Test RE 0.18316193994841698\n",
      "26 Train Loss 54781.273 Test MSE 11016.560455973715 Test RE 0.18363651949442678\n",
      "27 Train Loss 54390.523 Test MSE 10968.855910214173 Test RE 0.18323849140948448\n",
      "28 Train Loss 54046.977 Test MSE 10922.653479149714 Test RE 0.18285217045959434\n",
      "29 Train Loss 53800.74 Test MSE 10930.949016730014 Test RE 0.18292159356993215\n",
      "30 Train Loss 53595.168 Test MSE 10868.73428135501 Test RE 0.1824002912564819\n",
      "31 Train Loss 53321.113 Test MSE 10833.507497142677 Test RE 0.18210446147867024\n",
      "32 Train Loss 53036.93 Test MSE 10715.135640355116 Test RE 0.1811068505968515\n",
      "33 Train Loss 52774.06 Test MSE 10585.084897743822 Test RE 0.18000443879436692\n",
      "34 Train Loss 52402.227 Test MSE 10473.753542753202 Test RE 0.17905531502515487\n",
      "35 Train Loss 52192.598 Test MSE 10562.537781926305 Test RE 0.17981262434259399\n",
      "36 Train Loss 52023.434 Test MSE 10502.84353685247 Test RE 0.17930379835794405\n",
      "37 Train Loss 51853.78 Test MSE 10487.342571520225 Test RE 0.17917143381207365\n",
      "38 Train Loss 51688.86 Test MSE 10330.522405878537 Test RE 0.17782678788988543\n",
      "39 Train Loss 51564.797 Test MSE 10527.124456165717 Test RE 0.17951093977183902\n",
      "40 Train Loss 51398.375 Test MSE 10436.017747893422 Test RE 0.1787324655521302\n",
      "41 Train Loss 51206.28 Test MSE 10210.30085439846 Test RE 0.17678902936643348\n",
      "42 Train Loss 51016.375 Test MSE 10067.448383970876 Test RE 0.17554794410120916\n",
      "43 Train Loss 50852.5 Test MSE 10055.711684634936 Test RE 0.1754455867707254\n",
      "44 Train Loss 50687.395 Test MSE 10165.706449265488 Test RE 0.17640253591907382\n",
      "45 Train Loss 50421.62 Test MSE 10102.737379290213 Test RE 0.17585534529894514\n",
      "46 Train Loss 50261.26 Test MSE 10036.630931923388 Test RE 0.17527905338621189\n",
      "47 Train Loss 50118.79 Test MSE 9973.701615122805 Test RE 0.17472869264782526\n",
      "48 Train Loss 50009.785 Test MSE 9840.722456832524 Test RE 0.17355995686189374\n",
      "49 Train Loss 49795.504 Test MSE 9761.625354783197 Test RE 0.17286103527439037\n",
      "50 Train Loss 49614.51 Test MSE 9765.706760547568 Test RE 0.17289716871888586\n",
      "51 Train Loss 49511.61 Test MSE 9602.648743497533 Test RE 0.17144766052624424\n",
      "52 Train Loss 49389.066 Test MSE 9612.099267897513 Test RE 0.17153200557977638\n",
      "53 Train Loss 49204.97 Test MSE 9635.443072417274 Test RE 0.17174016933786604\n",
      "54 Train Loss 49070.324 Test MSE 9587.36189394417 Test RE 0.17131113889709768\n",
      "55 Train Loss 48944.426 Test MSE 9584.081937520956 Test RE 0.17128183254867285\n",
      "56 Train Loss 48810.125 Test MSE 9507.14688668505 Test RE 0.17059297530563483\n",
      "57 Train Loss 48661.55 Test MSE 9179.08845497513 Test RE 0.16762385308340663\n",
      "58 Train Loss 48555.87 Test MSE 9074.16702162775 Test RE 0.16666308871870975\n",
      "59 Train Loss 48413.797 Test MSE 8991.632833289437 Test RE 0.16590341436597625\n",
      "60 Train Loss 48225.914 Test MSE 8794.893739350773 Test RE 0.1640783727146308\n",
      "61 Train Loss 48047.78 Test MSE 8890.838242225682 Test RE 0.1649709199704106\n",
      "62 Train Loss 47950.5 Test MSE 8891.30602321448 Test RE 0.16497525978871203\n",
      "63 Train Loss 47756.18 Test MSE 8918.69102655613 Test RE 0.16522912433125278\n",
      "64 Train Loss 47616.96 Test MSE 8912.44926572922 Test RE 0.16517129627225355\n",
      "65 Train Loss 47539.2 Test MSE 8865.548266823122 Test RE 0.1647361231204364\n",
      "66 Train Loss 47431.16 Test MSE 8690.859659406009 Test RE 0.16310505097920977\n",
      "67 Train Loss 47249.867 Test MSE 8699.56664494749 Test RE 0.16318673437171774\n",
      "68 Train Loss 47198.703 Test MSE 8719.874687752466 Test RE 0.16337709278526347\n",
      "69 Train Loss 47134.36 Test MSE 8613.9253612573 Test RE 0.16238151687107089\n",
      "70 Train Loss 47014.117 Test MSE 8541.662365628805 Test RE 0.1616989657759413\n",
      "71 Train Loss 46927.016 Test MSE 8611.508065770895 Test RE 0.16235873099625922\n",
      "72 Train Loss 46849.992 Test MSE 8563.786463009654 Test RE 0.16190824179200317\n",
      "73 Train Loss 46756.992 Test MSE 8513.966995601913 Test RE 0.16143660767160564\n",
      "74 Train Loss 46675.684 Test MSE 8502.977390150249 Test RE 0.16133238495774874\n",
      "75 Train Loss 46607.523 Test MSE 8480.652402442625 Test RE 0.16112045267842642\n",
      "76 Train Loss 46517.266 Test MSE 8476.765641092026 Test RE 0.1610835269510466\n",
      "77 Train Loss 46442.434 Test MSE 8443.33375623338 Test RE 0.16076556096578928\n",
      "78 Train Loss 46354.508 Test MSE 8436.93377099391 Test RE 0.16070461985562015\n",
      "79 Train Loss 46285.992 Test MSE 8393.50977318309 Test RE 0.16029052157355594\n",
      "80 Train Loss 46211.37 Test MSE 8313.318289246965 Test RE 0.15952297711011393\n",
      "81 Train Loss 46152.67 Test MSE 8309.99505221662 Test RE 0.15949108938268602\n",
      "82 Train Loss 46060.82 Test MSE 8266.377693291253 Test RE 0.1590719716529499\n",
      "83 Train Loss 46002.5 Test MSE 8360.723681702018 Test RE 0.1599771579237299\n",
      "84 Train Loss 45960.086 Test MSE 8333.008089195355 Test RE 0.15971177764523992\n",
      "85 Train Loss 45895.246 Test MSE 8339.717683064751 Test RE 0.1597760632868001\n",
      "86 Train Loss 45858.332 Test MSE 8277.022757979496 Test RE 0.15917436152025907\n",
      "87 Train Loss 45812.207 Test MSE 8281.681209359975 Test RE 0.15921914825953049\n",
      "88 Train Loss 45761.008 Test MSE 8268.972733063381 Test RE 0.1590969381930927\n",
      "89 Train Loss 45711.2 Test MSE 8308.68468379651 Test RE 0.15947851414497982\n",
      "90 Train Loss 45661.8 Test MSE 8228.8057375109 Test RE 0.15871005667781132\n",
      "91 Train Loss 45600.84 Test MSE 8180.721030338971 Test RE 0.15824566924605246\n",
      "92 Train Loss 45534.918 Test MSE 8111.4150243810245 Test RE 0.15757392511015872\n",
      "93 Train Loss 45464.99 Test MSE 8092.837030252823 Test RE 0.15739337180551397\n",
      "94 Train Loss 45408.848 Test MSE 8079.515601046207 Test RE 0.15726377768436564\n",
      "95 Train Loss 45367.902 Test MSE 8049.488453603931 Test RE 0.15697127387048312\n",
      "96 Train Loss 45315.93 Test MSE 8074.849829024524 Test RE 0.1572183626537146\n",
      "97 Train Loss 45255.535 Test MSE 8097.675793269018 Test RE 0.15744041806492823\n",
      "98 Train Loss 45210.37 Test MSE 8011.004888736865 Test RE 0.15659559461867847\n",
      "99 Train Loss 45173.82 Test MSE 7985.798085682031 Test RE 0.15634903477327158\n",
      "100 Train Loss 45107.39 Test MSE 7952.235794440212 Test RE 0.15602014110429752\n",
      "101 Train Loss 45049.44 Test MSE 7888.979168134856 Test RE 0.15539836548068012\n",
      "102 Train Loss 45012.293 Test MSE 7826.1411263000955 Test RE 0.15477823125627485\n",
      "103 Train Loss 44938.324 Test MSE 7830.349269003179 Test RE 0.15481983805469818\n",
      "104 Train Loss 44884.45 Test MSE 7795.227170284166 Test RE 0.1544722348644711\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105 Train Loss 44847.574 Test MSE 7765.892410270801 Test RE 0.15418130831469187\n",
      "106 Train Loss 44801.76 Test MSE 7774.671423660758 Test RE 0.1542684314281228\n",
      "107 Train Loss 44741.684 Test MSE 7788.280793976389 Test RE 0.1544033939314342\n",
      "108 Train Loss 44648.47 Test MSE 7820.664782570092 Test RE 0.1547240687312925\n",
      "109 Train Loss 44586.977 Test MSE 7716.868504280842 Test RE 0.15369388612614113\n",
      "110 Train Loss 44550.81 Test MSE 7725.303657342088 Test RE 0.15377786302500032\n",
      "111 Train Loss 44497.105 Test MSE 7746.983339473466 Test RE 0.1539934868791692\n",
      "112 Train Loss 44428.277 Test MSE 7702.270101041325 Test RE 0.1535484419311976\n",
      "113 Train Loss 44361.516 Test MSE 7751.570193767801 Test RE 0.15403906856773303\n",
      "114 Train Loss 44309.406 Test MSE 7730.1687424054935 Test RE 0.1538262769558472\n",
      "115 Train Loss 44257.44 Test MSE 7693.0422357801735 Test RE 0.1534564334088426\n",
      "116 Train Loss 44222.96 Test MSE 7661.16054752471 Test RE 0.1531381243834154\n",
      "117 Train Loss 44188.305 Test MSE 7654.863954018158 Test RE 0.15307518047821708\n",
      "118 Train Loss 44143.484 Test MSE 7713.719957581628 Test RE 0.15366252873371394\n",
      "119 Train Loss 44110.598 Test MSE 7741.607844564046 Test RE 0.15394005092766475\n",
      "120 Train Loss 44065.133 Test MSE 7705.250273532902 Test RE 0.15357814464101216\n",
      "121 Train Loss 44029.05 Test MSE 7690.287533468237 Test RE 0.15342895633099035\n",
      "122 Train Loss 43992.855 Test MSE 7698.156922294353 Test RE 0.153507437362381\n",
      "123 Train Loss 43956.504 Test MSE 7666.257146075207 Test RE 0.1531890535981613\n",
      "124 Train Loss 43915.527 Test MSE 7645.881726760205 Test RE 0.15298534480984413\n",
      "125 Train Loss 43879.516 Test MSE 7618.259125102069 Test RE 0.15270874643765636\n",
      "126 Train Loss 43841.086 Test MSE 7626.254877357892 Test RE 0.15278886323801202\n",
      "127 Train Loss 43792.203 Test MSE 7578.4824216771985 Test RE 0.15230956074751806\n",
      "128 Train Loss 43727.414 Test MSE 7584.007738813256 Test RE 0.1523650735299095\n",
      "129 Train Loss 43698.12 Test MSE 7567.493223367745 Test RE 0.15219909224771566\n",
      "130 Train Loss 43663.918 Test MSE 7551.832988463191 Test RE 0.15204152964150516\n",
      "131 Train Loss 43621.46 Test MSE 7544.827202056087 Test RE 0.15197098930805938\n",
      "132 Train Loss 43581.246 Test MSE 7528.209441825264 Test RE 0.15180353619684248\n",
      "133 Train Loss 43525.28 Test MSE 7533.147588471627 Test RE 0.15185331597775772\n",
      "134 Train Loss 43488.367 Test MSE 7509.417830738414 Test RE 0.15161395490146037\n",
      "135 Train Loss 43456.992 Test MSE 7505.668715498089 Test RE 0.15157610315543169\n",
      "136 Train Loss 43391.258 Test MSE 7481.7469077411215 Test RE 0.15133436132694697\n",
      "137 Train Loss 43317.887 Test MSE 7449.981091385121 Test RE 0.15101275374741208\n",
      "138 Train Loss 43252.434 Test MSE 7492.669934144225 Test RE 0.15144479184184564\n",
      "139 Train Loss 43213.344 Test MSE 7476.128895011351 Test RE 0.1512775324846037\n",
      "140 Train Loss 43149.133 Test MSE 7445.456967959181 Test RE 0.15096689429615137\n",
      "141 Train Loss 43100.22 Test MSE 7459.124472932531 Test RE 0.15110539450811086\n",
      "142 Train Loss 43038.516 Test MSE 7447.931092689787 Test RE 0.15099197535591907\n",
      "143 Train Loss 42997.582 Test MSE 7461.564618854608 Test RE 0.15113010847173874\n",
      "144 Train Loss 42948.13 Test MSE 7448.046089930253 Test RE 0.15099314102147496\n",
      "145 Train Loss 42904.4 Test MSE 7475.634396592239 Test RE 0.15127252937813337\n",
      "146 Train Loss 42869.516 Test MSE 7458.427882501488 Test RE 0.1510983386512302\n",
      "147 Train Loss 42811.363 Test MSE 7427.309302978678 Test RE 0.15078279759702423\n",
      "148 Train Loss 42773.867 Test MSE 7426.662541202663 Test RE 0.15077623245552577\n",
      "149 Train Loss 42726.785 Test MSE 7399.068764800212 Test RE 0.15049586712379345\n",
      "150 Train Loss 42685.797 Test MSE 7396.581834702386 Test RE 0.1504705731193518\n",
      "151 Train Loss 42658.945 Test MSE 7360.129184681692 Test RE 0.15009933232355274\n",
      "152 Train Loss 42603.605 Test MSE 7311.099276261683 Test RE 0.1495985494217532\n",
      "153 Train Loss 42542.92 Test MSE 7228.4806192960605 Test RE 0.14875088296356767\n",
      "154 Train Loss 42494.977 Test MSE 7184.149156372259 Test RE 0.14829404519103787\n",
      "155 Train Loss 42429.8 Test MSE 7215.47118614234 Test RE 0.1486169657369518\n",
      "156 Train Loss 42380.37 Test MSE 7189.314478516881 Test RE 0.14834734648423428\n",
      "157 Train Loss 42321.902 Test MSE 7116.815975009719 Test RE 0.1475974688686255\n",
      "158 Train Loss 42265.98 Test MSE 7122.856391172995 Test RE 0.147660092452234\n",
      "159 Train Loss 42196.965 Test MSE 7124.044511371248 Test RE 0.14767240707809204\n",
      "160 Train Loss 42146.117 Test MSE 7104.09436638074 Test RE 0.14746549151402016\n",
      "161 Train Loss 42087.938 Test MSE 7089.077119162628 Test RE 0.1473095464223313\n",
      "162 Train Loss 41984.992 Test MSE 7087.862529347453 Test RE 0.1472969264200273\n",
      "163 Train Loss 41908.125 Test MSE 7060.69744117223 Test RE 0.14701438882558693\n",
      "164 Train Loss 41852.184 Test MSE 7045.589934273257 Test RE 0.14685702404376177\n",
      "165 Train Loss 41790.125 Test MSE 7030.535790993929 Test RE 0.14670004720364452\n",
      "166 Train Loss 41751.8 Test MSE 6997.5473030084295 Test RE 0.14635547154922074\n",
      "167 Train Loss 41688.508 Test MSE 6911.822422024134 Test RE 0.1454562301943913\n",
      "168 Train Loss 41613.797 Test MSE 6858.799615022207 Test RE 0.1448972353723625\n",
      "169 Train Loss 41556.797 Test MSE 6877.975833319038 Test RE 0.14509964991673258\n",
      "170 Train Loss 41483.32 Test MSE 6862.115552920076 Test RE 0.14493225696530404\n",
      "171 Train Loss 41424.574 Test MSE 6875.41175474103 Test RE 0.145072601145854\n",
      "172 Train Loss 41364.426 Test MSE 6815.61923585853 Test RE 0.144440406443791\n",
      "173 Train Loss 41326.95 Test MSE 6808.717127974069 Test RE 0.14436725125714642\n",
      "174 Train Loss 41263.87 Test MSE 6758.97037377504 Test RE 0.14383888678483492\n",
      "175 Train Loss 41198.277 Test MSE 6729.761478611567 Test RE 0.14352775035225024\n",
      "176 Train Loss 41160.598 Test MSE 6693.520476163326 Test RE 0.14314076699726816\n",
      "177 Train Loss 41107.785 Test MSE 6619.1723728920715 Test RE 0.14234358097602082\n",
      "178 Train Loss 41036.62 Test MSE 6587.876877696112 Test RE 0.14200668154459806\n",
      "179 Train Loss 40989.645 Test MSE 6603.649407436524 Test RE 0.14217657433638684\n",
      "180 Train Loss 40936.28 Test MSE 6598.383151264815 Test RE 0.14211987178066832\n",
      "181 Train Loss 40915.816 Test MSE 6629.616336596773 Test RE 0.1424558340763319\n",
      "182 Train Loss 40884.266 Test MSE 6583.270533775287 Test RE 0.14195702624509204\n",
      "183 Train Loss 40841.51 Test MSE 6570.853437223524 Test RE 0.14182308627883397\n",
      "184 Train Loss 40800.66 Test MSE 6556.937293258614 Test RE 0.14167282599825767\n",
      "185 Train Loss 40766.98 Test MSE 6538.287739261183 Test RE 0.14147120622531073\n",
      "186 Train Loss 40738.72 Test MSE 6517.271524081246 Test RE 0.1412436556431844\n",
      "187 Train Loss 40700.094 Test MSE 6475.549126681702 Test RE 0.14079082152854844\n",
      "188 Train Loss 40668.73 Test MSE 6485.256092359355 Test RE 0.14089630601103578\n",
      "189 Train Loss 40651.883 Test MSE 6463.485152719469 Test RE 0.14065961345753122\n",
      "190 Train Loss 40615.01 Test MSE 6454.310207184111 Test RE 0.1405597445346005\n",
      "191 Train Loss 40588.2 Test MSE 6469.846641677292 Test RE 0.14072881641026558\n",
      "192 Train Loss 40538.645 Test MSE 6452.6508232974975 Test RE 0.14054167462773823\n",
      "193 Train Loss 40493.6 Test MSE 6441.168942410954 Test RE 0.14041657865554252\n",
      "194 Train Loss 40442.92 Test MSE 6444.7055408632195 Test RE 0.14045512204047\n",
      "195 Train Loss 40381.258 Test MSE 6394.368757539286 Test RE 0.13990552994003821\n",
      "196 Train Loss 40307.258 Test MSE 6431.16760396289 Test RE 0.14030752241858907\n",
      "197 Train Loss 40244.344 Test MSE 6336.686652486645 Test RE 0.139273072864696\n",
      "198 Train Loss 40181.2 Test MSE 6260.371969677935 Test RE 0.1384318780930932\n",
      "199 Train Loss 40133.918 Test MSE 6279.4711471860555 Test RE 0.13864288165346889\n",
      "Training time: 191.22\n",
      "Training time: 191.22\n",
      "ES_stan\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 256474.86 Test MSE 78174.45032496029 Test RE 0.4891797378380076\n",
      "1 Train Loss 249826.31 Test MSE 75214.09106841905 Test RE 0.4798280666748493\n",
      "2 Train Loss 244868.42 Test MSE 73032.88522192613 Test RE 0.4728193818886183\n",
      "3 Train Loss 240395.28 Test MSE 70611.62919696883 Test RE 0.46491564159797266\n",
      "4 Train Loss 236372.72 Test MSE 69996.89174563151 Test RE 0.46288746422192134\n",
      "5 Train Loss 230718.39 Test MSE 67528.1611772851 Test RE 0.45465136966709363\n",
      "6 Train Loss 224382.31 Test MSE 64870.70334598352 Test RE 0.44561555802047736\n",
      "7 Train Loss 217888.55 Test MSE 62531.71328993841 Test RE 0.43750820868822554\n",
      "8 Train Loss 211282.31 Test MSE 61293.7467283394 Test RE 0.4331557925739396\n",
      "9 Train Loss 200381.67 Test MSE 56576.543460136745 Test RE 0.41615416660023263\n",
      "10 Train Loss 192482.52 Test MSE 54603.6596999316 Test RE 0.40883391671829217\n",
      "11 Train Loss 183729.52 Test MSE 50970.02451110714 Test RE 0.3949966961205502\n",
      "12 Train Loss 171455.11 Test MSE 44982.22193351279 Test RE 0.371070555017274\n",
      "13 Train Loss 157708.17 Test MSE 40402.06149481568 Test RE 0.3516720106804687\n",
      "14 Train Loss 145729.92 Test MSE 35072.49223012014 Test RE 0.3276569284104439\n",
      "15 Train Loss 138917.78 Test MSE 32814.06383312737 Test RE 0.31693197071952983\n",
      "16 Train Loss 133782.77 Test MSE 30822.108376575743 Test RE 0.3071618067165657\n",
      "17 Train Loss 125448.75 Test MSE 26986.723556115132 Test RE 0.2874161198013422\n",
      "18 Train Loss 116810.96 Test MSE 23610.234707993306 Test RE 0.2688352380618081\n",
      "19 Train Loss 110563.39 Test MSE 20956.65662450414 Test RE 0.2532777506630331\n",
      "20 Train Loss 104935.445 Test MSE 18965.122942003065 Test RE 0.24094275564651924\n",
      "21 Train Loss 100819.88 Test MSE 18813.00430040783 Test RE 0.23997451312732085\n",
      "22 Train Loss 98328.35 Test MSE 17321.39139426175 Test RE 0.23026473418763116\n",
      "23 Train Loss 90932.18 Test MSE 15655.980952742664 Test RE 0.21891533452682696\n",
      "24 Train Loss 84298.91 Test MSE 12887.188011480388 Test RE 0.1986164083342993\n",
      "25 Train Loss 81718.95 Test MSE 11108.922371866463 Test RE 0.18440470930924338\n",
      "26 Train Loss 77980.96 Test MSE 11559.605787661088 Test RE 0.1881081246489218\n",
      "27 Train Loss 75568.55 Test MSE 10300.535508098685 Test RE 0.17756850719341835\n",
      "28 Train Loss 73199.26 Test MSE 10030.196993187543 Test RE 0.1752228634411434\n",
      "29 Train Loss 67815.016 Test MSE 9246.843269258085 Test RE 0.168241367652575\n",
      "30 Train Loss 66919.78 Test MSE 9603.314510180078 Test RE 0.17145360379069388\n",
      "31 Train Loss 64683.027 Test MSE 9478.845643046865 Test RE 0.17033887216554075\n",
      "32 Train Loss 63298.7 Test MSE 8572.495345995447 Test RE 0.16199054660179044\n",
      "33 Train Loss 62271.61 Test MSE 8467.529412164587 Test RE 0.1609957452492276\n",
      "34 Train Loss 61366.023 Test MSE 8307.229312794088 Test RE 0.15946454619586703\n",
      "35 Train Loss 59430.25 Test MSE 7811.170373251407 Test RE 0.15463012149003721\n",
      "36 Train Loss 58239.75 Test MSE 7604.616294326919 Test RE 0.15257194922978823\n",
      "37 Train Loss 56840.17 Test MSE 7794.190590729619 Test RE 0.15446196395892917\n",
      "38 Train Loss 56124.0 Test MSE 7919.945520939122 Test RE 0.15570305684432034\n",
      "39 Train Loss 54490.06 Test MSE 7807.499698979294 Test RE 0.15459378484140718\n",
      "40 Train Loss 53755.414 Test MSE 7539.6956547036925 Test RE 0.15191929965676312\n",
      "41 Train Loss 53216.086 Test MSE 7513.921892397417 Test RE 0.15165941623093107\n",
      "42 Train Loss 52241.863 Test MSE 7674.1330778867605 Test RE 0.1532677228105476\n",
      "43 Train Loss 50929.0 Test MSE 7593.44603325033 Test RE 0.15245985318568886\n",
      "44 Train Loss 49514.07 Test MSE 7285.284254728632 Test RE 0.14933420443876622\n",
      "45 Train Loss 48893.594 Test MSE 7038.863309653086 Test RE 0.14678690301926148\n",
      "46 Train Loss 48443.703 Test MSE 7007.431497698829 Test RE 0.14645880028939845\n",
      "47 Train Loss 47962.336 Test MSE 6986.656575419284 Test RE 0.14624153603977275\n",
      "48 Train Loss 47377.93 Test MSE 6928.103145442598 Test RE 0.14562743972648062\n",
      "49 Train Loss 46973.85 Test MSE 6841.066030793292 Test RE 0.1447097965690078\n",
      "50 Train Loss 46412.13 Test MSE 6816.813114769868 Test RE 0.14445305656391988\n",
      "51 Train Loss 45944.184 Test MSE 6579.271905203855 Test RE 0.14191390788633773\n",
      "52 Train Loss 45407.594 Test MSE 6675.558503314504 Test RE 0.1429485798089599\n",
      "53 Train Loss 44809.61 Test MSE 6618.597466353331 Test RE 0.14233739923396596\n",
      "54 Train Loss 44301.867 Test MSE 6654.787332833718 Test RE 0.14272601246776398\n",
      "55 Train Loss 44069.605 Test MSE 6703.396043877945 Test RE 0.14324632246126318\n",
      "56 Train Loss 43502.035 Test MSE 6594.12605958949 Test RE 0.14207401850678844\n",
      "57 Train Loss 43385.645 Test MSE 6556.966635946097 Test RE 0.1416731429950979\n",
      "58 Train Loss 43334.805 Test MSE 6555.134504571171 Test RE 0.14165334863294227\n",
      "59 Train Loss 43266.336 Test MSE 6517.240861915695 Test RE 0.14124332338439866\n",
      "60 Train Loss 43217.2 Test MSE 6528.52989920545 Test RE 0.14136559991865852\n",
      "61 Train Loss 43133.75 Test MSE 6558.755494949596 Test RE 0.14169246717413783\n",
      "62 Train Loss 42876.043 Test MSE 6467.792910483671 Test RE 0.14070647877881332\n",
      "63 Train Loss 42810.117 Test MSE 6428.30239598674 Test RE 0.1402762640963922\n",
      "64 Train Loss 42793.9 Test MSE 6443.483589615713 Test RE 0.1404418058814351\n",
      "65 Train Loss 42660.883 Test MSE 6422.419370270699 Test RE 0.14021206069426959\n",
      "66 Train Loss 42491.676 Test MSE 6362.023930844743 Test RE 0.13955123717678936\n",
      "67 Train Loss 42471.07 Test MSE 6357.432984477128 Test RE 0.13950087679822593\n",
      "68 Train Loss 42300.24 Test MSE 6345.927836485082 Test RE 0.13937459115487075\n",
      "69 Train Loss 42170.066 Test MSE 6311.367275204274 Test RE 0.13899454899050775\n",
      "70 Train Loss 42095.45 Test MSE 6296.6679960090105 Test RE 0.13883259433034115\n",
      "71 Train Loss 42071.31 Test MSE 6276.506155377359 Test RE 0.13861014613045994\n",
      "72 Train Loss 42055.613 Test MSE 6288.2472056877305 Test RE 0.13873973003341827\n",
      "73 Train Loss 41981.21 Test MSE 6265.86829183733 Test RE 0.13849263321072802\n",
      "74 Train Loss 41928.38 Test MSE 6264.274898011873 Test RE 0.13847502293492145\n",
      "75 Train Loss 41918.02 Test MSE 6260.8440491511055 Test RE 0.13843709740086407\n",
      "76 Train Loss 41901.688 Test MSE 6269.983497815461 Test RE 0.1385381043361498\n",
      "77 Train Loss 41881.293 Test MSE 6256.946365593033 Test RE 0.1383939987387923\n",
      "78 Train Loss 41849.008 Test MSE 6262.528643479801 Test RE 0.13845572066172687\n",
      "79 Train Loss 41822.01 Test MSE 6255.089170145621 Test RE 0.13837345806568874\n",
      "80 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "81 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "82 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "83 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "84 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "85 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "86 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "87 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "88 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "89 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "90 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "91 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "92 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "93 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "94 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "95 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "96 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "97 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "98 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "99 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "100 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "101 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "102 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "103 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "104 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "106 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "107 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "108 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "109 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "110 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "111 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "112 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "113 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "114 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "115 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "116 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "117 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "118 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "119 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "120 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "121 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "122 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "123 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "124 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "125 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "126 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "127 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "128 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "129 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "130 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "131 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "132 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "133 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "134 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "135 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "136 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "137 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "138 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "139 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "140 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "141 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "142 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "143 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "144 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "145 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "146 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "147 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "148 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "149 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "150 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "151 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "152 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "153 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "154 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "155 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "156 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "157 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "158 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "159 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "160 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "161 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "162 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "163 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "164 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "165 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "166 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "167 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "168 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "169 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "170 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "171 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "172 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "173 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "174 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "175 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "176 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "177 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "178 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "179 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "180 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "181 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "182 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "183 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "184 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "185 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "186 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "187 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "188 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "189 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "190 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "191 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "192 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "193 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "194 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "195 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "196 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "197 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "198 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "199 Train Loss 41819.742 Test MSE 6255.046226132249 Test RE 0.13837298306671822\n",
      "Training time: 101.13\n",
      "Training time: 101.13\n",
      "ES_stan\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 248609.86 Test MSE 74450.14779971335 Test RE 0.4773850606031881\n",
      "1 Train Loss 239815.34 Test MSE 71085.46492045635 Test RE 0.46647292974870286\n",
      "2 Train Loss 227579.88 Test MSE 65297.15867562906 Test RE 0.4470778809411198\n",
      "3 Train Loss 214233.38 Test MSE 60773.08234065968 Test RE 0.43131213155207176\n",
      "4 Train Loss 197976.08 Test MSE 53251.63453449357 Test RE 0.40374068287216247\n",
      "5 Train Loss 188107.66 Test MSE 50147.856775693544 Test RE 0.3917980140062186\n",
      "6 Train Loss 178483.06 Test MSE 46108.42228202149 Test RE 0.3756870047734654\n",
      "7 Train Loss 170352.92 Test MSE 43467.80839828237 Test RE 0.36477067251655176\n",
      "8 Train Loss 153703.31 Test MSE 37492.98443444887 Test RE 0.33877476104393367\n",
      "9 Train Loss 140197.97 Test MSE 33091.195182227486 Test RE 0.31826748219560497\n",
      "10 Train Loss 126308.625 Test MSE 27692.943064327614 Test RE 0.2911525500677811\n",
      "11 Train Loss 116053.94 Test MSE 23800.094490076666 Test RE 0.2699139820967133\n",
      "12 Train Loss 103375.46 Test MSE 20516.126133188263 Test RE 0.25060153224929826\n",
      "13 Train Loss 96696.12 Test MSE 17688.40589201408 Test RE 0.23269143051130675\n",
      "14 Train Loss 87612.195 Test MSE 14907.19640705187 Test RE 0.2136161229768021\n",
      "15 Train Loss 81964.01 Test MSE 14339.732404822451 Test RE 0.20951087244231398\n",
      "16 Train Loss 79673.6 Test MSE 13410.166347352833 Test RE 0.20260638344873266\n",
      "17 Train Loss 77203.96 Test MSE 14068.302620213979 Test RE 0.20751853496321193\n",
      "18 Train Loss 73598.31 Test MSE 12192.279289978547 Test RE 0.19318726455518878\n",
      "19 Train Loss 70490.484 Test MSE 11513.991858901782 Test RE 0.1877366227153418\n",
      "20 Train Loss 67459.34 Test MSE 10805.566582517215 Test RE 0.18186947524743322\n",
      "21 Train Loss 65900.27 Test MSE 9987.204110724646 Test RE 0.17484692735812427\n",
      "22 Train Loss 63091.73 Test MSE 9958.151875695285 Test RE 0.17459243203126282\n",
      "23 Train Loss 60701.59 Test MSE 9523.899266390285 Test RE 0.17074320862497147\n",
      "24 Train Loss 59076.0 Test MSE 9401.53618979905 Test RE 0.16964280817099722\n",
      "25 Train Loss 57234.598 Test MSE 8812.989389109563 Test RE 0.16424708305817304\n",
      "26 Train Loss 55139.844 Test MSE 8559.246522330553 Test RE 0.16186531970017148\n",
      "27 Train Loss 52969.94 Test MSE 8665.162075427725 Test RE 0.16286373373098842\n",
      "28 Train Loss 51477.32 Test MSE 8485.66384002756 Test RE 0.16116805077607918\n",
      "29 Train Loss 50475.33 Test MSE 8382.470813807055 Test RE 0.16018508161047404\n",
      "30 Train Loss 49507.69 Test MSE 8033.433507598847 Test RE 0.15681465378015327\n",
      "31 Train Loss 48293.043 Test MSE 8191.792494460064 Test RE 0.15835271475389873\n",
      "32 Train Loss 47009.27 Test MSE 7875.299059612249 Test RE 0.15526357054312076\n",
      "33 Train Loss 46059.168 Test MSE 7901.0134298674275 Test RE 0.1555168467136607\n",
      "34 Train Loss 45276.305 Test MSE 8440.761179919898 Test RE 0.1607410674899791\n",
      "35 Train Loss 44608.504 Test MSE 8168.181479976914 Test RE 0.15812434188879962\n",
      "36 Train Loss 44087.28 Test MSE 7542.11090516895 Test RE 0.15194363046505002\n",
      "37 Train Loss 43680.53 Test MSE 7438.729666026162 Test RE 0.15089867622721206\n",
      "38 Train Loss 43330.4 Test MSE 7366.721961378283 Test RE 0.15016654241284103\n",
      "39 Train Loss 43143.21 Test MSE 7126.061465697995 Test RE 0.1476933100512025\n",
      "40 Train Loss 42889.426 Test MSE 7032.955825671561 Test RE 0.1467252934058034\n",
      "41 Train Loss 42569.875 Test MSE 6931.9775086118525 Test RE 0.14566815323194068\n",
      "42 Train Loss 42416.02 Test MSE 6877.731754357395 Test RE 0.14509707531579252\n",
      "43 Train Loss 42131.33 Test MSE 6652.587277072486 Test RE 0.14270241808924206\n",
      "44 Train Loss 41923.414 Test MSE 6873.809174313701 Test RE 0.14505569277230956\n",
      "45 Train Loss 41751.945 Test MSE 6744.963469700257 Test RE 0.14368976774246386\n",
      "46 Train Loss 41511.156 Test MSE 6644.65382877137 Test RE 0.14261730383709725\n",
      "47 Train Loss 41394.55 Test MSE 6584.381403350942 Test RE 0.1419690027458915\n",
      "48 Train Loss 41255.19 Test MSE 6396.201498404286 Test RE 0.1399255782216862\n",
      "49 Train Loss 41041.83 Test MSE 6190.170865716008 Test RE 0.1376535323253747\n",
      "50 Train Loss 40831.723 Test MSE 6147.931564694098 Test RE 0.13718308151883887\n",
      "51 Train Loss 40520.008 Test MSE 6093.178434674935 Test RE 0.13657084295586483\n",
      "52 Train Loss 40381.027 Test MSE 6127.915417200056 Test RE 0.13695958233180494\n",
      "53 Train Loss 40064.203 Test MSE 6102.133059297371 Test RE 0.13667115937225713\n",
      "54 Train Loss 39780.43 Test MSE 6139.83329421209 Test RE 0.13709270056930462\n",
      "55 Train Loss 39596.86 Test MSE 6163.882544210463 Test RE 0.13736092889719562\n",
      "56 Train Loss 39467.88 Test MSE 6126.1530810692475 Test RE 0.13693988671262874\n",
      "57 Train Loss 39272.777 Test MSE 6096.328778510956 Test RE 0.13660614386819475\n",
      "58 Train Loss 39154.805 Test MSE 6012.561145277945 Test RE 0.13566436764544462\n",
      "59 Train Loss 39044.066 Test MSE 6082.131703541164 Test RE 0.1364469875815408\n",
      "60 Train Loss 38826.25 Test MSE 6032.627799926487 Test RE 0.1358905659589972\n",
      "61 Train Loss 38619.48 Test MSE 5997.41458450047 Test RE 0.1354933802498696\n",
      "62 Train Loss 38514.45 Test MSE 5960.823032972764 Test RE 0.1350794103280086\n",
      "63 Train Loss 38239.71 Test MSE 5725.3880209780355 Test RE 0.1323849160448285\n",
      "64 Train Loss 37988.465 Test MSE 5669.407017357481 Test RE 0.13173611763678217\n",
      "65 Train Loss 37893.215 Test MSE 5659.963924075008 Test RE 0.13162636057729157\n",
      "66 Train Loss 37830.824 Test MSE 5695.348555740959 Test RE 0.1320371665086427\n",
      "67 Train Loss 37755.152 Test MSE 5628.257205873258 Test RE 0.1312571620067314\n",
      "68 Train Loss 37626.37 Test MSE 5546.866749000285 Test RE 0.13030464829650082\n",
      "69 Train Loss 37577.562 Test MSE 5577.313129465358 Test RE 0.13066177565324963\n",
      "70 Train Loss 37509.13 Test MSE 5570.508585865981 Test RE 0.13058204504851784\n",
      "71 Train Loss 37283.777 Test MSE 5559.942686635299 Test RE 0.13045814508714323\n",
      "72 Train Loss 37198.625 Test MSE 5538.208383835375 Test RE 0.13020290925195352\n",
      "73 Train Loss 37065.695 Test MSE 5459.0952622228815 Test RE 0.12926959205873895\n",
      "74 Train Loss 36926.465 Test MSE 5385.555264383453 Test RE 0.12839593825402562\n",
      "75 Train Loss 36650.848 Test MSE 5401.879125687939 Test RE 0.1285903779712477\n",
      "76 Train Loss 36387.035 Test MSE 5436.270057314997 Test RE 0.12899906228521407\n",
      "77 Train Loss 36293.65 Test MSE 5480.491645201313 Test RE 0.1295226739977012\n",
      "78 Train Loss 36227.477 Test MSE 5480.384599608859 Test RE 0.12952140906564993\n",
      "79 Train Loss 36054.902 Test MSE 5437.259732561313 Test RE 0.12901080391757033\n",
      "80 Train Loss 35858.17 Test MSE 5328.1672150638 Test RE 0.12771001761900228\n",
      "81 Train Loss 35689.55 Test MSE 5282.470169048112 Test RE 0.12716118559853049\n",
      "82 Train Loss 35571.336 Test MSE 5188.5863360344965 Test RE 0.1260261199799989\n",
      "83 Train Loss 35453.543 Test MSE 5176.001366236953 Test RE 0.12587318836095945\n",
      "84 Train Loss 35285.055 Test MSE 5006.259797266889 Test RE 0.12379204389724656\n",
      "85 Train Loss 35160.938 Test MSE 5017.66992776899 Test RE 0.12393303532853875\n",
      "86 Train Loss 35087.465 Test MSE 5043.288444849555 Test RE 0.12424901249999812\n",
      "87 Train Loss 34952.84 Test MSE 4983.452713894027 Test RE 0.12350974148959425\n",
      "88 Train Loss 34858.062 Test MSE 5042.347379992212 Test RE 0.12423741968356966\n",
      "89 Train Loss 34802.074 Test MSE 5044.104020365765 Test RE 0.12425905855993719\n",
      "90 Train Loss 34761.754 Test MSE 5102.854570314844 Test RE 0.12498060926632713\n",
      "91 Train Loss 34696.652 Test MSE 5007.627272038053 Test RE 0.12380894982555721\n",
      "92 Train Loss 34474.105 Test MSE 5005.287534871277 Test RE 0.12378002252817287\n",
      "93 Train Loss 34339.953 Test MSE 4972.268777480203 Test RE 0.12337107247406127\n",
      "94 Train Loss 34283.39 Test MSE 5063.482414790476 Test RE 0.12449751842992049\n",
      "95 Train Loss 34197.387 Test MSE 4957.1253264946445 Test RE 0.12318306087105649\n",
      "96 Train Loss 34137.46 Test MSE 4905.632863399444 Test RE 0.12254160468540959\n",
      "97 Train Loss 34058.625 Test MSE 4931.700997667036 Test RE 0.12286676135679056\n",
      "98 Train Loss 34011.965 Test MSE 4947.967921163996 Test RE 0.12306922890317189\n",
      "99 Train Loss 33915.01 Test MSE 4919.547871571218 Test RE 0.12271527850053282\n",
      "100 Train Loss 33815.45 Test MSE 4952.656197130506 Test RE 0.12312752009533699\n",
      "101 Train Loss 33566.133 Test MSE 4919.820274861941 Test RE 0.12271867592483175\n",
      "102 Train Loss 33201.844 Test MSE 4865.385578430329 Test RE 0.12203788533186834\n",
      "103 Train Loss 32815.66 Test MSE 4783.504776063196 Test RE 0.12100662489333006\n",
      "104 Train Loss 32370.066 Test MSE 4904.463173357098 Test RE 0.12252699451747014\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105 Train Loss 32224.396 Test MSE 4964.584058577593 Test RE 0.12327569965196673\n",
      "106 Train Loss 32081.008 Test MSE 4972.849284735967 Test RE 0.12337827398666279\n",
      "107 Train Loss 31978.197 Test MSE 5030.600970391544 Test RE 0.12409262655401743\n",
      "108 Train Loss 31711.61 Test MSE 5069.215938218051 Test RE 0.12456798450720051\n",
      "109 Train Loss 31452.793 Test MSE 5098.293810797977 Test RE 0.12492474505195657\n",
      "110 Train Loss 31143.15 Test MSE 5089.910127195263 Test RE 0.12482198906244091\n",
      "111 Train Loss 31012.14 Test MSE 5053.800744575508 Test RE 0.12437843826527399\n",
      "112 Train Loss 30852.129 Test MSE 4892.978913905164 Test RE 0.12238345622962221\n",
      "113 Train Loss 30749.648 Test MSE 4933.555147635173 Test RE 0.12288985602466337\n",
      "114 Train Loss 30668.756 Test MSE 5005.809904360963 Test RE 0.12378648141987797\n",
      "115 Train Loss 30555.414 Test MSE 4966.439823797252 Test RE 0.12329873777343564\n",
      "116 Train Loss 30444.863 Test MSE 4987.990510902762 Test RE 0.1235659610064932\n",
      "117 Train Loss 30311.168 Test MSE 5013.975618941593 Test RE 0.12388740346976546\n",
      "118 Train Loss 30175.754 Test MSE 4989.223411809096 Test RE 0.12358123120119856\n",
      "119 Train Loss 29951.568 Test MSE 4714.866172718956 Test RE 0.12013532481392027\n",
      "120 Train Loss 29788.678 Test MSE 4730.244282486655 Test RE 0.12033108331181383\n",
      "121 Train Loss 29508.48 Test MSE 4832.859285683034 Test RE 0.12162927471320242\n",
      "122 Train Loss 29444.719 Test MSE 4766.668190203413 Test RE 0.1207934826053077\n",
      "123 Train Loss 29317.53 Test MSE 4781.290608184216 Test RE 0.12097861614178124\n",
      "124 Train Loss 29122.924 Test MSE 4761.441231102864 Test RE 0.12072723551710161\n",
      "125 Train Loss 28860.03 Test MSE 4745.650568723336 Test RE 0.1205268816644375\n",
      "126 Train Loss 28762.672 Test MSE 4699.5190318022815 Test RE 0.1199396419990546\n",
      "127 Train Loss 28638.594 Test MSE 4767.742217019886 Test RE 0.12080709044701332\n",
      "128 Train Loss 28435.62 Test MSE 4719.945752791453 Test RE 0.12020002153146574\n",
      "129 Train Loss 28321.207 Test MSE 4636.524010082249 Test RE 0.11913306037578546\n",
      "130 Train Loss 28240.332 Test MSE 4714.772030311119 Test RE 0.12013412542831879\n",
      "131 Train Loss 27949.4 Test MSE 4687.462992031406 Test RE 0.11978569798289444\n",
      "132 Train Loss 27712.0 Test MSE 4616.262734069466 Test RE 0.11887247393100722\n",
      "133 Train Loss 27318.43 Test MSE 4706.707100016171 Test RE 0.1200313327559824\n",
      "134 Train Loss 27173.268 Test MSE 4660.7560170209845 Test RE 0.11944396905196494\n",
      "135 Train Loss 27097.287 Test MSE 4684.639447556909 Test RE 0.11974961543736308\n",
      "136 Train Loss 27021.312 Test MSE 4680.974623799566 Test RE 0.11970276582331198\n",
      "137 Train Loss 26882.9 Test MSE 4599.326464148512 Test RE 0.11865421227777707\n",
      "138 Train Loss 26794.914 Test MSE 4630.584922630497 Test RE 0.11905673505250011\n",
      "139 Train Loss 26575.297 Test MSE 4762.551240710357 Test RE 0.12074130694795097\n",
      "140 Train Loss 26343.736 Test MSE 4765.053043082224 Test RE 0.12077301592211773\n",
      "141 Train Loss 26251.73 Test MSE 4852.8116506332935 Test RE 0.12188008815003215\n",
      "142 Train Loss 26190.246 Test MSE 4966.8035761429155 Test RE 0.12330325301830654\n",
      "143 Train Loss 26140.553 Test MSE 4934.358704968066 Test RE 0.12289986351635636\n",
      "144 Train Loss 26056.94 Test MSE 4979.055576141093 Test RE 0.12345524020006392\n",
      "145 Train Loss 25995.832 Test MSE 4956.698513434093 Test RE 0.12317775766935009\n",
      "146 Train Loss 25963.977 Test MSE 4981.578460343129 Test RE 0.12348651358372376\n",
      "147 Train Loss 25923.162 Test MSE 4964.487410769874 Test RE 0.12327449971417118\n",
      "148 Train Loss 25754.695 Test MSE 5031.261978180459 Test RE 0.12410077900924614\n",
      "149 Train Loss 25635.98 Test MSE 5107.5807368445685 Test RE 0.12503847319901398\n",
      "150 Train Loss 25457.219 Test MSE 5278.404682205849 Test RE 0.12711224337865276\n",
      "151 Train Loss 25354.895 Test MSE 5286.361640277855 Test RE 0.12720801529922185\n",
      "152 Train Loss 25255.43 Test MSE 5333.155706669867 Test RE 0.12776978782501183\n",
      "153 Train Loss 25103.78 Test MSE 5401.56641516206 Test RE 0.12858665591955476\n",
      "154 Train Loss 24998.523 Test MSE 5401.716486541564 Test RE 0.1285884421646294\n",
      "155 Train Loss 24873.521 Test MSE 5264.499011342783 Test RE 0.12694469780279835\n",
      "156 Train Loss 24704.168 Test MSE 5278.323481635522 Test RE 0.12711126565650227\n",
      "157 Train Loss 24635.463 Test MSE 5405.454404191892 Test RE 0.1286329252364585\n",
      "158 Train Loss 24577.238 Test MSE 5459.318334703067 Test RE 0.1292722331734797\n",
      "159 Train Loss 24508.574 Test MSE 5504.932793316882 Test RE 0.1298111664763381\n",
      "160 Train Loss 24433.32 Test MSE 5557.705141866481 Test RE 0.13043189164073835\n",
      "161 Train Loss 24389.596 Test MSE 5569.728750488924 Test RE 0.13057290440534397\n",
      "162 Train Loss 24294.615 Test MSE 5459.941620623977 Test RE 0.12927961241556987\n",
      "163 Train Loss 24187.107 Test MSE 5444.459529300331 Test RE 0.12909619107283088\n",
      "164 Train Loss 24092.09 Test MSE 5309.244238289223 Test RE 0.1274830349527587\n",
      "165 Train Loss 24044.828 Test MSE 5242.231265892823 Test RE 0.1266759383524442\n",
      "166 Train Loss 23996.893 Test MSE 5170.571386416862 Test RE 0.12580714623934275\n",
      "167 Train Loss 23927.406 Test MSE 5274.722204239972 Test RE 0.12706789572699537\n",
      "168 Train Loss 23846.002 Test MSE 5223.062957227009 Test RE 0.1264441298832662\n",
      "169 Train Loss 23776.984 Test MSE 5217.5097078543495 Test RE 0.1263768932319662\n",
      "170 Train Loss 23734.045 Test MSE 5272.207649780936 Test RE 0.12703760434665057\n",
      "171 Train Loss 23626.197 Test MSE 5453.454762874705 Test RE 0.12920279220861203\n",
      "172 Train Loss 23514.043 Test MSE 5375.919599141067 Test RE 0.12828102585453305\n",
      "173 Train Loss 23418.719 Test MSE 5382.692100119395 Test RE 0.12836180365536423\n",
      "174 Train Loss 23379.99 Test MSE 5336.461389430673 Test RE 0.12780937985822616\n",
      "175 Train Loss 23320.516 Test MSE 5280.046785778484 Test RE 0.127132014052754\n",
      "176 Train Loss 23267.443 Test MSE 5276.67524193661 Test RE 0.12709141785888206\n",
      "177 Train Loss 23236.99 Test MSE 5303.112198975365 Test RE 0.12740939388471978\n",
      "178 Train Loss 23175.15 Test MSE 5409.702250194675 Test RE 0.1286834580418188\n",
      "179 Train Loss 23072.738 Test MSE 5382.719347545931 Test RE 0.1283621285415293\n",
      "180 Train Loss 22905.508 Test MSE 5217.388441101998 Test RE 0.1263754245807013\n",
      "181 Train Loss 22749.326 Test MSE 5186.883318089129 Test RE 0.12600543589148783\n",
      "182 Train Loss 22675.508 Test MSE 5103.5393012016375 Test RE 0.1249889942998007\n",
      "183 Train Loss 22625.355 Test MSE 5104.233056811538 Test RE 0.12499748927399763\n",
      "184 Train Loss 22588.838 Test MSE 5110.514970559847 Test RE 0.1250743844693592\n",
      "185 Train Loss 22472.27 Test MSE 5231.365015864373 Test RE 0.12654458145520583\n",
      "186 Train Loss 22354.006 Test MSE 5435.885441137884 Test RE 0.12899449886183162\n",
      "187 Train Loss 22272.896 Test MSE 5437.16251811072 Test RE 0.12900965060035724\n",
      "188 Train Loss 22242.215 Test MSE 5436.545535793158 Test RE 0.12900233070407516\n",
      "189 Train Loss 22199.576 Test MSE 5624.924703400028 Test RE 0.13121829743740387\n",
      "190 Train Loss 22107.121 Test MSE 5642.668305541247 Test RE 0.1314250959418668\n",
      "191 Train Loss 22067.307 Test MSE 5622.979056144681 Test RE 0.1311956014356936\n",
      "192 Train Loss 21977.367 Test MSE 5578.38533287605 Test RE 0.13067433450244398\n",
      "193 Train Loss 21887.543 Test MSE 5642.710691907113 Test RE 0.13142558955778014\n",
      "194 Train Loss 21792.488 Test MSE 5753.566068676355 Test RE 0.13271028874515917\n",
      "195 Train Loss 21725.44 Test MSE 5708.5755543519535 Test RE 0.13219040055352932\n",
      "196 Train Loss 21442.58 Test MSE 5913.999479822403 Test RE 0.13454782536070345\n",
      "197 Train Loss 21279.02 Test MSE 5709.003030990238 Test RE 0.13219534988367856\n",
      "198 Train Loss 21200.54 Test MSE 5557.452331689732 Test RE 0.1304289250489766\n",
      "199 Train Loss 21144.34 Test MSE 5478.518873718008 Test RE 0.12949936024661196\n",
      "Training time: 181.56\n",
      "Training time: 181.56\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 200 #200\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "beta_init =1.0\n",
    "\n",
    "N_T = 5000 #Total number of data points for 'y'\n",
    "N_f = 10000 #Total number of collocation points \n",
    "\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(label)\n",
    "    print(reps)\n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []\n",
    "    beta_val = []\n",
    "    \n",
    "    torch.manual_seed(reps*36)\n",
    "    \n",
    "    layers = np.array([2,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "\n",
    "    PINN = Sequentialmodel(layers,beta_init)\n",
    "   \n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "    \n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.5, \n",
    "                              max_iter = 20, \n",
    "                              max_eval = 30, \n",
    "                              tolerance_grad = 1e-08, \n",
    "                              tolerance_change = 1e-08, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "    \n",
    "    nan_flag = train_model(max_iter,reps)\n",
    "    \n",
    "    \n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    #elapsed_time[reps] = time.time() - start_time\n",
    "    beta_full.append(beta_val)\n",
    "\n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "    \n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"beta\": beta_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred = PINN.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "cannot reshape array of size 1541 into shape (500,500)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_23641/2850883497.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mcmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mimg3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_pred\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m500\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvmin\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvmax\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m500\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mextent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maspect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.75\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m: cannot reshape array of size 1541 into shape (500,500)"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "cmap = plt.cm.jet\n",
    "\n",
    "img3 = ax.imshow(np.transpose(u_pred.reshape(500,500)),vmin = 0,vmax = 500,cmap = cmap,extent=[0,1,0,1],aspect = 0.75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.12949936024661193\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + test_re_loss[-1]\n",
    "print(a/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + train_loss_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
