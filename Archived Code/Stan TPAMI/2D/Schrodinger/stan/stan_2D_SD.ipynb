{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 286,
     "status": "ok",
     "timestamp": 1660687093981,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "iAtv2UvNSq_u",
    "outputId": "68a82578-1b95-4343-a8ec-7635a4df93ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1256,
     "status": "ok",
     "timestamp": 1660687393066,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "xAgfGYA4acPE",
    "outputId": "527d048f-6a89-4e80-87ff-bfdb1c9d6222"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1856,
     "status": "ok",
     "timestamp": 1660687061284,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "7kSdyTofacUc",
    "outputId": "08ee5c9b-0706-46a5-86a1-2c7e56a6a74d"
   },
   "outputs": [],
   "source": [
    "# %cd '/content/gdrive/MyDrive/Virginia Tech /Fall 2022/Codes from GPU/PINN_Stan/2D Klein Gordon/stan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 32419,
     "status": "ok",
     "timestamp": 1660687093700,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "RHuSaD0gagsN",
    "outputId": "c232cd79-e56c-4a76-97c7-d59dafa084ef"
   },
   "outputs": [],
   "source": [
    "# !pip install smt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 280,
     "status": "ok",
     "timestamp": 1660687410736,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "mTLFQRt5Sq_y"
   },
   "outputs": [],
   "source": [
    "#https://chem.libretexts.org/Bookshelves/Physical_and_Theoretical_Chemistry_Textbook_Maps/Supplemental_Modules_(Physical_and_Theoretical_Chemistry)/Quantum_Mechanics/05.5%3A_Particle_in_Boxes/Particle_in_a_2-Dimensional_Box\n",
    "def true_2D_SD(xy): \n",
    "    y = 2 * np.sin(np.pi*xy[:,0].reshape(-1,1))*np.sin(np.pi*xy[:,1].reshape(-1,1))  \n",
    "    \n",
    "    return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 4312,
     "status": "ok",
     "timestamp": 1660687098957,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "81bNHCY3Sq_y"
   },
   "outputs": [],
   "source": [
    "loss_thresh = 0.1\n",
    "label = \"SD_stan\"\n",
    "\n",
    "x = np.linspace(0,1,500).reshape(-1,1)\n",
    "y = np.linspace(0,1,500).reshape(-1,1)\n",
    "\n",
    "X,Y = np.meshgrid(x,y)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "Y = Y.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xy = np.hstack((X,Y))\n",
    "\n",
    "u_true = true_2D_SD(xy)\n",
    "u_true_norm = np.linalg.norm(u_true,2)\n",
    "\n",
    "#bound_pts_idx = ((X == -5) + (X == 5) + (T == 0)).reshape(-1,)\n",
    "\n",
    "#xt_bound = xt[bound_pts_idx,:]\n",
    "#y_bound = y_true[bound_pts_idx,:]\n",
    "pi = torch.from_numpy(np.array(np.pi).reshape(-1,)).float().to(device)\n",
    "\n",
    "xy_test_tensor = torch.from_numpy(xy).float().to(device)\n",
    "\n",
    "\n",
    "lb_xy = xy[0]\n",
    "ub_xy = xy[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1660687098958,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "YQgCA-PuSq_z"
   },
   "outputs": [],
   "source": [
    "def trainingdata(N_T,N_f,seed):\n",
    "    '''Boundary Conditions''' \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    N_t = int(N_T/4)\n",
    "    \n",
    "    x_BC1 = np.random.uniform(size = N_t).reshape(-1,1)\n",
    "    y_BC1 = np.zeros((N_t,1))\n",
    "    u_BC1 = 0*np.ones((N_t,1))\n",
    "    \n",
    "    x_BC2 = np.ones((N_t,1))\n",
    "    y_BC2 = np.random.uniform(size = N_t).reshape(-1,1) \n",
    "    u_BC2 = 0*np.ones((N_t,1))\n",
    "    \n",
    "    x_BC3 = np.random.uniform(size = N_t).reshape(-1,1)\n",
    "    y_BC3 = np.ones((N_t,1)) \n",
    "    u_BC3 = 0*np.ones((N_t,1))\n",
    "    \n",
    "    x_BC4 = np.zeros((N_t,1))\n",
    "    y_BC4 = np.random.uniform(size = N_t).reshape(-1,1) \n",
    "    u_BC4 = 0*np.ones((N_t,1))\n",
    "    \n",
    "    XY_corners = np.array([[0,0],[1,0],[0,1],[1,1]]).reshape(-1,2)\n",
    "    U_corners = 0*np.ones((4,1))\n",
    "    \n",
    "    XY_1 = np.hstack((x_BC1,y_BC1))\n",
    "    XY_2 = np.hstack((x_BC2,y_BC2))\n",
    "    XY_3 = np.hstack((x_BC3,y_BC3))\n",
    "    XY_4 = np.hstack((x_BC4,y_BC4))\n",
    "    \n",
    "    xy_BC = np.vstack((XY_1,XY_2,XY_3,XY_4,XY_corners)) #choose indices from  set 'idx' (x,t)\n",
    "    u_BC = np.vstack((u_BC1,u_BC2,u_BC3,u_BC4,U_corners))\n",
    "    \n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    samples = sampling(N_f)\n",
    "    \n",
    "    xy_coll = lb_xy + (ub_xy - lb_xy)*samples\n",
    "    \n",
    "    xy_coll = np.vstack((xy_coll, xy_BC)) # append training points to collocation points \n",
    "\n",
    "    return xy_coll, xy_BC, u_BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1660687098958,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "gTJxct8bSq_0"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,beta_init):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "            \n",
    "        \n",
    "        self.beta = Parameter(beta_init*torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "\n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xy):\n",
    "        if torch.is_tensor(xy) != True:         \n",
    "            xy = torch.from_numpy(xy)                \n",
    "        \n",
    "        ubxy = torch.from_numpy(ub_xy).float().to(device)\n",
    "        lbxy = torch.from_numpy(lb_xy).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xy = (xy - lbxy)/(ubxy - lbxy)\n",
    "        \n",
    "        #convert to float\n",
    "        a = xy.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            z1 = self.activation(z)\n",
    "            a = z1 + self.beta[:,i]*z*z1            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC(self,xy,u):\n",
    "                \n",
    "        loss_bc = self.loss_function(self.forward(xy), u)\n",
    "                \n",
    "        return loss_bc\n",
    "    \n",
    "    def loss_PDE(self, xy_coll, f_hat):\n",
    "        \n",
    "        g = xy_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        \n",
    "        u_x_y = autograd.grad(u,g,torch.ones([xy_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_xx_yy = autograd.grad(u_x_y,g,torch.ones(xy_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        #du_dx = u_x_t[:,[0]]\n",
    "        \n",
    "        d2u_dx2 = u_xx_yy[:,[0]]\n",
    "        d2u_dy2 = u_xx_yy[:,[1]]    \n",
    "        \n",
    "\n",
    "        f = (1/2)*(d2u_dx2+d2u_dy2) + torch.pow(pi,2)*u         \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xy_BC,y_BC,xy_coll,f_hat):\n",
    "\n",
    "        loss_BC = self.loss_BC(xy_BC,y_BC)\n",
    "        loss_f = self.loss_PDE(xy_coll,f_hat)\n",
    "        \n",
    "        loss_val = 100000000*loss_BC + 100000000000*loss_f\n",
    "        \n",
    "        return loss_val\n",
    "         \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        y_pred = self.forward(xy_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return y_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "        \n",
    "        return test_mse, test_re  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1660687098958,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "VoQzfzYsYKVs"
   },
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1660687098959,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "_IUDZDkxXmyF"
   },
   "outputs": [],
   "source": [
    "def train_step(xt_BC, y_BC, xt_coll, f_hat,seed):\n",
    "    # x_coll_np_array = colloc_pts(N_f,seed*123)\n",
    "    # x_coll_train = torch.from_numpy(x_coll_np_array).float().to(device)        \n",
    "    \n",
    "    # f_hat = torch.zeros(x_coll_train.shape[0],1).to(device)\n",
    "#     xt_coll, xt_BC, y_BC = trainingdata(N_I,N_B,N_f,seed*123)\n",
    "#     xt_coll = torch.from_numpy(xt_coll).float().to(device)\n",
    "#     xt_BC = torch.from_numpy(xt_BC).float().to(device)\n",
    "#     y_BC = torch.from_numpy(y_BC).float().to(device)\n",
    "\n",
    "#     f_hat = torch.zeros(xt_coll.shape[0],1).to(device)\n",
    "    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(xt_BC, y_BC, xt_coll,f_hat)\n",
    "        loss.backward()\n",
    "        #print(loss.cpu().detach().numpy())\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "executionInfo": {
     "elapsed": 467,
     "status": "ok",
     "timestamp": 1660690085956,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "Vt9Dlr8MYIwW"
   },
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*9)\n",
    "    start_time = time.time() \n",
    "    thresh_flag = 0\n",
    "\n",
    "    xy_coll, xy_BC, u_BC = trainingdata(N_T,N_f,rep*11)\n",
    "    xy_coll = torch.from_numpy(xy_coll).float().to(device)\n",
    "    xy_BC = torch.from_numpy(xy_BC).float().to(device)\n",
    "    u_BC = torch.from_numpy(u_BC).float().to(device)\n",
    "\n",
    "    f_hat = torch.zeros(xy_coll.shape[0],1).to(device)\n",
    "    \n",
    "\n",
    "    for i in range(max_iter):\n",
    "        train_step(xy_BC, u_BC, xy_coll,f_hat,i)\n",
    "        loss_np = PINN.loss(xy_BC, u_BC, xy_coll,f_hat).cpu().detach().numpy()\n",
    "        \n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1          \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        \n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])   \n",
    "        \n",
    "      \n",
    "         \n",
    "\n",
    "    elapsed_time[rep] = time.time() - start_time  \n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "sP4Re5lSSq_1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SD_stan\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 465895500.0 Test MSE 1.0098465424490477 Test RE 1.006925061349366\n",
      "1 Train Loss 31665684.0 Test MSE 0.9786976971919796 Test RE 0.9912740608680952\n",
      "2 Train Loss 8612959.0 Test MSE 0.9672348654801763 Test RE 0.9854518977553207\n",
      "3 Train Loss 3892501.8 Test MSE 0.9690149853153179 Test RE 0.9863583043309313\n",
      "4 Train Loss 2362167.5 Test MSE 0.97101765836531 Test RE 0.9873770365777729\n",
      "5 Train Loss 1311562.6 Test MSE 0.977987165246929 Test RE 0.9909141643593133\n",
      "6 Train Loss 509157.06 Test MSE 0.9865967036540184 Test RE 0.9952662765708933\n",
      "7 Train Loss 400959.62 Test MSE 0.9878503546347018 Test RE 0.9958984094514486\n",
      "8 Train Loss 243758.2 Test MSE 0.9899589960211217 Test RE 0.9969607531248326\n",
      "9 Train Loss 211145.66 Test MSE 0.9916512473499999 Test RE 0.9978124994039363\n",
      "10 Train Loss 154815.5 Test MSE 0.988958733394692 Test RE 0.9964569572033997\n",
      "11 Train Loss 117622.734 Test MSE 0.9913592890203132 Test RE 0.9976656024394444\n",
      "12 Train Loss 79666.16 Test MSE 0.9939951440550298 Test RE 0.9989910332232885\n",
      "13 Train Loss 59923.645 Test MSE 0.9948359407757172 Test RE 0.9994134552249321\n",
      "14 Train Loss 55376.812 Test MSE 0.994879032966201 Test RE 0.9994351002253082\n",
      "15 Train Loss 44773.48 Test MSE 0.995032064724877 Test RE 0.9995119635549005\n",
      "16 Train Loss 41342.14 Test MSE 0.9946354697653554 Test RE 0.9993127534351549\n",
      "17 Train Loss 33476.336 Test MSE 0.9945472960673539 Test RE 0.999268458285582\n",
      "18 Train Loss 12868.328 Test MSE 0.9949519611909399 Test RE 0.9994717306544935\n",
      "19 Train Loss 10632.758 Test MSE 0.9949952072601053 Test RE 0.9994934516800348\n",
      "20 Train Loss 10491.829 Test MSE 0.9950399267953747 Test RE 0.9995159122809153\n",
      "21 Train Loss 10030.092 Test MSE 0.9951153524684577 Test RE 0.9995537940431485\n",
      "22 Train Loss 7697.6143 Test MSE 0.9949847694953715 Test RE 0.9994882091900282\n",
      "23 Train Loss 6019.0054 Test MSE 0.994686929137265 Test RE 0.9993386037808761\n",
      "24 Train Loss 5950.4385 Test MSE 0.994617831749788 Test RE 0.999303892916614\n",
      "25 Train Loss 5934.769 Test MSE 0.9946335189480086 Test RE 0.9993117734391316\n",
      "26 Train Loss 5843.496 Test MSE 0.9946226827573343 Test RE 0.9993063298449795\n",
      "27 Train Loss 5791.211 Test MSE 0.9946747150416914 Test RE 0.999332468154515\n",
      "28 Train Loss 5702.494 Test MSE 0.9947581148262663 Test RE 0.9993743624363096\n",
      "29 Train Loss 5599.111 Test MSE 0.9947474099601009 Test RE 0.9993689851504043\n",
      "30 Train Loss 5397.2188 Test MSE 0.9948904797417235 Test RE 0.9994408498068934\n",
      "31 Train Loss 5219.6455 Test MSE 0.9948808507343522 Test RE 0.9994360132712186\n",
      "32 Train Loss 5079.7397 Test MSE 0.9949437536267339 Test RE 0.9994676082216327\n",
      "33 Train Loss 4909.419 Test MSE 0.9950114375619408 Test RE 0.9995016034852561\n",
      "34 Train Loss 4798.125 Test MSE 0.9949939015774127 Test RE 0.9994927958870623\n",
      "35 Train Loss 4757.5654 Test MSE 0.9950163049214776 Test RE 0.9995040481444472\n",
      "36 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "37 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "38 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "39 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "40 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "41 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "42 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "43 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "44 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "45 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "46 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "47 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "48 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "49 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "50 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "51 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "52 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "53 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "54 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "55 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "56 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "57 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "58 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "59 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "60 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "61 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "62 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "63 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "64 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "65 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "66 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "67 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "68 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "69 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "70 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "71 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "72 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "73 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "74 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "75 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "76 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "77 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "78 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "79 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "80 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "81 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "82 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "83 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "84 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "85 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "86 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "87 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "88 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "89 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "90 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "91 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "92 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "93 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "94 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "95 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "97 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "98 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "99 Train Loss 4756.1074 Test MSE 0.995018537782908 Test RE 0.9995051696098822\n",
      "Training time: 121.33\n"
     ]
    }
   ],
   "source": [
    "max_reps = 1 #10\n",
    "max_iter = 100 #200\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "beta_init = 1\n",
    "\n",
    "N_T = 5000\n",
    "N_f = 10000 #Total number of collocation points\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(label)\n",
    "    print(reps)\n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []\n",
    "    beta_val = []\n",
    "\n",
    "    torch.manual_seed(reps*36)\n",
    "\n",
    "    layers = np.array([2,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "    #layers = np.array([2,50,50,50,50,50,50,50,1])\n",
    "\n",
    "    PINN = Sequentialmodel(layers,beta_init)\n",
    "\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.1, \n",
    "                            max_iter = 20, \n",
    "                            max_eval = 30, \n",
    "                            tolerance_grad = 1e-8, \n",
    "                            tolerance_change = 1e-8, \n",
    "                            history_size = 100, \n",
    "                            line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    nan_flag = train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "\n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    #elapsed_time[reps] = time.time() - start_time\n",
    "    beta_full.append(beta_val)\n",
    "\n",
    "\n",
    "  #print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"beta\": beta_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nan_tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1660688516819,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "_O3sPdAnSq_2"
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "aborted",
     "timestamp": 1660688516819,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "jQ4afiEWSq_2"
   },
   "outputs": [],
   "source": [
    "for tune_reps in range(20):\n",
    "    label = \"KG_stan_tune\"+str(tune_reps)+\".mat\"\n",
    "    data = sio.loadmat(label)\n",
    "    re = np.array(data[\"test_re_loss\"])\n",
    "    print(tune_reps,\" \",np.mean(re[:,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1660688534316,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "06syezgfv_qO",
    "outputId": "9f4852d5-694a-4977-8893-a6183a2ce493"
   },
   "outputs": [],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a+ test_re_full[i][-1]\n",
    "    print(test_re_full[i][-1])\n",
    "    \n",
    "print(\"a = \",a/10)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "stan_2D_KG_16Aug2022_tune.ipynb",
   "version": ""
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
