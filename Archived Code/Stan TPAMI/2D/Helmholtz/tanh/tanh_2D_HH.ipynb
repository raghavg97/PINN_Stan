{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda':\n",
    "    print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_2D_2(xy): #True function for 2D_2 Helmholtz equation (x,y) \\in [-1,1]\n",
    "    term1 = (xy[:,0] + xy[:,1]).reshape(-1,1)\n",
    "    term2 = np.sin(np.pi*xy[:,0]).reshape(-1,1)\n",
    "    term3 = np.sin(np.pi*xy[:,1]).reshape(-1,1)\n",
    "    \n",
    "    u = term1*term2*term3\n",
    "    \n",
    "    return u.reshape(-1,1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_thresh = 0.1\n",
    "label = \"HH_stan\"\n",
    "\n",
    "\n",
    "x_ll = -1.0\n",
    "x_ul = 1.0\n",
    "y_ll = -1.0\n",
    "y_ul = 1.0\n",
    "\n",
    "pi = torch.from_numpy(np.array(np.pi)).double().to(device)\n",
    "\n",
    "x = np.linspace(x_ll,x_ul,500).reshape(-1,1)\n",
    "y = np.linspace(y_ll,y_ul,500).reshape(-1,1)\n",
    "\n",
    "X,Y = np.meshgrid(x,y)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "Y = Y.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xy = np.hstack((X,Y))\n",
    "\n",
    "u_true = true_2D_2(xy)\n",
    "u_true_norm = np.linalg.norm(u_true,2)\n",
    "#bound_pts_idx = ((X == x_ll) + (X == x_ul) + (Y == y_ll) + (Y == y_ul)).reshape(-1,)\n",
    "\n",
    "#xy_bound = xy[bound_pts_idx,:]\n",
    "#u_bound = u_true[bound_pts_idx,:]\n",
    "\n",
    "\n",
    "xy_test_tensor = torch.from_numpy(xy).float().to(device)\n",
    "\n",
    "\n",
    "lb_xy = xy[0]\n",
    "ub_xy = xy[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_T,N_f,seed):\n",
    "    '''Boundary Conditions''' \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    #choose random N_u points for training\n",
    "    \n",
    "    x_1 = np.random.uniform(x_ll,x_ul,(int(N_T/4),1))\n",
    "    y_1 = y_ll*np.ones((int(N_T/4),1))\n",
    "    x_2 = np.random.uniform(x_ll,x_ul,(int(N_T/4),1))\n",
    "    y_2 = y_ul*np.ones((int(N_T/4),1))\n",
    "    x_3 = x_ll*np.ones((int(N_T/4),1))\n",
    "    y_3 = np.random.uniform(y_ll,y_ul,(int(N_T/4),1))\n",
    "    x_4 = x_ul*np.ones((int(N_T/4),1))\n",
    "    y_4 = np.random.uniform(y_ll,y_ul,(int(N_T/4),1))\n",
    "    \n",
    "    XY_1 = np.hstack((x_1,y_1))\n",
    "    XY_2 = np.hstack((x_2,y_2))\n",
    "    XY_3 = np.hstack((x_3,y_3))\n",
    "    XY_4 = np.hstack((x_4,y_4))\n",
    "    \n",
    "    xy_BC = np.vstack((XY_1,XY_2,XY_3,XY_4)) #choose indices from  set 'idx' (x,t)\n",
    "    u_BC = true_2D_2(xy_BC)      #choose corresponding u\n",
    "\n",
    "\n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    samples = sampling(N_f)\n",
    "    \n",
    "    xy_coll = lb_xy + (ub_xy - lb_xy)*samples\n",
    "    \n",
    "    xy_coll = np.vstack((xy_coll, xy_BC)) # append training points to collocation points \n",
    "\n",
    "    return xy_coll, xy_BC, u_BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,beta_init):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "        \n",
    "\n",
    "    \n",
    "    'foward pass'\n",
    "    def forward(self,xy):\n",
    "        if torch.is_tensor(xy) != True:         \n",
    "            xy = torch.from_numpy(xy)                \n",
    "        \n",
    "        ubxy = torch.from_numpy(ub_xy).float().to(device)\n",
    "        lbxy = torch.from_numpy(lb_xy).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xy = (xy - lbxy)/(ubxy - lbxy)\n",
    "        \n",
    "        #convert to float\n",
    "        a = xy.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC(self,xy,u):\n",
    "                \n",
    "        loss_bc = self.loss_function(self.forward(xy), u)\n",
    "                \n",
    "        return loss_bc\n",
    "    \n",
    "    def loss_PDE(self, xy_coll, f_hat):\n",
    "        \n",
    "        g = xy_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "    \n",
    "        u_x_y = autograd.grad(u,g,torch.ones([xy_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_xx_yy = autograd.grad(u_x_y,g,torch.ones(xy_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        #du_dx = u_x_t[:,[0]]\n",
    "        \n",
    "        d2u_dx2 = u_xx_yy[:,[0]]\n",
    "        d2u_dy2 = u_xx_yy[:,[1]]    \n",
    "        \n",
    "\n",
    "        f1 = d2u_dx2 + d2u_dy2 + u\n",
    "        \n",
    "        \n",
    "        f2 = 2*pi*torch.cos(pi*g[:,1].reshape(-1,1))*torch.sin(pi*g[:,0].reshape(-1,1)) + 2*pi*torch.cos(pi*g[:,0].reshape(-1,1))*torch.sin(pi*g[:,1].reshape(-1,1)) \n",
    "        f3 = (g[:,0].reshape(-1,1)+g[:,1].reshape(-1,1))*torch.sin(pi*g[:,0].reshape(-1,1))*torch.sin(pi*g[:,1].reshape(-1,1)) \n",
    "        f4 = 2*torch.square(pi)*(g[:,0].reshape(-1,1)+g[:,1].reshape(-1,1))*torch.sin(pi*g[:,0].reshape(-1,1))*torch.sin(pi*g[:,1].reshape(-1,1))\n",
    "        \n",
    "        f = f1 - f2 -f3 +f4\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xy_BC,u_BC,xy_coll,f_hat):\n",
    "\n",
    "        loss_BC = self.loss_BC(xy_BC,u_BC)\n",
    "        loss_f = self.loss_PDE(xy_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_BC + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "     \n",
    "    'callable for optimizer'                                       \n",
    "    def closure(self):\n",
    "        optimizer.zero_grad()\n",
    "    \n",
    "        loss = self.loss(xy_BC, u_BC, xy_coll,f_hat)\n",
    "        self.train_loss.append(loss.cpu().detach().numpy())\n",
    "        \n",
    "        u_pred = self.test(xy_test_tensor)\n",
    "        self.test_loss.append(np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1))))\n",
    "        rel_error = np.linalg.norm((u_pred.reshape(-1,1) - u_true.reshape(-1,1)),2)/np.linalg.norm(u_true.reshape(-1,1),2)\n",
    "        self.beta_val.append(self.beta.cpu().detach().numpy())\n",
    "        \n",
    "        print(self.iter,\"Train Loss\",self.train_loss[-1],\"Test Loss\",self.test_loss[-1],'Rel Error',rel_error)\n",
    "        \n",
    "        loss.backward()\n",
    "        self.iter += 1\n",
    "  \n",
    "\n",
    "        return loss        \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        u_pred = self.forward(xy_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "        \n",
    "        return test_mse, test_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "   \n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(xy_BC,u_BC,xy_coll,f_hat,seed):\n",
    "\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(xy_BC,u_BC,xy_coll,f_hat)\n",
    "        loss.backward()\n",
    "        #print(loss.cpu().detach().numpy())\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*9)\n",
    "    start_time = time.time() \n",
    "    thresh_flag = 0\n",
    "\n",
    "    xy_coll_np_array, xy_BC_np_array, u_BC_np_array = trainingdata(N_T,N_f,(reps+5)*22)\n",
    "        \n",
    "    xy_coll = torch.from_numpy(xy_coll_np_array).float().to(device)\n",
    "    xy_BC = torch.from_numpy(xy_BC_np_array).float().to(device)\n",
    "    u_BC = torch.from_numpy(u_BC_np_array).float().to(device)\n",
    "        \n",
    "    f_hat = torch.zeros(xy_coll.shape[0],1).to(device)\n",
    "    \n",
    "\n",
    "    for i in range(max_iter):\n",
    "        train_step(xy_BC,u_BC,xy_coll,f_hat,i)\n",
    "        loss_np = PINN.loss(xy_BC,u_BC,xy_coll,f_hat).cpu().detach().numpy()\n",
    "        \n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1          \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        \n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])   \n",
    "        \n",
    "      \n",
    "         \n",
    "\n",
    "    elapsed_time[rep] = time.time() - start_time  \n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "HH_stan\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 56.09937 Test MSE 0.08810721279615169 Test RE 0.7911306723993417\n",
      "1 Train Loss 38.3142 Test MSE 5.280511098809679 Test RE 6.124638654894997\n",
      "2 Train Loss 24.192667 Test MSE 1.9005304627924633 Test RE 3.674344805750392\n",
      "3 Train Loss 20.311314 Test MSE 0.688200521232879 Test RE 2.21105726023369\n",
      "4 Train Loss 18.650085 Test MSE 0.07576285082001546 Test RE 0.7336191259274867\n",
      "5 Train Loss 15.656459 Test MSE 0.09512108892476573 Test RE 0.8220171866314738\n",
      "6 Train Loss 12.583942 Test MSE 0.13609084529346413 Test RE 0.9832341335634539\n",
      "7 Train Loss 6.2172074 Test MSE 0.1711192474662046 Test RE 1.1025338243922558\n",
      "8 Train Loss 4.269241 Test MSE 0.25944546691978926 Test RE 1.3575803230962056\n",
      "9 Train Loss 3.1582265 Test MSE 0.4104486898287425 Test RE 1.707544096703492\n",
      "10 Train Loss 2.1236384 Test MSE 0.31121971022206063 Test RE 1.48688040513324\n",
      "11 Train Loss 1.3779315 Test MSE 0.1927006561566338 Test RE 1.169995221129218\n",
      "12 Train Loss 0.94046474 Test MSE 0.10686430735157774 Test RE 0.8712821741159275\n",
      "13 Train Loss 0.7223016 Test MSE 0.09739568595979159 Test RE 0.8317874270763469\n",
      "14 Train Loss 0.56951344 Test MSE 0.07530710045644948 Test RE 0.7314092597927004\n",
      "15 Train Loss 0.43998155 Test MSE 0.06577707710029043 Test RE 0.6835649352582861\n",
      "16 Train Loss 0.37059245 Test MSE 0.05808384990838666 Test RE 0.6423477221475113\n",
      "17 Train Loss 0.32159317 Test MSE 0.04463503284989253 Test RE 0.5630933785886537\n",
      "18 Train Loss 0.2788114 Test MSE 0.04903395805330232 Test RE 0.5901888041995096\n",
      "19 Train Loss 0.23589583 Test MSE 0.05356849148362694 Test RE 0.6168750417846518\n",
      "20 Train Loss 0.19549075 Test MSE 0.06273647239173877 Test RE 0.6675788033207718\n",
      "21 Train Loss 0.16982788 Test MSE 0.05649803920789625 Test RE 0.6335183210269971\n",
      "22 Train Loss 0.14910871 Test MSE 0.05354044420348642 Test RE 0.6167135295606258\n",
      "23 Train Loss 0.12394948 Test MSE 0.06435274627965282 Test RE 0.6761235044304442\n",
      "24 Train Loss 0.101769134 Test MSE 0.0675235626196331 Test RE 0.69258034946789\n",
      "25 Train Loss 0.087824054 Test MSE 0.06633491003922169 Test RE 0.6864573562423582\n",
      "26 Train Loss 0.06810786 Test MSE 0.0547031507285176 Test RE 0.6233739671515791\n",
      "27 Train Loss 0.058900844 Test MSE 0.05187804102538088 Test RE 0.6070637149633321\n",
      "28 Train Loss 0.051078442 Test MSE 0.05579997236076954 Test RE 0.6295924089360239\n",
      "29 Train Loss 0.04199446 Test MSE 0.055344565719166953 Test RE 0.627017963437694\n",
      "30 Train Loss 0.037755974 Test MSE 0.061046615157453604 Test RE 0.6585265443246011\n",
      "31 Train Loss 0.034538843 Test MSE 0.05892711051971942 Test RE 0.6469937184058867\n",
      "32 Train Loss 0.03190117 Test MSE 0.059151941585757484 Test RE 0.6482268163700774\n",
      "33 Train Loss 0.029746477 Test MSE 0.05850873642359809 Test RE 0.6446928455146992\n",
      "34 Train Loss 0.02790251 Test MSE 0.05735421231108338 Test RE 0.6383004505183936\n",
      "35 Train Loss 0.025868207 Test MSE 0.057850629784256896 Test RE 0.6410568375274891\n",
      "36 Train Loss 0.02335203 Test MSE 0.05617758712568077 Test RE 0.6317191351232663\n",
      "37 Train Loss 0.021593546 Test MSE 0.05810966731988202 Test RE 0.6424904633147457\n",
      "38 Train Loss 0.020431321 Test MSE 0.05900090106315282 Test RE 0.6473986854996562\n",
      "39 Train Loss 0.019352322 Test MSE 0.05798497360768969 Test RE 0.6418007541070548\n",
      "40 Train Loss 0.018415691 Test MSE 0.058019349784182606 Test RE 0.6419909705170311\n",
      "41 Train Loss 0.01706108 Test MSE 0.05819490937909952 Test RE 0.6429615307173435\n",
      "42 Train Loss 0.016421743 Test MSE 0.060853550622440286 Test RE 0.6574843997192625\n",
      "43 Train Loss 0.014953118 Test MSE 0.06248436296041863 Test RE 0.6662361048082177\n",
      "44 Train Loss 0.014503997 Test MSE 0.06274918037491538 Test RE 0.6676464127217346\n",
      "45 Train Loss 0.012994098 Test MSE 0.0612133781454543 Test RE 0.6594253898867547\n",
      "46 Train Loss 0.01274709 Test MSE 0.06087518322648775 Test RE 0.6576012526862328\n",
      "47 Train Loss 0.0119814165 Test MSE 0.06062578479316975 Test RE 0.6562528128505444\n",
      "48 Train Loss 0.011269681 Test MSE 0.05986384242583516 Test RE 0.6521158944296195\n",
      "49 Train Loss 0.0108203385 Test MSE 0.060591621571788185 Test RE 0.6560678843623958\n",
      "50 Train Loss 0.010403792 Test MSE 0.059135876776070365 Test RE 0.6481387858903979\n",
      "51 Train Loss 0.009738597 Test MSE 0.06031163738810021 Test RE 0.6545503369118945\n",
      "52 Train Loss 0.009428578 Test MSE 0.059942223801188715 Test RE 0.6525426714156299\n",
      "53 Train Loss 0.008869996 Test MSE 0.06063012039762158 Test RE 0.6562762781282266\n",
      "54 Train Loss 0.008489232 Test MSE 0.06066662222351591 Test RE 0.6564738010578725\n",
      "55 Train Loss 0.008306176 Test MSE 0.06082739034275897 Test RE 0.6573430618308002\n",
      "56 Train Loss 0.008040643 Test MSE 0.06085660141196759 Test RE 0.6575008804452119\n",
      "57 Train Loss 0.0076086083 Test MSE 0.060851596510253986 Test RE 0.6574738431569987\n",
      "58 Train Loss 0.0072821686 Test MSE 0.06186901786957282 Test RE 0.6629474464117003\n",
      "59 Train Loss 0.007076241 Test MSE 0.06157791214453625 Test RE 0.6613859593330716\n",
      "60 Train Loss 0.0068230904 Test MSE 0.06171318676550251 Test RE 0.6621120285470113\n",
      "61 Train Loss 0.006594456 Test MSE 0.06204040411428578 Test RE 0.6638650421642046\n",
      "62 Train Loss 0.0064490247 Test MSE 0.0624565769386577 Test RE 0.6660879548671239\n",
      "63 Train Loss 0.0061997795 Test MSE 0.062432424633594996 Test RE 0.6659591524586259\n",
      "64 Train Loss 0.0059659453 Test MSE 0.062356730797162814 Test RE 0.6655553215018337\n",
      "65 Train Loss 0.005867073 Test MSE 0.06191841410390974 Test RE 0.6632120422855915\n",
      "66 Train Loss 0.0057163807 Test MSE 0.06283240927903323 Test RE 0.6680890405403517\n",
      "67 Train Loss 0.005584717 Test MSE 0.0624322558639524 Test RE 0.6659582523352946\n",
      "68 Train Loss 0.0054179826 Test MSE 0.06260600169045359 Test RE 0.666884272668652\n",
      "69 Train Loss 0.0052581355 Test MSE 0.06313413785055316 Test RE 0.6696912401770335\n",
      "70 Train Loss 0.0051625245 Test MSE 0.06258412551239455 Test RE 0.6667677490658295\n",
      "71 Train Loss 0.0050057485 Test MSE 0.06323649645066738 Test RE 0.6702339013933959\n",
      "72 Train Loss 0.0049223606 Test MSE 0.0635584677384828 Test RE 0.6719379971120888\n",
      "73 Train Loss 0.0048308005 Test MSE 0.06345499329790494 Test RE 0.6713908101494921\n",
      "74 Train Loss 0.004722212 Test MSE 0.06330865260262451 Test RE 0.6706161783542037\n",
      "75 Train Loss 0.004508634 Test MSE 0.06340281687413898 Test RE 0.671114724898721\n",
      "76 Train Loss 0.004363128 Test MSE 0.06355016352324885 Test RE 0.671894099735209\n",
      "77 Train Loss 0.0043166946 Test MSE 0.06351205127088785 Test RE 0.6716925956278026\n",
      "78 Train Loss 0.0042026527 Test MSE 0.06284240247566569 Test RE 0.6681421666243865\n",
      "79 Train Loss 0.0040728236 Test MSE 0.06315680179094593 Test RE 0.6698114325244048\n",
      "80 Train Loss 0.0039780317 Test MSE 0.06292176956911923 Test RE 0.6685639500628694\n",
      "81 Train Loss 0.0039351312 Test MSE 0.0627038064904304 Test RE 0.6674049817632157\n",
      "82 Train Loss 0.0038812268 Test MSE 0.06252390794017218 Test RE 0.666446894555034\n",
      "83 Train Loss 0.0038037691 Test MSE 0.06240591274983202 Test RE 0.6658177379090021\n",
      "84 Train Loss 0.003777016 Test MSE 0.06265707604192179 Test RE 0.6671562413051761\n",
      "85 Train Loss 0.0037298114 Test MSE 0.062438198119759626 Test RE 0.6659899442874669\n",
      "86 Train Loss 0.0036817514 Test MSE 0.06231844697595432 Test RE 0.665350981788542\n",
      "87 Train Loss 0.0036445707 Test MSE 0.06257539095441701 Test RE 0.6667212186978467\n",
      "88 Train Loss 0.0035958823 Test MSE 0.06291632639489149 Test RE 0.6685350317028722\n",
      "89 Train Loss 0.0035248687 Test MSE 0.06296707235965832 Test RE 0.6688045850852217\n",
      "90 Train Loss 0.003486214 Test MSE 0.06247544560281459 Test RE 0.6661885626929076\n",
      "91 Train Loss 0.003458877 Test MSE 0.0622653879750971 Test RE 0.6650676758206239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92 Train Loss 0.0034089724 Test MSE 0.06252775791988502 Test RE 0.6664674128464474\n",
      "93 Train Loss 0.0033324698 Test MSE 0.06203667780677053 Test RE 0.6638451051370577\n",
      "94 Train Loss 0.0032469393 Test MSE 0.06238243648334562 Test RE 0.6656924902820599\n",
      "95 Train Loss 0.003199073 Test MSE 0.062222878324625565 Test RE 0.6648406104925458\n",
      "96 Train Loss 0.0031705904 Test MSE 0.0624605279773258 Test RE 0.6661090230858371\n",
      "97 Train Loss 0.0031176852 Test MSE 0.06247638476589821 Test RE 0.6661935699189264\n",
      "98 Train Loss 0.0030554035 Test MSE 0.062190312976055706 Test RE 0.6646666101861618\n",
      "99 Train Loss 0.0029934833 Test MSE 0.0627200517020094 Test RE 0.6674914313234114\n",
      "Training time: 244.64\n",
      "Training time: 244.64\n",
      "HH_stan\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 56.56858 Test MSE 0.2500181354701923 Test RE 1.33268726345694\n",
      "1 Train Loss 42.89366 Test MSE 2.333389125111045 Test RE 4.071327848619513\n",
      "2 Train Loss 29.968863 Test MSE 3.4384903241529736 Test RE 4.942269167824671\n",
      "3 Train Loss 23.531424 Test MSE 2.0908170509370594 Test RE 3.853900587646465\n",
      "4 Train Loss 19.02523 Test MSE 0.06897932715676852 Test RE 0.7000063312731988\n",
      "5 Train Loss 18.517307 Test MSE 0.06885591936420364 Test RE 0.6993798761328436\n",
      "6 Train Loss 15.509889 Test MSE 0.18772246089743308 Test RE 1.1547836081585925\n",
      "7 Train Loss 12.560667 Test MSE 0.13667271301382125 Test RE 0.9853338412188758\n",
      "8 Train Loss 8.597555 Test MSE 0.09451087203834851 Test RE 0.8193762589483103\n",
      "9 Train Loss 4.2765527 Test MSE 0.19745916055574678 Test RE 1.1843529178273458\n",
      "10 Train Loss 2.9291558 Test MSE 0.1617415826596623 Test RE 1.0718976908661755\n",
      "11 Train Loss 2.0687099 Test MSE 0.25284513145034015 Test RE 1.3402005412008249\n",
      "12 Train Loss 1.1769456 Test MSE 0.1528945658788406 Test RE 1.042169877242446\n",
      "13 Train Loss 0.8342529 Test MSE 0.08887541231932583 Test RE 0.7945720888713323\n",
      "14 Train Loss 0.6173847 Test MSE 0.09860833779422833 Test RE 0.8369496078129086\n",
      "15 Train Loss 0.4905803 Test MSE 0.07415155004240231 Test RE 0.7257760087246221\n",
      "16 Train Loss 0.4122274 Test MSE 0.07750531670567719 Test RE 0.7420074038612428\n",
      "17 Train Loss 0.32144246 Test MSE 0.10656095039580213 Test RE 0.8700446358028762\n",
      "18 Train Loss 0.2437244 Test MSE 0.06354445524151711 Test RE 0.6718639232043193\n",
      "19 Train Loss 0.18497685 Test MSE 0.05137578808734156 Test RE 0.6041179494016329\n",
      "20 Train Loss 0.15314826 Test MSE 0.04402594800603776 Test RE 0.5592382259096683\n",
      "21 Train Loss 0.122199714 Test MSE 0.04157287531642317 Test RE 0.5434348958303072\n",
      "22 Train Loss 0.10230277 Test MSE 0.04287845920599224 Test RE 0.5519021382315045\n",
      "23 Train Loss 0.079224706 Test MSE 0.050351788826315004 Test RE 0.5980671425859304\n",
      "24 Train Loss 0.060878232 Test MSE 0.059484558958713546 Test RE 0.6500467840907518\n",
      "25 Train Loss 0.049280755 Test MSE 0.05980252586181478 Test RE 0.6517818384404332\n",
      "26 Train Loss 0.040590115 Test MSE 0.055752022772071155 Test RE 0.6293218426261815\n",
      "27 Train Loss 0.034470603 Test MSE 0.05909755415657199 Test RE 0.6479287407974736\n",
      "28 Train Loss 0.02901598 Test MSE 0.06461088888408241 Test RE 0.6774782376023646\n",
      "29 Train Loss 0.024084957 Test MSE 0.06289125572461635 Test RE 0.6684018207199813\n",
      "30 Train Loss 0.021388453 Test MSE 0.0611429634539503 Test RE 0.6590460071626828\n",
      "31 Train Loss 0.01799227 Test MSE 0.05777374969670135 Test RE 0.6406307324796857\n",
      "32 Train Loss 0.0155897355 Test MSE 0.0589273744769295 Test RE 0.646995167471216\n",
      "33 Train Loss 0.014290433 Test MSE 0.059760329831542475 Test RE 0.6515518526757121\n",
      "34 Train Loss 0.013267333 Test MSE 0.05984798396998028 Test RE 0.652029513104202\n",
      "35 Train Loss 0.012335288 Test MSE 0.059648505923442195 Test RE 0.6509419732605699\n",
      "36 Train Loss 0.011525949 Test MSE 0.06025039503779703 Test RE 0.6542179269149961\n",
      "37 Train Loss 0.01075119 Test MSE 0.05915980118112061 Test RE 0.6482698803094566\n",
      "38 Train Loss 0.010105868 Test MSE 0.060287742039988025 Test RE 0.6544206583096709\n",
      "39 Train Loss 0.009419769 Test MSE 0.059397737760982436 Test RE 0.6495722201871926\n",
      "40 Train Loss 0.008749043 Test MSE 0.060147389471957836 Test RE 0.6536584541145778\n",
      "41 Train Loss 0.008272983 Test MSE 0.06069458050013257 Test RE 0.6566250519544554\n",
      "42 Train Loss 0.007813889 Test MSE 0.06241979854923669 Test RE 0.6658918085932095\n",
      "43 Train Loss 0.0073818574 Test MSE 0.06206749574429391 Test RE 0.6640099737080329\n",
      "44 Train Loss 0.0071629784 Test MSE 0.06167241340214712 Test RE 0.6618932665910637\n",
      "45 Train Loss 0.0066849478 Test MSE 0.062474068959576794 Test RE 0.6661812229370786\n",
      "46 Train Loss 0.006409295 Test MSE 0.06145828561367401 Test RE 0.6607432144877456\n",
      "47 Train Loss 0.0062255915 Test MSE 0.060700638506863705 Test RE 0.6566578204462911\n",
      "48 Train Loss 0.005781163 Test MSE 0.061425061990887625 Test RE 0.6605645953586406\n",
      "49 Train Loss 0.0056675724 Test MSE 0.06200688593657541 Test RE 0.6636856868471779\n",
      "50 Train Loss 0.005220089 Test MSE 0.06127716635966415 Test RE 0.6597688819205255\n",
      "51 Train Loss 0.0049080024 Test MSE 0.06158745277322257 Test RE 0.6614371935601078\n",
      "52 Train Loss 0.0047621937 Test MSE 0.06165501964655404 Test RE 0.6617999216014657\n",
      "53 Train Loss 0.0045442074 Test MSE 0.06142723433929589 Test RE 0.6605762759635413\n",
      "54 Train Loss 0.004387712 Test MSE 0.061418155032306854 Test RE 0.6605274556275839\n",
      "55 Train Loss 0.0041907523 Test MSE 0.0611334535192099 Test RE 0.6589947524647675\n",
      "56 Train Loss 0.0040168855 Test MSE 0.061705834603703734 Test RE 0.6620725872216501\n",
      "57 Train Loss 0.003793532 Test MSE 0.06186315071844492 Test RE 0.6629160114123174\n",
      "58 Train Loss 0.003713468 Test MSE 0.06128664382103025 Test RE 0.6598199016779726\n",
      "59 Train Loss 0.0034883316 Test MSE 0.060873994061167794 Test RE 0.6575948297041441\n",
      "60 Train Loss 0.0033701663 Test MSE 0.0609499022858298 Test RE 0.6580047034547626\n",
      "61 Train Loss 0.0033198362 Test MSE 0.06124881068840965 Test RE 0.6596162120484506\n",
      "62 Train Loss 0.0031806193 Test MSE 0.0607381525985567 Test RE 0.6568607023056623\n",
      "63 Train Loss 0.0030942303 Test MSE 0.06054098126811339 Test RE 0.6557936680542391\n",
      "64 Train Loss 0.0030465121 Test MSE 0.060538345908084434 Test RE 0.655779394489523\n",
      "65 Train Loss 0.0029408368 Test MSE 0.06103434193890497 Test RE 0.6584603437134334\n",
      "66 Train Loss 0.002870372 Test MSE 0.06089519538423037 Test RE 0.657709333992291\n",
      "67 Train Loss 0.0028303983 Test MSE 0.06075951503914687 Test RE 0.6569762056056558\n",
      "68 Train Loss 0.0027227881 Test MSE 0.061090525936982276 Test RE 0.6587633405483412\n",
      "69 Train Loss 0.0026122509 Test MSE 0.061053838909152607 Test RE 0.6585655054653007\n",
      "70 Train Loss 0.0025615585 Test MSE 0.06094570622462526 Test RE 0.6579820530859594\n",
      "71 Train Loss 0.0025236292 Test MSE 0.061249413629120064 Test RE 0.6596194587114\n",
      "72 Train Loss 0.002443043 Test MSE 0.06090527158737184 Test RE 0.6577637466490728\n",
      "73 Train Loss 0.0023863602 Test MSE 0.061652633075655085 Test RE 0.6617871128494167\n",
      "74 Train Loss 0.0023683934 Test MSE 0.06161312336804833 Test RE 0.6615750277818132\n",
      "75 Train Loss 0.0023241122 Test MSE 0.061569544914465354 Test RE 0.6613410231189579\n",
      "76 Train Loss 0.002277908 Test MSE 0.0617714753671236 Test RE 0.6624246398185739\n",
      "77 Train Loss 0.0022339749 Test MSE 0.06130077552450302 Test RE 0.6598959691631867\n",
      "78 Train Loss 0.0022103395 Test MSE 0.0614514261350854 Test RE 0.6607063400422549\n",
      "79 Train Loss 0.0021884525 Test MSE 0.061343573595923837 Test RE 0.6601262871881486\n",
      "80 Train Loss 0.0021533554 Test MSE 0.061008668364550246 Test RE 0.6583218412886507\n",
      "81 Train Loss 0.0021189475 Test MSE 0.061122966909022386 Test RE 0.6589382292596482\n",
      "82 Train Loss 0.0020954264 Test MSE 0.06103348278469163 Test RE 0.6584557092654202\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83 Train Loss 0.002074661 Test MSE 0.061080194264570654 Test RE 0.6587076329347995\n",
      "84 Train Loss 0.0020262455 Test MSE 0.06127148177882764 Test RE 0.6597382783796664\n",
      "85 Train Loss 0.0019889674 Test MSE 0.06119818392797797 Test RE 0.6593435444232758\n",
      "86 Train Loss 0.0019752164 Test MSE 0.061239023900399786 Test RE 0.6595635107644495\n",
      "87 Train Loss 0.001947919 Test MSE 0.06138152232005025 Test RE 0.6603304412323213\n",
      "88 Train Loss 0.0019121838 Test MSE 0.061120700920075335 Test RE 0.6589260148608814\n",
      "89 Train Loss 0.00188428 Test MSE 0.06103922603827343 Test RE 0.658486688893145\n",
      "90 Train Loss 0.0018562272 Test MSE 0.06128361899227181 Test RE 0.6598036186290361\n",
      "91 Train Loss 0.0018276878 Test MSE 0.06137501706797499 Test RE 0.6602954491888704\n",
      "92 Train Loss 0.0018085288 Test MSE 0.06141341401878045 Test RE 0.660501961292597\n",
      "93 Train Loss 0.00179147 Test MSE 0.0612267706665767 Test RE 0.6594975218760525\n",
      "94 Train Loss 0.0017658932 Test MSE 0.06113446317547286 Test RE 0.6590001942923891\n",
      "95 Train Loss 0.0017425024 Test MSE 0.060981333604524235 Test RE 0.6581743451528919\n",
      "96 Train Loss 0.0017244674 Test MSE 0.06107413719542805 Test RE 0.6586749714751864\n",
      "97 Train Loss 0.0017005133 Test MSE 0.060965035463410144 Test RE 0.658086385981173\n",
      "98 Train Loss 0.0016768682 Test MSE 0.06101459709121137 Test RE 0.6583538278533404\n",
      "99 Train Loss 0.0016537574 Test MSE 0.06119657897195684 Test RE 0.6593348985431053\n",
      "Training time: 87.05\n",
      "Training time: 87.05\n",
      "HH_stan\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 54.60301 Test MSE 0.30913168257211643 Test RE 1.4818841400441678\n",
      "1 Train Loss 37.18253 Test MSE 6.783321044650967 Test RE 6.941665416965864\n",
      "2 Train Loss 24.11902 Test MSE 2.0645826302143795 Test RE 3.829645953112326\n",
      "3 Train Loss 19.06265 Test MSE 0.08650796067041187 Test RE 0.7839178042409196\n",
      "4 Train Loss 18.729631 Test MSE 0.06708605016670674 Test RE 0.6903329490971419\n",
      "5 Train Loss 16.303492 Test MSE 0.06722561144103018 Test RE 0.6910506369856454\n",
      "6 Train Loss 13.063319 Test MSE 0.08669378141840148 Test RE 0.7847592877934503\n",
      "7 Train Loss 8.972359 Test MSE 0.1963241191610786 Test RE 1.1809440433572536\n",
      "8 Train Loss 4.257207 Test MSE 0.19404575560439352 Test RE 1.1740715517179967\n",
      "9 Train Loss 3.0047884 Test MSE 0.3066017421427873 Test RE 1.475807796216734\n",
      "10 Train Loss 2.2640057 Test MSE 0.3022547993327355 Test RE 1.465308584368614\n",
      "11 Train Loss 1.7180665 Test MSE 0.22612415107915645 Test RE 1.2674066043077235\n",
      "12 Train Loss 1.2671334 Test MSE 0.10827215202610879 Test RE 0.8770025893163929\n",
      "13 Train Loss 1.1056437 Test MSE 0.116076409656259 Test RE 0.9080598514582677\n",
      "14 Train Loss 0.976579 Test MSE 0.1431233733168999 Test RE 1.0083185871979652\n",
      "15 Train Loss 0.85147107 Test MSE 0.14491112477160414 Test RE 1.0145964883233585\n",
      "16 Train Loss 0.7477505 Test MSE 0.12199559355551447 Test RE 0.9309247210819759\n",
      "17 Train Loss 0.69121087 Test MSE 0.1333392304994963 Test RE 0.9732433913149127\n",
      "18 Train Loss 0.61449766 Test MSE 0.10141820563829271 Test RE 0.8487903870973411\n",
      "19 Train Loss 0.57350147 Test MSE 0.09276518714425484 Test RE 0.811773750117701\n",
      "20 Train Loss 0.5280665 Test MSE 0.08371417336918077 Test RE 0.771155548540426\n",
      "21 Train Loss 0.4744957 Test MSE 0.06110490591217667 Test RE 0.6588408684707053\n",
      "22 Train Loss 0.43940902 Test MSE 0.062033997476398116 Test RE 0.6638307640772171\n",
      "23 Train Loss 0.40928164 Test MSE 0.063667280001293 Test RE 0.6725129309484833\n",
      "24 Train Loss 0.37801617 Test MSE 0.0705536012023192 Test RE 0.7079491817131177\n",
      "25 Train Loss 0.3517937 Test MSE 0.06136646980754843 Test RE 0.660249470271584\n",
      "26 Train Loss 0.3298372 Test MSE 0.05063772010369284 Test RE 0.5997628521229156\n",
      "27 Train Loss 0.30518138 Test MSE 0.050802627008873016 Test RE 0.6007386527992399\n",
      "28 Train Loss 0.27843326 Test MSE 0.05078558836264925 Test RE 0.6006379037596106\n",
      "29 Train Loss 0.2515296 Test MSE 0.06300051932744521 Test RE 0.6689821899411985\n",
      "30 Train Loss 0.23073769 Test MSE 0.05290173164622383 Test RE 0.6130239402211594\n",
      "31 Train Loss 0.21364143 Test MSE 0.061874203686560214 Test RE 0.6629752297207755\n",
      "32 Train Loss 0.18801135 Test MSE 0.050953598739977665 Test RE 0.6016306074054525\n",
      "33 Train Loss 0.17132321 Test MSE 0.04940265247036024 Test RE 0.5924035122823236\n",
      "34 Train Loss 0.15745455 Test MSE 0.05496243462008395 Test RE 0.6248495654732745\n",
      "35 Train Loss 0.13180628 Test MSE 0.05000921729260006 Test RE 0.5960291767719313\n",
      "36 Train Loss 0.11353581 Test MSE 0.04871858737572574 Test RE 0.5882877901260213\n",
      "37 Train Loss 0.09842658 Test MSE 0.047107382564080084 Test RE 0.5784781747300539\n",
      "38 Train Loss 0.084672675 Test MSE 0.05474057038933814 Test RE 0.6235871400160143\n",
      "39 Train Loss 0.07174209 Test MSE 0.05369339014882814 Test RE 0.6175937666304265\n",
      "40 Train Loss 0.06483915 Test MSE 0.051282338131245137 Test RE 0.603568269353603\n",
      "41 Train Loss 0.059557635 Test MSE 0.050665413518980976 Test RE 0.5999268327624725\n",
      "42 Train Loss 0.055191755 Test MSE 0.056850583834875276 Test RE 0.6354918067743447\n",
      "43 Train Loss 0.046710737 Test MSE 0.06336254810450652 Test RE 0.670901569884376\n",
      "44 Train Loss 0.044335112 Test MSE 0.06292845021668189 Test RE 0.6685994411306313\n",
      "45 Train Loss 0.040500708 Test MSE 0.06220968496682343 Test RE 0.6647701223915143\n",
      "46 Train Loss 0.038084276 Test MSE 0.06302986168411738 Test RE 0.6691379603155162\n",
      "47 Train Loss 0.035482474 Test MSE 0.06152708599425291 Test RE 0.6611129503886104\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22815/530913886.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     47\u001b[0m                                       line_search_fn = 'strong_wolfe')\n\u001b[1;32m     48\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mnan_flag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_22815/3097633378.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(max_iter, rep)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy_BC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu_BC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxy_coll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_hat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mloss_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPINN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy_BC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu_BC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxy_coll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_22815/896708044.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(xy_BC, u_BC, xy_coll, f_hat, seed)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    380\u001b[0m                 \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_old\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    381\u001b[0m                     \u001b[0mal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mold_stps\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mq\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mro\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 382\u001b[0;31m                     \u001b[0mq\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dirs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malpha\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0mal\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    383\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m                 \u001b[0;31m# multiply by initial Hessian\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_reps = 10 #10\n",
    "max_iter = 100 #200\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "beta_init = 1\n",
    "\n",
    "N_T = 5000 #Total number of data points for BC\n",
    "N_f = 50000 #Total number of collocation points \n",
    "\n",
    "#for reps in range(max_reps):\n",
    "for reps in range(max_reps):\n",
    "    print(label)\n",
    "    print(reps)\n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []\n",
    "    \n",
    "    torch.manual_seed(reps*36)\n",
    " \n",
    "    layers = np.array([2,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "    \n",
    "\n",
    "    PINN = Sequentialmodel(layers,beta_init)\n",
    "   \n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "    \n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=1, \n",
    "                                      max_iter = 20, \n",
    "                                      max_eval = 30, \n",
    "                                      tolerance_grad = -1, \n",
    "                                      tolerance_change = -1, \n",
    "                                      history_size = 100, \n",
    "                                      line_search_fn = 'strong_wolfe')\n",
    "    \n",
    "    nan_flag = train_model(max_iter,reps)\n",
    "    \n",
    "    \n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    #elapsed_time[reps] = time.time() - start_time\n",
    "\n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "    \n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred = PINN.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = mpl.cm.jet\n",
    "#norm = mpl.colors.Normalize(vmin=0, vmax=5)\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "img = ax.imshow(u_pred.reshape(500,500),vmin = -1,vmax = 1,cmap = cmap)\n",
    "fig.colorbar(img,ax=ax, extend='both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "img = ax.imshow(u_true.reshape(500,500),vmin = -1,vmax = 1,cmap = cmap)\n",
    "fig.colorbar(img, ax=ax, extend='both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = mpl.cm.jet\n",
    "#norm = mpl.colors.Normalize(vmin=0, vmax=5)\n",
    "\n",
    "fig,ax = plt.subplots()\n",
    "img = ax.imshow(np.abs(u_pred-u_true).reshape(500,500),vmin = 0,vmax = 2,cmap = cmap)\n",
    "fig.colorbar(img,ax=ax, extend='both')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    print(test_loss_full[i][-1])\n",
    "    a = a + test_loss_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
