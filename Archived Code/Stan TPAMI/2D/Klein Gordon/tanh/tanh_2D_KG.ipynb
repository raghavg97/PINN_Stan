{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 286,
     "status": "ok",
     "timestamp": 1660687093981,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "iAtv2UvNSq_u",
    "outputId": "68a82578-1b95-4343-a8ec-7635a4df93ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1256,
     "status": "ok",
     "timestamp": 1660687393066,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "xAgfGYA4acPE",
    "outputId": "527d048f-6a89-4e80-87ff-bfdb1c9d6222"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1856,
     "status": "ok",
     "timestamp": 1660687061284,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "7kSdyTofacUc",
    "outputId": "08ee5c9b-0706-46a5-86a1-2c7e56a6a74d"
   },
   "outputs": [],
   "source": [
    "# %cd '/content/gdrive/MyDrive/Virginia Tech /Fall 2022/Codes from GPU/PINN_Stan/2D Klein Gordon/stan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 32419,
     "status": "ok",
     "timestamp": 1660687093700,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "RHuSaD0gagsN",
    "outputId": "c232cd79-e56c-4a76-97c7-d59dafa084ef"
   },
   "outputs": [],
   "source": [
    "# !pip install smt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 280,
     "status": "ok",
     "timestamp": 1660687410736,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "mTLFQRt5Sq_y"
   },
   "outputs": [],
   "source": [
    "def true_2D_1(xt): #True function for 2D_1 Klein Gordon Equation x \\in [-50,50] , t \\in [0,10]\n",
    "    y = xt[:,0]*np.cos(xt[:,1])\n",
    "    return y.reshape(-1,1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 4312,
     "status": "ok",
     "timestamp": 1660687098957,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "81bNHCY3Sq_y"
   },
   "outputs": [],
   "source": [
    "label = \"KG_tanh\"\n",
    "loss_thresh = 0.01\n",
    "\n",
    "x = np.linspace(-5,5,500).reshape(-1,1)\n",
    "t = np.linspace(0,10,1000).reshape(-1,1)\n",
    "\n",
    "X,T = np.meshgrid(x,t)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "T = T.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xt = np.hstack((X,T))\n",
    "\n",
    "y_true = true_2D_1(xt)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "#bound_pts_idx = ((X == -5) + (X == 5) + (T == 0)).reshape(-1,)\n",
    "\n",
    "#xt_bound = xt[bound_pts_idx,:]\n",
    "#y_bound = y_true[bound_pts_idx,:]\n",
    "\n",
    "\n",
    "xt_test_tensor = torch.from_numpy(xt).float().to(device)\n",
    "\n",
    "\n",
    "lb_xt = xt[0]\n",
    "ub_xt = xt[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1660687098958,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "YQgCA-PuSq_z"
   },
   "outputs": [],
   "source": [
    "def trainingdata(N_I,N_B,N_f,seed):\n",
    "    '''Boundary Conditions''' \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    x_BC1 = np.random.uniform(size = N_I).reshape(-1,1)\n",
    "    t_BC1 = np.zeros((N_I,1))\n",
    "    samples = np.hstack((x_BC1,t_BC1))\n",
    "    xt_BC1 = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    y_BC1 = true_2D_1(xt_BC1)\n",
    "    \n",
    "    x_BC2 = np.zeros((int(N_B/2),1))\n",
    "    t_BC2 = np.random.uniform(size = int(N_B/2)).reshape(-1,1)\n",
    "    samples = np.hstack((x_BC2,t_BC2))\n",
    "    xt_BC2 = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    y_BC2 = true_2D_1(xt_BC2)\n",
    "    \n",
    "    x_BC3 = np.ones((int(N_B/2),1))\n",
    "    t_BC3 = np.random.uniform(size = int(N_B/2)).reshape(-1,1)\n",
    "    samples = np.hstack((x_BC3,t_BC3))\n",
    "    xt_BC3 = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    y_BC3 = true_2D_1(xt_BC3)\n",
    "\n",
    "    xt_BC = np.vstack((xt_BC1,xt_BC2,xt_BC3))\n",
    "    y_BC = np.vstack((y_BC1,y_BC2,y_BC3))\n",
    "\n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    samples = sampling(N_f)\n",
    "    xt_coll = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    \n",
    "    xt_coll = np.vstack((xt_coll, xt_BC)) # append training points to collocation points \n",
    "\n",
    "    return xt_coll, xt_BC, y_BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1660687098958,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "gTJxct8bSq_0"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "            \n",
    "        \n",
    "\n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xt):\n",
    "        if torch.is_tensor(xt) != True:         \n",
    "            xt = torch.from_numpy(xt)                \n",
    "        \n",
    "        ubxt = torch.from_numpy(ub_xt).float().to(device)\n",
    "        lbxt = torch.from_numpy(lb_xt).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xt = (xt - lbxt)/(ubxt - lbxt)\n",
    "        \n",
    "        #convert to float\n",
    "        a = xt.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC(self,xt,y):\n",
    "                \n",
    "        loss_bc = self.loss_function(self.forward(xt), y)\n",
    "                \n",
    "        return loss_bc\n",
    "    \n",
    "    def loss_PDE(self, xt_coll, f_hat):\n",
    "        \n",
    "        g = xt_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        y = self.forward(g) \n",
    "        \n",
    "        \n",
    "        y_x_t = autograd.grad(y,g,torch.ones([xt_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        y_xx_tt = autograd.grad(y_x_t,g,torch.ones(xt_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        #du_dx = u_x_t[:,[0]]\n",
    "        \n",
    "        d2y_dx2 = y_xx_tt[:,[0]]\n",
    "        d2y_dt2 = y_xx_tt[:,[1]]    \n",
    "        \n",
    "\n",
    "        f = d2y_dt2 - d2y_dx2 + torch.pow(y,2) + (g[:,0]*torch.cos(g[:,1])).reshape(-1,1) - (torch.pow(g[:,0],2)*torch.pow(torch.cos(g[:,1]),2)).reshape(-1,1)\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xt_BC,y_BC,xt_coll,f_hat):\n",
    "\n",
    "        loss_BC = self.loss_BC(xt_BC,y_BC)\n",
    "        loss_f = self.loss_PDE(xt_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_BC + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "         \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        y_pred = self.forward(xt_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return y_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1660687098958,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "VoQzfzYsYKVs"
   },
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1660687098959,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "_IUDZDkxXmyF"
   },
   "outputs": [],
   "source": [
    "def train_step(xt_BC, y_BC, xt_coll, f_hat,seed):\n",
    "    # x_coll_np_array = colloc_pts(N_f,seed*123)\n",
    "    # x_coll_train = torch.from_numpy(x_coll_np_array).float().to(device)        \n",
    "    \n",
    "#     # f_hat = torch.zeros(x_coll_train.shape[0],1).to(device)\n",
    "#     xt_coll, xt_BC, y_BC = trainingdata(N_I,N_B,N_f,seed*11)\n",
    "#     xt_coll = torch.from_numpy(xt_coll).float().to(device)\n",
    "#     xt_BC = torch.from_numpy(xt_BC).float().to(device)\n",
    "#     y_BC = torch.from_numpy(y_BC).float().to(device)\n",
    "    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(xt_BC, y_BC, xt_coll,f_hat)\n",
    "        loss.backward()\n",
    "        #print(loss.cpu().detach().numpy())\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 467,
     "status": "ok",
     "timestamp": 1660690085956,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "Vt9Dlr8MYIwW"
   },
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "  print(rep) \n",
    "  torch.manual_seed(rep*9)\n",
    "  start_time = time.time() \n",
    "  thresh_flag = 0\n",
    "\n",
    "  xt_coll, xt_BC, y_BC = trainingdata(N_I,N_B,N_f,rep*11)\n",
    "  xt_coll = torch.from_numpy(xt_coll).float().to(device)\n",
    "  xt_BC = torch.from_numpy(xt_BC).float().to(device)\n",
    "  y_BC = torch.from_numpy(y_BC).float().to(device)\n",
    "\n",
    "  f_hat = torch.zeros(xt_coll.shape[0],1).to(device)\n",
    "\n",
    "  for i in range(max_iter):\n",
    "    train_step(xt_BC, y_BC, xt_coll,f_hat,i)\n",
    "    \n",
    "    loss_np = PINN.loss(xt_BC, y_BC, xt_coll,f_hat).cpu().detach().numpy()\n",
    "    if(thresh_flag == 0):\n",
    "        if(loss_np < loss_thresh):\n",
    "            time_threshold[rep] = time.time() - start_time\n",
    "            epoch_threshold[rep] = i+1            \n",
    "            thresh_flag = 1       \n",
    "    data_update(loss_np)\n",
    "    print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "\n",
    "  elapsed_time[rep] = time.time() - start_time  \n",
    "  print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "sP4Re5lSSq_1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 54.50709 Test MSE 7.609332695097407 Test RE 1.3185036811521091\n",
      "1 Train Loss 40.185413 Test MSE 7.736740745023647 Test RE 1.329496144770292\n",
      "2 Train Loss 35.97084 Test MSE 7.139023616143794 Test RE 1.2771075418426558\n",
      "3 Train Loss 31.473644 Test MSE 6.5440755344896875 Test RE 1.2227346290578711\n",
      "4 Train Loss 28.266048 Test MSE 6.750182999950978 Test RE 1.2418405411199989\n",
      "5 Train Loss 25.972214 Test MSE 5.910410091988429 Test RE 1.1620287360289239\n",
      "6 Train Loss 23.57084 Test MSE 5.368677482698561 Test RE 1.107494856128477\n",
      "7 Train Loss 22.154003 Test MSE 5.051813126879562 Test RE 1.0743151465693888\n",
      "8 Train Loss 20.76454 Test MSE 4.739916683341508 Test RE 1.0406229849134299\n",
      "9 Train Loss 16.919062 Test MSE 3.241163203278563 Test RE 0.8605151288955017\n",
      "10 Train Loss 12.145948 Test MSE 1.9563895662738255 Test RE 0.6685528265413442\n",
      "11 Train Loss 8.269224 Test MSE 1.923519362637208 Test RE 0.662912703379829\n",
      "12 Train Loss 7.198719 Test MSE 2.136895759399739 Test RE 0.6987144563868171\n",
      "13 Train Loss 6.3405566 Test MSE 2.1221146417031593 Test RE 0.6962937247776096\n",
      "14 Train Loss 5.8287554 Test MSE 2.0968659340626736 Test RE 0.6921391134834349\n",
      "15 Train Loss 5.518112 Test MSE 2.051001782482098 Test RE 0.684527782368311\n",
      "16 Train Loss 5.239835 Test MSE 2.064820584068025 Test RE 0.6868299436422296\n",
      "17 Train Loss 5.039182 Test MSE 2.0475664989139593 Test RE 0.6839542742033701\n",
      "18 Train Loss 4.90254 Test MSE 2.114863819806724 Test RE 0.6951031619164977\n",
      "19 Train Loss 4.780369 Test MSE 2.152175727571446 Test RE 0.7012081009968655\n",
      "20 Train Loss 4.698305 Test MSE 2.1551689333687816 Test RE 0.7016955451375366\n",
      "21 Train Loss 4.656782 Test MSE 2.157825602094196 Test RE 0.7021279006816498\n",
      "22 Train Loss 4.633649 Test MSE 2.1517179411418845 Test RE 0.701133520510547\n",
      "23 Train Loss 4.5943556 Test MSE 2.165653063262445 Test RE 0.7034002240448427\n",
      "24 Train Loss 4.5792165 Test MSE 2.1603319033109085 Test RE 0.7025355409755694\n",
      "25 Train Loss 4.558261 Test MSE 2.1452269239079365 Test RE 0.7000751784456998\n",
      "26 Train Loss 4.540113 Test MSE 2.1415956024797507 Test RE 0.6994824032238754\n",
      "27 Train Loss 4.5231485 Test MSE 2.1401916636361253 Test RE 0.6992530902031773\n",
      "28 Train Loss 4.5157766 Test MSE 2.1338255583164365 Test RE 0.6982123343351503\n",
      "29 Train Loss 4.498046 Test MSE 2.145005892119495 Test RE 0.7000391116657552\n",
      "30 Train Loss 4.4905987 Test MSE 2.1518442998332157 Test RE 0.7011541070873886\n",
      "31 Train Loss 4.476478 Test MSE 2.148928116598628 Test RE 0.7006788433647533\n",
      "32 Train Loss 4.4721 Test MSE 2.147447192675202 Test RE 0.7004373669541012\n",
      "33 Train Loss 4.4668555 Test MSE 2.142500411892734 Test RE 0.6996301508722287\n",
      "34 Train Loss 4.4621325 Test MSE 2.1428654217565235 Test RE 0.699689745033539\n",
      "35 Train Loss 4.447737 Test MSE 2.134816546222886 Test RE 0.6983744468518733\n",
      "36 Train Loss 4.43133 Test MSE 2.137633631193386 Test RE 0.6988350793017702\n",
      "37 Train Loss 4.383261 Test MSE 2.1294884772148945 Test RE 0.6975024019302764\n",
      "38 Train Loss 4.348796 Test MSE 2.140004151835714 Test RE 0.6992224571808634\n",
      "39 Train Loss 4.298991 Test MSE 2.096096149046348 Test RE 0.6920120554731246\n",
      "40 Train Loss 4.250701 Test MSE 2.1063521145065134 Test RE 0.6937029586791471\n",
      "41 Train Loss 4.1039333 Test MSE 2.052875981550217 Test RE 0.6848404706335415\n",
      "42 Train Loss 3.6765356 Test MSE 1.7397375642487023 Test RE 0.6304489571623935\n",
      "43 Train Loss 1.7952408 Test MSE 0.16970313112146165 Test RE 0.19690329692278502\n",
      "44 Train Loss 0.3976385 Test MSE 0.041508234721127 Test RE 0.09738121576635583\n",
      "45 Train Loss 0.22209609 Test MSE 0.03903739094920172 Test RE 0.0944383634160443\n",
      "46 Train Loss 0.16222803 Test MSE 0.024177918084956906 Test RE 0.07432202108846087\n",
      "47 Train Loss 0.13549383 Test MSE 0.019292552818484954 Test RE 0.06639003677910724\n",
      "48 Train Loss 0.101823054 Test MSE 0.016536696684668707 Test RE 0.061465644616272797\n",
      "49 Train Loss 0.07901268 Test MSE 0.011838487132191095 Test RE 0.052006324740199325\n",
      "50 Train Loss 0.05405686 Test MSE 0.0075476892468525885 Test RE 0.04152551868601514\n",
      "51 Train Loss 0.041215148 Test MSE 0.006243939540287841 Test RE 0.03776916685251279\n",
      "52 Train Loss 0.03441516 Test MSE 0.006732877303312048 Test RE 0.03922007414533223\n",
      "53 Train Loss 0.02938911 Test MSE 0.004756091542068408 Test RE 0.03296348821903111\n",
      "54 Train Loss 0.025311805 Test MSE 0.004309428040351768 Test RE 0.0313774666655671\n",
      "55 Train Loss 0.023635257 Test MSE 0.004121754466411717 Test RE 0.0306866244871223\n",
      "56 Train Loss 0.021483283 Test MSE 0.0031169505541469127 Test RE 0.026685356969218425\n",
      "57 Train Loss 0.019821158 Test MSE 0.0028075751464625097 Test RE 0.025326417095511526\n",
      "58 Train Loss 0.018800898 Test MSE 0.002630219474461432 Test RE 0.024513428383694327\n",
      "59 Train Loss 0.01491965 Test MSE 0.002381720845341797 Test RE 0.023326709942869447\n",
      "60 Train Loss 0.013624438 Test MSE 0.0022453376930029207 Test RE 0.022648992803896796\n",
      "61 Train Loss 0.012065974 Test MSE 0.0020243315571059003 Test RE 0.021505467251217516\n",
      "62 Train Loss 0.011255372 Test MSE 0.002082718741589672 Test RE 0.02181340048122999\n",
      "63 Train Loss 0.010780345 Test MSE 0.00199866917485029 Test RE 0.02136872044707362\n",
      "64 Train Loss 0.0098358095 Test MSE 0.0018230657608026071 Test RE 0.02040841267974417\n",
      "65 Train Loss 0.009115556 Test MSE 0.0016717897342943132 Test RE 0.019543344487221906\n",
      "66 Train Loss 0.008907856 Test MSE 0.0017218650724719333 Test RE 0.01983387713465514\n",
      "67 Train Loss 0.007888957 Test MSE 0.0013498729994096002 Test RE 0.01756121320448252\n",
      "68 Train Loss 0.0077477423 Test MSE 0.0013290253599185294 Test RE 0.01742507653534575\n",
      "69 Train Loss 0.007430156 Test MSE 0.0012683733091287423 Test RE 0.017022823941939134\n",
      "70 Train Loss 0.0065693147 Test MSE 0.0012087761465168628 Test RE 0.016618085951990307\n",
      "71 Train Loss 0.0059641544 Test MSE 0.0012279354636200663 Test RE 0.016749267991254403\n",
      "72 Train Loss 0.00574932 Test MSE 0.001172564776677226 Test RE 0.016367278877332678\n",
      "73 Train Loss 0.0056096395 Test MSE 0.0011536664760076463 Test RE 0.01623484687160331\n",
      "74 Train Loss 0.0050587375 Test MSE 0.0009674763236485187 Test RE 0.01486716745534631\n",
      "75 Train Loss 0.004591959 Test MSE 0.0008902391336858293 Test RE 0.014261375040660523\n",
      "76 Train Loss 0.004039071 Test MSE 0.0009131514753100163 Test RE 0.014443733703956803\n",
      "77 Train Loss 0.0036083853 Test MSE 0.0007000649277501676 Test RE 0.012646700119202287\n",
      "78 Train Loss 0.003324202 Test MSE 0.0006714318085460436 Test RE 0.012385370882549186\n",
      "79 Train Loss 0.003217025 Test MSE 0.0006574570743658147 Test RE 0.01225580272362352\n",
      "80 Train Loss 0.0031012446 Test MSE 0.0005986899376087937 Test RE 0.011695237340636589\n",
      "81 Train Loss 0.0028928516 Test MSE 0.0005615183637470156 Test RE 0.011326351680001616\n",
      "82 Train Loss 0.0027448353 Test MSE 0.0006025696255763873 Test RE 0.011733070446668471\n",
      "83 Train Loss 0.0026477338 Test MSE 0.0006726754391986624 Test RE 0.012396835711121169\n",
      "84 Train Loss 0.0025718412 Test MSE 0.0006512559564527214 Test RE 0.01219786758407317\n",
      "85 Train Loss 0.0024262508 Test MSE 0.0005321813144046043 Test RE 0.011026504808430582\n",
      "86 Train Loss 0.0023265323 Test MSE 0.00046514246426795496 Test RE 0.0103086324419289\n",
      "87 Train Loss 0.0021881156 Test MSE 0.0005051239985885666 Test RE 0.010742542029536475\n",
      "88 Train Loss 0.0021229642 Test MSE 0.0005661911852850736 Test RE 0.011373381634719409\n",
      "89 Train Loss 0.0020725538 Test MSE 0.0005176927705889086 Test RE 0.01087537172860718\n",
      "90 Train Loss 0.0018350673 Test MSE 0.0004561088099506158 Test RE 0.010208038297882763\n",
      "91 Train Loss 0.0016435889 Test MSE 0.00038500451818433495 Test RE 0.009378663919128788\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92 Train Loss 0.0015979385 Test MSE 0.0004174941430326947 Test RE 0.00976637180641998\n",
      "93 Train Loss 0.0015763225 Test MSE 0.00040381253404449 Test RE 0.009605013017267471\n",
      "94 Train Loss 0.0013410567 Test MSE 0.0003175262483299416 Test RE 0.008517220217697168\n",
      "95 Train Loss 0.001235441 Test MSE 0.0002593892785316492 Test RE 0.007698109261720236\n",
      "96 Train Loss 0.0011941027 Test MSE 0.0002738595246299188 Test RE 0.007909918072051086\n",
      "97 Train Loss 0.0011426368 Test MSE 0.00023516637325288862 Test RE 0.0073298598534792874\n",
      "98 Train Loss 0.0010992301 Test MSE 0.00021634441751671365 Test RE 0.0070304141108504005\n",
      "99 Train Loss 0.0010488344 Test MSE 0.00016867117590566106 Test RE 0.00620766821129004\n",
      "Training time: 127.85\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 54.47406 Test MSE 8.602784504922903 Test RE 1.401934044884623\n",
      "1 Train Loss 44.950993 Test MSE 8.417337802209548 Test RE 1.3867412636248322\n",
      "2 Train Loss 43.947243 Test MSE 8.487687302713482 Test RE 1.3925241826443528\n",
      "3 Train Loss 41.22202 Test MSE 7.786195005348806 Test RE 1.3337385328526372\n",
      "4 Train Loss 39.707447 Test MSE 7.66694973070655 Test RE 1.3234860503425405\n",
      "5 Train Loss 39.359173 Test MSE 7.972778653747521 Test RE 1.3496243766883664\n",
      "6 Train Loss 39.03311 Test MSE 7.819454381522057 Test RE 1.336584084584811\n",
      "7 Train Loss 36.38509 Test MSE 7.45571029894992 Test RE 1.3051263938414395\n",
      "8 Train Loss 34.143646 Test MSE 7.153360759031661 Test RE 1.2783892920220503\n",
      "9 Train Loss 33.289417 Test MSE 7.279843074340523 Test RE 1.2896417050780906\n",
      "10 Train Loss 32.00028 Test MSE 7.065945286373456 Test RE 1.2705541978007566\n",
      "11 Train Loss 30.295223 Test MSE 7.134740473616825 Test RE 1.2767243763975364\n",
      "12 Train Loss 27.838453 Test MSE 6.891228021180693 Test RE 1.2547475907332315\n",
      "13 Train Loss 25.346985 Test MSE 6.814890748177722 Test RE 1.2477785317116108\n",
      "14 Train Loss 22.641333 Test MSE 6.382364325262627 Test RE 1.2075325768860392\n",
      "15 Train Loss 20.333744 Test MSE 6.4545237296134905 Test RE 1.2143396096424772\n",
      "16 Train Loss 16.141567 Test MSE 4.989071767697767 Test RE 1.0676230359594616\n",
      "17 Train Loss 10.479503 Test MSE 4.128414310335478 Test RE 0.9711799273024566\n",
      "18 Train Loss 8.664563 Test MSE 3.8169125092902427 Test RE 0.9338221324404796\n",
      "19 Train Loss 7.2651353 Test MSE 3.8490678226639594 Test RE 0.9377473425946834\n",
      "20 Train Loss 6.520279 Test MSE 3.7552792358430076 Test RE 0.9262520415808573\n",
      "21 Train Loss 6.0549984 Test MSE 3.6465242142317025 Test RE 0.9127411089677463\n",
      "22 Train Loss 5.831272 Test MSE 3.6635177130565233 Test RE 0.9148654108195607\n",
      "23 Train Loss 5.5802207 Test MSE 3.5209842927495405 Test RE 0.8968919047031229\n",
      "24 Train Loss 5.3671193 Test MSE 3.478158812042851 Test RE 0.8914208015288414\n",
      "25 Train Loss 5.119371 Test MSE 3.1833374950323217 Test RE 0.8528043414610398\n",
      "26 Train Loss 4.498112 Test MSE 2.542777979263727 Test RE 0.7621883058170055\n",
      "27 Train Loss 3.917553 Test MSE 1.5840348603831484 Test RE 0.6015758976774834\n",
      "28 Train Loss 2.538341 Test MSE 0.8973341999334087 Test RE 0.4527778473885853\n",
      "29 Train Loss 1.8345225 Test MSE 0.7153736821066453 Test RE 0.404272816086589\n",
      "30 Train Loss 1.2439418 Test MSE 0.6377477467561666 Test RE 0.38170911142382263\n",
      "31 Train Loss 0.9900962 Test MSE 0.5336184788065351 Test RE 0.34915920060412536\n",
      "32 Train Loss 0.8360108 Test MSE 0.4612442385103743 Test RE 0.32461870243197444\n",
      "33 Train Loss 0.7515617 Test MSE 0.41117918056361386 Test RE 0.3064951584869792\n",
      "34 Train Loss 0.6238542 Test MSE 0.36008274783477995 Test RE 0.2868198585757307\n",
      "35 Train Loss 0.56158364 Test MSE 0.25403004109839966 Test RE 0.2409076546153463\n",
      "36 Train Loss 0.49255666 Test MSE 0.25875129670979785 Test RE 0.24313603369741435\n",
      "37 Train Loss 0.42272583 Test MSE 0.19563038310314376 Test RE 0.2114103343109505\n",
      "38 Train Loss 0.38986325 Test MSE 0.14955531722261717 Test RE 0.18484554561306174\n",
      "39 Train Loss 0.35329834 Test MSE 0.15987220513401593 Test RE 0.1911148979721096\n",
      "40 Train Loss 0.32560503 Test MSE 0.15768695418623638 Test RE 0.18980425431924522\n",
      "41 Train Loss 0.29702073 Test MSE 0.1297532724413726 Test RE 0.1721738616092495\n",
      "42 Train Loss 0.2771143 Test MSE 0.12415264946085607 Test RE 0.16841705011933422\n",
      "43 Train Loss 0.25930247 Test MSE 0.11395760656915747 Test RE 0.16135399397428674\n",
      "44 Train Loss 0.24794751 Test MSE 0.10127683986690975 Test RE 0.1521118789983001\n",
      "45 Train Loss 0.23545837 Test MSE 0.0961335424493323 Test RE 0.14819908876554144\n",
      "46 Train Loss 0.22258905 Test MSE 0.08709996559373379 Test RE 0.14106427852699166\n",
      "47 Train Loss 0.21188247 Test MSE 0.08286715750311509 Test RE 0.13759393189909444\n",
      "48 Train Loss 0.20229405 Test MSE 0.08207771771835355 Test RE 0.13693696437434208\n",
      "49 Train Loss 0.18926877 Test MSE 0.08530614203567814 Test RE 0.13960411216498145\n",
      "50 Train Loss 0.1822027 Test MSE 0.08512862590331 Test RE 0.1394587833100585\n",
      "51 Train Loss 0.1700483 Test MSE 0.08410550256350685 Test RE 0.13861820328782837\n",
      "52 Train Loss 0.15832628 Test MSE 0.0756805610198437 Test RE 0.13149227268528457\n",
      "53 Train Loss 0.14838956 Test MSE 0.07445420836968161 Test RE 0.13042254947751028\n",
      "54 Train Loss 0.14057058 Test MSE 0.06975430529555832 Test RE 0.12623900684752135\n",
      "55 Train Loss 0.13532129 Test MSE 0.06965518738616562 Test RE 0.1261492848260647\n",
      "56 Train Loss 0.12931186 Test MSE 0.06951768275012608 Test RE 0.1260247091753217\n",
      "57 Train Loss 0.12570113 Test MSE 0.07152505083030383 Test RE 0.12783128300771945\n",
      "58 Train Loss 0.11434454 Test MSE 0.07317747470358069 Test RE 0.12929947735370723\n",
      "59 Train Loss 0.10611214 Test MSE 0.07621374246935396 Test RE 0.1319546515975365\n",
      "60 Train Loss 0.10333817 Test MSE 0.07883150387982608 Test RE 0.13420168419609843\n",
      "61 Train Loss 0.09551104 Test MSE 0.08121604217885021 Test RE 0.13621626599680453\n",
      "62 Train Loss 0.090584 Test MSE 0.07848754081283121 Test RE 0.13390858509639803\n",
      "63 Train Loss 0.086645305 Test MSE 0.07398065088560189 Test RE 0.13000711898828438\n",
      "64 Train Loss 0.08423655 Test MSE 0.07566772674831174 Test RE 0.13148112266826922\n",
      "65 Train Loss 0.08011791 Test MSE 0.07275040101092006 Test RE 0.12892162057544146\n",
      "66 Train Loss 0.07684252 Test MSE 0.07039640484745657 Test RE 0.12681870099717985\n",
      "67 Train Loss 0.073804654 Test MSE 0.06812080788280447 Test RE 0.12475212591254124\n",
      "68 Train Loss 0.07035221 Test MSE 0.06419289858640002 Test RE 0.1211020663381452\n",
      "69 Train Loss 0.06849702 Test MSE 0.061660144236501895 Test RE 0.11868896110242624\n",
      "70 Train Loss 0.06705663 Test MSE 0.06102456141108257 Test RE 0.11807566322715261\n",
      "71 Train Loss 0.0647301 Test MSE 0.06231509773971033 Test RE 0.11931765256772912\n",
      "72 Train Loss 0.063066095 Test MSE 0.06139859042840663 Test RE 0.11843696249976575\n",
      "73 Train Loss 0.06116441 Test MSE 0.06088619238917006 Test RE 0.11794172301110224\n",
      "74 Train Loss 0.05985595 Test MSE 0.060957398788662596 Test RE 0.11801066927893118\n",
      "75 Train Loss 0.058902655 Test MSE 0.06160114449133767 Test RE 0.11863216351893\n",
      "76 Train Loss 0.057493936 Test MSE 0.06004885230916779 Test RE 0.11712791593853052\n",
      "77 Train Loss 0.054324076 Test MSE 0.05386009409652761 Test RE 0.11092811017117314\n",
      "78 Train Loss 0.05210133 Test MSE 0.050438390874782385 Test RE 0.10734669348218975\n",
      "79 Train Loss 0.04963937 Test MSE 0.05219018522242948 Test RE 0.10919493129082207\n",
      "80 Train Loss 0.04784526 Test MSE 0.053316772406028515 Test RE 0.11036719010954604\n",
      "81 Train Loss 0.046448108 Test MSE 0.0530984381879353 Test RE 0.11014097935846043\n",
      "82 Train Loss 0.044724967 Test MSE 0.05134517543522357 Test RE 0.10830733788426769\n",
      "83 Train Loss 0.04357793 Test MSE 0.04756728127779875 Test RE 0.10424667821708895\n",
      "84 Train Loss 0.042730443 Test MSE 0.04756962437113052 Test RE 0.1042492457034171\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85 Train Loss 0.042155374 Test MSE 0.04739139609831875 Test RE 0.10405376802810426\n",
      "86 Train Loss 0.041106652 Test MSE 0.04577373916206272 Test RE 0.10226246464623412\n",
      "87 Train Loss 0.04054768 Test MSE 0.04570031627405776 Test RE 0.1021804152114661\n",
      "88 Train Loss 0.038947523 Test MSE 0.04373301378714897 Test RE 0.09995689656505083\n",
      "89 Train Loss 0.038022317 Test MSE 0.041745083647706425 Test RE 0.09765865261717928\n",
      "90 Train Loss 0.036985826 Test MSE 0.04073221997521305 Test RE 0.09646662856720431\n",
      "91 Train Loss 0.03560885 Test MSE 0.04036102547676129 Test RE 0.09602607026497965\n",
      "92 Train Loss 0.03459997 Test MSE 0.038801290964679755 Test RE 0.09415234645762897\n",
      "93 Train Loss 0.03375441 Test MSE 0.03632242868765491 Test RE 0.09109520136252879\n",
      "94 Train Loss 0.032951128 Test MSE 0.03752047885327933 Test RE 0.09258534441708902\n",
      "95 Train Loss 0.031868204 Test MSE 0.040358924930563894 Test RE 0.0960235714456504\n",
      "96 Train Loss 0.030817281 Test MSE 0.04056310608922701 Test RE 0.09626616300430434\n",
      "97 Train Loss 0.030157214 Test MSE 0.0409181313120423 Test RE 0.09668652601502281\n",
      "98 Train Loss 0.029727278 Test MSE 0.03921403586871339 Test RE 0.09465178992357451\n",
      "99 Train Loss 0.029139927 Test MSE 0.03711093158536641 Test RE 0.09207865958258454\n",
      "Training time: 127.78\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 52.576897 Test MSE 8.482389116046685 Test RE 1.3920894938707562\n",
      "1 Train Loss 43.083275 Test MSE 8.308730622558137 Test RE 1.3777657987276062\n",
      "2 Train Loss 42.23568 Test MSE 8.38176076798005 Test RE 1.3838075341891927\n",
      "3 Train Loss 40.576233 Test MSE 7.936642616895829 Test RE 1.3465623662134063\n",
      "4 Train Loss 31.46926 Test MSE 6.966413503777563 Test RE 1.2615738690465856\n",
      "5 Train Loss 28.926994 Test MSE 6.558183224263923 Test RE 1.2240519025132204\n",
      "6 Train Loss 25.963394 Test MSE 6.322980736639809 Test RE 1.2019018112027018\n",
      "7 Train Loss 24.314323 Test MSE 6.230865266931709 Test RE 1.193114820682593\n",
      "8 Train Loss 23.459686 Test MSE 6.204175687520529 Test RE 1.1905567565253563\n",
      "9 Train Loss 22.840542 Test MSE 6.087300514716311 Test RE 1.179289499158524\n",
      "10 Train Loss 22.356245 Test MSE 5.937472170024606 Test RE 1.1646859964740184\n",
      "11 Train Loss 22.1118 Test MSE 5.807387346119953 Test RE 1.1518567117816816\n",
      "12 Train Loss 21.806984 Test MSE 5.648305700428112 Test RE 1.1359707716017373\n",
      "13 Train Loss 21.29237 Test MSE 5.698381321160508 Test RE 1.1409951914592646\n",
      "14 Train Loss 20.350689 Test MSE 5.66408974732687 Test RE 1.1375568850287887\n",
      "15 Train Loss 18.05656 Test MSE 6.117709156035888 Test RE 1.1822313547569365\n",
      "16 Train Loss 15.193424 Test MSE 6.020408985315117 Test RE 1.1727921702988124\n",
      "17 Train Loss 13.746613 Test MSE 6.244214322543179 Test RE 1.1943922062483503\n",
      "18 Train Loss 12.866837 Test MSE 6.130033447175191 Test RE 1.1834215742020915\n",
      "19 Train Loss 12.413338 Test MSE 6.135608732026009 Test RE 1.183959614728942\n",
      "20 Train Loss 12.093559 Test MSE 6.053500561063584 Test RE 1.1760109181693916\n",
      "21 Train Loss 11.781667 Test MSE 6.071287651548206 Test RE 1.1777373960275457\n",
      "22 Train Loss 11.396291 Test MSE 5.831355874667278 Test RE 1.1542312634417033\n",
      "23 Train Loss 10.972481 Test MSE 5.874257268792093 Test RE 1.1584693333217515\n",
      "24 Train Loss 9.909425 Test MSE 5.758628587306706 Test RE 1.147011031277082\n",
      "25 Train Loss 9.127296 Test MSE 5.506467113055303 Test RE 1.121617005241843\n",
      "26 Train Loss 8.529347 Test MSE 5.503095939002937 Test RE 1.1212736140100834\n",
      "27 Train Loss 7.9023757 Test MSE 5.579146493621458 Test RE 1.1289948029758343\n",
      "28 Train Loss 7.2651157 Test MSE 5.104523558846134 Test RE 1.0799052849055975\n",
      "29 Train Loss 6.7285795 Test MSE 4.965744043059645 Test RE 1.0651241345250577\n",
      "30 Train Loss 6.3816357 Test MSE 4.909519629903289 Test RE 1.0590770588541776\n",
      "31 Train Loss 6.0873766 Test MSE 4.697810156003313 Test RE 1.035990544234185\n",
      "32 Train Loss 5.9176326 Test MSE 4.047231588648416 Test RE 0.9615836898455106\n",
      "33 Train Loss 5.6815367 Test MSE 3.777371300019308 Test RE 0.9289725865927086\n",
      "34 Train Loss 5.389038 Test MSE 3.1582409206548636 Test RE 0.8494360499208009\n",
      "35 Train Loss 5.147782 Test MSE 2.785103630566764 Test RE 0.7976800674836023\n",
      "36 Train Loss 4.931165 Test MSE 2.6518684828761656 Test RE 0.7783663477598048\n",
      "37 Train Loss 4.713405 Test MSE 2.4889611370736118 Test RE 0.7540794715173751\n",
      "38 Train Loss 4.529084 Test MSE 2.3911269489615745 Test RE 0.7391105090739443\n",
      "39 Train Loss 4.38276 Test MSE 2.2701880921454403 Test RE 0.7201765581968456\n",
      "40 Train Loss 4.27954 Test MSE 2.2537058005817783 Test RE 0.7175574395630915\n",
      "41 Train Loss 4.161028 Test MSE 2.17463059426555 Test RE 0.7048566591939828\n",
      "42 Train Loss 4.0969343 Test MSE 2.1637660375311154 Test RE 0.7030937059761166\n",
      "43 Train Loss 4.011925 Test MSE 2.1612600428190567 Test RE 0.7026864393099008\n",
      "44 Train Loss 3.8715816 Test MSE 2.140700259613893 Test RE 0.6993361706555558\n",
      "45 Train Loss 2.9101584 Test MSE 1.9108132051903277 Test RE 0.6607195803170403\n",
      "46 Train Loss 2.0548606 Test MSE 1.2731784638744639 Test RE 0.5393276700178179\n",
      "47 Train Loss 1.6380601 Test MSE 0.9048906386299872 Test RE 0.45468026866650485\n",
      "48 Train Loss 0.94722193 Test MSE 0.19235588582200291 Test RE 0.20963355542154669\n",
      "49 Train Loss 0.40013945 Test MSE 0.11649068560015628 Test RE 0.16313744675245365\n",
      "50 Train Loss 0.20257924 Test MSE 0.05853927696191485 Test RE 0.11564629862123307\n",
      "51 Train Loss 0.140757 Test MSE 0.04689130373352108 Test RE 0.10350330417136493\n",
      "52 Train Loss 0.109302655 Test MSE 0.030627681208775074 Test RE 0.08364984090435723\n",
      "53 Train Loss 0.0898588 Test MSE 0.02652976558811221 Test RE 0.07785289498921942\n",
      "54 Train Loss 0.07353328 Test MSE 0.016615852835970706 Test RE 0.061612577694013966\n",
      "55 Train Loss 0.06273424 Test MSE 0.01239601909589538 Test RE 0.053216851747683154\n",
      "56 Train Loss 0.053638708 Test MSE 0.012961303626033807 Test RE 0.054416725209002835\n",
      "57 Train Loss 0.047195155 Test MSE 0.010868900960875519 Test RE 0.04983114610801165\n",
      "58 Train Loss 0.04101877 Test MSE 0.009497940681442045 Test RE 0.046582499366694737\n",
      "59 Train Loss 0.03398838 Test MSE 0.00826178215945867 Test RE 0.04344551304060675\n",
      "60 Train Loss 0.027357563 Test MSE 0.006747885930298952 Test RE 0.03926376362307811\n",
      "61 Train Loss 0.023099894 Test MSE 0.005804513791070111 Test RE 0.03641589464915607\n",
      "62 Train Loss 0.019692985 Test MSE 0.005107550235093078 Test RE 0.03415972640834409\n",
      "63 Train Loss 0.016939301 Test MSE 0.004014198558305707 Test RE 0.0302835989126801\n",
      "64 Train Loss 0.014501065 Test MSE 0.0033344235981975423 Test RE 0.02760059507862253\n",
      "65 Train Loss 0.013576032 Test MSE 0.002750725022774393 Test RE 0.025068690547241177\n",
      "66 Train Loss 0.012188163 Test MSE 0.0027249252166722617 Test RE 0.024950850513025723\n",
      "67 Train Loss 0.011049304 Test MSE 0.002461771416457004 Test RE 0.023715480191531108\n",
      "68 Train Loss 0.009509939 Test MSE 0.001590025330731859 Test RE 0.019059437616744813\n",
      "69 Train Loss 0.0080353925 Test MSE 0.0015685644284960559 Test RE 0.018930376049893602\n",
      "70 Train Loss 0.0071472237 Test MSE 0.0013257805402598494 Test RE 0.01740379184575467\n",
      "71 Train Loss 0.0062596803 Test MSE 0.001352253681438185 Test RE 0.017576692159753025\n",
      "72 Train Loss 0.0057569835 Test MSE 0.0010910167857238787 Test RE 0.015787878536097723\n",
      "73 Train Loss 0.00521709 Test MSE 0.0008622942173615371 Test RE 0.014035755591593984\n",
      "74 Train Loss 0.004742689 Test MSE 0.0008684168448387245 Test RE 0.014085497139191504\n",
      "75 Train Loss 0.0038909712 Test MSE 0.0009233251173733084 Test RE 0.014523971406266285\n",
      "76 Train Loss 0.003537748 Test MSE 0.0007933196180268949 Test RE 0.013462699663910399\n",
      "77 Train Loss 0.0031370847 Test MSE 0.0007964479152249172 Test RE 0.01348921727925647\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78 Train Loss 0.0028150484 Test MSE 0.000819017092107544 Test RE 0.013679006088777458\n",
      "79 Train Loss 0.002510628 Test MSE 0.0006607928318258681 Test RE 0.012286854682437037\n",
      "80 Train Loss 0.002364831 Test MSE 0.000606521006392982 Test RE 0.01177147768683951\n",
      "81 Train Loss 0.0021757642 Test MSE 0.00045230638162132886 Test RE 0.010165398723960129\n",
      "82 Train Loss 0.0019014223 Test MSE 0.00033655397093928634 Test RE 0.008768704196290555\n",
      "83 Train Loss 0.0016986188 Test MSE 0.0002537709710563543 Test RE 0.0076142832952174905\n",
      "84 Train Loss 0.0015912617 Test MSE 0.0002761090570300126 Test RE 0.007942338391021792\n",
      "85 Train Loss 0.0014751086 Test MSE 0.00027786837794648706 Test RE 0.00796760183988958\n",
      "86 Train Loss 0.0013885095 Test MSE 0.00026612218846820316 Test RE 0.007797378269446774\n",
      "87 Train Loss 0.0013000617 Test MSE 0.0002316103714618437 Test RE 0.007274230555577318\n",
      "88 Train Loss 0.0012505772 Test MSE 0.00020527177450758825 Test RE 0.006848140762407854\n",
      "89 Train Loss 0.0011828956 Test MSE 0.00018311689901376592 Test RE 0.006468033682934297\n",
      "90 Train Loss 0.0010707953 Test MSE 0.00019826786538188 Test RE 0.006730296940520557\n",
      "91 Train Loss 0.0009964837 Test MSE 0.00015226383773908289 Test RE 0.00589802278778017\n",
      "92 Train Loss 0.00092985854 Test MSE 0.00011274576626721071 Test RE 0.0050752586677143005\n",
      "93 Train Loss 0.0008845557 Test MSE 0.00010424383704560655 Test RE 0.004880150939428711\n",
      "94 Train Loss 0.00084855466 Test MSE 9.307497230303124e-05 Test RE 0.00461131211316385\n",
      "95 Train Loss 0.0007936995 Test MSE 9.884762479442498e-05 Test RE 0.0047521613667774005\n",
      "96 Train Loss 0.00074541767 Test MSE 9.43313606718501e-05 Test RE 0.004642331076410395\n",
      "97 Train Loss 0.0006272688 Test MSE 7.161125226684694e-05 Test RE 0.004044815294338864\n",
      "98 Train Loss 0.0005785475 Test MSE 7.440635940784712e-05 Test RE 0.0041229976580969534\n",
      "99 Train Loss 0.0005544512 Test MSE 6.976665703175529e-05 Test RE 0.0039923813402715945\n",
      "Training time: 127.62\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 55.04545 Test MSE 8.920163894274747 Test RE 1.4275603505917536\n",
      "1 Train Loss 45.509018 Test MSE 8.757549751137645 Test RE 1.4144883239599468\n",
      "2 Train Loss 44.048813 Test MSE 8.636161561849256 Test RE 1.4046510218090293\n",
      "3 Train Loss 42.83883 Test MSE 8.474866019352273 Test RE 1.391472029447476\n",
      "4 Train Loss 42.200912 Test MSE 8.434893875812469 Test RE 1.388186676116824\n",
      "5 Train Loss 41.271233 Test MSE 8.46923255716516 Test RE 1.3910094788808023\n",
      "6 Train Loss 40.500946 Test MSE 8.687433330625542 Test RE 1.4088144667747196\n",
      "7 Train Loss 40.037376 Test MSE 8.723281875369212 Test RE 1.4117181986842786\n",
      "8 Train Loss 34.228676 Test MSE 8.945016807959801 Test RE 1.4295476658744857\n",
      "9 Train Loss 32.25908 Test MSE 8.384844196212054 Test RE 1.384062043891364\n",
      "10 Train Loss 31.479877 Test MSE 8.664893538086698 Test RE 1.4069856752160066\n",
      "11 Train Loss 30.577543 Test MSE 8.170179853486998 Test RE 1.3662301613983538\n",
      "12 Train Loss 29.024467 Test MSE 7.845781997544617 Test RE 1.338832291476922\n",
      "13 Train Loss 26.874 Test MSE 7.990370545658762 Test RE 1.351112525596124\n",
      "14 Train Loss 24.097212 Test MSE 7.488405095992587 Test RE 1.3079848850565223\n",
      "15 Train Loss 21.979176 Test MSE 7.418589474414083 Test RE 1.3018733287753517\n",
      "16 Train Loss 20.78305 Test MSE 7.358271217795547 Test RE 1.2965699620664608\n",
      "17 Train Loss 19.877687 Test MSE 7.443902206798735 Test RE 1.3040924776337928\n",
      "18 Train Loss 15.447865 Test MSE 6.261918904182847 Test RE 1.1960842723094558\n",
      "19 Train Loss 14.060039 Test MSE 6.457557163392429 Test RE 1.2146249278355148\n",
      "20 Train Loss 13.118006 Test MSE 6.250478965332915 Test RE 1.194991205926087\n",
      "21 Train Loss 12.451134 Test MSE 6.1254603465307005 Test RE 1.182980066359713\n",
      "22 Train Loss 11.412366 Test MSE 6.275020780308101 Test RE 1.1973349080693914\n",
      "23 Train Loss 10.380693 Test MSE 6.475805372257734 Test RE 1.2163399024185853\n",
      "24 Train Loss 9.3378 Test MSE 6.760329636502352 Test RE 1.2427735360493803\n",
      "25 Train Loss 8.541058 Test MSE 6.554472855113354 Test RE 1.223705592657118\n",
      "26 Train Loss 8.190004 Test MSE 6.545109051202959 Test RE 1.22283117950087\n",
      "27 Train Loss 7.8103437 Test MSE 6.668256271601981 Test RE 1.2342814479934963\n",
      "28 Train Loss 7.628448 Test MSE 6.6743160967184165 Test RE 1.234842151640559\n",
      "29 Train Loss 7.356519 Test MSE 6.452822611177073 Test RE 1.2141795767948993\n",
      "30 Train Loss 7.0811405 Test MSE 6.518544491951425 Test RE 1.2203471109614967\n",
      "31 Train Loss 6.852973 Test MSE 6.52293460581011 Test RE 1.2207579819034335\n",
      "32 Train Loss 6.600579 Test MSE 6.496271413900572 Test RE 1.2182604377257993\n",
      "33 Train Loss 6.4381466 Test MSE 6.459936287681019 Test RE 1.2148486562114265\n",
      "34 Train Loss 6.209588 Test MSE 6.598646010832874 Test RE 1.2278221837360261\n",
      "35 Train Loss 6.0139127 Test MSE 6.6983825182138945 Test RE 1.2370664612005502\n",
      "36 Train Loss 5.11231 Test MSE 6.41651143882859 Test RE 1.2107585558867753\n",
      "37 Train Loss 4.096308 Test MSE 6.2114002437719975 Test RE 1.1912497367198482\n",
      "38 Train Loss 3.1066427 Test MSE 5.897389254600688 Test RE 1.1607480355809463\n",
      "39 Train Loss 2.4715133 Test MSE 5.733742767877623 Test RE 1.144529953167033\n",
      "40 Train Loss 2.0685952 Test MSE 5.90359822920916 Test RE 1.1613589125985255\n",
      "41 Train Loss 1.870205 Test MSE 5.901109967848217 Test RE 1.161114140769851\n",
      "42 Train Loss 1.7370282 Test MSE 5.8384986834082175 Test RE 1.1549379541521343\n",
      "43 Train Loss 1.6418868 Test MSE 5.877297314280384 Test RE 1.1587690600596547\n",
      "44 Train Loss 1.5618377 Test MSE 5.878259987997844 Test RE 1.1588639566408316\n",
      "45 Train Loss 1.4839946 Test MSE 5.922077583159487 Test RE 1.1631751264954184\n",
      "46 Train Loss 1.4197757 Test MSE 5.944994288836437 Test RE 1.1654235269562843\n",
      "47 Train Loss 1.355523 Test MSE 5.943810525050878 Test RE 1.1653074919542763\n",
      "48 Train Loss 1.2888721 Test MSE 5.943924874143612 Test RE 1.165318701195639\n",
      "49 Train Loss 1.2248167 Test MSE 5.99619165094021 Test RE 1.1704309918686315\n",
      "50 Train Loss 1.1711854 Test MSE 6.0342171269609 Test RE 1.1741363319310194\n",
      "51 Train Loss 1.1219473 Test MSE 6.058324837207059 Test RE 1.1764794298569647\n",
      "52 Train Loss 1.0758393 Test MSE 6.049245104595982 Test RE 1.1755974926622017\n",
      "53 Train Loss 1.0349858 Test MSE 6.095078923992309 Test RE 1.1800427121708303\n",
      "54 Train Loss 0.9943289 Test MSE 6.123563625989048 Test RE 1.182796900019029\n",
      "55 Train Loss 0.9622303 Test MSE 6.111641990146509 Test RE 1.1816449773375686\n",
      "56 Train Loss 0.94024444 Test MSE 6.140092491620372 Test RE 1.1843921407438367\n",
      "57 Train Loss 0.9198768 Test MSE 6.139531055776668 Test RE 1.1843379904681672\n",
      "58 Train Loss 0.90412974 Test MSE 6.086544291445665 Test RE 1.1792162455168769\n",
      "59 Train Loss 0.8885646 Test MSE 6.102858478318114 Test RE 1.1807955554926002\n",
      "60 Train Loss 0.8745975 Test MSE 6.124884830653981 Test RE 1.1829244917772392\n",
      "61 Train Loss 0.86360306 Test MSE 6.10759893021768 Test RE 1.181254063419458\n",
      "62 Train Loss 0.84997386 Test MSE 6.106218915785679 Test RE 1.1811206034651014\n",
      "63 Train Loss 0.8314455 Test MSE 6.124660870274647 Test RE 1.1829028643795745\n",
      "64 Train Loss 0.82109565 Test MSE 6.15722580452634 Test RE 1.1860434533914006\n",
      "65 Train Loss 0.8097548 Test MSE 6.199625572914209 Test RE 1.1901201019594014\n",
      "66 Train Loss 0.80104566 Test MSE 6.2281582978900385 Test RE 1.1928556210686434\n",
      "67 Train Loss 0.7905911 Test MSE 6.258874332823036 Test RE 1.1957934663505563\n",
      "68 Train Loss 0.7814373 Test MSE 6.282770770481092 Test RE 1.1980740666111875\n",
      "69 Train Loss 0.77324826 Test MSE 6.290309742688997 Test RE 1.1987926619171543\n",
      "70 Train Loss 0.76447064 Test MSE 6.321672924042872 Test RE 1.201777507201012\n",
      "71 Train Loss 0.7565739 Test MSE 6.347008016137684 Test RE 1.2041832548362628\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72 Train Loss 0.7476583 Test MSE 6.389277008526829 Test RE 1.208186333927612\n",
      "73 Train Loss 0.7408482 Test MSE 6.420383267486442 Test RE 1.2111237966216335\n",
      "74 Train Loss 0.73336136 Test MSE 6.446993650607469 Test RE 1.2136310568435638\n",
      "75 Train Loss 0.7258501 Test MSE 6.469298654162965 Test RE 1.2157286757497028\n",
      "76 Train Loss 0.7195274 Test MSE 6.491800713220304 Test RE 1.217841165284524\n",
      "77 Train Loss 0.7115152 Test MSE 6.505366848160058 Test RE 1.2191129830735523\n",
      "78 Train Loss 0.7042143 Test MSE 6.537504528216606 Test RE 1.2221205916125408\n",
      "79 Train Loss 0.6974133 Test MSE 6.567148947650057 Test RE 1.2248883203234833\n",
      "80 Train Loss 0.6909097 Test MSE 6.567712354540854 Test RE 1.2249408618239745\n",
      "81 Train Loss 0.68701273 Test MSE 6.5983163621425645 Test RE 1.2277915141843265\n",
      "82 Train Loss 0.6818762 Test MSE 6.614068083104452 Test RE 1.2292561530296726\n",
      "83 Train Loss 0.6764307 Test MSE 6.632301824347487 Test RE 1.2309494008038186\n",
      "84 Train Loss 0.668904 Test MSE 6.662609538016289 Test RE 1.233758737532278\n",
      "85 Train Loss 0.66339153 Test MSE 6.6875925278395405 Test RE 1.23606970453301\n",
      "86 Train Loss 0.6569384 Test MSE 6.670498669415041 Test RE 1.234488962316708\n",
      "87 Train Loss 0.649003 Test MSE 6.651394369469582 Test RE 1.232719907334455\n",
      "88 Train Loss 0.64222884 Test MSE 6.683803661397548 Test RE 1.2357195062721877\n",
      "89 Train Loss 0.634789 Test MSE 6.711439289120718 Test RE 1.2382715454065043\n",
      "90 Train Loss 0.6294402 Test MSE 6.734394396989896 Test RE 1.2403873652750583\n",
      "91 Train Loss 0.62156487 Test MSE 6.737410845490179 Test RE 1.240665129354766\n",
      "92 Train Loss 0.6148308 Test MSE 6.7604105036238815 Test RE 1.2427809690610405\n",
      "93 Train Loss 0.60789037 Test MSE 6.798567164368747 Test RE 1.2462832451600647\n",
      "94 Train Loss 0.6023752 Test MSE 6.832074203711606 Test RE 1.2493506514261257\n",
      "95 Train Loss 0.59705627 Test MSE 6.848892643727796 Test RE 1.2508874624242088\n",
      "96 Train Loss 0.592739 Test MSE 6.876976932188252 Test RE 1.2534495075860912\n",
      "97 Train Loss 0.58786213 Test MSE 6.8994154668564915 Test RE 1.2554927502413358\n",
      "98 Train Loss 0.5843458 Test MSE 6.888045147429883 Test RE 1.2544577901047487\n",
      "99 Train Loss 0.58101743 Test MSE 6.9154023908194 Test RE 1.2569464859652497\n",
      "Training time: 127.83\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 56.271263 Test MSE 8.447080159910403 Test RE 1.3891891031620975\n",
      "1 Train Loss 47.783142 Test MSE 7.888193999144025 Test RE 1.3424460820809108\n",
      "2 Train Loss 44.587105 Test MSE 8.097537475892604 Test RE 1.3601429149286186\n",
      "3 Train Loss 42.4318 Test MSE 7.2022750553315005 Test RE 1.2827526242977239\n",
      "4 Train Loss 35.59574 Test MSE 6.7802223691791985 Test RE 1.2446006660196767\n",
      "5 Train Loss 33.15847 Test MSE 6.371654506912497 Test RE 1.2065190117149507\n",
      "6 Train Loss 27.75748 Test MSE 5.562868620688982 Test RE 1.1273466065778694\n",
      "7 Train Loss 25.587166 Test MSE 5.624196025199445 Test RE 1.1335437451158443\n",
      "8 Train Loss 23.493172 Test MSE 4.174429011489017 Test RE 0.9765772441549481\n",
      "9 Train Loss 22.375984 Test MSE 4.1309794227442715 Test RE 0.9714815926214889\n",
      "10 Train Loss 21.858881 Test MSE 3.780542927389818 Test RE 0.9293625054594538\n",
      "11 Train Loss 21.118076 Test MSE 3.9880914580114704 Test RE 0.9545322693767037\n",
      "12 Train Loss 19.821468 Test MSE 3.671968959983911 Test RE 0.9159200392125523\n",
      "13 Train Loss 18.899693 Test MSE 3.130632566214156 Test RE 0.8457151481809163\n",
      "14 Train Loss 16.82392 Test MSE 1.1106167768629243 Test RE 0.5037211371133831\n",
      "15 Train Loss 10.873759 Test MSE 0.8408058117735855 Test RE 0.43828429888471887\n",
      "16 Train Loss 7.729738 Test MSE 0.6439772595191482 Test RE 0.3835688461306682\n",
      "17 Train Loss 6.5526786 Test MSE 0.4995106721426846 Test RE 0.3378161795005804\n",
      "18 Train Loss 5.6104736 Test MSE 0.4344998700437479 Test RE 0.3150669767927069\n",
      "19 Train Loss 4.897531 Test MSE 0.3467157329340397 Test RE 0.28144584065790584\n",
      "20 Train Loss 4.239083 Test MSE 0.34871945870263066 Test RE 0.28225792935258465\n",
      "21 Train Loss 3.8249521 Test MSE 0.3700421754232255 Test RE 0.2907593390820356\n",
      "22 Train Loss 3.5345683 Test MSE 0.33282945078183535 Test RE 0.27575216726764973\n",
      "23 Train Loss 3.1892614 Test MSE 0.2531734935210348 Test RE 0.240501161146043\n",
      "24 Train Loss 2.6781378 Test MSE 0.19634830901862457 Test RE 0.21179789671882257\n",
      "25 Train Loss 2.4398973 Test MSE 0.1771115780800969 Test RE 0.20115532633352803\n",
      "26 Train Loss 2.051471 Test MSE 0.12358071617551412 Test RE 0.1680286794060564\n",
      "27 Train Loss 1.7070321 Test MSE 0.12285059758912344 Test RE 0.16753158486132397\n",
      "28 Train Loss 1.2985016 Test MSE 0.10928401031983319 Test RE 0.1580106545413853\n",
      "29 Train Loss 0.96277845 Test MSE 0.07397175976001608 Test RE 0.12999930651069713\n",
      "30 Train Loss 0.854658 Test MSE 0.07276101835677279 Test RE 0.12893102777785262\n",
      "31 Train Loss 0.70263857 Test MSE 0.05575904177307601 Test RE 0.11286666981071666\n",
      "32 Train Loss 0.6032151 Test MSE 0.04072707496594925 Test RE 0.09646053587974597\n",
      "33 Train Loss 0.5066533 Test MSE 0.03863816619815996 Test RE 0.09395422475279656\n",
      "34 Train Loss 0.43867207 Test MSE 0.04187620620199911 Test RE 0.09781190673962242\n",
      "35 Train Loss 0.3836025 Test MSE 0.033089072541869005 Test RE 0.08694615028283179\n",
      "36 Train Loss 0.3631761 Test MSE 0.03294076858306223 Test RE 0.08675108680893269\n",
      "37 Train Loss 0.32870942 Test MSE 0.031232336766961924 Test RE 0.08447151818288728\n",
      "38 Train Loss 0.2679507 Test MSE 0.018879284249922582 Test RE 0.0656751121295985\n",
      "39 Train Loss 0.25183356 Test MSE 0.020755800699820946 Test RE 0.06886171050025347\n",
      "40 Train Loss 0.20674524 Test MSE 0.01758316307586522 Test RE 0.06338063153860256\n",
      "41 Train Loss 0.18275261 Test MSE 0.017080852824877545 Test RE 0.062468752849379736\n",
      "42 Train Loss 0.16399229 Test MSE 0.01669602806296175 Test RE 0.06176104608350226\n",
      "43 Train Loss 0.13889068 Test MSE 0.013282172088598432 Test RE 0.05508617426751251\n",
      "44 Train Loss 0.1315568 Test MSE 0.013981329275556748 Test RE 0.05651741531629121\n",
      "45 Train Loss 0.11947004 Test MSE 0.013791055091326299 Test RE 0.05613152054358358\n",
      "46 Train Loss 0.10947305 Test MSE 0.013914110398315768 Test RE 0.05638139053811135\n",
      "47 Train Loss 0.10516143 Test MSE 0.012986157999406098 Test RE 0.05446887450996558\n",
      "48 Train Loss 0.096625604 Test MSE 0.012987233436770556 Test RE 0.05447112985947435\n",
      "49 Train Loss 0.087571494 Test MSE 0.012377424995694178 Test RE 0.05317692397668152\n",
      "50 Train Loss 0.08315623 Test MSE 0.011908643139750482 Test RE 0.052160194343858046\n",
      "51 Train Loss 0.07994102 Test MSE 0.011277189431590406 Test RE 0.05075846711630345\n",
      "52 Train Loss 0.07542411 Test MSE 0.008791936540484217 Test RE 0.04481777919653981\n",
      "53 Train Loss 0.07072863 Test MSE 0.007557199649255676 Test RE 0.041551672390335394\n",
      "54 Train Loss 0.06458844 Test MSE 0.0066871011222182294 Test RE 0.03908652000880183\n",
      "55 Train Loss 0.0553192 Test MSE 0.004261518146816987 Test RE 0.031202560314198734\n",
      "56 Train Loss 0.05050323 Test MSE 0.003441676695335938 Test RE 0.028040974071214024\n",
      "57 Train Loss 0.04404558 Test MSE 0.0030662708935189488 Test RE 0.026467524303007778\n",
      "58 Train Loss 0.04026358 Test MSE 0.0027740543493311076 Test RE 0.025174771830072624\n",
      "59 Train Loss 0.038824283 Test MSE 0.002584645826786487 Test RE 0.024300129073200904\n",
      "60 Train Loss 0.03787359 Test MSE 0.0023377836220906814 Test RE 0.02311054653734386\n",
      "61 Train Loss 0.036042407 Test MSE 0.0025247459542667985 Test RE 0.024016897398935286\n",
      "62 Train Loss 0.033866238 Test MSE 0.002524665387147472 Test RE 0.024016514194503912\n",
      "63 Train Loss 0.032756336 Test MSE 0.0026223745993039847 Test RE 0.02447684429000879\n",
      "64 Train Loss 0.030432042 Test MSE 0.002457683043156387 Test RE 0.02369577933215276\n",
      "65 Train Loss 0.028052196 Test MSE 0.002453525922541227 Test RE 0.023675730387353627\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66 Train Loss 0.026454316 Test MSE 0.0025089256517428554 Test RE 0.023941533051466346\n",
      "67 Train Loss 0.025041487 Test MSE 0.0026245006895520137 Test RE 0.024486764580645623\n",
      "68 Train Loss 0.023840616 Test MSE 0.002711880437458459 Test RE 0.024891056430913213\n",
      "69 Train Loss 0.022587562 Test MSE 0.002626385406911907 Test RE 0.024495555271381482\n",
      "70 Train Loss 0.019841133 Test MSE 0.0021618231034709673 Test RE 0.022223790644000978\n",
      "71 Train Loss 0.018440563 Test MSE 0.0019180914751848524 Test RE 0.020933541987706375\n",
      "72 Train Loss 0.017610367 Test MSE 0.0019114585390834233 Test RE 0.020897315588725738\n",
      "73 Train Loss 0.016762776 Test MSE 0.0014953498283640124 Test RE 0.01848329788633216\n",
      "74 Train Loss 0.016543284 Test MSE 0.001338501389162615 Test RE 0.01748708710890312\n",
      "75 Train Loss 0.016175866 Test MSE 0.0014373748625746508 Test RE 0.01812145575461081\n",
      "76 Train Loss 0.015815187 Test MSE 0.0014736040017714835 Test RE 0.018348410862214188\n",
      "77 Train Loss 0.0155323995 Test MSE 0.0013025750424323645 Test RE 0.017250807806199094\n",
      "78 Train Loss 0.014169165 Test MSE 0.0010993216141628056 Test RE 0.015847853348691626\n",
      "79 Train Loss 0.01371246 Test MSE 0.0010258769578237303 Test RE 0.01530931284431914\n",
      "80 Train Loss 0.013555731 Test MSE 0.0009883202646327033 Test RE 0.015026467985640772\n",
      "81 Train Loss 0.0130933765 Test MSE 0.0009655158926671797 Test RE 0.014852096887278263\n",
      "82 Train Loss 0.01275085 Test MSE 0.0010500735683003463 Test RE 0.015488805412496975\n",
      "83 Train Loss 0.0124234855 Test MSE 0.0009746910355817915 Test RE 0.014922498577935967\n",
      "84 Train Loss 0.01210336 Test MSE 0.0009822585449022379 Test RE 0.014980315773263057\n",
      "85 Train Loss 0.011863591 Test MSE 0.0009314553386784114 Test RE 0.014587775745772923\n",
      "86 Train Loss 0.011764654 Test MSE 0.0009313265541388015 Test RE 0.014586767246037522\n",
      "87 Train Loss 0.011611885 Test MSE 0.0009523256952389432 Test RE 0.014750298579092085\n",
      "88 Train Loss 0.0114206355 Test MSE 0.0009204470617487038 Test RE 0.014501317726902416\n",
      "89 Train Loss 0.0110429535 Test MSE 0.0009901381273770488 Test RE 0.015040281072312898\n",
      "90 Train Loss 0.010948038 Test MSE 0.0009878087680038484 Test RE 0.01502257907295482\n",
      "91 Train Loss 0.010678321 Test MSE 0.0010088798086415368 Test RE 0.015181957643162994\n",
      "92 Train Loss 0.010357973 Test MSE 0.0010835724053492616 Test RE 0.01573392329448183\n",
      "93 Train Loss 0.009970193 Test MSE 0.0009419558778676111 Test RE 0.0146697712064106\n",
      "94 Train Loss 0.009694919 Test MSE 0.0009313186791800708 Test RE 0.014586705575706968\n",
      "95 Train Loss 0.009584552 Test MSE 0.0009128066848797333 Test RE 0.014441006592695052\n",
      "96 Train Loss 0.009363221 Test MSE 0.0008310178409041637 Test RE 0.013778858305293944\n",
      "97 Train Loss 0.009233541 Test MSE 0.0008808484470775486 Test RE 0.014185957577981138\n",
      "98 Train Loss 0.009099626 Test MSE 0.0009212009793533361 Test RE 0.014507255363835904\n",
      "99 Train Loss 0.008997607 Test MSE 0.0009062038723656268 Test RE 0.01438868208707847\n",
      "Training time: 131.58\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 56.13566 Test MSE 9.09970892546189 Test RE 1.4418557397344351\n",
      "1 Train Loss 46.616707 Test MSE 8.570688424679863 Test RE 1.399316367118326\n",
      "2 Train Loss 45.447033 Test MSE 8.68616222435613 Test RE 1.4087113972982965\n",
      "3 Train Loss 44.569466 Test MSE 8.531256831730156 Test RE 1.3960937039389691\n",
      "4 Train Loss 44.25982 Test MSE 8.372992164303431 Test RE 1.3830835076183046\n",
      "5 Train Loss 43.438866 Test MSE 8.001969219099056 Test RE 1.352092794910649\n",
      "6 Train Loss 40.969116 Test MSE 7.589479077112908 Test RE 1.3167824943872561\n",
      "7 Train Loss 39.94716 Test MSE 7.929681065191682 Test RE 1.3459716743739978\n",
      "8 Train Loss 39.372833 Test MSE 7.9639459268568205 Test RE 1.3488765717174822\n",
      "9 Train Loss 38.491577 Test MSE 7.495048972534796 Test RE 1.3085649927916931\n",
      "10 Train Loss 35.94812 Test MSE 7.804611803776831 Test RE 1.3353149566342566\n",
      "11 Train Loss 33.81939 Test MSE 7.645053771169238 Test RE 1.3215948342902568\n",
      "12 Train Loss 32.874405 Test MSE 7.536828873855102 Test RE 1.3122071129762074\n",
      "13 Train Loss 31.28792 Test MSE 7.389173961583701 Test RE 1.29928973061204\n",
      "14 Train Loss 29.28158 Test MSE 7.8956214930120066 Test RE 1.3430779544703209\n",
      "15 Train Loss 28.015461 Test MSE 7.590656701726218 Test RE 1.3168846499707871\n",
      "16 Train Loss 26.697277 Test MSE 6.987542213699675 Test RE 1.263485559174284\n",
      "17 Train Loss 26.211449 Test MSE 6.984793392928474 Test RE 1.2632370142010065\n",
      "18 Train Loss 25.559668 Test MSE 7.101476597010699 Test RE 1.2737447012471494\n",
      "19 Train Loss 24.73513 Test MSE 7.478275497841722 Test RE 1.3070999272615496\n",
      "20 Train Loss 23.976776 Test MSE 7.4245503700396 Test RE 1.3023962565375147\n",
      "21 Train Loss 23.447016 Test MSE 7.4796402505521185 Test RE 1.3072191918426277\n",
      "22 Train Loss 23.211075 Test MSE 7.584520633772013 Test RE 1.3163522765749436\n",
      "23 Train Loss 22.933235 Test MSE 7.643988532129225 Test RE 1.3215027575372156\n",
      "24 Train Loss 22.69651 Test MSE 7.726400433556993 Test RE 1.3286073983861608\n",
      "25 Train Loss 22.588476 Test MSE 7.6571332172349 Test RE 1.322638504682054\n",
      "26 Train Loss 22.25211 Test MSE 7.830633965478532 Test RE 1.3375392098541115\n",
      "27 Train Loss 22.10795 Test MSE 7.727999996936886 Test RE 1.328744919200546\n",
      "28 Train Loss 21.93869 Test MSE 7.588529600792844 Test RE 1.3167001242476286\n",
      "29 Train Loss 21.366688 Test MSE 7.45867002580526 Test RE 1.3053854190850878\n",
      "30 Train Loss 21.034143 Test MSE 7.5226275912899485 Test RE 1.310970265831587\n",
      "31 Train Loss 20.810032 Test MSE 7.467935943139388 Test RE 1.3061960085788302\n",
      "32 Train Loss 20.669197 Test MSE 7.7061023154712665 Test RE 1.3268610505533471\n",
      "33 Train Loss 20.513416 Test MSE 7.616450420018117 Test RE 1.3191201972976123\n",
      "34 Train Loss 20.415768 Test MSE 7.6424457233468495 Test RE 1.3213693894050136\n",
      "35 Train Loss 20.371195 Test MSE 7.658949327861337 Test RE 1.3227953463721918\n",
      "36 Train Loss 20.294662 Test MSE 7.648214104758606 Test RE 1.3218679683399697\n",
      "37 Train Loss 20.229122 Test MSE 7.686057786885309 Test RE 1.3251342617845272\n",
      "38 Train Loss 20.17144 Test MSE 7.616045574283912 Test RE 1.3190851384938305\n",
      "39 Train Loss 20.135431 Test MSE 7.652598474661051 Test RE 1.3222467971562253\n",
      "40 Train Loss 20.081192 Test MSE 7.64526591474822 Test RE 1.3216131707154504\n",
      "41 Train Loss 20.020786 Test MSE 7.698476758408766 Test RE 1.326204391807152\n",
      "42 Train Loss 19.99253 Test MSE 7.7007871953180524 Test RE 1.32640338453061\n",
      "43 Train Loss 19.962702 Test MSE 7.714936694377096 Test RE 1.3276213983373544\n",
      "44 Train Loss 19.919096 Test MSE 7.774134295143654 Test RE 1.3327051611006595\n",
      "45 Train Loss 19.836716 Test MSE 7.832747322309439 Test RE 1.3377196873827744\n",
      "46 Train Loss 19.80149 Test MSE 7.836732341386706 Test RE 1.338059935854531\n",
      "47 Train Loss 19.733139 Test MSE 7.801478480987832 Test RE 1.335046884825611\n",
      "48 Train Loss 19.652237 Test MSE 7.851976394579287 Test RE 1.3393607042280378\n",
      "49 Train Loss 19.54932 Test MSE 7.888383613009235 Test RE 1.3424622166270965\n",
      "50 Train Loss 19.306877 Test MSE 7.815750958912336 Test RE 1.3362675329618494\n",
      "51 Train Loss 19.175562 Test MSE 7.8779431606639205 Test RE 1.3415735330798055\n",
      "52 Train Loss 18.940456 Test MSE 7.997633126247575 Test RE 1.351726410450264\n",
      "53 Train Loss 18.838314 Test MSE 7.992794180683091 Test RE 1.3513174194333382\n",
      "54 Train Loss 18.751928 Test MSE 7.942976457225495 Test RE 1.347099571331913\n",
      "55 Train Loss 18.471214 Test MSE 8.103474651667335 Test RE 1.360641457102313\n",
      "56 Train Loss 18.307926 Test MSE 8.189067043564059 Test RE 1.3678084224253826\n",
      "57 Train Loss 18.227085 Test MSE 8.115764695996342 Test RE 1.3616728670163776\n",
      "58 Train Loss 18.153236 Test MSE 8.108367295369098 Test RE 1.3610521531061002\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "59 Train Loss 18.071045 Test MSE 8.054206975359904 Test RE 1.3564989227040263\n",
      "60 Train Loss 18.006058 Test MSE 8.063652772861953 Test RE 1.357294125733443\n",
      "61 Train Loss 17.931988 Test MSE 8.019507548701059 Test RE 1.3535737097431368\n",
      "62 Train Loss 17.76373 Test MSE 8.199159393804601 Test RE 1.3686510184750702\n",
      "63 Train Loss 17.62661 Test MSE 8.110086681238935 Test RE 1.3611964515640465\n",
      "64 Train Loss 17.527592 Test MSE 8.104475969071334 Test RE 1.3607255193067989\n",
      "65 Train Loss 17.447535 Test MSE 8.15144421703138 Test RE 1.364662761192632\n",
      "66 Train Loss 17.295033 Test MSE 8.20240639113165 Test RE 1.3689219954190515\n",
      "67 Train Loss 17.049099 Test MSE 8.093097477469314 Test RE 1.3597699706560404\n",
      "68 Train Loss 16.897264 Test MSE 8.078623619050049 Test RE 1.3585535065335463\n",
      "69 Train Loss 16.806786 Test MSE 8.190161104582508 Test RE 1.3678997891152034\n",
      "70 Train Loss 16.62933 Test MSE 8.095529631837236 Test RE 1.3599742757475466\n",
      "71 Train Loss 16.55249 Test MSE 7.929699279635379 Test RE 1.3459732202187176\n",
      "72 Train Loss 16.341093 Test MSE 7.901693383852921 Test RE 1.343594282127128\n",
      "73 Train Loss 16.110346 Test MSE 8.033143539255631 Test RE 1.35472399726921\n",
      "74 Train Loss 16.025673 Test MSE 8.052113452515565 Test RE 1.356322614468073\n",
      "75 Train Loss 15.92466 Test MSE 8.14262756952313 Test RE 1.3639245480996276\n",
      "76 Train Loss 15.853933 Test MSE 8.025492322309589 Test RE 1.3540786859753862\n",
      "77 Train Loss 15.770938 Test MSE 8.044370812081848 Test RE 1.3556703606176577\n",
      "78 Train Loss 15.608145 Test MSE 7.979906438602598 Test RE 1.3502275342261298\n",
      "79 Train Loss 15.54269 Test MSE 7.890116469147866 Test RE 1.3426096591365997\n",
      "80 Train Loss 15.466991 Test MSE 7.946201773499352 Test RE 1.3473730444574732\n",
      "81 Train Loss 15.370418 Test MSE 7.98214994120986 Test RE 1.3504173250547309\n",
      "82 Train Loss 15.310082 Test MSE 7.990020618752594 Test RE 1.3510829402471294\n",
      "83 Train Loss 15.21122 Test MSE 7.922034813686075 Test RE 1.3453225864724647\n",
      "84 Train Loss 15.157234 Test MSE 7.886875313999231 Test RE 1.342333867694631\n",
      "85 Train Loss 15.145248 Test MSE 7.857355193953282 Test RE 1.3398193734102868\n",
      "86 Train Loss 15.096943 Test MSE 7.814084746560622 Test RE 1.3361250882909423\n",
      "87 Train Loss 14.968859 Test MSE 7.7979993502202 Test RE 1.3347491642931102\n",
      "88 Train Loss 14.955903 Test MSE 7.796997225512015 Test RE 1.3346633969037522\n",
      "89 Train Loss 14.854953 Test MSE 7.8813656287633025 Test RE 1.3418649158656615\n",
      "90 Train Loss 14.759283 Test MSE 7.901198726756452 Test RE 1.3435522260249755\n",
      "91 Train Loss 14.737634 Test MSE 7.8831429895144165 Test RE 1.3420162122082357\n",
      "92 Train Loss 14.692911 Test MSE 7.881888637341084 Test RE 1.341909438303248\n",
      "93 Train Loss 14.629601 Test MSE 7.9637385306788815 Test RE 1.3488590079576706\n",
      "94 Train Loss 14.595751 Test MSE 7.929057160179891 Test RE 1.345918723001483\n",
      "95 Train Loss 14.543301 Test MSE 7.942117602186918 Test RE 1.3470267400364195\n",
      "96 Train Loss 14.519812 Test MSE 7.988402294445072 Test RE 1.3509461069901256\n",
      "97 Train Loss 14.4830475 Test MSE 7.976067862548024 Test RE 1.3499027450472114\n",
      "98 Train Loss 14.4701 Test MSE 7.963131481101 Test RE 1.348807597436537\n",
      "99 Train Loss 14.42862 Test MSE 7.986033309497322 Test RE 1.350745778552734\n",
      "Training time: 132.33\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 58.309334 Test MSE 8.39842150436571 Test RE 1.3851821741724368\n",
      "1 Train Loss 50.224056 Test MSE 8.502837397017686 Test RE 1.3937664213199001\n",
      "2 Train Loss 46.107265 Test MSE 8.539004943876112 Test RE 1.3967275282326284\n",
      "3 Train Loss 45.69084 Test MSE 8.43454868356527 Test RE 1.3881582705323519\n",
      "4 Train Loss 41.20739 Test MSE 8.119427755365217 Test RE 1.3619801286266149\n",
      "5 Train Loss 39.827713 Test MSE 7.776613443634874 Test RE 1.3329176420341668\n",
      "6 Train Loss 39.530205 Test MSE 7.792402432709679 Test RE 1.3342700784675798\n",
      "7 Train Loss 39.219357 Test MSE 7.950850065516982 Test RE 1.3477670734476\n",
      "8 Train Loss 38.69286 Test MSE 7.744036891301518 Test RE 1.3301228888217196\n",
      "9 Train Loss 38.23076 Test MSE 7.763038525831941 Test RE 1.3317537581131846\n",
      "10 Train Loss 37.665707 Test MSE 7.766741324281543 Test RE 1.332071328859114\n",
      "11 Train Loss 37.28128 Test MSE 7.646281648393214 Test RE 1.3217009611464026\n",
      "12 Train Loss 36.844433 Test MSE 7.73677743157514 Test RE 1.3294992969097108\n",
      "13 Train Loss 35.91699 Test MSE 7.5138973332464225 Test RE 1.3102093328261861\n",
      "14 Train Loss 34.175426 Test MSE 6.792972681043872 Test RE 1.2457703616642593\n",
      "15 Train Loss 32.808475 Test MSE 6.5426970169865175 Test RE 1.2226058370035364\n",
      "16 Train Loss 31.4672 Test MSE 5.760049531387429 Test RE 1.1471525352812273\n",
      "17 Train Loss 29.586649 Test MSE 5.232591435629301 Test RE 1.093368287551264\n",
      "18 Train Loss 25.216633 Test MSE 4.517457009132515 Test RE 1.0159096236779452\n",
      "19 Train Loss 21.198902 Test MSE 4.334956914668552 Test RE 0.9951772811189613\n",
      "20 Train Loss 17.756218 Test MSE 3.5126343658515795 Test RE 0.8958277950007301\n",
      "21 Train Loss 14.209101 Test MSE 3.000008744878359 Test RE 0.8278836783732673\n",
      "22 Train Loss 11.820555 Test MSE 2.443747847951244 Test RE 0.7471989560071163\n",
      "23 Train Loss 8.4555 Test MSE 2.2352363692035935 Test RE 0.7146111494992334\n",
      "24 Train Loss 7.436569 Test MSE 2.291607534472642 Test RE 0.7235660493638493\n",
      "25 Train Loss 6.5499525 Test MSE 1.941519936381713 Test RE 0.6660072970351191\n",
      "26 Train Loss 6.035443 Test MSE 1.654568322939615 Test RE 0.6148234349975101\n",
      "27 Train Loss 4.689528 Test MSE 0.5606839974234054 Test RE 0.35790448507673334\n",
      "28 Train Loss 2.594036 Test MSE 0.3841077390488674 Test RE 0.29623378595401156\n",
      "29 Train Loss 1.2645142 Test MSE 0.17241468060565648 Test RE 0.19847014238592967\n",
      "30 Train Loss 0.9789828 Test MSE 0.1572118521458829 Test RE 0.18951810377625097\n",
      "31 Train Loss 0.82398343 Test MSE 0.1328733528461146 Test RE 0.17423163304485187\n",
      "32 Train Loss 0.6668922 Test MSE 0.1497119209954319 Test RE 0.1849422988960145\n",
      "33 Train Loss 0.5769649 Test MSE 0.1241169562290414 Test RE 0.16839283887230316\n",
      "34 Train Loss 0.509585 Test MSE 0.1346474210691734 Test RE 0.17539090948733874\n",
      "35 Train Loss 0.42970607 Test MSE 0.14426877657637194 Test RE 0.18154915619551287\n",
      "36 Train Loss 0.40077925 Test MSE 0.13641620104259627 Test RE 0.17653915185605992\n",
      "37 Train Loss 0.33759022 Test MSE 0.10735692507174116 Test RE 0.15661129915477343\n",
      "38 Train Loss 0.3095721 Test MSE 0.10254694234169971 Test RE 0.15306271694292273\n",
      "39 Train Loss 0.29002053 Test MSE 0.09602913561999173 Test RE 0.14811859032550748\n",
      "40 Train Loss 0.26926976 Test MSE 0.08305628920064898 Test RE 0.13775086103130713\n",
      "41 Train Loss 0.23420727 Test MSE 0.06288126948904141 Test RE 0.11985846479779143\n",
      "42 Train Loss 0.22115648 Test MSE 0.05804833694776851 Test RE 0.11516034340432722\n",
      "43 Train Loss 0.19628052 Test MSE 0.044105409131995385 Test RE 0.1003815709013821\n",
      "44 Train Loss 0.18092817 Test MSE 0.04155289943574998 Test RE 0.09743359494852237\n",
      "45 Train Loss 0.16319008 Test MSE 0.04199905663467108 Test RE 0.09795527496918814\n",
      "46 Train Loss 0.1501256 Test MSE 0.03009375342419851 Test RE 0.08291750762651832\n",
      "47 Train Loss 0.14132476 Test MSE 0.03177185197309029 Test RE 0.08519798540026194\n",
      "48 Train Loss 0.1341864 Test MSE 0.03441514964975785 Test RE 0.08867126239308656\n",
      "49 Train Loss 0.12687474 Test MSE 0.031758042638053666 Test RE 0.08517946813857905\n",
      "50 Train Loss 0.11371687 Test MSE 0.02572614551058188 Test RE 0.07666469657380351\n",
      "51 Train Loss 0.10868955 Test MSE 0.022597618286952278 Test RE 0.07185208822327434\n",
      "52 Train Loss 0.102527946 Test MSE 0.02326652256349155 Test RE 0.07290776739190793\n",
      "53 Train Loss 0.093763866 Test MSE 0.019950387444096225 Test RE 0.0675124280445496\n",
      "54 Train Loss 0.090220064 Test MSE 0.020030011786180933 Test RE 0.06764701890555598\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 Train Loss 0.079468235 Test MSE 0.018812632283405063 Test RE 0.06555907897774321\n",
      "56 Train Loss 0.073313385 Test MSE 0.016198283182923854 Test RE 0.06083346497270699\n",
      "57 Train Loss 0.07017259 Test MSE 0.01610343239677086 Test RE 0.06065509503077093\n",
      "58 Train Loss 0.066218 Test MSE 0.01647082897318903 Test RE 0.061343109838257054\n",
      "59 Train Loss 0.05971183 Test MSE 0.014779839172760743 Test RE 0.058108934800906856\n",
      "60 Train Loss 0.056732673 Test MSE 0.015494552725305458 Test RE 0.059497344463911625\n",
      "61 Train Loss 0.052890297 Test MSE 0.014084490701943016 Test RE 0.05672553936543616\n",
      "62 Train Loss 0.05086296 Test MSE 0.014815458128562286 Test RE 0.05817891303376672\n",
      "63 Train Loss 0.04949251 Test MSE 0.013849782257789225 Test RE 0.0562509074659498\n",
      "64 Train Loss 0.04736457 Test MSE 0.013555464102639777 Test RE 0.0556500111778291\n",
      "65 Train Loss 0.044248495 Test MSE 0.013132623256172883 Test RE 0.05477517869651174\n",
      "66 Train Loss 0.040364586 Test MSE 0.012218619516993665 Test RE 0.05283468602209797\n",
      "67 Train Loss 0.03846657 Test MSE 0.01174328769968853 Test RE 0.051796797707516724\n",
      "68 Train Loss 0.037008416 Test MSE 0.010451729670262878 Test RE 0.04886547722115136\n",
      "69 Train Loss 0.035396937 Test MSE 0.010359299452867683 Test RE 0.04864892565483375\n",
      "70 Train Loss 0.033742692 Test MSE 0.009855104178858152 Test RE 0.04745026804388464\n",
      "71 Train Loss 0.031922378 Test MSE 0.009983651729075713 Test RE 0.047758730227948604\n",
      "72 Train Loss 0.030012483 Test MSE 0.009473512740462068 Test RE 0.046522557576046085\n",
      "73 Train Loss 0.028629605 Test MSE 0.008508085541196512 Test RE 0.044088364050387255\n",
      "74 Train Loss 0.027511539 Test MSE 0.008301349465045956 Test RE 0.043549423346383814\n",
      "75 Train Loss 0.025750736 Test MSE 0.007609123136133107 Test RE 0.041694173222585514\n",
      "76 Train Loss 0.024497045 Test MSE 0.0073538281287751535 Test RE 0.04098876168552727\n",
      "77 Train Loss 0.024029337 Test MSE 0.007021094554362656 Test RE 0.04005073321395601\n",
      "78 Train Loss 0.023084477 Test MSE 0.007092185911909079 Test RE 0.04025298728002945\n",
      "79 Train Loss 0.02262517 Test MSE 0.007488735237252571 Test RE 0.04136302557407517\n",
      "80 Train Loss 0.021851934 Test MSE 0.006889771265187956 Test RE 0.03967440864611018\n",
      "81 Train Loss 0.020790052 Test MSE 0.00734967830162525 Test RE 0.04097719490176731\n",
      "82 Train Loss 0.020022752 Test MSE 0.007177781342493951 Test RE 0.04049516495665528\n",
      "83 Train Loss 0.01933667 Test MSE 0.007162041533947539 Test RE 0.040450740647807244\n",
      "84 Train Loss 0.01887592 Test MSE 0.006842359958063177 Test RE 0.039537665154157596\n",
      "85 Train Loss 0.018407376 Test MSE 0.00623711840344892 Test RE 0.0377485309180015\n",
      "86 Train Loss 0.017799214 Test MSE 0.005715084572828679 Test RE 0.0361342787955418\n",
      "87 Train Loss 0.016910564 Test MSE 0.005480810293877382 Test RE 0.03538591625573384\n",
      "88 Train Loss 0.01647686 Test MSE 0.0056470634850467865 Test RE 0.03591859954843066\n",
      "89 Train Loss 0.015964624 Test MSE 0.0056196100380576965 Test RE 0.03583118325033717\n",
      "90 Train Loss 0.01564837 Test MSE 0.005690165329152272 Test RE 0.036055415344869175\n",
      "91 Train Loss 0.015118312 Test MSE 0.005536899602551968 Test RE 0.03556652089004065\n",
      "92 Train Loss 0.014519125 Test MSE 0.005385396871443484 Test RE 0.03507655371068918\n",
      "93 Train Loss 0.013527767 Test MSE 0.004702480228030496 Test RE 0.03277717722761803\n",
      "94 Train Loss 0.01272193 Test MSE 0.004361984856141989 Test RE 0.03156822308802145\n",
      "95 Train Loss 0.012077627 Test MSE 0.004327848754603567 Test RE 0.03144445687820618\n",
      "96 Train Loss 0.011837005 Test MSE 0.0041815014444309935 Test RE 0.03090823359496497\n",
      "97 Train Loss 0.011650844 Test MSE 0.004461600648780362 Test RE 0.03192665414070532\n",
      "98 Train Loss 0.011423581 Test MSE 0.0042928404604458785 Test RE 0.03131702035540332\n",
      "99 Train Loss 0.011165983 Test MSE 0.0041861418073066 Test RE 0.030925378830068205\n",
      "Training time: 128.05\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 54.555984 Test MSE 8.325892260612703 Test RE 1.3791879486394982\n",
      "1 Train Loss 46.489616 Test MSE 7.987724037626606 Test RE 1.3508887546044477\n",
      "2 Train Loss 42.669693 Test MSE 8.0318519603461 Test RE 1.3546150857782553\n",
      "3 Train Loss 41.299744 Test MSE 7.873382752726163 Test RE 1.3411851697633705\n",
      "4 Train Loss 40.071526 Test MSE 8.138382147725832 Test RE 1.3635689386902987\n",
      "5 Train Loss 39.384613 Test MSE 7.979439894839677 Test RE 1.3501880632469232\n",
      "6 Train Loss 38.799202 Test MSE 7.7906519814111235 Test RE 1.3341202077448011\n",
      "7 Train Loss 37.738464 Test MSE 7.386037421054075 Test RE 1.2990139414995658\n",
      "8 Train Loss 34.613983 Test MSE 5.6697643227415035 Test RE 1.1381265737743937\n",
      "9 Train Loss 31.776604 Test MSE 4.9768090553679745 Test RE 1.0663101656095497\n",
      "10 Train Loss 29.988407 Test MSE 4.578597550903186 Test RE 1.0227613225791503\n",
      "11 Train Loss 26.467636 Test MSE 3.3574411378661524 Test RE 0.8758147679897387\n",
      "12 Train Loss 20.5846 Test MSE 3.1148808167508166 Test RE 0.843584861109343\n",
      "13 Train Loss 18.370552 Test MSE 2.591148100018893 Test RE 0.7694035369444225\n",
      "14 Train Loss 17.437973 Test MSE 2.497286172587691 Test RE 0.7553395349288967\n",
      "15 Train Loss 16.344734 Test MSE 2.327548612289954 Test RE 0.7292181010439869\n",
      "16 Train Loss 13.828905 Test MSE 2.2083103424281516 Test RE 0.7102939466584595\n",
      "17 Train Loss 13.141956 Test MSE 2.167337650734688 Test RE 0.703673746356461\n",
      "18 Train Loss 12.186705 Test MSE 2.082156514366382 Test RE 0.6897071787098321\n",
      "19 Train Loss 11.868768 Test MSE 2.035900117448886 Test RE 0.6820030140602799\n",
      "20 Train Loss 11.404085 Test MSE 1.9222783302218254 Test RE 0.6626988170756984\n",
      "21 Train Loss 10.664324 Test MSE 2.0465897307390386 Test RE 0.6837911184619886\n",
      "22 Train Loss 10.308931 Test MSE 1.9569939368626321 Test RE 0.6686560837023935\n",
      "23 Train Loss 9.989946 Test MSE 1.882392340568118 Test RE 0.6557874992354783\n",
      "24 Train Loss 9.704262 Test MSE 1.8536106317841172 Test RE 0.6507547040985807\n",
      "25 Train Loss 9.431848 Test MSE 1.9360370264922895 Test RE 0.6650662199770782\n",
      "26 Train Loss 9.130323 Test MSE 1.871643937093087 Test RE 0.6539125555177379\n",
      "27 Train Loss 9.044565 Test MSE 1.8959514280596492 Test RE 0.658145117514655\n",
      "28 Train Loss 8.75842 Test MSE 1.948470962138524 Test RE 0.6671984508362307\n",
      "29 Train Loss 8.636029 Test MSE 1.9581697169541445 Test RE 0.6688569209262609\n",
      "30 Train Loss 8.603941 Test MSE 1.9111042976441097 Test RE 0.6607699052674451\n",
      "31 Train Loss 8.564385 Test MSE 1.8998316088925655 Test RE 0.6588182404598386\n",
      "32 Train Loss 8.496151 Test MSE 1.9136654886184745 Test RE 0.6612125266592791\n",
      "33 Train Loss 8.4691 Test MSE 1.9123818024721229 Test RE 0.6609907188890655\n",
      "34 Train Loss 8.443462 Test MSE 1.9024616708195394 Test RE 0.6592741054674662\n",
      "35 Train Loss 8.348616 Test MSE 1.8897439503441447 Test RE 0.6570668275953561\n",
      "36 Train Loss 8.25144 Test MSE 1.8411385204211015 Test RE 0.6485616912207015\n",
      "37 Train Loss 8.228382 Test MSE 1.8396432288311524 Test RE 0.6482982711095504\n",
      "38 Train Loss 8.119242 Test MSE 1.8557299070436941 Test RE 0.6511266091655838\n",
      "39 Train Loss 8.063351 Test MSE 1.856667324811557 Test RE 0.6512910459693945\n",
      "40 Train Loss 8.037823 Test MSE 1.8945593508299619 Test RE 0.6579034559786557\n",
      "41 Train Loss 7.98162 Test MSE 1.9520804109707712 Test RE 0.6678161414435028\n",
      "42 Train Loss 7.9317646 Test MSE 1.9963530648705876 Test RE 0.6753466278971747\n",
      "43 Train Loss 7.912139 Test MSE 1.9670593389971618 Test RE 0.6703734269335532\n",
      "44 Train Loss 7.885133 Test MSE 1.953674965793711 Test RE 0.668088838221922\n",
      "45 Train Loss 7.8360324 Test MSE 2.010222788733203 Test RE 0.6776885629286241\n",
      "46 Train Loss 7.8119607 Test MSE 2.0592473157803184 Test RE 0.6859023874953039\n",
      "47 Train Loss 7.7657123 Test MSE 2.013302550949865 Test RE 0.6782074906965375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48 Train Loss 7.7276487 Test MSE 2.0047292136390116 Test RE 0.6767619293139832\n",
      "49 Train Loss 7.679315 Test MSE 1.9669971993772517 Test RE 0.6703628382649472\n",
      "50 Train Loss 7.635895 Test MSE 1.9445025722145268 Test RE 0.6665186734211519\n",
      "51 Train Loss 7.5448275 Test MSE 2.078326839195215 Test RE 0.6890726034251935\n",
      "52 Train Loss 7.4925117 Test MSE 2.0700403485287326 Test RE 0.68769753177177\n",
      "53 Train Loss 7.43316 Test MSE 2.0618612376535426 Test RE 0.6863375772474551\n",
      "54 Train Loss 7.3110743 Test MSE 2.1145663844314226 Test RE 0.6950542803909581\n",
      "55 Train Loss 7.1725626 Test MSE 2.1539654260713714 Test RE 0.7014995944566972\n",
      "56 Train Loss 7.111928 Test MSE 2.1416534291278264 Test RE 0.6994918467550627\n",
      "57 Train Loss 7.027411 Test MSE 2.1174551242956645 Test RE 0.6955288802329329\n",
      "58 Train Loss 6.9680195 Test MSE 2.104819039071021 Test RE 0.6934504623036886\n",
      "59 Train Loss 6.8253613 Test MSE 2.1625049466880806 Test RE 0.7028887868100208\n",
      "60 Train Loss 6.7062674 Test MSE 2.2182903823445534 Test RE 0.711897156762556\n",
      "61 Train Loss 6.617687 Test MSE 2.1826853764289496 Test RE 0.7061608391483195\n",
      "62 Train Loss 6.5827036 Test MSE 2.1520955117852405 Test RE 0.701195033178443\n",
      "63 Train Loss 6.524014 Test MSE 2.134116781518573 Test RE 0.6982599785049554\n",
      "64 Train Loss 6.4688425 Test MSE 2.16368731833761 Test RE 0.7030809163602499\n",
      "65 Train Loss 6.419565 Test MSE 2.151981159979259 Test RE 0.7011764038975608\n",
      "66 Train Loss 6.3451405 Test MSE 2.188055867401931 Test RE 0.7070290587512469\n",
      "67 Train Loss 6.31826 Test MSE 2.201503103160401 Test RE 0.7091983414226305\n",
      "68 Train Loss 6.247262 Test MSE 2.23372994039701 Test RE 0.7143703042189191\n",
      "69 Train Loss 6.214465 Test MSE 2.196642499003366 Test RE 0.7084150045291302\n",
      "70 Train Loss 6.155007 Test MSE 2.19401389868432 Test RE 0.7079910171663177\n",
      "71 Train Loss 6.1284122 Test MSE 2.195847944666107 Test RE 0.7082868714524839\n",
      "72 Train Loss 6.0975947 Test MSE 2.1967968503572415 Test RE 0.7084398931703949\n",
      "73 Train Loss 6.0602527 Test MSE 2.1941637956442617 Test RE 0.7080152020378483\n",
      "74 Train Loss 6.004484 Test MSE 2.1668597560819585 Test RE 0.7035961625845016\n",
      "75 Train Loss 5.925948 Test MSE 2.1910066298224433 Test RE 0.707505639789701\n",
      "76 Train Loss 5.8235226 Test MSE 2.1493134185910843 Test RE 0.7007416562741074\n",
      "77 Train Loss 5.7986283 Test MSE 2.1753260344280556 Test RE 0.7049693556784354\n",
      "78 Train Loss 5.7602077 Test MSE 2.118662696656366 Test RE 0.6957271800052887\n",
      "79 Train Loss 5.60406 Test MSE 2.097441506301534 Test RE 0.692234100177315\n",
      "80 Train Loss 5.5048094 Test MSE 2.1077295232343487 Test RE 0.6939297385096647\n",
      "81 Train Loss 5.423309 Test MSE 2.1180319606937825 Test RE 0.6956236116510996\n",
      "82 Train Loss 5.3092637 Test MSE 2.144427560054302 Test RE 0.6999447337530001\n",
      "83 Train Loss 5.255782 Test MSE 2.1381845708696248 Test RE 0.698925130085506\n",
      "84 Train Loss 5.20147 Test MSE 2.0913065503020514 Test RE 0.6912209763919981\n",
      "85 Train Loss 5.182833 Test MSE 2.09232584321185 Test RE 0.6913894047870992\n",
      "86 Train Loss 5.1456666 Test MSE 2.088752753094779 Test RE 0.6907988055168256\n",
      "87 Train Loss 5.1024213 Test MSE 2.1194320800156494 Test RE 0.6958534937239177\n",
      "88 Train Loss 5.0577126 Test MSE 2.1036338773777827 Test RE 0.6932552040341116\n",
      "89 Train Loss 5.0197577 Test MSE 2.1124706798716044 Test RE 0.6947097677941293\n",
      "90 Train Loss 5.0014186 Test MSE 2.1105137334119832 Test RE 0.6943879112982164\n",
      "91 Train Loss 4.957332 Test MSE 2.0896815529626633 Test RE 0.6909523762369701\n",
      "92 Train Loss 4.9291406 Test MSE 2.129046147679044 Test RE 0.6974299568477758\n",
      "93 Train Loss 4.921762 Test MSE 2.1319560751517033 Test RE 0.6979064090866987\n",
      "94 Train Loss 4.9150944 Test MSE 2.143565541417442 Test RE 0.6998040374515678\n",
      "95 Train Loss 4.901473 Test MSE 2.142480055773421 Test RE 0.6996268272349089\n",
      "96 Train Loss 4.8792944 Test MSE 2.1337592215844103 Test RE 0.6982014811788735\n",
      "97 Train Loss 4.866577 Test MSE 2.1450433187108833 Test RE 0.7000452188665793\n",
      "98 Train Loss 4.8592095 Test MSE 2.1570300732797287 Test RE 0.7019984614751291\n",
      "99 Train Loss 4.853745 Test MSE 2.147694648411049 Test RE 0.7004777223704535\n",
      "Training time: 133.73\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 54.964787 Test MSE 8.949030610724266 Test RE 1.429868362803372\n",
      "1 Train Loss 42.201355 Test MSE 6.973901512189637 Test RE 1.262251702669899\n",
      "2 Train Loss 36.396683 Test MSE 7.622956871826332 Test RE 1.3196835148085166\n",
      "3 Train Loss 34.20643 Test MSE 6.945386587370763 Test RE 1.2596685087535573\n",
      "4 Train Loss 31.476215 Test MSE 6.402377194412952 Test RE 1.2094242956661732\n",
      "5 Train Loss 27.779352 Test MSE 6.16892714343812 Test RE 1.1871699111054022\n",
      "6 Train Loss 25.214474 Test MSE 5.290690219417386 Test RE 1.0994215034093038\n",
      "7 Train Loss 24.201385 Test MSE 5.257463426353149 Test RE 1.0959637517036318\n",
      "8 Train Loss 22.156786 Test MSE 4.163938267577324 Test RE 0.97534935567803\n",
      "9 Train Loss 19.798588 Test MSE 4.1555003023364145 Test RE 0.9743606117244888\n",
      "10 Train Loss 18.30566 Test MSE 3.2365269071488325 Test RE 0.8598994501319439\n",
      "11 Train Loss 15.848331 Test MSE 2.552144429025501 Test RE 0.7635907948269809\n",
      "12 Train Loss 13.616458 Test MSE 2.0161876045938794 Test RE 0.6786932509018866\n",
      "13 Train Loss 12.380129 Test MSE 2.1773493921571143 Test RE 0.7052971395685361\n",
      "14 Train Loss 10.26977 Test MSE 2.5424540294477374 Test RE 0.7621397528902384\n",
      "15 Train Loss 7.8976083 Test MSE 2.142287814629451 Test RE 0.6995954383595626\n",
      "16 Train Loss 5.6756897 Test MSE 2.261471363579177 Test RE 0.718792615358047\n",
      "17 Train Loss 4.7898946 Test MSE 2.2050324551078178 Test RE 0.7097665913988204\n",
      "18 Train Loss 4.212705 Test MSE 2.137825758750658 Test RE 0.6988664837618237\n",
      "19 Train Loss 3.753935 Test MSE 2.107815535699338 Test RE 0.6939438973468799\n",
      "20 Train Loss 3.5093133 Test MSE 2.0922319292979146 Test RE 0.6913738881279966\n",
      "21 Train Loss 3.2935197 Test MSE 2.0770416635866598 Test RE 0.6888595194597473\n",
      "22 Train Loss 3.1342726 Test MSE 2.0656282469991365 Test RE 0.6869642586656284\n",
      "23 Train Loss 3.0671322 Test MSE 2.051444250918724 Test RE 0.684601615947259\n",
      "24 Train Loss 2.9371047 Test MSE 2.1340641057293483 Test RE 0.6982513609769128\n",
      "25 Train Loss 2.8511977 Test MSE 2.148648482698633 Test RE 0.7006332532096928\n",
      "26 Train Loss 2.819887 Test MSE 2.1363375810017278 Test RE 0.6986231948486746\n",
      "27 Train Loss 2.7566588 Test MSE 2.1513735853617844 Test RE 0.7010774144019329\n",
      "28 Train Loss 2.7320304 Test MSE 2.1367926961680004 Test RE 0.6986976065620356\n",
      "29 Train Loss 2.7182705 Test MSE 2.149972488719876 Test RE 0.7008490865098144\n",
      "30 Train Loss 2.6910453 Test MSE 2.1496666202550267 Test RE 0.7007992311604625\n",
      "31 Train Loss 2.6830852 Test MSE 2.150875733535818 Test RE 0.7009962911446499\n",
      "32 Train Loss 2.658591 Test MSE 2.163915212132368 Test RE 0.7031179419410382\n",
      "33 Train Loss 2.6500988 Test MSE 2.1658754736684727 Test RE 0.7034363423676805\n",
      "34 Train Loss 2.6473982 Test MSE 2.1659693132885014 Test RE 0.7034515808901404\n",
      "35 Train Loss 2.6411192 Test MSE 2.1665752329783623 Test RE 0.7035499676391954\n",
      "36 Train Loss 2.6374817 Test MSE 2.165718434663277 Test RE 0.7034108402215005\n",
      "37 Train Loss 2.6322885 Test MSE 2.1630009198707025 Test RE 0.702969386394934\n",
      "38 Train Loss 2.6190069 Test MSE 2.1605286073828833 Test RE 0.7025675241296088\n",
      "39 Train Loss 2.6161013 Test MSE 2.1679742147149526 Test RE 0.7037770760049552\n",
      "40 Train Loss 2.6119602 Test MSE 2.1604883743757246 Test RE 0.7025609825509128\n",
      "41 Train Loss 2.600552 Test MSE 2.1619147912225403 Test RE 0.7027928698122955\n",
      "42 Train Loss 2.5982187 Test MSE 2.1682136406347903 Test RE 0.7038159366658991\n",
      "43 Train Loss 2.5915685 Test MSE 2.1692686892575717 Test RE 0.703987153586283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 Train Loss 2.589126 Test MSE 2.1653918299944546 Test RE 0.7033577987173578\n",
      "45 Train Loss 2.5859065 Test MSE 2.161972183782846 Test RE 0.7028021983054059\n",
      "46 Train Loss 2.5820317 Test MSE 2.1543475072131604 Test RE 0.7015618094481055\n",
      "47 Train Loss 2.5781095 Test MSE 2.159684488037091 Test RE 0.7024302640224221\n",
      "48 Train Loss 2.5769565 Test MSE 2.1616488856992953 Test RE 0.7027496483465079\n",
      "49 Train Loss 2.5729578 Test MSE 2.1548401256866345 Test RE 0.7016420152997963\n",
      "50 Train Loss 2.5683174 Test MSE 2.1602012359035423 Test RE 0.702514294266735\n",
      "51 Train Loss 2.5657701 Test MSE 2.1664512295803027 Test RE 0.7035298335968306\n",
      "52 Train Loss 2.5633502 Test MSE 2.1666382287865082 Test RE 0.7035601958500471\n",
      "53 Train Loss 2.5609574 Test MSE 2.167790769570591 Test RE 0.7037473000007896\n",
      "54 Train Loss 2.5593624 Test MSE 2.16143243466431 Test RE 0.702714463470341\n",
      "55 Train Loss 2.5551114 Test MSE 2.163257950960398 Test RE 0.7030111523549786\n",
      "56 Train Loss 2.551022 Test MSE 2.163080755474465 Test RE 0.7029823594517234\n",
      "57 Train Loss 2.5499606 Test MSE 2.162176895683942 Test RE 0.7028354708353738\n",
      "58 Train Loss 2.5472736 Test MSE 2.1597382162624412 Test RE 0.702439001432216\n",
      "59 Train Loss 2.5387127 Test MSE 2.1563073073346004 Test RE 0.7018808407016687\n",
      "60 Train Loss 2.5370438 Test MSE 2.161803599332402 Test RE 0.7027747965115567\n",
      "61 Train Loss 2.535549 Test MSE 2.161098123767152 Test RE 0.7026601165974785\n",
      "62 Train Loss 2.534001 Test MSE 2.1595356255702134 Test RE 0.7024060550918069\n",
      "63 Train Loss 2.5316658 Test MSE 2.158554596806851 Test RE 0.7022464932833103\n",
      "64 Train Loss 2.528177 Test MSE 2.152406510398266 Test RE 0.7012456960797463\n",
      "65 Train Loss 2.5234268 Test MSE 2.1577635303849627 Test RE 0.7021178019525681\n",
      "66 Train Loss 2.51982 Test MSE 2.1556463391900627 Test RE 0.7017732594603499\n",
      "67 Train Loss 2.5173876 Test MSE 2.1579354160213704 Test RE 0.7021457664538265\n",
      "68 Train Loss 2.5135603 Test MSE 2.157288305215537 Test RE 0.7020404805917633\n",
      "69 Train Loss 2.5113363 Test MSE 2.151233656264432 Test RE 0.7010546143811326\n",
      "70 Train Loss 2.5063083 Test MSE 2.148441144218244 Test RE 0.7005994478339277\n",
      "71 Train Loss 2.4968657 Test MSE 2.1434408066348962 Test RE 0.6997836762420213\n",
      "72 Train Loss 2.4916303 Test MSE 2.1356890088659473 Test RE 0.698517139060338\n",
      "73 Train Loss 2.48334 Test MSE 2.1402430258168694 Test RE 0.6992614807947157\n",
      "74 Train Loss 2.473323 Test MSE 2.1338044037692727 Test RE 0.6982088733206576\n",
      "75 Train Loss 2.4644496 Test MSE 2.12606894121568 Test RE 0.6969421516914848\n",
      "76 Train Loss 2.4443123 Test MSE 2.1232269295627724 Test RE 0.6964761790107729\n",
      "77 Train Loss 2.4060156 Test MSE 2.102335082809621 Test RE 0.6930411612951947\n",
      "78 Train Loss 2.3136065 Test MSE 2.0338945197625833 Test RE 0.681667005263094\n",
      "79 Train Loss 2.0498185 Test MSE 1.8344938605927108 Test RE 0.6473903054135465\n",
      "80 Train Loss 1.8303251 Test MSE 1.4629519487465015 Test RE 0.5781267852332528\n",
      "81 Train Loss 1.5133802 Test MSE 0.8689224806989829 Test RE 0.4455521841591028\n",
      "82 Train Loss 0.6459177 Test MSE 0.05785202214227874 Test RE 0.11496544697546779\n",
      "83 Train Loss 0.27936178 Test MSE 0.023080929553652898 Test RE 0.07261639888015757\n",
      "84 Train Loss 0.19090892 Test MSE 0.021647308324513746 Test RE 0.07032504386415744\n",
      "85 Train Loss 0.14452443 Test MSE 0.01709756738623775 Test RE 0.06249930994455299\n",
      "86 Train Loss 0.11801056 Test MSE 0.011915102337060958 Test RE 0.05217433815984151\n",
      "87 Train Loss 0.094317034 Test MSE 0.01233242341119071 Test RE 0.05308016617170325\n",
      "88 Train Loss 0.08254285 Test MSE 0.010233619427440577 Test RE 0.048352918376311255\n",
      "89 Train Loss 0.073081926 Test MSE 0.008493591037037292 Test RE 0.04405079323764548\n",
      "90 Train Loss 0.0617688 Test MSE 0.006576466683845709 Test RE 0.03876183891963133\n",
      "91 Train Loss 0.054613847 Test MSE 0.0067928205859941505 Test RE 0.03939427681643785\n",
      "92 Train Loss 0.051325247 Test MSE 0.006967925556082441 Test RE 0.039898797917505904\n",
      "93 Train Loss 0.04713996 Test MSE 0.008699744231270316 Test RE 0.04458218016961474\n",
      "94 Train Loss 0.042958878 Test MSE 0.00826438460985896 Test RE 0.043452355141270306\n",
      "95 Train Loss 0.038791243 Test MSE 0.008887010728453144 Test RE 0.04505945271062898\n",
      "96 Train Loss 0.03557798 Test MSE 0.00912370367108988 Test RE 0.04565555704261139\n",
      "97 Train Loss 0.0305585 Test MSE 0.007408251024649838 Test RE 0.04114015325195503\n",
      "98 Train Loss 0.028822506 Test MSE 0.00648763888329494 Test RE 0.03849917244791642\n",
      "99 Train Loss 0.027087007 Test MSE 0.0054394448038626595 Test RE 0.03525212872970865\n",
      "Training time: 131.35\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 52.33442 Test MSE 7.297710573894817 Test RE 1.291223370425315\n",
      "1 Train Loss 43.60495 Test MSE 8.501815653881025 Test RE 1.3936826778592155\n",
      "2 Train Loss 43.344067 Test MSE 8.481502565166648 Test RE 1.3920167437057855\n",
      "3 Train Loss 43.285362 Test MSE 8.485414450302839 Test RE 1.3923377234980765\n",
      "4 Train Loss 42.709045 Test MSE 8.444305325157849 Test RE 1.3889609126661038\n",
      "5 Train Loss 41.238625 Test MSE 8.369976233195997 Test RE 1.382834393551182\n",
      "6 Train Loss 39.358986 Test MSE 7.920192670909821 Test RE 1.345166160728932\n",
      "7 Train Loss 38.66973 Test MSE 7.87662801990196 Test RE 1.3414615475244391\n",
      "8 Train Loss 35.4578 Test MSE 7.471078907556291 Test RE 1.3064708432691194\n",
      "9 Train Loss 33.600975 Test MSE 7.23809738502021 Test RE 1.2859387141974779\n",
      "10 Train Loss 33.068703 Test MSE 7.265478310925407 Test RE 1.2883687005975413\n",
      "11 Train Loss 32.712173 Test MSE 7.10874516111071 Test RE 1.274396391572457\n",
      "12 Train Loss 32.35968 Test MSE 7.040320056799252 Test RE 1.2682482207434642\n",
      "13 Train Loss 31.773666 Test MSE 7.070107858343984 Test RE 1.2709283865608818\n",
      "14 Train Loss 31.064625 Test MSE 6.95352918712054 Test RE 1.2604066945881134\n",
      "15 Train Loss 30.034134 Test MSE 6.778159091249829 Test RE 1.2444112804554848\n",
      "16 Train Loss 29.052141 Test MSE 6.57283978846258 Test RE 1.225418926148776\n",
      "17 Train Loss 26.996403 Test MSE 6.646261403953921 Test RE 1.232244162727441\n",
      "18 Train Loss 22.187305 Test MSE 5.887283865291022 Test RE 1.1597531173674247\n",
      "19 Train Loss 17.995316 Test MSE 4.855103914703393 Test RE 1.0531914507957454\n",
      "20 Train Loss 13.796494 Test MSE 4.107948993470139 Test RE 0.9687697770202874\n",
      "21 Train Loss 12.591455 Test MSE 3.898906191485466 Test RE 0.943798870644155\n",
      "22 Train Loss 11.849338 Test MSE 3.8473984217200523 Test RE 0.9375439626919744\n",
      "23 Train Loss 11.273024 Test MSE 3.799969831784301 Test RE 0.9317472822170421\n",
      "24 Train Loss 10.596775 Test MSE 3.927156628441639 Test RE 0.9472119569832995\n",
      "25 Train Loss 10.255333 Test MSE 3.9749189625761363 Test RE 0.9529545759150295\n",
      "26 Train Loss 9.794668 Test MSE 3.899934551639799 Test RE 0.9439233287761206\n",
      "27 Train Loss 9.286849 Test MSE 3.916887482461544 Test RE 0.9459727111440601\n",
      "28 Train Loss 8.876823 Test MSE 3.9128818244009143 Test RE 0.9454888815078483\n",
      "29 Train Loss 8.344218 Test MSE 3.914074341280626 Test RE 0.9456329473916925\n",
      "30 Train Loss 7.658718 Test MSE 3.8524467392097894 Test RE 0.9381588545604198\n",
      "31 Train Loss 7.150436 Test MSE 3.7965734815082137 Test RE 0.931330798968632\n",
      "32 Train Loss 6.2693057 Test MSE 3.890446750240758 Test RE 0.9427744363381555\n",
      "33 Train Loss 5.6339717 Test MSE 3.8461610377552438 Test RE 0.9373931861112398\n",
      "34 Train Loss 5.2726135 Test MSE 3.7637029759321603 Test RE 0.9272903313156318\n",
      "35 Train Loss 4.7658243 Test MSE 3.5890406565413624 Test RE 0.9055183369446596\n",
      "36 Train Loss 4.309805 Test MSE 3.3206919562999344 Test RE 0.871008424080783\n",
      "37 Train Loss 3.6503096 Test MSE 2.6744484172928766 Test RE 0.7816731118534491\n",
      "38 Train Loss 2.8400831 Test MSE 2.5101947699500657 Test RE 0.7572892126335619\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "39 Train Loss 2.3764653 Test MSE 2.1103893442398425 Test RE 0.6943674481264351\n",
      "40 Train Loss 2.0182567 Test MSE 1.8891766323017565 Test RE 0.6569681915215658\n",
      "41 Train Loss 1.5848811 Test MSE 1.724910569402612 Test RE 0.6277566932743719\n",
      "42 Train Loss 1.1408852 Test MSE 1.5535000915206776 Test RE 0.5957495210476513\n",
      "43 Train Loss 0.7606033 Test MSE 1.0326286452549875 Test RE 0.485713466358395\n",
      "44 Train Loss 0.48958686 Test MSE 0.5150331986973863 Test RE 0.34302492055129163\n",
      "45 Train Loss 0.361996 Test MSE 0.24112884608966592 Test RE 0.23471056787580696\n",
      "46 Train Loss 0.30300757 Test MSE 0.20538652206475372 Test RE 0.21661774508961287\n",
      "47 Train Loss 0.23434614 Test MSE 0.1864868785529115 Test RE 0.20641069689769143\n",
      "48 Train Loss 0.20679659 Test MSE 0.1549544677418826 Test RE 0.18815255128386138\n",
      "49 Train Loss 0.17184299 Test MSE 0.127322070158258 Test RE 0.17055321322267925\n",
      "50 Train Loss 0.14675908 Test MSE 0.11134059548807067 Test RE 0.1594905038903923\n",
      "51 Train Loss 0.12724647 Test MSE 0.074625951470969 Test RE 0.13057288531130756\n",
      "52 Train Loss 0.118216574 Test MSE 0.06991767619129355 Test RE 0.12638675197930202\n",
      "53 Train Loss 0.109555155 Test MSE 0.06146147817853948 Test RE 0.11849760174089081\n",
      "54 Train Loss 0.099056035 Test MSE 0.05815858956681356 Test RE 0.11526965494583269\n",
      "55 Train Loss 0.09248887 Test MSE 0.04851746274236214 Test RE 0.10528272104111006\n",
      "56 Train Loss 0.08405907 Test MSE 0.044569107263951926 Test RE 0.10090786739780405\n",
      "57 Train Loss 0.07334227 Test MSE 0.04726962413136524 Test RE 0.1039199992121365\n",
      "58 Train Loss 0.06793222 Test MSE 0.04166295134216569 Test RE 0.09756253495828272\n",
      "59 Train Loss 0.06201049 Test MSE 0.04224742850520471 Test RE 0.09824448945864499\n",
      "60 Train Loss 0.05775435 Test MSE 0.0421461168254373 Test RE 0.09812662088151583\n",
      "61 Train Loss 0.05534554 Test MSE 0.04098282603649749 Test RE 0.09676293025757984\n",
      "62 Train Loss 0.049408004 Test MSE 0.038796841674846164 Test RE 0.09414694814384658\n",
      "63 Train Loss 0.04527501 Test MSE 0.036382658473344646 Test RE 0.0911706970280366\n",
      "64 Train Loss 0.04045982 Test MSE 0.03163868107344298 Test RE 0.08501924532619977\n",
      "65 Train Loss 0.037421092 Test MSE 0.028903417007225592 Test RE 0.08126109209576082\n",
      "66 Train Loss 0.031475443 Test MSE 0.029875255718329823 Test RE 0.0826159452008456\n",
      "67 Train Loss 0.029906224 Test MSE 0.030380002577708343 Test RE 0.0833109263439165\n",
      "68 Train Loss 0.028064156 Test MSE 0.032085481618372484 Test RE 0.08561746040601288\n",
      "69 Train Loss 0.02485327 Test MSE 0.029340719615965155 Test RE 0.08187351593105835\n",
      "70 Train Loss 0.022694578 Test MSE 0.027219483417430794 Test RE 0.07885840698126986\n",
      "71 Train Loss 0.020807933 Test MSE 0.025582374661947866 Test RE 0.07645017568857373\n",
      "72 Train Loss 0.019762628 Test MSE 0.025725880057645416 Test RE 0.07666430104386847\n",
      "73 Train Loss 0.018858453 Test MSE 0.025280267431951216 Test RE 0.0759974275864108\n",
      "74 Train Loss 0.018054612 Test MSE 0.022017518246095946 Test RE 0.07092384022013648\n",
      "75 Train Loss 0.017535059 Test MSE 0.02122534727500999 Test RE 0.06963626392049657\n",
      "76 Train Loss 0.016308649 Test MSE 0.018614486506578518 Test RE 0.06521291154545825\n",
      "77 Train Loss 0.015995659 Test MSE 0.018985874398503005 Test RE 0.06586024804160565\n",
      "78 Train Loss 0.014502417 Test MSE 0.01507561553758685 Test RE 0.05868749694348767\n",
      "79 Train Loss 0.014095536 Test MSE 0.012405348535699549 Test RE 0.05323687390281734\n",
      "80 Train Loss 0.013758291 Test MSE 0.011779921430270424 Test RE 0.0518775260538657\n",
      "81 Train Loss 0.013304533 Test MSE 0.010941635538678407 Test RE 0.04999760285923901\n",
      "82 Train Loss 0.012917781 Test MSE 0.01089018450013247 Test RE 0.04987991205298068\n",
      "83 Train Loss 0.012520947 Test MSE 0.010839118446408851 Test RE 0.049762826647941925\n",
      "84 Train Loss 0.012018979 Test MSE 0.012125819072819986 Test RE 0.0526336638208295\n",
      "85 Train Loss 0.011707421 Test MSE 0.011962748945484155 Test RE 0.05227855253987668\n",
      "86 Train Loss 0.011484298 Test MSE 0.011567170519960495 Test RE 0.051406925319455336\n",
      "87 Train Loss 0.010911507 Test MSE 0.011449185677107825 Test RE 0.05114407868391024\n",
      "88 Train Loss 0.010321787 Test MSE 0.010793339876016954 Test RE 0.049657629832279664\n",
      "89 Train Loss 0.0096678175 Test MSE 0.010627698711209024 Test RE 0.04927511848544433\n",
      "90 Train Loss 0.009211319 Test MSE 0.009545877895835815 Test RE 0.046699905066762325\n",
      "91 Train Loss 0.008616932 Test MSE 0.010071791020093625 Test RE 0.047969082659571255\n",
      "92 Train Loss 0.008257188 Test MSE 0.010149145472808842 Test RE 0.048152938970649276\n",
      "93 Train Loss 0.0077654445 Test MSE 0.008350885496319444 Test RE 0.04367916471850293\n",
      "94 Train Loss 0.0071225977 Test MSE 0.00809826394562234 Test RE 0.04301342488779646\n",
      "95 Train Loss 0.0069082854 Test MSE 0.007833382296061371 Test RE 0.042304125450145794\n",
      "96 Train Loss 0.006658448 Test MSE 0.006943467815194337 Test RE 0.03982871305038771\n",
      "97 Train Loss 0.0065021263 Test MSE 0.00585116714452242 Test RE 0.03656194680548382\n",
      "98 Train Loss 0.0064123957 Test MSE 0.005965460930677166 Test RE 0.03691731125391604\n",
      "99 Train Loss 0.0061893407 Test MSE 0.0062349474176697905 Test RE 0.03774196068387512\n",
      "Training time: 127.76\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10 #10\n",
    "max_iter = 100 #100\n",
    "\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "N_I = 200  #Total number of data points for 'y'\n",
    "N_B = 400\n",
    "N_f = 10000 #Total number of collocation points\n",
    "\n",
    "for reps in range(max_reps):\n",
    "  print(reps)\n",
    "\n",
    "  train_loss = []\n",
    "  test_mse_loss = []\n",
    "  test_re_loss = []\n",
    "\n",
    "\n",
    "  torch.manual_seed(reps*36)\n",
    "\n",
    "  layers = np.array([2,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "\n",
    "  PINN = Sequentialmodel(layers)\n",
    "\n",
    "  PINN.to(device)\n",
    "\n",
    "  'Neural Network Summary'\n",
    "  print(PINN)\n",
    "\n",
    "  params = list(PINN.parameters())\n",
    "\n",
    "\n",
    "  optimizer = torch.optim.LBFGS(PINN.parameters(), lr=1, \n",
    "                            max_iter = 20, \n",
    "                            max_eval = 30, \n",
    "                            tolerance_grad = 1e-8, \n",
    "                            tolerance_change = 1e-8, \n",
    "                            history_size = 100, \n",
    "                            line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  train_model(max_iter,reps)\n",
    "\n",
    "  torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "  train_loss_full.append(train_loss)\n",
    "  test_mse_full.append(test_mse_loss)\n",
    "  test_re_full.append(test_re_loss)\n",
    "  #elapsed_time[reps] = time.time() - start_time\n",
    "\n",
    "\n",
    "\n",
    "  #print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1660688516819,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "_O3sPdAnSq_2"
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "aborted",
     "timestamp": 1660688516819,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "jQ4afiEWSq_2"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'KG_tanh_tune0.mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'KG_tanh_tune0.mat'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21546/2033931457.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtune_reps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"KG_tanh_tune\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtune_reps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".mat\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test_re_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \"\"\"\n\u001b[1;32m    223\u001b[0m     \u001b[0mvariable_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'variable_names'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0mMR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mmatfile_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file_context\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mappendmat\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.mat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mfile_like\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'.mat'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             raise IOError(\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'KG_tanh_tune0.mat'"
     ]
    }
   ],
   "source": [
    "for tune_reps in range(5):\n",
    "    label = \"KG_tanh_tune\"+str(tune_reps)+\".mat\"\n",
    "    data = sio.loadmat(label)\n",
    "    re = np.array(data[\"test_re_loss\"])\n",
    "    print(np.mean(re[:,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1660688534316,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "06syezgfv_qO",
    "outputId": "9f4852d5-694a-4977-8893-a6183a2ce493"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.00620766821129004\n",
      "0.09207865958258454\n",
      "0.0039923813402715945\n",
      "1.2569464859652497\n",
      "0.01438868208707847\n",
      "1.350745778552734\n",
      "0.030925378830068205\n",
      "0.7004777223704535\n",
      "0.03525212872970865\n",
      "0.03774196068387512\n",
      "a =  0.35287568463533137\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a+ test_re_full[i][-1]\n",
    "    print(test_re_full[i][-1])\n",
    "    \n",
    "print(\"a = \",a/10)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "stan_2D_KG_16Aug2022_tune.ipynb",
   "version": ""
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
