{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 286,
     "status": "ok",
     "timestamp": 1660687093981,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "iAtv2UvNSq_u",
    "outputId": "68a82578-1b95-4343-a8ec-7635a4df93ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1256,
     "status": "ok",
     "timestamp": 1660687393066,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "xAgfGYA4acPE",
    "outputId": "527d048f-6a89-4e80-87ff-bfdb1c9d6222"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1856,
     "status": "ok",
     "timestamp": 1660687061284,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "7kSdyTofacUc",
    "outputId": "08ee5c9b-0706-46a5-86a1-2c7e56a6a74d"
   },
   "outputs": [],
   "source": [
    "# %cd '/content/gdrive/MyDrive/Virginia Tech /Fall 2022/Codes from GPU/PINN_Stan/2D Klein Gordon/stan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 32419,
     "status": "ok",
     "timestamp": 1660687093700,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "RHuSaD0gagsN",
    "outputId": "c232cd79-e56c-4a76-97c7-d59dafa084ef"
   },
   "outputs": [],
   "source": [
    "# !pip install smt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 280,
     "status": "ok",
     "timestamp": 1660687410736,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "mTLFQRt5Sq_y"
   },
   "outputs": [],
   "source": [
    "def true_2D_1(xt): #True function for 2D_1 Klein Gordon Equation x \\in [-50,50] , t \\in [0,10]\n",
    "    y = xt[:,0]*np.cos(xt[:,1])\n",
    "    return y.reshape(-1,1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 4312,
     "status": "ok",
     "timestamp": 1660687098957,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "81bNHCY3Sq_y"
   },
   "outputs": [],
   "source": [
    "label = \"KG_tanh\"\n",
    "loss_thresh = 0.01\n",
    "\n",
    "x = np.linspace(-10,10,500).reshape(-1,1)\n",
    "t = np.linspace(0,10,1000).reshape(-1,1)\n",
    "\n",
    "X,T = np.meshgrid(x,t)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "T = T.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xt = np.hstack((X,T))\n",
    "\n",
    "y_true = true_2D_1(xt)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "#bound_pts_idx = ((X == -5) + (X == 5) + (T == 0)).reshape(-1,)\n",
    "\n",
    "#xt_bound = xt[bound_pts_idx,:]\n",
    "#y_bound = y_true[bound_pts_idx,:]\n",
    "\n",
    "\n",
    "xt_test_tensor = torch.from_numpy(xt).float().to(device)\n",
    "\n",
    "\n",
    "lb_xt = xt[0]\n",
    "ub_xt = xt[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1660687098958,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "YQgCA-PuSq_z"
   },
   "outputs": [],
   "source": [
    "def trainingdata(N_I,N_B,N_f,seed):\n",
    "    '''Boundary Conditions''' \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    x_BC1 = np.random.uniform(size = N_I).reshape(-1,1)\n",
    "    t_BC1 = np.zeros((N_I,1))\n",
    "    samples = np.hstack((x_BC1,t_BC1))\n",
    "    xt_BC1 = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    y_BC1 = true_2D_1(xt_BC1)\n",
    "    \n",
    "    x_BC2 = np.zeros((int(N_B/2),1))\n",
    "    t_BC2 = np.random.uniform(size = int(N_B/2)).reshape(-1,1)\n",
    "    samples = np.hstack((x_BC2,t_BC2))\n",
    "    xt_BC2 = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    y_BC2 = true_2D_1(xt_BC2)\n",
    "    \n",
    "    x_BC3 = np.ones((int(N_B/2),1))\n",
    "    t_BC3 = np.random.uniform(size = int(N_B/2)).reshape(-1,1)\n",
    "    samples = np.hstack((x_BC3,t_BC3))\n",
    "    xt_BC3 = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    y_BC3 = true_2D_1(xt_BC3)\n",
    "\n",
    "    xt_BC = np.vstack((xt_BC1,xt_BC2,xt_BC3))\n",
    "    y_BC = np.vstack((y_BC1,y_BC2,y_BC3))\n",
    "\n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    samples = sampling(N_f)\n",
    "    xt_coll = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    \n",
    "    xt_coll = np.vstack((xt_coll, xt_BC)) # append training points to collocation points \n",
    "\n",
    "    return xt_coll, xt_BC, y_BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1660687098958,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "gTJxct8bSq_0"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "            \n",
    "        \n",
    "\n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xt):\n",
    "        if torch.is_tensor(xt) != True:         \n",
    "            xt = torch.from_numpy(xt)                \n",
    "        \n",
    "        ubxt = torch.from_numpy(ub_xt).float().to(device)\n",
    "        lbxt = torch.from_numpy(lb_xt).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xt = (xt - lbxt)/(ubxt - lbxt)\n",
    "        \n",
    "        #convert to float\n",
    "        a = xt.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC(self,xt,y):\n",
    "                \n",
    "        loss_bc = self.loss_function(self.forward(xt), y)\n",
    "                \n",
    "        return loss_bc\n",
    "    \n",
    "    def loss_PDE(self, xt_coll, f_hat):\n",
    "        \n",
    "        g = xt_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        y = self.forward(g) \n",
    "        \n",
    "        \n",
    "        y_x_t = autograd.grad(y,g,torch.ones([xt_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        y_xx_tt = autograd.grad(y_x_t,g,torch.ones(xt_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        #du_dx = u_x_t[:,[0]]\n",
    "        \n",
    "        d2y_dx2 = y_xx_tt[:,[0]]\n",
    "        d2y_dt2 = y_xx_tt[:,[1]]    \n",
    "        \n",
    "\n",
    "        f = d2y_dt2 - d2y_dx2 + torch.pow(y,2) + (g[:,0]*torch.cos(g[:,1])).reshape(-1,1) - (torch.pow(g[:,0],2)*torch.pow(torch.cos(g[:,1]),2)).reshape(-1,1)\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xt_BC,y_BC,xt_coll,f_hat):\n",
    "\n",
    "        loss_BC = self.loss_BC(xt_BC,y_BC)\n",
    "        loss_f = self.loss_PDE(xt_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_BC + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "         \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        y_pred = self.forward(xt_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return y_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1660687098958,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "VoQzfzYsYKVs"
   },
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1660687098959,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "_IUDZDkxXmyF"
   },
   "outputs": [],
   "source": [
    "def train_step(xt_BC, y_BC, xt_coll, f_hat,seed):\n",
    "    # x_coll_np_array = colloc_pts(N_f,seed*123)\n",
    "    # x_coll_train = torch.from_numpy(x_coll_np_array).float().to(device)        \n",
    "    \n",
    "    # f_hat = torch.zeros(x_coll_train.shape[0],1).to(device)\n",
    "    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(xt_BC, y_BC, xt_coll,f_hat)\n",
    "        loss.backward()\n",
    "        #print(loss.cpu().detach().numpy())\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 467,
     "status": "ok",
     "timestamp": 1660690085956,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "Vt9Dlr8MYIwW"
   },
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "  print(rep) \n",
    "  torch.manual_seed(rep*9)\n",
    "  start_time = time.time() \n",
    "  thresh_flag = 0\n",
    "\n",
    "  xt_coll, xt_BC, y_BC = trainingdata(N_I,N_B,N_f,rep*11)\n",
    "  xt_coll = torch.from_numpy(xt_coll).float().to(device)\n",
    "  xt_BC = torch.from_numpy(xt_BC).float().to(device)\n",
    "  y_BC = torch.from_numpy(y_BC).float().to(device)\n",
    "\n",
    "  f_hat = torch.zeros(xt_coll.shape[0],1).to(device)\n",
    "\n",
    "  for i in range(max_iter):\n",
    "    train_step(xt_BC, y_BC, xt_coll,f_hat,i)\n",
    "    \n",
    "    loss_np = PINN.loss(xt_BC, y_BC, xt_coll,f_hat).cpu().detach().numpy()\n",
    "    if(thresh_flag == 0):\n",
    "        if(loss_np < loss_thresh):\n",
    "            time_threshold[rep] = time.time() - start_time\n",
    "            epoch_threshold[rep] = i+1            \n",
    "            thresh_flag = 1       \n",
    "    data_update(loss_np)\n",
    "    print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "\n",
    "  elapsed_time[rep] = time.time() - start_time  \n",
    "  print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "sP4Re5lSSq_1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 591.6888 Test MSE 33.65236517187654 Test RE 1.3863914171154386\n",
      "1 Train Loss 409.86728 Test MSE 37.407027248613396 Test RE 1.4616879362775792\n",
      "2 Train Loss 389.46866 Test MSE 38.40262952203168 Test RE 1.4810118924973024\n",
      "3 Train Loss 385.97372 Test MSE 38.0496991857139 Test RE 1.4741907384998996\n",
      "4 Train Loss 376.8397 Test MSE 38.80003354092841 Test RE 1.48865518745214\n",
      "5 Train Loss 367.63672 Test MSE 38.722971754674724 Test RE 1.4871761237071024\n",
      "6 Train Loss 354.7878 Test MSE 37.91881299920015 Test RE 1.471653038779878\n",
      "7 Train Loss 342.4644 Test MSE 38.14170272729986 Test RE 1.475971946795262\n",
      "8 Train Loss 325.60342 Test MSE 38.53395204831414 Test RE 1.4835419827340541\n",
      "9 Train Loss 316.70166 Test MSE 38.33361658504034 Test RE 1.4796805391153223\n",
      "10 Train Loss 304.16476 Test MSE 38.198553588555924 Test RE 1.4770715179499556\n",
      "11 Train Loss 288.62576 Test MSE 38.90234319127179 Test RE 1.4906165713688908\n",
      "12 Train Loss 258.24533 Test MSE 37.99000296186861 Test RE 1.473033854653861\n",
      "13 Train Loss 188.03235 Test MSE 36.94195949688221 Test RE 1.45257320456447\n",
      "14 Train Loss 140.78406 Test MSE 37.41791156601067 Test RE 1.46190057437633\n",
      "15 Train Loss 116.39928 Test MSE 37.661641314946806 Test RE 1.466654050766788\n",
      "16 Train Loss 106.40631 Test MSE 38.14776310554405 Test RE 1.4760892015629548\n",
      "17 Train Loss 99.54578 Test MSE 38.23638106538113 Test RE 1.4778026982884118\n",
      "18 Train Loss 94.434875 Test MSE 38.27292079518602 Test RE 1.4785086439786634\n",
      "19 Train Loss 91.932526 Test MSE 38.4799014685371 Test RE 1.4825011545996518\n",
      "20 Train Loss 89.56444 Test MSE 38.41562338349085 Test RE 1.481262427890537\n",
      "21 Train Loss 87.02358 Test MSE 38.470385942781334 Test RE 1.4823178426681007\n",
      "22 Train Loss 85.08065 Test MSE 38.41646897381851 Test RE 1.4812787302979806\n",
      "23 Train Loss 83.79675 Test MSE 38.32185830340075 Test RE 1.4794535864239937\n",
      "24 Train Loss 82.41241 Test MSE 38.32626154064877 Test RE 1.479538579666878\n",
      "25 Train Loss 81.29071 Test MSE 38.232583400660054 Test RE 1.477729308255484\n",
      "26 Train Loss 80.15891 Test MSE 37.98012280628244 Test RE 1.4728422943890929\n",
      "27 Train Loss 79.04738 Test MSE 37.81319312170108 Test RE 1.46960202253033\n",
      "28 Train Loss 78.291664 Test MSE 37.89645999177925 Test RE 1.4712192076796378\n",
      "29 Train Loss 77.588776 Test MSE 37.79169219724288 Test RE 1.4691841486027701\n",
      "30 Train Loss 77.19672 Test MSE 37.76815619973025 Test RE 1.4687265863994599\n",
      "31 Train Loss 76.755775 Test MSE 37.71355724772313 Test RE 1.4676645814839766\n",
      "32 Train Loss 76.43422 Test MSE 37.59909891218277 Test RE 1.4654357530689586\n",
      "33 Train Loss 76.13561 Test MSE 37.55415863785154 Test RE 1.4645597111247108\n",
      "34 Train Loss 75.91824 Test MSE 37.56428875834241 Test RE 1.4647572280776942\n",
      "35 Train Loss 75.740715 Test MSE 37.53434098769035 Test RE 1.4641732297984171\n",
      "36 Train Loss 75.56857 Test MSE 37.50033044122503 Test RE 1.4635097225136946\n",
      "37 Train Loss 75.40119 Test MSE 37.52732574571642 Test RE 1.4640363949797466\n",
      "38 Train Loss 75.20481 Test MSE 37.561455864720024 Test RE 1.464701995038775\n",
      "39 Train Loss 75.01181 Test MSE 37.462890458740986 Test RE 1.4627789627982086\n",
      "40 Train Loss 74.83863 Test MSE 37.358445928068285 Test RE 1.460738464932582\n",
      "41 Train Loss 74.65475 Test MSE 37.270950873104276 Test RE 1.4590269066833785\n",
      "42 Train Loss 74.3552 Test MSE 37.230247517532035 Test RE 1.4582299922962232\n",
      "43 Train Loss 74.10494 Test MSE 37.1642954025892 Test RE 1.4569378174088456\n",
      "44 Train Loss 73.74035 Test MSE 37.03221191402329 Test RE 1.4543465034247922\n",
      "45 Train Loss 73.27976 Test MSE 36.92168383639238 Test RE 1.4521745261534929\n",
      "46 Train Loss 72.969025 Test MSE 36.825440964793785 Test RE 1.4502806168157616\n",
      "47 Train Loss 72.55356 Test MSE 36.70742089469653 Test RE 1.4479547848211147\n",
      "48 Train Loss 72.34399 Test MSE 36.72007083050866 Test RE 1.4482042569560825\n",
      "49 Train Loss 71.972206 Test MSE 36.31875860790322 Test RE 1.4402688319699093\n",
      "50 Train Loss 71.58677 Test MSE 36.01209987617629 Test RE 1.4341754617189302\n",
      "51 Train Loss 70.723236 Test MSE 34.92878040678081 Test RE 1.4124392433207196\n",
      "52 Train Loss 69.74973 Test MSE 34.88469101416807 Test RE 1.4115475252092209\n",
      "53 Train Loss 69.06947 Test MSE 34.434470692851505 Test RE 1.402409259127355\n",
      "54 Train Loss 65.45494 Test MSE 29.215061802865446 Test RE 1.2917589204015347\n",
      "55 Train Loss 55.77433 Test MSE 26.370147605845826 Test RE 1.2272536855238345\n",
      "56 Train Loss 51.932266 Test MSE 25.351254280464943 Test RE 1.2033107326540664\n",
      "57 Train Loss 50.79862 Test MSE 25.4294873383983 Test RE 1.2051659892406377\n",
      "58 Train Loss 49.95328 Test MSE 25.24290391369947 Test RE 1.200736525373185\n",
      "59 Train Loss 49.568775 Test MSE 25.412259296166667 Test RE 1.2047576804127955\n",
      "60 Train Loss 49.34881 Test MSE 25.310394762411264 Test RE 1.2023406322604067\n",
      "61 Train Loss 49.103523 Test MSE 25.343883790933383 Test RE 1.2031357978404953\n",
      "62 Train Loss 48.95572 Test MSE 25.32829996821957 Test RE 1.2027658399790437\n",
      "63 Train Loss 48.817413 Test MSE 25.43973703764263 Test RE 1.2054088440079411\n",
      "64 Train Loss 48.680725 Test MSE 25.528019576113117 Test RE 1.2074985745153382\n",
      "65 Train Loss 48.50077 Test MSE 25.58086660216049 Test RE 1.2087477845642607\n",
      "66 Train Loss 48.34571 Test MSE 25.597317658344153 Test RE 1.2091363949661857\n",
      "67 Train Loss 48.236523 Test MSE 25.629431008583882 Test RE 1.2098946238212345\n",
      "68 Train Loss 48.132187 Test MSE 25.438088356879085 Test RE 1.2053697837265818\n",
      "69 Train Loss 48.013676 Test MSE 25.348958724734945 Test RE 1.2032562515354914\n",
      "70 Train Loss 47.92951 Test MSE 25.33722168077566 Test RE 1.2029776541616106\n",
      "71 Train Loss 47.860897 Test MSE 25.27299933582051 Test RE 1.201452090989043\n",
      "72 Train Loss 47.76523 Test MSE 25.274005822377447 Test RE 1.2014760144126768\n",
      "73 Train Loss 47.67676 Test MSE 25.230378693349163 Test RE 1.2004385930180155\n",
      "74 Train Loss 47.617397 Test MSE 25.15218289071755 Test RE 1.1985769066567549\n",
      "75 Train Loss 47.57025 Test MSE 25.083569661333634 Test RE 1.1969409772005126\n",
      "76 Train Loss 47.48114 Test MSE 24.9097351609189 Test RE 1.1927862379437444\n",
      "77 Train Loss 47.376156 Test MSE 24.94476257696438 Test RE 1.193624575691924\n",
      "78 Train Loss 47.179386 Test MSE 24.7926454738768 Test RE 1.189979554596984\n",
      "79 Train Loss 46.962025 Test MSE 24.59874525495939 Test RE 1.185317078960884\n",
      "80 Train Loss 46.754852 Test MSE 24.507894306895157 Test RE 1.183126178693236\n",
      "81 Train Loss 46.516247 Test MSE 24.134656287743905 Test RE 1.1740825240497017\n",
      "82 Train Loss 45.81419 Test MSE 23.07930575369655 Test RE 1.148125690843911\n",
      "83 Train Loss 44.72237 Test MSE 22.37619139383562 Test RE 1.1305015149477315\n",
      "84 Train Loss 41.906754 Test MSE 20.88365229657101 Test RE 1.0921474887650655\n",
      "85 Train Loss 40.445827 Test MSE 20.690583048260986 Test RE 1.0870873175921223\n",
      "86 Train Loss 39.847057 Test MSE 20.568799623429673 Test RE 1.0838833334335207\n",
      "87 Train Loss 39.356667 Test MSE 20.43174215032574 Test RE 1.080266141145332\n",
      "88 Train Loss 38.88466 Test MSE 20.271938492012307 Test RE 1.0760332823633207\n",
      "89 Train Loss 38.53704 Test MSE 20.204772153931174 Test RE 1.074249210757456\n",
      "90 Train Loss 38.36584 Test MSE 20.230619983188603 Test RE 1.0749361310331855\n",
      "91 Train Loss 38.22556 Test MSE 20.239718839919277 Test RE 1.0751778337188564\n",
      "92 Train Loss 38.13077 Test MSE 20.215494241193806 Test RE 1.0745342094202517\n",
      "93 Train Loss 38.062862 Test MSE 20.21418748238154 Test RE 1.0744994791355775\n",
      "94 Train Loss 38.03355 Test MSE 20.183863892008187 Test RE 1.073693240678052\n",
      "95 Train Loss 38.00783 Test MSE 20.197200990048437 Test RE 1.0740479197248554\n",
      "96 Train Loss 37.95962 Test MSE 20.190409923942532 Test RE 1.073867336691232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97 Train Loss 37.91612 Test MSE 20.170364358617363 Test RE 1.0733341225676523\n",
      "98 Train Loss 37.890335 Test MSE 20.162652117057213 Test RE 1.0731289055646764\n",
      "99 Train Loss 37.87365 Test MSE 20.18582039332296 Test RE 1.0737452780714845\n",
      "100 Train Loss 37.84345 Test MSE 20.190929237732327 Test RE 1.0738811469737508\n",
      "101 Train Loss 37.816677 Test MSE 20.1932629874043 Test RE 1.0739432069545183\n",
      "102 Train Loss 37.796303 Test MSE 20.189238740199283 Test RE 1.0738361903645557\n",
      "103 Train Loss 37.778633 Test MSE 20.18509045774883 Test RE 1.0737258641475946\n",
      "104 Train Loss 37.76632 Test MSE 20.205800601695188 Test RE 1.0742765507128809\n",
      "105 Train Loss 37.742073 Test MSE 20.21632472322562 Test RE 1.0745562809110092\n",
      "106 Train Loss 37.729305 Test MSE 20.223265572340008 Test RE 1.0747407282093517\n",
      "107 Train Loss 37.71796 Test MSE 20.232234019619554 Test RE 1.0749790103783108\n",
      "108 Train Loss 37.703342 Test MSE 20.227994625134624 Test RE 1.0748663807290875\n",
      "109 Train Loss 37.691547 Test MSE 20.246334194116276 Test RE 1.0753535303565125\n",
      "110 Train Loss 37.679935 Test MSE 20.25935509630704 Test RE 1.075699267576435\n",
      "111 Train Loss 37.668846 Test MSE 20.275648704381602 Test RE 1.0761317467835338\n",
      "112 Train Loss 37.65999 Test MSE 20.269144741716644 Test RE 1.0759591337592829\n",
      "113 Train Loss 37.64412 Test MSE 20.301157486047675 Test RE 1.0768084743604847\n",
      "114 Train Loss 37.632202 Test MSE 20.296453915881823 Test RE 1.076683724389456\n",
      "115 Train Loss 37.626076 Test MSE 20.296291106193266 Test RE 1.0766794060268965\n",
      "116 Train Loss 37.6181 Test MSE 20.300166410989423 Test RE 1.0767821898728318\n",
      "117 Train Loss 37.60967 Test MSE 20.297681099933243 Test RE 1.076716273649756\n",
      "118 Train Loss 37.60243 Test MSE 20.314552067486247 Test RE 1.077163651657151\n",
      "119 Train Loss 37.592857 Test MSE 20.310533565428777 Test RE 1.077057107382367\n",
      "120 Train Loss 37.586575 Test MSE 20.313918814926414 Test RE 1.0771468626589646\n",
      "121 Train Loss 37.5771 Test MSE 20.321288390690864 Test RE 1.0773422310578513\n",
      "122 Train Loss 37.567036 Test MSE 20.33895435685435 Test RE 1.0778104138935518\n",
      "123 Train Loss 37.559303 Test MSE 20.34488283300771 Test RE 1.0779674845979612\n",
      "124 Train Loss 37.552925 Test MSE 20.350650347311984 Test RE 1.0781202687741285\n",
      "125 Train Loss 37.54107 Test MSE 20.348059219239282 Test RE 1.078051631247228\n",
      "126 Train Loss 37.53547 Test MSE 20.35650784212783 Test RE 1.0782754144188065\n",
      "127 Train Loss 37.524113 Test MSE 20.367346346398772 Test RE 1.0785624316607425\n",
      "128 Train Loss 37.513058 Test MSE 20.36747461620614 Test RE 1.0785658279494683\n",
      "129 Train Loss 37.501392 Test MSE 20.359931453937097 Test RE 1.0783660842249563\n",
      "130 Train Loss 37.49456 Test MSE 20.36971105062657 Test RE 1.0786250418573184\n",
      "131 Train Loss 37.48626 Test MSE 20.3732194904388 Test RE 1.0787179280080887\n",
      "132 Train Loss 37.47871 Test MSE 20.3731408192581 Test RE 1.0787158452716072\n",
      "133 Train Loss 37.47209 Test MSE 20.383047493482103 Test RE 1.0789780823945714\n",
      "134 Train Loss 37.464333 Test MSE 20.385555929619777 Test RE 1.079044472475579\n",
      "135 Train Loss 37.455105 Test MSE 20.37904499170646 Test RE 1.0788721408341126\n",
      "136 Train Loss 37.448433 Test MSE 20.372044700120064 Test RE 1.0786868262559006\n",
      "137 Train Loss 37.438934 Test MSE 20.373188270097287 Test RE 1.0787171014829775\n",
      "138 Train Loss 37.43265 Test MSE 20.36770036558351 Test RE 1.0785718052464577\n",
      "139 Train Loss 37.42322 Test MSE 20.36034677617283 Test RE 1.0783770829635826\n",
      "140 Train Loss 37.411137 Test MSE 20.387642259118522 Test RE 1.0790996876662782\n",
      "141 Train Loss 37.404167 Test MSE 20.38677917818549 Test RE 1.0790768463732423\n",
      "142 Train Loss 37.39872 Test MSE 20.384199778653336 Test RE 1.079008580112688\n",
      "143 Train Loss 37.39187 Test MSE 20.388328264743265 Test RE 1.0791178423459349\n",
      "144 Train Loss 37.387146 Test MSE 20.388507071119175 Test RE 1.0791225742868038\n",
      "145 Train Loss 37.380417 Test MSE 20.379510732993793 Test RE 1.0788844689983887\n",
      "146 Train Loss 37.374336 Test MSE 20.385056173611833 Test RE 1.0790312458982694\n",
      "147 Train Loss 37.368397 Test MSE 20.387637250161138 Test RE 1.0790995551064506\n",
      "148 Train Loss 37.363457 Test MSE 20.39331226478805 Test RE 1.079249731401705\n",
      "149 Train Loss 37.360947 Test MSE 20.391312841231418 Test RE 1.0791968236101144\n",
      "150 Train Loss 37.35565 Test MSE 20.397457704451174 Test RE 1.0793594177843606\n",
      "151 Train Loss 37.350006 Test MSE 20.396474333397638 Test RE 1.0793333992575356\n",
      "152 Train Loss 37.34471 Test MSE 20.390745830944148 Test RE 1.0791818191825036\n",
      "153 Train Loss 37.338634 Test MSE 20.393950315233425 Test RE 1.079266614642071\n",
      "154 Train Loss 37.332657 Test MSE 20.40310042224694 Test RE 1.0795087035317183\n",
      "155 Train Loss 37.327747 Test MSE 20.39591947111392 Test RE 1.0793187181548596\n",
      "156 Train Loss 37.319965 Test MSE 20.400218279921933 Test RE 1.0794324551296732\n",
      "157 Train Loss 37.316597 Test MSE 20.402076956126184 Test RE 1.0794816278810098\n",
      "158 Train Loss 37.310543 Test MSE 20.402363660529797 Test RE 1.0794892126737174\n",
      "159 Train Loss 37.305035 Test MSE 20.39922812658202 Test RE 1.0794062589241897\n",
      "160 Train Loss 37.297413 Test MSE 20.409622555729033 Test RE 1.0796812296953755\n",
      "161 Train Loss 37.28994 Test MSE 20.411854687047843 Test RE 1.0797402686215254\n",
      "162 Train Loss 37.28269 Test MSE 20.408344119153078 Test RE 1.0796474141364034\n",
      "163 Train Loss 37.271732 Test MSE 20.42482732108989 Test RE 1.0800833254078481\n",
      "164 Train Loss 37.260223 Test MSE 20.433504648443286 Test RE 1.0803127335006881\n",
      "165 Train Loss 37.252003 Test MSE 20.427913643437087 Test RE 1.0801649260826884\n",
      "166 Train Loss 37.243618 Test MSE 20.421519818424333 Test RE 1.0799958700023018\n",
      "167 Train Loss 37.231556 Test MSE 20.426543169830705 Test RE 1.080128692271034\n",
      "168 Train Loss 37.21801 Test MSE 20.428691187567505 Test RE 1.0801854829521018\n",
      "169 Train Loss 37.197487 Test MSE 20.424799193145507 Test RE 1.0800825816920336\n",
      "170 Train Loss 37.180264 Test MSE 20.452104175658253 Test RE 1.0808042971371576\n",
      "171 Train Loss 37.16193 Test MSE 20.429363053672446 Test RE 1.0802032455693784\n",
      "172 Train Loss 37.147587 Test MSE 20.418733310484505 Test RE 1.0799221849935725\n",
      "173 Train Loss 37.132996 Test MSE 20.42786919588718 Test RE 1.0801637509575284\n",
      "174 Train Loss 37.114174 Test MSE 20.434470893448175 Test RE 1.0803382757280382\n",
      "175 Train Loss 37.090996 Test MSE 20.454657270175257 Test RE 1.080871754974035\n",
      "176 Train Loss 37.065212 Test MSE 20.44864808928983 Test RE 1.0807129737509769\n",
      "177 Train Loss 37.045216 Test MSE 20.437150655899057 Test RE 1.080409110814818\n",
      "178 Train Loss 37.02255 Test MSE 20.47082167227096 Test RE 1.0812987529683906\n",
      "179 Train Loss 37.00334 Test MSE 20.483322275349398 Test RE 1.0816288526543907\n",
      "180 Train Loss 36.982307 Test MSE 20.494697182564614 Test RE 1.0819291389079533\n",
      "181 Train Loss 36.921684 Test MSE 20.499772760797534 Test RE 1.0820631022446259\n",
      "182 Train Loss 36.873882 Test MSE 20.5032025635271 Test RE 1.082153618071453\n",
      "183 Train Loss 36.831238 Test MSE 20.4796270649224 Test RE 1.081531284827363\n",
      "184 Train Loss 36.786404 Test MSE 20.458886772880795 Test RE 1.0809834975873671\n",
      "185 Train Loss 36.692135 Test MSE 20.4888324916283 Test RE 1.0817743273010345\n",
      "186 Train Loss 36.615902 Test MSE 20.494890714887926 Test RE 1.0819342472480213\n",
      "187 Train Loss 36.540977 Test MSE 20.493297898363874 Test RE 1.0818922036905458\n",
      "188 Train Loss 36.437412 Test MSE 20.53799897978545 Test RE 1.083071501628276\n",
      "189 Train Loss 36.29623 Test MSE 20.447779579408106 Test RE 1.0806900230928342\n",
      "190 Train Loss 36.12978 Test MSE 20.443357403461416 Test RE 1.0805731580850673\n",
      "191 Train Loss 35.82817 Test MSE 20.463745999349282 Test RE 1.081111863119219\n",
      "192 Train Loss 35.607582 Test MSE 20.42180279834121 Test RE 1.0800033526991215\n",
      "193 Train Loss 35.14722 Test MSE 20.24200000428069 Test RE 1.0752384222149627\n",
      "194 Train Loss 34.705486 Test MSE 20.146629365142715 Test RE 1.0727024265606002\n",
      "195 Train Loss 34.168964 Test MSE 20.126752697451916 Test RE 1.072173131786681\n",
      "196 Train Loss 33.638893 Test MSE 20.0905620838504 Test RE 1.0712087421782912\n",
      "197 Train Loss 32.697323 Test MSE 19.98729173525399 Test RE 1.0684520590909006\n",
      "198 Train Loss 31.971542 Test MSE 20.069862326154727 Test RE 1.0706567547379338\n",
      "199 Train Loss 30.681196 Test MSE 20.03198327941659 Test RE 1.0696459204243811\n",
      "Training time: 245.66\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 611.7686 Test MSE 34.64882580565448 Test RE 1.4067674967931918\n",
      "1 Train Loss 424.89053 Test MSE 34.3409764316006 Test RE 1.4005040993223246\n",
      "2 Train Loss 397.15536 Test MSE 35.49046691038986 Test RE 1.42375060319809\n",
      "3 Train Loss 390.60806 Test MSE 34.715215002607565 Test RE 1.4081145766417233\n",
      "4 Train Loss 377.99738 Test MSE 35.31259196217419 Test RE 1.4201782681397417\n",
      "5 Train Loss 373.29764 Test MSE 35.83151317200122 Test RE 1.4305750256883694\n",
      "6 Train Loss 368.86618 Test MSE 35.7845424627522 Test RE 1.429637064195507\n",
      "7 Train Loss 353.55484 Test MSE 37.4887026232224 Test RE 1.4632828079257267\n",
      "8 Train Loss 347.94876 Test MSE 37.86625390047571 Test RE 1.4706327592270942\n",
      "9 Train Loss 336.90417 Test MSE 36.74203214546224 Test RE 1.4486372587879257\n",
      "10 Train Loss 327.2791 Test MSE 35.62549499377677 Test RE 1.4264564538448676\n",
      "11 Train Loss 306.03568 Test MSE 38.03733875150828 Test RE 1.4739512738249656\n",
      "12 Train Loss 273.8936 Test MSE 37.46530253978043 Test RE 1.4628260531930748\n",
      "13 Train Loss 250.34297 Test MSE 36.87026224483475 Test RE 1.4511629370896364\n",
      "14 Train Loss 234.95181 Test MSE 37.15276307565293 Test RE 1.4567117511442986\n",
      "15 Train Loss 226.34216 Test MSE 36.90842993608569 Test RE 1.4519138567853138\n",
      "16 Train Loss 215.95609 Test MSE 37.59338550684728 Test RE 1.4653244080263323\n",
      "17 Train Loss 185.94727 Test MSE 37.503615013701555 Test RE 1.4635738139289307\n",
      "18 Train Loss 165.66281 Test MSE 39.18375151619658 Test RE 1.495998201686787\n",
      "19 Train Loss 155.65782 Test MSE 39.492330950424595 Test RE 1.5018772836763943\n",
      "20 Train Loss 150.01007 Test MSE 39.47536073655855 Test RE 1.5015545638381664\n",
      "21 Train Loss 145.72333 Test MSE 39.42482672818262 Test RE 1.500593155643615\n",
      "22 Train Loss 142.1936 Test MSE 39.5106177498272 Test RE 1.5022249632049012\n",
      "23 Train Loss 139.5494 Test MSE 39.38806361003049 Test RE 1.499893351046383\n",
      "24 Train Loss 135.61884 Test MSE 39.64556878008497 Test RE 1.504788248802791\n",
      "25 Train Loss 131.14594 Test MSE 38.65160932241807 Test RE 1.4858051358716855\n",
      "26 Train Loss 127.4933 Test MSE 38.14842424610662 Test RE 1.4761019925910175\n",
      "27 Train Loss 124.24416 Test MSE 38.31841882913411 Test RE 1.4793871927658218\n",
      "28 Train Loss 122.55359 Test MSE 38.42278572973216 Test RE 1.48140050738769\n",
      "29 Train Loss 119.61561 Test MSE 38.002217552788046 Test RE 1.4732706414205352\n",
      "30 Train Loss 118.569565 Test MSE 38.076293119952446 Test RE 1.4747058238293136\n",
      "31 Train Loss 117.54431 Test MSE 38.047707620238604 Test RE 1.474152157566799\n",
      "32 Train Loss 117.05122 Test MSE 38.12789759899438 Test RE 1.4757048136481525\n",
      "33 Train Loss 116.62088 Test MSE 38.12169747155179 Test RE 1.475584823686994\n",
      "34 Train Loss 116.08482 Test MSE 38.034864119636666 Test RE 1.4739033269095718\n",
      "35 Train Loss 115.50098 Test MSE 37.984488401392625 Test RE 1.4729269393018263\n",
      "36 Train Loss 114.548615 Test MSE 37.86128982513166 Test RE 1.4705363597903518\n",
      "37 Train Loss 114.14957 Test MSE 37.7922186501315 Test RE 1.4691943817188386\n",
      "38 Train Loss 113.64661 Test MSE 37.72271380571864 Test RE 1.4678427394360727\n",
      "39 Train Loss 113.30276 Test MSE 37.78781974408985 Test RE 1.4691088742208909\n",
      "40 Train Loss 112.767654 Test MSE 37.70059567879229 Test RE 1.4674123529512166\n",
      "41 Train Loss 112.441864 Test MSE 37.67511958352524 Test RE 1.4669164688161018\n",
      "42 Train Loss 112.11473 Test MSE 37.532158833413746 Test RE 1.4641306674644254\n",
      "43 Train Loss 111.7063 Test MSE 37.62865338044683 Test RE 1.4660115869074009\n",
      "44 Train Loss 111.37071 Test MSE 37.675728624630445 Test RE 1.466928325564357\n",
      "45 Train Loss 110.903694 Test MSE 37.82976485678241 Test RE 1.469924015804114\n",
      "46 Train Loss 110.47658 Test MSE 37.76026142071341 Test RE 1.468573072447317\n",
      "47 Train Loss 110.13126 Test MSE 37.61669252104145 Test RE 1.465778570966775\n",
      "48 Train Loss 109.67792 Test MSE 37.53438393426019 Test RE 1.4641740674473303\n",
      "49 Train Loss 109.416504 Test MSE 37.440026522400416 Test RE 1.462332521144056\n",
      "50 Train Loss 109.132355 Test MSE 37.413255137118036 Test RE 1.4618096092799275\n",
      "51 Train Loss 108.87388 Test MSE 37.435205837480986 Test RE 1.462238374959898\n",
      "52 Train Loss 108.65052 Test MSE 37.39327310193269 Test RE 1.4614191883961696\n",
      "53 Train Loss 108.30817 Test MSE 37.45761940404412 Test RE 1.4626760521703681\n",
      "54 Train Loss 108.17398 Test MSE 37.35905094507457 Test RE 1.460750293155263\n",
      "55 Train Loss 107.90279 Test MSE 37.394809443183355 Test RE 1.461449210046267\n",
      "56 Train Loss 107.62859 Test MSE 37.441346528566406 Test RE 1.4623582993166488\n",
      "57 Train Loss 107.222176 Test MSE 37.30559580562759 Test RE 1.45970486292546\n",
      "58 Train Loss 107.14024 Test MSE 37.35054027576052 Test RE 1.4605838987945263\n",
      "59 Train Loss 106.99593 Test MSE 37.32838106168628 Test RE 1.4601505691441068\n",
      "60 Train Loss 106.80348 Test MSE 37.32772812053475 Test RE 1.4601377987461408\n",
      "61 Train Loss 106.74068 Test MSE 37.355791249283556 Test RE 1.4606865642149849\n",
      "62 Train Loss 106.616 Test MSE 37.335983788349985 Test RE 1.460299257088256\n",
      "63 Train Loss 106.46775 Test MSE 37.34961707128257 Test RE 1.460565847838392\n",
      "64 Train Loss 106.241974 Test MSE 37.28823992702178 Test RE 1.4593652703367237\n",
      "65 Train Loss 105.99055 Test MSE 37.24153519762706 Test RE 1.4584510328121032\n",
      "66 Train Loss 105.767426 Test MSE 37.17610127467412 Test RE 1.4571692096031124\n",
      "67 Train Loss 105.53857 Test MSE 37.144816522033366 Test RE 1.4565559557722778\n",
      "68 Train Loss 105.33475 Test MSE 37.102430429712875 Test RE 1.4557246777726092\n",
      "69 Train Loss 105.02547 Test MSE 37.0669642636386 Test RE 1.4550287487391185\n",
      "70 Train Loss 104.82001 Test MSE 36.939184238524234 Test RE 1.4525186413835185\n",
      "71 Train Loss 104.46109 Test MSE 36.78346748785932 Test RE 1.44945386940824\n",
      "72 Train Loss 103.94349 Test MSE 36.63498147834959 Test RE 1.4465253627073957\n",
      "73 Train Loss 103.48206 Test MSE 36.434419755509886 Test RE 1.4425603575815422\n",
      "74 Train Loss 103.26752 Test MSE 36.392385035130715 Test RE 1.441727969947712\n",
      "75 Train Loss 103.02378 Test MSE 36.283279787564204 Test RE 1.4395651801036642\n",
      "76 Train Loss 102.74872 Test MSE 36.29679141894889 Test RE 1.4398331969929512\n",
      "77 Train Loss 102.40263 Test MSE 36.334014410332635 Test RE 1.4405712948110618\n",
      "78 Train Loss 102.00503 Test MSE 36.33993459579345 Test RE 1.4406886518111033\n",
      "79 Train Loss 101.41811 Test MSE 36.182977428330545 Test RE 1.4375740189838766\n",
      "80 Train Loss 101.02022 Test MSE 35.96687325225007 Test RE 1.4332746076703176\n",
      "81 Train Loss 100.71801 Test MSE 35.955612670855594 Test RE 1.4330502238503497\n",
      "82 Train Loss 100.309616 Test MSE 35.81497391169278 Test RE 1.4302448221660786\n",
      "83 Train Loss 100.06289 Test MSE 35.721868761847084 Test RE 1.4283845693186656\n",
      "84 Train Loss 99.66912 Test MSE 35.28498474836785 Test RE 1.4196230153403486\n",
      "85 Train Loss 99.27818 Test MSE 34.55797131113887 Test RE 1.4049219069074035\n",
      "86 Train Loss 95.17235 Test MSE 28.881823395822693 Test RE 1.2843706381500584\n",
      "87 Train Loss 90.20516 Test MSE 27.44741963834105 Test RE 1.2520706223959381\n",
      "88 Train Loss 87.42864 Test MSE 27.089094090685894 Test RE 1.2438708934256135\n",
      "89 Train Loss 85.460045 Test MSE 26.960860017500284 Test RE 1.2409232893736688\n",
      "90 Train Loss 84.38383 Test MSE 27.073374628872894 Test RE 1.2435099397083205\n",
      "91 Train Loss 83.80159 Test MSE 26.968394030912393 Test RE 1.2410966606914635\n",
      "92 Train Loss 83.32822 Test MSE 27.020204826225715 Test RE 1.2422882657752856\n",
      "93 Train Loss 82.74269 Test MSE 26.8666514298298 Test RE 1.238753330138905\n",
      "94 Train Loss 82.27028 Test MSE 26.94777527398413 Test RE 1.2406221281068874\n",
      "95 Train Loss 81.861755 Test MSE 26.87028190433785 Test RE 1.2388370233092416\n",
      "96 Train Loss 81.58101 Test MSE 26.7257680797686 Test RE 1.2355011736882628\n",
      "97 Train Loss 81.1348 Test MSE 26.74735858820382 Test RE 1.2360001249852648\n",
      "98 Train Loss 80.58751 Test MSE 26.641946979180844 Test RE 1.2335621761612618\n",
      "99 Train Loss 80.352196 Test MSE 26.620302447122928 Test RE 1.2330609871679998\n",
      "100 Train Loss 80.137215 Test MSE 26.587463994061647 Test RE 1.2323002086103667\n",
      "101 Train Loss 79.85844 Test MSE 26.474223425384896 Test RE 1.2296731193607586\n",
      "102 Train Loss 79.59294 Test MSE 26.420465772843347 Test RE 1.228424018855635\n",
      "103 Train Loss 79.33706 Test MSE 26.450371377052765 Test RE 1.2291190552910631\n",
      "104 Train Loss 79.05913 Test MSE 26.53772382559737 Test RE 1.2311469674697353\n",
      "105 Train Loss 78.709755 Test MSE 26.587474624694583 Test RE 1.232300454969513\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "106 Train Loss 78.58881 Test MSE 26.59430917758692 Test RE 1.2324588318288214\n",
      "107 Train Loss 78.30472 Test MSE 26.499274684805346 Test RE 1.2302547714829792\n",
      "108 Train Loss 78.02005 Test MSE 26.64006957849703 Test RE 1.2335187121592643\n",
      "109 Train Loss 77.901855 Test MSE 26.612105216825633 Test RE 1.2328711233762373\n",
      "110 Train Loss 77.750244 Test MSE 26.599507458621908 Test RE 1.2325792777913098\n",
      "111 Train Loss 77.661835 Test MSE 26.62528741219787 Test RE 1.233176434357826\n",
      "112 Train Loss 77.50852 Test MSE 26.586244943971465 Test RE 1.232271957459735\n",
      "113 Train Loss 77.34843 Test MSE 26.586965071314733 Test RE 1.2322886462911922\n",
      "114 Train Loss 77.26251 Test MSE 26.586303430411427 Test RE 1.2322733128816905\n",
      "115 Train Loss 77.1045 Test MSE 26.521542306923575 Test RE 1.2307715610188723\n",
      "116 Train Loss 76.99227 Test MSE 26.502767173300917 Test RE 1.2303358399108237\n",
      "117 Train Loss 76.839714 Test MSE 26.42575220030844 Test RE 1.2285469093776686\n",
      "118 Train Loss 76.609695 Test MSE 26.371599038325783 Test RE 1.2272874595355872\n",
      "119 Train Loss 76.53796 Test MSE 26.320779231735017 Test RE 1.2261043574369777\n",
      "120 Train Loss 76.40929 Test MSE 26.33451954340309 Test RE 1.2264243490632694\n",
      "121 Train Loss 76.33573 Test MSE 26.21726730231963 Test RE 1.2236910273859487\n",
      "122 Train Loss 76.23988 Test MSE 26.188647710024664 Test RE 1.2230229352318758\n",
      "123 Train Loss 76.179825 Test MSE 26.215344602107173 Test RE 1.2236461555442435\n",
      "124 Train Loss 76.10348 Test MSE 26.258565660807427 Test RE 1.224654448632621\n",
      "125 Train Loss 76.03668 Test MSE 26.265378017801904 Test RE 1.2248132966515324\n",
      "126 Train Loss 75.965096 Test MSE 26.225817429998912 Test RE 1.223890549737044\n",
      "127 Train Loss 75.839905 Test MSE 26.247077724679116 Test RE 1.22438653051119\n",
      "128 Train Loss 75.72465 Test MSE 26.22070596729093 Test RE 1.2237712746034777\n",
      "129 Train Loss 75.63027 Test MSE 26.2195911687845 Test RE 1.2237452594212044\n",
      "130 Train Loss 75.54322 Test MSE 26.244711679309525 Test RE 1.2243313430461134\n",
      "131 Train Loss 75.428085 Test MSE 26.20571939257247 Test RE 1.2234214983423675\n",
      "132 Train Loss 75.28996 Test MSE 26.145211167047574 Test RE 1.2220082601900428\n",
      "133 Train Loss 74.97483 Test MSE 25.924313060579095 Test RE 1.2168350010677573\n",
      "134 Train Loss 74.67827 Test MSE 25.866460190256916 Test RE 1.2154764941698966\n",
      "135 Train Loss 74.53425 Test MSE 25.850990917531043 Test RE 1.215112985802934\n",
      "136 Train Loss 74.40779 Test MSE 25.78392990763935 Test RE 1.2135358776024254\n",
      "137 Train Loss 74.30939 Test MSE 25.753111461661533 Test RE 1.212810416585563\n",
      "138 Train Loss 74.23048 Test MSE 25.738413844219608 Test RE 1.2124642842826334\n",
      "139 Train Loss 74.12287 Test MSE 25.766066700316458 Test RE 1.2131154335711405\n",
      "140 Train Loss 74.01685 Test MSE 25.70045811574285 Test RE 1.2115699606453068\n",
      "141 Train Loss 73.88578 Test MSE 25.758728365702186 Test RE 1.2129426698927563\n",
      "142 Train Loss 73.717834 Test MSE 25.555860573927756 Test RE 1.2081568473610778\n",
      "143 Train Loss 73.57886 Test MSE 25.47484372697788 Test RE 1.2062402858619228\n",
      "144 Train Loss 73.46786 Test MSE 25.475423582632303 Test RE 1.2062540139396636\n",
      "145 Train Loss 73.24592 Test MSE 25.36188895244203 Test RE 1.2035630963640587\n",
      "146 Train Loss 72.98993 Test MSE 25.36958029091152 Test RE 1.2037455809857402\n",
      "147 Train Loss 72.88579 Test MSE 25.298997004216286 Test RE 1.2020698832052652\n",
      "148 Train Loss 72.80532 Test MSE 25.282465005861734 Test RE 1.2016770639780459\n",
      "149 Train Loss 72.705025 Test MSE 25.285364588654332 Test RE 1.2017459706736289\n",
      "150 Train Loss 72.50921 Test MSE 25.102368070657583 Test RE 1.19738940564687\n",
      "151 Train Loss 72.296364 Test MSE 25.075485224582884 Test RE 1.1967480745639925\n",
      "152 Train Loss 72.09448 Test MSE 25.093952265132508 Test RE 1.1971886707781327\n",
      "153 Train Loss 71.89173 Test MSE 25.1050432951151 Test RE 1.197453208393947\n",
      "154 Train Loss 71.62859 Test MSE 25.087156857686736 Test RE 1.1970265612873925\n",
      "155 Train Loss 71.30067 Test MSE 24.83641453647386 Test RE 1.1910294893945919\n",
      "156 Train Loss 71.000084 Test MSE 24.796206201268514 Test RE 1.1900650041443965\n",
      "157 Train Loss 70.667076 Test MSE 24.759841185316592 Test RE 1.18919203569334\n",
      "158 Train Loss 70.32689 Test MSE 24.784762066854316 Test RE 1.1897903485067987\n",
      "159 Train Loss 69.99958 Test MSE 24.669447346832545 Test RE 1.1870192849994532\n",
      "160 Train Loss 69.72652 Test MSE 24.588364002365566 Test RE 1.1850669366384763\n",
      "161 Train Loss 69.42267 Test MSE 24.641331771012904 Test RE 1.186342673873862\n",
      "162 Train Loss 69.17013 Test MSE 24.601332504853588 Test RE 1.1853794120356378\n",
      "163 Train Loss 68.903244 Test MSE 24.519495617420368 Test RE 1.1834061739889072\n",
      "164 Train Loss 68.70798 Test MSE 24.53962390801025 Test RE 1.1838918091183497\n",
      "165 Train Loss 68.46333 Test MSE 24.557032370560684 Test RE 1.1843116623819538\n",
      "166 Train Loss 68.29882 Test MSE 24.51888467476833 Test RE 1.1833914306634057\n",
      "167 Train Loss 68.10817 Test MSE 24.493999760489633 Test RE 1.182790749381419\n",
      "168 Train Loss 67.95187 Test MSE 24.49627943229949 Test RE 1.1828457896365687\n",
      "169 Train Loss 67.75134 Test MSE 24.500349831424288 Test RE 1.1829440587327245\n",
      "170 Train Loss 67.494804 Test MSE 24.553409776033885 Test RE 1.1842243057519652\n",
      "171 Train Loss 67.35331 Test MSE 24.533278783389108 Test RE 1.1837387418435246\n",
      "172 Train Loss 67.14922 Test MSE 24.552046851607013 Test RE 1.1841914380027123\n",
      "173 Train Loss 66.96971 Test MSE 24.567621296723306 Test RE 1.1845669708610416\n",
      "174 Train Loss 66.82456 Test MSE 24.57884483428874 Test RE 1.184837520330361\n",
      "175 Train Loss 66.522156 Test MSE 24.56967133626633 Test RE 1.1846163927901103\n",
      "176 Train Loss 66.28935 Test MSE 24.457429615338764 Test RE 1.1819074516961408\n",
      "177 Train Loss 66.12833 Test MSE 24.396113311445696 Test RE 1.180424964033359\n",
      "178 Train Loss 65.99042 Test MSE 24.38170980567905 Test RE 1.1800764501565615\n",
      "179 Train Loss 65.79625 Test MSE 24.319092639379495 Test RE 1.1785601383800712\n",
      "180 Train Loss 65.64304 Test MSE 24.351194646416058 Test RE 1.1793377510375198\n",
      "181 Train Loss 65.52221 Test MSE 24.299419508762536 Test RE 1.178083338959474\n",
      "182 Train Loss 65.39203 Test MSE 24.286870968805832 Test RE 1.1777791108090447\n",
      "183 Train Loss 65.158936 Test MSE 24.2184547777775 Test RE 1.1761190371679213\n",
      "184 Train Loss 64.69902 Test MSE 23.817849400463032 Test RE 1.166351190717775\n",
      "185 Train Loss 64.288704 Test MSE 23.78911905718401 Test RE 1.1656475213622668\n",
      "186 Train Loss 63.883446 Test MSE 23.631197515366978 Test RE 1.1617720652789585\n",
      "187 Train Loss 63.5876 Test MSE 23.570791514600764 Test RE 1.160286255919635\n",
      "188 Train Loss 62.94216 Test MSE 23.14183996028389 Test RE 1.149680082502349\n",
      "189 Train Loss 60.657288 Test MSE 22.24940403043508 Test RE 1.127294156757106\n",
      "190 Train Loss 52.386265 Test MSE 21.893394262410457 Test RE 1.1182389456052415\n",
      "191 Train Loss 48.62784 Test MSE 21.840965967243918 Test RE 1.116899214959747\n",
      "192 Train Loss 45.657753 Test MSE 21.751392879305662 Test RE 1.1146065760578034\n",
      "193 Train Loss 44.06557 Test MSE 21.932926258492895 Test RE 1.119248069173491\n",
      "194 Train Loss 42.9534 Test MSE 21.757755400847703 Test RE 1.114769581476941\n",
      "195 Train Loss 42.505478 Test MSE 21.608242762058666 Test RE 1.1109328010573576\n",
      "196 Train Loss 41.70952 Test MSE 21.494537578740587 Test RE 1.1080060144122534\n",
      "197 Train Loss 41.212463 Test MSE 21.387800216773766 Test RE 1.1052515279561403\n",
      "198 Train Loss 40.7421 Test MSE 21.410613447258463 Test RE 1.1058408274043021\n",
      "199 Train Loss 40.464886 Test MSE 21.37928445409425 Test RE 1.1050314726750827\n",
      "Training time: 239.01\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 629.39636 Test MSE 35.93307963159624 Test RE 1.4326011140278037\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 Train Loss 438.26807 Test MSE 35.188849255659946 Test RE 1.4176877836165993\n",
      "2 Train Loss 394.2174 Test MSE 34.57124308821461 Test RE 1.4051916568347183\n",
      "3 Train Loss 389.41376 Test MSE 34.59843005659114 Test RE 1.4057440725124224\n",
      "4 Train Loss 378.80133 Test MSE 34.749834530351265 Test RE 1.4088165183441055\n",
      "5 Train Loss 376.8815 Test MSE 34.44658508802106 Test RE 1.402655928316979\n",
      "6 Train Loss 370.1175 Test MSE 34.224589250659946 Test RE 1.3981288164310905\n",
      "7 Train Loss 355.67737 Test MSE 33.15046386411483 Test RE 1.3760140535782328\n",
      "8 Train Loss 350.40622 Test MSE 32.772322389572835 Test RE 1.3681435705086202\n",
      "9 Train Loss 340.14758 Test MSE 32.7675531785557 Test RE 1.368044016947592\n",
      "10 Train Loss 334.43503 Test MSE 32.306990796829936 Test RE 1.3583957639795503\n",
      "11 Train Loss 332.08035 Test MSE 32.31516445599378 Test RE 1.3585675899765155\n",
      "12 Train Loss 329.76538 Test MSE 32.72476456887863 Test RE 1.3671505138033937\n",
      "13 Train Loss 328.21405 Test MSE 32.94044432058478 Test RE 1.371648367887225\n",
      "14 Train Loss 326.31537 Test MSE 32.80964498191928 Test RE 1.3689224004967004\n",
      "15 Train Loss 324.27466 Test MSE 32.640258858882106 Test RE 1.3653841648640859\n",
      "16 Train Loss 318.20148 Test MSE 33.41922868989731 Test RE 1.381580757289065\n",
      "17 Train Loss 299.27866 Test MSE 31.011287433357637 Test RE 1.330877131979198\n",
      "18 Train Loss 271.90878 Test MSE 29.865208088906503 Test RE 1.3060531084695521\n",
      "19 Train Loss 222.03416 Test MSE 30.754072409961925 Test RE 1.325346333286789\n",
      "20 Train Loss 194.89226 Test MSE 31.06166808417114 Test RE 1.3319577586045284\n",
      "21 Train Loss 178.79146 Test MSE 31.502526355953652 Test RE 1.3413766942072272\n",
      "22 Train Loss 166.12503 Test MSE 31.308768591683553 Test RE 1.337245231619262\n",
      "23 Train Loss 155.65799 Test MSE 31.50999326614874 Test RE 1.3415356552023185\n",
      "24 Train Loss 146.03067 Test MSE 30.861837994561444 Test RE 1.3276663808431233\n",
      "25 Train Loss 138.63693 Test MSE 31.00729320069982 Test RE 1.3307914211860645\n",
      "26 Train Loss 124.58333 Test MSE 31.470823840621044 Test RE 1.3407015782043763\n",
      "27 Train Loss 120.28934 Test MSE 31.07203794667564 Test RE 1.3321800754836406\n",
      "28 Train Loss 114.06639 Test MSE 31.084524512710235 Test RE 1.332447722615156\n",
      "29 Train Loss 108.19273 Test MSE 31.289845168782875 Test RE 1.3368410464174787\n",
      "30 Train Loss 105.237885 Test MSE 31.18394908032674 Test RE 1.334576953741105\n",
      "31 Train Loss 102.96084 Test MSE 30.738123500788777 Test RE 1.3250026297080442\n",
      "32 Train Loss 100.88021 Test MSE 30.768015563236947 Test RE 1.3256467392420619\n",
      "33 Train Loss 98.50502 Test MSE 30.713307686744937 Test RE 1.3244676644229094\n",
      "34 Train Loss 97.71038 Test MSE 30.675324842761075 Test RE 1.3236484330272704\n",
      "35 Train Loss 96.52055 Test MSE 30.654845104360962 Test RE 1.3232065061722196\n",
      "36 Train Loss 95.5559 Test MSE 30.767058653262144 Test RE 1.3256261247430796\n",
      "37 Train Loss 93.924515 Test MSE 30.742205788779998 Test RE 1.3250906126771982\n",
      "38 Train Loss 93.20783 Test MSE 30.79299063814415 Test RE 1.326184658410457\n",
      "39 Train Loss 92.30674 Test MSE 30.739497269141236 Test RE 1.325032238320003\n",
      "40 Train Loss 91.29498 Test MSE 30.57250674141153 Test RE 1.3214282568369775\n",
      "41 Train Loss 90.40559 Test MSE 30.550840096426136 Test RE 1.3209599277003359\n",
      "42 Train Loss 89.825035 Test MSE 30.533317404348672 Test RE 1.3205810495099861\n",
      "43 Train Loss 89.04033 Test MSE 30.627088807169393 Test RE 1.3226073246999475\n",
      "44 Train Loss 88.471664 Test MSE 30.673937305727698 Test RE 1.3236184963925528\n",
      "45 Train Loss 87.71464 Test MSE 30.853343179373713 Test RE 1.3274836461540822\n",
      "46 Train Loss 87.04671 Test MSE 30.766340829569494 Test RE 1.3256106606163172\n",
      "47 Train Loss 86.66048 Test MSE 30.581884116930727 Test RE 1.3216308993457557\n",
      "48 Train Loss 85.50153 Test MSE 30.484526776660406 Test RE 1.3195255183049217\n",
      "49 Train Loss 84.8274 Test MSE 30.69139788961175 Test RE 1.323995165738762\n",
      "50 Train Loss 84.540504 Test MSE 30.833400973180325 Test RE 1.3270545641004945\n",
      "51 Train Loss 84.08963 Test MSE 30.89225770786195 Test RE 1.3283205428442881\n",
      "52 Train Loss 83.6861 Test MSE 31.0111757463677 Test RE 1.3308747354035155\n",
      "53 Train Loss 83.28135 Test MSE 30.851580092441093 Test RE 1.3274457166743958\n",
      "54 Train Loss 83.025154 Test MSE 30.84930477497484 Test RE 1.327396765922021\n",
      "55 Train Loss 82.60759 Test MSE 30.79351462771408 Test RE 1.3261959418869593\n",
      "56 Train Loss 82.11278 Test MSE 30.890002326515464 Test RE 1.3282720529591294\n",
      "57 Train Loss 81.8785 Test MSE 30.880065664558465 Test RE 1.3280583972294306\n",
      "58 Train Loss 81.2395 Test MSE 30.769850683407853 Test RE 1.3256862719313345\n",
      "59 Train Loss 80.818665 Test MSE 30.456564263719326 Test RE 1.3189201995031719\n",
      "60 Train Loss 80.066475 Test MSE 30.633174687569724 Test RE 1.3227387252098253\n",
      "61 Train Loss 79.92621 Test MSE 30.582541190814286 Test RE 1.3216450973670384\n",
      "62 Train Loss 79.40763 Test MSE 30.444129061855275 Test RE 1.3186509190743194\n",
      "63 Train Loss 78.609886 Test MSE 30.41489142080802 Test RE 1.3180175703062176\n",
      "64 Train Loss 78.12654 Test MSE 30.245424320139204 Test RE 1.3143405455265689\n",
      "65 Train Loss 77.52807 Test MSE 30.109352807029403 Test RE 1.3113806612503778\n",
      "66 Train Loss 77.03847 Test MSE 30.150729957479456 Test RE 1.3122814206628195\n",
      "67 Train Loss 75.8027 Test MSE 29.896020225658148 Test RE 1.30672666668911\n",
      "68 Train Loss 74.5317 Test MSE 29.808964460629316 Test RE 1.304822717198466\n",
      "69 Train Loss 72.70524 Test MSE 29.702147224522648 Test RE 1.3024827727531998\n",
      "70 Train Loss 70.33809 Test MSE 29.17719046403123 Test RE 1.2909213985733052\n",
      "71 Train Loss 68.91743 Test MSE 28.95775903147583 Test RE 1.2860579533493763\n",
      "72 Train Loss 65.782295 Test MSE 28.358398982612936 Test RE 1.2726791195471299\n",
      "73 Train Loss 64.30043 Test MSE 28.124914949404932 Test RE 1.2674290978713658\n",
      "74 Train Loss 63.31939 Test MSE 28.325407220296107 Test RE 1.2719385956164042\n",
      "75 Train Loss 62.84967 Test MSE 28.209676840191776 Test RE 1.2693375279367374\n",
      "76 Train Loss 62.363113 Test MSE 28.27569776815274 Test RE 1.2708220161667412\n",
      "77 Train Loss 61.533318 Test MSE 28.200448850094887 Test RE 1.2691298971955323\n",
      "78 Train Loss 61.1197 Test MSE 28.222137285609044 Test RE 1.2696178353803067\n",
      "79 Train Loss 60.827633 Test MSE 28.261006843981907 Test RE 1.2704918390718078\n",
      "80 Train Loss 60.36254 Test MSE 28.321900056129813 Test RE 1.271859849430661\n",
      "81 Train Loss 60.23565 Test MSE 28.28676531347581 Test RE 1.271070701527986\n",
      "82 Train Loss 60.080544 Test MSE 28.358714605672933 Test RE 1.2726862018538692\n",
      "83 Train Loss 59.963673 Test MSE 28.299450669315252 Test RE 1.2713556789131721\n",
      "84 Train Loss 59.86496 Test MSE 28.40210863890035 Test RE 1.2736595512703217\n",
      "85 Train Loss 59.494457 Test MSE 28.257197026052573 Test RE 1.2704061997710776\n",
      "86 Train Loss 59.420654 Test MSE 28.301258367421088 Test RE 1.2713962837747186\n",
      "87 Train Loss 59.331955 Test MSE 28.309419496736695 Test RE 1.2715795844858042\n",
      "88 Train Loss 59.1942 Test MSE 28.23383736555912 Test RE 1.2698809814839127\n",
      "89 Train Loss 58.997276 Test MSE 28.312549820128442 Test RE 1.2716498852083131\n",
      "90 Train Loss 58.936768 Test MSE 28.315191490490516 Test RE 1.27170920874998\n",
      "91 Train Loss 58.871544 Test MSE 28.326088267622858 Test RE 1.2719538865735067\n",
      "92 Train Loss 58.83945 Test MSE 28.306752854382736 Test RE 1.271519694034291\n",
      "93 Train Loss 58.804356 Test MSE 28.27741242282444 Test RE 1.2708605472737629\n",
      "94 Train Loss 58.751152 Test MSE 28.33594464733901 Test RE 1.2721751626278193\n",
      "95 Train Loss 58.70694 Test MSE 28.35015801400561 Test RE 1.2724941854337597\n",
      "96 Train Loss 58.67873 Test MSE 28.340403589474416 Test RE 1.2722752533847395\n",
      "97 Train Loss 58.543865 Test MSE 28.400689657251913 Test RE 1.273627734581524\n",
      "98 Train Loss 58.49166 Test MSE 28.369786537601186 Test RE 1.2729346214328368\n",
      "99 Train Loss 58.46153 Test MSE 28.393801125976005 Test RE 1.2734732672666778\n",
      "100 Train Loss 58.35429 Test MSE 28.313516314248602 Test RE 1.2716715899232915\n",
      "101 Train Loss 58.297836 Test MSE 28.319772266316463 Test RE 1.2718120718935342\n",
      "102 Train Loss 58.26097 Test MSE 28.338576924230562 Test RE 1.2722342508205342\n",
      "103 Train Loss 58.21033 Test MSE 28.312675180006632 Test RE 1.2716527004563085\n",
      "104 Train Loss 58.125034 Test MSE 28.332120618574674 Test RE 1.2720893176239183\n",
      "105 Train Loss 58.066452 Test MSE 28.325918571119775 Test RE 1.2719500765442529\n",
      "106 Train Loss 58.049397 Test MSE 28.320312146513114 Test RE 1.271824194570098\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "107 Train Loss 58.027218 Test MSE 28.332174681584817 Test RE 1.27209053131607\n",
      "108 Train Loss 58.002884 Test MSE 28.319300515038062 Test RE 1.2718014789151761\n",
      "109 Train Loss 57.962414 Test MSE 28.34874560551768 Test RE 1.272462487127648\n",
      "110 Train Loss 57.934692 Test MSE 28.326030928114278 Test RE 1.2719525991869989\n",
      "111 Train Loss 57.905907 Test MSE 28.324428871053307 Test RE 1.271916629278944\n",
      "112 Train Loss 57.869923 Test MSE 28.333196382926257 Test RE 1.2721134678696497\n",
      "113 Train Loss 57.826637 Test MSE 28.329171053681847 Test RE 1.2720230993605135\n",
      "114 Train Loss 57.739044 Test MSE 28.37893691786871 Test RE 1.2731398908090386\n",
      "115 Train Loss 57.693047 Test MSE 28.38706524416208 Test RE 1.2733222048185222\n",
      "116 Train Loss 57.62892 Test MSE 28.397454536640275 Test RE 1.2735551930917046\n",
      "117 Train Loss 57.6046 Test MSE 28.37389279942624 Test RE 1.2730267408026557\n",
      "118 Train Loss 57.585083 Test MSE 28.370712697007384 Test RE 1.2729553993606118\n",
      "119 Train Loss 57.573265 Test MSE 28.36724009465989 Test RE 1.2728774914978287\n",
      "120 Train Loss 57.55304 Test MSE 28.380329364911265 Test RE 1.2731711245042168\n",
      "121 Train Loss 57.529762 Test MSE 28.360819208786484 Test RE 1.27273342630827\n",
      "122 Train Loss 57.49122 Test MSE 28.31925962884833 Test RE 1.2718005608286698\n",
      "123 Train Loss 57.469406 Test MSE 28.309479215338357 Test RE 1.271580925680997\n",
      "124 Train Loss 57.44503 Test MSE 28.28389223589077 Test RE 1.271006148769196\n",
      "125 Train Loss 57.418552 Test MSE 28.31465734052564 Test RE 1.2716972136583518\n",
      "126 Train Loss 57.400494 Test MSE 28.285342425286444 Test RE 1.2710387322622594\n",
      "127 Train Loss 57.37661 Test MSE 28.271115709913847 Test RE 1.2707190440587004\n",
      "128 Train Loss 57.342613 Test MSE 28.249697467575622 Test RE 1.2702376034694567\n",
      "129 Train Loss 57.309498 Test MSE 28.23850927749041 Test RE 1.2699860420646822\n",
      "130 Train Loss 57.2931 Test MSE 28.239386256969514 Test RE 1.2700057623523155\n",
      "131 Train Loss 57.277626 Test MSE 28.24050922602407 Test RE 1.2700310136622974\n",
      "132 Train Loss 57.263557 Test MSE 28.239608236751742 Test RE 1.2700107538745453\n",
      "133 Train Loss 57.24971 Test MSE 28.232727315113003 Test RE 1.2698560177181408\n",
      "134 Train Loss 57.24044 Test MSE 28.224574019145066 Test RE 1.26967264436855\n",
      "135 Train Loss 57.226223 Test MSE 28.248720597143887 Test RE 1.2702156409637535\n",
      "136 Train Loss 57.18747 Test MSE 28.208928604855434 Test RE 1.2693206938287913\n",
      "137 Train Loss 57.15117 Test MSE 28.190064656300287 Test RE 1.2688962111236282\n",
      "138 Train Loss 57.13557 Test MSE 28.195028121893536 Test RE 1.2690079144055575\n",
      "139 Train Loss 57.125576 Test MSE 28.187300448442148 Test RE 1.2688339980902827\n",
      "140 Train Loss 57.113483 Test MSE 28.178374986980554 Test RE 1.2686330951052887\n",
      "141 Train Loss 57.089897 Test MSE 28.186987264829156 Test RE 1.2688269491868172\n",
      "142 Train Loss 57.07136 Test MSE 28.190021034719795 Test RE 1.2688952293720728\n",
      "143 Train Loss 57.060905 Test MSE 28.18499440733768 Test RE 1.2687820945174875\n",
      "144 Train Loss 57.049263 Test MSE 28.189347077264486 Test RE 1.2688800611231705\n",
      "145 Train Loss 57.02352 Test MSE 28.194813324778572 Test RE 1.2690030805788834\n",
      "146 Train Loss 57.00423 Test MSE 28.2258499205067 Test RE 1.2697013420336978\n",
      "147 Train Loss 56.99303 Test MSE 28.19839712351259 Test RE 1.269083728517085\n",
      "148 Train Loss 56.98268 Test MSE 28.20514488888971 Test RE 1.2692355628033745\n",
      "149 Train Loss 56.95892 Test MSE 28.191006764347755 Test RE 1.2689174141140933\n",
      "150 Train Loss 56.924644 Test MSE 28.223645005148914 Test RE 1.2696517485108714\n",
      "151 Train Loss 56.904106 Test MSE 28.244925417973324 Test RE 1.2701303121813299\n",
      "152 Train Loss 56.882263 Test MSE 28.241155294180622 Test RE 1.270045541054427\n",
      "153 Train Loss 56.858513 Test MSE 28.24531713266855 Test RE 1.270139119550721\n",
      "154 Train Loss 56.845303 Test MSE 28.25713314581603 Test RE 1.2704047637847984\n",
      "155 Train Loss 56.836605 Test MSE 28.249026655127864 Test RE 1.27022252195831\n",
      "156 Train Loss 56.82432 Test MSE 28.24492913322255 Test RE 1.2701303957158134\n",
      "157 Train Loss 56.81112 Test MSE 28.22371180642063 Test RE 1.2696532510506962\n",
      "158 Train Loss 56.79061 Test MSE 28.246289487885676 Test RE 1.270160981860935\n",
      "159 Train Loss 56.773956 Test MSE 28.252320240258403 Test RE 1.2702965681215062\n",
      "160 Train Loss 56.73718 Test MSE 28.241857303567492 Test RE 1.2700613261443372\n",
      "161 Train Loss 56.705883 Test MSE 28.23906472783894 Test RE 1.2699985322905991\n",
      "162 Train Loss 56.695 Test MSE 28.22800621041715 Test RE 1.2697498399870633\n",
      "163 Train Loss 56.68387 Test MSE 28.22280478671318 Test RE 1.2696328495921534\n",
      "164 Train Loss 56.675755 Test MSE 28.21966802238197 Test RE 1.2695622922940693\n",
      "165 Train Loss 56.654236 Test MSE 28.210728291332757 Test RE 1.269361183541929\n",
      "166 Train Loss 56.639965 Test MSE 28.189066541265255 Test RE 1.2688737472589204\n",
      "167 Train Loss 56.630493 Test MSE 28.194991104666357 Test RE 1.2690070813656067\n",
      "168 Train Loss 56.625423 Test MSE 28.19753125742789 Test RE 1.2690642439876993\n",
      "169 Train Loss 56.617443 Test MSE 28.203061932482708 Test RE 1.2691886952696256\n",
      "170 Train Loss 56.6099 Test MSE 28.191910132225537 Test RE 1.2689377448894008\n",
      "171 Train Loss 56.604546 Test MSE 28.183596431346103 Test RE 1.2687506283253034\n",
      "172 Train Loss 56.590843 Test MSE 28.17166253495799 Test RE 1.2684819837302859\n",
      "173 Train Loss 56.574684 Test MSE 28.17734910736869 Test RE 1.268610001568605\n",
      "174 Train Loss 56.558083 Test MSE 28.131018696115294 Test RE 1.2675666208918013\n",
      "175 Train Loss 56.54369 Test MSE 28.12606953494564 Test RE 1.267455112888563\n",
      "176 Train Loss 56.541832 Test MSE 28.117695742864914 Test RE 1.2672664232521298\n",
      "177 Train Loss 56.541832 Test MSE 28.117695742864914 Test RE 1.2672664232521298\n",
      "178 Train Loss 56.541832 Test MSE 28.117695742864914 Test RE 1.2672664232521298\n",
      "179 Train Loss 56.541832 Test MSE 28.117695742864914 Test RE 1.2672664232521298\n",
      "180 Train Loss 56.541832 Test MSE 28.117695742864914 Test RE 1.2672664232521298\n",
      "181 Train Loss 56.541832 Test MSE 28.117695742864914 Test RE 1.2672664232521298\n",
      "182 Train Loss 56.541832 Test MSE 28.117695742864914 Test RE 1.2672664232521298\n",
      "183 Train Loss 56.541832 Test MSE 28.117695742864914 Test RE 1.2672664232521298\n",
      "184 Train Loss 56.541832 Test MSE 28.117695742864914 Test RE 1.2672664232521298\n",
      "185 Train Loss 56.541832 Test MSE 28.117695742864914 Test RE 1.2672664232521298\n",
      "186 Train Loss 56.541832 Test MSE 28.117695742864914 Test RE 1.2672664232521298\n",
      "187 Train Loss 56.541832 Test MSE 28.117695742864914 Test RE 1.2672664232521298\n",
      "188 Train Loss 56.541832 Test MSE 28.117695742864914 Test RE 1.2672664232521298\n",
      "189 Train Loss 56.541832 Test MSE 28.117695742864914 Test RE 1.2672664232521298\n",
      "190 Train Loss 56.541832 Test MSE 28.117695742864914 Test RE 1.2672664232521298\n",
      "191 Train Loss 56.541832 Test MSE 28.117695742864914 Test RE 1.2672664232521298\n",
      "192 Train Loss 56.541832 Test MSE 28.117695742864914 Test RE 1.2672664232521298\n",
      "193 Train Loss 56.541832 Test MSE 28.117695742864914 Test RE 1.2672664232521298\n",
      "194 Train Loss 56.541832 Test MSE 28.117695742864914 Test RE 1.2672664232521298\n",
      "195 Train Loss 56.541832 Test MSE 28.117695742864914 Test RE 1.2672664232521298\n",
      "196 Train Loss 56.541832 Test MSE 28.117695742864914 Test RE 1.2672664232521298\n",
      "197 Train Loss 56.541832 Test MSE 28.117695742864914 Test RE 1.2672664232521298\n",
      "198 Train Loss 56.541832 Test MSE 28.117695742864914 Test RE 1.2672664232521298\n",
      "199 Train Loss 56.541832 Test MSE 28.117695742864914 Test RE 1.2672664232521298\n",
      "Training time: 222.24\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 649.1416 Test MSE 38.11659136790596 Test RE 1.4754859988462719\n",
      "1 Train Loss 490.40915 Test MSE 34.584592410579404 Test RE 1.4054629306485857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Train Loss 403.4451 Test MSE 35.07142510154481 Test RE 1.4153204158062855\n",
      "3 Train Loss 395.40167 Test MSE 35.576865380899335 Test RE 1.4254825485903098\n",
      "4 Train Loss 384.62067 Test MSE 35.38143440386782 Test RE 1.4215619238028028\n",
      "5 Train Loss 376.09125 Test MSE 35.158689495459384 Test RE 1.4170801154202015\n",
      "6 Train Loss 371.16574 Test MSE 35.15863480024714 Test RE 1.4170790131674016\n",
      "7 Train Loss 363.2605 Test MSE 35.69688789771348 Test RE 1.4278850360835518\n",
      "8 Train Loss 340.19904 Test MSE 36.50552889116273 Test RE 1.4439673954677528\n",
      "9 Train Loss 324.15747 Test MSE 36.749748414058644 Test RE 1.4487893664247924\n",
      "10 Train Loss 285.08038 Test MSE 37.056871080469655 Test RE 1.4548306360553476\n",
      "11 Train Loss 240.98724 Test MSE 36.9182975876487 Test RE 1.452107931995148\n",
      "12 Train Loss 220.19339 Test MSE 36.76121850428904 Test RE 1.4490154420990733\n",
      "13 Train Loss 208.63611 Test MSE 36.681174656322746 Test RE 1.4474370398917003\n",
      "14 Train Loss 197.82935 Test MSE 37.23011246231039 Test RE 1.4582273473802398\n",
      "15 Train Loss 169.45102 Test MSE 37.670448102826015 Test RE 1.4668255217337491\n",
      "16 Train Loss 142.53967 Test MSE 38.60536184272182 Test RE 1.4849159709568083\n",
      "17 Train Loss 131.18616 Test MSE 38.915428922208655 Test RE 1.4908672525071969\n",
      "18 Train Loss 124.46835 Test MSE 39.11274523724051 Test RE 1.4946421111030754\n",
      "19 Train Loss 119.42712 Test MSE 39.00708477176642 Test RE 1.4926219079672858\n",
      "20 Train Loss 116.80386 Test MSE 38.71786610751539 Test RE 1.4870780779411248\n",
      "21 Train Loss 114.14255 Test MSE 38.6893210641148 Test RE 1.4865297970137714\n",
      "22 Train Loss 111.62692 Test MSE 37.96518666679935 Test RE 1.4725526594492004\n",
      "23 Train Loss 109.62291 Test MSE 37.59114763557603 Test RE 1.4652807932234295\n",
      "24 Train Loss 108.07294 Test MSE 37.56216012018017 Test RE 1.4647157261298458\n",
      "25 Train Loss 106.90062 Test MSE 37.207695857175906 Test RE 1.4577882749939721\n",
      "26 Train Loss 105.90036 Test MSE 37.16480439027811 Test RE 1.4569477942006228\n",
      "27 Train Loss 105.30912 Test MSE 37.08662224051682 Test RE 1.4554145252759547\n",
      "28 Train Loss 104.46714 Test MSE 37.08944471524552 Test RE 1.4554699063313719\n",
      "29 Train Loss 103.80848 Test MSE 37.099649362079894 Test RE 1.4556701187504022\n",
      "30 Train Loss 103.12907 Test MSE 37.048188451065165 Test RE 1.4546601886470798\n",
      "31 Train Loss 102.58373 Test MSE 36.83495741151703 Test RE 1.4504679957897624\n",
      "32 Train Loss 102.05005 Test MSE 36.97465315901017 Test RE 1.4532158266086435\n",
      "33 Train Loss 101.295975 Test MSE 36.75427719494291 Test RE 1.4488786329877352\n",
      "34 Train Loss 100.50835 Test MSE 36.78443501281695 Test RE 1.449472931960899\n",
      "35 Train Loss 99.966995 Test MSE 36.70147179238813 Test RE 1.4478374464194221\n",
      "36 Train Loss 99.386154 Test MSE 36.511926763883814 Test RE 1.4440939230808865\n",
      "37 Train Loss 98.72832 Test MSE 36.23039302864622 Test RE 1.4385156374403698\n",
      "38 Train Loss 98.18556 Test MSE 35.961668615588195 Test RE 1.4331709019158245\n",
      "39 Train Loss 97.55337 Test MSE 35.86207316605683 Test RE 1.4311849503433889\n",
      "40 Train Loss 96.87262 Test MSE 35.750227628922175 Test RE 1.4289514396033436\n",
      "41 Train Loss 96.36351 Test MSE 35.658005736357005 Test RE 1.4271071757964666\n",
      "42 Train Loss 95.671936 Test MSE 35.5131435667209 Test RE 1.424205383739351\n",
      "43 Train Loss 94.93723 Test MSE 35.20261778695396 Test RE 1.4179651096733925\n",
      "44 Train Loss 94.27811 Test MSE 34.54890346314457 Test RE 1.40473757238738\n",
      "45 Train Loss 93.28779 Test MSE 34.03340035937509 Test RE 1.3942181623588208\n",
      "46 Train Loss 91.88757 Test MSE 33.548177734205595 Test RE 1.3842436253954193\n",
      "47 Train Loss 89.78765 Test MSE 32.50809518112151 Test RE 1.3626170723357622\n",
      "48 Train Loss 87.92769 Test MSE 32.22637653273338 Test RE 1.356699931761984\n",
      "49 Train Loss 86.48566 Test MSE 31.873352927668673 Test RE 1.3492484891545988\n",
      "50 Train Loss 85.20461 Test MSE 31.594485432361374 Test RE 1.3433330749842312\n",
      "51 Train Loss 84.33532 Test MSE 31.61872887655753 Test RE 1.3438483670512673\n",
      "52 Train Loss 83.47269 Test MSE 31.43852872643476 Test RE 1.3400134928761012\n",
      "53 Train Loss 82.695854 Test MSE 31.35258102807117 Test RE 1.3381805524910075\n",
      "54 Train Loss 82.0549 Test MSE 31.06852841710731 Test RE 1.332104839713813\n",
      "55 Train Loss 81.50668 Test MSE 30.662175771375757 Test RE 1.3233647096539478\n",
      "56 Train Loss 80.95423 Test MSE 30.352851034258236 Test RE 1.31667263601354\n",
      "57 Train Loss 80.30531 Test MSE 30.01338855979549 Test RE 1.3092891833557645\n",
      "58 Train Loss 75.58491 Test MSE 23.683740461580097 Test RE 1.1630629231203993\n",
      "59 Train Loss 65.6016 Test MSE 22.791959869175432 Test RE 1.140956010109171\n",
      "60 Train Loss 60.869415 Test MSE 22.33211007852107 Test RE 1.1293874164750508\n",
      "61 Train Loss 58.33778 Test MSE 22.326195868228908 Test RE 1.1292378587989982\n",
      "62 Train Loss 50.775555 Test MSE 21.34814272353992 Test RE 1.1042263677802406\n",
      "63 Train Loss 44.440784 Test MSE 21.515459524295498 Test RE 1.1085451282019818\n",
      "64 Train Loss 42.496677 Test MSE 21.363081310145834 Test RE 1.1046126472030333\n",
      "65 Train Loss 41.32254 Test MSE 21.350922030785075 Test RE 1.1042982448634842\n",
      "66 Train Loss 40.43221 Test MSE 21.397746192856225 Test RE 1.1055084858351838\n",
      "67 Train Loss 39.82076 Test MSE 21.4059127117054 Test RE 1.105719426150212\n",
      "68 Train Loss 39.356327 Test MSE 21.4111888897028 Test RE 1.1058556878723287\n",
      "69 Train Loss 39.00509 Test MSE 21.446440672933786 Test RE 1.1067656642553687\n",
      "70 Train Loss 38.631325 Test MSE 21.4593970063522 Test RE 1.1070999262455716\n",
      "71 Train Loss 38.348072 Test MSE 21.402975102428908 Test RE 1.105643552648942\n",
      "72 Train Loss 38.113857 Test MSE 21.440080031791297 Test RE 1.1066015283651764\n",
      "73 Train Loss 37.928764 Test MSE 21.4436708006456 Test RE 1.1066941908911185\n",
      "74 Train Loss 37.76547 Test MSE 21.48376365054464 Test RE 1.1077282909797324\n",
      "75 Train Loss 37.561333 Test MSE 21.49431378419866 Test RE 1.108000246287608\n",
      "76 Train Loss 37.358276 Test MSE 21.570430381130052 Test RE 1.1099603617920615\n",
      "77 Train Loss 37.250847 Test MSE 21.623611566040395 Test RE 1.1113278048023696\n",
      "78 Train Loss 37.148026 Test MSE 21.644954190231324 Test RE 1.1118761128843662\n",
      "79 Train Loss 37.067623 Test MSE 21.702430219495568 Test RE 1.1133513725572288\n",
      "80 Train Loss 36.986835 Test MSE 21.745075666626192 Test RE 1.1144447078466722\n",
      "81 Train Loss 36.866398 Test MSE 21.79122661278202 Test RE 1.115626709181371\n",
      "82 Train Loss 36.781155 Test MSE 21.84417960109942 Test RE 1.1169813810396443\n",
      "83 Train Loss 36.731224 Test MSE 21.92264180469627 Test RE 1.1189856280243053\n",
      "84 Train Loss 36.654408 Test MSE 21.98727116608588 Test RE 1.1206338350492644\n",
      "85 Train Loss 36.575718 Test MSE 22.032982285028584 Test RE 1.1217981184568322\n",
      "86 Train Loss 36.513367 Test MSE 22.11062855448466 Test RE 1.1237730402498054\n",
      "87 Train Loss 36.43605 Test MSE 22.18161999561904 Test RE 1.1255756650203217\n",
      "88 Train Loss 36.366844 Test MSE 22.210516398614878 Test RE 1.1263085804287127\n",
      "89 Train Loss 36.283623 Test MSE 22.225308994450067 Test RE 1.1266835886929474\n",
      "90 Train Loss 36.19189 Test MSE 22.298617822398967 Test RE 1.128540207562124\n",
      "91 Train Loss 36.09288 Test MSE 22.29357295214514 Test RE 1.128412539089074\n",
      "92 Train Loss 35.9739 Test MSE 22.38572074657839 Test RE 1.1307422127679603\n",
      "93 Train Loss 35.830513 Test MSE 22.46305679821872 Test RE 1.132693718893669\n",
      "94 Train Loss 35.705826 Test MSE 22.454580679481293 Test RE 1.132479995703683\n",
      "95 Train Loss 35.531578 Test MSE 22.52568012910826 Test RE 1.1342715024861585\n",
      "96 Train Loss 35.394485 Test MSE 22.57328778934736 Test RE 1.1354695020598278\n",
      "97 Train Loss 35.268604 Test MSE 22.56276861152178 Test RE 1.1352049061899916\n",
      "98 Train Loss 35.128475 Test MSE 22.542363097964433 Test RE 1.1346914568149038\n",
      "99 Train Loss 34.932472 Test MSE 22.55552062649446 Test RE 1.135022556913842\n",
      "100 Train Loss 34.751854 Test MSE 22.568316087093912 Test RE 1.1353444532143797\n",
      "101 Train Loss 34.50698 Test MSE 22.71379393661418 Test RE 1.1389978527725486\n",
      "102 Train Loss 34.262287 Test MSE 22.78559551955441 Test RE 1.1407967006426267\n",
      "103 Train Loss 33.99162 Test MSE 22.820830069141092 Test RE 1.1416783965106256\n",
      "104 Train Loss 33.69795 Test MSE 22.787005297516036 Test RE 1.1408319914729708\n",
      "105 Train Loss 33.256096 Test MSE 22.793959676417277 Test RE 1.1410060637696942\n",
      "106 Train Loss 32.65397 Test MSE 22.883448106565957 Test RE 1.1432436482437744\n",
      "107 Train Loss 31.965576 Test MSE 22.929587369409948 Test RE 1.144395613124955\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "108 Train Loss 31.172094 Test MSE 22.859104137034702 Test RE 1.1426353812073287\n",
      "109 Train Loss 30.446796 Test MSE 22.99719644922637 Test RE 1.146081526254024\n",
      "110 Train Loss 29.800304 Test MSE 23.036881414392195 Test RE 1.1470699641477315\n",
      "111 Train Loss 28.935 Test MSE 23.171000718751884 Test RE 1.150404203588284\n",
      "112 Train Loss 27.824135 Test MSE 23.206686020564234 Test RE 1.151289722685622\n",
      "113 Train Loss 26.768986 Test MSE 23.154272083209985 Test RE 1.1499888532192803\n",
      "114 Train Loss 25.485334 Test MSE 23.050773815437648 Test RE 1.1474157825317552\n",
      "115 Train Loss 24.407883 Test MSE 23.049426353017942 Test RE 1.1473822452142097\n",
      "116 Train Loss 23.24024 Test MSE 22.804739311566976 Test RE 1.1412758320639171\n",
      "117 Train Loss 22.142683 Test MSE 22.70625616170763 Test RE 1.138808843796992\n",
      "118 Train Loss 21.55465 Test MSE 22.730799955231237 Test RE 1.1394241618803758\n",
      "119 Train Loss 21.180962 Test MSE 22.79044195137941 Test RE 1.140918016310516\n",
      "120 Train Loss 20.770058 Test MSE 22.905269416942417 Test RE 1.1437886082933637\n",
      "121 Train Loss 20.428196 Test MSE 22.852506837518778 Test RE 1.1424704829728463\n",
      "122 Train Loss 20.122164 Test MSE 22.775565041958362 Test RE 1.1405455771791124\n",
      "123 Train Loss 19.81401 Test MSE 22.81208265853256 Test RE 1.1414595682105786\n",
      "124 Train Loss 19.40166 Test MSE 22.814142205613912 Test RE 1.1415110943403566\n",
      "125 Train Loss 18.823086 Test MSE 22.569851054593595 Test RE 1.1353830623688803\n",
      "126 Train Loss 18.253344 Test MSE 22.5298996330192 Test RE 1.1343777332171148\n",
      "127 Train Loss 17.575903 Test MSE 22.450454998943695 Test RE 1.132375953117066\n",
      "128 Train Loss 16.924583 Test MSE 22.48181905509591 Test RE 1.133166661087212\n",
      "129 Train Loss 16.001572 Test MSE 22.354049172422712 Test RE 1.129942036124996\n",
      "130 Train Loss 15.246559 Test MSE 22.355244594880784 Test RE 1.1299722485491035\n",
      "131 Train Loss 14.305395 Test MSE 22.430312716061902 Test RE 1.1318678619673903\n",
      "132 Train Loss 12.860489 Test MSE 22.36267683754978 Test RE 1.1301600686827729\n",
      "133 Train Loss 11.462558 Test MSE 22.24226217680893 Test RE 1.1271132167187645\n",
      "134 Train Loss 10.072189 Test MSE 22.203902769227867 Test RE 1.126140877391178\n",
      "135 Train Loss 9.065378 Test MSE 21.926227417287883 Test RE 1.119077133531454\n",
      "136 Train Loss 7.9492216 Test MSE 21.996353996963208 Test RE 1.120865275244019\n",
      "137 Train Loss 7.1154556 Test MSE 21.808580429715 Test RE 1.1160708450040508\n",
      "138 Train Loss 6.1126165 Test MSE 21.518758416993233 Test RE 1.1086301096837246\n",
      "139 Train Loss 5.160195 Test MSE 21.430536199136462 Test RE 1.1063552047657994\n",
      "140 Train Loss 4.6669426 Test MSE 21.368889763223383 Test RE 1.1047628047242066\n",
      "141 Train Loss 4.2564874 Test MSE 21.39844266778749 Test RE 1.10552647727864\n",
      "142 Train Loss 3.9697585 Test MSE 21.296305933224236 Test RE 1.1028849316555125\n",
      "143 Train Loss 3.7470758 Test MSE 21.29094255136389 Test RE 1.1027460445280728\n",
      "144 Train Loss 3.600038 Test MSE 21.321975417083674 Test RE 1.1035494121802807\n",
      "145 Train Loss 3.473358 Test MSE 21.32584206703387 Test RE 1.1036494696505985\n",
      "146 Train Loss 3.3832178 Test MSE 21.349418506072247 Test RE 1.1042593620243668\n",
      "147 Train Loss 3.2897334 Test MSE 21.335891156355576 Test RE 1.1039094679626518\n",
      "148 Train Loss 3.2043395 Test MSE 21.300392199490286 Test RE 1.102990735576262\n",
      "149 Train Loss 3.139217 Test MSE 21.230549879572 Test RE 1.1011809406408077\n",
      "150 Train Loss 3.066608 Test MSE 21.192934049038737 Test RE 1.1002049838300931\n",
      "151 Train Loss 2.983191 Test MSE 21.105859791505672 Test RE 1.0979424812400171\n",
      "152 Train Loss 2.9408422 Test MSE 21.083517494256352 Test RE 1.097361195927195\n",
      "153 Train Loss 2.9145255 Test MSE 21.043969199451748 Test RE 1.0963315021161395\n",
      "154 Train Loss 2.8663929 Test MSE 20.957843163584403 Test RE 1.0940857400685415\n",
      "155 Train Loss 2.8308163 Test MSE 20.933163811559638 Test RE 1.0934413683983883\n",
      "156 Train Loss 2.7967436 Test MSE 20.923680356104608 Test RE 1.0931936567453022\n",
      "157 Train Loss 2.7592556 Test MSE 20.882836046237944 Test RE 1.0921261449299489\n",
      "158 Train Loss 2.7276483 Test MSE 20.88252091348128 Test RE 1.092117904525716\n",
      "159 Train Loss 2.6916642 Test MSE 20.853509954479183 Test RE 1.0913590306440546\n",
      "160 Train Loss 2.6662552 Test MSE 20.879756771147274 Test RE 1.0920456223226096\n",
      "161 Train Loss 2.632928 Test MSE 20.833859466192692 Test RE 1.090844709735623\n",
      "162 Train Loss 2.6077905 Test MSE 20.80730474862059 Test RE 1.0901492958726164\n",
      "163 Train Loss 2.5893955 Test MSE 20.800556386981434 Test RE 1.089972499349234\n",
      "164 Train Loss 2.5658922 Test MSE 20.77103371257311 Test RE 1.0891987140529442\n",
      "165 Train Loss 2.5351493 Test MSE 20.77864143571552 Test RE 1.0893981640121435\n",
      "166 Train Loss 2.5016575 Test MSE 20.775195530113425 Test RE 1.089307828007766\n",
      "167 Train Loss 2.4821556 Test MSE 20.78012028827636 Test RE 1.0894369305168052\n",
      "168 Train Loss 2.4636865 Test MSE 20.797433560274662 Test RE 1.0898906764659557\n",
      "169 Train Loss 2.4470565 Test MSE 20.80237146991224 Test RE 1.0900200545031558\n",
      "170 Train Loss 2.42897 Test MSE 20.800089309965667 Test RE 1.0899602616006183\n",
      "171 Train Loss 2.414121 Test MSE 20.775388956017657 Test RE 1.089312898955495\n",
      "172 Train Loss 2.393724 Test MSE 20.760798845039204 Test RE 1.0889303312100058\n",
      "173 Train Loss 2.3761926 Test MSE 20.779953469617926 Test RE 1.0894325576169035\n",
      "174 Train Loss 2.3638244 Test MSE 20.769824914362857 Test RE 1.0891670199006314\n",
      "175 Train Loss 2.3431726 Test MSE 20.776382396269465 Test RE 1.0893389430971094\n",
      "176 Train Loss 2.3287027 Test MSE 20.781483856983055 Test RE 1.0894726737588312\n",
      "177 Train Loss 2.3146899 Test MSE 20.78373517280807 Test RE 1.089531684959947\n",
      "178 Train Loss 2.3013597 Test MSE 20.794057799742347 Test RE 1.0898022194159438\n",
      "179 Train Loss 2.2912674 Test MSE 20.76390400823822 Test RE 1.0890117630467855\n",
      "180 Train Loss 2.281858 Test MSE 20.77156968378079 Test RE 1.089212766684902\n",
      "181 Train Loss 2.2700226 Test MSE 20.772547807107436 Test RE 1.0892384116382972\n",
      "182 Train Loss 2.258052 Test MSE 20.789139458158605 Test RE 1.0896733283498197\n",
      "183 Train Loss 2.2442377 Test MSE 20.790823378606213 Test RE 1.0897174592299559\n",
      "184 Train Loss 2.2350433 Test MSE 20.78813127889701 Test RE 1.089646905914846\n",
      "185 Train Loss 2.2272272 Test MSE 20.802216062520817 Test RE 1.0900159829123304\n",
      "186 Train Loss 2.2164817 Test MSE 20.805927850979042 Test RE 1.0901132256356134\n",
      "187 Train Loss 2.2072463 Test MSE 20.812656648978404 Test RE 1.09028948691026\n",
      "188 Train Loss 2.1971247 Test MSE 20.780995290036444 Test RE 1.089459867082711\n",
      "189 Train Loss 2.1860006 Test MSE 20.786623650828552 Test RE 1.089607392693956\n",
      "190 Train Loss 2.1780877 Test MSE 20.79421608404498 Test RE 1.0898063671935854\n",
      "191 Train Loss 2.1694732 Test MSE 20.78606588965572 Test RE 1.0895927740434148\n",
      "192 Train Loss 2.1604238 Test MSE 20.77548079640242 Test RE 1.089315306679495\n",
      "193 Train Loss 2.1488187 Test MSE 20.754464309283673 Test RE 1.0887641913032629\n",
      "194 Train Loss 2.138751 Test MSE 20.748333550073212 Test RE 1.0886033718283892\n",
      "195 Train Loss 2.1269133 Test MSE 20.750644161974797 Test RE 1.0886639856091411\n",
      "196 Train Loss 2.115228 Test MSE 20.725352642656148 Test RE 1.0880003348584646\n",
      "197 Train Loss 2.1083407 Test MSE 20.719526730612152 Test RE 1.087847405258845\n",
      "198 Train Loss 2.0986845 Test MSE 20.710870581968262 Test RE 1.0876201425255552\n",
      "199 Train Loss 2.0917125 Test MSE 20.699526641264477 Test RE 1.0873222412790893\n",
      "Training time: 246.49\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 553.3424 Test MSE 35.018016360868295 Test RE 1.414242340405348\n",
      "1 Train Loss 433.10165 Test MSE 35.14474958339089 Test RE 1.416799161679948\n",
      "2 Train Loss 393.3264 Test MSE 34.6378983598872 Test RE 1.4065456481823688\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3 Train Loss 389.5912 Test MSE 34.652654369791684 Test RE 1.4068452158888645\n",
      "4 Train Loss 382.80798 Test MSE 34.40804436765421 Test RE 1.4018710246846622\n",
      "5 Train Loss 369.49597 Test MSE 34.311304904755836 Test RE 1.3998989319776838\n",
      "6 Train Loss 359.50647 Test MSE 33.553454618704976 Test RE 1.3843524868449852\n",
      "7 Train Loss 351.8464 Test MSE 33.726884869058026 Test RE 1.387925578786241\n",
      "8 Train Loss 348.5437 Test MSE 33.51300576628097 Test RE 1.383517813396874\n",
      "9 Train Loss 341.3212 Test MSE 32.74318915333368 Test RE 1.3675353238387367\n",
      "10 Train Loss 317.88477 Test MSE 33.33725440830122 Test RE 1.3798852719314922\n",
      "11 Train Loss 289.07864 Test MSE 31.87612383377343 Test RE 1.3493071362576023\n",
      "12 Train Loss 256.52808 Test MSE 30.932611465288005 Test RE 1.329187835094941\n",
      "13 Train Loss 220.01912 Test MSE 32.19815311530765 Test RE 1.3561057120589273\n",
      "14 Train Loss 197.6036 Test MSE 32.025267890198165 Test RE 1.3524600649951768\n",
      "15 Train Loss 177.2105 Test MSE 32.27702651499216 Test RE 1.3577656713147437\n",
      "16 Train Loss 155.57292 Test MSE 31.483675119740802 Test RE 1.3409752915917503\n",
      "17 Train Loss 129.12718 Test MSE 31.39554166425438 Test RE 1.3390970545135341\n",
      "18 Train Loss 109.962326 Test MSE 31.145366368545133 Test RE 1.3337510876066958\n",
      "19 Train Loss 101.91531 Test MSE 31.28395289224535 Test RE 1.3367151683896192\n",
      "20 Train Loss 98.042 Test MSE 31.585852585178745 Test RE 1.3431495369193571\n",
      "21 Train Loss 95.43965 Test MSE 31.419776708427204 Test RE 1.3396137969253257\n",
      "22 Train Loss 94.16271 Test MSE 31.594964875087754 Test RE 1.3433432674089496\n",
      "23 Train Loss 92.10281 Test MSE 31.415918547336013 Test RE 1.3395315461100221\n",
      "24 Train Loss 89.95886 Test MSE 31.445656185020518 Test RE 1.3401653821311044\n",
      "25 Train Loss 88.05735 Test MSE 31.25032090193703 Test RE 1.3359964535760218\n",
      "26 Train Loss 85.71784 Test MSE 31.154872490180765 Test RE 1.3339546143949026\n",
      "27 Train Loss 83.371284 Test MSE 31.039982839944816 Test RE 1.3314927341219205\n",
      "28 Train Loss 81.72134 Test MSE 30.958163511927186 Test RE 1.3297367130738083\n",
      "29 Train Loss 80.42457 Test MSE 30.87213689422631 Test RE 1.3278878900463622\n",
      "30 Train Loss 79.14304 Test MSE 30.742625975553256 Test RE 1.3250996683651683\n",
      "31 Train Loss 77.89445 Test MSE 30.345898512836452 Test RE 1.3165218310864455\n",
      "32 Train Loss 76.44088 Test MSE 30.415627722723308 Test RE 1.3180335238891963\n",
      "33 Train Loss 73.80473 Test MSE 29.785385392552772 Test RE 1.3043065538435366\n",
      "34 Train Loss 71.55329 Test MSE 29.572278065624275 Test RE 1.2996321767014927\n",
      "35 Train Loss 68.15149 Test MSE 29.08349246922417 Test RE 1.288846935452588\n",
      "36 Train Loss 65.21308 Test MSE 28.83246030289965 Test RE 1.283272583855537\n",
      "37 Train Loss 63.42874 Test MSE 28.568738609087934 Test RE 1.2773902511120696\n",
      "38 Train Loss 62.177113 Test MSE 28.54166489475223 Test RE 1.2767848359310119\n",
      "39 Train Loss 61.36548 Test MSE 28.325992812492245 Test RE 1.271951743414425\n",
      "40 Train Loss 60.584892 Test MSE 28.24208799420217 Test RE 1.2700665133147828\n",
      "41 Train Loss 59.73008 Test MSE 27.795852321210777 Test RE 1.2599927970871605\n",
      "42 Train Loss 58.280087 Test MSE 27.028881929919788 Test RE 1.2424877202001403\n",
      "43 Train Loss 56.53166 Test MSE 26.77134582983276 Test RE 1.2365542281005373\n",
      "44 Train Loss 55.411797 Test MSE 26.743631007934635 Test RE 1.2359139959158179\n",
      "45 Train Loss 55.035084 Test MSE 26.812274174227635 Test RE 1.2374990963389605\n",
      "46 Train Loss 54.77044 Test MSE 26.816013085660938 Test RE 1.2375853765664138\n",
      "47 Train Loss 54.59013 Test MSE 26.82625931996549 Test RE 1.2378217908804494\n",
      "48 Train Loss 54.436394 Test MSE 26.757795448206593 Test RE 1.2362412460304468\n",
      "49 Train Loss 54.302437 Test MSE 26.690778243131255 Test RE 1.2346921390754186\n",
      "50 Train Loss 54.22515 Test MSE 26.729863595918463 Test RE 1.2355958355359182\n",
      "51 Train Loss 54.132885 Test MSE 26.724818372880524 Test RE 1.2354792215708248\n",
      "52 Train Loss 54.055138 Test MSE 26.71143690430544 Test RE 1.2351698724822806\n",
      "53 Train Loss 54.00104 Test MSE 26.700833540728915 Test RE 1.2349246918264722\n",
      "54 Train Loss 53.945423 Test MSE 26.68247852558721 Test RE 1.2345001552995556\n",
      "55 Train Loss 53.89791 Test MSE 26.722532936686054 Test RE 1.235426392974189\n",
      "56 Train Loss 53.844627 Test MSE 26.69087506711731 Test RE 1.2346943785699909\n",
      "57 Train Loss 53.807083 Test MSE 26.680602427068155 Test RE 1.234456754447124\n",
      "58 Train Loss 53.777184 Test MSE 26.65524243577393 Test RE 1.2338699375782807\n",
      "59 Train Loss 53.735893 Test MSE 26.64029840572836 Test RE 1.2335240098568685\n",
      "60 Train Loss 53.686592 Test MSE 26.652678382404705 Test RE 1.2338105911952246\n",
      "61 Train Loss 53.649372 Test MSE 26.621922905209463 Test RE 1.2330985166672381\n",
      "62 Train Loss 53.584587 Test MSE 26.61306665439864 Test RE 1.2328933936533422\n",
      "63 Train Loss 53.536053 Test MSE 26.612778350771396 Test RE 1.2328867155691388\n",
      "64 Train Loss 53.497704 Test MSE 26.57125057145146 Test RE 1.2319244140067098\n",
      "65 Train Loss 53.465324 Test MSE 26.567579362204736 Test RE 1.231839306824488\n",
      "66 Train Loss 53.438416 Test MSE 26.52249939867102 Test RE 1.2307937684505375\n",
      "67 Train Loss 53.405495 Test MSE 26.50508543394348 Test RE 1.2303896489483068\n",
      "68 Train Loss 53.36281 Test MSE 26.505642578444757 Test RE 1.2304025804520877\n",
      "69 Train Loss 53.33942 Test MSE 26.512609509317222 Test RE 1.2305642736931048\n",
      "70 Train Loss 53.30777 Test MSE 26.464778417687068 Test RE 1.229453749221732\n",
      "71 Train Loss 53.27233 Test MSE 26.446666692130968 Test RE 1.2290329760027952\n",
      "72 Train Loss 53.232822 Test MSE 26.42891248732479 Test RE 1.2286203688717297\n",
      "73 Train Loss 53.20217 Test MSE 26.404750092104816 Test RE 1.2280586128937059\n",
      "74 Train Loss 53.174973 Test MSE 26.366793600254304 Test RE 1.2271756361575938\n",
      "75 Train Loss 53.129936 Test MSE 26.304179018187877 Test RE 1.2257176514775472\n",
      "76 Train Loss 53.06175 Test MSE 26.25431509293199 Test RE 1.224555325024739\n",
      "77 Train Loss 53.006866 Test MSE 26.209183057874846 Test RE 1.2235023467731851\n",
      "78 Train Loss 52.914135 Test MSE 26.06650764900909 Test RE 1.2201676011818516\n",
      "79 Train Loss 52.83354 Test MSE 26.04340884931517 Test RE 1.2196268564443198\n",
      "80 Train Loss 52.722717 Test MSE 25.92035105596377 Test RE 1.216742013259456\n",
      "81 Train Loss 52.605038 Test MSE 25.829866788753527 Test RE 1.214616419750442\n",
      "82 Train Loss 52.47508 Test MSE 25.714578226507754 Test RE 1.211902739789751\n",
      "83 Train Loss 52.35867 Test MSE 25.624888674871617 Test RE 1.209787403558285\n",
      "84 Train Loss 52.165638 Test MSE 25.42472060388059 Test RE 1.2050530303067966\n",
      "85 Train Loss 51.987606 Test MSE 25.273325132370868 Test RE 1.2014598349785865\n",
      "86 Train Loss 51.81084 Test MSE 25.194553799491416 Test RE 1.1995860322504013\n",
      "87 Train Loss 51.57894 Test MSE 24.978150585388118 Test RE 1.1944231285079472\n",
      "88 Train Loss 51.30805 Test MSE 24.95978768260584 Test RE 1.193984002555256\n",
      "89 Train Loss 51.095863 Test MSE 24.56386877823468 Test RE 1.1844765005799005\n",
      "90 Train Loss 50.907238 Test MSE 24.402132634951982 Test RE 1.180570579882394\n",
      "91 Train Loss 50.141735 Test MSE 23.996066680719263 Test RE 1.1707066752443107\n",
      "92 Train Loss 49.68011 Test MSE 23.976280950429853 Test RE 1.1702239281430977\n",
      "93 Train Loss 49.122356 Test MSE 24.176389680808754 Test RE 1.1750971911590482\n",
      "94 Train Loss 48.81402 Test MSE 24.21588294731513 Test RE 1.17605658770321\n",
      "95 Train Loss 48.569527 Test MSE 24.287283151576926 Test RE 1.177789105060641\n",
      "96 Train Loss 48.382816 Test MSE 24.42049011711635 Test RE 1.1810145621616452\n",
      "97 Train Loss 48.23347 Test MSE 24.46524928557534 Test RE 1.1820963797243378\n",
      "98 Train Loss 48.148323 Test MSE 24.488100246060377 Test RE 1.182648299977943\n",
      "99 Train Loss 48.034836 Test MSE 24.466035046594392 Test RE 1.1821153625229026\n",
      "100 Train Loss 47.970398 Test MSE 24.437778097802013 Test RE 1.1814325256138398\n",
      "101 Train Loss 47.92505 Test MSE 24.493336935532994 Test RE 1.1827747456960531\n",
      "102 Train Loss 47.860783 Test MSE 24.52410984478043 Test RE 1.18351751902548\n",
      "103 Train Loss 47.78933 Test MSE 24.55658435912038 Test RE 1.184300859211896\n",
      "104 Train Loss 47.745018 Test MSE 24.567106986259123 Test RE 1.1845545716472585\n",
      "105 Train Loss 47.70332 Test MSE 24.53769772621399 Test RE 1.1838453447631851\n",
      "106 Train Loss 47.670403 Test MSE 24.550258970495687 Test RE 1.1841483207819377\n",
      "107 Train Loss 47.62907 Test MSE 24.54576877992509 Test RE 1.1840400267152198\n",
      "108 Train Loss 47.60078 Test MSE 24.535292897558893 Test RE 1.1837873316806715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "109 Train Loss 47.55709 Test MSE 24.53847965796156 Test RE 1.1838642071457752\n",
      "110 Train Loss 47.514442 Test MSE 24.556603621773245 Test RE 1.18430132370589\n",
      "111 Train Loss 47.436527 Test MSE 24.500692400941826 Test RE 1.1829523288015549\n",
      "112 Train Loss 47.384026 Test MSE 24.47483197257602 Test RE 1.1823278621522522\n",
      "113 Train Loss 47.348232 Test MSE 24.502981452884047 Test RE 1.1830075879762225\n",
      "114 Train Loss 47.28707 Test MSE 24.420186355010912 Test RE 1.181007216924432\n",
      "115 Train Loss 47.246254 Test MSE 24.426194756742895 Test RE 1.1811524969251868\n",
      "116 Train Loss 47.20647 Test MSE 24.4629883005602 Test RE 1.1820417560431546\n",
      "117 Train Loss 47.15613 Test MSE 24.441990727991687 Test RE 1.1815343500071938\n",
      "118 Train Loss 47.098804 Test MSE 24.42036468902813 Test RE 1.1810115292047267\n",
      "119 Train Loss 47.061966 Test MSE 24.41662245226748 Test RE 1.1809210351816617\n",
      "120 Train Loss 47.032364 Test MSE 24.412371352081284 Test RE 1.1808182275108887\n",
      "121 Train Loss 47.014595 Test MSE 24.402015279946678 Test RE 1.1805677410724562\n",
      "122 Train Loss 46.98973 Test MSE 24.408182430434685 Test RE 1.1807169147959706\n",
      "123 Train Loss 46.96869 Test MSE 24.393462548576043 Test RE 1.1803608326771478\n",
      "124 Train Loss 46.94133 Test MSE 24.379658236561685 Test RE 1.180026801068148\n",
      "125 Train Loss 46.91947 Test MSE 24.332629812509182 Test RE 1.1788881142887342\n",
      "126 Train Loss 46.883736 Test MSE 24.28031460507939 Test RE 1.177620126362985\n",
      "127 Train Loss 46.85983 Test MSE 24.25597739534752 Test RE 1.1770297885964844\n",
      "128 Train Loss 46.82443 Test MSE 24.245692785661266 Test RE 1.176780229998826\n",
      "129 Train Loss 46.788494 Test MSE 24.24190749576893 Test RE 1.1766883656783707\n",
      "130 Train Loss 46.761276 Test MSE 24.24901152781184 Test RE 1.176860765881043\n",
      "131 Train Loss 46.734203 Test MSE 24.307692751763586 Test RE 1.1782838733852192\n",
      "132 Train Loss 46.708305 Test MSE 24.31003818163535 Test RE 1.1783407178492649\n",
      "133 Train Loss 46.6726 Test MSE 24.403203450957033 Test RE 1.1805964825365307\n",
      "134 Train Loss 46.65134 Test MSE 24.413425166458357 Test RE 1.1808437135601193\n",
      "135 Train Loss 46.623695 Test MSE 24.465582255530578 Test RE 1.1821044238124\n",
      "136 Train Loss 46.585293 Test MSE 24.44964377747484 Test RE 1.181719311069584\n",
      "137 Train Loss 46.55569 Test MSE 24.41719528832497 Test RE 1.1809348878384676\n",
      "138 Train Loss 46.521355 Test MSE 24.441623003177586 Test RE 1.1815254620009303\n",
      "139 Train Loss 46.498447 Test MSE 24.432691924369845 Test RE 1.1813095749226972\n",
      "140 Train Loss 46.46893 Test MSE 24.379685996872293 Test RE 1.1800274728967044\n",
      "141 Train Loss 46.437397 Test MSE 24.397145079386487 Test RE 1.1804499252175835\n",
      "142 Train Loss 46.395863 Test MSE 24.341255135018663 Test RE 1.1790970392862326\n",
      "143 Train Loss 46.368347 Test MSE 24.33956658321239 Test RE 1.179056141621878\n",
      "144 Train Loss 46.332474 Test MSE 24.29437710732677 Test RE 1.17796109988041\n",
      "145 Train Loss 46.29707 Test MSE 24.33026812293051 Test RE 1.1788309023195496\n",
      "146 Train Loss 46.273132 Test MSE 24.30945048790992 Test RE 1.1783264746048387\n",
      "147 Train Loss 46.24165 Test MSE 24.28770148332905 Test RE 1.1777992483207014\n",
      "148 Train Loss 46.216324 Test MSE 24.28099115106598 Test RE 1.1776365328353342\n",
      "149 Train Loss 46.192318 Test MSE 24.296635248244264 Test RE 1.1780158438308057\n",
      "150 Train Loss 46.167732 Test MSE 24.296699363101524 Test RE 1.17801739812561\n",
      "151 Train Loss 46.14224 Test MSE 24.292563309586953 Test RE 1.1779171262676789\n",
      "152 Train Loss 46.113823 Test MSE 24.27567243429777 Test RE 1.1775075459629247\n",
      "153 Train Loss 46.078957 Test MSE 24.338336474885146 Test RE 1.1790263468205044\n",
      "154 Train Loss 46.03755 Test MSE 24.35585678089641 Test RE 1.1794506401202416\n",
      "155 Train Loss 45.99181 Test MSE 24.356031755326725 Test RE 1.179454876746673\n",
      "156 Train Loss 45.96068 Test MSE 24.355472901253304 Test RE 1.1794413452537353\n",
      "157 Train Loss 45.935425 Test MSE 24.366859219594943 Test RE 1.1797170107161308\n",
      "158 Train Loss 45.908195 Test MSE 24.39285968561251 Test RE 1.1803462467973271\n",
      "159 Train Loss 45.88134 Test MSE 24.44506529229445 Test RE 1.181608660425338\n",
      "160 Train Loss 45.8304 Test MSE 24.472878405524582 Test RE 1.1822806748446786\n",
      "161 Train Loss 45.801937 Test MSE 24.48862763259753 Test RE 1.182661034927311\n",
      "162 Train Loss 45.758663 Test MSE 24.558918333823307 Test RE 1.1843571386709972\n",
      "163 Train Loss 45.709232 Test MSE 24.63011030387657 Test RE 1.1860725175893199\n",
      "164 Train Loss 45.664448 Test MSE 24.617247517837942 Test RE 1.1857627709175727\n",
      "165 Train Loss 45.59957 Test MSE 24.617890106121614 Test RE 1.1857782469022649\n",
      "166 Train Loss 45.52486 Test MSE 24.570584045753062 Test RE 1.1846383955382151\n",
      "167 Train Loss 45.45883 Test MSE 24.623871623057937 Test RE 1.1859222950284243\n",
      "168 Train Loss 45.375656 Test MSE 24.71414061831781 Test RE 1.1880940510426499\n",
      "169 Train Loss 45.22977 Test MSE 24.847356730132905 Test RE 1.1912918267821453\n",
      "170 Train Loss 45.098484 Test MSE 24.96331349506001 Test RE 1.1940683304963189\n",
      "171 Train Loss 44.92556 Test MSE 25.110828627884853 Test RE 1.1975911740237961\n",
      "172 Train Loss 44.573856 Test MSE 25.2669373697301 Test RE 1.2013079925675594\n",
      "173 Train Loss 44.21779 Test MSE 25.18221495773619 Test RE 1.199292252193483\n",
      "174 Train Loss 43.795086 Test MSE 25.08619133617149 Test RE 1.1970035262733747\n",
      "175 Train Loss 43.328285 Test MSE 25.029648606380682 Test RE 1.1956537791803192\n",
      "176 Train Loss 42.775078 Test MSE 25.043616517163553 Test RE 1.1959873527002618\n",
      "177 Train Loss 42.310837 Test MSE 24.85529281997874 Test RE 1.1914820571613807\n",
      "178 Train Loss 41.850178 Test MSE 24.848480850690954 Test RE 1.1913187741251123\n",
      "179 Train Loss 41.41796 Test MSE 24.89256814738559 Test RE 1.1923751515446344\n",
      "180 Train Loss 41.06984 Test MSE 24.946637440368526 Test RE 1.193669431620694\n",
      "181 Train Loss 40.70675 Test MSE 24.87847439139104 Test RE 1.19203755231225\n",
      "182 Train Loss 40.415115 Test MSE 24.90103197140148 Test RE 1.1925778464949817\n",
      "183 Train Loss 40.197937 Test MSE 24.880363163621045 Test RE 1.192082801161897\n",
      "184 Train Loss 40.01716 Test MSE 24.87635125654092 Test RE 1.1919866868446085\n",
      "185 Train Loss 39.87473 Test MSE 24.881541634849118 Test RE 1.1921110326358229\n",
      "186 Train Loss 39.69497 Test MSE 24.867211156159335 Test RE 1.1917676860997797\n",
      "187 Train Loss 39.542496 Test MSE 24.847366859091252 Test RE 1.1912920695955804\n",
      "188 Train Loss 39.37518 Test MSE 24.84690541176204 Test RE 1.1912810076367977\n",
      "189 Train Loss 39.25054 Test MSE 24.822226982677574 Test RE 1.1906892589480316\n",
      "190 Train Loss 38.964455 Test MSE 24.51461088876751 Test RE 1.183288290120782\n",
      "191 Train Loss 38.705406 Test MSE 24.536465954524072 Test RE 1.1838156303723304\n",
      "192 Train Loss 38.573723 Test MSE 24.553956111989365 Test RE 1.184237480718793\n",
      "193 Train Loss 38.503525 Test MSE 24.526892508961502 Test RE 1.1835846618986223\n",
      "194 Train Loss 38.453663 Test MSE 24.51038038034684 Test RE 1.1831861851539638\n",
      "195 Train Loss 38.405495 Test MSE 24.51515813469825 Test RE 1.1833014974706921\n",
      "196 Train Loss 38.37805 Test MSE 24.50207343453964 Test RE 1.1829856681428048\n",
      "197 Train Loss 38.338997 Test MSE 24.471828186341035 Test RE 1.1822553066156998\n",
      "198 Train Loss 38.298843 Test MSE 24.453694194217746 Test RE 1.1818171909713842\n",
      "199 Train Loss 38.27559 Test MSE 24.463338913890297 Test RE 1.1820502267603528\n",
      "Training time: 118.80\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 656.9583 Test MSE 38.368757232052154 Test RE 1.4803585995898307\n",
      "1 Train Loss 569.5286 Test MSE 36.185769441422494 Test RE 1.4376294821925646\n",
      "2 Train Loss 413.29825 Test MSE 35.12161169882969 Test RE 1.4163327032023982\n",
      "3 Train Loss 400.2833 Test MSE 35.39182952065124 Test RE 1.421770736944822\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4 Train Loss 391.4057 Test MSE 35.25446205482633 Test RE 1.4190088717713976\n",
      "5 Train Loss 386.81003 Test MSE 35.31761763501843 Test RE 1.4202793241233365\n",
      "6 Train Loss 379.88608 Test MSE 35.47948903075916 Test RE 1.4235303895331282\n",
      "7 Train Loss 366.23587 Test MSE 35.4153285942751 Test RE 1.4222426643877852\n",
      "8 Train Loss 356.10617 Test MSE 35.53234084779137 Test RE 1.4245902719019317\n",
      "9 Train Loss 347.50867 Test MSE 35.1884429814669 Test RE 1.4176795996089524\n",
      "10 Train Loss 340.0882 Test MSE 35.221787438573806 Test RE 1.4183511349094267\n",
      "11 Train Loss 330.6333 Test MSE 34.873188690932146 Test RE 1.4113147954070815\n",
      "12 Train Loss 322.3056 Test MSE 35.16782800457441 Test RE 1.4172642684468066\n",
      "13 Train Loss 313.12253 Test MSE 35.387905508981966 Test RE 1.4216919164982835\n",
      "14 Train Loss 301.1575 Test MSE 34.96919986600687 Test RE 1.4132562418416355\n",
      "15 Train Loss 280.67212 Test MSE 34.445322056034804 Test RE 1.402630212915315\n",
      "16 Train Loss 264.49 Test MSE 33.999321362801616 Test RE 1.393519944494344\n",
      "17 Train Loss 243.035 Test MSE 34.36804851954615 Test RE 1.4010560216132921\n",
      "18 Train Loss 209.59726 Test MSE 34.76872862358178 Test RE 1.4091994653965867\n",
      "19 Train Loss 193.8456 Test MSE 34.4562639275925 Test RE 1.4028529743833422\n",
      "20 Train Loss 183.8013 Test MSE 34.301699081913966 Test RE 1.399702959797708\n",
      "21 Train Loss 162.04817 Test MSE 35.61223559136466 Test RE 1.4261909737390366\n",
      "22 Train Loss 138.9545 Test MSE 36.36910078148679 Test RE 1.4412666793374858\n",
      "23 Train Loss 120.63945 Test MSE 36.15921820053809 Test RE 1.4371019564304974\n",
      "24 Train Loss 109.80192 Test MSE 36.36894775436391 Test RE 1.4412636471876594\n",
      "25 Train Loss 102.93091 Test MSE 36.55212394094021 Test RE 1.444888629580522\n",
      "26 Train Loss 97.9709 Test MSE 36.60655211788826 Test RE 1.445963989798524\n",
      "27 Train Loss 95.16376 Test MSE 36.66854315575474 Test RE 1.4471877992902225\n",
      "28 Train Loss 93.15621 Test MSE 36.350504054922034 Test RE 1.4408981485113423\n",
      "29 Train Loss 91.59412 Test MSE 36.17821937842961 Test RE 1.4374794956156294\n",
      "30 Train Loss 89.73767 Test MSE 35.83657546814399 Test RE 1.4306760783376047\n",
      "31 Train Loss 88.12171 Test MSE 35.91812388064302 Test RE 1.4323029506702942\n",
      "32 Train Loss 86.79431 Test MSE 36.09246464459309 Test RE 1.4357748262211607\n",
      "33 Train Loss 85.66879 Test MSE 35.98010305645703 Test RE 1.4335381863099559\n",
      "34 Train Loss 83.98455 Test MSE 36.06673723486359 Test RE 1.4352630109071864\n",
      "35 Train Loss 82.69392 Test MSE 36.065525617195924 Test RE 1.4352389027571275\n",
      "36 Train Loss 81.79386 Test MSE 36.09463351650385 Test RE 1.4358179649336449\n",
      "37 Train Loss 80.912476 Test MSE 36.22345075881809 Test RE 1.4383778105868534\n",
      "38 Train Loss 79.945595 Test MSE 36.28407624138854 Test RE 1.4395809799556603\n",
      "39 Train Loss 79.28329 Test MSE 36.201360597001276 Test RE 1.437939160436341\n",
      "40 Train Loss 78.79214 Test MSE 36.08679558311469 Test RE 1.4356620628579086\n",
      "41 Train Loss 78.32258 Test MSE 36.07949299687934 Test RE 1.4355167939819182\n",
      "42 Train Loss 77.59578 Test MSE 36.148628805576266 Test RE 1.4368915100311277\n",
      "43 Train Loss 77.1089 Test MSE 36.003575526728085 Test RE 1.4340057113239737\n",
      "44 Train Loss 76.5587 Test MSE 35.87856014829269 Test RE 1.4315138940914258\n",
      "45 Train Loss 76.0169 Test MSE 35.873803239243045 Test RE 1.4314189933053572\n",
      "46 Train Loss 75.653564 Test MSE 35.83811796722847 Test RE 1.4307068680094126\n",
      "47 Train Loss 75.08007 Test MSE 35.66458887560492 Test RE 1.4272389051443326\n",
      "48 Train Loss 74.578 Test MSE 35.58656190418692 Test RE 1.425676793964266\n",
      "49 Train Loss 74.117615 Test MSE 35.58422547453006 Test RE 1.4256299919125925\n",
      "50 Train Loss 73.68528 Test MSE 35.483710922036195 Test RE 1.4236150837031802\n",
      "51 Train Loss 73.3302 Test MSE 35.40971104190709 Test RE 1.422129862391976\n",
      "52 Train Loss 72.95491 Test MSE 35.30093634139425 Test RE 1.4199438698053901\n",
      "53 Train Loss 72.61595 Test MSE 35.271773561480956 Test RE 1.4193572272019788\n",
      "54 Train Loss 72.0959 Test MSE 35.27871226368083 Test RE 1.4194968290963021\n",
      "55 Train Loss 71.596634 Test MSE 35.40681661999357 Test RE 1.4220717381114285\n",
      "56 Train Loss 71.29393 Test MSE 35.35520009799199 Test RE 1.4210348026364328\n",
      "57 Train Loss 70.98783 Test MSE 35.34139963138735 Test RE 1.420757433856458\n",
      "58 Train Loss 70.59654 Test MSE 35.311610641711816 Test RE 1.420158534956765\n",
      "59 Train Loss 70.370865 Test MSE 35.30835524740458 Test RE 1.4200930708999837\n",
      "60 Train Loss 69.932915 Test MSE 35.21099730168698 Test RE 1.4181338634945408\n",
      "61 Train Loss 69.671875 Test MSE 35.212725828344844 Test RE 1.4181686715416333\n",
      "62 Train Loss 69.37211 Test MSE 35.20620975776303 Test RE 1.4180374503071722\n",
      "63 Train Loss 68.94669 Test MSE 35.10620763165835 Test RE 1.4160220728555912\n",
      "64 Train Loss 68.53319 Test MSE 34.952219696638686 Test RE 1.412913079226069\n",
      "65 Train Loss 67.9484 Test MSE 34.91291244199948 Test RE 1.4121183749428234\n",
      "66 Train Loss 67.3649 Test MSE 34.918525550322165 Test RE 1.4122318867395258\n",
      "67 Train Loss 66.79684 Test MSE 34.95088107703561 Test RE 1.4128860227004596\n",
      "68 Train Loss 66.45651 Test MSE 34.897201885822355 Test RE 1.411800617702612\n",
      "69 Train Loss 66.1044 Test MSE 34.90846749578717 Test RE 1.4120284799787761\n",
      "70 Train Loss 65.66501 Test MSE 34.83683027388993 Test RE 1.4105788926196539\n",
      "71 Train Loss 65.38146 Test MSE 34.904347167803046 Test RE 1.4119451450092604\n",
      "72 Train Loss 64.94533 Test MSE 34.786134224002566 Test RE 1.4095521514855844\n",
      "73 Train Loss 64.42223 Test MSE 34.749441667142904 Test RE 1.4088085546552247\n",
      "74 Train Loss 64.21605 Test MSE 34.75788414882001 Test RE 1.408979681405976\n",
      "75 Train Loss 63.77951 Test MSE 34.676777024817056 Test RE 1.4073348023058254\n",
      "76 Train Loss 63.42798 Test MSE 34.72563909465109 Test RE 1.4083259711878646\n",
      "77 Train Loss 63.165844 Test MSE 34.664497731581385 Test RE 1.4070856066147888\n",
      "78 Train Loss 62.83051 Test MSE 34.67612271533126 Test RE 1.4073215248773323\n",
      "79 Train Loss 62.584606 Test MSE 34.734753371540116 Test RE 1.408510777444187\n",
      "80 Train Loss 62.299225 Test MSE 34.76717503215015 Test RE 1.4091679810030127\n",
      "81 Train Loss 62.026257 Test MSE 34.88541695828699 Test RE 1.4115622121573317\n",
      "82 Train Loss 61.809883 Test MSE 34.845824067127026 Test RE 1.410760964813049\n",
      "83 Train Loss 61.557518 Test MSE 34.75745353469369 Test RE 1.4089709534806645\n",
      "84 Train Loss 61.347656 Test MSE 34.7356344614536 Test RE 1.4085286416385259\n",
      "85 Train Loss 61.08099 Test MSE 34.769989285628206 Test RE 1.4092250128964376\n",
      "86 Train Loss 60.777035 Test MSE 34.72616736453696 Test RE 1.4083366833502613\n",
      "87 Train Loss 60.46588 Test MSE 34.65847830892566 Test RE 1.4069634324703735\n",
      "88 Train Loss 60.293816 Test MSE 34.65314585138225 Test RE 1.406855192557107\n",
      "89 Train Loss 60.04794 Test MSE 34.56474086751951 Test RE 1.4050595051514794\n",
      "90 Train Loss 59.826035 Test MSE 34.390432538865646 Test RE 1.4015122034689038\n",
      "91 Train Loss 59.64028 Test MSE 34.09287245854854 Test RE 1.3954358027934215\n",
      "92 Train Loss 59.169178 Test MSE 32.546207440942894 Test RE 1.363415599644513\n",
      "93 Train Loss 51.705193 Test MSE 26.302777883874548 Test RE 1.2256850061338298\n",
      "94 Train Loss 47.096233 Test MSE 26.00787873202933 Test RE 1.2187946252416506\n",
      "95 Train Loss 45.41614 Test MSE 25.75893990209774 Test RE 1.2129476503597656\n",
      "96 Train Loss 44.255615 Test MSE 25.675016178835634 Test RE 1.2109701207749042\n",
      "97 Train Loss 43.387024 Test MSE 25.535957699247483 Test RE 1.2076863001495932\n",
      "98 Train Loss 42.70925 Test MSE 25.45581531028925 Test RE 1.205789701536735\n",
      "99 Train Loss 42.30494 Test MSE 25.313721131475877 Test RE 1.2024196372962068\n",
      "100 Train Loss 41.907314 Test MSE 25.17039432862051 Test RE 1.1990107429429702\n",
      "101 Train Loss 41.669624 Test MSE 25.107957097787825 Test RE 1.1975226972429123\n",
      "102 Train Loss 41.401848 Test MSE 25.00746825315693 Test RE 1.195123889579302\n",
      "103 Train Loss 41.232677 Test MSE 24.919406997135617 Test RE 1.193017780219277\n",
      "104 Train Loss 40.921413 Test MSE 24.75931230450341 Test RE 1.1891793347998898\n",
      "105 Train Loss 40.483154 Test MSE 24.608850554412456 Test RE 1.1855605213398108\n",
      "106 Train Loss 40.098495 Test MSE 24.43147855877728 Test RE 1.1812802417204256\n",
      "107 Train Loss 39.543304 Test MSE 24.313740809085765 Test RE 1.1784304501347298\n",
      "108 Train Loss 39.122658 Test MSE 24.230782386296113 Test RE 1.1764183314589436\n",
      "109 Train Loss 38.781097 Test MSE 24.004287944808514 Test RE 1.1709072052862057\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "110 Train Loss 38.262016 Test MSE 23.702263127634787 Test RE 1.1635176404758505\n",
      "111 Train Loss 37.823315 Test MSE 23.14950363835481 Test RE 1.1498704314271941\n",
      "112 Train Loss 36.79447 Test MSE 22.698862099390738 Test RE 1.1386234079172037\n",
      "113 Train Loss 35.661293 Test MSE 22.31929012933669 Test RE 1.1290632024231781\n",
      "114 Train Loss 34.754745 Test MSE 22.012455776946393 Test RE 1.1212754483209082\n",
      "115 Train Loss 33.920074 Test MSE 21.744962545013408 Test RE 1.1144418090766564\n",
      "116 Train Loss 33.006416 Test MSE 21.392934491007757 Test RE 1.1053841812413268\n",
      "117 Train Loss 31.921642 Test MSE 21.25932429275817 Test RE 1.1019269200580273\n",
      "118 Train Loss 30.869339 Test MSE 20.785777050372324 Test RE 1.08958520362843\n",
      "119 Train Loss 29.387981 Test MSE 20.399031362362113 Test RE 1.0794010531134268\n",
      "120 Train Loss 27.663715 Test MSE 20.311303580275325 Test RE 1.0770775239337131\n",
      "121 Train Loss 26.672466 Test MSE 20.30701766273255 Test RE 1.0769638800953591\n",
      "122 Train Loss 25.71944 Test MSE 20.212149571443444 Test RE 1.0744453144693946\n",
      "123 Train Loss 24.874329 Test MSE 20.27535499005807 Test RE 1.0761239522991908\n",
      "124 Train Loss 24.248507 Test MSE 20.16646449627031 Test RE 1.0732303550400726\n",
      "125 Train Loss 23.810436 Test MSE 20.171940107505854 Test RE 1.0733760472446139\n",
      "126 Train Loss 23.466833 Test MSE 20.28484046495171 Test RE 1.0763756458739773\n",
      "127 Train Loss 23.20188 Test MSE 20.32694895630274 Test RE 1.0774922693242974\n",
      "128 Train Loss 22.878927 Test MSE 20.304422469417936 Test RE 1.0768950610592427\n",
      "129 Train Loss 22.700771 Test MSE 20.269498956259973 Test RE 1.0759685352094428\n",
      "130 Train Loss 22.544361 Test MSE 20.26101758772834 Test RE 1.0757434028440496\n",
      "131 Train Loss 22.38406 Test MSE 20.2292068288948 Test RE 1.0748985870246828\n",
      "132 Train Loss 22.249607 Test MSE 20.21220911521462 Test RE 1.0744468970937118\n",
      "133 Train Loss 22.163185 Test MSE 20.227348491257818 Test RE 1.0748492136015324\n",
      "134 Train Loss 22.037287 Test MSE 20.278278477278665 Test RE 1.076201532228765\n",
      "135 Train Loss 21.91537 Test MSE 20.296453151269983 Test RE 1.0766837041089399\n",
      "136 Train Loss 21.781307 Test MSE 20.27439100170217 Test RE 1.0760983699286055\n",
      "137 Train Loss 21.699034 Test MSE 20.306223501783847 Test RE 1.0769428210941523\n",
      "138 Train Loss 21.635334 Test MSE 20.31336692368056 Test RE 1.0771322305250295\n",
      "139 Train Loss 21.563614 Test MSE 20.28826398818293 Test RE 1.0764664733460274\n",
      "140 Train Loss 21.456457 Test MSE 20.273990436360172 Test RE 1.0760877395269173\n",
      "141 Train Loss 21.37539 Test MSE 20.271263458287137 Test RE 1.0760153668393042\n",
      "142 Train Loss 21.295982 Test MSE 20.251537845770667 Test RE 1.0754917135361974\n",
      "143 Train Loss 21.229994 Test MSE 20.254583142581133 Test RE 1.07557257328111\n",
      "144 Train Loss 21.16793 Test MSE 20.252463753741665 Test RE 1.075516299199173\n",
      "145 Train Loss 21.128069 Test MSE 20.25891627817333 Test RE 1.0756876176769468\n",
      "146 Train Loss 21.079372 Test MSE 20.282811844406655 Test RE 1.0763218221244504\n",
      "147 Train Loss 21.028738 Test MSE 20.306964949565195 Test RE 1.0769624822974395\n",
      "148 Train Loss 20.954336 Test MSE 20.328907836256498 Test RE 1.0775441862933153\n",
      "149 Train Loss 20.886984 Test MSE 20.328069823348073 Test RE 1.0775219764124164\n",
      "150 Train Loss 20.840307 Test MSE 20.349622584837533 Test RE 1.0780930444464898\n",
      "151 Train Loss 20.76791 Test MSE 20.39059258879166 Test RE 1.0791777639984268\n",
      "152 Train Loss 20.714434 Test MSE 20.37486106271145 Test RE 1.0787613859841951\n",
      "153 Train Loss 20.678757 Test MSE 20.406881522653507 Test RE 1.0796087261171192\n",
      "154 Train Loss 20.642885 Test MSE 20.40772496782806 Test RE 1.0796310367617081\n",
      "155 Train Loss 20.589716 Test MSE 20.414503374030613 Test RE 1.0798103210804424\n",
      "156 Train Loss 20.551552 Test MSE 20.406357822606708 Test RE 1.0795948730752616\n",
      "157 Train Loss 20.521177 Test MSE 20.414279635592354 Test RE 1.0798044038231769\n",
      "158 Train Loss 20.483467 Test MSE 20.43142769467461 Test RE 1.0802578281709083\n",
      "159 Train Loss 20.43932 Test MSE 20.484015676840798 Test RE 1.0816471601511513\n",
      "160 Train Loss 20.406801 Test MSE 20.510415120105357 Test RE 1.0823439397511985\n",
      "161 Train Loss 20.379372 Test MSE 20.518298191253088 Test RE 1.0825519163953774\n",
      "162 Train Loss 20.34785 Test MSE 20.51638743439142 Test RE 1.0825015091529686\n",
      "163 Train Loss 20.289558 Test MSE 20.573544152066287 Test RE 1.0840083338976052\n",
      "164 Train Loss 20.264896 Test MSE 20.60247625265009 Test RE 1.084770274074896\n",
      "165 Train Loss 20.224342 Test MSE 20.64749321763921 Test RE 1.0859547535171663\n",
      "166 Train Loss 20.190323 Test MSE 20.65953791462348 Test RE 1.0862714527118829\n",
      "167 Train Loss 20.156225 Test MSE 20.690340914858076 Test RE 1.08708095670512\n",
      "168 Train Loss 20.12478 Test MSE 20.703710677930516 Test RE 1.0874321270347576\n",
      "169 Train Loss 20.098295 Test MSE 20.74037350298101 Test RE 1.0883945317942834\n",
      "170 Train Loss 20.0743 Test MSE 20.794940737800705 Test RE 1.0898253562575062\n",
      "171 Train Loss 20.047432 Test MSE 20.76438958809806 Test RE 1.0890244966610296\n",
      "172 Train Loss 20.01712 Test MSE 20.76425173862601 Test RE 1.0890208817774705\n",
      "173 Train Loss 19.987198 Test MSE 20.73856445167349 Test RE 1.0883470638813395\n",
      "174 Train Loss 19.959085 Test MSE 20.755521521859883 Test RE 1.0887919212538018\n",
      "175 Train Loss 19.938486 Test MSE 20.780079548209525 Test RE 1.0894358625789278\n",
      "176 Train Loss 19.918766 Test MSE 20.788019428980625 Test RE 1.089643974504667\n",
      "177 Train Loss 19.896376 Test MSE 20.803487124257753 Test RE 1.0900492836058977\n",
      "178 Train Loss 19.87259 Test MSE 20.827500821481664 Test RE 1.0906782301813205\n",
      "179 Train Loss 19.841085 Test MSE 20.872455318815376 Test RE 1.0918546666440814\n",
      "180 Train Loss 19.815208 Test MSE 20.87998248805143 Test RE 1.0920515249893317\n",
      "181 Train Loss 19.794647 Test MSE 20.919249318800837 Test RE 1.093077897034959\n",
      "182 Train Loss 19.778488 Test MSE 20.937251444534553 Test RE 1.0935481217028653\n",
      "183 Train Loss 19.759848 Test MSE 20.97038751516617 Test RE 1.0944131244985944\n",
      "184 Train Loss 19.736908 Test MSE 21.011789320539872 Test RE 1.0954929409062282\n",
      "185 Train Loss 19.717634 Test MSE 21.04542550532155 Test RE 1.096369436176139\n",
      "186 Train Loss 19.701162 Test MSE 21.062428955353237 Test RE 1.0968122473471669\n",
      "187 Train Loss 19.680439 Test MSE 21.062650772535935 Test RE 1.0968180228245021\n",
      "188 Train Loss 19.65694 Test MSE 21.11130997689094 Test RE 1.0980842334325671\n",
      "189 Train Loss 19.643448 Test MSE 21.1065646353881 Test RE 1.0979608143377415\n",
      "190 Train Loss 19.633432 Test MSE 21.12686238827111 Test RE 1.0984886307354687\n",
      "191 Train Loss 19.61985 Test MSE 21.12694430114424 Test RE 1.0984907602583238\n",
      "192 Train Loss 19.601328 Test MSE 21.130957876828983 Test RE 1.098595097789798\n",
      "193 Train Loss 19.588408 Test MSE 21.150088883784914 Test RE 1.0990922942662047\n",
      "194 Train Loss 19.573765 Test MSE 21.17978388549809 Test RE 1.0998635936184076\n",
      "195 Train Loss 19.557274 Test MSE 21.20057941571112 Test RE 1.1004034158174345\n",
      "196 Train Loss 19.53247 Test MSE 21.193150373449225 Test RE 1.1002105989229976\n",
      "197 Train Loss 19.505703 Test MSE 21.187440972665772 Test RE 1.1000623914535752\n",
      "198 Train Loss 19.486412 Test MSE 21.196607540050028 Test RE 1.1003003320717868\n",
      "199 Train Loss 19.471083 Test MSE 21.19994576126063 Test RE 1.1003869709666056\n",
      "Training time: 102.77\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 634.56287 Test MSE 32.24712259005964 Test RE 1.3571365561857893\n",
      "1 Train Loss 413.61865 Test MSE 35.31732230593309 Test RE 1.4202733858593954\n",
      "2 Train Loss 397.4858 Test MSE 34.94121615137215 Test RE 1.4126906573459876\n",
      "3 Train Loss 387.4321 Test MSE 35.08005888486964 Test RE 1.4154946148560497\n",
      "4 Train Loss 384.96295 Test MSE 34.796745287973955 Test RE 1.4097671179939264\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5 Train Loss 383.0192 Test MSE 34.81832196294064 Test RE 1.4102041326155728\n",
      "6 Train Loss 380.38135 Test MSE 35.08034362647305 Test RE 1.415500359564029\n",
      "7 Train Loss 377.92215 Test MSE 35.05556179500067 Test RE 1.415000294782899\n",
      "8 Train Loss 373.35284 Test MSE 35.46714960525226 Test RE 1.423282822907647\n",
      "9 Train Loss 367.5945 Test MSE 35.26557628948882 Test RE 1.4192325307618996\n",
      "10 Train Loss 363.9483 Test MSE 35.321394484748176 Test RE 1.420355264100854\n",
      "11 Train Loss 352.16675 Test MSE 35.64691426398938 Test RE 1.4268852066964444\n",
      "12 Train Loss 343.10742 Test MSE 35.535022409155836 Test RE 1.424644026510697\n",
      "13 Train Loss 334.30615 Test MSE 35.50580190072157 Test RE 1.4240581624623203\n",
      "14 Train Loss 329.45807 Test MSE 35.807479253982315 Test RE 1.430095167495865\n",
      "15 Train Loss 323.8628 Test MSE 36.23920738091487 Test RE 1.4386906122531522\n",
      "16 Train Loss 319.44617 Test MSE 37.26617749933863 Test RE 1.4589334732774688\n",
      "17 Train Loss 314.91623 Test MSE 37.840308326316375 Test RE 1.470128841546817\n",
      "18 Train Loss 309.73004 Test MSE 38.32938923249774 Test RE 1.4795989487997356\n",
      "19 Train Loss 304.51996 Test MSE 38.41518793777698 Test RE 1.481254032722811\n",
      "20 Train Loss 296.9749 Test MSE 38.04992692594157 Test RE 1.4741951502566182\n",
      "21 Train Loss 288.42432 Test MSE 38.31646764937743 Test RE 1.4793495269750891\n",
      "22 Train Loss 278.30615 Test MSE 37.66711681087409 Test RE 1.4667606627748568\n",
      "23 Train Loss 252.50479 Test MSE 37.44893666962902 Test RE 1.4625065170536782\n",
      "24 Train Loss 241.06741 Test MSE 37.07867639780097 Test RE 1.4552586049777745\n",
      "25 Train Loss 221.53061 Test MSE 35.783586587840524 Test RE 1.4296179698710534\n",
      "26 Train Loss 210.27997 Test MSE 35.174560668493335 Test RE 1.4173999252089848\n",
      "27 Train Loss 201.65446 Test MSE 35.01602583219977 Test RE 1.4142021449542486\n",
      "28 Train Loss 196.45476 Test MSE 34.47887237464568 Test RE 1.4033131392797384\n",
      "29 Train Loss 192.83408 Test MSE 33.973523040675275 Test RE 1.3929911501350118\n",
      "30 Train Loss 188.07224 Test MSE 33.881630677643486 Test RE 1.3911059773276928\n",
      "31 Train Loss 183.75128 Test MSE 33.69532645211182 Test RE 1.3872760823205226\n",
      "32 Train Loss 179.92993 Test MSE 33.29853442791537 Test RE 1.3790836964176227\n",
      "33 Train Loss 178.7771 Test MSE 33.285144181051265 Test RE 1.3788063849941699\n",
      "34 Train Loss 175.78949 Test MSE 33.356355327910975 Test RE 1.3802805249871157\n",
      "35 Train Loss 169.14636 Test MSE 33.694155395599985 Test RE 1.3872519752265058\n",
      "36 Train Loss 164.95255 Test MSE 34.570330494184425 Test RE 1.4051731099462397\n",
      "37 Train Loss 163.01955 Test MSE 34.35802578737715 Test RE 1.4008517121775317\n",
      "38 Train Loss 161.70964 Test MSE 34.21297640538418 Test RE 1.3978915947214563\n",
      "39 Train Loss 160.12209 Test MSE 34.193010315439935 Test RE 1.397483642747635\n",
      "40 Train Loss 157.72299 Test MSE 34.111779039104036 Test RE 1.395822676365402\n",
      "41 Train Loss 155.0718 Test MSE 34.006976793769915 Test RE 1.3936768210857775\n",
      "42 Train Loss 153.44136 Test MSE 34.00066481545417 Test RE 1.3935474760669944\n",
      "43 Train Loss 152.23193 Test MSE 34.04297574753675 Test RE 1.3944142820684375\n",
      "44 Train Loss 151.56926 Test MSE 33.96036749518726 Test RE 1.3927214204868652\n",
      "45 Train Loss 151.11913 Test MSE 34.002972796766684 Test RE 1.3935947725973932\n",
      "46 Train Loss 149.5014 Test MSE 34.102052087642214 Test RE 1.3956236532153632\n",
      "47 Train Loss 147.34138 Test MSE 34.188255073025985 Test RE 1.397386464925478\n",
      "48 Train Loss 145.75304 Test MSE 34.45514976372915 Test RE 1.4028302931557959\n",
      "49 Train Loss 144.80618 Test MSE 34.4880143562689 Test RE 1.4034991693288976\n",
      "50 Train Loss 142.65854 Test MSE 34.5194204128788 Test RE 1.4041380629402718\n",
      "51 Train Loss 139.45184 Test MSE 34.15674942595988 Test RE 1.3967424467951706\n",
      "52 Train Loss 135.73605 Test MSE 33.82273597757149 Test RE 1.3898964073957725\n",
      "53 Train Loss 130.02077 Test MSE 33.75082737379978 Test RE 1.388418131233002\n",
      "54 Train Loss 124.892265 Test MSE 33.80481419254059 Test RE 1.3895281236881347\n",
      "55 Train Loss 121.27999 Test MSE 33.55879335622429 Test RE 1.3844626156034527\n",
      "56 Train Loss 118.41022 Test MSE 33.94827466762772 Test RE 1.3924734337779083\n",
      "57 Train Loss 114.88788 Test MSE 33.715929662798764 Test RE 1.3877001466721421\n",
      "58 Train Loss 111.36544 Test MSE 33.699190006270804 Test RE 1.3873556135672802\n",
      "59 Train Loss 109.063934 Test MSE 33.98648056896484 Test RE 1.3932567687683364\n",
      "60 Train Loss 106.353935 Test MSE 33.6294630872023 Test RE 1.3859195831540745\n",
      "61 Train Loss 105.22847 Test MSE 33.682857126137854 Test RE 1.3870193701860865\n",
      "62 Train Loss 104.0963 Test MSE 33.68910346382672 Test RE 1.3871479725446467\n",
      "63 Train Loss 102.509705 Test MSE 33.73753768052118 Test RE 1.388144753226058\n",
      "64 Train Loss 101.04657 Test MSE 33.76772458148224 Test RE 1.3887656405496263\n",
      "65 Train Loss 99.705475 Test MSE 33.68291094324688 Test RE 1.3870204782474156\n",
      "66 Train Loss 98.92247 Test MSE 33.747630457665935 Test RE 1.3883523734163312\n",
      "67 Train Loss 97.57608 Test MSE 33.86655023084022 Test RE 1.3907963577177447\n",
      "68 Train Loss 96.5927 Test MSE 33.866879350557404 Test RE 1.390803115674913\n",
      "69 Train Loss 96.06381 Test MSE 33.962666098286206 Test RE 1.3927685527743066\n",
      "70 Train Loss 95.2848 Test MSE 33.96797941073516 Test RE 1.3928774948264164\n",
      "71 Train Loss 94.303116 Test MSE 33.949421042547236 Test RE 1.3924969442973736\n",
      "72 Train Loss 93.60588 Test MSE 33.86654894514693 Test RE 1.3907963313179852\n",
      "73 Train Loss 92.43317 Test MSE 34.05487283207013 Test RE 1.3946579155212537\n",
      "74 Train Loss 91.13974 Test MSE 34.07461844400249 Test RE 1.3950621804771524\n",
      "75 Train Loss 90.160225 Test MSE 33.73896334264005 Test RE 1.388174082641\n",
      "76 Train Loss 89.91245 Test MSE 33.78469847417083 Test RE 1.3891146394484508\n",
      "77 Train Loss 89.48064 Test MSE 33.81496156110242 Test RE 1.3897366590161913\n",
      "78 Train Loss 89.12745 Test MSE 33.655660748937194 Test RE 1.386459300128622\n",
      "79 Train Loss 88.73882 Test MSE 33.66602048469966 Test RE 1.386672670577002\n",
      "80 Train Loss 88.05382 Test MSE 33.75733624582686 Test RE 1.388552003510834\n",
      "81 Train Loss 87.03218 Test MSE 33.61977732552595 Test RE 1.3857199865467427\n",
      "82 Train Loss 86.483826 Test MSE 33.699913091282745 Test RE 1.3873704977628496\n",
      "83 Train Loss 85.2348 Test MSE 33.681292954075346 Test RE 1.3869871644541203\n",
      "84 Train Loss 84.06673 Test MSE 33.591530125781915 Test RE 1.385137726078266\n",
      "85 Train Loss 83.55271 Test MSE 33.74905332340604 Test RE 1.3883816409260634\n",
      "86 Train Loss 82.98303 Test MSE 33.42051277410262 Test RE 1.3816072996279698\n",
      "87 Train Loss 81.9288 Test MSE 33.07651867180287 Test RE 1.3744785331688751\n",
      "88 Train Loss 80.38086 Test MSE 32.848596253936535 Test RE 1.3697347447417176\n",
      "89 Train Loss 79.99995 Test MSE 32.680563413094994 Test RE 1.3662269005563539\n",
      "90 Train Loss 79.42544 Test MSE 32.553703050443644 Test RE 1.3635725924768742\n",
      "91 Train Loss 78.80548 Test MSE 32.28906395006198 Test RE 1.3580188311364119\n",
      "92 Train Loss 77.74388 Test MSE 32.19215419224623 Test RE 1.3559793763533143\n",
      "93 Train Loss 77.48325 Test MSE 32.03557074965994 Test RE 1.3526775976870307\n",
      "94 Train Loss 76.96691 Test MSE 31.945582532972082 Test RE 1.3507764196545218\n",
      "95 Train Loss 76.25917 Test MSE 32.15725097157906 Test RE 1.3552440903187781\n",
      "96 Train Loss 76.0136 Test MSE 32.06271813782729 Test RE 1.353250615220868\n",
      "97 Train Loss 75.44115 Test MSE 32.100383967166685 Test RE 1.3540452506849914\n",
      "98 Train Loss 75.249725 Test MSE 32.04053063725431 Test RE 1.352782307374027\n",
      "99 Train Loss 74.944275 Test MSE 32.104295233186846 Test RE 1.354127739883104\n",
      "100 Train Loss 74.36706 Test MSE 31.91654757856708 Test RE 1.3501624279179045\n",
      "101 Train Loss 74.25045 Test MSE 31.993588370582252 Test RE 1.3517909701068795\n",
      "102 Train Loss 74.118835 Test MSE 32.03368257275294 Test RE 1.3526377336830735\n",
      "103 Train Loss 73.85141 Test MSE 31.957616321526135 Test RE 1.3510308120621897\n",
      "104 Train Loss 73.47125 Test MSE 31.847382088927336 Test RE 1.3486986841824575\n",
      "105 Train Loss 72.98976 Test MSE 31.888183021283513 Test RE 1.3495623430889228\n",
      "106 Train Loss 72.68764 Test MSE 31.85779273060661 Test RE 1.3489191053057108\n",
      "107 Train Loss 72.51083 Test MSE 31.81549530465409 Test RE 1.3480233314331758\n",
      "108 Train Loss 72.28274 Test MSE 31.775127201931955 Test RE 1.3471678612270852\n",
      "109 Train Loss 71.90321 Test MSE 31.741815593521352 Test RE 1.346461521080601\n",
      "110 Train Loss 71.59209 Test MSE 31.702965178103288 Test RE 1.3456372675737072\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111 Train Loss 71.34708 Test MSE 31.707689887257942 Test RE 1.3457375343440927\n",
      "112 Train Loss 71.09483 Test MSE 31.882219877249543 Test RE 1.3494361519714355\n",
      "113 Train Loss 70.91261 Test MSE 31.892066757448717 Test RE 1.3496445237590533\n",
      "114 Train Loss 70.63938 Test MSE 31.86158685270759 Test RE 1.3489994280615984\n",
      "115 Train Loss 70.4548 Test MSE 31.88993722443347 Test RE 1.349599463014417\n",
      "116 Train Loss 70.19754 Test MSE 31.97897771394609 Test RE 1.3514822706123435\n",
      "117 Train Loss 69.94176 Test MSE 31.857709451966603 Test RE 1.3489173422171168\n",
      "118 Train Loss 69.71106 Test MSE 31.898034128109348 Test RE 1.3497707848209182\n",
      "119 Train Loss 69.53513 Test MSE 31.81424690215812 Test RE 1.3479968837512473\n",
      "120 Train Loss 69.408676 Test MSE 31.73183914480628 Test RE 1.3462499081070416\n",
      "121 Train Loss 69.21725 Test MSE 31.789513536079863 Test RE 1.347472794920835\n",
      "122 Train Loss 69.03859 Test MSE 31.782732457292727 Test RE 1.3473290713230832\n",
      "123 Train Loss 68.94849 Test MSE 31.841007316422903 Test RE 1.3485636954171716\n",
      "124 Train Loss 68.86409 Test MSE 31.86559744917504 Test RE 1.3490843284483955\n",
      "125 Train Loss 68.788895 Test MSE 31.876953331303845 Test RE 1.3493246923391036\n",
      "126 Train Loss 68.551094 Test MSE 31.907029968990507 Test RE 1.349961101678499\n",
      "127 Train Loss 68.462425 Test MSE 31.85835196986072 Test RE 1.348930944876803\n",
      "128 Train Loss 68.313 Test MSE 31.829620554747994 Test RE 1.3483225418262608\n",
      "129 Train Loss 68.22159 Test MSE 31.836748596226748 Test RE 1.3484735075120382\n",
      "130 Train Loss 68.14293 Test MSE 31.81204130403623 Test RE 1.3479501564008243\n",
      "131 Train Loss 68.01059 Test MSE 31.82436642885631 Test RE 1.3482112532116286\n",
      "132 Train Loss 67.89716 Test MSE 31.874727483870505 Test RE 1.3492775823894374\n",
      "133 Train Loss 67.785736 Test MSE 31.82506619926425 Test RE 1.3482260757086388\n",
      "134 Train Loss 67.64167 Test MSE 31.85663267560136 Test RE 1.348894545622332\n",
      "135 Train Loss 67.452835 Test MSE 31.82673529506374 Test RE 1.3482614297431188\n",
      "136 Train Loss 67.29994 Test MSE 31.86311784311794 Test RE 1.3490318382549125\n",
      "137 Train Loss 67.24977 Test MSE 31.83237086123043 Test RE 1.3483807929153424\n",
      "138 Train Loss 67.15132 Test MSE 31.787620101984576 Test RE 1.3474326655095867\n",
      "139 Train Loss 66.919395 Test MSE 31.755016960362383 Test RE 1.3467414874966233\n",
      "140 Train Loss 66.60569 Test MSE 31.758183562590087 Test RE 1.3468086341822794\n",
      "141 Train Loss 66.4917 Test MSE 31.757143660125887 Test RE 1.3467865837857267\n",
      "142 Train Loss 66.42843 Test MSE 31.790867900356368 Test RE 1.3475014985620144\n",
      "143 Train Loss 66.3166 Test MSE 31.700951079130743 Test RE 1.345594522523364\n",
      "144 Train Loss 66.153595 Test MSE 31.630221167120038 Test RE 1.3440925655592744\n",
      "145 Train Loss 65.99261 Test MSE 31.59861595853978 Test RE 1.3434208828883023\n",
      "146 Train Loss 65.798744 Test MSE 31.60036574483081 Test RE 1.3434580786139823\n",
      "147 Train Loss 65.638695 Test MSE 31.597296261346504 Test RE 1.3433928290127404\n",
      "148 Train Loss 65.41188 Test MSE 31.475354307348375 Test RE 1.3407980768746677\n",
      "149 Train Loss 65.369026 Test MSE 31.419823214411327 Test RE 1.3396147883396736\n",
      "150 Train Loss 65.17576 Test MSE 31.408543819416014 Test RE 1.3393743127557185\n",
      "151 Train Loss 64.97902 Test MSE 31.445133567793526 Test RE 1.3401542455146238\n",
      "152 Train Loss 64.90822 Test MSE 31.459327518169683 Test RE 1.3404566760436023\n",
      "153 Train Loss 64.82016 Test MSE 31.422458611480277 Test RE 1.339670968523051\n",
      "154 Train Loss 64.734 Test MSE 31.391958561451453 Test RE 1.3390206382604195\n",
      "155 Train Loss 64.65771 Test MSE 31.40036024173167 Test RE 1.3391998126600766\n",
      "156 Train Loss 64.56175 Test MSE 31.395854220876934 Test RE 1.3391037201512133\n",
      "157 Train Loss 64.46564 Test MSE 31.38239953577092 Test RE 1.3388167531286803\n",
      "158 Train Loss 64.39863 Test MSE 31.372545696536232 Test RE 1.338606547392451\n",
      "159 Train Loss 64.3038 Test MSE 31.367376317420565 Test RE 1.3384962590870348\n",
      "160 Train Loss 64.24359 Test MSE 31.350291889892773 Test RE 1.3381316994778178\n",
      "161 Train Loss 64.168724 Test MSE 31.397952013072977 Test RE 1.3391484571790757\n",
      "162 Train Loss 64.046555 Test MSE 31.478925372030602 Test RE 1.3408741354443403\n",
      "163 Train Loss 63.922287 Test MSE 31.400934444523305 Test RE 1.3392120572450052\n",
      "164 Train Loss 63.79227 Test MSE 31.442194707616583 Test RE 1.3400916186861749\n",
      "165 Train Loss 63.678707 Test MSE 31.39452874002679 Test RE 1.339075452484736\n",
      "166 Train Loss 63.585052 Test MSE 31.35543301336421 Test RE 1.338241414856125\n",
      "167 Train Loss 63.509766 Test MSE 31.349946011182823 Test RE 1.338124317847303\n",
      "168 Train Loss 63.358368 Test MSE 31.32733198621429 Test RE 1.3376416083123106\n",
      "169 Train Loss 63.248543 Test MSE 31.290184228497537 Test RE 1.336848289465717\n",
      "170 Train Loss 63.18834 Test MSE 31.2620922206415 Test RE 1.336248050342324\n",
      "171 Train Loss 63.092957 Test MSE 31.283568692736793 Test RE 1.3367069602375146\n",
      "172 Train Loss 62.91566 Test MSE 31.268602916104925 Test RE 1.3363871877222055\n",
      "173 Train Loss 62.838474 Test MSE 31.250198135528333 Test RE 1.3359938293526061\n",
      "174 Train Loss 62.7657 Test MSE 31.29580707867081 Test RE 1.3369683999756878\n",
      "175 Train Loss 62.651005 Test MSE 31.284468195950062 Test RE 1.3367261774114698\n",
      "176 Train Loss 62.483093 Test MSE 31.21005393673028 Test RE 1.3351354406018046\n",
      "177 Train Loss 62.385674 Test MSE 31.21673297445868 Test RE 1.3352782942950614\n",
      "178 Train Loss 62.296288 Test MSE 31.20762890931042 Test RE 1.3350835694490832\n",
      "179 Train Loss 62.13067 Test MSE 31.185876551543963 Test RE 1.334618198018905\n",
      "180 Train Loss 62.00233 Test MSE 31.169833725077833 Test RE 1.3342748726930551\n",
      "181 Train Loss 61.929455 Test MSE 31.143085757411562 Test RE 1.3337022549291633\n",
      "182 Train Loss 61.86907 Test MSE 31.119237611115228 Test RE 1.3331915088509\n",
      "183 Train Loss 61.762405 Test MSE 31.10766692049787 Test RE 1.3329436335518174\n",
      "184 Train Loss 61.691788 Test MSE 31.13776725624737 Test RE 1.3335883676939932\n",
      "185 Train Loss 61.644226 Test MSE 31.136641716761485 Test RE 1.3335642648108623\n",
      "186 Train Loss 61.599792 Test MSE 31.150962743238534 Test RE 1.3338709101747581\n",
      "187 Train Loss 61.532993 Test MSE 31.14785539370996 Test RE 1.3338043808262814\n",
      "188 Train Loss 61.469574 Test MSE 31.127019441703695 Test RE 1.3333581906650431\n",
      "189 Train Loss 61.394867 Test MSE 31.093309064341614 Test RE 1.332635985571635\n",
      "190 Train Loss 61.32643 Test MSE 31.09091372319718 Test RE 1.3325846533174754\n",
      "191 Train Loss 61.255432 Test MSE 31.068550147368974 Test RE 1.3321053055707934\n",
      "192 Train Loss 61.19216 Test MSE 31.06421543345281 Test RE 1.3320123740242404\n",
      "193 Train Loss 61.128056 Test MSE 30.97975142153421 Test RE 1.3302002617804376\n",
      "194 Train Loss 61.085495 Test MSE 30.95853212635225 Test RE 1.3297446295426698\n",
      "195 Train Loss 60.98078 Test MSE 30.965588885557285 Test RE 1.3298961734360084\n",
      "196 Train Loss 60.929398 Test MSE 30.952656417554184 Test RE 1.329618435527005\n",
      "197 Train Loss 60.86409 Test MSE 30.91009528360194 Test RE 1.3287039819440705\n",
      "198 Train Loss 60.804867 Test MSE 30.914573091481994 Test RE 1.3288002201737759\n",
      "199 Train Loss 60.751648 Test MSE 30.92807293859687 Test RE 1.329090320280054\n",
      "Training time: 104.60\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 609.1266 Test MSE 36.807661569307065 Test RE 1.4499304753781468\n",
      "1 Train Loss 488.05316 Test MSE 35.489339206117535 Test RE 1.4237279832872785\n",
      "2 Train Loss 425.62485 Test MSE 34.830619469105706 Test RE 1.4104531460954017\n",
      "3 Train Loss 405.27448 Test MSE 34.6227999378738 Test RE 1.406239062974955\n",
      "4 Train Loss 398.25955 Test MSE 34.11098550002081 Test RE 1.395806440825657\n",
      "5 Train Loss 384.14465 Test MSE 33.84879377658394 Test RE 1.3904317080914326\n",
      "6 Train Loss 370.99725 Test MSE 33.999799845676804 Test RE 1.3935297501765906\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7 Train Loss 361.08765 Test MSE 32.991442079942885 Test RE 1.372709736997072\n",
      "8 Train Loss 350.5503 Test MSE 31.931916681650332 Test RE 1.3504874675857956\n",
      "9 Train Loss 340.33472 Test MSE 32.13736179447587 Test RE 1.3548249180006093\n",
      "10 Train Loss 331.38 Test MSE 31.566026594313765 Test RE 1.3427279328104755\n",
      "11 Train Loss 282.75073 Test MSE 31.301704954554644 Test RE 1.3370943737553775\n",
      "12 Train Loss 252.75775 Test MSE 31.22776870225966 Test RE 1.3355142969813583\n",
      "13 Train Loss 220.56116 Test MSE 31.701328729368267 Test RE 1.3456025374655085\n",
      "14 Train Loss 205.37114 Test MSE 31.926956667393384 Test RE 1.3503825772765379\n",
      "15 Train Loss 193.13629 Test MSE 32.062614931991334 Test RE 1.353248437247797\n",
      "16 Train Loss 184.22064 Test MSE 32.15738149261333 Test RE 1.3552468406732943\n",
      "17 Train Loss 176.49097 Test MSE 31.956948165349164 Test RE 1.3510166886011084\n",
      "18 Train Loss 161.18079 Test MSE 31.513889649665714 Test RE 1.341618596759766\n",
      "19 Train Loss 124.327835 Test MSE 31.423333040216082 Test RE 1.3396896086723817\n",
      "20 Train Loss 104.187195 Test MSE 30.9157214869768 Test RE 1.328824900670098\n",
      "21 Train Loss 97.77259 Test MSE 30.844291712421626 Test RE 1.3272889094658453\n",
      "22 Train Loss 93.17864 Test MSE 31.171531592727238 Test RE 1.3343112121761647\n",
      "23 Train Loss 90.64539 Test MSE 30.953019982326126 Test RE 1.329626244242956\n",
      "24 Train Loss 88.59129 Test MSE 31.13585348967862 Test RE 1.333547385050155\n",
      "25 Train Loss 87.063965 Test MSE 30.96688130850399 Test RE 1.3299239263459175\n",
      "26 Train Loss 85.673416 Test MSE 30.809874785476488 Test RE 1.3265481896583404\n",
      "27 Train Loss 84.67157 Test MSE 30.888882892925633 Test RE 1.3282479848835074\n",
      "28 Train Loss 83.78425 Test MSE 30.69941723302111 Test RE 1.324168127531996\n",
      "29 Train Loss 82.63779 Test MSE 30.858085361370943 Test RE 1.327585659850578\n",
      "30 Train Loss 82.24394 Test MSE 30.894656816298852 Test RE 1.3283721208672015\n",
      "31 Train Loss 81.8694 Test MSE 30.823195733066502 Test RE 1.326834931642883\n",
      "32 Train Loss 81.11976 Test MSE 30.826085099677094 Test RE 1.3268971189440244\n",
      "33 Train Loss 80.63822 Test MSE 30.802960401575092 Test RE 1.3263993286489912\n",
      "34 Train Loss 79.94924 Test MSE 30.539384653355004 Test RE 1.320712248749899\n",
      "35 Train Loss 79.28816 Test MSE 30.42917435434839 Test RE 1.318327006686099\n",
      "36 Train Loss 78.50491 Test MSE 30.104993392300777 Test RE 1.311285722990641\n",
      "37 Train Loss 77.687035 Test MSE 30.126775657749764 Test RE 1.3117600231938187\n",
      "38 Train Loss 75.8721 Test MSE 29.39219073648888 Test RE 1.295668926092025\n",
      "39 Train Loss 72.6122 Test MSE 29.266282278203793 Test RE 1.2928907942874575\n",
      "40 Train Loss 68.95766 Test MSE 28.81687131778797 Test RE 1.2829256203677926\n",
      "41 Train Loss 64.905235 Test MSE 28.449909800966605 Test RE 1.2747308944660156\n",
      "42 Train Loss 63.497997 Test MSE 28.610308644201815 Test RE 1.2783192710391111\n",
      "43 Train Loss 62.44977 Test MSE 28.503592656377588 Test RE 1.275932988673972\n",
      "44 Train Loss 61.38947 Test MSE 28.124821364952815 Test RE 1.2674269892115422\n",
      "45 Train Loss 60.604553 Test MSE 27.946754622071374 Test RE 1.2634083867336559\n",
      "46 Train Loss 59.90203 Test MSE 27.565604858227243 Test RE 1.2547633580685567\n",
      "47 Train Loss 58.843357 Test MSE 26.99097795650795 Test RE 1.241616212747012\n",
      "48 Train Loss 58.184578 Test MSE 26.919392151716526 Test RE 1.2399686046498792\n",
      "49 Train Loss 56.203182 Test MSE 26.58141016161724 Test RE 1.2321599063293132\n",
      "50 Train Loss 54.8499 Test MSE 26.44838251710977 Test RE 1.2290728443782983\n",
      "51 Train Loss 54.42971 Test MSE 26.384119338084155 Test RE 1.2275787612402125\n",
      "52 Train Loss 54.198143 Test MSE 26.455733550144124 Test RE 1.2292436360584846\n",
      "53 Train Loss 53.857178 Test MSE 26.368560504537687 Test RE 1.2272167535114191\n",
      "54 Train Loss 53.768562 Test MSE 26.35626559511093 Test RE 1.226930612027212\n",
      "55 Train Loss 53.70926 Test MSE 26.333671334477753 Test RE 1.226404597944324\n",
      "56 Train Loss 53.6372 Test MSE 26.35210855217961 Test RE 1.22683384937431\n",
      "57 Train Loss 53.508907 Test MSE 26.37359500770294 Test RE 1.2273339030946895\n",
      "58 Train Loss 53.427124 Test MSE 26.328342802812976 Test RE 1.2262805122013014\n",
      "59 Train Loss 53.303284 Test MSE 26.28740180045938 Test RE 1.2253266981621855\n",
      "60 Train Loss 53.21252 Test MSE 26.23200851268826 Test RE 1.2240350020593518\n",
      "61 Train Loss 53.017662 Test MSE 26.131349492563647 Test RE 1.2216842749218029\n",
      "62 Train Loss 52.51091 Test MSE 25.87289907063824 Test RE 1.215627767686044\n",
      "63 Train Loss 51.834293 Test MSE 25.685550363851533 Test RE 1.2112185193544414\n",
      "64 Train Loss 49.586662 Test MSE 24.812879205888557 Test RE 1.1904650376178978\n",
      "65 Train Loss 48.983017 Test MSE 24.92892838729985 Test RE 1.1932456769528623\n",
      "66 Train Loss 48.569866 Test MSE 24.804024856715305 Test RE 1.1902526129823086\n",
      "67 Train Loss 48.145306 Test MSE 24.4772614884177 Test RE 1.1823865431064868\n",
      "68 Train Loss 46.153206 Test MSE 23.39598974014856 Test RE 1.1559758888441294\n",
      "69 Train Loss 45.60457 Test MSE 23.25480193623028 Test RE 1.1524826262133485\n",
      "70 Train Loss 45.29017 Test MSE 23.34863942685585 Test RE 1.1548055270933473\n",
      "71 Train Loss 45.012577 Test MSE 23.48377059248723 Test RE 1.1581424470405717\n",
      "72 Train Loss 43.349617 Test MSE 22.018940140142345 Test RE 1.121440587139418\n",
      "73 Train Loss 42.146545 Test MSE 21.604309060962212 Test RE 1.1108316758401664\n",
      "74 Train Loss 41.435802 Test MSE 21.203133501289837 Test RE 1.100469697964668\n",
      "75 Train Loss 40.9246 Test MSE 20.982725252460313 Test RE 1.094735021177071\n",
      "76 Train Loss 40.402374 Test MSE 20.771970244007132 Test RE 1.0892232688572374\n",
      "77 Train Loss 39.99448 Test MSE 20.345114891749503 Test RE 1.0779736323616749\n",
      "78 Train Loss 39.33996 Test MSE 19.92458578883339 Test RE 1.066774720076299\n",
      "79 Train Loss 38.980568 Test MSE 19.84852543565129 Test RE 1.0647366138268373\n",
      "80 Train Loss 38.806564 Test MSE 19.7856132006054 Test RE 1.0630478706556523\n",
      "81 Train Loss 38.68132 Test MSE 19.74708749470079 Test RE 1.0620124055188478\n",
      "82 Train Loss 38.582844 Test MSE 19.74936718438149 Test RE 1.0620737054156537\n",
      "83 Train Loss 38.503468 Test MSE 19.72456965477747 Test RE 1.0614067200814512\n",
      "84 Train Loss 38.45227 Test MSE 19.707592067626845 Test RE 1.0609498278727365\n",
      "85 Train Loss 38.390728 Test MSE 19.64488889523881 Test RE 1.0592606839312548\n",
      "86 Train Loss 38.352406 Test MSE 19.598054081963515 Test RE 1.0579972540255593\n",
      "87 Train Loss 38.306236 Test MSE 19.507648303489514 Test RE 1.0555541638248132\n",
      "88 Train Loss 38.26839 Test MSE 19.437619263039547 Test RE 1.053657833142326\n",
      "89 Train Loss 38.22546 Test MSE 19.402232130678772 Test RE 1.0526982785184509\n",
      "90 Train Loss 38.191257 Test MSE 19.38419997158198 Test RE 1.0522089833883708\n",
      "91 Train Loss 38.160168 Test MSE 19.423405569696246 Test RE 1.0532725208264844\n",
      "92 Train Loss 38.144165 Test MSE 19.40424710460685 Test RE 1.052752939871379\n",
      "93 Train Loss 38.12239 Test MSE 19.345713371929516 Test RE 1.0511639037171057\n",
      "94 Train Loss 38.102764 Test MSE 19.31079458145554 Test RE 1.0502148058585627\n",
      "95 Train Loss 38.07479 Test MSE 19.281379561286958 Test RE 1.0494146351931757\n",
      "96 Train Loss 38.063873 Test MSE 19.25327266474569 Test RE 1.0486494786986598\n",
      "97 Train Loss 38.043236 Test MSE 19.268681520571533 Test RE 1.0490690244350398\n",
      "98 Train Loss 38.02826 Test MSE 19.270900651908622 Test RE 1.0491294321728493\n",
      "99 Train Loss 38.022316 Test MSE 19.258922607609485 Test RE 1.0488033324133368\n",
      "100 Train Loss 38.00435 Test MSE 19.253150495942602 Test RE 1.0486461516680523\n",
      "101 Train Loss 37.993362 Test MSE 19.223812454519333 Test RE 1.047846881095973\n",
      "102 Train Loss 37.984272 Test MSE 19.20312157275427 Test RE 1.0472828225102544\n",
      "103 Train Loss 37.976738 Test MSE 19.180670478875477 Test RE 1.0466704345378848\n",
      "104 Train Loss 37.966686 Test MSE 19.1739627685457 Test RE 1.0464874019371249\n",
      "105 Train Loss 37.9535 Test MSE 19.170792209860164 Test RE 1.0464008760860297\n",
      "106 Train Loss 37.93811 Test MSE 19.112953424818652 Test RE 1.0448211741831268\n",
      "107 Train Loss 37.92959 Test MSE 19.088103193571378 Test RE 1.0441417267902642\n",
      "108 Train Loss 37.922314 Test MSE 19.100103999654607 Test RE 1.0444699043302608\n",
      "109 Train Loss 37.911957 Test MSE 19.107772949436246 Test RE 1.0446795676749403\n",
      "110 Train Loss 37.896328 Test MSE 19.101436024651957 Test RE 1.0445063239164083\n",
      "111 Train Loss 37.885094 Test MSE 19.087314823991992 Test RE 1.0441201641953497\n",
      "112 Train Loss 37.879456 Test MSE 19.064896863510366 Test RE 1.0435068269685168\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113 Train Loss 37.874634 Test MSE 19.059489279906902 Test RE 1.043358825889872\n",
      "114 Train Loss 37.86387 Test MSE 19.07324827078513 Test RE 1.0437353568262944\n",
      "115 Train Loss 37.85355 Test MSE 19.06672262572249 Test RE 1.0435567918263267\n",
      "116 Train Loss 37.846764 Test MSE 19.078769588245226 Test RE 1.0438864159740335\n",
      "117 Train Loss 37.84032 Test MSE 19.07559028014229 Test RE 1.043799435133872\n",
      "118 Train Loss 37.834343 Test MSE 19.06635090833831 Test RE 1.0435466193887462\n",
      "119 Train Loss 37.8306 Test MSE 19.067586946999008 Test RE 1.0435804445047256\n",
      "120 Train Loss 37.823074 Test MSE 19.075334350259894 Test RE 1.043792432981939\n",
      "121 Train Loss 37.819077 Test MSE 19.06858946655875 Test RE 1.04360787839186\n",
      "122 Train Loss 37.812115 Test MSE 19.078430245032546 Test RE 1.0438771324260665\n",
      "123 Train Loss 37.806145 Test MSE 19.073538366729657 Test RE 1.043743294180189\n",
      "124 Train Loss 37.801193 Test MSE 19.077497668794994 Test RE 1.0438516191397844\n",
      "125 Train Loss 37.797523 Test MSE 19.074175308292325 Test RE 1.0437607214116242\n",
      "126 Train Loss 37.79354 Test MSE 19.070371024624134 Test RE 1.0436566288411593\n",
      "127 Train Loss 37.78865 Test MSE 19.07345364606911 Test RE 1.0437409761331173\n",
      "128 Train Loss 37.784863 Test MSE 19.07485734197862 Test RE 1.0437793820770889\n",
      "129 Train Loss 37.781307 Test MSE 19.06396826115879 Test RE 1.0434814133851114\n",
      "130 Train Loss 37.77665 Test MSE 19.08203778077999 Test RE 1.0439758209979235\n",
      "131 Train Loss 37.77175 Test MSE 19.079994837734887 Test RE 1.0439199349270851\n",
      "132 Train Loss 37.76689 Test MSE 19.068150278925405 Test RE 1.0435958601375943\n",
      "133 Train Loss 37.76314 Test MSE 19.06675434110202 Test RE 1.0435576597464882\n",
      "134 Train Loss 37.75997 Test MSE 19.06759049253533 Test RE 1.0435805415293837\n",
      "135 Train Loss 37.756824 Test MSE 19.065373081787715 Test RE 1.0435198596621607\n",
      "136 Train Loss 37.752895 Test MSE 19.056392222305217 Test RE 1.0432740525359048\n",
      "137 Train Loss 37.749813 Test MSE 19.052384788547997 Test RE 1.0431643499376402\n",
      "138 Train Loss 37.74492 Test MSE 19.054810352511577 Test RE 1.0432307505839749\n",
      "139 Train Loss 37.74068 Test MSE 19.060333119886746 Test RE 1.0433819224727048\n",
      "140 Train Loss 37.738045 Test MSE 19.059314693018145 Test RE 1.0433540472416685\n",
      "141 Train Loss 37.735077 Test MSE 19.06945520933262 Test RE 1.0436315688102382\n",
      "142 Train Loss 37.732967 Test MSE 19.065543019431395 Test RE 1.0435245103162594\n",
      "143 Train Loss 37.731503 Test MSE 19.06578082697538 Test RE 1.0435310183193836\n",
      "144 Train Loss 37.729446 Test MSE 19.055445981388214 Test RE 1.0432481504438509\n",
      "145 Train Loss 37.726437 Test MSE 19.056672976461986 Test RE 1.043281737685451\n",
      "146 Train Loss 37.724422 Test MSE 19.05380382889257 Test RE 1.043203197167334\n",
      "147 Train Loss 37.72257 Test MSE 19.058515793031553 Test RE 1.0433321801314737\n",
      "148 Train Loss 37.720604 Test MSE 19.06619498964538 Test RE 1.0435423524800351\n",
      "149 Train Loss 37.718433 Test MSE 19.058498541482916 Test RE 1.0433317079252402\n",
      "150 Train Loss 37.715755 Test MSE 19.059177829998234 Test RE 1.0433503011247596\n",
      "151 Train Loss 37.713223 Test MSE 19.06115180043008 Test RE 1.0434043299327855\n",
      "152 Train Loss 37.711285 Test MSE 19.06009390243181 Test RE 1.0433753749493853\n",
      "153 Train Loss 37.70876 Test MSE 19.060087639055162 Test RE 1.0433752035165076\n",
      "154 Train Loss 37.70666 Test MSE 19.056322815801504 Test RE 1.0432721526466318\n",
      "155 Train Loss 37.704876 Test MSE 19.054048988337303 Test RE 1.0432099084333946\n",
      "156 Train Loss 37.70336 Test MSE 19.05151736197633 Test RE 1.0431406028113526\n",
      "157 Train Loss 37.701324 Test MSE 19.05787147879467 Test RE 1.0433145439349716\n",
      "158 Train Loss 37.700085 Test MSE 19.05444150496345 Test RE 1.0432206535281787\n",
      "159 Train Loss 37.699055 Test MSE 19.0557450784177 Test RE 1.0432563378984687\n",
      "160 Train Loss 37.69751 Test MSE 19.053490977333183 Test RE 1.0431946327596808\n",
      "161 Train Loss 37.69634 Test MSE 19.053454916080188 Test RE 1.0431936455672701\n",
      "162 Train Loss 37.69492 Test MSE 19.050542164174736 Test RE 1.0431139046376205\n",
      "163 Train Loss 37.6929 Test MSE 19.045994728061626 Test RE 1.042989399599774\n",
      "164 Train Loss 37.691456 Test MSE 19.04705729239625 Test RE 1.0430184930626414\n",
      "165 Train Loss 37.6903 Test MSE 19.045799533283986 Test RE 1.0429840549956078\n",
      "166 Train Loss 37.689243 Test MSE 19.04449619064617 Test RE 1.0429483676293752\n",
      "167 Train Loss 37.6888 Test MSE 19.043265956176906 Test RE 1.0429146809479248\n",
      "168 Train Loss 37.6888 Test MSE 19.043295629433544 Test RE 1.0429154934835214\n",
      "169 Train Loss 37.68791 Test MSE 19.04337798852814 Test RE 1.0429177486993355\n",
      "170 Train Loss 37.68683 Test MSE 19.037652731068185 Test RE 1.0427609639767235\n",
      "171 Train Loss 37.685856 Test MSE 19.040964851659716 Test RE 1.0428516684323648\n",
      "172 Train Loss 37.684822 Test MSE 19.03902315147875 Test RE 1.0427984947378044\n",
      "173 Train Loss 37.682476 Test MSE 19.037259423905297 Test RE 1.0427501924945362\n",
      "174 Train Loss 37.680187 Test MSE 19.034687280224222 Test RE 1.0426797465899973\n",
      "175 Train Loss 37.679024 Test MSE 19.03789681635574 Test RE 1.0427676486714579\n",
      "176 Train Loss 37.67832 Test MSE 19.039810052316458 Test RE 1.042820044439275\n",
      "177 Train Loss 37.678295 Test MSE 19.039058461498524 Test RE 1.0427994617311762\n",
      "178 Train Loss 37.678226 Test MSE 19.038766657208928 Test RE 1.0427914704084256\n",
      "179 Train Loss 37.678226 Test MSE 19.038766657208928 Test RE 1.0427914704084256\n",
      "180 Train Loss 37.678226 Test MSE 19.038766657208928 Test RE 1.0427914704084256\n",
      "181 Train Loss 37.678226 Test MSE 19.038766657208928 Test RE 1.0427914704084256\n",
      "182 Train Loss 37.678226 Test MSE 19.038766657208928 Test RE 1.0427914704084256\n",
      "183 Train Loss 37.678226 Test MSE 19.038766657208928 Test RE 1.0427914704084256\n",
      "184 Train Loss 37.678226 Test MSE 19.038766657208928 Test RE 1.0427914704084256\n",
      "185 Train Loss 37.678226 Test MSE 19.038766657208928 Test RE 1.0427914704084256\n",
      "186 Train Loss 37.678226 Test MSE 19.038766657208928 Test RE 1.0427914704084256\n",
      "187 Train Loss 37.678226 Test MSE 19.038766657208928 Test RE 1.0427914704084256\n",
      "188 Train Loss 37.678226 Test MSE 19.038766657208928 Test RE 1.0427914704084256\n",
      "189 Train Loss 37.678226 Test MSE 19.038766657208928 Test RE 1.0427914704084256\n",
      "190 Train Loss 37.678226 Test MSE 19.038766657208928 Test RE 1.0427914704084256\n",
      "191 Train Loss 37.678226 Test MSE 19.038766657208928 Test RE 1.0427914704084256\n",
      "192 Train Loss 37.678226 Test MSE 19.038766657208928 Test RE 1.0427914704084256\n",
      "193 Train Loss 37.678226 Test MSE 19.038766657208928 Test RE 1.0427914704084256\n",
      "194 Train Loss 37.678226 Test MSE 19.038766657208928 Test RE 1.0427914704084256\n",
      "195 Train Loss 37.678226 Test MSE 19.038766657208928 Test RE 1.0427914704084256\n",
      "196 Train Loss 37.678226 Test MSE 19.038766657208928 Test RE 1.0427914704084256\n",
      "197 Train Loss 37.678226 Test MSE 19.038766657208928 Test RE 1.0427914704084256\n",
      "198 Train Loss 37.678226 Test MSE 19.038766657208928 Test RE 1.0427914704084256\n",
      "199 Train Loss 37.678226 Test MSE 19.038766657208928 Test RE 1.0427914704084256\n",
      "Training time: 92.27\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 587.05896 Test MSE 36.54438195322886 Test RE 1.4447356028722877\n",
      "1 Train Loss 408.0057 Test MSE 33.97276341432249 Test RE 1.3929755768501837\n",
      "2 Train Loss 395.52252 Test MSE 34.85652104238185 Test RE 1.4109774859825601\n",
      "3 Train Loss 393.4836 Test MSE 34.86934575512242 Test RE 1.4112370316321818\n",
      "4 Train Loss 386.38586 Test MSE 34.900297843127134 Test RE 1.4118632413122596\n",
      "5 Train Loss 373.32336 Test MSE 34.05923388845337 Test RE 1.3947472123911435\n",
      "6 Train Loss 361.398 Test MSE 33.53430105909285 Test RE 1.3839573105026473\n",
      "7 Train Loss 353.9914 Test MSE 33.3215682553445 Test RE 1.3795605955509114\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 Train Loss 344.95898 Test MSE 32.39874300940203 Test RE 1.3603233259016303\n",
      "9 Train Loss 339.22165 Test MSE 32.011517037409966 Test RE 1.3521696774787975\n",
      "10 Train Loss 333.75104 Test MSE 32.170130976109036 Test RE 1.3555154723861043\n",
      "11 Train Loss 306.9477 Test MSE 31.918457467094434 Test RE 1.3502028242225037\n",
      "12 Train Loss 260.66348 Test MSE 31.58465616903137 Test RE 1.3431240986158084\n",
      "13 Train Loss 223.02979 Test MSE 31.80945843040752 Test RE 1.3478954341095022\n",
      "14 Train Loss 155.29765 Test MSE 31.09188702180636 Test RE 1.3326055113831268\n",
      "15 Train Loss 131.64496 Test MSE 31.170709882175583 Test RE 1.334293625218259\n",
      "16 Train Loss 111.68281 Test MSE 31.100692826497298 Test RE 1.3327942074423846\n",
      "17 Train Loss 103.8443 Test MSE 30.864459968012135 Test RE 1.3277227778741163\n",
      "18 Train Loss 97.18713 Test MSE 30.65335668056561 Test RE 1.3231743821156368\n",
      "19 Train Loss 93.325096 Test MSE 30.87395768071454 Test RE 1.3279270477614236\n",
      "20 Train Loss 90.18947 Test MSE 31.11726720652144 Test RE 1.3331493007433226\n",
      "21 Train Loss 87.84099 Test MSE 30.975285347913065 Test RE 1.3301043767886318\n",
      "22 Train Loss 85.70981 Test MSE 30.671196115352757 Test RE 1.3235593521863358\n",
      "23 Train Loss 84.223976 Test MSE 30.54946030950471 Test RE 1.3209300976890177\n",
      "24 Train Loss 82.996376 Test MSE 30.486888051801692 Test RE 1.3195766213201419\n",
      "25 Train Loss 81.38059 Test MSE 30.237755609832224 Test RE 1.314173909810655\n",
      "26 Train Loss 80.45455 Test MSE 30.067515920773552 Test RE 1.3104692641248592\n",
      "27 Train Loss 78.46794 Test MSE 28.474708605134065 Test RE 1.2752863429069918\n",
      "28 Train Loss 74.14669 Test MSE 27.655934349269224 Test RE 1.25681753818637\n",
      "29 Train Loss 71.318726 Test MSE 27.775855743961582 Test RE 1.259539490670919\n",
      "30 Train Loss 67.8617 Test MSE 27.037933813008106 Test RE 1.2426957552963613\n",
      "31 Train Loss 64.19812 Test MSE 27.017437652126553 Test RE 1.2422246519730897\n",
      "32 Train Loss 60.52386 Test MSE 26.711922768892418 Test RE 1.2351811059208473\n",
      "33 Train Loss 58.262142 Test MSE 26.722100966089624 Test RE 1.2354164075803309\n",
      "34 Train Loss 56.545956 Test MSE 26.48660052370369 Test RE 1.2299605311272872\n",
      "35 Train Loss 54.83875 Test MSE 26.50762444520964 Test RE 1.230448579118101\n",
      "36 Train Loss 53.013927 Test MSE 25.213510211275263 Test RE 1.2000372323632307\n",
      "37 Train Loss 51.21306 Test MSE 24.745693971030583 Test RE 1.1888522484084498\n",
      "38 Train Loss 50.469223 Test MSE 24.89309433518606 Test RE 1.1923877538992615\n",
      "39 Train Loss 49.851387 Test MSE 24.913951015828438 Test RE 1.192887170389142\n",
      "40 Train Loss 49.459007 Test MSE 24.868292441511773 Test RE 1.1917935962616875\n",
      "41 Train Loss 49.15705 Test MSE 24.845100700564938 Test RE 1.1912377435526382\n",
      "42 Train Loss 48.888447 Test MSE 24.86290968807237 Test RE 1.1916646071428822\n",
      "43 Train Loss 48.75149 Test MSE 24.855362736235477 Test RE 1.1914837329394017\n",
      "44 Train Loss 48.591805 Test MSE 24.862122677452312 Test RE 1.19164574651605\n",
      "45 Train Loss 48.394753 Test MSE 24.951645536633542 Test RE 1.1937892415838922\n",
      "46 Train Loss 48.246548 Test MSE 25.01525772266658 Test RE 1.1953100071059128\n",
      "47 Train Loss 48.106792 Test MSE 24.991019079347016 Test RE 1.1947307663292142\n",
      "48 Train Loss 48.00096 Test MSE 24.95785458520245 Test RE 1.1939377655420336\n",
      "49 Train Loss 47.90426 Test MSE 24.950951539304484 Test RE 1.1937726396266226\n",
      "50 Train Loss 47.809658 Test MSE 24.958087919861313 Test RE 1.1939433466790086\n",
      "51 Train Loss 47.712223 Test MSE 25.00610894645843 Test RE 1.1950914080428126\n",
      "52 Train Loss 47.651318 Test MSE 25.04376846254152 Test RE 1.1959909808598488\n",
      "53 Train Loss 47.552647 Test MSE 25.045105472366373 Test RE 1.1960229055750262\n",
      "54 Train Loss 47.469078 Test MSE 25.098612486578908 Test RE 1.1972998311337448\n",
      "55 Train Loss 47.349636 Test MSE 25.04933577287004 Test RE 1.1961239097947005\n",
      "56 Train Loss 47.25093 Test MSE 25.033729480601803 Test RE 1.1957512458666284\n",
      "57 Train Loss 47.158318 Test MSE 25.060255401773727 Test RE 1.1963845914793898\n",
      "58 Train Loss 47.064186 Test MSE 25.035752569687507 Test RE 1.1957995619280393\n",
      "59 Train Loss 46.99327 Test MSE 24.9997213085788 Test RE 1.1949387593694953\n",
      "60 Train Loss 46.933804 Test MSE 25.03153790824188 Test RE 1.1956989038306813\n",
      "61 Train Loss 46.858814 Test MSE 25.07222764493401 Test RE 1.1966703367104605\n",
      "62 Train Loss 46.79425 Test MSE 25.05219192300873 Test RE 1.1961920994689623\n",
      "63 Train Loss 46.695156 Test MSE 25.04910438151453 Test RE 1.1961183852296076\n",
      "64 Train Loss 46.623898 Test MSE 25.037891042750445 Test RE 1.1958506315042472\n",
      "65 Train Loss 46.57781 Test MSE 25.046624405230077 Test RE 1.1960591731594339\n",
      "66 Train Loss 46.489082 Test MSE 24.98510623774471 Test RE 1.1945894221196414\n",
      "67 Train Loss 46.37744 Test MSE 24.962264974582727 Test RE 1.1940432533317198\n",
      "68 Train Loss 46.233032 Test MSE 24.85358278170645 Test RE 1.1914410696144022\n",
      "69 Train Loss 46.045727 Test MSE 24.48379251760318 Test RE 1.1825442749259554\n",
      "70 Train Loss 45.881138 Test MSE 24.39267609566941 Test RE 1.1803418049214902\n",
      "71 Train Loss 45.731094 Test MSE 24.298582301547327 Test RE 1.1780630440638131\n",
      "72 Train Loss 45.590736 Test MSE 24.122123628913815 Test RE 1.1737776453256084\n",
      "73 Train Loss 45.408516 Test MSE 24.00084459845388 Test RE 1.170823220631765\n",
      "74 Train Loss 45.27469 Test MSE 23.968997978377793 Test RE 1.1700461825726296\n",
      "75 Train Loss 45.11458 Test MSE 23.94290281649241 Test RE 1.169409091707724\n",
      "76 Train Loss 44.91859 Test MSE 24.034561515838906 Test RE 1.1716453311826667\n",
      "77 Train Loss 44.747795 Test MSE 23.950690689623737 Test RE 1.169599262240989\n",
      "78 Train Loss 44.538857 Test MSE 23.83831413310106 Test RE 1.1668521582882676\n",
      "79 Train Loss 44.322483 Test MSE 23.83423143612929 Test RE 1.1667522328544069\n",
      "80 Train Loss 44.093502 Test MSE 23.752481309645948 Test RE 1.1647495655493398\n",
      "81 Train Loss 43.81993 Test MSE 23.612438686701815 Test RE 1.1613108569582806\n",
      "82 Train Loss 43.563816 Test MSE 23.435248077445763 Test RE 1.1569453428030436\n",
      "83 Train Loss 43.296318 Test MSE 23.262485813144668 Test RE 1.1526730127682856\n",
      "84 Train Loss 42.84691 Test MSE 22.771105314867288 Test RE 1.1404339055032489\n",
      "85 Train Loss 41.698536 Test MSE 21.337427340906412 Test RE 1.1039492079982174\n",
      "86 Train Loss 40.42857 Test MSE 20.847361604749974 Test RE 1.0911981332297\n",
      "87 Train Loss 39.8722 Test MSE 20.756101115186492 Test RE 1.0888071232839098\n",
      "88 Train Loss 39.507355 Test MSE 20.401206552543538 Test RE 1.0794586009436102\n",
      "89 Train Loss 39.1866 Test MSE 20.268877765304733 Test RE 1.0759520477016702\n",
      "90 Train Loss 39.01413 Test MSE 20.099647890725333 Test RE 1.0714509378810855\n",
      "91 Train Loss 38.86541 Test MSE 19.934270811307172 Test RE 1.0670339596416756\n",
      "92 Train Loss 38.702496 Test MSE 19.739027785949204 Test RE 1.0617956549665537\n",
      "93 Train Loss 38.5876 Test MSE 19.62671058932651 Test RE 1.0587704795470534\n",
      "94 Train Loss 38.48892 Test MSE 19.57708591544443 Test RE 1.0574311212993865\n",
      "95 Train Loss 38.423134 Test MSE 19.565378261038433 Test RE 1.0571148870641889\n",
      "96 Train Loss 38.37329 Test MSE 19.494111630381205 Test RE 1.055187867203854\n",
      "97 Train Loss 38.324387 Test MSE 19.437024819372866 Test RE 1.0536417214723202\n",
      "98 Train Loss 38.27141 Test MSE 19.367379823030745 Test RE 1.051752370466107\n",
      "99 Train Loss 38.204586 Test MSE 19.234704280795857 Test RE 1.048143683566573\n",
      "100 Train Loss 38.162254 Test MSE 19.19823715985019 Test RE 1.0471496231490425\n",
      "101 Train Loss 38.13431 Test MSE 19.167816182442067 Test RE 1.04631965256303\n",
      "102 Train Loss 38.11245 Test MSE 19.142977105568814 Test RE 1.0456414835083605\n",
      "103 Train Loss 38.088272 Test MSE 19.13656878658417 Test RE 1.0454664489508168\n",
      "104 Train Loss 38.075855 Test MSE 19.121012585281047 Test RE 1.045041430411024\n",
      "105 Train Loss 38.06467 Test MSE 19.110598050590866 Test RE 1.0447567932202642\n",
      "106 Train Loss 38.052353 Test MSE 19.110422447278683 Test RE 1.0447519931827594\n",
      "107 Train Loss 38.047523 Test MSE 19.110442769457574 Test RE 1.0447525486815015\n",
      "108 Train Loss 38.03886 Test MSE 19.105499402318618 Test RE 1.044617414987606\n",
      "109 Train Loss 38.033604 Test MSE 19.106552666063713 Test RE 1.0446462088564283\n",
      "110 Train Loss 38.02657 Test MSE 19.09831674464529 Test RE 1.0444210360691333\n",
      "111 Train Loss 38.017242 Test MSE 19.094751453439105 Test RE 1.0443235447827082\n",
      "112 Train Loss 38.01296 Test MSE 19.096410679245867 Test RE 1.044368916699175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "113 Train Loss 38.007362 Test MSE 19.103053777248842 Test RE 1.044550554021408\n",
      "114 Train Loss 38.00056 Test MSE 19.102251507222842 Test RE 1.0445286198215527\n",
      "115 Train Loss 37.99416 Test MSE 19.102442390275147 Test RE 1.0445338386386382\n",
      "116 Train Loss 37.988342 Test MSE 19.11070551797203 Test RE 1.0447597307818826\n",
      "117 Train Loss 37.98345 Test MSE 19.104354392720158 Test RE 1.044586112092481\n",
      "118 Train Loss 37.976215 Test MSE 19.098347431578407 Test RE 1.0444218751500332\n",
      "119 Train Loss 37.96917 Test MSE 19.084776842027377 Test RE 1.0440507451527783\n",
      "120 Train Loss 37.96493 Test MSE 19.089832683855118 Test RE 1.0441890282966058\n",
      "121 Train Loss 37.960308 Test MSE 19.080102767357243 Test RE 1.0439228874888118\n",
      "122 Train Loss 37.953598 Test MSE 19.084801807665254 Test RE 1.0440514280369566\n",
      "123 Train Loss 37.94735 Test MSE 19.082897541749034 Test RE 1.043999339438769\n",
      "124 Train Loss 37.9424 Test MSE 19.07695974223854 Test RE 1.043836902339002\n",
      "125 Train Loss 37.937622 Test MSE 19.075757111490734 Test RE 1.0438039995558368\n",
      "126 Train Loss 37.932373 Test MSE 19.066381091122274 Test RE 1.043547445376106\n",
      "127 Train Loss 37.92942 Test MSE 19.06107711073372 Test RE 1.0434022856799055\n",
      "128 Train Loss 37.9259 Test MSE 19.06098004722067 Test RE 1.043399629050909\n",
      "129 Train Loss 37.921104 Test MSE 19.05705990028953 Test RE 1.0432923289495888\n",
      "130 Train Loss 37.917004 Test MSE 19.052276694604032 Test RE 1.0431613907304342\n",
      "131 Train Loss 37.91339 Test MSE 19.052111637633914 Test RE 1.0431568720727908\n",
      "132 Train Loss 37.908043 Test MSE 19.04680205962995 Test RE 1.0430115047551294\n",
      "133 Train Loss 37.904423 Test MSE 19.037279189237257 Test RE 1.042750733809278\n",
      "134 Train Loss 37.89947 Test MSE 19.04578060500376 Test RE 1.0429835367212412\n",
      "135 Train Loss 37.895557 Test MSE 19.041674974210736 Test RE 1.0428711145473173\n",
      "136 Train Loss 37.890507 Test MSE 19.048759447552854 Test RE 1.04306509710274\n",
      "137 Train Loss 37.88815 Test MSE 19.05159194947825 Test RE 1.0431426447792986\n",
      "138 Train Loss 37.884842 Test MSE 19.052351400761907 Test RE 1.043163435906042\n",
      "139 Train Loss 37.88167 Test MSE 19.052304207733563 Test RE 1.0431621439376375\n",
      "140 Train Loss 37.878117 Test MSE 19.055593646221006 Test RE 1.0432521926154537\n",
      "141 Train Loss 37.875256 Test MSE 19.05501203532427 Test RE 1.043236271529905\n",
      "142 Train Loss 37.87212 Test MSE 19.04964686618209 Test RE 1.0430893932943346\n",
      "143 Train Loss 37.86891 Test MSE 19.042921348131095 Test RE 1.0429052445829412\n",
      "144 Train Loss 37.86584 Test MSE 19.050067010382776 Test RE 1.0431008960152532\n",
      "145 Train Loss 37.86367 Test MSE 19.05325512103158 Test RE 1.0431881760744026\n",
      "146 Train Loss 37.860836 Test MSE 19.05314861011933 Test RE 1.0431852602713327\n",
      "147 Train Loss 37.85857 Test MSE 19.048147496354918 Test RE 1.0430483424667225\n",
      "148 Train Loss 37.8561 Test MSE 19.05045919135693 Test RE 1.0431116330436294\n",
      "149 Train Loss 37.85348 Test MSE 19.049030597384814 Test RE 1.043072520838656\n",
      "150 Train Loss 37.84994 Test MSE 19.049608463295794 Test RE 1.043088341892599\n",
      "151 Train Loss 37.847973 Test MSE 19.050905205798443 Test RE 1.0431238437763135\n",
      "152 Train Loss 37.84493 Test MSE 19.049124572912774 Test RE 1.0430750937561977\n",
      "153 Train Loss 37.84227 Test MSE 19.045499797655303 Test RE 1.042975847917956\n",
      "154 Train Loss 37.83917 Test MSE 19.0512333152434 Test RE 1.0431328264810213\n",
      "155 Train Loss 37.83638 Test MSE 19.05094367502011 Test RE 1.0431248969583458\n",
      "156 Train Loss 37.83393 Test MSE 19.047188395918944 Test RE 1.043022082676545\n",
      "157 Train Loss 37.831905 Test MSE 19.05005607754202 Test RE 1.0431005966972084\n",
      "158 Train Loss 37.830177 Test MSE 19.048438403868637 Test RE 1.0430563072685897\n",
      "159 Train Loss 37.827705 Test MSE 19.045685312318508 Test RE 1.0429809275127169\n",
      "160 Train Loss 37.8255 Test MSE 19.043983166634984 Test RE 1.0429343199708527\n",
      "161 Train Loss 37.823326 Test MSE 19.0401712509159 Test RE 1.0428299359075226\n",
      "162 Train Loss 37.820362 Test MSE 19.043399291738666 Test RE 1.0429183320383357\n",
      "163 Train Loss 37.818638 Test MSE 19.04377655204603 Test RE 1.0429286623826157\n",
      "164 Train Loss 37.816097 Test MSE 19.047661410832212 Test RE 1.043035033720204\n",
      "165 Train Loss 37.814423 Test MSE 19.05031841659042 Test RE 1.0431077789615213\n",
      "166 Train Loss 37.81294 Test MSE 19.0508639774411 Test RE 1.0431227150555151\n",
      "167 Train Loss 37.81205 Test MSE 19.044175504442585 Test RE 1.0429395866000113\n",
      "168 Train Loss 37.810207 Test MSE 19.047873775324625 Test RE 1.0430408481607103\n",
      "169 Train Loss 37.808353 Test MSE 19.047553832716183 Test RE 1.043032088269319\n",
      "170 Train Loss 37.805557 Test MSE 19.043229625570934 Test RE 1.0429136861148829\n",
      "171 Train Loss 37.803463 Test MSE 19.044884367366837 Test RE 1.0429589965851156\n",
      "172 Train Loss 37.801273 Test MSE 19.042987435760477 Test RE 1.042907054259992\n",
      "173 Train Loss 37.798138 Test MSE 19.0379609108169 Test RE 1.0427694040012339\n",
      "174 Train Loss 37.79371 Test MSE 19.04250328152652 Test RE 1.0428937965955356\n",
      "175 Train Loss 37.79042 Test MSE 19.045323127594926 Test RE 1.0429710104751142\n",
      "176 Train Loss 37.786995 Test MSE 19.049154064225124 Test RE 1.0430759011854671\n",
      "177 Train Loss 37.784595 Test MSE 19.04916179278761 Test RE 1.0430761127821724\n",
      "178 Train Loss 37.782917 Test MSE 19.049554328144698 Test RE 1.0430868597680394\n",
      "179 Train Loss 37.780807 Test MSE 19.048670631854375 Test RE 1.0430626654309192\n",
      "180 Train Loss 37.777916 Test MSE 19.04719334441165 Test RE 1.0430222181660131\n",
      "181 Train Loss 37.776203 Test MSE 19.045356167145766 Test RE 1.0429719151402266\n",
      "182 Train Loss 37.77457 Test MSE 19.04400377475499 Test RE 1.0429348842674502\n",
      "183 Train Loss 37.772472 Test MSE 19.04160580458224 Test RE 1.0428692204105574\n",
      "184 Train Loss 37.770332 Test MSE 19.046447334241915 Test RE 1.0430017922484776\n",
      "185 Train Loss 37.769028 Test MSE 19.044069298991666 Test RE 1.0429366784661445\n",
      "186 Train Loss 37.767666 Test MSE 19.05068097557237 Test RE 1.0431177049450862\n",
      "187 Train Loss 37.766342 Test MSE 19.04872588456392 Test RE 1.043064178187316\n",
      "188 Train Loss 37.764763 Test MSE 19.04862756409372 Test RE 1.043061486283054\n",
      "189 Train Loss 37.763206 Test MSE 19.042315143864084 Test RE 1.0428886447496153\n",
      "190 Train Loss 37.761574 Test MSE 19.044611252998088 Test RE 1.042951518248579\n",
      "191 Train Loss 37.760117 Test MSE 19.040627441212106 Test RE 1.0428424286010383\n",
      "192 Train Loss 37.758297 Test MSE 19.042865221072347 Test RE 1.042903707653654\n",
      "193 Train Loss 37.756332 Test MSE 19.03883194799693 Test RE 1.042793258460658\n",
      "194 Train Loss 37.75497 Test MSE 19.04191189565051 Test RE 1.0428776023630624\n",
      "195 Train Loss 37.753284 Test MSE 19.049671016115887 Test RE 1.0430900544754051\n",
      "196 Train Loss 37.752186 Test MSE 19.045290183441182 Test RE 1.0429701084213143\n",
      "197 Train Loss 37.75051 Test MSE 19.0442291631153 Test RE 1.0429410558868941\n",
      "198 Train Loss 37.748722 Test MSE 19.046645919220325 Test RE 1.0430072295866035\n",
      "199 Train Loss 37.7473 Test MSE 19.04550686976784 Test RE 1.0429760415606004\n",
      "Training time: 101.47\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 651.51117 Test MSE 36.16688672020004 Test RE 1.437254336112985\n",
      "1 Train Loss 435.32513 Test MSE 35.6019980343841 Test RE 1.4259859632897505\n",
      "2 Train Loss 409.3081 Test MSE 34.94045028630527 Test RE 1.4126751751093538\n",
      "3 Train Loss 399.0537 Test MSE 35.17185379726598 Test RE 1.417345385893935\n",
      "4 Train Loss 393.02655 Test MSE 34.999531618486046 Test RE 1.4138690274824934\n",
      "5 Train Loss 386.9518 Test MSE 34.90534391550771 Test RE 1.4119653050057854\n",
      "6 Train Loss 384.85617 Test MSE 34.91541825931667 Test RE 1.4121690502795392\n",
      "7 Train Loss 384.11194 Test MSE 34.907132257236626 Test RE 1.4120014748848044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8 Train Loss 383.5522 Test MSE 35.04538626898195 Test RE 1.414794914857732\n",
      "9 Train Loss 382.80704 Test MSE 34.8538137188338 Test RE 1.4109226892524642\n",
      "10 Train Loss 382.33453 Test MSE 34.985346385448125 Test RE 1.4135824794446523\n",
      "11 Train Loss 382.04645 Test MSE 35.02040544294969 Test RE 1.4142905824784175\n",
      "12 Train Loss 381.1192 Test MSE 35.01223819710363 Test RE 1.4141256567405147\n",
      "13 Train Loss 380.30835 Test MSE 35.24157860741567 Test RE 1.4187495656348097\n",
      "14 Train Loss 378.88034 Test MSE 34.95469047860133 Test RE 1.412963017949396\n",
      "15 Train Loss 378.30606 Test MSE 34.97518178659283 Test RE 1.4133771142826614\n",
      "16 Train Loss 377.86783 Test MSE 34.9753942937065 Test RE 1.4133814080735658\n",
      "17 Train Loss 376.04572 Test MSE 34.982949009373044 Test RE 1.4135340456398664\n",
      "18 Train Loss 374.46252 Test MSE 35.11564247771491 Test RE 1.4162123391028627\n",
      "19 Train Loss 373.11264 Test MSE 35.199866398936884 Test RE 1.4179096954926993\n",
      "20 Train Loss 367.05365 Test MSE 35.692895228959124 Test RE 1.427805179943028\n",
      "21 Train Loss 365.98328 Test MSE 35.98456989164105 Test RE 1.4336271685464692\n",
      "22 Train Loss 363.59317 Test MSE 35.972081382441715 Test RE 1.4333783755249256\n",
      "23 Train Loss 358.1236 Test MSE 36.10365645860527 Test RE 1.4359974167186158\n",
      "24 Train Loss 355.87985 Test MSE 35.6803615848158 Test RE 1.4275544693574456\n",
      "25 Train Loss 354.2798 Test MSE 35.2331745140072 Test RE 1.418580390256333\n",
      "26 Train Loss 353.45337 Test MSE 35.04488734966494 Test RE 1.4147848440452793\n",
      "27 Train Loss 351.82166 Test MSE 35.267638130016 Test RE 1.4192740186473563\n",
      "28 Train Loss 348.07306 Test MSE 35.913116672876356 Test RE 1.4322031112626223\n",
      "29 Train Loss 343.6149 Test MSE 35.41958416616352 Test RE 1.4223281114856043\n",
      "30 Train Loss 341.54822 Test MSE 35.02357403856663 Test RE 1.4143545625140985\n",
      "31 Train Loss 340.6982 Test MSE 34.70833343129559 Test RE 1.4079750049707163\n",
      "32 Train Loss 339.05292 Test MSE 35.226114568303096 Test RE 1.4184382571329133\n",
      "33 Train Loss 335.952 Test MSE 36.56681330764315 Test RE 1.4451789323682782\n",
      "34 Train Loss 331.85193 Test MSE 37.080742573861954 Test RE 1.45529915090467\n",
      "35 Train Loss 328.16248 Test MSE 37.784195727924384 Test RE 1.4690384255683666\n",
      "36 Train Loss 323.1515 Test MSE 35.84219258898484 Test RE 1.4307881979448607\n",
      "37 Train Loss 320.40894 Test MSE 36.59262499084517 Test RE 1.4456889018867312\n",
      "38 Train Loss 315.36737 Test MSE 36.44561737721427 Test RE 1.4427820161718141\n",
      "39 Train Loss 306.03903 Test MSE 37.65775619132309 Test RE 1.4665783997985617\n",
      "40 Train Loss 284.60944 Test MSE 35.60624128539497 Test RE 1.4260709393645816\n",
      "41 Train Loss 263.37805 Test MSE 35.442495066989395 Test RE 1.4227880484263964\n",
      "42 Train Loss 226.54338 Test MSE 34.630302773146774 Test RE 1.406391422237213\n",
      "43 Train Loss 204.3766 Test MSE 34.45045640106183 Test RE 1.402734745529402\n",
      "44 Train Loss 188.9672 Test MSE 34.10952050463793 Test RE 1.3957764670222441\n",
      "45 Train Loss 178.61179 Test MSE 32.50706442835994 Test RE 1.3625954695251987\n",
      "46 Train Loss 165.13663 Test MSE 33.18378631302881 Test RE 1.3767054562908885\n",
      "47 Train Loss 150.66052 Test MSE 32.947563173780615 Test RE 1.371796575297049\n",
      "48 Train Loss 142.90009 Test MSE 33.238379474956346 Test RE 1.3778374521438488\n",
      "49 Train Loss 138.90266 Test MSE 32.91285403617628 Test RE 1.3710738143700059\n",
      "50 Train Loss 134.0195 Test MSE 33.095910716447584 Test RE 1.374881387590009\n",
      "51 Train Loss 123.89471 Test MSE 33.9429023266824 Test RE 1.3923632494150433\n",
      "52 Train Loss 118.27122 Test MSE 34.34152243601656 Test RE 1.4005152329367816\n",
      "53 Train Loss 109.2507 Test MSE 34.20430869724057 Test RE 1.3977145086397058\n",
      "54 Train Loss 105.5244 Test MSE 34.4596540442012 Test RE 1.4029219853214303\n",
      "55 Train Loss 102.55032 Test MSE 34.56455664449481 Test RE 1.405055760806081\n",
      "56 Train Loss 98.93818 Test MSE 34.48801237310854 Test RE 1.4034991289762708\n",
      "57 Train Loss 95.4666 Test MSE 34.59430956014714 Test RE 1.4056603615476042\n",
      "58 Train Loss 92.66305 Test MSE 34.318747800513044 Test RE 1.4000507585675814\n",
      "59 Train Loss 91.47111 Test MSE 34.304446368 Test RE 1.3997590110659552\n",
      "60 Train Loss 89.87718 Test MSE 34.33928841143351 Test RE 1.4004696782245356\n",
      "61 Train Loss 88.77521 Test MSE 34.60493579343988 Test RE 1.4058762312711095\n",
      "62 Train Loss 87.79762 Test MSE 34.518060422691796 Test RE 1.4041104026729068\n",
      "63 Train Loss 87.13191 Test MSE 34.448015731615925 Test RE 1.402685055730905\n",
      "64 Train Loss 85.83917 Test MSE 34.22662697165654 Test RE 1.3981704378807818\n",
      "65 Train Loss 85.4817 Test MSE 34.1350099416155 Test RE 1.3962978890714035\n",
      "66 Train Loss 84.9626 Test MSE 34.1275730392936 Test RE 1.3961457769629946\n",
      "67 Train Loss 84.37218 Test MSE 34.026692023949025 Test RE 1.3940807481726922\n",
      "68 Train Loss 83.79929 Test MSE 33.83685357035803 Test RE 1.3901864481598167\n",
      "69 Train Loss 83.16147 Test MSE 33.748870902671115 Test RE 1.3883778886735312\n",
      "70 Train Loss 82.82945 Test MSE 33.76284875803425 Test RE 1.3886653729033698\n",
      "71 Train Loss 82.22981 Test MSE 33.791364851925174 Test RE 1.389251682349254\n",
      "72 Train Loss 81.91897 Test MSE 33.80154055255498 Test RE 1.389460841481931\n",
      "73 Train Loss 81.79064 Test MSE 33.85858840952752 Test RE 1.3906328641983516\n",
      "74 Train Loss 81.269135 Test MSE 33.71533408148748 Test RE 1.387687889974325\n",
      "75 Train Loss 80.92758 Test MSE 33.745945439700556 Test RE 1.3883177127918367\n",
      "76 Train Loss 80.659195 Test MSE 33.79545569271017 Test RE 1.389335772404115\n",
      "77 Train Loss 80.36798 Test MSE 33.75939014100979 Test RE 1.3885942446531998\n",
      "78 Train Loss 80.29821 Test MSE 33.79297472179829 Test RE 1.3892847749426176\n",
      "79 Train Loss 80.10795 Test MSE 33.84331848694088 Test RE 1.3903192473007902\n",
      "80 Train Loss 79.90906 Test MSE 33.63265283904967 Test RE 1.3859853087756449\n",
      "81 Train Loss 79.751656 Test MSE 33.645092601509745 Test RE 1.3862416033671279\n",
      "82 Train Loss 79.62953 Test MSE 33.65689669361787 Test RE 1.3864847575290562\n",
      "83 Train Loss 79.49619 Test MSE 33.672032343968176 Test RE 1.3867964765494867\n",
      "84 Train Loss 79.39678 Test MSE 33.60644867193228 Test RE 1.3854452730579883\n",
      "85 Train Loss 79.26343 Test MSE 33.597749788651264 Test RE 1.385265953207326\n",
      "86 Train Loss 79.12659 Test MSE 33.58609758179365 Test RE 1.3850257168030335\n",
      "87 Train Loss 78.952225 Test MSE 33.638727911889895 Test RE 1.3861104785259473\n",
      "88 Train Loss 78.76904 Test MSE 33.44326832004016 Test RE 1.3820775778486913\n",
      "89 Train Loss 78.65055 Test MSE 33.48786008160313 Test RE 1.382998671198135\n",
      "90 Train Loss 78.46861 Test MSE 33.47714374871424 Test RE 1.3827773692102343\n",
      "91 Train Loss 78.30983 Test MSE 33.46207307365782 Test RE 1.382466086168138\n",
      "92 Train Loss 78.22923 Test MSE 33.483596960713434 Test RE 1.3829106381293603\n",
      "93 Train Loss 78.18968 Test MSE 33.449713244884144 Test RE 1.3822107430186361\n",
      "94 Train Loss 78.149994 Test MSE 33.48249931254953 Test RE 1.3828879708847837\n",
      "95 Train Loss 78.052826 Test MSE 33.472762830046506 Test RE 1.3826868890689903\n",
      "96 Train Loss 77.88147 Test MSE 33.46262008490849 Test RE 1.3824773858477877\n",
      "97 Train Loss 77.82412 Test MSE 33.50848943251705 Test RE 1.3834245863257693\n",
      "98 Train Loss 77.80062 Test MSE 33.50764691400905 Test RE 1.3834071941937338\n",
      "99 Train Loss 77.74388 Test MSE 33.511833531384795 Test RE 1.3834936164917377\n",
      "100 Train Loss 77.59876 Test MSE 33.46138398269901 Test RE 1.3824518513987727\n",
      "101 Train Loss 77.558945 Test MSE 33.421809264317616 Test RE 1.3816340978848574\n",
      "102 Train Loss 77.49202 Test MSE 33.43242315014926 Test RE 1.3818534657589854\n",
      "103 Train Loss 77.384865 Test MSE 33.44902828245697 Test RE 1.3821965909201512\n",
      "104 Train Loss 77.27071 Test MSE 33.446889305318734 Test RE 1.3821523963003561\n",
      "105 Train Loss 77.20491 Test MSE 33.46713139299514 Test RE 1.3825705729802313\n",
      "106 Train Loss 77.17615 Test MSE 33.47483733602707 Test RE 1.3827297350658048\n",
      "107 Train Loss 77.14457 Test MSE 33.49063008207214 Test RE 1.3830558684592016\n",
      "108 Train Loss 77.10419 Test MSE 33.49118297327897 Test RE 1.3830672847310121\n",
      "109 Train Loss 77.07428 Test MSE 33.50043312158735 Test RE 1.383258270732477\n",
      "110 Train Loss 77.02992 Test MSE 33.53189692460927 Test RE 1.3839077004164875\n",
      "111 Train Loss 76.95333 Test MSE 33.48197287091809 Test RE 1.3828770993447843\n",
      "112 Train Loss 76.827065 Test MSE 33.494083238106406 Test RE 1.3831271687692255\n",
      "113 Train Loss 76.736916 Test MSE 33.50985454343063 Test RE 1.3834527658826363\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "114 Train Loss 76.70575 Test MSE 33.499221476957324 Test RE 1.3832332556438003\n",
      "115 Train Loss 76.692635 Test MSE 33.47361142914371 Test RE 1.3827044158476314\n",
      "116 Train Loss 76.64344 Test MSE 33.46119900382225 Test RE 1.382448030206795\n",
      "117 Train Loss 76.57849 Test MSE 33.47395204322445 Test RE 1.3827114507532678\n",
      "118 Train Loss 76.48453 Test MSE 33.376829479593766 Test RE 1.3807040685273708\n",
      "119 Train Loss 76.3879 Test MSE 33.306472012566324 Test RE 1.3792480571243995\n",
      "120 Train Loss 76.27792 Test MSE 33.36633928688036 Test RE 1.3804870768265904\n",
      "121 Train Loss 76.19798 Test MSE 33.36572775692778 Test RE 1.3804744261570412\n",
      "122 Train Loss 76.17783 Test MSE 33.358266980309644 Test RE 1.3803200763521677\n",
      "123 Train Loss 76.13654 Test MSE 33.39130536478181 Test RE 1.3810034490740246\n",
      "124 Train Loss 76.06306 Test MSE 33.297997987104445 Test RE 1.3790725878242756\n",
      "125 Train Loss 75.988144 Test MSE 33.34209563181161 Test RE 1.3799854615036\n",
      "126 Train Loss 75.946815 Test MSE 33.382551898076315 Test RE 1.38082242350566\n",
      "127 Train Loss 75.878494 Test MSE 33.392534457491 Test RE 1.3810288653557894\n",
      "128 Train Loss 75.75608 Test MSE 33.38826791075447 Test RE 1.380940635868003\n",
      "129 Train Loss 75.4896 Test MSE 33.337964011126324 Test RE 1.3798999576831252\n",
      "130 Train Loss 75.35446 Test MSE 33.311695657128396 Test RE 1.379356210565931\n",
      "131 Train Loss 75.292946 Test MSE 33.3089949631039 Test RE 1.3793002948506063\n",
      "132 Train Loss 75.26281 Test MSE 33.31468941876268 Test RE 1.379418191363455\n",
      "133 Train Loss 75.24513 Test MSE 33.32009875131453 Test RE 1.3795301754309652\n",
      "134 Train Loss 75.214226 Test MSE 33.339897772489856 Test RE 1.3799399775015073\n",
      "135 Train Loss 75.13917 Test MSE 33.397170137038074 Test RE 1.3811247218877463\n",
      "136 Train Loss 75.08379 Test MSE 33.43779489879346 Test RE 1.3819644758299687\n",
      "137 Train Loss 75.05576 Test MSE 33.38643715810555 Test RE 1.380902775332814\n",
      "138 Train Loss 75.030464 Test MSE 33.36975289915643 Test RE 1.380557691811784\n",
      "139 Train Loss 75.0132 Test MSE 33.401273834126286 Test RE 1.3812095725409763\n",
      "140 Train Loss 74.99643 Test MSE 33.40189740063451 Test RE 1.3812224653428318\n",
      "141 Train Loss 74.97785 Test MSE 33.40412310286196 Test RE 1.3812684827691943\n",
      "142 Train Loss 74.94825 Test MSE 33.41725641159535 Test RE 1.3815399888144348\n",
      "143 Train Loss 74.92126 Test MSE 33.42791247221332 Test RE 1.3817602432880023\n",
      "144 Train Loss 74.90715 Test MSE 33.44477986446551 Test RE 1.3821088105624564\n",
      "145 Train Loss 74.889885 Test MSE 33.43145123026563 Test RE 1.381833379559808\n",
      "146 Train Loss 74.836685 Test MSE 33.509082262932054 Test RE 1.3834368240087878\n",
      "147 Train Loss 74.77089 Test MSE 33.519967580280145 Test RE 1.3836615082594905\n",
      "148 Train Loss 74.730034 Test MSE 33.53260922646706 Test RE 1.3839223991791805\n",
      "149 Train Loss 74.7262 Test MSE 33.527968901526584 Test RE 1.3838266405735133\n",
      "150 Train Loss 74.7262 Test MSE 33.527968901526584 Test RE 1.3838266405735133\n",
      "151 Train Loss 74.7262 Test MSE 33.527968901526584 Test RE 1.3838266405735133\n",
      "152 Train Loss 74.7262 Test MSE 33.527968901526584 Test RE 1.3838266405735133\n",
      "153 Train Loss 74.7262 Test MSE 33.527968901526584 Test RE 1.3838266405735133\n",
      "154 Train Loss 74.7262 Test MSE 33.527968901526584 Test RE 1.3838266405735133\n",
      "155 Train Loss 74.7262 Test MSE 33.527968901526584 Test RE 1.3838266405735133\n",
      "156 Train Loss 74.7262 Test MSE 33.527968901526584 Test RE 1.3838266405735133\n",
      "157 Train Loss 74.7262 Test MSE 33.527968901526584 Test RE 1.3838266405735133\n",
      "158 Train Loss 74.7262 Test MSE 33.527968901526584 Test RE 1.3838266405735133\n",
      "159 Train Loss 74.7262 Test MSE 33.527968901526584 Test RE 1.3838266405735133\n",
      "160 Train Loss 74.7262 Test MSE 33.527968901526584 Test RE 1.3838266405735133\n",
      "161 Train Loss 74.7262 Test MSE 33.527968901526584 Test RE 1.3838266405735133\n",
      "162 Train Loss 74.7262 Test MSE 33.527968901526584 Test RE 1.3838266405735133\n",
      "163 Train Loss 74.7262 Test MSE 33.527968901526584 Test RE 1.3838266405735133\n",
      "164 Train Loss 74.7262 Test MSE 33.527968901526584 Test RE 1.3838266405735133\n",
      "165 Train Loss 74.7262 Test MSE 33.527968901526584 Test RE 1.3838266405735133\n",
      "166 Train Loss 74.7262 Test MSE 33.527968901526584 Test RE 1.3838266405735133\n",
      "167 Train Loss 74.7262 Test MSE 33.527968901526584 Test RE 1.3838266405735133\n",
      "168 Train Loss 74.7262 Test MSE 33.527968901526584 Test RE 1.3838266405735133\n",
      "169 Train Loss 74.7262 Test MSE 33.527968901526584 Test RE 1.3838266405735133\n",
      "170 Train Loss 74.7262 Test MSE 33.527968901526584 Test RE 1.3838266405735133\n",
      "171 Train Loss 74.7262 Test MSE 33.527968901526584 Test RE 1.3838266405735133\n",
      "172 Train Loss 74.7262 Test MSE 33.527968901526584 Test RE 1.3838266405735133\n",
      "173 Train Loss 74.7262 Test MSE 33.527968901526584 Test RE 1.3838266405735133\n",
      "174 Train Loss 74.7262 Test MSE 33.527968901526584 Test RE 1.3838266405735133\n",
      "175 Train Loss 74.7262 Test MSE 33.527968901526584 Test RE 1.3838266405735133\n",
      "176 Train Loss 74.7262 Test MSE 33.527968901526584 Test RE 1.3838266405735133\n",
      "177 Train Loss 74.7262 Test MSE 33.527968901526584 Test RE 1.3838266405735133\n",
      "178 Train Loss 74.7262 Test MSE 33.527968901526584 Test RE 1.3838266405735133\n",
      "179 Train Loss 74.7262 Test MSE 33.527968901526584 Test RE 1.3838266405735133\n",
      "180 Train Loss 74.7262 Test MSE 33.527968901526584 Test RE 1.3838266405735133\n",
      "181 Train Loss 74.7262 Test MSE 33.527968901526584 Test RE 1.3838266405735133\n",
      "182 Train Loss 74.7262 Test MSE 33.527968901526584 Test RE 1.3838266405735133\n",
      "183 Train Loss 74.7262 Test MSE 33.527968901526584 Test RE 1.3838266405735133\n",
      "184 Train Loss 74.7262 Test MSE 33.527968901526584 Test RE 1.3838266405735133\n",
      "185 Train Loss 74.7262 Test MSE 33.527968901526584 Test RE 1.3838266405735133\n",
      "186 Train Loss 74.7262 Test MSE 33.527968901526584 Test RE 1.3838266405735133\n",
      "187 Train Loss 74.7262 Test MSE 33.527968901526584 Test RE 1.3838266405735133\n",
      "188 Train Loss 74.7262 Test MSE 33.527968901526584 Test RE 1.3838266405735133\n",
      "189 Train Loss 74.7262 Test MSE 33.527968901526584 Test RE 1.3838266405735133\n",
      "190 Train Loss 74.7262 Test MSE 33.527968901526584 Test RE 1.3838266405735133\n",
      "191 Train Loss 74.7262 Test MSE 33.527968901526584 Test RE 1.3838266405735133\n",
      "192 Train Loss 74.7262 Test MSE 33.527968901526584 Test RE 1.3838266405735133\n",
      "193 Train Loss 74.7262 Test MSE 33.527968901526584 Test RE 1.3838266405735133\n",
      "194 Train Loss 74.7262 Test MSE 33.527968901526584 Test RE 1.3838266405735133\n",
      "195 Train Loss 74.7262 Test MSE 33.527968901526584 Test RE 1.3838266405735133\n",
      "196 Train Loss 74.7262 Test MSE 33.527968901526584 Test RE 1.3838266405735133\n",
      "197 Train Loss 74.7262 Test MSE 33.527968901526584 Test RE 1.3838266405735133\n",
      "198 Train Loss 74.7262 Test MSE 33.527968901526584 Test RE 1.3838266405735133\n",
      "199 Train Loss 74.7262 Test MSE 33.527968901526584 Test RE 1.3838266405735133\n",
      "Training time: 81.72\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10 #10\n",
    "max_iter = 200 #100\n",
    "\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "N_I = 200  #Total number of data points for 'y'\n",
    "N_B = 400\n",
    "N_f = 10000 #Total number of collocation points\n",
    "\n",
    "for reps in range(max_reps):\n",
    "  print(reps)\n",
    "\n",
    "  train_loss = []\n",
    "  test_mse_loss = []\n",
    "  test_re_loss = []\n",
    "\n",
    "\n",
    "  torch.manual_seed(reps*36)\n",
    "\n",
    "  layers = np.array([2,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "\n",
    "  PINN = Sequentialmodel(layers)\n",
    "\n",
    "  PINN.to(device)\n",
    "\n",
    "  'Neural Network Summary'\n",
    "  print(PINN)\n",
    "\n",
    "  params = list(PINN.parameters())\n",
    "\n",
    "\n",
    "  optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.5, \n",
    "                            max_iter = 20, \n",
    "                            max_eval = 30, \n",
    "                            tolerance_grad = 1e-8, \n",
    "                            tolerance_change = 1e-8, \n",
    "                            history_size = 100, \n",
    "                            line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  train_model(max_iter,reps)\n",
    "\n",
    "  torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "  train_loss_full.append(train_loss)\n",
    "  test_mse_full.append(test_mse_loss)\n",
    "  test_re_full.append(test_re_loss)\n",
    "  #elapsed_time[reps] = time.time() - start_time\n",
    "\n",
    "\n",
    "\n",
    "  #print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1660688516819,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "_O3sPdAnSq_2"
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "aborted",
     "timestamp": 1660688516819,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "jQ4afiEWSq_2"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'KG_tanh_tune0.mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'KG_tanh_tune0.mat'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-ad80b2a7a910>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtune_reps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"KG_tanh_tune\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtune_reps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".mat\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test_re_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \"\"\"\n\u001b[1;32m    223\u001b[0m     \u001b[0mvariable_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'variable_names'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0mMR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mmatfile_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file_context\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mappendmat\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.mat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mfile_like\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'.mat'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             raise IOError(\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'KG_tanh_tune0.mat'"
     ]
    }
   ],
   "source": [
    "for tune_reps in range(5):\n",
    "    label = \"KG_tanh_tune\"+str(tune_reps)+\".mat\"\n",
    "    data = sio.loadmat(label)\n",
    "    re = np.array(data[\"test_re_loss\"])\n",
    "    print(np.mean(re[:,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1660688534316,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "06syezgfv_qO",
    "outputId": "9f4852d5-694a-4977-8893-a6183a2ce493"
   },
   "outputs": [],
   "source": [
    "label"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "stan_2D_KG_16Aug2022_tune.ipynb",
   "version": ""
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
