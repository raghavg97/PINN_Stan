{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1256,
     "status": "ok",
     "timestamp": 1660687393066,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "xAgfGYA4acPE",
    "outputId": "527d048f-6a89-4e80-87ff-bfdb1c9d6222"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1856,
     "status": "ok",
     "timestamp": 1660687061284,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "7kSdyTofacUc",
    "outputId": "08ee5c9b-0706-46a5-86a1-2c7e56a6a74d"
   },
   "outputs": [],
   "source": [
    "# %cd '/content/gdrive/MyDrive/Virginia Tech /Fall 2022/Codes from GPU/PINN_Stan/2D Klein Gordon/stan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 32419,
     "status": "ok",
     "timestamp": 1660687093700,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "RHuSaD0gagsN",
    "outputId": "c232cd79-e56c-4a76-97c7-d59dafa084ef"
   },
   "outputs": [],
   "source": [
    "# !pip install smt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 280,
     "status": "ok",
     "timestamp": 1660687410736,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "mTLFQRt5Sq_y"
   },
   "outputs": [],
   "source": [
    "def true_2D_1(xt): #True function for 2D_1 Klein Gordon Equation x \\in [-50,50] , t \\in [0,10]\n",
    "    y = xt[:,0]*np.cos(xt[:,1])\n",
    "    return y.reshape(-1,1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 4312,
     "status": "ok",
     "timestamp": 1660687098957,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "81bNHCY3Sq_y"
   },
   "outputs": [],
   "source": [
    "label = \"KG_atanh\"\n",
    "loss_thresh = 0.01\n",
    "\n",
    "x = np.linspace(-5,5,500).reshape(-1,1)\n",
    "t = np.linspace(0,10,1000).reshape(-1,1)\n",
    "\n",
    "X,T = np.meshgrid(x,t)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "T = T.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xt = np.hstack((X,T))\n",
    "\n",
    "y_true = true_2D_1(xt)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "#bound_pts_idx = ((X == -5) + (X == 5) + (T == 0)).reshape(-1,)\n",
    "\n",
    "#xt_bound = xt[bound_pts_idx,:]\n",
    "#y_bound = y_true[bound_pts_idx,:]\n",
    "\n",
    "\n",
    "xt_test_tensor = torch.from_numpy(xt).float().to(device)\n",
    "\n",
    "\n",
    "lb_xt = xt[0]\n",
    "ub_xt = xt[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1660687098958,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "YQgCA-PuSq_z"
   },
   "outputs": [],
   "source": [
    "def trainingdata(N_I,N_B,N_f,seed):\n",
    "    '''Boundary Conditions''' \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    x_BC1 = np.random.uniform(size = N_I).reshape(-1,1)\n",
    "    t_BC1 = np.zeros((N_I,1))\n",
    "    samples = np.hstack((x_BC1,t_BC1))\n",
    "    xt_BC1 = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    y_BC1 = true_2D_1(xt_BC1)\n",
    "    \n",
    "    x_BC2 = np.zeros((int(N_B/2),1))\n",
    "    t_BC2 = np.random.uniform(size = int(N_B/2)).reshape(-1,1)\n",
    "    samples = np.hstack((x_BC2,t_BC2))\n",
    "    xt_BC2 = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    y_BC2 = true_2D_1(xt_BC2)\n",
    "    \n",
    "    x_BC3 = np.ones((int(N_B/2),1))\n",
    "    t_BC3 = np.random.uniform(size = int(N_B/2)).reshape(-1,1)\n",
    "    samples = np.hstack((x_BC3,t_BC3))\n",
    "    xt_BC3 = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    y_BC3 = true_2D_1(xt_BC3)\n",
    "\n",
    "    xt_BC = np.vstack((xt_BC1,xt_BC2,xt_BC3))\n",
    "    y_BC = np.vstack((y_BC1,y_BC2,y_BC3))\n",
    "\n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    samples = sampling(N_f)\n",
    "    xt_coll = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    \n",
    "    xt_coll = np.vstack((xt_coll, xt_BC)) # append training points to collocation points \n",
    "\n",
    "    return xt_coll, xt_BC, y_BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1660687098958,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "gTJxct8bSq_0"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,n_val):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "            \n",
    "        \n",
    "        self.alpha = Parameter(torch.ones((50,len(layers)-2)))\n",
    "        self.alpha.requiresGrad = True\n",
    "        \n",
    "        self.n = torch.tensor(n_val)\n",
    "\n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xt):\n",
    "        if torch.is_tensor(xt) != True:         \n",
    "            xt = torch.from_numpy(xt)                \n",
    "        \n",
    "        ubxt = torch.from_numpy(ub_xt).float().to(device)\n",
    "        lbxt = torch.from_numpy(lb_xt).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xt = (xt - lbxt)/(ubxt - lbxt)\n",
    "        \n",
    "        #convert to float\n",
    "        a = xt.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(self.n*self.alpha[:,i]*z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC(self,xt,y):\n",
    "                \n",
    "        loss_bc = self.loss_function(self.forward(xt), y)\n",
    "                \n",
    "        return loss_bc\n",
    "    \n",
    "    def loss_PDE(self, xt_coll, f_hat):\n",
    "        \n",
    "        g = xt_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        y = self.forward(g) \n",
    "        \n",
    "        \n",
    "        y_x_t = autograd.grad(y,g,torch.ones([xt_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        y_xx_tt = autograd.grad(y_x_t,g,torch.ones(xt_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        #du_dx = u_x_t[:,[0]]\n",
    "        \n",
    "        d2y_dx2 = y_xx_tt[:,[0]]\n",
    "        d2y_dt2 = y_xx_tt[:,[1]]    \n",
    "        \n",
    "\n",
    "        f = d2y_dt2 - d2y_dx2 + torch.pow(y,2) + (g[:,0]*torch.cos(g[:,1])).reshape(-1,1) - (torch.pow(g[:,0],2)*torch.pow(torch.cos(g[:,1]),2)).reshape(-1,1)\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xt_BC,y_BC,xt_coll,f_hat):\n",
    "\n",
    "        loss_BC = self.loss_BC(xt_BC,y_BC)\n",
    "        loss_f = self.loss_PDE(xt_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_BC + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "         \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        y_pred = self.forward(xt_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return y_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1660687098958,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "VoQzfzYsYKVs"
   },
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    alpha_val.append(PINN.alpha.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1660687098959,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "_IUDZDkxXmyF"
   },
   "outputs": [],
   "source": [
    "def train_step(xt_BC, y_BC, xt_coll, f_hat,seed):\n",
    "    # x_coll_np_array = colloc_pts(N_f,seed*123)\n",
    "    # x_coll_train = torch.from_numpy(x_coll_np_array).float().to(device)        \n",
    "    \n",
    "    # f_hat = torch.zeros(x_coll_train.shape[0],1).to(device)\n",
    "    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(xt_BC, y_BC, xt_coll,f_hat)\n",
    "        loss.backward()\n",
    "        #print(loss.cpu().detach().numpy())\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 467,
     "status": "ok",
     "timestamp": 1660690085956,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "Vt9Dlr8MYIwW"
   },
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "  print(rep) \n",
    "  torch.manual_seed(rep*11)\n",
    "  start_time = time.time() \n",
    "  thresh_flag = 0\n",
    "\n",
    "  xt_coll, xt_BC, y_BC = trainingdata(N_I,N_B,N_f,rep*11)\n",
    "  xt_coll = torch.from_numpy(xt_coll).float().to(device)\n",
    "  xt_BC = torch.from_numpy(xt_BC).float().to(device)\n",
    "  y_BC = torch.from_numpy(y_BC).float().to(device)\n",
    "\n",
    "  f_hat = torch.zeros(xt_coll.shape[0],1).to(device)\n",
    " \n",
    "  for i in range(max_iter):\n",
    "    train_step(xt_BC, y_BC, xt_coll,f_hat,i)\n",
    "    \n",
    "    loss_np = PINN.loss(xt_BC, y_BC, xt_coll,f_hat).cpu().detach().numpy()\n",
    "    if(thresh_flag == 0):\n",
    "        if(loss_np < loss_thresh):\n",
    "            time_threshold[rep] = time.time() - start_time\n",
    "            epoch_threshold[rep] = i+1            \n",
    "            thresh_flag = 1       \n",
    "    data_update(loss_np)\n",
    "    \n",
    "    print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "\n",
    "\n",
    "  elapsed_time[rep] = time.time() - start_time  \n",
    "  print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "sP4Re5lSSq_1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KG_atanh\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 56.616932 Test MSE 7.951808284558346 Test RE 1.3478482859691505\n",
      "1 Train Loss 45.676773 Test MSE 8.089870267254938 Test RE 1.359498832126889\n",
      "2 Train Loss 37.604042 Test MSE 7.597744013822945 Test RE 1.317499286841015\n",
      "3 Train Loss 33.48955 Test MSE 6.608636222178596 Test RE 1.2287512806983496\n",
      "4 Train Loss 29.58458 Test MSE 6.334931074950105 Test RE 1.203037063209381\n",
      "5 Train Loss 27.407467 Test MSE 6.0707025127751395 Test RE 1.1776806406521012\n",
      "6 Train Loss 25.455597 Test MSE 6.028567846571892 Test RE 1.1735865855087526\n",
      "7 Train Loss 24.177952 Test MSE 5.947628048241919 Test RE 1.1656816521237972\n",
      "8 Train Loss 23.4338 Test MSE 5.913485745519226 Test RE 1.1623310444044672\n",
      "9 Train Loss 22.704634 Test MSE 5.710512540572854 Test RE 1.142209071614665\n",
      "10 Train Loss 21.62101 Test MSE 5.776997236585897 Test RE 1.1488389203719354\n",
      "11 Train Loss 19.553932 Test MSE 5.294619237669034 Test RE 1.0998296586119547\n",
      "12 Train Loss 16.302143 Test MSE 4.306760451656008 Test RE 0.991935465989812\n",
      "13 Train Loss 12.264147 Test MSE 4.301911317336998 Test RE 0.9913768810542776\n",
      "14 Train Loss 10.692729 Test MSE 4.326587528634975 Test RE 0.9942161358246971\n",
      "15 Train Loss 10.086377 Test MSE 4.252510820325331 Test RE 0.9856682647660404\n",
      "16 Train Loss 9.50172 Test MSE 4.254928302806657 Test RE 0.9859483930606668\n",
      "17 Train Loss 9.3267565 Test MSE 4.242846160037891 Test RE 0.9845475659595099\n",
      "18 Train Loss 9.128649 Test MSE 4.249912863188973 Test RE 0.9853671350062813\n",
      "19 Train Loss 8.942232 Test MSE 4.243312214270141 Test RE 0.9846016381486395\n",
      "20 Train Loss 8.791636 Test MSE 4.218128051664576 Test RE 0.9816754724727421\n",
      "21 Train Loss 8.690283 Test MSE 4.203621866831463 Test RE 0.9799860227970543\n",
      "22 Train Loss 8.572602 Test MSE 4.24301516477983 Test RE 0.984567174442128\n",
      "23 Train Loss 8.511526 Test MSE 4.280743380877424 Test RE 0.9889347942562418\n",
      "24 Train Loss 8.418276 Test MSE 4.2801730870713035 Test RE 0.9888689175938508\n",
      "25 Train Loss 8.288913 Test MSE 4.230713422165183 Test RE 0.9831388642398998\n",
      "26 Train Loss 8.09278 Test MSE 4.1619095785355364 Test RE 0.9751117294829713\n",
      "27 Train Loss 6.809003 Test MSE 3.2539176084403354 Test RE 0.862206586828227\n",
      "28 Train Loss 5.790745 Test MSE 2.992843367283986 Test RE 0.826894407001941\n",
      "29 Train Loss 5.4182043 Test MSE 3.042238747691273 Test RE 0.8336902203442565\n",
      "30 Train Loss 5.12485 Test MSE 2.808427005941517 Test RE 0.8010131218512413\n",
      "31 Train Loss 4.850327 Test MSE 2.5666346626826084 Test RE 0.7657554348915269\n",
      "32 Train Loss 4.618728 Test MSE 2.198182554505128 Test RE 0.7086632941960457\n",
      "33 Train Loss 4.3254156 Test MSE 2.187386498449739 Test RE 0.7069209034916094\n",
      "34 Train Loss 4.209194 Test MSE 2.124152159562 Test RE 0.6966279127842387\n",
      "35 Train Loss 4.13447 Test MSE 2.1581296644077157 Test RE 0.70217736786597\n",
      "36 Train Loss 4.092052 Test MSE 2.1744014140843624 Test RE 0.7048195164641803\n",
      "37 Train Loss 4.0525436 Test MSE 2.1717572397301637 Test RE 0.7043908392662332\n",
      "38 Train Loss 4.0231824 Test MSE 2.169364936091612 Test RE 0.7040027707810383\n",
      "39 Train Loss 3.9885375 Test MSE 2.1622682300920246 Test RE 0.7028503152229773\n",
      "40 Train Loss 3.9464047 Test MSE 2.157243244330446 Test RE 0.7020331485326813\n",
      "41 Train Loss 3.9241064 Test MSE 2.1631762037789892 Test RE 0.7029978692134299\n",
      "42 Train Loss 3.8962984 Test MSE 2.142909319859422 Test RE 0.699696911814712\n",
      "43 Train Loss 3.8682308 Test MSE 2.14159185453795 Test RE 0.6994817911520909\n",
      "44 Train Loss 3.8456786 Test MSE 2.117730362525922 Test RE 0.6955740830604501\n",
      "45 Train Loss 3.827533 Test MSE 2.12581840213257 Test RE 0.6969010861388363\n",
      "46 Train Loss 3.8123374 Test MSE 2.1260684587499195 Test RE 0.6969420726134415\n",
      "47 Train Loss 3.7888486 Test MSE 2.135538931680285 Test RE 0.6984925958502423\n",
      "48 Train Loss 3.7679825 Test MSE 2.1288580299807025 Test RE 0.6973991445009842\n",
      "49 Train Loss 3.7473912 Test MSE 2.1209089680558835 Test RE 0.6960958979843612\n",
      "50 Train Loss 3.7241445 Test MSE 2.1148509888471763 Test RE 0.6951010533045868\n",
      "51 Train Loss 3.7119234 Test MSE 2.114845981740117 Test RE 0.695100230445797\n",
      "52 Train Loss 3.6859393 Test MSE 2.1219756639136818 Test RE 0.6962709241839921\n",
      "53 Train Loss 3.6699824 Test MSE 2.120766912890217 Test RE 0.6960725858867117\n",
      "54 Train Loss 3.63045 Test MSE 2.1042757303319184 Test RE 0.6933609576942189\n",
      "55 Train Loss 3.5772057 Test MSE 2.1121956026225597 Test RE 0.6946645351963322\n",
      "56 Train Loss 3.51505 Test MSE 2.1239359435542102 Test RE 0.6965924572405613\n",
      "57 Train Loss 3.2157762 Test MSE 2.027566696366468 Test RE 0.6806057829106703\n",
      "58 Train Loss 2.005445 Test MSE 1.4669977114113224 Test RE 0.5789256320131099\n",
      "59 Train Loss 1.512927 Test MSE 1.158101484019349 Test RE 0.5143767964750668\n",
      "60 Train Loss 1.0026157 Test MSE 0.8279347104210085 Test RE 0.43491672140771437\n",
      "61 Train Loss 0.7616028 Test MSE 0.5573317854417028 Test RE 0.3568329631628669\n",
      "62 Train Loss 0.59816134 Test MSE 0.3642945062433126 Test RE 0.28849239639556556\n",
      "63 Train Loss 0.47470236 Test MSE 0.16686132034995105 Test RE 0.19524768720306135\n",
      "64 Train Loss 0.35615078 Test MSE 0.05115329207458252 Test RE 0.10810476939802252\n",
      "65 Train Loss 0.27464 Test MSE 0.03556542746950734 Test RE 0.09014093878097862\n",
      "66 Train Loss 0.21709247 Test MSE 0.04058999637796904 Test RE 0.09629806633093589\n",
      "67 Train Loss 0.1838809 Test MSE 0.04224357561450273 Test RE 0.09824000949472828\n",
      "68 Train Loss 0.16592063 Test MSE 0.03515798262051165 Test RE 0.08962311487119246\n",
      "69 Train Loss 0.14087029 Test MSE 0.03407611380937914 Test RE 0.08823341535633558\n",
      "70 Train Loss 0.11166048 Test MSE 0.03408388944573131 Test RE 0.08824348152500959\n",
      "71 Train Loss 0.07821851 Test MSE 0.013365583855406989 Test RE 0.055258873574044354\n",
      "72 Train Loss 0.06552715 Test MSE 0.012372966714123919 Test RE 0.053167346093859265\n",
      "73 Train Loss 0.057071976 Test MSE 0.011123407066553724 Test RE 0.05041119303977823\n",
      "74 Train Loss 0.05215668 Test MSE 0.010086016366944875 Test RE 0.04800294635179897\n",
      "75 Train Loss 0.04624791 Test MSE 0.007071504900172042 Test RE 0.0401942550162787\n",
      "76 Train Loss 0.03836242 Test MSE 0.006063721090498283 Test RE 0.03722011141558314\n",
      "77 Train Loss 0.035549488 Test MSE 0.005755649357553587 Test RE 0.03626228980249141\n",
      "78 Train Loss 0.032415733 Test MSE 0.004862763632759229 Test RE 0.03333109951957225\n",
      "79 Train Loss 0.030960103 Test MSE 0.004610153306827009 Test RE 0.032453814100973304\n",
      "80 Train Loss 0.029173365 Test MSE 0.004538537144985832 Test RE 0.03220075150176626\n",
      "81 Train Loss 0.02449808 Test MSE 0.003630572545789446 Test RE 0.02880020770609822\n",
      "82 Train Loss 0.022057226 Test MSE 0.0035765161016119078 Test RE 0.028584997105725964\n",
      "83 Train Loss 0.019869894 Test MSE 0.003519255047192285 Test RE 0.028355246778526827\n",
      "84 Train Loss 0.018640399 Test MSE 0.003468991122920856 Test RE 0.02815202597426528\n",
      "85 Train Loss 0.017167613 Test MSE 0.0035530731128920346 Test RE 0.028491160049707313\n",
      "86 Train Loss 0.016483687 Test MSE 0.003208482567917398 Test RE 0.027074341484980675\n",
      "87 Train Loss 0.015728138 Test MSE 0.0032474871515900537 Test RE 0.027238411775964328\n",
      "88 Train Loss 0.014511719 Test MSE 0.0032436198795901035 Test RE 0.027222188505245944\n",
      "89 Train Loss 0.014019962 Test MSE 0.0030981181503767657 Test RE 0.026604619273398328\n",
      "90 Train Loss 0.013097051 Test MSE 0.0031667632189535853 Test RE 0.02689774402883538\n",
      "91 Train Loss 0.011912247 Test MSE 0.0027539641240599292 Test RE 0.025083445954816867\n",
      "92 Train Loss 0.011688172 Test MSE 0.002588485337569137 Test RE 0.024318171387287778\n",
      "93 Train Loss 0.011226698 Test MSE 0.002417316896399571 Test RE 0.02350037833661146\n",
      "94 Train Loss 0.010701246 Test MSE 0.0022999868487609764 Test RE 0.022922962115601308\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95 Train Loss 0.010175719 Test MSE 0.00206056938187433 Test RE 0.021697099541836358\n",
      "96 Train Loss 0.009654796 Test MSE 0.0017844389717618153 Test RE 0.02019105027032558\n",
      "97 Train Loss 0.00900599 Test MSE 0.0016967827972814943 Test RE 0.019688887909677894\n",
      "98 Train Loss 0.008304529 Test MSE 0.0015525647162219459 Test RE 0.01883358152505673\n",
      "99 Train Loss 0.007865839 Test MSE 0.001581546197847251 Test RE 0.019008550524342072\n",
      "Training time: 138.71\n",
      "KG_atanh\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 56.277283 Test MSE 9.020183041963925 Test RE 1.4355414455959763\n",
      "1 Train Loss 50.02105 Test MSE 8.797698907193636 Test RE 1.4177269907985608\n",
      "2 Train Loss 46.044487 Test MSE 8.73952043686427 Test RE 1.4130315584112731\n",
      "3 Train Loss 45.08272 Test MSE 8.607797130240806 Test RE 1.402342421190791\n",
      "4 Train Loss 44.029068 Test MSE 8.540118243679963 Test RE 1.396818576651794\n",
      "5 Train Loss 42.93174 Test MSE 8.48062299870192 Test RE 1.3919445629228742\n",
      "6 Train Loss 41.86283 Test MSE 8.990518489178216 Test RE 1.4331789788695737\n",
      "7 Train Loss 40.964924 Test MSE 9.52135011966953 Test RE 1.4748821786007078\n",
      "8 Train Loss 40.104637 Test MSE 9.282379251522999 Test RE 1.4562559556872587\n",
      "9 Train Loss 38.66231 Test MSE 9.471301863660168 Test RE 1.4710007680434778\n",
      "10 Train Loss 36.03591 Test MSE 8.58587318559767 Test RE 1.400555408856716\n",
      "11 Train Loss 32.126934 Test MSE 8.377302239132423 Test RE 1.3834394393060576\n",
      "12 Train Loss 30.209122 Test MSE 9.309635539348943 Test RE 1.458392424992864\n",
      "13 Train Loss 29.382914 Test MSE 8.86573347637798 Test RE 1.4231982332041968\n",
      "14 Train Loss 28.225086 Test MSE 8.958381836211542 Test RE 1.4306152331010988\n",
      "15 Train Loss 27.456448 Test MSE 8.816398696558545 Test RE 1.4192329029545874\n",
      "16 Train Loss 26.163519 Test MSE 9.063709110087267 Test RE 1.4390008143641935\n",
      "17 Train Loss 24.723698 Test MSE 8.780479648426434 Test RE 1.416338891191486\n",
      "18 Train Loss 22.52702 Test MSE 7.967001179927384 Test RE 1.3491352854346117\n",
      "19 Train Loss 20.478136 Test MSE 7.554160416007401 Test RE 1.3137150093136876\n",
      "20 Train Loss 19.348421 Test MSE 7.735606103451837 Test RE 1.3293986517201177\n",
      "21 Train Loss 18.83843 Test MSE 7.656738187212433 Test RE 1.3226043869096278\n",
      "22 Train Loss 17.917313 Test MSE 7.482009155948168 Test RE 1.3074261826431373\n",
      "23 Train Loss 16.671616 Test MSE 6.934899366092555 Test RE 1.258717128094138\n",
      "24 Train Loss 15.196266 Test MSE 5.397780274099481 Test RE 1.1104925804063082\n",
      "25 Train Loss 12.610142 Test MSE 5.255631063551452 Test RE 1.0957727491177813\n",
      "26 Train Loss 11.78302 Test MSE 5.2406823153204165 Test RE 1.0942132698489229\n",
      "27 Train Loss 11.37332 Test MSE 5.294138937486883 Test RE 1.0997797720816702\n",
      "28 Train Loss 11.205915 Test MSE 5.2830956578668795 Test RE 1.0986321335476559\n",
      "29 Train Loss 11.006267 Test MSE 5.361658604585013 Test RE 1.1067706634597183\n",
      "30 Train Loss 10.744129 Test MSE 5.455547709082237 Test RE 1.1164190521881066\n",
      "31 Train Loss 10.630248 Test MSE 5.4620320499186565 Test RE 1.1170823303972077\n",
      "32 Train Loss 10.530231 Test MSE 5.456081909245005 Test RE 1.1164737100036415\n",
      "33 Train Loss 10.423981 Test MSE 5.420097200231346 Test RE 1.112785858065902\n",
      "34 Train Loss 10.267115 Test MSE 5.44700936741511 Test RE 1.1155450702839171\n",
      "35 Train Loss 10.10133 Test MSE 5.466735440463603 Test RE 1.1175631902521301\n",
      "36 Train Loss 9.979724 Test MSE 5.429307383685657 Test RE 1.1137309159946167\n",
      "37 Train Loss 9.863062 Test MSE 5.40204869668134 Test RE 1.1109315677918492\n",
      "38 Train Loss 9.700602 Test MSE 5.324173071412398 Test RE 1.1028949357458233\n",
      "39 Train Loss 9.55714 Test MSE 5.2574381918380215 Test RE 1.0959611215239413\n",
      "40 Train Loss 9.291036 Test MSE 5.14721960890123 Test RE 1.0844122359873383\n",
      "41 Train Loss 7.8686423 Test MSE 4.019252192701723 Test RE 0.9582541062949187\n",
      "42 Train Loss 5.25304 Test MSE 3.288441721311666 Test RE 0.8667685303602759\n",
      "43 Train Loss 4.337414 Test MSE 2.7298223409957503 Test RE 0.789723846461191\n",
      "44 Train Loss 3.5976057 Test MSE 2.3115159888644934 Test RE 0.7267022606103234\n",
      "45 Train Loss 2.8939097 Test MSE 2.1837168153551634 Test RE 0.7063276693653774\n",
      "46 Train Loss 2.5148542 Test MSE 2.2031646198745953 Test RE 0.7094659137576942\n",
      "47 Train Loss 2.2857587 Test MSE 2.183139521850835 Test RE 0.7062342998118747\n",
      "48 Train Loss 2.095531 Test MSE 2.1393007441974405 Test RE 0.6991075324412074\n",
      "49 Train Loss 1.9203578 Test MSE 2.133431019984616 Test RE 0.6981477826047556\n",
      "50 Train Loss 1.8243984 Test MSE 2.092737761605987 Test RE 0.6914574587182359\n",
      "51 Train Loss 1.7792958 Test MSE 2.075643280705025 Test RE 0.6886275906624855\n",
      "52 Train Loss 1.7345476 Test MSE 1.9982091337188383 Test RE 0.6756604998879371\n",
      "53 Train Loss 1.6724936 Test MSE 1.967824516760958 Test RE 0.6705038004700666\n",
      "54 Train Loss 1.6053404 Test MSE 1.8718447923595802 Test RE 0.6539476418539945\n",
      "55 Train Loss 1.5456632 Test MSE 1.7554399961840599 Test RE 0.6332877026440603\n",
      "56 Train Loss 1.4494039 Test MSE 1.7750983709106511 Test RE 0.6368237810583945\n",
      "57 Train Loss 1.3347542 Test MSE 1.6618994581931468 Test RE 0.6161840230981052\n",
      "58 Train Loss 1.2615376 Test MSE 1.5058617705012949 Test RE 0.5865440238784831\n",
      "59 Train Loss 1.2118392 Test MSE 1.3208209687213353 Test RE 0.5493258517292915\n",
      "60 Train Loss 1.1564084 Test MSE 1.3298548340375091 Test RE 0.5512012305096146\n",
      "61 Train Loss 1.0952368 Test MSE 1.2951796853910622 Test RE 0.5439676493658478\n",
      "62 Train Loss 1.0516357 Test MSE 1.236106642816506 Test RE 0.531417718381493\n",
      "63 Train Loss 0.9983617 Test MSE 1.1710338506768403 Test RE 0.517240812102107\n",
      "64 Train Loss 0.9539563 Test MSE 1.139563130920983 Test RE 0.5102432347384719\n",
      "65 Train Loss 0.9167757 Test MSE 1.0452019787903877 Test RE 0.4886615541696429\n",
      "66 Train Loss 0.87649184 Test MSE 0.8978636044602571 Test RE 0.4529113914449025\n",
      "67 Train Loss 0.83057314 Test MSE 0.7386881888902737 Test RE 0.4108077591742021\n",
      "68 Train Loss 0.64455664 Test MSE 0.2600880370311706 Test RE 0.2437632596570019\n",
      "69 Train Loss 0.49247938 Test MSE 0.11590784108743402 Test RE 0.16272881752107068\n",
      "70 Train Loss 0.4014007 Test MSE 0.07172096613028142 Test RE 0.12800623554285856\n",
      "71 Train Loss 0.30608836 Test MSE 0.06856342569748862 Test RE 0.12515676078661328\n",
      "72 Train Loss 0.245286 Test MSE 0.05865940711836089 Test RE 0.1157648983769334\n",
      "73 Train Loss 0.21464233 Test MSE 0.05456776804636927 Test RE 0.11165448056862155\n",
      "74 Train Loss 0.18823087 Test MSE 0.061840588509226004 Test RE 0.11886250186973182\n",
      "75 Train Loss 0.16827673 Test MSE 0.05663222770787408 Test RE 0.11374698213594135\n",
      "76 Train Loss 0.15108415 Test MSE 0.044319880868620924 Test RE 0.10062533806483936\n",
      "77 Train Loss 0.13677923 Test MSE 0.03825129650209829 Test RE 0.09348267693865543\n",
      "78 Train Loss 0.12584 Test MSE 0.03756692663538126 Test RE 0.09264263384892321\n",
      "79 Train Loss 0.11562045 Test MSE 0.0369989861182355 Test RE 0.09193967662998441\n",
      "80 Train Loss 0.10542613 Test MSE 0.035836437531782894 Test RE 0.09048372589110824\n",
      "81 Train Loss 0.09264387 Test MSE 0.03290202190856672 Test RE 0.0867000511882025\n",
      "82 Train Loss 0.084842384 Test MSE 0.030999956089365923 Test RE 0.0841566810621501\n",
      "83 Train Loss 0.078689225 Test MSE 0.03475032322415981 Test RE 0.08910200658769397\n",
      "84 Train Loss 0.07133535 Test MSE 0.035228413020928516 Test RE 0.08971283892995313\n",
      "85 Train Loss 0.066445746 Test MSE 0.03436144770991267 Test RE 0.08860205336882769\n",
      "86 Train Loss 0.062252484 Test MSE 0.03845308761824211 Test RE 0.09372893212566734\n",
      "87 Train Loss 0.057078764 Test MSE 0.037652205245602755 Test RE 0.09274772571117096\n",
      "88 Train Loss 0.051548183 Test MSE 0.03271589093426655 Test RE 0.08645446669341245\n",
      "89 Train Loss 0.04860953 Test MSE 0.03215769674950076 Test RE 0.0857137563124772\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 Train Loss 0.04603374 Test MSE 0.03143641581467177 Test RE 0.08474704668557681\n",
      "91 Train Loss 0.04361316 Test MSE 0.030973255268526685 Test RE 0.08412043042163028\n",
      "92 Train Loss 0.04188542 Test MSE 0.028245331014629203 Test RE 0.08033067100160744\n",
      "93 Train Loss 0.04052292 Test MSE 0.02864302959534989 Test RE 0.08089422825964399\n",
      "94 Train Loss 0.03815218 Test MSE 0.026736026627347927 Test RE 0.0781549505829246\n",
      "95 Train Loss 0.0365456 Test MSE 0.027362013053319974 Test RE 0.07906460091732394\n",
      "96 Train Loss 0.035243735 Test MSE 0.026529155326703645 Test RE 0.07785199956309097\n",
      "97 Train Loss 0.033817936 Test MSE 0.026114906519825738 Test RE 0.07724178440043247\n",
      "98 Train Loss 0.0327215 Test MSE 0.026061321361949642 Test RE 0.07716249752649242\n",
      "99 Train Loss 0.031041233 Test MSE 0.022333116253307108 Test RE 0.07143034103102627\n",
      "Training time: 139.53\n",
      "KG_atanh\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 52.05397 Test MSE 8.481854951021303 Test RE 1.3920456608646974\n",
      "1 Train Loss 46.752014 Test MSE 7.785510112003164 Test RE 1.3336798720604284\n",
      "2 Train Loss 38.29451 Test MSE 7.946292543727008 Test RE 1.3473807400214803\n",
      "3 Train Loss 35.758163 Test MSE 7.620300257451247 Test RE 1.3194535387139068\n",
      "4 Train Loss 33.255867 Test MSE 7.347901351256262 Test RE 1.29565602471336\n",
      "5 Train Loss 29.197826 Test MSE 6.7220133280392895 Test RE 1.2392466252449668\n",
      "6 Train Loss 26.424343 Test MSE 6.021305974168681 Test RE 1.1728795349883\n",
      "7 Train Loss 24.557991 Test MSE 6.374396617247405 Test RE 1.2067786030324\n",
      "8 Train Loss 23.504536 Test MSE 6.2434799643287535 Test RE 1.1943219702288488\n",
      "9 Train Loss 22.88633 Test MSE 6.238114674638789 Test RE 1.1938086939290973\n",
      "10 Train Loss 20.984642 Test MSE 6.033773991489394 Test RE 1.174093218549546\n",
      "11 Train Loss 19.268211 Test MSE 6.016750408864613 Test RE 1.1724357657844582\n",
      "12 Train Loss 18.26448 Test MSE 6.037886768849248 Test RE 1.1744932966299098\n",
      "13 Train Loss 15.525943 Test MSE 5.049055597190416 Test RE 1.074021899351816\n",
      "14 Train Loss 10.98592 Test MSE 3.7535739216355606 Test RE 0.9260417070051034\n",
      "15 Train Loss 7.341481 Test MSE 2.637651316193311 Test RE 0.7762770593923888\n",
      "16 Train Loss 6.0479 Test MSE 2.407262076275607 Test RE 0.7416000446984266\n",
      "17 Train Loss 5.361908 Test MSE 2.3332387891750885 Test RE 0.7301089196181658\n",
      "18 Train Loss 5.01238 Test MSE 2.3224274612410265 Test RE 0.7284154341427835\n",
      "19 Train Loss 4.7425056 Test MSE 2.262601814875175 Test RE 0.7189722458715221\n",
      "20 Train Loss 4.534495 Test MSE 2.1996254474549484 Test RE 0.7088958402889215\n",
      "21 Train Loss 4.412658 Test MSE 2.175270613864532 Test RE 0.7049603754044013\n",
      "22 Train Loss 4.323594 Test MSE 2.1652942125362613 Test RE 0.7033419445949336\n",
      "23 Train Loss 4.15829 Test MSE 2.1238153509322855 Test RE 0.6965726814315234\n",
      "24 Train Loss 3.978066 Test MSE 2.148230691828939 Test RE 0.7005651330845942\n",
      "25 Train Loss 3.3798058 Test MSE 2.134211650547023 Test RE 0.6982754983935978\n",
      "26 Train Loss 2.393633 Test MSE 1.941477218424114 Test RE 0.6659999701394642\n",
      "27 Train Loss 1.4057709 Test MSE 1.3773747538735426 Test RE 0.5609628740758433\n",
      "28 Train Loss 0.8369838 Test MSE 0.44642555194767747 Test RE 0.3193615168874673\n",
      "29 Train Loss 0.5656507 Test MSE 0.30324164256037367 Test RE 0.26321005775983897\n",
      "30 Train Loss 0.38856634 Test MSE 0.20688077402469635 Test RE 0.21740429840738792\n",
      "31 Train Loss 0.2802824 Test MSE 0.17496682975194663 Test RE 0.19993366249423014\n",
      "32 Train Loss 0.19625351 Test MSE 0.07766878689668512 Test RE 0.13320831085538962\n",
      "33 Train Loss 0.14560343 Test MSE 0.048566399349364596 Test RE 0.10533580379044527\n",
      "34 Train Loss 0.10954734 Test MSE 0.0419587229152895 Test RE 0.09790822808403113\n",
      "35 Train Loss 0.088719174 Test MSE 0.032755116613746374 Test RE 0.08650627972993594\n",
      "36 Train Loss 0.07109633 Test MSE 0.02263192228525066 Test RE 0.07190660457033174\n",
      "37 Train Loss 0.060574695 Test MSE 0.021156061117889466 Test RE 0.06952251376749918\n",
      "38 Train Loss 0.05295067 Test MSE 0.022941799442971124 Test RE 0.07239720495914238\n",
      "39 Train Loss 0.04749607 Test MSE 0.019794839538515616 Test RE 0.06724872474069936\n",
      "40 Train Loss 0.038325306 Test MSE 0.017736848615389853 Test RE 0.06365701794363071\n",
      "41 Train Loss 0.0339615 Test MSE 0.017154562255912543 Test RE 0.06260339423552694\n",
      "42 Train Loss 0.028559081 Test MSE 0.017177520167783875 Test RE 0.0626452712197058\n",
      "43 Train Loss 0.02535648 Test MSE 0.01743742077577502 Test RE 0.06311741211681729\n",
      "44 Train Loss 0.022170816 Test MSE 0.016362368525250758 Test RE 0.06114080397998133\n",
      "45 Train Loss 0.019008812 Test MSE 0.01491762530671746 Test RE 0.05837916884466751\n",
      "46 Train Loss 0.017460946 Test MSE 0.011919714661843352 Test RE 0.05218443550110797\n",
      "47 Train Loss 0.014820026 Test MSE 0.010484784699672895 Test RE 0.04894268811068152\n",
      "48 Train Loss 0.012836583 Test MSE 0.008800772524617338 Test RE 0.044840294698995295\n",
      "49 Train Loss 0.010406845 Test MSE 0.006934001791957641 Test RE 0.03980155456715443\n",
      "50 Train Loss 0.009218077 Test MSE 0.00568233329426348 Test RE 0.03603059317432628\n",
      "51 Train Loss 0.008153202 Test MSE 0.00423188237203277 Test RE 0.03109387541499654\n",
      "52 Train Loss 0.0068599866 Test MSE 0.004724055994934597 Test RE 0.03285228476420501\n",
      "53 Train Loss 0.005900331 Test MSE 0.003785219270304624 Test RE 0.029407193652038956\n",
      "54 Train Loss 0.005473012 Test MSE 0.003262561787470617 Test RE 0.027301558097642984\n",
      "55 Train Loss 0.0051220856 Test MSE 0.0035621648353249607 Test RE 0.028527588827890245\n",
      "56 Train Loss 0.0048257117 Test MSE 0.0029956600687170982 Test RE 0.02616099907790205\n",
      "57 Train Loss 0.0045400243 Test MSE 0.003030140357040638 Test RE 0.02631112592257518\n",
      "58 Train Loss 0.0043174378 Test MSE 0.002814552835718704 Test RE 0.025357869540175475\n",
      "59 Train Loss 0.0040964643 Test MSE 0.0028582521466295856 Test RE 0.025553966989939288\n",
      "60 Train Loss 0.0038905237 Test MSE 0.0028157089012552953 Test RE 0.025363076823729232\n",
      "61 Train Loss 0.0035564448 Test MSE 0.002477064841686801 Test RE 0.02378903075991554\n",
      "62 Train Loss 0.003452345 Test MSE 0.002421982363726155 Test RE 0.02352304549218935\n",
      "63 Train Loss 0.003155574 Test MSE 0.002326798536509138 Test RE 0.023056185162127703\n",
      "64 Train Loss 0.002885492 Test MSE 0.0021103203853572883 Test RE 0.02195746794271907\n",
      "65 Train Loss 0.0027701485 Test MSE 0.0019909590996115426 Test RE 0.021327464585159293\n",
      "66 Train Loss 0.0026177382 Test MSE 0.0018953259802619224 Test RE 0.020808942883949605\n",
      "67 Train Loss 0.0024461069 Test MSE 0.0016252136796336852 Test RE 0.019269182716489246\n",
      "68 Train Loss 0.002310364 Test MSE 0.001486040657225589 Test RE 0.01842567497781568\n",
      "69 Train Loss 0.0021946216 Test MSE 0.0014519455113173251 Test RE 0.0182130726312118\n",
      "70 Train Loss 0.002044197 Test MSE 0.0015148547824816395 Test RE 0.018603453002166835\n",
      "71 Train Loss 0.0019463106 Test MSE 0.0013874788467875296 Test RE 0.01780415004358206\n",
      "72 Train Loss 0.0018820999 Test MSE 0.0013957443274226818 Test RE 0.017857102686132317\n",
      "73 Train Loss 0.0017873852 Test MSE 0.0014711410168387297 Test RE 0.018333070662716996\n",
      "74 Train Loss 0.0017027572 Test MSE 0.0014275316667806354 Test RE 0.018059300965312674\n",
      "75 Train Loss 0.0016491795 Test MSE 0.0013413019299697972 Test RE 0.017505371629020022\n",
      "76 Train Loss 0.0016082758 Test MSE 0.001318629672129426 Test RE 0.017356792932512948\n",
      "77 Train Loss 0.0015822202 Test MSE 0.0012787227724047059 Test RE 0.017092132860526004\n",
      "78 Train Loss 0.0015076357 Test MSE 0.001205244439849604 Test RE 0.016593791488137506\n",
      "79 Train Loss 0.0014337676 Test MSE 0.0010981579676565486 Test RE 0.015839463546463727\n",
      "80 Train Loss 0.0013899364 Test MSE 0.0010656470019972531 Test RE 0.015603238397782833\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "81 Train Loss 0.0013498554 Test MSE 0.0010333139186945127 Test RE 0.01536470406861486\n",
      "82 Train Loss 0.0013122716 Test MSE 0.0010275500422669047 Test RE 0.015321791601252873\n",
      "83 Train Loss 0.001280633 Test MSE 0.0010238929840327203 Test RE 0.01529450211367026\n",
      "84 Train Loss 0.0012569515 Test MSE 0.000994465523234422 Test RE 0.01507311199229345\n",
      "85 Train Loss 0.0012201393 Test MSE 0.0009620517422393239 Test RE 0.014825429211425422\n",
      "86 Train Loss 0.001174265 Test MSE 0.0009727492601290731 Test RE 0.014907626897421835\n",
      "87 Train Loss 0.0011089905 Test MSE 0.0009302662705613774 Test RE 0.014578461612472535\n",
      "88 Train Loss 0.0010798182 Test MSE 0.0009410557461358898 Test RE 0.014662760324593793\n",
      "89 Train Loss 0.0010417526 Test MSE 0.0009569018938839257 Test RE 0.014785695818383965\n",
      "90 Train Loss 0.0010154345 Test MSE 0.0009327029906216148 Test RE 0.0145975423847928\n",
      "91 Train Loss 0.0009916255 Test MSE 0.000943353410642791 Test RE 0.01468064957551767\n",
      "92 Train Loss 0.00096701935 Test MSE 0.0008704156637561922 Test RE 0.014101697985788243\n",
      "93 Train Loss 0.00092729914 Test MSE 0.0007423518668792343 Test RE 0.01302305764153633\n",
      "94 Train Loss 0.00089345966 Test MSE 0.0007489083200797804 Test RE 0.013080441053274624\n",
      "95 Train Loss 0.0008513116 Test MSE 0.0007125545647369149 Test RE 0.012759014282116064\n",
      "96 Train Loss 0.0008169881 Test MSE 0.0007328478812373249 Test RE 0.01293942502729339\n",
      "97 Train Loss 0.0007956432 Test MSE 0.0007542244784754556 Test RE 0.013126784995938234\n",
      "98 Train Loss 0.00077837845 Test MSE 0.0007632819559683197 Test RE 0.01320536951313973\n",
      "99 Train Loss 0.0007594586 Test MSE 0.0007286172500253715 Test RE 0.012902022197801778\n",
      "Training time: 138.96\n",
      "KG_atanh\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 55.72149 Test MSE 8.648566758339637 Test RE 1.40565949755812\n",
      "1 Train Loss 43.97393 Test MSE 8.603307935530385 Test RE 1.4019766941004994\n",
      "2 Train Loss 43.721947 Test MSE 8.53363840557347 Test RE 1.3962885561338119\n",
      "3 Train Loss 42.782738 Test MSE 8.482357998315928 Test RE 1.3920869404211802\n",
      "4 Train Loss 42.32188 Test MSE 8.319571104552436 Test RE 1.3786642980000348\n",
      "5 Train Loss 41.871677 Test MSE 8.32915267332143 Test RE 1.3794579666527476\n",
      "6 Train Loss 41.423325 Test MSE 8.594431198454366 Test RE 1.4012532403791846\n",
      "7 Train Loss 40.624466 Test MSE 8.627267793875541 Test RE 1.4039275604564407\n",
      "8 Train Loss 39.79142 Test MSE 8.70637296146291 Test RE 1.4103493216636662\n",
      "9 Train Loss 38.067703 Test MSE 9.232217391407115 Test RE 1.4523158309938993\n",
      "10 Train Loss 34.74102 Test MSE 9.275996148763127 Test RE 1.455755166518495\n",
      "11 Train Loss 32.23227 Test MSE 8.784410174521817 Test RE 1.4166558632892965\n",
      "12 Train Loss 27.5811 Test MSE 7.733325081642145 Test RE 1.3292026353477038\n",
      "13 Train Loss 26.857346 Test MSE 7.7520216555546435 Test RE 1.3308084473706674\n",
      "14 Train Loss 25.828035 Test MSE 7.408400811308593 Test RE 1.300979027626153\n",
      "15 Train Loss 23.5807 Test MSE 6.340838813614889 Test RE 1.20359788796599\n",
      "16 Train Loss 19.366035 Test MSE 5.741644615781086 Test RE 1.1453183376657905\n",
      "17 Train Loss 16.762518 Test MSE 5.512863929736534 Test RE 1.1222683026809532\n",
      "18 Train Loss 15.928559 Test MSE 5.076912058843596 Test RE 1.076980600883673\n",
      "19 Train Loss 15.200556 Test MSE 4.876595145068348 Test RE 1.0555198651682176\n",
      "20 Train Loss 14.375524 Test MSE 5.019650762401643 Test RE 1.0708898728269738\n",
      "21 Train Loss 13.400935 Test MSE 4.269508464782923 Test RE 0.9876361998253729\n",
      "22 Train Loss 12.367953 Test MSE 4.2364339105409545 Test RE 0.9838033070958454\n",
      "23 Train Loss 11.695862 Test MSE 4.186553717483157 Test RE 0.9779944591208276\n",
      "24 Train Loss 9.807074 Test MSE 3.483485610022683 Test RE 0.8921031455680487\n",
      "25 Train Loss 7.3998632 Test MSE 3.3338789981126653 Test RE 0.8727361729953854\n",
      "26 Train Loss 6.4318147 Test MSE 3.2298555241591673 Test RE 0.8590127468813056\n",
      "27 Train Loss 5.777217 Test MSE 2.828894889181702 Test RE 0.8039267242896995\n",
      "28 Train Loss 5.4545817 Test MSE 2.7445477715381057 Test RE 0.7918509781563746\n",
      "29 Train Loss 5.110211 Test MSE 2.7298116307796323 Test RE 0.789722297253897\n",
      "30 Train Loss 4.9598546 Test MSE 2.789295736940815 Test RE 0.7982801712431602\n",
      "31 Train Loss 4.838587 Test MSE 2.7236371346259713 Test RE 0.7888286645835308\n",
      "32 Train Loss 4.6061106 Test MSE 2.71964190274747 Test RE 0.7882498962210828\n",
      "33 Train Loss 4.3942738 Test MSE 2.565298738300426 Test RE 0.7655561224374313\n",
      "34 Train Loss 4.2589707 Test MSE 2.4814166972508103 Test RE 0.7529357363250647\n",
      "35 Train Loss 4.0258255 Test MSE 2.658380398895795 Test RE 0.7793214381977711\n",
      "36 Train Loss 3.8416936 Test MSE 2.7275304359880854 Test RE 0.7893922585635168\n",
      "37 Train Loss 3.6007044 Test MSE 2.7582181711499287 Test RE 0.7938206056013931\n",
      "38 Train Loss 3.4284153 Test MSE 2.8193871915879307 Test RE 0.8025746193876292\n",
      "39 Train Loss 3.274948 Test MSE 2.7177721427069397 Test RE 0.78797888782834\n",
      "40 Train Loss 3.1891806 Test MSE 2.650519358215856 Test RE 0.7781683276024649\n",
      "41 Train Loss 3.0582018 Test MSE 2.6349931955665817 Test RE 0.7758858101442647\n",
      "42 Train Loss 2.8844557 Test MSE 2.5480793007651705 Test RE 0.7629824178985468\n",
      "43 Train Loss 2.7459972 Test MSE 2.482148364717593 Test RE 0.7530467329948545\n",
      "44 Train Loss 2.5904782 Test MSE 2.4677449326659997 Test RE 0.7508586610802411\n",
      "45 Train Loss 2.349041 Test MSE 2.33897571252192 Test RE 0.7310059575439324\n",
      "46 Train Loss 2.1852527 Test MSE 2.365404137066151 Test RE 0.7351242279580658\n",
      "47 Train Loss 1.9453577 Test MSE 2.421669366409762 Test RE 0.7438159455680778\n",
      "48 Train Loss 1.7690947 Test MSE 2.5195597142757684 Test RE 0.7587005312076888\n",
      "49 Train Loss 1.6616493 Test MSE 2.5550925996447997 Test RE 0.7640317076071163\n",
      "50 Train Loss 1.5271926 Test MSE 2.5717340937810547 Test RE 0.7665157650373812\n",
      "51 Train Loss 1.4621589 Test MSE 2.596532362623062 Test RE 0.7702025111258435\n",
      "52 Train Loss 1.3883629 Test MSE 2.6702912007678488 Test RE 0.7810653513627189\n",
      "53 Train Loss 1.3346682 Test MSE 2.7680570381478273 Test RE 0.7952351678702535\n",
      "54 Train Loss 1.2764099 Test MSE 2.8488865620434076 Test RE 0.8067623802355474\n",
      "55 Train Loss 1.2231048 Test MSE 2.9050704899036726 Test RE 0.8146787674429968\n",
      "56 Train Loss 1.1381028 Test MSE 2.982219992824876 Test RE 0.8254255332623912\n",
      "57 Train Loss 1.1026711 Test MSE 3.0086150774261857 Test RE 0.8290703315039332\n",
      "58 Train Loss 1.0558048 Test MSE 3.04569998887495 Test RE 0.8341643420146544\n",
      "59 Train Loss 1.0117784 Test MSE 3.103415775892478 Test RE 0.842030925075872\n",
      "60 Train Loss 0.94580406 Test MSE 3.1134355332336074 Test RE 0.8433891295826734\n",
      "61 Train Loss 0.90647274 Test MSE 3.093932734233146 Test RE 0.8407434527928788\n",
      "62 Train Loss 0.86806244 Test MSE 3.196254140985487 Test RE 0.8545327507858268\n",
      "63 Train Loss 0.8367845 Test MSE 3.220315598883489 Test RE 0.8577431889123622\n",
      "64 Train Loss 0.8014157 Test MSE 3.260896171025604 Test RE 0.8631306636971103\n",
      "65 Train Loss 0.7800534 Test MSE 3.25621204200437 Test RE 0.862510516945347\n",
      "66 Train Loss 0.7500484 Test MSE 3.277095601437419 Test RE 0.8652719313454598\n",
      "67 Train Loss 0.7210784 Test MSE 3.346631369752967 Test RE 0.8744037250503297\n",
      "68 Train Loss 0.69547284 Test MSE 3.384073708722895 Test RE 0.8792815637214695\n",
      "69 Train Loss 0.6796419 Test MSE 3.368660740580952 Test RE 0.8772769080349768\n",
      "70 Train Loss 0.65965587 Test MSE 3.3887196393338215 Test RE 0.8798849311955602\n",
      "71 Train Loss 0.6427802 Test MSE 3.4095557204766145 Test RE 0.8825858419662278\n",
      "72 Train Loss 0.62389785 Test MSE 3.4151503859004197 Test RE 0.8833096530706152\n",
      "73 Train Loss 0.6087351 Test MSE 3.4133396229263484 Test RE 0.8830754501359305\n",
      "74 Train Loss 0.5921465 Test MSE 3.4164136118454134 Test RE 0.8834730011493124\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "75 Train Loss 0.5774001 Test MSE 3.427222573605516 Test RE 0.8848694779371994\n",
      "76 Train Loss 0.57006264 Test MSE 3.4620228298446145 Test RE 0.8893506442431125\n",
      "77 Train Loss 0.5598219 Test MSE 3.4683003038927565 Test RE 0.8901565816073969\n",
      "78 Train Loss 0.5496231 Test MSE 3.439241117865323 Test RE 0.8864196450962774\n",
      "79 Train Loss 0.5384375 Test MSE 3.4119192499201674 Test RE 0.8828916965128969\n",
      "80 Train Loss 0.5278741 Test MSE 3.3882941796068833 Test RE 0.8798296939092988\n",
      "81 Train Loss 0.520037 Test MSE 3.386334885084398 Test RE 0.8795752746225128\n",
      "82 Train Loss 0.5135556 Test MSE 3.383938499522172 Test RE 0.8792639978888052\n",
      "83 Train Loss 0.5035595 Test MSE 3.35729982688351 Test RE 0.8757963367585307\n",
      "84 Train Loss 0.49595082 Test MSE 3.3475437302523163 Test RE 0.8745229071421518\n",
      "85 Train Loss 0.4864627 Test MSE 3.3483202508750627 Test RE 0.8746243316119069\n",
      "86 Train Loss 0.47787282 Test MSE 3.344951608393537 Test RE 0.8741842545171612\n",
      "87 Train Loss 0.47154343 Test MSE 3.3419150137165254 Test RE 0.8737873659674158\n",
      "88 Train Loss 0.46370128 Test MSE 3.331954772115945 Test RE 0.8724842766276624\n",
      "89 Train Loss 0.45618206 Test MSE 3.347900801387926 Test RE 0.8745695470946157\n",
      "90 Train Loss 0.45010927 Test MSE 3.311509038793357 Test RE 0.8698032632455927\n",
      "91 Train Loss 0.44318336 Test MSE 3.305963998338928 Test RE 0.8690747260506241\n",
      "92 Train Loss 0.43818772 Test MSE 3.307849003328241 Test RE 0.8693224566281652\n",
      "93 Train Loss 0.43353885 Test MSE 3.305186571514499 Test RE 0.8689725347184922\n",
      "94 Train Loss 0.42782027 Test MSE 3.3045292867877607 Test RE 0.8688861264686868\n",
      "95 Train Loss 0.42222092 Test MSE 3.3321728918038462 Test RE 0.8725128338705926\n",
      "96 Train Loss 0.41725913 Test MSE 3.3306056000998 Test RE 0.8723076159830958\n",
      "97 Train Loss 0.4134066 Test MSE 3.3389602414968667 Test RE 0.8734009986354808\n",
      "98 Train Loss 0.40942958 Test MSE 3.372541467349994 Test RE 0.8777820779395668\n",
      "99 Train Loss 0.4067673 Test MSE 3.388781190875205 Test RE 0.8798929221233429\n",
      "Training time: 139.59\n",
      "KG_atanh\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 56.131058 Test MSE 8.030868320208034 Test RE 1.3545321351353266\n",
      "1 Train Loss 46.01455 Test MSE 8.294656086039103 Test RE 1.3765983740695733\n",
      "2 Train Loss 44.511017 Test MSE 8.052307898549346 Test RE 1.3563389909114942\n",
      "3 Train Loss 42.666687 Test MSE 8.05343192362677 Test RE 1.3564336535758337\n",
      "4 Train Loss 42.052124 Test MSE 8.088341253180541 Test RE 1.3593703510135249\n",
      "5 Train Loss 41.418472 Test MSE 8.298520711907303 Test RE 1.3769190273975085\n",
      "6 Train Loss 40.28566 Test MSE 8.157908118972513 Test RE 1.3652037265730683\n",
      "7 Train Loss 39.400295 Test MSE 8.024415409011658 Test RE 1.3539878333393944\n",
      "8 Train Loss 36.333527 Test MSE 6.422129031261721 Test RE 1.2112884435169913\n",
      "9 Train Loss 30.318804 Test MSE 6.339419118089846 Test RE 1.2034631393749853\n",
      "10 Train Loss 28.001358 Test MSE 6.041822805161439 Test RE 1.1748760543139771\n",
      "11 Train Loss 27.393559 Test MSE 6.287636073494662 Test RE 1.1985378639977702\n",
      "12 Train Loss 27.063934 Test MSE 6.199909042192497 Test RE 1.1901473099436692\n",
      "13 Train Loss 26.55167 Test MSE 6.088516031953514 Test RE 1.179407234036209\n",
      "14 Train Loss 25.748093 Test MSE 5.782227236917333 Test RE 1.1493588330899123\n",
      "15 Train Loss 24.958708 Test MSE 5.2298781915510295 Test RE 1.0930847798734051\n",
      "16 Train Loss 24.61701 Test MSE 5.431664649345799 Test RE 1.1139726664199783\n",
      "17 Train Loss 24.233902 Test MSE 5.399495211907751 Test RE 1.1106689746252922\n",
      "18 Train Loss 24.032795 Test MSE 5.2589995228364925 Test RE 1.096123846313613\n",
      "19 Train Loss 23.512703 Test MSE 4.578272911569593 Test RE 1.0227250631748002\n",
      "20 Train Loss 21.148212 Test MSE 4.183475064706807 Test RE 0.9776348006411258\n",
      "21 Train Loss 18.59108 Test MSE 3.669029766853374 Test RE 0.9155533960243675\n",
      "22 Train Loss 17.795504 Test MSE 3.552952753507524 Test RE 0.9009543291956847\n",
      "23 Train Loss 17.175262 Test MSE 3.975788364438127 Test RE 0.9530587862388932\n",
      "24 Train Loss 16.711657 Test MSE 3.805778175555228 Test RE 0.9324591091898479\n",
      "25 Train Loss 16.372765 Test MSE 3.6644052017567432 Test RE 0.9149762173736862\n",
      "26 Train Loss 16.177391 Test MSE 3.5991633750476084 Test RE 0.9067944234979953\n",
      "27 Train Loss 15.837046 Test MSE 3.3553812227305113 Test RE 0.8755460542835726\n",
      "28 Train Loss 15.639351 Test MSE 3.3089441131339656 Test RE 0.8694663454158852\n",
      "29 Train Loss 14.427464 Test MSE 2.431925343401514 Test RE 0.7453893435486854\n",
      "30 Train Loss 11.775099 Test MSE 2.3300597003283436 Test RE 0.7296113553945497\n",
      "31 Train Loss 8.860764 Test MSE 1.9743110529518877 Test RE 0.6716079814887768\n",
      "32 Train Loss 7.9688234 Test MSE 2.1036426126979104 Test RE 0.6932566434005135\n",
      "33 Train Loss 6.9029408 Test MSE 2.210828396543385 Test RE 0.7106987921629223\n",
      "34 Train Loss 6.280528 Test MSE 2.2845661820828522 Test RE 0.7224535544881516\n",
      "35 Train Loss 6.0202374 Test MSE 2.3036840307012754 Test RE 0.7254700971838178\n",
      "36 Train Loss 5.744686 Test MSE 2.3048622306828546 Test RE 0.7256555912843855\n",
      "37 Train Loss 5.62241 Test MSE 2.303298473748766 Test RE 0.7254093853543084\n",
      "38 Train Loss 5.493143 Test MSE 2.249618545716562 Test RE 0.7169064737026475\n",
      "39 Train Loss 5.40773 Test MSE 2.292988022732971 Test RE 0.7237839584154424\n",
      "40 Train Loss 5.350255 Test MSE 2.264154869695221 Test RE 0.7192189556020157\n",
      "41 Train Loss 5.270159 Test MSE 2.302978031045874 Test RE 0.7253589228909669\n",
      "42 Train Loss 5.2224693 Test MSE 2.269791419625392 Test RE 0.7201136368179201\n",
      "43 Train Loss 5.1705246 Test MSE 2.2410429999347667 Test RE 0.7155387452922966\n",
      "44 Train Loss 5.086193 Test MSE 2.247926018480499 Test RE 0.7166367364072794\n",
      "45 Train Loss 5.016819 Test MSE 2.2694801441582264 Test RE 0.720064257524228\n",
      "46 Train Loss 4.9450054 Test MSE 2.255769129086005 Test RE 0.7178858360203204\n",
      "47 Train Loss 4.894145 Test MSE 2.2341955459126415 Test RE 0.714444753107486\n",
      "48 Train Loss 4.805423 Test MSE 2.256860945114096 Test RE 0.7180595471579472\n",
      "49 Train Loss 4.6808443 Test MSE 2.2065237550466414 Test RE 0.7100065642768512\n",
      "50 Train Loss 4.583268 Test MSE 2.190371544108387 Test RE 0.7074030934798139\n",
      "51 Train Loss 4.4697385 Test MSE 2.1578969376012687 Test RE 0.702139506400812\n",
      "52 Train Loss 4.301305 Test MSE 2.1193386061322945 Test RE 0.6958381488477837\n",
      "53 Train Loss 3.4363415 Test MSE 1.6141935373096663 Test RE 0.6072756428212397\n",
      "54 Train Loss 2.4841304 Test MSE 1.4553165904884995 Test RE 0.5766161478534817\n",
      "55 Train Loss 1.982444 Test MSE 1.2310849845395864 Test RE 0.5303371829760952\n",
      "56 Train Loss 1.7095397 Test MSE 1.1551777315713168 Test RE 0.5137270862230449\n",
      "57 Train Loss 1.5405487 Test MSE 1.0817628909986312 Test RE 0.4971347251259984\n",
      "58 Train Loss 1.273617 Test MSE 0.9592087613564955 Test RE 0.4681280113331984\n",
      "59 Train Loss 1.2025828 Test MSE 0.8612919147548094 Test RE 0.44359153039391147\n",
      "60 Train Loss 1.1146497 Test MSE 0.712751787551944 Test RE 0.40353129196219184\n",
      "61 Train Loss 0.98333555 Test MSE 0.5371050592691745 Test RE 0.35029801947564143\n",
      "62 Train Loss 0.8413719 Test MSE 0.37157827244968406 Test RE 0.29136220550468134\n",
      "63 Train Loss 0.70122766 Test MSE 0.2979966014134563 Test RE 0.2609238126229931\n",
      "64 Train Loss 0.5067113 Test MSE 0.17893310292677445 Test RE 0.20218708311460545\n",
      "65 Train Loss 0.42301363 Test MSE 0.12087061055142569 Test RE 0.16617604488045895\n",
      "66 Train Loss 0.36771175 Test MSE 0.06578539812662489 Test RE 0.1225950158011235\n",
      "67 Train Loss 0.30194148 Test MSE 0.04210207816934196 Test RE 0.09807534102135831\n",
      "68 Train Loss 0.25611028 Test MSE 0.041209554866179735 Test RE 0.09703022132253225\n",
      "69 Train Loss 0.22778496 Test MSE 0.03374783362260914 Test RE 0.08780737820241348\n",
      "70 Train Loss 0.18481147 Test MSE 0.040629542152572216 Test RE 0.09634496525756991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71 Train Loss 0.1565116 Test MSE 0.03895772267817346 Test RE 0.0943419483698466\n",
      "72 Train Loss 0.1356704 Test MSE 0.032995740606274415 Test RE 0.08682344235839838\n",
      "73 Train Loss 0.12342517 Test MSE 0.03100815577920293 Test RE 0.08416781032077254\n",
      "74 Train Loss 0.10291768 Test MSE 0.02093639483428519 Test RE 0.06916064107613193\n",
      "75 Train Loss 0.0912689 Test MSE 0.019238982596623405 Test RE 0.06629779908128836\n",
      "76 Train Loss 0.082680374 Test MSE 0.017480271331076176 Test RE 0.06319491660148097\n",
      "77 Train Loss 0.076465204 Test MSE 0.017337175050214607 Test RE 0.06293572324358078\n",
      "78 Train Loss 0.07111855 Test MSE 0.016305325445041072 Test RE 0.061034135291043005\n",
      "79 Train Loss 0.06810081 Test MSE 0.014946880470084162 Test RE 0.05843638490834882\n",
      "80 Train Loss 0.057766188 Test MSE 0.014790478206280142 Test RE 0.0581298454374648\n",
      "81 Train Loss 0.050763655 Test MSE 0.014378264197522026 Test RE 0.05731407530720778\n",
      "82 Train Loss 0.04729543 Test MSE 0.012154284963268282 Test RE 0.05269540751911893\n",
      "83 Train Loss 0.04336898 Test MSE 0.010369520245096051 Test RE 0.04867291897495989\n",
      "84 Train Loss 0.04078739 Test MSE 0.010082540720271795 Test RE 0.047994674718549984\n",
      "85 Train Loss 0.037949033 Test MSE 0.01000247296054485 Test RE 0.047803726532763734\n",
      "86 Train Loss 0.035009935 Test MSE 0.00895199785206603 Test RE 0.045223903416085905\n",
      "87 Train Loss 0.033227038 Test MSE 0.0079568178256615 Test RE 0.04263612901548773\n",
      "88 Train Loss 0.032148305 Test MSE 0.00782671654772809 Test RE 0.042286122456194175\n",
      "89 Train Loss 0.030311657 Test MSE 0.0068903121096003965 Test RE 0.03967596582851282\n",
      "90 Train Loss 0.026393283 Test MSE 0.004537457794699118 Test RE 0.03219692229910648\n",
      "91 Train Loss 0.024189496 Test MSE 0.003857969229068391 Test RE 0.029688444245247178\n",
      "92 Train Loss 0.022789642 Test MSE 0.0033110403780496093 Test RE 0.02750364784992566\n",
      "93 Train Loss 0.0216532 Test MSE 0.003136394900829623 Test RE 0.02676846265401378\n",
      "94 Train Loss 0.021017723 Test MSE 0.003172950374827235 Test RE 0.026924007332329355\n",
      "95 Train Loss 0.01980165 Test MSE 0.0029812745162973433 Test RE 0.026098109211887648\n",
      "96 Train Loss 0.018680044 Test MSE 0.003283748366125589 Test RE 0.027390060726685855\n",
      "97 Train Loss 0.018251874 Test MSE 0.003569093068035537 Test RE 0.028555317719415184\n",
      "98 Train Loss 0.017885255 Test MSE 0.004288304158353413 Test RE 0.03130046942412462\n",
      "99 Train Loss 0.016669806 Test MSE 0.003959911799234439 Test RE 0.030078128935536574\n",
      "Training time: 144.66\n",
      "KG_atanh\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 56.59252 Test MSE 8.611391171317658 Test RE 1.4026351528966696\n",
      "1 Train Loss 47.85994 Test MSE 8.891860325620224 Test RE 1.4252937355944015\n",
      "2 Train Loss 45.612556 Test MSE 8.620962639594984 Test RE 1.4034144433250415\n",
      "3 Train Loss 44.625656 Test MSE 8.585172005352465 Test RE 1.4004982182941124\n",
      "4 Train Loss 44.262398 Test MSE 8.339478535578664 Test RE 1.3803127763527954\n",
      "5 Train Loss 44.069138 Test MSE 8.455260575376606 Test RE 1.3898616073761936\n",
      "6 Train Loss 40.382244 Test MSE 7.606566027616558 Test RE 1.3182639628321993\n",
      "7 Train Loss 39.89955 Test MSE 7.855731279483374 Test RE 1.3396809130477645\n",
      "8 Train Loss 39.63379 Test MSE 7.881872307318981 Test RE 1.3419080481908559\n",
      "9 Train Loss 38.75862 Test MSE 7.733547904218164 Test RE 1.329221784563514\n",
      "10 Train Loss 36.721767 Test MSE 7.965533823565158 Test RE 1.3490110383458103\n",
      "11 Train Loss 35.26961 Test MSE 8.206784070338006 Test RE 1.3692872481080693\n",
      "12 Train Loss 34.352528 Test MSE 8.293554999852416 Test RE 1.3765070017604233\n",
      "13 Train Loss 33.774727 Test MSE 8.201429290796673 Test RE 1.3688404575194986\n",
      "14 Train Loss 31.658756 Test MSE 8.61595478731141 Test RE 1.4030067676711337\n",
      "15 Train Loss 30.683441 Test MSE 8.970852903920962 Test RE 1.4316106748456157\n",
      "16 Train Loss 29.625772 Test MSE 8.525108106665842 Test RE 1.395590510576224\n",
      "17 Train Loss 29.138231 Test MSE 8.567534408471596 Test RE 1.3990588689806218\n",
      "18 Train Loss 28.389568 Test MSE 8.547742050346862 Test RE 1.3974419110666616\n",
      "19 Train Loss 28.131973 Test MSE 8.456520211650211 Test RE 1.3899651319737558\n",
      "20 Train Loss 27.892456 Test MSE 8.667445540659696 Test RE 1.407192854144773\n",
      "21 Train Loss 27.65784 Test MSE 8.703588513310132 Test RE 1.4101237766166521\n",
      "22 Train Loss 27.44673 Test MSE 8.741495543838983 Test RE 1.4131912199648589\n",
      "23 Train Loss 27.296965 Test MSE 8.802136785995156 Test RE 1.4180845222044818\n",
      "24 Train Loss 27.10595 Test MSE 8.774396312217839 Test RE 1.4158481686524456\n",
      "25 Train Loss 26.988102 Test MSE 8.670867135918934 Test RE 1.4074705812610258\n",
      "26 Train Loss 26.752037 Test MSE 8.587559680204949 Test RE 1.4006929553306262\n",
      "27 Train Loss 26.583874 Test MSE 8.498540262979798 Test RE 1.3934141884274047\n",
      "28 Train Loss 26.458538 Test MSE 8.482221581646089 Test RE 1.3920757463270317\n",
      "29 Train Loss 26.329243 Test MSE 8.564122296967295 Test RE 1.39878024621544\n",
      "30 Train Loss 26.105965 Test MSE 8.484777421235641 Test RE 1.392285458741074\n",
      "31 Train Loss 25.88727 Test MSE 8.601281703649553 Test RE 1.4018115891678766\n",
      "32 Train Loss 24.33908 Test MSE 7.777111921264216 Test RE 1.3329603610778769\n",
      "33 Train Loss 23.55654 Test MSE 7.696216464192913 Test RE 1.326009688863372\n",
      "34 Train Loss 22.858488 Test MSE 7.6478291644495044 Test RE 1.3218347026219803\n",
      "35 Train Loss 22.496407 Test MSE 7.892310914574641 Test RE 1.3427963533892782\n",
      "36 Train Loss 22.328224 Test MSE 7.9772214534248675 Test RE 1.350000360765626\n",
      "37 Train Loss 22.10646 Test MSE 7.799815013165415 Test RE 1.334904544759231\n",
      "38 Train Loss 21.93998 Test MSE 7.844858049399805 Test RE 1.3387534562484376\n",
      "39 Train Loss 21.806534 Test MSE 7.883106309639777 Test RE 1.3420130900370566\n",
      "40 Train Loss 21.695192 Test MSE 7.838711875594483 Test RE 1.3382289200723934\n",
      "41 Train Loss 21.50242 Test MSE 7.903614125614453 Test RE 1.3437575724949706\n",
      "42 Train Loss 21.339054 Test MSE 8.037047398100396 Test RE 1.355053134229046\n",
      "43 Train Loss 21.143486 Test MSE 7.835834305769391 Test RE 1.3379832674259924\n",
      "44 Train Loss 21.061478 Test MSE 7.848918310677946 Test RE 1.339099860582976\n",
      "45 Train Loss 20.954634 Test MSE 7.867595015529376 Test RE 1.3406921253752961\n",
      "46 Train Loss 20.843336 Test MSE 7.893260843843913 Test RE 1.3428771613549544\n",
      "47 Train Loss 20.706274 Test MSE 7.9535424938876025 Test RE 1.3479952540271416\n",
      "48 Train Loss 20.581894 Test MSE 7.963601664636249 Test RE 1.3488474170580584\n",
      "49 Train Loss 20.338303 Test MSE 8.073119739141562 Test RE 1.358090643693932\n",
      "50 Train Loss 19.85855 Test MSE 8.025213073428102 Test RE 1.3540551280282462\n",
      "51 Train Loss 19.599426 Test MSE 8.027330127290142 Test RE 1.3542337163463325\n",
      "52 Train Loss 19.133667 Test MSE 7.878655976213172 Test RE 1.3416342261328242\n",
      "53 Train Loss 18.783073 Test MSE 8.046874637695744 Test RE 1.3558813216839667\n",
      "54 Train Loss 18.530323 Test MSE 8.075815426545404 Test RE 1.3583173641193413\n",
      "55 Train Loss 18.0769 Test MSE 8.019014786542384 Test RE 1.3535321236391726\n",
      "56 Train Loss 17.765163 Test MSE 8.05412847988876 Test RE 1.3564923125385684\n",
      "57 Train Loss 17.666698 Test MSE 8.08841147693626 Test RE 1.3593762520927006\n",
      "58 Train Loss 17.49786 Test MSE 8.0858190814895 Test RE 1.3591583895845973\n",
      "59 Train Loss 17.386126 Test MSE 8.117395807163659 Test RE 1.3618096950490743\n",
      "60 Train Loss 17.139055 Test MSE 8.10642065681694 Test RE 1.3608887641317602\n",
      "61 Train Loss 17.093582 Test MSE 8.128359054602516 Test RE 1.3627290058348585\n",
      "62 Train Loss 16.880703 Test MSE 8.250666195562959 Test RE 1.3729431945524686\n",
      "63 Train Loss 16.752048 Test MSE 8.169376194552914 Test RE 1.3661629651984541\n",
      "64 Train Loss 16.616648 Test MSE 8.122763326661937 Test RE 1.362259859875956\n",
      "65 Train Loss 16.527945 Test MSE 8.081638649738618 Test RE 1.358806996397859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66 Train Loss 16.461266 Test MSE 8.079364959525261 Test RE 1.3586158394039223\n",
      "67 Train Loss 16.399452 Test MSE 8.139651358696597 Test RE 1.3636752613693426\n",
      "68 Train Loss 16.194561 Test MSE 8.166180338333028 Test RE 1.3658957178964897\n",
      "69 Train Loss 16.115477 Test MSE 8.12437028784137 Test RE 1.362394604073472\n",
      "70 Train Loss 16.025106 Test MSE 8.12450601754469 Test RE 1.3624059844412137\n",
      "71 Train Loss 15.96003 Test MSE 8.13634846185885 Test RE 1.3633985581349244\n",
      "72 Train Loss 15.925512 Test MSE 8.143313593827935 Test RE 1.3639820028754144\n",
      "73 Train Loss 15.855818 Test MSE 8.118696794819098 Test RE 1.3619188203561254\n",
      "74 Train Loss 15.787106 Test MSE 8.140421477293994 Test RE 1.3637397706949872\n",
      "75 Train Loss 15.749645 Test MSE 8.112530527987223 Test RE 1.3614015236606318\n",
      "76 Train Loss 15.68391 Test MSE 8.128828167498304 Test RE 1.3627683289333126\n",
      "77 Train Loss 15.642542 Test MSE 8.07849445128375 Test RE 1.3585426456473637\n",
      "78 Train Loss 15.583764 Test MSE 8.08547518191602 Test RE 1.359129485959882\n",
      "79 Train Loss 15.550032 Test MSE 8.057236346741455 Test RE 1.3567540038496113\n",
      "80 Train Loss 15.537657 Test MSE 8.059380960689554 Test RE 1.3569345570715836\n",
      "81 Train Loss 15.52101 Test MSE 8.06954137948869 Test RE 1.3577896277575456\n",
      "82 Train Loss 15.487039 Test MSE 8.032543818124406 Test RE 1.3546734272919407\n",
      "83 Train Loss 15.474888 Test MSE 8.016356266646326 Test RE 1.353307738819658\n",
      "84 Train Loss 15.465524 Test MSE 8.017776942861651 Test RE 1.3534276515878643\n",
      "85 Train Loss 15.41942 Test MSE 8.030912708983472 Test RE 1.3545358785623938\n",
      "86 Train Loss 15.391373 Test MSE 8.024311127495823 Test RE 1.3539790354173522\n",
      "87 Train Loss 15.361626 Test MSE 8.012504403791244 Test RE 1.3529825672597486\n",
      "88 Train Loss 15.345331 Test MSE 8.057761365590883 Test RE 1.356798206960446\n",
      "89 Train Loss 15.281559 Test MSE 8.057915146678672 Test RE 1.3568111540371341\n",
      "90 Train Loss 15.24746 Test MSE 8.048922452237546 Test RE 1.3560538369140496\n",
      "91 Train Loss 15.203859 Test MSE 8.013571660543086 Test RE 1.3530726721524475\n",
      "92 Train Loss 15.147953 Test MSE 8.020285274746076 Test RE 1.3536393424518793\n",
      "93 Train Loss 15.103909 Test MSE 8.020843731639033 Test RE 1.3536864689590196\n",
      "94 Train Loss 15.087745 Test MSE 8.00954584126565 Test RE 1.3527327544111616\n",
      "95 Train Loss 15.057646 Test MSE 7.996154868404458 Test RE 1.3516014802065108\n",
      "96 Train Loss 15.019579 Test MSE 8.028243989901938 Test RE 1.3543107997800858\n",
      "97 Train Loss 14.995083 Test MSE 8.044515990191949 Test RE 1.355682593567723\n",
      "98 Train Loss 14.903875 Test MSE 8.044687129148896 Test RE 1.355697013880354\n",
      "99 Train Loss 14.846681 Test MSE 8.096036392341423 Test RE 1.3600168406256998\n",
      "Training time: 140.60\n",
      "KG_atanh\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 58.80519 Test MSE 8.518829089593602 Test RE 1.3950764671956029\n",
      "1 Train Loss 56.288017 Test MSE 8.732777299166214 Test RE 1.412486327979261\n",
      "2 Train Loss 47.927624 Test MSE 9.047065752125174 Test RE 1.4376790149764074\n",
      "3 Train Loss 45.778496 Test MSE 8.45612145111785 Test RE 1.3899323602333578\n",
      "4 Train Loss 44.559746 Test MSE 8.578051314770118 Test RE 1.3999172990551327\n",
      "5 Train Loss 43.89246 Test MSE 8.686459217628496 Test RE 1.4087354800992342\n",
      "6 Train Loss 43.571632 Test MSE 8.635385632635293 Test RE 1.4045879188762223\n",
      "7 Train Loss 43.24385 Test MSE 8.703285460870092 Test RE 1.4100992266755217\n",
      "8 Train Loss 42.95889 Test MSE 8.816324883322848 Test RE 1.419226961844411\n",
      "9 Train Loss 42.451294 Test MSE 9.263369731624751 Test RE 1.454764047561546\n",
      "10 Train Loss 39.259506 Test MSE 8.313494543965762 Test RE 1.3781607223160397\n",
      "11 Train Loss 36.454605 Test MSE 7.909611267083764 Test RE 1.344267287165309\n",
      "12 Train Loss 32.063835 Test MSE 7.365489234134842 Test RE 1.2972057343116994\n",
      "13 Train Loss 28.645813 Test MSE 6.43585164707374 Test RE 1.212581875645703\n",
      "14 Train Loss 24.404903 Test MSE 5.365167513321079 Test RE 1.1071327642899575\n",
      "15 Train Loss 21.562023 Test MSE 5.350901732433365 Test RE 1.1056598721238478\n",
      "16 Train Loss 18.12339 Test MSE 3.3124058322224723 Test RE 0.8699210314652558\n",
      "17 Train Loss 15.6684065 Test MSE 2.1500410773152034 Test RE 0.7008602656926299\n",
      "18 Train Loss 12.988863 Test MSE 2.2995577780362417 Test RE 0.7248200916868627\n",
      "19 Train Loss 11.218495 Test MSE 1.9356295432202535 Test RE 0.6649962270955005\n",
      "20 Train Loss 9.351524 Test MSE 1.0965605507645915 Test RE 0.5005233806509753\n",
      "21 Train Loss 5.2707043 Test MSE 0.4503149711775015 Test RE 0.32074969580024115\n",
      "22 Train Loss 3.9642665 Test MSE 0.4400869155889493 Test RE 0.3170861614332651\n",
      "23 Train Loss 2.641647 Test MSE 0.22290655705032256 Test RE 0.22566774138299645\n",
      "24 Train Loss 1.7873439 Test MSE 0.14575184723752066 Test RE 0.18247992513998335\n",
      "25 Train Loss 1.2276963 Test MSE 0.06735236666681743 Test RE 0.1240464930380248\n",
      "26 Train Loss 0.9722223 Test MSE 0.07242746216814722 Test RE 0.12863516095259045\n",
      "27 Train Loss 0.7292552 Test MSE 0.04464458468869032 Test RE 0.10099327458498543\n",
      "28 Train Loss 0.57220423 Test MSE 0.05237549310987394 Test RE 0.10938861476133352\n",
      "29 Train Loss 0.47353572 Test MSE 0.06346438741893297 Test RE 0.12041292509747097\n",
      "30 Train Loss 0.38724443 Test MSE 0.044563440675300846 Test RE 0.10090145239885671\n",
      "31 Train Loss 0.32940292 Test MSE 0.03201985504315913 Test RE 0.08552985603668249\n",
      "32 Train Loss 0.29446524 Test MSE 0.03727162624171755 Test RE 0.0922777999007936\n",
      "33 Train Loss 0.272807 Test MSE 0.03357208066882061 Test RE 0.08757843682680189\n",
      "34 Train Loss 0.24364823 Test MSE 0.026451525528976835 Test RE 0.07773801058348682\n",
      "35 Train Loss 0.23047128 Test MSE 0.025263433989286335 Test RE 0.07597212106271946\n",
      "36 Train Loss 0.21292944 Test MSE 0.02319284107661843 Test RE 0.07279223202396874\n",
      "37 Train Loss 0.19079573 Test MSE 0.015144641403718294 Test RE 0.058821698061608134\n",
      "38 Train Loss 0.16455427 Test MSE 0.014025377496017361 Test RE 0.05660637444942018\n",
      "39 Train Loss 0.13829854 Test MSE 0.013188479337821038 Test RE 0.05489154084576653\n",
      "40 Train Loss 0.121801466 Test MSE 0.012755627618546234 Test RE 0.05398324368022836\n",
      "41 Train Loss 0.11605468 Test MSE 0.013732031399529717 Test RE 0.05601127441168508\n",
      "42 Train Loss 0.10728597 Test MSE 0.013703154744884977 Test RE 0.055952351249483376\n",
      "43 Train Loss 0.09852743 Test MSE 0.013243932942797201 Test RE 0.05500682103639765\n",
      "44 Train Loss 0.0937507 Test MSE 0.013025481856144551 Test RE 0.05455128176072169\n",
      "45 Train Loss 0.08423882 Test MSE 0.012136319147435563 Test RE 0.05265644734603376\n",
      "46 Train Loss 0.08092675 Test MSE 0.012974029283031556 Test RE 0.054443432348526284\n",
      "47 Train Loss 0.07670859 Test MSE 0.013638743220932122 Test RE 0.0558206946574377\n",
      "48 Train Loss 0.07271318 Test MSE 0.013426116232577964 Test RE 0.055383865204292185\n",
      "49 Train Loss 0.068944484 Test MSE 0.01357417707752679 Test RE 0.05568840964588866\n",
      "50 Train Loss 0.06596245 Test MSE 0.013060663587092545 Test RE 0.0546249033873202\n",
      "51 Train Loss 0.06400946 Test MSE 0.01094694253079704 Test RE 0.05000972649090621\n",
      "52 Train Loss 0.06139537 Test MSE 0.010686418488138597 Test RE 0.049411057524877657\n",
      "53 Train Loss 0.058508843 Test MSE 0.010322153870342627 Test RE 0.04856162653121197\n",
      "54 Train Loss 0.054652102 Test MSE 0.007483050550184646 Test RE 0.04134732328998585\n",
      "55 Train Loss 0.0503091 Test MSE 0.00726825497506249 Test RE 0.04074958004760747\n",
      "56 Train Loss 0.048071068 Test MSE 0.006814463242525455 Test RE 0.03945698410828425\n",
      "57 Train Loss 0.04717307 Test MSE 0.00744998003431614 Test RE 0.04125585715644825\n",
      "58 Train Loss 0.045369476 Test MSE 0.006898928413262318 Test RE 0.03970076538456698\n",
      "59 Train Loss 0.041825112 Test MSE 0.007825343738145883 Test RE 0.04228241379116809\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 Train Loss 0.03971908 Test MSE 0.007397993203523836 Test RE 0.04111166106694133\n",
      "61 Train Loss 0.03803417 Test MSE 0.006810087567132911 Test RE 0.039444314095059094\n",
      "62 Train Loss 0.03664517 Test MSE 0.006418933286422862 Test RE 0.03829477227419437\n",
      "63 Train Loss 0.03529134 Test MSE 0.006362764028036167 Test RE 0.0381268537779755\n",
      "64 Train Loss 0.03359552 Test MSE 0.007036172664219207 Test RE 0.04009371550670862\n",
      "65 Train Loss 0.03114381 Test MSE 0.0066877076002521164 Test RE 0.03908829241944145\n",
      "66 Train Loss 0.029260002 Test MSE 0.006514663978853678 Test RE 0.03857927574576266\n",
      "67 Train Loss 0.027682612 Test MSE 0.005492009328146144 Test RE 0.035422050133620986\n",
      "68 Train Loss 0.02629913 Test MSE 0.004659245972051009 Test RE 0.03262615382801233\n",
      "69 Train Loss 0.02537442 Test MSE 0.004773247548543256 Test RE 0.033022887070739124\n",
      "70 Train Loss 0.024509229 Test MSE 0.0041713140352303965 Test RE 0.030870559706270822\n",
      "71 Train Loss 0.022282638 Test MSE 0.003762350149289525 Test RE 0.02931822449507588\n",
      "72 Train Loss 0.02107339 Test MSE 0.0036235010889927614 Test RE 0.02877214619676396\n",
      "73 Train Loss 0.02019255 Test MSE 0.0033609106490876547 Test RE 0.02771000114072038\n",
      "74 Train Loss 0.019764924 Test MSE 0.0033239976425389903 Test RE 0.027557411022107906\n",
      "75 Train Loss 0.019007681 Test MSE 0.002741197661114958 Test RE 0.02502523915764644\n",
      "76 Train Loss 0.018211655 Test MSE 0.002560130707034708 Test RE 0.024184612298063412\n",
      "77 Train Loss 0.016808625 Test MSE 0.002616469388869537 Test RE 0.02444926959121704\n",
      "78 Train Loss 0.016108828 Test MSE 0.0026103014301894437 Test RE 0.024420434730044065\n",
      "79 Train Loss 0.01448898 Test MSE 0.002253576530000008 Test RE 0.022690507828401266\n",
      "80 Train Loss 0.01392968 Test MSE 0.0022311983298713626 Test RE 0.022577567448414427\n",
      "81 Train Loss 0.0134586245 Test MSE 0.001844853138378117 Test RE 0.02053000050074521\n",
      "82 Train Loss 0.013048976 Test MSE 0.0018295747128187448 Test RE 0.020444812634282928\n",
      "83 Train Loss 0.012592697 Test MSE 0.0017981817449843204 Test RE 0.020268651360919994\n",
      "84 Train Loss 0.01223884 Test MSE 0.001750647116245485 Test RE 0.01999895792216306\n",
      "85 Train Loss 0.011740507 Test MSE 0.0016459128493207078 Test RE 0.019391503284585374\n",
      "86 Train Loss 0.011518026 Test MSE 0.001646822325531223 Test RE 0.01939686009169787\n",
      "87 Train Loss 0.011323679 Test MSE 0.0017742298831202638 Test RE 0.020133209150143203\n",
      "88 Train Loss 0.011168332 Test MSE 0.0017374387567103992 Test RE 0.019923370568855906\n",
      "89 Train Loss 0.011024838 Test MSE 0.001748800905462341 Test RE 0.019988409813692452\n",
      "90 Train Loss 0.010828339 Test MSE 0.0017919862019696185 Test RE 0.020233703932979245\n",
      "91 Train Loss 0.010691491 Test MSE 0.0018128182727715958 Test RE 0.020350973807822293\n",
      "92 Train Loss 0.010419931 Test MSE 0.0018371911402105543 Test RE 0.020487323799968527\n",
      "93 Train Loss 0.010207549 Test MSE 0.001777649228676811 Test RE 0.020152600454244864\n",
      "94 Train Loss 0.010034125 Test MSE 0.0016502423023109242 Test RE 0.01941699049835321\n",
      "95 Train Loss 0.009830557 Test MSE 0.001676496253478948 Test RE 0.019570834929708576\n",
      "96 Train Loss 0.009663779 Test MSE 0.0018238983701108907 Test RE 0.02041307249373687\n",
      "97 Train Loss 0.009540545 Test MSE 0.0018217708405632613 Test RE 0.020401163363324183\n",
      "98 Train Loss 0.009406576 Test MSE 0.0017267019884833813 Test RE 0.01986171541331054\n",
      "99 Train Loss 0.008996247 Test MSE 0.0015645199311892513 Test RE 0.01890595458791737\n",
      "Training time: 139.70\n",
      "KG_atanh\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 55.308678 Test MSE 8.62038097877315 Test RE 1.4033670979761341\n",
      "1 Train Loss 50.102165 Test MSE 7.264110357026548 Test RE 1.2882474070035301\n",
      "2 Train Loss 44.45336 Test MSE 7.958257527331096 Test RE 1.3483947553196702\n",
      "3 Train Loss 43.190178 Test MSE 7.810753846458134 Test RE 1.3358402837542516\n",
      "4 Train Loss 40.62043 Test MSE 7.651524071739876 Test RE 1.3221539740653148\n",
      "5 Train Loss 39.240643 Test MSE 7.401984559644739 Test RE 1.3004155310482264\n",
      "6 Train Loss 37.192543 Test MSE 7.253830961667496 Test RE 1.2873355890455467\n",
      "7 Train Loss 34.927223 Test MSE 6.716527610447623 Test RE 1.2387408582811712\n",
      "8 Train Loss 32.075912 Test MSE 6.7469864467370595 Test RE 1.2415464691343243\n",
      "9 Train Loss 28.383125 Test MSE 5.986258196025938 Test RE 1.1694611060377256\n",
      "10 Train Loss 26.16091 Test MSE 5.917378322337163 Test RE 1.162713536115463\n",
      "11 Train Loss 19.03772 Test MSE 2.842427500416539 Test RE 0.8058473060906598\n",
      "12 Train Loss 15.803417 Test MSE 2.8811403113775946 Test RE 0.8113164183341592\n",
      "13 Train Loss 10.43847 Test MSE 2.387546728922113 Test RE 0.7385569689270965\n",
      "14 Train Loss 8.478136 Test MSE 2.0850246759407645 Test RE 0.6901820495577023\n",
      "15 Train Loss 7.578031 Test MSE 2.03993945914407 Test RE 0.6826792452024119\n",
      "16 Train Loss 7.1238165 Test MSE 2.0989058984809286 Test RE 0.6924757100922037\n",
      "17 Train Loss 6.628682 Test MSE 2.003409863237859 Test RE 0.6765391977172314\n",
      "18 Train Loss 6.2020144 Test MSE 2.1154166355116484 Test RE 0.6951940043700223\n",
      "19 Train Loss 5.949936 Test MSE 2.068143216270984 Test RE 0.6873823320636271\n",
      "20 Train Loss 5.7577534 Test MSE 2.0643210339789437 Test RE 0.6867468548942459\n",
      "21 Train Loss 5.574629 Test MSE 2.1150193293062363 Test RE 0.6951287174999766\n",
      "22 Train Loss 5.3393373 Test MSE 2.15727741140302 Test RE 0.7020387080175988\n",
      "23 Train Loss 5.215253 Test MSE 2.1491967723044945 Test RE 0.7007226408944963\n",
      "24 Train Loss 5.147135 Test MSE 2.174087084409199 Test RE 0.7047685705589258\n",
      "25 Train Loss 5.0623856 Test MSE 2.1400305966591446 Test RE 0.6992267774428776\n",
      "26 Train Loss 4.963978 Test MSE 2.154469159628808 Test RE 0.7015816171819431\n",
      "27 Train Loss 4.901616 Test MSE 2.15628107515237 Test RE 0.7018765713839209\n",
      "28 Train Loss 4.8557587 Test MSE 2.1526063153377177 Test RE 0.7012782431617475\n",
      "29 Train Loss 4.8175864 Test MSE 2.1533411014836497 Test RE 0.7013979226331376\n",
      "30 Train Loss 4.7758436 Test MSE 2.1555729290444865 Test RE 0.7017613099780038\n",
      "31 Train Loss 4.7155094 Test MSE 2.1507824259251724 Test RE 0.7009810859430273\n",
      "32 Train Loss 4.6875443 Test MSE 2.1587666172746633 Test RE 0.702280980938682\n",
      "33 Train Loss 4.663693 Test MSE 2.1531597977563126 Test RE 0.7013683943957214\n",
      "34 Train Loss 4.641127 Test MSE 2.1540197921462747 Test RE 0.7015084473236903\n",
      "35 Train Loss 4.6064224 Test MSE 2.141417145394013 Test RE 0.6994532590213919\n",
      "36 Train Loss 4.5799623 Test MSE 2.1445596309173456 Test RE 0.6999662874961768\n",
      "37 Train Loss 4.560196 Test MSE 2.1438454387317867 Test RE 0.6998497246193303\n",
      "38 Train Loss 4.5457487 Test MSE 2.137920424323362 Test RE 0.6988819569273231\n",
      "39 Train Loss 4.5272365 Test MSE 2.1295982830160898 Test RE 0.6975203848457976\n",
      "40 Train Loss 4.5136814 Test MSE 2.129762026411366 Test RE 0.6975472002695369\n",
      "41 Train Loss 4.5016303 Test MSE 2.1296659993744935 Test RE 0.697531474534702\n",
      "42 Train Loss 4.487621 Test MSE 2.127856376661132 Test RE 0.6972350578528957\n",
      "43 Train Loss 4.474169 Test MSE 2.119383167240342 Test RE 0.6958454641384731\n",
      "44 Train Loss 4.4186296 Test MSE 2.107933180240522 Test RE 0.6939632627920349\n",
      "45 Train Loss 4.1933727 Test MSE 2.0813404026148756 Test RE 0.6895719983580426\n",
      "46 Train Loss 2.731698 Test MSE 1.6200380355698958 Test RE 0.6083740286402636\n",
      "47 Train Loss 1.6960913 Test MSE 1.3108230681371247 Test RE 0.5472428526048526\n",
      "48 Train Loss 1.1635281 Test MSE 1.0111252306141099 Test RE 0.4806296224834675\n",
      "49 Train Loss 0.93246365 Test MSE 0.6374713221112952 Test RE 0.38162637868032784\n",
      "50 Train Loss 0.5011874 Test MSE 0.09427348479351673 Test RE 0.146758356895774\n",
      "51 Train Loss 0.3083568 Test MSE 0.06037294319654695 Test RE 0.1174435673432915\n",
      "52 Train Loss 0.17081602 Test MSE 0.029944790014318438 Test RE 0.0827120331285859\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53 Train Loss 0.13496569 Test MSE 0.019311611488480554 Test RE 0.06642282128089112\n",
      "54 Train Loss 0.10168146 Test MSE 0.009401692636250722 Test RE 0.04634587487233857\n",
      "55 Train Loss 0.08552521 Test MSE 0.0077891456572979885 Test RE 0.04218450650645139\n",
      "56 Train Loss 0.07026392 Test MSE 0.0069579934874039635 Test RE 0.03987035193943151\n",
      "57 Train Loss 0.06155081 Test MSE 0.00634860148399827 Test RE 0.03808439785373353\n",
      "58 Train Loss 0.051941633 Test MSE 0.005479943019378116 Test RE 0.03538311644007625\n",
      "59 Train Loss 0.04329242 Test MSE 0.005687859333195007 Test RE 0.03604810869711674\n",
      "60 Train Loss 0.037262112 Test MSE 0.004586853009372961 Test RE 0.03237169737343535\n",
      "61 Train Loss 0.033794556 Test MSE 0.005300670003236712 Test RE 0.03479953526696867\n",
      "62 Train Loss 0.031844065 Test MSE 0.005615620141733189 Test RE 0.035818461007147064\n",
      "63 Train Loss 0.027390296 Test MSE 0.004116988385706068 Test RE 0.030668877525963027\n",
      "64 Train Loss 0.02415821 Test MSE 0.003001442874428272 Test RE 0.026186237428107766\n",
      "65 Train Loss 0.021544332 Test MSE 0.0022395677794781344 Test RE 0.02261987317676583\n",
      "66 Train Loss 0.018765146 Test MSE 0.0014722127242206098 Test RE 0.01833974715054773\n",
      "67 Train Loss 0.017331215 Test MSE 0.0015071240309288626 Test RE 0.018555922825730656\n",
      "68 Train Loss 0.016383082 Test MSE 0.0014211693118657016 Test RE 0.018019011844271477\n",
      "69 Train Loss 0.014617143 Test MSE 0.001175461613043302 Test RE 0.01638748419217571\n",
      "70 Train Loss 0.013857134 Test MSE 0.0012325665316939552 Test RE 0.016780822582510232\n",
      "71 Train Loss 0.012561465 Test MSE 0.0015283285809959348 Test RE 0.01868600357947298\n",
      "72 Train Loss 0.011473957 Test MSE 0.0017234605153934274 Test RE 0.019843063827811466\n",
      "73 Train Loss 0.010506057 Test MSE 0.0014385086914024598 Test RE 0.018128601621418067\n",
      "74 Train Loss 0.009586342 Test MSE 0.0015246524998462948 Test RE 0.018663517374014206\n",
      "75 Train Loss 0.008697029 Test MSE 0.0014248844518733633 Test RE 0.018042548610295108\n",
      "76 Train Loss 0.008291198 Test MSE 0.0012240545188385904 Test RE 0.016722778640362448\n",
      "77 Train Loss 0.007903212 Test MSE 0.001289278318526499 Test RE 0.017162533578038038\n",
      "78 Train Loss 0.0075308215 Test MSE 0.001263056692236055 Test RE 0.016987109348773774\n",
      "79 Train Loss 0.0072814906 Test MSE 0.0011975666350338811 Test RE 0.016540853080770804\n",
      "80 Train Loss 0.006872314 Test MSE 0.0013012798220397347 Test RE 0.017242228970256707\n",
      "81 Train Loss 0.0064108726 Test MSE 0.0012483970279296263 Test RE 0.01688824121243808\n",
      "82 Train Loss 0.006210001 Test MSE 0.0012660548778233563 Test RE 0.017007259005725455\n",
      "83 Train Loss 0.0060094697 Test MSE 0.001404522573435833 Test RE 0.01791316895176483\n",
      "84 Train Loss 0.005751154 Test MSE 0.0015168844288916113 Test RE 0.01861591155365322\n",
      "85 Train Loss 0.005464508 Test MSE 0.001375570567209137 Test RE 0.01772758178547653\n",
      "86 Train Loss 0.0050660307 Test MSE 0.0012906056546276089 Test RE 0.017171365879748546\n",
      "87 Train Loss 0.0047809887 Test MSE 0.000974196006593738 Test RE 0.014918708655139652\n",
      "88 Train Loss 0.00451694 Test MSE 0.0010330380607452103 Test RE 0.015362653017817265\n",
      "89 Train Loss 0.0042946953 Test MSE 0.0011171572788385774 Test RE 0.01597589582844929\n",
      "90 Train Loss 0.004115372 Test MSE 0.0011276427622317039 Test RE 0.016050694497954147\n",
      "91 Train Loss 0.0039350586 Test MSE 0.0011666372476677835 Test RE 0.016325856670434687\n",
      "92 Train Loss 0.0037929427 Test MSE 0.0012070566095912128 Test RE 0.016606261768418944\n",
      "93 Train Loss 0.0036529037 Test MSE 0.0011827134909633225 Test RE 0.016437956833792514\n",
      "94 Train Loss 0.0035168126 Test MSE 0.0011157163081136155 Test RE 0.015965589210574865\n",
      "95 Train Loss 0.0032188215 Test MSE 0.0011453267145080876 Test RE 0.01617606024151149\n",
      "96 Train Loss 0.0031463748 Test MSE 0.0010465088061082399 Test RE 0.01546249256690541\n",
      "97 Train Loss 0.0030116267 Test MSE 0.0010085311436365029 Test RE 0.015179334003202132\n",
      "98 Train Loss 0.002907808 Test MSE 0.0009363441861291569 Test RE 0.014626008430013228\n",
      "99 Train Loss 0.0028025133 Test MSE 0.0009299060951344212 Test RE 0.014575639134573127\n",
      "Training time: 140.07\n",
      "KG_atanh\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 54.48372 Test MSE 8.401700233242618 Test RE 1.3854525341214248\n",
      "1 Train Loss 45.67131 Test MSE 8.455849178356354 Test RE 1.3899099833215223\n",
      "2 Train Loss 42.58535 Test MSE 8.176790723530136 Test RE 1.3667827896085447\n",
      "3 Train Loss 41.53782 Test MSE 8.378034626498174 Test RE 1.383499911722688\n",
      "4 Train Loss 39.528404 Test MSE 8.161629802143842 Test RE 1.3655150978409236\n",
      "5 Train Loss 38.792637 Test MSE 7.795536564945382 Test RE 1.3345383756007867\n",
      "6 Train Loss 37.64734 Test MSE 7.06948708337382 Test RE 1.270872589827222\n",
      "7 Train Loss 30.588627 Test MSE 6.500624985165487 Test RE 1.2186685868772582\n",
      "8 Train Loss 25.698547 Test MSE 6.114430275846849 Test RE 1.1819144947680356\n",
      "9 Train Loss 24.217426 Test MSE 5.772706234550151 Test RE 1.1484121774039464\n",
      "10 Train Loss 23.3522 Test MSE 5.7308041440495066 Test RE 1.1442366217272422\n",
      "11 Train Loss 22.622442 Test MSE 5.374758789878187 Test RE 1.1081219295807545\n",
      "12 Train Loss 21.39861 Test MSE 5.641940697248476 Test RE 1.1353305357431638\n",
      "13 Train Loss 19.032673 Test MSE 5.470059246437183 Test RE 1.1179028809871947\n",
      "14 Train Loss 17.039734 Test MSE 5.634328877214452 Test RE 1.1345644120843967\n",
      "15 Train Loss 15.999183 Test MSE 5.761827029036145 Test RE 1.147329521923315\n",
      "16 Train Loss 15.347544 Test MSE 5.656260314005761 Test RE 1.1367703945131018\n",
      "17 Train Loss 14.520413 Test MSE 5.606374150957509 Test RE 1.1317463412519129\n",
      "18 Train Loss 13.704258 Test MSE 5.593930734197772 Test RE 1.1304896810712457\n",
      "19 Train Loss 13.043776 Test MSE 5.846919028329187 Test RE 1.155770485991539\n",
      "20 Train Loss 12.380928 Test MSE 5.918792901064318 Test RE 1.1628525040383937\n",
      "21 Train Loss 11.92978 Test MSE 5.961085534888688 Test RE 1.16699968020819\n",
      "22 Train Loss 11.605602 Test MSE 5.998258153798639 Test RE 1.1706326607590178\n",
      "23 Train Loss 11.318169 Test MSE 5.89189444982034 Test RE 1.1602071546976427\n",
      "24 Train Loss 11.12064 Test MSE 5.903387422697087 Test RE 1.1613381774300993\n",
      "25 Train Loss 10.87907 Test MSE 5.801158306584102 Test RE 1.1512388017320345\n",
      "26 Train Loss 10.612556 Test MSE 5.809020306933577 Test RE 1.1520186438654287\n",
      "27 Train Loss 10.261461 Test MSE 5.78330342492978 Test RE 1.1494657874345515\n",
      "28 Train Loss 9.1812 Test MSE 5.496371552828995 Test RE 1.1205883468826778\n",
      "29 Train Loss 8.123506 Test MSE 4.677280296959485 Test RE 1.0337243790814474\n",
      "30 Train Loss 7.5802207 Test MSE 4.189718213915239 Test RE 0.9783640083464117\n",
      "31 Train Loss 7.2020073 Test MSE 3.898315535867943 Test RE 0.9437273786420713\n",
      "32 Train Loss 6.9554586 Test MSE 3.80325128998688 Test RE 0.932149499880532\n",
      "33 Train Loss 6.757804 Test MSE 3.722483367761856 Test RE 0.9221985674963051\n",
      "34 Train Loss 6.612742 Test MSE 3.733080102962531 Test RE 0.923510238802738\n",
      "35 Train Loss 6.4779763 Test MSE 3.7600361097334 Test RE 0.9268385052564545\n",
      "36 Train Loss 6.344904 Test MSE 3.6839724545044548 Test RE 0.9174158676131345\n",
      "37 Train Loss 6.200682 Test MSE 3.643646776730568 Test RE 0.9123809202436375\n",
      "38 Train Loss 5.433455 Test MSE 3.0465734305877374 Test RE 0.834283943699898\n",
      "39 Train Loss 3.2695594 Test MSE 2.5341734635081172 Test RE 0.7608976271030425\n",
      "40 Train Loss 2.9056797 Test MSE 2.3257134004894704 Test RE 0.7289305595595926\n",
      "41 Train Loss 2.511309 Test MSE 2.4244190201630405 Test RE 0.7442381039569977\n",
      "42 Train Loss 2.0459795 Test MSE 2.4218308658795733 Test RE 0.7438407474423145\n",
      "43 Train Loss 1.8619214 Test MSE 2.3313528422124197 Test RE 0.7298137880046963\n",
      "44 Train Loss 1.7534134 Test MSE 2.315598052307406 Test RE 0.7273436440659571\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "45 Train Loss 1.6269069 Test MSE 2.1141514980578524 Test RE 0.6949860908325453\n",
      "46 Train Loss 1.4486172 Test MSE 1.850913489866039 Test RE 0.650281083397356\n",
      "47 Train Loss 1.2727199 Test MSE 1.6474285517890181 Test RE 0.6134954618940927\n",
      "48 Train Loss 0.89343077 Test MSE 0.8139749061732765 Test RE 0.43123456958042\n",
      "49 Train Loss 0.58941126 Test MSE 0.4367101363691966 Test RE 0.315867320604644\n",
      "50 Train Loss 0.33998436 Test MSE 0.05570233927393808 Test RE 0.1128092670120036\n",
      "51 Train Loss 0.22706114 Test MSE 0.04443543352065347 Test RE 0.10075642997403268\n",
      "52 Train Loss 0.1487733 Test MSE 0.021253168061630447 Test RE 0.06968188628881918\n",
      "53 Train Loss 0.10465523 Test MSE 0.020359634780018038 Test RE 0.06820136265042524\n",
      "54 Train Loss 0.07072458 Test MSE 0.009397938684749838 Test RE 0.04633662135046549\n",
      "55 Train Loss 0.048520286 Test MSE 0.006071872172734701 Test RE 0.037245119351949775\n",
      "56 Train Loss 0.040520042 Test MSE 0.0051155027257525 Test RE 0.03418630952870471\n",
      "57 Train Loss 0.032948345 Test MSE 0.003682953729367705 Test RE 0.0290072255311675\n",
      "58 Train Loss 0.0271318 Test MSE 0.002639695456251985 Test RE 0.024557546367111608\n",
      "59 Train Loss 0.021801203 Test MSE 0.0021835460585975567 Test RE 0.022335168790228184\n",
      "60 Train Loss 0.019503795 Test MSE 0.0019577544178250215 Test RE 0.021148869949300913\n",
      "61 Train Loss 0.016300445 Test MSE 0.0020576680295781263 Test RE 0.02168181903126212\n",
      "62 Train Loss 0.014814543 Test MSE 0.0016224786148594523 Test RE 0.019252961878762378\n",
      "63 Train Loss 0.0131073 Test MSE 0.001119587121762094 Test RE 0.015993260363002176\n",
      "64 Train Loss 0.011650078 Test MSE 0.0011533688145224917 Test RE 0.0162327523319406\n",
      "65 Train Loss 0.0101575125 Test MSE 0.001042318700794935 Test RE 0.015431506466020735\n",
      "66 Train Loss 0.008980433 Test MSE 0.0010803315321696464 Test RE 0.01571037625931644\n",
      "67 Train Loss 0.008348041 Test MSE 0.0008723648910200255 Test RE 0.014117478976156967\n",
      "68 Train Loss 0.0071427864 Test MSE 0.0008179097081366358 Test RE 0.013669755344182277\n",
      "69 Train Loss 0.0067117168 Test MSE 0.0008070785148278449 Test RE 0.013578942626565033\n",
      "70 Train Loss 0.006002214 Test MSE 0.0007249326279110557 Test RE 0.01286935804518268\n",
      "71 Train Loss 0.0054633804 Test MSE 0.0007178274040426879 Test RE 0.012806135046055604\n",
      "72 Train Loss 0.0048831366 Test MSE 0.0006171370296446118 Test RE 0.011874049722882545\n",
      "73 Train Loss 0.004252067 Test MSE 0.0006134575783603788 Test RE 0.011838599489299637\n",
      "74 Train Loss 0.0038264182 Test MSE 0.0007919426430432 Test RE 0.013451010899828593\n",
      "75 Train Loss 0.0034264163 Test MSE 0.0006421640070797466 Test RE 0.012112423308158395\n",
      "76 Train Loss 0.0031463215 Test MSE 0.0005548898141820326 Test RE 0.011259301186337075\n",
      "77 Train Loss 0.0029349094 Test MSE 0.0006094391483955406 Test RE 0.011799761641006356\n",
      "78 Train Loss 0.0027576727 Test MSE 0.0005782594988350598 Test RE 0.01149395382296687\n",
      "79 Train Loss 0.00243612 Test MSE 0.00046202588293529657 Test RE 0.010274039074274096\n",
      "80 Train Loss 0.0023392218 Test MSE 0.0004702584683229892 Test RE 0.010365168648760026\n",
      "81 Train Loss 0.0022840817 Test MSE 0.0004963572867725558 Test RE 0.010648912566254339\n",
      "82 Train Loss 0.002172944 Test MSE 0.00048528647247961617 Test RE 0.010529485551044694\n",
      "83 Train Loss 0.002060635 Test MSE 0.0005268814211130599 Test RE 0.010971461984873723\n",
      "84 Train Loss 0.001975657 Test MSE 0.0004859954306617447 Test RE 0.010537174041188008\n",
      "85 Train Loss 0.0018762175 Test MSE 0.0004526274039676173 Test RE 0.010169005506282604\n",
      "86 Train Loss 0.0018270089 Test MSE 0.0004314951394315162 Test RE 0.009928782891562741\n",
      "87 Train Loss 0.0017945195 Test MSE 0.0004173098884622171 Test RE 0.009764216450157641\n",
      "88 Train Loss 0.0017526168 Test MSE 0.0004159844721096406 Test RE 0.009748698072992949\n",
      "89 Train Loss 0.001671437 Test MSE 0.0003777090593925817 Test RE 0.009289380687905387\n",
      "90 Train Loss 0.0016090579 Test MSE 0.00035929101111747253 Test RE 0.009060063389712728\n",
      "91 Train Loss 0.0015257382 Test MSE 0.00035547223137174106 Test RE 0.009011786629514549\n",
      "92 Train Loss 0.0014930647 Test MSE 0.00036933990914550155 Test RE 0.009185888705629636\n",
      "93 Train Loss 0.0014469074 Test MSE 0.00033517448368591845 Test RE 0.008750714902664108\n",
      "94 Train Loss 0.0013881088 Test MSE 0.00032486419703069966 Test RE 0.008615073473711677\n",
      "95 Train Loss 0.00133232 Test MSE 0.0003106531736159846 Test RE 0.00842453536295437\n",
      "96 Train Loss 0.0012932624 Test MSE 0.00029078604843669104 Test RE 0.008150698773647795\n",
      "97 Train Loss 0.0012677591 Test MSE 0.00028062255243727535 Test RE 0.008006991095350427\n",
      "98 Train Loss 0.0012270889 Test MSE 0.0002690805337314444 Test RE 0.007840598235618076\n",
      "99 Train Loss 0.0011249002 Test MSE 0.00022434421819847225 Test RE 0.007159216581810301\n",
      "Training time: 142.86\n",
      "KG_atanh\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 55.212776 Test MSE 8.970420120229615 Test RE 1.4315761416060302\n",
      "1 Train Loss 45.080498 Test MSE 8.635279540298061 Test RE 1.4045792906296564\n",
      "2 Train Loss 43.88714 Test MSE 8.62595726708734 Test RE 1.4038209244624071\n",
      "3 Train Loss 43.57166 Test MSE 8.457090803391571 Test RE 1.3900120241485108\n",
      "4 Train Loss 43.163174 Test MSE 8.491968952368945 Test RE 1.3928753707087151\n",
      "5 Train Loss 42.978767 Test MSE 8.293428543690633 Test RE 1.3764965075600684\n",
      "6 Train Loss 42.29155 Test MSE 8.117823520732557 Test RE 1.3618455721225973\n",
      "7 Train Loss 42.007114 Test MSE 8.063148130061006 Test RE 1.3572516537021326\n",
      "8 Train Loss 41.425804 Test MSE 8.389893398201657 Test RE 1.3844787097771847\n",
      "9 Train Loss 39.458046 Test MSE 7.618595721368498 Test RE 1.319305960411637\n",
      "10 Train Loss 38.677994 Test MSE 7.866899955305364 Test RE 1.3406329025537527\n",
      "11 Train Loss 38.499485 Test MSE 7.771518184323785 Test RE 1.332480904763962\n",
      "12 Train Loss 37.35615 Test MSE 7.4159876975048995 Test RE 1.301645018460468\n",
      "13 Train Loss 34.22886 Test MSE 7.17151931667012 Test RE 1.2800108370061007\n",
      "14 Train Loss 33.558064 Test MSE 7.236714743296942 Test RE 1.2858158865162082\n",
      "15 Train Loss 33.295853 Test MSE 7.367600338982162 Test RE 1.2973916242793637\n",
      "16 Train Loss 33.081676 Test MSE 7.212317366140613 Test RE 1.2836465996888136\n",
      "17 Train Loss 32.83718 Test MSE 7.2716117159394456 Test RE 1.2889123963948825\n",
      "18 Train Loss 32.518677 Test MSE 7.260303183228486 Test RE 1.287909772776309\n",
      "19 Train Loss 32.28978 Test MSE 7.25982953816801 Test RE 1.2878677619927315\n",
      "20 Train Loss 32.083633 Test MSE 7.165939785335158 Test RE 1.279512807933494\n",
      "21 Train Loss 31.841888 Test MSE 7.166461522315234 Test RE 1.279559386401758\n",
      "22 Train Loss 31.344048 Test MSE 7.285938949157803 Test RE 1.290181541532641\n",
      "23 Train Loss 30.759342 Test MSE 7.2264816008018045 Test RE 1.2849064550633258\n",
      "24 Train Loss 30.380192 Test MSE 7.259522817577237 Test RE 1.287840556138168\n",
      "25 Train Loss 29.916462 Test MSE 7.12731178114965 Test RE 1.2760595405239372\n",
      "26 Train Loss 29.329258 Test MSE 7.050127935472145 Test RE 1.2691313123824255\n",
      "27 Train Loss 29.011253 Test MSE 6.933585393652646 Test RE 1.2585978763300445\n",
      "28 Train Loss 28.620602 Test MSE 6.897948651586624 Test RE 1.2553592843096024\n",
      "29 Train Loss 27.812605 Test MSE 6.629256227269194 Test RE 1.2306667382768846\n",
      "30 Train Loss 25.30045 Test MSE 5.512648929000738 Test RE 1.1222464183331338\n",
      "31 Train Loss 20.829052 Test MSE 4.9909629452216855 Test RE 1.0678253655190668\n",
      "32 Train Loss 16.666271 Test MSE 4.1328662232552595 Test RE 0.9717034265407042\n",
      "33 Train Loss 13.599243 Test MSE 3.95851624904703 Test RE 0.9509863345496897\n",
      "34 Train Loss 9.749088 Test MSE 1.9975453304565969 Test RE 0.6755482636632835\n",
      "35 Train Loss 6.837244 Test MSE 1.908888558605006 Test RE 0.6603867450465687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "36 Train Loss 5.6320004 Test MSE 1.4975625232579757 Test RE 0.5849254823940512\n",
      "37 Train Loss 3.9521878 Test MSE 0.5883671281330741 Test RE 0.36663359788869276\n",
      "38 Train Loss 3.0538273 Test MSE 0.37648575239819154 Test RE 0.29327992263561026\n",
      "39 Train Loss 2.368588 Test MSE 0.2619781665512063 Test RE 0.244647402730933\n",
      "40 Train Loss 1.8839486 Test MSE 0.16670128008540863 Test RE 0.19515403162256031\n",
      "41 Train Loss 1.5317867 Test MSE 0.13904725432626255 Test RE 0.17823347273685547\n",
      "42 Train Loss 1.2079184 Test MSE 0.1034473414379887 Test RE 0.15373322124006447\n",
      "43 Train Loss 1.0010798 Test MSE 0.08244202648652846 Test RE 0.13724053094171348\n",
      "44 Train Loss 0.8127874 Test MSE 0.07363144183313057 Test RE 0.1296999213370123\n",
      "45 Train Loss 0.6765146 Test MSE 0.04790460200119007 Test RE 0.10461565498166948\n",
      "46 Train Loss 0.5304737 Test MSE 0.049043636572784995 Test RE 0.1058520791931957\n",
      "47 Train Loss 0.4485244 Test MSE 0.0430916046084517 Test RE 0.0992211813222075\n",
      "48 Train Loss 0.33732837 Test MSE 0.035356219785435156 Test RE 0.08987542824487542\n",
      "49 Train Loss 0.31129697 Test MSE 0.02938920414825246 Test RE 0.08194113459592976\n",
      "50 Train Loss 0.2755018 Test MSE 0.031021145458744296 Test RE 0.08418543791551135\n",
      "51 Train Loss 0.23898743 Test MSE 0.02836824893479408 Test RE 0.08050527259888604\n",
      "52 Train Loss 0.21402927 Test MSE 0.025116482492970296 Test RE 0.07575084276829672\n",
      "53 Train Loss 0.19512303 Test MSE 0.023248576361352743 Test RE 0.07287964393833196\n",
      "54 Train Loss 0.18216111 Test MSE 0.021973543355459903 Test RE 0.07085297785173747\n",
      "55 Train Loss 0.16637787 Test MSE 0.018051966831831308 Test RE 0.06422000332616362\n",
      "56 Train Loss 0.1613827 Test MSE 0.01695238317088731 Test RE 0.062233387369026054\n",
      "57 Train Loss 0.15372485 Test MSE 0.014453402286315815 Test RE 0.05746363642508242\n",
      "58 Train Loss 0.1469148 Test MSE 0.013618118050874326 Test RE 0.05577847130525445\n",
      "59 Train Loss 0.14262894 Test MSE 0.0117144463871933 Test RE 0.051733152583773104\n",
      "60 Train Loss 0.13000612 Test MSE 0.011049119808245777 Test RE 0.050242576409008574\n",
      "61 Train Loss 0.12193376 Test MSE 0.010696020854283261 Test RE 0.049433251889210085\n",
      "62 Train Loss 0.11780375 Test MSE 0.010275172609611753 Test RE 0.04845098642468933\n",
      "63 Train Loss 0.11302923 Test MSE 0.009716977241342153 Test RE 0.047116568477456156\n",
      "64 Train Loss 0.10584217 Test MSE 0.008466168008261641 Test RE 0.043979622939547075\n",
      "65 Train Loss 0.099982336 Test MSE 0.009171912339202217 Test RE 0.04577601765153414\n",
      "66 Train Loss 0.096000016 Test MSE 0.00852888099302384 Test RE 0.0441422115285716\n",
      "67 Train Loss 0.086826354 Test MSE 0.008818978014601537 Test RE 0.044886649595394436\n",
      "68 Train Loss 0.08262614 Test MSE 0.007437778162597277 Test RE 0.041222058074780465\n",
      "69 Train Loss 0.07932313 Test MSE 0.006710392621844386 Test RE 0.03915453096746821\n",
      "70 Train Loss 0.07514019 Test MSE 0.00727810006286376 Test RE 0.040777169024110056\n",
      "71 Train Loss 0.0714183 Test MSE 0.007655293285508529 Test RE 0.04182047653372932\n",
      "72 Train Loss 0.068493634 Test MSE 0.007597609405808519 Test RE 0.04166261655079307\n",
      "73 Train Loss 0.065985784 Test MSE 0.007846996367368912 Test RE 0.04234087083931058\n",
      "74 Train Loss 0.061620906 Test MSE 0.007323094322426975 Test RE 0.04090301997344706\n",
      "75 Train Loss 0.060086764 Test MSE 0.006629743263359691 Test RE 0.03891852888394219\n",
      "76 Train Loss 0.058504954 Test MSE 0.0058060544925867315 Test RE 0.03642072729350409\n",
      "77 Train Loss 0.056214932 Test MSE 0.005731445082463077 Test RE 0.036185962434309236\n",
      "78 Train Loss 0.054031253 Test MSE 0.006017018890143082 Test RE 0.03707650149304204\n",
      "79 Train Loss 0.050543606 Test MSE 0.005230014146858763 Test RE 0.03456682508946837\n",
      "80 Train Loss 0.04814657 Test MSE 0.004747364382467085 Test RE 0.03293323126230255\n",
      "81 Train Loss 0.045219682 Test MSE 0.003768833793782365 Test RE 0.029343475616427923\n",
      "82 Train Loss 0.04169613 Test MSE 0.003322615248677941 Test RE 0.02755168009785197\n",
      "83 Train Loss 0.038569618 Test MSE 0.003276251825969494 Test RE 0.027358778181613252\n",
      "84 Train Loss 0.03214883 Test MSE 0.003224782383007024 Test RE 0.027143026242536596\n",
      "85 Train Loss 0.030250682 Test MSE 0.0034199982953268216 Test RE 0.027952522457990285\n",
      "86 Train Loss 0.029292865 Test MSE 0.0038658419887448296 Test RE 0.029718720648991344\n",
      "87 Train Loss 0.028369522 Test MSE 0.0038933576497916554 Test RE 0.02982429665612189\n",
      "88 Train Loss 0.027414544 Test MSE 0.004058158059569827 Test RE 0.030448965307751687\n",
      "89 Train Loss 0.026197623 Test MSE 0.003950861663778042 Test RE 0.030043738415823286\n",
      "90 Train Loss 0.025660824 Test MSE 0.004002735691747162 Test RE 0.03024032937556521\n",
      "91 Train Loss 0.024208657 Test MSE 0.003611709630389843 Test RE 0.028725293440449412\n",
      "92 Train Loss 0.023281297 Test MSE 0.003298154523507056 Test RE 0.027450076546413155\n",
      "93 Train Loss 0.022710118 Test MSE 0.00339453247975949 Test RE 0.027848258686264536\n",
      "94 Train Loss 0.021966489 Test MSE 0.0033502296187241528 Test RE 0.02766593467788161\n",
      "95 Train Loss 0.021151664 Test MSE 0.003058760982380659 Test RE 0.026435092300445173\n",
      "96 Train Loss 0.020610087 Test MSE 0.0031899430577233974 Test RE 0.026996006599793287\n",
      "97 Train Loss 0.020030178 Test MSE 0.003263222772532168 Test RE 0.02730432356375757\n",
      "98 Train Loss 0.018799042 Test MSE 0.0028856417298066183 Test RE 0.025676112221242775\n",
      "99 Train Loss 0.018201914 Test MSE 0.0026703415770407235 Test RE 0.02469968809965618\n",
      "Training time: 104.32\n"
     ]
    }
   ],
   "source": [
    "#for tune_reps in range(25):\n",
    "  \n",
    "max_reps = 10 #10\n",
    "max_iter = 100 #100\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "alpha_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "N_I = 200  #Total number of data points for 'y'\n",
    "N_B = 400\n",
    "N_f = 10000 #Total number of collocation points\n",
    "n_val = 1.0  \n",
    "\n",
    "for reps in range(max_reps):\n",
    "  print(label)\n",
    "  print(reps)\n",
    "\n",
    "  train_loss = []\n",
    "  test_mse_loss = []\n",
    "  test_re_loss = []\n",
    "  alpha_val = []\n",
    "\n",
    "  torch.manual_seed(reps*36)\n",
    "\n",
    "  layers = np.array([2,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "\n",
    "  PINN = Sequentialmodel(layers,n_val)\n",
    "\n",
    "  PINN.to(device)\n",
    "\n",
    "  'Neural Network Summary'\n",
    "  print(PINN)\n",
    "\n",
    "  params = list(PINN.parameters())\n",
    "\n",
    "\n",
    "  optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.25, \n",
    "                            max_iter = 20, \n",
    "                            max_eval = 30, \n",
    "                            tolerance_grad = 1e-8, \n",
    "                            tolerance_change = 1e-8, \n",
    "                            history_size = 100, \n",
    "                            line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  nan_flag = train_model(max_iter,reps)\n",
    "\n",
    "  torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "  train_loss_full.append(train_loss)\n",
    "  test_mse_full.append(test_mse_loss)\n",
    "  test_re_full.append(test_re_loss)\n",
    "  #elapsed_time[reps] = time.time() - start_time\n",
    "  alpha_full.append(alpha_val)  \n",
    "\n",
    "\n",
    "  #print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"alpha\": alpha_full, \"label\": label}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'KG_atanh'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1660688516819,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "_O3sPdAnSq_2"
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "aborted",
     "timestamp": 1660688516819,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "jQ4afiEWSq_2"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 's' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_22041/775165135.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtune_reps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m25\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0;32mif\u001b[0m \u001b[0mtune_reps\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0ms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m         \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"KG_stan_tune\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtune_reps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".mat\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test_re_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 's' is not defined"
     ]
    }
   ],
   "source": [
    "for tune_reps in range(25):\n",
    "    if tune_reps not in s:\n",
    "        label = \"KG_stan_tune\"+str(tune_reps)+\".mat\"\n",
    "        data = sio.loadmat(label)\n",
    "        re = np.array(data[\"test_re_loss\"])\n",
    "        print(tune_reps,\" \",np.mean(re[:,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1660688534316,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "06syezgfv_qO",
    "outputId": "9f4852d5-694a-4977-8893-a6183a2ce493"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.019008550524342072\n",
      "0.07143034103102627\n",
      "0.012902022197801778\n",
      "0.8798929221233429\n",
      "0.030078128935536574\n",
      "1.3600168406256998\n",
      "0.01890595458791737\n",
      "0.014575639134573127\n",
      "0.007159216581810301\n",
      "0.02469968809965618\n",
      "a =  0.24386693038417065\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a+ test_re_full[i][-1]\n",
    "    print(test_re_full[i][-1])\n",
    "    \n",
    "print(\"a = \",a/10)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "stan_2D_KG_16Aug2022_tune.ipynb",
   "version": ""
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
