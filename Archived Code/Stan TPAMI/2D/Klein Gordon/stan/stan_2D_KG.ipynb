{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 286,
     "status": "ok",
     "timestamp": 1660687093981,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "iAtv2UvNSq_u",
    "outputId": "68a82578-1b95-4343-a8ec-7635a4df93ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1256,
     "status": "ok",
     "timestamp": 1660687393066,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "xAgfGYA4acPE",
    "outputId": "527d048f-6a89-4e80-87ff-bfdb1c9d6222"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1856,
     "status": "ok",
     "timestamp": 1660687061284,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "7kSdyTofacUc",
    "outputId": "08ee5c9b-0706-46a5-86a1-2c7e56a6a74d"
   },
   "outputs": [],
   "source": [
    "# %cd '/content/gdrive/MyDrive/Virginia Tech /Fall 2022/Codes from GPU/PINN_Stan/2D Klein Gordon/stan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 32419,
     "status": "ok",
     "timestamp": 1660687093700,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "RHuSaD0gagsN",
    "outputId": "c232cd79-e56c-4a76-97c7-d59dafa084ef"
   },
   "outputs": [],
   "source": [
    "# !pip install smt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 280,
     "status": "ok",
     "timestamp": 1660687410736,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "mTLFQRt5Sq_y"
   },
   "outputs": [],
   "source": [
    "def true_2D_1(xt): #True function for 2D_1 Klein Gordon Equation x \\in [-50,50] , t \\in [0,10]\n",
    "    y = xt[:,0]*np.cos(xt[:,1])\n",
    "    return y.reshape(-1,1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 4312,
     "status": "ok",
     "timestamp": 1660687098957,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "81bNHCY3Sq_y"
   },
   "outputs": [],
   "source": [
    "loss_thresh = 0.1\n",
    "label = \"KG_stan\"\n",
    "\n",
    "x = np.linspace(-5,5,500).reshape(-1,1)\n",
    "t = np.linspace(0,10,1000).reshape(-1,1)\n",
    "\n",
    "X,T = np.meshgrid(x,t)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "T = T.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xt = np.hstack((X,T))\n",
    "\n",
    "y_true = true_2D_1(xt)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "#bound_pts_idx = ((X == -5) + (X == 5) + (T == 0)).reshape(-1,)\n",
    "\n",
    "#xt_bound = xt[bound_pts_idx,:]\n",
    "#y_bound = y_true[bound_pts_idx,:]\n",
    "\n",
    "\n",
    "xt_test_tensor = torch.from_numpy(xt).float().to(device)\n",
    "\n",
    "\n",
    "lb_xt = xt[0]\n",
    "ub_xt = xt[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1660687098958,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "YQgCA-PuSq_z"
   },
   "outputs": [],
   "source": [
    "def trainingdata(N_I,N_B,N_f,seed):\n",
    "    '''Boundary Conditions''' \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    x_BC1 = np.random.uniform(size = N_I).reshape(-1,1)\n",
    "    t_BC1 = np.zeros((N_I,1))\n",
    "    samples = np.hstack((x_BC1,t_BC1))\n",
    "    xt_BC1 = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    y_BC1 = true_2D_1(xt_BC1)\n",
    "    \n",
    "    x_BC2 = np.zeros((int(N_B/2),1))\n",
    "    t_BC2 = np.random.uniform(size = int(N_B/2)).reshape(-1,1)\n",
    "    samples = np.hstack((x_BC2,t_BC2))\n",
    "    xt_BC2 = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    y_BC2 = true_2D_1(xt_BC2)\n",
    "    \n",
    "    x_BC3 = np.ones((int(N_B/2),1))\n",
    "    t_BC3 = np.random.uniform(size = int(N_B/2)).reshape(-1,1)\n",
    "    samples = np.hstack((x_BC3,t_BC3))\n",
    "    xt_BC3 = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    y_BC3 = true_2D_1(xt_BC3)\n",
    "\n",
    "    xt_BC = np.vstack((xt_BC1,xt_BC2,xt_BC3))\n",
    "    y_BC = np.vstack((y_BC1,y_BC2,y_BC3))\n",
    "\n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    samples = sampling(N_f)\n",
    "    xt_coll = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    \n",
    "    xt_coll = np.vstack((xt_coll, xt_BC)) # append training points to collocation points \n",
    "\n",
    "    return xt_coll, xt_BC, y_BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1660687098958,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "gTJxct8bSq_0"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,beta_init):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "            \n",
    "        \n",
    "        self.beta = Parameter(beta_init*torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "\n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xt):\n",
    "        if torch.is_tensor(xt) != True:         \n",
    "            xt = torch.from_numpy(xt)                \n",
    "        \n",
    "        ubxt = torch.from_numpy(ub_xt).float().to(device)\n",
    "        lbxt = torch.from_numpy(lb_xt).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xt = (xt - lbxt)/(ubxt - lbxt)\n",
    "        \n",
    "        #convert to float\n",
    "        a = xt.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            z1 = self.activation(z)\n",
    "            a = z1 + self.beta[:,i]*z*z1            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC(self,xt,y):\n",
    "                \n",
    "        loss_bc = self.loss_function(self.forward(xt), y)\n",
    "                \n",
    "        return loss_bc\n",
    "    \n",
    "    def loss_PDE(self, xt_coll, f_hat):\n",
    "        \n",
    "        g = xt_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        y = self.forward(g) \n",
    "        \n",
    "        \n",
    "        y_x_t = autograd.grad(y,g,torch.ones([xt_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        y_xx_tt = autograd.grad(y_x_t,g,torch.ones(xt_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        #du_dx = u_x_t[:,[0]]\n",
    "        \n",
    "        d2y_dx2 = y_xx_tt[:,[0]]\n",
    "        d2y_dt2 = y_xx_tt[:,[1]]    \n",
    "        \n",
    "\n",
    "        f = d2y_dt2 - d2y_dx2 + torch.pow(y,2) + (g[:,0]*torch.cos(g[:,1])).reshape(-1,1) - (torch.pow(g[:,0],2)*torch.pow(torch.cos(g[:,1]),2)).reshape(-1,1)\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xt_BC,y_BC,xt_coll,f_hat):\n",
    "\n",
    "        loss_BC = self.loss_BC(xt_BC,y_BC)\n",
    "        loss_f = self.loss_PDE(xt_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_BC + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "         \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        y_pred = self.forward(xt_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return y_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1660687098958,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "VoQzfzYsYKVs"
   },
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1660687098959,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "_IUDZDkxXmyF"
   },
   "outputs": [],
   "source": [
    "def train_step(xt_BC, y_BC, xt_coll, f_hat,seed):\n",
    "    # x_coll_np_array = colloc_pts(N_f,seed*123)\n",
    "    # x_coll_train = torch.from_numpy(x_coll_np_array).float().to(device)        \n",
    "    \n",
    "    # f_hat = torch.zeros(x_coll_train.shape[0],1).to(device)\n",
    "#     xt_coll, xt_BC, y_BC = trainingdata(N_I,N_B,N_f,seed*123)\n",
    "#     xt_coll = torch.from_numpy(xt_coll).float().to(device)\n",
    "#     xt_BC = torch.from_numpy(xt_BC).float().to(device)\n",
    "#     y_BC = torch.from_numpy(y_BC).float().to(device)\n",
    "\n",
    "#     f_hat = torch.zeros(xt_coll.shape[0],1).to(device)\n",
    "    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(xt_BC, y_BC, xt_coll,f_hat)\n",
    "        loss.backward()\n",
    "        #print(loss.cpu().detach().numpy())\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 467,
     "status": "ok",
     "timestamp": 1660690085956,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "Vt9Dlr8MYIwW"
   },
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*9)\n",
    "    start_time = time.time() \n",
    "    thresh_flag = 0\n",
    "\n",
    "    xt_coll, xt_BC, y_BC = trainingdata(N_I,N_B,N_f,rep*11)\n",
    "    xt_coll = torch.from_numpy(xt_coll).float().to(device)\n",
    "    xt_BC = torch.from_numpy(xt_BC).float().to(device)\n",
    "    y_BC = torch.from_numpy(y_BC).float().to(device)\n",
    "\n",
    "    f_hat = torch.zeros(xt_coll.shape[0],1).to(device)\n",
    "    \n",
    "\n",
    "    for i in range(max_iter):\n",
    "        train_step(xt_BC, y_BC, xt_coll,f_hat,i)\n",
    "        loss_np = PINN.loss(xt_BC, y_BC, xt_coll,f_hat).cpu().detach().numpy()\n",
    "        \n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1          \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        \n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])   \n",
    "        \n",
    "      \n",
    "         \n",
    "\n",
    "    elapsed_time[rep] = time.time() - start_time  \n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "sP4Re5lSSq_1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KG_stan\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 58.162746 Test MSE 8.522083219762841 Test RE 1.3953428962386416\n",
      "1 Train Loss 40.840633 Test MSE 7.151209556321126 Test RE 1.2781970550170474\n",
      "2 Train Loss 34.543537 Test MSE 7.194036724115823 Test RE 1.2820187752975112\n",
      "3 Train Loss 30.870075 Test MSE 6.403641979394024 Test RE 1.2095437502140722\n",
      "4 Train Loss 28.162403 Test MSE 6.327541331176232 Test RE 1.2023351826634203\n",
      "5 Train Loss 25.967978 Test MSE 6.240676782348499 Test RE 1.194053828286196\n",
      "6 Train Loss 25.076096 Test MSE 6.357074348091949 Test RE 1.20513779167529\n",
      "7 Train Loss 23.523716 Test MSE 6.09419093931081 Test RE 1.1799567495415413\n",
      "8 Train Loss 22.703793 Test MSE 5.90391119129851 Test RE 1.1613896952230223\n",
      "9 Train Loss 20.803112 Test MSE 5.923517636889914 Test RE 1.1633165407913835\n",
      "10 Train Loss 18.567297 Test MSE 6.2506012723354845 Test RE 1.1950028974363165\n",
      "11 Train Loss 13.205114 Test MSE 4.167472237867339 Test RE 0.9757631615980589\n",
      "12 Train Loss 7.0435257 Test MSE 3.4931205310836004 Test RE 0.8933360210725659\n",
      "13 Train Loss 6.1503277 Test MSE 3.0806070500036866 Test RE 0.8389309422704644\n",
      "14 Train Loss 5.4825673 Test MSE 2.7284619298222235 Test RE 0.7895270418902778\n",
      "15 Train Loss 5.0472145 Test MSE 2.4683974621152474 Test RE 0.7509579268101241\n",
      "16 Train Loss 4.626863 Test MSE 2.2427607507717844 Test RE 0.7158129216111997\n",
      "17 Train Loss 4.425809 Test MSE 2.1907815985945773 Test RE 0.7074693060449976\n",
      "18 Train Loss 4.293662 Test MSE 2.1615940623064547 Test RE 0.7027407367779875\n",
      "19 Train Loss 4.2077756 Test MSE 2.1564753699758503 Test RE 0.7019081924731834\n",
      "20 Train Loss 4.087126 Test MSE 2.179939473981992 Test RE 0.7057165105633185\n",
      "21 Train Loss 3.9931743 Test MSE 2.1606062754764146 Test RE 0.7025801521942396\n",
      "22 Train Loss 3.923822 Test MSE 2.132733244866778 Test RE 0.6980336026791041\n",
      "23 Train Loss 3.8630657 Test MSE 2.139808629590127 Test RE 0.6991905140961413\n",
      "24 Train Loss 3.7226076 Test MSE 2.104454136903489 Test RE 0.6933903496431297\n",
      "25 Train Loss 3.5363717 Test MSE 2.120129424436062 Test RE 0.6959679606246746\n",
      "26 Train Loss 2.5166018 Test MSE 1.8978203195526653 Test RE 0.6584694134936386\n",
      "27 Train Loss 1.8788617 Test MSE 1.5517409720240678 Test RE 0.5954121244012466\n",
      "28 Train Loss 1.6010239 Test MSE 1.5764914643332464 Test RE 0.600141793913352\n",
      "29 Train Loss 1.3649509 Test MSE 1.4997544926840767 Test RE 0.5853534010729742\n",
      "30 Train Loss 1.1946566 Test MSE 1.4172300570707965 Test RE 0.5690209252457927\n",
      "31 Train Loss 1.0864775 Test MSE 1.2587202163706315 Test RE 0.536256616912183\n",
      "32 Train Loss 0.88830835 Test MSE 0.4336467552943868 Test RE 0.3147575171465349\n",
      "33 Train Loss 0.72919226 Test MSE 0.2102816122906822 Test RE 0.21918392987971966\n",
      "34 Train Loss 0.5911709 Test MSE 0.1059521801148696 Test RE 0.15558331078440232\n",
      "35 Train Loss 0.4607309 Test MSE 0.0922671581847052 Test RE 0.14518830415663375\n",
      "36 Train Loss 0.3439814 Test MSE 0.0713484875809043 Test RE 0.12767340648300027\n",
      "37 Train Loss 0.29198098 Test MSE 0.05841375848925228 Test RE 0.11552224912015077\n",
      "38 Train Loss 0.23607771 Test MSE 0.06004874252769282 Test RE 0.11712780887169454\n",
      "39 Train Loss 0.17992093 Test MSE 0.031435579586211376 Test RE 0.0847459195156717\n",
      "40 Train Loss 0.15837587 Test MSE 0.03296078955741554 Test RE 0.08677744590028905\n",
      "41 Train Loss 0.13119875 Test MSE 0.02764800032714201 Test RE 0.07947671775841378\n",
      "42 Train Loss 0.11559284 Test MSE 0.024664489598121127 Test RE 0.07506614729063289\n",
      "43 Train Loss 0.09681769 Test MSE 0.02049690964581144 Test RE 0.0684309002772038\n",
      "44 Train Loss 0.08869124 Test MSE 0.01812386659333736 Test RE 0.06434776819518546\n",
      "45 Train Loss 0.081128 Test MSE 0.01733476421449372 Test RE 0.06293134730100944\n",
      "46 Train Loss 0.0719096 Test MSE 0.014981263703227825 Test RE 0.05850355871386144\n",
      "47 Train Loss 0.065246776 Test MSE 0.013989933890402134 Test RE 0.05653480407028635\n",
      "48 Train Loss 0.06012153 Test MSE 0.01433411709379237 Test RE 0.057226018932522396\n",
      "49 Train Loss 0.056621294 Test MSE 0.013331803804200423 Test RE 0.055188999016791356\n",
      "50 Train Loss 0.052834522 Test MSE 0.010990457757039095 Test RE 0.050109024820307074\n",
      "51 Train Loss 0.050274186 Test MSE 0.011058017929522929 Test RE 0.05026280311367807\n",
      "52 Train Loss 0.04487505 Test MSE 0.009493815883182644 Test RE 0.0465723832644544\n",
      "53 Train Loss 0.041889563 Test MSE 0.008044854325541178 Test RE 0.04287134930994181\n",
      "54 Train Loss 0.03757704 Test MSE 0.006108522530895234 Test RE 0.037357357656099446\n",
      "55 Train Loss 0.03419333 Test MSE 0.007411968123593694 Test RE 0.04115047301854385\n",
      "56 Train Loss 0.031239737 Test MSE 0.007476289524930909 Test RE 0.04132864016952181\n",
      "57 Train Loss 0.02883019 Test MSE 0.007470424852281206 Test RE 0.04131242714725961\n",
      "58 Train Loss 0.026822295 Test MSE 0.0071087991567287375 Test RE 0.04030010544478447\n",
      "59 Train Loss 0.025885245 Test MSE 0.007101884367448961 Test RE 0.04028050054884414\n",
      "60 Train Loss 0.024454871 Test MSE 0.00658544409549996 Test RE 0.03878828642758535\n",
      "61 Train Loss 0.023057435 Test MSE 0.006732003213202438 Test RE 0.039217528206197255\n",
      "62 Train Loss 0.0223031 Test MSE 0.00659545914208288 Test RE 0.0388177695503996\n",
      "63 Train Loss 0.020737862 Test MSE 0.005162723107638117 Test RE 0.03434373123862803\n",
      "64 Train Loss 0.019393846 Test MSE 0.004853576292277827 Test RE 0.03329959799502021\n",
      "65 Train Loss 0.018045155 Test MSE 0.004866267185609636 Test RE 0.03334310465194402\n",
      "66 Train Loss 0.016589591 Test MSE 0.005990996711073808 Test RE 0.03699624108621962\n",
      "67 Train Loss 0.015218334 Test MSE 0.005266946463788506 Test RE 0.03468865908987136\n",
      "68 Train Loss 0.013929476 Test MSE 0.004814445408577299 Test RE 0.0331650910220181\n",
      "69 Train Loss 0.013116431 Test MSE 0.004703171047994451 Test RE 0.032779584712153254\n",
      "70 Train Loss 0.011836041 Test MSE 0.0039036850791765534 Test RE 0.029863826075442558\n",
      "71 Train Loss 0.010852392 Test MSE 0.00315808753360688 Test RE 0.026860874141027516\n",
      "72 Train Loss 0.009870116 Test MSE 0.0032745683778282954 Test RE 0.027351748349252743\n",
      "73 Train Loss 0.009139421 Test MSE 0.003093840746927997 Test RE 0.02658624715349136\n",
      "74 Train Loss 0.00830171 Test MSE 0.0026040576268386226 Test RE 0.024391210579917534\n",
      "75 Train Loss 0.0076065413 Test MSE 0.0025148482687266573 Test RE 0.023969774810425552\n",
      "76 Train Loss 0.007363964 Test MSE 0.0024921093329701247 Test RE 0.02386116292323536\n",
      "77 Train Loss 0.0068890788 Test MSE 0.0022622122640314243 Test RE 0.02273394141650031\n",
      "78 Train Loss 0.006252517 Test MSE 0.0019660373242008917 Test RE 0.02119356125903299\n",
      "79 Train Loss 0.0060357098 Test MSE 0.0020582070847966504 Test RE 0.0216846588801008\n",
      "80 Train Loss 0.0057134572 Test MSE 0.001921539593128484 Test RE 0.020952349461711168\n",
      "81 Train Loss 0.0053716027 Test MSE 0.0018654651759272853 Test RE 0.020644369962640775\n",
      "82 Train Loss 0.00503733 Test MSE 0.001725224631880431 Test RE 0.01985321680854671\n",
      "83 Train Loss 0.0048554237 Test MSE 0.0016922049413698348 Test RE 0.019662310033007786\n",
      "84 Train Loss 0.0047027217 Test MSE 0.0015904145805553966 Test RE 0.019061770418805274\n",
      "85 Train Loss 0.004460518 Test MSE 0.0014352109771500919 Test RE 0.01810781021082892\n",
      "86 Train Loss 0.0042179334 Test MSE 0.001429125744644888 Test RE 0.01806938126700797\n",
      "87 Train Loss 0.0038829756 Test MSE 0.0013178790038138079 Test RE 0.0173518517994448\n",
      "88 Train Loss 0.003712895 Test MSE 0.001187655588209991 Test RE 0.016472264928372147\n",
      "89 Train Loss 0.00358275 Test MSE 0.0011747820220348118 Test RE 0.016382746310083688\n",
      "90 Train Loss 0.003445946 Test MSE 0.0011159110905203514 Test RE 0.015966982790713586\n",
      "91 Train Loss 0.0032575342 Test MSE 0.0010443493336480547 Test RE 0.015446530890343533\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "92 Train Loss 0.003122013 Test MSE 0.0010090875177041002 Test RE 0.015183520400129628\n",
      "93 Train Loss 0.0029801542 Test MSE 0.0009425234125492712 Test RE 0.014674189858306593\n",
      "94 Train Loss 0.0028886308 Test MSE 0.0009176611239968836 Test RE 0.014479355364123161\n",
      "95 Train Loss 0.0027903053 Test MSE 0.0008356369210295396 Test RE 0.013817099033298062\n",
      "96 Train Loss 0.0026477717 Test MSE 0.0007831095828819455 Test RE 0.013375786543755864\n",
      "97 Train Loss 0.002536909 Test MSE 0.000749723560035164 Test RE 0.013087558611876179\n",
      "98 Train Loss 0.002446138 Test MSE 0.0007921571236676511 Test RE 0.013452832234956403\n",
      "99 Train Loss 0.002387318 Test MSE 0.0007612059302001305 Test RE 0.013187398857835885\n",
      "Training time: 125.27\n",
      "KG_stan\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 57.295475 Test MSE 8.836697093773683 Test RE 1.4208657460050875\n",
      "1 Train Loss 48.5848 Test MSE 9.32893317005821 Test RE 1.4599031688337807\n",
      "2 Train Loss 44.60527 Test MSE 8.518061506184768 Test RE 1.3950136145628138\n",
      "3 Train Loss 44.18707 Test MSE 8.425684886329135 Test RE 1.3874286767959927\n",
      "4 Train Loss 44.002525 Test MSE 8.519343998043963 Test RE 1.3951186282602244\n",
      "5 Train Loss 43.62628 Test MSE 8.562803640638164 Test RE 1.3986725538350577\n",
      "6 Train Loss 43.036076 Test MSE 8.37991176725803 Test RE 1.3836548930959187\n",
      "7 Train Loss 41.62873 Test MSE 8.445422164034591 Test RE 1.3890527612076646\n",
      "8 Train Loss 39.01271 Test MSE 7.918512077426503 Test RE 1.3450234370911236\n",
      "9 Train Loss 34.656754 Test MSE 7.668003008085025 Test RE 1.3235769567716493\n",
      "10 Train Loss 30.730503 Test MSE 7.94964346711246 Test RE 1.3476648029174578\n",
      "11 Train Loss 28.567749 Test MSE 8.045882391698049 Test RE 1.3557977234329899\n",
      "12 Train Loss 27.406055 Test MSE 8.093592112389011 Test RE 1.359811523314684\n",
      "13 Train Loss 25.38052 Test MSE 7.400837734822479 Test RE 1.3003147873511702\n",
      "14 Train Loss 24.109577 Test MSE 6.812954810723212 Test RE 1.2476012880211191\n",
      "15 Train Loss 23.117245 Test MSE 6.318250106687555 Test RE 1.2014521169198749\n",
      "16 Train Loss 20.815018 Test MSE 5.844685027066116 Test RE 1.1555496654710868\n",
      "17 Train Loss 19.68394 Test MSE 5.555186009324115 Test RE 1.1265678754212904\n",
      "18 Train Loss 18.920416 Test MSE 5.450397215444737 Test RE 1.1158919312353746\n",
      "19 Train Loss 18.587831 Test MSE 5.304829440561721 Test RE 1.1008896097534917\n",
      "20 Train Loss 18.315256 Test MSE 5.284458630058977 Test RE 1.098773841040656\n",
      "21 Train Loss 18.046623 Test MSE 5.3316877739357285 Test RE 1.1036729913094074\n",
      "22 Train Loss 17.828289 Test MSE 5.395260209681252 Test RE 1.1102333220252227\n",
      "23 Train Loss 17.530245 Test MSE 5.51086931718308 Test RE 1.122065260034932\n",
      "24 Train Loss 17.306398 Test MSE 5.607317310743329 Test RE 1.1318415340377455\n",
      "25 Train Loss 17.10072 Test MSE 5.543292645348686 Test RE 1.125361267674937\n",
      "26 Train Loss 16.82889 Test MSE 5.394084148017677 Test RE 1.110112310808051\n",
      "27 Train Loss 16.446177 Test MSE 5.2872467170306106 Test RE 1.099063660043736\n",
      "28 Train Loss 16.06003 Test MSE 4.956047687041987 Test RE 1.0640837195032868\n",
      "29 Train Loss 15.09317 Test MSE 4.912233727498305 Test RE 1.0593697597251939\n",
      "30 Train Loss 13.212938 Test MSE 4.144729983092865 Test RE 0.9730971076132566\n",
      "31 Train Loss 12.201834 Test MSE 3.7080465476656057 Test RE 0.9204085597168638\n",
      "32 Train Loss 11.553924 Test MSE 3.733587894157251 Test RE 0.9235730467801238\n",
      "33 Train Loss 11.176266 Test MSE 3.7081341642064727 Test RE 0.9204194337089755\n",
      "34 Train Loss 10.844116 Test MSE 3.7174807394542144 Test RE 0.9215786899531652\n",
      "35 Train Loss 10.390643 Test MSE 3.6504002955339145 Test RE 0.9132260802573907\n",
      "36 Train Loss 9.912426 Test MSE 3.4362143607897666 Test RE 0.8860295056150055\n",
      "37 Train Loss 9.44202 Test MSE 3.632901742038553 Test RE 0.9110346312723528\n",
      "38 Train Loss 9.267616 Test MSE 3.677098159141045 Test RE 0.9165595185910655\n",
      "39 Train Loss 8.858376 Test MSE 3.6129155694994433 Test RE 0.9085251762402091\n",
      "40 Train Loss 8.474461 Test MSE 3.547288603144995 Test RE 0.9002358878546397\n",
      "41 Train Loss 8.174601 Test MSE 3.383345763647303 Test RE 0.8791869878989078\n",
      "42 Train Loss 7.8877892 Test MSE 3.214740884218279 Test RE 0.857000444206296\n",
      "43 Train Loss 7.659431 Test MSE 3.1248056600284224 Test RE 0.8449277357520113\n",
      "44 Train Loss 6.9700665 Test MSE 2.9852362250853255 Test RE 0.8258428475330825\n",
      "45 Train Loss 5.732987 Test MSE 2.116101115156189 Test RE 0.6953064662962387\n",
      "46 Train Loss 4.8606634 Test MSE 1.9793906041639757 Test RE 0.6724713904422381\n",
      "47 Train Loss 4.2962923 Test MSE 1.913864047316396 Test RE 0.661246828916833\n",
      "48 Train Loss 3.7126718 Test MSE 1.8736452271879072 Test RE 0.6542620661943201\n",
      "49 Train Loss 3.412635 Test MSE 1.97100799099682 Test RE 0.6710459395077624\n",
      "50 Train Loss 3.1293483 Test MSE 2.121183978283804 Test RE 0.6961410265799849\n",
      "51 Train Loss 2.942143 Test MSE 2.096114143838494 Test RE 0.6920150258965904\n",
      "52 Train Loss 2.7063084 Test MSE 2.1694834859098475 Test RE 0.7040220064243711\n",
      "53 Train Loss 2.3383164 Test MSE 2.3299104087029288 Test RE 0.7295879811844456\n",
      "54 Train Loss 2.137338 Test MSE 2.3257748700896586 Test RE 0.7289401924692169\n",
      "55 Train Loss 2.0358682 Test MSE 2.3471508357836846 Test RE 0.7322823391323593\n",
      "56 Train Loss 1.9065223 Test MSE 2.341044429048384 Test RE 0.7313291566834976\n",
      "57 Train Loss 1.7403371 Test MSE 2.412258301477179 Test RE 0.7423692339661618\n",
      "58 Train Loss 1.5608587 Test MSE 2.4268417149321717 Test RE 0.7446098655061605\n",
      "59 Train Loss 1.4469582 Test MSE 2.4459910252135013 Test RE 0.7475418136495025\n",
      "60 Train Loss 1.3907983 Test MSE 2.4219607349186743 Test RE 0.7438606911529712\n",
      "61 Train Loss 1.3373438 Test MSE 2.409676044267576 Test RE 0.7419717844887953\n",
      "62 Train Loss 1.298917 Test MSE 2.4553668167741693 Test RE 0.7489731542794426\n",
      "63 Train Loss 1.2648674 Test MSE 2.4479918799235256 Test RE 0.7478475009533273\n",
      "64 Train Loss 1.2383561 Test MSE 2.4398379780504267 Test RE 0.7466009769257981\n",
      "65 Train Loss 1.2192416 Test MSE 2.4654881603807413 Test RE 0.750515249452971\n",
      "66 Train Loss 1.1965175 Test MSE 2.4345952919834732 Test RE 0.7457984032453239\n",
      "67 Train Loss 1.1641247 Test MSE 2.4511750975172943 Test RE 0.7483335703641352\n",
      "68 Train Loss 1.1467186 Test MSE 2.461645649918047 Test RE 0.7499301752149253\n",
      "69 Train Loss 1.1225982 Test MSE 2.4855975150500225 Test RE 0.7535697617019952\n",
      "70 Train Loss 1.1060262 Test MSE 2.497036475271582 Test RE 0.7553017717419034\n",
      "71 Train Loss 1.0907589 Test MSE 2.4886102139211377 Test RE 0.7540263101261562\n",
      "72 Train Loss 1.0771102 Test MSE 2.505250223515875 Test RE 0.7565429961484426\n",
      "73 Train Loss 1.0632306 Test MSE 2.5129592643130576 Test RE 0.7577061017330297\n",
      "74 Train Loss 1.05124 Test MSE 2.505436361400917 Test RE 0.7565711008655472\n",
      "75 Train Loss 1.0408462 Test MSE 2.506843343161222 Test RE 0.7567835054494304\n",
      "76 Train Loss 1.0345544 Test MSE 2.5136358643642973 Test RE 0.757808098906484\n",
      "77 Train Loss 1.0223413 Test MSE 2.5656005027201103 Test RE 0.7656011485394866\n",
      "78 Train Loss 1.0115554 Test MSE 2.567248632765728 Test RE 0.7658470184000332\n",
      "79 Train Loss 1.0031204 Test MSE 2.5596129220918398 Test RE 0.7647072493702656\n",
      "80 Train Loss 0.9956781 Test MSE 2.541983042962244 Test RE 0.7620691568941727\n",
      "81 Train Loss 0.9839535 Test MSE 2.5417036177072463 Test RE 0.7620272708514527\n",
      "82 Train Loss 0.9785734 Test MSE 2.538342262617749 Test RE 0.7615232208014723\n",
      "83 Train Loss 0.96585584 Test MSE 2.542948760066985 Test RE 0.7622139008446932\n",
      "84 Train Loss 0.9600625 Test MSE 2.5405539839916416 Test RE 0.7618549157191475\n",
      "85 Train Loss 0.94988143 Test MSE 2.533824437139135 Test RE 0.7608452268856497\n",
      "86 Train Loss 0.9424404 Test MSE 2.531261190355007 Test RE 0.7604602894917954\n",
      "87 Train Loss 0.9340454 Test MSE 2.541462247998575 Test RE 0.7619910875085232\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88 Train Loss 0.92505103 Test MSE 2.5376675040120777 Test RE 0.7614219975529134\n",
      "89 Train Loss 0.9190612 Test MSE 2.524477449730249 Test RE 0.7594405949673817\n",
      "90 Train Loss 0.90900695 Test MSE 2.54416634910806 Test RE 0.7623963567876346\n",
      "91 Train Loss 0.8984829 Test MSE 2.576823311875316 Test RE 0.7672738212273884\n",
      "92 Train Loss 0.8785628 Test MSE 2.611627342912597 Test RE 0.7724380587295463\n",
      "93 Train Loss 0.8633232 Test MSE 2.630220179597373 Test RE 0.7751827734283536\n",
      "94 Train Loss 0.8479759 Test MSE 2.6483609089230296 Test RE 0.7778514125489294\n",
      "95 Train Loss 0.827513 Test MSE 2.6293745826915194 Test RE 0.7750581555632328\n",
      "96 Train Loss 0.81040484 Test MSE 2.632493907706992 Test RE 0.7755177594733205\n",
      "97 Train Loss 0.791877 Test MSE 2.6591238698641475 Test RE 0.7794304072451891\n",
      "98 Train Loss 0.7792686 Test MSE 2.698204568427062 Test RE 0.7851370952999489\n",
      "99 Train Loss 0.76317286 Test MSE 2.743414453586497 Test RE 0.7916874700548301\n",
      "Training time: 126.07\n",
      "KG_stan\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 53.000534 Test MSE 6.908050746100592 Test RE 1.2562781892656376\n",
      "1 Train Loss 39.390476 Test MSE 8.283940622303204 Test RE 1.3757089063886583\n",
      "2 Train Loss 36.24849 Test MSE 7.487556077979785 Test RE 1.3079107348074384\n",
      "3 Train Loss 32.940742 Test MSE 6.934864468725476 Test RE 1.2587139610713298\n",
      "4 Train Loss 30.056185 Test MSE 6.383456397181272 Test RE 1.2076358815506751\n",
      "5 Train Loss 26.617348 Test MSE 6.394196751487334 Test RE 1.2086513961271599\n",
      "6 Train Loss 24.475426 Test MSE 6.134408213379842 Test RE 1.1838437798386936\n",
      "7 Train Loss 23.93482 Test MSE 6.132844428510407 Test RE 1.183692877345468\n",
      "8 Train Loss 23.312408 Test MSE 6.044637918894521 Test RE 1.1751497320309283\n",
      "9 Train Loss 23.004993 Test MSE 5.784389551412374 Test RE 1.1495737194009132\n",
      "10 Train Loss 22.693222 Test MSE 5.776229792079394 Test RE 1.1487626091530205\n",
      "11 Train Loss 22.397207 Test MSE 5.614810106126698 Test RE 1.1325974949871838\n",
      "12 Train Loss 22.137589 Test MSE 5.788716103255238 Test RE 1.150003562563656\n",
      "13 Train Loss 21.564913 Test MSE 5.662643729786723 Test RE 1.1374116690910876\n",
      "14 Train Loss 20.830606 Test MSE 5.522470158635568 Test RE 1.1232456598245377\n",
      "15 Train Loss 19.434366 Test MSE 5.379839923342625 Test RE 1.108645598226947\n",
      "16 Train Loss 17.369686 Test MSE 5.355160696060051 Test RE 1.10609980064252\n",
      "17 Train Loss 14.767672 Test MSE 5.634827723565319 Test RE 1.134614636414773\n",
      "18 Train Loss 13.246222 Test MSE 6.03710921850875 Test RE 1.1744176694195463\n",
      "19 Train Loss 12.22398 Test MSE 6.079458215907722 Test RE 1.1785296121443483\n",
      "20 Train Loss 11.705845 Test MSE 5.988938419714488 Test RE 1.1697228777899993\n",
      "21 Train Loss 11.319102 Test MSE 5.9353727313424836 Test RE 1.164480066833345\n",
      "22 Train Loss 11.082626 Test MSE 5.941271798517572 Test RE 1.1650586017009024\n",
      "23 Train Loss 10.907786 Test MSE 5.948498272599222 Test RE 1.1657669270817717\n",
      "24 Train Loss 10.742344 Test MSE 5.919880204602112 Test RE 1.1629593092265922\n",
      "25 Train Loss 10.620419 Test MSE 5.841371415689411 Test RE 1.155222052830923\n",
      "26 Train Loss 10.380643 Test MSE 5.82757404453824 Test RE 1.1538569238694898\n",
      "27 Train Loss 9.625142 Test MSE 5.26157422827353 Test RE 1.0963921341006495\n",
      "28 Train Loss 8.525337 Test MSE 4.645487560304127 Test RE 1.0302051367621894\n",
      "29 Train Loss 7.9838076 Test MSE 4.509955839139182 Test RE 1.0150658218625728\n",
      "30 Train Loss 7.6971216 Test MSE 4.359506198971264 Test RE 0.9979911959341725\n",
      "31 Train Loss 7.432252 Test MSE 4.066120661578168 Test RE 0.9638250096789176\n",
      "32 Train Loss 7.23085 Test MSE 3.9515705628137217 Test RE 0.9501516590998831\n",
      "33 Train Loss 7.0516777 Test MSE 3.903799223098503 Test RE 0.9443909071681327\n",
      "34 Train Loss 6.9116125 Test MSE 3.856723876651696 Test RE 0.9386795004650468\n",
      "35 Train Loss 6.8376827 Test MSE 3.819757645912194 Test RE 0.934170104344386\n",
      "36 Train Loss 6.7383595 Test MSE 3.778987688162342 Test RE 0.9291713252859248\n",
      "37 Train Loss 6.663921 Test MSE 3.722283828220973 Test RE 0.92217385045574\n",
      "38 Train Loss 6.573271 Test MSE 3.6628099522805577 Test RE 0.9147770344037802\n",
      "39 Train Loss 6.5201225 Test MSE 3.6776573734197324 Test RE 0.9166292112756296\n",
      "40 Train Loss 6.376923 Test MSE 3.5779520448307176 Test RE 0.9041184212343061\n",
      "41 Train Loss 6.169484 Test MSE 3.3067542922715614 Test RE 0.8691785964257609\n",
      "42 Train Loss 5.58891 Test MSE 2.911191510093524 Test RE 0.8155365850455494\n",
      "43 Train Loss 5.0337005 Test MSE 2.7279908028664623 Test RE 0.7894588746334651\n",
      "44 Train Loss 4.038868 Test MSE 2.2123045747906214 Test RE 0.7109360206679496\n",
      "45 Train Loss 3.724144 Test MSE 2.1517542079515763 Test RE 0.7011394292238424\n",
      "46 Train Loss 3.6363735 Test MSE 2.167069017509661 Test RE 0.7036301361682792\n",
      "47 Train Loss 3.5571373 Test MSE 2.1381405065494996 Test RE 0.6989179282237568\n",
      "48 Train Loss 3.4881952 Test MSE 2.1396446037777546 Test RE 0.69916371555716\n",
      "49 Train Loss 3.4251115 Test MSE 2.137230891961026 Test RE 0.6987692444559298\n",
      "50 Train Loss 3.3492537 Test MSE 2.132007429101558 Test RE 0.6979148145213437\n",
      "51 Train Loss 3.2513223 Test MSE 2.088289051733741 Test RE 0.6907221228827143\n",
      "52 Train Loss 2.9771285 Test MSE 1.9202232979678113 Test RE 0.6623444897140447\n",
      "53 Train Loss 2.703868 Test MSE 1.566461857170523 Test RE 0.598229702845307\n",
      "54 Train Loss 2.291572 Test MSE 1.3504985300275816 Test RE 0.5554629779702405\n",
      "55 Train Loss 2.0341063 Test MSE 1.076489492390591 Test RE 0.49592152381156424\n",
      "56 Train Loss 1.1858664 Test MSE 0.1291839564210776 Test RE 0.17179572434157964\n",
      "57 Train Loss 0.48114738 Test MSE 0.05768808206401651 Test RE 0.11480243784440693\n",
      "58 Train Loss 0.3002166 Test MSE 0.038388175324612465 Test RE 0.09364978724857853\n",
      "59 Train Loss 0.21227652 Test MSE 0.02350931670018044 Test RE 0.07328718888213782\n",
      "60 Train Loss 0.17882112 Test MSE 0.02044922122272447 Test RE 0.06835124773105858\n",
      "61 Train Loss 0.15347292 Test MSE 0.02357812295331267 Test RE 0.07339435772681917\n",
      "62 Train Loss 0.12023653 Test MSE 0.01748913204253797 Test RE 0.06321093125507621\n",
      "63 Train Loss 0.10178878 Test MSE 0.01501580997534733 Test RE 0.05857097345622947\n",
      "64 Train Loss 0.08653591 Test MSE 0.010293888638740653 Test RE 0.04849509261876803\n",
      "65 Train Loss 0.07432872 Test MSE 0.008994910568096808 Test RE 0.04533216753659857\n",
      "66 Train Loss 0.06546396 Test MSE 0.006506916758579091 Test RE 0.03855632973822218\n",
      "67 Train Loss 0.057142243 Test MSE 0.0051305923561897225 Test RE 0.03423669352285239\n",
      "68 Train Loss 0.049833767 Test MSE 0.004895358098374568 Test RE 0.033442619945012804\n",
      "69 Train Loss 0.04149367 Test MSE 0.0036021302919041044 Test RE 0.02868717409567583\n",
      "70 Train Loss 0.037415013 Test MSE 0.002818947368763893 Test RE 0.025377658213212022\n",
      "71 Train Loss 0.031616308 Test MSE 0.003078634865007244 Test RE 0.026520832459474807\n",
      "72 Train Loss 0.02908837 Test MSE 0.0032394708448001785 Test RE 0.027204772477355157\n",
      "73 Train Loss 0.025627105 Test MSE 0.003762575695312961 Test RE 0.0293191032689255\n",
      "74 Train Loss 0.02364558 Test MSE 0.0040770181870792765 Test RE 0.030519638465372288\n",
      "75 Train Loss 0.022281276 Test MSE 0.004224346222773674 Test RE 0.031066177043255504\n",
      "76 Train Loss 0.021194402 Test MSE 0.00395808910797558 Test RE 0.03007120587064213\n",
      "77 Train Loss 0.020260967 Test MSE 0.003485255051148761 Test RE 0.028217942419472626\n",
      "78 Train Loss 0.019102251 Test MSE 0.0032263946098045207 Test RE 0.02714981045940952\n",
      "79 Train Loss 0.015683852 Test MSE 0.002644695375725705 Test RE 0.024580792924977186\n",
      "80 Train Loss 0.015124223 Test MSE 0.0025069614105121276 Test RE 0.023932159287384625\n",
      "81 Train Loss 0.013641339 Test MSE 0.0021173974979261374 Test RE 0.02199425510758166\n",
      "82 Train Loss 0.013100298 Test MSE 0.0019918791518417902 Test RE 0.02133239188751698\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "83 Train Loss 0.012776345 Test MSE 0.0019166739973927821 Test RE 0.020925805569664092\n",
      "84 Train Loss 0.012302169 Test MSE 0.0017926497904943838 Test RE 0.02023744994702158\n",
      "85 Train Loss 0.011564048 Test MSE 0.0015469002893601948 Test RE 0.01879919360972873\n",
      "86 Train Loss 0.010920439 Test MSE 0.0016773507135509989 Test RE 0.019575821629438116\n",
      "87 Train Loss 0.010017211 Test MSE 0.001814417532729782 Test RE 0.020359948596713584\n",
      "88 Train Loss 0.009170726 Test MSE 0.001737576148314772 Test RE 0.019924158294386953\n",
      "89 Train Loss 0.00858777 Test MSE 0.0016045092973222257 Test RE 0.019146049581945643\n",
      "90 Train Loss 0.007855144 Test MSE 0.0015715888763884452 Test RE 0.01894861768624331\n",
      "91 Train Loss 0.007647471 Test MSE 0.001418295956967459 Test RE 0.018000786988162\n",
      "92 Train Loss 0.007553477 Test MSE 0.0014382549067737118 Test RE 0.018127002408498294\n",
      "93 Train Loss 0.007309642 Test MSE 0.0013397393486577341 Test RE 0.017495172009849173\n",
      "94 Train Loss 0.00715579 Test MSE 0.001347174050429935 Test RE 0.01754364839149356\n",
      "95 Train Loss 0.006663943 Test MSE 0.0010857737193059626 Test RE 0.0157498971841445\n",
      "96 Train Loss 0.0063471165 Test MSE 0.0010847047408860822 Test RE 0.015742142140016114\n",
      "97 Train Loss 0.0061173844 Test MSE 0.0010137358627848713 Test RE 0.015218451537624677\n",
      "98 Train Loss 0.0060210135 Test MSE 0.0009764395052142963 Test RE 0.014935877096833807\n",
      "99 Train Loss 0.0057715876 Test MSE 0.0010561410957542117 Test RE 0.015533489608380632\n",
      "Training time: 124.89\n",
      "KG_stan\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 56.247677 Test MSE 8.558594951584753 Test RE 1.398328782004184\n",
      "1 Train Loss 51.262177 Test MSE 8.293118306920071 Test RE 1.3764707616443324\n",
      "2 Train Loss 44.065697 Test MSE 8.689421589967628 Test RE 1.408975672516852\n",
      "3 Train Loss 43.35836 Test MSE 8.512077523313945 Test RE 1.394523526285945\n",
      "4 Train Loss 42.410484 Test MSE 8.396326531375905 Test RE 1.3850093976434363\n",
      "5 Train Loss 41.356857 Test MSE 8.56718446822198 Test RE 1.399030296465713\n",
      "6 Train Loss 39.624573 Test MSE 8.522210927080428 Test RE 1.3953533511254468\n",
      "7 Train Loss 37.54172 Test MSE 9.0198281276212 Test RE 1.4355132034162954\n",
      "8 Train Loss 33.331535 Test MSE 8.257928316383405 Test RE 1.3735472843385617\n",
      "9 Train Loss 31.597359 Test MSE 8.512344595601785 Test RE 1.3945454031818176\n",
      "10 Train Loss 30.49887 Test MSE 8.586574912519133 Test RE 1.4006126416700633\n",
      "11 Train Loss 29.728413 Test MSE 8.5538962394443 Test RE 1.3979448845013067\n",
      "12 Train Loss 28.072132 Test MSE 8.88089972279496 Test RE 1.4244150164566967\n",
      "13 Train Loss 26.367706 Test MSE 8.820077476930818 Test RE 1.419528970646144\n",
      "14 Train Loss 25.14807 Test MSE 8.680384266421173 Test RE 1.4082427882041517\n",
      "15 Train Loss 23.915691 Test MSE 8.842777775113795 Test RE 1.4213545228605282\n",
      "16 Train Loss 22.920061 Test MSE 8.48362807599474 Test RE 1.392191156275528\n",
      "17 Train Loss 20.34382 Test MSE 7.592311869235568 Test RE 1.3170282176464205\n",
      "18 Train Loss 19.213423 Test MSE 7.485103187541308 Test RE 1.307696484357068\n",
      "19 Train Loss 18.367014 Test MSE 7.2486868276374254 Test RE 1.2868790439923559\n",
      "20 Train Loss 16.947437 Test MSE 6.617766562243657 Test RE 1.229599795005691\n",
      "21 Train Loss 14.590239 Test MSE 5.982928630302433 Test RE 1.1691358327976857\n",
      "22 Train Loss 12.921728 Test MSE 5.91172995537272 Test RE 1.162158476058933\n",
      "23 Train Loss 11.3323345 Test MSE 5.661036659101113 Test RE 1.137250257677349\n",
      "24 Train Loss 10.265429 Test MSE 5.493544152347126 Test RE 1.1203000876587472\n",
      "25 Train Loss 9.539705 Test MSE 5.596768954220942 Test RE 1.130776435744875\n",
      "26 Train Loss 9.004936 Test MSE 5.434104276102817 Test RE 1.1142228081911463\n",
      "27 Train Loss 8.398055 Test MSE 5.3958242908897285 Test RE 1.1102913586500915\n",
      "28 Train Loss 7.7607374 Test MSE 5.265365824064206 Test RE 1.0967871040252348\n",
      "29 Train Loss 7.3189716 Test MSE 5.193170035087053 Test RE 1.0892418811746478\n",
      "30 Train Loss 6.437396 Test MSE 4.742353284130429 Test RE 1.040890421788252\n",
      "31 Train Loss 4.761388 Test MSE 3.9778667634804132 Test RE 0.9533078661039611\n",
      "32 Train Loss 3.8077538 Test MSE 4.009554757710312 Test RE 0.9570973962400847\n",
      "33 Train Loss 3.416741 Test MSE 4.009067099040951 Test RE 0.9570391913939877\n",
      "34 Train Loss 3.0883193 Test MSE 4.075073446142292 Test RE 0.9648855012163104\n",
      "35 Train Loss 2.8961222 Test MSE 4.105893048887729 Test RE 0.9685273219355075\n",
      "36 Train Loss 2.7792456 Test MSE 4.090536658612567 Test RE 0.9667144378579333\n",
      "37 Train Loss 2.6461926 Test MSE 4.027050905482373 Test RE 0.9591833247900022\n",
      "38 Train Loss 2.5315177 Test MSE 3.979862181896447 Test RE 0.9535469401716428\n",
      "39 Train Loss 2.4009728 Test MSE 4.025175430252196 Test RE 0.9589599436971977\n",
      "40 Train Loss 2.3163486 Test MSE 3.92248272143109 Test RE 0.9466481268305523\n",
      "41 Train Loss 2.170704 Test MSE 3.7055457177306037 Test RE 0.9200981302926312\n",
      "42 Train Loss 2.0186186 Test MSE 3.629482606482621 Test RE 0.9106058164595311\n",
      "43 Train Loss 1.8979168 Test MSE 3.5575567393685197 Test RE 0.9015378773585578\n",
      "44 Train Loss 1.8135622 Test MSE 3.5370671651924086 Test RE 0.8989379467370295\n",
      "45 Train Loss 1.694687 Test MSE 3.465693832470182 Test RE 0.8898220367927991\n",
      "46 Train Loss 1.6383297 Test MSE 3.473896014998404 Test RE 0.8908743757944815\n",
      "47 Train Loss 1.584841 Test MSE 3.517473187526971 Test RE 0.8964446054478538\n",
      "48 Train Loss 1.553587 Test MSE 3.490583231750156 Test RE 0.8930115157156742\n",
      "49 Train Loss 1.5084066 Test MSE 3.425508034120536 Test RE 0.8846481130536025\n",
      "50 Train Loss 1.4654691 Test MSE 3.4340948755594143 Test RE 0.8857562083286524\n",
      "51 Train Loss 1.43257 Test MSE 3.385495831939049 Test RE 0.8794662989678883\n",
      "52 Train Loss 1.4002467 Test MSE 3.4125125263306075 Test RE 0.8829684533189838\n",
      "53 Train Loss 1.369956 Test MSE 3.3601313477645816 Test RE 0.8761655788719466\n",
      "54 Train Loss 1.3306351 Test MSE 3.314520699591796 Test RE 0.8701986958173484\n",
      "55 Train Loss 1.3027748 Test MSE 3.257459154131431 Test RE 0.8626756696349483\n",
      "56 Train Loss 1.2680044 Test MSE 3.1201205999589745 Test RE 0.8442940928136046\n",
      "57 Train Loss 1.1980302 Test MSE 2.886710296912023 Test RE 0.8121002813583508\n",
      "58 Train Loss 1.0694877 Test MSE 2.732179005839209 Test RE 0.7900646585524671\n",
      "59 Train Loss 0.9943382 Test MSE 2.7287146174800476 Test RE 0.7895636007877903\n",
      "60 Train Loss 0.9438742 Test MSE 2.6472088122490076 Test RE 0.7776822026980521\n",
      "61 Train Loss 0.9010775 Test MSE 2.6440175308447516 Test RE 0.7772133029343499\n",
      "62 Train Loss 0.8779092 Test MSE 2.66134316807894 Test RE 0.7797555947556659\n",
      "63 Train Loss 0.851429 Test MSE 2.6169719574685373 Test RE 0.7732280400623612\n",
      "64 Train Loss 0.8304189 Test MSE 2.5828196884022616 Test RE 0.7681660417415986\n",
      "65 Train Loss 0.8093176 Test MSE 2.580106305851099 Test RE 0.7677624370969914\n",
      "66 Train Loss 0.7916812 Test MSE 2.555486452795293 Test RE 0.7640905909328937\n",
      "67 Train Loss 0.7681168 Test MSE 2.569787164283716 Test RE 0.7662255649910896\n",
      "68 Train Loss 0.7544441 Test MSE 2.5611284500055786 Test RE 0.7649336046276922\n",
      "69 Train Loss 0.7325722 Test MSE 2.57622078205894 Test RE 0.7671841114711033\n",
      "70 Train Loss 0.7176365 Test MSE 2.607595428143743 Test RE 0.7718415709910094\n",
      "71 Train Loss 0.70771796 Test MSE 2.6124004686107365 Test RE 0.7725523835248881\n",
      "72 Train Loss 0.69494593 Test MSE 2.627181022110272 Test RE 0.7747347912615079\n",
      "73 Train Loss 0.6841004 Test MSE 2.6252571875412465 Test RE 0.7744510775547233\n",
      "74 Train Loss 0.6706986 Test MSE 2.6536654483070414 Test RE 0.7786300223276786\n",
      "75 Train Loss 0.65463746 Test MSE 2.681318266513299 Test RE 0.7826764088966714\n",
      "76 Train Loss 0.6420225 Test MSE 2.737467742973241 Test RE 0.7908289613612964\n",
      "77 Train Loss 0.62820584 Test MSE 2.753411102320364 Test RE 0.7931285620195417\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "78 Train Loss 0.62000203 Test MSE 2.7533247802775582 Test RE 0.7931161292567335\n",
      "79 Train Loss 0.61170995 Test MSE 2.7473516862227165 Test RE 0.7922553645949862\n",
      "80 Train Loss 0.60280335 Test MSE 2.758787192416813 Test RE 0.7939024840976807\n",
      "81 Train Loss 0.59593105 Test MSE 2.7803934401347505 Test RE 0.7970052604696576\n",
      "82 Train Loss 0.58684796 Test MSE 2.7984134958002254 Test RE 0.7995838314514638\n",
      "83 Train Loss 0.5796224 Test MSE 2.8152194482360424 Test RE 0.8019811992797016\n",
      "84 Train Loss 0.5727607 Test MSE 2.842302614455438 Test RE 0.8058296028914121\n",
      "85 Train Loss 0.5654696 Test MSE 2.838412133877937 Test RE 0.8052779132262579\n",
      "86 Train Loss 0.55931616 Test MSE 2.847114255483917 Test RE 0.8065113957416705\n",
      "87 Train Loss 0.55144405 Test MSE 2.863507466777003 Test RE 0.8088299424525177\n",
      "88 Train Loss 0.54394925 Test MSE 2.902666640184682 Test RE 0.8143416378240563\n",
      "89 Train Loss 0.5388553 Test MSE 2.922868433288094 Test RE 0.8171705256297581\n",
      "90 Train Loss 0.53076416 Test MSE 2.9423575807692335 Test RE 0.8198903704601753\n",
      "91 Train Loss 0.5225954 Test MSE 2.9480622177027147 Test RE 0.8206847864905386\n",
      "92 Train Loss 0.51424587 Test MSE 2.9461844743758014 Test RE 0.820423380737698\n",
      "93 Train Loss 0.5034845 Test MSE 2.9766500687981208 Test RE 0.8246543449773324\n",
      "94 Train Loss 0.49608102 Test MSE 2.989168246347338 Test RE 0.8263865504123414\n",
      "95 Train Loss 0.48745987 Test MSE 2.993586556417167 Test RE 0.8269970687045423\n",
      "96 Train Loss 0.47979587 Test MSE 3.0083647311984545 Test RE 0.8290358374024116\n",
      "97 Train Loss 0.47102687 Test MSE 3.0497850243944162 Test RE 0.83472356471208\n",
      "98 Train Loss 0.46196246 Test MSE 3.074819468966922 Test RE 0.8381425159397072\n",
      "99 Train Loss 0.4574287 Test MSE 3.0911147232466294 Test RE 0.8403604832490177\n",
      "Training time: 119.94\n",
      "KG_stan\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 56.55169 Test MSE 8.629510386237461 Test RE 1.4041100187226332\n",
      "1 Train Loss 48.835266 Test MSE 7.846805341160528 Test RE 1.3389196021366965\n",
      "2 Train Loss 48.111523 Test MSE 8.13542410384751 Test RE 1.3633211091304915\n",
      "3 Train Loss 45.931347 Test MSE 8.236990233233346 Test RE 1.3718048557728142\n",
      "4 Train Loss 44.742355 Test MSE 7.988330341821287 Test RE 1.3509400228989465\n",
      "5 Train Loss 44.13943 Test MSE 8.165492041767113 Test RE 1.3658381535821984\n",
      "6 Train Loss 43.521297 Test MSE 8.256421762161276 Test RE 1.3734219854881549\n",
      "7 Train Loss 42.930855 Test MSE 8.342528924916891 Test RE 1.3805651965972763\n",
      "8 Train Loss 42.197163 Test MSE 7.998361089863958 Test RE 1.3517879277291838\n",
      "9 Train Loss 41.440407 Test MSE 7.695073929362393 Test RE 1.3259112594277005\n",
      "10 Train Loss 40.100704 Test MSE 7.487219988469048 Test RE 1.3078813807694036\n",
      "11 Train Loss 38.515774 Test MSE 7.150300180825322 Test RE 1.278115782189952\n",
      "12 Train Loss 33.711975 Test MSE 5.85204919154634 Test RE 1.156277418887893\n",
      "13 Train Loss 28.297466 Test MSE 4.955168347161548 Test RE 1.0639893163802912\n",
      "14 Train Loss 25.832993 Test MSE 5.053686918701983 Test RE 1.0745143677453892\n",
      "15 Train Loss 24.03401 Test MSE 4.097479357034236 Test RE 0.9675344722376449\n",
      "16 Train Loss 20.047918 Test MSE 3.6711607858445645 Test RE 0.9158192399368241\n",
      "17 Train Loss 16.889616 Test MSE 3.2322151049387036 Test RE 0.8593264667823368\n",
      "18 Train Loss 15.855948 Test MSE 3.3455742535804336 Test RE 0.8742656131367554\n",
      "19 Train Loss 14.996227 Test MSE 3.1750205602383366 Test RE 0.8516895748264031\n",
      "20 Train Loss 14.420238 Test MSE 3.1559387805482806 Test RE 0.8491264032951044\n",
      "21 Train Loss 13.411192 Test MSE 3.190329978210738 Test RE 0.853740457868373\n",
      "22 Train Loss 12.624729 Test MSE 2.971179907331953 Test RE 0.8238962668232174\n",
      "23 Train Loss 11.746579 Test MSE 2.7250508108427955 Test RE 0.7890333547832702\n",
      "24 Train Loss 10.846252 Test MSE 2.6648335039148767 Test RE 0.780266749643442\n",
      "25 Train Loss 9.759418 Test MSE 2.7392936780797448 Test RE 0.7910926652267679\n",
      "26 Train Loss 8.679186 Test MSE 2.492770942483749 Test RE 0.7546563783794905\n",
      "27 Train Loss 7.4854755 Test MSE 2.3728473595177233 Test RE 0.7362799280315585\n",
      "28 Train Loss 6.845725 Test MSE 2.2840649943358433 Test RE 0.7223743042714493\n",
      "29 Train Loss 5.917364 Test MSE 1.922338175951061 Test RE 0.6627091327996688\n",
      "30 Train Loss 5.2284236 Test MSE 1.8851965030430542 Test RE 0.6562757742647547\n",
      "31 Train Loss 4.7881193 Test MSE 1.8292743582945155 Test RE 0.646468671875534\n",
      "32 Train Loss 4.6466923 Test MSE 1.7905596485126678 Test RE 0.639591166278102\n",
      "33 Train Loss 4.522743 Test MSE 1.8257128811374095 Test RE 0.6458390492230253\n",
      "34 Train Loss 4.4133835 Test MSE 1.7997133485695467 Test RE 0.6412239413325194\n",
      "35 Train Loss 4.201429 Test MSE 1.7271560356803026 Test RE 0.6281651631558893\n",
      "36 Train Loss 4.0418057 Test MSE 1.7198914928227518 Test RE 0.6268427171233553\n",
      "37 Train Loss 3.9403148 Test MSE 1.7392539363307087 Test RE 0.6303613221317814\n",
      "38 Train Loss 3.8846438 Test MSE 1.7508692782211879 Test RE 0.6324627054558749\n",
      "39 Train Loss 3.8208666 Test MSE 1.7473575629376776 Test RE 0.6318281224594998\n",
      "40 Train Loss 3.7682571 Test MSE 1.722707826808892 Test RE 0.6273557367916595\n",
      "41 Train Loss 3.703165 Test MSE 1.7065701052384052 Test RE 0.6244103987925693\n",
      "42 Train Loss 3.5992446 Test MSE 1.6442948827743697 Test RE 0.6129117015821064\n",
      "43 Train Loss 1.7888548 Test MSE 0.5017958890665716 Test RE 0.33858803720933445\n",
      "44 Train Loss 0.7961564 Test MSE 0.23364632558822634 Test RE 0.2310401926633805\n",
      "45 Train Loss 0.4171887 Test MSE 0.05234827822097547 Test RE 0.10936019129797638\n",
      "46 Train Loss 0.25878167 Test MSE 0.029735148927546157 Test RE 0.08242199442873803\n",
      "47 Train Loss 0.18158428 Test MSE 0.02237484802408222 Test RE 0.07149704742672376\n",
      "48 Train Loss 0.13952537 Test MSE 0.015932797139673223 Test RE 0.060332882314999996\n",
      "49 Train Loss 0.12158957 Test MSE 0.012143577611158798 Test RE 0.052672191319895305\n",
      "50 Train Loss 0.10076682 Test MSE 0.010238698583613716 Test RE 0.04836491616263043\n",
      "51 Train Loss 0.090299845 Test MSE 0.00786147421717813 Test RE 0.042379912674520154\n",
      "52 Train Loss 0.080963984 Test MSE 0.005951736296228567 Test RE 0.03687481928481616\n",
      "53 Train Loss 0.07291328 Test MSE 0.005741623747986698 Test RE 0.03621808011300333\n",
      "54 Train Loss 0.066286385 Test MSE 0.004856453346177235 Test RE 0.0333094660324381\n",
      "55 Train Loss 0.0605929 Test MSE 0.0051124832513107284 Test RE 0.034176218641234574\n",
      "56 Train Loss 0.05531273 Test MSE 0.004076184505525028 Test RE 0.030516517929812182\n",
      "57 Train Loss 0.048775773 Test MSE 0.0034354578113919574 Test RE 0.028015628516556538\n",
      "58 Train Loss 0.044722926 Test MSE 0.0034509069845287503 Test RE 0.028078550667959494\n",
      "59 Train Loss 0.041560728 Test MSE 0.0032062896087259513 Test RE 0.027065087410290636\n",
      "60 Train Loss 0.033232212 Test MSE 0.003390496923352547 Test RE 0.02783170019982745\n",
      "61 Train Loss 0.029503807 Test MSE 0.004043628240769089 Test RE 0.030394406729618514\n",
      "62 Train Loss 0.024489654 Test MSE 0.002571082171875834 Test RE 0.02423628432603693\n",
      "63 Train Loss 0.021102944 Test MSE 0.002033374447649381 Test RE 0.021553447259361116\n",
      "64 Train Loss 0.019496057 Test MSE 0.002377611314206686 Test RE 0.023306576763155635\n",
      "65 Train Loss 0.0182774 Test MSE 0.0027176543410511187 Test RE 0.024917540294505314\n",
      "66 Train Loss 0.01641876 Test MSE 0.002911364762479394 Test RE 0.025790298628165228\n",
      "67 Train Loss 0.015582038 Test MSE 0.002603819676296505 Test RE 0.024390096158655852\n",
      "68 Train Loss 0.014288804 Test MSE 0.002219802068974235 Test RE 0.022519834068307733\n",
      "69 Train Loss 0.013031898 Test MSE 0.002170711887917245 Test RE 0.022269432639743906\n",
      "70 Train Loss 0.01189913 Test MSE 0.002334977341547506 Test RE 0.02309667139699914\n",
      "71 Train Loss 0.011006285 Test MSE 0.0021234522081568807 Test RE 0.022025679008420052\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "72 Train Loss 0.010133525 Test MSE 0.002234022508141925 Test RE 0.02259185190488557\n",
      "73 Train Loss 0.009011781 Test MSE 0.0019335801183202595 Test RE 0.021017891521889712\n",
      "74 Train Loss 0.008218116 Test MSE 0.0016697703572400612 Test RE 0.019531537587854118\n",
      "75 Train Loss 0.007893832 Test MSE 0.001732290373141424 Test RE 0.01989383017431369\n",
      "76 Train Loss 0.0074241227 Test MSE 0.0013639213697150103 Test RE 0.017652358025226484\n",
      "77 Train Loss 0.0072165322 Test MSE 0.0012669227849705404 Test RE 0.017013087423227054\n",
      "78 Train Loss 0.006398742 Test MSE 0.0011850062307346098 Test RE 0.016453881954474562\n",
      "79 Train Loss 0.006027609 Test MSE 0.001229265443295362 Test RE 0.016758336121494858\n",
      "80 Train Loss 0.0054418077 Test MSE 0.001045533825462814 Test RE 0.01545528806760409\n",
      "81 Train Loss 0.0047433814 Test MSE 0.0009382192885231094 Test RE 0.014640645967817043\n",
      "82 Train Loss 0.0043420345 Test MSE 0.0007210304020832586 Test RE 0.012834674198139202\n",
      "83 Train Loss 0.0040858365 Test MSE 0.0006058797048596302 Test RE 0.011765252788478923\n",
      "84 Train Loss 0.0038013237 Test MSE 0.0006974541597561445 Test RE 0.012623096279439274\n",
      "85 Train Loss 0.0035715098 Test MSE 0.0006704189104923103 Test RE 0.012376025292458459\n",
      "86 Train Loss 0.0033829825 Test MSE 0.0005936074590697045 Test RE 0.011645489148320313\n",
      "87 Train Loss 0.0032431418 Test MSE 0.0005262900938762802 Test RE 0.010965303535002796\n",
      "88 Train Loss 0.0031621908 Test MSE 0.0005915502285971752 Test RE 0.011625292090644187\n",
      "89 Train Loss 0.003007276 Test MSE 0.0005027115244835138 Test RE 0.010716858115775258\n",
      "90 Train Loss 0.0026972622 Test MSE 0.0004571575562021399 Test RE 0.010219767401521598\n",
      "91 Train Loss 0.0025598914 Test MSE 0.0004659192431005175 Test RE 0.01031723645873381\n",
      "92 Train Loss 0.0024999874 Test MSE 0.0004553364194990757 Test RE 0.010199391312802139\n",
      "93 Train Loss 0.0023990416 Test MSE 0.00044752584591153954 Test RE 0.010111535742432029\n",
      "94 Train Loss 0.002264016 Test MSE 0.0004192152830887273 Test RE 0.009786482274234362\n",
      "95 Train Loss 0.0021370328 Test MSE 0.00044076652156209276 Test RE 0.010034884088996845\n",
      "96 Train Loss 0.0020187565 Test MSE 0.0004505299325653476 Test RE 0.010145416605898015\n",
      "97 Train Loss 0.0019647558 Test MSE 0.0004411384519613348 Test RE 0.010039117045955153\n",
      "98 Train Loss 0.0019190677 Test MSE 0.00042499039225841074 Test RE 0.009853660985399916\n",
      "99 Train Loss 0.0018793672 Test MSE 0.00041228847859384334 Test RE 0.009705293188510641\n",
      "Training time: 114.32\n",
      "KG_stan\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 57.648827 Test MSE 8.463381207849917 Test RE 1.390528875199196\n",
      "1 Train Loss 48.464928 Test MSE 9.721225012842018 Test RE 1.4902823505158351\n",
      "2 Train Loss 45.766415 Test MSE 8.70777718423517 Test RE 1.4104630524212765\n",
      "3 Train Loss 44.87986 Test MSE 8.615378049764374 Test RE 1.4029598094266238\n",
      "4 Train Loss 44.394268 Test MSE 8.46364913313033 Test RE 1.390550885013075\n",
      "5 Train Loss 44.16092 Test MSE 8.45418206559833 Test RE 1.389772962731583\n",
      "6 Train Loss 43.960346 Test MSE 8.308888172952347 Test RE 1.3777788612834632\n",
      "7 Train Loss 42.910767 Test MSE 8.408898609797067 Test RE 1.3860459184077956\n",
      "8 Train Loss 41.73985 Test MSE 8.425795807177188 Test RE 1.387437809244039\n",
      "9 Train Loss 40.196503 Test MSE 8.502649014957626 Test RE 1.3937509816477225\n",
      "10 Train Loss 36.820053 Test MSE 8.659272264203883 Test RE 1.4065292163487908\n",
      "11 Train Loss 33.192844 Test MSE 7.868160779086584 Test RE 1.3407403295030282\n",
      "12 Train Loss 31.63162 Test MSE 7.669419786737375 Test RE 1.3236992264789353\n",
      "13 Train Loss 30.751947 Test MSE 7.778791763160261 Test RE 1.333104311800348\n",
      "14 Train Loss 28.764662 Test MSE 7.176261820134295 Test RE 1.2804340006551806\n",
      "15 Train Loss 25.300392 Test MSE 6.451713300409107 Test RE 1.2140752069360221\n",
      "16 Train Loss 20.47416 Test MSE 5.568333459063308 Test RE 1.1279002107620948\n",
      "17 Train Loss 18.892876 Test MSE 5.4717478720310515 Test RE 1.1180754178526482\n",
      "18 Train Loss 17.546103 Test MSE 5.735181415758078 Test RE 1.1446735306159768\n",
      "19 Train Loss 16.31752 Test MSE 5.944298646738021 Test RE 1.165355340062742\n",
      "20 Train Loss 15.561135 Test MSE 5.741072264449666 Test RE 1.145261251163881\n",
      "21 Train Loss 15.126475 Test MSE 5.712453181069041 Test RE 1.1424031372851176\n",
      "22 Train Loss 14.677248 Test MSE 5.7696999721964515 Test RE 1.1481131081499403\n",
      "23 Train Loss 14.263722 Test MSE 5.812285998290646 Test RE 1.152342416940105\n",
      "24 Train Loss 13.924799 Test MSE 5.7784543996147875 Test RE 1.148983800140983\n",
      "25 Train Loss 13.597607 Test MSE 5.774287515647995 Test RE 1.1485694552952086\n",
      "26 Train Loss 13.253759 Test MSE 5.890731822860149 Test RE 1.1600926792366206\n",
      "27 Train Loss 13.023893 Test MSE 5.885217412409175 Test RE 1.1595495612314801\n",
      "28 Train Loss 12.878921 Test MSE 5.8910755028252675 Test RE 1.1601265200910675\n",
      "29 Train Loss 12.689518 Test MSE 5.894498422153833 Test RE 1.1604635080340824\n",
      "30 Train Loss 12.560141 Test MSE 5.8396488889598 Test RE 1.1550517120460186\n",
      "31 Train Loss 12.389967 Test MSE 5.821417230470471 Test RE 1.153247239647638\n",
      "32 Train Loss 12.016563 Test MSE 5.848650085348394 Test RE 1.155941563831072\n",
      "33 Train Loss 10.601198 Test MSE 4.61314667186257 Test RE 1.0266128399951129\n",
      "34 Train Loss 7.947442 Test MSE 4.1042410884818405 Test RE 0.9683324642354163\n",
      "35 Train Loss 4.3579426 Test MSE 2.98987950879614 Test RE 0.826484862503247\n",
      "36 Train Loss 3.4191942 Test MSE 2.9646532782540955 Test RE 0.8229908653103081\n",
      "37 Train Loss 2.922762 Test MSE 2.685657804005492 Test RE 0.7833095080217775\n",
      "38 Train Loss 2.555442 Test MSE 2.317091069827144 Test RE 0.727578089291709\n",
      "39 Train Loss 2.231644 Test MSE 2.254384071214174 Test RE 0.7176654087409629\n",
      "40 Train Loss 1.8548427 Test MSE 1.804159330565848 Test RE 0.6420154872714631\n",
      "41 Train Loss 1.3671448 Test MSE 1.6923508477552338 Test RE 0.621803642690737\n",
      "42 Train Loss 1.0743754 Test MSE 1.453220613190314 Test RE 0.5762007709414384\n",
      "43 Train Loss 0.92208374 Test MSE 1.2092860121456548 Test RE 0.5256208388226885\n",
      "44 Train Loss 0.72802764 Test MSE 1.0591474762277988 Test RE 0.4919107098891666\n",
      "45 Train Loss 0.6068555 Test MSE 0.8327146606460832 Test RE 0.4361703760515784\n",
      "46 Train Loss 0.46293703 Test MSE 0.5649924762345421 Test RE 0.35927698087758664\n",
      "47 Train Loss 0.33724135 Test MSE 0.42098901375570025 Test RE 0.3101297589108529\n",
      "48 Train Loss 0.25594452 Test MSE 0.5181495699262606 Test RE 0.34406114578855024\n",
      "49 Train Loss 0.2154144 Test MSE 0.4967484104556242 Test RE 0.33688083380357786\n",
      "50 Train Loss 0.15276839 Test MSE 0.236144382107519 Test RE 0.2322720052756622\n",
      "51 Train Loss 0.11834169 Test MSE 0.17313494570896468 Test RE 0.19888426644134216\n",
      "52 Train Loss 0.09825717 Test MSE 0.15124613555372873 Test RE 0.18588750732501177\n",
      "53 Train Loss 0.088957235 Test MSE 0.09593381209185636 Test RE 0.1480450569539871\n",
      "54 Train Loss 0.076721504 Test MSE 0.09679234460259759 Test RE 0.14870602513775746\n",
      "55 Train Loss 0.06504195 Test MSE 0.07575199222831341 Test RE 0.13155431263678627\n",
      "56 Train Loss 0.05529526 Test MSE 0.04625879522027924 Test RE 0.10280286523404111\n",
      "57 Train Loss 0.049105063 Test MSE 0.03149623599171268 Test RE 0.08482764070014048\n",
      "58 Train Loss 0.042720698 Test MSE 0.01706941927837894 Test RE 0.06244784174611738\n",
      "59 Train Loss 0.037726995 Test MSE 0.01501868280170494 Test RE 0.058576576090696074\n",
      "60 Train Loss 0.032348216 Test MSE 0.013676944455074565 Test RE 0.055898815063677144\n",
      "61 Train Loss 0.028137334 Test MSE 0.010799321264211084 Test RE 0.04967138740987989\n",
      "62 Train Loss 0.02524609 Test MSE 0.010892481416926352 Test RE 0.04988517201745322\n",
      "63 Train Loss 0.022104654 Test MSE 0.008527145781364067 Test RE 0.044137720905681896\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 Train Loss 0.019730182 Test MSE 0.00710995789084299 Test RE 0.04030338976912711\n",
      "65 Train Loss 0.018282093 Test MSE 0.0067219857349250225 Test RE 0.03918833875238671\n",
      "66 Train Loss 0.016354254 Test MSE 0.006265389449282418 Test RE 0.03783398575433025\n",
      "67 Train Loss 0.01463763 Test MSE 0.0059584199619066094 Test RE 0.03689551827067784\n",
      "68 Train Loss 0.013594089 Test MSE 0.005190394077264918 Test RE 0.034435645364866284\n",
      "69 Train Loss 0.012493033 Test MSE 0.004197800051135859 Test RE 0.030968411884487106\n",
      "70 Train Loss 0.011528874 Test MSE 0.0035950690234649424 Test RE 0.028659042519128455\n",
      "71 Train Loss 0.010350712 Test MSE 0.004290678343399841 Test RE 0.031309132851535824\n",
      "72 Train Loss 0.0092675 Test MSE 0.0036762222861260463 Test RE 0.028980704726252995\n",
      "73 Train Loss 0.008451585 Test MSE 0.0028425784630420996 Test RE 0.02548380603692025\n",
      "74 Train Loss 0.007730039 Test MSE 0.002389183325569727 Test RE 0.023363225341160056\n",
      "75 Train Loss 0.007295138 Test MSE 0.0022224513842829976 Test RE 0.022533268678318546\n",
      "76 Train Loss 0.006680719 Test MSE 0.001881372299040194 Test RE 0.02073220206042598\n",
      "77 Train Loss 0.006093134 Test MSE 0.0017186791342276748 Test RE 0.01981551949287415\n",
      "78 Train Loss 0.005858032 Test MSE 0.0016213101008984826 Test RE 0.019246027609594155\n",
      "79 Train Loss 0.005521005 Test MSE 0.001520079684634056 Test RE 0.01863550807191343\n",
      "80 Train Loss 0.0049446896 Test MSE 0.0010404411927940956 Test RE 0.015417601968471735\n",
      "81 Train Loss 0.0045664394 Test MSE 0.0008563266027993933 Test RE 0.013987103171541981\n",
      "82 Train Loss 0.0041849897 Test MSE 0.0009470227854826671 Test RE 0.014709173627929667\n",
      "83 Train Loss 0.003997304 Test MSE 0.000905456073137041 Test RE 0.014382744093231226\n",
      "84 Train Loss 0.0037348557 Test MSE 0.0009421178417463845 Test RE 0.014671032343463937\n",
      "85 Train Loss 0.0035211518 Test MSE 0.0010285563000054052 Test RE 0.015329291916580908\n",
      "86 Train Loss 0.0033617395 Test MSE 0.0009490137989228389 Test RE 0.01472462773644981\n",
      "87 Train Loss 0.0031192624 Test MSE 0.0009950864028313439 Test RE 0.015077816593514882\n",
      "88 Train Loss 0.002807338 Test MSE 0.0008363462807594176 Test RE 0.013822962353976401\n",
      "89 Train Loss 0.0027360197 Test MSE 0.0008308631651000985 Test RE 0.013777575928993776\n",
      "90 Train Loss 0.0026544675 Test MSE 0.0008175511128618709 Test RE 0.01366675840758438\n",
      "91 Train Loss 0.0025086836 Test MSE 0.0008012777056362134 Test RE 0.013530055869686048\n",
      "92 Train Loss 0.0022377975 Test MSE 0.0008075201835612714 Test RE 0.013582657614633818\n",
      "93 Train Loss 0.0021596246 Test MSE 0.0008313278390290051 Test RE 0.013781428058766507\n",
      "94 Train Loss 0.0020344649 Test MSE 0.0007875280286088992 Test RE 0.01341346776938267\n",
      "95 Train Loss 0.0019191486 Test MSE 0.0007859418349629429 Test RE 0.01339995264307656\n",
      "96 Train Loss 0.0018102129 Test MSE 0.0007619464491280877 Test RE 0.01319381180353216\n",
      "97 Train Loss 0.0017181853 Test MSE 0.0008799298931849342 Test RE 0.014178559049064473\n",
      "98 Train Loss 0.0016317691 Test MSE 0.0008395755241041208 Test RE 0.01384962278469415\n",
      "99 Train Loss 0.0015566649 Test MSE 0.0009218937947317436 Test RE 0.0145127096353756\n",
      "Training time: 114.03\n",
      "KG_stan\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 58.576424 Test MSE 8.197129530013331 Test RE 1.3684815896969995\n",
      "1 Train Loss 53.096066 Test MSE 9.013428928119177 Test RE 1.4350038940856968\n",
      "2 Train Loss 47.1641 Test MSE 8.770554898774058 Test RE 1.415538206952144\n",
      "3 Train Loss 46.015564 Test MSE 8.558662544123226 Test RE 1.3983343037285982\n",
      "4 Train Loss 45.66635 Test MSE 8.166702886215019 Test RE 1.3659394185292937\n",
      "5 Train Loss 44.88864 Test MSE 8.456942779652763 Test RE 1.3899998594643468\n",
      "6 Train Loss 44.520203 Test MSE 8.448557848593314 Test RE 1.3893106066407601\n",
      "7 Train Loss 44.089775 Test MSE 8.561332390168094 Test RE 1.3985523895677023\n",
      "8 Train Loss 43.455105 Test MSE 8.706729724123345 Test RE 1.410378217440585\n",
      "9 Train Loss 42.24821 Test MSE 8.933659356049413 Test RE 1.4286398321501974\n",
      "10 Train Loss 38.726154 Test MSE 8.266810676340777 Test RE 1.374285790533652\n",
      "11 Train Loss 34.694324 Test MSE 8.486216269253378 Test RE 1.3924035055766513\n",
      "12 Train Loss 31.324038 Test MSE 8.575936644031708 Test RE 1.399744733835456\n",
      "13 Train Loss 25.75526 Test MSE 7.999725365567143 Test RE 1.3519032096456998\n",
      "14 Train Loss 22.677816 Test MSE 7.257035683537849 Test RE 1.2876199282433292\n",
      "15 Train Loss 20.722874 Test MSE 7.725300988019606 Test RE 1.3285128664332888\n",
      "16 Train Loss 19.81751 Test MSE 8.28025397105308 Test RE 1.3754027523736096\n",
      "17 Train Loss 19.032742 Test MSE 8.330275137356832 Test RE 1.3795509136699027\n",
      "18 Train Loss 17.909649 Test MSE 8.573618899682948 Test RE 1.3995555725922155\n",
      "19 Train Loss 16.996124 Test MSE 8.437705598702191 Test RE 1.3884180288218584\n",
      "20 Train Loss 16.374912 Test MSE 8.413960260108835 Test RE 1.3864630137162333\n",
      "21 Train Loss 15.847838 Test MSE 8.440539428649835 Test RE 1.3886511615285175\n",
      "22 Train Loss 15.561792 Test MSE 8.202896116616277 Test RE 1.3689628606206417\n",
      "23 Train Loss 13.254991 Test MSE 6.811301192533121 Test RE 1.2474498719732414\n",
      "24 Train Loss 11.812719 Test MSE 6.713935337387622 Test RE 1.2385017864987653\n",
      "25 Train Loss 11.191369 Test MSE 6.475388559436413 Test RE 1.216300757154365\n",
      "26 Train Loss 10.787865 Test MSE 6.304967657749364 Test RE 1.2001885849340062\n",
      "27 Train Loss 10.201754 Test MSE 5.9986804085167424 Test RE 1.1706738640927603\n",
      "28 Train Loss 7.8479934 Test MSE 5.01291427297297 Test RE 1.070171051874393\n",
      "29 Train Loss 5.706972 Test MSE 4.2273977172196355 Test RE 0.9827535347441803\n",
      "30 Train Loss 5.272437 Test MSE 4.175118434244189 Test RE 0.9766578835401531\n",
      "31 Train Loss 4.97332 Test MSE 4.396916425234701 Test RE 1.002264079228271\n",
      "32 Train Loss 4.7631626 Test MSE 4.261270087293687 Test RE 0.9866828759567764\n",
      "33 Train Loss 4.4995575 Test MSE 3.7314115601094238 Test RE 0.9233038290034216\n",
      "34 Train Loss 3.493873 Test MSE 3.2310819856276645 Test RE 0.8591758263180309\n",
      "35 Train Loss 2.7266304 Test MSE 3.1504273050266383 Test RE 0.8483846296519849\n",
      "36 Train Loss 2.1448498 Test MSE 3.063126615341668 Test RE 0.8365473633330293\n",
      "37 Train Loss 1.6155714 Test MSE 2.9026926258369254 Test RE 0.8143452829466059\n",
      "38 Train Loss 1.4886175 Test MSE 2.9453503558936593 Test RE 0.8203072341157779\n",
      "39 Train Loss 1.421505 Test MSE 2.9202993366733714 Test RE 0.8168113148742998\n",
      "40 Train Loss 1.3360252 Test MSE 2.878030882279179 Test RE 0.8108784993803352\n",
      "41 Train Loss 1.2520785 Test MSE 2.7817082968304607 Test RE 0.7971936913267755\n",
      "42 Train Loss 1.2073171 Test MSE 2.8494004753059565 Test RE 0.8068351432574862\n",
      "43 Train Loss 1.0981386 Test MSE 2.8064082134095534 Test RE 0.8007251724026124\n",
      "44 Train Loss 1.0525244 Test MSE 2.8366179395628106 Test RE 0.8050233600851268\n",
      "45 Train Loss 0.93845105 Test MSE 2.932515621198955 Test RE 0.8185179867872743\n",
      "46 Train Loss 0.8880482 Test MSE 2.884255447167402 Test RE 0.8117549040987699\n",
      "47 Train Loss 0.8415183 Test MSE 2.8849969167980096 Test RE 0.8118592382921309\n",
      "48 Train Loss 0.8014102 Test MSE 2.948743181835054 Test RE 0.8207795647879158\n",
      "49 Train Loss 0.7743035 Test MSE 2.9728128116330086 Test RE 0.8241226346310538\n",
      "50 Train Loss 0.74411803 Test MSE 3.023305025106552 Test RE 0.8310918878151283\n",
      "51 Train Loss 0.7198978 Test MSE 2.945384000349645 Test RE 0.8203119192480176\n",
      "52 Train Loss 0.6859554 Test MSE 2.9396153894487167 Test RE 0.8195082244638491\n",
      "53 Train Loss 0.668507 Test MSE 2.985819940108823 Test RE 0.82592358374263\n",
      "54 Train Loss 0.6491808 Test MSE 2.976843462596455 Test RE 0.824681133555444\n",
      "55 Train Loss 0.6418343 Test MSE 2.9727849170831977 Test RE 0.8241187681609359\n",
      "56 Train Loss 0.6217688 Test MSE 2.999896931270355 Test RE 0.8278682501643003\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 Train Loss 0.61078215 Test MSE 3.0167711422524732 Test RE 0.8301933357180579\n",
      "58 Train Loss 0.5971455 Test MSE 3.0132374384537526 Test RE 0.8297069685399902\n",
      "59 Train Loss 0.5867683 Test MSE 3.009315245633931 Test RE 0.8291667969712835\n",
      "60 Train Loss 0.5786018 Test MSE 3.0254716114324074 Test RE 0.8313896265189943\n",
      "61 Train Loss 0.5651484 Test MSE 3.0477265605007946 Test RE 0.834441817254833\n",
      "62 Train Loss 0.5559639 Test MSE 3.0636847005194126 Test RE 0.836623567075065\n",
      "63 Train Loss 0.54861784 Test MSE 3.0859726795613467 Test RE 0.8396612259814482\n",
      "64 Train Loss 0.5380481 Test MSE 3.1101798722628162 Test RE 0.8429480561729955\n",
      "65 Train Loss 0.53154933 Test MSE 3.1174229743053465 Test RE 0.8439290296951621\n",
      "66 Train Loss 0.52429485 Test MSE 3.1217519995332546 Test RE 0.844514789575266\n",
      "67 Train Loss 0.5179622 Test MSE 3.16613690726792 Test RE 0.8504972338069687\n",
      "68 Train Loss 0.51169264 Test MSE 3.1614880807811603 Test RE 0.8498726135532951\n",
      "69 Train Loss 0.50655144 Test MSE 3.15209183517053 Test RE 0.8486087223183266\n",
      "70 Train Loss 0.50001144 Test MSE 3.1475354106376594 Test RE 0.8479951583522509\n",
      "71 Train Loss 0.4952643 Test MSE 3.1628988389981583 Test RE 0.8500622127258942\n",
      "72 Train Loss 0.48881912 Test MSE 3.142229880021531 Test RE 0.8472801605509429\n",
      "73 Train Loss 0.4847164 Test MSE 3.133961038176513 Test RE 0.846164608679669\n",
      "74 Train Loss 0.47861293 Test MSE 3.1371078270184003 Test RE 0.8465893161428234\n",
      "75 Train Loss 0.47229743 Test MSE 3.153804927548602 Test RE 0.8488392910555703\n",
      "76 Train Loss 0.4660286 Test MSE 3.1449177433615914 Test RE 0.8476424648000225\n",
      "77 Train Loss 0.46126503 Test MSE 3.154008161198531 Test RE 0.8488666405480348\n",
      "78 Train Loss 0.45633695 Test MSE 3.157611815953109 Test RE 0.8493514441480714\n",
      "79 Train Loss 0.45159605 Test MSE 3.1689631724384055 Test RE 0.8508767490639114\n",
      "80 Train Loss 0.44789895 Test MSE 3.187755611711217 Test RE 0.8533959348248598\n",
      "81 Train Loss 0.44151667 Test MSE 3.219611569488228 Test RE 0.8576494233467375\n",
      "82 Train Loss 0.43730676 Test MSE 3.241726947589954 Test RE 0.8605899615074442\n",
      "83 Train Loss 0.43279743 Test MSE 3.2505791805138458 Test RE 0.8617641734720212\n",
      "84 Train Loss 0.42923898 Test MSE 3.2647811884553666 Test RE 0.863644675692777\n",
      "85 Train Loss 0.42582557 Test MSE 3.2699153518944444 Test RE 0.8643234885716286\n",
      "86 Train Loss 0.42313075 Test MSE 3.2643869067654427 Test RE 0.8635925237193118\n",
      "87 Train Loss 0.4205302 Test MSE 3.2649762948203835 Test RE 0.8636704814123981\n",
      "88 Train Loss 0.418563 Test MSE 3.2767673818260183 Test RE 0.8652285993278761\n",
      "89 Train Loss 0.41486725 Test MSE 3.296130443125588 Test RE 0.8677812366412518\n",
      "90 Train Loss 0.41173205 Test MSE 3.289703960565895 Test RE 0.8669348651153063\n",
      "91 Train Loss 0.40898535 Test MSE 3.2879751198660854 Test RE 0.8667070346989543\n",
      "92 Train Loss 0.407102 Test MSE 3.2766490563296333 Test RE 0.865212977296187\n",
      "93 Train Loss 0.40379548 Test MSE 3.2834805241450495 Test RE 0.8661144465915485\n",
      "94 Train Loss 0.4011747 Test MSE 3.292407373420524 Test RE 0.8672910068208212\n",
      "95 Train Loss 0.39883494 Test MSE 3.306843894284742 Test RE 0.8691903722666012\n",
      "96 Train Loss 0.3967923 Test MSE 3.317419831614932 Test RE 0.8705791836333082\n",
      "97 Train Loss 0.39366063 Test MSE 3.3105707636790163 Test RE 0.8696800305193777\n",
      "98 Train Loss 0.39158338 Test MSE 3.3205496164790915 Test RE 0.8709897562077504\n",
      "99 Train Loss 0.39031065 Test MSE 3.31377066833608 Test RE 0.8701002331425253\n",
      "Training time: 98.38\n",
      "KG_stan\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 55.56257 Test MSE 8.442561893135448 Test RE 1.3888175211233467\n",
      "1 Train Loss 54.37904 Test MSE 8.630649194549198 Test RE 1.4042026635737157\n",
      "2 Train Loss 44.781067 Test MSE 8.060325526167027 Test RE 1.357014071614438\n",
      "3 Train Loss 44.163055 Test MSE 8.000350182978526 Test RE 1.351956003718764\n",
      "4 Train Loss 43.541565 Test MSE 7.970177686116653 Test RE 1.349404214069326\n",
      "5 Train Loss 43.213448 Test MSE 8.024447679475301 Test RE 1.3539905558910699\n",
      "6 Train Loss 42.903637 Test MSE 8.097586056256597 Test RE 1.360146994943005\n",
      "7 Train Loss 41.699524 Test MSE 8.041716539096907 Test RE 1.3554466876811573\n",
      "8 Train Loss 39.684032 Test MSE 6.94448009434579 Test RE 1.2595863018120266\n",
      "9 Train Loss 36.924484 Test MSE 6.538647082875122 Test RE 1.2222273814866602\n",
      "10 Train Loss 32.58287 Test MSE 5.602118005244958 Test RE 1.1313166703315531\n",
      "11 Train Loss 30.138458 Test MSE 5.267493161469109 Test RE 1.0970086461446185\n",
      "12 Train Loss 27.757994 Test MSE 4.9883166736850395 Test RE 1.0675422407429376\n",
      "13 Train Loss 25.214382 Test MSE 4.112429135549748 Test RE 0.9692979047464516\n",
      "14 Train Loss 19.87549 Test MSE 3.660195795047507 Test RE 0.9144505367330613\n",
      "15 Train Loss 16.033575 Test MSE 4.214221621255309 Test RE 0.9812207997897388\n",
      "16 Train Loss 13.437563 Test MSE 4.0369643865608 Test RE 0.9603632205898855\n",
      "17 Train Loss 11.963406 Test MSE 3.767133473074237 Test RE 0.9277128330806887\n",
      "18 Train Loss 10.984763 Test MSE 3.8569314884855963 Test RE 0.9387047652177453\n",
      "19 Train Loss 10.506083 Test MSE 3.9086059290687296 Test RE 0.9449721374876854\n",
      "20 Train Loss 9.958502 Test MSE 3.9363951127664802 Test RE 0.9483254422867232\n",
      "21 Train Loss 9.373708 Test MSE 3.9145142548398577 Test RE 0.9456860870399525\n",
      "22 Train Loss 8.942785 Test MSE 3.8207993539094316 Test RE 0.9342974771021094\n",
      "23 Train Loss 8.595694 Test MSE 3.673347035083006 Test RE 0.9160918936330983\n",
      "24 Train Loss 8.129481 Test MSE 3.5333003404921333 Test RE 0.8984591540667409\n",
      "25 Train Loss 7.925227 Test MSE 3.5676422141750868 Test RE 0.9028148777263532\n",
      "26 Train Loss 7.617882 Test MSE 3.6530172449221023 Test RE 0.9135533646744277\n",
      "27 Train Loss 6.7239304 Test MSE 3.471507502716154 Test RE 0.8905680582962321\n",
      "28 Train Loss 6.015194 Test MSE 3.2745227018806244 Test RE 0.8649321953141325\n",
      "29 Train Loss 5.4368935 Test MSE 2.846752279114657 Test RE 0.806460124995172\n",
      "30 Train Loss 4.283201 Test MSE 1.8681521192093247 Test RE 0.6533022873249995\n",
      "31 Train Loss 2.7779112 Test MSE 1.4350554670085045 Test RE 0.572588213551292\n",
      "32 Train Loss 2.0528643 Test MSE 0.7418047474445748 Test RE 0.4116734552005904\n",
      "33 Train Loss 1.4356029 Test MSE 0.20963413087658614 Test RE 0.2188462233799721\n",
      "34 Train Loss 1.0177361 Test MSE 0.12501129186823703 Test RE 0.1689984346140241\n",
      "35 Train Loss 0.6729487 Test MSE 0.10508641609950158 Test RE 0.15494635019483144\n",
      "36 Train Loss 0.44506755 Test MSE 0.06712070610541075 Test RE 0.12383297839551939\n",
      "37 Train Loss 0.32002765 Test MSE 0.06144098439998843 Test RE 0.11847784411260073\n",
      "38 Train Loss 0.25799215 Test MSE 0.05039803278586458 Test RE 0.10730373836117785\n",
      "39 Train Loss 0.18498929 Test MSE 0.042267272965930844 Test RE 0.0982675604489134\n",
      "40 Train Loss 0.15833664 Test MSE 0.04426138462168131 Test RE 0.1005589102220781\n",
      "41 Train Loss 0.11585902 Test MSE 0.030898635884352356 Test RE 0.08401903972509424\n",
      "42 Train Loss 0.09721021 Test MSE 0.020437385060616167 Test RE 0.06833146376129956\n",
      "43 Train Loss 0.07952867 Test MSE 0.012646410840492528 Test RE 0.05375163803442839\n",
      "44 Train Loss 0.07258055 Test MSE 0.008783979526280346 Test RE 0.04479749376636142\n",
      "45 Train Loss 0.06285388 Test MSE 0.00862294300491316 Test RE 0.044384958490354724\n",
      "46 Train Loss 0.056596994 Test MSE 0.008437078022886018 Test RE 0.04390400032824937\n",
      "47 Train Loss 0.04832631 Test MSE 0.00678426304313611 Test RE 0.03936945469360903\n",
      "48 Train Loss 0.043245047 Test MSE 0.005874501058845415 Test RE 0.03663477710161953\n",
      "49 Train Loss 0.03768702 Test MSE 0.00479201900839164 Test RE 0.033087756904025156\n",
      "50 Train Loss 0.033015586 Test MSE 0.004787271179499431 Test RE 0.033071361524665714\n",
      "51 Train Loss 0.024998192 Test MSE 0.0032510959327851775 Test RE 0.02725354196284857\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 Train Loss 0.021409215 Test MSE 0.0026786038857944667 Test RE 0.02473787026263652\n",
      "53 Train Loss 0.020062305 Test MSE 0.0026283440902411075 Test RE 0.02450468761228015\n",
      "54 Train Loss 0.019005364 Test MSE 0.0019011591616242506 Test RE 0.020840939777601344\n",
      "55 Train Loss 0.016946737 Test MSE 0.0017810513287692205 Test RE 0.02017187545819061\n",
      "56 Train Loss 0.016248101 Test MSE 0.0015450067778606817 Test RE 0.018787684338791386\n",
      "57 Train Loss 0.014970676 Test MSE 0.0013600527085162813 Test RE 0.017627305449450608\n",
      "58 Train Loss 0.013138995 Test MSE 0.0012435510994425315 Test RE 0.016855431624979655\n",
      "59 Train Loss 0.012793303 Test MSE 0.0013212147495293924 Test RE 0.017373797966295824\n",
      "60 Train Loss 0.011892842 Test MSE 0.0010440688379475796 Test RE 0.015444456404190213\n",
      "61 Train Loss 0.011279946 Test MSE 0.0010574958375493624 Test RE 0.015543449036915584\n",
      "62 Train Loss 0.011073942 Test MSE 0.000983795634607092 Test RE 0.014992032183470535\n",
      "63 Train Loss 0.01016727 Test MSE 0.0008286843218847717 Test RE 0.013759499014883854\n",
      "64 Train Loss 0.009961441 Test MSE 0.0007527961577659418 Test RE 0.013114349610676907\n",
      "65 Train Loss 0.009487642 Test MSE 0.0007431795180468441 Test RE 0.01303031534960916\n",
      "66 Train Loss 0.0089515615 Test MSE 0.0007859567738269925 Test RE 0.01340007999290747\n",
      "67 Train Loss 0.008452326 Test MSE 0.0007630990858464183 Test RE 0.013203787520946707\n",
      "68 Train Loss 0.00815784 Test MSE 0.0008514632417028375 Test RE 0.01394732793182005\n",
      "69 Train Loss 0.0078123976 Test MSE 0.0008699388947022191 Test RE 0.014097835363363329\n",
      "70 Train Loss 0.0073475 Test MSE 0.0008025775218141562 Test RE 0.013541025511680368\n",
      "71 Train Loss 0.0069990326 Test MSE 0.0008879252624580743 Test RE 0.014242829205939113\n",
      "72 Train Loss 0.0066778082 Test MSE 0.0008406991508631105 Test RE 0.01385888734940577\n",
      "73 Train Loss 0.0063902903 Test MSE 0.0008171754484710371 Test RE 0.013663618111945691\n",
      "74 Train Loss 0.0055541745 Test MSE 0.0008745955785121394 Test RE 0.014135517057629313\n",
      "75 Train Loss 0.005175892 Test MSE 0.0007872791640900721 Test RE 0.013411348225797308\n",
      "76 Train Loss 0.0048484183 Test MSE 0.0007460564922765873 Test RE 0.013055512270488378\n",
      "77 Train Loss 0.0046084323 Test MSE 0.0006353552203068318 Test RE 0.012048038913661694\n",
      "78 Train Loss 0.0044801636 Test MSE 0.0005789715704870545 Test RE 0.011501028501929288\n",
      "79 Train Loss 0.0043802587 Test MSE 0.0005925329882169757 Test RE 0.011634944801355955\n",
      "80 Train Loss 0.0042376546 Test MSE 0.0005259399955501912 Test RE 0.01096165576250709\n",
      "81 Train Loss 0.0039738254 Test MSE 0.0004778698330670189 Test RE 0.010448714625383682\n",
      "82 Train Loss 0.0038007349 Test MSE 0.0004272409996082093 Test RE 0.009879717385254689\n",
      "83 Train Loss 0.0036729495 Test MSE 0.00043309719808173885 Test RE 0.00994719764757285\n",
      "84 Train Loss 0.0035043266 Test MSE 0.00037837500203923827 Test RE 0.009297566181836352\n",
      "85 Train Loss 0.003250368 Test MSE 0.0003650928059516373 Test RE 0.009132920936223473\n",
      "86 Train Loss 0.0031497416 Test MSE 0.00033737212492014927 Test RE 0.008779355972992826\n",
      "87 Train Loss 0.003057399 Test MSE 0.0003006107666692785 Test RE 0.008287247816242566\n",
      "88 Train Loss 0.0029850306 Test MSE 0.0002969110053701702 Test RE 0.008236092357715954\n",
      "89 Train Loss 0.0029016794 Test MSE 0.00028945797304170685 Test RE 0.008132064574168256\n",
      "90 Train Loss 0.0028397287 Test MSE 0.0002897403071381684 Test RE 0.008136029570346127\n",
      "91 Train Loss 0.002734841 Test MSE 0.00030849116659381604 Test RE 0.00839516868159534\n",
      "92 Train Loss 0.0023651153 Test MSE 0.00022111762258336266 Test RE 0.007107546970297472\n",
      "93 Train Loss 0.0022021811 Test MSE 0.0002127339851154248 Test RE 0.006971504277167258\n",
      "94 Train Loss 0.0021259245 Test MSE 0.00021796651269430326 Test RE 0.007056721016865488\n",
      "95 Train Loss 0.0020960313 Test MSE 0.00020301779964834483 Test RE 0.006810439174729923\n",
      "96 Train Loss 0.0020748028 Test MSE 0.00020653380125858713 Test RE 0.006869159954621925\n",
      "97 Train Loss 0.0020520003 Test MSE 0.0002063742287660428 Test RE 0.006866505810907806\n",
      "98 Train Loss 0.001950453 Test MSE 0.00020511112296030495 Test RE 0.006845460462713898\n",
      "99 Train Loss 0.0018219529 Test MSE 0.0002257490957315575 Test RE 0.007181597648319921\n",
      "Training time: 111.10\n",
      "KG_stan\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 56.635925 Test MSE 8.63794673227667 Test RE 1.4047961911575702\n",
      "1 Train Loss 46.16784 Test MSE 8.794840964722209 Test RE 1.4174966969835183\n",
      "2 Train Loss 42.785408 Test MSE 8.227943070140295 Test RE 1.3710512824591776\n",
      "3 Train Loss 41.60066 Test MSE 8.285016969593437 Test RE 1.3757982776508004\n",
      "4 Train Loss 39.900635 Test MSE 8.096347760832383 Test RE 1.3600429930723623\n",
      "5 Train Loss 39.296043 Test MSE 8.052596236593354 Test RE 1.356363274671855\n",
      "6 Train Loss 38.999535 Test MSE 7.866334618326045 Test RE 1.3405847309120218\n",
      "7 Train Loss 38.318016 Test MSE 7.0913821829308095 Test RE 1.2728390953019002\n",
      "8 Train Loss 36.47049 Test MSE 5.397124417016307 Test RE 1.1104251131778564\n",
      "9 Train Loss 25.094143 Test MSE 3.6775512504388344 Test RE 0.9166159859916843\n",
      "10 Train Loss 20.096466 Test MSE 3.442558368545129 Test RE 0.8868470311630043\n",
      "11 Train Loss 17.22741 Test MSE 3.34902341505056 Test RE 0.874716164555862\n",
      "12 Train Loss 16.37725 Test MSE 3.015251135466167 Test RE 0.8299841619959732\n",
      "13 Train Loss 15.86606 Test MSE 2.964217162681785 Test RE 0.822930330013391\n",
      "14 Train Loss 15.100178 Test MSE 2.9372659239842585 Test RE 0.8191806661124122\n",
      "15 Train Loss 14.7956295 Test MSE 2.8353633833111243 Test RE 0.8048453208138611\n",
      "16 Train Loss 14.286583 Test MSE 2.8038947802537724 Test RE 0.8003665253593896\n",
      "17 Train Loss 13.489893 Test MSE 2.667656961343707 Test RE 0.7806799962327478\n",
      "18 Train Loss 12.16687 Test MSE 2.4341119985019457 Test RE 0.7457243750490217\n",
      "19 Train Loss 10.618805 Test MSE 2.001718705226873 Test RE 0.676253590598255\n",
      "20 Train Loss 9.055655 Test MSE 2.0970197517051776 Test RE 0.6921644992872277\n",
      "21 Train Loss 8.526333 Test MSE 2.016769189545199 Test RE 0.6787911310097797\n",
      "22 Train Loss 7.982587 Test MSE 1.9537170825953372 Test RE 0.6680960394232263\n",
      "23 Train Loss 7.719097 Test MSE 1.9945224400752655 Test RE 0.6750369156892824\n",
      "24 Train Loss 7.5838156 Test MSE 2.0298203238721033 Test RE 0.6809839223674891\n",
      "25 Train Loss 7.411105 Test MSE 2.0454993841324747 Test RE 0.6836089450044193\n",
      "26 Train Loss 7.2904654 Test MSE 2.0580523278277285 Test RE 0.6857033429151255\n",
      "27 Train Loss 7.231085 Test MSE 2.103891857915473 Test RE 0.6932977116371021\n",
      "28 Train Loss 7.1133146 Test MSE 2.066524174207763 Test RE 0.6871132213962017\n",
      "29 Train Loss 6.9652014 Test MSE 2.086397146589326 Test RE 0.6904091688777875\n",
      "30 Train Loss 6.7047486 Test MSE 2.082106310137504 Test RE 0.689698863670702\n",
      "31 Train Loss 6.404668 Test MSE 1.968995044003832 Test RE 0.6707031897652665\n",
      "32 Train Loss 6.206155 Test MSE 1.9863907660697702 Test RE 0.6736594464979333\n",
      "33 Train Loss 6.126521 Test MSE 1.9766621909526008 Test RE 0.6720077597348189\n",
      "34 Train Loss 5.9872293 Test MSE 1.9431276183539188 Test RE 0.6662829847428576\n",
      "35 Train Loss 5.8574195 Test MSE 1.9288791835467396 Test RE 0.6638356526718688\n",
      "36 Train Loss 5.777549 Test MSE 1.942512104299261 Test RE 0.6661774489510833\n",
      "37 Train Loss 5.7085238 Test MSE 1.925419931671447 Test RE 0.66324012412611\n",
      "38 Train Loss 5.603605 Test MSE 1.9296069048719595 Test RE 0.6639608657513592\n",
      "39 Train Loss 5.515784 Test MSE 1.957422838447856 Test RE 0.6687293521824956\n",
      "40 Train Loss 5.473282 Test MSE 1.9718427201259234 Test RE 0.6711880196774729\n",
      "41 Train Loss 5.4055233 Test MSE 1.9616475507651057 Test RE 0.6694506236202882\n",
      "42 Train Loss 5.3234096 Test MSE 2.01180118640532 Test RE 0.6779545663208266\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "43 Train Loss 5.2730303 Test MSE 2.025720853934047 Test RE 0.6802959097287542\n",
      "44 Train Loss 5.244485 Test MSE 2.0345682608293476 Test RE 0.6817798992758671\n",
      "45 Train Loss 5.2247634 Test MSE 2.025162704946145 Test RE 0.6802021819499277\n",
      "46 Train Loss 5.205559 Test MSE 2.016794018968576 Test RE 0.678795309460291\n",
      "47 Train Loss 5.1433806 Test MSE 2.024636083714868 Test RE 0.6801137366611124\n",
      "48 Train Loss 5.0907273 Test MSE 1.9997402865872345 Test RE 0.6759193169928833\n",
      "49 Train Loss 5.0320263 Test MSE 1.9937908220971305 Test RE 0.6749130979694894\n",
      "50 Train Loss 4.966297 Test MSE 2.0106135442191717 Test RE 0.6777544256921281\n",
      "51 Train Loss 4.915601 Test MSE 1.9770938455645453 Test RE 0.6720811307477002\n",
      "52 Train Loss 4.8271217 Test MSE 2.0133483911379093 Test RE 0.6782152115882457\n",
      "53 Train Loss 4.7976437 Test MSE 2.018289953204276 Test RE 0.6790470071810754\n",
      "54 Train Loss 4.7386546 Test MSE 2.00053991600882 Test RE 0.6760544422780096\n",
      "55 Train Loss 4.7155366 Test MSE 2.000428864293232 Test RE 0.6760356778317291\n",
      "56 Train Loss 4.700975 Test MSE 1.9980859202530803 Test RE 0.675639668295826\n",
      "57 Train Loss 4.6750164 Test MSE 2.0055284079443423 Test RE 0.6768968129637404\n",
      "58 Train Loss 4.621723 Test MSE 2.0232403814338236 Test RE 0.6798792747869618\n",
      "59 Train Loss 4.5870104 Test MSE 2.0377629479517263 Test RE 0.6823149560670668\n",
      "60 Train Loss 4.5563264 Test MSE 2.064635830358647 Test RE 0.6867992152536629\n",
      "61 Train Loss 4.5412483 Test MSE 2.0683673675713115 Test RE 0.6874195812898227\n",
      "62 Train Loss 4.5238833 Test MSE 2.083562280953903 Test RE 0.6899399670965012\n",
      "63 Train Loss 4.4943194 Test MSE 2.0975126359408294 Test RE 0.6922458377977119\n",
      "64 Train Loss 4.4838877 Test MSE 2.1128441561644866 Test RE 0.6947711760220457\n",
      "65 Train Loss 4.4639783 Test MSE 2.1108589671220885 Test RE 0.694444702284315\n",
      "66 Train Loss 4.4531302 Test MSE 2.1084270461849264 Test RE 0.694044552074244\n",
      "67 Train Loss 4.4406447 Test MSE 2.1324766546165295 Test RE 0.6979916110224563\n",
      "68 Train Loss 4.420098 Test MSE 2.1469322297192295 Test RE 0.7003533786434872\n",
      "69 Train Loss 4.413412 Test MSE 2.149677389624899 Test RE 0.7008009865853744\n",
      "70 Train Loss 4.4065995 Test MSE 2.1493390859364663 Test RE 0.7007458404299083\n",
      "71 Train Loss 4.3847365 Test MSE 2.154047397118411 Test RE 0.7015129424214535\n",
      "72 Train Loss 4.373336 Test MSE 2.147258919504845 Test RE 0.7004066615531191\n",
      "73 Train Loss 4.3596 Test MSE 2.152639898964839 Test RE 0.7012837135942647\n",
      "74 Train Loss 4.355622 Test MSE 2.1493579670690584 Test RE 0.7007489183170031\n",
      "75 Train Loss 4.3502407 Test MSE 2.146342704545061 Test RE 0.7002572171791065\n",
      "76 Train Loss 4.345665 Test MSE 2.145291796939427 Test RE 0.7000857637258031\n",
      "77 Train Loss 4.3428645 Test MSE 2.142420038841343 Test RE 0.6996170279021865\n",
      "78 Train Loss 4.3398175 Test MSE 2.1469956972436774 Test RE 0.7003637304762246\n",
      "79 Train Loss 4.332643 Test MSE 2.1509159010912686 Test RE 0.701002836658995\n",
      "80 Train Loss 4.3254623 Test MSE 2.151906427120141 Test RE 0.7011642287509262\n",
      "81 Train Loss 4.324407 Test MSE 2.1542994780570806 Test RE 0.701553989073413\n",
      "82 Train Loss 4.3195133 Test MSE 2.1523537834323743 Test RE 0.7012371069066134\n",
      "83 Train Loss 4.31323 Test MSE 2.141630535556563 Test RE 0.6994881080761006\n",
      "84 Train Loss 4.310255 Test MSE 2.142469947206448 Test RE 0.699625176756772\n",
      "85 Train Loss 4.305334 Test MSE 2.1360083858039784 Test RE 0.6985693662161517\n",
      "86 Train Loss 4.301633 Test MSE 2.140140514111195 Test RE 0.6992447342513065\n",
      "87 Train Loss 4.297579 Test MSE 2.1465956461397755 Test RE 0.7002984778218585\n",
      "88 Train Loss 4.2955003 Test MSE 2.1428243542410197 Test RE 0.6996830403061136\n",
      "89 Train Loss 4.2922215 Test MSE 2.1385491726674206 Test RE 0.6989847176674245\n",
      "90 Train Loss 4.28026 Test MSE 2.1442222876465955 Test RE 0.6999112323230665\n",
      "91 Train Loss 4.2741528 Test MSE 2.137300632766175 Test RE 0.698780645267222\n",
      "92 Train Loss 4.2674885 Test MSE 2.137865790550605 Test RE 0.6988730270341611\n",
      "93 Train Loss 4.261889 Test MSE 2.1412319563654334 Test RE 0.6994230141271578\n",
      "94 Train Loss 4.2518272 Test MSE 2.128071936043521 Test RE 0.6972703731481208\n",
      "95 Train Loss 4.241297 Test MSE 2.121775375290437 Test RE 0.6962380636667737\n",
      "96 Train Loss 4.230903 Test MSE 2.112762557282397 Test RE 0.6947577597228621\n",
      "97 Train Loss 4.212595 Test MSE 2.107002078151385 Test RE 0.6938099794671837\n",
      "98 Train Loss 4.191825 Test MSE 2.075008413955926 Test RE 0.6885222690498942\n",
      "99 Train Loss 4.1313543 Test MSE 2.035173458503787 Test RE 0.6818812920230212\n",
      "Training time: 97.30\n",
      "KG_stan\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 56.09686 Test MSE 8.481546444318832 Test RE 1.3920203445085366\n",
      "1 Train Loss 43.821465 Test MSE 8.606671673190892 Test RE 1.4022507410702478\n",
      "2 Train Loss 43.480732 Test MSE 8.53092748536081 Test RE 1.3960667558093203\n",
      "3 Train Loss 43.41156 Test MSE 8.487708380346286 Test RE 1.3925259116839686\n",
      "4 Train Loss 43.22679 Test MSE 8.490619793995362 Test RE 1.392764720038306\n",
      "5 Train Loss 42.92012 Test MSE 8.533054610079477 Test RE 1.3962407945068018\n",
      "6 Train Loss 42.253746 Test MSE 8.401276029237414 Test RE 1.3854175577525751\n",
      "7 Train Loss 41.18275 Test MSE 8.32753487507733 Test RE 1.3793239918571616\n",
      "8 Train Loss 38.04955 Test MSE 7.925100270496462 Test RE 1.34558284974494\n",
      "9 Train Loss 36.631435 Test MSE 7.617609192588585 Test RE 1.319220539457203\n",
      "10 Train Loss 35.613922 Test MSE 7.57853926630846 Test RE 1.3158331178829425\n",
      "11 Train Loss 34.61254 Test MSE 7.474107968329274 Test RE 1.3067356630223599\n",
      "12 Train Loss 33.645706 Test MSE 7.00732865796296 Test RE 1.265273184419221\n",
      "13 Train Loss 32.902885 Test MSE 6.814732392273168 Test RE 1.2477640344692307\n",
      "14 Train Loss 32.18856 Test MSE 6.948827632204405 Test RE 1.2599805172363472\n",
      "15 Train Loss 31.733612 Test MSE 6.9192459444688685 Test RE 1.2572957404181684\n",
      "16 Train Loss 30.798824 Test MSE 6.888230884225075 Test RE 1.2544747032777874\n",
      "17 Train Loss 30.03963 Test MSE 6.728877594974237 Test RE 1.2398791997946281\n",
      "18 Train Loss 28.021542 Test MSE 7.135869002561616 Test RE 1.2768253445720703\n",
      "19 Train Loss 25.40191 Test MSE 7.23811344590962 Test RE 1.2859401409058322\n",
      "20 Train Loss 21.95539 Test MSE 6.512794812986588 Test RE 1.2198087889439695\n",
      "21 Train Loss 20.230387 Test MSE 6.300727058201488 Test RE 1.199784905207721\n",
      "22 Train Loss 18.671072 Test MSE 6.056023761149143 Test RE 1.1762559831262431\n",
      "23 Train Loss 17.170399 Test MSE 5.3929505142492395 Test RE 1.109995652734595\n",
      "24 Train Loss 14.250399 Test MSE 4.618651842368348 Test RE 1.0272252195708214\n",
      "25 Train Loss 12.907545 Test MSE 4.394683334522988 Test RE 1.0020095336971249\n",
      "26 Train Loss 12.462433 Test MSE 4.285193791637278 Test RE 0.9894487263411977\n",
      "27 Train Loss 11.91856 Test MSE 4.293825857065668 Test RE 0.9904447943907934\n",
      "28 Train Loss 11.520802 Test MSE 4.186937728789368 Test RE 0.9780393113242596\n",
      "29 Train Loss 10.529551 Test MSE 3.6197686545236474 Test RE 0.9093864267242584\n",
      "30 Train Loss 9.942067 Test MSE 3.5690892977437456 Test RE 0.9029979560796837\n",
      "31 Train Loss 8.826567 Test MSE 3.5005707879030625 Test RE 0.8942881837141073\n",
      "32 Train Loss 6.6661315 Test MSE 1.9251902138985464 Test RE 0.663200558058896\n",
      "33 Train Loss 4.5091405 Test MSE 1.8839519777158669 Test RE 0.6560591160378919\n",
      "34 Train Loss 3.6565385 Test MSE 1.8527689676315 Test RE 0.650606944076489\n",
      "35 Train Loss 3.1750433 Test MSE 1.7065132275767068 Test RE 0.6243999933309842\n",
      "36 Train Loss 2.861677 Test MSE 1.6869468634713847 Test RE 0.6208100827989605\n",
      "37 Train Loss 2.4793882 Test MSE 1.6305745807586516 Test RE 0.6103492203704195\n",
      "38 Train Loss 2.1974325 Test MSE 1.5569641169148825 Test RE 0.5964133582583826\n",
      "39 Train Loss 1.9759129 Test MSE 1.4645240472292853 Test RE 0.5784373313874431\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "40 Train Loss 1.6754146 Test MSE 1.3904840444400872 Test RE 0.5636260601084591\n",
      "41 Train Loss 1.5390922 Test MSE 1.2372743152873984 Test RE 0.5316686576394668\n",
      "42 Train Loss 1.3041542 Test MSE 1.2801106199874839 Test RE 0.5407939327722024\n",
      "43 Train Loss 1.1753979 Test MSE 1.148128856553411 Test RE 0.5121573108690513\n",
      "44 Train Loss 1.0633832 Test MSE 0.8459794711416164 Test RE 0.43963065982314037\n",
      "45 Train Loss 0.83713746 Test MSE 0.45976411851950605 Test RE 0.3240974377007515\n",
      "46 Train Loss 0.6013392 Test MSE 0.2986111085646921 Test RE 0.2611927032294166\n",
      "47 Train Loss 0.4869387 Test MSE 0.1852959970299803 Test RE 0.2057505851140642\n",
      "48 Train Loss 0.35941204 Test MSE 0.10074260777122711 Test RE 0.1517101558769071\n",
      "49 Train Loss 0.32497147 Test MSE 0.09599857351787498 Test RE 0.1480950184393005\n",
      "50 Train Loss 0.2994066 Test MSE 0.06506232209186914 Test RE 0.12191940651866778\n",
      "51 Train Loss 0.28032768 Test MSE 0.057043379437998754 Test RE 0.11415913873097043\n",
      "52 Train Loss 0.242225 Test MSE 0.04460389115430064 Test RE 0.10094723641190509\n",
      "53 Train Loss 0.23254727 Test MSE 0.04180122741908152 Test RE 0.09772430205780835\n",
      "54 Train Loss 0.21878496 Test MSE 0.047176797408188885 Test RE 0.103817911521171\n",
      "55 Train Loss 0.21151473 Test MSE 0.04914161933452149 Test RE 0.10595776572700134\n",
      "56 Train Loss 0.20056258 Test MSE 0.050049285770026854 Test RE 0.10693183076994606\n",
      "57 Train Loss 0.19393548 Test MSE 0.048009669168246075 Test RE 0.10473031672340963\n",
      "58 Train Loss 0.18333589 Test MSE 0.0482087975373378 Test RE 0.10494728548782645\n",
      "59 Train Loss 0.17352584 Test MSE 0.04589984970632007 Test RE 0.10240323864538985\n",
      "60 Train Loss 0.16497424 Test MSE 0.036814378914537635 Test RE 0.0917100223499658\n",
      "61 Train Loss 0.15497094 Test MSE 0.03781437279991715 Test RE 0.09294724273197898\n",
      "62 Train Loss 0.14874479 Test MSE 0.0322564979458045 Test RE 0.08584532865364523\n",
      "63 Train Loss 0.14537397 Test MSE 0.03015811559654884 Test RE 0.08300612901634949\n",
      "64 Train Loss 0.13912973 Test MSE 0.027171371455887976 Test RE 0.07878868283165186\n",
      "65 Train Loss 0.13633879 Test MSE 0.02534262712647022 Test RE 0.07609110257241494\n",
      "66 Train Loss 0.1314007 Test MSE 0.02490547194882647 Test RE 0.07543196968751173\n",
      "67 Train Loss 0.12196754 Test MSE 0.02443079506771614 Test RE 0.07470967732101239\n",
      "68 Train Loss 0.11928627 Test MSE 0.024697070708277674 Test RE 0.07511571108251607\n",
      "69 Train Loss 0.11262315 Test MSE 0.022409956184532946 Test RE 0.07155311810766597\n",
      "70 Train Loss 0.1090053 Test MSE 0.020259580810739943 Test RE 0.06803357424451195\n",
      "71 Train Loss 0.10761891 Test MSE 0.01930414924833604 Test RE 0.06640998675034013\n",
      "72 Train Loss 0.105648495 Test MSE 0.01803102903518667 Test RE 0.06418274933871582\n",
      "73 Train Loss 0.102760926 Test MSE 0.018183350820668173 Test RE 0.06445327939216244\n",
      "74 Train Loss 0.09802913 Test MSE 0.016466513522624644 Test RE 0.06133507319054435\n",
      "75 Train Loss 0.09407027 Test MSE 0.016079421783765244 Test RE 0.06060985904467279\n",
      "76 Train Loss 0.089802586 Test MSE 0.016517898943194907 Test RE 0.061430699794634364\n",
      "77 Train Loss 0.080983125 Test MSE 0.01097235831793238 Test RE 0.050067747242615965\n",
      "78 Train Loss 0.07814438 Test MSE 0.010171779142416012 Test RE 0.04820660214628844\n",
      "79 Train Loss 0.07625744 Test MSE 0.009988290624250032 Test RE 0.047769824465792754\n",
      "80 Train Loss 0.07308972 Test MSE 0.009992093238168381 Test RE 0.047778916757989204\n",
      "81 Train Loss 0.06906636 Test MSE 0.010551125335066101 Test RE 0.04909728209017265\n",
      "82 Train Loss 0.06607396 Test MSE 0.010582073261900625 Test RE 0.04916923396640447\n",
      "83 Train Loss 0.06271339 Test MSE 0.009676133860468028 Test RE 0.047017441641340973\n",
      "84 Train Loss 0.060663793 Test MSE 0.010120334537332422 Test RE 0.048084543203012545\n",
      "85 Train Loss 0.059395783 Test MSE 0.009408279695870795 Test RE 0.0463621075657077\n",
      "86 Train Loss 0.05809694 Test MSE 0.009613590592644959 Test RE 0.04686524285471525\n",
      "87 Train Loss 0.056657612 Test MSE 0.010060417512055036 Test RE 0.04794199061330732\n",
      "88 Train Loss 0.05546774 Test MSE 0.010278109569159971 Test RE 0.04845791031909647\n",
      "89 Train Loss 0.054292977 Test MSE 0.010159954751635532 Test RE 0.04817857462787975\n",
      "90 Train Loss 0.050842725 Test MSE 0.00818762486552918 Test RE 0.0432500912825785\n",
      "91 Train Loss 0.049102135 Test MSE 0.007161166800255939 Test RE 0.040448270353130436\n",
      "92 Train Loss 0.04821094 Test MSE 0.007153038072526161 Test RE 0.04042530717246194\n",
      "93 Train Loss 0.047343858 Test MSE 0.00712274999941421 Test RE 0.040339630043333526\n",
      "94 Train Loss 0.046661682 Test MSE 0.006510636592285052 Test RE 0.03856734898499485\n",
      "95 Train Loss 0.045913916 Test MSE 0.006803911893406352 Test RE 0.03942642515732758\n",
      "96 Train Loss 0.045447458 Test MSE 0.00655201301646019 Test RE 0.03868970657765115\n",
      "97 Train Loss 0.04458067 Test MSE 0.006668262896713201 Test RE 0.039031425882928766\n",
      "98 Train Loss 0.044019453 Test MSE 0.006591501968008807 Test RE 0.03880612277072943\n",
      "99 Train Loss 0.043297708 Test MSE 0.006871910390561656 Test RE 0.0396229497906773\n",
      "Training time: 105.32\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10 #10\n",
    "max_iter = 100 #200\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "beta_init = 0\n",
    "\n",
    "N_I = 200  #Total number of data points for 'y'\n",
    "N_B = 400\n",
    "N_f = 10000 #Total number of collocation points\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(label)\n",
    "    print(reps)\n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []\n",
    "    beta_val = []\n",
    "\n",
    "    torch.manual_seed(reps*36)\n",
    "\n",
    "    layers = np.array([2,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "    #layers = np.array([2,50,50,50,50,50,50,50,1])\n",
    "\n",
    "    PINN = Sequentialmodel(layers,beta_init)\n",
    "\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.1, \n",
    "                            max_iter = 20, \n",
    "                            max_eval = 30, \n",
    "                            tolerance_grad = 1e-8, \n",
    "                            tolerance_change = 1e-8, \n",
    "                            history_size = 100, \n",
    "                            line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    nan_flag = train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "\n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    #elapsed_time[reps] = time.time() - start_time\n",
    "    beta_full.append(beta_val)\n",
    "\n",
    "\n",
    "  #print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"beta\": beta_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'nan_tune' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_29293/1697650768.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mnan_tune\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'nan_tune' is not defined"
     ]
    }
   ],
   "source": [
    "nan_tune"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1660688516819,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "_O3sPdAnSq_2"
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "aborted",
     "timestamp": 1660688516819,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "jQ4afiEWSq_2"
   },
   "outputs": [],
   "source": [
    "for tune_reps in range(20):\n",
    "    label = \"KG_stan_tune\"+str(tune_reps)+\".mat\"\n",
    "    data = sio.loadmat(label)\n",
    "    re = np.array(data[\"test_re_loss\"])\n",
    "    print(tune_reps,\" \",np.mean(re[:,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1660688534316,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "06syezgfv_qO",
    "outputId": "9f4852d5-694a-4977-8893-a6183a2ce493"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.013187398857835885\n",
      "0.7916874700548301\n",
      "0.015533489608380632\n",
      "0.8403604832490177\n",
      "0.009705293188510641\n",
      "0.0145127096353756\n",
      "0.8701002331425253\n",
      "0.007181597648319921\n",
      "0.6818812920230212\n",
      "0.0396229497906773\n",
      "a =  0.3283772917198494\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a+ test_re_full[i][-1]\n",
    "    print(test_re_full[i][-1])\n",
    "    \n",
    "print(\"a = \",a/10)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "stan_2D_KG_16Aug2022_tune.ipynb",
   "version": ""
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
