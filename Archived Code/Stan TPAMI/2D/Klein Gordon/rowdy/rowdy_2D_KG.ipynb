{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 286,
     "status": "ok",
     "timestamp": 1660687093981,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "iAtv2UvNSq_u",
    "outputId": "68a82578-1b95-4343-a8ec-7635a4df93ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1256,
     "status": "ok",
     "timestamp": 1660687393066,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "xAgfGYA4acPE",
    "outputId": "527d048f-6a89-4e80-87ff-bfdb1c9d6222"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1856,
     "status": "ok",
     "timestamp": 1660687061284,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "7kSdyTofacUc",
    "outputId": "08ee5c9b-0706-46a5-86a1-2c7e56a6a74d"
   },
   "outputs": [],
   "source": [
    "# %cd '/content/gdrive/MyDrive/Virginia Tech /Fall 2022/Codes from GPU/PINN_Stan/2D Klein Gordon/stan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 32419,
     "status": "ok",
     "timestamp": 1660687093700,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "RHuSaD0gagsN",
    "outputId": "c232cd79-e56c-4a76-97c7-d59dafa084ef"
   },
   "outputs": [],
   "source": [
    "# !pip install smt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 280,
     "status": "ok",
     "timestamp": 1660687410736,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "mTLFQRt5Sq_y"
   },
   "outputs": [],
   "source": [
    "def true_2D_1(xt): #True function for 2D_1 Klein Gordon Equation x \\in [-50,50] , t \\in [0,10]\n",
    "    y = xt[:,0]*np.cos(xt[:,1])\n",
    "    return y.reshape(-1,1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 4312,
     "status": "ok",
     "timestamp": 1660687098957,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "81bNHCY3Sq_y"
   },
   "outputs": [],
   "source": [
    "loss_thresh = 0.01\n",
    "label = \"KG_rowdy\"\n",
    "\n",
    "x = np.linspace(-5,5,500).reshape(-1,1)\n",
    "t = np.linspace(0,10,1000).reshape(-1,1)\n",
    "\n",
    "X,T = np.meshgrid(x,t)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "T = T.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xt = np.hstack((X,T))\n",
    "\n",
    "y_true = true_2D_1(xt)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "#bound_pts_idx = ((X == -5) + (X == 5) + (T == 0)).reshape(-1,)\n",
    "\n",
    "#xt_bound = xt[bound_pts_idx,:]\n",
    "#y_bound = y_true[bound_pts_idx,:]\n",
    "\n",
    "\n",
    "xt_test_tensor = torch.from_numpy(xt).float().to(device)\n",
    "\n",
    "\n",
    "lb_xt = xt[0]\n",
    "ub_xt = xt[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500000, 2)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xt.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1660687098958,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "YQgCA-PuSq_z"
   },
   "outputs": [],
   "source": [
    "def trainingdata(N_I,N_B,N_f,seed):\n",
    "    '''Boundary Conditions''' \n",
    "    \n",
    "    x01 = np.array([[0.0,1.0],[0.0,0.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state=seed)\n",
    "    samples = sampling(N_I)\n",
    "    xt_BC1 = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    y_BC1 = true_2D_1(xt_BC1)\n",
    "    \n",
    "\n",
    "    x01 = np.array([[0.0,0.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state=seed+1)\n",
    "    samples = sampling(int(N_B/2))\n",
    "    xt_coll = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    xt_BC2 = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    y_BC2 = true_2D_1(xt_BC2)\n",
    "    \n",
    "    x01 = np.array([[1.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state=seed+2)\n",
    "    samples = sampling(int(N_B/2))\n",
    "    xt_coll = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    xt_BC3 = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    y_BC3 = true_2D_1(xt_BC3)\n",
    "\n",
    "    xt_BC = np.vstack((xt_BC1,xt_BC2,xt_BC3))\n",
    "    y_BC = np.vstack((y_BC1,y_BC2,y_BC3))\n",
    "\n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    samples = sampling(N_f)\n",
    "    xt_coll = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    \n",
    "    xt_coll = np.vstack((xt_coll, xt_BC)) # append training points to collocation points \n",
    "\n",
    "    return xt_coll, xt_BC, y_BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1660687098958,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "gTJxct8bSq_0"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,n_val,rowdy_terms):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "            \n",
    "        \n",
    "        self.omega1 = Parameter(torch.ones((len(layers)-2,1))) \n",
    "        self.omega1.requiresGrad = True\n",
    "        \n",
    "        \n",
    "        self.alpha = Parameter(torch.zeros(rowdy_terms,len(layers)-2))\n",
    "        self.alpha.requiresGrad = True\n",
    "        \n",
    "        \n",
    "        self.omega = Parameter((1/n_val)*torch.ones(rowdy_terms,len(layers)-2))\n",
    "        self.omega.requiresGrad = True\n",
    "        \n",
    "        self.n = torch.tensor(n_val)\n",
    "\n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xt):\n",
    "        if torch.is_tensor(xt) != True:         \n",
    "            xt = torch.from_numpy(xt)                \n",
    "        \n",
    "        ubxt = torch.from_numpy(ub_xt).float().to(device)\n",
    "        lbxt = torch.from_numpy(lb_xt).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xt = (xt - lbxt)/(ubxt - lbxt)\n",
    "        \n",
    "        #convert to float\n",
    "        a = xt.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(self.omega1[i,0]*z)\n",
    "            for j in range(rowdy_terms):\n",
    "                a = a + self.alpha[j,i]*self.n*torch.sin((j+1)*self.n*self.omega[j,i]*z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC(self,xt,y):\n",
    "                \n",
    "        loss_bc = self.loss_function(self.forward(xt), y)\n",
    "                \n",
    "        return loss_bc\n",
    "    \n",
    "    def loss_PDE(self, xt_coll, f_hat):\n",
    "        \n",
    "        g = xt_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        y = self.forward(g) \n",
    "        \n",
    "        \n",
    "        y_x_t = autograd.grad(y,g,torch.ones([xt_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        y_xx_tt = autograd.grad(y_x_t,g,torch.ones(xt_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        #du_dx = u_x_t[:,[0]]\n",
    "        \n",
    "        d2y_dx2 = y_xx_tt[:,[0]]\n",
    "        d2y_dt2 = y_xx_tt[:,[1]]    \n",
    "        \n",
    "\n",
    "        f = d2y_dt2 - d2y_dx2 + torch.pow(y,2) + (g[:,0]*torch.cos(g[:,1])).reshape(-1,1) - (torch.pow(g[:,0],2)*torch.pow(torch.cos(g[:,1]),2)).reshape(-1,1)\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xt_BC,y_BC,xt_coll,f_hat):\n",
    "\n",
    "        loss_BC = self.loss_BC(xt_BC,y_BC)\n",
    "        loss_f = self.loss_PDE(xt_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_BC + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "         \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        y_pred1 = self.forward(xt_test_tensor[:250000])\n",
    "        y_pred1 = y_pred1.cpu().detach().numpy()\n",
    "        \n",
    "        y_pred2 = self.forward(xt_test_tensor[250000:])\n",
    "          \n",
    "        y_pred2 = y_pred2.cpu().detach().numpy()\n",
    "        \n",
    "        y_pred = np.vstack((y_pred1.reshape(-1,1),y_pred2.reshape(-1,1)))\n",
    "   \n",
    "        return y_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1660687098958,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "VoQzfzYsYKVs"
   },
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    alpha_val.append(PINN.alpha.cpu().detach().numpy())\n",
    "    omega_val.append(PINN.omega.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1660687098959,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "_IUDZDkxXmyF"
   },
   "outputs": [],
   "source": [
    "def train_step(xt_BC, y_BC, xt_coll, f_hat,seed):\n",
    "    # x_coll_np_array = colloc_pts(N_f,seed*123)\n",
    "    # x_coll_train = torch.from_numpy(x_coll_np_array).float().to(device)        \n",
    "    \n",
    "    # f_hat = torch.zeros(x_coll_train.shape[0],1).to(device)\n",
    "    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(xt_BC, y_BC, xt_coll,f_hat)\n",
    "        loss.backward()\n",
    "        #print(loss.cpu().detach().numpy())\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 467,
     "status": "ok",
     "timestamp": 1660690085956,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "Vt9Dlr8MYIwW"
   },
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*11)\n",
    "    start_time = time.time() \n",
    "    thresh_flag = 0\n",
    "\n",
    "    xt_coll, xt_BC, y_BC = trainingdata(N_I,N_B,N_f,rep*11)\n",
    "    xt_coll = torch.from_numpy(xt_coll).float().to(device)\n",
    "    xt_BC = torch.from_numpy(xt_BC).float().to(device)\n",
    "    y_BC = torch.from_numpy(y_BC).float().to(device)\n",
    "\n",
    "    f_hat = torch.zeros(xt_coll.shape[0],1).to(device)\n",
    " \n",
    "    for i in range(max_iter):\n",
    "        train_step(xt_BC, y_BC, xt_coll,f_hat,i)\n",
    "\n",
    "        loss_np = PINN.loss(xt_BC, y_BC, xt_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        \n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "        \n",
    "   \n",
    "            \n",
    "        \n",
    "\n",
    "    elapsed_time[rep] = time.time() - start_time  \n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))\n",
    "    \n",
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "sP4Re5lSSq_1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "KG_rowdy\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 41.311394 Test MSE 9.291133415797278 Test RE 1.4569424876214552\n",
      "1 Train Loss 34.419327 Test MSE 9.100978031060254 Test RE 1.4419562815984348\n",
      "2 Train Loss 28.744106 Test MSE 8.93486506829422 Test RE 1.4287362355586426\n",
      "3 Train Loss 21.769268 Test MSE 9.316529091746462 Test RE 1.4589322766418285\n",
      "4 Train Loss 18.097534 Test MSE 9.249498166542661 Test RE 1.453674410903181\n",
      "5 Train Loss 15.0596485 Test MSE 9.039008163194744 Test RE 1.4370386522721645\n",
      "6 Train Loss 12.862648 Test MSE 8.484376396278128 Test RE 1.3922525558264343\n",
      "7 Train Loss 8.8905945 Test MSE 6.805549329497632 Test RE 1.2469230507370221\n",
      "8 Train Loss 4.897272 Test MSE 5.645461224969381 Test RE 1.1356846992760565\n",
      "9 Train Loss 3.3263118 Test MSE 5.118894256444025 Test RE 1.0814243380404989\n",
      "10 Train Loss 2.5696287 Test MSE 4.637647348840022 Test RE 1.029335428598472\n",
      "11 Train Loss 1.9978331 Test MSE 4.279276545957568 Test RE 0.9887653460731225\n",
      "12 Train Loss 1.6154372 Test MSE 3.8284379317193857 Test RE 0.9352309389167075\n",
      "13 Train Loss 1.3569257 Test MSE 3.5653560458059927 Test RE 0.9025255665436848\n",
      "14 Train Loss 1.1570331 Test MSE 3.2238080150483492 Test RE 0.8582081719783277\n",
      "15 Train Loss 1.0407559 Test MSE 3.0224778165750283 Test RE 0.8309781822302296\n",
      "16 Train Loss 0.9388641 Test MSE 2.886787690979855 Test RE 0.8121111676816274\n",
      "17 Train Loss 0.8806807 Test MSE 2.8536843519753647 Test RE 0.8074414259025362\n",
      "18 Train Loss 0.8289711 Test MSE 2.7553972925467796 Test RE 0.7934145745605664\n",
      "19 Train Loss 0.7910309 Test MSE 2.708880703392713 Test RE 0.7866888594711654\n",
      "20 Train Loss 0.7591162 Test MSE 2.6912694885287682 Test RE 0.7841274439544454\n",
      "21 Train Loss 0.7332636 Test MSE 2.674650357997505 Test RE 0.7817026223570628\n",
      "22 Train Loss 0.7133615 Test MSE 2.6956689151055193 Test RE 0.7847680900266735\n",
      "23 Train Loss 0.6899226 Test MSE 2.688041590788147 Test RE 0.7836570632263707\n",
      "24 Train Loss 0.6703715 Test MSE 2.6908841339330944 Test RE 0.7840713035469575\n",
      "25 Train Loss 0.65344584 Test MSE 2.707058043646763 Test RE 0.7864241546820392\n",
      "26 Train Loss 0.63516796 Test MSE 2.7645157166508247 Test RE 0.7947263119008888\n",
      "27 Train Loss 0.62265974 Test MSE 2.7568597966538935 Test RE 0.7936251101052865\n",
      "28 Train Loss 0.6103337 Test MSE 2.7626206304824708 Test RE 0.7944538712296771\n",
      "29 Train Loss 0.5996943 Test MSE 2.7780761706631116 Test RE 0.7966730663089174\n",
      "30 Train Loss 0.5891129 Test MSE 2.8144179691067412 Test RE 0.8018670311020343\n",
      "31 Train Loss 0.5794433 Test MSE 2.8230943757532385 Test RE 0.8031020947422722\n",
      "32 Train Loss 0.5711788 Test MSE 2.8409620418512884 Test RE 0.8056395456308729\n",
      "33 Train Loss 0.56288934 Test MSE 2.880061711593302 Test RE 0.8111645396446379\n",
      "34 Train Loss 0.5552342 Test MSE 2.8842730243005743 Test RE 0.8117573775798079\n",
      "35 Train Loss 0.5446042 Test MSE 2.879135524219704 Test RE 0.8110340995984533\n",
      "36 Train Loss 0.53461885 Test MSE 2.913192335520184 Test RE 0.8158167909434918\n",
      "37 Train Loss 0.5257225 Test MSE 2.910151622100217 Test RE 0.815390915747155\n",
      "38 Train Loss 0.5174019 Test MSE 2.90093074013019 Test RE 0.8140980984720856\n",
      "39 Train Loss 0.509336 Test MSE 2.921143739569481 Test RE 0.8169293966032036\n",
      "40 Train Loss 0.5027108 Test MSE 2.9177724735140647 Test RE 0.8164578550315523\n",
      "41 Train Loss 0.49582353 Test MSE 2.940227277230623 Test RE 0.8195935112976115\n",
      "42 Train Loss 0.48681545 Test MSE 2.9704968467419084 Test RE 0.8238015563990687\n",
      "43 Train Loss 0.4782157 Test MSE 2.987062367331258 Test RE 0.8260954030819174\n",
      "44 Train Loss 0.46784395 Test MSE 2.98962991407531 Test RE 0.8264503643632237\n",
      "45 Train Loss 0.45877215 Test MSE 3.0064322863611084 Test RE 0.8287695260464972\n",
      "46 Train Loss 0.45144308 Test MSE 3.0316388791152913 Test RE 0.8322365675429524\n",
      "47 Train Loss 0.4453676 Test MSE 3.044837582532936 Test RE 0.834046234592163\n",
      "48 Train Loss 0.44102308 Test MSE 3.049365191083155 Test RE 0.8346661087247331\n",
      "49 Train Loss 0.4349715 Test MSE 3.0529126736872954 Test RE 0.8351514725063612\n",
      "50 Train Loss 0.42934048 Test MSE 3.071003871584009 Test RE 0.8376223216394091\n",
      "51 Train Loss 0.4240804 Test MSE 3.0829343478846005 Test RE 0.8392477748882177\n",
      "52 Train Loss 0.41638085 Test MSE 3.0847381260344777 Test RE 0.8394932545619342\n",
      "53 Train Loss 0.40912822 Test MSE 3.1095701266409344 Test RE 0.8428654228363835\n",
      "54 Train Loss 0.40078437 Test MSE 3.1377854641532505 Test RE 0.8466807457993452\n",
      "55 Train Loss 0.39441925 Test MSE 3.173276786069241 Test RE 0.85145566165539\n",
      "56 Train Loss 0.38696852 Test MSE 3.2025694190251 Test RE 0.8553765430189793\n",
      "57 Train Loss 0.37776527 Test MSE 3.219850142428456 Test RE 0.8576811986323432\n",
      "58 Train Loss 0.37089065 Test MSE 3.2551685973035003 Test RE 0.8623723112438259\n",
      "59 Train Loss 0.36485577 Test MSE 3.260543510207914 Test RE 0.8630839893198834\n",
      "60 Train Loss 0.35995668 Test MSE 3.2780903082518877 Test RE 0.8654032406951018\n",
      "61 Train Loss 0.3561756 Test MSE 3.2950500188604788 Test RE 0.8676390018656107\n",
      "62 Train Loss 0.3510598 Test MSE 3.3038326816192023 Test RE 0.8687945396773992\n",
      "63 Train Loss 0.34489945 Test MSE 3.31585091127559 Test RE 0.8703732960780342\n",
      "64 Train Loss 0.340402 Test MSE 3.336877992599085 Test RE 0.8731286201464747\n",
      "65 Train Loss 0.33497274 Test MSE 3.347015873038962 Test RE 0.874453954880259\n",
      "66 Train Loss 0.33037123 Test MSE 3.35199202746357 Test RE 0.8751037578573277\n",
      "67 Train Loss 0.32674894 Test MSE 3.3609257753709905 Test RE 0.8762691475892245\n",
      "68 Train Loss 0.32252342 Test MSE 3.3643600866289716 Test RE 0.8767167344774157\n",
      "69 Train Loss 0.31978437 Test MSE 3.376616753398788 Test RE 0.8783122616440168\n",
      "70 Train Loss 0.31718123 Test MSE 3.376844578388772 Test RE 0.8783418916141404\n",
      "71 Train Loss 0.31492338 Test MSE 3.3739036307928743 Test RE 0.877959327346255\n",
      "72 Train Loss 0.31199124 Test MSE 3.380164547190094 Test RE 0.8787735595845365\n",
      "73 Train Loss 0.30942234 Test MSE 3.3882926953433765 Test RE 0.8798295012018212\n",
      "74 Train Loss 0.3075058 Test MSE 3.389904794840047 Test RE 0.8800387812365614\n",
      "75 Train Loss 0.30555233 Test MSE 3.392571205460842 Test RE 0.8803848209697784\n",
      "76 Train Loss 0.30327925 Test MSE 3.385618613661084 Test RE 0.8794822466182455\n",
      "77 Train Loss 0.30103472 Test MSE 3.3938676972045987 Test RE 0.8805530271110907\n",
      "78 Train Loss 0.29903144 Test MSE 3.4047192065201397 Test RE 0.8819596381508802\n",
      "79 Train Loss 0.29712296 Test MSE 3.408739955097556 Test RE 0.8824802525429091\n",
      "80 Train Loss 0.2950764 Test MSE 3.4145973207168447 Test RE 0.8832381265578066\n",
      "81 Train Loss 0.29341418 Test MSE 3.419243052338862 Test RE 0.8838387673033229\n",
      "82 Train Loss 0.29131222 Test MSE 3.419053229311964 Test RE 0.8838142333237082\n",
      "83 Train Loss 0.28888234 Test MSE 3.4258620303943697 Test RE 0.8846938221925581\n",
      "84 Train Loss 0.2869177 Test MSE 3.4336171341509565 Test RE 0.8856945942567994\n",
      "85 Train Loss 0.2851046 Test MSE 3.43592618113497 Test RE 0.8859923512055388\n",
      "86 Train Loss 0.2829378 Test MSE 3.4408942757642786 Test RE 0.8866326594700625\n",
      "87 Train Loss 0.281165 Test MSE 3.4421429778292425 Test RE 0.8867935245703595\n",
      "88 Train Loss 0.2789623 Test MSE 3.4494993125143534 Test RE 0.887740618908551\n",
      "89 Train Loss 0.27690852 Test MSE 3.4599866235947787 Test RE 0.8890890677342111\n",
      "90 Train Loss 0.2753898 Test MSE 3.462861240257552 Test RE 0.8894583263323665\n",
      "91 Train Loss 0.27376795 Test MSE 3.4584282995967146 Test RE 0.8888888292626234\n",
      "92 Train Loss 0.27217555 Test MSE 3.4614338285870327 Test RE 0.8892749874731325\n",
      "93 Train Loss 0.27043322 Test MSE 3.469886022917652 Test RE 0.8903600497041222\n",
      "94 Train Loss 0.26874143 Test MSE 3.4695016229533495 Test RE 0.8903107305264216\n",
      "95 Train Loss 0.26722306 Test MSE 3.4685072286260854 Test RE 0.8901831353440778\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "96 Train Loss 0.26564506 Test MSE 3.466015841630826 Test RE 0.8898633739976585\n",
      "97 Train Loss 0.2643405 Test MSE 3.476417195250743 Test RE 0.891197593228605\n",
      "98 Train Loss 0.26293993 Test MSE 3.479070859029169 Test RE 0.8915376685853617\n",
      "99 Train Loss 0.26135677 Test MSE 3.4814657106764186 Test RE 0.8918444650315832\n",
      "100 Train Loss 0.26019266 Test MSE 3.48595130482088 Test RE 0.8924188157368663\n",
      "101 Train Loss 0.25899714 Test MSE 3.4874994647210014 Test RE 0.8926169616000931\n",
      "102 Train Loss 0.25785795 Test MSE 3.49029464754619 Test RE 0.8929746000590297\n",
      "103 Train Loss 0.2568129 Test MSE 3.491365460322459 Test RE 0.8931115706271172\n",
      "104 Train Loss 0.25578958 Test MSE 3.4960955992828 Test RE 0.8937163643659967\n",
      "105 Train Loss 0.25477517 Test MSE 3.4950443028813503 Test RE 0.8935819813929671\n",
      "106 Train Loss 0.2538118 Test MSE 3.4929725828581586 Test RE 0.8933171026187997\n",
      "107 Train Loss 0.2529637 Test MSE 3.493549100912632 Test RE 0.893390820945628\n",
      "108 Train Loss 0.25197828 Test MSE 3.489696891303034 Test RE 0.8928981302975645\n",
      "109 Train Loss 0.25101596 Test MSE 3.4850608700888857 Test RE 0.8923048308600252\n",
      "110 Train Loss 0.2500595 Test MSE 3.4823977537023634 Test RE 0.8919638374247869\n",
      "111 Train Loss 0.24898487 Test MSE 3.480595645581872 Test RE 0.8917330161060132\n",
      "112 Train Loss 0.24791083 Test MSE 3.484075731023284 Test RE 0.892178705878312\n",
      "113 Train Loss 0.24689358 Test MSE 3.4843919976420943 Test RE 0.8922191986748741\n",
      "114 Train Loss 0.24595411 Test MSE 3.495658703473913 Test RE 0.8936605201932245\n",
      "115 Train Loss 0.24526834 Test MSE 3.4987989825986046 Test RE 0.8940618341853215\n",
      "116 Train Loss 0.24420217 Test MSE 3.5024671385676074 Test RE 0.8945303805557429\n",
      "117 Train Loss 0.24354312 Test MSE 3.508948388248984 Test RE 0.895357653856443\n",
      "118 Train Loss 0.2427702 Test MSE 3.5097313816604103 Test RE 0.8954575441884913\n",
      "119 Train Loss 0.24187462 Test MSE 3.5169678466281105 Test RE 0.8963802088810233\n",
      "120 Train Loss 0.24095893 Test MSE 3.5194145549797433 Test RE 0.8966919546418266\n",
      "121 Train Loss 0.23985294 Test MSE 3.521647907878668 Test RE 0.8969764212680593\n",
      "122 Train Loss 0.23909895 Test MSE 3.515216702833897 Test RE 0.8961570214437936\n",
      "123 Train Loss 0.23828292 Test MSE 3.5192471170399537 Test RE 0.8966706240999202\n",
      "124 Train Loss 0.23746368 Test MSE 3.5206313262334765 Test RE 0.8968469484183458\n",
      "125 Train Loss 0.23656738 Test MSE 3.5272566270182963 Test RE 0.8976904175912994\n",
      "126 Train Loss 0.23593919 Test MSE 3.533348334659934 Test RE 0.8984652561029131\n",
      "127 Train Loss 0.2353017 Test MSE 3.5341356398573254 Test RE 0.8985653491165553\n",
      "128 Train Loss 0.2345264 Test MSE 3.5348954946429383 Test RE 0.8986619416860112\n",
      "129 Train Loss 0.23375791 Test MSE 3.5303300420754407 Test RE 0.8980814260722398\n",
      "130 Train Loss 0.23315829 Test MSE 3.528787365671364 Test RE 0.8978851837387786\n",
      "131 Train Loss 0.232429 Test MSE 3.5238003434365877 Test RE 0.8972504959449245\n",
      "132 Train Loss 0.23141734 Test MSE 3.53269399594889 Test RE 0.8983820591233527\n",
      "133 Train Loss 0.23056334 Test MSE 3.531274393321601 Test RE 0.8982015348998973\n",
      "134 Train Loss 0.23002961 Test MSE 3.5324842042653697 Test RE 0.8983553831809865\n",
      "135 Train Loss 0.22939134 Test MSE 3.5324334987695316 Test RE 0.8983489356335989\n",
      "136 Train Loss 0.22895256 Test MSE 3.5296649528085724 Test RE 0.8979968259860205\n",
      "137 Train Loss 0.22838703 Test MSE 3.5320878810161545 Test RE 0.8983049867615349\n",
      "138 Train Loss 0.22787085 Test MSE 3.532706739057263 Test RE 0.8983836794405422\n",
      "139 Train Loss 0.22750647 Test MSE 3.5306644542175722 Test RE 0.8981239606537166\n",
      "140 Train Loss 0.22684926 Test MSE 3.535222846016693 Test RE 0.8987035513193469\n",
      "141 Train Loss 0.22622058 Test MSE 3.5418943872969906 Test RE 0.8995511516029716\n",
      "142 Train Loss 0.22572869 Test MSE 3.5461980813475025 Test RE 0.9000975001405795\n",
      "143 Train Loss 0.22517236 Test MSE 3.5449009115006724 Test RE 0.8999328609831151\n",
      "144 Train Loss 0.22457364 Test MSE 3.5453734403948447 Test RE 0.8999928386976006\n",
      "145 Train Loss 0.22398534 Test MSE 3.5452604770348115 Test RE 0.8999785007122495\n",
      "146 Train Loss 0.22340742 Test MSE 3.547777702889369 Test RE 0.9002979479258129\n",
      "147 Train Loss 0.22293273 Test MSE 3.553845265804849 Test RE 0.9010674832933296\n",
      "148 Train Loss 0.22236878 Test MSE 3.5518868638030896 Test RE 0.9008191754172364\n",
      "149 Train Loss 0.22187841 Test MSE 3.551412564794083 Test RE 0.9007590282443498\n",
      "150 Train Loss 0.22151369 Test MSE 3.5527406294623916 Test RE 0.9009274336889497\n",
      "151 Train Loss 0.2211522 Test MSE 3.553793691360086 Test RE 0.9010609449918174\n",
      "152 Train Loss 0.22082323 Test MSE 3.5503578339121957 Test RE 0.900625260497486\n",
      "153 Train Loss 0.22044109 Test MSE 3.5525906529273237 Test RE 0.9009084174681004\n",
      "154 Train Loss 0.22002247 Test MSE 3.549067685721683 Test RE 0.9004616085950978\n",
      "155 Train Loss 0.21968977 Test MSE 3.5455248025606223 Test RE 0.9000120501309817\n",
      "156 Train Loss 0.2193506 Test MSE 3.551129415141495 Test RE 0.9007231193369561\n",
      "157 Train Loss 0.2188919 Test MSE 3.5514439835665836 Test RE 0.9007630126702475\n",
      "158 Train Loss 0.21853897 Test MSE 3.5553827356923 Test RE 0.9012623728026278\n",
      "159 Train Loss 0.21822047 Test MSE 3.560934250451451 Test RE 0.9019657316453059\n",
      "160 Train Loss 0.21791217 Test MSE 3.5641520739313624 Test RE 0.9023731684194068\n",
      "161 Train Loss 0.21755287 Test MSE 3.5672729555785443 Test RE 0.9027681548823423\n",
      "162 Train Loss 0.21730533 Test MSE 3.5669188412658004 Test RE 0.9027233459957441\n",
      "163 Train Loss 0.21712327 Test MSE 3.569782079054748 Test RE 0.9030855904454012\n",
      "164 Train Loss 0.21675727 Test MSE 3.577103284877654 Test RE 0.9040111776192578\n",
      "165 Train Loss 0.21645856 Test MSE 3.581746650348563 Test RE 0.9045977266526735\n",
      "166 Train Loss 0.21620609 Test MSE 3.5838849465103433 Test RE 0.9048677079535211\n",
      "167 Train Loss 0.21599782 Test MSE 3.587196778828973 Test RE 0.905285701020034\n",
      "168 Train Loss 0.21575774 Test MSE 3.583661968596566 Test RE 0.904839558521386\n",
      "169 Train Loss 0.21557271 Test MSE 3.583890834029432 Test RE 0.9048684512005554\n",
      "170 Train Loss 0.21537241 Test MSE 3.5852155418087017 Test RE 0.9050356682818472\n",
      "171 Train Loss 0.21516225 Test MSE 3.5850365404785762 Test RE 0.9050130748549393\n",
      "172 Train Loss 0.21497789 Test MSE 3.5879354288428877 Test RE 0.9053789012147111\n",
      "173 Train Loss 0.214784 Test MSE 3.5908077951571284 Test RE 0.9057412348792432\n",
      "174 Train Loss 0.21463957 Test MSE 3.593428764226937 Test RE 0.9060717296965553\n",
      "175 Train Loss 0.21446726 Test MSE 3.593683191012002 Test RE 0.9061038055845079\n",
      "176 Train Loss 0.21429242 Test MSE 3.5952169833533425 Test RE 0.906297148562371\n",
      "177 Train Loss 0.2141555 Test MSE 3.595828018563677 Test RE 0.9063741614302414\n",
      "178 Train Loss 0.21399695 Test MSE 3.596109258288129 Test RE 0.9064096057610072\n",
      "179 Train Loss 0.2138519 Test MSE 3.5970535303053333 Test RE 0.906528601178977\n",
      "180 Train Loss 0.21375713 Test MSE 3.5952848739444256 Test RE 0.9063057055922984\n",
      "181 Train Loss 0.21362598 Test MSE 3.5971867194156846 Test RE 0.9065453841680591\n",
      "182 Train Loss 0.21347313 Test MSE 3.598800026400748 Test RE 0.9067486502434277\n",
      "183 Train Loss 0.21328253 Test MSE 3.597881502347266 Test RE 0.9066329278373617\n",
      "184 Train Loss 0.21311183 Test MSE 3.599431335467927 Test RE 0.9068281786331623\n",
      "185 Train Loss 0.21297209 Test MSE 3.60254959186079 Test RE 0.9072208949179424\n",
      "186 Train Loss 0.21283832 Test MSE 3.604462919827651 Test RE 0.9074617771949665\n",
      "187 Train Loss 0.21269721 Test MSE 3.604873501638395 Test RE 0.907513459886433\n",
      "188 Train Loss 0.21256249 Test MSE 3.6033639870675915 Test RE 0.9073234326594404\n",
      "189 Train Loss 0.21243492 Test MSE 3.6052912993572237 Test RE 0.9075660478715485\n",
      "190 Train Loss 0.21234068 Test MSE 3.60586988577767 Test RE 0.907638869216827\n",
      "191 Train Loss 0.21222526 Test MSE 3.6063092542637194 Test RE 0.9076941645798505\n",
      "192 Train Loss 0.21214506 Test MSE 3.6066024439354742 Test RE 0.9077310611861396\n",
      "193 Train Loss 0.21201727 Test MSE 3.6076844374135932 Test RE 0.907867212238495\n",
      "194 Train Loss 0.21191671 Test MSE 3.6094678106534994 Test RE 0.9080915758271745\n",
      "195 Train Loss 0.21183737 Test MSE 3.6110657485155704 Test RE 0.9082925629840948\n",
      "196 Train Loss 0.21174967 Test MSE 3.612794040094218 Test RE 0.9085098958592127\n",
      "197 Train Loss 0.21166119 Test MSE 3.6155844888734365 Test RE 0.908860685437303\n",
      "198 Train Loss 0.21157002 Test MSE 3.615956380839862 Test RE 0.9089074260538684\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199 Train Loss 0.21148294 Test MSE 3.615507699247158 Test RE 0.9088510339628102\n",
      "Training time: 294.71\n",
      "1\n",
      "KG_rowdy\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 55.323456 Test MSE 8.701219596743089 Test RE 1.4099318619310415\n",
      "1 Train Loss 44.01976 Test MSE 8.4133903023919 Test RE 1.3864160537442158\n",
      "2 Train Loss 40.11522 Test MSE 9.180903367262783 Test RE 1.4482741143986906\n",
      "3 Train Loss 33.046318 Test MSE 8.599631198217331 Test RE 1.4016770854550884\n",
      "4 Train Loss 28.040413 Test MSE 8.654274843707396 Test RE 1.4061232912227606\n",
      "5 Train Loss 24.4985 Test MSE 8.693719651405313 Test RE 1.4093240912966232\n",
      "6 Train Loss 20.772217 Test MSE 8.567448313058152 Test RE 1.3990518393704312\n",
      "7 Train Loss 17.495872 Test MSE 8.695596763323428 Test RE 1.4094762307933288\n",
      "8 Train Loss 14.063179 Test MSE 8.652161425918782 Test RE 1.405951589521571\n",
      "9 Train Loss 11.6587925 Test MSE 8.182323519572458 Test RE 1.367245125742318\n",
      "10 Train Loss 8.068566 Test MSE 7.105232514394038 Test RE 1.2740814937015978\n",
      "11 Train Loss 5.4988837 Test MSE 7.25726572936052 Test RE 1.2876403366615305\n",
      "12 Train Loss 4.301209 Test MSE 7.08632753097205 Test RE 1.272385382235428\n",
      "13 Train Loss 3.6760356 Test MSE 6.913622702591787 Test RE 1.2567847371126228\n",
      "14 Train Loss 3.2648988 Test MSE 7.076623968423114 Test RE 1.2715139223240568\n",
      "15 Train Loss 2.8405464 Test MSE 6.720995977321883 Test RE 1.2391528441047281\n",
      "16 Train Loss 2.4982524 Test MSE 6.386400972955974 Test RE 1.2079143799970993\n",
      "17 Train Loss 2.1578386 Test MSE 5.963166927495441 Test RE 1.167203399186987\n",
      "18 Train Loss 1.9279332 Test MSE 5.629916948630441 Test RE 1.134120118047085\n",
      "19 Train Loss 1.8016247 Test MSE 5.555216358594494 Test RE 1.126570952767962\n",
      "20 Train Loss 1.6917168 Test MSE 5.571270957945697 Test RE 1.1281976757746472\n",
      "21 Train Loss 1.6098729 Test MSE 5.4873490922666415 Test RE 1.119668229252663\n",
      "22 Train Loss 1.5039307 Test MSE 5.625527582557683 Test RE 1.1336779233326257\n",
      "23 Train Loss 1.4409236 Test MSE 5.640888659927557 Test RE 1.1352246798113066\n",
      "24 Train Loss 1.3828951 Test MSE 5.657655175595411 Test RE 1.1369105524555494\n",
      "25 Train Loss 1.3249539 Test MSE 5.672953695964031 Test RE 1.138446639995676\n",
      "26 Train Loss 1.2814138 Test MSE 5.691065066979274 Test RE 1.1402624840843776\n",
      "27 Train Loss 1.2412038 Test MSE 5.7159005062693415 Test RE 1.1427477914136872\n",
      "28 Train Loss 1.2083429 Test MSE 5.7106073084520395 Test RE 1.1422185492474906\n",
      "29 Train Loss 1.1720614 Test MSE 5.677621763075188 Test RE 1.1389149368307079\n",
      "30 Train Loss 1.1487387 Test MSE 5.6454702553330165 Test RE 1.1356856075846344\n",
      "31 Train Loss 1.1232728 Test MSE 5.6497873071556635 Test RE 1.1361197500450324\n",
      "32 Train Loss 1.1011994 Test MSE 5.660146338497778 Test RE 1.1371608255459449\n",
      "33 Train Loss 1.0752013 Test MSE 5.6825918884673685 Test RE 1.1394133243624862\n",
      "34 Train Loss 1.0494697 Test MSE 5.766298062147175 Test RE 1.1477745850519792\n",
      "35 Train Loss 1.0265425 Test MSE 5.773584902351187 Test RE 1.1484995744034294\n",
      "36 Train Loss 1.0099013 Test MSE 5.7939826060777335 Test RE 1.1505265731928471\n",
      "37 Train Loss 0.99718726 Test MSE 5.800870181753629 Test RE 1.1512102122171668\n",
      "38 Train Loss 0.9878857 Test MSE 5.825597500202654 Test RE 1.1536612298495699\n",
      "39 Train Loss 0.9773605 Test MSE 5.8479377309789395 Test RE 1.1558711659540664\n",
      "40 Train Loss 0.96136165 Test MSE 5.8598102419838485 Test RE 1.1570438986042915\n",
      "41 Train Loss 0.94254774 Test MSE 5.941742354318151 Test RE 1.1651047378016433\n",
      "42 Train Loss 0.93137705 Test MSE 5.955068264771266 Test RE 1.1664105320370197\n",
      "43 Train Loss 0.9180023 Test MSE 5.945592064275773 Test RE 1.1654821177652368\n",
      "44 Train Loss 0.9021958 Test MSE 5.954559900704489 Test RE 1.1663607447104472\n",
      "45 Train Loss 0.8948103 Test MSE 5.9587378738710495 Test RE 1.1667698571800988\n",
      "46 Train Loss 0.88377106 Test MSE 5.987552758611349 Test RE 1.1695875505296485\n",
      "47 Train Loss 0.8762491 Test MSE 6.021030675136936 Test RE 1.1728527221762184\n",
      "48 Train Loss 0.8701444 Test MSE 6.018053494695058 Test RE 1.172562719839234\n",
      "49 Train Loss 0.86479497 Test MSE 6.047185883356343 Test RE 1.1753973832818925\n",
      "50 Train Loss 0.8584106 Test MSE 6.05261887851899 Test RE 1.1759252730087413\n",
      "51 Train Loss 0.85329336 Test MSE 6.064101873432908 Test RE 1.1770402238875015\n",
      "52 Train Loss 0.8465037 Test MSE 6.069566322225461 Test RE 1.1775704283475406\n",
      "53 Train Loss 0.84044576 Test MSE 6.080815609499954 Test RE 1.1786611731487935\n",
      "54 Train Loss 0.8345146 Test MSE 6.073821034922411 Test RE 1.1779830893074825\n",
      "55 Train Loss 0.8273262 Test MSE 6.092435043180808 Test RE 1.179786749062965\n",
      "56 Train Loss 0.8220248 Test MSE 6.087405634031818 Test RE 1.179299681469177\n",
      "57 Train Loss 0.81523955 Test MSE 6.1078347659808205 Test RE 1.1812768693754063\n",
      "58 Train Loss 0.80942464 Test MSE 6.12038405007376 Test RE 1.1824897846628897\n",
      "59 Train Loss 0.80460465 Test MSE 6.134127289773322 Test RE 1.1838166726214852\n",
      "60 Train Loss 0.7988775 Test MSE 6.134543762252272 Test RE 1.1838568591608927\n",
      "61 Train Loss 0.7938956 Test MSE 6.155451004164063 Test RE 1.1858725044828649\n",
      "62 Train Loss 0.78882873 Test MSE 6.167271443103063 Test RE 1.1870105860331475\n",
      "63 Train Loss 0.78536403 Test MSE 6.176956143229466 Test RE 1.1879422242892\n",
      "64 Train Loss 0.781173 Test MSE 6.195194631242644 Test RE 1.1896947298736351\n",
      "65 Train Loss 0.77528745 Test MSE 6.207703133184005 Test RE 1.1908951598616364\n",
      "66 Train Loss 0.7706025 Test MSE 6.207792728975424 Test RE 1.1909037539267084\n",
      "67 Train Loss 0.766662 Test MSE 6.220054290619505 Test RE 1.192079303509838\n",
      "68 Train Loss 0.76318455 Test MSE 6.230765251048228 Test RE 1.1931052448928239\n",
      "69 Train Loss 0.7602587 Test MSE 6.224245825479559 Test RE 1.1924808916817027\n",
      "70 Train Loss 0.7571304 Test MSE 6.24412899032855 Test RE 1.1943840450550405\n",
      "71 Train Loss 0.75309366 Test MSE 6.255137022073856 Test RE 1.1954363958232292\n",
      "72 Train Loss 0.7468595 Test MSE 6.264104502406788 Test RE 1.1962929888096157\n",
      "73 Train Loss 0.7433683 Test MSE 6.267257089135889 Test RE 1.196593984985573\n",
      "74 Train Loss 0.74004513 Test MSE 6.273550449680142 Test RE 1.1971946231693458\n",
      "75 Train Loss 0.7371792 Test MSE 6.282739642092382 Test RE 1.1980710986403025\n",
      "76 Train Loss 0.7335664 Test MSE 6.281642740435135 Test RE 1.197966508635508\n",
      "77 Train Loss 0.7302468 Test MSE 6.282571788060356 Test RE 1.1980550942839847\n",
      "78 Train Loss 0.7276219 Test MSE 6.291786038830165 Test RE 1.1989333282139483\n",
      "79 Train Loss 0.72387516 Test MSE 6.299185592744047 Test RE 1.1996381332479515\n",
      "80 Train Loss 0.72056794 Test MSE 6.299082856616726 Test RE 1.1996283505009038\n",
      "81 Train Loss 0.71755743 Test MSE 6.3094444328264965 Test RE 1.2006145998812003\n",
      "82 Train Loss 0.714988 Test MSE 6.314140407063725 Test RE 1.2010613116805862\n",
      "83 Train Loss 0.71188176 Test MSE 6.3143756360963605 Test RE 1.2010836838358598\n",
      "84 Train Loss 0.70885813 Test MSE 6.320921034903965 Test RE 1.2017060363786698\n",
      "85 Train Loss 0.70532954 Test MSE 6.324003873096719 Test RE 1.2019990485556493\n",
      "86 Train Loss 0.7022676 Test MSE 6.335410807918195 Test RE 1.2030826142665483\n",
      "87 Train Loss 0.699844 Test MSE 6.349210264821447 Test RE 1.2043921470406587\n",
      "88 Train Loss 0.697685 Test MSE 6.362070700736379 Test RE 1.205611288634436\n",
      "89 Train Loss 0.69498384 Test MSE 6.361735392318706 Test RE 1.2055795177724693\n",
      "90 Train Loss 0.69272006 Test MSE 6.353421234107087 Test RE 1.2047914736884455\n",
      "91 Train Loss 0.69016564 Test MSE 6.375334433514736 Test RE 1.2068673718249665\n",
      "92 Train Loss 0.6879643 Test MSE 6.380523060764729 Test RE 1.2073583822513136\n",
      "93 Train Loss 0.68546045 Test MSE 6.369905269988152 Test RE 1.2063533849948376\n",
      "94 Train Loss 0.683407 Test MSE 6.3631625486538335 Test RE 1.2057147366912342\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95 Train Loss 0.68112695 Test MSE 6.38169374130739 Test RE 1.207469138542414\n",
      "96 Train Loss 0.67869544 Test MSE 6.395409750139041 Test RE 1.2087660331212107\n",
      "97 Train Loss 0.6768583 Test MSE 6.394555158433842 Test RE 1.208685269262677\n",
      "98 Train Loss 0.67457044 Test MSE 6.4030029912250725 Test RE 1.2094834014757874\n",
      "99 Train Loss 0.6721691 Test MSE 6.4044579002965305 Test RE 1.2096208048493275\n",
      "100 Train Loss 0.66997766 Test MSE 6.422531293867012 Test RE 1.2113263786326554\n",
      "101 Train Loss 0.66856956 Test MSE 6.426191939081213 Test RE 1.211671538838451\n",
      "102 Train Loss 0.666751 Test MSE 6.438541920473449 Test RE 1.2128352870127272\n",
      "103 Train Loss 0.6651076 Test MSE 6.444200532099871 Test RE 1.2133681294402279\n",
      "104 Train Loss 0.66318536 Test MSE 6.457919848338217 Test RE 1.2146590367082395\n",
      "105 Train Loss 0.6614183 Test MSE 6.45798720495841 Test RE 1.214665371187142\n",
      "106 Train Loss 0.65948707 Test MSE 6.478250834947614 Test RE 1.2165695443638491\n",
      "107 Train Loss 0.65710974 Test MSE 6.507863445532953 Test RE 1.2193468931957754\n",
      "108 Train Loss 0.6551222 Test MSE 6.5164362695719475 Test RE 1.220149753165817\n",
      "109 Train Loss 0.65343595 Test MSE 6.5248240516229465 Test RE 1.2209347726571806\n",
      "110 Train Loss 0.6519669 Test MSE 6.521196472151294 Test RE 1.2205953264397704\n",
      "111 Train Loss 0.6502259 Test MSE 6.530051101810994 Test RE 1.221423721451693\n",
      "112 Train Loss 0.6484529 Test MSE 6.54339619533041 Test RE 1.2226711614932693\n",
      "113 Train Loss 0.64657223 Test MSE 6.557350219091711 Test RE 1.223974161932395\n",
      "114 Train Loss 0.6448164 Test MSE 6.562513311251067 Test RE 1.2244559303087266\n",
      "115 Train Loss 0.64268965 Test MSE 6.563076627119834 Test RE 1.224508481871077\n",
      "116 Train Loss 0.64129704 Test MSE 6.569154847428664 Test RE 1.2250753737647277\n",
      "117 Train Loss 0.6398003 Test MSE 6.574311095174859 Test RE 1.2255560712994085\n",
      "118 Train Loss 0.6378816 Test MSE 6.5773679245183105 Test RE 1.2258409589728598\n",
      "119 Train Loss 0.63640493 Test MSE 6.581282369820732 Test RE 1.2262056773305352\n",
      "120 Train Loss 0.6351614 Test MSE 6.5911599742068425 Test RE 1.2271255157981291\n",
      "121 Train Loss 0.63379097 Test MSE 6.594379358221124 Test RE 1.2274251676042076\n",
      "122 Train Loss 0.6322583 Test MSE 6.6222266037206925 Test RE 1.2300140693948998\n",
      "123 Train Loss 0.6309231 Test MSE 6.624660960052127 Test RE 1.2302401276068058\n",
      "124 Train Loss 0.6293889 Test MSE 6.6432919269216875 Test RE 1.2319688553537136\n",
      "125 Train Loss 0.6280998 Test MSE 6.639182312873688 Test RE 1.2315877416071919\n",
      "126 Train Loss 0.6269407 Test MSE 6.660652424469938 Test RE 1.2335775185016178\n",
      "127 Train Loss 0.62554336 Test MSE 6.6581166301377355 Test RE 1.233342676892738\n",
      "128 Train Loss 0.6242757 Test MSE 6.674549046323835 Test RE 1.2348637009257502\n",
      "129 Train Loss 0.62335455 Test MSE 6.676034714837079 Test RE 1.2350011256437181\n",
      "130 Train Loss 0.6225531 Test MSE 6.672532542252531 Test RE 1.234677149312421\n",
      "131 Train Loss 0.621271 Test MSE 6.674163092501619 Test RE 1.234827997595683\n",
      "132 Train Loss 0.6202203 Test MSE 6.677746460209091 Test RE 1.235159443571967\n",
      "133 Train Loss 0.61897933 Test MSE 6.6915223904357015 Test RE 1.236432830027425\n",
      "134 Train Loss 0.61752254 Test MSE 6.697626896556224 Test RE 1.236996684611156\n",
      "135 Train Loss 0.61646646 Test MSE 6.697929512058221 Test RE 1.2370246295945224\n",
      "136 Train Loss 0.6152953 Test MSE 6.699109696690738 Test RE 1.2371336075369634\n",
      "137 Train Loss 0.6140876 Test MSE 6.709337348961876 Test RE 1.2380776245202592\n",
      "138 Train Loss 0.6129656 Test MSE 6.713495014922389 Test RE 1.2384611732761146\n",
      "139 Train Loss 0.61198354 Test MSE 6.719164358409847 Test RE 1.2389839844263677\n",
      "140 Train Loss 0.6108648 Test MSE 6.719939582536879 Test RE 1.2390554563032212\n",
      "141 Train Loss 0.60990757 Test MSE 6.723320288826377 Test RE 1.2393670927259413\n",
      "142 Train Loss 0.6088701 Test MSE 6.722129089754151 Test RE 1.2392572959093824\n",
      "143 Train Loss 0.6075723 Test MSE 6.71838105659119 Test RE 1.2389117636379865\n",
      "144 Train Loss 0.60603553 Test MSE 6.7218313543509245 Test RE 1.239229851118345\n",
      "145 Train Loss 0.6047193 Test MSE 6.727550103692366 Test RE 1.239756890390763\n",
      "146 Train Loss 0.60318965 Test MSE 6.726900823315431 Test RE 1.239697064062368\n",
      "147 Train Loss 0.6022093 Test MSE 6.727014055643046 Test RE 1.2397074977819271\n",
      "148 Train Loss 0.6009415 Test MSE 6.729542321100867 Test RE 1.239940440301143\n",
      "149 Train Loss 0.59951097 Test MSE 6.730337680465673 Test RE 1.240013711933608\n",
      "150 Train Loss 0.59824497 Test MSE 6.741309922698267 Test RE 1.241024076549724\n",
      "151 Train Loss 0.59614605 Test MSE 6.748816749673977 Test RE 1.241714859279663\n",
      "152 Train Loss 0.5951065 Test MSE 6.7568305334092 Test RE 1.2424518686304449\n",
      "153 Train Loss 0.59402007 Test MSE 6.754764109159952 Test RE 1.2422618661592395\n",
      "154 Train Loss 0.59271526 Test MSE 6.762553075734148 Test RE 1.2429778903056736\n",
      "155 Train Loss 0.59128726 Test MSE 6.770784512500172 Test RE 1.2437341418440555\n",
      "156 Train Loss 0.5895846 Test MSE 6.781823991427149 Test RE 1.2447476569517548\n",
      "157 Train Loss 0.5882584 Test MSE 6.780426771652098 Test RE 1.2446194262846608\n",
      "158 Train Loss 0.58705187 Test MSE 6.792780218084532 Test RE 1.245752713547592\n",
      "159 Train Loss 0.5858351 Test MSE 6.7829465568671115 Test RE 1.2448506714862704\n",
      "160 Train Loss 0.5845832 Test MSE 6.79081804512073 Test RE 1.2455727755278756\n",
      "161 Train Loss 0.5831905 Test MSE 6.7987992781116775 Test RE 1.2463045200105385\n",
      "162 Train Loss 0.5816403 Test MSE 6.818789617941613 Test RE 1.248135414202421\n",
      "163 Train Loss 0.57979524 Test MSE 6.829853999969352 Test RE 1.2491476355999258\n",
      "164 Train Loss 0.5776776 Test MSE 6.84172308552672 Test RE 1.2502325639455356\n",
      "165 Train Loss 0.57651556 Test MSE 6.848417049386797 Test RE 1.2508440301967272\n",
      "166 Train Loss 0.5751862 Test MSE 6.849680926801302 Test RE 1.2509594466918565\n",
      "167 Train Loss 0.57373524 Test MSE 6.856705483267744 Test RE 1.2516007308314567\n",
      "168 Train Loss 0.5721133 Test MSE 6.8590707800277135 Test RE 1.2518165890084139\n",
      "169 Train Loss 0.5710753 Test MSE 6.866990456411391 Test RE 1.2525390718712268\n",
      "170 Train Loss 0.5696665 Test MSE 6.863508447753261 Test RE 1.2522214724124294\n",
      "171 Train Loss 0.5682025 Test MSE 6.867399416943439 Test RE 1.2525763685168372\n",
      "172 Train Loss 0.5668099 Test MSE 6.884712604648024 Test RE 1.254154290349868\n",
      "173 Train Loss 0.5657888 Test MSE 6.8908160806169345 Test RE 1.254710087319252\n",
      "174 Train Loss 0.5645647 Test MSE 6.894554364097059 Test RE 1.2550503827184336\n",
      "175 Train Loss 0.56251174 Test MSE 6.917781956278036 Test RE 1.2571627227812463\n",
      "176 Train Loss 0.56125426 Test MSE 6.919344212330818 Test RE 1.2573046685099407\n",
      "177 Train Loss 0.5596539 Test MSE 6.931088169878984 Test RE 1.25837120546575\n",
      "178 Train Loss 0.558277 Test MSE 6.926981560137669 Test RE 1.2579983632080745\n",
      "179 Train Loss 0.5566429 Test MSE 6.930946775550906 Test RE 1.2583583700022456\n",
      "180 Train Loss 0.55509025 Test MSE 6.94239713944981 Test RE 1.2593973849898892\n",
      "181 Train Loss 0.5538798 Test MSE 6.940112899878898 Test RE 1.2591901797582399\n",
      "182 Train Loss 0.552397 Test MSE 6.948357537011122 Test RE 1.2599378970395254\n",
      "183 Train Loss 0.5511148 Test MSE 6.94971969855944 Test RE 1.2600613906020328\n",
      "184 Train Loss 0.5495788 Test MSE 6.965032135504399 Test RE 1.2614487842727369\n",
      "185 Train Loss 0.54839325 Test MSE 6.970783127761241 Test RE 1.2619694627957863\n",
      "186 Train Loss 0.5464934 Test MSE 6.974513173843344 Test RE 1.2623070557627254\n",
      "187 Train Loss 0.54473495 Test MSE 6.984845570863391 Test RE 1.2632417325206404\n",
      "188 Train Loss 0.5429142 Test MSE 6.992126976098734 Test RE 1.2638999989735802\n",
      "189 Train Loss 0.5407889 Test MSE 7.005948580614965 Test RE 1.2651485819543709\n",
      "190 Train Loss 0.53930736 Test MSE 7.002748109604032 Test RE 1.2648595751306493\n",
      "191 Train Loss 0.5373633 Test MSE 7.008606037318962 Test RE 1.265388503699541\n",
      "192 Train Loss 0.5359439 Test MSE 7.018814425334144 Test RE 1.2663097194458686\n",
      "193 Train Loss 0.5345029 Test MSE 7.024585382111235 Test RE 1.266830200305376\n",
      "194 Train Loss 0.53307956 Test MSE 7.0391239821910325 Test RE 1.268140485305724\n",
      "195 Train Loss 0.53183454 Test MSE 7.049681756093559 Test RE 1.2690911521764716\n",
      "196 Train Loss 0.53078824 Test MSE 7.040668618657178 Test RE 1.268279615444354\n",
      "197 Train Loss 0.52923226 Test MSE 7.053135802219501 Test RE 1.2694020146188076\n",
      "198 Train Loss 0.52817523 Test MSE 7.040191014282095 Test RE 1.2682365977851753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "199 Train Loss 0.5269486 Test MSE 7.047063371764815 Test RE 1.268855448132649\n",
      "Training time: 289.01\n",
      "2\n",
      "KG_rowdy\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 53.5143 Test MSE 9.04279995483546 Test RE 1.4373400337801059\n",
      "1 Train Loss 36.188377 Test MSE 6.999661031594093 Test RE 1.2645807452662063\n",
      "2 Train Loss 29.089884 Test MSE 6.3343007095927994 Test RE 1.2029772068553504\n",
      "3 Train Loss 23.978203 Test MSE 5.990875393965662 Test RE 1.169912021488605\n",
      "4 Train Loss 17.753096 Test MSE 5.501379406759922 Test RE 1.121098725870271\n",
      "5 Train Loss 12.018045 Test MSE 5.45799880053295 Test RE 1.1166698187516817\n",
      "6 Train Loss 9.525223 Test MSE 5.415251841146225 Test RE 1.1122883529075067\n",
      "7 Train Loss 7.6444473 Test MSE 4.875772710844229 Test RE 1.055430855086435\n",
      "8 Train Loss 5.6646695 Test MSE 4.58609354306265 Test RE 1.0235982028059882\n",
      "9 Train Loss 4.7029085 Test MSE 4.175690613476656 Test RE 0.9767248043020796\n",
      "10 Train Loss 3.6928189 Test MSE 3.629234919288353 Test RE 0.910574744641268\n",
      "11 Train Loss 3.0689254 Test MSE 3.400507366638457 Test RE 0.8814139511086532\n",
      "12 Train Loss 2.516225 Test MSE 2.796503935434685 Test RE 0.7993109778943707\n",
      "13 Train Loss 1.9854817 Test MSE 2.124143756496183 Test RE 0.6966265348660035\n",
      "14 Train Loss 1.3321514 Test MSE 1.427074759786775 Test RE 0.5709938396815316\n",
      "15 Train Loss 0.8279738 Test MSE 0.9029842639017561 Test RE 0.4542010682276792\n",
      "16 Train Loss 0.502001 Test MSE 0.6339764175896248 Test RE 0.38057881717463404\n",
      "17 Train Loss 0.34705842 Test MSE 0.48334796882927306 Test RE 0.3323058671767643\n",
      "18 Train Loss 0.24263456 Test MSE 0.3933445666219523 Test RE 0.29977446600758695\n",
      "19 Train Loss 0.17496252 Test MSE 0.3587084459166951 Test RE 0.28627199296646183\n",
      "20 Train Loss 0.14247929 Test MSE 0.322100918774214 Test RE 0.2712714208327968\n",
      "21 Train Loss 0.105576195 Test MSE 0.3106546265535243 Test RE 0.26640782275362207\n",
      "22 Train Loss 0.08300461 Test MSE 0.25540828662415055 Test RE 0.24156029544945112\n",
      "23 Train Loss 0.066723906 Test MSE 0.23779514518295494 Test RE 0.23308243806054546\n",
      "24 Train Loss 0.054542333 Test MSE 0.21405980552467116 Test RE 0.22114423566027022\n",
      "25 Train Loss 0.045742955 Test MSE 0.1971862745931694 Test RE 0.21224936582162374\n",
      "26 Train Loss 0.04007241 Test MSE 0.1791878298971374 Test RE 0.2023309474575877\n",
      "27 Train Loss 0.033319373 Test MSE 0.156374787074488 Test RE 0.1890128915009468\n",
      "28 Train Loss 0.027801437 Test MSE 0.15589802777985892 Test RE 0.1887245379735036\n",
      "29 Train Loss 0.023965642 Test MSE 0.1367090308788004 Test RE 0.1767285289892497\n",
      "30 Train Loss 0.019119097 Test MSE 0.12283623965397446 Test RE 0.16752179460403563\n",
      "31 Train Loss 0.017416531 Test MSE 0.11212199213603014 Test RE 0.16004918354806802\n",
      "32 Train Loss 0.015467202 Test MSE 0.09854349178177965 Test RE 0.15004517478475266\n",
      "33 Train Loss 0.014109661 Test MSE 0.09413795240231056 Test RE 0.14665282528669532\n",
      "34 Train Loss 0.01247642 Test MSE 0.08595819127070518 Test RE 0.14013663811393703\n",
      "35 Train Loss 0.0111001935 Test MSE 0.08339956449847385 Test RE 0.13803523268904908\n",
      "36 Train Loss 0.010455861 Test MSE 0.079143481694754 Test RE 0.13446697538373484\n",
      "37 Train Loss 0.009743652 Test MSE 0.0780435912724334 Test RE 0.13352933408579845\n",
      "38 Train Loss 0.00909762 Test MSE 0.07558672412622398 Test RE 0.1314107282743998\n",
      "39 Train Loss 0.008570504 Test MSE 0.07427106217955472 Test RE 0.13026204075075243\n",
      "40 Train Loss 0.008001142 Test MSE 0.0691456119849682 Test RE 0.12568700359027862\n",
      "41 Train Loss 0.0074263834 Test MSE 0.06760779332959907 Test RE 0.12428148699482469\n",
      "42 Train Loss 0.007024209 Test MSE 0.06655949130345479 Test RE 0.12331419078355396\n",
      "43 Train Loss 0.0066195056 Test MSE 0.06576205843840023 Test RE 0.1225732664208462\n",
      "44 Train Loss 0.0061720195 Test MSE 0.06487138305408727 Test RE 0.12174037602965607\n",
      "45 Train Loss 0.0055048624 Test MSE 0.06145179281691506 Test RE 0.11848826469452763\n",
      "46 Train Loss 0.0051392596 Test MSE 0.06038256295386228 Test RE 0.11745292363400257\n",
      "47 Train Loss 0.0049491683 Test MSE 0.06125490458974985 Test RE 0.11829829740903602\n",
      "48 Train Loss 0.004719682 Test MSE 0.05895135833241211 Test RE 0.11605262506338478\n",
      "49 Train Loss 0.0044896617 Test MSE 0.05614494485598804 Test RE 0.11325656616717028\n",
      "50 Train Loss 0.0042969915 Test MSE 0.055864045891364525 Test RE 0.11297289375591223\n",
      "51 Train Loss 0.0041277413 Test MSE 0.05577563675754349 Test RE 0.11288346422855154\n",
      "52 Train Loss 0.003993391 Test MSE 0.05413406374487481 Test RE 0.11120988080770301\n",
      "53 Train Loss 0.0038818098 Test MSE 0.053187492627997074 Test RE 0.11023330254541812\n",
      "54 Train Loss 0.0037050836 Test MSE 0.05171543715000508 Test RE 0.1086971507882754\n",
      "55 Train Loss 0.0034839604 Test MSE 0.051163396889462205 Test RE 0.10811544637203843\n",
      "56 Train Loss 0.003317117 Test MSE 0.050273981792405865 Test RE 0.10717159692829781\n",
      "57 Train Loss 0.0031961945 Test MSE 0.048452651735341204 Test RE 0.10521237771461019\n",
      "58 Train Loss 0.0030864181 Test MSE 0.04732046589724342 Test RE 0.1039758707834055\n",
      "59 Train Loss 0.0029788057 Test MSE 0.04448648676523037 Test RE 0.100814294448329\n",
      "60 Train Loss 0.0029002281 Test MSE 0.0428899923713161 Test RE 0.09898879664099046\n",
      "61 Train Loss 0.0028137933 Test MSE 0.04102352625609153 Test RE 0.09681096617405059\n",
      "62 Train Loss 0.0026787228 Test MSE 0.038873535437508065 Test RE 0.09423995724634823\n",
      "63 Train Loss 0.0025488215 Test MSE 0.038052627450549936 Test RE 0.09323959638727813\n",
      "64 Train Loss 0.002440395 Test MSE 0.037055907379731925 Test RE 0.09201037196063745\n",
      "65 Train Loss 0.0023625868 Test MSE 0.03593950732191206 Test RE 0.09061375335950214\n",
      "66 Train Loss 0.0022426073 Test MSE 0.034251048425015945 Test RE 0.08845960487761884\n",
      "67 Train Loss 0.0021398067 Test MSE 0.03293944699964733 Test RE 0.086749346564967\n",
      "68 Train Loss 0.002053085 Test MSE 0.03152097115756151 Test RE 0.08486094331398024\n",
      "69 Train Loss 0.0019515725 Test MSE 0.030115029588473785 Test RE 0.08294681362208806\n",
      "70 Train Loss 0.0018430485 Test MSE 0.029129031347471808 Test RE 0.08157762957280776\n",
      "71 Train Loss 0.0017563864 Test MSE 0.02857995601680701 Test RE 0.08080511232967577\n",
      "72 Train Loss 0.0016314134 Test MSE 0.026974018907884683 Test RE 0.0785020303031164\n",
      "73 Train Loss 0.0015666635 Test MSE 0.02714716079270946 Test RE 0.07875357324262452\n",
      "74 Train Loss 0.0015199806 Test MSE 0.026701910866269076 Test RE 0.07810507094353543\n",
      "75 Train Loss 0.001437127 Test MSE 0.026120839564222936 Test RE 0.07725055818083414\n",
      "76 Train Loss 0.0013931093 Test MSE 0.024667995694818968 Test RE 0.07507148248756777\n",
      "77 Train Loss 0.0013572444 Test MSE 0.02382390718951554 Test RE 0.073775906512878\n",
      "78 Train Loss 0.0013145964 Test MSE 0.02346919987153057 Test RE 0.07322463272729185\n",
      "79 Train Loss 0.0012763803 Test MSE 0.02316008197843801 Test RE 0.0727408055836434\n",
      "80 Train Loss 0.0012279832 Test MSE 0.02285388238468221 Test RE 0.07225835231767946\n",
      "81 Train Loss 0.0011239054 Test MSE 0.022373472922982612 Test RE 0.07149485037986486\n",
      "82 Train Loss 0.0010583112 Test MSE 0.022203520288538958 Test RE 0.0712227893317196\n",
      "83 Train Loss 0.0009958234 Test MSE 0.021733921905239817 Test RE 0.07046559305195152\n",
      "84 Train Loss 0.0009776751 Test MSE 0.021362029087798686 Test RE 0.0698601174307181\n",
      "85 Train Loss 0.0009484641 Test MSE 0.021464241759568094 Test RE 0.07002705073220486\n",
      "86 Train Loss 0.0009139328 Test MSE 0.021460512378487933 Test RE 0.07002096691834485\n",
      "87 Train Loss 0.00088995986 Test MSE 0.021234044628354133 Test RE 0.06965052962730559\n",
      "88 Train Loss 0.0008602268 Test MSE 0.020674570488943066 Test RE 0.06872682929448505\n",
      "89 Train Loss 0.0008456696 Test MSE 0.02039499056416211 Test RE 0.06826055493803891\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "90 Train Loss 0.0008278299 Test MSE 0.020420172728635617 Test RE 0.0683026833773017\n",
      "91 Train Loss 0.00081352424 Test MSE 0.020175814613550325 Test RE 0.06789278118451314\n",
      "92 Train Loss 0.00079630205 Test MSE 0.020039720971641247 Test RE 0.06766341225284947\n",
      "93 Train Loss 0.00076994894 Test MSE 0.019728100923015437 Test RE 0.06713526395514716\n",
      "94 Train Loss 0.00074859604 Test MSE 0.019513325393899154 Test RE 0.0667688203947791\n",
      "95 Train Loss 0.0007307719 Test MSE 0.01923797633521854 Test RE 0.06629606526328483\n",
      "96 Train Loss 0.00071752915 Test MSE 0.019172948081719255 Test RE 0.06618392335850995\n",
      "97 Train Loss 0.00070515194 Test MSE 0.01893983062681329 Test RE 0.06578033876433062\n",
      "98 Train Loss 0.00068523764 Test MSE 0.01880763997598482 Test RE 0.06555037969558801\n",
      "99 Train Loss 0.00067224505 Test MSE 0.018618326771859293 Test RE 0.06521963807978423\n",
      "100 Train Loss 0.0006626486 Test MSE 0.018503847452399646 Test RE 0.06501881948887706\n",
      "101 Train Loss 0.0006562545 Test MSE 0.018170315167823383 Test RE 0.06443017195667354\n",
      "102 Train Loss 0.0006487344 Test MSE 0.017973362724226286 Test RE 0.06408003344908508\n",
      "103 Train Loss 0.00064016157 Test MSE 0.01772561827080448 Test RE 0.06363686206760881\n",
      "104 Train Loss 0.00063247245 Test MSE 0.017584659947387164 Test RE 0.06338332930813692\n",
      "105 Train Loss 0.0006263393 Test MSE 0.017348048486697767 Test RE 0.06295545599481148\n",
      "106 Train Loss 0.00062348065 Test MSE 0.017387649392547094 Test RE 0.06302727018026934\n",
      "107 Train Loss 0.0006196753 Test MSE 0.017206237058174072 Test RE 0.06269761364537349\n",
      "108 Train Loss 0.0006144777 Test MSE 0.017222982446111867 Test RE 0.06272811539134031\n",
      "109 Train Loss 0.0006104323 Test MSE 0.017126755329043425 Test RE 0.06255263473498211\n",
      "110 Train Loss 0.00060403976 Test MSE 0.017069021559351718 Test RE 0.06244711422167315\n",
      "111 Train Loss 0.0005993559 Test MSE 0.016807684102618736 Test RE 0.06196721795854288\n",
      "112 Train Loss 0.00058811763 Test MSE 0.016528396506008333 Test RE 0.06145021711357772\n",
      "113 Train Loss 0.00057993585 Test MSE 0.016454585334746512 Test RE 0.06131285389021911\n",
      "114 Train Loss 0.00057128206 Test MSE 0.01625231467328107 Test RE 0.06093483949217993\n",
      "115 Train Loss 0.00056553393 Test MSE 0.016263666902334047 Test RE 0.06095611724600271\n",
      "116 Train Loss 0.0005595285 Test MSE 0.016110025318156973 Test RE 0.060667510189895274\n",
      "117 Train Loss 0.0005524809 Test MSE 0.015861671973215186 Test RE 0.060198066500139084\n",
      "118 Train Loss 0.00054680824 Test MSE 0.015812262069839076 Test RE 0.060104233371915686\n",
      "119 Train Loss 0.0005448888 Test MSE 0.01581517156708757 Test RE 0.06010976278501644\n",
      "120 Train Loss 0.0005378635 Test MSE 0.0156295076344978 Test RE 0.05975588857248216\n",
      "121 Train Loss 0.0005272338 Test MSE 0.015049538079951668 Test RE 0.05863671682422093\n",
      "122 Train Loss 0.0005159166 Test MSE 0.014495972322055399 Test RE 0.057548198883729224\n",
      "123 Train Loss 0.00050869177 Test MSE 0.014584756004679901 Test RE 0.05772416298829126\n",
      "124 Train Loss 0.0004998158 Test MSE 0.014448052674825244 Test RE 0.057453000985274255\n",
      "125 Train Loss 0.0004888811 Test MSE 0.01396002682357096 Test RE 0.056474342928429504\n",
      "126 Train Loss 0.00048298226 Test MSE 0.013710611917024353 Test RE 0.05596757364069041\n",
      "127 Train Loss 0.00047764505 Test MSE 0.013656556077332266 Test RE 0.05585713502274086\n",
      "128 Train Loss 0.00046917994 Test MSE 0.01345044077629828 Test RE 0.05543401290126239\n",
      "129 Train Loss 0.0004648096 Test MSE 0.013394942353293122 Test RE 0.055319530393545105\n",
      "130 Train Loss 0.0004605853 Test MSE 0.013304634059542427 Test RE 0.0551327337214144\n",
      "131 Train Loss 0.0004529867 Test MSE 0.012980758863103405 Test RE 0.05445755031957188\n",
      "132 Train Loss 0.00044848214 Test MSE 0.012831501779409478 Test RE 0.05414355960260512\n",
      "133 Train Loss 0.00044103578 Test MSE 0.012610600547964167 Test RE 0.053675480993674946\n",
      "134 Train Loss 0.0004353455 Test MSE 0.012460179126564153 Test RE 0.05335439542623412\n",
      "135 Train Loss 0.00043058695 Test MSE 0.012204144237631004 Test RE 0.0528033803788845\n",
      "136 Train Loss 0.00042664708 Test MSE 0.01229094416080958 Test RE 0.052990825257628354\n",
      "137 Train Loss 0.00042184355 Test MSE 0.012084951846193577 Test RE 0.052544894259433074\n",
      "138 Train Loss 0.00041704316 Test MSE 0.012027121543576576 Test RE 0.052419021555609374\n",
      "139 Train Loss 0.00041377387 Test MSE 0.011834854306794016 Test RE 0.05199834464986069\n",
      "140 Train Loss 0.00041091663 Test MSE 0.011624614116996366 Test RE 0.05153441291720075\n",
      "141 Train Loss 0.00040664454 Test MSE 0.011674371814784167 Test RE 0.05164458842947205\n",
      "142 Train Loss 0.00040461187 Test MSE 0.011511017071731624 Test RE 0.051281994679545216\n",
      "143 Train Loss 0.00039925502 Test MSE 0.011398193597215932 Test RE 0.05103005951690721\n",
      "144 Train Loss 0.0003969284 Test MSE 0.011264648497685489 Test RE 0.05073023598403829\n",
      "145 Train Loss 0.00039480958 Test MSE 0.01114160066102422 Test RE 0.050452402809127744\n",
      "146 Train Loss 0.0003909309 Test MSE 0.011169667268682468 Test RE 0.05051590971387947\n",
      "147 Train Loss 0.00038975777 Test MSE 0.011068522219577841 Test RE 0.0502866703990069\n",
      "148 Train Loss 0.00038728965 Test MSE 0.010898922546783481 Test RE 0.04989991931749413\n",
      "149 Train Loss 0.00038274014 Test MSE 0.010992930438601969 Test RE 0.05011466137777501\n",
      "150 Train Loss 0.00037990804 Test MSE 0.01098069326770877 Test RE 0.0500867601533234\n",
      "151 Train Loss 0.00037388408 Test MSE 0.011095995268848356 Test RE 0.050349039699066594\n",
      "152 Train Loss 0.000366294 Test MSE 0.010951189681387577 Test RE 0.05001942683606583\n",
      "153 Train Loss 0.0003617903 Test MSE 0.010728150403179298 Test RE 0.049507441961957666\n",
      "154 Train Loss 0.00035749582 Test MSE 0.01056570056200095 Test RE 0.04913118165273024\n",
      "155 Train Loss 0.00035351544 Test MSE 0.010503787067541856 Test RE 0.048987019300233144\n",
      "156 Train Loss 0.00034618672 Test MSE 0.010397923355943806 Test RE 0.04873953328788602\n",
      "157 Train Loss 0.00034042037 Test MSE 0.010454954184737417 Test RE 0.04887301450409274\n",
      "158 Train Loss 0.00033512776 Test MSE 0.010116548666402509 Test RE 0.04807554849529747\n",
      "159 Train Loss 0.000333777 Test MSE 0.01001638890270268 Test RE 0.04783696844604371\n",
      "160 Train Loss 0.00033132185 Test MSE 0.009749359597652944 Test RE 0.04719501244346\n",
      "161 Train Loss 0.00032888813 Test MSE 0.009730337156909463 Test RE 0.04714894774228428\n",
      "162 Train Loss 0.00032442252 Test MSE 0.00968102638831004 Test RE 0.047029326815471915\n",
      "163 Train Loss 0.00032060844 Test MSE 0.009427023820296655 Test RE 0.0464082682207725\n",
      "164 Train Loss 0.0003117603 Test MSE 0.009212150455289964 Test RE 0.04587631979508057\n",
      "165 Train Loss 0.0003065226 Test MSE 0.008903104278653496 Test RE 0.045100233496259017\n",
      "166 Train Loss 0.00030278484 Test MSE 0.008530862773939277 Test RE 0.0441473396990364\n",
      "167 Train Loss 0.00029844718 Test MSE 0.00828484998428008 Test RE 0.043506123141458145\n",
      "168 Train Loss 0.00029446516 Test MSE 0.008267846380053238 Test RE 0.04346145480967683\n",
      "169 Train Loss 0.00029117873 Test MSE 0.008394707433875902 Test RE 0.0437936197120038\n",
      "170 Train Loss 0.00028860202 Test MSE 0.008403747435408617 Test RE 0.04381719336648619\n",
      "171 Train Loss 0.0002850556 Test MSE 0.008340026118719077 Test RE 0.04365075558475951\n",
      "172 Train Loss 0.00027972617 Test MSE 0.008499385247617744 Test RE 0.04406581609757336\n",
      "173 Train Loss 0.00027675935 Test MSE 0.008381717672844697 Test RE 0.04375972401683203\n",
      "174 Train Loss 0.00027359207 Test MSE 0.008512311999746287 Test RE 0.04409931331193716\n",
      "175 Train Loss 0.00026974332 Test MSE 0.008619627688531799 Test RE 0.044376425189917704\n",
      "176 Train Loss 0.00026568523 Test MSE 0.008520988958830474 Test RE 0.04412178373328845\n",
      "177 Train Loss 0.00026392817 Test MSE 0.008573003408994057 Test RE 0.04425624457458249\n",
      "178 Train Loss 0.00026228148 Test MSE 0.008481648258784509 Test RE 0.04401981258907256\n",
      "179 Train Loss 0.00026034875 Test MSE 0.008401471134992332 Test RE 0.043811258642047324\n",
      "180 Train Loss 0.0002575806 Test MSE 0.008303303648870514 Test RE 0.04355454893296198\n",
      "181 Train Loss 0.00025467863 Test MSE 0.008066851715605445 Test RE 0.042929921782123216\n",
      "182 Train Loss 0.00025232427 Test MSE 0.008025364221731153 Test RE 0.042819386048305846\n",
      "183 Train Loss 0.00024801763 Test MSE 0.0077444083738933325 Test RE 0.04206318781836126\n",
      "184 Train Loss 0.00024598904 Test MSE 0.007691156857408685 Test RE 0.04191832250176578\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "185 Train Loss 0.0002441194 Test MSE 0.007721192794337451 Test RE 0.042000093636747796\n",
      "186 Train Loss 0.00024309137 Test MSE 0.007656051980054763 Test RE 0.04182254883717875\n",
      "187 Train Loss 0.00024248721 Test MSE 0.007585143414117928 Test RE 0.04162842296304955\n",
      "188 Train Loss 0.00024211683 Test MSE 0.007551059103288268 Test RE 0.04153478770968025\n",
      "189 Train Loss 0.00024210758 Test MSE 0.0075503181338795215 Test RE 0.04153274979938822\n",
      "190 Train Loss 0.00024210641 Test MSE 0.0075494321074876134 Test RE 0.041530312803260944\n",
      "191 Train Loss 0.00024187885 Test MSE 0.007545607014875323 Test RE 0.04151979032834509\n",
      "192 Train Loss 0.00024187885 Test MSE 0.007545607014875323 Test RE 0.04151979032834509\n",
      "193 Train Loss 0.00024187885 Test MSE 0.007545607014875323 Test RE 0.04151979032834509\n",
      "194 Train Loss 0.00024187885 Test MSE 0.007545607014875323 Test RE 0.04151979032834509\n",
      "195 Train Loss 0.00024187885 Test MSE 0.007545607014875323 Test RE 0.04151979032834509\n",
      "196 Train Loss 0.00024187885 Test MSE 0.007545607014875323 Test RE 0.04151979032834509\n",
      "197 Train Loss 0.00024187885 Test MSE 0.007545607014875323 Test RE 0.04151979032834509\n",
      "198 Train Loss 0.00024187885 Test MSE 0.007545607014875323 Test RE 0.04151979032834509\n",
      "199 Train Loss 0.00024187885 Test MSE 0.007545607014875323 Test RE 0.04151979032834509\n",
      "Training time: 270.43\n",
      "3\n",
      "KG_rowdy\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 48.759926 Test MSE 8.804964530645226 Test RE 1.4183122884282966\n",
      "1 Train Loss 38.689766 Test MSE 9.038784215044423 Test RE 1.43702085031117\n",
      "2 Train Loss 31.860733 Test MSE 8.834089031784238 Test RE 1.4206560534372248\n",
      "3 Train Loss 25.84731 Test MSE 8.501725483266146 Test RE 1.39367528711103\n",
      "4 Train Loss 21.993979 Test MSE 8.723087901347569 Test RE 1.4117025028611556\n",
      "5 Train Loss 17.852306 Test MSE 8.501910705459572 Test RE 1.393690468628485\n",
      "6 Train Loss 14.623796 Test MSE 8.409397349884168 Test RE 1.3860870216749643\n",
      "7 Train Loss 12.670184 Test MSE 8.654023470382937 Test RE 1.4061028698437643\n",
      "8 Train Loss 9.327437 Test MSE 7.318470580081471 Test RE 1.2930586561731106\n",
      "9 Train Loss 6.8297896 Test MSE 7.624462126434046 Test RE 1.3198138029415347\n",
      "10 Train Loss 4.0562487 Test MSE 6.944089639062873 Test RE 1.259550891022996\n",
      "11 Train Loss 3.18751 Test MSE 6.581718196374856 Test RE 1.2262462776358987\n",
      "12 Train Loss 2.4693983 Test MSE 6.357582718779415 Test RE 1.2051859777154525\n",
      "13 Train Loss 2.1451526 Test MSE 6.209365391003065 Test RE 1.1910545942165869\n",
      "14 Train Loss 1.8766181 Test MSE 5.946680091745872 Test RE 1.1655887529445412\n",
      "15 Train Loss 1.6172318 Test MSE 5.793522480175241 Test RE 1.1504808880722237\n",
      "16 Train Loss 1.4445082 Test MSE 5.709480039557528 Test RE 1.1421058072114705\n",
      "17 Train Loss 1.3336729 Test MSE 5.678357277453773 Test RE 1.1389887054981973\n",
      "18 Train Loss 1.2484976 Test MSE 5.679449067206763 Test RE 1.1390981981179895\n",
      "19 Train Loss 1.1816466 Test MSE 5.797828094538236 Test RE 1.1509083143105447\n",
      "20 Train Loss 1.131452 Test MSE 5.830044578502849 Test RE 1.154101480235358\n",
      "21 Train Loss 1.0822927 Test MSE 5.917932944298548 Test RE 1.1627680240421303\n",
      "22 Train Loss 1.0469446 Test MSE 5.892267005376524 Test RE 1.1602438351569444\n",
      "23 Train Loss 1.0155591 Test MSE 5.883275985264081 Test RE 1.1593582882059374\n",
      "24 Train Loss 0.98763144 Test MSE 5.959213214271415 Test RE 1.1668163940309038\n",
      "25 Train Loss 0.9585117 Test MSE 5.998596493145536 Test RE 1.1706656758022747\n",
      "26 Train Loss 0.938009 Test MSE 5.9807314348927605 Test RE 1.1689211339492473\n",
      "27 Train Loss 0.9155768 Test MSE 5.9996824966495454 Test RE 1.1707716413802651\n",
      "28 Train Loss 0.89149904 Test MSE 6.0381839868895755 Test RE 1.1745222037884528\n",
      "29 Train Loss 0.87036043 Test MSE 6.099336102720047 Test RE 1.180454747510631\n",
      "30 Train Loss 0.85199326 Test MSE 6.150515011037914 Test RE 1.1853969396173096\n",
      "31 Train Loss 0.83420545 Test MSE 6.170524339673572 Test RE 1.1873235861733757\n",
      "32 Train Loss 0.81740546 Test MSE 6.174636348859695 Test RE 1.1877191337891253\n",
      "33 Train Loss 0.7988765 Test MSE 6.219008930582395 Test RE 1.191979127179448\n",
      "34 Train Loss 0.7852122 Test MSE 6.263844481522011 Test RE 1.1962681596908356\n",
      "35 Train Loss 0.770557 Test MSE 6.280410266836284 Test RE 1.1978489808976467\n",
      "36 Train Loss 0.7572284 Test MSE 6.334909657328159 Test RE 1.2030350295475716\n",
      "37 Train Loss 0.74702704 Test MSE 6.380069218465215 Test RE 1.207315442194245\n",
      "38 Train Loss 0.73775786 Test MSE 6.371579208304124 Test RE 1.2065118825249679\n",
      "39 Train Loss 0.730588 Test MSE 6.396119676634626 Test RE 1.2088331211776657\n",
      "40 Train Loss 0.7250158 Test MSE 6.41347315667949 Test RE 1.2104718688207607\n",
      "41 Train Loss 0.7184531 Test MSE 6.431383443500224 Test RE 1.2121608743788856\n",
      "42 Train Loss 0.71073425 Test MSE 6.442051615680469 Test RE 1.2131658045675855\n",
      "43 Train Loss 0.7043351 Test MSE 6.448862020106965 Test RE 1.2138069021377447\n",
      "44 Train Loss 0.69684595 Test MSE 6.463973500540833 Test RE 1.2152282138681385\n",
      "45 Train Loss 0.68936074 Test MSE 6.478011859836629 Test RE 1.2165471052417378\n",
      "46 Train Loss 0.6834982 Test MSE 6.472612988070817 Test RE 1.2160400553169712\n",
      "47 Train Loss 0.67866087 Test MSE 6.473890242901405 Test RE 1.2161600313159626\n",
      "48 Train Loss 0.6730452 Test MSE 6.507100821601051 Test RE 1.2192754465240607\n",
      "49 Train Loss 0.66750586 Test MSE 6.52560994650954 Test RE 1.2210082993527547\n",
      "50 Train Loss 0.66146165 Test MSE 6.53575785147742 Test RE 1.2219573188987334\n",
      "51 Train Loss 0.65414256 Test MSE 6.582977530912693 Test RE 1.2263635859421478\n",
      "52 Train Loss 0.64851344 Test MSE 6.602829614901569 Test RE 1.2282113475092074\n",
      "53 Train Loss 0.64098096 Test MSE 6.624464153249436 Test RE 1.2302218533565625\n",
      "54 Train Loss 0.6361922 Test MSE 6.650066017091133 Test RE 1.2325968077185083\n",
      "55 Train Loss 0.6317882 Test MSE 6.654074711641765 Test RE 1.2329682596474842\n",
      "56 Train Loss 0.6289356 Test MSE 6.671426157827971 Test RE 1.2345747830677256\n",
      "57 Train Loss 0.6238698 Test MSE 6.683157421768367 Test RE 1.2356597655721435\n",
      "58 Train Loss 0.6188013 Test MSE 6.679204338728596 Test RE 1.2352942655631882\n",
      "59 Train Loss 0.6151566 Test MSE 6.693501711693341 Test RE 1.2366156820522158\n",
      "60 Train Loss 0.61076903 Test MSE 6.709272154326116 Test RE 1.2380716093050703\n",
      "61 Train Loss 0.6068003 Test MSE 6.722683245661173 Test RE 1.2393083755293168\n",
      "62 Train Loss 0.6028354 Test MSE 6.732679546499025 Test RE 1.240229428704653\n",
      "63 Train Loss 0.59909874 Test MSE 6.728792528309394 Test RE 1.2398713624755358\n",
      "64 Train Loss 0.59667695 Test MSE 6.726953327241697 Test RE 1.2397019020135984\n",
      "65 Train Loss 0.59482706 Test MSE 6.7334132360183965 Test RE 1.2402970034747276\n",
      "66 Train Loss 0.5918563 Test MSE 6.745084129680903 Test RE 1.2413714293928655\n",
      "67 Train Loss 0.5892399 Test MSE 6.759070414261185 Test RE 1.2426577871849283\n",
      "68 Train Loss 0.5868137 Test MSE 6.766639556488958 Test RE 1.2433533874038516\n",
      "69 Train Loss 0.5842699 Test MSE 6.789565842615296 Test RE 1.2454579306558973\n",
      "70 Train Loss 0.58114266 Test MSE 6.78801323999186 Test RE 1.2453155200912769\n",
      "71 Train Loss 0.5785311 Test MSE 6.799507349204659 Test RE 1.246369417442583\n",
      "72 Train Loss 0.5761489 Test MSE 6.8070898046065285 Test RE 1.2470641668426148\n",
      "73 Train Loss 0.5733252 Test MSE 6.81129323188163 Test RE 1.2474491429997088\n",
      "74 Train Loss 0.57103586 Test MSE 6.825588078275815 Test RE 1.2487574663266932\n",
      "75 Train Loss 0.56906253 Test MSE 6.8253439663050015 Test RE 1.2487351356961094\n",
      "76 Train Loss 0.5668373 Test MSE 6.831354686572804 Test RE 1.2492848622639976\n",
      "77 Train Loss 0.5643497 Test MSE 6.829095437588591 Test RE 1.2490782649577357\n",
      "78 Train Loss 0.56179285 Test MSE 6.828250185726211 Test RE 1.2490009620110563\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79 Train Loss 0.55924284 Test MSE 6.854771479364715 Test RE 1.251424204989911\n",
      "80 Train Loss 0.55589443 Test MSE 6.874005484925709 Test RE 1.2531786791708992\n",
      "81 Train Loss 0.55356205 Test MSE 6.878139803845533 Test RE 1.2535554799709805\n",
      "82 Train Loss 0.55069584 Test MSE 6.89496434171314 Test RE 1.2550876973067184\n",
      "83 Train Loss 0.5482203 Test MSE 6.90120656149396 Test RE 1.255655703206012\n",
      "84 Train Loss 0.54516315 Test MSE 6.909049649143681 Test RE 1.2563690147959523\n",
      "85 Train Loss 0.5427476 Test MSE 6.909122536496077 Test RE 1.2563756418412233\n",
      "86 Train Loss 0.5401714 Test MSE 6.925577794026491 Test RE 1.2578708888578487\n",
      "87 Train Loss 0.53764844 Test MSE 6.940803364642966 Test RE 1.2592528159746403\n",
      "88 Train Loss 0.53457606 Test MSE 6.946324765769961 Test RE 1.2597535834908595\n",
      "89 Train Loss 0.5323504 Test MSE 6.948409594382116 Test RE 1.2599426167831007\n",
      "90 Train Loss 0.53024846 Test MSE 6.962566955565902 Test RE 1.2612255280562796\n",
      "91 Train Loss 0.52785456 Test MSE 6.9800651071353546 Test RE 1.2628093740071444\n",
      "92 Train Loss 0.5258775 Test MSE 7.002638071307344 Test RE 1.264849637350667\n",
      "93 Train Loss 0.5237986 Test MSE 7.014744051442147 Test RE 1.265942484952402\n",
      "94 Train Loss 0.5220408 Test MSE 7.021574073331213 Test RE 1.266558637955407\n",
      "95 Train Loss 0.51996505 Test MSE 7.036186663566284 Test RE 1.2678758698971766\n",
      "96 Train Loss 0.5176713 Test MSE 7.043107557149549 Test RE 1.2684992670360955\n",
      "97 Train Loss 0.51571393 Test MSE 7.063735343203305 Test RE 1.2703554931661978\n",
      "98 Train Loss 0.51252604 Test MSE 7.088482154997081 Test RE 1.2725788042683244\n",
      "99 Train Loss 0.5107673 Test MSE 7.092485093429733 Test RE 1.2729380726934685\n",
      "100 Train Loss 0.508952 Test MSE 7.103469245109392 Test RE 1.2739233927418223\n",
      "101 Train Loss 0.5068687 Test MSE 7.109132164501601 Test RE 1.2744310804663068\n",
      "102 Train Loss 0.5051687 Test MSE 7.119513541397585 Test RE 1.2753612589018153\n",
      "103 Train Loss 0.5035005 Test MSE 7.122313288845663 Test RE 1.2756120020484494\n",
      "104 Train Loss 0.5018878 Test MSE 7.12560796035684 Test RE 1.2759070070865517\n",
      "105 Train Loss 0.5004978 Test MSE 7.130782319245323 Test RE 1.276370181786951\n",
      "106 Train Loss 0.4991656 Test MSE 7.13944300119966 Test RE 1.2771450534115203\n",
      "107 Train Loss 0.49772537 Test MSE 7.144257078943998 Test RE 1.2775755659522807\n",
      "108 Train Loss 0.4955262 Test MSE 7.138524910519289 Test RE 1.2770629340733215\n",
      "109 Train Loss 0.49405724 Test MSE 7.143113225548814 Test RE 1.277473286762779\n",
      "110 Train Loss 0.4923467 Test MSE 7.14749846820115 Test RE 1.2778653546639118\n",
      "111 Train Loss 0.49117953 Test MSE 7.142957122390894 Test RE 1.2774593279539628\n",
      "112 Train Loss 0.48986387 Test MSE 7.154488730853507 Test RE 1.2784900789323688\n",
      "113 Train Loss 0.48852065 Test MSE 7.148749704514992 Test RE 1.2779772008974644\n",
      "114 Train Loss 0.48686156 Test MSE 7.156576506504377 Test RE 1.278676605591252\n",
      "115 Train Loss 0.4850061 Test MSE 7.165388866309394 Test RE 1.2794636223751716\n",
      "116 Train Loss 0.48368776 Test MSE 7.164735528191095 Test RE 1.2794052904809277\n",
      "117 Train Loss 0.48248398 Test MSE 7.161473039048631 Test RE 1.27911396631916\n",
      "118 Train Loss 0.48149964 Test MSE 7.161040184186546 Test RE 1.279075309532841\n",
      "119 Train Loss 0.4800392 Test MSE 7.171464560533709 Test RE 1.280005950413825\n",
      "120 Train Loss 0.47879407 Test MSE 7.170145278794083 Test RE 1.279888208344978\n",
      "121 Train Loss 0.4777659 Test MSE 7.180932136453371 Test RE 1.2808505865687692\n",
      "122 Train Loss 0.47654277 Test MSE 7.180618547003899 Test RE 1.280822619056483\n",
      "123 Train Loss 0.47556412 Test MSE 7.18791732259559 Test RE 1.2814734029999764\n",
      "124 Train Loss 0.47439432 Test MSE 7.187030563904733 Test RE 1.2813943541826411\n",
      "125 Train Loss 0.47312057 Test MSE 7.184876096226889 Test RE 1.2812022769664881\n",
      "126 Train Loss 0.47211224 Test MSE 7.1921497598861865 Test RE 1.2818506303244126\n",
      "127 Train Loss 0.47111574 Test MSE 7.19892638991308 Test RE 1.2824543845935386\n",
      "128 Train Loss 0.4703753 Test MSE 7.194308757519421 Test RE 1.2820430140279664\n",
      "129 Train Loss 0.4695573 Test MSE 7.1967896131551345 Test RE 1.2822640421765994\n",
      "130 Train Loss 0.4687124 Test MSE 7.200742772843468 Test RE 1.2826161643652207\n",
      "131 Train Loss 0.46811673 Test MSE 7.206486086820968 Test RE 1.2831275698749314\n",
      "132 Train Loss 0.46743923 Test MSE 7.204439976248564 Test RE 1.2829454001435416\n",
      "133 Train Loss 0.46673563 Test MSE 7.203296484542133 Test RE 1.282843581289715\n",
      "134 Train Loss 0.46597567 Test MSE 7.205607631822048 Test RE 1.2830493622598462\n",
      "135 Train Loss 0.46483138 Test MSE 7.2083320778619955 Test RE 1.2832919003338668\n",
      "136 Train Loss 0.46378103 Test MSE 7.2088976563349485 Test RE 1.2833422440215003\n",
      "137 Train Loss 0.46275464 Test MSE 7.208715707245495 Test RE 1.283326048450604\n",
      "138 Train Loss 0.46210885 Test MSE 7.213015231624793 Test RE 1.2837087011564572\n",
      "139 Train Loss 0.46121106 Test MSE 7.214099171574758 Test RE 1.283805152559152\n",
      "140 Train Loss 0.46032864 Test MSE 7.216688559679638 Test RE 1.2840355327817425\n",
      "141 Train Loss 0.45965675 Test MSE 7.212579452525143 Test RE 1.2836699225144033\n",
      "142 Train Loss 0.45887 Test MSE 7.214873102597363 Test RE 1.283874014239906\n",
      "143 Train Loss 0.4582882 Test MSE 7.2109243681199455 Test RE 1.2835226309676602\n",
      "144 Train Loss 0.45755365 Test MSE 7.215913417197531 Test RE 1.2839665719827764\n",
      "145 Train Loss 0.45707756 Test MSE 7.206833005168742 Test RE 1.2831584542153749\n",
      "146 Train Loss 0.4564459 Test MSE 7.211227621409716 Test RE 1.283549619766064\n",
      "147 Train Loss 0.45592678 Test MSE 7.215990583357398 Test RE 1.2839734372608873\n",
      "148 Train Loss 0.45535123 Test MSE 7.219616106096556 Test RE 1.28429594948393\n",
      "149 Train Loss 0.45490712 Test MSE 7.2154965002524 Test RE 1.2839294793014837\n",
      "150 Train Loss 0.4544466 Test MSE 7.219355220842684 Test RE 1.2842727448635731\n",
      "151 Train Loss 0.45392725 Test MSE 7.223579874631387 Test RE 1.2846484580705237\n",
      "152 Train Loss 0.45354417 Test MSE 7.225556662504777 Test RE 1.284824222983798\n",
      "153 Train Loss 0.45318225 Test MSE 7.2289860773789645 Test RE 1.2851290903324677\n",
      "154 Train Loss 0.45263726 Test MSE 7.231041946871594 Test RE 1.2853118178854037\n",
      "155 Train Loss 0.4521947 Test MSE 7.2397328268280585 Test RE 1.2860839843589529\n",
      "156 Train Loss 0.45175737 Test MSE 7.247443315382375 Test RE 1.2867686572052983\n",
      "157 Train Loss 0.45130983 Test MSE 7.251138072917218 Test RE 1.287096613713701\n",
      "158 Train Loss 0.45094746 Test MSE 7.252838340085487 Test RE 1.2872475058786066\n",
      "159 Train Loss 0.45054877 Test MSE 7.257387061113715 Test RE 1.2876511004265876\n",
      "160 Train Loss 0.45014256 Test MSE 7.260892929793733 Test RE 1.287962079472196\n",
      "161 Train Loss 0.4497706 Test MSE 7.263469178657379 Test RE 1.2881905511355236\n",
      "162 Train Loss 0.44952363 Test MSE 7.2660447507459915 Test RE 1.2884189222815705\n",
      "163 Train Loss 0.44929624 Test MSE 7.2678890706749755 Test RE 1.2885824298022526\n",
      "164 Train Loss 0.44903505 Test MSE 7.2652782920582375 Test RE 1.288350966058217\n",
      "165 Train Loss 0.4487433 Test MSE 7.271669241670759 Test RE 1.2889174946787603\n",
      "166 Train Loss 0.44844365 Test MSE 7.273045255713845 Test RE 1.2890394394908247\n",
      "167 Train Loss 0.44817534 Test MSE 7.278787370942342 Test RE 1.2895481914899758\n",
      "168 Train Loss 0.44793853 Test MSE 7.279698865558544 Test RE 1.2896289315485914\n",
      "169 Train Loss 0.4475647 Test MSE 7.280586278963389 Test RE 1.289707533641003\n",
      "170 Train Loss 0.4473508 Test MSE 7.2796309041771865 Test RE 1.289622911713856\n",
      "171 Train Loss 0.4470112 Test MSE 7.2868836746214924 Test RE 1.2902651840017363\n",
      "172 Train Loss 0.44671842 Test MSE 7.291398085366987 Test RE 1.2906647982538273\n",
      "173 Train Loss 0.44647145 Test MSE 7.294152150264091 Test RE 1.2909085264787301\n",
      "174 Train Loss 0.4461681 Test MSE 7.29320629364163 Test RE 1.2908248255948296\n",
      "175 Train Loss 0.4458769 Test MSE 7.295463031979462 Test RE 1.291024520245043\n",
      "176 Train Loss 0.44557762 Test MSE 7.2961475219408145 Test RE 1.2910850834055398\n",
      "177 Train Loss 0.4452199 Test MSE 7.2985953207299845 Test RE 1.2913016395725885\n",
      "178 Train Loss 0.44470337 Test MSE 7.3072542692497935 Test RE 1.2920674033639425\n",
      "179 Train Loss 0.44433692 Test MSE 7.312082327188944 Test RE 1.2924941810543937\n",
      "180 Train Loss 0.4440136 Test MSE 7.319954599719315 Test RE 1.2931897509933286\n",
      "181 Train Loss 0.44372538 Test MSE 7.320635530277617 Test RE 1.2932498983569174\n",
      "182 Train Loss 0.4434034 Test MSE 7.326603138872754 Test RE 1.2937769043125396\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "183 Train Loss 0.44324946 Test MSE 7.327249009183481 Test RE 1.2938339289432255\n",
      "184 Train Loss 0.443043 Test MSE 7.32656520605028 Test RE 1.2937735551019829\n",
      "185 Train Loss 0.44282097 Test MSE 7.328173319086891 Test RE 1.293915532953126\n",
      "186 Train Loss 0.4426891 Test MSE 7.331652151051265 Test RE 1.2942226204379437\n",
      "187 Train Loss 0.44239247 Test MSE 7.336510202484359 Test RE 1.294651334099093\n",
      "188 Train Loss 0.4420832 Test MSE 7.3383738273382715 Test RE 1.2948157577238508\n",
      "189 Train Loss 0.44174725 Test MSE 7.345981654627753 Test RE 1.295486763528491\n",
      "190 Train Loss 0.44148922 Test MSE 7.344067837937204 Test RE 1.2953179987663443\n",
      "191 Train Loss 0.4412182 Test MSE 7.346007834536051 Test RE 1.295489071980652\n",
      "192 Train Loss 0.44091824 Test MSE 7.346802826007647 Test RE 1.295559169571359\n",
      "193 Train Loss 0.44065988 Test MSE 7.347088434014156 Test RE 1.2955843518502637\n",
      "194 Train Loss 0.44045037 Test MSE 7.346412028010707 Test RE 1.2955247118165043\n",
      "195 Train Loss 0.44018823 Test MSE 7.348099007056766 Test RE 1.2956734509276155\n",
      "196 Train Loss 0.4399544 Test MSE 7.349314311753192 Test RE 1.2957805924431514\n",
      "197 Train Loss 0.4397867 Test MSE 7.348521250898394 Test RE 1.2957106770371005\n",
      "198 Train Loss 0.43964654 Test MSE 7.3490568904734594 Test RE 1.2957578988689529\n",
      "199 Train Loss 0.43947163 Test MSE 7.349742037132389 Test RE 1.2958182986942317\n",
      "Training time: 282.34\n",
      "4\n",
      "KG_rowdy\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 54.819565 Test MSE 8.337226856746108 Test RE 1.3801264199236047\n",
      "1 Train Loss 42.20137 Test MSE 8.119390116698682 Test RE 1.3619769718046781\n",
      "2 Train Loss 28.604305 Test MSE 5.964641009190819 Test RE 1.1673476553238975\n",
      "3 Train Loss 17.840559 Test MSE 5.6890475772724 Test RE 1.1400603539282885\n",
      "4 Train Loss 12.398645 Test MSE 5.442552865602863 Test RE 1.1150886321213567\n",
      "5 Train Loss 9.202553 Test MSE 5.443512881498278 Test RE 1.1151869734361344\n",
      "6 Train Loss 6.2188234 Test MSE 4.91960311293387 Test RE 1.0601641008238847\n",
      "7 Train Loss 4.4597707 Test MSE 4.447582904981357 Test RE 1.0080221758524035\n",
      "8 Train Loss 3.1525273 Test MSE 3.8186097735162376 Test RE 0.9340297304323426\n",
      "9 Train Loss 2.4653385 Test MSE 3.430958541744767 Test RE 0.8853516385910358\n",
      "10 Train Loss 2.1034298 Test MSE 2.9550680933075193 Test RE 0.8216593594909999\n",
      "11 Train Loss 1.8385527 Test MSE 2.667671290344931 Test RE 0.7806820928946804\n",
      "12 Train Loss 1.6078851 Test MSE 2.218576178479966 Test RE 0.7119430143529518\n",
      "13 Train Loss 1.4288026 Test MSE 1.9672752690656599 Test RE 0.6704102203857967\n",
      "14 Train Loss 1.2424669 Test MSE 1.6354220151536951 Test RE 0.6112557818428784\n",
      "15 Train Loss 0.9481738 Test MSE 1.1081307616847698 Test RE 0.5031570541773495\n",
      "16 Train Loss 0.61682916 Test MSE 0.6849983205516944 Test RE 0.39559683982671107\n",
      "17 Train Loss 0.37726286 Test MSE 0.3149440202901261 Test RE 0.26824074324932773\n",
      "18 Train Loss 0.22106811 Test MSE 0.1576936473234741 Test RE 0.18980828246617942\n",
      "19 Train Loss 0.15201421 Test MSE 0.09297468571310613 Test RE 0.14574391107994686\n",
      "20 Train Loss 0.11004489 Test MSE 0.08547020561381773 Test RE 0.13973829329067713\n",
      "21 Train Loss 0.08942148 Test MSE 0.07258362700085681 Test RE 0.1287737649508674\n",
      "22 Train Loss 0.06948744 Test MSE 0.04368469841797599 Test RE 0.09990166610470347\n",
      "23 Train Loss 0.05471222 Test MSE 0.029646256932988876 Test RE 0.08229870364951256\n",
      "24 Train Loss 0.044898175 Test MSE 0.02095989246186545 Test RE 0.06919944086177922\n",
      "25 Train Loss 0.03804207 Test MSE 0.017963063310947585 Test RE 0.06406167068264167\n",
      "26 Train Loss 0.0308385 Test MSE 0.017250751766967047 Test RE 0.06277866458027286\n",
      "27 Train Loss 0.02599902 Test MSE 0.013046651444231797 Test RE 0.0545955933393158\n",
      "28 Train Loss 0.021710826 Test MSE 0.011791599810653834 Test RE 0.051903234858664576\n",
      "29 Train Loss 0.018883029 Test MSE 0.009097865767343748 Test RE 0.04559086400293596\n",
      "30 Train Loss 0.01600024 Test MSE 0.007342278811979131 Test RE 0.040956562251485217\n",
      "31 Train Loss 0.013505882 Test MSE 0.005978689563752468 Test RE 0.03695822134685928\n",
      "32 Train Loss 0.01216725 Test MSE 0.005535425855708044 Test RE 0.035561787235828916\n",
      "33 Train Loss 0.010607345 Test MSE 0.00449552602028231 Test RE 0.03204780711915294\n",
      "34 Train Loss 0.009448923 Test MSE 0.0038765776973037932 Test RE 0.02975995750303285\n",
      "35 Train Loss 0.008440299 Test MSE 0.0038535976460158427 Test RE 0.029671619035034656\n",
      "36 Train Loss 0.0078619085 Test MSE 0.003321310997917657 Test RE 0.02754627203460864\n",
      "37 Train Loss 0.0071574505 Test MSE 0.0033613767593106076 Test RE 0.02771192256446176\n",
      "38 Train Loss 0.006299779 Test MSE 0.0026951077352674784 Test RE 0.024813962727109427\n",
      "39 Train Loss 0.005762298 Test MSE 0.003084813327808168 Test RE 0.02654743123708041\n",
      "40 Train Loss 0.005218244 Test MSE 0.003041906060038461 Test RE 0.026362158042078233\n",
      "41 Train Loss 0.0047025695 Test MSE 0.0029996151297089083 Test RE 0.02617826308912819\n",
      "42 Train Loss 0.004302273 Test MSE 0.00279081816130941 Test RE 0.02525072374477496\n",
      "43 Train Loss 0.003929512 Test MSE 0.002756918945452799 Test RE 0.025096898786688835\n",
      "44 Train Loss 0.0036188988 Test MSE 0.002182131528408504 Test RE 0.022327933108438645\n",
      "45 Train Loss 0.0032816338 Test MSE 0.0018439928319533014 Test RE 0.020525213086905494\n",
      "46 Train Loss 0.0030920657 Test MSE 0.0017751839748836045 Test RE 0.020138621737175226\n",
      "47 Train Loss 0.0028350558 Test MSE 0.0014295143291035756 Test RE 0.018071837665102446\n",
      "48 Train Loss 0.0026613455 Test MSE 0.0014432399549133138 Test RE 0.01815838968669715\n",
      "49 Train Loss 0.0024654367 Test MSE 0.0013856621092109723 Test RE 0.017792490023126318\n",
      "50 Train Loss 0.002234965 Test MSE 0.0012227432214422529 Test RE 0.016713818903779307\n",
      "51 Train Loss 0.0019406222 Test MSE 0.0010652081647756712 Test RE 0.015600025332456198\n",
      "52 Train Loss 0.0017717611 Test MSE 0.000992548970787892 Test RE 0.015058580396683204\n",
      "53 Train Loss 0.0016511937 Test MSE 0.0009172248661518455 Test RE 0.014475913198421331\n",
      "54 Train Loss 0.0014965711 Test MSE 0.000767729804063799 Test RE 0.013243789233894557\n",
      "55 Train Loss 0.0014005123 Test MSE 0.0007184259648306242 Test RE 0.012811473134980577\n",
      "56 Train Loss 0.001283049 Test MSE 0.0007503244702067617 Test RE 0.01309280245926564\n",
      "57 Train Loss 0.0012173059 Test MSE 0.0006513418729704297 Test RE 0.01219867215540496\n",
      "58 Train Loss 0.0011292261 Test MSE 0.0006014379240881988 Test RE 0.011722047178005499\n",
      "59 Train Loss 0.0010764961 Test MSE 0.0006169702185745478 Test RE 0.011872444846890902\n",
      "60 Train Loss 0.0010228887 Test MSE 0.0005979805433375697 Test RE 0.011688306379408169\n",
      "61 Train Loss 0.00095287175 Test MSE 0.0005661572024246767 Test RE 0.011373040313757516\n",
      "62 Train Loss 0.0008791387 Test MSE 0.0005639776749590491 Test RE 0.011351127887297797\n",
      "63 Train Loss 0.0008303225 Test MSE 0.0005171031576768382 Test RE 0.010869176851288739\n",
      "64 Train Loss 0.000791992 Test MSE 0.0004758764055469238 Test RE 0.010426898514611531\n",
      "65 Train Loss 0.00075930415 Test MSE 0.0004455181737029317 Test RE 0.010088829267842348\n",
      "66 Train Loss 0.000704266 Test MSE 0.0004586035737727825 Test RE 0.010235917518229526\n",
      "67 Train Loss 0.0006623012 Test MSE 0.0004157135387519259 Test RE 0.009745522861074172\n",
      "68 Train Loss 0.0006289363 Test MSE 0.00037976363857504666 Test RE 0.009314611594353116\n",
      "69 Train Loss 0.00059319026 Test MSE 0.00034917167466366804 Test RE 0.008931565000371225\n",
      "70 Train Loss 0.00055272714 Test MSE 0.00032187489981145174 Test RE 0.008575345285246288\n",
      "71 Train Loss 0.00052306714 Test MSE 0.0003126848425701323 Test RE 0.008452038661059761\n",
      "72 Train Loss 0.0004986835 Test MSE 0.0002873303273536581 Test RE 0.008102122292777543\n",
      "73 Train Loss 0.00047639772 Test MSE 0.0002640322518543824 Test RE 0.00776670035591766\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74 Train Loss 0.00045650784 Test MSE 0.00023938302753602112 Test RE 0.007395281975904076\n",
      "75 Train Loss 0.00044877882 Test MSE 0.00023558464472815818 Test RE 0.007336375473056416\n",
      "76 Train Loss 0.00043477738 Test MSE 0.00022764407919752806 Test RE 0.007211676548099957\n",
      "77 Train Loss 0.00042189247 Test MSE 0.0002327515304091703 Test RE 0.0072921288331666495\n",
      "78 Train Loss 0.0004097569 Test MSE 0.00023842150189437288 Test RE 0.007380414781461535\n",
      "79 Train Loss 0.00040220725 Test MSE 0.0002418986588591256 Test RE 0.007434038236829802\n",
      "80 Train Loss 0.00039285712 Test MSE 0.00023175745681761935 Test RE 0.00727653995772077\n",
      "81 Train Loss 0.00037812834 Test MSE 0.00024153162255677893 Test RE 0.0074283962094487555\n",
      "82 Train Loss 0.00036589405 Test MSE 0.00024204548414984937 Test RE 0.00743629401462747\n",
      "83 Train Loss 0.00035325455 Test MSE 0.0002554452687110406 Test RE 0.007639360273014648\n",
      "84 Train Loss 0.00033782556 Test MSE 0.0002348941127777881 Test RE 0.007325615604957702\n",
      "85 Train Loss 0.0003270553 Test MSE 0.0002236040304164532 Test RE 0.007147396480434771\n",
      "86 Train Loss 0.0003183248 Test MSE 0.00021569380980744293 Test RE 0.00701983494799026\n",
      "87 Train Loss 0.00030564697 Test MSE 0.00020813449889999026 Test RE 0.006895727581186615\n",
      "88 Train Loss 0.00029853283 Test MSE 0.000201011825743736 Test RE 0.006776709428456673\n",
      "89 Train Loss 0.00028240756 Test MSE 0.0002065703554322151 Test RE 0.006869767809982152\n",
      "90 Train Loss 0.00027307146 Test MSE 0.00019391481081091076 Test RE 0.006656003638786474\n",
      "91 Train Loss 0.00026312613 Test MSE 0.00018526167052626804 Test RE 0.006505802097691998\n",
      "92 Train Loss 0.00025694972 Test MSE 0.0001811974359009808 Test RE 0.006434044853340977\n",
      "93 Train Loss 0.00024893606 Test MSE 0.00017930500135760824 Test RE 0.006400357931989048\n",
      "94 Train Loss 0.00024086365 Test MSE 0.00017171487547981554 Test RE 0.0062634269972550345\n",
      "95 Train Loss 0.00023547866 Test MSE 0.00017574117285929243 Test RE 0.006336432612092503\n",
      "96 Train Loss 0.00022950055 Test MSE 0.00016464952922131638 Test RE 0.006133216663071397\n",
      "97 Train Loss 0.0002259464 Test MSE 0.00016341539189656886 Test RE 0.006110187538965961\n",
      "98 Train Loss 0.00021733824 Test MSE 0.00015890485186906208 Test RE 0.006025271750976794\n",
      "99 Train Loss 0.00021428596 Test MSE 0.000150483176591683 Test RE 0.005863433924564166\n",
      "100 Train Loss 0.00021200062 Test MSE 0.00014565338366722671 Test RE 0.005768572421642199\n",
      "101 Train Loss 0.00020832136 Test MSE 0.00014663714719852033 Test RE 0.005788020514674915\n",
      "102 Train Loss 0.00020504564 Test MSE 0.00014120154120292054 Test RE 0.005679731148960237\n",
      "103 Train Loss 0.00020086528 Test MSE 0.00015016000827076774 Test RE 0.005857134567511339\n",
      "104 Train Loss 0.00019432824 Test MSE 0.00013689711343141934 Test RE 0.005592489864737163\n",
      "105 Train Loss 0.00018764731 Test MSE 0.0001280990882047145 Test RE 0.0054097983802330885\n",
      "106 Train Loss 0.00018263777 Test MSE 0.00012203092560864707 Test RE 0.005280110519745233\n",
      "107 Train Loss 0.0001789451 Test MSE 0.000114274605656981 Test RE 0.005109553206979269\n",
      "108 Train Loss 0.00017536234 Test MSE 0.00011593049515125506 Test RE 0.0051464399025065175\n",
      "109 Train Loss 0.00017173395 Test MSE 0.00011182764256962494 Test RE 0.005054551724083853\n",
      "110 Train Loss 0.00016670294 Test MSE 0.00010804992385355377 Test RE 0.004968442783512759\n",
      "111 Train Loss 0.0001634018 Test MSE 0.00010286091465504195 Test RE 0.004847672267705236\n",
      "112 Train Loss 0.00016001571 Test MSE 9.804650536816943e-05 Test RE 0.004732865031358636\n",
      "113 Train Loss 0.00015598442 Test MSE 9.018826212425362e-05 Test RE 0.004539239205529786\n",
      "114 Train Loss 0.00015363414 Test MSE 9.03629694010568e-05 Test RE 0.004543633648979441\n",
      "115 Train Loss 0.00015199427 Test MSE 9.516419052896198e-05 Test RE 0.004662779078842969\n",
      "116 Train Loss 0.00014929494 Test MSE 9.38897166675959e-05 Test RE 0.004631451009336555\n",
      "117 Train Loss 0.00014654701 Test MSE 9.70923740172753e-05 Test RE 0.004709779991798355\n",
      "118 Train Loss 0.00014486804 Test MSE 0.0001007061965158371 Test RE 0.0047966293130549285\n",
      "119 Train Loss 0.00014391905 Test MSE 9.873392975721479e-05 Test RE 0.00474942760045224\n",
      "120 Train Loss 0.00014206232 Test MSE 9.639017637527051e-05 Test RE 0.004692717901245202\n",
      "121 Train Loss 0.0001405498 Test MSE 9.660782240483201e-05 Test RE 0.004698012919692509\n",
      "122 Train Loss 0.00013868716 Test MSE 9.631322894640507e-05 Test RE 0.004690844449595316\n",
      "123 Train Loss 0.00013663847 Test MSE 0.0001000977056570457 Test RE 0.004782116167690136\n",
      "124 Train Loss 0.00013388229 Test MSE 9.472953364960731e-05 Test RE 0.004652118406424318\n",
      "125 Train Loss 0.00013154234 Test MSE 9.627488985884261e-05 Test RE 0.004689910722219691\n",
      "126 Train Loss 0.0001297549 Test MSE 9.864512352002949e-05 Test RE 0.004747291183518706\n",
      "127 Train Loss 0.00012949221 Test MSE 9.76211752157489e-05 Test RE 0.004722588183144115\n",
      "128 Train Loss 0.00012880981 Test MSE 9.5162209883662e-05 Test RE 0.004662730555551435\n",
      "129 Train Loss 0.00012706219 Test MSE 9.310645761283658e-05 Test RE 0.004612092002270517\n",
      "130 Train Loss 0.00012393191 Test MSE 9.85425063090176e-05 Test RE 0.004744821317183211\n",
      "131 Train Loss 0.000121069235 Test MSE 9.630137793729248e-05 Test RE 0.004690555844638819\n",
      "132 Train Loss 0.000118777774 Test MSE 9.208411403038044e-05 Test RE 0.0045867008648520875\n",
      "133 Train Loss 0.0001166284 Test MSE 9.011522965757574e-05 Test RE 0.004537400945382943\n",
      "134 Train Loss 0.000115411545 Test MSE 8.907602288569181e-05 Test RE 0.0045111624786043185\n",
      "135 Train Loss 0.000113148766 Test MSE 8.183916882281997e-05 Test RE 0.004324029669759908\n",
      "136 Train Loss 0.00011059203 Test MSE 7.653922275995783e-05 Test RE 0.004181673148854438\n",
      "137 Train Loss 0.000109333145 Test MSE 7.669461825599224e-05 Test RE 0.004185915964888509\n",
      "138 Train Loss 0.00010925853 Test MSE 7.619266592356597e-05 Test RE 0.004172195448996579\n",
      "139 Train Loss 0.00010909622 Test MSE 7.469213431899983e-05 Test RE 0.004130907735482347\n",
      "140 Train Loss 0.000108346576 Test MSE 7.270091499994329e-05 Test RE 0.004075472797466331\n",
      "141 Train Loss 0.00010715761 Test MSE 7.263552518882354e-05 Test RE 0.0040736395719349235\n",
      "142 Train Loss 0.00010655944 Test MSE 7.197634133034051e-05 Test RE 0.00405511284123281\n",
      "143 Train Loss 0.00010572543 Test MSE 7.424226702179954e-05 Test RE 0.004118448813413803\n",
      "144 Train Loss 0.000104283565 Test MSE 7.708003753604685e-05 Test RE 0.004196420684322671\n",
      "145 Train Loss 0.00010238548 Test MSE 7.360218333166625e-05 Test RE 0.004100656667660981\n",
      "146 Train Loss 0.000101919504 Test MSE 7.24909255818887e-05 Test RE 0.004069582740697132\n",
      "147 Train Loss 0.000101626785 Test MSE 7.196096435388152e-05 Test RE 0.004054679652324541\n",
      "148 Train Loss 0.000101626785 Test MSE 7.196096435388152e-05 Test RE 0.004054679652324541\n",
      "149 Train Loss 0.000101626785 Test MSE 7.196096435388152e-05 Test RE 0.004054679652324541\n",
      "150 Train Loss 0.000101626785 Test MSE 7.196096435388152e-05 Test RE 0.004054679652324541\n",
      "151 Train Loss 0.000101626785 Test MSE 7.196096435388152e-05 Test RE 0.004054679652324541\n",
      "152 Train Loss 0.000101626785 Test MSE 7.196096435388152e-05 Test RE 0.004054679652324541\n",
      "153 Train Loss 0.000101626785 Test MSE 7.196096435388152e-05 Test RE 0.004054679652324541\n",
      "154 Train Loss 0.000101626785 Test MSE 7.196096435388152e-05 Test RE 0.004054679652324541\n",
      "155 Train Loss 0.000101626785 Test MSE 7.196096435388152e-05 Test RE 0.004054679652324541\n",
      "156 Train Loss 0.000101626785 Test MSE 7.196096435388152e-05 Test RE 0.004054679652324541\n",
      "157 Train Loss 0.000101626785 Test MSE 7.196096435388152e-05 Test RE 0.004054679652324541\n",
      "158 Train Loss 0.000101626785 Test MSE 7.196096435388152e-05 Test RE 0.004054679652324541\n",
      "159 Train Loss 0.000101626785 Test MSE 7.196096435388152e-05 Test RE 0.004054679652324541\n",
      "160 Train Loss 0.000101626785 Test MSE 7.196096435388152e-05 Test RE 0.004054679652324541\n",
      "161 Train Loss 0.000101626785 Test MSE 7.196096435388152e-05 Test RE 0.004054679652324541\n",
      "162 Train Loss 0.000101626785 Test MSE 7.196096435388152e-05 Test RE 0.004054679652324541\n",
      "163 Train Loss 0.000101626785 Test MSE 7.196096435388152e-05 Test RE 0.004054679652324541\n",
      "164 Train Loss 0.000101626785 Test MSE 7.196096435388152e-05 Test RE 0.004054679652324541\n",
      "165 Train Loss 0.000101626785 Test MSE 7.196096435388152e-05 Test RE 0.004054679652324541\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "166 Train Loss 0.000101626785 Test MSE 7.196096435388152e-05 Test RE 0.004054679652324541\n",
      "167 Train Loss 0.000101626785 Test MSE 7.196096435388152e-05 Test RE 0.004054679652324541\n",
      "168 Train Loss 0.000101626785 Test MSE 7.196096435388152e-05 Test RE 0.004054679652324541\n",
      "169 Train Loss 0.000101626785 Test MSE 7.196096435388152e-05 Test RE 0.004054679652324541\n",
      "170 Train Loss 0.000101626785 Test MSE 7.196096435388152e-05 Test RE 0.004054679652324541\n",
      "171 Train Loss 0.000101626785 Test MSE 7.196096435388152e-05 Test RE 0.004054679652324541\n",
      "172 Train Loss 0.000101626785 Test MSE 7.196096435388152e-05 Test RE 0.004054679652324541\n",
      "173 Train Loss 0.000101626785 Test MSE 7.196096435388152e-05 Test RE 0.004054679652324541\n",
      "174 Train Loss 0.000101626785 Test MSE 7.196096435388152e-05 Test RE 0.004054679652324541\n",
      "175 Train Loss 0.000101626785 Test MSE 7.196096435388152e-05 Test RE 0.004054679652324541\n",
      "176 Train Loss 0.000101626785 Test MSE 7.196096435388152e-05 Test RE 0.004054679652324541\n",
      "177 Train Loss 0.000101626785 Test MSE 7.196096435388152e-05 Test RE 0.004054679652324541\n",
      "178 Train Loss 0.000101626785 Test MSE 7.196096435388152e-05 Test RE 0.004054679652324541\n",
      "179 Train Loss 0.000101626785 Test MSE 7.196096435388152e-05 Test RE 0.004054679652324541\n",
      "180 Train Loss 0.000101626785 Test MSE 7.196096435388152e-05 Test RE 0.004054679652324541\n",
      "181 Train Loss 0.000101626785 Test MSE 7.196096435388152e-05 Test RE 0.004054679652324541\n",
      "182 Train Loss 0.000101626785 Test MSE 7.196096435388152e-05 Test RE 0.004054679652324541\n",
      "183 Train Loss 0.000101626785 Test MSE 7.196096435388152e-05 Test RE 0.004054679652324541\n",
      "184 Train Loss 0.000101626785 Test MSE 7.196096435388152e-05 Test RE 0.004054679652324541\n",
      "185 Train Loss 0.000101626785 Test MSE 7.196096435388152e-05 Test RE 0.004054679652324541\n",
      "186 Train Loss 0.000101626785 Test MSE 7.196096435388152e-05 Test RE 0.004054679652324541\n",
      "187 Train Loss 0.000101626785 Test MSE 7.196096435388152e-05 Test RE 0.004054679652324541\n",
      "188 Train Loss 0.000101626785 Test MSE 7.196096435388152e-05 Test RE 0.004054679652324541\n",
      "189 Train Loss 0.000101626785 Test MSE 7.196096435388152e-05 Test RE 0.004054679652324541\n",
      "190 Train Loss 0.000101626785 Test MSE 7.196096435388152e-05 Test RE 0.004054679652324541\n",
      "191 Train Loss 0.000101626785 Test MSE 7.196096435388152e-05 Test RE 0.004054679652324541\n",
      "192 Train Loss 0.000101626785 Test MSE 7.196096435388152e-05 Test RE 0.004054679652324541\n",
      "193 Train Loss 0.000101626785 Test MSE 7.196096435388152e-05 Test RE 0.004054679652324541\n",
      "194 Train Loss 0.000101626785 Test MSE 7.196096435388152e-05 Test RE 0.004054679652324541\n",
      "195 Train Loss 0.000101626785 Test MSE 7.196096435388152e-05 Test RE 0.004054679652324541\n",
      "196 Train Loss 0.000101626785 Test MSE 7.196096435388152e-05 Test RE 0.004054679652324541\n",
      "197 Train Loss 0.000101626785 Test MSE 7.196096435388152e-05 Test RE 0.004054679652324541\n",
      "198 Train Loss 0.000101626785 Test MSE 7.196096435388152e-05 Test RE 0.004054679652324541\n",
      "199 Train Loss 0.000101626785 Test MSE 7.196096435388152e-05 Test RE 0.004054679652324541\n",
      "Training time: 220.94\n",
      "5\n",
      "KG_rowdy\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 57.415638 Test MSE 8.642042090997474 Test RE 1.4051291674286015\n",
      "1 Train Loss 57.19581 Test MSE 8.436475674831632 Test RE 1.3883168336160376\n",
      "2 Train Loss 49.080856 Test MSE 9.089524551700755 Test RE 1.4410486529984494\n",
      "3 Train Loss 45.11419 Test MSE 8.662016242584253 Test RE 1.4067520514751208\n",
      "4 Train Loss 44.781616 Test MSE 8.684658922609524 Test RE 1.408589490184149\n",
      "5 Train Loss 44.16696 Test MSE 8.584597505318115 Test RE 1.4004513584446685\n",
      "6 Train Loss 43.343765 Test MSE 8.468090321712236 Test RE 1.3909156738100978\n",
      "7 Train Loss 43.100746 Test MSE 8.53272048679436 Test RE 1.3962134583915855\n",
      "8 Train Loss 42.111763 Test MSE 8.621250165762593 Test RE 1.4034378464546478\n",
      "9 Train Loss 40.910484 Test MSE 8.942619327276162 Test RE 1.4293560763744146\n",
      "10 Train Loss 39.674232 Test MSE 9.091464767283547 Test RE 1.4412024451934728\n",
      "11 Train Loss 34.058044 Test MSE 9.407437964763789 Test RE 1.466032985103595\n",
      "12 Train Loss 23.737314 Test MSE 9.235583449632815 Test RE 1.45258056339405\n",
      "13 Train Loss 17.17318 Test MSE 8.646798372431682 Test RE 1.4055157815184836\n",
      "14 Train Loss 11.140911 Test MSE 7.142867973569917 Test RE 1.2774513561611658\n",
      "15 Train Loss 7.430509 Test MSE 6.7142734142319105 Test RE 1.2385329681741721\n",
      "16 Train Loss 4.2524295 Test MSE 6.089590382019082 Test RE 1.1795112856919066\n",
      "17 Train Loss 2.6145093 Test MSE 5.855255105348925 Test RE 1.1565940958386083\n",
      "18 Train Loss 1.9552686 Test MSE 5.866493769877485 Test RE 1.1577035557485493\n",
      "19 Train Loss 1.6231169 Test MSE 5.881284019860421 Test RE 1.159162003251831\n",
      "20 Train Loss 1.4033684 Test MSE 5.885949718850013 Test RE 1.1596217012340648\n",
      "21 Train Loss 1.2918011 Test MSE 5.899862111779925 Test RE 1.1609913689574922\n",
      "22 Train Loss 1.188513 Test MSE 6.009163131873856 Test RE 1.1716962967632025\n",
      "23 Train Loss 1.0947428 Test MSE 6.046275330877216 Test RE 1.1753088874663584\n",
      "24 Train Loss 1.0345705 Test MSE 6.115280471203729 Test RE 1.1819966629539542\n",
      "25 Train Loss 0.9852309 Test MSE 6.134156365439943 Test RE 1.183819478254341\n",
      "26 Train Loss 0.90935147 Test MSE 6.167421516751716 Test RE 1.187025028232391\n",
      "27 Train Loss 0.8650266 Test MSE 6.316796727997645 Test RE 1.2013139247541051\n",
      "28 Train Loss 0.8375962 Test MSE 6.275239920757746 Test RE 1.1973558149499066\n",
      "29 Train Loss 0.8123763 Test MSE 6.321756357172322 Test RE 1.201785437675168\n",
      "30 Train Loss 0.7940481 Test MSE 6.299498154017904 Test RE 1.1996678954899405\n",
      "31 Train Loss 0.77814436 Test MSE 6.317847295603396 Test RE 1.201413817870662\n",
      "32 Train Loss 0.7572702 Test MSE 6.39790836381911 Test RE 1.2090021356172729\n",
      "33 Train Loss 0.74327534 Test MSE 6.418199769397342 Test RE 1.2109178345113147\n",
      "34 Train Loss 0.7289793 Test MSE 6.410136027753707 Test RE 1.210156904822729\n",
      "35 Train Loss 0.7169089 Test MSE 6.457168336605285 Test RE 1.2145883593773632\n",
      "36 Train Loss 0.70477045 Test MSE 6.469764797745634 Test RE 1.2157724744628784\n",
      "37 Train Loss 0.692819 Test MSE 6.516433220252805 Test RE 1.2201494676856646\n",
      "38 Train Loss 0.6803442 Test MSE 6.549555108761429 Test RE 1.2232464403304275\n",
      "39 Train Loss 0.6714782 Test MSE 6.586866803959414 Test RE 1.2267258049029208\n",
      "40 Train Loss 0.6645083 Test MSE 6.609061126522249 Test RE 1.2287907815383845\n",
      "41 Train Loss 0.65769815 Test MSE 6.650736490586345 Test RE 1.2326589426400527\n",
      "42 Train Loss 0.65121925 Test MSE 6.66119299394095 Test RE 1.233627575220903\n",
      "43 Train Loss 0.6431111 Test MSE 6.708463459257318 Test RE 1.2379969922263192\n",
      "44 Train Loss 0.6360705 Test MSE 6.754556244154192 Test RE 1.2422427518902284\n",
      "45 Train Loss 0.6271109 Test MSE 6.748399744298283 Test RE 1.2416764962754865\n",
      "46 Train Loss 0.6183241 Test MSE 6.782736016827519 Test RE 1.2448313514944869\n",
      "47 Train Loss 0.6129845 Test MSE 6.780259464650784 Test RE 1.2446040707004509\n",
      "48 Train Loss 0.6037756 Test MSE 6.800306587322009 Test RE 1.2464426666226085\n",
      "49 Train Loss 0.59836495 Test MSE 6.827071450681165 Test RE 1.2488931521983286\n",
      "50 Train Loss 0.5909031 Test MSE 6.866040105556537 Test RE 1.25245239687346\n",
      "51 Train Loss 0.5845637 Test MSE 6.8777869660231366 Test RE 1.2535233268401258\n",
      "52 Train Loss 0.5779823 Test MSE 6.88141520186869 Test RE 1.2538539185433062\n",
      "53 Train Loss 0.57103264 Test MSE 6.9236834789709745 Test RE 1.2576988478531403\n",
      "54 Train Loss 0.5671142 Test MSE 6.940192976484423 Test RE 1.259197444149029\n",
      "55 Train Loss 0.5599472 Test MSE 6.99802973368402 Test RE 1.2644333789777726\n",
      "56 Train Loss 0.55254257 Test MSE 7.01467174400888 Test RE 1.2659359603175064\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "57 Train Loss 0.5473557 Test MSE 7.049130595987935 Test RE 1.2690415409947333\n",
      "58 Train Loss 0.5402059 Test MSE 7.067294107408705 Test RE 1.2706754603057828\n",
      "59 Train Loss 0.53542745 Test MSE 7.094031169979609 Test RE 1.273076807737396\n",
      "60 Train Loss 0.5303347 Test MSE 7.090137308063019 Test RE 1.2727273684970541\n",
      "61 Train Loss 0.5242831 Test MSE 7.120454740063845 Test RE 1.2754455574033488\n",
      "62 Train Loss 0.5194458 Test MSE 7.1474232705028795 Test RE 1.2778586325368244\n",
      "63 Train Loss 0.5157652 Test MSE 7.155081186132746 Test RE 1.2785430130094915\n",
      "64 Train Loss 0.5109627 Test MSE 7.173716277869969 Test RE 1.2802068846413426\n",
      "65 Train Loss 0.5065073 Test MSE 7.187668073435363 Test RE 1.2814511845377843\n",
      "66 Train Loss 0.5027282 Test MSE 7.215147250918592 Test RE 1.2838984061146628\n",
      "67 Train Loss 0.49860525 Test MSE 7.227855675929221 Test RE 1.2850286081510047\n",
      "68 Train Loss 0.49393728 Test MSE 7.220856804588415 Test RE 1.28440629853658\n",
      "69 Train Loss 0.49064642 Test MSE 7.233765899000824 Test RE 1.2855538852271433\n",
      "70 Train Loss 0.48788232 Test MSE 7.240407009003712 Test RE 1.2861438646566614\n",
      "71 Train Loss 0.48537 Test MSE 7.235836879721232 Test RE 1.2857378949685312\n",
      "72 Train Loss 0.48312965 Test MSE 7.242897157137423 Test RE 1.2863650133722944\n",
      "73 Train Loss 0.4807226 Test MSE 7.249588171719228 Test RE 1.2869590505357673\n",
      "74 Train Loss 0.47864458 Test MSE 7.259460778148541 Test RE 1.2878350532239227\n",
      "75 Train Loss 0.47669518 Test MSE 7.274219633015482 Test RE 1.289143505772648\n",
      "76 Train Loss 0.4740688 Test MSE 7.289045671148606 Test RE 1.2904565787110118\n",
      "77 Train Loss 0.47230726 Test MSE 7.286882554354341 Test RE 1.2902650848206714\n",
      "78 Train Loss 0.47073662 Test MSE 7.300045636573325 Test RE 1.2914299315326712\n",
      "79 Train Loss 0.4687609 Test MSE 7.3169420073064835 Test RE 1.2929236117362526\n",
      "80 Train Loss 0.4666648 Test MSE 7.3217969824284665 Test RE 1.2933524842798105\n",
      "81 Train Loss 0.46518493 Test MSE 7.345644149725449 Test RE 1.2954570031658663\n",
      "82 Train Loss 0.46359658 Test MSE 7.365878383667805 Test RE 1.2972400022552142\n",
      "83 Train Loss 0.4622118 Test MSE 7.381551911724749 Test RE 1.2986194387554995\n",
      "84 Train Loss 0.46101004 Test MSE 7.390854102863899 Test RE 1.2994374376659932\n",
      "85 Train Loss 0.45977998 Test MSE 7.395139568841293 Test RE 1.2998141118538566\n",
      "86 Train Loss 0.4585691 Test MSE 7.409051849059627 Test RE 1.3010361902901841\n",
      "87 Train Loss 0.45758447 Test MSE 7.40051133387168 Test RE 1.3002861129849792\n",
      "88 Train Loss 0.456308 Test MSE 7.4225071137804965 Test RE 1.302217032727112\n",
      "89 Train Loss 0.45553738 Test MSE 7.425149442698662 Test RE 1.3024487993994716\n",
      "90 Train Loss 0.45425066 Test MSE 7.432626357235757 Test RE 1.303104398867421\n",
      "91 Train Loss 0.45311883 Test MSE 7.43575839956171 Test RE 1.3033789282237171\n",
      "92 Train Loss 0.4518203 Test MSE 7.4441481119204225 Test RE 1.3041140174365828\n",
      "93 Train Loss 0.4509075 Test MSE 7.454094036933332 Test RE 1.3049849223741812\n",
      "94 Train Loss 0.44990873 Test MSE 7.474626281966803 Test RE 1.3067809719191374\n",
      "95 Train Loss 0.44906715 Test MSE 7.473287517082437 Test RE 1.30666393925731\n",
      "96 Train Loss 0.44816136 Test MSE 7.497450993290956 Test RE 1.3087746611001552\n",
      "97 Train Loss 0.44702077 Test MSE 7.502076325520892 Test RE 1.3091783039035574\n",
      "98 Train Loss 0.44617862 Test MSE 7.506215114809836 Test RE 1.3095393816770333\n",
      "99 Train Loss 0.44530496 Test MSE 7.514980091126219 Test RE 1.3103037304678713\n",
      "100 Train Loss 0.44441694 Test MSE 7.520062240788366 Test RE 1.3107467146218368\n",
      "101 Train Loss 0.443582 Test MSE 7.527168673979617 Test RE 1.311365893965114\n",
      "102 Train Loss 0.442797 Test MSE 7.54158501834878 Test RE 1.3126210843188562\n",
      "103 Train Loss 0.44211507 Test MSE 7.5570318665816325 Test RE 1.3139646670588419\n",
      "104 Train Loss 0.44159266 Test MSE 7.562938557318076 Test RE 1.3144780741141349\n",
      "105 Train Loss 0.44110394 Test MSE 7.569051772094345 Test RE 1.315009221071438\n",
      "106 Train Loss 0.44057375 Test MSE 7.5770808948231085 Test RE 1.315706506028127\n",
      "107 Train Loss 0.43996358 Test MSE 7.5923602030265265 Test RE 1.3170324098388868\n",
      "108 Train Loss 0.43938088 Test MSE 7.599739827834833 Test RE 1.317672319182832\n",
      "109 Train Loss 0.438708 Test MSE 7.601875681698546 Test RE 1.3178574674817753\n",
      "110 Train Loss 0.43813792 Test MSE 7.605889763108863 Test RE 1.3182053611621314\n",
      "111 Train Loss 0.43746746 Test MSE 7.612655454812345 Test RE 1.3187915245758481\n",
      "112 Train Loss 0.43666515 Test MSE 7.626225366210744 Test RE 1.3199664047662645\n",
      "113 Train Loss 0.43599105 Test MSE 7.642648735286253 Test RE 1.3213869395447646\n",
      "114 Train Loss 0.43530774 Test MSE 7.650230584788531 Test RE 1.3220422145531001\n",
      "115 Train Loss 0.43489003 Test MSE 7.657497377694155 Test RE 1.322669955546569\n",
      "116 Train Loss 0.43437845 Test MSE 7.666490220554082 Test RE 1.3234463889119388\n",
      "117 Train Loss 0.43383473 Test MSE 7.675439108882523 Test RE 1.3242185753691649\n",
      "118 Train Loss 0.43340126 Test MSE 7.676959190777743 Test RE 1.3243496962731847\n",
      "119 Train Loss 0.43281937 Test MSE 7.692340340970675 Test RE 1.3256757309839196\n",
      "120 Train Loss 0.43248767 Test MSE 7.6981878634880365 Test RE 1.3261795078386003\n",
      "121 Train Loss 0.43205935 Test MSE 7.702734658432126 Test RE 1.3265710919550944\n",
      "122 Train Loss 0.43140805 Test MSE 7.710993478352256 Test RE 1.327282071698336\n",
      "123 Train Loss 0.4308904 Test MSE 7.730284780193683 Test RE 1.3289413264073118\n",
      "124 Train Loss 0.430453 Test MSE 7.7374140950887105 Test RE 1.3295539983851172\n",
      "125 Train Loss 0.42996612 Test MSE 7.74698838056365 Test RE 1.330376339927731\n",
      "126 Train Loss 0.4296107 Test MSE 7.75842258751831 Test RE 1.3313577657933022\n",
      "127 Train Loss 0.4291951 Test MSE 7.772217246126236 Test RE 1.332540832923084\n",
      "128 Train Loss 0.42882085 Test MSE 7.786862641285885 Test RE 1.333795713072707\n",
      "129 Train Loss 0.4285034 Test MSE 7.790367896902103 Test RE 1.3340958833146876\n",
      "130 Train Loss 0.42811838 Test MSE 7.803374244867566 Test RE 1.3352090835634687\n",
      "131 Train Loss 0.42779195 Test MSE 7.807936749395841 Test RE 1.335599363915339\n",
      "132 Train Loss 0.4274311 Test MSE 7.821017003922715 Test RE 1.3367176279111956\n",
      "133 Train Loss 0.42713702 Test MSE 7.828467701501522 Test RE 1.337354188856007\n",
      "134 Train Loss 0.42682287 Test MSE 7.837201467291973 Test RE 1.3380999850202704\n",
      "135 Train Loss 0.42641222 Test MSE 7.852986758180711 Test RE 1.3394468734742881\n",
      "136 Train Loss 0.42604128 Test MSE 7.8608764498653185 Test RE 1.3401195582892025\n",
      "137 Train Loss 0.42567784 Test MSE 7.874980093325811 Test RE 1.3413212117254742\n",
      "138 Train Loss 0.4255038 Test MSE 7.878269175316924 Test RE 1.3416012921087845\n",
      "139 Train Loss 0.42519468 Test MSE 7.889942183214172 Test RE 1.3425948305044009\n",
      "140 Train Loss 0.42504472 Test MSE 7.894161196970597 Test RE 1.3429537475212157\n",
      "141 Train Loss 0.42485458 Test MSE 7.898455824686139 Test RE 1.343318998625088\n",
      "142 Train Loss 0.4246192 Test MSE 7.906192663639225 Test RE 1.3439767537063239\n",
      "143 Train Loss 0.42428136 Test MSE 7.910115464682022 Test RE 1.3443101315930472\n",
      "144 Train Loss 0.42393172 Test MSE 7.922625252346904 Test RE 1.3453727197832224\n",
      "145 Train Loss 0.4237186 Test MSE 7.928684481222169 Test RE 1.3458870924134554\n",
      "146 Train Loss 0.42344564 Test MSE 7.93774349811058 Test RE 1.3466557529187777\n",
      "147 Train Loss 0.42318055 Test MSE 7.943137399132907 Test RE 1.3471132188403392\n",
      "148 Train Loss 0.42282033 Test MSE 7.953635192326914 Test RE 1.3480031094382525\n",
      "149 Train Loss 0.4225536 Test MSE 7.963432580908281 Test RE 1.3488330975723568\n",
      "150 Train Loss 0.42231342 Test MSE 7.965638507967704 Test RE 1.3490199027831302\n",
      "151 Train Loss 0.4220722 Test MSE 7.973757310091406 Test RE 1.349707207153467\n",
      "152 Train Loss 0.42181358 Test MSE 7.979611779078569 Test RE 1.3502026052950427\n",
      "153 Train Loss 0.42153466 Test MSE 7.985076400322041 Test RE 1.3506648510321377\n",
      "154 Train Loss 0.42130142 Test MSE 7.995163531935904 Test RE 1.3515176940998297\n",
      "155 Train Loss 0.42105126 Test MSE 8.001846717402007 Test RE 1.3520824453147344\n",
      "156 Train Loss 0.42072785 Test MSE 8.01312405672314 Test RE 1.353034883200172\n",
      "157 Train Loss 0.42047402 Test MSE 8.016359203532016 Test RE 1.3533079867196784\n",
      "158 Train Loss 0.42015505 Test MSE 8.034741809811552 Test RE 1.354858758199274\n",
      "159 Train Loss 0.4199075 Test MSE 8.042329580044594 Test RE 1.3554983513080867\n",
      "160 Train Loss 0.4196454 Test MSE 8.053293690772106 Test RE 1.3564220122965334\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "161 Train Loss 0.41940838 Test MSE 8.065964270868502 Test RE 1.3574886503458474\n",
      "162 Train Loss 0.41912246 Test MSE 8.069311122794154 Test RE 1.3577702560010154\n",
      "163 Train Loss 0.41883013 Test MSE 8.083661355142429 Test RE 1.3589770296312562\n",
      "164 Train Loss 0.41861397 Test MSE 8.09046497998381 Test RE 1.359548801803479\n",
      "165 Train Loss 0.4183696 Test MSE 8.102315707653958 Test RE 1.3605441554061597\n",
      "166 Train Loss 0.4181109 Test MSE 8.110393445350612 Test RE 1.3612221949554648\n",
      "167 Train Loss 0.4178382 Test MSE 8.124760191441476 Test RE 1.3624272956033308\n",
      "168 Train Loss 0.4176394 Test MSE 8.137865567269339 Test RE 1.3635256620150247\n",
      "169 Train Loss 0.41741243 Test MSE 8.144548164284219 Test RE 1.3640853924873815\n",
      "170 Train Loss 0.4171637 Test MSE 8.158790293344351 Test RE 1.3652775393180383\n",
      "171 Train Loss 0.4169087 Test MSE 8.167426152258441 Test RE 1.3659999028973364\n",
      "172 Train Loss 0.41673788 Test MSE 8.178940381749694 Test RE 1.3669624397229285\n",
      "173 Train Loss 0.41650292 Test MSE 8.186107106629967 Test RE 1.3675612030195112\n",
      "174 Train Loss 0.41632348 Test MSE 8.193221009451813 Test RE 1.368155294000771\n",
      "175 Train Loss 0.41610807 Test MSE 8.19481294056845 Test RE 1.3682882028528982\n",
      "176 Train Loss 0.41590175 Test MSE 8.208898697904852 Test RE 1.3694636476599096\n",
      "177 Train Loss 0.41575608 Test MSE 8.212975127505047 Test RE 1.369803634147479\n",
      "178 Train Loss 0.41557318 Test MSE 8.224993085545599 Test RE 1.3708054772366187\n",
      "179 Train Loss 0.41535273 Test MSE 8.233488987289727 Test RE 1.371513272527157\n",
      "180 Train Loss 0.41516078 Test MSE 8.240998619350059 Test RE 1.3721385975025921\n",
      "181 Train Loss 0.4149877 Test MSE 8.250392379828384 Test RE 1.3729204123577026\n",
      "182 Train Loss 0.4148129 Test MSE 8.261687293116163 Test RE 1.373859865452108\n",
      "183 Train Loss 0.414662 Test MSE 8.264144765318608 Test RE 1.374064180338354\n",
      "184 Train Loss 0.41453874 Test MSE 8.276118232186883 Test RE 1.3750592232317687\n",
      "185 Train Loss 0.41437313 Test MSE 8.287978179237305 Test RE 1.376044122841926\n",
      "186 Train Loss 0.41430178 Test MSE 8.292356699477574 Test RE 1.376407555356457\n",
      "187 Train Loss 0.41420734 Test MSE 8.296704823070431 Test RE 1.3767683699023927\n",
      "188 Train Loss 0.41405115 Test MSE 8.308897986802133 Test RE 1.3777796749489322\n",
      "189 Train Loss 0.41392264 Test MSE 8.315825521019937 Test RE 1.3783539163966994\n",
      "190 Train Loss 0.41376263 Test MSE 8.325193982460245 Test RE 1.379130112375477\n",
      "191 Train Loss 0.41356713 Test MSE 8.334915011146034 Test RE 1.3799350577079927\n",
      "192 Train Loss 0.41344017 Test MSE 8.340299080369348 Test RE 1.380380681114139\n",
      "193 Train Loss 0.4133255 Test MSE 8.347384913890304 Test RE 1.380966935335885\n",
      "194 Train Loss 0.41320646 Test MSE 8.352592167354633 Test RE 1.3813976045697356\n",
      "195 Train Loss 0.4129986 Test MSE 8.353766751744779 Test RE 1.3814947307666667\n",
      "196 Train Loss 0.41271234 Test MSE 8.371673449569583 Test RE 1.382974588107599\n",
      "197 Train Loss 0.41257864 Test MSE 8.375943686313843 Test RE 1.3833272580989635\n",
      "198 Train Loss 0.41240814 Test MSE 8.384338915628312 Test RE 1.38402034065889\n",
      "199 Train Loss 0.41225827 Test MSE 8.387154599938599 Test RE 1.3842527165967153\n",
      "Training time: 282.53\n",
      "6\n",
      "KG_rowdy\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 57.226776 Test MSE 8.677493944892948 Test RE 1.4080083162498773\n",
      "1 Train Loss 53.19763 Test MSE 7.4251501246424185 Test RE 1.3024488592095007\n",
      "2 Train Loss 45.64511 Test MSE 8.299285109060472 Test RE 1.3769824416363197\n",
      "3 Train Loss 42.239708 Test MSE 8.598303451114758 Test RE 1.401568874737264\n",
      "4 Train Loss 38.938896 Test MSE 9.314595063864754 Test RE 1.4587808381541956\n",
      "5 Train Loss 35.796642 Test MSE 9.062710603393924 Test RE 1.4389215481663038\n",
      "6 Train Loss 29.546036 Test MSE 8.809316626959761 Test RE 1.4186627650884793\n",
      "7 Train Loss 18.585545 Test MSE 6.899760323424214 Test RE 1.2555241267769712\n",
      "8 Train Loss 10.995906 Test MSE 5.337168272981713 Test RE 1.1042400842690165\n",
      "9 Train Loss 6.4802194 Test MSE 3.9385040375849183 Test RE 0.9485794410860797\n",
      "10 Train Loss 3.0892344 Test MSE 2.8695645739580242 Test RE 0.8096849396321244\n",
      "11 Train Loss 2.016251 Test MSE 2.849316258487138 Test RE 0.8068232197683615\n",
      "12 Train Loss 1.3909779 Test MSE 2.828293720994157 Test RE 0.80384129855462\n",
      "13 Train Loss 1.1159413 Test MSE 2.866446754537327 Test RE 0.8092449534513835\n",
      "14 Train Loss 0.90856224 Test MSE 2.921752612643821 Test RE 0.8170145311328363\n",
      "15 Train Loss 0.7882897 Test MSE 2.936331810567545 Test RE 0.8190503972640152\n",
      "16 Train Loss 0.69243985 Test MSE 3.0293029189032388 Test RE 0.831915875300131\n",
      "17 Train Loss 0.6153619 Test MSE 3.0506584473588783 Test RE 0.8348430837163674\n",
      "18 Train Loss 0.5688047 Test MSE 3.06699559377425 Test RE 0.8370755103443349\n",
      "19 Train Loss 0.5311803 Test MSE 3.0602181855667943 Test RE 0.8361501193760396\n",
      "20 Train Loss 0.4965821 Test MSE 3.1287531350316877 Test RE 0.8454612534788104\n",
      "21 Train Loss 0.46404022 Test MSE 3.1689179747534872 Test RE 0.8508706811807832\n",
      "22 Train Loss 0.43860695 Test MSE 3.2084380381503417 Test RE 0.8561599111354313\n",
      "23 Train Loss 0.41897085 Test MSE 3.223088567588817 Test RE 0.8581124047772637\n",
      "24 Train Loss 0.40524882 Test MSE 3.2532343902801313 Test RE 0.8621160642345685\n",
      "25 Train Loss 0.39485908 Test MSE 3.2932376238848065 Test RE 0.8674003528539784\n",
      "26 Train Loss 0.38445973 Test MSE 3.2898947460109778 Test RE 0.866960003571242\n",
      "27 Train Loss 0.37556618 Test MSE 3.304891270761576 Test RE 0.8689337148258596\n",
      "28 Train Loss 0.36567956 Test MSE 3.337629642611405 Test RE 0.8732269531070506\n",
      "29 Train Loss 0.35798234 Test MSE 3.3409059574475153 Test RE 0.8736554405323118\n",
      "30 Train Loss 0.3538541 Test MSE 3.3555695299434025 Test RE 0.8755706221811039\n",
      "31 Train Loss 0.3471456 Test MSE 3.3662629980019747 Test RE 0.8769646387393576\n",
      "32 Train Loss 0.3393595 Test MSE 3.4054233202763236 Test RE 0.882050830371555\n",
      "33 Train Loss 0.33314002 Test MSE 3.4525583873625556 Test RE 0.8881341634414466\n",
      "34 Train Loss 0.32808352 Test MSE 3.4624481961956506 Test RE 0.8894052782298812\n",
      "35 Train Loss 0.3222093 Test MSE 3.476819691279827 Test RE 0.8912491827085607\n",
      "36 Train Loss 0.31802538 Test MSE 3.4984196554724796 Test RE 0.8940133673970808\n",
      "37 Train Loss 0.31450245 Test MSE 3.5087531115410715 Test RE 0.8953327397072002\n",
      "38 Train Loss 0.30926585 Test MSE 3.5259700021902187 Test RE 0.8975266789913017\n",
      "39 Train Loss 0.3052104 Test MSE 3.563253252142343 Test RE 0.902259379250189\n",
      "40 Train Loss 0.30244416 Test MSE 3.5766112582527168 Test RE 0.9039490026092551\n",
      "41 Train Loss 0.2993873 Test MSE 3.5951257759398847 Test RE 0.9062856525186839\n",
      "42 Train Loss 0.294249 Test MSE 3.624615006975505 Test RE 0.9099949922696051\n",
      "43 Train Loss 0.29112422 Test MSE 3.6376332974076133 Test RE 0.9116277120196921\n",
      "44 Train Loss 0.28825313 Test MSE 3.674994946726252 Test RE 0.9162973560098415\n",
      "45 Train Loss 0.2854031 Test MSE 3.674884125313539 Test RE 0.9162835401904349\n",
      "46 Train Loss 0.28269416 Test MSE 3.6932578839437906 Test RE 0.9185713104665779\n",
      "47 Train Loss 0.28009924 Test MSE 3.706571119302454 Test RE 0.920225426631754\n",
      "48 Train Loss 0.2777176 Test MSE 3.706580170588912 Test RE 0.9202265502064255\n",
      "49 Train Loss 0.27566072 Test MSE 3.7204299028182124 Test RE 0.921944172315166\n",
      "50 Train Loss 0.27357972 Test MSE 3.7273822400672088 Test RE 0.9228051850676533\n",
      "51 Train Loss 0.2712635 Test MSE 3.7480466215978256 Test RE 0.9253596375741264\n",
      "52 Train Loss 0.26899213 Test MSE 3.760857169019129 Test RE 0.926939694154663\n",
      "53 Train Loss 0.26672715 Test MSE 3.7784614260477793 Test RE 0.9291066247983754\n",
      "54 Train Loss 0.2651981 Test MSE 3.7835773485601343 Test RE 0.9297354031708063\n",
      "55 Train Loss 0.26337013 Test MSE 3.791581778048664 Test RE 0.9307183445683678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "56 Train Loss 0.26127434 Test MSE 3.791318528540137 Test RE 0.9306860341219559\n",
      "57 Train Loss 0.25924432 Test MSE 3.8121173470814367 Test RE 0.933235370779174\n",
      "58 Train Loss 0.25768614 Test MSE 3.8210926277674324 Test RE 0.9343333334429539\n",
      "59 Train Loss 0.25574905 Test MSE 3.825914219190632 Test RE 0.934922635219611\n",
      "60 Train Loss 0.25413805 Test MSE 3.8430676921611484 Test RE 0.9370161524510879\n",
      "61 Train Loss 0.25275207 Test MSE 3.8490414860016893 Test RE 0.9377441343922768\n",
      "62 Train Loss 0.25094646 Test MSE 3.851788390634395 Test RE 0.9380786896708635\n",
      "63 Train Loss 0.24923027 Test MSE 3.8691089098885962 Test RE 0.9401854753395861\n",
      "64 Train Loss 0.2473751 Test MSE 3.8788112357324964 Test RE 0.9413635598218738\n",
      "65 Train Loss 0.2457385 Test MSE 3.8866276302353 Test RE 0.9423115778266811\n",
      "66 Train Loss 0.2441642 Test MSE 3.890611153742502 Test RE 0.9427943561303044\n",
      "67 Train Loss 0.24254586 Test MSE 3.9008131588374417 Test RE 0.9440296499354971\n",
      "68 Train Loss 0.24124578 Test MSE 3.9157059091038966 Test RE 0.9458300187046288\n",
      "69 Train Loss 0.24028012 Test MSE 3.9307595233683257 Test RE 0.9476463581424148\n",
      "70 Train Loss 0.23924701 Test MSE 3.9348531927017714 Test RE 0.9481396904494824\n",
      "71 Train Loss 0.23808536 Test MSE 3.938973311498434 Test RE 0.9486359511624177\n",
      "72 Train Loss 0.23707928 Test MSE 3.9370998532725183 Test RE 0.9484103287656983\n",
      "73 Train Loss 0.23610605 Test MSE 3.9518342491995524 Test RE 0.9501833601505788\n",
      "74 Train Loss 0.23527366 Test MSE 3.952236298256963 Test RE 0.9502316934793941\n",
      "75 Train Loss 0.23417044 Test MSE 3.9521595034861314 Test RE 0.9502224615946947\n",
      "76 Train Loss 0.23347676 Test MSE 3.9629370636480843 Test RE 0.9515172103734072\n",
      "77 Train Loss 0.23248002 Test MSE 3.9588740896607413 Test RE 0.9510293170489278\n",
      "78 Train Loss 0.23150826 Test MSE 3.963505920331704 Test RE 0.9515855003175957\n",
      "79 Train Loss 0.23042864 Test MSE 3.9603621483969835 Test RE 0.9512080363630161\n",
      "80 Train Loss 0.22930504 Test MSE 3.9649624071722585 Test RE 0.9517603258991452\n",
      "81 Train Loss 0.22840327 Test MSE 3.9732030746922518 Test RE 0.9527488686125049\n",
      "82 Train Loss 0.22730467 Test MSE 3.970370944305611 Test RE 0.9524092446383053\n",
      "83 Train Loss 0.2264038 Test MSE 3.968189730096763 Test RE 0.9521475947820849\n",
      "84 Train Loss 0.225304 Test MSE 3.9744909448203347 Test RE 0.9529032676413857\n",
      "85 Train Loss 0.22409292 Test MSE 3.9729712291904984 Test RE 0.9527210706673964\n",
      "86 Train Loss 0.22298342 Test MSE 3.992425059817128 Test RE 0.95505074289791\n",
      "87 Train Loss 0.22178438 Test MSE 3.995633929936642 Test RE 0.9554344718597639\n",
      "88 Train Loss 0.22093163 Test MSE 4.00464320319471 Test RE 0.9565110123655681\n",
      "89 Train Loss 0.21987031 Test MSE 4.022016890261741 Test RE 0.9585836237315996\n",
      "90 Train Loss 0.21855132 Test MSE 4.033934933409293 Test RE 0.9600028109935874\n",
      "91 Train Loss 0.21717356 Test MSE 4.034454172212624 Test RE 0.9600645936796661\n",
      "92 Train Loss 0.21610606 Test MSE 4.0438410626193715 Test RE 0.9611808271034731\n",
      "93 Train Loss 0.21482044 Test MSE 4.057644062452905 Test RE 0.9628198475950892\n",
      "94 Train Loss 0.21342593 Test MSE 4.073447914332025 Test RE 0.9646930373822384\n",
      "95 Train Loss 0.21235994 Test MSE 4.066046657401345 Test RE 0.9638162387388831\n",
      "96 Train Loss 0.21128735 Test MSE 4.090057907909492 Test RE 0.966657864748091\n",
      "97 Train Loss 0.21037509 Test MSE 4.099747674182673 Test RE 0.9678022431269209\n",
      "98 Train Loss 0.2093417 Test MSE 4.109794471492792 Test RE 0.9689873603672622\n",
      "99 Train Loss 0.20814297 Test MSE 4.124919201919757 Test RE 0.970768740633069\n",
      "100 Train Loss 0.20737067 Test MSE 4.13006554798878 Test RE 0.9713741288073406\n",
      "101 Train Loss 0.20641822 Test MSE 4.158613292426171 Test RE 0.9747255024552336\n",
      "102 Train Loss 0.20539214 Test MSE 4.170491976727436 Test RE 0.9761166152261801\n",
      "103 Train Loss 0.20443532 Test MSE 4.177544517266724 Test RE 0.9769416011204052\n",
      "104 Train Loss 0.20366307 Test MSE 4.1910004172312965 Test RE 0.9785137040477632\n",
      "105 Train Loss 0.20305383 Test MSE 4.1979965818225145 Test RE 0.9793300948418071\n",
      "106 Train Loss 0.20228685 Test MSE 4.1983583626728604 Test RE 0.9793722929754286\n",
      "107 Train Loss 0.20164183 Test MSE 4.206480431757608 Test RE 0.9803191733130074\n",
      "108 Train Loss 0.20094977 Test MSE 4.219496288396854 Test RE 0.9818346728954251\n",
      "109 Train Loss 0.20031486 Test MSE 4.228755702879207 Test RE 0.9829113692012719\n",
      "110 Train Loss 0.19970989 Test MSE 4.235730457540497 Test RE 0.9837216242307192\n",
      "111 Train Loss 0.19908212 Test MSE 4.231766295216558 Test RE 0.9832611906562209\n",
      "112 Train Loss 0.19855896 Test MSE 4.230105194522772 Test RE 0.9830681913217327\n",
      "113 Train Loss 0.19815253 Test MSE 4.22442748311823 Test RE 0.9824082252686391\n",
      "114 Train Loss 0.19758315 Test MSE 4.234045431795747 Test RE 0.9835259364780448\n",
      "115 Train Loss 0.19714144 Test MSE 4.234875254588846 Test RE 0.9836223114777598\n",
      "116 Train Loss 0.19669145 Test MSE 4.247748708746097 Test RE 0.985116217131158\n",
      "117 Train Loss 0.19623473 Test MSE 4.248767383055956 Test RE 0.9852343329254608\n",
      "118 Train Loss 0.19573627 Test MSE 4.24849553736155 Test RE 0.985202813666874\n",
      "119 Train Loss 0.19525556 Test MSE 4.249636643893142 Test RE 0.9853351129569411\n",
      "120 Train Loss 0.19468094 Test MSE 4.244143569236406 Test RE 0.9846980856089325\n",
      "121 Train Loss 0.19422285 Test MSE 4.248037022912244 Test RE 0.9851496487393875\n",
      "122 Train Loss 0.19385095 Test MSE 4.249053195925625 Test RE 0.9852674705264524\n",
      "123 Train Loss 0.1934899 Test MSE 4.255372777672328 Test RE 0.9859998883861444\n",
      "124 Train Loss 0.1930912 Test MSE 4.264647179947876 Test RE 0.98707377579443\n",
      "125 Train Loss 0.19265416 Test MSE 4.270351118737417 Test RE 0.9877336577056697\n",
      "126 Train Loss 0.192191 Test MSE 4.269381993624511 Test RE 0.9876215718628891\n",
      "127 Train Loss 0.19183625 Test MSE 4.271224950182632 Test RE 0.987834711290722\n",
      "128 Train Loss 0.19148645 Test MSE 4.279091050585848 Test RE 0.9887439156536241\n",
      "129 Train Loss 0.19117217 Test MSE 4.279426525264968 Test RE 0.9887826729554664\n",
      "130 Train Loss 0.19079965 Test MSE 4.283144161104129 Test RE 0.9892120687968268\n",
      "131 Train Loss 0.19048299 Test MSE 4.286190609781651 Test RE 0.9895638020110362\n",
      "132 Train Loss 0.19018883 Test MSE 4.2947339115268255 Test RE 0.9905495180450745\n",
      "133 Train Loss 0.18986998 Test MSE 4.300484460377659 Test RE 0.9912124575908658\n",
      "134 Train Loss 0.18952763 Test MSE 4.311327501361521 Test RE 0.9924612693473219\n",
      "135 Train Loss 0.18905364 Test MSE 4.326022180313558 Test RE 0.9941511773808344\n",
      "136 Train Loss 0.18864661 Test MSE 4.331126543239504 Test RE 0.9947375142284379\n",
      "137 Train Loss 0.1883044 Test MSE 4.330266378224754 Test RE 0.9946387315114142\n",
      "138 Train Loss 0.18783885 Test MSE 4.3431807823113315 Test RE 0.9961208116436353\n",
      "139 Train Loss 0.18737996 Test MSE 4.337346997672267 Test RE 0.9954515893286597\n",
      "140 Train Loss 0.18697298 Test MSE 4.340188245749783 Test RE 0.995777579142572\n",
      "141 Train Loss 0.18653831 Test MSE 4.360790067854648 Test RE 0.9981381386684198\n",
      "142 Train Loss 0.18619639 Test MSE 4.364485620166479 Test RE 0.9985609852728299\n",
      "143 Train Loss 0.18585896 Test MSE 4.365277829882878 Test RE 0.9986516069279043\n",
      "144 Train Loss 0.18545674 Test MSE 4.3704680047454785 Test RE 0.9992451128771724\n",
      "145 Train Loss 0.18514788 Test MSE 4.379075860182911 Test RE 1.0002286604553121\n",
      "146 Train Loss 0.18490055 Test MSE 4.3815110015740375 Test RE 1.0005067282222693\n",
      "147 Train Loss 0.18466769 Test MSE 4.379026444499211 Test RE 1.0002230168989854\n",
      "148 Train Loss 0.184428 Test MSE 4.385540889306587 Test RE 1.0009667297063245\n",
      "149 Train Loss 0.18414113 Test MSE 4.389727174691953 Test RE 1.0014443598308647\n",
      "150 Train Loss 0.1839623 Test MSE 4.397396519841692 Test RE 1.002318795806718\n",
      "151 Train Loss 0.18380128 Test MSE 4.398639509299098 Test RE 1.002460445946969\n",
      "152 Train Loss 0.18350846 Test MSE 4.407889547033682 Test RE 1.0035139452207495\n",
      "153 Train Loss 0.18328193 Test MSE 4.406713919982254 Test RE 1.0033801128275333\n",
      "154 Train Loss 0.18300638 Test MSE 4.411743375129663 Test RE 1.0039525366659492\n",
      "155 Train Loss 0.18272224 Test MSE 4.4118636457989515 Test RE 1.003966221190448\n",
      "156 Train Loss 0.18244493 Test MSE 4.423291840628417 Test RE 1.005265683509151\n",
      "157 Train Loss 0.18222722 Test MSE 4.424234484700491 Test RE 1.005372793472835\n",
      "158 Train Loss 0.1819338 Test MSE 4.431665416591815 Test RE 1.006216749913623\n",
      "159 Train Loss 0.18165106 Test MSE 4.427022233008272 Test RE 1.005689490626537\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "160 Train Loss 0.18129802 Test MSE 4.434029595800618 Test RE 1.0064851094707372\n",
      "161 Train Loss 0.18090601 Test MSE 4.440685095883087 Test RE 1.0072401957628077\n",
      "162 Train Loss 0.18052308 Test MSE 4.453766601717302 Test RE 1.0087226842474424\n",
      "163 Train Loss 0.18016228 Test MSE 4.45689857418766 Test RE 1.0090772983007406\n",
      "164 Train Loss 0.17976648 Test MSE 4.46750710826539 Test RE 1.0102775127832673\n",
      "165 Train Loss 0.17950243 Test MSE 4.475206009137709 Test RE 1.0111476488717095\n",
      "166 Train Loss 0.17914249 Test MSE 4.4849508618398755 Test RE 1.0122479475696051\n",
      "167 Train Loss 0.17866431 Test MSE 4.478506010388604 Test RE 1.0115203885391075\n",
      "168 Train Loss 0.17839551 Test MSE 4.485590634451289 Test RE 1.01232014294468\n",
      "169 Train Loss 0.17794262 Test MSE 4.4920466165553155 Test RE 1.0130483826964365\n",
      "170 Train Loss 0.17765883 Test MSE 4.491198919794501 Test RE 1.0129527917082009\n",
      "171 Train Loss 0.17738126 Test MSE 4.497800175486157 Test RE 1.0136969476920399\n",
      "172 Train Loss 0.17711174 Test MSE 4.505403422604366 Test RE 1.014553381240944\n",
      "173 Train Loss 0.17679128 Test MSE 4.5171372780232275 Test RE 1.015873671630383\n",
      "174 Train Loss 0.1764649 Test MSE 4.532658233118407 Test RE 1.0176174539963856\n",
      "175 Train Loss 0.17620571 Test MSE 4.546196339190187 Test RE 1.0191360266186917\n",
      "176 Train Loss 0.17600481 Test MSE 4.551990435179563 Test RE 1.0197852607542734\n",
      "177 Train Loss 0.17581 Test MSE 4.558119705867375 Test RE 1.0204716019623268\n",
      "178 Train Loss 0.17559579 Test MSE 4.562401366948901 Test RE 1.0209507784901337\n",
      "179 Train Loss 0.17540231 Test MSE 4.566461057064903 Test RE 1.0214049058068642\n",
      "180 Train Loss 0.17510958 Test MSE 4.579786001467447 Test RE 1.0228940512577236\n",
      "181 Train Loss 0.17489666 Test MSE 4.586857616152858 Test RE 1.0236834683224258\n",
      "182 Train Loss 0.17465676 Test MSE 4.593188303457306 Test RE 1.0243896582585155\n",
      "183 Train Loss 0.17417136 Test MSE 4.60248667070531 Test RE 1.0254260119692575\n",
      "184 Train Loss 0.17387527 Test MSE 4.604434830260913 Test RE 1.0256430122859819\n",
      "185 Train Loss 0.17337744 Test MSE 4.609684647356685 Test RE 1.0262275470340392\n",
      "186 Train Loss 0.1730353 Test MSE 4.61517552572168 Test RE 1.0268385664474184\n",
      "187 Train Loss 0.1725615 Test MSE 4.616713948030722 Test RE 1.0270096953380017\n",
      "188 Train Loss 0.1721835 Test MSE 4.622932224045117 Test RE 1.0277011049490803\n",
      "189 Train Loss 0.17186765 Test MSE 4.633851278399931 Test RE 1.028914069510816\n",
      "190 Train Loss 0.1715928 Test MSE 4.637407540967475 Test RE 1.0293088153275851\n",
      "191 Train Loss 0.1712344 Test MSE 4.645131396268312 Test RE 1.0301656437006317\n",
      "192 Train Loss 0.17082992 Test MSE 4.656758715989476 Test RE 1.0314541518199691\n",
      "193 Train Loss 0.17042871 Test MSE 4.66057340798972 Test RE 1.0318765351422834\n",
      "194 Train Loss 0.17006032 Test MSE 4.674055327044754 Test RE 1.033367942791742\n",
      "195 Train Loss 0.1695798 Test MSE 4.683833061300111 Test RE 1.0344482379279587\n",
      "196 Train Loss 0.1691702 Test MSE 4.687798387127261 Test RE 1.0348860264291326\n",
      "197 Train Loss 0.1687322 Test MSE 4.705754515020592 Test RE 1.036866144139021\n",
      "198 Train Loss 0.16822879 Test MSE 4.70843452218653 Test RE 1.0371613585573964\n",
      "199 Train Loss 0.16783363 Test MSE 4.711239748274233 Test RE 1.0374702764003436\n",
      "Training time: 282.23\n",
      "7\n",
      "KG_rowdy\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 53.887405 Test MSE 8.588095639324594 Test RE 1.4007366640487215\n",
      "1 Train Loss 34.65278 Test MSE 6.706768948756583 Test RE 1.2378406277022387\n",
      "2 Train Loss 24.423573 Test MSE 5.830745257563835 Test RE 1.1541708305152019\n",
      "3 Train Loss 17.887972 Test MSE 5.514249159046668 Test RE 1.122409291223562\n",
      "4 Train Loss 12.903082 Test MSE 5.7873059339010196 Test RE 1.1498634798122773\n",
      "5 Train Loss 10.2819195 Test MSE 5.661965852806781 Test RE 1.1373435870873194\n",
      "6 Train Loss 9.099362 Test MSE 5.470510908280554 Test RE 1.1179490325580417\n",
      "7 Train Loss 8.159347 Test MSE 5.175053402344395 Test RE 1.0873402839504034\n",
      "8 Train Loss 6.7837806 Test MSE 4.877611844732813 Test RE 1.0556298897584042\n",
      "9 Train Loss 5.57625 Test MSE 4.178274944241696 Test RE 0.9770270045561268\n",
      "10 Train Loss 4.4651213 Test MSE 2.961401047596023 Test RE 0.8225393301185049\n",
      "11 Train Loss 3.1821027 Test MSE 2.324274505561517 Test RE 0.7287050337566648\n",
      "12 Train Loss 2.4332166 Test MSE 2.1606117521823083 Test RE 0.7025810426439403\n",
      "13 Train Loss 2.046543 Test MSE 2.1291218460752153 Test RE 0.6974423553248974\n",
      "14 Train Loss 1.738852 Test MSE 1.941841414968457 Test RE 0.6660624337912047\n",
      "15 Train Loss 1.495091 Test MSE 1.8156997975544824 Test RE 0.6440655689406876\n",
      "16 Train Loss 1.3131486 Test MSE 1.6470728964299042 Test RE 0.6134292360398657\n",
      "17 Train Loss 1.1180066 Test MSE 1.3096079244558427 Test RE 0.546989144498748\n",
      "18 Train Loss 0.91307724 Test MSE 1.0874462135587561 Test RE 0.49843892763443914\n",
      "19 Train Loss 0.74274445 Test MSE 0.8207195056844504 Test RE 0.43301748961151953\n",
      "20 Train Loss 0.4221646 Test MSE 0.3952873381569154 Test RE 0.3005138634638444\n",
      "21 Train Loss 0.2911564 Test MSE 0.23351993430907542 Test RE 0.2309776934658787\n",
      "22 Train Loss 0.1886122 Test MSE 0.12595252545710936 Test RE 0.16963345210433906\n",
      "23 Train Loss 0.116139285 Test MSE 0.053496412267564186 Test RE 0.11055296349505203\n",
      "24 Train Loss 0.08085903 Test MSE 0.021121704967472878 Test RE 0.06946604067403833\n",
      "25 Train Loss 0.055209003 Test MSE 0.007833462527741331 Test RE 0.042304342094899944\n",
      "26 Train Loss 0.035535038 Test MSE 0.006497157295269176 Test RE 0.038527404342328334\n",
      "27 Train Loss 0.024976866 Test MSE 0.006289092450711994 Test RE 0.037905484298877246\n",
      "28 Train Loss 0.019050144 Test MSE 0.0057805731218853185 Test RE 0.036340718524897744\n",
      "29 Train Loss 0.014363436 Test MSE 0.0044265099165212195 Test RE 0.0318008538584892\n",
      "30 Train Loss 0.011524778 Test MSE 0.0036204865413724417 Test RE 0.028760175308681035\n",
      "31 Train Loss 0.00921168 Test MSE 0.00392387401020078 Test RE 0.029940950790138168\n",
      "32 Train Loss 0.0075978893 Test MSE 0.0027931987626459262 Test RE 0.025261491034086075\n",
      "33 Train Loss 0.006519954 Test MSE 0.002675775494133945 Test RE 0.024724806205958485\n",
      "34 Train Loss 0.005947246 Test MSE 0.0025427343696203626 Test RE 0.024102303838608844\n",
      "35 Train Loss 0.005147849 Test MSE 0.00178748254300104 Test RE 0.020208262043716115\n",
      "36 Train Loss 0.004584766 Test MSE 0.0015806777434290676 Test RE 0.01900333084547767\n",
      "37 Train Loss 0.004072544 Test MSE 0.0019438316340984138 Test RE 0.02107353452550643\n",
      "38 Train Loss 0.0036235433 Test MSE 0.0014971851579913267 Test RE 0.018494637220132615\n",
      "39 Train Loss 0.003167727 Test MSE 0.001465987813792504 Test RE 0.018300933392365593\n",
      "40 Train Loss 0.0028587477 Test MSE 0.001342187946153917 Test RE 0.01751115238640443\n",
      "41 Train Loss 0.0025753796 Test MSE 0.0012610954427916198 Test RE 0.016973915601367573\n",
      "42 Train Loss 0.0023495425 Test MSE 0.001097245115303259 Test RE 0.015832878839137098\n",
      "43 Train Loss 0.0020606848 Test MSE 0.0008360512187750056 Test RE 0.013820523776443416\n",
      "44 Train Loss 0.0018600314 Test MSE 0.0007641633402819784 Test RE 0.01321299162932908\n",
      "45 Train Loss 0.0017292313 Test MSE 0.0007021244727799634 Test RE 0.012665289337669255\n",
      "46 Train Loss 0.001609058 Test MSE 0.0007435075204668278 Test RE 0.013033190498471691\n",
      "47 Train Loss 0.0014892973 Test MSE 0.0006217362083929554 Test RE 0.011918212936057859\n",
      "48 Train Loss 0.0013622482 Test MSE 0.0006221723901802245 Test RE 0.011922392840581721\n",
      "49 Train Loss 0.001212776 Test MSE 0.0005209781745758601 Test RE 0.010909826025666685\n",
      "50 Train Loss 0.0011153608 Test MSE 0.0004656550208579147 Test RE 0.010314310597666073\n",
      "51 Train Loss 0.0010069888 Test MSE 0.00040500225959297966 Test RE 0.00961915191081576\n",
      "52 Train Loss 0.00095195544 Test MSE 0.0003851461342719676 Test RE 0.009380388636003687\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "53 Train Loss 0.0008658771 Test MSE 0.00030985269696080336 Test RE 0.008413674386397738\n",
      "54 Train Loss 0.00083092495 Test MSE 0.00029984019639777896 Test RE 0.008276619447121339\n",
      "55 Train Loss 0.00076051784 Test MSE 0.0002759538884395156 Test RE 0.007940106347889135\n",
      "56 Train Loss 0.00070847676 Test MSE 0.00024348373236346855 Test RE 0.007458354735406665\n",
      "57 Train Loss 0.0006833508 Test MSE 0.0002428364131674868 Test RE 0.007448433848144174\n",
      "58 Train Loss 0.00066509645 Test MSE 0.00023809805035803088 Test RE 0.00737540680887093\n",
      "59 Train Loss 0.00062741776 Test MSE 0.00022790390874465044 Test RE 0.007215791024257847\n",
      "60 Train Loss 0.0006070661 Test MSE 0.0002234899312990029 Test RE 0.007145572685736337\n",
      "61 Train Loss 0.0005876948 Test MSE 0.00021330267758901917 Test RE 0.006980816366910395\n",
      "62 Train Loss 0.00056641625 Test MSE 0.00021591455329080404 Test RE 0.007023426118636329\n",
      "63 Train Loss 0.0005450294 Test MSE 0.0002183428698726648 Test RE 0.007062810718719955\n",
      "64 Train Loss 0.0005161788 Test MSE 0.00020549745518676446 Test RE 0.00685190423283003\n",
      "65 Train Loss 0.0004964973 Test MSE 0.0001963539758380295 Test RE 0.006697734222392659\n",
      "66 Train Loss 0.00047682435 Test MSE 0.00017524260528396966 Test RE 0.006327438181389646\n",
      "67 Train Loss 0.0004655617 Test MSE 0.00017599387376537437 Test RE 0.006340986602043641\n",
      "68 Train Loss 0.00044736272 Test MSE 0.00017055192266019755 Test RE 0.0062421811820773465\n",
      "69 Train Loss 0.0004318354 Test MSE 0.0001588515309009242 Test RE 0.00602426076775369\n",
      "70 Train Loss 0.00041758583 Test MSE 0.00016426097673411877 Test RE 0.006125975572850928\n",
      "71 Train Loss 0.0004048833 Test MSE 0.00015951786620424938 Test RE 0.006036882543686334\n",
      "72 Train Loss 0.00039168284 Test MSE 0.00015040578464847734 Test RE 0.005861925978910422\n",
      "73 Train Loss 0.0003823517 Test MSE 0.00013233042266140032 Test RE 0.005498419993474606\n",
      "74 Train Loss 0.00037138295 Test MSE 0.00012866444328733944 Test RE 0.005421723093809902\n",
      "75 Train Loss 0.00035763287 Test MSE 0.00012189615980979051 Test RE 0.00527719414884783\n",
      "76 Train Loss 0.00034527062 Test MSE 0.0001235903620477515 Test RE 0.005313740756670076\n",
      "77 Train Loss 0.00034017887 Test MSE 0.00011668865216659401 Test RE 0.005163240706583188\n",
      "78 Train Loss 0.00033047947 Test MSE 0.00012375445829717385 Test RE 0.005317267227787104\n",
      "79 Train Loss 0.0003153038 Test MSE 0.00011504272864236986 Test RE 0.005126696963509715\n",
      "80 Train Loss 0.00030159164 Test MSE 0.00011122664155066819 Test RE 0.005040950957717112\n",
      "81 Train Loss 0.00028778653 Test MSE 0.00011113530261087001 Test RE 0.005038880726721182\n",
      "82 Train Loss 0.0002783646 Test MSE 0.00010651890657818756 Test RE 0.004933116935137374\n",
      "83 Train Loss 0.00027338133 Test MSE 0.0001134067039363418 Test RE 0.005090113007800555\n",
      "84 Train Loss 0.0002699595 Test MSE 0.00010831515387343966 Test RE 0.004974537061203881\n",
      "85 Train Loss 0.0002654408 Test MSE 0.00011124100044431889 Test RE 0.00504127633004596\n",
      "86 Train Loss 0.00026003923 Test MSE 0.00010409308677465808 Test RE 0.0048766209933730435\n",
      "87 Train Loss 0.00025593545 Test MSE 0.00011207575877638294 Test RE 0.005060155979452952\n",
      "88 Train Loss 0.00024923688 Test MSE 0.00010275609345327264 Test RE 0.004845201609359044\n",
      "89 Train Loss 0.00024142124 Test MSE 9.920542390590259e-05 Test RE 0.004760754305827741\n",
      "90 Train Loss 0.00023360304 Test MSE 8.64211754636571e-05 Test RE 0.0044434279739049695\n",
      "91 Train Loss 0.00022583568 Test MSE 8.893263359667723e-05 Test RE 0.004507530115565781\n",
      "92 Train Loss 0.0002216975 Test MSE 8.2407524730717e-05 Test RE 0.004339018432096633\n",
      "93 Train Loss 0.00021545656 Test MSE 7.524519736656586e-05 Test RE 0.004146173324097231\n",
      "94 Train Loss 0.00020710813 Test MSE 6.867902060270602e-05 Test RE 0.003961139223044462\n",
      "95 Train Loss 0.00020108189 Test MSE 6.627546585297696e-05 Test RE 0.0038912080778940182\n",
      "96 Train Loss 0.00019194096 Test MSE 6.205589038072012e-05 Test RE 0.0037652998406469008\n",
      "97 Train Loss 0.00018183945 Test MSE 6.248102367389234e-05 Test RE 0.003778175508414171\n",
      "98 Train Loss 0.0001756328 Test MSE 6.327633584184226e-05 Test RE 0.0038021454047434144\n",
      "99 Train Loss 0.00017123166 Test MSE 5.91424299995602e-05 Test RE 0.003675848829188256\n",
      "100 Train Loss 0.00016606407 Test MSE 5.827464319876047e-05 Test RE 0.0036487816222979235\n",
      "101 Train Loss 0.00016291077 Test MSE 5.934394084111735e-05 Test RE 0.0036821057035936063\n",
      "102 Train Loss 0.00015968834 Test MSE 6.0750373480092806e-05 Test RE 0.003725482574631216\n",
      "103 Train Loss 0.00015717547 Test MSE 5.6664933162403264e-05 Test RE 0.0035980338986051313\n",
      "104 Train Loss 0.0001529418 Test MSE 5.746496775701157e-05 Test RE 0.0036233446347695548\n",
      "105 Train Loss 0.0001498538 Test MSE 5.803722202737154e-05 Test RE 0.0036413411462687707\n",
      "106 Train Loss 0.00014671455 Test MSE 5.785274989229012e-05 Test RE 0.003635549513115428\n",
      "107 Train Loss 0.0001433817 Test MSE 5.323418164362787e-05 Test RE 0.0034874127528400152\n",
      "108 Train Loss 0.0001409153 Test MSE 5.036731490121058e-05 Test RE 0.0033922078890067215\n",
      "109 Train Loss 0.00013672719 Test MSE 4.6150892365163524e-05 Test RE 0.0032471183034221912\n",
      "110 Train Loss 0.00013275933 Test MSE 4.601225854382877e-05 Test RE 0.0032422375853429454\n",
      "111 Train Loss 0.00012879849 Test MSE 4.470538012771575e-05 Test RE 0.0031958615470053828\n",
      "112 Train Loss 0.00012494209 Test MSE 4.434639211387602e-05 Test RE 0.0031830041631622447\n",
      "113 Train Loss 0.00012137747 Test MSE 4.022273079895171e-05 Test RE 0.003031404119499211\n",
      "114 Train Loss 0.00011772652 Test MSE 4.2106039408335664e-05 Test RE 0.0031015604965099414\n",
      "115 Train Loss 0.000114140625 Test MSE 4.1441766279829166e-05 Test RE 0.003076997821912476\n",
      "116 Train Loss 0.000110941386 Test MSE 4.272870974688451e-05 Test RE 0.003124409500593436\n",
      "117 Train Loss 0.00010794465 Test MSE 4.0065671510601455e-05 Test RE 0.0030254799089169254\n",
      "118 Train Loss 0.00010511911 Test MSE 4.199226174907632e-05 Test RE 0.003097367190609631\n",
      "119 Train Loss 0.00010277903 Test MSE 4.152528303248043e-05 Test RE 0.00308009676709835\n",
      "120 Train Loss 0.00010045493 Test MSE 4.099375520560217e-05 Test RE 0.0030603205035101156\n",
      "121 Train Loss 9.947105e-05 Test MSE 3.913596319721636e-05 Test RE 0.0029901713343746675\n",
      "122 Train Loss 9.811769e-05 Test MSE 3.856522026867636e-05 Test RE 0.0029682875355329943\n",
      "123 Train Loss 9.7667864e-05 Test MSE 3.677958218187052e-05 Test RE 0.002898754634150105\n",
      "124 Train Loss 9.7508455e-05 Test MSE 3.607648712167878e-05 Test RE 0.002870913988882768\n",
      "125 Train Loss 9.628618e-05 Test MSE 3.4213224228178335e-05 Test RE 0.002795793315097269\n",
      "126 Train Loss 9.4955336e-05 Test MSE 3.46472133628262e-05 Test RE 0.0028134695264746047\n",
      "127 Train Loss 9.381767e-05 Test MSE 3.431598934909402e-05 Test RE 0.002799988982446221\n",
      "128 Train Loss 9.2152404e-05 Test MSE 3.440877298420199e-05 Test RE 0.002803771734898912\n",
      "129 Train Loss 9.167619e-05 Test MSE 3.512747433473268e-05 Test RE 0.0028329018162981064\n",
      "130 Train Loss 9.056153e-05 Test MSE 3.6462170844277315e-05 Test RE 0.0028862192643034656\n",
      "131 Train Loss 8.888245e-05 Test MSE 3.464648807178098e-05 Test RE 0.002813440078291983\n",
      "132 Train Loss 8.778426e-05 Test MSE 3.2469485169979736e-05 Test RE 0.0027236152774073865\n",
      "133 Train Loss 8.62795e-05 Test MSE 3.17744244478281e-05 Test RE 0.002694305927672495\n",
      "134 Train Loss 8.524771e-05 Test MSE 3.148386219680657e-05 Test RE 0.0026819585512923433\n",
      "135 Train Loss 8.502865e-05 Test MSE 3.131443076418903e-05 Test RE 0.002674732292324116\n",
      "136 Train Loss 8.462837e-05 Test MSE 3.0633277142199783e-05 Test RE 0.0026454818744009985\n",
      "137 Train Loss 8.4078696e-05 Test MSE 3.077249968032766e-05 Test RE 0.002651486670619681\n",
      "138 Train Loss 8.390477e-05 Test MSE 3.0938000879852934e-05 Test RE 0.0026586072456029584\n",
      "139 Train Loss 8.3506064e-05 Test MSE 3.0828352586042194e-05 Test RE 0.0026538918391821797\n",
      "140 Train Loss 8.2734055e-05 Test MSE 3.0321333789960258e-05 Test RE 0.0026319777342252034\n",
      "141 Train Loss 8.21733e-05 Test MSE 3.0527626977341404e-05 Test RE 0.002640915973775026\n",
      "142 Train Loss 8.168521e-05 Test MSE 3.0391896569225275e-05 Test RE 0.0026350384791877083\n",
      "143 Train Loss 8.07975e-05 Test MSE 2.9016760431634946e-05 Test RE 0.002574734914896903\n",
      "144 Train Loss 8.020823e-05 Test MSE 2.8558203958284025e-05 Test RE 0.0025543094243374114\n",
      "145 Train Loss 7.961123e-05 Test MSE 2.8346850103947535e-05 Test RE 0.002544839891303293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "146 Train Loss 7.811537e-05 Test MSE 2.8011617973695663e-05 Test RE 0.0025297473966675046\n",
      "147 Train Loss 7.7468314e-05 Test MSE 2.7872302890163577e-05 Test RE 0.0025234487375502147\n",
      "148 Train Loss 7.68641e-05 Test MSE 2.6501749251870497e-05 Test RE 0.0024606244244827777\n",
      "149 Train Loss 7.608475e-05 Test MSE 2.5748063838040497e-05 Test RE 0.0024253831100317447\n",
      "150 Train Loss 7.48593e-05 Test MSE 2.4946532665041396e-05 Test RE 0.0023873338498508335\n",
      "151 Train Loss 7.396743e-05 Test MSE 2.4127157438646892e-05 Test RE 0.0023478002220348702\n",
      "152 Train Loss 7.2592884e-05 Test MSE 2.5181477144948394e-05 Test RE 0.002398549366110095\n",
      "153 Train Loss 7.1564304e-05 Test MSE 2.7094121401229928e-05 Test RE 0.002487972619050127\n",
      "154 Train Loss 7.0834176e-05 Test MSE 2.728395551150366e-05 Test RE 0.002496673356282195\n",
      "155 Train Loss 7.030756e-05 Test MSE 2.6726637052527986e-05 Test RE 0.00247104251864996\n",
      "156 Train Loss 6.981757e-05 Test MSE 2.669734756696394e-05 Test RE 0.0024696881506491536\n",
      "157 Train Loss 6.95271e-05 Test MSE 2.6553198355756006e-05 Test RE 0.0024630117298989314\n",
      "158 Train Loss 6.8678084e-05 Test MSE 2.821182432042942e-05 Test RE 0.002538771683190274\n",
      "159 Train Loss 6.8011424e-05 Test MSE 2.879609030975557e-05 Test RE 0.002564925904478576\n",
      "160 Train Loss 6.789592e-05 Test MSE 2.863445430932022e-05 Test RE 0.0025577171518846404\n",
      "161 Train Loss 6.7855595e-05 Test MSE 2.858147311780019e-05 Test RE 0.0025553498351989796\n",
      "162 Train Loss 6.7855595e-05 Test MSE 2.858147311780019e-05 Test RE 0.0025553498351989796\n",
      "163 Train Loss 6.7855595e-05 Test MSE 2.858147311780019e-05 Test RE 0.0025553498351989796\n",
      "164 Train Loss 6.7855595e-05 Test MSE 2.858147311780019e-05 Test RE 0.0025553498351989796\n",
      "165 Train Loss 6.7855595e-05 Test MSE 2.858147311780019e-05 Test RE 0.0025553498351989796\n",
      "166 Train Loss 6.7855595e-05 Test MSE 2.858147311780019e-05 Test RE 0.0025553498351989796\n",
      "167 Train Loss 6.7855595e-05 Test MSE 2.858147311780019e-05 Test RE 0.0025553498351989796\n",
      "168 Train Loss 6.7855595e-05 Test MSE 2.858147311780019e-05 Test RE 0.0025553498351989796\n",
      "169 Train Loss 6.7855595e-05 Test MSE 2.858147311780019e-05 Test RE 0.0025553498351989796\n",
      "170 Train Loss 6.7855595e-05 Test MSE 2.858147311780019e-05 Test RE 0.0025553498351989796\n",
      "171 Train Loss 6.7855595e-05 Test MSE 2.858147311780019e-05 Test RE 0.0025553498351989796\n",
      "172 Train Loss 6.7855595e-05 Test MSE 2.858147311780019e-05 Test RE 0.0025553498351989796\n",
      "173 Train Loss 6.7855595e-05 Test MSE 2.858147311780019e-05 Test RE 0.0025553498351989796\n",
      "174 Train Loss 6.7855595e-05 Test MSE 2.858147311780019e-05 Test RE 0.0025553498351989796\n",
      "175 Train Loss 6.7855595e-05 Test MSE 2.858147311780019e-05 Test RE 0.0025553498351989796\n",
      "176 Train Loss 6.7855595e-05 Test MSE 2.858147311780019e-05 Test RE 0.0025553498351989796\n",
      "177 Train Loss 6.7855595e-05 Test MSE 2.858147311780019e-05 Test RE 0.0025553498351989796\n",
      "178 Train Loss 6.7855595e-05 Test MSE 2.858147311780019e-05 Test RE 0.0025553498351989796\n",
      "179 Train Loss 6.7855595e-05 Test MSE 2.858147311780019e-05 Test RE 0.0025553498351989796\n",
      "180 Train Loss 6.7855595e-05 Test MSE 2.858147311780019e-05 Test RE 0.0025553498351989796\n",
      "181 Train Loss 6.7855595e-05 Test MSE 2.858147311780019e-05 Test RE 0.0025553498351989796\n",
      "182 Train Loss 6.7855595e-05 Test MSE 2.858147311780019e-05 Test RE 0.0025553498351989796\n",
      "183 Train Loss 6.7855595e-05 Test MSE 2.858147311780019e-05 Test RE 0.0025553498351989796\n",
      "184 Train Loss 6.7855595e-05 Test MSE 2.858147311780019e-05 Test RE 0.0025553498351989796\n",
      "185 Train Loss 6.7855595e-05 Test MSE 2.858147311780019e-05 Test RE 0.0025553498351989796\n",
      "186 Train Loss 6.7855595e-05 Test MSE 2.858147311780019e-05 Test RE 0.0025553498351989796\n",
      "187 Train Loss 6.7855595e-05 Test MSE 2.858147311780019e-05 Test RE 0.0025553498351989796\n",
      "188 Train Loss 6.7855595e-05 Test MSE 2.858147311780019e-05 Test RE 0.0025553498351989796\n",
      "189 Train Loss 6.7855595e-05 Test MSE 2.858147311780019e-05 Test RE 0.0025553498351989796\n",
      "190 Train Loss 6.7855595e-05 Test MSE 2.858147311780019e-05 Test RE 0.0025553498351989796\n",
      "191 Train Loss 6.7855595e-05 Test MSE 2.858147311780019e-05 Test RE 0.0025553498351989796\n",
      "192 Train Loss 6.7855595e-05 Test MSE 2.858147311780019e-05 Test RE 0.0025553498351989796\n",
      "193 Train Loss 6.7855595e-05 Test MSE 2.858147311780019e-05 Test RE 0.0025553498351989796\n",
      "194 Train Loss 6.7855595e-05 Test MSE 2.858147311780019e-05 Test RE 0.0025553498351989796\n",
      "195 Train Loss 6.7855595e-05 Test MSE 2.858147311780019e-05 Test RE 0.0025553498351989796\n",
      "196 Train Loss 6.7855595e-05 Test MSE 2.858147311780019e-05 Test RE 0.0025553498351989796\n",
      "197 Train Loss 6.7855595e-05 Test MSE 2.858147311780019e-05 Test RE 0.0025553498351989796\n",
      "198 Train Loss 6.7855595e-05 Test MSE 2.858147311780019e-05 Test RE 0.0025553498351989796\n",
      "199 Train Loss 6.7855595e-05 Test MSE 2.858147311780019e-05 Test RE 0.0025553498351989796\n",
      "Training time: 237.38\n",
      "8\n",
      "KG_rowdy\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 40.98155 Test MSE 9.02334530690751 Test RE 1.4357930571525828\n",
      "1 Train Loss 25.274685 Test MSE 7.991618787357682 Test RE 1.3512180556916171\n",
      "2 Train Loss 18.642195 Test MSE 7.39530167355461 Test RE 1.299828358024421\n",
      "3 Train Loss 14.841704 Test MSE 7.35249796521031 Test RE 1.2960612220041254\n",
      "4 Train Loss 12.695689 Test MSE 7.0515522938002215 Test RE 1.2692595090957575\n",
      "5 Train Loss 10.077286 Test MSE 6.768342472835742 Test RE 1.2435098309083301\n",
      "6 Train Loss 7.592505 Test MSE 6.360573982975888 Test RE 1.2054694664154615\n",
      "7 Train Loss 5.371128 Test MSE 6.236638522435354 Test RE 1.1936674375006642\n",
      "8 Train Loss 4.113101 Test MSE 5.868610171405563 Test RE 1.1579123640017768\n",
      "9 Train Loss 3.2756472 Test MSE 5.470018908743251 Test RE 1.1178987591207428\n",
      "10 Train Loss 2.7916484 Test MSE 5.28580461661858 Test RE 1.0989137646131524\n",
      "11 Train Loss 2.5443196 Test MSE 4.987356651933075 Test RE 1.0674395093854634\n",
      "12 Train Loss 2.3201213 Test MSE 4.6271466322359025 Test RE 1.0281694403140758\n",
      "13 Train Loss 2.0451643 Test MSE 4.267640344971954 Test RE 0.9874201064937463\n",
      "14 Train Loss 1.8368169 Test MSE 4.023671722616246 Test RE 0.9587808049101237\n",
      "15 Train Loss 1.5706081 Test MSE 3.5030741292185112 Test RE 0.8946078899275303\n",
      "16 Train Loss 1.3140309 Test MSE 3.091500579323418 Test RE 0.8404129316551321\n",
      "17 Train Loss 1.1047822 Test MSE 2.8208582268478555 Test RE 0.8027839665835176\n",
      "18 Train Loss 1.0067483 Test MSE 2.7253510249878916 Test RE 0.7890768168077831\n",
      "19 Train Loss 0.9146354 Test MSE 2.673498084857022 Test RE 0.7815342205385237\n",
      "20 Train Loss 0.8463767 Test MSE 2.6754856775707156 Test RE 0.78182467953073\n",
      "21 Train Loss 0.79372704 Test MSE 2.694906519101352 Test RE 0.7846571071158758\n",
      "22 Train Loss 0.7618044 Test MSE 2.6721313304053416 Test RE 0.7813344257577137\n",
      "23 Train Loss 0.73859876 Test MSE 2.6572596195420104 Test RE 0.7791571390140407\n",
      "24 Train Loss 0.71794194 Test MSE 2.660508167690682 Test RE 0.7796332604188556\n",
      "25 Train Loss 0.6986181 Test MSE 2.6455806315990555 Test RE 0.7774430070091375\n",
      "26 Train Loss 0.67776155 Test MSE 2.6445948175474308 Test RE 0.7772981454915172\n",
      "27 Train Loss 0.6568495 Test MSE 2.6711359849896787 Test RE 0.7811888921035852\n",
      "28 Train Loss 0.6453445 Test MSE 2.698153684835877 Test RE 0.7851296920837877\n",
      "29 Train Loss 0.63287574 Test MSE 2.702416444544286 Test RE 0.7857496527602102\n",
      "30 Train Loss 0.6219404 Test MSE 2.6879317772411095 Test RE 0.7836410558399912\n",
      "31 Train Loss 0.6085801 Test MSE 2.709725898094741 Test RE 0.7868115768354532\n",
      "32 Train Loss 0.5974529 Test MSE 2.705915115659653 Test RE 0.7862581218424328\n",
      "33 Train Loss 0.58795005 Test MSE 2.707203778190274 Test RE 0.7864453229800374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "34 Train Loss 0.57587814 Test MSE 2.7233799174309326 Test RE 0.7887914156628624\n",
      "35 Train Loss 0.5659981 Test MSE 2.7185976041775763 Test RE 0.7880985440753254\n",
      "36 Train Loss 0.5557211 Test MSE 2.7303142073531372 Test RE 0.7897949904858362\n",
      "37 Train Loss 0.5440046 Test MSE 2.749357211186172 Test RE 0.7925444790281463\n",
      "38 Train Loss 0.528408 Test MSE 2.7985426307003145 Test RE 0.7996022799379336\n",
      "39 Train Loss 0.51887894 Test MSE 2.8121865097658927 Test RE 0.8015490810830727\n",
      "40 Train Loss 0.5113238 Test MSE 2.833016016222614 Test RE 0.8045120904038588\n",
      "41 Train Loss 0.50467056 Test MSE 2.8564381850046594 Test RE 0.8078309263541283\n",
      "42 Train Loss 0.49803782 Test MSE 2.8862671059619727 Test RE 0.8120379388865488\n",
      "43 Train Loss 0.4915623 Test MSE 2.8768929110793917 Test RE 0.8107181731627062\n",
      "44 Train Loss 0.48544776 Test MSE 2.8904628182667302 Test RE 0.8126279466968209\n",
      "45 Train Loss 0.48076677 Test MSE 2.911550599494314 Test RE 0.8155868808630113\n",
      "46 Train Loss 0.47384775 Test MSE 2.9388156263379495 Test RE 0.8193967375948819\n",
      "47 Train Loss 0.46900323 Test MSE 2.9387361169096824 Test RE 0.819385653162397\n",
      "48 Train Loss 0.46351817 Test MSE 2.9656507895443203 Test RE 0.8231293087570131\n",
      "49 Train Loss 0.45829698 Test MSE 2.9808052778256275 Test RE 0.8252297260436722\n",
      "50 Train Loss 0.45408046 Test MSE 2.9842295942923176 Test RE 0.8257035974251413\n",
      "51 Train Loss 0.44836733 Test MSE 2.9955171546711137 Test RE 0.8272636956641252\n",
      "52 Train Loss 0.44285035 Test MSE 3.0112515507056616 Test RE 0.8294335124137592\n",
      "53 Train Loss 0.43823373 Test MSE 3.032397377376854 Test RE 0.8323406713851375\n",
      "54 Train Loss 0.43159544 Test MSE 3.0565862083307813 Test RE 0.835653785516901\n",
      "55 Train Loss 0.42531458 Test MSE 3.070473286909881 Test RE 0.8375499595091248\n",
      "56 Train Loss 0.4202764 Test MSE 3.07102729833917 Test RE 0.8376255164799963\n",
      "57 Train Loss 0.4128469 Test MSE 3.107017330287633 Test RE 0.8425193773264511\n",
      "58 Train Loss 0.40721095 Test MSE 3.1318980946920174 Test RE 0.8458860670672942\n",
      "59 Train Loss 0.4025185 Test MSE 3.1582046547670166 Test RE 0.8494311728956262\n",
      "60 Train Loss 0.396428 Test MSE 3.1790595222351223 Test RE 0.8522311222689084\n",
      "61 Train Loss 0.3906619 Test MSE 3.211833609152122 Test RE 0.8566128391585528\n",
      "62 Train Loss 0.38568217 Test MSE 3.217732835410465 Test RE 0.8573991555285116\n",
      "63 Train Loss 0.38120517 Test MSE 3.2292150108200226 Test RE 0.858927567157401\n",
      "64 Train Loss 0.37708214 Test MSE 3.250624324411721 Test RE 0.8617701575223407\n",
      "65 Train Loss 0.37229303 Test MSE 3.256042082684819 Test RE 0.8624880071069857\n",
      "66 Train Loss 0.366653 Test MSE 3.265707890271913 Test RE 0.8637672389302287\n",
      "67 Train Loss 0.36246508 Test MSE 3.2677031428007286 Test RE 0.8640310669675324\n",
      "68 Train Loss 0.35857996 Test MSE 3.278227910415359 Test RE 0.8654214037256127\n",
      "69 Train Loss 0.35516477 Test MSE 3.2750627059613224 Test RE 0.865003510693962\n",
      "70 Train Loss 0.35269743 Test MSE 3.283657167354003 Test RE 0.8661377436960127\n",
      "71 Train Loss 0.34971857 Test MSE 3.2939821579972186 Test RE 0.8674983981104423\n",
      "72 Train Loss 0.34727114 Test MSE 3.2890908064180744 Test RE 0.8668540691704207\n",
      "73 Train Loss 0.3438123 Test MSE 3.3238761600282802 Test RE 0.8714259279863532\n",
      "74 Train Loss 0.34058958 Test MSE 3.321385157070221 Test RE 0.8710993316704482\n",
      "75 Train Loss 0.33702165 Test MSE 3.3309287025610717 Test RE 0.8723499262918554\n",
      "76 Train Loss 0.33330035 Test MSE 3.336478745572512 Test RE 0.8730763850282216\n",
      "77 Train Loss 0.32994628 Test MSE 3.343239366359037 Test RE 0.8739604834733073\n",
      "78 Train Loss 0.3276996 Test MSE 3.339604209834247 Test RE 0.8734852187866702\n",
      "79 Train Loss 0.32554793 Test MSE 3.350502117811557 Test RE 0.8749092510626304\n",
      "80 Train Loss 0.3217054 Test MSE 3.3490205617015234 Test RE 0.8747157919291664\n",
      "81 Train Loss 0.31892243 Test MSE 3.3576002882155866 Test RE 0.8758355255501655\n",
      "82 Train Loss 0.3169619 Test MSE 3.3601040258495805 Test RE 0.8761620167239738\n",
      "83 Train Loss 0.31522566 Test MSE 3.358347028988742 Test RE 0.8759329144114463\n",
      "84 Train Loss 0.31297007 Test MSE 3.3585898292119163 Test RE 0.8759645777359047\n",
      "85 Train Loss 0.31080136 Test MSE 3.3623077120021967 Test RE 0.8764492800976031\n",
      "86 Train Loss 0.30942026 Test MSE 3.3592580399257828 Test RE 0.8760517124910895\n",
      "87 Train Loss 0.30770344 Test MSE 3.3613065397577455 Test RE 0.8763187831148891\n",
      "88 Train Loss 0.30565578 Test MSE 3.375640197012736 Test RE 0.8781852434575601\n",
      "89 Train Loss 0.30393666 Test MSE 3.378800598081723 Test RE 0.8785962422772854\n",
      "90 Train Loss 0.301285 Test MSE 3.384020124147673 Test RE 0.8792746022720948\n",
      "91 Train Loss 0.2995091 Test MSE 3.393531932294736 Test RE 0.8805094682356264\n",
      "92 Train Loss 0.2973898 Test MSE 3.387440547491322 Test RE 0.8797188566925144\n",
      "93 Train Loss 0.29478118 Test MSE 3.3990165408427213 Test RE 0.8812207183690484\n",
      "94 Train Loss 0.2928586 Test MSE 3.3952003856559116 Test RE 0.8807258959109184\n",
      "95 Train Loss 0.29073474 Test MSE 3.395960481751821 Test RE 0.8808244760793779\n",
      "96 Train Loss 0.28945556 Test MSE 3.393870048112962 Test RE 0.8805533320874924\n",
      "97 Train Loss 0.28794318 Test MSE 3.4051871689880504 Test RE 0.8820202466479501\n",
      "98 Train Loss 0.28618526 Test MSE 3.4118708094861376 Test RE 0.8828854291006798\n",
      "99 Train Loss 0.2841453 Test MSE 3.417639751028062 Test RE 0.8836315246492987\n",
      "100 Train Loss 0.2824875 Test MSE 3.4199770024383427 Test RE 0.8839336214470654\n",
      "101 Train Loss 0.28093725 Test MSE 3.4159559116044655 Test RE 0.8834138193002056\n",
      "102 Train Loss 0.2793386 Test MSE 3.4163593810798463 Test RE 0.8834659891752383\n",
      "103 Train Loss 0.27818605 Test MSE 3.4198634110309967 Test RE 0.8839189418018896\n",
      "104 Train Loss 0.2765842 Test MSE 3.413487392097113 Test RE 0.8830945648343106\n",
      "105 Train Loss 0.2753296 Test MSE 3.414730255190557 Test RE 0.8832553191638088\n",
      "106 Train Loss 0.27391106 Test MSE 3.4158509437849327 Test RE 0.8834002461252092\n",
      "107 Train Loss 0.27242854 Test MSE 3.4262317189139857 Test RE 0.8847415550439739\n",
      "108 Train Loss 0.27090895 Test MSE 3.4197698759564963 Test RE 0.8839068538937684\n",
      "109 Train Loss 0.26973373 Test MSE 3.4226924438450723 Test RE 0.8842844708378363\n",
      "110 Train Loss 0.2683239 Test MSE 3.4225623784424495 Test RE 0.8842676688741328\n",
      "111 Train Loss 0.26689002 Test MSE 3.4308450995526956 Test RE 0.8853370017104615\n",
      "112 Train Loss 0.26572213 Test MSE 3.4245539751529677 Test RE 0.8845249101253773\n",
      "113 Train Loss 0.26474458 Test MSE 3.4192656237341823 Test RE 0.8838416845339774\n",
      "114 Train Loss 0.26289073 Test MSE 3.425063485265636 Test RE 0.8845907081282902\n",
      "115 Train Loss 0.26158828 Test MSE 3.4191501459235503 Test RE 0.883826759550711\n",
      "116 Train Loss 0.26046205 Test MSE 3.421065093912089 Test RE 0.8840742253381589\n",
      "117 Train Loss 0.25927222 Test MSE 3.416428810954652 Test RE 0.883474966366239\n",
      "118 Train Loss 0.25808194 Test MSE 3.4200989017369663 Test RE 0.883949374466497\n",
      "119 Train Loss 0.25683275 Test MSE 3.423771739813112 Test RE 0.8844238829311093\n",
      "120 Train Loss 0.25546038 Test MSE 3.431486196597979 Test RE 0.8854197160864643\n",
      "121 Train Loss 0.2545761 Test MSE 3.43564974866496 Test RE 0.8859567098722487\n",
      "122 Train Loss 0.2537724 Test MSE 3.4384465762846124 Test RE 0.8863172478051816\n",
      "123 Train Loss 0.25281817 Test MSE 3.439042988388465 Test RE 0.8863941120812896\n",
      "124 Train Loss 0.2518665 Test MSE 3.446198672586722 Test RE 0.8873158016932475\n",
      "125 Train Loss 0.25099802 Test MSE 3.454914283096112 Test RE 0.8884371264094894\n",
      "126 Train Loss 0.2502668 Test MSE 3.455226399267306 Test RE 0.8884772561072329\n",
      "127 Train Loss 0.24944803 Test MSE 3.4590825597768244 Test RE 0.888972904601604\n",
      "128 Train Loss 0.24885197 Test MSE 3.4614577041662136 Test RE 0.8892780543985478\n",
      "129 Train Loss 0.24821576 Test MSE 3.4653061661834452 Test RE 0.8897722684536785\n",
      "130 Train Loss 0.24772348 Test MSE 3.462921458934433 Test RE 0.8894660600772352\n",
      "131 Train Loss 0.24714585 Test MSE 3.4635719219539336 Test RE 0.8895495932470886\n",
      "132 Train Loss 0.2467212 Test MSE 3.4584379602859507 Test RE 0.8888900707619407\n",
      "133 Train Loss 0.24622524 Test MSE 3.4577927695627197 Test RE 0.8888071532158872\n",
      "134 Train Loss 0.24549717 Test MSE 3.4566430070234855 Test RE 0.8886593707717237\n",
      "135 Train Loss 0.24495026 Test MSE 3.4598340709131836 Test RE 0.8890694673092266\n",
      "136 Train Loss 0.2443611 Test MSE 3.4633301365903217 Test RE 0.889518543823197\n",
      "137 Train Loss 0.24396312 Test MSE 3.460498150229778 Test RE 0.8891547870534653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138 Train Loss 0.24353912 Test MSE 3.4639904281568565 Test RE 0.8896033341229626\n",
      "139 Train Loss 0.2430673 Test MSE 3.460453623304487 Test RE 0.8891490665683768\n",
      "140 Train Loss 0.24254364 Test MSE 3.4638224089746723 Test RE 0.8895817589713694\n",
      "141 Train Loss 0.24204361 Test MSE 3.462671841101255 Test RE 0.8894340018000743\n",
      "142 Train Loss 0.24156645 Test MSE 3.4637471559893003 Test RE 0.8895720956516623\n",
      "143 Train Loss 0.24118356 Test MSE 3.4641635705569427 Test RE 0.8896255665937102\n",
      "144 Train Loss 0.24059516 Test MSE 3.4646779265866146 Test RE 0.8896916095620688\n",
      "145 Train Loss 0.24008955 Test MSE 3.467339397978615 Test RE 0.8900332624161501\n",
      "146 Train Loss 0.23967801 Test MSE 3.4639585318075223 Test RE 0.8895992383893538\n",
      "147 Train Loss 0.239292 Test MSE 3.4613337349066664 Test RE 0.8892621298750607\n",
      "148 Train Loss 0.23882672 Test MSE 3.46380548511306 Test RE 0.8895795857685902\n",
      "149 Train Loss 0.23852418 Test MSE 3.462881786707686 Test RE 0.889460965072814\n",
      "150 Train Loss 0.23824191 Test MSE 3.460833185924632 Test RE 0.8891978287441272\n",
      "151 Train Loss 0.23796245 Test MSE 3.4629091866206565 Test RE 0.8894644839779237\n",
      "152 Train Loss 0.23767498 Test MSE 3.4647461778695736 Test RE 0.8897003726124926\n",
      "153 Train Loss 0.23744217 Test MSE 3.4623422365414362 Test RE 0.8893916691025512\n",
      "154 Train Loss 0.23711862 Test MSE 3.4635348897507 Test RE 0.8895448377417722\n",
      "155 Train Loss 0.23674625 Test MSE 3.4655505553297448 Test RE 0.8898036432934321\n",
      "156 Train Loss 0.23637065 Test MSE 3.468986075400874 Test RE 0.8902445805976565\n",
      "157 Train Loss 0.23598233 Test MSE 3.4722591512342618 Test RE 0.8906644656827201\n",
      "158 Train Loss 0.23563632 Test MSE 3.471058952470159 Test RE 0.8905105217035915\n",
      "159 Train Loss 0.2352362 Test MSE 3.4727916816254902 Test RE 0.8907327623868783\n",
      "160 Train Loss 0.23488724 Test MSE 3.475106237108447 Test RE 0.8910295419217403\n",
      "161 Train Loss 0.23447838 Test MSE 3.4754480308809788 Test RE 0.8910733594110057\n",
      "162 Train Loss 0.2340283 Test MSE 3.477366767064765 Test RE 0.8913192987500668\n",
      "163 Train Loss 0.23357117 Test MSE 3.480260130902234 Test RE 0.8916900354271027\n",
      "164 Train Loss 0.23333809 Test MSE 3.4803563020317334 Test RE 0.891702355518451\n",
      "165 Train Loss 0.23289832 Test MSE 3.4838523977469458 Test RE 0.8921501105775725\n",
      "166 Train Loss 0.23249009 Test MSE 3.481040680277886 Test RE 0.8917900235104096\n",
      "167 Train Loss 0.23210096 Test MSE 3.486745355082853 Test RE 0.8925204501234878\n",
      "168 Train Loss 0.23177779 Test MSE 3.485479440974667 Test RE 0.8923584140861883\n",
      "169 Train Loss 0.23132965 Test MSE 3.487130395245938 Test RE 0.8925697291355376\n",
      "170 Train Loss 0.23081352 Test MSE 3.489759946854905 Test RE 0.8929061971772668\n",
      "171 Train Loss 0.23035558 Test MSE 3.4925014537129644 Test RE 0.8932568556650249\n",
      "172 Train Loss 0.22983384 Test MSE 3.493030175290936 Test RE 0.8933244671331054\n",
      "173 Train Loss 0.22948065 Test MSE 3.4939874949632608 Test RE 0.8934468735333122\n",
      "174 Train Loss 0.22912422 Test MSE 3.492549159609443 Test RE 0.8932629563745208\n",
      "175 Train Loss 0.22887816 Test MSE 3.492771918118778 Test RE 0.8932914425522085\n",
      "176 Train Loss 0.22851413 Test MSE 3.493114272216387 Test RE 0.8933352207460398\n",
      "177 Train Loss 0.22812267 Test MSE 3.4955293926630295 Test RE 0.8936439909715473\n",
      "178 Train Loss 0.22774506 Test MSE 3.4945767664240295 Test RE 0.893522211603167\n",
      "179 Train Loss 0.2274602 Test MSE 3.4981793029372223 Test RE 0.8939826560914909\n",
      "180 Train Loss 0.22709839 Test MSE 3.4974229067359097 Test RE 0.8938859998621037\n",
      "181 Train Loss 0.2268419 Test MSE 3.4970032680841756 Test RE 0.8938323717510144\n",
      "182 Train Loss 0.22648172 Test MSE 3.4927456247834177 Test RE 0.8932880802290698\n",
      "183 Train Loss 0.22623208 Test MSE 3.492609792804366 Test RE 0.893270710188341\n",
      "184 Train Loss 0.22606403 Test MSE 3.4915721437822125 Test RE 0.8931380056508434\n",
      "185 Train Loss 0.2258931 Test MSE 3.492252563199802 Test RE 0.893225026455903\n",
      "186 Train Loss 0.22562575 Test MSE 3.49343281286438 Test RE 0.8933759518917738\n",
      "187 Train Loss 0.22544873 Test MSE 3.497867143867747 Test RE 0.89394276805321\n",
      "188 Train Loss 0.22519897 Test MSE 3.5007118162405955 Test RE 0.8943061977342822\n",
      "189 Train Loss 0.22494683 Test MSE 3.5037525075223295 Test RE 0.8946945071646144\n",
      "190 Train Loss 0.22477543 Test MSE 3.5049214360161285 Test RE 0.8948437395499933\n",
      "191 Train Loss 0.22453533 Test MSE 3.5053436028423843 Test RE 0.8948976297688509\n",
      "192 Train Loss 0.2242533 Test MSE 3.5069082087559122 Test RE 0.8950973257297677\n",
      "193 Train Loss 0.22396773 Test MSE 3.509641722012807 Test RE 0.8954461064298883\n",
      "194 Train Loss 0.22372532 Test MSE 3.510948342529626 Test RE 0.8956127757754185\n",
      "195 Train Loss 0.22347257 Test MSE 3.5138764504345175 Test RE 0.8959861655328081\n",
      "196 Train Loss 0.22324474 Test MSE 3.5154532235953533 Test RE 0.8961871698234976\n",
      "197 Train Loss 0.2229987 Test MSE 3.517891359224444 Test RE 0.8964978903773435\n",
      "198 Train Loss 0.22277027 Test MSE 3.520827422242486 Test RE 0.8968719248561102\n",
      "199 Train Loss 0.22258297 Test MSE 3.5194445651881736 Test RE 0.8966957777002745\n",
      "Training time: 284.84\n",
      "9\n",
      "KG_rowdy\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 57.35725 Test MSE 8.676684244054737 Test RE 1.4079426237747594\n",
      "1 Train Loss 54.788353 Test MSE 8.966127777387715 Test RE 1.4312335962864897\n",
      "2 Train Loss 45.578087 Test MSE 8.73913782041534 Test RE 1.413000626792167\n",
      "3 Train Loss 44.456223 Test MSE 8.584818647161635 Test RE 1.4004693963550376\n",
      "4 Train Loss 43.181313 Test MSE 8.526390146131837 Test RE 1.395695443840927\n",
      "5 Train Loss 42.48908 Test MSE 8.388481525145806 Test RE 1.384362213038983\n",
      "6 Train Loss 41.509262 Test MSE 8.857041182565279 Test RE 1.4225003839552424\n",
      "7 Train Loss 40.337673 Test MSE 9.479185154651487 Test RE 1.4716128230316854\n",
      "8 Train Loss 39.303856 Test MSE 8.963008914902934 Test RE 1.4309846477362806\n",
      "9 Train Loss 37.17484 Test MSE 8.089625690101201 Test RE 1.3594782814346635\n",
      "10 Train Loss 32.38623 Test MSE 7.79939279370966 Test RE 1.334868413755025\n",
      "11 Train Loss 30.9537 Test MSE 7.721871393266291 Test RE 1.3282179415781918\n",
      "12 Train Loss 28.732376 Test MSE 8.44026058165684 Test RE 1.388628223160528\n",
      "13 Train Loss 25.18686 Test MSE 8.459876663985996 Test RE 1.390240948075488\n",
      "14 Train Loss 19.631182 Test MSE 6.418460684954679 Test RE 1.2109424476506758\n",
      "15 Train Loss 13.399719 Test MSE 5.971182446806896 Test RE 1.1679875965506672\n",
      "16 Train Loss 7.467691 Test MSE 5.076647679107383 Test RE 1.07695255868491\n",
      "17 Train Loss 5.076326 Test MSE 4.994082775601057 Test RE 1.0681590599982682\n",
      "18 Train Loss 3.7069454 Test MSE 4.813351463499715 Test RE 1.0486531058378425\n",
      "19 Train Loss 2.8803527 Test MSE 4.7649181523017194 Test RE 1.0433638438045811\n",
      "20 Train Loss 2.4618495 Test MSE 4.815827503367405 Test RE 1.0489227903897802\n",
      "21 Train Loss 2.2352467 Test MSE 4.872755233846398 Test RE 1.0551042164790352\n",
      "22 Train Loss 2.0875244 Test MSE 4.983207429226747 Test RE 1.0669953897719642\n",
      "23 Train Loss 1.9780997 Test MSE 4.985366216744243 Test RE 1.0672264825920241\n",
      "24 Train Loss 1.8459172 Test MSE 5.171051506023701 Test RE 1.086919779627705\n",
      "25 Train Loss 1.6953895 Test MSE 5.275545107491202 Test RE 1.097846775510539\n",
      "26 Train Loss 1.556773 Test MSE 5.341323133131347 Test RE 1.1046698131373172\n",
      "27 Train Loss 1.4478786 Test MSE 5.388915298000444 Test RE 1.1095803039704975\n",
      "28 Train Loss 1.3468583 Test MSE 5.527918490004265 Test RE 1.1237996063120175\n",
      "29 Train Loss 1.2876242 Test MSE 5.593642146313831 Test RE 1.1304605200172215\n",
      "30 Train Loss 1.2094042 Test MSE 5.607918510224266 Test RE 1.1319022087121713\n",
      "31 Train Loss 1.1302042 Test MSE 5.649910909027229 Test RE 1.1361321775712174\n",
      "32 Train Loss 1.0714413 Test MSE 5.796399594023052 Test RE 1.1507665220415675\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 Train Loss 1.025771 Test MSE 5.825278336718275 Test RE 1.153629626946931\n",
      "34 Train Loss 0.97462547 Test MSE 5.931738151108312 Test RE 1.1641234721794973\n",
      "35 Train Loss 0.93706477 Test MSE 5.960078193293308 Test RE 1.1669010725820559\n",
      "36 Train Loss 0.91151977 Test MSE 5.986000246801649 Test RE 1.1694359095940539\n",
      "37 Train Loss 0.8902232 Test MSE 6.047379795965742 Test RE 1.1754162286217666\n",
      "38 Train Loss 0.87210584 Test MSE 6.049407720072467 Test RE 1.1756132937298038\n",
      "39 Train Loss 0.8616776 Test MSE 6.029886140469647 Test RE 1.1737148952087684\n",
      "40 Train Loss 0.84788126 Test MSE 6.071827627092344 Test RE 1.1777897683814997\n",
      "41 Train Loss 0.8336705 Test MSE 6.092417254618738 Test RE 1.1797850267035974\n",
      "42 Train Loss 0.82130724 Test MSE 6.102768873897145 Test RE 1.1807868870226703\n",
      "43 Train Loss 0.8103733 Test MSE 6.116099320585286 Test RE 1.1820757962695045\n",
      "44 Train Loss 0.8015362 Test MSE 6.134537769765564 Test RE 1.1838562809395532\n",
      "45 Train Loss 0.7918036 Test MSE 6.143969997813518 Test RE 1.1847660571831957\n",
      "46 Train Loss 0.78439516 Test MSE 6.178938741607028 Test RE 1.1881328540521394\n",
      "47 Train Loss 0.77253664 Test MSE 6.208315331222492 Test RE 1.1909538809128253\n",
      "48 Train Loss 0.76337135 Test MSE 6.228886768568977 Test RE 1.1929253796469257\n",
      "49 Train Loss 0.75382507 Test MSE 6.288464542137053 Test RE 1.1986168219971989\n",
      "50 Train Loss 0.7455492 Test MSE 6.304622370802488 Test RE 1.2001557207586033\n",
      "51 Train Loss 0.7379348 Test MSE 6.3145584485519946 Test RE 1.201101070469241\n",
      "52 Train Loss 0.729174 Test MSE 6.342960858151176 Test RE 1.203799270996146\n",
      "53 Train Loss 0.720512 Test MSE 6.344018815043514 Test RE 1.2038996590218118\n",
      "54 Train Loss 0.7157417 Test MSE 6.342062291801734 Test RE 1.2037140007373155\n",
      "55 Train Loss 0.70951176 Test MSE 6.378857392251986 Test RE 1.2072007783928045\n",
      "56 Train Loss 0.70259815 Test MSE 6.382989613050029 Test RE 1.2075917271352434\n",
      "57 Train Loss 0.6966225 Test MSE 6.408389517932665 Test RE 1.209992033524007\n",
      "58 Train Loss 0.69206065 Test MSE 6.415143505199205 Test RE 1.210629488429792\n",
      "59 Train Loss 0.6871578 Test MSE 6.425167316953501 Test RE 1.211574937696298\n",
      "60 Train Loss 0.6819459 Test MSE 6.454283825441668 Test RE 1.2143170419165192\n",
      "61 Train Loss 0.67571473 Test MSE 6.442998152712057 Test RE 1.2132549271212865\n",
      "62 Train Loss 0.6695349 Test MSE 6.4543335206859656 Test RE 1.2143217167702693\n",
      "63 Train Loss 0.6648232 Test MSE 6.471869173032707 Test RE 1.2159701813055654\n",
      "64 Train Loss 0.6598226 Test MSE 6.465980433009157 Test RE 1.2154168510649237\n",
      "65 Train Loss 0.6552252 Test MSE 6.491446827984828 Test RE 1.217807970959855\n",
      "66 Train Loss 0.65100724 Test MSE 6.498199988415389 Test RE 1.2184412592718792\n",
      "67 Train Loss 0.6450465 Test MSE 6.498555031655788 Test RE 1.2184745449067365\n",
      "68 Train Loss 0.6391793 Test MSE 6.529958890463061 Test RE 1.2214150975126452\n",
      "69 Train Loss 0.6329682 Test MSE 6.546696334645928 Test RE 1.222979447620236\n",
      "70 Train Loss 0.6288801 Test MSE 6.56876064646742 Test RE 1.2250386161340885\n",
      "71 Train Loss 0.6246687 Test MSE 6.592458717367843 Test RE 1.2272464082038166\n",
      "72 Train Loss 0.6188333 Test MSE 6.608391290404866 Test RE 1.2287285102651488\n",
      "73 Train Loss 0.6144886 Test MSE 6.628108407739345 Test RE 1.2305601920506508\n",
      "74 Train Loss 0.60943925 Test MSE 6.653608786249325 Test RE 1.23292509201699\n",
      "75 Train Loss 0.60688895 Test MSE 6.6937380514348135 Test RE 1.236637513588486\n",
      "76 Train Loss 0.6022357 Test MSE 6.7049930534889315 Test RE 1.2376767320268356\n",
      "77 Train Loss 0.59569246 Test MSE 6.7390832605564 Test RE 1.240819103808413\n",
      "78 Train Loss 0.59105545 Test MSE 6.752258461595308 Test RE 1.242031439227936\n",
      "79 Train Loss 0.5867184 Test MSE 6.778523391266764 Test RE 1.2444447211694967\n",
      "80 Train Loss 0.58294004 Test MSE 6.801221246339673 Test RE 1.2465264887030814\n",
      "81 Train Loss 0.57880884 Test MSE 6.800802821720029 Test RE 1.2464881437224236\n",
      "82 Train Loss 0.5756805 Test MSE 6.802249165881526 Test RE 1.246620683589574\n",
      "83 Train Loss 0.57112396 Test MSE 6.812705360730162 Test RE 1.2475784479326382\n",
      "84 Train Loss 0.5650023 Test MSE 6.839353346017501 Test RE 1.2500160262359075\n",
      "85 Train Loss 0.562246 Test MSE 6.8375602885965385 Test RE 1.2498521585922546\n",
      "86 Train Loss 0.55897856 Test MSE 6.863133076885649 Test RE 1.2521872294380734\n",
      "87 Train Loss 0.555755 Test MSE 6.864518755086365 Test RE 1.2523136324201445\n",
      "88 Train Loss 0.55275965 Test MSE 6.882964341329803 Test RE 1.2539950439674648\n",
      "89 Train Loss 0.5511549 Test MSE 6.879736115730935 Test RE 1.2537009371351253\n",
      "90 Train Loss 0.54870605 Test MSE 6.887448535455492 Test RE 1.254403461138407\n",
      "91 Train Loss 0.54605937 Test MSE 6.897416808443759 Test RE 1.2553108882479094\n",
      "92 Train Loss 0.543952 Test MSE 6.903271248864634 Test RE 1.2558435212760066\n",
      "93 Train Loss 0.5419373 Test MSE 6.917118542037337 Test RE 1.2571024404786968\n",
      "94 Train Loss 0.5395016 Test MSE 6.925858505930429 Test RE 1.2578963810091668\n",
      "95 Train Loss 0.53585076 Test MSE 6.9421190673515145 Test RE 1.2593721626658674\n",
      "96 Train Loss 0.5337664 Test MSE 6.944970067745233 Test RE 1.2596307365924613\n",
      "97 Train Loss 0.5302311 Test MSE 6.970946061992488 Test RE 1.2619842112694513\n",
      "98 Train Loss 0.5279458 Test MSE 6.9767260379720835 Test RE 1.2625072914210393\n",
      "99 Train Loss 0.52590823 Test MSE 6.980276504608914 Test RE 1.2628284965143095\n",
      "100 Train Loss 0.5229907 Test MSE 6.9728756379258785 Test RE 1.2621588594344164\n",
      "101 Train Loss 0.5211946 Test MSE 6.948542669686203 Test RE 1.2599546818781275\n",
      "102 Train Loss 0.5189915 Test MSE 6.961432925385576 Test RE 1.2611228126306535\n",
      "103 Train Loss 0.5164173 Test MSE 6.978141132314825 Test RE 1.2626353225586076\n",
      "104 Train Loss 0.5149351 Test MSE 6.980709219441687 Test RE 1.2628676379546893\n",
      "105 Train Loss 0.5134534 Test MSE 6.983395803112597 Test RE 1.2631106271047925\n",
      "106 Train Loss 0.51230544 Test MSE 6.9925983926291435 Test RE 1.2639426049868807\n",
      "107 Train Loss 0.51099706 Test MSE 6.994884335493916 Test RE 1.2641491851678934\n",
      "108 Train Loss 0.509462 Test MSE 7.005263216738703 Test RE 1.2650866982327624\n",
      "109 Train Loss 0.5080671 Test MSE 7.012067955350342 Test RE 1.26570098598397\n",
      "110 Train Loss 0.50652736 Test MSE 7.021104748343741 Test RE 1.2665163085899427\n",
      "111 Train Loss 0.50551194 Test MSE 7.025660575770343 Test RE 1.2669271480686692\n",
      "112 Train Loss 0.50429153 Test MSE 7.025607322613939 Test RE 1.2669223465274702\n",
      "113 Train Loss 0.50283223 Test MSE 7.032628480580502 Test RE 1.2675552484404757\n",
      "114 Train Loss 0.50150824 Test MSE 7.035529590936395 Test RE 1.2678166683706722\n",
      "115 Train Loss 0.49933916 Test MSE 7.051499824233937 Test RE 1.2692547869000574\n",
      "116 Train Loss 0.4978736 Test MSE 7.055054334618341 Test RE 1.2695746487045323\n",
      "117 Train Loss 0.49662396 Test MSE 7.05227022315996 Test RE 1.269324120080422\n",
      "118 Train Loss 0.49548966 Test MSE 7.06215608423914 Test RE 1.2702134767719404\n",
      "119 Train Loss 0.49364132 Test MSE 7.069075228912462 Test RE 1.2708355700149454\n",
      "120 Train Loss 0.49218795 Test MSE 7.075366658197582 Test RE 1.271400961788076\n",
      "121 Train Loss 0.4908381 Test MSE 7.072096246110223 Test RE 1.27110709110484\n",
      "122 Train Loss 0.48950368 Test MSE 7.086450107924128 Test RE 1.2723963868387926\n",
      "123 Train Loss 0.48786187 Test MSE 7.088559723035794 Test RE 1.2725857670545568\n",
      "124 Train Loss 0.486599 Test MSE 7.095818109404367 Test RE 1.2732371374457994\n",
      "125 Train Loss 0.4853723 Test MSE 7.105287626177462 Test RE 1.2740864349028216\n",
      "126 Train Loss 0.484168 Test MSE 7.114381320472432 Test RE 1.2749014932363534\n",
      "127 Train Loss 0.48307338 Test MSE 7.118341596970075 Test RE 1.275256285856567\n",
      "128 Train Loss 0.48234636 Test MSE 7.121595949182606 Test RE 1.2755477623774296\n",
      "129 Train Loss 0.48140475 Test MSE 7.120298333640196 Test RE 1.275431549240881\n",
      "130 Train Loss 0.480263 Test MSE 7.133478716210171 Test RE 1.2766114789717737\n",
      "131 Train Loss 0.47926372 Test MSE 7.137697605432985 Test RE 1.2769889306001427\n",
      "132 Train Loss 0.47829306 Test MSE 7.145635147703436 Test RE 1.2776987769498196\n",
      "133 Train Loss 0.4773625 Test MSE 7.143097177047972 Test RE 1.2774718517062436\n",
      "134 Train Loss 0.47671887 Test MSE 7.143879683787556 Test RE 1.2775418215620093\n",
      "135 Train Loss 0.4757924 Test MSE 7.135490975438746 Test RE 1.2767915238135952\n",
      "136 Train Loss 0.47486115 Test MSE 7.135594688337458 Test RE 1.276800802731421\n",
      "137 Train Loss 0.47426215 Test MSE 7.144573759271658 Test RE 1.2776038809023886\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "138 Train Loss 0.47356966 Test MSE 7.153094824289097 Test RE 1.2783655289756681\n",
      "139 Train Loss 0.47263974 Test MSE 7.154021795113392 Test RE 1.278448358099873\n",
      "140 Train Loss 0.47173974 Test MSE 7.161974938930596 Test RE 1.279158787817297\n",
      "141 Train Loss 0.47069687 Test MSE 7.164879905176628 Test RE 1.2794181810996974\n",
      "142 Train Loss 0.4698813 Test MSE 7.1711364927044725 Test RE 1.2799766723233688\n",
      "143 Train Loss 0.4690097 Test MSE 7.177954382289397 Test RE 1.2805849905726023\n",
      "144 Train Loss 0.46828994 Test MSE 7.18208262186876 Test RE 1.2809531875175777\n",
      "145 Train Loss 0.46731687 Test MSE 7.1896086096385465 Test RE 1.2816241568128428\n",
      "146 Train Loss 0.466362 Test MSE 7.197451693715715 Test RE 1.2823230228205376\n",
      "147 Train Loss 0.46545848 Test MSE 7.205660582860543 Test RE 1.2830540765514664\n",
      "148 Train Loss 0.4647621 Test MSE 7.211067189879022 Test RE 1.2835353418243356\n",
      "149 Train Loss 0.4640267 Test MSE 7.222019996573915 Test RE 1.2845097454675414\n",
      "150 Train Loss 0.46335357 Test MSE 7.225836194066852 Test RE 1.2848490754253008\n",
      "151 Train Loss 0.46272692 Test MSE 7.233726599749625 Test RE 1.28555039317512\n",
      "152 Train Loss 0.46212488 Test MSE 7.2341643198535905 Test RE 1.2855892875632131\n",
      "153 Train Loss 0.46169636 Test MSE 7.236023589688627 Test RE 1.285754483134894\n",
      "154 Train Loss 0.46105647 Test MSE 7.233312465063087 Test RE 1.2855135934273072\n",
      "155 Train Loss 0.46046925 Test MSE 7.238676892503861 Test RE 1.2859901915507996\n",
      "156 Train Loss 0.45961365 Test MSE 7.247643803439357 Test RE 1.2867864552033308\n",
      "157 Train Loss 0.4589923 Test MSE 7.251908665240094 Test RE 1.2871650030072104\n",
      "158 Train Loss 0.45849144 Test MSE 7.25540480105005 Test RE 1.2874752359145178\n",
      "159 Train Loss 0.4579484 Test MSE 7.25302972113661 Test RE 1.2872644891025569\n",
      "160 Train Loss 0.4573799 Test MSE 7.252171712123624 Test RE 1.2871883473216494\n",
      "161 Train Loss 0.4565514 Test MSE 7.262124767325955 Test RE 1.288071328620969\n",
      "162 Train Loss 0.4558792 Test MSE 7.266284937830378 Test RE 1.2884402171564076\n",
      "163 Train Loss 0.45556465 Test MSE 7.267408569149766 Test RE 1.2885398331107711\n",
      "164 Train Loss 0.4550978 Test MSE 7.264781769124566 Test RE 1.288306941128584\n",
      "165 Train Loss 0.4544741 Test MSE 7.266075642575624 Test RE 1.288421661156495\n",
      "166 Train Loss 0.45398423 Test MSE 7.269470440798552 Test RE 1.288722609082977\n",
      "167 Train Loss 0.45360476 Test MSE 7.273776987771342 Test RE 1.2891042821890997\n",
      "168 Train Loss 0.453294 Test MSE 7.277916512710853 Test RE 1.2894710462738492\n",
      "169 Train Loss 0.45290348 Test MSE 7.276714231659046 Test RE 1.2893645342910314\n",
      "170 Train Loss 0.4523269 Test MSE 7.274779410344501 Test RE 1.2891931069300404\n",
      "171 Train Loss 0.45179075 Test MSE 7.283614918782433 Test RE 1.2899757574699098\n",
      "172 Train Loss 0.45140052 Test MSE 7.281411733564876 Test RE 1.2897806434706143\n",
      "173 Train Loss 0.45079234 Test MSE 7.286320410957449 Test RE 1.2902153153903764\n",
      "174 Train Loss 0.45040017 Test MSE 7.287667567193393 Test RE 1.2903345828105273\n",
      "175 Train Loss 0.4498018 Test MSE 7.290664246762581 Test RE 1.2905998475080644\n",
      "176 Train Loss 0.44933194 Test MSE 7.296280645397539 Test RE 1.2910968617395668\n",
      "177 Train Loss 0.4488019 Test MSE 7.3025363522553635 Test RE 1.2916502253925697\n",
      "178 Train Loss 0.44796616 Test MSE 7.30389512794637 Test RE 1.2917703878409152\n",
      "179 Train Loss 0.44749585 Test MSE 7.304159719745008 Test RE 1.291793785545083\n",
      "180 Train Loss 0.44671094 Test MSE 7.308854345174265 Test RE 1.2922088581912758\n",
      "181 Train Loss 0.44612253 Test MSE 7.314853672486967 Test RE 1.2927390913397123\n",
      "182 Train Loss 0.44572443 Test MSE 7.311355234232755 Test RE 1.292429918458875\n",
      "183 Train Loss 0.4450827 Test MSE 7.309233806606887 Test RE 1.2922424022350594\n",
      "184 Train Loss 0.44459373 Test MSE 7.319214737635291 Test RE 1.2931243949703506\n",
      "185 Train Loss 0.44418293 Test MSE 7.317153935598255 Test RE 1.2929423357542984\n",
      "186 Train Loss 0.44355693 Test MSE 7.317497645770627 Test RE 1.2929727022204425\n",
      "187 Train Loss 0.44303012 Test MSE 7.324953058276998 Test RE 1.2936312053891212\n",
      "188 Train Loss 0.44253907 Test MSE 7.32907466081426 Test RE 1.2939951042290365\n",
      "189 Train Loss 0.44218636 Test MSE 7.33111942012509 Test RE 1.2941755993214148\n",
      "190 Train Loss 0.44193593 Test MSE 7.334503526025332 Test RE 1.294474265986705\n",
      "191 Train Loss 0.44155475 Test MSE 7.338883937946351 Test RE 1.2948607600493784\n",
      "192 Train Loss 0.44121403 Test MSE 7.344789747198822 Test RE 1.2953816609697775\n",
      "193 Train Loss 0.4407707 Test MSE 7.344925382796206 Test RE 1.2953936217651383\n",
      "194 Train Loss 0.44034505 Test MSE 7.345792282537307 Test RE 1.2954700652416453\n",
      "195 Train Loss 0.4398881 Test MSE 7.353710281938016 Test RE 1.296168068136147\n",
      "196 Train Loss 0.4397342 Test MSE 7.353535620776266 Test RE 1.2961526751198178\n",
      "197 Train Loss 0.43957436 Test MSE 7.353243392562633 Test RE 1.2961269204241492\n",
      "198 Train Loss 0.43927854 Test MSE 7.354107617213824 Test RE 1.2962030849045874\n",
      "199 Train Loss 0.43888238 Test MSE 7.361306259625107 Test RE 1.2968373304644212\n",
      "Training time: 283.64\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10 #10\n",
    "max_iter = 200 #200\n",
    "\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "alpha_full = []\n",
    "omega_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "n_val = 3\n",
    "rowdy_terms = 2\n",
    "\n",
    "N_I = 200  #Total number of data points for 'y'\n",
    "N_B = 400\n",
    "N_f = 10000 #Total number of collocation points\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(reps)\n",
    "    print(label)\n",
    "\n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []\n",
    "    alpha_val = []\n",
    "    omega_val = []\n",
    "\n",
    "    torch.manual_seed(reps*36)\n",
    "\n",
    "    layers = np.array([2,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "\n",
    "    PINN = Sequentialmodel(layers,n_val,rowdy_terms)\n",
    "\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.05, \n",
    "                            max_iter = 20, \n",
    "                            max_eval = 30, \n",
    "                            tolerance_grad = 1e-8, \n",
    "                            tolerance_change = 1e-8, \n",
    "                            history_size = 100, \n",
    "                            line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    nan_flag = train_model(max_iter,reps)\n",
    "\n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    #elapsed_time[reps] = time.time() - start_time\n",
    "    alpha_full.append(alpha_val)\n",
    "    omega_full.append(omega_val)\n",
    "\n",
    "\n",
    "\n",
    "  #print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"alpha\": alpha_full,  \"omega\": omega_full,\"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1660688516819,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "_O3sPdAnSq_2"
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "aborted",
     "timestamp": 1660688516819,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "jQ4afiEWSq_2"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'KG_rowdy_tune0.mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'KG_rowdy_tune0.mat'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-43370c9270bd>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtune_reps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m75\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"KG_rowdy_tune\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtune_reps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".mat\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test_re_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtune_reps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \"\"\"\n\u001b[1;32m    223\u001b[0m     \u001b[0mvariable_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'variable_names'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0mMR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mmatfile_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file_context\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mappendmat\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.mat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mfile_like\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'.mat'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             raise IOError(\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'KG_rowdy_tune0.mat'"
     ]
    }
   ],
   "source": [
    "for tune_reps in range(75):\n",
    "    label = \"KG_rowdy_tune\"+str(tune_reps)+\".mat\"\n",
    "    data = sio.loadmat(label)\n",
    "    re = np.array(data[\"test_re_loss\"])\n",
    "    print(tune_reps,\" \",np.mean(re[:,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1660688534316,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "06syezgfv_qO",
    "outputId": "9f4852d5-694a-4977-8893-a6183a2ce493"
   },
   "outputs": [],
   "source": [
    "lrnr_tune[1]"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "stan_2D_KG_16Aug2022_tune.ipynb",
   "version": ""
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
