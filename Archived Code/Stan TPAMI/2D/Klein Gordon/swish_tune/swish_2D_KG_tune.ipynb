{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 286,
     "status": "ok",
     "timestamp": 1660687093981,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "iAtv2UvNSq_u",
    "outputId": "68a82578-1b95-4343-a8ec-7635a4df93ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1256,
     "status": "ok",
     "timestamp": 1660687393066,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "xAgfGYA4acPE",
    "outputId": "527d048f-6a89-4e80-87ff-bfdb1c9d6222"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1856,
     "status": "ok",
     "timestamp": 1660687061284,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "7kSdyTofacUc",
    "outputId": "08ee5c9b-0706-46a5-86a1-2c7e56a6a74d"
   },
   "outputs": [],
   "source": [
    "# %cd '/content/gdrive/MyDrive/Virginia Tech /Fall 2022/Codes from GPU/PINN_Stan/2D Klein Gordon/stan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 32419,
     "status": "ok",
     "timestamp": 1660687093700,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "RHuSaD0gagsN",
    "outputId": "c232cd79-e56c-4a76-97c7-d59dafa084ef"
   },
   "outputs": [],
   "source": [
    "# !pip install smt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 2,
     "status": "ok",
     "timestamp": 1660687406024,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "T7Wci76Yf-U8"
   },
   "outputs": [],
   "source": [
    "lr_tune = np.array([0.05,0.1,0.25,0.5,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 280,
     "status": "ok",
     "timestamp": 1660687410736,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "mTLFQRt5Sq_y"
   },
   "outputs": [],
   "source": [
    "def true_2D_1(xt): #True function for 2D_1 Klein Gordon Equation x \\in [-50,50] , t \\in [0,10]\n",
    "    y = xt[:,0]*np.cos(xt[:,1])\n",
    "    return y.reshape(-1,1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 4312,
     "status": "ok",
     "timestamp": 1660687098957,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "81bNHCY3Sq_y"
   },
   "outputs": [],
   "source": [
    "\n",
    "loss_thresh = 0.01\n",
    "\n",
    "x = np.linspace(-5,5,500).reshape(-1,1)\n",
    "t = np.linspace(0,10,1000).reshape(-1,1)\n",
    "\n",
    "X,T = np.meshgrid(x,t)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "T = T.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xt = np.hstack((X,T))\n",
    "\n",
    "y_true = true_2D_1(xt)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "#bound_pts_idx = ((X == -5) + (X == 5) + (T == 0)).reshape(-1,)\n",
    "\n",
    "#xt_bound = xt[bound_pts_idx,:]\n",
    "#y_bound = y_true[bound_pts_idx,:]\n",
    "\n",
    "\n",
    "xt_test_tensor = torch.from_numpy(xt).float().to(device)\n",
    "\n",
    "\n",
    "lb_xt = xt[0]\n",
    "ub_xt = xt[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1660687098958,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "YQgCA-PuSq_z"
   },
   "outputs": [],
   "source": [
    "def trainingdata(N_I,N_B,N_f,seed):\n",
    "    '''Boundary Conditions''' \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    x_BC1 = np.random.uniform(size = N_I).reshape(-1,1)\n",
    "    t_BC1 = np.zeros((N_I,1))\n",
    "    samples = np.hstack((x_BC1,t_BC1))\n",
    "    xt_BC1 = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    y_BC1 = true_2D_1(xt_BC1)\n",
    "    \n",
    "    x_BC2 = np.zeros((int(N_B/2),1))\n",
    "    t_BC2 = np.random.uniform(size = int(N_B/2)).reshape(-1,1)\n",
    "    samples = np.hstack((x_BC2,t_BC2))\n",
    "    xt_BC2 = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    y_BC2 = true_2D_1(xt_BC2)\n",
    "    \n",
    "    x_BC3 = np.ones((int(N_B/2),1))\n",
    "    t_BC3 = np.random.uniform(size = int(N_B/2)).reshape(-1,1)\n",
    "    samples = np.hstack((x_BC3,t_BC3))\n",
    "    xt_BC3 = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    y_BC3 = true_2D_1(xt_BC3)\n",
    "\n",
    "    xt_BC = np.vstack((xt_BC1,xt_BC2,xt_BC3))\n",
    "    y_BC = np.vstack((y_BC1,y_BC2,y_BC3))\n",
    "\n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    samples = sampling(N_f)\n",
    "    xt_coll = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    \n",
    "    xt_coll = np.vstack((xt_coll, xt_BC)) # append training points to collocation points \n",
    "\n",
    "    return xt_coll, xt_BC, y_BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1660687098958,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "gTJxct8bSq_0"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "            \n",
    "        \n",
    "        self.beta = Parameter(torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "\n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xt):\n",
    "        if torch.is_tensor(xt) != True:         \n",
    "            xt = torch.from_numpy(xt)                \n",
    "        \n",
    "        ubxt = torch.from_numpy(ub_xt).float().to(device)\n",
    "        lbxt = torch.from_numpy(lb_xt).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xt = (xt - lbxt)/(ubxt - lbxt)\n",
    "        \n",
    "        #convert to float\n",
    "        a = xt.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = z*self.activation(self.beta[:,i]*z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC(self,xt,y):\n",
    "                \n",
    "        loss_bc = self.loss_function(self.forward(xt), y)\n",
    "                \n",
    "        return loss_bc\n",
    "    \n",
    "    def loss_PDE(self, xt_coll, f_hat):\n",
    "        \n",
    "        g = xt_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        y = self.forward(g) \n",
    "        \n",
    "        \n",
    "        y_x_t = autograd.grad(y,g,torch.ones([xt_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        y_xx_tt = autograd.grad(y_x_t,g,torch.ones(xt_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        #du_dx = u_x_t[:,[0]]\n",
    "        \n",
    "        d2y_dx2 = y_xx_tt[:,[0]]\n",
    "        d2y_dt2 = y_xx_tt[:,[1]]    \n",
    "        \n",
    "\n",
    "        f = d2y_dt2 - d2y_dx2 + torch.pow(y,2) + (g[:,0]*torch.cos(g[:,1])).reshape(-1,1) - (torch.pow(g[:,0],2)*torch.pow(torch.cos(g[:,1]),2)).reshape(-1,1)\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xt_BC,y_BC,xt_coll,f_hat):\n",
    "\n",
    "        loss_BC = self.loss_BC(xt_BC,y_BC)\n",
    "        loss_f = self.loss_PDE(xt_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_BC + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "         \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        y_pred = self.forward(xt_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return y_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1660687098958,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "VoQzfzYsYKVs"
   },
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1660687098959,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "_IUDZDkxXmyF"
   },
   "outputs": [],
   "source": [
    "def train_step(xt_BC, y_BC, xt_coll, f_hat,seed):\n",
    "    # x_coll_np_array = colloc_pts(N_f,seed*123)\n",
    "    # x_coll_train = torch.from_numpy(x_coll_np_array).float().to(device)        \n",
    "    \n",
    "    # f_hat = torch.zeros(x_coll_train.shape[0],1).to(device)\n",
    "    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(xt_BC, y_BC, xt_coll,f_hat)\n",
    "        loss.backward()\n",
    "        #print(loss.cpu().detach().numpy())\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 467,
     "status": "ok",
     "timestamp": 1660690085956,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "Vt9Dlr8MYIwW"
   },
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "  print(rep) \n",
    "  torch.manual_seed(rep*11)\n",
    "  start_time = time.time() \n",
    "  thresh_flag = 0\n",
    "\n",
    "  xt_coll, xt_BC, y_BC = trainingdata(N_I,N_B,N_f,rep*11)\n",
    "  xt_coll = torch.from_numpy(xt_coll).float().to(device)\n",
    "  xt_BC = torch.from_numpy(xt_BC).float().to(device)\n",
    "  y_BC = torch.from_numpy(y_BC).float().to(device)\n",
    "\n",
    "  f_hat = torch.zeros(xt_coll.shape[0],1).to(device)\n",
    "\n",
    "  nan_flag = 0  \n",
    "  for i in range(max_iter):\n",
    "    train_step(xt_BC, y_BC, xt_coll,f_hat,i)\n",
    "    \n",
    "    loss_np = PINN.loss(xt_BC, y_BC, xt_coll,f_hat).cpu().detach().numpy()\n",
    "    if(thresh_flag == 0):\n",
    "        if(loss_np < loss_thresh):\n",
    "            time_threshold[rep] = time.time() - start_time\n",
    "            epoch_threshold[rep] = i+1            \n",
    "            thresh_flag = 1       \n",
    "    data_update(loss_np)\n",
    "    \n",
    "    print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "    \n",
    "    if(np.isnan(loss_np)):\n",
    "            nan_flag =1\n",
    "            print(\"NAN BREAK!\")\n",
    "            break\n",
    "            \n",
    "    \n",
    "\n",
    "  elapsed_time[rep] = time.time() - start_time  \n",
    "  print('Training time: %.2f' % (elapsed_time[rep]))\n",
    "    \n",
    "  return nan_flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "sP4Re5lSSq_1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 58.679802 Test MSE 8.713152247558138 Test RE 1.4108983046381993\n",
      "1 Train Loss 58.223907 Test MSE 8.39443530988614 Test RE 1.3848534063869096\n",
      "2 Train Loss 58.10724 Test MSE 8.678306331339545 Test RE 1.4080742235331103\n",
      "3 Train Loss 58.076767 Test MSE 8.811811303587694 Test RE 1.4188636237062315\n",
      "4 Train Loss 57.279938 Test MSE 9.161864159455751 Test RE 1.4467716314082941\n",
      "5 Train Loss 54.339447 Test MSE 8.336739676432492 Test RE 1.3800860959492\n",
      "6 Train Loss 45.664703 Test MSE 8.8467825310321 Test RE 1.421676341058014\n",
      "7 Train Loss 43.47985 Test MSE 8.556284410544635 Test RE 1.3981400177183865\n",
      "8 Train Loss 43.326416 Test MSE 8.544756279684014 Test RE 1.3971978228931996\n",
      "9 Train Loss 43.130203 Test MSE 8.413034325721286 Test RE 1.386386723297529\n",
      "10 Train Loss 43.070786 Test MSE 8.37986014328165 Test RE 1.3836506311251773\n",
      "11 Train Loss 42.887115 Test MSE 8.326423476323535 Test RE 1.3792319460032474\n",
      "12 Train Loss 42.781494 Test MSE 8.266954947817561 Test RE 1.3742977824240707\n",
      "13 Train Loss 42.738297 Test MSE 8.301701430629931 Test RE 1.3771828799985777\n",
      "14 Train Loss 42.593964 Test MSE 8.329414151922688 Test RE 1.3794796192699426\n",
      "15 Train Loss 42.56153 Test MSE 8.325436906680185 Test RE 1.3791502333278258\n",
      "16 Train Loss 42.4963 Test MSE 8.384835750038848 Test RE 1.3840613467984657\n",
      "17 Train Loss 42.426605 Test MSE 8.414620303066444 Test RE 1.386517394006464\n",
      "18 Train Loss 42.339893 Test MSE 8.45461128599038 Test RE 1.3898082417993947\n",
      "19 Train Loss 42.06759 Test MSE 8.383461234341189 Test RE 1.3839478984211866\n",
      "20 Train Loss 41.759224 Test MSE 8.167058856518375 Test RE 1.3659691874934392\n",
      "21 Train Loss 41.375927 Test MSE 8.011047039796338 Test RE 1.3528595172339202\n",
      "22 Train Loss 40.489655 Test MSE 7.782383561350016 Test RE 1.333412051690658\n",
      "23 Train Loss 40.11949 Test MSE 7.834591418874371 Test RE 1.3378771505945124\n",
      "24 Train Loss 39.58548 Test MSE 7.943730146646236 Test RE 1.347163481291706\n",
      "25 Train Loss 39.22161 Test MSE 7.719478417318826 Test RE 1.3280121210130589\n",
      "26 Train Loss 37.555336 Test MSE 7.588010746790838 Test RE 1.3166551098085337\n",
      "27 Train Loss 36.524128 Test MSE 7.201056407395399 Test RE 1.2826440967868105\n",
      "28 Train Loss 34.942978 Test MSE 7.533626176998579 Test RE 1.3119282789848832\n",
      "29 Train Loss 34.338882 Test MSE 7.8930559823891535 Test RE 1.3428597347451339\n",
      "30 Train Loss 32.54877 Test MSE 7.382184371889946 Test RE 1.2986750711944712\n",
      "31 Train Loss 31.144272 Test MSE 6.777126502394287 Test RE 1.2443164896616694\n",
      "32 Train Loss 30.481728 Test MSE 6.79925805302724 Test RE 1.2463465688769801\n",
      "33 Train Loss 29.68932 Test MSE 6.931648277030522 Test RE 1.258422049464228\n",
      "34 Train Loss 29.26067 Test MSE 6.804641752751646 Test RE 1.2468399041721905\n",
      "35 Train Loss 28.775013 Test MSE 6.692842304447761 Test RE 1.2365547682390978\n",
      "36 Train Loss 28.185263 Test MSE 6.319032938783836 Test RE 1.2015265446595238\n",
      "37 Train Loss 27.826532 Test MSE 6.429415437781866 Test RE 1.2119753993504394\n",
      "38 Train Loss 27.486847 Test MSE 6.2711878382733905 Test RE 1.1969691709275418\n",
      "39 Train Loss 27.339535 Test MSE 6.146158790173306 Test RE 1.1849770751388782\n",
      "40 Train Loss 27.234125 Test MSE 6.136619427115923 Test RE 1.1840571252569794\n",
      "41 Train Loss 27.06855 Test MSE 6.128967145152861 Test RE 1.1833186433038494\n",
      "42 Train Loss 27.01599 Test MSE 6.125137895637607 Test RE 1.1829489292714384\n",
      "43 Train Loss 26.446865 Test MSE 5.920054077685864 Test RE 1.1629763877678503\n",
      "44 Train Loss 26.097073 Test MSE 5.677909476305619 Test RE 1.138943793697028\n",
      "45 Train Loss 25.906075 Test MSE 5.7607999685884375 Test RE 1.1472272601508455\n",
      "46 Train Loss 25.679941 Test MSE 5.878366936433883 Test RE 1.1588744987165707\n",
      "47 Train Loss 25.576544 Test MSE 5.822484580338149 Test RE 1.1533529580511839\n",
      "48 Train Loss 25.47592 Test MSE 5.801319607261522 Test RE 1.1512548066657033\n",
      "49 Train Loss 25.351055 Test MSE 5.627831613858893 Test RE 1.1339100581845944\n",
      "50 Train Loss 25.29291 Test MSE 5.618516515217805 Test RE 1.1329712544762982\n",
      "51 Train Loss 24.952168 Test MSE 5.432685197436269 Test RE 1.114077312910056\n",
      "52 Train Loss 24.849417 Test MSE 5.504708967238489 Test RE 1.1214379318326593\n",
      "53 Train Loss 24.774578 Test MSE 5.632750182158698 Test RE 1.134405452894255\n",
      "54 Train Loss 24.752258 Test MSE 5.601141283636218 Test RE 1.1312180442758002\n",
      "55 Train Loss 24.713413 Test MSE 5.524572849132948 Test RE 1.1234594783814127\n",
      "56 Train Loss 24.631382 Test MSE 5.48013692158397 Test RE 1.118932182268732\n",
      "57 Train Loss 24.567883 Test MSE 5.5105803405886045 Test RE 1.1220358404615574\n",
      "58 Train Loss 24.531502 Test MSE 5.5835782470734525 Test RE 1.129443118269837\n",
      "59 Train Loss 24.506666 Test MSE 5.578760976260705 Test RE 1.1289557956996885\n",
      "60 Train Loss 24.46477 Test MSE 5.531036482404278 Test RE 1.1241164981651082\n",
      "61 Train Loss 24.451157 Test MSE 5.537071597546777 Test RE 1.12472961316771\n",
      "62 Train Loss 24.41702 Test MSE 5.494162711768328 Test RE 1.1203631573864987\n",
      "63 Train Loss 24.393696 Test MSE 5.466817820794613 Test RE 1.1175716107143694\n",
      "64 Train Loss 24.381514 Test MSE 5.490039731320676 Test RE 1.1199427018446622\n",
      "65 Train Loss 24.365484 Test MSE 5.50551944536396 Test RE 1.1215204854667846\n",
      "66 Train Loss 24.346676 Test MSE 5.514892521670972 Test RE 1.1224747666042278\n",
      "67 Train Loss 24.292171 Test MSE 5.532058351235694 Test RE 1.124220334630917\n",
      "68 Train Loss 24.22132 Test MSE 5.505085697530595 Test RE 1.1214763055608623\n",
      "69 Train Loss 24.207966 Test MSE 5.485601258302359 Test RE 1.1194898963283777\n",
      "70 Train Loss 24.190868 Test MSE 5.508222840668276 Test RE 1.1217958038216365\n",
      "71 Train Loss 24.151423 Test MSE 5.497187270661608 Test RE 1.1206714971916092\n",
      "72 Train Loss 24.081173 Test MSE 5.447210215633349 Test RE 1.1155656369067422\n",
      "73 Train Loss 24.053198 Test MSE 5.469070275458701 Test RE 1.1178018196063308\n",
      "74 Train Loss 24.029354 Test MSE 5.46963360161147 Test RE 1.1178593861352777\n",
      "75 Train Loss 24.005974 Test MSE 5.417133455199494 Test RE 1.1124815771274128\n",
      "76 Train Loss 23.970705 Test MSE 5.383993704646864 Test RE 1.1090735089893642\n",
      "77 Train Loss 23.94289 Test MSE 5.348287156148659 Test RE 1.1053897134250343\n",
      "78 Train Loss 23.901817 Test MSE 5.252832749663772 Test RE 1.0954809930630491\n",
      "79 Train Loss 23.837646 Test MSE 5.238100448193749 Test RE 1.0939436998634902\n",
      "80 Train Loss 23.757248 Test MSE 5.22802865381876 Test RE 1.0928914789737139\n",
      "81 Train Loss 23.656427 Test MSE 5.106337770104964 Test RE 1.0800971737523986\n",
      "82 Train Loss 23.433512 Test MSE 5.1491865317117735 Test RE 1.0846194110833902\n",
      "83 Train Loss 23.222813 Test MSE 5.125897175476068 Test RE 1.0821638081642753\n",
      "84 Train Loss 23.084324 Test MSE 4.98744812547317 Test RE 1.0674492983408024\n",
      "85 Train Loss 22.766914 Test MSE 4.824524530918981 Test RE 1.049869501569893\n",
      "86 Train Loss 22.387371 Test MSE 4.632558706560504 Test RE 1.0287705562614051\n",
      "87 Train Loss 21.997978 Test MSE 4.61866367667868 Test RE 1.0272265355927537\n",
      "88 Train Loss 21.606178 Test MSE 4.659476896239131 Test RE 1.0317551411413979\n",
      "89 Train Loss 21.313965 Test MSE 4.731683642136354 Test RE 1.0397188323653301\n",
      "90 Train Loss 21.11129 Test MSE 4.627230949772095 Test RE 1.02817880810874\n",
      "91 Train Loss 20.966717 Test MSE 4.565119668034671 Test RE 1.021254876938789\n",
      "92 Train Loss 20.723803 Test MSE 4.376844019481026 Test RE 0.999973739598206\n",
      "93 Train Loss 20.5007 Test MSE 4.392404095920056 Test RE 1.0017496611613803\n",
      "94 Train Loss 20.175217 Test MSE 4.35401408783192 Test RE 0.9973623625675604\n",
      "95 Train Loss 19.98074 Test MSE 4.18581120786956 Test RE 0.977907728776798\n",
      "96 Train Loss 19.857452 Test MSE 4.122220809285011 Test RE 0.9704511654718538\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97 Train Loss 19.796864 Test MSE 4.057734296745274 Test RE 0.9628305531773542\n",
      "98 Train Loss 19.522972 Test MSE 4.100779175912717 Test RE 0.9679239856075097\n",
      "99 Train Loss 19.409264 Test MSE 4.148200193208979 Test RE 0.9735043892591039\n",
      "Training time: 85.07\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 56.888855 Test MSE 8.714103652555124 Test RE 1.4109753318160863\n",
      "1 Train Loss 56.761482 Test MSE 8.346060382514127 Test RE 1.38085736769165\n",
      "2 Train Loss 56.509018 Test MSE 8.6914423894646 Test RE 1.4091394976795228\n",
      "3 Train Loss 56.5073 Test MSE 8.731270761116695 Test RE 1.4123644849441561\n",
      "4 Train Loss 56.38231 Test MSE 8.599739576707664 Test RE 1.4016859178784993\n",
      "5 Train Loss 56.147697 Test MSE 8.671423509936266 Test RE 1.407515736346898\n",
      "6 Train Loss 53.547466 Test MSE 9.748004668945564 Test RE 1.4923336249935104\n",
      "7 Train Loss 49.67075 Test MSE 8.220967477888443 Test RE 1.3704699754100373\n",
      "8 Train Loss 47.341583 Test MSE 8.12875300575145 Test RE 1.3627620286226885\n",
      "9 Train Loss 45.42138 Test MSE 7.4714091440809 Test RE 1.3064997172529549\n",
      "10 Train Loss 43.2045 Test MSE 8.154132035678641 Test RE 1.3648877313699717\n",
      "11 Train Loss 43.1484 Test MSE 8.182715434510257 Test RE 1.367277869337145\n",
      "12 Train Loss 43.00759 Test MSE 8.252372730345424 Test RE 1.3730851742496193\n",
      "13 Train Loss 42.820595 Test MSE 8.209084750793348 Test RE 1.3694791668685777\n",
      "14 Train Loss 42.81798 Test MSE 8.17252188679639 Test RE 1.3664259665904643\n",
      "15 Train Loss 42.808826 Test MSE 8.127928009519676 Test RE 1.362692872746934\n",
      "16 Train Loss 42.786804 Test MSE 8.16534677804039 Test RE 1.365826004429295\n",
      "17 Train Loss 42.785313 Test MSE 8.181460119205145 Test RE 1.3671729878536298\n",
      "18 Train Loss 42.77419 Test MSE 8.192128930349181 Test RE 1.3680641098623343\n",
      "19 Train Loss 42.453552 Test MSE 8.302000066947596 Test RE 1.3772076504103323\n",
      "20 Train Loss 42.360947 Test MSE 8.339106090849066 Test RE 1.3802819533249193\n",
      "21 Train Loss 42.30998 Test MSE 8.3327087979729 Test RE 1.379752414430488\n",
      "22 Train Loss 42.267788 Test MSE 8.295436759487078 Test RE 1.3766631536398632\n",
      "23 Train Loss 42.149704 Test MSE 8.39807639833851 Test RE 1.3851537140845647\n",
      "24 Train Loss 41.999023 Test MSE 8.41523594684835 Test RE 1.386568114369881\n",
      "25 Train Loss 41.953255 Test MSE 8.320598396338827 Test RE 1.3787494133728055\n",
      "26 Train Loss 41.908794 Test MSE 8.390477302149876 Test RE 1.3845268861056284\n",
      "27 Train Loss 41.646004 Test MSE 8.342247847946503 Test RE 1.3805419393598761\n",
      "28 Train Loss 41.501892 Test MSE 8.342916554057313 Test RE 1.3805972696706281\n",
      "29 Train Loss 41.2458 Test MSE 8.30733765653935 Test RE 1.3776503020608164\n",
      "30 Train Loss 40.45469 Test MSE 8.343719159601944 Test RE 1.3806636762065125\n",
      "31 Train Loss 39.68832 Test MSE 8.144615860508983 Test RE 1.3640910615090132\n",
      "32 Train Loss 39.4184 Test MSE 8.157796758477454 Test RE 1.3651944086033483\n",
      "33 Train Loss 38.538784 Test MSE 8.301585741113028 Test RE 1.3771732840031976\n",
      "34 Train Loss 34.73078 Test MSE 6.815832918985829 Test RE 1.2478647825350577\n",
      "35 Train Loss 32.311 Test MSE 6.252548830688233 Test RE 1.1951890520567838\n",
      "36 Train Loss 26.919596 Test MSE 5.664458111136003 Test RE 1.1375938749065289\n",
      "37 Train Loss 24.957228 Test MSE 4.377281169679723 Test RE 1.0000236760138548\n",
      "38 Train Loss 24.009796 Test MSE 4.280047445324555 Test RE 0.9888544036796505\n",
      "39 Train Loss 23.159943 Test MSE 3.888796209993935 Test RE 0.9425744268880726\n",
      "40 Train Loss 22.551321 Test MSE 3.7425461273004546 Test RE 0.9246803765155996\n",
      "41 Train Loss 21.881962 Test MSE 4.134114297453359 Test RE 0.9718501366401516\n",
      "42 Train Loss 21.693092 Test MSE 4.275832073530785 Test RE 0.988367327791993\n",
      "43 Train Loss 21.417215 Test MSE 4.276988989248825 Test RE 0.9885010304656394\n",
      "44 Train Loss 21.367758 Test MSE 4.372932227145829 Test RE 0.9995267778149208\n",
      "45 Train Loss 21.068636 Test MSE 4.394956152625982 Test RE 1.002040635152782\n",
      "46 Train Loss 20.570333 Test MSE 4.289758219097815 Test RE 0.9899755478990374\n",
      "47 Train Loss 20.532478 Test MSE 4.24425066303573 Test RE 0.9847105091276255\n",
      "48 Train Loss 20.364092 Test MSE 4.412344202848859 Test RE 1.0040208976205218\n",
      "49 Train Loss 20.086784 Test MSE 4.54552430457324 Test RE 1.0190606977117425\n",
      "50 Train Loss 19.96566 Test MSE 4.544346197105636 Test RE 1.0189286292485764\n",
      "51 Train Loss 19.69136 Test MSE 4.351953990672802 Test RE 0.9971263841777064\n",
      "52 Train Loss 19.446514 Test MSE 4.140190891946163 Test RE 0.9725641191608099\n",
      "53 Train Loss 19.286999 Test MSE 4.087143062461067 Test RE 0.966313351231166\n",
      "54 Train Loss 19.028765 Test MSE 4.411275219110833 Test RE 1.003899267600623\n",
      "55 Train Loss 18.81612 Test MSE 4.5781600172901635 Test RE 1.022712453559442\n",
      "56 Train Loss 18.698599 Test MSE 4.504421841706001 Test RE 1.0144428561254766\n",
      "57 Train Loss 18.604324 Test MSE 4.461904791123029 Test RE 1.0096438629197362\n",
      "58 Train Loss 18.254284 Test MSE 4.255036426911024 Test RE 0.9859609201941612\n",
      "59 Train Loss 18.011238 Test MSE 4.287265438912916 Test RE 0.9896878685105818\n",
      "60 Train Loss 17.808891 Test MSE 4.4230951889530346 Test RE 1.0052433371000467\n",
      "61 Train Loss 17.73256 Test MSE 4.424810213794526 Test RE 1.0054382063188456\n",
      "62 Train Loss 17.298775 Test MSE 4.466859586526134 Test RE 1.0102042951764223\n",
      "63 Train Loss 16.939293 Test MSE 4.306295219130512 Test RE 0.9918818882356573\n",
      "64 Train Loss 16.647123 Test MSE 4.388292366667176 Test RE 1.0012806824792828\n",
      "65 Train Loss 16.586159 Test MSE 4.38381281450908 Test RE 1.000769500236478\n",
      "66 Train Loss 16.55215 Test MSE 4.348494557914286 Test RE 0.9967299901130151\n",
      "67 Train Loss 16.519293 Test MSE 4.333427325136629 Test RE 0.9950016914783425\n",
      "68 Train Loss 16.445045 Test MSE 4.317512665904159 Test RE 0.9931729221033768\n",
      "69 Train Loss 15.960494 Test MSE 4.205372115938829 Test RE 0.9801900184480596\n",
      "70 Train Loss 15.750843 Test MSE 4.265593483021144 Test RE 0.9871832829936825\n",
      "71 Train Loss 15.666768 Test MSE 4.309577961419934 Test RE 0.9922598781494253\n",
      "72 Train Loss 15.609558 Test MSE 4.334382353988697 Test RE 0.9951113279048741\n",
      "73 Train Loss 15.540294 Test MSE 4.29680005690779 Test RE 0.9907877601878506\n",
      "74 Train Loss 15.372715 Test MSE 4.1757997083189355 Test RE 0.9767375632624979\n",
      "75 Train Loss 15.271183 Test MSE 4.3615753867670435 Test RE 0.9982280101662427\n",
      "76 Train Loss 15.236059 Test MSE 4.372846304516964 Test RE 0.9995169580434513\n",
      "77 Train Loss 15.144516 Test MSE 4.332313633672825 Test RE 0.9948738254714456\n",
      "78 Train Loss 15.065388 Test MSE 4.379774100084169 Test RE 1.0003084000791307\n",
      "79 Train Loss 14.949628 Test MSE 4.3954719279491385 Test RE 1.0020994312649747\n",
      "80 Train Loss 14.873671 Test MSE 4.325611183767352 Test RE 0.9941039512700578\n",
      "81 Train Loss 14.748019 Test MSE 4.300898030344145 Test RE 0.991260118017432\n",
      "82 Train Loss 14.715379 Test MSE 4.345316681045829 Test RE 0.9963657187487482\n",
      "83 Train Loss 14.699524 Test MSE 4.34106359002968 Test RE 0.9958779900249779\n",
      "84 Train Loss 14.685543 Test MSE 4.346920723237697 Test RE 0.9965496023760838\n",
      "85 Train Loss 14.599042 Test MSE 4.318113396382259 Test RE 0.9932420137932543\n",
      "86 Train Loss 14.4566 Test MSE 4.293166258623115 Test RE 0.9903687176055914\n",
      "87 Train Loss 14.256638 Test MSE 4.350742557916126 Test RE 0.9969875918511373\n",
      "88 Train Loss 14.193363 Test MSE 4.3462577356928165 Test RE 0.9964736031561152\n",
      "89 Train Loss 14.130913 Test MSE 4.323355734069179 Test RE 0.9938447458408021\n",
      "90 Train Loss 14.092545 Test MSE 4.309779730503788 Test RE 0.9922831060646692\n",
      "91 Train Loss 14.0672035 Test MSE 4.288580007733256 Test RE 0.9898395867932414\n",
      "92 Train Loss 13.976335 Test MSE 4.240428686132651 Test RE 0.9842670399764996\n",
      "93 Train Loss 13.641771 Test MSE 4.122214365115918 Test RE 0.9704504069300216\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94 Train Loss 13.278489 Test MSE 4.055132727934569 Test RE 0.9625218499239264\n",
      "95 Train Loss 13.055972 Test MSE 3.9695743153493375 Test RE 0.9523136925028943\n",
      "96 Train Loss 13.016237 Test MSE 3.9524166623562986 Test RE 0.9502533756000096\n",
      "97 Train Loss 12.995108 Test MSE 3.9907749302021713 Test RE 0.9548533540469673\n",
      "98 Train Loss 12.964328 Test MSE 3.9948574337400014 Test RE 0.9553416296106836\n",
      "99 Train Loss 12.878113 Test MSE 3.9682759660193367 Test RE 0.9521579406684663\n",
      "Training time: 84.12\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 56.47855 Test MSE 8.63308561069445 Test RE 1.4044008514820958\n",
      "1 Train Loss 56.47785 Test MSE 8.629501298910757 Test RE 1.4041092794217196\n",
      "2 Train Loss 56.45752 Test MSE 8.537520289801082 Test RE 1.3966061003043584\n",
      "3 Train Loss 56.238373 Test MSE 8.699477136241871 Test RE 1.4097906820908024\n",
      "4 Train Loss 56.227962 Test MSE 8.645510744821367 Test RE 1.4054111272736747\n",
      "5 Train Loss 56.091087 Test MSE 8.588883627980932 Test RE 1.400800923887614\n",
      "6 Train Loss 55.926773 Test MSE 8.626076050837327 Test RE 1.4038305900875951\n",
      "7 Train Loss 55.688915 Test MSE 9.159475081409829 Test RE 1.4465829866281412\n",
      "8 Train Loss 53.383476 Test MSE 9.43774873375177 Test RE 1.4683928651888423\n",
      "9 Train Loss 49.397415 Test MSE 8.372917569599464 Test RE 1.3830773466824897\n",
      "10 Train Loss 48.79074 Test MSE 8.336323906933487 Test RE 1.38005168172486\n",
      "11 Train Loss 44.67179 Test MSE 8.252828769978462 Test RE 1.3731231131932868\n",
      "12 Train Loss 44.12239 Test MSE 8.547348262950097 Test RE 1.397409721200197\n",
      "13 Train Loss 44.08934 Test MSE 8.611006942611716 Test RE 1.4026038606994666\n",
      "14 Train Loss 44.087173 Test MSE 8.624927348666299 Test RE 1.4037371155502691\n",
      "15 Train Loss 44.053093 Test MSE 8.657058056310106 Test RE 1.4063493774944575\n",
      "16 Train Loss 44.0019 Test MSE 8.6344832553961 Test RE 1.4045145289121868\n",
      "17 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "18 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "19 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "20 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "21 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "22 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "23 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "24 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "25 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "26 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "27 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "28 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "29 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "30 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "31 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "32 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "33 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "34 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "35 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "36 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "37 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "38 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "39 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "40 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "41 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "42 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "43 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "44 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "45 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "46 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "47 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "48 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "49 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "50 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "51 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "52 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "53 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "54 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "55 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "56 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "57 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "58 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "59 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "60 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "61 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "62 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "63 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "64 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "65 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "66 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "67 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "68 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "69 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "70 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "71 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "72 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "73 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "74 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "75 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "76 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "77 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "78 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "79 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "80 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "81 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "82 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "83 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "84 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "85 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "86 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "87 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "88 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "89 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "90 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "92 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "93 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "94 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "95 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "96 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "97 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "98 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "99 Train Loss 43.997948 Test MSE 8.627704912776938 Test RE 1.4039631264962542\n",
      "Training time: 38.12\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 56.51782 Test MSE 8.922277755740405 Test RE 1.4277294890792127\n",
      "1 Train Loss 56.05368 Test MSE 8.675591358857508 Test RE 1.4078539511859691\n",
      "2 Train Loss 55.70813 Test MSE 8.15354868000249 Test RE 1.3648389077034577\n",
      "3 Train Loss 52.825546 Test MSE 8.892991536552955 Test RE 1.4253843947209146\n",
      "4 Train Loss 50.96586 Test MSE 8.253151654970404 Test RE 1.3731499740746433\n",
      "5 Train Loss 43.891563 Test MSE 9.033413698838006 Test RE 1.4365938741662918\n",
      "6 Train Loss 43.012085 Test MSE 8.571599162065828 Test RE 1.3993907121308669\n",
      "7 Train Loss 42.945778 Test MSE 8.45917257785378 Test RE 1.390183094410579\n",
      "8 Train Loss 42.71055 Test MSE 8.587588782044733 Test RE 1.4006953286886554\n",
      "9 Train Loss 42.624043 Test MSE 8.540993220446348 Test RE 1.396890130256995\n",
      "10 Train Loss 42.526104 Test MSE 8.421084736175136 Test RE 1.3870498795997839\n",
      "11 Train Loss 42.32926 Test MSE 8.431352837114087 Test RE 1.387895259364803\n",
      "12 Train Loss 42.17666 Test MSE 8.4705229314989 Test RE 1.3911154421161904\n",
      "13 Train Loss 41.779583 Test MSE 8.165829123463554 Test RE 1.3658663450425637\n",
      "14 Train Loss 40.803856 Test MSE 8.009637496944626 Test RE 1.3527404942560182\n",
      "15 Train Loss 39.01279 Test MSE 7.515027509865759 Test RE 1.3103078644011983\n",
      "16 Train Loss 37.991394 Test MSE 7.428316984550943 Test RE 1.302726579747252\n",
      "17 Train Loss 37.43074 Test MSE 7.659123642182761 Test RE 1.3228103994069706\n",
      "18 Train Loss 37.2503 Test MSE 7.688230900806965 Test RE 1.3253215791503072\n",
      "19 Train Loss 37.023712 Test MSE 7.716019470582885 Test RE 1.3277145595960094\n",
      "20 Train Loss 36.91429 Test MSE 7.70947146479429 Test RE 1.327151074245874\n",
      "21 Train Loss 36.82439 Test MSE 7.762717842584102 Test RE 1.3317262511306023\n",
      "22 Train Loss 36.613457 Test MSE 7.707158554482795 Test RE 1.3269519807231498\n",
      "23 Train Loss 36.379524 Test MSE 7.766365760244074 Test RE 1.3320391220355885\n",
      "24 Train Loss 36.2726 Test MSE 7.7010968530790445 Test RE 1.3264300523869534\n",
      "25 Train Loss 36.207817 Test MSE 7.687262412361647 Test RE 1.3252381009693448\n",
      "26 Train Loss 36.160667 Test MSE 7.688941717080482 Test RE 1.325382844113905\n",
      "27 Train Loss 36.050915 Test MSE 7.721425736998082 Test RE 1.328179612968333\n",
      "28 Train Loss 35.949863 Test MSE 7.641603258070752 Test RE 1.3212965567969657\n",
      "29 Train Loss 35.84415 Test MSE 7.820304015331428 Test RE 1.336656696823841\n",
      "30 Train Loss 35.799522 Test MSE 7.792407991775092 Test RE 1.33427055439868\n",
      "31 Train Loss 35.768295 Test MSE 7.766164505711374 Test RE 1.3320218629561127\n",
      "32 Train Loss 35.63389 Test MSE 7.749270420138569 Test RE 1.330572270769732\n",
      "33 Train Loss 35.53892 Test MSE 7.8570901951726615 Test RE 1.339796779707628\n",
      "34 Train Loss 35.518196 Test MSE 7.913066099672079 Test RE 1.344560835814474\n",
      "35 Train Loss 35.47708 Test MSE 7.967314737718185 Test RE 1.3491618341765907\n",
      "36 Train Loss 35.164425 Test MSE 8.090022381732863 Test RE 1.3595116134494656\n",
      "37 Train Loss 34.950607 Test MSE 8.253245765576766 Test RE 1.3731578030600842\n",
      "38 Train Loss 34.701256 Test MSE 8.250883255032978 Test RE 1.3729612542069936\n",
      "39 Train Loss 34.163097 Test MSE 8.291689880047937 Test RE 1.3763522132030583\n",
      "40 Train Loss 34.06163 Test MSE 8.300010360239998 Test RE 1.3770426056329974\n",
      "41 Train Loss 33.97649 Test MSE 8.169366139983932 Test RE 1.3661621244865276\n",
      "42 Train Loss 33.677273 Test MSE 8.006078729869412 Test RE 1.352439942377698\n",
      "43 Train Loss 33.47165 Test MSE 8.00314775470017 Test RE 1.3521923598291672\n",
      "44 Train Loss 32.557953 Test MSE 6.432691455312087 Test RE 1.2122841324732037\n",
      "45 Train Loss 27.877972 Test MSE 7.012041689878661 Test RE 1.2656986154803793\n",
      "46 Train Loss 27.476025 Test MSE 7.19298096695619 Test RE 1.2819247008429018\n",
      "47 Train Loss 27.167355 Test MSE 7.390038209462591 Test RE 1.29936571174114\n",
      "48 Train Loss 26.944 Test MSE 7.514290969492457 Test RE 1.3102436518423974\n",
      "49 Train Loss 26.809498 Test MSE 7.662188995561349 Test RE 1.3230750821490176\n",
      "50 Train Loss 26.580967 Test MSE 7.740528939106725 Test RE 1.3298215901513224\n",
      "51 Train Loss 26.421999 Test MSE 7.90537696142911 Test RE 1.3439074211506261\n",
      "52 Train Loss 26.309235 Test MSE 7.920442875087996 Test RE 1.3451874079100103\n",
      "53 Train Loss 26.079575 Test MSE 7.788622744648952 Test RE 1.3339464468009354\n",
      "54 Train Loss 25.91584 Test MSE 7.758056484536602 Test RE 1.3313263534931594\n",
      "55 Train Loss 25.602512 Test MSE 7.562607784407139 Test RE 1.3144493287733006\n",
      "56 Train Loss 25.306702 Test MSE 7.621489650389286 Test RE 1.3195565062765968\n",
      "57 Train Loss 23.631618 Test MSE 7.543331654784353 Test RE 1.3127730775056774\n",
      "58 Train Loss 22.965103 Test MSE 7.297648420383837 Test RE 1.2912178718353122\n",
      "59 Train Loss 22.084229 Test MSE 7.1269970092835955 Test RE 1.2760313621524826\n",
      "60 Train Loss 21.071943 Test MSE 6.690363630398759 Test RE 1.2363257698623833\n",
      "61 Train Loss 20.278702 Test MSE 6.325644536873333 Test RE 1.2021549583596733\n",
      "62 Train Loss 19.329714 Test MSE 5.8215196264758236 Test RE 1.1532573821427194\n",
      "63 Train Loss 18.292278 Test MSE 5.3416398832658665 Test RE 1.104702567111928\n",
      "64 Train Loss 16.808039 Test MSE 4.342040005137564 Test RE 0.9959899828145621\n",
      "65 Train Loss 15.136385 Test MSE 4.3470870178570244 Test RE 0.9965686640593312\n",
      "66 Train Loss 14.225196 Test MSE 4.31783422924933 Test RE 0.9932099065994929\n",
      "67 Train Loss 13.73809 Test MSE 4.207695593831419 Test RE 0.9804607596914514\n",
      "68 Train Loss 13.223957 Test MSE 4.1223374133101505 Test RE 0.9704648908054931\n",
      "69 Train Loss 12.833183 Test MSE 4.0109506436625795 Test RE 0.9572639836335366\n",
      "70 Train Loss 12.576333 Test MSE 4.0602525106902805 Test RE 0.9631292712659119\n",
      "71 Train Loss 12.3131485 Test MSE 3.9869136101372407 Test RE 0.9543913025958449\n",
      "72 Train Loss 12.105984 Test MSE 4.033327021387254 Test RE 0.9599304722910251\n",
      "73 Train Loss 11.94133 Test MSE 3.9561351815821566 Test RE 0.9507002799919868\n",
      "74 Train Loss 11.850952 Test MSE 3.866724849027929 Test RE 0.9398957697721281\n",
      "75 Train Loss 11.572523 Test MSE 3.8250793612999128 Test RE 0.9348206242910928\n",
      "76 Train Loss 11.425411 Test MSE 3.811463480886405 Test RE 0.9331553316337378\n",
      "77 Train Loss 11.19586 Test MSE 3.7996545927330354 Test RE 0.9317086333284702\n",
      "78 Train Loss 11.07583 Test MSE 3.7769433051693566 Test RE 0.9289199565076247\n",
      "79 Train Loss 10.964642 Test MSE 3.7012323866908603 Test RE 0.919562467841197\n",
      "80 Train Loss 10.717967 Test MSE 3.8032125783359874 Test RE 0.9321447558950455\n",
      "81 Train Loss 10.620163 Test MSE 3.8702852882352774 Test RE 0.9403283932315921\n",
      "82 Train Loss 10.504332 Test MSE 3.8439219907400535 Test RE 0.9371202941355433\n",
      "83 Train Loss 10.450293 Test MSE 3.853241926210679 Test RE 0.9382556726716298\n",
      "84 Train Loss 10.35199 Test MSE 3.8412852934461066 Test RE 0.936798835165083\n",
      "85 Train Loss 10.270149 Test MSE 3.833554909411063 Test RE 0.9358557313184277\n",
      "86 Train Loss 10.219088 Test MSE 3.8025550712606213 Test RE 0.9320641768777774\n",
      "87 Train Loss 10.138946 Test MSE 3.82837723279889 Test RE 0.9352235249616653\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "88 Train Loss 10.035139 Test MSE 3.8237811261740013 Test RE 0.9346619713783855\n",
      "89 Train Loss 9.864943 Test MSE 3.8186492750740726 Test RE 0.9340345614488891\n",
      "90 Train Loss 9.771688 Test MSE 3.8399658875807563 Test RE 0.9366379353700242\n",
      "91 Train Loss 9.719086 Test MSE 3.8123719701108407 Test RE 0.933266537087535\n",
      "92 Train Loss 9.664425 Test MSE 3.7726484630014334 Test RE 0.9283916590091879\n",
      "93 Train Loss 9.605186 Test MSE 3.774854964457194 Test RE 0.9286631126218937\n",
      "94 Train Loss 9.507164 Test MSE 3.827023222678767 Test RE 0.9350581266707251\n",
      "95 Train Loss 9.399088 Test MSE 3.7821789346961383 Test RE 0.9295635717379208\n",
      "96 Train Loss 9.37146 Test MSE 3.833519489208957 Test RE 0.9358514078801964\n",
      "97 Train Loss 9.344646 Test MSE 3.833690601352725 Test RE 0.9358722938772808\n",
      "98 Train Loss 9.28567 Test MSE 3.7559743637813208 Test RE 0.92633776541635\n",
      "99 Train Loss 9.179633 Test MSE 3.7890898694775617 Test RE 0.9304124503072381\n",
      "Training time: 84.57\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 55.77038 Test MSE 8.673202807024191 Test RE 1.4076601336417462\n",
      "1 Train Loss 55.77038 Test MSE 8.673203406110694 Test RE 1.4076601822575914\n",
      "2 Train Loss 55.77038 Test MSE 8.673203655097392 Test RE 1.4076602024628515\n",
      "3 Train Loss 55.77038 Test MSE 8.673204133437219 Test RE 1.4076602412801065\n",
      "4 Train Loss 55.770374 Test MSE 8.67320454822492 Test RE 1.40766027494011\n",
      "5 Train Loss 55.77037 Test MSE 8.673205384378258 Test RE 1.4076603427939116\n",
      "6 Train Loss 55.77037 Test MSE 8.673205537697633 Test RE 1.4076603552357714\n",
      "7 Train Loss 55.77037 Test MSE 8.673205382944808 Test RE 1.4076603426775873\n",
      "8 Train Loss 55.77037 Test MSE 8.673205450347952 Test RE 1.407660348147349\n",
      "9 Train Loss 55.77037 Test MSE 8.673205665380381 Test RE 1.4076603655972193\n",
      "10 Train Loss 55.77037 Test MSE 8.673205491016043 Test RE 1.4076603514475623\n",
      "11 Train Loss 55.77037 Test MSE 8.673205571943654 Test RE 1.4076603580148341\n",
      "12 Train Loss 55.77037 Test MSE 8.673205803629923 Test RE 1.4076603768161629\n",
      "13 Train Loss 55.77037 Test MSE 8.673205647082439 Test RE 1.4076603641123424\n",
      "14 Train Loss 55.77037 Test MSE 8.673205685766755 Test RE 1.4076603672515722\n",
      "15 Train Loss 55.770367 Test MSE 8.673205376066383 Test RE 1.4076603421194038\n",
      "16 Train Loss 55.770363 Test MSE 8.673205150207794 Test RE 1.4076603237909906\n",
      "17 Train Loss 55.770363 Test MSE 8.673205020710284 Test RE 1.4076603132822743\n",
      "18 Train Loss 55.770363 Test MSE 8.67320504163302 Test RE 1.407660314980153\n",
      "19 Train Loss 55.770363 Test MSE 8.673204998190947 Test RE 1.407660311454831\n",
      "20 Train Loss 55.770363 Test MSE 8.673204862873854 Test RE 1.4076603004738557\n",
      "21 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "22 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "23 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "24 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "25 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "26 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "27 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "28 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "29 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "30 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "31 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "32 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "33 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "34 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "35 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "36 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "37 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "38 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "39 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "40 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "41 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "42 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "43 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "44 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "45 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "46 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "47 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "48 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "49 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "50 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "51 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "52 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "53 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "54 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "55 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "56 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "57 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "58 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "59 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "60 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "61 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "62 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "63 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "64 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "65 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "66 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "67 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "68 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "69 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "70 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "71 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "72 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "73 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "74 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "75 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "76 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "77 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "78 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "79 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "80 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "81 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "82 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "83 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "84 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "85 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "86 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "87 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "88 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "89 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "90 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "91 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "92 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "93 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "94 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "95 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "96 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "97 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "98 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "99 Train Loss 55.77036 Test MSE 8.673204899295744 Test RE 1.4076603034294901\n",
      "Training time: 24.27\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "1 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "2 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "3 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "4 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "5 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "6 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "7 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "8 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "9 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "10 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "11 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "12 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "13 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "14 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "15 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "16 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "17 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "18 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "19 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "20 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "21 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "22 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "23 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "24 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "25 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "26 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "27 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "28 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "29 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "30 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "31 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "32 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "33 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "34 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "35 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "36 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "37 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "38 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "39 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "40 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "41 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "42 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "43 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "44 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "45 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "46 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "47 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "48 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "49 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "50 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "51 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "52 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "53 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "54 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "55 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "56 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "57 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "58 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "59 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "60 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "61 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "62 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "63 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "64 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "65 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "66 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "67 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "68 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "69 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "70 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "71 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "72 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "73 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "74 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "75 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "76 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "77 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "78 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "79 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "80 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "81 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "82 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "83 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "84 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "85 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "86 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "87 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "88 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "89 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "90 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "91 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "92 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "93 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "94 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "95 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "96 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "97 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "98 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "99 Train Loss 56.93834 Test MSE 8.708798947452802 Test RE 1.410545801265366\n",
      "Training time: 33.36\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 56.261257 Test MSE 8.754679264462704 Test RE 1.4142564895560763\n",
      "1 Train Loss 56.258595 Test MSE 8.750462871679845 Test RE 1.4139158843308757\n",
      "2 Train Loss 56.210793 Test MSE 9.003773247491022 Test RE 1.4342350605922547\n",
      "3 Train Loss 55.34053 Test MSE 9.0106000563191 Test RE 1.43477868786029\n",
      "4 Train Loss 50.41898 Test MSE 7.720878176764727 Test RE 1.3281325186066626\n",
      "5 Train Loss 46.656803 Test MSE 8.095376399332732 Test RE 1.3599614048632476\n",
      "6 Train Loss 44.298553 Test MSE 8.263214087555406 Test RE 1.3739868071182604\n",
      "7 Train Loss 41.67993 Test MSE 8.314507660728452 Test RE 1.3782446939351252\n",
      "8 Train Loss 40.829746 Test MSE 8.102439599236043 Test RE 1.360554557328872\n",
      "9 Train Loss 40.32452 Test MSE 8.416694515280051 Test RE 1.3866882724300482\n",
      "10 Train Loss 39.79509 Test MSE 8.329234096708973 Test RE 1.3794647092273236\n",
      "11 Train Loss 38.918884 Test MSE 8.32125376045098 Test RE 1.3788037102544943\n",
      "12 Train Loss 38.683884 Test MSE 8.088992628842494 Test RE 1.3594250867688615\n",
      "13 Train Loss 38.02119 Test MSE 8.423502292244976 Test RE 1.3872489650102648\n",
      "14 Train Loss 37.105865 Test MSE 7.933736035796645 Test RE 1.3463157725769996\n",
      "15 Train Loss 35.954533 Test MSE 7.565900994642325 Test RE 1.314735492419446\n",
      "16 Train Loss 34.74257 Test MSE 7.476698240415316 Test RE 1.3069620785165328\n",
      "17 Train Loss 34.270126 Test MSE 7.1547860664666585 Test RE 1.2785166452389438\n",
      "18 Train Loss 32.869076 Test MSE 6.146418526503304 Test RE 1.185002113407128\n",
      "19 Train Loss 27.390308 Test MSE 5.1858167703159435 Test RE 1.088470452436274\n",
      "20 Train Loss 22.46297 Test MSE 4.785942230662084 Test RE 1.0456631088861568\n",
      "21 Train Loss 21.522743 Test MSE 5.113867902588435 Test RE 1.0808932705762229\n",
      "22 Train Loss 20.442545 Test MSE 4.939789574482603 Test RE 1.0623369441212194\n",
      "23 Train Loss 19.231953 Test MSE 4.7544293148380925 Test RE 1.0422148520972236\n",
      "24 Train Loss 18.741926 Test MSE 4.698085848030251 Test RE 1.0360209424552\n",
      "25 Train Loss 18.015278 Test MSE 4.578264736699004 Test RE 1.0227241500959026\n",
      "26 Train Loss 17.840591 Test MSE 4.591784647614245 Test RE 1.0242331220679397\n",
      "27 Train Loss 17.323856 Test MSE 4.481410119700957 Test RE 1.0118482980816832\n",
      "28 Train Loss 16.936836 Test MSE 4.4566482964460405 Test RE 1.009048965466964\n",
      "29 Train Loss 16.630163 Test MSE 4.423989773779777 Test RE 1.0053449887639174\n",
      "30 Train Loss 16.50516 Test MSE 4.401056989396527 Test RE 1.002735882850304\n",
      "31 Train Loss 16.330177 Test MSE 4.410883369544454 Test RE 1.0038546788808778\n",
      "32 Train Loss 16.09254 Test MSE 4.3586905651269685 Test RE 0.9978978328976993\n",
      "33 Train Loss 15.979445 Test MSE 4.3836677527532455 Test RE 1.0007529422094803\n",
      "34 Train Loss 15.717362 Test MSE 4.525721128008395 Test RE 1.0168384384642293\n",
      "35 Train Loss 15.525251 Test MSE 4.388177833670672 Test RE 1.0012676158447462\n",
      "36 Train Loss 15.405533 Test MSE 4.3357924285961005 Test RE 0.9952731810822338\n",
      "37 Train Loss 15.320936 Test MSE 4.420210800537992 Test RE 1.0049155140515311\n",
      "38 Train Loss 15.217193 Test MSE 4.362545753917457 Test RE 0.9983390473160557\n",
      "39 Train Loss 15.168108 Test MSE 4.433819896286144 Test RE 1.006461309231057\n",
      "40 Train Loss 15.016425 Test MSE 4.41291159909118 Test RE 1.0040854505367705\n",
      "41 Train Loss 14.929445 Test MSE 4.386108159912021 Test RE 1.001031465237976\n",
      "42 Train Loss 14.811386 Test MSE 4.390132858928662 Test RE 1.0014906338689138\n",
      "43 Train Loss 14.77318 Test MSE 4.3698743534213955 Test RE 0.9991772456225531\n",
      "44 Train Loss 14.730206 Test MSE 4.369701284914351 Test RE 0.9991574592611612\n",
      "45 Train Loss 14.673403 Test MSE 4.3533488796519135 Test RE 0.9972861709279329\n",
      "46 Train Loss 14.573437 Test MSE 4.330985636484989 Test RE 0.9947213329447284\n",
      "47 Train Loss 14.498139 Test MSE 4.310694833753575 Test RE 0.9923884470977896\n",
      "48 Train Loss 14.462387 Test MSE 4.2705012366946 Test RE 0.9877510187202244\n",
      "49 Train Loss 14.417833 Test MSE 4.259694512733037 Test RE 0.9865004495752742\n",
      "50 Train Loss 14.347895 Test MSE 4.212095204853074 Test RE 0.9809732158398504\n",
      "51 Train Loss 14.296749 Test MSE 4.1786238957231605 Test RE 0.9770678022426316\n",
      "52 Train Loss 14.223068 Test MSE 4.180183536688465 Test RE 0.9772501269553843\n",
      "53 Train Loss 14.175579 Test MSE 4.215046119369424 Test RE 0.9813167813499524\n",
      "54 Train Loss 14.164782 Test MSE 4.219609528994275 Test RE 0.9818478477854315\n",
      "55 Train Loss 14.14566 Test MSE 4.206775010920192 Test RE 0.9803534985107795\n",
      "56 Train Loss 14.054312 Test MSE 4.152168841740734 Test RE 0.9739699614301524\n",
      "57 Train Loss 14.02583 Test MSE 4.162988463047069 Test RE 0.9752381095438271\n",
      "58 Train Loss 14.004005 Test MSE 4.200780483749357 Test RE 0.979654762458659\n",
      "59 Train Loss 13.98802 Test MSE 4.1976857335654865 Test RE 0.9792938360356477\n",
      "60 Train Loss 13.955993 Test MSE 4.2034266974110865 Test RE 0.9799632727103208\n",
      "61 Train Loss 13.911985 Test MSE 4.188762533835136 Test RE 0.9782524189421788\n",
      "62 Train Loss 13.893361 Test MSE 4.179899828060182 Test RE 0.9772169634596862\n",
      "63 Train Loss 13.8755665 Test MSE 4.163331239649892 Test RE 0.975278258817375\n",
      "64 Train Loss 13.791872 Test MSE 4.138642808535912 Test RE 0.9723822735422346\n",
      "65 Train Loss 13.741489 Test MSE 4.141995578090257 Test RE 0.9727760637075757\n",
      "66 Train Loss 13.69063 Test MSE 4.1517504861162955 Test RE 0.9739208935705475\n",
      "67 Train Loss 13.679605 Test MSE 4.149803741837263 Test RE 0.9736925323960746\n",
      "68 Train Loss 13.650354 Test MSE 4.172735311902458 Test RE 0.9763791097182091\n",
      "69 Train Loss 13.635911 Test MSE 4.155157564590674 Test RE 0.9743204291927263\n",
      "70 Train Loss 13.612106 Test MSE 4.128139833519669 Test RE 0.9711476424123288\n",
      "71 Train Loss 13.587082 Test MSE 4.156654665589377 Test RE 0.9744959369442794\n",
      "72 Train Loss 13.562916 Test MSE 4.180074547257491 Test RE 0.9772373870088499\n",
      "73 Train Loss 13.537878 Test MSE 4.156138360950929 Test RE 0.9744354132253635\n",
      "74 Train Loss 13.5259285 Test MSE 4.147515178697973 Test RE 0.9734240059440379\n",
      "75 Train Loss 13.491802 Test MSE 4.139260469189116 Test RE 0.9724548311282448\n",
      "76 Train Loss 13.471718 Test MSE 4.147160805129202 Test RE 0.973382419223697\n",
      "77 Train Loss 13.447825 Test MSE 4.1533089171620325 Test RE 0.9741036654105721\n",
      "78 Train Loss 13.41829 Test MSE 4.132553579945668 Test RE 0.9716666721063202\n",
      "79 Train Loss 13.370226 Test MSE 4.144808389885314 Test RE 0.9731063117185217\n",
      "80 Train Loss 13.339277 Test MSE 4.153447011907458 Test RE 0.9741198594238554\n",
      "81 Train Loss 13.317701 Test MSE 4.175020153178068 Test RE 0.9766463883521213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "82 Train Loss 13.303546 Test MSE 4.170784194432342 Test RE 0.9761508118584088\n",
      "83 Train Loss 13.252217 Test MSE 4.165843908409188 Test RE 0.9755725161610326\n",
      "84 Train Loss 13.170216 Test MSE 4.170411905652223 Test RE 0.9761072447391366\n",
      "85 Train Loss 13.073425 Test MSE 4.166830202129747 Test RE 0.9756879962561252\n",
      "86 Train Loss 13.011206 Test MSE 4.124056489160038 Test RE 0.9706672188416541\n",
      "87 Train Loss 12.982701 Test MSE 4.105860317384007 Test RE 0.9685234614575986\n",
      "88 Train Loss 12.9320755 Test MSE 4.12610664467912 Test RE 0.9709084584442764\n",
      "89 Train Loss 12.872953 Test MSE 4.070754633600701 Test RE 0.9643740669629108\n",
      "90 Train Loss 12.838309 Test MSE 4.071339044973713 Test RE 0.9644432888882022\n",
      "91 Train Loss 12.808338 Test MSE 4.050334805889674 Test RE 0.9619522666420862\n",
      "92 Train Loss 12.7334 Test MSE 3.982075305915647 Test RE 0.9538120277858837\n",
      "93 Train Loss 12.6390505 Test MSE 4.042682115079839 Test RE 0.9610430820780487\n",
      "94 Train Loss 12.583357 Test MSE 4.032044371018916 Test RE 0.959777824976329\n",
      "95 Train Loss 12.508339 Test MSE 3.9867453436572236 Test RE 0.9543711624855715\n",
      "96 Train Loss 12.42028 Test MSE 3.9868241913240627 Test RE 0.9543805999540975\n",
      "97 Train Loss 12.326045 Test MSE 3.9191592865438674 Test RE 0.9462470045915032\n",
      "98 Train Loss 12.266632 Test MSE 3.925593188089914 Test RE 0.9470233909346457\n",
      "99 Train Loss 12.221094 Test MSE 3.9545918334689807 Test RE 0.9505148206302474\n",
      "Training time: 84.30\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 55.621593 Test MSE 8.81840358729696 Test RE 1.419394263916947\n",
      "1 Train Loss 55.189957 Test MSE 8.804568643905153 Test RE 1.4182804031584662\n",
      "2 Train Loss 55.100124 Test MSE 8.474717987821819 Test RE 1.391459876887594\n",
      "3 Train Loss 54.199875 Test MSE 8.04730709001722 Test RE 1.3559177548442876\n",
      "4 Train Loss 51.40233 Test MSE 9.157472603723994 Test RE 1.4464248493731404\n",
      "5 Train Loss 49.89971 Test MSE 8.607891212529802 Test RE 1.4023500848951302\n",
      "6 Train Loss 45.125732 Test MSE 8.938560060774876 Test RE 1.4290316302804795\n",
      "7 Train Loss 43.400124 Test MSE 8.430817336396002 Test RE 1.3878511839522463\n",
      "8 Train Loss 43.153732 Test MSE 8.366362817666685 Test RE 1.3825358685292106\n",
      "9 Train Loss 42.86033 Test MSE 8.602069501580885 Test RE 1.4018757841926608\n",
      "10 Train Loss 42.682617 Test MSE 8.439596926249203 Test RE 1.388573628356434\n",
      "11 Train Loss 42.670067 Test MSE 8.393242690652015 Test RE 1.384755028031847\n",
      "12 Train Loss 42.399895 Test MSE 8.285376226125006 Test RE 1.3758281061448883\n",
      "13 Train Loss 42.26433 Test MSE 8.290623262598707 Test RE 1.3762636855010917\n",
      "14 Train Loss 41.883167 Test MSE 8.35270691060395 Test RE 1.3814070929717917\n",
      "15 Train Loss 39.76055 Test MSE 7.574977026780637 Test RE 1.3155238324442213\n",
      "16 Train Loss 39.100357 Test MSE 7.713117847207284 Test RE 1.3274648913653513\n",
      "17 Train Loss 38.645313 Test MSE 7.874146332471615 Test RE 1.341250203881102\n",
      "18 Train Loss 38.441517 Test MSE 7.738019079086082 Test RE 1.3296059759037238\n",
      "19 Train Loss 38.33396 Test MSE 7.611016126030368 Test RE 1.3186495209531917\n",
      "20 Train Loss 37.71296 Test MSE 7.819122986704424 Test RE 1.3365557615260362\n",
      "21 Train Loss 37.286594 Test MSE 7.688675896324143 Test RE 1.3253599334632513\n",
      "22 Train Loss 37.094498 Test MSE 7.603627920771466 Test RE 1.3180093423867882\n",
      "23 Train Loss 36.975334 Test MSE 7.619418788490531 Test RE 1.319377223415562\n",
      "24 Train Loss 36.9487 Test MSE 7.641729662711624 Test RE 1.3213074849569622\n",
      "25 Train Loss 36.762203 Test MSE 7.661596865185744 Test RE 1.323023957850059\n",
      "26 Train Loss 36.72031 Test MSE 7.6324899600692815 Test RE 1.3205084393620843\n",
      "27 Train Loss 36.50817 Test MSE 7.682328867646699 Test RE 1.3248127759038784\n",
      "28 Train Loss 36.341045 Test MSE 7.740400874600447 Test RE 1.3298105893753338\n",
      "29 Train Loss 36.123848 Test MSE 7.618921914079224 Test RE 1.3193342033711646\n",
      "30 Train Loss 35.95287 Test MSE 7.693430429034274 Test RE 1.3257696589715948\n",
      "31 Train Loss 35.81382 Test MSE 7.6743381139237705 Test RE 1.32412359642121\n",
      "32 Train Loss 35.620274 Test MSE 7.519211706844195 Test RE 1.3106725885001653\n",
      "33 Train Loss 35.517143 Test MSE 7.544396216342049 Test RE 1.3128657075604584\n",
      "34 Train Loss 35.377026 Test MSE 7.565172111745218 Test RE 1.3146721614760264\n",
      "35 Train Loss 35.051502 Test MSE 7.739901342520954 Test RE 1.329767678563759\n",
      "36 Train Loss 34.85895 Test MSE 7.683517041301432 Test RE 1.3249152218421787\n",
      "37 Train Loss 34.621708 Test MSE 7.721443086825597 Test RE 1.3281811051587031\n",
      "38 Train Loss 34.04698 Test MSE 7.839505949259408 Test RE 1.3382967006879598\n",
      "39 Train Loss 33.740326 Test MSE 7.698410702759005 Test RE 1.3261987021438926\n",
      "40 Train Loss 33.21546 Test MSE 7.845756669544128 Test RE 1.3388301304448977\n",
      "41 Train Loss 32.67831 Test MSE 8.39861284529494 Test RE 1.3851979533599086\n",
      "42 Train Loss 32.416138 Test MSE 8.246426153301643 Test RE 1.3725903693913117\n",
      "43 Train Loss 31.095732 Test MSE 8.632554196003916 Test RE 1.4043576264543995\n",
      "44 Train Loss 28.873009 Test MSE 8.820820277177083 Test RE 1.419588743605765\n",
      "45 Train Loss 27.721867 Test MSE 8.787648828836083 Test RE 1.4169169870145413\n",
      "46 Train Loss 26.992073 Test MSE 8.8325703545082 Test RE 1.4205339349714199\n",
      "47 Train Loss 26.595974 Test MSE 8.666092401307546 Test RE 1.4070830061889497\n",
      "48 Train Loss 26.159351 Test MSE 8.58576200511615 Test RE 1.4005463407674559\n",
      "49 Train Loss 25.652805 Test MSE 8.16337632102828 Test RE 1.3656611942940762\n",
      "50 Train Loss 24.814363 Test MSE 8.108367660647342 Test RE 1.3610521837634881\n",
      "51 Train Loss 24.13404 Test MSE 7.909844888085753 Test RE 1.3442871393894598\n",
      "52 Train Loss 23.345808 Test MSE 7.740362042417574 Test RE 1.3298072536622614\n",
      "53 Train Loss 22.08208 Test MSE 7.5091990529487 Test RE 1.3097996457500856\n",
      "54 Train Loss 21.361519 Test MSE 7.560360120920687 Test RE 1.3142539821813366\n",
      "55 Train Loss 21.0821 Test MSE 7.5437735592426955 Test RE 1.3128115294660554\n",
      "56 Train Loss 20.301743 Test MSE 7.589920746304022 Test RE 1.3168208088612952\n",
      "57 Train Loss 19.969107 Test MSE 7.642888898891705 Test RE 1.3214077011007075\n",
      "58 Train Loss 19.124002 Test MSE 7.484760396273529 Test RE 1.3076665400759477\n",
      "59 Train Loss 18.7934 Test MSE 7.379375926251675 Test RE 1.298428016589209\n",
      "60 Train Loss 18.565014 Test MSE 7.368276540085446 Test RE 1.2974511604565866\n",
      "61 Train Loss 18.41092 Test MSE 7.241795285044639 Test RE 1.2862671613946364\n",
      "62 Train Loss 18.248055 Test MSE 7.039628067905424 Test RE 1.2681858915281456\n",
      "63 Train Loss 17.968895 Test MSE 6.858522267342744 Test RE 1.2517665347788898\n",
      "64 Train Loss 17.80312 Test MSE 6.790043697557939 Test RE 1.245501758035323\n",
      "65 Train Loss 17.517181 Test MSE 6.821974682727754 Test RE 1.2484268829424479\n",
      "66 Train Loss 16.832272 Test MSE 6.959166962694097 Test RE 1.2609175467137936\n",
      "67 Train Loss 16.65107 Test MSE 7.002986639217458 Test RE 1.2648811169519183\n",
      "68 Train Loss 16.301308 Test MSE 6.859875505379793 Test RE 1.2518900201649472\n",
      "69 Train Loss 15.593601 Test MSE 6.116729972107586 Test RE 1.1821367385993398\n",
      "70 Train Loss 15.341589 Test MSE 6.083983079112566 Test RE 1.1789681128458207\n",
      "71 Train Loss 14.969398 Test MSE 6.256295896579399 Test RE 1.1955471285376003\n",
      "72 Train Loss 14.497564 Test MSE 6.4601607254204305 Test RE 1.2148697597878229\n",
      "73 Train Loss 14.154425 Test MSE 6.471135162356631 Test RE 1.215901224379886\n",
      "74 Train Loss 13.659639 Test MSE 6.215158372737617 Test RE 1.1916100575011195\n",
      "75 Train Loss 12.964361 Test MSE 5.501045759890121 Test RE 1.1210647292373828\n",
      "76 Train Loss 11.938696 Test MSE 4.273955128159604 Test RE 0.9881503741001171\n",
      "77 Train Loss 10.927181 Test MSE 3.9685743978429318 Test RE 0.9521937432299321\n",
      "78 Train Loss 10.323626 Test MSE 3.945807540306861 Test RE 0.9494585494563622\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "79 Train Loss 9.825459 Test MSE 3.8236169955509816 Test RE 0.934641911613869\n",
      "80 Train Loss 9.410159 Test MSE 3.891578560737938 Test RE 0.9429115625446326\n",
      "81 Train Loss 9.060799 Test MSE 3.876528310387878 Test RE 0.9410864930987901\n",
      "82 Train Loss 8.7895975 Test MSE 3.827370938550212 Test RE 0.9351006044854016\n",
      "83 Train Loss 8.480446 Test MSE 3.85917578992378 Test RE 0.9389778359319857\n",
      "84 Train Loss 8.218051 Test MSE 3.784416825742281 Test RE 0.929838539474972\n",
      "85 Train Loss 8.0560465 Test MSE 3.7970866776487124 Test RE 0.9313937424653469\n",
      "86 Train Loss 7.932015 Test MSE 3.807810524518269 Test RE 0.9327080503107411\n",
      "87 Train Loss 7.7528706 Test MSE 3.730045959762464 Test RE 0.923134860854108\n",
      "88 Train Loss 7.6917505 Test MSE 3.688234955015541 Test RE 0.9179464572362245\n",
      "89 Train Loss 7.596504 Test MSE 3.7926770916911265 Test RE 0.9308527680025932\n",
      "90 Train Loss 7.5190105 Test MSE 3.842322430015972 Test RE 0.9369252931971791\n",
      "91 Train Loss 7.4488015 Test MSE 3.801422280958792 Test RE 0.931925334456564\n",
      "92 Train Loss 7.4029737 Test MSE 3.770557173970182 Test RE 0.9281343060581724\n",
      "93 Train Loss 7.357984 Test MSE 3.789365784153657 Test RE 0.9304463251670996\n",
      "94 Train Loss 7.271554 Test MSE 3.8374447926343964 Test RE 0.9363304142952109\n",
      "95 Train Loss 7.242122 Test MSE 3.835512585715804 Test RE 0.9360946569101448\n",
      "96 Train Loss 7.2099323 Test MSE 3.846289815676014 Test RE 0.937408878971449\n",
      "97 Train Loss 7.1598234 Test MSE 3.822099965897289 Test RE 0.9344564824558403\n",
      "98 Train Loss 7.1134796 Test MSE 3.7495261995356977 Test RE 0.9255422669206532\n",
      "99 Train Loss 7.0748653 Test MSE 3.7316167324576988 Test RE 0.9233292126695947\n",
      "Training time: 85.61\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 56.84957 Test MSE 8.578118155583411 Test RE 1.39992275317432\n",
      "1 Train Loss 56.828148 Test MSE 8.693037757894874 Test RE 1.4092688199147834\n",
      "2 Train Loss 56.65603 Test MSE 8.449929043076308 Test RE 1.389423344091564\n",
      "3 Train Loss 52.888474 Test MSE 8.690119104347778 Test RE 1.4090322217969802\n",
      "4 Train Loss 48.34452 Test MSE 7.134504865602519 Test RE 1.2767032958144617\n",
      "5 Train Loss 46.69296 Test MSE 8.105200049415666 Test RE 1.3607863037795023\n",
      "6 Train Loss 45.81806 Test MSE 8.110695014999255 Test RE 1.3612475019572732\n",
      "7 Train Loss 44.12796 Test MSE 8.191593272708122 Test RE 1.3680193824184896\n",
      "8 Train Loss 43.576157 Test MSE 7.8306242317016315 Test RE 1.3375383785477084\n",
      "9 Train Loss 42.99305 Test MSE 8.171835135338064 Test RE 1.3663685537898118\n",
      "10 Train Loss 42.947495 Test MSE 8.248030361338854 Test RE 1.372723870461042\n",
      "11 Train Loss 42.80497 Test MSE 8.169658803793455 Test RE 1.3661865953343324\n",
      "12 Train Loss 42.786488 Test MSE 8.120199559473868 Test RE 1.3620448595972958\n",
      "13 Train Loss 42.72695 Test MSE 8.04459769535458 Test RE 1.3556894781326658\n",
      "14 Train Loss 42.595592 Test MSE 8.123394537742994 Test RE 1.3623127887118673\n",
      "15 Train Loss 42.403095 Test MSE 8.134710793346684 Test RE 1.3632613401148796\n",
      "16 Train Loss 42.16459 Test MSE 8.132362553954437 Test RE 1.363064560221781\n",
      "17 Train Loss 42.037636 Test MSE 8.249716271729783 Test RE 1.3728641567528403\n",
      "18 Train Loss 41.9412 Test MSE 8.31892573437849 Test RE 1.3786108237245993\n",
      "19 Train Loss 41.925236 Test MSE 8.315684916823523 Test RE 1.3783422637253255\n",
      "20 Train Loss 41.91075 Test MSE 8.281697318871318 Test RE 1.3755226217680714\n",
      "21 Train Loss 41.825935 Test MSE 8.242728076882797 Test RE 1.372282568582653\n",
      "22 Train Loss 41.72101 Test MSE 8.284005591709125 Test RE 1.3757143010926527\n",
      "23 Train Loss 41.651714 Test MSE 8.26596744205017 Test RE 1.3742156985416019\n",
      "24 Train Loss 41.58922 Test MSE 8.257698647492829 Test RE 1.3735281837080957\n",
      "25 Train Loss 41.056618 Test MSE 8.282906798485739 Test RE 1.3756230604685626\n",
      "26 Train Loss 40.647217 Test MSE 8.203292599595219 Test RE 1.3689959442966293\n",
      "27 Train Loss 39.802555 Test MSE 8.133322116459077 Test RE 1.3631449739425698\n",
      "28 Train Loss 39.13799 Test MSE 8.135249534804196 Test RE 1.3633064820541745\n",
      "29 Train Loss 38.409645 Test MSE 8.466366579133618 Test RE 1.3907741010054353\n",
      "30 Train Loss 37.365105 Test MSE 7.980352106196915 Test RE 1.3502652379423343\n",
      "31 Train Loss 36.298847 Test MSE 7.965588840903458 Test RE 1.3490156970962568\n",
      "32 Train Loss 34.267616 Test MSE 7.4136661999428 Test RE 1.301441269318721\n",
      "33 Train Loss 33.405212 Test MSE 7.0652437441367795 Test RE 1.2704911227610365\n",
      "34 Train Loss 32.572353 Test MSE 6.1787270182315375 Test RE 1.1881124979955535\n",
      "35 Train Loss 31.237682 Test MSE 5.9278374967085865 Test RE 1.1637406509914037\n",
      "36 Train Loss 27.758686 Test MSE 4.967187068793627 Test RE 1.0652788837305391\n",
      "37 Train Loss 25.419392 Test MSE 5.133210714041921 Test RE 1.0829355389890516\n",
      "38 Train Loss 24.604942 Test MSE 5.154517925981431 Test RE 1.0851807655481738\n",
      "39 Train Loss 22.982376 Test MSE 5.145773975235671 Test RE 1.0842599428035793\n",
      "40 Train Loss 21.368055 Test MSE 5.165480380390223 Test RE 1.0863341155241324\n",
      "41 Train Loss 20.217907 Test MSE 4.901269637617306 Test RE 1.0581868443423585\n",
      "42 Train Loss 19.232002 Test MSE 5.014078556005379 Test RE 1.0702953218689613\n",
      "43 Train Loss 18.649605 Test MSE 4.845960785607464 Test RE 1.0521992986820676\n",
      "44 Train Loss 17.639534 Test MSE 5.255127746533261 Test RE 1.095720278323021\n",
      "45 Train Loss 17.127209 Test MSE 5.539536884534504 Test RE 1.1249799686730046\n",
      "46 Train Loss 16.033497 Test MSE 5.844964271232951 Test RE 1.1555772697515534\n",
      "47 Train Loss 15.551889 Test MSE 6.014819532225736 Test RE 1.1722476234874335\n",
      "48 Train Loss 14.545139 Test MSE 5.896963525904823 Test RE 1.1607061380018018\n",
      "49 Train Loss 14.070218 Test MSE 6.023621163269603 Test RE 1.173104999111598\n",
      "50 Train Loss 13.583514 Test MSE 6.178911556985986 Test RE 1.18813024041748\n",
      "51 Train Loss 13.34359 Test MSE 6.043370467655361 Test RE 1.175026521750209\n",
      "52 Train Loss 12.955709 Test MSE 6.1025339336578925 Test RE 1.1807641582392734\n",
      "53 Train Loss 12.67942 Test MSE 6.029709129392911 Test RE 1.1736976675151354\n",
      "54 Train Loss 12.495726 Test MSE 5.993547247323522 Test RE 1.1701728752632887\n",
      "55 Train Loss 12.411046 Test MSE 6.047539694140304 Test RE 1.1754317680515114\n",
      "56 Train Loss 12.261072 Test MSE 6.037512928338482 Test RE 1.17445693622872\n",
      "57 Train Loss 12.149799 Test MSE 6.007001853241103 Test RE 1.1714855694243522\n",
      "58 Train Loss 12.058666 Test MSE 5.97132181774127 Test RE 1.1680012272322169\n",
      "59 Train Loss 11.978547 Test MSE 5.915062597157296 Test RE 1.1624860038923477\n",
      "60 Train Loss 11.777239 Test MSE 5.9392686700436235 Test RE 1.1648621825771028\n",
      "61 Train Loss 11.734501 Test MSE 5.930271571811421 Test RE 1.1639795527345567\n",
      "62 Train Loss 11.675678 Test MSE 5.833363154714846 Test RE 1.154429902153743\n",
      "63 Train Loss 11.58407 Test MSE 5.8124173843591 Test RE 1.1523554411515877\n",
      "64 Train Loss 11.493179 Test MSE 5.89068654927327 Test RE 1.1600882212457297\n",
      "65 Train Loss 11.446407 Test MSE 5.808712640201995 Test RE 1.151988135923869\n",
      "66 Train Loss 11.344187 Test MSE 5.78558966960632 Test RE 1.1496929673156315\n",
      "67 Train Loss 11.290022 Test MSE 5.832623003783475 Test RE 1.154356661430504\n",
      "68 Train Loss 11.22621 Test MSE 5.803879141728239 Test RE 1.1515087447014742\n",
      "69 Train Loss 11.150297 Test MSE 5.759557868604295 Test RE 1.1471035752643675\n",
      "70 Train Loss 11.040425 Test MSE 5.650600697603616 Test RE 1.1362015297337151\n",
      "71 Train Loss 10.975132 Test MSE 5.679192020293737 Test RE 1.1390724205306693\n",
      "72 Train Loss 10.901646 Test MSE 5.631010528080675 Test RE 1.1342302609003856\n",
      "73 Train Loss 10.572107 Test MSE 5.333381789227682 Test RE 1.1038483101314454\n",
      "74 Train Loss 9.793058 Test MSE 4.942243417471162 Test RE 1.0626007695706228\n",
      "75 Train Loss 9.52483 Test MSE 4.843751454454402 Test RE 1.0519594162508283\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "76 Train Loss 9.421524 Test MSE 4.845674316039658 Test RE 1.0521681977769561\n",
      "77 Train Loss 9.297695 Test MSE 4.890253270038333 Test RE 1.0569969554032812\n",
      "78 Train Loss 9.213519 Test MSE 4.841495805544952 Test RE 1.0517144483323309\n",
      "79 Train Loss 9.154898 Test MSE 4.909132021826909 Test RE 1.0590352508001117\n",
      "80 Train Loss 9.092508 Test MSE 4.972503237473232 Test RE 1.0658487925841447\n",
      "81 Train Loss 9.020183 Test MSE 4.9477580882807475 Test RE 1.0631934416843936\n",
      "82 Train Loss 8.9491825 Test MSE 4.977458991108938 Test RE 1.0663797895849783\n",
      "83 Train Loss 8.906139 Test MSE 4.973281171708732 Test RE 1.065932163855977\n",
      "84 Train Loss 8.851058 Test MSE 4.8982183002162545 Test RE 1.0578574003375525\n",
      "85 Train Loss 8.812511 Test MSE 4.931293341437636 Test RE 1.0614229632236842\n",
      "86 Train Loss 8.738126 Test MSE 4.965688121916933 Test RE 1.0651181371230667\n",
      "87 Train Loss 8.71627 Test MSE 4.973698658792871 Test RE 1.0659769032902404\n",
      "88 Train Loss 8.620405 Test MSE 4.993073196152665 Test RE 1.0680510876248475\n",
      "89 Train Loss 8.553691 Test MSE 4.998547239060867 Test RE 1.0686363940778787\n",
      "90 Train Loss 8.437082 Test MSE 5.019926394682738 Test RE 1.0709192740523295\n",
      "91 Train Loss 8.344668 Test MSE 4.897058708170788 Test RE 1.0577321756578273\n",
      "92 Train Loss 8.296974 Test MSE 4.861794681070591 Test RE 1.0539168968419579\n",
      "93 Train Loss 8.22375 Test MSE 4.850532481868874 Test RE 1.0526955059179304\n",
      "94 Train Loss 8.075828 Test MSE 4.799195845019564 Test RE 1.0471099748795658\n",
      "95 Train Loss 7.563093 Test MSE 4.230335020983442 Test RE 0.98309489656946\n",
      "96 Train Loss 6.578457 Test MSE 3.7101735951159935 Test RE 0.9206725089390594\n",
      "97 Train Loss 6.311408 Test MSE 3.566395859605333 Test RE 0.9026571649187666\n",
      "98 Train Loss 6.077724 Test MSE 3.3459295782570897 Test RE 0.87431203863428\n",
      "99 Train Loss 5.878248 Test MSE 3.1333661273007634 Test RE 0.8460842923620913\n",
      "Training time: 83.93\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 56.236115 Test MSE 8.674380309567452 Test RE 1.40775568468387\n",
      "1 Train Loss 56.2278 Test MSE 8.65946293820663 Test RE 1.4065447018925608\n",
      "2 Train Loss 55.90132 Test MSE 8.677346860859418 Test RE 1.4079963832873603\n",
      "3 Train Loss 50.36366 Test MSE 8.795089211970783 Test RE 1.4175167023010218\n",
      "4 Train Loss 48.27904 Test MSE 7.690591554741738 Test RE 1.325525032032936\n",
      "5 Train Loss 43.337814 Test MSE 8.369504578451338 Test RE 1.382795431099368\n",
      "6 Train Loss 42.788986 Test MSE 8.483459212012624 Test RE 1.3921773006402813\n",
      "7 Train Loss 42.46067 Test MSE 8.284948738749193 Test RE 1.3757926124800708\n",
      "8 Train Loss 42.440796 Test MSE 8.283375614129197 Test RE 1.3756619903092944\n",
      "9 Train Loss 42.41993 Test MSE 8.275777935769474 Test RE 1.3750309531826703\n",
      "10 Train Loss 42.328743 Test MSE 8.248116675902093 Test RE 1.3727310531304886\n",
      "11 Train Loss 42.317467 Test MSE 8.294046292899955 Test RE 1.3765477718707668\n",
      "12 Train Loss 42.24613 Test MSE 8.334568701017197 Test RE 1.3799063897221144\n",
      "13 Train Loss 42.065784 Test MSE 8.303245891116777 Test RE 1.3773109805837735\n",
      "14 Train Loss 42.036804 Test MSE 8.337152923722735 Test RE 1.3801203005540095\n",
      "15 Train Loss 42.035404 Test MSE 8.332076286931985 Test RE 1.379700046993991\n",
      "16 Train Loss 42.024406 Test MSE 8.338626403007964 Test RE 1.3802422539863524\n",
      "17 Train Loss 42.001106 Test MSE 8.328528410962097 Test RE 1.3794062711289212\n",
      "18 Train Loss 41.98541 Test MSE 8.334207520613909 Test RE 1.379876490121796\n",
      "19 Train Loss 41.978928 Test MSE 8.310506732098633 Test RE 1.3779130493926703\n",
      "20 Train Loss 41.96771 Test MSE 8.305482640426412 Test RE 1.377496479844043\n",
      "21 Train Loss 41.958874 Test MSE 8.323582927451751 Test RE 1.3789966643152354\n",
      "22 Train Loss 41.95244 Test MSE 8.334792427743116 Test RE 1.3799249101685813\n",
      "23 Train Loss 41.946083 Test MSE 8.32856200062165 Test RE 1.3794090527572014\n",
      "24 Train Loss 41.93447 Test MSE 8.339674075071489 Test RE 1.3803289586642515\n",
      "25 Train Loss 41.92076 Test MSE 8.326310277852281 Test RE 1.3792225705806014\n",
      "26 Train Loss 41.91332 Test MSE 8.337320751937852 Test RE 1.3801341915047436\n",
      "27 Train Loss 41.901634 Test MSE 8.352146818058216 Test RE 1.3813607769215261\n",
      "28 Train Loss 41.891014 Test MSE 8.316931463103979 Test RE 1.3784455686878412\n",
      "29 Train Loss 41.872498 Test MSE 8.330000552202435 Test RE 1.3795281768865435\n",
      "30 Train Loss 41.782677 Test MSE 8.408702747875827 Test RE 1.386029776270167\n",
      "31 Train Loss 41.496002 Test MSE 8.434222097197026 Test RE 1.3881313954751504\n",
      "32 Train Loss 41.25403 Test MSE 8.26758054528309 Test RE 1.3743497810711054\n",
      "33 Train Loss 41.050613 Test MSE 8.386493928072543 Test RE 1.3841981954343168\n",
      "34 Train Loss 40.65479 Test MSE 8.056680515697677 Test RE 1.3567072049865763\n",
      "35 Train Loss 40.554726 Test MSE 8.121268092449183 Test RE 1.36213447204834\n",
      "36 Train Loss 40.116993 Test MSE 7.881788395169403 Test RE 1.3419009050474393\n",
      "37 Train Loss 39.77961 Test MSE 7.777822586743254 Test RE 1.3330212620449406\n",
      "38 Train Loss 39.61557 Test MSE 8.020899961480074 Test RE 1.3536912139362387\n",
      "39 Train Loss 38.919212 Test MSE 7.839075498157013 Test RE 1.3382599586278223\n",
      "40 Train Loss 38.81933 Test MSE 7.974191348479175 Test RE 1.3497439412015129\n",
      "41 Train Loss 38.611225 Test MSE 7.954017954721155 Test RE 1.3480355448390278\n",
      "42 Train Loss 38.192017 Test MSE 7.764102694103869 Test RE 1.3318450343271138\n",
      "43 Train Loss 37.70409 Test MSE 7.37201935079658 Test RE 1.2977806471112545\n",
      "44 Train Loss 36.598328 Test MSE 6.75491244768157 Test RE 1.2422755064784297\n",
      "45 Train Loss 34.712032 Test MSE 6.085356628087488 Test RE 1.1791011900612678\n",
      "46 Train Loss 31.078884 Test MSE 5.005177522745865 Test RE 1.069344901378727\n",
      "47 Train Loss 26.564253 Test MSE 4.879181997108004 Test RE 1.0557997850331293\n",
      "48 Train Loss 24.757034 Test MSE 4.881662331380902 Test RE 1.0560681090748754\n",
      "49 Train Loss 23.885036 Test MSE 4.542765344482572 Test RE 1.0187513852582544\n",
      "50 Train Loss 23.125656 Test MSE 4.744524390133865 Test RE 1.041128660560973\n",
      "51 Train Loss 21.926434 Test MSE 4.613686753608544 Test RE 1.026672933318317\n",
      "52 Train Loss 21.66284 Test MSE 4.518469443644302 Test RE 1.016023458114008\n",
      "53 Train Loss 21.397194 Test MSE 4.38518648296449 Test RE 1.0009262835979067\n",
      "54 Train Loss 21.22612 Test MSE 4.3911904605127985 Test RE 1.001611258270552\n",
      "55 Train Loss 21.00418 Test MSE 4.353612039141497 Test RE 0.9973163133953626\n",
      "56 Train Loss 20.930765 Test MSE 4.393811220281537 Test RE 1.001910105584065\n",
      "57 Train Loss 20.790928 Test MSE 4.388674399299474 Test RE 1.0013242658982842\n",
      "58 Train Loss 20.654991 Test MSE 4.297635593008291 Test RE 0.9908840875349549\n",
      "59 Train Loss 20.595745 Test MSE 4.167200011216466 Test RE 0.9757312917906141\n",
      "60 Train Loss 20.51265 Test MSE 4.128659781955436 Test RE 0.9712087995943945\n",
      "61 Train Loss 20.392637 Test MSE 4.100858187135165 Test RE 0.967933310236524\n",
      "62 Train Loss 20.32093 Test MSE 4.052006095102944 Test RE 0.9621507113030652\n",
      "63 Train Loss 20.302258 Test MSE 4.0580841609829355 Test RE 0.9628720606655252\n",
      "64 Train Loss 20.282223 Test MSE 4.092702413892842 Test RE 0.9669703199121394\n",
      "65 Train Loss 20.194378 Test MSE 4.149106063196339 Test RE 0.9736106787615522\n",
      "66 Train Loss 20.120554 Test MSE 4.152556440579533 Test RE 0.9740154196991779\n",
      "67 Train Loss 20.062042 Test MSE 4.127125504888682 Test RE 0.9710283243397912\n",
      "68 Train Loss 19.95348 Test MSE 4.095991878064245 Test RE 0.9673588376958961\n",
      "69 Train Loss 19.809769 Test MSE 4.121275916415018 Test RE 0.9703399359935775\n",
      "70 Train Loss 19.750866 Test MSE 4.13871399911164 Test RE 0.9723906366892122\n",
      "71 Train Loss 19.660301 Test MSE 4.134326933885091 Test RE 0.9718751296704824\n",
      "72 Train Loss 19.55231 Test MSE 4.100052763070907 Test RE 0.9678382526866423\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "73 Train Loss 19.446012 Test MSE 4.0972356433442805 Test RE 0.9675056978519494\n",
      "74 Train Loss 19.35944 Test MSE 4.171323550573231 Test RE 0.9762139265977665\n",
      "75 Train Loss 19.271236 Test MSE 4.141928653995595 Test RE 0.9727682048845616\n",
      "76 Train Loss 19.213379 Test MSE 4.118324876463606 Test RE 0.9699924677866744\n",
      "77 Train Loss 19.088295 Test MSE 4.122517704671589 Test RE 0.9704861123244812\n",
      "78 Train Loss 18.907837 Test MSE 4.114065636489668 Test RE 0.9694907468724112\n",
      "79 Train Loss 18.854847 Test MSE 4.141124729260045 Test RE 0.9726737959222295\n",
      "80 Train Loss 18.831833 Test MSE 4.18435342347908 Test RE 0.9777374269291946\n",
      "81 Train Loss 18.800333 Test MSE 4.189016455857863 Test RE 0.9782820692346668\n",
      "82 Train Loss 18.791372 Test MSE 4.168607979723683 Test RE 0.9758961126421648\n",
      "83 Train Loss 18.786137 Test MSE 4.141321257702466 Test RE 0.9726968761010052\n",
      "84 Train Loss 18.72872 Test MSE 4.1230604839129885 Test RE 0.9705499983300906\n",
      "85 Train Loss 18.564032 Test MSE 4.302666899371631 Test RE 0.9914639392962127\n",
      "86 Train Loss 18.508575 Test MSE 4.360759166415846 Test RE 0.9981346021579748\n",
      "87 Train Loss 18.466866 Test MSE 4.309265290981096 Test RE 0.9922238820539981\n",
      "88 Train Loss 18.434189 Test MSE 4.341688574317765 Test RE 0.9959496758783694\n",
      "89 Train Loss 18.417381 Test MSE 4.396908916188176 Test RE 1.0022632233954456\n",
      "90 Train Loss 18.396864 Test MSE 4.400512675418108 Test RE 1.0026738727427296\n",
      "91 Train Loss 18.377047 Test MSE 4.385878362850592 Test RE 1.0010052418647946\n",
      "92 Train Loss 18.34511 Test MSE 4.443340156758589 Test RE 1.0075412624183848\n",
      "93 Train Loss 18.322392 Test MSE 4.453522520769824 Test RE 1.0086950432208803\n",
      "94 Train Loss 18.303871 Test MSE 4.4519421439187425 Test RE 1.0085160545960377\n",
      "95 Train Loss 18.261803 Test MSE 4.449677743974411 Test RE 1.0082595402381982\n",
      "96 Train Loss 18.240572 Test MSE 4.447336299200186 Test RE 1.0079942294883515\n",
      "97 Train Loss 18.206633 Test MSE 4.457142089391509 Test RE 1.0091048648140366\n",
      "98 Train Loss 18.179909 Test MSE 4.458757275841064 Test RE 1.009287688789256\n",
      "99 Train Loss 18.153305 Test MSE 4.464063996162907 Test RE 1.0098881268096358\n",
      "Training time: 86.22\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 58.68093 Test MSE 8.71762297622562 Test RE 1.4112602250085835\n",
      "1 Train Loss 58.26307 Test MSE 8.360608683165513 Test RE 1.3820603533120137\n",
      "2 Train Loss 58.129936 Test MSE 8.703815451596517 Test RE 1.4101421603549797\n",
      "3 Train Loss 58.02867 Test MSE 8.992896709616161 Test RE 1.4333685224520478\n",
      "4 Train Loss 54.649178 Test MSE 7.8673182349686615 Test RE 1.3406685425154041\n",
      "5 Train Loss 52.438618 Test MSE 8.54213630713 Test RE 1.3969836037584278\n",
      "6 Train Loss 49.47934 Test MSE 8.436447416164414 Test RE 1.38831450847354\n",
      "7 Train Loss 46.391735 Test MSE 7.879188507648199 Test RE 1.341679567008816\n",
      "8 Train Loss 44.08361 Test MSE 8.230896060299889 Test RE 1.371297293990167\n",
      "9 Train Loss 43.362442 Test MSE 8.093006297523194 Test RE 1.3597623107889611\n",
      "10 Train Loss 43.03699 Test MSE 8.273297900147986 Test RE 1.3748249072199334\n",
      "11 Train Loss 43.015553 Test MSE 8.283638981049043 Test RE 1.3756838594721954\n",
      "12 Train Loss 42.992226 Test MSE 8.320066528890091 Test RE 1.3787053466111958\n",
      "13 Train Loss 42.902344 Test MSE 8.309461823816914 Test RE 1.3778264218220795\n",
      "14 Train Loss 42.868984 Test MSE 8.292336952458587 Test RE 1.3764059165001403\n",
      "15 Train Loss 42.810104 Test MSE 8.343921064598824 Test RE 1.3806803810597992\n",
      "16 Train Loss 42.744835 Test MSE 8.299813578180697 Test RE 1.3770262816241063\n",
      "17 Train Loss 42.697662 Test MSE 8.303532652124536 Test RE 1.3773347637933735\n",
      "18 Train Loss 42.650047 Test MSE 8.36273764631586 Test RE 1.3822363075031008\n",
      "19 Train Loss 42.58336 Test MSE 8.368932306289036 Test RE 1.3827481553704715\n",
      "20 Train Loss 42.271843 Test MSE 8.361302152761628 Test RE 1.3821176695313357\n",
      "21 Train Loss 41.98078 Test MSE 8.317541945324999 Test RE 1.3784961583238327\n",
      "22 Train Loss 41.615097 Test MSE 8.220336143277295 Test RE 1.3704173513275697\n",
      "23 Train Loss 41.030083 Test MSE 8.44564535396927 Test RE 1.3890711155622948\n",
      "24 Train Loss 40.577576 Test MSE 8.07582828208317 Test RE 1.3583184452419463\n",
      "25 Train Loss 40.338745 Test MSE 8.067595610001305 Test RE 1.3576259192651527\n",
      "26 Train Loss 39.885338 Test MSE 8.229189016006954 Test RE 1.3711550867149316\n",
      "27 Train Loss 39.578342 Test MSE 8.223753534653746 Test RE 1.370702179449129\n",
      "28 Train Loss 38.753952 Test MSE 8.194097301078715 Test RE 1.368228456373184\n",
      "29 Train Loss 38.35801 Test MSE 8.150661348491846 Test RE 1.3645972281921697\n",
      "30 Train Loss 38.02101 Test MSE 8.080031900215818 Test RE 1.3586719142001324\n",
      "31 Train Loss 37.674896 Test MSE 8.114347922183738 Test RE 1.361554007812497\n",
      "32 Train Loss 36.97995 Test MSE 7.992803609496503 Test RE 1.3513182164835165\n",
      "33 Train Loss 36.407578 Test MSE 8.010825058916396 Test RE 1.352840773677369\n",
      "34 Train Loss 35.397804 Test MSE 7.915467592274708 Test RE 1.3447648469975748\n",
      "35 Train Loss 35.063805 Test MSE 7.643284840796565 Test RE 1.321441928597223\n",
      "36 Train Loss 34.815323 Test MSE 7.527262975662613 Test RE 1.3113741084497936\n",
      "37 Train Loss 34.488735 Test MSE 7.758504638130629 Test RE 1.331364805783059\n",
      "38 Train Loss 33.558243 Test MSE 7.86499951849075 Test RE 1.3404709618893438\n",
      "39 Train Loss 33.38343 Test MSE 7.857465084606844 Test RE 1.3398287425369322\n",
      "40 Train Loss 32.39781 Test MSE 7.716465092272249 Test RE 1.3277528986557445\n",
      "41 Train Loss 31.446178 Test MSE 7.703135571033359 Test RE 1.3266056142494915\n",
      "42 Train Loss 30.634045 Test MSE 7.542209409009031 Test RE 1.312675421131354\n",
      "43 Train Loss 30.064743 Test MSE 7.431682924009699 Test RE 1.3030216938488721\n",
      "44 Train Loss 29.613602 Test MSE 7.452478100686347 Test RE 1.304843464087811\n",
      "45 Train Loss 29.402384 Test MSE 7.345339727652006 Test RE 1.295430159380292\n",
      "46 Train Loss 29.200678 Test MSE 7.2603901094160515 Test RE 1.2879174826989224\n",
      "47 Train Loss 29.131691 Test MSE 7.104580543193828 Test RE 1.2740230379441613\n",
      "48 Train Loss 28.97618 Test MSE 7.038320201393747 Test RE 1.268068080268347\n",
      "49 Train Loss 28.764196 Test MSE 6.7861110712734 Test RE 1.245141023805288\n",
      "50 Train Loss 28.45383 Test MSE 6.583430668334598 Test RE 1.226405793413487\n",
      "51 Train Loss 28.07349 Test MSE 6.338100225442293 Test RE 1.2033379448406023\n",
      "52 Train Loss 27.58461 Test MSE 6.180476897914673 Test RE 1.188280728986647\n",
      "53 Train Loss 27.392994 Test MSE 6.140186195067803 Test RE 1.1844011781649697\n",
      "54 Train Loss 26.717484 Test MSE 6.043433536591138 Test RE 1.175032653053865\n",
      "55 Train Loss 26.386631 Test MSE 6.0246635803462665 Test RE 1.1732065004966918\n",
      "56 Train Loss 26.251572 Test MSE 6.038865878021832 Test RE 1.1745885212166027\n",
      "57 Train Loss 26.193398 Test MSE 6.116830963322933 Test RE 1.1821464974847005\n",
      "58 Train Loss 26.104628 Test MSE 6.045956690200695 Test RE 1.1752779174779988\n",
      "59 Train Loss 26.022755 Test MSE 5.974717512704235 Test RE 1.1683332820357262\n",
      "60 Train Loss 25.816051 Test MSE 5.92806878646592 Test RE 1.1637633539304995\n",
      "61 Train Loss 25.387436 Test MSE 5.848683253156105 Test RE 1.1559448415099038\n",
      "62 Train Loss 25.238012 Test MSE 5.79951734468548 Test RE 1.1510759659220855\n",
      "63 Train Loss 25.18876 Test MSE 5.777035882026475 Test RE 1.1488427629661206\n",
      "64 Train Loss 25.17252 Test MSE 5.766384504363099 Test RE 1.1477831881284775\n",
      "65 Train Loss 25.125244 Test MSE 5.765407995365441 Test RE 1.147685998280566\n",
      "66 Train Loss 25.065346 Test MSE 5.703752493169954 Test RE 1.1415328036008867\n",
      "67 Train Loss 25.021183 Test MSE 5.686422924067409 Test RE 1.1397973390543212\n",
      "68 Train Loss 24.994236 Test MSE 5.713742944949855 Test RE 1.1425320965166699\n",
      "69 Train Loss 24.964012 Test MSE 5.668311879508775 Test RE 1.1379807855169526\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "70 Train Loss 24.926218 Test MSE 5.665083493621696 Test RE 1.137656670997169\n",
      "71 Train Loss 24.880505 Test MSE 5.6669877547517515 Test RE 1.1378478608850078\n",
      "72 Train Loss 24.822086 Test MSE 5.540901949182558 Test RE 1.125118570132194\n",
      "73 Train Loss 24.808662 Test MSE 5.551838974903133 Test RE 1.1262284421797\n",
      "74 Train Loss 24.785316 Test MSE 5.538962360011676 Test RE 1.1249216293792805\n",
      "75 Train Loss 24.759487 Test MSE 5.465871173319279 Test RE 1.117474845815522\n",
      "76 Train Loss 24.750954 Test MSE 5.444704431666564 Test RE 1.1153090204071097\n",
      "77 Train Loss 24.733257 Test MSE 5.447547520147553 Test RE 1.1156001756417442\n",
      "78 Train Loss 24.706837 Test MSE 5.428424497908016 Test RE 1.1136403577434182\n",
      "79 Train Loss 24.683025 Test MSE 5.468387998792447 Test RE 1.117732093505953\n",
      "80 Train Loss 24.617474 Test MSE 5.41215364357597 Test RE 1.111970123745462\n",
      "81 Train Loss 24.527958 Test MSE 5.441173341467837 Test RE 1.1149473023806293\n",
      "82 Train Loss 24.458084 Test MSE 5.427022958177745 Test RE 1.113496585644822\n",
      "83 Train Loss 24.40932 Test MSE 5.435825960507054 Test RE 1.114399303526131\n",
      "84 Train Loss 24.362984 Test MSE 5.481451103181122 Test RE 1.1190663387647946\n",
      "85 Train Loss 24.332329 Test MSE 5.460480029387047 Test RE 1.1169236112728187\n",
      "86 Train Loss 24.28393 Test MSE 5.426603470658137 Test RE 1.1134535503609322\n",
      "87 Train Loss 24.148113 Test MSE 5.442757200253722 Test RE 1.115109564311946\n",
      "88 Train Loss 24.048903 Test MSE 5.456607789277874 Test RE 1.1165275139140503\n",
      "89 Train Loss 23.96302 Test MSE 5.266153159401322 Test RE 1.0968691027879127\n",
      "90 Train Loss 23.919643 Test MSE 5.213442158228803 Test RE 1.0913657995326573\n",
      "91 Train Loss 23.883102 Test MSE 5.255066036381487 Test RE 1.0957138448673145\n",
      "92 Train Loss 23.828396 Test MSE 5.288839726184436 Test RE 1.0992292175358878\n",
      "93 Train Loss 23.788837 Test MSE 5.272579089610078 Test RE 1.0975381163190019\n",
      "94 Train Loss 23.769978 Test MSE 5.288183145779259 Test RE 1.0991609837829694\n",
      "95 Train Loss 23.755234 Test MSE 5.326004690200796 Test RE 1.1030846280559528\n",
      "96 Train Loss 23.736862 Test MSE 5.350158806583238 Test RE 1.1055831138637109\n",
      "97 Train Loss 23.709713 Test MSE 5.373922164361487 Test RE 1.1080356820673323\n",
      "98 Train Loss 23.677315 Test MSE 5.372085877452244 Test RE 1.1078463561710035\n",
      "99 Train Loss 23.669752 Test MSE 5.388154168620425 Test RE 1.1095019427470696\n",
      "Training time: 92.09\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 56.829556 Test MSE 8.968732837385517 Test RE 1.4314414997817824\n",
      "1 Train Loss 56.48062 Test MSE 8.811249046685774 Test RE 1.4188183561350123\n",
      "2 Train Loss 56.256123 Test MSE 8.166355155205993 Test RE 1.3659103379716289\n",
      "3 Train Loss 52.307945 Test MSE 8.19535171037245 Test RE 1.3683331813201243\n",
      "4 Train Loss 52.06574 Test MSE 8.300836565291887 Test RE 1.3771111411646753\n",
      "5 Train Loss 51.96576 Test MSE 8.321514740359591 Test RE 1.3788253318307182\n",
      "6 Train Loss 47.608852 Test MSE 8.391341201584055 Test RE 1.3845981610262357\n",
      "7 Train Loss 45.94493 Test MSE 8.316680870808407 Test RE 1.3784248019878318\n",
      "8 Train Loss 45.078766 Test MSE 7.981233687182026 Test RE 1.3503398170630259\n",
      "9 Train Loss 43.52047 Test MSE 8.09258223723739 Test RE 1.3597266856623615\n",
      "10 Train Loss 42.717415 Test MSE 8.318278250143333 Test RE 1.3785571721971739\n",
      "11 Train Loss 41.897514 Test MSE 8.216411043878871 Test RE 1.3700901343814678\n",
      "12 Train Loss 41.796375 Test MSE 8.33590742390627 Test RE 1.3800172075796873\n",
      "13 Train Loss 41.702312 Test MSE 8.319018753742636 Test RE 1.3786185312789065\n",
      "14 Train Loss 41.245804 Test MSE 8.074286566232194 Test RE 1.3581887841744016\n",
      "15 Train Loss 39.13819 Test MSE 8.254665389716981 Test RE 1.37327589503271\n",
      "16 Train Loss 37.671288 Test MSE 8.504476197460972 Test RE 1.3939007291324514\n",
      "17 Train Loss 36.92776 Test MSE 8.723781149708019 Test RE 1.4117585977333598\n",
      "18 Train Loss 36.69525 Test MSE 8.687175635959749 Test RE 1.4087935718422546\n",
      "19 Train Loss 36.359325 Test MSE 8.767017021623914 Test RE 1.4152526773892748\n",
      "20 Train Loss 36.103397 Test MSE 8.688558311876184 Test RE 1.4089056812045073\n",
      "21 Train Loss 35.876854 Test MSE 8.63747910305756 Test RE 1.404758165185809\n",
      "22 Train Loss 35.732742 Test MSE 8.746933059659726 Test RE 1.4136306788069586\n",
      "23 Train Loss 35.56214 Test MSE 8.75543247179574 Test RE 1.4143173259051787\n",
      "24 Train Loss 35.17189 Test MSE 8.73161809750891 Test RE 1.4123925771153552\n",
      "25 Train Loss 34.99287 Test MSE 8.824809919248386 Test RE 1.4199097461048151\n",
      "26 Train Loss 34.879734 Test MSE 8.880301971763659 Test RE 1.4243670787533496\n",
      "27 Train Loss 34.749565 Test MSE 8.943467297588084 Test RE 1.4294238430288027\n",
      "28 Train Loss 34.651695 Test MSE 8.868820489596 Test RE 1.4234459876281356\n",
      "29 Train Loss 34.467762 Test MSE 8.967759717856683 Test RE 1.431363841014946\n",
      "30 Train Loss 34.28322 Test MSE 9.047651719613384 Test RE 1.4377255725865732\n",
      "31 Train Loss 34.136303 Test MSE 9.098319537679409 Test RE 1.441745660761613\n",
      "32 Train Loss 33.872147 Test MSE 9.01715192407025 Test RE 1.4353002275988789\n",
      "33 Train Loss 33.526386 Test MSE 8.9966232837871 Test RE 1.4336654790961358\n",
      "34 Train Loss 33.19943 Test MSE 8.81689021642909 Test RE 1.4192724639745897\n",
      "35 Train Loss 32.737144 Test MSE 8.523735485915669 Test RE 1.3954781546046773\n",
      "36 Train Loss 32.175488 Test MSE 8.281767680871319 Test RE 1.3755284650338544\n",
      "37 Train Loss 31.874882 Test MSE 8.096400128478393 Test RE 1.3600473914836555\n",
      "38 Train Loss 31.426016 Test MSE 8.060528244024658 Test RE 1.3570311360155503\n",
      "39 Train Loss 30.553345 Test MSE 7.820876507122251 Test RE 1.336705621455182\n",
      "40 Train Loss 30.173117 Test MSE 7.440970319968154 Test RE 1.3038356346494837\n",
      "41 Train Loss 29.442085 Test MSE 7.121972413786375 Test RE 1.2755814761854323\n",
      "42 Train Loss 28.968987 Test MSE 6.996758264153274 Test RE 1.2643185065345326\n",
      "43 Train Loss 28.52035 Test MSE 7.092219421384201 Test RE 1.2729142314569943\n",
      "44 Train Loss 28.266508 Test MSE 6.895853657487264 Test RE 1.2551686356004588\n",
      "45 Train Loss 28.108406 Test MSE 6.839683284265098 Test RE 1.2500461769754891\n",
      "46 Train Loss 27.848621 Test MSE 6.424158746735908 Test RE 1.2114798424018662\n",
      "47 Train Loss 27.661982 Test MSE 6.267801434659268 Test RE 1.1966459492202315\n",
      "48 Train Loss 27.425991 Test MSE 6.408150597384527 Test RE 1.2099694775718721\n",
      "49 Train Loss 26.985565 Test MSE 6.511890204468331 Test RE 1.2197240720311877\n",
      "50 Train Loss 26.568073 Test MSE 6.327524395878594 Test RE 1.2023335736721281\n",
      "51 Train Loss 26.008064 Test MSE 6.5750859508816175 Test RE 1.2256282918805932\n",
      "52 Train Loss 25.707485 Test MSE 6.387207044372972 Test RE 1.207990607166261\n",
      "53 Train Loss 25.116772 Test MSE 6.233880632320585 Test RE 1.1934034837993026\n",
      "54 Train Loss 24.65318 Test MSE 6.465838155797726 Test RE 1.2154034789960344\n",
      "55 Train Loss 24.266794 Test MSE 6.485991592310962 Test RE 1.2172961570307426\n",
      "56 Train Loss 23.923576 Test MSE 6.686206160999992 Test RE 1.2359415765958368\n",
      "57 Train Loss 23.635828 Test MSE 6.635062157263091 Test RE 1.2312055318403556\n",
      "58 Train Loss 22.89331 Test MSE 6.489681680245593 Test RE 1.2176423871390512\n",
      "59 Train Loss 22.819977 Test MSE 6.440735406028339 Test RE 1.213041864074178\n",
      "60 Train Loss 22.684904 Test MSE 6.345570200418435 Test RE 1.2040468526390047\n",
      "61 Train Loss 22.497143 Test MSE 6.319463901832088 Test RE 1.2015675164916642\n",
      "62 Train Loss 22.416313 Test MSE 6.154169561981479 Test RE 1.1857490605510859\n",
      "63 Train Loss 22.333244 Test MSE 6.220389869486534 Test RE 1.1921114600801774\n",
      "64 Train Loss 22.233635 Test MSE 6.219820272072126 Test RE 1.1920568783606424\n",
      "65 Train Loss 22.131727 Test MSE 6.1282598227738845 Test RE 1.183250360028064\n",
      "66 Train Loss 22.05057 Test MSE 6.182507327188262 Test RE 1.1884759017776163\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "67 Train Loss 21.957636 Test MSE 6.2092065894922 Test RE 1.1910393637971735\n",
      "68 Train Loss 21.68313 Test MSE 6.122817092730872 Test RE 1.1827247995077093\n",
      "69 Train Loss 21.484913 Test MSE 6.049595540053826 Test RE 1.1756315436117177\n",
      "70 Train Loss 21.331118 Test MSE 5.987695746382127 Test RE 1.169601515811033\n",
      "71 Train Loss 21.246588 Test MSE 5.918415212450361 Test RE 1.1628154016118921\n",
      "72 Train Loss 21.212013 Test MSE 5.808994597565767 Test RE 1.1520160945798135\n",
      "73 Train Loss 21.13326 Test MSE 5.708347130359746 Test RE 1.1419924898553202\n",
      "74 Train Loss 21.083994 Test MSE 5.70100306203478 Test RE 1.141257638772577\n",
      "75 Train Loss 20.950535 Test MSE 5.484214572494638 Test RE 1.1193483914325935\n",
      "76 Train Loss 17.972174 Test MSE 2.662476343989002 Test RE 0.779921583537022\n",
      "77 Train Loss 11.920675 Test MSE 2.6756016356804477 Test RE 0.7818416218620704\n",
      "78 Train Loss 10.022851 Test MSE 2.5397243044395945 Test RE 0.7617305044560653\n",
      "79 Train Loss 8.304441 Test MSE 2.515525089864765 Test RE 0.7580928262010564\n",
      "80 Train Loss 7.351017 Test MSE 2.5438677557719247 Test RE 0.7623516165623201\n",
      "81 Train Loss 7.227888 Test MSE 2.5077106593629055 Test RE 0.7569144098863537\n",
      "82 Train Loss 7.098939 Test MSE 2.478328325884242 Test RE 0.7524670385135894\n",
      "83 Train Loss 7.03868 Test MSE 2.5043182436670977 Test RE 0.7564022620199087\n",
      "84 Train Loss 6.920883 Test MSE 2.511576266643659 Test RE 0.7574975726881888\n",
      "85 Train Loss 6.8369107 Test MSE 2.502285941651455 Test RE 0.756095282296646\n",
      "86 Train Loss 6.767159 Test MSE 2.4437316677686893 Test RE 0.7471964823813011\n",
      "87 Train Loss 6.632098 Test MSE 2.4222455331735935 Test RE 0.7439044251401061\n",
      "88 Train Loss 6.5566225 Test MSE 2.41327040083758 Test RE 0.742524953739774\n",
      "89 Train Loss 6.504695 Test MSE 2.402773079032034 Test RE 0.7409082641868138\n",
      "90 Train Loss 6.395729 Test MSE 2.385670973107011 Test RE 0.7382667914162199\n",
      "91 Train Loss 6.292333 Test MSE 2.3396721187204226 Test RE 0.7311147742344527\n",
      "92 Train Loss 6.191942 Test MSE 2.3705684421074036 Test RE 0.7359262760765077\n",
      "93 Train Loss 6.105979 Test MSE 2.322572681557724 Test RE 0.7284382075271509\n",
      "94 Train Loss 5.950898 Test MSE 2.3267933135334986 Test RE 0.7290997743607938\n",
      "95 Train Loss 5.5795574 Test MSE 2.293296508598299 Test RE 0.723832643713708\n",
      "96 Train Loss 5.5021267 Test MSE 2.250308504889454 Test RE 0.7170164030678058\n",
      "97 Train Loss 5.4015265 Test MSE 2.2120273831974697 Test RE 0.7108914807651531\n",
      "98 Train Loss 5.271937 Test MSE 2.2071853796086858 Test RE 0.7101130037774929\n",
      "99 Train Loss 5.1744566 Test MSE 2.1730214052434524 Test RE 0.7045958200634905\n",
      "Training time: 90.28\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 56.4609 Test MSE 8.747294173571328 Test RE 1.4136598591168945\n",
      "1 Train Loss 56.217014 Test MSE 8.59937227793766 Test RE 1.401655984238862\n",
      "2 Train Loss 55.519436 Test MSE 9.356212930709214 Test RE 1.4620361424561048\n",
      "3 Train Loss 53.44799 Test MSE 8.39046178002396 Test RE 1.3845256054388788\n",
      "4 Train Loss 46.162704 Test MSE 9.613407069599035 Test RE 1.4819949593646127\n",
      "5 Train Loss 44.286125 Test MSE 8.509232925252 Test RE 1.3942904932584794\n",
      "6 Train Loss 43.84591 Test MSE 8.61214913811125 Test RE 1.4026968808715636\n",
      "7 Train Loss 43.355515 Test MSE 8.49159205472185 Test RE 1.3928444604876629\n",
      "8 Train Loss 43.253914 Test MSE 8.390694715240741 Test RE 1.3845448238408087\n",
      "9 Train Loss 42.96203 Test MSE 8.515637826184223 Test RE 1.3948151358868166\n",
      "10 Train Loss 42.799683 Test MSE 8.499173451520788 Test RE 1.3934660960160214\n",
      "11 Train Loss 42.754135 Test MSE 8.47513295753144 Test RE 1.391493943309256\n",
      "12 Train Loss 42.69644 Test MSE 8.473247648049291 Test RE 1.3913391644609505\n",
      "13 Train Loss 42.60363 Test MSE 8.432588063915528 Test RE 1.3879969217355599\n",
      "14 Train Loss 42.471855 Test MSE 8.480272530620097 Test RE 1.3919158010511845\n",
      "15 Train Loss 42.30905 Test MSE 8.410587199549658 Test RE 1.3861850772654432\n",
      "16 Train Loss 42.1791 Test MSE 8.395344336478662 Test RE 1.384928386683264\n",
      "17 Train Loss 41.941467 Test MSE 8.304767141269675 Test RE 1.3774371444122513\n",
      "18 Train Loss 41.375786 Test MSE 8.266640204996188 Test RE 1.3742716207671297\n",
      "19 Train Loss 39.04586 Test MSE 7.697418920330552 Test RE 1.326113272632184\n",
      "20 Train Loss 38.467106 Test MSE 7.820936781334818 Test RE 1.3367107723307565\n",
      "21 Train Loss 38.31844 Test MSE 7.724279894007082 Test RE 1.3284250652354004\n",
      "22 Train Loss 37.738983 Test MSE 7.681407934926025 Test RE 1.324733366390069\n",
      "23 Train Loss 34.379807 Test MSE 6.980410265100713 Test RE 1.2628405960172036\n",
      "24 Train Loss 33.31671 Test MSE 7.276833289025737 Test RE 1.2893750821642458\n",
      "25 Train Loss 33.006516 Test MSE 7.1568258354993946 Test RE 1.2786988793960532\n",
      "26 Train Loss 32.951233 Test MSE 7.188032099783759 Test RE 1.2814836342895248\n",
      "27 Train Loss 32.8816 Test MSE 7.23820356500645 Test RE 1.2859481462655316\n",
      "28 Train Loss 32.827015 Test MSE 7.220597183251794 Test RE 1.2843832083212225\n",
      "29 Train Loss 32.8024 Test MSE 7.220212877897165 Test RE 1.2843490281904304\n",
      "30 Train Loss 32.758095 Test MSE 7.21839386465984 Test RE 1.2841872327498782\n",
      "31 Train Loss 32.693768 Test MSE 7.20066324993671 Test RE 1.282609081925918\n",
      "32 Train Loss 32.659027 Test MSE 7.222179307624861 Test RE 1.2845239129351673\n",
      "33 Train Loss 32.60738 Test MSE 7.18985027353981 Test RE 1.2816456962114304\n",
      "34 Train Loss 32.574844 Test MSE 7.172391522268448 Test RE 1.280088672582174\n",
      "35 Train Loss 32.47451 Test MSE 7.198593149819378 Test RE 1.2824247016837569\n",
      "36 Train Loss 32.36397 Test MSE 7.168856231972793 Test RE 1.2797731541918997\n",
      "37 Train Loss 32.280323 Test MSE 7.108915738407642 Test RE 1.274411681316479\n",
      "38 Train Loss 32.089046 Test MSE 7.127882306566974 Test RE 1.2761106123624961\n",
      "39 Train Loss 32.031548 Test MSE 7.112311103311748 Test RE 1.2747159876482377\n",
      "40 Train Loss 31.934868 Test MSE 7.154738182394785 Test RE 1.278512366935945\n",
      "41 Train Loss 31.868244 Test MSE 7.175553674910083 Test RE 1.280370823222754\n",
      "42 Train Loss 31.742023 Test MSE 7.145119232550498 Test RE 1.2776526511649875\n",
      "43 Train Loss 31.64497 Test MSE 7.157322524834373 Test RE 1.2787432499795797\n",
      "44 Train Loss 31.59152 Test MSE 7.1629042381106665 Test RE 1.279241773492675\n",
      "45 Train Loss 31.490002 Test MSE 7.173603007845749 Test RE 1.2801967776318353\n",
      "46 Train Loss 31.405006 Test MSE 7.168464705201342 Test RE 1.2797382063371343\n",
      "47 Train Loss 31.351255 Test MSE 7.2385794841707005 Test RE 1.2859815389624683\n",
      "48 Train Loss 31.309551 Test MSE 7.2457511544153475 Test RE 1.2866184285858058\n",
      "49 Train Loss 31.24088 Test MSE 7.287337903566354 Test RE 1.2903053978082342\n",
      "50 Train Loss 31.16801 Test MSE 7.285911788402749 Test RE 1.2901791367404982\n",
      "51 Train Loss 31.14436 Test MSE 7.321387125815598 Test RE 1.2933162843900698\n",
      "52 Train Loss 31.008976 Test MSE 7.314629777218644 Test RE 1.2927193069124692\n",
      "53 Train Loss 30.91667 Test MSE 7.309933043837021 Test RE 1.2923042118865653\n",
      "54 Train Loss 30.830254 Test MSE 7.291737650863457 Test RE 1.2906948514840448\n",
      "55 Train Loss 30.761036 Test MSE 7.268860189380272 Test RE 1.2886685156443962\n",
      "56 Train Loss 30.642708 Test MSE 7.27778928213704 Test RE 1.2894597751320422\n",
      "57 Train Loss 30.451525 Test MSE 7.281231286460836 Test RE 1.2897646617741791\n",
      "58 Train Loss 30.12654 Test MSE 7.258520589518636 Test RE 1.287751655357513\n",
      "59 Train Loss 29.912243 Test MSE 7.293895141883035 Test RE 1.2908857837921095\n",
      "60 Train Loss 29.56797 Test MSE 7.330761559251381 Test RE 1.2941440120264764\n",
      "61 Train Loss 29.15801 Test MSE 7.133883436904421 Test RE 1.2766476929829487\n",
      "62 Train Loss 28.54816 Test MSE 7.342577636418581 Test RE 1.2951865741375816\n",
      "63 Train Loss 28.191223 Test MSE 7.27079732121686 Test RE 1.2888402175653217\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64 Train Loss 28.014187 Test MSE 7.233466953742529 Test RE 1.2855273213174008\n",
      "65 Train Loss 27.87603 Test MSE 7.286271214764682 Test RE 1.290210959708432\n",
      "66 Train Loss 27.756975 Test MSE 7.215124307535823 Test RE 1.2838963647844617\n",
      "67 Train Loss 27.519203 Test MSE 6.96550097332444 Test RE 1.26149123956472\n",
      "68 Train Loss 26.926308 Test MSE 6.66578940990879 Test RE 1.2340531211903547\n",
      "69 Train Loss 26.59751 Test MSE 6.397318435328275 Test RE 1.2089463954279438\n",
      "70 Train Loss 26.04911 Test MSE 6.436191542475921 Test RE 1.2126138951502947\n",
      "71 Train Loss 25.551754 Test MSE 6.417653828121573 Test RE 1.2108663322123767\n",
      "72 Train Loss 23.807186 Test MSE 5.635052039453296 Test RE 1.1346372200295043\n",
      "73 Train Loss 21.835693 Test MSE 5.6711245139274356 Test RE 1.1382630853479487\n",
      "74 Train Loss 19.315434 Test MSE 5.273427562169313 Test RE 1.0976264216338931\n",
      "75 Train Loss 18.14174 Test MSE 4.747425412185457 Test RE 1.041446909072529\n",
      "76 Train Loss 17.598753 Test MSE 4.657598886703073 Test RE 1.0315471949148933\n",
      "77 Train Loss 16.944513 Test MSE 4.436719437946982 Test RE 1.0067903482551008\n",
      "78 Train Loss 16.401073 Test MSE 4.6500843855487055 Test RE 1.0307147175092133\n",
      "79 Train Loss 15.888898 Test MSE 4.368424524546868 Test RE 0.9990114792777239\n",
      "80 Train Loss 15.199379 Test MSE 4.46086864108352 Test RE 1.0095266256277422\n",
      "81 Train Loss 14.886495 Test MSE 4.389747581772636 Test RE 1.001446687598774\n",
      "82 Train Loss 14.32025 Test MSE 4.50555286514533 Test RE 1.0145702072788996\n",
      "83 Train Loss 14.208346 Test MSE 4.5937924449567085 Test RE 1.0244570249670402\n",
      "84 Train Loss 13.8330765 Test MSE 4.9946880380369105 Test RE 1.068223786294966\n",
      "85 Train Loss 13.663549 Test MSE 5.03395857546365 Test RE 1.072414997780631\n",
      "86 Train Loss 13.392181 Test MSE 5.1897580056629 Test RE 1.0888839941822837\n",
      "87 Train Loss 13.180996 Test MSE 5.275984425231233 Test RE 1.0978924858128896\n",
      "88 Train Loss 12.89303 Test MSE 5.265996807211483 Test RE 1.0968528196345404\n",
      "89 Train Loss 12.706539 Test MSE 5.2892744075198275 Test RE 1.0992743885593652\n",
      "90 Train Loss 12.540585 Test MSE 5.482364285554923 Test RE 1.1191595503139478\n",
      "91 Train Loss 12.290266 Test MSE 5.5300974771645715 Test RE 1.124021073366405\n",
      "92 Train Loss 12.17559 Test MSE 5.672018922589213 Test RE 1.1383528411118045\n",
      "93 Train Loss 12.100021 Test MSE 5.6938946534030395 Test RE 1.1405459169969694\n",
      "94 Train Loss 12.011338 Test MSE 5.72013349825719 Test RE 1.1431708522244104\n",
      "95 Train Loss 11.773146 Test MSE 5.7191271551284135 Test RE 1.143070288772928\n",
      "96 Train Loss 11.559564 Test MSE 5.614164537795739 Test RE 1.132532382358387\n",
      "97 Train Loss 11.462104 Test MSE 5.6200689101028765 Test RE 1.1331277635211754\n",
      "98 Train Loss 11.25138 Test MSE 5.679258616991808 Test RE 1.1390790991426458\n",
      "99 Train Loss 11.006832 Test MSE 5.633067093469585 Test RE 1.134437364558275\n",
      "Training time: 90.24\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 56.549904 Test MSE 8.487295498676495 Test RE 1.3924920417988296\n",
      "1 Train Loss 56.18061 Test MSE 8.475802700979557 Test RE 1.3915489233045797\n",
      "2 Train Loss 56.092415 Test MSE 8.735223536432478 Test RE 1.4126841479072711\n",
      "3 Train Loss 55.733658 Test MSE 8.67106292615324 Test RE 1.407486471680921\n",
      "4 Train Loss 55.493145 Test MSE 8.396746318670589 Test RE 1.385044020051167\n",
      "5 Train Loss 52.904476 Test MSE 8.910696784853629 Test RE 1.426802603425087\n",
      "6 Train Loss 51.746704 Test MSE 8.705486990396814 Test RE 1.41027756040972\n",
      "7 Train Loss 51.250275 Test MSE 8.646880040137201 Test RE 1.4055224189450182\n",
      "8 Train Loss 51.05739 Test MSE 8.624856418406305 Test RE 1.4037313434643572\n",
      "9 Train Loss 46.732452 Test MSE 9.226557487187316 Test RE 1.4518705842903281\n",
      "10 Train Loss 43.0163 Test MSE 8.641302056922354 Test RE 1.405069004233678\n",
      "11 Train Loss 42.858395 Test MSE 8.438646567088783 Test RE 1.3884954444791946\n",
      "12 Train Loss 42.699993 Test MSE 8.30513675192606 Test RE 1.377467796071346\n",
      "13 Train Loss 42.491634 Test MSE 8.439299678120472 Test RE 1.3885491749050043\n",
      "14 Train Loss 42.397964 Test MSE 8.423561330487049 Test RE 1.3872538264440166\n",
      "15 Train Loss 42.327637 Test MSE 8.346155736892122 Test RE 1.3808652558696213\n",
      "16 Train Loss 42.227882 Test MSE 8.346267619540715 Test RE 1.3808745112890797\n",
      "17 Train Loss 41.94701 Test MSE 8.313155493035625 Test RE 1.3781326191257053\n",
      "18 Train Loss 41.69908 Test MSE 8.190788878934596 Test RE 1.3679522127450976\n",
      "19 Train Loss 40.9152 Test MSE 7.914535990823758 Test RE 1.3446857094272757\n",
      "20 Train Loss 38.637314 Test MSE 7.360211650742449 Test RE 1.296740908553328\n",
      "21 Train Loss 37.98314 Test MSE 7.553940387703762 Test RE 1.313695877035984\n",
      "22 Train Loss 37.4671 Test MSE 7.714487587004926 Test RE 1.32758275555527\n",
      "23 Train Loss 37.320164 Test MSE 7.7456444651846885 Test RE 1.3302609408408947\n",
      "24 Train Loss 37.08132 Test MSE 7.7803688435320755 Test RE 1.3332394424322767\n",
      "25 Train Loss 37.035656 Test MSE 7.640151536859585 Test RE 1.3211710435190294\n",
      "26 Train Loss 36.645523 Test MSE 7.618579265263929 Test RE 1.319304535565815\n",
      "27 Train Loss 36.55461 Test MSE 7.575251999137238 Test RE 1.3155477090450627\n",
      "28 Train Loss 36.4813 Test MSE 7.423485373196988 Test RE 1.302302843631957\n",
      "29 Train Loss 36.292793 Test MSE 7.375679843915485 Test RE 1.2981028063016333\n",
      "30 Train Loss 36.25692 Test MSE 7.398436617771961 Test RE 1.3001038341185043\n",
      "31 Train Loss 36.233383 Test MSE 7.396629581887199 Test RE 1.2999450520761122\n",
      "32 Train Loss 36.196003 Test MSE 7.33588589181869 Test RE 1.2945962478380697\n",
      "33 Train Loss 36.14762 Test MSE 7.479007349705244 Test RE 1.3071638845278204\n",
      "34 Train Loss 36.109097 Test MSE 7.530675552043714 Test RE 1.3116713384802265\n",
      "35 Train Loss 35.983906 Test MSE 7.587234855756516 Test RE 1.3165877926258658\n",
      "36 Train Loss 35.83938 Test MSE 7.612322658671777 Test RE 1.318762698003509\n",
      "37 Train Loss 35.78009 Test MSE 7.600183473699262 Test RE 1.3177107791404845\n",
      "38 Train Loss 35.747356 Test MSE 7.479897323007509 Test RE 1.3072416559684876\n",
      "39 Train Loss 35.05086 Test MSE 7.2119724201823034 Test RE 1.2836159026202807\n",
      "40 Train Loss 33.39312 Test MSE 7.463362171016632 Test RE 1.3057959544035649\n",
      "41 Train Loss 32.723587 Test MSE 7.455418438590789 Test RE 1.3051008484292237\n",
      "42 Train Loss 32.357418 Test MSE 7.528763571266718 Test RE 1.3115048162624439\n",
      "43 Train Loss 31.697647 Test MSE 7.541975211441723 Test RE 1.3126550406459965\n",
      "44 Train Loss 31.25559 Test MSE 7.615770908139678 Test RE 1.3190613524429193\n",
      "45 Train Loss 31.02464 Test MSE 7.615396998582173 Test RE 1.3190289712364425\n",
      "46 Train Loss 30.985416 Test MSE 7.599765460411304 Test RE 1.3176745413186397\n",
      "47 Train Loss 30.66949 Test MSE 7.570503078923856 Test RE 1.3151352864258252\n",
      "48 Train Loss 30.236021 Test MSE 7.531188370576697 Test RE 1.311715998346124\n",
      "49 Train Loss 29.945953 Test MSE 7.6022970245694355 Test RE 1.3178939888732129\n",
      "50 Train Loss 29.76756 Test MSE 7.50760196148037 Test RE 1.309660351195254\n",
      "51 Train Loss 29.603628 Test MSE 7.501865520865625 Test RE 1.3091599101410965\n",
      "52 Train Loss 29.493729 Test MSE 7.495901589573396 Test RE 1.3086394201293232\n",
      "53 Train Loss 29.424068 Test MSE 7.473469365503758 Test RE 1.3066798367675125\n",
      "54 Train Loss 29.38086 Test MSE 7.475461084631853 Test RE 1.3068539437132562\n",
      "55 Train Loss 29.293684 Test MSE 7.378130240340386 Test RE 1.2983184204819436\n",
      "56 Train Loss 29.205605 Test MSE 7.355594708694442 Test RE 1.296334132424568\n",
      "57 Train Loss 29.153263 Test MSE 7.296969745416295 Test RE 1.2911578293687593\n",
      "58 Train Loss 29.102688 Test MSE 7.2687710233295855 Test RE 1.288660611665535\n",
      "59 Train Loss 29.03737 Test MSE 7.242696050978236 Test RE 1.2863471546537937\n",
      "60 Train Loss 28.9269 Test MSE 7.264928544734036 Test RE 1.2883199553585634\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61 Train Loss 28.80553 Test MSE 7.178696779713298 Test RE 1.2806512126720007\n",
      "62 Train Loss 28.72134 Test MSE 7.3092026994674315 Test RE 1.292239652425131\n",
      "63 Train Loss 28.676735 Test MSE 7.331154879673974 Test RE 1.2941787291863653\n",
      "64 Train Loss 28.654705 Test MSE 7.346450504201972 Test RE 1.2955281044108178\n",
      "65 Train Loss 28.6146 Test MSE 7.367603564057526 Test RE 1.2973919082378522\n",
      "66 Train Loss 28.527996 Test MSE 7.382624962189214 Test RE 1.2987138249746737\n",
      "67 Train Loss 28.435936 Test MSE 7.343693082925183 Test RE 1.2952849494321272\n",
      "68 Train Loss 28.293594 Test MSE 7.20634672218921 Test RE 1.2831151627555895\n",
      "69 Train Loss 28.250505 Test MSE 7.279169864127023 Test RE 1.2895820732954013\n",
      "70 Train Loss 28.08308 Test MSE 7.19947865237974 Test RE 1.2825035751117273\n",
      "71 Train Loss 27.95771 Test MSE 7.206160582175316 Test RE 1.2830985912093018\n",
      "72 Train Loss 27.919628 Test MSE 7.203059787718672 Test RE 1.2828225043081007\n",
      "73 Train Loss 27.909193 Test MSE 7.191542216125235 Test RE 1.2817964881815118\n",
      "74 Train Loss 27.871668 Test MSE 7.202318445189641 Test RE 1.282756488241418\n",
      "75 Train Loss 27.782734 Test MSE 7.191205051408845 Test RE 1.2817664402723132\n",
      "76 Train Loss 27.701035 Test MSE 7.245303943231978 Test RE 1.2865787226245922\n",
      "77 Train Loss 27.593784 Test MSE 7.307384916591029 Test RE 1.292078953832163\n",
      "78 Train Loss 27.555115 Test MSE 7.323603394907783 Test RE 1.293512020523995\n",
      "79 Train Loss 27.489075 Test MSE 7.387317545051287 Test RE 1.299126507059325\n",
      "80 Train Loss 27.4205 Test MSE 7.473585259724851 Test RE 1.3066899683441207\n",
      "81 Train Loss 27.376114 Test MSE 7.464787165406146 Test RE 1.3059206075461742\n",
      "82 Train Loss 27.344841 Test MSE 7.504335462013447 Test RE 1.309375408660079\n",
      "83 Train Loss 27.315113 Test MSE 7.496393240246522 Test RE 1.3086823357742605\n",
      "84 Train Loss 27.27037 Test MSE 7.521572295680053 Test RE 1.310878309286363\n",
      "85 Train Loss 27.21732 Test MSE 7.5549218902245086 Test RE 1.3137812201726606\n",
      "86 Train Loss 27.203884 Test MSE 7.557433911657446 Test RE 1.3139996190087264\n",
      "87 Train Loss 27.185196 Test MSE 7.538269998553884 Test RE 1.3123325612066983\n",
      "88 Train Loss 27.148705 Test MSE 7.4510967600385385 Test RE 1.3047225300292968\n",
      "89 Train Loss 27.125103 Test MSE 7.46999943195719 Test RE 1.3063764556739619\n",
      "90 Train Loss 27.111673 Test MSE 7.4745844847777665 Test RE 1.3067773182349525\n",
      "91 Train Loss 27.094376 Test MSE 7.45461254623292 Test RE 1.3050303091809607\n",
      "92 Train Loss 27.06557 Test MSE 7.425711273013579 Test RE 1.3024980739213572\n",
      "93 Train Loss 27.025528 Test MSE 7.450538539757271 Test RE 1.3046736555989444\n",
      "94 Train Loss 26.989994 Test MSE 7.4661916618665565 Test RE 1.306043455965306\n",
      "95 Train Loss 26.967062 Test MSE 7.4933285692948495 Test RE 1.3084148010675225\n",
      "96 Train Loss 26.926134 Test MSE 7.44677902920257 Test RE 1.3043444476322326\n",
      "97 Train Loss 26.9067 Test MSE 7.381265519653563 Test RE 1.2985942463621742\n",
      "98 Train Loss 26.858997 Test MSE 7.392662963357253 Test RE 1.2995964421066335\n",
      "99 Train Loss 26.79695 Test MSE 7.358945563076504 Test RE 1.2966293724753986\n",
      "Training time: 90.31\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 55.77038 Test MSE 8.673225156881536 Test RE 1.4076619473306229\n",
      "1 Train Loss 55.77038 Test MSE 8.673225213008456 Test RE 1.4076619518853144\n",
      "2 Train Loss 55.770374 Test MSE 8.673225501157917 Test RE 1.4076619752686013\n",
      "3 Train Loss 55.770374 Test MSE 8.673225354209736 Test RE 1.4076619633437775\n",
      "4 Train Loss 55.77037 Test MSE 8.673224911520359 Test RE 1.4076619274195983\n",
      "5 Train Loss 55.770363 Test MSE 8.673223868597077 Test RE 1.4076618427865202\n",
      "6 Train Loss 55.770363 Test MSE 8.673223328365276 Test RE 1.4076617989467834\n",
      "7 Train Loss 55.770355 Test MSE 8.673221714744741 Test RE 1.4076616680016893\n",
      "8 Train Loss 55.770355 Test MSE 8.673221382549126 Test RE 1.407661641044057\n",
      "9 Train Loss 55.770355 Test MSE 8.673220798650764 Test RE 1.4076615936607781\n",
      "10 Train Loss 55.770348 Test MSE 8.67321977277842 Test RE 1.4076615104113617\n",
      "11 Train Loss 55.770348 Test MSE 8.673219165593752 Test RE 1.4076614611383969\n",
      "12 Train Loss 55.770344 Test MSE 8.67321721551857 Test RE 1.4076613028900133\n",
      "13 Train Loss 55.77034 Test MSE 8.673215792398627 Test RE 1.4076611874039768\n",
      "14 Train Loss 55.770336 Test MSE 8.673213867331498 Test RE 1.407661031184963\n",
      "15 Train Loss 55.770336 Test MSE 8.673213207346755 Test RE 1.407660977627261\n",
      "16 Train Loss 55.770325 Test MSE 8.673210503544206 Test RE 1.4076607582139045\n",
      "17 Train Loss 55.770325 Test MSE 8.673209779791797 Test RE 1.4076606994814496\n",
      "18 Train Loss 55.770325 Test MSE 8.67320886455968 Test RE 1.4076606252104271\n",
      "19 Train Loss 55.770317 Test MSE 8.673206237639244 Test RE 1.407660412035996\n",
      "20 Train Loss 55.77031 Test MSE 8.67320373964558 Test RE 1.4076602093239334\n",
      "21 Train Loss 55.77031 Test MSE 8.673203025516678 Test RE 1.4076601513724027\n",
      "22 Train Loss 55.77031 Test MSE 8.673202167161195 Test RE 1.4076600817168876\n",
      "23 Train Loss 55.770306 Test MSE 8.673200176894118 Test RE 1.4076599202067777\n",
      "24 Train Loss 55.7703 Test MSE 8.673198097649404 Test RE 1.4076597514761156\n",
      "25 Train Loss 55.77029 Test MSE 8.673194553531108 Test RE 1.4076594638709479\n",
      "26 Train Loss 55.77029 Test MSE 8.673193766365085 Test RE 1.4076593999924238\n",
      "27 Train Loss 55.770287 Test MSE 8.673191731807389 Test RE 1.4076592348880512\n",
      "28 Train Loss 55.770283 Test MSE 8.673189534335487 Test RE 1.4076590565631673\n",
      "29 Train Loss 55.77028 Test MSE 8.673187397122526 Test RE 1.4076588831282755\n",
      "30 Train Loss 55.77028 Test MSE 8.67318637446443 Test RE 1.4076588001395358\n",
      "31 Train Loss 55.77028 Test MSE 8.673185147158648 Test RE 1.4076587005436223\n",
      "32 Train Loss 55.77027 Test MSE 8.673183182979717 Test RE 1.4076585411504074\n",
      "33 Train Loss 55.770267 Test MSE 8.673181234303849 Test RE 1.4076583830152503\n",
      "34 Train Loss 55.770264 Test MSE 8.67317915837284 Test RE 1.4076582145533103\n",
      "35 Train Loss 55.77026 Test MSE 8.67317681715326 Test RE 1.407658024563162\n",
      "36 Train Loss 55.77025 Test MSE 8.673173437266723 Test RE 1.407657750285065\n",
      "37 Train Loss 55.77025 Test MSE 8.673172389809643 Test RE 1.4076576652838173\n",
      "38 Train Loss 55.77024 Test MSE 8.673169080204683 Test RE 1.4076573967090082\n",
      "39 Train Loss 55.770237 Test MSE 8.673166795807104 Test RE 1.407657211329862\n",
      "40 Train Loss 55.770233 Test MSE 8.673164350751893 Test RE 1.4076570129133021\n",
      "41 Train Loss 55.770226 Test MSE 8.673161067894242 Test RE 1.4076567465089165\n",
      "42 Train Loss 55.770218 Test MSE 8.673158537197398 Test RE 1.4076565411424582\n",
      "43 Train Loss 55.770218 Test MSE 8.67315760512792 Test RE 1.4076564655048616\n",
      "44 Train Loss 55.770206 Test MSE 8.673153621438402 Test RE 1.4076561422277343\n",
      "45 Train Loss 55.770203 Test MSE 8.673151155410311 Test RE 1.40765594210907\n",
      "46 Train Loss 55.7702 Test MSE 8.673148526283812 Test RE 1.4076557287549065\n",
      "47 Train Loss 55.770195 Test MSE 8.673145991947763 Test RE 1.4076555230929773\n",
      "48 Train Loss 55.770195 Test MSE 8.673144887173429 Test RE 1.4076554334402867\n",
      "49 Train Loss 55.770187 Test MSE 8.673142104488296 Test RE 1.4076552076247253\n",
      "50 Train Loss 55.770187 Test MSE 8.673140951653242 Test RE 1.4076551140718763\n",
      "51 Train Loss 55.770176 Test MSE 8.673137112922063 Test RE 1.4076548025578461\n",
      "52 Train Loss 55.770172 Test MSE 8.673134267765741 Test RE 1.407654571672629\n",
      "53 Train Loss 55.770172 Test MSE 8.67313279714808 Test RE 1.4076544523315875\n",
      "54 Train Loss 55.770172 Test MSE 8.673131641912708 Test RE 1.407654358583901\n",
      "55 Train Loss 55.77016 Test MSE 8.673126585982773 Test RE 1.407653948293642\n",
      "56 Train Loss 55.770153 Test MSE 8.673123592116655 Test RE 1.4076537053404359\n",
      "57 Train Loss 55.770153 Test MSE 8.673122227999764 Test RE 1.4076535946418938\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "58 Train Loss 55.77015 Test MSE 8.67311949126221 Test RE 1.4076533725547\n",
      "59 Train Loss 55.77014 Test MSE 8.673116419456825 Test RE 1.4076531232765943\n",
      "60 Train Loss 55.77014 Test MSE 8.673115436263341 Test RE 1.4076530434900822\n",
      "61 Train Loss 55.770134 Test MSE 8.673112016236809 Test RE 1.4076527659536382\n",
      "62 Train Loss 55.770134 Test MSE 8.673110623523556 Test RE 1.4076526529344162\n",
      "63 Train Loss 55.770126 Test MSE 8.67310647245824 Test RE 1.4076523160737988\n",
      "64 Train Loss 55.77012 Test MSE 8.673104174745285 Test RE 1.4076521296134352\n",
      "65 Train Loss 55.77012 Test MSE 8.673102920978126 Test RE 1.407652027869697\n",
      "66 Train Loss 55.77011 Test MSE 8.673100047989408 Test RE 1.407651794725414\n",
      "67 Train Loss 55.77011 Test MSE 8.673098674868704 Test RE 1.4076516832960577\n",
      "68 Train Loss 55.7701 Test MSE 8.67309544022431 Test RE 1.4076514208031923\n",
      "69 Train Loss 55.7701 Test MSE 8.673094255053437 Test RE 1.407651324626024\n",
      "70 Train Loss 55.770096 Test MSE 8.673091378546024 Test RE 1.4076510911960805\n",
      "71 Train Loss 55.77009 Test MSE 8.673087713815303 Test RE 1.4076507938013845\n",
      "72 Train Loss 55.77009 Test MSE 8.673086224609765 Test RE 1.407650672951607\n",
      "73 Train Loss 55.77008 Test MSE 8.673083218687548 Test RE 1.407650429019476\n",
      "74 Train Loss 55.77008 Test MSE 8.673082072022757 Test RE 1.4076503359670294\n",
      "75 Train Loss 55.770073 Test MSE 8.673078619907631 Test RE 1.4076500558260447\n",
      "76 Train Loss 55.770073 Test MSE 8.673077134120687 Test RE 1.4076499352536247\n",
      "77 Train Loss 55.770065 Test MSE 8.673073364428904 Test RE 1.4076496293410343\n",
      "78 Train Loss 55.770058 Test MSE 8.6730697322892 Test RE 1.4076493345908103\n",
      "79 Train Loss 55.77005 Test MSE 8.673065284568446 Test RE 1.4076489736556301\n",
      "80 Train Loss 55.770042 Test MSE 8.67306218706463 Test RE 1.4076487222913026\n",
      "81 Train Loss 55.770042 Test MSE 8.673060731128542 Test RE 1.4076486041411835\n",
      "82 Train Loss 55.770035 Test MSE 8.673056863662126 Test RE 1.40764829029383\n",
      "83 Train Loss 55.770027 Test MSE 8.673052906343703 Test RE 1.407647969154857\n",
      "84 Train Loss 55.770027 Test MSE 8.673051193470323 Test RE 1.4076478301540412\n",
      "85 Train Loss 55.770016 Test MSE 8.673047421834493 Test RE 1.4076475240832336\n",
      "86 Train Loss 55.770008 Test MSE 8.67304188614519 Test RE 1.4076470748581686\n",
      "87 Train Loss 55.769997 Test MSE 8.673035888812215 Test RE 1.4076465881702347\n",
      "88 Train Loss 55.769997 Test MSE 8.673034318530778 Test RE 1.407646460740726\n",
      "89 Train Loss 55.76999 Test MSE 8.673030908388524 Test RE 1.4076461840051004\n",
      "90 Train Loss 55.76998 Test MSE 8.673027433719753 Test RE 1.4076459020330436\n",
      "91 Train Loss 55.769974 Test MSE 8.673023355985208 Test RE 1.4076455711216613\n",
      "92 Train Loss 55.76997 Test MSE 8.673021719695425 Test RE 1.4076454383354298\n",
      "93 Train Loss 55.76997 Test MSE 8.673021719695425 Test RE 1.4076454383354298\n",
      "94 Train Loss 55.76997 Test MSE 8.673021719695425 Test RE 1.4076454383354298\n",
      "95 Train Loss 55.76997 Test MSE 8.673021719695425 Test RE 1.4076454383354298\n",
      "96 Train Loss 55.76997 Test MSE 8.673021719695425 Test RE 1.4076454383354298\n",
      "97 Train Loss 55.76997 Test MSE 8.673021719695425 Test RE 1.4076454383354298\n",
      "98 Train Loss 55.76997 Test MSE 8.673021719695425 Test RE 1.4076454383354298\n",
      "99 Train Loss 55.76997 Test MSE 8.673021719695425 Test RE 1.4076454383354298\n",
      "Training time: 45.20\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 56.936317 Test MSE 8.672175604641533 Test RE 1.4075767737405402\n",
      "1 Train Loss 56.495167 Test MSE 8.734816267898784 Test RE 1.4126512152350055\n",
      "2 Train Loss 56.341858 Test MSE 8.367505946192027 Test RE 1.3826303159158566\n",
      "3 Train Loss 54.82341 Test MSE 8.525266277319384 Test RE 1.3956034570653766\n",
      "4 Train Loss 48.461254 Test MSE 7.816978990747425 Test RE 1.3363725078149127\n",
      "5 Train Loss 45.78518 Test MSE 7.754848195489989 Test RE 1.3310510447578747\n",
      "6 Train Loss 44.469425 Test MSE 7.988981198255342 Test RE 1.350995056307634\n",
      "7 Train Loss 43.11872 Test MSE 8.111814244958323 Test RE 1.3613414209387182\n",
      "8 Train Loss 42.935863 Test MSE 8.159549626524047 Test RE 1.3653410705755955\n",
      "9 Train Loss 42.325546 Test MSE 8.37116240742348 Test RE 1.3829323761716004\n",
      "10 Train Loss 42.050186 Test MSE 8.326914100744752 Test RE 1.3792725801908574\n",
      "11 Train Loss 41.551285 Test MSE 8.374584061875622 Test RE 1.383214979295034\n",
      "12 Train Loss 39.84208 Test MSE 8.242207041224137 Test RE 1.3722391958397415\n",
      "13 Train Loss 35.766388 Test MSE 8.000588720458195 Test RE 1.3519761584474046\n",
      "14 Train Loss 32.924397 Test MSE 7.817316957250621 Test RE 1.3364013964854797\n",
      "15 Train Loss 29.717876 Test MSE 7.037660267588462 Test RE 1.2680086299601256\n",
      "16 Train Loss 28.514772 Test MSE 6.616221412078165 Test RE 1.2294562399843416\n",
      "17 Train Loss 27.514215 Test MSE 6.091712099195424 Test RE 1.1797167487238447\n",
      "18 Train Loss 26.660622 Test MSE 6.1628724496026015 Test RE 1.1865871748570098\n",
      "19 Train Loss 25.64332 Test MSE 6.169135224306723 Test RE 1.1871899328415558\n",
      "20 Train Loss 24.676918 Test MSE 5.965226519488164 Test RE 1.1674049494090737\n",
      "21 Train Loss 24.062246 Test MSE 5.999745250448146 Test RE 1.1707777642189083\n",
      "22 Train Loss 23.255459 Test MSE 5.95729587049113 Test RE 1.1666286705802524\n",
      "23 Train Loss 23.11206 Test MSE 5.841850531193852 Test RE 1.155269428131029\n",
      "24 Train Loss 22.944397 Test MSE 5.755584824580957 Test RE 1.146707860928415\n",
      "25 Train Loss 22.850428 Test MSE 5.761108815961373 Test RE 1.1472580122493912\n",
      "26 Train Loss 22.567783 Test MSE 5.562452308158001 Test RE 1.1273044217509136\n",
      "27 Train Loss 22.348236 Test MSE 5.4455548697097225 Test RE 1.11539612010166\n",
      "28 Train Loss 22.130356 Test MSE 5.279373477948269 Test RE 1.0982450473334833\n",
      "29 Train Loss 21.638195 Test MSE 5.194641348657251 Test RE 1.0893961706408233\n",
      "30 Train Loss 21.289093 Test MSE 4.963506962114615 Test RE 1.0648841868650318\n",
      "31 Train Loss 20.78687 Test MSE 4.956379007385607 Test RE 1.0641192868256633\n",
      "32 Train Loss 19.861214 Test MSE 4.714192623731769 Test RE 1.0377953543839058\n",
      "33 Train Loss 19.352058 Test MSE 4.550681050706087 Test RE 1.0196385790944613\n",
      "34 Train Loss 18.952206 Test MSE 4.5523456530351005 Test RE 1.0198250498170844\n",
      "35 Train Loss 18.533743 Test MSE 4.557452746193617 Test RE 1.0203969397825374\n",
      "36 Train Loss 18.246792 Test MSE 4.554455819893825 Test RE 1.020061384210626\n",
      "37 Train Loss 17.948296 Test MSE 4.509463037886469 Test RE 1.0150103624088602\n",
      "38 Train Loss 17.728687 Test MSE 4.51228339251842 Test RE 1.015327721891559\n",
      "39 Train Loss 17.425652 Test MSE 4.41730433772481 Test RE 1.0045850739448587\n",
      "40 Train Loss 16.997532 Test MSE 4.535592390067025 Test RE 1.0179467713763715\n",
      "41 Train Loss 16.530558 Test MSE 4.390530855283221 Test RE 1.001536028917411\n",
      "42 Train Loss 16.240858 Test MSE 4.350857992130339 Test RE 0.997000817836627\n",
      "43 Train Loss 15.984905 Test MSE 4.384943644952707 Test RE 1.0008985691193197\n",
      "44 Train Loss 15.530597 Test MSE 4.273546351426099 Test RE 0.9881031178083582\n",
      "45 Train Loss 14.949345 Test MSE 4.180687086740323 Test RE 0.9773089855592123\n",
      "46 Train Loss 14.79434 Test MSE 4.112396799606551 Test RE 0.9692940939545122\n",
      "47 Train Loss 14.602661 Test MSE 4.1412726431046885 Test RE 0.9726911668836775\n",
      "48 Train Loss 14.351955 Test MSE 4.21523212619478 Test RE 0.9813384334980301\n",
      "49 Train Loss 14.07589 Test MSE 4.143738400724301 Test RE 0.9729806991069718\n",
      "50 Train Loss 13.919037 Test MSE 4.151795332925216 Test RE 0.9739261536561931\n",
      "51 Train Loss 13.653475 Test MSE 4.099012334576888 Test RE 0.9677154456834285\n",
      "52 Train Loss 13.390722 Test MSE 4.14125478735619 Test RE 0.9726890699259235\n",
      "53 Train Loss 13.008905 Test MSE 4.181551865152725 Test RE 0.9774100588989121\n",
      "54 Train Loss 12.569804 Test MSE 4.004108817595142 Test RE 0.9564471911038416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "55 Train Loss 12.171162 Test MSE 3.9858399687680444 Test RE 0.9542627892797407\n",
      "56 Train Loss 11.744104 Test MSE 4.102173605178471 Test RE 0.9680885380932709\n",
      "57 Train Loss 11.397267 Test MSE 3.9381993944705083 Test RE 0.9485427540875689\n",
      "58 Train Loss 11.151007 Test MSE 3.857959864210725 Test RE 0.9388299005562537\n",
      "59 Train Loss 10.602713 Test MSE 3.716379762049577 Test RE 0.9214422114451656\n",
      "60 Train Loss 10.4513 Test MSE 3.6896367622379005 Test RE 0.9181208850782271\n",
      "61 Train Loss 9.873322 Test MSE 3.6722445281171865 Test RE 0.915954406828805\n",
      "62 Train Loss 9.358112 Test MSE 3.6267327992086553 Test RE 0.9102607996590543\n",
      "63 Train Loss 9.014504 Test MSE 3.4858188474639786 Test RE 0.8924018607437864\n",
      "64 Train Loss 8.7346115 Test MSE 3.5492536344346903 Test RE 0.9004851975343523\n",
      "65 Train Loss 8.437171 Test MSE 3.413564250927085 Test RE 0.883104506753696\n",
      "66 Train Loss 8.168541 Test MSE 3.3845468750116923 Test RE 0.8793430328099237\n",
      "67 Train Loss 7.827827 Test MSE 3.33055707853282 Test RE 0.8723012619004377\n",
      "68 Train Loss 7.637857 Test MSE 3.32897222012716 Test RE 0.8720936932478632\n",
      "69 Train Loss 7.3922253 Test MSE 3.24740592574949 Test RE 0.8613434384537332\n",
      "70 Train Loss 7.2805624 Test MSE 3.2385056636539833 Test RE 0.8601622737832311\n",
      "71 Train Loss 7.1315928 Test MSE 3.2474597501041007 Test RE 0.8613505766224179\n",
      "72 Train Loss 7.0191746 Test MSE 3.151611571650348 Test RE 0.8485440713882644\n",
      "73 Train Loss 6.945074 Test MSE 3.127750726649608 Test RE 0.8453258056976871\n",
      "74 Train Loss 6.8532696 Test MSE 3.1079461383594316 Test RE 0.8426452987876248\n",
      "75 Train Loss 6.7481904 Test MSE 3.078592950437419 Test RE 0.8386566510853883\n",
      "76 Train Loss 6.649227 Test MSE 3.06465749415647 Test RE 0.8367563805933065\n",
      "77 Train Loss 6.575281 Test MSE 3.0422698284578913 Test RE 0.8336944789950627\n",
      "78 Train Loss 6.4840198 Test MSE 2.925517194581777 Test RE 0.8175407098433534\n",
      "79 Train Loss 6.3962193 Test MSE 2.9458067866244315 Test RE 0.8203707917372528\n",
      "80 Train Loss 6.2520123 Test MSE 2.8836442726031923 Test RE 0.8116688939925517\n",
      "81 Train Loss 6.161401 Test MSE 2.8696518481913618 Test RE 0.8096972523181164\n",
      "82 Train Loss 6.0843077 Test MSE 2.8284212942356715 Test RE 0.8038594274149626\n",
      "83 Train Loss 5.959136 Test MSE 2.7978140071055178 Test RE 0.7994981816462139\n",
      "84 Train Loss 5.9149237 Test MSE 2.7844364515900755 Test RE 0.7975845185709282\n",
      "85 Train Loss 5.822487 Test MSE 2.7795090754231824 Test RE 0.796878497945022\n",
      "86 Train Loss 5.7565336 Test MSE 2.743498737735478 Test RE 0.7916996312126332\n",
      "87 Train Loss 5.6751266 Test MSE 2.703306234445207 Test RE 0.7858789989544945\n",
      "88 Train Loss 5.6302433 Test MSE 2.729656143053096 Test RE 0.7896998059816431\n",
      "89 Train Loss 5.590419 Test MSE 2.7387942835848196 Test RE 0.7910205507739161\n",
      "90 Train Loss 5.5032387 Test MSE 2.7124315100455556 Test RE 0.7872042873450744\n",
      "91 Train Loss 5.4513736 Test MSE 2.7231667870001237 Test RE 0.7887605498364586\n",
      "92 Train Loss 5.3484435 Test MSE 2.733838277804083 Test RE 0.7903045281147026\n",
      "93 Train Loss 5.294489 Test MSE 2.7389430079206485 Test RE 0.7910420278176077\n",
      "94 Train Loss 5.237849 Test MSE 2.732434330302408 Test RE 0.7901015737988394\n",
      "95 Train Loss 5.1388454 Test MSE 2.674914813303479 Test RE 0.78174126671591\n",
      "96 Train Loss 5.052766 Test MSE 2.60779854275609 Test RE 0.7718716311082785\n",
      "97 Train Loss 4.986923 Test MSE 2.615591451569865 Test RE 0.7730240664050838\n",
      "98 Train Loss 4.9249754 Test MSE 2.5799378072578083 Test RE 0.7677373666178132\n",
      "99 Train Loss 4.886156 Test MSE 2.5919364259297786 Test RE 0.7695205689693219\n",
      "Training time: 91.56\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 56.261894 Test MSE 8.754052217168926 Test RE 1.4142058411223954\n",
      "1 Train Loss 56.260838 Test MSE 8.761020014704522 Test RE 1.4147685485237516\n",
      "2 Train Loss 56.26065 Test MSE 8.756245607525582 Test RE 1.4143829997170947\n",
      "3 Train Loss 56.259087 Test MSE 8.742344339597587 Test RE 1.4132598284535896\n",
      "4 Train Loss 56.064472 Test MSE 8.904344559416796 Test RE 1.4262939458053943\n",
      "5 Train Loss 55.489635 Test MSE 9.216452977709386 Test RE 1.4510753549265056\n",
      "6 Train Loss 49.021408 Test MSE 7.744969986557702 Test RE 1.3302030210570601\n",
      "7 Train Loss 43.781902 Test MSE 8.598630702174672 Test RE 1.4015955463090637\n",
      "8 Train Loss 42.31938 Test MSE 8.382878948540192 Test RE 1.3838998355041252\n",
      "9 Train Loss 41.621017 Test MSE 8.16092852052607 Test RE 1.3654564311755946\n",
      "10 Train Loss 41.427223 Test MSE 8.275041930438565 Test RE 1.3749698077156365\n",
      "11 Train Loss 41.35414 Test MSE 8.35256693492008 Test RE 1.3813955180287991\n",
      "12 Train Loss 41.319603 Test MSE 8.298557191827998 Test RE 1.3769220538309814\n",
      "13 Train Loss 41.280663 Test MSE 8.320567647124939 Test RE 1.3787468657495598\n",
      "14 Train Loss 41.26095 Test MSE 8.288977194972569 Test RE 1.3761270530981566\n",
      "15 Train Loss 41.204636 Test MSE 8.311933197742091 Test RE 1.3780313009837397\n",
      "16 Train Loss 41.181858 Test MSE 8.341530756363955 Test RE 1.3804826030576378\n",
      "17 Train Loss 41.136154 Test MSE 8.334862717999947 Test RE 1.3799307288540905\n",
      "18 Train Loss 41.10157 Test MSE 8.329837503642821 Test RE 1.3795146756159573\n",
      "19 Train Loss 41.074295 Test MSE 8.360507015159191 Test RE 1.3820519501111685\n",
      "20 Train Loss 41.06436 Test MSE 8.37301318363279 Test RE 1.3830852436446237\n",
      "21 Train Loss 41.043476 Test MSE 8.351242685088208 Test RE 1.3812860076645503\n",
      "22 Train Loss 41.0384 Test MSE 8.349278860447848 Test RE 1.3811235909388069\n",
      "23 Train Loss 41.03299 Test MSE 8.353127574109733 Test RE 1.3814418781163285\n",
      "24 Train Loss 41.029854 Test MSE 8.356945170082584 Test RE 1.3817575194421154\n",
      "25 Train Loss 41.018112 Test MSE 8.361387283594572 Test RE 1.3821247055483818\n",
      "26 Train Loss 40.970932 Test MSE 8.379346282207504 Test RE 1.3836082070818405\n",
      "27 Train Loss 40.940506 Test MSE 8.389355228978816 Test RE 1.3844343054120367\n",
      "28 Train Loss 40.86549 Test MSE 8.426909418259068 Test RE 1.3875294928729143\n",
      "29 Train Loss 40.82458 Test MSE 8.393404050524806 Test RE 1.3847683389076009\n",
      "30 Train Loss 40.74482 Test MSE 8.491337362894136 Test RE 1.3928235722514446\n",
      "31 Train Loss 40.48995 Test MSE 8.437104785702822 Test RE 1.388368596281186\n",
      "32 Train Loss 40.240044 Test MSE 8.40482996704352 Test RE 1.3857105589121432\n",
      "33 Train Loss 39.922104 Test MSE 8.314238297803678 Test RE 1.378222368438306\n",
      "34 Train Loss 39.80651 Test MSE 8.364136814254357 Test RE 1.3823519335050136\n",
      "35 Train Loss 39.64524 Test MSE 8.272143494945608 Test RE 1.3747289865597563\n",
      "36 Train Loss 39.365402 Test MSE 8.377924271484247 Test RE 1.3834908000000943\n",
      "37 Train Loss 38.53354 Test MSE 8.037672506228914 Test RE 1.3551058300896557\n",
      "38 Train Loss 37.365555 Test MSE 8.002106521374888 Test RE 1.352104394844143\n",
      "39 Train Loss 36.80394 Test MSE 7.9782155297817585 Test RE 1.350084472861714\n",
      "40 Train Loss 36.136566 Test MSE 7.562886473671354 Test RE 1.3144735479019365\n",
      "41 Train Loss 34.944775 Test MSE 7.082590861720466 Test RE 1.2720498692178304\n",
      "42 Train Loss 31.699875 Test MSE 6.768693917772386 Test RE 1.2435421149987507\n",
      "43 Train Loss 30.098125 Test MSE 6.636446566556731 Test RE 1.231333970989735\n",
      "44 Train Loss 28.945274 Test MSE 6.512314397154427 Test RE 1.2197637985613081\n",
      "45 Train Loss 27.900421 Test MSE 5.985279181146385 Test RE 1.1693654731230303\n",
      "46 Train Loss 26.8723 Test MSE 5.638961570824178 Test RE 1.1350307506170227\n",
      "47 Train Loss 25.233051 Test MSE 4.763744579818594 Test RE 1.0432353485753127\n",
      "48 Train Loss 24.462587 Test MSE 4.400365753192348 Test RE 1.0026571342036543\n",
      "49 Train Loss 22.853846 Test MSE 3.539306936927533 Test RE 0.8992225182582252\n",
      "50 Train Loss 18.288008 Test MSE 3.024081782918802 Test RE 0.8311986444360844\n",
      "51 Train Loss 15.8176775 Test MSE 2.6502145523027383 Test RE 0.7781235821969184\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "52 Train Loss 14.020819 Test MSE 2.4883464660987027 Test RE 0.7539863524691166\n",
      "53 Train Loss 12.314623 Test MSE 1.299056194205256 Test RE 0.54478109630377\n",
      "54 Train Loss 11.773235 Test MSE 1.0289249863433132 Test RE 0.48484164620846587\n",
      "55 Train Loss 10.293329 Test MSE 0.7019522899109796 Test RE 0.40046250332567757\n",
      "56 Train Loss 6.8807106 Test MSE 0.6407576203982245 Test RE 0.3826087961536667\n",
      "57 Train Loss 5.459422 Test MSE 0.4971027283176753 Test RE 0.33700095660342366\n",
      "58 Train Loss 4.753072 Test MSE 0.44309307352639915 Test RE 0.3181672987929076\n",
      "59 Train Loss 4.1241674 Test MSE 0.414882498395782 Test RE 0.30787230105292\n",
      "60 Train Loss 3.9706953 Test MSE 0.4248236349303327 Test RE 0.3115389813718888\n",
      "61 Train Loss 3.6741278 Test MSE 0.3657593864192582 Test RE 0.2890718490041527\n",
      "62 Train Loss 3.3278944 Test MSE 0.33273893653136394 Test RE 0.27571466878627077\n",
      "63 Train Loss 3.1794703 Test MSE 0.3139949899905572 Test RE 0.26783628937689824\n",
      "64 Train Loss 3.0143683 Test MSE 0.28643928035218974 Test RE 0.2558140227999064\n",
      "65 Train Loss 2.7856302 Test MSE 0.2647247059368988 Test RE 0.24592648219539112\n",
      "66 Train Loss 2.6283524 Test MSE 0.28428168110144436 Test RE 0.2548487441308919\n",
      "67 Train Loss 2.5241718 Test MSE 0.28669002574270525 Test RE 0.2559259665168297\n",
      "68 Train Loss 2.211382 Test MSE 0.24761074137688843 Test RE 0.237844328819982\n",
      "69 Train Loss 2.0625434 Test MSE 0.2337631323775398 Test RE 0.2310979373999438\n",
      "70 Train Loss 1.8797778 Test MSE 0.21781375449494972 Test RE 0.22307490198062305\n",
      "71 Train Loss 1.7558975 Test MSE 0.21985676943789076 Test RE 0.22411864152753158\n",
      "72 Train Loss 1.6443632 Test MSE 0.22607261048621657 Test RE 0.2272647266416084\n",
      "73 Train Loss 1.5232099 Test MSE 0.19502584062772932 Test RE 0.21108342850142037\n",
      "74 Train Loss 1.4217712 Test MSE 0.16704259830555807 Test RE 0.1953537168499828\n",
      "75 Train Loss 1.3207645 Test MSE 0.16591829516023954 Test RE 0.194695179376805\n",
      "76 Train Loss 1.240602 Test MSE 0.159294969696055 Test RE 0.1907695657398134\n",
      "77 Train Loss 1.2053083 Test MSE 0.15116055216819468 Test RE 0.18583490719306694\n",
      "78 Train Loss 1.190475 Test MSE 0.14395900009370138 Test RE 0.18135413866459624\n",
      "79 Train Loss 1.1623333 Test MSE 0.13958060166264413 Test RE 0.17857497306498216\n",
      "80 Train Loss 1.132344 Test MSE 0.14820585929587077 Test RE 0.1840097126582633\n",
      "81 Train Loss 1.0410877 Test MSE 0.10687194337838411 Test RE 0.15625715531702308\n",
      "82 Train Loss 0.93878955 Test MSE 0.09258421915235283 Test RE 0.1454375481250815\n",
      "83 Train Loss 0.79127485 Test MSE 0.07781666814995501 Test RE 0.13333506476088836\n",
      "84 Train Loss 0.6953715 Test MSE 0.06840020888301471 Test RE 0.1250077027484928\n",
      "85 Train Loss 0.63940024 Test MSE 0.0558749626488596 Test RE 0.1129839316016247\n",
      "86 Train Loss 0.6090371 Test MSE 0.050382624647215456 Test RE 0.10728733417657502\n",
      "87 Train Loss 0.5601279 Test MSE 0.04976004819721339 Test RE 0.10662240060559375\n",
      "88 Train Loss 0.5035182 Test MSE 0.0439240992905908 Test RE 0.1001750326325943\n",
      "89 Train Loss 0.4661713 Test MSE 0.03446341590227817 Test RE 0.08873342004825498\n",
      "90 Train Loss 0.45458964 Test MSE 0.03236271011788053 Test RE 0.08598654555698797\n",
      "91 Train Loss 0.4256994 Test MSE 0.030756898057374506 Test RE 0.08382611269095275\n",
      "92 Train Loss 0.41182172 Test MSE 0.029680914242141126 Test RE 0.08234679434976407\n",
      "93 Train Loss 0.404342 Test MSE 0.028150761813451018 Test RE 0.08019607925552971\n",
      "94 Train Loss 0.39727017 Test MSE 0.028635196149775704 Test RE 0.08088316581487501\n",
      "95 Train Loss 0.39363477 Test MSE 0.028662386139299834 Test RE 0.08092155721636254\n",
      "96 Train Loss 0.38811815 Test MSE 0.028593679421093338 Test RE 0.08082451033254374\n",
      "97 Train Loss 0.37747812 Test MSE 0.02856999964645647 Test RE 0.08079103610604113\n",
      "98 Train Loss 0.35686773 Test MSE 0.02620616312378585 Test RE 0.07737662454932101\n",
      "99 Train Loss 0.34755537 Test MSE 0.026267662711050053 Test RE 0.0774673635461146\n",
      "Training time: 93.94\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 55.619556 Test MSE 8.832080747748401 Test RE 1.4204945629297776\n",
      "1 Train Loss 55.13891 Test MSE 8.594648055980395 Test RE 1.4012709187099088\n",
      "2 Train Loss 52.45218 Test MSE 8.762400968411258 Test RE 1.4148800454091024\n",
      "3 Train Loss 51.26506 Test MSE 8.744126522017638 Test RE 1.413403872109038\n",
      "4 Train Loss 50.689438 Test MSE 8.67175223753453 Test RE 1.4075424150624527\n",
      "5 Train Loss 50.42032 Test MSE 8.64447500062508 Test RE 1.4053269396495323\n",
      "6 Train Loss 46.59937 Test MSE 8.335094114281022 Test RE 1.3799498838560036\n",
      "7 Train Loss 43.31611 Test MSE 8.64015425388301 Test RE 1.4049756851932922\n",
      "8 Train Loss 42.976288 Test MSE 8.377839636507808 Test RE 1.3834838118725357\n",
      "9 Train Loss 42.806877 Test MSE 8.36602054949983 Test RE 1.3825075884469704\n",
      "10 Train Loss 42.715233 Test MSE 8.420041151782861 Test RE 1.3869639317379983\n",
      "11 Train Loss 42.685356 Test MSE 8.454164992608879 Test RE 1.3897715594266309\n",
      "12 Train Loss 42.639603 Test MSE 8.421718392381177 Test RE 1.3871020638645604\n",
      "13 Train Loss 42.610558 Test MSE 8.433548821605376 Test RE 1.3880759894384145\n",
      "14 Train Loss 42.534134 Test MSE 8.385352013316975 Test RE 1.3841039552091083\n",
      "15 Train Loss 42.478996 Test MSE 8.368772282444239 Test RE 1.3827349354204306\n",
      "16 Train Loss 42.419983 Test MSE 8.420997799092135 Test RE 1.387042719810664\n",
      "17 Train Loss 42.369705 Test MSE 8.334590606367058 Test RE 1.3799082030920358\n",
      "18 Train Loss 41.737633 Test MSE 8.421127193190053 Test RE 1.387053376175643\n",
      "19 Train Loss 41.54647 Test MSE 8.370260928992781 Test RE 1.3828579111682497\n",
      "20 Train Loss 41.40745 Test MSE 8.366858649039068 Test RE 1.3825768358238233\n",
      "21 Train Loss 41.283955 Test MSE 8.477355088150572 Test RE 1.3916763521669333\n",
      "22 Train Loss 41.21554 Test MSE 8.449892506350936 Test RE 1.3894203402181722\n",
      "23 Train Loss 41.12809 Test MSE 8.323478139380578 Test RE 1.3789879839875048\n",
      "24 Train Loss 40.93688 Test MSE 8.466667038741077 Test RE 1.3907987791084437\n",
      "25 Train Loss 40.8174 Test MSE 8.411694169455005 Test RE 1.3862762965014228\n",
      "26 Train Loss 40.46097 Test MSE 8.387044255698717 Test RE 1.3842436107185307\n",
      "27 Train Loss 39.982872 Test MSE 8.130182100070225 Test RE 1.3628818151312847\n",
      "28 Train Loss 38.734097 Test MSE 7.9748668511614795 Test RE 1.3498011091515105\n",
      "29 Train Loss 37.609913 Test MSE 7.767830494336029 Test RE 1.3321647271892314\n",
      "30 Train Loss 37.10324 Test MSE 7.808580560696383 Test RE 1.3356544268766806\n",
      "31 Train Loss 36.66342 Test MSE 7.644663347927115 Test RE 1.3215610877697477\n",
      "32 Train Loss 36.360542 Test MSE 7.6254336093734505 Test RE 1.3198978833482211\n",
      "33 Train Loss 36.224552 Test MSE 7.572020980948173 Test RE 1.3152671235335982\n",
      "34 Train Loss 35.919556 Test MSE 7.489075886434759 Test RE 1.308043466563575\n",
      "35 Train Loss 35.70059 Test MSE 7.380820536462698 Test RE 1.298555102578554\n",
      "36 Train Loss 35.4491 Test MSE 6.975334851320021 Test RE 1.2623814106820967\n",
      "37 Train Loss 35.13549 Test MSE 6.946437485572559 Test RE 1.259763804622464\n",
      "38 Train Loss 34.76851 Test MSE 6.2914516842382175 Test RE 1.1989014712668842\n",
      "39 Train Loss 33.979614 Test MSE 6.044791974040295 Test RE 1.1751647070147253\n",
      "40 Train Loss 33.338257 Test MSE 5.971029103012043 Test RE 1.1679725991190966\n",
      "41 Train Loss 32.730095 Test MSE 5.840801725398178 Test RE 1.155165718899409\n",
      "42 Train Loss 32.50745 Test MSE 5.811040989960335 Test RE 1.1522189928050028\n",
      "43 Train Loss 32.083332 Test MSE 5.588618983132706 Test RE 1.129952821908823\n",
      "44 Train Loss 31.345013 Test MSE 5.527259362150337 Test RE 1.123732605531725\n",
      "45 Train Loss 30.184898 Test MSE 5.2444515010106985 Test RE 1.0946066872946187\n",
      "46 Train Loss 28.895607 Test MSE 5.116602994063565 Test RE 1.081182283397904\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "47 Train Loss 28.495232 Test MSE 5.208190627689046 Test RE 1.0908159914900148\n",
      "48 Train Loss 28.209482 Test MSE 5.227753767837817 Test RE 1.092862746872645\n",
      "49 Train Loss 28.036388 Test MSE 5.218447387754994 Test RE 1.0918895634580135\n",
      "50 Train Loss 27.91273 Test MSE 5.2270470687993384 Test RE 1.0927888766035383\n",
      "51 Train Loss 27.753702 Test MSE 5.304596954077722 Test RE 1.1008654860034477\n",
      "52 Train Loss 27.662596 Test MSE 5.326483463042453 Test RE 1.1031342069733983\n",
      "53 Train Loss 27.462627 Test MSE 5.319271603217654 Test RE 1.1023871527432014\n",
      "54 Train Loss 27.329323 Test MSE 5.4229168221862185 Test RE 1.11307526500498\n",
      "55 Train Loss 27.217411 Test MSE 5.346526152463543 Test RE 1.10520771537923\n",
      "56 Train Loss 27.157524 Test MSE 5.348365984558875 Test RE 1.1053978595651126\n",
      "57 Train Loss 27.02137 Test MSE 5.29207101894869 Test RE 1.0995649612188603\n",
      "58 Train Loss 26.920006 Test MSE 5.236932284167197 Test RE 1.093821711277627\n",
      "59 Train Loss 26.785007 Test MSE 5.056073344764237 Test RE 1.0747680386281548\n",
      "60 Train Loss 26.739506 Test MSE 4.9527637334628505 Test RE 1.0637311219484609\n",
      "61 Train Loss 26.679945 Test MSE 4.848958915016485 Test RE 1.0525247389838404\n",
      "62 Train Loss 26.60331 Test MSE 4.625102983334504 Test RE 1.0279423619897885\n",
      "63 Train Loss 26.465776 Test MSE 4.216433208429455 Test RE 0.9814782341274317\n",
      "64 Train Loss 26.115265 Test MSE 3.6169266822934527 Test RE 0.9090293654453862\n",
      "65 Train Loss 25.935404 Test MSE 3.5039084989972493 Test RE 0.8947144234061455\n",
      "66 Train Loss 25.356092 Test MSE 3.38528674348196 Test RE 0.8794391405987845\n",
      "67 Train Loss 25.181595 Test MSE 3.309362751037415 Test RE 0.8695213448422071\n",
      "68 Train Loss 25.039291 Test MSE 3.2082943829664496 Test RE 0.856140743991675\n",
      "69 Train Loss 24.94371 Test MSE 3.1646169384542016 Test RE 0.8502930599988\n",
      "70 Train Loss 24.6927 Test MSE 3.2314450426364387 Test RE 0.8592240951402059\n",
      "71 Train Loss 24.507381 Test MSE 3.2503694665188343 Test RE 0.8617363742841644\n",
      "72 Train Loss 24.211578 Test MSE 3.1679231446944387 Test RE 0.8507371122179435\n",
      "73 Train Loss 23.80119 Test MSE 3.2116288184219006 Test RE 0.8565855293432518\n",
      "74 Train Loss 23.660181 Test MSE 3.124745603884579 Test RE 0.8449196163118017\n",
      "75 Train Loss 23.506115 Test MSE 3.125686960573337 Test RE 0.8450468764057478\n",
      "76 Train Loss 23.345814 Test MSE 3.154964117472304 Test RE 0.8489952733654118\n",
      "77 Train Loss 23.13161 Test MSE 3.1211165115854627 Test RE 0.844428827223677\n",
      "78 Train Loss 22.810246 Test MSE 3.1281695611530504 Test RE 0.845382402241517\n",
      "79 Train Loss 22.605192 Test MSE 3.0226715584721937 Test RE 0.8310048148011561\n",
      "80 Train Loss 22.51177 Test MSE 3.0530101096733495 Test RE 0.835164799641242\n",
      "81 Train Loss 22.290043 Test MSE 3.1017656382200105 Test RE 0.8418070344016957\n",
      "82 Train Loss 22.16299 Test MSE 3.196284957546484 Test RE 0.8545368702481734\n",
      "83 Train Loss 21.967358 Test MSE 3.1166059950599405 Test RE 0.8438184387221432\n",
      "84 Train Loss 21.81032 Test MSE 3.233413539170262 Test RE 0.8594857616895528\n",
      "85 Train Loss 21.750154 Test MSE 3.2110884157213713 Test RE 0.8565134598991243\n",
      "86 Train Loss 21.689075 Test MSE 3.1462761627624483 Test RE 0.8478255108682954\n",
      "87 Train Loss 21.62688 Test MSE 3.214882182226261 Test RE 0.8570192779365088\n",
      "88 Train Loss 21.456097 Test MSE 3.1985572549677577 Test RE 0.8548405692247252\n",
      "89 Train Loss 21.4014 Test MSE 3.2976253005962124 Test RE 0.8679779919340037\n",
      "90 Train Loss 21.32142 Test MSE 3.236319019965888 Test RE 0.8598718333454834\n",
      "91 Train Loss 21.289993 Test MSE 3.2542102584492034 Test RE 0.8622453584141306\n",
      "92 Train Loss 21.248734 Test MSE 3.2345930917783794 Test RE 0.8596425180426777\n",
      "93 Train Loss 21.229435 Test MSE 3.2188384245309027 Test RE 0.8575464408711991\n",
      "94 Train Loss 21.19064 Test MSE 3.2399706035742883 Test RE 0.8603567992396786\n",
      "95 Train Loss 21.16419 Test MSE 3.2385167758842073 Test RE 0.8601637495121646\n",
      "96 Train Loss 21.137062 Test MSE 3.303478997171574 Test RE 0.8687480350007569\n",
      "97 Train Loss 21.098043 Test MSE 3.291099534032341 Test RE 0.8671187331809895\n",
      "98 Train Loss 21.062792 Test MSE 3.308095503555375 Test RE 0.8693548468900495\n",
      "99 Train Loss 20.978195 Test MSE 3.3090036194118118 Test RE 0.8694741633891647\n",
      "Training time: 92.00\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "1 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "2 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "3 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "4 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "5 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "6 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "7 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "8 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "9 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "10 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "11 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "12 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "13 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "14 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "15 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "16 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "17 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "18 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "19 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "20 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "21 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "22 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "23 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "24 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "25 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "26 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "27 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "28 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "29 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "30 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "31 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "32 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "33 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "34 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "35 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "36 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "37 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "38 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "39 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "40 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "41 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "42 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "43 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "44 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "45 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "46 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "47 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "48 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "49 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "50 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "51 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "52 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "53 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "54 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "55 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "56 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "57 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "58 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "59 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "60 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "61 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "62 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "63 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "64 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "65 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "66 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "67 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "68 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "69 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "70 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "71 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "72 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "73 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "74 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "75 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "76 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "77 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "78 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "79 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "80 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "81 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "82 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "83 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "84 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "85 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "86 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "87 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "88 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "89 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "90 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "91 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "92 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "93 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "94 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "95 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "96 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "97 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "98 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "99 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "Training time: 38.22\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 56.220306 Test MSE 8.604161445270217 Test RE 1.4020462354383219\n",
      "1 Train Loss 55.958164 Test MSE 8.846869478922402 Test RE 1.4216833272959148\n",
      "2 Train Loss 55.69814 Test MSE 8.581254360991402 Test RE 1.4001786393728122\n",
      "3 Train Loss 50.7341 Test MSE 8.565864836485156 Test RE 1.398922543686106\n",
      "4 Train Loss 49.694553 Test MSE 8.015051970518297 Test RE 1.3531976398052905\n",
      "5 Train Loss 49.282764 Test MSE 7.90556781563276 Test RE 1.343923643579603\n",
      "6 Train Loss 48.803246 Test MSE 7.920837608002696 Test RE 1.3452209276979261\n",
      "7 Train Loss 47.988792 Test MSE 7.568437430900933 Test RE 1.3149558537044443\n",
      "8 Train Loss 45.492455 Test MSE 7.088838407503271 Test RE 1.272610782460718\n",
      "9 Train Loss 39.807728 Test MSE 6.5055481360088425 Test RE 1.2191299697274616\n",
      "10 Train Loss 31.953964 Test MSE 5.429092348129662 Test RE 1.1137088603159433\n",
      "11 Train Loss 26.465855 Test MSE 3.306260325999579 Test RE 0.8691136746160035\n",
      "12 Train Loss 20.378696 Test MSE 3.242012423696835 Test RE 0.8606278537277429\n",
      "13 Train Loss 13.984135 Test MSE 2.471321170445168 Test RE 0.751402533528022\n",
      "14 Train Loss 11.587505 Test MSE 2.351749448962676 Test RE 0.7329993436294097\n",
      "15 Train Loss 9.540644 Test MSE 2.1501259943767463 Test RE 0.7008741059878949\n",
      "16 Train Loss 7.6490593 Test MSE 2.020925996835946 Test RE 0.6794903065812476\n",
      "17 Train Loss 7.02099 Test MSE 2.189239446225632 Test RE 0.7072202585052049\n",
      "18 Train Loss 6.289774 Test MSE 2.159806698167182 Test RE 0.7024501379623519\n",
      "19 Train Loss 5.6938267 Test MSE 1.942803242413946 Test RE 0.666227369458527\n",
      "20 Train Loss 5.293084 Test MSE 1.9251467799772324 Test RE 0.6631930768336342\n",
      "21 Train Loss 5.105824 Test MSE 1.923486760766688 Test RE 0.6629070854780498\n",
      "22 Train Loss 4.9005423 Test MSE 1.812993359976131 Test RE 0.643585375763983\n",
      "23 Train Loss 4.7102284 Test MSE 1.8780133583202987 Test RE 0.6550242805937043\n",
      "24 Train Loss 4.438034 Test MSE 1.8070851006710755 Test RE 0.6425358484080267\n",
      "25 Train Loss 4.372861 Test MSE 1.7412583827874095 Test RE 0.6307244553282845\n",
      "26 Train Loss 4.3057613 Test MSE 1.724497888030488 Test RE 0.6276815940213746\n",
      "27 Train Loss 4.2131534 Test MSE 1.713245651437262 Test RE 0.625630451858561\n",
      "28 Train Loss 4.160986 Test MSE 1.7093824076520991 Test RE 0.6249246782848752\n",
      "29 Train Loss 4.136511 Test MSE 1.6530344352702162 Test RE 0.6145383791478238\n",
      "30 Train Loss 4.1088924 Test MSE 1.6553948275903494 Test RE 0.6149769768542453\n",
      "31 Train Loss 3.9859428 Test MSE 1.5471497902397593 Test RE 0.5945306401924982\n",
      "32 Train Loss 3.9645383 Test MSE 1.5051926146419505 Test RE 0.5864136888802481\n",
      "33 Train Loss 3.9423716 Test MSE 1.4777002643883672 Test RE 0.5810335843506171\n",
      "34 Train Loss 3.890602 Test MSE 1.4138191364254198 Test RE 0.5683357667471978\n",
      "35 Train Loss 3.8433955 Test MSE 1.3541676551589896 Test RE 0.5562170257227157\n",
      "36 Train Loss 3.7939496 Test MSE 1.3339454512964128 Test RE 0.5520483237306956\n",
      "37 Train Loss 3.6523194 Test MSE 0.8527430721212219 Test RE 0.4413845834413747\n",
      "38 Train Loss 3.176552 Test MSE 0.3944709109072006 Test RE 0.3002033620895776\n",
      "39 Train Loss 2.160784 Test MSE 0.11960256155052222 Test RE 0.16530207333734792\n",
      "40 Train Loss 1.7151102 Test MSE 0.14177941400076371 Test RE 0.17997602215860606\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "41 Train Loss 1.3854713 Test MSE 0.12094877776172473 Test RE 0.16622976934822276\n",
      "42 Train Loss 1.1151031 Test MSE 0.11548642556655672 Test RE 0.16243272497650296\n",
      "43 Train Loss 0.9638766 Test MSE 0.11168563603220291 Test RE 0.15973744042509108\n",
      "44 Train Loss 0.7828014 Test MSE 0.08105556719550754 Test RE 0.13608162442185082\n",
      "45 Train Loss 0.61971164 Test MSE 0.07450472052213225 Test RE 0.13046678342250795\n",
      "46 Train Loss 0.55854416 Test MSE 0.06392445439617023 Test RE 0.12084858655236175\n",
      "47 Train Loss 0.48795468 Test MSE 0.0655818410460839 Test RE 0.12240519846691508\n",
      "48 Train Loss 0.37289464 Test MSE 0.06556583377209532 Test RE 0.12239025917050138\n",
      "49 Train Loss 0.30748662 Test MSE 0.048920978326645485 Test RE 0.10571962818138593\n",
      "50 Train Loss 0.2776657 Test MSE 0.03134273482753768 Test RE 0.08462067874035162\n",
      "51 Train Loss 0.24961898 Test MSE 0.03595496723894025 Test RE 0.0906332406952063\n",
      "52 Train Loss 0.23005965 Test MSE 0.029992084161933576 Test RE 0.0827773241486603\n",
      "53 Train Loss 0.21930145 Test MSE 0.024587625648732413 Test RE 0.07494908865805819\n",
      "54 Train Loss 0.20831859 Test MSE 0.023057063316018228 Test RE 0.07257884561629788\n",
      "55 Train Loss 0.18544821 Test MSE 0.019457778445838018 Test RE 0.06667372006532399\n",
      "56 Train Loss 0.15790716 Test MSE 0.01811884694700735 Test RE 0.06433885659221988\n",
      "57 Train Loss 0.14920622 Test MSE 0.017152905389404036 Test RE 0.06260037090067372\n",
      "58 Train Loss 0.14330399 Test MSE 0.016888203271978104 Test RE 0.06211547120403106\n",
      "59 Train Loss 0.1357298 Test MSE 0.017079692964765848 Test RE 0.06246663187083607\n",
      "60 Train Loss 0.12890579 Test MSE 0.018031823197773317 Test RE 0.06418416276267778\n",
      "61 Train Loss 0.12119467 Test MSE 0.018154350201303904 Test RE 0.06440186062654495\n",
      "62 Train Loss 0.11633826 Test MSE 0.016638774848826504 Test RE 0.06165506114570955\n",
      "63 Train Loss 0.10847459 Test MSE 0.013315562087062116 Test RE 0.055155371261169664\n",
      "64 Train Loss 0.10393693 Test MSE 0.012719934026101451 Test RE 0.05390766112922465\n",
      "65 Train Loss 0.10160327 Test MSE 0.011857282539342982 Test RE 0.052047592360377544\n",
      "66 Train Loss 0.100505695 Test MSE 0.011992020350203286 Test RE 0.05234247312056759\n",
      "67 Train Loss 0.09179599 Test MSE 0.01180147812308151 Test RE 0.0519249710540148\n",
      "68 Train Loss 0.072822765 Test MSE 0.008065757223759499 Test RE 0.04292700936691742\n",
      "69 Train Loss 0.06746648 Test MSE 0.0071419020730372675 Test RE 0.04039382743316111\n",
      "70 Train Loss 0.06290436 Test MSE 0.00691875614171061 Test RE 0.0397577750509623\n",
      "71 Train Loss 0.061697543 Test MSE 0.0066969637226882575 Test RE 0.03911533314394801\n",
      "72 Train Loss 0.060569953 Test MSE 0.0066548630201853165 Test RE 0.03899218932984698\n",
      "73 Train Loss 0.060128573 Test MSE 0.006783107664571585 Test RE 0.03936610218792419\n",
      "74 Train Loss 0.059638884 Test MSE 0.006724528786515937 Test RE 0.03919575088827247\n",
      "75 Train Loss 0.058343638 Test MSE 0.0062081740443669705 Test RE 0.03766083997502206\n",
      "76 Train Loss 0.057369135 Test MSE 0.006695907472068934 Test RE 0.039112248371073306\n",
      "77 Train Loss 0.05627407 Test MSE 0.0064818139358886325 Test RE 0.0384818852638957\n",
      "78 Train Loss 0.055340547 Test MSE 0.006215408106523147 Test RE 0.03768277569526803\n",
      "79 Train Loss 0.0542576 Test MSE 0.006149749954218053 Test RE 0.037483211137509895\n",
      "80 Train Loss 0.05232577 Test MSE 0.006193941462272583 Test RE 0.037617645425850645\n",
      "81 Train Loss 0.04977978 Test MSE 0.006295673232215065 Test RE 0.03792531088687294\n",
      "82 Train Loss 0.048027247 Test MSE 0.006064077047743134 Test RE 0.037221203861433266\n",
      "83 Train Loss 0.046528645 Test MSE 0.005896416807796215 Test RE 0.03670304938101571\n",
      "84 Train Loss 0.04552245 Test MSE 0.0050609440248301 Test RE 0.03400351610738029\n",
      "85 Train Loss 0.045024686 Test MSE 0.004884025241605823 Test RE 0.03340388733169451\n",
      "86 Train Loss 0.044592075 Test MSE 0.004859935164957597 Test RE 0.033321404450883524\n",
      "87 Train Loss 0.04409361 Test MSE 0.0044957476776531875 Test RE 0.032048597187442525\n",
      "88 Train Loss 0.042784035 Test MSE 0.004429271463996666 Test RE 0.0318107720419694\n",
      "89 Train Loss 0.04071064 Test MSE 0.003927465271440067 Test RE 0.02995464913827736\n",
      "90 Train Loss 0.033738546 Test MSE 0.002394122619511537 Test RE 0.02338736293152516\n",
      "91 Train Loss 0.030098379 Test MSE 0.0029602234899322247 Test RE 0.026005805526523037\n",
      "92 Train Loss 0.02900698 Test MSE 0.0026218473544482174 Test RE 0.0244743835548075\n",
      "93 Train Loss 0.028631972 Test MSE 0.0024799515155976976 Test RE 0.023802888124140033\n",
      "94 Train Loss 0.028021974 Test MSE 0.002298811197937757 Test RE 0.022917102768185926\n",
      "95 Train Loss 0.026987901 Test MSE 0.0021009128036721075 Test RE 0.02190847125902006\n",
      "96 Train Loss 0.026129248 Test MSE 0.0018741842467850465 Test RE 0.02069255898540879\n",
      "97 Train Loss 0.025168415 Test MSE 0.0017917071239424071 Test RE 0.02023212830643689\n",
      "98 Train Loss 0.023478558 Test MSE 0.0017386278647390802 Test RE 0.01993018720847646\n",
      "99 Train Loss 0.022746775 Test MSE 0.001733310287909467 Test RE 0.01989968572424364\n",
      "Training time: 90.98\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 58.596603 Test MSE 9.159200885691964 Test RE 1.4465613341957981\n",
      "1 Train Loss 56.66512 Test MSE 7.994772702553832 Test RE 1.3514846604240032\n",
      "2 Train Loss 51.150448 Test MSE 8.050140835636368 Test RE 1.3561564674818856\n",
      "3 Train Loss 51.064606 Test MSE 8.272732368194099 Test RE 1.3747779174515995\n",
      "4 Train Loss 50.08573 Test MSE 8.55467124229719 Test RE 1.3980082115925268\n",
      "5 Train Loss 45.956955 Test MSE 8.299277860029562 Test RE 1.376981840271845\n",
      "6 Train Loss 44.807983 Test MSE 7.814310823235658 Test RE 1.3361444165003473\n",
      "7 Train Loss 43.750504 Test MSE 8.141893514434933 Test RE 1.3638630680484227\n",
      "8 Train Loss 43.338722 Test MSE 8.197586003302861 Test RE 1.368519692461772\n",
      "9 Train Loss 42.794403 Test MSE 8.33725382421407 Test RE 1.380128651989828\n",
      "10 Train Loss 42.503647 Test MSE 8.250137152961278 Test RE 1.3728991764655472\n",
      "11 Train Loss 42.104767 Test MSE 8.389996475938826 Test RE 1.3844872145630893\n",
      "12 Train Loss 41.147995 Test MSE 8.298973560084775 Test RE 1.3769565959466161\n",
      "13 Train Loss 39.941032 Test MSE 8.001027876717687 Test RE 1.3520132632571953\n",
      "14 Train Loss 39.141174 Test MSE 7.928686148416351 Test RE 1.3458872339158083\n",
      "15 Train Loss 37.458694 Test MSE 7.164357118519945 Test RE 1.2793715037677946\n",
      "16 Train Loss 35.514427 Test MSE 6.869604441415931 Test RE 1.2527774446234898\n",
      "17 Train Loss 34.282173 Test MSE 6.321269384787195 Test RE 1.2017391493053922\n",
      "18 Train Loss 32.44462 Test MSE 6.147292215504031 Test RE 1.1850863320881935\n",
      "19 Train Loss 29.266407 Test MSE 6.082987300030009 Test RE 1.1788716267271135\n",
      "20 Train Loss 23.960968 Test MSE 4.2092592904592365 Test RE 0.9806429260027084\n",
      "21 Train Loss 22.952684 Test MSE 4.190472427539579 Test RE 0.9784520646573434\n",
      "22 Train Loss 21.440542 Test MSE 4.182089604299515 Test RE 0.9774729033609284\n",
      "23 Train Loss 19.123964 Test MSE 3.7289991255285533 Test RE 0.9230053132563745\n",
      "24 Train Loss 18.162998 Test MSE 3.435374061488509 Test RE 0.885921163203984\n",
      "25 Train Loss 17.638935 Test MSE 3.445982114459908 Test RE 0.8872879219212138\n",
      "26 Train Loss 16.70883 Test MSE 3.560104053344555 Test RE 0.9018605832575868\n",
      "27 Train Loss 15.940725 Test MSE 3.6275791899654743 Test RE 0.910367009769472\n",
      "28 Train Loss 15.34821 Test MSE 3.5920087913382126 Test RE 0.9058926911649298\n",
      "29 Train Loss 15.065683 Test MSE 3.5098663471603477 Test RE 0.8954747612772284\n",
      "30 Train Loss 14.843102 Test MSE 3.403825994643533 Test RE 0.8818439416059011\n",
      "31 Train Loss 14.356699 Test MSE 3.462572340174155 Test RE 0.8894212226301722\n",
      "32 Train Loss 14.095922 Test MSE 3.3955120852399507 Test RE 0.8807663229202005\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33 Train Loss 14.018096 Test MSE 3.448440797257521 Test RE 0.8876044021705024\n",
      "34 Train Loss 13.798051 Test MSE 3.4190009726415953 Test RE 0.8838074792075195\n",
      "35 Train Loss 13.6588745 Test MSE 3.381716630837896 Test RE 0.8789752914112785\n",
      "36 Train Loss 13.450244 Test MSE 3.3915947002726172 Test RE 0.8802581084904538\n",
      "37 Train Loss 13.081709 Test MSE 3.3313089785874803 Test RE 0.8723997208575247\n",
      "38 Train Loss 12.99183 Test MSE 3.307695511278036 Test RE 0.8693022870808291\n",
      "39 Train Loss 12.903177 Test MSE 3.313206446286411 Test RE 0.8700261558020842\n",
      "40 Train Loss 12.779345 Test MSE 3.2862700401361784 Test RE 0.8664822768795363\n",
      "41 Train Loss 12.6457405 Test MSE 3.300860422397161 Test RE 0.8684036506896798\n",
      "42 Train Loss 12.414947 Test MSE 3.2764596270002193 Test RE 0.8651879671287669\n",
      "43 Train Loss 12.274231 Test MSE 3.264500108078979 Test RE 0.8636074972753472\n",
      "44 Train Loss 12.081123 Test MSE 3.2627245538272702 Test RE 0.8633726082463546\n",
      "45 Train Loss 11.8763485 Test MSE 3.191177877669755 Test RE 0.8538539003628158\n",
      "46 Train Loss 11.7702265 Test MSE 3.152790425944772 Test RE 0.8487027546962039\n",
      "47 Train Loss 11.373398 Test MSE 3.271951128982166 Test RE 0.8645925010662893\n",
      "48 Train Loss 11.04858 Test MSE 3.2898722454291316 Test RE 0.8669570388657762\n",
      "49 Train Loss 10.865755 Test MSE 3.261559226485312 Test RE 0.8632184117239663\n",
      "50 Train Loss 10.661301 Test MSE 3.182965443206779 Test RE 0.8527545043508008\n",
      "51 Train Loss 10.459699 Test MSE 3.1656635379162785 Test RE 0.8504336524797518\n",
      "52 Train Loss 10.344996 Test MSE 3.113846345493695 Test RE 0.8434447695922924\n",
      "53 Train Loss 10.2612095 Test MSE 3.089089302664735 Test RE 0.8400851194270845\n",
      "54 Train Loss 10.117063 Test MSE 3.045892525191248 Test RE 0.8341907077749136\n",
      "55 Train Loss 10.000919 Test MSE 3.013954479321628 Test RE 0.8298056827017544\n",
      "56 Train Loss 9.931978 Test MSE 2.9784707516051028 Test RE 0.82490650838808\n",
      "57 Train Loss 9.696663 Test MSE 2.956293250928334 Test RE 0.8218296699333408\n",
      "58 Train Loss 9.441183 Test MSE 2.855782049043558 Test RE 0.8077381399311531\n",
      "59 Train Loss 9.370275 Test MSE 2.875459530368941 Test RE 0.8105161822284006\n",
      "60 Train Loss 9.322064 Test MSE 2.8936773275773167 Test RE 0.8130796865055877\n",
      "61 Train Loss 9.220019 Test MSE 2.8684628380419674 Test RE 0.8095294901560725\n",
      "62 Train Loss 9.075431 Test MSE 2.898891531716141 Test RE 0.8138119131598438\n",
      "63 Train Loss 8.974682 Test MSE 2.9002009926294217 Test RE 0.8139956962653131\n",
      "64 Train Loss 8.899835 Test MSE 2.8760882131105214 Test RE 0.8106047819254456\n",
      "65 Train Loss 8.77326 Test MSE 2.850450182641595 Test RE 0.8069837469353239\n",
      "66 Train Loss 8.564524 Test MSE 2.7755889073778266 Test RE 0.7963163483372807\n",
      "67 Train Loss 8.208029 Test MSE 2.7626458287284823 Test RE 0.7944574943833999\n",
      "68 Train Loss 8.03916 Test MSE 2.7804299392197493 Test RE 0.7970104917198216\n",
      "69 Train Loss 7.636471 Test MSE 2.621559120079362 Test RE 0.7739054202304273\n",
      "70 Train Loss 7.290304 Test MSE 2.6055322661546936 Test RE 0.7715361652354306\n",
      "71 Train Loss 7.022884 Test MSE 2.5427035940789153 Test RE 0.7621771573932943\n",
      "72 Train Loss 6.7308545 Test MSE 2.5091764565452137 Test RE 0.7571355918885159\n",
      "73 Train Loss 6.5889025 Test MSE 2.4807753617079795 Test RE 0.7528384298841987\n",
      "74 Train Loss 6.4781904 Test MSE 2.4810044208345174 Test RE 0.7528731852545176\n",
      "75 Train Loss 6.3193865 Test MSE 2.3981409870109953 Test RE 0.7401937541639496\n",
      "76 Train Loss 6.174101 Test MSE 2.3498674239107724 Test RE 0.7327059877119262\n",
      "77 Train Loss 6.0369344 Test MSE 2.3012070792191834 Test RE 0.7250799747764998\n",
      "78 Train Loss 5.853413 Test MSE 2.2394628209216534 Test RE 0.7152864344951189\n",
      "79 Train Loss 5.60276 Test MSE 2.2196636410921275 Test RE 0.7121174768231886\n",
      "80 Train Loss 5.499092 Test MSE 2.2216319428538784 Test RE 0.7124331443555617\n",
      "81 Train Loss 5.4149594 Test MSE 2.2364418909703287 Test RE 0.7148038278181892\n",
      "82 Train Loss 5.278623 Test MSE 2.2311823607448265 Test RE 0.7139628166305908\n",
      "83 Train Loss 5.191693 Test MSE 2.2230478776758935 Test RE 0.7126601392503069\n",
      "84 Train Loss 5.0848045 Test MSE 2.1856295864012507 Test RE 0.7066369465074674\n",
      "85 Train Loss 4.907937 Test MSE 2.152800494580067 Test RE 0.70130987240258\n",
      "86 Train Loss 4.842378 Test MSE 2.1703316426391055 Test RE 0.704159611217553\n",
      "87 Train Loss 4.7489376 Test MSE 2.1293813490517213 Test RE 0.6974848570851854\n",
      "88 Train Loss 4.665196 Test MSE 2.1643779190293286 Test RE 0.7031931112752001\n",
      "89 Train Loss 4.6354156 Test MSE 2.1741353236790033 Test RE 0.7047763893194723\n",
      "90 Train Loss 4.6118946 Test MSE 2.1729917717648877 Test RE 0.7045910157628369\n",
      "91 Train Loss 4.575043 Test MSE 2.1569585981635915 Test RE 0.7019868307060818\n",
      "92 Train Loss 4.5258455 Test MSE 2.1401004011765794 Test RE 0.6992381812026741\n",
      "93 Train Loss 4.456408 Test MSE 2.1273507335426287 Test RE 0.6971522108478997\n",
      "94 Train Loss 4.4062824 Test MSE 2.10074895018767 Test RE 0.6927796752413813\n",
      "95 Train Loss 4.369553 Test MSE 2.0846239695880775 Test RE 0.6901157257333881\n",
      "96 Train Loss 4.358051 Test MSE 2.0807401171705546 Test RE 0.6894725504533118\n",
      "97 Train Loss 4.3301234 Test MSE 2.0618982813495013 Test RE 0.6863437426396449\n",
      "98 Train Loss 4.3172436 Test MSE 2.07917869124787 Test RE 0.6892138054164166\n",
      "99 Train Loss 4.2852097 Test MSE 2.074926901728614 Test RE 0.6885087453613866\n",
      "Training time: 84.63\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 56.888855 Test MSE 8.713653039791593 Test RE 1.410938850053958\n",
      "1 Train Loss 56.884552 Test MSE 8.74172767986953 Test RE 1.4132099839406957\n",
      "2 Train Loss 56.72988 Test MSE 9.10243798056857 Test RE 1.4420719339457564\n",
      "3 Train Loss 54.099743 Test MSE 7.670330555166975 Test RE 1.3237778209299944\n",
      "4 Train Loss nan Test MSE nan Test RE nan\n",
      "NAN BREAK!\n",
      "Training time: 3.56\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smartlab/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio5.py:493: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.\n",
      "  narr = np.asanyarray(source)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 58.667637 Test MSE 8.681059262170749 Test RE 1.4082975403550275\n",
      "1 Train Loss 58.138466 Test MSE 8.584567811402113 Test RE 1.4004489363788155\n",
      "2 Train Loss 57.87563 Test MSE 8.203334235856993 Test RE 1.3689994184992433\n",
      "3 Train Loss 52.968864 Test MSE 8.878531511213824 Test RE 1.424225084064785\n",
      "4 Train Loss 45.34414 Test MSE 8.649828065225282 Test RE 1.4057619945044337\n",
      "5 Train Loss 43.540394 Test MSE 8.208855768592713 Test RE 1.3694600667770516\n",
      "6 Train Loss 43.25643 Test MSE 8.258429074562065 Test RE 1.3735889294451278\n",
      "7 Train Loss 43.116653 Test MSE 8.226203065801068 Test RE 1.3709063032523858\n",
      "8 Train Loss 43.009193 Test MSE 8.25347852446415 Test RE 1.3731771658924836\n",
      "9 Train Loss 42.919724 Test MSE 8.297038023275027 Test RE 1.3767960155102938\n",
      "10 Train Loss 42.862373 Test MSE 8.33825644423284 Test RE 1.3802116351306266\n",
      "11 Train Loss 42.80237 Test MSE 8.285841089498668 Test RE 1.3758667020460735\n",
      "12 Train Loss 42.79191 Test MSE 8.282644367687002 Test RE 1.3756012680756973\n",
      "13 Train Loss 42.69034 Test MSE 8.304776452444028 Test RE 1.3774379165924702\n",
      "14 Train Loss 42.654778 Test MSE 8.328837508997225 Test RE 1.3794318679565412\n",
      "15 Train Loss 42.642418 Test MSE 8.340654868042453 Test RE 1.380410123535102\n",
      "16 Train Loss 42.62809 Test MSE 8.344433575306086 Test RE 1.3807227833432982\n",
      "17 Train Loss 42.610367 Test MSE 8.354099015027181 Test RE 1.3815222043258004\n",
      "18 Train Loss 42.598854 Test MSE 8.3516223489258 Test RE 1.3813174052903914\n",
      "19 Train Loss 42.589993 Test MSE 8.346419253024111 Test RE 1.3808870549715258\n",
      "20 Train Loss 42.57716 Test MSE 8.348505389444103 Test RE 1.3810596163243603\n",
      "21 Train Loss 42.567886 Test MSE 8.367125174468237 Test RE 1.3825988565705258\n",
      "22 Train Loss 42.513573 Test MSE 8.365652546204235 Test RE 1.3824771813411827\n",
      "23 Train Loss 42.430355 Test MSE 8.364281672228243 Test RE 1.3823639038874767\n",
      "24 Train Loss 42.37632 Test MSE 8.354478313804432 Test RE 1.381553566399327\n",
      "25 Train Loss 42.306923 Test MSE 8.417305311148262 Test RE 1.3867385872002949\n",
      "26 Train Loss 42.22171 Test MSE 8.370465643075072 Test RE 1.3828748215584836\n",
      "27 Train Loss 41.995068 Test MSE 8.378421350689774 Test RE 1.383531842046132\n",
      "28 Train Loss 41.764168 Test MSE 8.353760863923366 Test RE 1.3814942439206765\n",
      "29 Train Loss 41.41802 Test MSE 8.43894053234704 Test RE 1.388519628799751\n",
      "30 Train Loss 41.216995 Test MSE 8.404084856438544 Test RE 1.3856491340813273\n",
      "31 Train Loss 41.01188 Test MSE 8.440037428107194 Test RE 1.3886098659449904\n",
      "32 Train Loss 40.766533 Test MSE 8.465024056073176 Test RE 1.3906638281800237\n",
      "33 Train Loss 40.288033 Test MSE 8.694153305743281 Test RE 1.4093592403415156\n",
      "34 Train Loss 39.94085 Test MSE 8.661493250619424 Test RE 1.406709582660447\n",
      "35 Train Loss 39.76835 Test MSE 8.707954471839132 Test RE 1.4104774106401992\n",
      "36 Train Loss 39.507614 Test MSE 8.663549987858167 Test RE 1.4068765896743913\n",
      "37 Train Loss 39.4147 Test MSE 8.653986847964115 Test RE 1.406099894641372\n",
      "38 Train Loss 39.34045 Test MSE 8.587384865525094 Test RE 1.4006786984901594\n",
      "39 Train Loss 39.299965 Test MSE 8.497337382852486 Test RE 1.3933155732830562\n",
      "40 Train Loss 39.242016 Test MSE 8.539507214913364 Test RE 1.3967686059385822\n",
      "41 Train Loss 39.20412 Test MSE 8.48496567680568 Test RE 1.3923009042870111\n",
      "42 Train Loss 39.175007 Test MSE 8.46298184085199 Test RE 1.3904960669231579\n",
      "43 Train Loss 39.11682 Test MSE 8.381584227409894 Test RE 1.3837929609113206\n",
      "44 Train Loss 39.0774 Test MSE 8.388727245162114 Test RE 1.3843824886403238\n",
      "45 Train Loss 39.04442 Test MSE 8.33949312851693 Test RE 1.3803139840308432\n",
      "46 Train Loss 38.98922 Test MSE 8.371038252017671 Test RE 1.382922120775835\n",
      "47 Train Loss 38.88382 Test MSE 8.246722907943667 Test RE 1.372615066083657\n",
      "48 Train Loss 38.757137 Test MSE 8.254564748807054 Test RE 1.3732675235148837\n",
      "49 Train Loss 38.64428 Test MSE 8.074361779086065 Test RE 1.3581951099974827\n",
      "50 Train Loss 38.557194 Test MSE 8.0910848798254 Test RE 1.35960088582744\n",
      "51 Train Loss 38.468506 Test MSE 8.111891417303099 Test RE 1.361347896534464\n",
      "52 Train Loss 38.378757 Test MSE 8.092417400066674 Test RE 1.3597128375089178\n",
      "53 Train Loss 38.321266 Test MSE 8.099525494692276 Test RE 1.3603098683851516\n",
      "54 Train Loss 38.135654 Test MSE 8.13942496472533 Test RE 1.363656296797577\n",
      "55 Train Loss 38.062088 Test MSE 8.133063032889003 Test RE 1.363123262563513\n",
      "56 Train Loss 38.014446 Test MSE 8.12019173559902 Test RE 1.3620442034267786\n",
      "57 Train Loss 37.948807 Test MSE 8.114108165950855 Test RE 1.3615338926119718\n",
      "58 Train Loss 37.914314 Test MSE 8.11841228905698 Test RE 1.3618949570960257\n",
      "59 Train Loss 37.853287 Test MSE 8.118316787320722 Test RE 1.361886946680264\n",
      "60 Train Loss 37.807175 Test MSE 8.064175424390884 Test RE 1.3573381120244852\n",
      "61 Train Loss 37.75031 Test MSE 7.991168853387829 Test RE 1.3511800178749067\n",
      "62 Train Loss 37.678577 Test MSE 8.02774626255025 Test RE 1.3542688174999875\n",
      "63 Train Loss 37.43348 Test MSE 7.910808392279314 Test RE 1.3443690112133548\n",
      "64 Train Loss 37.246838 Test MSE 7.865682026818009 Test RE 1.3405291222709654\n",
      "65 Train Loss 37.080894 Test MSE 7.849227282574239 Test RE 1.339126217090663\n",
      "66 Train Loss 36.949932 Test MSE 7.728512654237815 Test RE 1.3287889913723498\n",
      "67 Train Loss 36.782692 Test MSE 7.632903211928966 Test RE 1.32054418753763\n",
      "68 Train Loss 36.663677 Test MSE 7.618688255319524 Test RE 1.3193139724014384\n",
      "69 Train Loss 36.35966 Test MSE 7.612015180735145 Test RE 1.318736063916502\n",
      "70 Train Loss 35.90044 Test MSE 7.3912735258280655 Test RE 1.2994743079763968\n",
      "71 Train Loss 35.091553 Test MSE 7.408431506883279 Test RE 1.3009817228270848\n",
      "72 Train Loss 34.79256 Test MSE 7.468340018268854 Test RE 1.3062313459321444\n",
      "73 Train Loss 34.431602 Test MSE 7.479438877271956 Test RE 1.3072015946869127\n",
      "74 Train Loss 34.235596 Test MSE 7.499812916493479 Test RE 1.3089807966135942\n",
      "75 Train Loss 34.08055 Test MSE 7.539968341812764 Test RE 1.3124803846223354\n",
      "76 Train Loss 33.89406 Test MSE 7.55451348504973 Test RE 1.3137457093951745\n",
      "77 Train Loss 33.744167 Test MSE 7.581292260342842 Test RE 1.31607209215483\n",
      "78 Train Loss 33.613968 Test MSE 7.527666381277531 Test RE 1.3114092479548625\n",
      "79 Train Loss 33.548714 Test MSE 7.593882743102024 Test RE 1.3171644592972134\n",
      "80 Train Loss 33.441933 Test MSE 7.594501937322266 Test RE 1.3172181580454858\n",
      "81 Train Loss 33.35925 Test MSE 7.665315558430603 Test RE 1.3233449955849717\n",
      "82 Train Loss 33.28386 Test MSE 7.620092214780336 Test RE 1.3194355273165437\n",
      "83 Train Loss 33.06012 Test MSE 7.660002161738337 Test RE 1.322886261968804\n",
      "84 Train Loss 32.44834 Test MSE 7.699342052180891 Test RE 1.3262789211064439\n",
      "85 Train Loss 31.749382 Test MSE 7.566189976741436 Test RE 1.3147606005588983\n",
      "86 Train Loss 30.569504 Test MSE 7.286310386205615 Test RE 1.2902144278303962\n",
      "87 Train Loss 29.531929 Test MSE 7.019900274710804 Test RE 1.2664076682132428\n",
      "88 Train Loss 28.864809 Test MSE 6.995550418412502 Test RE 1.2642093725915857\n",
      "89 Train Loss 28.077625 Test MSE 6.674358479491084 Test RE 1.2348460723382269\n",
      "90 Train Loss 27.795212 Test MSE 6.5897849666888595 Test RE 1.2269975114085818\n",
      "91 Train Loss 27.584248 Test MSE 6.543734035519873 Test RE 1.2227027247757691\n",
      "92 Train Loss 27.280155 Test MSE 6.55473154611111 Test RE 1.223729740937558\n",
      "93 Train Loss 26.955185 Test MSE 6.372079099591784 Test RE 1.2065592108975374\n",
      "94 Train Loss 26.698793 Test MSE 6.19159799910629 Test RE 1.1893493399574944\n",
      "95 Train Loss 26.377945 Test MSE 6.249828893580945 Test RE 1.1949290626703108\n",
      "96 Train Loss 26.172768 Test MSE 6.188230984253876 Test RE 1.1890259096036204\n",
      "97 Train Loss 25.955013 Test MSE 6.1497786070547855 Test RE 1.1853259734334178\n",
      "98 Train Loss 25.750881 Test MSE 6.21168735751888 Test RE 1.1912772683721042\n",
      "99 Train Loss 25.428871 Test MSE 6.149513711452758 Test RE 1.185300444789329\n",
      "Training time: 85.14\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 56.87461 Test MSE 8.581138204485503 Test RE 1.4001691628789386\n",
      "1 Train Loss 56.510048 Test MSE 8.720739409961148 Test RE 1.4115124558054222\n",
      "2 Train Loss 55.306023 Test MSE 7.000399765538237 Test RE 1.2646474745028435\n",
      "3 Train Loss 46.897064 Test MSE 8.022291359912911 Test RE 1.353808622342108\n",
      "4 Train Loss 43.933952 Test MSE 8.322723839837876 Test RE 1.3789254984763264\n",
      "5 Train Loss 43.419624 Test MSE 8.153486501073896 Test RE 1.3648337035655964\n",
      "6 Train Loss 43.137188 Test MSE 8.357389448922364 Test RE 1.3817942480227354\n",
      "7 Train Loss 42.849068 Test MSE 8.16670070533522 Test RE 1.365939236145416\n",
      "8 Train Loss 42.847622 Test MSE 8.163047204683298 Test RE 1.3656336648789593\n",
      "9 Train Loss 42.822235 Test MSE 8.162111629144539 Test RE 1.3655554042717297\n",
      "10 Train Loss 42.821384 Test MSE 8.16408887923607 Test RE 1.3657207953598998\n",
      "11 Train Loss 42.80039 Test MSE 8.179888221632508 Test RE 1.3670416446029332\n",
      "12 Train Loss 42.771904 Test MSE 8.207051213385892 Test RE 1.3693095340966492\n",
      "13 Train Loss 42.64554 Test MSE 8.220434833150254 Test RE 1.370425577627932\n",
      "14 Train Loss 42.426178 Test MSE 8.173654468769344 Test RE 1.3665206457986978\n",
      "15 Train Loss 42.1164 Test MSE 8.229750128918267 Test RE 1.3712018324942825\n",
      "16 Train Loss 41.650967 Test MSE 7.862960267877053 Test RE 1.3402971708141815\n",
      "17 Train Loss 39.387077 Test MSE 7.9218306287765765 Test RE 1.3453052489861332\n",
      "18 Train Loss 39.110382 Test MSE 7.846639117902089 Test RE 1.338905420520864\n",
      "19 Train Loss 38.330116 Test MSE 7.721085024225173 Test RE 1.3281503092641729\n",
      "20 Train Loss 37.22902 Test MSE 7.229531270302623 Test RE 1.2851775501073817\n",
      "21 Train Loss 34.135292 Test MSE 6.337276106728491 Test RE 1.2032597096026998\n",
      "22 Train Loss 31.210419 Test MSE 6.63125483242218 Test RE 1.2308522365775572\n",
      "23 Train Loss 28.49261 Test MSE 6.539316756913479 Test RE 1.2222899688216207\n",
      "24 Train Loss 27.795795 Test MSE 6.343995936330646 Test RE 1.2038974881818243\n",
      "25 Train Loss 26.271347 Test MSE 6.687148074596152 Test RE 1.236028629638343\n",
      "26 Train Loss 25.696493 Test MSE 6.52520060521475 Test RE 1.2209700027827324\n",
      "27 Train Loss 25.169548 Test MSE 6.659399477665365 Test RE 1.2334614878483854\n",
      "28 Train Loss 24.701092 Test MSE 6.56417977751906 Test RE 1.2246113878966622\n",
      "29 Train Loss 24.174835 Test MSE 6.586874656515591 Test RE 1.2267265361254214\n",
      "30 Train Loss 23.873852 Test MSE 6.506761036181558 Test RE 1.219243612267999\n",
      "31 Train Loss 23.061646 Test MSE 6.417282239845899 Test RE 1.210831276547352\n",
      "32 Train Loss 21.711346 Test MSE 6.280353637379733 Test RE 1.1978435804789764\n",
      "33 Train Loss 20.657486 Test MSE 5.907980424881207 Test RE 1.1617898665452402\n",
      "34 Train Loss 19.902874 Test MSE 5.226413399224608 Test RE 1.0927226357564945\n",
      "35 Train Loss 19.480156 Test MSE 4.8773995616621155 Test RE 1.0556069179845182\n",
      "36 Train Loss 17.456415 Test MSE 3.648318197601195 Test RE 0.9129656023386974\n",
      "37 Train Loss 15.400456 Test MSE 3.6404357036416255 Test RE 0.911978800093096\n",
      "38 Train Loss 14.302938 Test MSE 3.680853104103324 Test RE 0.9170273810498978\n",
      "39 Train Loss 13.611547 Test MSE 3.4998443558322796 Test RE 0.8941953883725703\n",
      "40 Train Loss 12.757181 Test MSE 3.5622463437179057 Test RE 0.902131889468309\n",
      "41 Train Loss 12.201485 Test MSE 3.5752895905552444 Test RE 0.9037819687184456\n",
      "42 Train Loss 11.343956 Test MSE 3.3700691469885187 Test RE 0.877460279617375\n",
      "43 Train Loss 11.078475 Test MSE 3.2831442700495135 Test RE 0.8660700970081895\n",
      "44 Train Loss 10.975052 Test MSE 3.1992715444470745 Test RE 0.854936013747179\n",
      "45 Train Loss 10.945957 Test MSE 3.115118028406896 Test RE 0.8436169818170184\n",
      "46 Train Loss 10.761289 Test MSE 2.927531554635014 Test RE 0.8178221195662864\n",
      "47 Train Loss 10.184856 Test MSE 2.2654736027368387 Test RE 0.7194283758472926\n",
      "48 Train Loss 9.792801 Test MSE 2.317368479858717 Test RE 0.7276216420511463\n",
      "49 Train Loss 9.039013 Test MSE 2.4053143574770974 Test RE 0.7412999692199955\n",
      "50 Train Loss 8.717359 Test MSE 2.256593058069921 Test RE 0.7180169294329647\n",
      "51 Train Loss 8.372903 Test MSE 2.140285424923346 Test RE 0.6992684070911194\n",
      "52 Train Loss 8.184483 Test MSE 2.144690633303857 Test RE 0.6999876662101348\n",
      "53 Train Loss 8.140032 Test MSE 2.1233355057730052 Test RE 0.6964939867580856\n",
      "54 Train Loss 8.10058 Test MSE 2.079317109416252 Test RE 0.6892367467165684\n",
      "55 Train Loss 8.024839 Test MSE 2.1380915323825236 Test RE 0.6989099238107586\n",
      "56 Train Loss 7.916548 Test MSE 2.1079228687422162 Test RE 0.6939615654399021\n",
      "57 Train Loss 7.669585 Test MSE 2.086517574738432 Test RE 0.6904290940149345\n",
      "58 Train Loss 7.5787787 Test MSE 2.042792777821799 Test RE 0.6831565193594478\n",
      "59 Train Loss 7.534705 Test MSE 2.093866740511868 Test RE 0.6916439454448413\n",
      "60 Train Loss 7.460121 Test MSE 2.079787137907616 Test RE 0.6893146431088639\n",
      "61 Train Loss 7.416214 Test MSE 2.074260322506384 Test RE 0.6883981432748538\n",
      "62 Train Loss 7.343734 Test MSE 2.121559296696682 Test RE 0.6962026108158579\n",
      "63 Train Loss 7.2677984 Test MSE 2.116782624597173 Test RE 0.6954184221407288\n",
      "64 Train Loss 6.9355164 Test MSE 2.0692435883560076 Test RE 0.6875651713713051\n",
      "65 Train Loss 6.52485 Test MSE 1.933047116159795 Test RE 0.6645524754935263\n",
      "66 Train Loss 6.4146233 Test MSE 1.9773783093389943 Test RE 0.6721294784422925\n",
      "67 Train Loss 6.364477 Test MSE 1.9802069027387816 Test RE 0.6726100393912694\n",
      "68 Train Loss 6.329564 Test MSE 1.983114125790868 Test RE 0.6731036015095588\n",
      "69 Train Loss 6.292196 Test MSE 1.9789959630686604 Test RE 0.6724043500928923\n",
      "70 Train Loss 6.215644 Test MSE 1.9510897114922223 Test RE 0.6676466583977706\n",
      "71 Train Loss 6.082179 Test MSE 2.025693209431903 Test RE 0.6802912677994725\n",
      "72 Train Loss 6.0339036 Test MSE 2.02930863721815 Test RE 0.6808980841425348\n",
      "73 Train Loss 5.9949675 Test MSE 2.0362747959625054 Test RE 0.6820657676594692\n",
      "74 Train Loss 5.9719696 Test MSE 2.0013314627917325 Test RE 0.676188175124805\n",
      "75 Train Loss 5.9417677 Test MSE 2.0102777783289927 Test RE 0.6776978319423641\n",
      "76 Train Loss 5.8382874 Test MSE 1.99909395045611 Test RE 0.6758100762116646\n",
      "77 Train Loss 5.7997303 Test MSE 1.99573194358019 Test RE 0.6752415601091101\n",
      "78 Train Loss 5.758276 Test MSE 1.953634767704168 Test RE 0.6680819650128271\n",
      "79 Train Loss 5.7307034 Test MSE 1.9876510626594508 Test RE 0.6738731194804133\n",
      "80 Train Loss 5.7065897 Test MSE 1.9843344500408016 Test RE 0.6733106693508485\n",
      "81 Train Loss 5.6492147 Test MSE 1.9994635616014802 Test RE 0.675872548361009\n",
      "82 Train Loss 5.6129723 Test MSE 2.00803314932268 Test RE 0.6773193755187642\n",
      "83 Train Loss 5.5865374 Test MSE 2.0202442126211206 Test RE 0.6793756797110477\n",
      "84 Train Loss 5.566473 Test MSE 2.0156058247264075 Test RE 0.6785953238645643\n",
      "85 Train Loss 5.5570216 Test MSE 2.0133362004380886 Test RE 0.6782131583095856\n",
      "86 Train Loss 5.544182 Test MSE 2.021432795373005 Test RE 0.6795755009691218\n",
      "87 Train Loss 5.5307083 Test MSE 2.035767127731654 Test RE 0.6819807386872306\n",
      "88 Train Loss 5.510235 Test MSE 2.0041230622533717 Test RE 0.6766596084639358\n",
      "89 Train Loss 5.500558 Test MSE 1.9975404610169882 Test RE 0.6755474402668311\n",
      "90 Train Loss 5.4920864 Test MSE 1.9945152472626595 Test RE 0.6750356985010683\n",
      "91 Train Loss 5.485692 Test MSE 2.004317441393711 Test RE 0.6766924221484849\n",
      "92 Train Loss 5.4792833 Test MSE 1.9917649817227712 Test RE 0.6745701297715232\n",
      "93 Train Loss 5.4751616 Test MSE 1.9833875160039431 Test RE 0.6731499966193708\n",
      "94 Train Loss 5.465726 Test MSE 1.9868804108916496 Test RE 0.6737424698236063\n",
      "95 Train Loss 5.4625225 Test MSE 1.9910167584507823 Test RE 0.6744434138977501\n",
      "96 Train Loss 5.458337 Test MSE 1.9983215548672257 Test RE 0.6756795062721267\n",
      "97 Train Loss 5.4499283 Test MSE 1.9955802889217789 Test RE 0.6752159039898238\n",
      "98 Train Loss 5.437803 Test MSE 2.005480330912666 Test RE 0.676888699544747\n",
      "99 Train Loss 5.430485 Test MSE 2.001377194747759 Test RE 0.6761959007893966\n",
      "Training time: 85.85\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 56.40176 Test MSE 8.592981953541514 Test RE 1.4011350915046237\n",
      "1 Train Loss 56.192154 Test MSE 8.679482567615056 Test RE 1.4081696437568632\n",
      "2 Train Loss 53.79621 Test MSE 8.379212443903933 Test RE 1.3835971572631283\n",
      "3 Train Loss 52.882675 Test MSE 8.838661188715687 Test RE 1.421023642113924\n",
      "4 Train Loss 50.04314 Test MSE 8.800640679123404 Test RE 1.417964000551021\n",
      "5 Train Loss 43.06001 Test MSE 8.329837090187961 Test RE 1.3795146413795714\n",
      "6 Train Loss 42.718376 Test MSE 8.512988586399361 Test RE 1.39459815347942\n",
      "7 Train Loss 42.485466 Test MSE 8.483370933128716 Test RE 1.3921700571223394\n",
      "8 Train Loss 42.404205 Test MSE 8.473513166124793 Test RE 1.3913609638239945\n",
      "9 Train Loss 42.248894 Test MSE 8.40721272876658 Test RE 1.3859069688400967\n",
      "10 Train Loss 41.697666 Test MSE 8.451522345781756 Test RE 1.3895543314416081\n",
      "11 Train Loss 39.689636 Test MSE 8.066725658639088 Test RE 1.357552718994828\n",
      "12 Train Loss 38.67398 Test MSE 7.787843132531532 Test RE 1.333879683595593\n",
      "13 Train Loss 38.207993 Test MSE 7.889302543075348 Test RE 1.3425404071049722\n",
      "14 Train Loss 37.990246 Test MSE 7.919484108523065 Test RE 1.3451059882367027\n",
      "15 Train Loss 37.75283 Test MSE 7.970425960805402 Test RE 1.3494252311855972\n",
      "16 Train Loss 37.675293 Test MSE 7.982538761658212 Test RE 1.350450214907691\n",
      "17 Train Loss 37.38244 Test MSE 7.936797539578767 Test RE 1.3465755085486841\n",
      "18 Train Loss 37.138786 Test MSE 7.895219065896128 Test RE 1.3430437267736626\n",
      "19 Train Loss 36.928997 Test MSE 7.883104235198491 Test RE 1.3420129134615142\n",
      "20 Train Loss 36.857044 Test MSE 7.916085633208701 Test RE 1.3448173456964512\n",
      "21 Train Loss 36.651333 Test MSE 7.885453513741916 Test RE 1.3422128681488683\n",
      "22 Train Loss 36.57777 Test MSE 7.914352306698573 Test RE 1.344670105300161\n",
      "23 Train Loss 36.356068 Test MSE 7.866563560747506 Test RE 1.3406042390118076\n",
      "24 Train Loss 36.093792 Test MSE 7.786703145026974 Test RE 1.3337820531094107\n",
      "25 Train Loss 35.829918 Test MSE 7.747023477782558 Test RE 1.3303793535153645\n",
      "26 Train Loss 35.69716 Test MSE 7.69701628702046 Test RE 1.3260785892945313\n",
      "27 Train Loss 35.343983 Test MSE 7.723415426615169 Test RE 1.3283507274136575\n",
      "28 Train Loss 34.939194 Test MSE 7.607206213284049 Test RE 1.3183194356918198\n",
      "29 Train Loss 34.185574 Test MSE 7.279086078116141 Test RE 1.289574651484222\n",
      "30 Train Loss 33.24402 Test MSE 7.239982285905784 Test RE 1.286106141430814\n",
      "31 Train Loss 32.855442 Test MSE 7.168082874462537 Test RE 1.279704123035339\n",
      "32 Train Loss 31.956139 Test MSE 7.078404261857737 Test RE 1.2716738520833637\n",
      "33 Train Loss 31.794207 Test MSE 7.032370811619892 Test RE 1.2675320272056276\n",
      "34 Train Loss 31.352688 Test MSE 6.877513534752575 Test RE 1.2534984092405925\n",
      "35 Train Loss 30.93581 Test MSE 7.046595226388194 Test RE 1.2688133015926062\n",
      "36 Train Loss 30.171278 Test MSE 7.154446919737124 Test RE 1.2784863431535036\n",
      "37 Train Loss 29.970463 Test MSE 7.191221828456863 Test RE 1.2817679354490203\n",
      "38 Train Loss 29.552322 Test MSE 7.05103129373142 Test RE 1.269212618958727\n",
      "39 Train Loss 29.210014 Test MSE 7.021360632786682 Test RE 1.2665393874983892\n",
      "40 Train Loss 29.081959 Test MSE 7.019693854334151 Test RE 1.2663890486987643\n",
      "41 Train Loss 28.509031 Test MSE 7.144820388651201 Test RE 1.2776259320382481\n",
      "42 Train Loss 27.358143 Test MSE 7.107786541756208 Test RE 1.2743104620443315\n",
      "43 Train Loss 27.007565 Test MSE 7.151543200966762 Test RE 1.2782268722546761\n",
      "44 Train Loss 25.642689 Test MSE 6.7936311258505375 Test RE 1.2458307366419854\n",
      "45 Train Loss 25.1156 Test MSE 6.871832542355364 Test RE 1.2529805922957216\n",
      "46 Train Loss 24.321522 Test MSE 6.622068520492242 Test RE 1.2299993880975653\n",
      "47 Train Loss 24.082283 Test MSE 6.475929916355833 Test RE 1.2163515988191915\n",
      "48 Train Loss 23.811975 Test MSE 6.310931669708528 Test RE 1.200756093567526\n",
      "49 Train Loss 23.447409 Test MSE 6.2892980859095 Test RE 1.1986962584379275\n",
      "50 Train Loss 23.201702 Test MSE 6.156064050907154 Test RE 1.1859315559785137\n",
      "51 Train Loss 22.784363 Test MSE 6.01155219743342 Test RE 1.1719291895169075\n",
      "52 Train Loss 22.54206 Test MSE 6.276459787004459 Test RE 1.197472188426126\n",
      "53 Train Loss 22.046934 Test MSE 6.2650735504412784 Test RE 1.196385517641018\n",
      "54 Train Loss 21.220736 Test MSE 6.250685542254647 Test RE 1.1950109528580084\n",
      "55 Train Loss 20.544773 Test MSE 6.287101072476272 Test RE 1.198486872447776\n",
      "56 Train Loss 19.041815 Test MSE 6.0741270628533774 Test RE 1.1780127651231431\n",
      "57 Train Loss 18.333296 Test MSE 5.630715978896787 Test RE 1.1342005956240542\n",
      "58 Train Loss 16.469402 Test MSE 4.376645109110468 Test RE 0.9999510169005129\n",
      "59 Train Loss 14.508911 Test MSE 3.5953825897007747 Test RE 0.9063180216880847\n",
      "60 Train Loss 12.701676 Test MSE 2.581695165604294 Test RE 0.7679987992750299\n",
      "61 Train Loss 9.094843 Test MSE 1.001747603497993 Test RE 0.4783956437666935\n",
      "62 Train Loss 6.909752 Test MSE 0.9332280204842363 Test RE 0.4617447277891338\n",
      "63 Train Loss 5.335267 Test MSE 0.6053500005762548 Test RE 0.37188728840709306\n",
      "64 Train Loss 3.8623226 Test MSE 0.35306022935785203 Test RE 0.2840092335270493\n",
      "65 Train Loss 3.363237 Test MSE 0.2444285447653972 Test RE 0.23631104535513475\n",
      "66 Train Loss 2.7846227 Test MSE 0.20766154798212455 Test RE 0.21781415708666263\n",
      "67 Train Loss 2.550866 Test MSE 0.18161470632100604 Test RE 0.20369649992232663\n",
      "68 Train Loss 2.3771086 Test MSE 0.17012586125125556 Test RE 0.19714838722962608\n",
      "69 Train Loss 2.0727727 Test MSE 0.20135000638745942 Test RE 0.21447855950133982\n",
      "70 Train Loss 1.785567 Test MSE 0.18870360873616177 Test RE 0.20763385299037893\n",
      "71 Train Loss 1.669902 Test MSE 0.17762084995298394 Test RE 0.20144432272453225\n",
      "72 Train Loss 1.5545111 Test MSE 0.16105385239365827 Test RE 0.19181988180899198\n",
      "73 Train Loss 1.426656 Test MSE 0.11991315515208682 Test RE 0.1655165690669377\n",
      "74 Train Loss 1.3871149 Test MSE 0.11762535361370392 Test RE 0.16393003488584398\n",
      "75 Train Loss 1.3168702 Test MSE 0.0901716591700956 Test RE 0.1435301341257679\n",
      "76 Train Loss 1.261341 Test MSE 0.10300495529913035 Test RE 0.15340415376166222\n",
      "77 Train Loss 1.1136174 Test MSE 0.08015248440101685 Test RE 0.13532142248847323\n",
      "78 Train Loss 1.0664792 Test MSE 0.07590509709282527 Test RE 0.13168718992275044\n",
      "79 Train Loss 0.9025187 Test MSE 0.06331568213916865 Test RE 0.12027177082347158\n",
      "80 Train Loss 0.8259307 Test MSE 0.059982483417372535 Test RE 0.11706317032873707\n",
      "81 Train Loss 0.7696267 Test MSE 0.0645771670485302 Test RE 0.12146399321036351\n",
      "82 Train Loss 0.71458775 Test MSE 0.0638261730716845 Test RE 0.1207556508524063\n",
      "83 Train Loss 0.6011216 Test MSE 0.05662298806348116 Test RE 0.11373770275051807\n",
      "84 Train Loss 0.5503847 Test MSE 0.0457969476315618 Test RE 0.10228838621800432\n",
      "85 Train Loss 0.5399214 Test MSE 0.0486850420813252 Test RE 0.10546438759106024\n",
      "86 Train Loss 0.4968891 Test MSE 0.05145951162163164 Test RE 0.10842786100667558\n",
      "87 Train Loss 0.44271666 Test MSE 0.04413789335440387 Test RE 0.1004185302806417\n",
      "88 Train Loss 0.42497185 Test MSE 0.0421596213142435 Test RE 0.0981423405233088\n",
      "89 Train Loss 0.40856263 Test MSE 0.03814350862811005 Test RE 0.09335087215798343\n",
      "90 Train Loss 0.3855988 Test MSE 0.04015400948220065 Test RE 0.09577948970767204\n",
      "91 Train Loss 0.3741141 Test MSE 0.044225780752583196 Test RE 0.10051845727881466\n",
      "92 Train Loss 0.3538627 Test MSE 0.040705221020713035 Test RE 0.09643465228587621\n",
      "93 Train Loss 0.33833945 Test MSE 0.04005762624129685 Test RE 0.09566446901468119\n",
      "94 Train Loss 0.29941496 Test MSE 0.026382333145291093 Test RE 0.07763626975195101\n",
      "95 Train Loss 0.26884067 Test MSE 0.026004976341725182 Test RE 0.07707903907614083\n",
      "96 Train Loss 0.25163826 Test MSE 0.024718102502492706 Test RE 0.0751476881942959\n",
      "97 Train Loss 0.22763668 Test MSE 0.02129904177983004 Test RE 0.0697570478874156\n",
      "98 Train Loss 0.2224608 Test MSE 0.02128424537664308 Test RE 0.06973281363459376\n",
      "99 Train Loss 0.21248932 Test MSE 0.021807184022334697 Test RE 0.07058425813644814\n",
      "Training time: 85.97\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 56.55311 Test MSE 8.806875082687442 Test RE 1.4184661568766517\n",
      "1 Train Loss 55.879303 Test MSE 8.690302077350593 Test RE 1.4090470555119454\n",
      "2 Train Loss 51.958557 Test MSE 8.48790832845752 Test RE 1.392542313713446\n",
      "3 Train Loss 46.980095 Test MSE 8.76811614894929 Test RE 1.41534139023963\n",
      "4 Train Loss 42.97379 Test MSE 8.316648034084867 Test RE 1.3784220807701053\n",
      "5 Train Loss 42.82012 Test MSE 8.340340075589728 Test RE 1.380384073614825\n",
      "6 Train Loss 42.706512 Test MSE 8.495180760463334 Test RE 1.3931387504674364\n",
      "7 Train Loss 42.58979 Test MSE 8.424971312284983 Test RE 1.3873699246549103\n",
      "8 Train Loss 42.55623 Test MSE 8.527900883543081 Test RE 1.3958190856104569\n",
      "9 Train Loss 42.39845 Test MSE 8.471637750682218 Test RE 1.391206982577728\n",
      "10 Train Loss 42.124107 Test MSE 8.40347684687797 Test RE 1.3855990094589614\n",
      "11 Train Loss 41.256115 Test MSE 8.087889891481774 Test RE 1.3593324213409388\n",
      "12 Train Loss 38.943916 Test MSE 7.38168545010148 Test RE 1.2986311852520098\n",
      "13 Train Loss 37.70235 Test MSE 7.7256788067261954 Test RE 1.328545352602876\n",
      "14 Train Loss 37.51068 Test MSE 7.719561421497694 Test RE 1.328019260760938\n",
      "15 Train Loss 37.189087 Test MSE 7.658733051173036 Test RE 1.3227766694098382\n",
      "16 Train Loss 37.021454 Test MSE 7.731908879856931 Test RE 1.3290809215108461\n",
      "17 Train Loss 36.93707 Test MSE 7.755982688923206 Test RE 1.3311484040747021\n",
      "18 Train Loss 36.86737 Test MSE 7.724541448193282 Test RE 1.3284475561471982\n",
      "19 Train Loss 36.829647 Test MSE 7.670808783514153 Test RE 1.323819087613841\n",
      "20 Train Loss 36.76509 Test MSE 7.634282154078265 Test RE 1.3206634653376215\n",
      "21 Train Loss 36.686752 Test MSE 7.668818080069079 Test RE 1.323647299847186\n",
      "22 Train Loss 36.61976 Test MSE 7.612633583733023 Test RE 1.3187896301373752\n",
      "23 Train Loss 36.576057 Test MSE 7.617673701727448 Test RE 1.319226125304327\n",
      "24 Train Loss 36.53112 Test MSE 7.649840212581787 Test RE 1.3220084838625787\n",
      "25 Train Loss 36.49643 Test MSE 7.701855340587613 Test RE 1.3264953713846044\n",
      "26 Train Loss 36.441074 Test MSE 7.654774794574195 Test RE 1.3224348004521747\n",
      "27 Train Loss 36.3749 Test MSE 7.7053099939204985 Test RE 1.3267928365855988\n",
      "28 Train Loss 36.145824 Test MSE 7.711764334014095 Test RE 1.3273484131712925\n",
      "29 Train Loss 35.504967 Test MSE 7.8205076293973885 Test RE 1.3366740977031912\n",
      "30 Train Loss 34.908703 Test MSE 8.02145239727812 Test RE 1.3537378305636463\n",
      "31 Train Loss 34.61973 Test MSE 7.970340320860424 Test RE 1.349417981572242\n",
      "32 Train Loss 34.369198 Test MSE 8.152026414706626 Test RE 1.3647114942318355\n",
      "33 Train Loss 33.97302 Test MSE 8.205466716344203 Test RE 1.3691773446182671\n",
      "34 Train Loss 33.302254 Test MSE 8.10723988303334 Test RE 1.3609575273772643\n",
      "35 Train Loss 32.7209 Test MSE 7.795789733036052 Test RE 1.3345600456799789\n",
      "36 Train Loss 28.7564 Test MSE 7.197627504888954 Test RE 1.2823386842899012\n",
      "37 Train Loss 27.977718 Test MSE 7.302330188066896 Test RE 1.2916319924193151\n",
      "38 Train Loss 27.741356 Test MSE 7.279671141808147 Test RE 1.2896264758574085\n",
      "39 Train Loss 27.318157 Test MSE 7.038985833858646 Test RE 1.268128041118543\n",
      "40 Train Loss 26.423206 Test MSE 6.6836569448304095 Test RE 1.2357059435219904\n",
      "41 Train Loss 25.726854 Test MSE 6.613227949579234 Test RE 1.2291780791355462\n",
      "42 Train Loss 25.052284 Test MSE 6.687939723188858 Test RE 1.2361017902261149\n",
      "43 Train Loss 23.58971 Test MSE 6.921775793802877 Test RE 1.2575255687931144\n",
      "44 Train Loss 23.028725 Test MSE 7.161896188232693 Test RE 1.2791517551953575\n",
      "45 Train Loss 22.425209 Test MSE 7.432653285513048 Test RE 1.3031067594276808\n",
      "46 Train Loss 21.775673 Test MSE 7.2174969834356135 Test RE 1.28410745051454\n",
      "47 Train Loss 21.372898 Test MSE 7.193905198085525 Test RE 1.2820070558959111\n",
      "48 Train Loss 20.840595 Test MSE 7.102410867751018 Test RE 1.2738284854610002\n",
      "49 Train Loss 20.292793 Test MSE 7.031058376208031 Test RE 1.267413743373168\n",
      "50 Train Loss 19.468075 Test MSE 6.720406226448469 Test RE 1.239098476604245\n",
      "51 Train Loss 17.90868 Test MSE 6.522061427804139 Test RE 1.2206762721547082\n",
      "52 Train Loss 16.559393 Test MSE 6.094797785593691 Test RE 1.1800154968428405\n",
      "53 Train Loss 15.855272 Test MSE 5.819482351849809 Test RE 1.1530555699232203\n",
      "54 Train Loss 15.074364 Test MSE 5.3778240198532945 Test RE 1.1084378660299494\n",
      "55 Train Loss 14.1708355 Test MSE 5.015060070507928 Test RE 1.0704000728181058\n",
      "56 Train Loss 12.777606 Test MSE 4.770878816346702 Test RE 1.044016236711563\n",
      "57 Train Loss 11.710257 Test MSE 4.5197369477555895 Test RE 1.0161659536676315\n",
      "58 Train Loss 11.35054 Test MSE 4.601414412173198 Test RE 1.025306556344892\n",
      "59 Train Loss 10.705094 Test MSE 4.570826762126623 Test RE 1.0218930395325487\n",
      "60 Train Loss 10.240629 Test MSE 4.322681634035967 Test RE 0.9937672624073075\n",
      "61 Train Loss 9.959301 Test MSE 4.2002832402706725 Test RE 0.9795967802146504\n",
      "62 Train Loss 9.611513 Test MSE 3.945801147218462 Test RE 0.9494577802887285\n",
      "63 Train Loss 9.297625 Test MSE 3.781732960328321 Test RE 0.9295087655329722\n",
      "64 Train Loss 9.129151 Test MSE 3.772241000879991 Test RE 0.9283415225263888\n",
      "65 Train Loss 8.997452 Test MSE 3.825747599016234 Test RE 0.9349022768608397\n",
      "66 Train Loss 8.894446 Test MSE 3.8284613145834516 Test RE 0.9352337949568102\n",
      "67 Train Loss 8.850833 Test MSE 3.8394251200587357 Test RE 0.9365719815017283\n",
      "68 Train Loss 8.745727 Test MSE 3.8106733471890375 Test RE 0.933058602950336\n",
      "69 Train Loss 8.678602 Test MSE 3.775406772580154 Test RE 0.9287309861068325\n",
      "70 Train Loss 8.598048 Test MSE 3.720757231792942 Test RE 0.921984728435173\n",
      "71 Train Loss 8.515232 Test MSE 3.6800895405447176 Test RE 0.9169322611160736\n",
      "72 Train Loss 8.40959 Test MSE 3.7570914304160805 Test RE 0.9264755065166312\n",
      "73 Train Loss 8.381086 Test MSE 3.7167581816181103 Test RE 0.9214891230783266\n",
      "74 Train Loss 8.324575 Test MSE 3.7348129013904314 Test RE 0.9237245486205695\n",
      "75 Train Loss 8.297971 Test MSE 3.756878709936332 Test RE 0.9264492783681361\n",
      "76 Train Loss 8.258028 Test MSE 3.7139739818766495 Test RE 0.9211439176192482\n",
      "77 Train Loss 8.237463 Test MSE 3.691216413895401 Test RE 0.9183174025233584\n",
      "78 Train Loss 8.198064 Test MSE 3.7445357033301563 Test RE 0.9249261286601128\n",
      "79 Train Loss 8.140964 Test MSE 3.72172248310899 Test RE 0.922104312864671\n",
      "80 Train Loss 8.103705 Test MSE 3.718125692729347 Test RE 0.9216586297643475\n",
      "81 Train Loss 8.076706 Test MSE 3.6971275136373585 Test RE 0.9190524033591309\n",
      "82 Train Loss 8.059083 Test MSE 3.7294707988793925 Test RE 0.9230636859226468\n",
      "83 Train Loss 8.029728 Test MSE 3.704599535506743 Test RE 0.9199806528490242\n",
      "84 Train Loss 8.016773 Test MSE 3.7341921033579912 Test RE 0.9236477750004594\n",
      "85 Train Loss 8.008509 Test MSE 3.7399601373011797 Test RE 0.9243608577531853\n",
      "86 Train Loss 7.9948215 Test MSE 3.760300236488976 Test RE 0.9268710579422865\n",
      "87 Train Loss 7.9560246 Test MSE 3.767299148833626 Test RE 0.9277332329204282\n",
      "88 Train Loss 7.929372 Test MSE 3.7451863655109796 Test RE 0.9250064841912966\n",
      "89 Train Loss 7.910908 Test MSE 3.7737681425380845 Test RE 0.9285294168782516\n",
      "90 Train Loss 7.8899875 Test MSE 3.805304260160201 Test RE 0.9324010500407485\n",
      "91 Train Loss 7.869249 Test MSE 3.8091387764920137 Test RE 0.9328707111473175\n",
      "92 Train Loss 7.8298807 Test MSE 3.8068823659045328 Test RE 0.9325943690015243\n",
      "93 Train Loss 7.791195 Test MSE 3.7980836148691233 Test RE 0.9315160046399676\n",
      "94 Train Loss 7.7380915 Test MSE 3.7706513884289494 Test RE 0.9281459015758963\n",
      "95 Train Loss 7.720459 Test MSE 3.776204225582463 Test RE 0.928829065632163\n",
      "96 Train Loss 7.71097 Test MSE 3.776734053838654 Test RE 0.9288942240020468\n",
      "97 Train Loss 7.699725 Test MSE 3.800727705538804 Test RE 0.931840192376458\n",
      "98 Train Loss 7.675378 Test MSE 3.79790978525542 Test RE 0.9314946877159023\n",
      "99 Train Loss 7.6550894 Test MSE 3.8022252150250893 Test RE 0.9320237496068192\n",
      "Training time: 85.26\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 55.768982 Test MSE 8.648421960749662 Test RE 1.405647730466231\n",
      "1 Train Loss 55.65703 Test MSE 8.490633507189427 Test RE 1.3927658447645124\n",
      "2 Train Loss 55.36726 Test MSE 8.743636687868364 Test RE 1.4133642830673143\n",
      "3 Train Loss 54.04268 Test MSE 8.53399634123417 Test RE 1.3963178388501927\n",
      "4 Train Loss 45.491173 Test MSE 8.673286643703213 Test RE 1.4076669369685126\n",
      "5 Train Loss 43.83725 Test MSE 8.598055244592397 Test RE 1.401548645104244\n",
      "6 Train Loss 43.63657 Test MSE 8.831234891086376 Test RE 1.4204265402523841\n",
      "7 Train Loss 43.414757 Test MSE 8.72536945653648 Test RE 1.4118871087384872\n",
      "8 Train Loss 43.273224 Test MSE 8.651547648529792 Test RE 1.4059017200983241\n",
      "9 Train Loss 43.256992 Test MSE 8.627286081207872 Test RE 1.4039290484174543\n",
      "10 Train Loss 43.240074 Test MSE 8.653613301384924 Test RE 1.4060695473873843\n",
      "11 Train Loss 43.193897 Test MSE 8.653191735973564 Test RE 1.4060352982543505\n",
      "12 Train Loss 43.094986 Test MSE 8.662804717622961 Test RE 1.406816076039776\n",
      "13 Train Loss 43.05768 Test MSE 8.589727110051253 Test RE 1.4008697059032447\n",
      "14 Train Loss 42.710186 Test MSE 8.719459201196207 Test RE 1.4114088466470756\n",
      "15 Train Loss 42.029213 Test MSE 8.549888368224046 Test RE 1.3976173472391207\n",
      "16 Train Loss 41.92235 Test MSE 8.448556306997629 Test RE 1.3893104798880425\n",
      "17 Train Loss 41.78464 Test MSE 8.394028961651467 Test RE 1.384819887780453\n",
      "18 Train Loss 41.66512 Test MSE 8.449644007181764 Test RE 1.389399909642374\n",
      "19 Train Loss 41.404152 Test MSE 8.31306493439281 Test RE 1.3781251128208132\n",
      "20 Train Loss 40.47519 Test MSE 8.213534068603245 Test RE 1.369850244937697\n",
      "21 Train Loss 39.219696 Test MSE 7.727382847704868 Test RE 1.3286918621172679\n",
      "22 Train Loss 38.17906 Test MSE 7.89227495000444 Test RE 1.3427932938830538\n",
      "23 Train Loss 37.38546 Test MSE 7.41178458576279 Test RE 1.3012761036854419\n",
      "24 Train Loss 34.725876 Test MSE 6.9033450151594895 Test RE 1.255850231042483\n",
      "25 Train Loss 33.93835 Test MSE 7.31909660463373 Test RE 1.2931139593370182\n",
      "26 Train Loss 33.461422 Test MSE 7.36199213314653 Test RE 1.2968977440403298\n",
      "27 Train Loss 33.12871 Test MSE 7.320284946989807 Test RE 1.2932189312893874\n",
      "28 Train Loss 33.02976 Test MSE 7.219453885810936 Test RE 1.2842815207370957\n",
      "29 Train Loss 32.766804 Test MSE 7.294835978991262 Test RE 1.2909690365714448\n",
      "30 Train Loss 32.383377 Test MSE 7.2906583103334 Test RE 1.2905993220719805\n",
      "31 Train Loss 31.989334 Test MSE 7.2712167280152915 Test RE 1.2888773895927126\n",
      "32 Train Loss 31.225372 Test MSE 6.2070330667287505 Test RE 1.190830884845896\n",
      "33 Train Loss 28.174753 Test MSE 6.913446285511194 Test RE 1.2567687021250173\n",
      "34 Train Loss 26.723553 Test MSE 6.458064229492251 Test RE 1.2146726148338116\n",
      "35 Train Loss 23.99436 Test MSE 5.819249258272296 Test RE 1.1530324774449727\n",
      "36 Train Loss 21.243809 Test MSE 5.270523062164611 Test RE 1.0973241044967244\n",
      "37 Train Loss 20.466507 Test MSE 5.551771725676782 Test RE 1.1262216211766838\n",
      "38 Train Loss 18.678137 Test MSE 3.83710528344816 Test RE 0.9362889935288282\n",
      "39 Train Loss 15.338585 Test MSE 2.3799236276378246 Test RE 0.7373769719611534\n",
      "40 Train Loss 11.879569 Test MSE 2.364530264885603 Test RE 0.7349884236899367\n",
      "41 Train Loss 11.388165 Test MSE 2.220037473590696 Test RE 0.7121774411896054\n",
      "42 Train Loss 10.793829 Test MSE 2.205605394024707 Test RE 0.7098587955950004\n",
      "43 Train Loss 10.289653 Test MSE 2.048854247007624 Test RE 0.6841693154182787\n",
      "44 Train Loss 9.711094 Test MSE 1.931914267694478 Test RE 0.6643577188378952\n",
      "45 Train Loss 9.319096 Test MSE 1.8450310909324816 Test RE 0.6492469300335464\n",
      "46 Train Loss 8.975034 Test MSE 1.7228701646410112 Test RE 0.6273852952488517\n",
      "47 Train Loss 7.7842293 Test MSE 1.3067376834868951 Test RE 0.5463894032342589\n",
      "48 Train Loss 7.4290686 Test MSE 1.1482029813576364 Test RE 0.5121738433982437\n",
      "49 Train Loss 7.236043 Test MSE 0.9878500249742227 Test RE 0.4750655825249027\n",
      "50 Train Loss 6.4913583 Test MSE 0.7204267946670194 Test RE 0.4056981140372378\n",
      "51 Train Loss 6.080808 Test MSE 0.6825460683011504 Test RE 0.3948880986540309\n",
      "52 Train Loss 5.85154 Test MSE 0.7097418370656668 Test RE 0.40267833426497024\n",
      "53 Train Loss 5.6477785 Test MSE 0.6233305255585705 Test RE 0.3773699011282035\n",
      "54 Train Loss 5.49347 Test MSE 0.6359849665851199 Test RE 0.38118121080442496\n",
      "55 Train Loss 5.419358 Test MSE 0.597947122537517 Test RE 0.3696063725077797\n",
      "56 Train Loss 5.363367 Test MSE 0.5641587010635656 Test RE 0.359011785466801\n",
      "57 Train Loss 5.3400626 Test MSE 0.5624691328404743 Test RE 0.35847378992212875\n",
      "58 Train Loss 5.183914 Test MSE 0.4827199513386332 Test RE 0.33208991330529114\n",
      "59 Train Loss 5.0816474 Test MSE 0.5046092720229353 Test RE 0.33953587912706396\n",
      "60 Train Loss 5.042792 Test MSE 0.5134685506118223 Test RE 0.3425034769484685\n",
      "61 Train Loss 4.9707866 Test MSE 0.5190611081304447 Test RE 0.3443636521348784\n",
      "62 Train Loss 4.9373074 Test MSE 0.517791604360926 Test RE 0.3439422772911108\n",
      "63 Train Loss 4.7505436 Test MSE 0.48516306445154606 Test RE 0.33292922939480846\n",
      "64 Train Loss 4.6697874 Test MSE 0.4616967783779212 Test RE 0.3247779097210292\n",
      "65 Train Loss 4.647876 Test MSE 0.4645303776247359 Test RE 0.32577302465528507\n",
      "66 Train Loss 4.6102247 Test MSE 0.4643836861061149 Test RE 0.32572158354906333\n",
      "67 Train Loss 4.4099236 Test MSE 0.4092365174944713 Test RE 0.3057702655468107\n",
      "68 Train Loss 4.17933 Test MSE 0.39100079612546457 Test RE 0.2988800182829583\n",
      "69 Train Loss 4.042976 Test MSE 0.3670880972206139 Test RE 0.28959643525759904\n",
      "70 Train Loss 3.9222593 Test MSE 0.363939672606724 Test RE 0.2883518620976161\n",
      "71 Train Loss 3.8613048 Test MSE 0.3474972616472631 Test RE 0.2817628643360774\n",
      "72 Train Loss 3.8025608 Test MSE 0.3541221337891689 Test RE 0.284436022222418\n",
      "73 Train Loss 3.7311523 Test MSE 0.3319084899053548 Test RE 0.2753703908361752\n",
      "74 Train Loss 3.5787137 Test MSE 0.29897458887148104 Test RE 0.2613516215149873\n",
      "75 Train Loss 3.5316625 Test MSE 0.2845416743482001 Test RE 0.2549652549971633\n",
      "76 Train Loss 3.5240276 Test MSE 0.27583348416193737 Test RE 0.2510334258177334\n",
      "77 Train Loss 3.4989731 Test MSE 0.2614943838128902 Test RE 0.244421408933084\n",
      "78 Train Loss 3.4818795 Test MSE 0.2495064257776007 Test RE 0.23875304970121905\n",
      "79 Train Loss 3.4246316 Test MSE 0.2553950972265188 Test RE 0.24155405822832585\n",
      "80 Train Loss 3.3613636 Test MSE 0.2447080018151266 Test RE 0.2364460948902351\n",
      "81 Train Loss 3.2935746 Test MSE 0.2403278090845111 Test RE 0.2343203859076971\n",
      "82 Train Loss 3.257805 Test MSE 0.23500554230301596 Test RE 0.23171124601451185\n",
      "83 Train Loss 3.2158701 Test MSE 0.24305265134491721 Test RE 0.235645006823415\n",
      "84 Train Loss 3.1750586 Test MSE 0.2534730216770273 Test RE 0.24064338688677914\n",
      "85 Train Loss 3.136556 Test MSE 0.25664175887308854 Test RE 0.24214289018504642\n",
      "86 Train Loss 3.108125 Test MSE 0.2522095876175876 Test RE 0.2400428952371237\n",
      "87 Train Loss 3.0233586 Test MSE 0.22765391611493407 Test RE 0.22805816364985793\n",
      "88 Train Loss 3.005476 Test MSE 0.22296028135730836 Test RE 0.22569493464449067\n",
      "89 Train Loss 2.9245331 Test MSE 0.21391211943869887 Test RE 0.2210679355773977\n",
      "90 Train Loss 2.8865936 Test MSE 0.20984134513012936 Test RE 0.2189543566691856\n",
      "91 Train Loss 2.8567517 Test MSE 0.20818281626664242 Test RE 0.21808736233608028\n",
      "92 Train Loss 2.7639499 Test MSE 0.2002332231885233 Test RE 0.21388293223563518\n",
      "93 Train Loss 2.5821724 Test MSE 0.19318473351447132 Test RE 0.2100847179020263\n",
      "94 Train Loss 2.4557 Test MSE 0.18413777596706124 Test RE 0.20510653930740047\n",
      "95 Train Loss 2.3905473 Test MSE 0.17154724367461738 Test RE 0.19797025044992445\n",
      "96 Train Loss 2.35543 Test MSE 0.17905536282794057 Test RE 0.20225614566179542\n",
      "97 Train Loss 2.316101 Test MSE 0.1846597546854342 Test RE 0.20539704320640823\n",
      "98 Train Loss 2.290071 Test MSE 0.18809112599978148 Test RE 0.20729661643454608\n",
      "99 Train Loss 2.269707 Test MSE 0.18699485093893234 Test RE 0.20669162719992412\n",
      "Training time: 85.47\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 Train Loss 56.936543 Test MSE 8.701629490070056 Test RE 1.4099650707731648\n",
      "1 Train Loss nan Test MSE nan Test RE nan\n",
      "NAN BREAK!\n",
      "Training time: 2.27\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 58.67505 Test MSE 8.694732515149182 Test RE 1.4094061857141251\n",
      "1 Train Loss nan Test MSE nan Test RE nan\n",
      "NAN BREAK!\n",
      "Training time: 2.15\n"
     ]
    }
   ],
   "source": [
    "nan_tune = []\n",
    "for tune_reps in range(5):\n",
    "#for tune_reps in range(4,5):\n",
    "  max_reps = 10 #10\n",
    "  max_iter = 100 #100\n",
    "  label = \"KG_swish_tune\"+str(tune_reps)\n",
    "\n",
    "  train_loss_full = []\n",
    "  test_mse_full = []\n",
    "  test_re_full = []\n",
    "  beta_full = []\n",
    "  elapsed_time= np.zeros((max_reps,1))\n",
    "  time_threshold = np.empty((max_reps,1))\n",
    "  time_threshold[:] = np.nan\n",
    "  epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "  N_I = 200  #Total number of data points for 'y'\n",
    "  N_B = 400\n",
    "  N_f = 10000 #Total number of collocation points\n",
    "\n",
    "  for reps in range(max_reps):\n",
    "      print(reps)\n",
    "      print(label)\n",
    "\n",
    "      train_loss = []\n",
    "      test_mse_loss = []\n",
    "      test_re_loss = []\n",
    "      beta_val = []\n",
    "      \n",
    "      torch.manual_seed(reps*36)\n",
    "\n",
    "      layers = np.array([2,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "\n",
    "      PINN = Sequentialmodel(layers)\n",
    "    \n",
    "      PINN.to(device)\n",
    "\n",
    "      'Neural Network Summary'\n",
    "      print(PINN)\n",
    "\n",
    "      params = list(PINN.parameters())\n",
    "      \n",
    "\n",
    "      optimizer = torch.optim.LBFGS(PINN.parameters(), lr=lr_tune[tune_reps], \n",
    "                                max_iter = 10, \n",
    "                                max_eval = 15, \n",
    "                                tolerance_grad = 1e-8, \n",
    "                                tolerance_change = 1e-8, \n",
    "                                history_size = 100, \n",
    "                                line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "          \n",
    "      nan_flag = train_model(max_iter,reps)\n",
    "    \n",
    "      torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "      train_loss_full.append(train_loss)\n",
    "      test_mse_full.append(test_mse_loss)\n",
    "      test_re_full.append(test_re_loss)\n",
    "      #elapsed_time[reps] = time.time() - start_time\n",
    "      beta_full.append(beta_val)\n",
    "    \n",
    "      if(nan_flag == 1):\n",
    "            nan_tune.append(tune_reps)\n",
    "            break\n",
    "            \n",
    "      #print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "      \n",
    "  mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"beta\": beta_full, \"label\": label}\n",
    "  savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1660688516819,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "_O3sPdAnSq_2"
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "aborted",
     "timestamp": 1660688516819,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "jQ4afiEWSq_2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tune_reps   1.0808060463897486\n",
      "tune_reps   0.8801059752546545\n",
      "tune_reps   [[1.41093885 1.41320998 1.44207193 1.32377782        nan]]\n",
      "tune_reps   [[1.40996507        nan]]\n",
      "tune_reps   nan\n"
     ]
    }
   ],
   "source": [
    "for tune_reps in range(5):\n",
    "    label = \"KG_swish_tune\"+str(tune_reps)+\".mat\"\n",
    "    data = sio.loadmat(label)\n",
    " \n",
    "    re = np.array(data[\"test_re_loss\"])\n",
    "    print(\"tune_reps\",\" \",np.mean(re[:,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1660688534316,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "06syezgfv_qO",
    "outputId": "9f4852d5-694a-4977-8893-a6183a2ce493"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4   ()\n",
      "4   ()\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "index 2 is out of bounds for axis 0 with size 2",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_20236/2210093097.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mk\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtune_reps\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\" \"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test_re_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m: index 2 is out of bounds for axis 0 with size 2"
     ]
    }
   ],
   "source": [
    "# tune_reps = 4\n",
    "# label = \"KG_swish_tune\"+str(tune_reps)+\".mat\"\n",
    "# data = sio.loadmat(label)\n",
    "# for k in range(10):\n",
    "#     print(tune_reps,\" \",data[\"test_re_loss\"][0][k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(data[\"test_re_loss\"][:,-1])"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "stan_2D_KG_16Aug2022_tune.ipynb",
   "version": ""
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
