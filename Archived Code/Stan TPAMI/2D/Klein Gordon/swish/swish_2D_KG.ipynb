{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 286,
     "status": "ok",
     "timestamp": 1660687093981,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "iAtv2UvNSq_u",
    "outputId": "68a82578-1b95-4343-a8ec-7635a4df93ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1256,
     "status": "ok",
     "timestamp": 1660687393066,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "xAgfGYA4acPE",
    "outputId": "527d048f-6a89-4e80-87ff-bfdb1c9d6222"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 1856,
     "status": "ok",
     "timestamp": 1660687061284,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "7kSdyTofacUc",
    "outputId": "08ee5c9b-0706-46a5-86a1-2c7e56a6a74d"
   },
   "outputs": [],
   "source": [
    "# %cd '/content/gdrive/MyDrive/Virginia Tech /Fall 2022/Codes from GPU/PINN_Stan/2D Klein Gordon/stan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "executionInfo": {
     "elapsed": 32419,
     "status": "ok",
     "timestamp": 1660687093700,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "RHuSaD0gagsN",
    "outputId": "c232cd79-e56c-4a76-97c7-d59dafa084ef"
   },
   "outputs": [],
   "source": [
    "# !pip install smt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 280,
     "status": "ok",
     "timestamp": 1660687410736,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "mTLFQRt5Sq_y"
   },
   "outputs": [],
   "source": [
    "def true_2D_1(xt): #True function for 2D_1 Klein Gordon Equation x \\in [-50,50] , t \\in [0,10]\n",
    "    y = xt[:,0]*np.cos(xt[:,1])\n",
    "    return y.reshape(-1,1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 4312,
     "status": "ok",
     "timestamp": 1660687098957,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "81bNHCY3Sq_y"
   },
   "outputs": [],
   "source": [
    "label = \"KG_swish\"\n",
    "loss_thresh = 0.01\n",
    "\n",
    "x = np.linspace(-5,5,500).reshape(-1,1)\n",
    "t = np.linspace(0,10,1000).reshape(-1,1)\n",
    "\n",
    "X,T = np.meshgrid(x,t)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "T = T.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xt = np.hstack((X,T))\n",
    "\n",
    "y_true = true_2D_1(xt)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "#bound_pts_idx = ((X == -5) + (X == 5) + (T == 0)).reshape(-1,)\n",
    "\n",
    "#xt_bound = xt[bound_pts_idx,:]\n",
    "#y_bound = y_true[bound_pts_idx,:]\n",
    "\n",
    "\n",
    "xt_test_tensor = torch.from_numpy(xt).float().to(device)\n",
    "\n",
    "\n",
    "lb_xt = xt[0]\n",
    "ub_xt = xt[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1660687098958,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "YQgCA-PuSq_z"
   },
   "outputs": [],
   "source": [
    "def trainingdata(N_I,N_B,N_f,seed):\n",
    "    '''Boundary Conditions''' \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    x_BC1 = np.random.uniform(size = N_I).reshape(-1,1)\n",
    "    t_BC1 = np.zeros((N_I,1))\n",
    "    samples = np.hstack((x_BC1,t_BC1))\n",
    "    xt_BC1 = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    y_BC1 = true_2D_1(xt_BC1)\n",
    "    \n",
    "    x_BC2 = np.zeros((int(N_B/2),1))\n",
    "    t_BC2 = np.random.uniform(size = int(N_B/2)).reshape(-1,1)\n",
    "    samples = np.hstack((x_BC2,t_BC2))\n",
    "    xt_BC2 = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    y_BC2 = true_2D_1(xt_BC2)\n",
    "    \n",
    "    x_BC3 = np.ones((int(N_B/2),1))\n",
    "    t_BC3 = np.random.uniform(size = int(N_B/2)).reshape(-1,1)\n",
    "    samples = np.hstack((x_BC3,t_BC3))\n",
    "    xt_BC3 = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    y_BC3 = true_2D_1(xt_BC3)\n",
    "\n",
    "    xt_BC = np.vstack((xt_BC1,xt_BC2,xt_BC3))\n",
    "    y_BC = np.vstack((y_BC1,y_BC2,y_BC3))\n",
    "\n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    samples = sampling(N_f)\n",
    "    xt_coll = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    \n",
    "    xt_coll = np.vstack((xt_coll, xt_BC)) # append training points to collocation points \n",
    "\n",
    "    return xt_coll, xt_BC, y_BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1660687098958,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "gTJxct8bSq_0"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "            \n",
    "        \n",
    "        self.beta = Parameter(torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "\n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xt):\n",
    "        if torch.is_tensor(xt) != True:         \n",
    "            xt = torch.from_numpy(xt)                \n",
    "        \n",
    "        ubxt = torch.from_numpy(ub_xt).float().to(device)\n",
    "        lbxt = torch.from_numpy(lb_xt).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xt = (xt - lbxt)/(ubxt - lbxt)\n",
    "        \n",
    "        #convert to float\n",
    "        a = xt.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = z*self.activation(self.beta[:,i]*z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC(self,xt,y):\n",
    "                \n",
    "        loss_bc = self.loss_function(self.forward(xt), y)\n",
    "                \n",
    "        return loss_bc\n",
    "    \n",
    "    def loss_PDE(self, xt_coll, f_hat):\n",
    "        \n",
    "        g = xt_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        y = self.forward(g) \n",
    "        \n",
    "        \n",
    "        y_x_t = autograd.grad(y,g,torch.ones([xt_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        y_xx_tt = autograd.grad(y_x_t,g,torch.ones(xt_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        #du_dx = u_x_t[:,[0]]\n",
    "        \n",
    "        d2y_dx2 = y_xx_tt[:,[0]]\n",
    "        d2y_dt2 = y_xx_tt[:,[1]]    \n",
    "        \n",
    "\n",
    "        f = d2y_dt2 - d2y_dx2 + torch.pow(y,2) + (g[:,0]*torch.cos(g[:,1])).reshape(-1,1) - (torch.pow(g[:,0],2)*torch.pow(torch.cos(g[:,1]),2)).reshape(-1,1)\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xt_BC,y_BC,xt_coll,f_hat):\n",
    "\n",
    "        loss_BC = self.loss_BC(xt_BC,y_BC)\n",
    "        loss_f = self.loss_PDE(xt_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_BC + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "         \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        y_pred = self.forward(xt_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return y_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1660687098958,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "VoQzfzYsYKVs"
   },
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1660687098959,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "_IUDZDkxXmyF"
   },
   "outputs": [],
   "source": [
    "def train_step(xt_BC, y_BC, xt_coll, f_hat,seed):\n",
    "    # x_coll_np_array = colloc_pts(N_f,seed*123)\n",
    "    # x_coll_train = torch.from_numpy(x_coll_np_array).float().to(device)        \n",
    "    \n",
    "    # f_hat = torch.zeros(x_coll_train.shape[0],1).to(device)\n",
    "    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(xt_BC, y_BC, xt_coll,f_hat)\n",
    "        loss.backward()\n",
    "        #print(loss.cpu().detach().numpy())\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 467,
     "status": "ok",
     "timestamp": 1660690085956,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "Vt9Dlr8MYIwW"
   },
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "  print(rep) \n",
    "  torch.manual_seed(rep*11)\n",
    "  start_time = time.time() \n",
    "  thresh_flag = 0\n",
    "\n",
    "  xt_coll, xt_BC, y_BC = trainingdata(N_I,N_B,N_f,rep*11)\n",
    "  xt_coll = torch.from_numpy(xt_coll).float().to(device)\n",
    "  xt_BC = torch.from_numpy(xt_BC).float().to(device)\n",
    "  y_BC = torch.from_numpy(y_BC).float().to(device)\n",
    "\n",
    "  f_hat = torch.zeros(xt_coll.shape[0],1).to(device)\n",
    "\n",
    "  for i in range(max_iter):\n",
    "    train_step(xt_BC, y_BC, xt_coll,f_hat,i)\n",
    "    \n",
    "    loss_np = PINN.loss(xt_BC, y_BC, xt_coll,f_hat).cpu().detach().numpy()\n",
    "    if(thresh_flag == 0):\n",
    "        if(loss_np < loss_thresh):\n",
    "            time_threshold[rep] = time.time() - start_time\n",
    "            epoch_threshold[rep] = i+1            \n",
    "            thresh_flag = 1       \n",
    "    data_update(loss_np)\n",
    "    \n",
    "    print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "  elapsed_time[rep] = time.time() - start_time  \n",
    "  print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "sP4Re5lSSq_1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 58.234894 Test MSE 8.478368929871452 Test RE 1.3917595678250378\n",
      "1 Train Loss 54.276566 Test MSE 7.921540478804159 Test RE 1.3452806117606328\n",
      "2 Train Loss 49.209198 Test MSE 8.15753356495465 Test RE 1.3651723859154523\n",
      "3 Train Loss 46.18009 Test MSE 8.466621798569216 Test RE 1.3907950633570423\n",
      "4 Train Loss 44.100857 Test MSE 8.39625227421321 Test RE 1.3850032731141444\n",
      "5 Train Loss 43.068 Test MSE 8.319108377737543 Test RE 1.378625957453261\n",
      "6 Train Loss 42.99762 Test MSE 8.289123295469523 Test RE 1.3761391807681742\n",
      "7 Train Loss 42.81282 Test MSE 8.325525328733946 Test RE 1.3791575570859451\n",
      "8 Train Loss 42.718567 Test MSE 8.368574672924712 Test RE 1.382718610254085\n",
      "9 Train Loss 42.671356 Test MSE 8.354131471550492 Test RE 1.3815248880002458\n",
      "10 Train Loss 42.638687 Test MSE 8.355332386042331 Test RE 1.381624182202444\n",
      "11 Train Loss 42.630928 Test MSE 8.349605739111048 Test RE 1.3811506265320614\n",
      "12 Train Loss 42.608788 Test MSE 8.364175019866568 Test RE 1.3823550906473925\n",
      "13 Train Loss 42.58343 Test MSE 8.344958255585297 Test RE 1.3807661911205618\n",
      "14 Train Loss 42.467373 Test MSE 8.381577651874949 Test RE 1.3837924181033914\n",
      "15 Train Loss 41.721554 Test MSE 8.11145095969271 Test RE 1.3613109369556038\n",
      "16 Train Loss 40.93482 Test MSE 8.140334848802084 Test RE 1.3637325143732892\n",
      "17 Train Loss 39.774418 Test MSE 7.936320232533556 Test RE 1.3465350174288448\n",
      "18 Train Loss 38.260895 Test MSE 7.759312538182432 Test RE 1.3314341220847263\n",
      "19 Train Loss 35.93795 Test MSE 7.440083617812283 Test RE 1.3037579466422402\n",
      "20 Train Loss 34.886223 Test MSE 7.5058499728114505 Test RE 1.3095075298292798\n",
      "21 Train Loss 34.236977 Test MSE 7.40696682171393 Test RE 1.3008531110672423\n",
      "22 Train Loss 32.90562 Test MSE 7.2076626746059365 Test RE 1.2832323123659481\n",
      "23 Train Loss 28.789072 Test MSE 6.9853756617792016 Test RE 1.2632896663121398\n",
      "24 Train Loss 27.391094 Test MSE 6.82679843973105 Test RE 1.2488681806627795\n",
      "25 Train Loss 26.717327 Test MSE 6.814046904362735 Test RE 1.2477012771501095\n",
      "26 Train Loss 26.312693 Test MSE 6.757930578798088 Test RE 1.2425530031652114\n",
      "27 Train Loss 25.714642 Test MSE 6.757720832047894 Test RE 1.2425337203778557\n",
      "28 Train Loss 25.516317 Test MSE 6.738454406188748 Test RE 1.240761209233826\n",
      "29 Train Loss 25.261219 Test MSE 6.744217912091101 Test RE 1.2412917170963138\n",
      "30 Train Loss 25.008911 Test MSE 6.67346860860975 Test RE 1.2347637505544042\n",
      "31 Train Loss 24.834679 Test MSE 6.621694543388743 Test RE 1.2299646558910309\n",
      "32 Train Loss 24.566284 Test MSE 6.512934912730658 Test RE 1.219821908809174\n",
      "33 Train Loss 24.106766 Test MSE 6.311536510046569 Test RE 1.2008136324824628\n",
      "34 Train Loss 23.611202 Test MSE 6.334012084860965 Test RE 1.2029497994946239\n",
      "35 Train Loss 23.052023 Test MSE 6.145396539762641 Test RE 1.1849035920659448\n",
      "36 Train Loss 22.309345 Test MSE 5.8819455170615065 Test RE 1.1592271897704918\n",
      "37 Train Loss 21.137783 Test MSE 5.355968022512659 Test RE 1.106183173488423\n",
      "38 Train Loss 19.400196 Test MSE 5.271400143715214 Test RE 1.0974154049880047\n",
      "39 Train Loss 18.474554 Test MSE 5.43175312961725 Test RE 1.1139817395318947\n",
      "40 Train Loss 17.816566 Test MSE 5.455147919706251 Test RE 1.116378145144739\n",
      "41 Train Loss 17.543596 Test MSE 5.570987141038017 Test RE 1.128168938556864\n",
      "42 Train Loss 17.090706 Test MSE 5.602289435953572 Test RE 1.1313339799396864\n",
      "43 Train Loss 16.687649 Test MSE 5.648771388114273 Test RE 1.1360175995112791\n",
      "44 Train Loss 16.283943 Test MSE 5.9762770188049705 Test RE 1.1684857498300347\n",
      "45 Train Loss 15.773754 Test MSE 5.88643586603721 Test RE 1.1596695894437064\n",
      "46 Train Loss 15.440876 Test MSE 5.843962930687875 Test RE 1.1554782806147075\n",
      "47 Train Loss 15.235557 Test MSE 5.830059537351556 Test RE 1.1541029608431077\n",
      "48 Train Loss 15.1711035 Test MSE 5.817694212584962 Test RE 1.1528784079162067\n",
      "49 Train Loss 15.015666 Test MSE 5.7991702080181415 Test RE 1.1510415159299199\n",
      "50 Train Loss 14.9191885 Test MSE 5.82701103220993 Test RE 1.1538011844348934\n",
      "51 Train Loss 14.816605 Test MSE 5.828968087552181 Test RE 1.1539949255479862\n",
      "52 Train Loss 14.735058 Test MSE 5.822111026380323 Test RE 1.1533159595441993\n",
      "53 Train Loss 14.624115 Test MSE 5.847577182983282 Test RE 1.1558355334392374\n",
      "54 Train Loss 14.482633 Test MSE 5.8021272969205295 Test RE 1.1513349456946282\n",
      "55 Train Loss 14.443511 Test MSE 5.8006155893761955 Test RE 1.1511849494073159\n",
      "56 Train Loss 14.353781 Test MSE 5.8205378565909385 Test RE 1.153160132525751\n",
      "57 Train Loss 14.248617 Test MSE 5.883745188116136 Test RE 1.1594045178381542\n",
      "58 Train Loss 14.185324 Test MSE 5.894005689397683 Test RE 1.160415004303\n",
      "59 Train Loss 14.120859 Test MSE 5.910705494380677 Test RE 1.1620577747736252\n",
      "60 Train Loss 14.057174 Test MSE 5.928827104763361 Test RE 1.163837785828672\n",
      "61 Train Loss 13.9431925 Test MSE 5.946621963988301 Test RE 1.1655830562175453\n",
      "62 Train Loss 13.869029 Test MSE 5.926742403998737 Test RE 1.1636331528720063\n",
      "63 Train Loss 13.804356 Test MSE 5.926397695924594 Test RE 1.1635993130702815\n",
      "64 Train Loss 13.71746 Test MSE 5.968273398502733 Test RE 1.16770305104431\n",
      "65 Train Loss 13.623615 Test MSE 6.034289662631251 Test RE 1.1741433888953432\n",
      "66 Train Loss 13.510652 Test MSE 6.067515936253792 Test RE 1.1773715115136094\n",
      "67 Train Loss 13.437544 Test MSE 6.07087638985541 Test RE 1.177697506097625\n",
      "68 Train Loss 13.261571 Test MSE 6.096214734720544 Test RE 1.1801526568289487\n",
      "69 Train Loss 13.196836 Test MSE 6.079068763045904 Test RE 1.178491862967961\n",
      "70 Train Loss 13.107966 Test MSE 6.0960044389590005 Test RE 1.1801323013089366\n",
      "71 Train Loss 13.035291 Test MSE 6.092843948636235 Test RE 1.1798263402231601\n",
      "72 Train Loss 12.952213 Test MSE 6.078599146417118 Test RE 1.1784463420102107\n",
      "73 Train Loss 12.834504 Test MSE 6.0398408931353575 Test RE 1.1746833399591035\n",
      "74 Train Loss 12.734466 Test MSE 6.009171567866717 Test RE 1.171697119208684\n",
      "75 Train Loss 12.674599 Test MSE 6.067094467815808 Test RE 1.1773306188686845\n",
      "76 Train Loss 12.633308 Test MSE 6.063799610541639 Test RE 1.1770108889569446\n",
      "77 Train Loss 12.5798645 Test MSE 6.06353174679111 Test RE 1.1769848918880064\n",
      "78 Train Loss 12.525701 Test MSE 6.020640742813141 Test RE 1.1728147435794665\n",
      "79 Train Loss 12.4795265 Test MSE 5.997459746284545 Test RE 1.1705547487225196\n",
      "80 Train Loss 12.362087 Test MSE 6.0196409981280805 Test RE 1.1727173649087008\n",
      "81 Train Loss 12.320835 Test MSE 5.968151182122043 Test RE 1.1676910950595798\n",
      "82 Train Loss 12.250147 Test MSE 5.9499190913096855 Test RE 1.1659061424342985\n",
      "83 Train Loss 12.080301 Test MSE 5.941604474498678 Test RE 1.1650912194299865\n",
      "84 Train Loss 11.95103 Test MSE 5.982904101363036 Test RE 1.1691334361711205\n",
      "85 Train Loss 11.777568 Test MSE 5.762293965436973 Test RE 1.1473760105684476\n",
      "86 Train Loss 10.996244 Test MSE 5.074823993903679 Test RE 1.0767591043636477\n",
      "87 Train Loss 10.1665 Test MSE 5.048765794379028 Test RE 1.0739910758615252\n",
      "88 Train Loss 9.865364 Test MSE 5.128887056229353 Test RE 1.0824793694167443\n",
      "89 Train Loss 9.6670265 Test MSE 5.185157331347674 Test RE 1.0884012441819166\n",
      "90 Train Loss 9.542353 Test MSE 5.115920854814 Test RE 1.0811102100461951\n",
      "91 Train Loss 9.4185505 Test MSE 5.115401418180047 Test RE 1.08105532427689\n",
      "92 Train Loss 9.304033 Test MSE 5.023983067993529 Test RE 1.071351899151128\n",
      "93 Train Loss 9.216801 Test MSE 4.951368367241474 Test RE 1.0635812663208575\n",
      "94 Train Loss 9.100878 Test MSE 5.004104705235886 Test RE 1.0692302927150406\n",
      "95 Train Loss 8.969204 Test MSE 4.953904521850917 Test RE 1.0638536214566408\n",
      "96 Train Loss 8.699209 Test MSE 4.891411793159854 Test RE 1.0571221516689453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "97 Train Loss 8.478026 Test MSE 4.814268591871501 Test RE 1.0487530054311536\n",
      "98 Train Loss 8.133154 Test MSE 4.8634199919197245 Test RE 1.0540930457275544\n",
      "99 Train Loss 7.9841175 Test MSE 4.745120020371364 Test RE 1.0411940104463138\n",
      "Training time: 155.79\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 56.48062 Test MSE 8.811249046685774 Test RE 1.4188183561350123\n",
      "1 Train Loss 52.307945 Test MSE 8.19535171037245 Test RE 1.3683331813201243\n",
      "2 Train Loss 51.83358 Test MSE 8.39001336676246 Test RE 1.3844886081940437\n",
      "3 Train Loss 45.888504 Test MSE 8.29515696309051 Test RE 1.3766399367406887\n",
      "4 Train Loss 43.101677 Test MSE 8.31627258471834 Test RE 1.378390966459634\n",
      "5 Train Loss 41.874306 Test MSE 8.213910441178538 Test RE 1.3698816302192296\n",
      "6 Train Loss 41.685974 Test MSE 8.308825635333077 Test RE 1.3777736762833646\n",
      "7 Train Loss 38.2628 Test MSE 8.380204281325987 Test RE 1.3836790422159049\n",
      "8 Train Loss 36.80714 Test MSE 8.615881537412 Test RE 1.403000803716728\n",
      "9 Train Loss 36.203094 Test MSE 8.738884528019934 Test RE 1.4129801496630623\n",
      "10 Train Loss 35.755558 Test MSE 8.752775351911495 Test RE 1.41410269939374\n",
      "11 Train Loss 35.337215 Test MSE 8.665409179792738 Test RE 1.4070275389560263\n",
      "12 Train Loss 34.88706 Test MSE 8.877385353808863 Test RE 1.4241331522621987\n",
      "13 Train Loss 34.67066 Test MSE 8.914017529685259 Test RE 1.42706844155965\n",
      "14 Train Loss 34.243534 Test MSE 9.07797858778281 Test RE 1.440133116508504\n",
      "15 Train Loss 33.85276 Test MSE 9.014690667168022 Test RE 1.4351043296193517\n",
      "16 Train Loss 33.10945 Test MSE 8.769445409139838 Test RE 1.4154486701635816\n",
      "17 Train Loss 32.11759 Test MSE 8.256326551225424 Test RE 1.3734140664904746\n",
      "18 Train Loss 31.039555 Test MSE 7.97390634343959 Test RE 1.3497198204320282\n",
      "19 Train Loss 29.99984 Test MSE 7.196740784670709 Test RE 1.2822596922428529\n",
      "20 Train Loss 28.783257 Test MSE 7.090856024855088 Test RE 1.272791874113213\n",
      "21 Train Loss 28.23329 Test MSE 6.894726455720355 Test RE 1.255066045972252\n",
      "22 Train Loss 27.784382 Test MSE 6.302715180539202 Test RE 1.199974179478222\n",
      "23 Train Loss 27.340544 Test MSE 6.444976791887856 Test RE 1.2134412075916028\n",
      "24 Train Loss 26.401966 Test MSE 6.485009030770474 Test RE 1.2172039495654898\n",
      "25 Train Loss 25.408234 Test MSE 6.329013612145249 Test RE 1.202475053124698\n",
      "26 Train Loss 24.408798 Test MSE 6.4479215157054135 Test RE 1.2137183878885582\n",
      "27 Train Loss 23.816925 Test MSE 6.637962847025915 Test RE 1.2314746291717906\n",
      "28 Train Loss 22.827545 Test MSE 6.462998253005711 Test RE 1.2151365370515823\n",
      "29 Train Loss 22.519257 Test MSE 6.354575048717925 Test RE 1.2049008669466004\n",
      "30 Train Loss 22.341915 Test MSE 6.208768903213216 Test RE 1.1909973849415256\n",
      "31 Train Loss 22.136463 Test MSE 6.124553917351692 Test RE 1.1828925360141418\n",
      "32 Train Loss 21.957636 Test MSE 6.2092065894922 Test RE 1.1910393637971735\n",
      "33 Train Loss 21.484913 Test MSE 6.049595540053826 Test RE 1.1756315436117177\n",
      "34 Train Loss 21.24384 Test MSE 5.914227498038171 Test RE 1.1624039400689463\n",
      "35 Train Loss 21.12659 Test MSE 5.701649850180187 Test RE 1.1413223757117292\n",
      "36 Train Loss 20.8265 Test MSE 5.001822543734609 Test RE 1.068986449447326\n",
      "37 Train Loss 10.694499 Test MSE 2.570964404620196 Test RE 0.7664010519674317\n",
      "38 Train Loss 7.558949 Test MSE 2.500282423599286 Test RE 0.7557925283488949\n",
      "39 Train Loss 7.1151395 Test MSE 2.4951425521939643 Test RE 0.7550152811725327\n",
      "40 Train Loss 6.9419937 Test MSE 2.532689673379261 Test RE 0.760674836967417\n",
      "41 Train Loss 6.767159 Test MSE 2.4437316677686893 Test RE 0.7471964823813011\n",
      "42 Train Loss 6.5566225 Test MSE 2.41327040083758 Test RE 0.742524953739774\n",
      "43 Train Loss 6.374569 Test MSE 2.3723250155570583 Test RE 0.7361988835134723\n",
      "44 Train Loss 6.1674833 Test MSE 2.3530767887817627 Test RE 0.7332061688021151\n",
      "45 Train Loss 5.849164 Test MSE 2.3574812725934318 Test RE 0.7338920548164902\n",
      "46 Train Loss 5.4673495 Test MSE 2.229457048619017 Test RE 0.7136867193133855\n",
      "47 Train Loss 5.2202363 Test MSE 2.1710470551319623 Test RE 0.7042756587066652\n",
      "48 Train Loss 5.014731 Test MSE 2.072919002045002 Test RE 0.6881755309473386\n",
      "49 Train Loss 4.7897024 Test MSE 1.9616166154282473 Test RE 0.6694453449548642\n",
      "50 Train Loss 4.4555106 Test MSE 1.8933669354329619 Test RE 0.6576963846912788\n",
      "51 Train Loss 4.2807455 Test MSE 1.8524464146013644 Test RE 0.650550308743306\n",
      "52 Train Loss 4.196974 Test MSE 1.8073547977576527 Test RE 0.6425837940142185\n",
      "53 Train Loss 4.1272883 Test MSE 1.8299921235327237 Test RE 0.6465954891651607\n",
      "54 Train Loss 4.032482 Test MSE 1.8280541548183027 Test RE 0.6462530249154044\n",
      "55 Train Loss 3.9175797 Test MSE 1.8125429288854902 Test RE 0.6435054226662705\n",
      "56 Train Loss 3.8858151 Test MSE 1.798248231814703 Test RE 0.6409628833090348\n",
      "57 Train Loss 3.8471937 Test MSE 1.798045071033725 Test RE 0.6409266752383744\n",
      "58 Train Loss 3.8018665 Test MSE 1.7922702963656332 Test RE 0.639896616604673\n",
      "59 Train Loss 3.7314837 Test MSE 1.7846780636146455 Test RE 0.6385398458135824\n",
      "60 Train Loss 3.6965463 Test MSE 1.7943192641154242 Test RE 0.6402622849469339\n",
      "61 Train Loss 3.6096926 Test MSE 1.7951453587626254 Test RE 0.64040965459105\n",
      "62 Train Loss 3.5440571 Test MSE 1.770227114290684 Test RE 0.6359493891748731\n",
      "63 Train Loss 3.446491 Test MSE 1.737107227236676 Test RE 0.629972183959758\n",
      "64 Train Loss 3.4033763 Test MSE 1.7579615452185213 Test RE 0.6337423729738771\n",
      "65 Train Loss 3.3613014 Test MSE 1.7636799848687197 Test RE 0.6347722803636819\n",
      "66 Train Loss 3.3215437 Test MSE 1.746592324507517 Test RE 0.6316897557856247\n",
      "67 Train Loss 3.2928925 Test MSE 1.744317000611594 Test RE 0.6312781636867579\n",
      "68 Train Loss 3.2808642 Test MSE 1.7498422882831233 Test RE 0.6322771895757835\n",
      "69 Train Loss 3.2727094 Test MSE 1.741457908698702 Test RE 0.630760590765763\n",
      "70 Train Loss 3.26928 Test MSE 1.7478637741714518 Test RE 0.6319196364515867\n",
      "71 Train Loss 3.2551908 Test MSE 1.7194425951455683 Test RE 0.626760907717333\n",
      "72 Train Loss 3.2288535 Test MSE 1.6643852539634656 Test RE 0.6166446813207237\n",
      "73 Train Loss 3.2050853 Test MSE 1.6607279959199988 Test RE 0.6159668127301374\n",
      "74 Train Loss 3.187211 Test MSE 1.6567029104963555 Test RE 0.6152199044037014\n",
      "75 Train Loss 3.172839 Test MSE 1.6575977231507204 Test RE 0.6153860272003218\n",
      "76 Train Loss 3.1489966 Test MSE 1.6432594352950824 Test RE 0.6127186894205431\n",
      "77 Train Loss 3.1267223 Test MSE 1.6492159451256525 Test RE 0.6138281805514537\n",
      "78 Train Loss 3.0981286 Test MSE 1.6175617470516632 Test RE 0.607908889637891\n",
      "79 Train Loss 3.0898206 Test MSE 1.6284289790676778 Test RE 0.6099475222761764\n",
      "80 Train Loss 3.0734344 Test MSE 1.611391306642888 Test RE 0.6067483003499934\n",
      "81 Train Loss 3.0442114 Test MSE 1.597886714147633 Test RE 0.6042004621978655\n",
      "82 Train Loss 3.0263758 Test MSE 1.5968536911711448 Test RE 0.6040051248611761\n",
      "83 Train Loss 3.0039217 Test MSE 1.6303499511286779 Test RE 0.6103071777544866\n",
      "84 Train Loss 2.9423637 Test MSE 1.638306458853188 Test RE 0.611794589604544\n",
      "85 Train Loss 2.8997211 Test MSE 1.647484345482366 Test RE 0.6135058504744739\n",
      "86 Train Loss 2.8859358 Test MSE 1.6465100951320686 Test RE 0.6133244233543433\n",
      "87 Train Loss 2.8799355 Test MSE 1.6417662969804852 Test RE 0.6124402544851442\n",
      "88 Train Loss 2.8719811 Test MSE 1.6380052101382916 Test RE 0.6117383391985307\n",
      "89 Train Loss 2.8607814 Test MSE 1.633466294860486 Test RE 0.6108901872339175\n",
      "90 Train Loss 2.8412457 Test MSE 1.6178776928966514 Test RE 0.6079682558136716\n",
      "91 Train Loss 2.8211827 Test MSE 1.6303878345269363 Test RE 0.6103142683719185\n",
      "92 Train Loss 2.8057284 Test MSE 1.6338453234518153 Test RE 0.6109610584289729\n",
      "93 Train Loss 2.7826936 Test MSE 1.6227883395135472 Test RE 0.608890221442818\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "94 Train Loss 2.7643912 Test MSE 1.635398227150764 Test RE 0.6112513363214565\n",
      "95 Train Loss 2.747778 Test MSE 1.6292913402850522 Test RE 0.6101090047466158\n",
      "96 Train Loss 2.7293153 Test MSE 1.6498707382215096 Test RE 0.6139500234698606\n",
      "97 Train Loss 2.6832957 Test MSE 1.6377979809568008 Test RE 0.6116996415056583\n",
      "98 Train Loss 2.611024 Test MSE 1.628266487908211 Test RE 0.609917090014797\n",
      "99 Train Loss 2.5940313 Test MSE 1.6406168469580524 Test RE 0.612225823013911\n",
      "Training time: 160.20\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 56.217014 Test MSE 8.59937227793766 Test RE 1.401655984238862\n",
      "1 Train Loss 51.949783 Test MSE 8.172560441109125 Test RE 1.3664291896807568\n",
      "2 Train Loss 43.96778 Test MSE 8.375778883425966 Test RE 1.3833136490384828\n",
      "3 Train Loss 43.31934 Test MSE 8.45044342670268 Test RE 1.3894656335438376\n",
      "4 Train Loss 42.87336 Test MSE 8.533297195559573 Test RE 1.3962606411708458\n",
      "5 Train Loss 42.712772 Test MSE 8.448471220073111 Test RE 1.389303483873027\n",
      "6 Train Loss 42.54799 Test MSE 8.530187309947278 Test RE 1.396006190489487\n",
      "7 Train Loss 42.222324 Test MSE 8.398771840712717 Test RE 1.385211064994473\n",
      "8 Train Loss 41.668907 Test MSE 8.39792792787216 Test RE 1.3851414698921392\n",
      "9 Train Loss 38.50293 Test MSE 7.840334218141574 Test RE 1.3383673964845235\n",
      "10 Train Loss 37.988266 Test MSE 7.670005010306234 Test RE 1.3237497286831243\n",
      "11 Train Loss 33.412697 Test MSE 7.292497766377496 Test RE 1.2907621229398423\n",
      "12 Train Loss 32.957626 Test MSE 7.198574367172273 Test RE 1.2824230286244644\n",
      "13 Train Loss 32.8322 Test MSE 7.226007410450226 Test RE 1.284864297601865\n",
      "14 Train Loss 32.763252 Test MSE 7.2141976321089 Test RE 1.2838139134388376\n",
      "15 Train Loss 32.66312 Test MSE 7.225244395896876 Test RE 1.2847964595840051\n",
      "16 Train Loss 32.563335 Test MSE 7.173649720921041 Test RE 1.2802009458183834\n",
      "17 Train Loss 32.36172 Test MSE 7.172007324618031 Test RE 1.2800543873897783\n",
      "18 Train Loss 32.08178 Test MSE 7.135167477946624 Test RE 1.2767625809182244\n",
      "19 Train Loss 31.910477 Test MSE 7.16740162951454 Test RE 1.2796433109117566\n",
      "20 Train Loss 31.708166 Test MSE 7.168750312052574 Test RE 1.2797636998266073\n",
      "21 Train Loss 31.54722 Test MSE 7.158316447125518 Test RE 1.2788320350870817\n",
      "22 Train Loss 31.37008 Test MSE 7.2227744989166505 Test RE 1.284576841674781\n",
      "23 Train Loss 31.282593 Test MSE 7.2412101935391435 Test RE 1.2862151991972854\n",
      "24 Train Loss 31.155666 Test MSE 7.2991188732585215 Test RE 1.2913479534237577\n",
      "25 Train Loss 30.925241 Test MSE 7.3189060920855065 Test RE 1.2930971296617186\n",
      "26 Train Loss 30.774319 Test MSE 7.276020819016397 Test RE 1.28930309976572\n",
      "27 Train Loss 30.461521 Test MSE 7.29175886722107 Test RE 1.2906967292136708\n",
      "28 Train Loss 29.912243 Test MSE 7.293895141883035 Test RE 1.2908857837921095\n",
      "29 Train Loss 29.11582 Test MSE 7.1441036245039 Test RE 1.2775618450927324\n",
      "30 Train Loss 28.184769 Test MSE 7.265158256584784 Test RE 1.2883403230758885\n",
      "31 Train Loss 27.825369 Test MSE 7.323867305925413 Test RE 1.293535326605708\n",
      "32 Train Loss 27.360907 Test MSE 6.873674801338491 Test RE 1.2531485358573708\n",
      "33 Train Loss 26.571796 Test MSE 6.416767832901558 Test RE 1.210782745683634\n",
      "34 Train Loss 25.359142 Test MSE 6.404829428638856 Test RE 1.2096558899339673\n",
      "35 Train Loss 21.426748 Test MSE 5.669373810587396 Test RE 1.1380873781493213\n",
      "36 Train Loss 17.841919 Test MSE 4.785799205651019 Test RE 1.0456474842622197\n",
      "37 Train Loss 16.685793 Test MSE 4.471418083375898 Test RE 1.0107196280110418\n",
      "38 Train Loss 15.438695 Test MSE 4.466501671764129 Test RE 1.0101638221924432\n",
      "39 Train Loss 14.501027 Test MSE 4.469559218287221 Test RE 1.010509517180422\n",
      "40 Train Loss 13.96159 Test MSE 4.928251652119715 Test RE 1.0610955626141754\n",
      "41 Train Loss 13.501075 Test MSE 5.170670717608805 Test RE 1.086879759326293\n",
      "42 Train Loss 13.023376 Test MSE 5.106773668110756 Test RE 1.080143273538401\n",
      "43 Train Loss 12.622654 Test MSE 5.38703595954165 Test RE 1.109386808743684\n",
      "44 Train Loss 12.19236 Test MSE 5.647404161114448 Test RE 1.13588011050503\n",
      "45 Train Loss 12.0259285 Test MSE 5.703662625711045 Test RE 1.1415238106566339\n",
      "46 Train Loss 11.571524 Test MSE 5.616071615598215 Test RE 1.1327247212153468\n",
      "47 Train Loss 11.25138 Test MSE 5.679258616991808 Test RE 1.1390790991426458\n",
      "48 Train Loss 10.874084 Test MSE 5.615753123989095 Test RE 1.1326926019281673\n",
      "49 Train Loss 10.622484 Test MSE 5.440932098682185 Test RE 1.1149225856552474\n",
      "50 Train Loss 10.475569 Test MSE 5.519386314846527 Test RE 1.1229319960323665\n",
      "51 Train Loss 10.359425 Test MSE 5.621161731889334 Test RE 1.133237926451143\n",
      "52 Train Loss 10.180986 Test MSE 5.766368021971058 Test RE 1.1477815477396007\n",
      "53 Train Loss 10.100565 Test MSE 5.8391092140335585 Test RE 1.1549983383876103\n",
      "54 Train Loss 10.024837 Test MSE 5.784991264428067 Test RE 1.1496335092435248\n",
      "55 Train Loss 9.820455 Test MSE 6.567992275805943 Test RE 1.2249669655358713\n",
      "56 Train Loss 9.456678 Test MSE 6.4249341485194 Test RE 1.2115529535533094\n",
      "57 Train Loss 9.249843 Test MSE 6.502202293485288 Test RE 1.218816426471013\n",
      "58 Train Loss 9.083426 Test MSE 6.477273578478277 Test RE 1.2164777799873012\n",
      "59 Train Loss 8.830845 Test MSE 6.460482394148806 Test RE 1.2149000052231098\n",
      "60 Train Loss 8.723751 Test MSE 6.466769602899671 Test RE 1.2154910193313166\n",
      "61 Train Loss 8.66963 Test MSE 6.379631726034095 Test RE 1.2072740476258705\n",
      "62 Train Loss 8.616274 Test MSE 6.331646892553502 Test RE 1.2027251809407167\n",
      "63 Train Loss 8.547396 Test MSE 6.346627259140751 Test RE 1.2041471348191393\n",
      "64 Train Loss 8.50869 Test MSE 6.3445635701158585 Test RE 1.2039513467915588\n",
      "65 Train Loss 8.457153 Test MSE 6.335407516377588 Test RE 1.2030823017378174\n",
      "66 Train Loss 8.404302 Test MSE 6.357136193960677 Test RE 1.205143653854046\n",
      "67 Train Loss 8.250158 Test MSE 6.213575829358142 Test RE 1.1914583401552692\n",
      "68 Train Loss 8.043916 Test MSE 5.960942695012739 Test RE 1.166985698267554\n",
      "69 Train Loss 7.4990883 Test MSE 5.334877847185458 Test RE 1.1040031185926351\n",
      "70 Train Loss 6.807045 Test MSE 5.140611623891569 Test RE 1.0837159298576777\n",
      "71 Train Loss 6.3677273 Test MSE 5.018880952830402 Test RE 1.0708077542771082\n",
      "72 Train Loss 5.9599504 Test MSE 5.010628834263372 Test RE 1.0699270731181567\n",
      "73 Train Loss 5.7812157 Test MSE 4.730527382877649 Test RE 1.0395917890051525\n",
      "74 Train Loss 5.563187 Test MSE 4.1994976144429845 Test RE 0.9795051634739264\n",
      "75 Train Loss 5.3635335 Test MSE 3.730869698453024 Test RE 0.9232367872018895\n",
      "76 Train Loss 4.6363134 Test MSE 1.3263608612388424 Test RE 0.5504766594116853\n",
      "77 Train Loss 2.554875 Test MSE 0.2917403058523297 Test RE 0.25817029870572433\n",
      "78 Train Loss 1.0559676 Test MSE 0.15271558729033488 Test RE 0.18678833186573185\n",
      "79 Train Loss 0.7415165 Test MSE 0.14761777927380035 Test RE 0.18364427502817848\n",
      "80 Train Loss 0.541136 Test MSE 0.12137070284156976 Test RE 0.16651945994841913\n",
      "81 Train Loss 0.4700418 Test MSE 0.1231304335761412 Test RE 0.16772228275143142\n",
      "82 Train Loss 0.4063539 Test MSE 0.11675200745352217 Test RE 0.1633203261728579\n",
      "83 Train Loss 0.37202886 Test MSE 0.10135582902553647 Test RE 0.15217118598056384\n",
      "84 Train Loss 0.33113787 Test MSE 0.07653578094237139 Test RE 0.1322331425756603\n",
      "85 Train Loss 0.30636156 Test MSE 0.06922024348192195 Test RE 0.12575481468643493\n",
      "86 Train Loss 0.2767748 Test MSE 0.05187409525152328 Test RE 0.10886375942344155\n",
      "87 Train Loss 0.2524236 Test MSE 0.054101952854319214 Test RE 0.11117689253946539\n",
      "88 Train Loss 0.23341177 Test MSE 0.04654728877390952 Test RE 0.10312293265947421\n",
      "89 Train Loss 0.21190116 Test MSE 0.04164059389133468 Test RE 0.09753635411636297\n",
      "90 Train Loss 0.20482396 Test MSE 0.03877294671444815 Test RE 0.09411795114312678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "91 Train Loss 0.1997531 Test MSE 0.03247856846326618 Test RE 0.08614032375683801\n",
      "92 Train Loss 0.1534611 Test MSE 0.022556634341844798 Test RE 0.07178690174193834\n",
      "93 Train Loss 0.13124685 Test MSE 0.01599851216618634 Test RE 0.0604571761581361\n",
      "94 Train Loss 0.11760164 Test MSE 0.015128537280280794 Test RE 0.0587904155829883\n",
      "95 Train Loss 0.105784185 Test MSE 0.011978696401927386 Test RE 0.05231338701960943\n",
      "96 Train Loss 0.09945594 Test MSE 0.010521093950961687 Test RE 0.04902736016410579\n",
      "97 Train Loss 0.08869704 Test MSE 0.009297338678613308 Test RE 0.04608794946616616\n",
      "98 Train Loss 0.07918316 Test MSE 0.00877747198390474 Test RE 0.04478089675580298\n",
      "99 Train Loss 0.06733979 Test MSE 0.0063587514097013615 Test RE 0.03811482970796047\n",
      "Training time: 159.10\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 56.18125 Test MSE 8.453260455742338 Test RE 1.3896972094914395\n",
      "1 Train Loss 55.733658 Test MSE 8.67106292615324 Test RE 1.407486471680921\n",
      "2 Train Loss 52.81041 Test MSE 8.814212848242885 Test RE 1.4190569569552753\n",
      "3 Train Loss 51.21601 Test MSE 8.716235166486806 Test RE 1.4111478871244774\n",
      "4 Train Loss 45.010796 Test MSE 8.751539716408521 Test RE 1.4140028809303522\n",
      "5 Train Loss 42.838608 Test MSE 8.394065796885611 Test RE 1.3848229262563199\n",
      "6 Train Loss 42.416847 Test MSE 8.427476999644849 Test RE 1.3875762195322927\n",
      "7 Train Loss 42.25016 Test MSE 8.335021772097308 Test RE 1.3799438953930507\n",
      "8 Train Loss 41.776833 Test MSE 8.25584229558045 Test RE 1.3733737886948372\n",
      "9 Train Loss 39.043835 Test MSE 7.287775180583097 Test RE 1.2903441096422903\n",
      "10 Train Loss 37.521225 Test MSE 7.722498302724511 Test RE 1.328271856973052\n",
      "11 Train Loss 37.090813 Test MSE 7.790744971374029 Test RE 1.3341281698138765\n",
      "12 Train Loss 36.68196 Test MSE 7.619250420790536 Test RE 1.3193626460743055\n",
      "13 Train Loss 36.510418 Test MSE 7.4893742189310055 Test RE 1.3080695197103374\n",
      "14 Train Loss 36.260647 Test MSE 7.395343003114958 Test RE 1.299831990145319\n",
      "15 Train Loss 36.19943 Test MSE 7.337248847664109 Test RE 1.2947165056774232\n",
      "16 Train Loss 36.102432 Test MSE 7.534118286651907 Test RE 1.311971127011358\n",
      "17 Train Loss 35.833298 Test MSE 7.615810896249575 Test RE 1.3190648154345006\n",
      "18 Train Loss 35.71134 Test MSE 7.361255294887448 Test RE 1.2968328412415575\n",
      "19 Train Loss 33.16845 Test MSE 7.447875819927852 Test RE 1.3044404985701734\n",
      "20 Train Loss 32.18098 Test MSE 7.6836809584437 Test RE 1.3249293543779346\n",
      "21 Train Loss 31.086662 Test MSE 7.55721418166364 Test RE 1.313980516808813\n",
      "22 Train Loss 30.869158 Test MSE 7.577292362721949 Test RE 1.3157248658525067\n",
      "23 Train Loss 30.02699 Test MSE 7.605913738651515 Test RE 1.31820743880623\n",
      "24 Train Loss 29.63816 Test MSE 7.4882150298062715 Test RE 1.3079682857090533\n",
      "25 Train Loss 29.427757 Test MSE 7.477147723973086 Test RE 1.3070013638479123\n",
      "26 Train Loss 29.293684 Test MSE 7.378130240340386 Test RE 1.2983184204819436\n",
      "27 Train Loss 29.153263 Test MSE 7.296969745416295 Test RE 1.2911578293687593\n",
      "28 Train Loss 29.01984 Test MSE 7.203988535084747 Test RE 1.2829052038590467\n",
      "29 Train Loss 28.792215 Test MSE 7.19785549317221 Test RE 1.2823589934737172\n",
      "30 Train Loss 28.67028 Test MSE 7.335406712549239 Test RE 1.294553965704463\n",
      "31 Train Loss 28.59464 Test MSE 7.3682272804257885 Test RE 1.2974468234783894\n",
      "32 Train Loss 28.389267 Test MSE 7.3545378642375265 Test RE 1.296241001086392\n",
      "33 Train Loss 28.234362 Test MSE 7.258739321873579 Test RE 1.2877710581288282\n",
      "34 Train Loss 27.937819 Test MSE 7.207425669728239 Test RE 1.2832112143463013\n",
      "35 Train Loss 27.903194 Test MSE 7.189329481486677 Test RE 1.28159927779133\n",
      "36 Train Loss 27.752102 Test MSE 7.183504326483635 Test RE 1.2810799645987316\n",
      "37 Train Loss 27.579308 Test MSE 7.309538523962771 Test RE 1.292269338340436\n",
      "38 Train Loss 27.45592 Test MSE 7.397165257760062 Test RE 1.2999921232850666\n",
      "39 Train Loss 27.361053 Test MSE 7.485794511507009 Test RE 1.307756872372002\n",
      "40 Train Loss 27.289886 Test MSE 7.508480017029779 Test RE 1.3097369349648578\n",
      "41 Train Loss 27.208344 Test MSE 7.5576849282183085 Test RE 1.3140214407626711\n",
      "42 Train Loss 27.160053 Test MSE 7.453562337821477 Test RE 1.3049383793808658\n",
      "43 Train Loss 27.115791 Test MSE 7.4713423444442935 Test RE 1.3064938767281133\n",
      "44 Train Loss 27.080568 Test MSE 7.428139682856452 Test RE 1.3027110326853268\n",
      "45 Train Loss 26.999752 Test MSE 7.455950730780374 Test RE 1.305147437537177\n",
      "46 Train Loss 26.941874 Test MSE 7.4888565727599286 Test RE 1.3080243137385739\n",
      "47 Train Loss 26.870403 Test MSE 7.390061553296857 Test RE 1.299367763973687\n",
      "48 Train Loss 26.775078 Test MSE 7.379414650884853 Test RE 1.2984314234547212\n",
      "49 Train Loss 26.704618 Test MSE 7.327513171195432 Test RE 1.293857251386133\n",
      "50 Train Loss 26.385508 Test MSE 7.032278101046428 Test RE 1.2675236719857743\n",
      "51 Train Loss 26.276653 Test MSE 6.814592215734153 Test RE 1.2477512013795915\n",
      "52 Train Loss 26.2111 Test MSE 6.8224577182336095 Test RE 1.2484710801041758\n",
      "53 Train Loss 25.485876 Test MSE 6.144344290127453 Test RE 1.184802144767903\n",
      "54 Train Loss 24.34319 Test MSE 5.663927191160035 Test RE 1.1375405612926013\n",
      "55 Train Loss 23.737404 Test MSE 5.7389334174191236 Test RE 1.1450478966979434\n",
      "56 Train Loss 22.685034 Test MSE 6.019142616297372 Test RE 1.1726688177341125\n",
      "57 Train Loss 22.393446 Test MSE 6.007068007715626 Test RE 1.1714920201297774\n",
      "58 Train Loss 22.09042 Test MSE 6.067454126284102 Test RE 1.1773655145386082\n",
      "59 Train Loss 21.675074 Test MSE 5.807361614446293 Test RE 1.151854159925565\n",
      "60 Train Loss 21.31992 Test MSE 5.6613611200266005 Test RE 1.1372828478199588\n",
      "61 Train Loss 21.224388 Test MSE 5.68286745148205 Test RE 1.1394409505206258\n",
      "62 Train Loss 20.98367 Test MSE 5.473970889068324 Test RE 1.1183025160637685\n",
      "63 Train Loss 20.688442 Test MSE 5.250931520291335 Test RE 1.0952827239352445\n",
      "64 Train Loss 20.26809 Test MSE 5.137506812759075 Test RE 1.0833886106845372\n",
      "65 Train Loss 19.97365 Test MSE 5.145145097064421 Test RE 1.0841936856916528\n",
      "66 Train Loss 19.553076 Test MSE 5.1817111263774045 Test RE 1.0880394926499781\n",
      "67 Train Loss 19.410593 Test MSE 5.298994929930128 Test RE 1.1002840371049878\n",
      "68 Train Loss 19.391079 Test MSE 5.332263795928127 Test RE 1.1037326087112849\n",
      "69 Train Loss 19.157513 Test MSE 5.356561414247591 Test RE 1.1062444492251298\n",
      "70 Train Loss 18.846737 Test MSE 5.2781078668895995 Test RE 1.0981133996677894\n",
      "71 Train Loss 18.77781 Test MSE 5.225761924547838 Test RE 1.0926545294609338\n",
      "72 Train Loss 18.743975 Test MSE 5.272226639723339 Test RE 1.0975014327868178\n",
      "73 Train Loss 18.704891 Test MSE 5.3136797843802155 Test RE 1.1018075649153964\n",
      "74 Train Loss 18.61093 Test MSE 5.4074880770184715 Test RE 1.1114907314809968\n",
      "75 Train Loss 18.297548 Test MSE 5.270604824185223 Test RE 1.0973326158993915\n",
      "76 Train Loss 18.055595 Test MSE 5.094550565495445 Test RE 1.0788498334339816\n",
      "77 Train Loss 17.985342 Test MSE 5.115468707637698 Test RE 1.0810624345094306\n",
      "78 Train Loss 17.838326 Test MSE 4.907058495436601 Test RE 1.0588115687458453\n",
      "79 Train Loss 17.694244 Test MSE 4.922264039802304 Test RE 1.060450774130581\n",
      "80 Train Loss 17.650661 Test MSE 4.8522205238014635 Test RE 1.0528786651469524\n",
      "81 Train Loss 17.612587 Test MSE 4.847686563392002 Test RE 1.0523866403274365\n",
      "82 Train Loss 17.565392 Test MSE 4.948015337551362 Test RE 1.0632210806854836\n",
      "83 Train Loss 17.409782 Test MSE 5.151637547021549 Test RE 1.0848775200573595\n",
      "84 Train Loss 17.269306 Test MSE 5.118231221086452 Test RE 1.0813542989112752\n",
      "85 Train Loss 17.257254 Test MSE 5.107017430065136 Test RE 1.0801690525047847\n",
      "86 Train Loss 17.21526 Test MSE 5.130264158731972 Test RE 1.0826246821311583\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "87 Train Loss 17.154518 Test MSE 5.16319537136175 Test RE 1.0860938128168918\n",
      "88 Train Loss 17.113878 Test MSE 5.07106116677499 Test RE 1.076359838330219\n",
      "89 Train Loss 17.071491 Test MSE 4.997909761241757 Test RE 1.0685682489062367\n",
      "90 Train Loss 17.039461 Test MSE 4.978930289337091 Test RE 1.0665373847325816\n",
      "91 Train Loss 17.01722 Test MSE 4.98789965167234 Test RE 1.0674976166795709\n",
      "92 Train Loss 17.000982 Test MSE 5.03965869050138 Test RE 1.0730219911922012\n",
      "93 Train Loss 16.888487 Test MSE 4.954229215373788 Test RE 1.0638884849384425\n",
      "94 Train Loss 16.622673 Test MSE 4.846536825508617 Test RE 1.0522618343479384\n",
      "95 Train Loss 16.358397 Test MSE 4.760368002790082 Test RE 1.0428655566101372\n",
      "96 Train Loss 16.202103 Test MSE 4.656171492477265 Test RE 1.0313891158944928\n",
      "97 Train Loss 15.847779 Test MSE 4.995391757433824 Test RE 1.068299036572494\n",
      "98 Train Loss 15.665281 Test MSE 4.875972293151471 Test RE 1.0554524560902263\n",
      "99 Train Loss 15.347095 Test MSE 4.774045012642242 Test RE 1.0443626102221957\n",
      "Training time: 158.17\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 55.77038 Test MSE 8.673225156881536 Test RE 1.4076619473306229\n",
      "1 Train Loss 55.77038 Test MSE 8.673225213008456 Test RE 1.4076619518853144\n",
      "2 Train Loss 55.770374 Test MSE 8.673225501157917 Test RE 1.4076619752686013\n",
      "3 Train Loss 55.770374 Test MSE 8.673225354209736 Test RE 1.4076619633437775\n",
      "4 Train Loss 55.77037 Test MSE 8.673224911520359 Test RE 1.4076619274195983\n",
      "5 Train Loss 55.770363 Test MSE 8.673223328365276 Test RE 1.4076617989467834\n",
      "6 Train Loss 55.770355 Test MSE 8.673221714744741 Test RE 1.4076616680016893\n",
      "7 Train Loss 55.770355 Test MSE 8.673221382549126 Test RE 1.407661641044057\n",
      "8 Train Loss 55.770355 Test MSE 8.673220798650764 Test RE 1.4076615936607781\n",
      "9 Train Loss 55.770348 Test MSE 8.67321977277842 Test RE 1.4076615104113617\n",
      "10 Train Loss 55.770348 Test MSE 8.673219165593752 Test RE 1.4076614611383969\n",
      "11 Train Loss 55.770344 Test MSE 8.67321721551857 Test RE 1.4076613028900133\n",
      "12 Train Loss 55.77034 Test MSE 8.673215792398627 Test RE 1.4076611874039768\n",
      "13 Train Loss 55.770336 Test MSE 8.673213867331498 Test RE 1.407661031184963\n",
      "14 Train Loss 55.770336 Test MSE 8.673213207346755 Test RE 1.407660977627261\n",
      "15 Train Loss 55.770325 Test MSE 8.673209779791797 Test RE 1.4076606994814496\n",
      "16 Train Loss 55.770325 Test MSE 8.67320886455968 Test RE 1.4076606252104271\n",
      "17 Train Loss 55.770317 Test MSE 8.673206237639244 Test RE 1.407660412035996\n",
      "18 Train Loss 55.77031 Test MSE 8.673203025516678 Test RE 1.4076601513724027\n",
      "19 Train Loss 55.77031 Test MSE 8.673202167161195 Test RE 1.4076600817168876\n",
      "20 Train Loss 55.770306 Test MSE 8.673200176894118 Test RE 1.4076599202067777\n",
      "21 Train Loss 55.7703 Test MSE 8.673198097649404 Test RE 1.4076597514761156\n",
      "22 Train Loss 55.77029 Test MSE 8.673193766365085 Test RE 1.4076593999924238\n",
      "23 Train Loss 55.770287 Test MSE 8.673191731807389 Test RE 1.4076592348880512\n",
      "24 Train Loss 55.770283 Test MSE 8.673189534335487 Test RE 1.4076590565631673\n",
      "25 Train Loss 55.77028 Test MSE 8.673187397122526 Test RE 1.4076588831282755\n",
      "26 Train Loss 55.77028 Test MSE 8.67318637446443 Test RE 1.4076588001395358\n",
      "27 Train Loss 55.77028 Test MSE 8.673185147158648 Test RE 1.4076587005436223\n",
      "28 Train Loss 55.77027 Test MSE 8.673183182979717 Test RE 1.4076585411504074\n",
      "29 Train Loss 55.770267 Test MSE 8.673181234303849 Test RE 1.4076583830152503\n",
      "30 Train Loss 55.770264 Test MSE 8.67317915837284 Test RE 1.4076582145533103\n",
      "31 Train Loss 55.77026 Test MSE 8.67317681715326 Test RE 1.407658024563162\n",
      "32 Train Loss 55.77025 Test MSE 8.673173437266723 Test RE 1.407657750285065\n",
      "33 Train Loss 55.77025 Test MSE 8.673172389809643 Test RE 1.4076576652838173\n",
      "34 Train Loss 55.77024 Test MSE 8.673169080204683 Test RE 1.4076573967090082\n",
      "35 Train Loss 55.770237 Test MSE 8.673166795807104 Test RE 1.407657211329862\n",
      "36 Train Loss 55.770233 Test MSE 8.673164350751893 Test RE 1.4076570129133021\n",
      "37 Train Loss 55.770226 Test MSE 8.673161067894242 Test RE 1.4076567465089165\n",
      "38 Train Loss 55.770218 Test MSE 8.673158537197398 Test RE 1.4076565411424582\n",
      "39 Train Loss 55.770218 Test MSE 8.67315760512792 Test RE 1.4076564655048616\n",
      "40 Train Loss 55.770203 Test MSE 8.673151155410311 Test RE 1.40765594210907\n",
      "41 Train Loss 55.7702 Test MSE 8.673148526283812 Test RE 1.4076557287549065\n",
      "42 Train Loss 55.770195 Test MSE 8.673145991947763 Test RE 1.4076555230929773\n",
      "43 Train Loss 55.770195 Test MSE 8.673144887173429 Test RE 1.4076554334402867\n",
      "44 Train Loss 55.770187 Test MSE 8.673140951653242 Test RE 1.4076551140718763\n",
      "45 Train Loss 55.770176 Test MSE 8.673137112922063 Test RE 1.4076548025578461\n",
      "46 Train Loss 55.770172 Test MSE 8.673134267765741 Test RE 1.407654571672629\n",
      "47 Train Loss 55.770172 Test MSE 8.67313279714808 Test RE 1.4076544523315875\n",
      "48 Train Loss 55.770172 Test MSE 8.673131641912708 Test RE 1.407654358583901\n",
      "49 Train Loss 55.770153 Test MSE 8.673123592116655 Test RE 1.4076537053404359\n",
      "50 Train Loss 55.770153 Test MSE 8.673122227999764 Test RE 1.4076535946418938\n",
      "51 Train Loss 55.77015 Test MSE 8.67311949126221 Test RE 1.4076533725547\n",
      "52 Train Loss 55.77014 Test MSE 8.673116419456825 Test RE 1.4076531232765943\n",
      "53 Train Loss 55.77014 Test MSE 8.673115436263341 Test RE 1.4076530434900822\n",
      "54 Train Loss 55.770134 Test MSE 8.673110623523556 Test RE 1.4076526529344162\n",
      "55 Train Loss 55.770126 Test MSE 8.67310647245824 Test RE 1.4076523160737988\n",
      "56 Train Loss 55.77012 Test MSE 8.673104174745285 Test RE 1.4076521296134352\n",
      "57 Train Loss 55.77012 Test MSE 8.673102920978126 Test RE 1.407652027869697\n",
      "58 Train Loss 55.77011 Test MSE 8.673100047989408 Test RE 1.407651794725414\n",
      "59 Train Loss 55.77011 Test MSE 8.673098674868704 Test RE 1.4076516832960577\n",
      "60 Train Loss 55.7701 Test MSE 8.673094255053437 Test RE 1.407651324626024\n",
      "61 Train Loss 55.770096 Test MSE 8.673091378546024 Test RE 1.4076510911960805\n",
      "62 Train Loss 55.77009 Test MSE 8.673086224609765 Test RE 1.407650672951607\n",
      "63 Train Loss 55.77008 Test MSE 8.673083218687548 Test RE 1.407650429019476\n",
      "64 Train Loss 55.77008 Test MSE 8.673082072022757 Test RE 1.4076503359670294\n",
      "65 Train Loss 55.770073 Test MSE 8.673077134120687 Test RE 1.4076499352536247\n",
      "66 Train Loss 55.770058 Test MSE 8.6730697322892 Test RE 1.4076493345908103\n",
      "67 Train Loss 55.77005 Test MSE 8.673065284568446 Test RE 1.4076489736556301\n",
      "68 Train Loss 55.770042 Test MSE 8.67306218706463 Test RE 1.4076487222913026\n",
      "69 Train Loss 55.770042 Test MSE 8.673060731128542 Test RE 1.4076486041411835\n",
      "70 Train Loss 55.770027 Test MSE 8.673052906343703 Test RE 1.407647969154857\n",
      "71 Train Loss 55.770027 Test MSE 8.673051193470323 Test RE 1.4076478301540412\n",
      "72 Train Loss 55.770008 Test MSE 8.67304188614519 Test RE 1.4076470748581686\n",
      "73 Train Loss 55.769997 Test MSE 8.673034318530778 Test RE 1.407646460740726\n",
      "74 Train Loss 55.76999 Test MSE 8.673030908388524 Test RE 1.4076461840051004\n",
      "75 Train Loss 55.76998 Test MSE 8.673027433719753 Test RE 1.4076459020330436\n",
      "76 Train Loss 55.76997 Test MSE 8.673021719695425 Test RE 1.4076454383354298\n",
      "77 Train Loss 55.76997 Test MSE 8.673021719695425 Test RE 1.4076454383354298\n",
      "78 Train Loss 55.76997 Test MSE 8.673021719695425 Test RE 1.4076454383354298\n",
      "79 Train Loss 55.76997 Test MSE 8.673021719695425 Test RE 1.4076454383354298\n",
      "80 Train Loss 55.76997 Test MSE 8.673021719695425 Test RE 1.4076454383354298\n",
      "81 Train Loss 55.76997 Test MSE 8.673021719695425 Test RE 1.4076454383354298\n",
      "82 Train Loss 55.76997 Test MSE 8.673021719695425 Test RE 1.4076454383354298\n",
      "83 Train Loss 55.76997 Test MSE 8.673021719695425 Test RE 1.4076454383354298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "84 Train Loss 55.76997 Test MSE 8.673021719695425 Test RE 1.4076454383354298\n",
      "85 Train Loss 55.76997 Test MSE 8.673021719695425 Test RE 1.4076454383354298\n",
      "86 Train Loss 55.76997 Test MSE 8.673021719695425 Test RE 1.4076454383354298\n",
      "87 Train Loss 55.76997 Test MSE 8.673021719695425 Test RE 1.4076454383354298\n",
      "88 Train Loss 55.76997 Test MSE 8.673021719695425 Test RE 1.4076454383354298\n",
      "89 Train Loss 55.76997 Test MSE 8.673021719695425 Test RE 1.4076454383354298\n",
      "90 Train Loss 55.76997 Test MSE 8.673021719695425 Test RE 1.4076454383354298\n",
      "91 Train Loss 55.76997 Test MSE 8.673021719695425 Test RE 1.4076454383354298\n",
      "92 Train Loss 55.76997 Test MSE 8.673021719695425 Test RE 1.4076454383354298\n",
      "93 Train Loss 55.76997 Test MSE 8.673021719695425 Test RE 1.4076454383354298\n",
      "94 Train Loss 55.76997 Test MSE 8.673021719695425 Test RE 1.4076454383354298\n",
      "95 Train Loss 55.76997 Test MSE 8.673021719695425 Test RE 1.4076454383354298\n",
      "96 Train Loss 55.76997 Test MSE 8.673021719695425 Test RE 1.4076454383354298\n",
      "97 Train Loss 55.76997 Test MSE 8.673021719695425 Test RE 1.4076454383354298\n",
      "98 Train Loss 55.76997 Test MSE 8.673021719695425 Test RE 1.4076454383354298\n",
      "99 Train Loss 55.76997 Test MSE 8.673021719695425 Test RE 1.4076454383354298\n",
      "Training time: 44.72\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 56.495167 Test MSE 8.734816267898784 Test RE 1.4126512152350055\n",
      "1 Train Loss 54.82341 Test MSE 8.525266277319384 Test RE 1.3956034570653766\n",
      "2 Train Loss 45.78518 Test MSE 7.754848195489989 Test RE 1.3310510447578747\n",
      "3 Train Loss 43.066208 Test MSE 8.132708532742232 Test RE 1.3630935546502405\n",
      "4 Train Loss 42.259045 Test MSE 8.336069480239056 Test RE 1.3800306218026108\n",
      "5 Train Loss 41.37391 Test MSE 8.244635774699793 Test RE 1.3724413600035097\n",
      "6 Train Loss 34.064587 Test MSE 7.83232662805627 Test RE 1.3376837626606792\n",
      "7 Train Loss 28.750135 Test MSE 6.59628233692805 Test RE 1.2276022574669876\n",
      "8 Train Loss 26.863464 Test MSE 6.146494395840869 Test RE 1.1850094270197242\n",
      "9 Train Loss 24.867638 Test MSE 5.978271486152583 Test RE 1.168680713371623\n",
      "10 Train Loss 23.315056 Test MSE 5.989342577286335 Test RE 1.1697623459185176\n",
      "11 Train Loss 22.944397 Test MSE 5.755584824580957 Test RE 1.146707860928415\n",
      "12 Train Loss 22.546406 Test MSE 5.5515559683767535 Test RE 1.1261997369108048\n",
      "13 Train Loss 22.104885 Test MSE 5.267390849056138 Test RE 1.0969979922962687\n",
      "14 Train Loss 21.270103 Test MSE 4.925734102805308 Test RE 1.0608245028305492\n",
      "15 Train Loss 19.693388 Test MSE 4.6791894354766095 Test RE 1.0339353266363733\n",
      "16 Train Loss 18.923206 Test MSE 4.551696255158906 Test RE 1.0197523075549522\n",
      "17 Train Loss 18.178413 Test MSE 4.606616754927835 Test RE 1.025885996584287\n",
      "18 Train Loss 17.629387 Test MSE 4.54558078447005 Test RE 1.0190670288036479\n",
      "19 Train Loss 16.759785 Test MSE 4.46741428523792 Test RE 1.0102670172762473\n",
      "20 Train Loss 16.108646 Test MSE 4.32282037154137 Test RE 0.9937832098788495\n",
      "21 Train Loss 15.179649 Test MSE 4.195591216304102 Test RE 0.9790494866604603\n",
      "22 Train Loss 14.689781 Test MSE 4.134762820050017 Test RE 0.9719263611967465\n",
      "23 Train Loss 14.135973 Test MSE 4.149626812188597 Test RE 0.9736717751597461\n",
      "24 Train Loss 13.725307 Test MSE 4.1105957117186085 Test RE 0.9690818120265158\n",
      "25 Train Loss 13.083813 Test MSE 4.199551274446119 Test RE 0.9795114213751176\n",
      "26 Train Loss 12.294616 Test MSE 3.981069573685374 Test RE 0.9536915704863616\n",
      "27 Train Loss 11.461506 Test MSE 3.9762599466635558 Test RE 0.9531153073883347\n",
      "28 Train Loss 10.696995 Test MSE 3.7266371082035707 Test RE 0.9227129425940948\n",
      "29 Train Loss 9.873322 Test MSE 3.6722445281171865 Test RE 0.915954406828805\n",
      "30 Train Loss 8.969787 Test MSE 3.4887508539123115 Test RE 0.8927770922388415\n",
      "31 Train Loss 8.370347 Test MSE 3.3669025296932937 Test RE 0.8770479388326233\n",
      "32 Train Loss 7.7917113 Test MSE 3.3403750335498588 Test RE 0.8735860187965366\n",
      "33 Train Loss 7.3441563 Test MSE 3.269470161421489 Test RE 0.8642646488710348\n",
      "34 Train Loss 7.106618 Test MSE 3.225946891703665 Test RE 0.8584928193663746\n",
      "35 Train Loss 6.9343395 Test MSE 3.1313626458490305 Test RE 0.8458137549960283\n",
      "36 Train Loss 6.6985455 Test MSE 3.0648263147960937 Test RE 0.8367794271820865\n",
      "37 Train Loss 6.554696 Test MSE 2.999938941994289 Test RE 0.8278740469005794\n",
      "38 Train Loss 6.305499 Test MSE 2.8960941039505923 Test RE 0.8134191544956291\n",
      "39 Train Loss 6.103488 Test MSE 2.837708425891937 Test RE 0.805178083532857\n",
      "40 Train Loss 5.9149237 Test MSE 2.7844364515900755 Test RE 0.7975845185709282\n",
      "41 Train Loss 5.7565336 Test MSE 2.743498737735478 Test RE 0.7916996312126332\n",
      "42 Train Loss 5.6302433 Test MSE 2.729656143053096 Test RE 0.7896998059816431\n",
      "43 Train Loss 5.49811 Test MSE 2.7118446479896656 Test RE 0.7871191229241475\n",
      "44 Train Loss 5.331647 Test MSE 2.7378971274506476 Test RE 0.790890981519605\n",
      "45 Train Loss 5.1916595 Test MSE 2.6839548433955187 Test RE 0.7830611225490517\n",
      "46 Train Loss 5.016341 Test MSE 2.6026510564832632 Test RE 0.7711094631045069\n",
      "47 Train Loss 4.906428 Test MSE 2.586667711584431 Test RE 0.7687380562335021\n",
      "48 Train Loss 4.8216295 Test MSE 2.6084350275041777 Test RE 0.7719658206208813\n",
      "49 Train Loss 4.6703706 Test MSE 2.526381656553709 Test RE 0.759726963016045\n",
      "50 Train Loss 4.176259 Test MSE 2.2723938934044807 Test RE 0.7205263487436614\n",
      "51 Train Loss 3.4438887 Test MSE 2.0752841217683438 Test RE 0.6885680097471368\n",
      "52 Train Loss 3.050811 Test MSE 1.9880000357224623 Test RE 0.6739322730335592\n",
      "53 Train Loss 2.7342596 Test MSE 1.658958367136879 Test RE 0.6156385461096012\n",
      "54 Train Loss 2.5339324 Test MSE 1.6848481076729576 Test RE 0.6204237830613141\n",
      "55 Train Loss 2.300104 Test MSE 1.4474561787445417 Test RE 0.5750568386716612\n",
      "56 Train Loss 2.0067368 Test MSE 1.2242281069555268 Test RE 0.5288581888738388\n",
      "57 Train Loss 1.8890036 Test MSE 1.1246850411499758 Test RE 0.5069014338641985\n",
      "58 Train Loss 1.7863111 Test MSE 1.1049434789179746 Test RE 0.5024329254482451\n",
      "59 Train Loss 1.7232947 Test MSE 1.0741889673645355 Test RE 0.49539133282059167\n",
      "60 Train Loss 1.6618463 Test MSE 1.044305749391724 Test RE 0.48845200291406315\n",
      "61 Train Loss 1.616019 Test MSE 1.02785773662899 Test RE 0.48459013061448564\n",
      "62 Train Loss 1.5391117 Test MSE 1.0061169105432413 Test RE 0.4794378140427794\n",
      "63 Train Loss 1.4837985 Test MSE 0.9959641945616402 Test RE 0.47701267936292513\n",
      "64 Train Loss 1.4069571 Test MSE 1.0039318236446433 Test RE 0.4789169090187805\n",
      "65 Train Loss 1.3588326 Test MSE 0.9869895787096281 Test RE 0.47485863944311835\n",
      "66 Train Loss 1.301909 Test MSE 0.9544110377391255 Test RE 0.4669558138036386\n",
      "67 Train Loss 1.265637 Test MSE 0.9537657714523068 Test RE 0.46679793539612635\n",
      "68 Train Loss 1.2093202 Test MSE 0.8940890218638976 Test RE 0.4519583781167032\n",
      "69 Train Loss 1.1574067 Test MSE 0.8882759027510442 Test RE 0.4504867280709186\n",
      "70 Train Loss 1.0870713 Test MSE 0.8628787916051488 Test RE 0.44399998726103224\n",
      "71 Train Loss 1.029779 Test MSE 0.8221822722395822 Test RE 0.4334032008807288\n",
      "72 Train Loss 0.97211295 Test MSE 0.7688747535412269 Test RE 0.4191175634735377\n",
      "73 Train Loss 0.9377734 Test MSE 0.7579163647720333 Test RE 0.4161201079314241\n",
      "74 Train Loss 0.9100712 Test MSE 0.7393016810190128 Test RE 0.410978314913433\n",
      "75 Train Loss 0.879712 Test MSE 0.7178793625816517 Test RE 0.40498020374853383\n",
      "76 Train Loss 0.85055745 Test MSE 0.7146656332420345 Test RE 0.4040726998542205\n",
      "77 Train Loss 0.8308251 Test MSE 0.7035849784261436 Test RE 0.4009279557436021\n",
      "78 Train Loss 0.80095863 Test MSE 0.6836447139349016 Test RE 0.3952057824447374\n",
      "79 Train Loss 0.78186995 Test MSE 0.6877061538132954 Test RE 0.396377975852175\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "80 Train Loss 0.76321316 Test MSE 0.679659720151431 Test RE 0.3940522634485103\n",
      "81 Train Loss 0.7482951 Test MSE 0.6715270143630702 Test RE 0.3916875833073536\n",
      "82 Train Loss 0.73844403 Test MSE 0.6665991054678596 Test RE 0.390247764159478\n",
      "83 Train Loss 0.7206225 Test MSE 0.6627383763344 Test RE 0.3891160279060417\n",
      "84 Train Loss 0.7083092 Test MSE 0.665464303482562 Test RE 0.3899154485514802\n",
      "85 Train Loss 0.70121807 Test MSE 0.6735872640262426 Test RE 0.39228797330755427\n",
      "86 Train Loss 0.6942973 Test MSE 0.6766666857087209 Test RE 0.39318365723268983\n",
      "87 Train Loss 0.6737659 Test MSE 0.6790144372648901 Test RE 0.3938651583747126\n",
      "88 Train Loss 0.6530762 Test MSE 0.6750215951659586 Test RE 0.392705418579839\n",
      "89 Train Loss 0.6451451 Test MSE 0.6649131972779259 Test RE 0.3897539603021028\n",
      "90 Train Loss 0.6370374 Test MSE 0.662319429230758 Test RE 0.38899301948388115\n",
      "91 Train Loss 0.6263292 Test MSE 0.6551148816467254 Test RE 0.3868715493843229\n",
      "92 Train Loss 0.6125631 Test MSE 0.641880069135171 Test RE 0.38294376747083364\n",
      "93 Train Loss 0.6042837 Test MSE 0.6442817763002763 Test RE 0.38365952429030775\n",
      "94 Train Loss 0.59884286 Test MSE 0.6487790806584263 Test RE 0.3849962321951286\n",
      "95 Train Loss 0.5857607 Test MSE 0.6553314614117819 Test RE 0.3869354936082135\n",
      "96 Train Loss 0.57945764 Test MSE 0.6526881510527098 Test RE 0.38615434382931635\n",
      "97 Train Loss 0.57114553 Test MSE 0.6452851216597862 Test RE 0.3839581461552523\n",
      "98 Train Loss 0.5628939 Test MSE 0.6364279473264959 Test RE 0.38131393921097884\n",
      "99 Train Loss 0.55265516 Test MSE 0.6325570803757852 Test RE 0.3801525612974972\n",
      "Training time: 160.75\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 56.260838 Test MSE 8.761020014704522 Test RE 1.4147685485237516\n",
      "1 Train Loss 56.26053 Test MSE 8.756456824921422 Test RE 1.414400058426145\n",
      "2 Train Loss 55.99021 Test MSE 8.75737466818718 Test RE 1.4144741845011946\n",
      "3 Train Loss 46.53006 Test MSE 8.453074520269992 Test RE 1.3896819257184316\n",
      "4 Train Loss 41.93347 Test MSE 8.072751438721259 Test RE 1.358059664897886\n",
      "5 Train Loss 41.397545 Test MSE 8.295350874165708 Test RE 1.3766560271035015\n",
      "6 Train Loss 41.298714 Test MSE 8.298565465398479 Test RE 1.376922740218901\n",
      "7 Train Loss 41.22857 Test MSE 8.305820150694839 Test RE 1.3775244682526435\n",
      "8 Train Loss 41.150967 Test MSE 8.332609061211288 Test RE 1.3797441570646807\n",
      "9 Train Loss 41.079796 Test MSE 8.344626847154117 Test RE 1.3807387732422147\n",
      "10 Train Loss 41.049072 Test MSE 8.350044678290633 Test RE 1.3811869296327417\n",
      "11 Train Loss 41.03458 Test MSE 8.34818599847406 Test RE 1.3810331982909012\n",
      "12 Train Loss 41.026436 Test MSE 8.346982346023596 Test RE 1.380933635109421\n",
      "13 Train Loss 40.954025 Test MSE 8.356732691021659 Test RE 1.381739953429631\n",
      "14 Train Loss 40.830055 Test MSE 8.41301677513122 Test RE 1.3863852772129894\n",
      "15 Train Loss 40.504005 Test MSE 8.383308007544422 Test RE 1.383935250967682\n",
      "16 Train Loss 39.922104 Test MSE 8.314238297803678 Test RE 1.378222368438306\n",
      "17 Train Loss 39.649914 Test MSE 8.28174209838602 Test RE 1.375526340519786\n",
      "18 Train Loss 38.53354 Test MSE 8.037672506228914 Test RE 1.3551058300896557\n",
      "19 Train Loss 36.80394 Test MSE 7.9782155297817585 Test RE 1.350084472861714\n",
      "20 Train Loss 34.589558 Test MSE 7.139198859946317 Test RE 1.2771232165217483\n",
      "21 Train Loss 29.743 Test MSE 6.303681447348657 Test RE 1.2000661597400286\n",
      "22 Train Loss 27.573523 Test MSE 5.913846837596043 Test RE 1.1623665312680467\n",
      "23 Train Loss 24.821495 Test MSE 4.566286427314105 Test RE 1.021385375432331\n",
      "24 Train Loss 20.284517 Test MSE 3.2755865014811167 Test RE 0.8650726798808837\n",
      "25 Train Loss 14.519216 Test MSE 2.596887645612193 Test RE 0.770255202649055\n",
      "26 Train Loss 12.053739 Test MSE 1.15323177847139 Test RE 0.5132942047063274\n",
      "27 Train Loss 7.7739863 Test MSE 0.674222484736226 Test RE 0.39247290162858905\n",
      "28 Train Loss 4.9889126 Test MSE 0.4713961550651749 Test RE 0.3281716632955875\n",
      "29 Train Loss 3.9837618 Test MSE 0.42263489830090106 Test RE 0.310735403993964\n",
      "30 Train Loss 3.3278944 Test MSE 0.33273893653136394 Test RE 0.27571466878627077\n",
      "31 Train Loss 3.0143683 Test MSE 0.28643928035218974 Test RE 0.2558140227999064\n",
      "32 Train Loss 2.6283524 Test MSE 0.28428168110144436 Test RE 0.2548487441308919\n",
      "33 Train Loss 2.211382 Test MSE 0.24761074137688843 Test RE 0.237844328819982\n",
      "34 Train Loss 1.8692191 Test MSE 0.21709561479975056 Test RE 0.2227068554217225\n",
      "35 Train Loss 1.6443632 Test MSE 0.22607261048621657 Test RE 0.2272647266416084\n",
      "36 Train Loss 1.4217712 Test MSE 0.16704259830555807 Test RE 0.1953537168499828\n",
      "37 Train Loss 1.240602 Test MSE 0.159294969696055 Test RE 0.1907695657398134\n",
      "38 Train Loss 1.190475 Test MSE 0.14395900009370138 Test RE 0.18135413866459624\n",
      "39 Train Loss 1.1143259 Test MSE 0.14191745930172306 Test RE 0.18006361879468483\n",
      "40 Train Loss 0.92127573 Test MSE 0.09013087151751548 Test RE 0.14349766871774405\n",
      "41 Train Loss 0.6837079 Test MSE 0.06682117271171349 Test RE 0.12355636042688241\n",
      "42 Train Loss 0.6018293 Test MSE 0.05029784634943559 Test RE 0.10719703055370994\n",
      "43 Train Loss 0.4987134 Test MSE 0.04331098381053572 Test RE 0.09947342801935875\n",
      "44 Train Loss 0.45371282 Test MSE 0.03233354248106665 Test RE 0.08594778814636007\n",
      "45 Train Loss 0.4110492 Test MSE 0.02954413872702972 Test RE 0.08215684011406367\n",
      "46 Train Loss 0.39727017 Test MSE 0.028635196149775704 Test RE 0.08088316581487501\n",
      "47 Train Loss 0.38784248 Test MSE 0.028597877970679877 Test RE 0.08083044404351196\n",
      "48 Train Loss 0.35245043 Test MSE 0.02646667250962613 Test RE 0.07776026502059087\n",
      "49 Train Loss 0.3447677 Test MSE 0.02645391374273487 Test RE 0.07774151984793079\n",
      "50 Train Loss 0.3380596 Test MSE 0.02553657531273618 Test RE 0.07638171181759547\n",
      "51 Train Loss 0.32305503 Test MSE 0.022733977849641217 Test RE 0.07206854871861762\n",
      "52 Train Loss 0.31814283 Test MSE 0.021054690568650405 Test RE 0.06935575309722576\n",
      "53 Train Loss 0.3137253 Test MSE 0.02028450823412699 Test RE 0.06807541569192743\n",
      "54 Train Loss 0.29923934 Test MSE 0.01826034167106894 Test RE 0.06458958735734399\n",
      "55 Train Loss 0.25427535 Test MSE 0.017690212856208355 Test RE 0.06357327569041325\n",
      "56 Train Loss 0.24541369 Test MSE 0.018851985534308238 Test RE 0.0656276131155788\n",
      "57 Train Loss 0.24032666 Test MSE 0.01930122810395074 Test RE 0.06640496191094301\n",
      "58 Train Loss 0.22936352 Test MSE 0.0195210409316576 Test RE 0.06678201923278881\n",
      "59 Train Loss 0.2225633 Test MSE 0.019754024521257227 Test RE 0.06717935882915853\n",
      "60 Train Loss 0.21037164 Test MSE 0.017545079132761432 Test RE 0.06331195524879571\n",
      "61 Train Loss 0.20318809 Test MSE 0.01806215959961174 Test RE 0.06423813119038312\n",
      "62 Train Loss 0.19806367 Test MSE 0.017754238581962112 Test RE 0.06368821633278976\n",
      "63 Train Loss 0.19544333 Test MSE 0.016775189582874352 Test RE 0.06190728795436796\n",
      "64 Train Loss 0.19095089 Test MSE 0.01613180375459895 Test RE 0.06070850321310486\n",
      "65 Train Loss 0.18620367 Test MSE 0.014905494291847949 Test RE 0.058355427043069646\n",
      "66 Train Loss 0.18307282 Test MSE 0.014808630389583424 Test RE 0.058165505543916717\n",
      "67 Train Loss 0.17909074 Test MSE 0.013742729903790633 Test RE 0.05603308910795726\n",
      "68 Train Loss 0.17119062 Test MSE 0.012284115432237832 Test RE 0.052976102619840114\n",
      "69 Train Loss 0.16773088 Test MSE 0.012090410266869297 Test RE 0.052556759418765905\n",
      "70 Train Loss 0.16205332 Test MSE 0.010883988142605081 Test RE 0.04986571955947197\n",
      "71 Train Loss 0.16035742 Test MSE 0.010518282003101767 Test RE 0.04902080801306986\n",
      "72 Train Loss 0.16030855 Test MSE 0.010532261004856207 Test RE 0.04905337200168333\n",
      "73 Train Loss 0.16030855 Test MSE 0.010532261004856207 Test RE 0.04905337200168333\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "74 Train Loss 0.16030855 Test MSE 0.010532261004856207 Test RE 0.04905337200168333\n",
      "75 Train Loss 0.16030855 Test MSE 0.010532261004856207 Test RE 0.04905337200168333\n",
      "76 Train Loss 0.16030855 Test MSE 0.010532261004856207 Test RE 0.04905337200168333\n",
      "77 Train Loss 0.16030855 Test MSE 0.010532261004856207 Test RE 0.04905337200168333\n",
      "78 Train Loss 0.16030855 Test MSE 0.010532261004856207 Test RE 0.04905337200168333\n",
      "79 Train Loss 0.16030855 Test MSE 0.010532261004856207 Test RE 0.04905337200168333\n",
      "80 Train Loss 0.16030855 Test MSE 0.010532261004856207 Test RE 0.04905337200168333\n",
      "81 Train Loss 0.16030855 Test MSE 0.010532261004856207 Test RE 0.04905337200168333\n",
      "82 Train Loss 0.16030855 Test MSE 0.010532261004856207 Test RE 0.04905337200168333\n",
      "83 Train Loss 0.16030855 Test MSE 0.010532261004856207 Test RE 0.04905337200168333\n",
      "84 Train Loss 0.16030855 Test MSE 0.010532261004856207 Test RE 0.04905337200168333\n",
      "85 Train Loss 0.16030855 Test MSE 0.010532261004856207 Test RE 0.04905337200168333\n",
      "86 Train Loss 0.16030855 Test MSE 0.010532261004856207 Test RE 0.04905337200168333\n",
      "87 Train Loss 0.16030855 Test MSE 0.010532261004856207 Test RE 0.04905337200168333\n",
      "88 Train Loss 0.16030855 Test MSE 0.010532261004856207 Test RE 0.04905337200168333\n",
      "89 Train Loss 0.16030855 Test MSE 0.010532261004856207 Test RE 0.04905337200168333\n",
      "90 Train Loss 0.16030855 Test MSE 0.010532261004856207 Test RE 0.04905337200168333\n",
      "91 Train Loss 0.16030855 Test MSE 0.010532261004856207 Test RE 0.04905337200168333\n",
      "92 Train Loss 0.16030855 Test MSE 0.010532261004856207 Test RE 0.04905337200168333\n",
      "93 Train Loss 0.16030855 Test MSE 0.010532261004856207 Test RE 0.04905337200168333\n",
      "94 Train Loss 0.16030855 Test MSE 0.010532261004856207 Test RE 0.04905337200168333\n",
      "95 Train Loss 0.16030855 Test MSE 0.010532261004856207 Test RE 0.04905337200168333\n",
      "96 Train Loss 0.16030855 Test MSE 0.010532261004856207 Test RE 0.04905337200168333\n",
      "97 Train Loss 0.16030855 Test MSE 0.010532261004856207 Test RE 0.04905337200168333\n",
      "98 Train Loss 0.16030855 Test MSE 0.010532261004856207 Test RE 0.04905337200168333\n",
      "99 Train Loss 0.16030855 Test MSE 0.010532261004856207 Test RE 0.04905337200168333\n",
      "Training time: 122.37\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 55.30075 Test MSE 8.987048722485097 Test RE 1.4329023943444241\n",
      "1 Train Loss 51.34739 Test MSE 8.750821681891747 Test RE 1.4139448726406885\n",
      "2 Train Loss 50.42032 Test MSE 8.64447500062508 Test RE 1.4053269396495323\n",
      "3 Train Loss 43.194427 Test MSE 8.491955779410404 Test RE 1.3928742903764193\n",
      "4 Train Loss 42.78453 Test MSE 8.403757129620445 Test RE 1.3856221163383649\n",
      "5 Train Loss 42.680576 Test MSE 8.457508963581239 Test RE 1.3900463882413414\n",
      "6 Train Loss 42.587433 Test MSE 8.413010152579778 Test RE 1.3863847315460864\n",
      "7 Train Loss 42.451454 Test MSE 8.368527814744672 Test RE 1.3827147391188674\n",
      "8 Train Loss 42.22603 Test MSE 8.37331084717109 Test RE 1.3831098280074725\n",
      "9 Train Loss 41.49878 Test MSE 8.359986247973517 Test RE 1.382008906159927\n",
      "10 Train Loss 41.232334 Test MSE 8.368854667391993 Test RE 1.3827417414524994\n",
      "11 Train Loss 40.996666 Test MSE 8.37704433131994 Test RE 1.383418143513241\n",
      "12 Train Loss 40.604713 Test MSE 8.3426077284519 Test RE 1.3805717169918632\n",
      "13 Train Loss 39.527603 Test MSE 7.678928729124309 Test RE 1.3245195675837493\n",
      "14 Train Loss 37.229267 Test MSE 7.682618107249405 Test RE 1.3248377152628195\n",
      "15 Train Loss 36.44535 Test MSE 7.62623153069375 Test RE 1.3199669382483243\n",
      "16 Train Loss 35.92373 Test MSE 7.484270228265632 Test RE 1.3076237206159025\n",
      "17 Train Loss 35.482925 Test MSE 7.018636743620724 Test RE 1.2662936909907037\n",
      "18 Train Loss 34.76851 Test MSE 6.2914516842382175 Test RE 1.1989014712668842\n",
      "19 Train Loss 33.241142 Test MSE 5.968625328119888 Test RE 1.1677374783570649\n",
      "20 Train Loss 32.450592 Test MSE 5.735346400484015 Test RE 1.1446899949853548\n",
      "21 Train Loss 31.173405 Test MSE 5.494107513650613 Test RE 1.1203575294049466\n",
      "22 Train Loss 28.764565 Test MSE 5.139511994665951 Test RE 1.0836000147165252\n",
      "23 Train Loss 28.142323 Test MSE 5.233449177191034 Test RE 1.0934578979292529\n",
      "24 Train Loss 27.87954 Test MSE 5.23944201414091 Test RE 1.0940837796188578\n",
      "25 Train Loss 27.558323 Test MSE 5.301799535342338 Test RE 1.1005751729146749\n",
      "26 Train Loss 27.272175 Test MSE 5.383558070078932 Test RE 1.109028638902299\n",
      "27 Train Loss 27.063293 Test MSE 5.312555510588097 Test RE 1.1016909979646667\n",
      "28 Train Loss 26.819473 Test MSE 5.122012998522722 Test RE 1.0817537226560447\n",
      "29 Train Loss 26.690239 Test MSE 4.863253519506241 Test RE 1.0540750050365075\n",
      "30 Train Loss 26.505775 Test MSE 4.283779452730572 Test RE 0.9892854278549288\n",
      "31 Train Loss 25.971645 Test MSE 3.5158205161804745 Test RE 0.8962339851676623\n",
      "32 Train Loss 25.220158 Test MSE 3.3187546710018334 Test RE 0.8707543146942751\n",
      "33 Train Loss 24.961613 Test MSE 3.171813839905127 Test RE 0.8512593697027889\n",
      "34 Train Loss 24.588226 Test MSE 3.2490494872091618 Test RE 0.8615613803793085\n",
      "35 Train Loss 23.866089 Test MSE 3.247035050612546 Test RE 0.8612942515035393\n",
      "36 Train Loss 23.555332 Test MSE 3.118088634114981 Test RE 0.8440191264925129\n",
      "37 Train Loss 23.15736 Test MSE 3.123924920656261 Test RE 0.8448086541757326\n",
      "38 Train Loss 22.617006 Test MSE 3.0313023297484216 Test RE 0.8321903719908401\n",
      "39 Train Loss 22.290043 Test MSE 3.1017656382200105 Test RE 0.8418070344016957\n",
      "40 Train Loss 21.939732 Test MSE 3.1245982430893635 Test RE 0.8448996931708006\n",
      "41 Train Loss 21.746391 Test MSE 3.207743548566129 Test RE 0.8560672451226915\n",
      "42 Train Loss 21.609198 Test MSE 3.2239137761546752 Test RE 0.8582222491660667\n",
      "43 Train Loss 21.38811 Test MSE 3.313247835082067 Test RE 0.8700315899974634\n",
      "44 Train Loss 21.283745 Test MSE 3.2509435941349145 Test RE 0.8618124771410772\n",
      "45 Train Loss 21.225924 Test MSE 3.2246574314454177 Test RE 0.858321225869599\n",
      "46 Train Loss 21.160301 Test MSE 3.2444515003357357 Test RE 0.8609515327199567\n",
      "47 Train Loss 21.09531 Test MSE 3.288358673917335 Test RE 0.8667575854616724\n",
      "48 Train Loss 20.959581 Test MSE 3.3107155367532695 Test RE 0.8696990461037236\n",
      "49 Train Loss 20.877888 Test MSE 3.3614660935354435 Test RE 0.8763395813240773\n",
      "50 Train Loss 20.77744 Test MSE 3.321855540432475 Test RE 0.8711610131838299\n",
      "51 Train Loss 20.695953 Test MSE 3.314507289488353 Test RE 0.8701969354623251\n",
      "52 Train Loss 20.648293 Test MSE 3.285495948808069 Test RE 0.866380219545313\n",
      "53 Train Loss 20.58766 Test MSE 3.293262395797376 Test RE 0.8674036151641655\n",
      "54 Train Loss 20.500637 Test MSE 3.190220142632789 Test RE 0.8537257616011623\n",
      "55 Train Loss 20.468662 Test MSE 3.115625311617035 Test RE 0.8436856686702335\n",
      "56 Train Loss 20.39101 Test MSE 3.1006344313927965 Test RE 0.8416535178545103\n",
      "57 Train Loss 20.255808 Test MSE 3.012791563671147 Test RE 0.8296455795674677\n",
      "58 Train Loss 19.983593 Test MSE 2.944479426155602 Test RE 0.8201859441672792\n",
      "59 Train Loss 19.867966 Test MSE 2.949390957679879 Test RE 0.8208697136999553\n",
      "60 Train Loss 19.767044 Test MSE 2.890077613566441 Test RE 0.8125737964526996\n",
      "61 Train Loss 19.72482 Test MSE 2.9206438995612922 Test RE 0.816859500784681\n",
      "62 Train Loss 19.663208 Test MSE 2.894529450688161 Test RE 0.8131993945645998\n",
      "63 Train Loss 19.494759 Test MSE 2.7786944970263883 Test RE 0.7967617205650366\n",
      "64 Train Loss 19.371323 Test MSE 2.667702661422563 Test RE 0.7806866831840896\n",
      "65 Train Loss 19.281261 Test MSE 2.5200844863544667 Test RE 0.7587795378935478\n",
      "66 Train Loss 19.169708 Test MSE 2.353245644840629 Test RE 0.733232475653552\n",
      "67 Train Loss 18.989016 Test MSE 2.210532129961718 Test RE 0.7106511712447728\n",
      "68 Train Loss 18.765516 Test MSE 2.2251169359697522 Test RE 0.7129917093544561\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "69 Train Loss 18.663733 Test MSE 2.2334206176768174 Test RE 0.7143208401784878\n",
      "70 Train Loss 18.616432 Test MSE 2.233292932332112 Test RE 0.7143004209154024\n",
      "71 Train Loss 18.550144 Test MSE 2.2265490732561193 Test RE 0.7132211215140832\n",
      "72 Train Loss 18.48906 Test MSE 2.2082690731591343 Test RE 0.7102873095820581\n",
      "73 Train Loss 18.418724 Test MSE 2.188184016484424 Test RE 0.7070497629297676\n",
      "74 Train Loss 18.326185 Test MSE 2.122472388171591 Test RE 0.6963524129650502\n",
      "75 Train Loss 18.279594 Test MSE 2.120336526126933 Test RE 0.6960019520923695\n",
      "76 Train Loss 18.203716 Test MSE 2.088943153379544 Test RE 0.6908302896864723\n",
      "77 Train Loss 18.123842 Test MSE 2.024733536804415 Test RE 0.6801301046365397\n",
      "78 Train Loss 17.64127 Test MSE 1.974631356242477 Test RE 0.6716624585991317\n",
      "79 Train Loss 16.748081 Test MSE 1.8834583187833382 Test RE 0.6559731556032077\n",
      "80 Train Loss 16.135677 Test MSE 1.7873449070700507 Test RE 0.6390167525910719\n",
      "81 Train Loss 15.785927 Test MSE 1.7375955846942561 Test RE 0.6300607305933786\n",
      "82 Train Loss 15.28603 Test MSE 1.5845782266483834 Test RE 0.6016790671309957\n",
      "83 Train Loss 14.109987 Test MSE 1.3170425203099918 Test RE 0.5485395657846817\n",
      "84 Train Loss 13.534475 Test MSE 1.2216527260996508 Test RE 0.5283016225322305\n",
      "85 Train Loss 13.2536955 Test MSE 1.265434241279666 Test RE 0.5376849136074762\n",
      "86 Train Loss 13.0071945 Test MSE 1.1705669586734881 Test RE 0.517137689688202\n",
      "87 Train Loss 12.785866 Test MSE 1.1275624378616107 Test RE 0.5075494485365226\n",
      "88 Train Loss 12.481378 Test MSE 1.0402809003778961 Test RE 0.48750982508177393\n",
      "89 Train Loss 12.128884 Test MSE 1.024242604187341 Test RE 0.48373719127182485\n",
      "90 Train Loss 11.878497 Test MSE 0.9076243196772363 Test RE 0.4553665469052585\n",
      "91 Train Loss 11.595663 Test MSE 0.8964392833947441 Test RE 0.45255201209718793\n",
      "92 Train Loss 11.3365755 Test MSE 0.8596603586501538 Test RE 0.4431711807240909\n",
      "93 Train Loss 11.109201 Test MSE 0.8626907118543726 Test RE 0.4439515957843049\n",
      "94 Train Loss 11.00705 Test MSE 0.8288309370366267 Test RE 0.4351520530767871\n",
      "95 Train Loss 10.819281 Test MSE 0.6304892998794172 Test RE 0.37953070792590426\n",
      "96 Train Loss 6.8941946 Test MSE 0.15479733950702523 Test RE 0.1880571310704131\n",
      "97 Train Loss 4.4214997 Test MSE 0.14302230100446756 Test RE 0.18076316661839328\n",
      "98 Train Loss 3.504621 Test MSE 0.13877815337870933 Test RE 0.17806091979808883\n",
      "99 Train Loss 2.5796928 Test MSE 0.09427480300773253 Test RE 0.14675938294395968\n",
      "Training time: 159.25\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "1 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "2 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "3 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "4 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "5 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "6 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "7 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "8 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "9 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "10 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "11 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "12 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "13 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "14 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "15 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "16 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "17 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "18 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "19 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "20 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "21 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "22 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "23 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "24 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "25 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "26 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "27 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "28 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "29 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "30 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "31 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "32 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "33 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "34 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "35 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "36 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "37 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "38 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "39 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "40 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "41 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "42 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "43 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "44 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "45 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "46 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "47 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "48 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "49 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "50 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "51 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "52 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "53 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "54 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "55 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "56 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "57 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "58 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "59 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "60 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "61 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "62 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "63 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "64 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "65 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "66 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "67 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "68 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "69 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "70 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "71 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "72 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "73 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "74 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "75 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "76 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "77 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "78 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "79 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "80 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "81 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "82 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "83 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "84 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "85 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "86 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "87 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "88 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "89 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "90 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "91 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "92 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "93 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "94 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "95 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "96 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "97 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "98 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "99 Train Loss 57.017754 Test MSE 8.725380877053066 Test RE 1.411888032738035\n",
      "Training time: 35.06\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 55.958164 Test MSE 8.846869478922402 Test RE 1.4216833272959148\n",
      "1 Train Loss 51.46941 Test MSE 8.821014741437807 Test RE 1.4196043916830068\n",
      "2 Train Loss 49.303535 Test MSE 7.9063654493323074 Test RE 1.3439914395795387\n",
      "3 Train Loss 48.124115 Test MSE 7.6805529294601556 Test RE 1.324659637342027\n",
      "4 Train Loss 39.993477 Test MSE 6.530701264578971 Test RE 1.2214845252991302\n",
      "5 Train Loss 26.631271 Test MSE 3.3586864923783004 Test RE 0.8759771831603232\n",
      "6 Train Loss 14.250145 Test MSE 2.5089157063119725 Test RE 0.7570962506116355\n",
      "7 Train Loss 9.652714 Test MSE 2.084901825250843 Test RE 0.6901617163235364\n",
      "8 Train Loss 7.1415005 Test MSE 2.1822007143421276 Test RE 0.706082433806273\n",
      "9 Train Loss 5.6938267 Test MSE 1.942803242413946 Test RE 0.666227369458527\n",
      "10 Train Loss 5.059019 Test MSE 1.9157520958033658 Test RE 0.6615729122459791\n",
      "11 Train Loss 4.569637 Test MSE 1.8566482373798656 Test RE 0.6512876981684353\n",
      "12 Train Loss 4.347968 Test MSE 1.7218903101237932 Test RE 0.627206862259581\n",
      "13 Train Loss 4.1940727 Test MSE 1.7106571557879957 Test RE 0.6251576492991856\n",
      "14 Train Loss 4.1298394 Test MSE 1.641342979944838 Test RE 0.6123612928486748\n",
      "15 Train Loss 3.973358 Test MSE 1.5305638385880576 Test RE 0.5913352718346166\n",
      "16 Train Loss 3.9097388 Test MSE 1.4193821169969107 Test RE 0.5694527897131758\n",
      "17 Train Loss 3.805643 Test MSE 1.3602497178865027 Test RE 0.5574647134890209\n",
      "18 Train Loss 3.176552 Test MSE 0.3944709109072006 Test RE 0.3002033620895776\n",
      "19 Train Loss 1.7151102 Test MSE 0.14177941400076371 Test RE 0.17997602215860606\n",
      "20 Train Loss 1.1151031 Test MSE 0.11548642556655672 Test RE 0.16243272497650296\n",
      "21 Train Loss 0.7828014 Test MSE 0.08105556719550754 Test RE 0.13608162442185082\n",
      "22 Train Loss 0.55854416 Test MSE 0.06392445439617023 Test RE 0.12084858655236175\n",
      "23 Train Loss 0.37108248 Test MSE 0.06513100761203262 Test RE 0.12198374397729603\n",
      "24 Train Loss 0.27677938 Test MSE 0.03134725024347044 Test RE 0.08462677399340343\n",
      "25 Train Loss 0.22792478 Test MSE 0.028167142667410456 Test RE 0.08021940880554196\n",
      "26 Train Loss 0.2064577 Test MSE 0.02274807060421908 Test RE 0.07209088284391453\n",
      "27 Train Loss 0.15376516 Test MSE 0.01755882998844287 Test RE 0.06333676058136442\n",
      "28 Train Loss 0.14108366 Test MSE 0.016043003862420406 Test RE 0.060541183182628376\n",
      "29 Train Loss 0.12543637 Test MSE 0.017288289178381617 Test RE 0.06284693024284618\n",
      "30 Train Loss 0.1100584 Test MSE 0.01369411880766987 Test RE 0.05593390055999894\n",
      "31 Train Loss 0.10252423 Test MSE 0.012327024587683238 Test RE 0.05306854632175381\n",
      "32 Train Loss 0.09732805 Test MSE 0.01277795010146553 Test RE 0.05403045865559504\n",
      "33 Train Loss 0.068151124 Test MSE 0.007299824947563272 Test RE 0.04083798291733696\n",
      "34 Train Loss 0.062031046 Test MSE 0.006883855214363197 Test RE 0.03965737134424981\n",
      "35 Train Loss 0.06023183 Test MSE 0.006734046363670663 Test RE 0.0392234789781948\n",
      "36 Train Loss 0.05887339 Test MSE 0.006299723320653669 Test RE 0.03793750784834738\n",
      "37 Train Loss 0.056537557 Test MSE 0.006725771860932228 Test RE 0.03919937352007366\n",
      "38 Train Loss 0.054539148 Test MSE 0.006230818734628706 Test RE 0.03772946255139227\n",
      "39 Train Loss 0.050207287 Test MSE 0.006342429863349137 Test RE 0.03806588199578318\n",
      "40 Train Loss 0.046528645 Test MSE 0.005896416807796215 Test RE 0.03670304938101571\n",
      "41 Train Loss 0.045024686 Test MSE 0.004884025241605823 Test RE 0.03340388733169451\n",
      "42 Train Loss 0.043998506 Test MSE 0.004455265189949342 Test RE 0.031903978213883584\n",
      "43 Train Loss 0.040314607 Test MSE 0.0038394540460844886 Test RE 0.029617118108695644\n",
      "44 Train Loss 0.030009355 Test MSE 0.0029567924961261633 Test RE 0.025990730376478158\n",
      "45 Train Loss 0.02858428 Test MSE 0.0024702376097406934 Test RE 0.023756224736336207\n",
      "46 Train Loss 0.026954995 Test MSE 0.0020946889303291812 Test RE 0.0218759956877124\n",
      "47 Train Loss 0.025168415 Test MSE 0.0017917071239424071 Test RE 0.02023212830643689\n",
      "48 Train Loss 0.022714866 Test MSE 0.0017196629609984291 Test RE 0.019821190199252093\n",
      "49 Train Loss 0.021800643 Test MSE 0.0015010814245547369 Test RE 0.018518686755676125\n",
      "50 Train Loss 0.020002088 Test MSE 0.0015678660814138733 Test RE 0.018926161545090275\n",
      "51 Train Loss 0.018153168 Test MSE 0.0015451248644872985 Test RE 0.0187884023071223\n",
      "52 Train Loss 0.017670982 Test MSE 0.001464292213872964 Test RE 0.018290346660701437\n",
      "53 Train Loss 0.017287297 Test MSE 0.0015677786402861049 Test RE 0.01892563377425442\n",
      "54 Train Loss 0.016229073 Test MSE 0.0014863486688700858 Test RE 0.01842758442368876\n",
      "55 Train Loss 0.014020014 Test MSE 0.001164258113677449 Test RE 0.016309201440530652\n",
      "56 Train Loss 0.012480756 Test MSE 0.0009918552327877097 Test RE 0.015053316910539549\n",
      "57 Train Loss 0.01192014 Test MSE 0.0009097672893774373 Test RE 0.014416944253325802\n",
      "58 Train Loss 0.011499862 Test MSE 0.0007928376918453457 Test RE 0.013458609879079781\n",
      "59 Train Loss 0.01076152 Test MSE 0.0007610265233829397 Test RE 0.013185844712904434\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "60 Train Loss 0.010204628 Test MSE 0.0007903034307075786 Test RE 0.013437082817635579\n",
      "61 Train Loss 0.009859505 Test MSE 0.0008624144401494265 Test RE 0.014036734004078785\n",
      "62 Train Loss 0.009669168 Test MSE 0.0009196877606715471 Test RE 0.014495335232985304\n",
      "63 Train Loss 0.009252343 Test MSE 0.0009213172351867989 Test RE 0.014508170744894902\n",
      "64 Train Loss 0.00894065 Test MSE 0.0010256352433077008 Test RE 0.015307509167417646\n",
      "65 Train Loss 0.008824288 Test MSE 0.001047139852899565 Test RE 0.01546715382047078\n",
      "66 Train Loss 0.008608729 Test MSE 0.0010400202953179984 Test RE 0.015414483153964048\n",
      "67 Train Loss 0.00794317 Test MSE 0.001133604629763286 Test RE 0.016093068718478556\n",
      "68 Train Loss 0.007626447 Test MSE 0.001087292873381892 Test RE 0.01576091152201421\n",
      "69 Train Loss 0.007327945 Test MSE 0.001081776773318168 Test RE 0.015720881226071938\n",
      "70 Train Loss 0.0070026545 Test MSE 0.0010978308395598454 Test RE 0.015837104177569616\n",
      "71 Train Loss 0.006832489 Test MSE 0.001054082301732122 Test RE 0.015518342079274845\n",
      "72 Train Loss 0.006525262 Test MSE 0.001068528742689062 Test RE 0.015624321425056995\n",
      "73 Train Loss 0.00624782 Test MSE 0.0010531709441158766 Test RE 0.015511632063607504\n",
      "74 Train Loss 0.005932025 Test MSE 0.000999666199996022 Test RE 0.015112473921173635\n",
      "75 Train Loss 0.005645993 Test MSE 0.0010032723470955367 Test RE 0.015139707383836203\n",
      "76 Train Loss 0.005366802 Test MSE 0.0009414533066794355 Test RE 0.01466585722902796\n",
      "77 Train Loss 0.0051299264 Test MSE 0.000946343026666413 Test RE 0.014703893667680235\n",
      "78 Train Loss 0.0050692516 Test MSE 0.0009937682869531268 Test RE 0.015067827061316293\n",
      "79 Train Loss 0.005011672 Test MSE 0.00103769350699396 Test RE 0.015397230451051387\n",
      "80 Train Loss 0.004977042 Test MSE 0.0010123283124761188 Test RE 0.015207882622352531\n",
      "81 Train Loss 0.004884695 Test MSE 0.0009776504749902783 Test RE 0.01494513588415588\n",
      "82 Train Loss 0.004813223 Test MSE 0.0009801297346565623 Test RE 0.014964073844273664\n",
      "83 Train Loss 0.004761705 Test MSE 0.0009567248949561434 Test RE 0.014784328294004572\n",
      "84 Train Loss 0.0047026677 Test MSE 0.0009527202004075123 Test RE 0.014753353451252652\n",
      "85 Train Loss 0.0045916922 Test MSE 0.000935189358450872 Test RE 0.014616986250591057\n",
      "86 Train Loss 0.0045149755 Test MSE 0.0009287489786901281 Test RE 0.014566567809123944\n",
      "87 Train Loss 0.0043803975 Test MSE 0.0008973648582193517 Test RE 0.014318337311775333\n",
      "88 Train Loss 0.0043153837 Test MSE 0.0009564658872570372 Test RE 0.014782326927636537\n",
      "89 Train Loss 0.0042151227 Test MSE 0.0009654626339077073 Test RE 0.014851687253853161\n",
      "90 Train Loss 0.004126161 Test MSE 0.000922005317679362 Test RE 0.014513587421526536\n",
      "91 Train Loss 0.0040854714 Test MSE 0.0008979060880139559 Test RE 0.014322654587267237\n",
      "92 Train Loss 0.004027449 Test MSE 0.000886036977261703 Test RE 0.01422767655821298\n",
      "93 Train Loss 0.0039412905 Test MSE 0.0008476530529150457 Test RE 0.013916086690774241\n",
      "94 Train Loss 0.0038273737 Test MSE 0.0008612153180542217 Test RE 0.014026972100753753\n",
      "95 Train Loss 0.0036946216 Test MSE 0.0007718073341881692 Test RE 0.013278912551962748\n",
      "96 Train Loss 0.0036509996 Test MSE 0.0007512514007051441 Test RE 0.013100887209627793\n",
      "97 Train Loss 0.0036185675 Test MSE 0.0007158766383848884 Test RE 0.012788722250157147\n",
      "98 Train Loss 0.0035649126 Test MSE 0.0006709718498493825 Test RE 0.012381127908847479\n",
      "99 Train Loss 0.003507948 Test MSE 0.0006195032639442976 Test RE 0.011896791757174185\n",
      "Training time: 160.43\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#for tune_reps in range(4,5):\n",
    "max_reps = 10 #10\n",
    "max_iter = 100 #100\n",
    "\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "N_I = 200  #Total number of data points for 'y'\n",
    "N_B = 400\n",
    "N_f = 10000 #Total number of collocation points\n",
    "\n",
    "for reps in range(max_reps):\n",
    "  print(reps)\n",
    "\n",
    "  train_loss = []\n",
    "  test_mse_loss = []\n",
    "  test_re_loss = []\n",
    "  beta_val = []\n",
    "\n",
    "  torch.manual_seed(reps*36)\n",
    "\n",
    "  layers = np.array([2,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "\n",
    "  PINN = Sequentialmodel(layers)\n",
    "\n",
    "  PINN.to(device)\n",
    "\n",
    "  'Neural Network Summary'\n",
    "  print(PINN)\n",
    "\n",
    "  params = list(PINN.parameters())\n",
    "\n",
    "\n",
    "  optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.1, \n",
    "                            max_iter = 20, \n",
    "                            max_eval = 30, \n",
    "                            tolerance_grad = 1e-8, \n",
    "                            tolerance_change = 1e-8, \n",
    "                            history_size = 100, \n",
    "                            line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "  nan_flag = train_model(max_iter,reps)\n",
    "\n",
    "  torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "  train_loss_full.append(train_loss)\n",
    "  test_mse_full.append(test_mse_loss)\n",
    "  test_re_full.append(test_re_loss)\n",
    "  #elapsed_time[reps] = time.time() - start_time\n",
    "  beta_full.append(beta_val)\n",
    "\n",
    "  #print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"beta\": beta_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "executionInfo": {
     "elapsed": 4,
     "status": "aborted",
     "timestamp": 1660688516819,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "_O3sPdAnSq_2"
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "executionInfo": {
     "elapsed": 3,
     "status": "aborted",
     "timestamp": 1660688516819,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "jQ4afiEWSq_2"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'KG_swish_tune4.mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'KG_swish_tune4.mat'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_21962/3813187685.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtune_reps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m4\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"KG_swish_tune\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtune_reps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".mat\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0;31m#re = np.array(data[\"test_re_loss\"])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \"\"\"\n\u001b[1;32m    223\u001b[0m     \u001b[0mvariable_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'variable_names'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0mMR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mmatfile_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file_context\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mappendmat\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.mat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mfile_like\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'.mat'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             raise IOError(\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'KG_swish_tune4.mat'"
     ]
    }
   ],
   "source": [
    "for tune_reps in range(4,5):\n",
    "    label = \"KG_swish_tune\"+str(tune_reps)+\".mat\"\n",
    "    data = sio.loadmat(label)\n",
    " \n",
    "    #re = np.array(data[\"test_re_loss\"])\n",
    "    #print(\"tune_reps\",\" \",np.mean(re[0][0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1660688534316,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "06syezgfv_qO",
    "outputId": "9f4852d5-694a-4977-8893-a6183a2ce493"
   },
   "outputs": [],
   "source": [
    "tune_reps = 4\n",
    "label = \"KG_swish_tune\"+str(tune_reps)+\".mat\"\n",
    "data = sio.loadmat(label)\n",
    "for k in range(10):\n",
    "    print(tune_reps,\" \",data[\"test_re_loss\"][0][k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.mean(data[\"test_re_loss\"][:,-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.0411940104463138\n",
      "0.612225823013911\n",
      "0.03811482970796047\n",
      "1.0443626102221957\n",
      "1.4076454383354298\n",
      "0.3801525612974972\n",
      "0.04905337200168333\n",
      "0.14675938294395968\n",
      "1.411888032738035\n",
      "0.011896791757174185\n",
      "a =  0.6143292852464161\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a+ test_re_full[i][-1]\n",
    "    print(test_re_full[i][-1])\n",
    "    \n",
    "print(\"a = \",a/10)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "stan_2D_KG_16Aug2022_tune.ipynb",
   "version": ""
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
