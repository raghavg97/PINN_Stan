{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def true_2D_1(xt): #True function for 2D_1 Klein Gordon Equation x \\in [-50,50] , t \\in [0,10]\n",
    "#     y = xt[:,0]*np.cos(xt[:,1])\n",
    "#     return y.reshape(-1,1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_thresh = 25000\n",
    "level = \"_medium\"\n",
    "label = \"ES_rowdy\" + level\n",
    "ES_val = 200.0\n",
    "\n",
    "\n",
    "x = np.linspace(0,1,500).reshape(-1,1)\n",
    "y = np.linspace(0,1,500).reshape(-1,1)\n",
    "\n",
    "X,Y = np.meshgrid(x,y)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "Y = Y.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xy = np.hstack((X,Y))\n",
    "\n",
    "# bound_pts_1 = (X == 0).reshape(-1,)\n",
    "# bound_pts_2 = np.logical_and(Y == 0,X != 0).reshape(-1,)\n",
    "# bound_pts_3 = np.logical_and(X == 1,Y != 0).reshape(-1,) \n",
    "# bound_pts_4 = np.logical_and(Y == 1,X != 1).reshape(-1,) \n",
    "\n",
    "# xy_bound_1 = xy[bound_pts_1,:]\n",
    "# xy_bound_2 = xy[bound_pts_2,:]\n",
    "# xy_bound_3 = xy[bound_pts_3,:]\n",
    "# xy_bound_4 = xy[bound_pts_4,:]\n",
    "\n",
    "# u_bound_1 = 1000*np.ones((np.shape(xy_bound_1)[0],1))\n",
    "# u_bound_2 = 800*np.ones((np.shape(xy_bound_2)[0],1))\n",
    "# u_bound_3 = 500*np.ones((np.shape(xy_bound_3)[0],1))\n",
    "# u_bound_4 = np.zeros((np.shape(xy_bound_4)[0],1))\n",
    "\n",
    "# xy_bound = np.vstack((xy_bound_1,xy_bound_2,xy_bound_3,xy_bound_4))\n",
    "# u_bound = np.vstack((u_bound_1,u_bound_2,u_bound_3,u_bound_4))\n",
    "\n",
    "xy_test_tensor = torch.from_numpy(xy).float().to(device)\n",
    "\n",
    "lb_xy = xy[0]\n",
    "ub_xy = xy[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_data = scipy.io.loadmat('./../ES_FEA' + level + '.mat')\n",
    "\n",
    "xy = np.array(fea_data['xy'])\n",
    "u_true = np.array(fea_data['u'])\n",
    "\n",
    "xy_test_tensor = torch.from_numpy(xy).float().to(device)\n",
    "u_true_norm = np.linalg.norm(u_true,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_T,N_f,seed):\n",
    "    '''Boundary Conditions''' \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    N_t = int(N_T/4)\n",
    "    \n",
    "    x_BC1 = np.random.uniform(size = N_t).reshape(-1,1)\n",
    "    y_BC1 = np.zeros((N_t,1))\n",
    "    u_BC1 = 0*np.ones((N_t,1))\n",
    "    \n",
    "    x_BC2 = np.ones((N_t,1))\n",
    "    y_BC2 = np.random.uniform(size = N_t).reshape(-1,1) \n",
    "    u_BC2 = ES_val*np.ones((N_t,1))\n",
    "    \n",
    "    x_BC3 = np.random.uniform(size = N_t).reshape(-1,1)\n",
    "    y_BC3 = np.ones((N_t,1)) \n",
    "    u_BC3 = 0*np.ones((N_t,1))\n",
    "    \n",
    "    x_BC4 = np.zeros((N_t,1))\n",
    "    y_BC4 = np.random.uniform(size = N_t).reshape(-1,1) \n",
    "    u_BC4 = ES_val*np.ones((N_t,1))\n",
    "    \n",
    "    XY_corners = np.array([[0,0],[1,0],[0,1],[1,1]]).reshape(-1,2)\n",
    "    U_corners = ES_val*np.ones((4,1))\n",
    "    \n",
    "    XY_1 = np.hstack((x_BC1,y_BC1))\n",
    "    XY_2 = np.hstack((x_BC2,y_BC2))\n",
    "    XY_3 = np.hstack((x_BC3,y_BC3))\n",
    "    XY_4 = np.hstack((x_BC4,y_BC4))\n",
    "    \n",
    "    xy_BC = np.vstack((XY_1,XY_2,XY_3,XY_4,XY_corners)) #choose indices from  set 'idx' (x,t)\n",
    "    u_BC = np.vstack((u_BC1,u_BC2,u_BC3,u_BC4,U_corners))\n",
    "    \n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    samples = sampling(N_f)\n",
    "    \n",
    "    xy_coll = lb_xy + (ub_xy - lb_xy)*samples\n",
    "    \n",
    "    xy_coll = np.vstack((xy_coll, xy_BC)) # append training points to collocation points \n",
    "\n",
    "    return xy_coll, xy_BC, u_BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,n_val,rowdy_terms):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        self.iter = 0\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)   \n",
    "\n",
    "        \n",
    "        self.omega1 = Parameter(torch.ones((len(layers)-2,1))) \n",
    "        self.omega1.requiresGrad = True\n",
    "        \n",
    "        \n",
    "        self.alpha = Parameter(torch.zeros(rowdy_terms,len(layers)-2))\n",
    "        self.alpha.requiresGrad = True\n",
    "        \n",
    "        \n",
    "        self.omega = Parameter((1/n_val)*torch.ones(rowdy_terms,len(layers)-2))\n",
    "        self.omega.requiresGrad = True\n",
    "        \n",
    "        self.n = torch.tensor(n_val)\n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xy):\n",
    "        if torch.is_tensor(xy) != True:         \n",
    "            xy = torch.from_numpy(xy)                \n",
    "        \n",
    "        ubxy = torch.from_numpy(ub_xy).float().to(device)\n",
    "        lbxy = torch.from_numpy(lb_xy).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xy = 2.0*(xy - lbxy)/(ubxy - lbxy) - 1.0\n",
    "        \n",
    "        #convert to float\n",
    "        a = xy.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(self.omega1[i,0]*z)\n",
    "            for j in range(rowdy_terms):\n",
    "                a = a + self.alpha[j,i]*self.n*torch.sin((j+1)*self.n*self.omega[j,i]*z)\n",
    "                \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC(self,xy,u):\n",
    "                \n",
    "        loss_bc = self.loss_function(self.forward(xy), u)\n",
    "                \n",
    "        return loss_bc\n",
    "    \n",
    "    def loss_PDE(self, xy_coll, f_hat):\n",
    "        \n",
    "        g = xy_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_y = autograd.grad(u,g,torch.ones([xy_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_xx_yy = autograd.grad(u_x_y,g,torch.ones(xy_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        #du_dx = u_x_t[:,[0]]\n",
    "        \n",
    "        d2u_dx2 = u_xx_yy[:,[0]]\n",
    "        d2u_dy2 = u_xx_yy[:,[1]]    \n",
    "        \n",
    "\n",
    "        f = d2u_dx2 + d2u_dy2\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xy_BC,u_BC,xy_coll,f_hat):\n",
    "\n",
    "        loss_BC = self.loss_BC(xy_BC,u_BC)\n",
    "        loss_f = self.loss_PDE(xy_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_BC + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "     \n",
    "    'callable for optimizer'  \n",
    "    def closure(self):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = self.loss(xy_BC, u_BC, xy_coll,f_hat)\n",
    "        self.train_loss.append(loss.cpu().detach().numpy())\n",
    "        \n",
    "        u_pred = self.test(xy_test_tensor)\n",
    "        #self.test_loss.append(np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))) #Commented because no true values yet\n",
    "        self.beta_val.append(self.beta.cpu().detach().numpy())\n",
    "        \n",
    "        #print(self.iter,\"Train Loss\",self.train_loss[-1],\"Test Loss\",self.test_loss[-1])\n",
    "        print(self.iter,\"Train Loss\",self.train_loss[-1])\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        self.iter += 1\n",
    "  \n",
    "\n",
    "        return loss        \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        u_pred = self.forward(xy_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "     \n",
    "        \n",
    "        return test_mse, test_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    alpha_val.append(PINN.alpha.cpu().detach().numpy())\n",
    "    omega_val.append(PINN.omega.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(xy_BC,u_BC,xy_coll,f_hat,seed):\n",
    "\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(xy_BC,u_BC,xy_coll,f_hat)\n",
    "        loss.backward()\n",
    "        #print(loss.cpu().detach().numpy())\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*9)\n",
    "    start_time = time.time() \n",
    "    thresh_flag = 0\n",
    "\n",
    "    xy_coll_np_array, xy_BC_np_array, u_BC_np_array = trainingdata(N_T,N_f,rep*22)\n",
    "        \n",
    "    xy_coll = torch.from_numpy(xy_coll_np_array).float().to(device)\n",
    "    xy_BC = torch.from_numpy(xy_BC_np_array).float().to(device)\n",
    "    u_BC = torch.from_numpy(u_BC_np_array).float().to(device)\n",
    "        \n",
    "    f_hat = torch.zeros(xy_coll.shape[0],1).to(device)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        train_step(xy_BC,u_BC,xy_coll,f_hat,i)\n",
    "        loss_np = PINN.loss(xy_BC,u_BC,xy_coll,f_hat).cpu().detach().numpy()\n",
    "        \n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1          \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        \n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])   \n",
    "        \n",
    "\n",
    "            \n",
    "    elapsed_time[rep] = time.time() - start_time  \n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ES_rowdy_medium\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 18892.32 Test MSE 11918.513954017657 Test RE 0.9554467741281972\n",
      "1 Train Loss 16132.558 Test MSE 9092.459970646149 Test RE 0.8345188342188734\n",
      "2 Train Loss 11191.358 Test MSE 4083.355755541488 Test RE 0.5592475023767522\n",
      "3 Train Loss 10004.26 Test MSE 3002.8250430961884 Test RE 0.47957933196450553\n",
      "4 Train Loss 9996.266 Test MSE 3001.5737742094148 Test RE 0.4794794018629443\n",
      "5 Train Loss 9986.961 Test MSE 2994.103375712531 Test RE 0.47888235945549\n",
      "6 Train Loss 9963.353 Test MSE 2985.8259564909235 Test RE 0.4782199485475994\n",
      "7 Train Loss 9912.267 Test MSE 2961.1294464312896 Test RE 0.4762381038116287\n",
      "8 Train Loss 9794.129 Test MSE 2908.299419097481 Test RE 0.4719706605282295\n",
      "9 Train Loss 9612.099 Test MSE 2848.2304840702327 Test RE 0.4670711137637552\n",
      "10 Train Loss 9242.32 Test MSE 2707.650504606712 Test RE 0.4553986584707365\n",
      "11 Train Loss 8925.987 Test MSE 2581.5127119220406 Test RE 0.44466462190806116\n",
      "12 Train Loss 8780.842 Test MSE 2496.6421389257516 Test RE 0.4372940739679121\n",
      "13 Train Loss 8368.948 Test MSE 2390.260371022142 Test RE 0.4278761204720888\n",
      "14 Train Loss 7981.557 Test MSE 2189.9555665689477 Test RE 0.4095558139466811\n",
      "15 Train Loss 7604.895 Test MSE 2116.174678912628 Test RE 0.4025976175939424\n",
      "16 Train Loss 7249.4517 Test MSE 1954.4858277844498 Test RE 0.38691156092974094\n",
      "17 Train Loss 7051.7104 Test MSE 1894.3346698173293 Test RE 0.3809112484113706\n",
      "18 Train Loss 6617.158 Test MSE 1717.5953791437944 Test RE 0.3627069482954506\n",
      "19 Train Loss 6407.051 Test MSE 1609.9268728709428 Test RE 0.3511547267552288\n",
      "20 Train Loss 6258.2695 Test MSE 1587.8476779515604 Test RE 0.3487384740870266\n",
      "21 Train Loss 6031.2183 Test MSE 1488.2005048571466 Test RE 0.3376184467977831\n",
      "22 Train Loss 5867.463 Test MSE 1458.2724408450574 Test RE 0.33420641207491275\n",
      "23 Train Loss 5766.488 Test MSE 1448.6079510190868 Test RE 0.3330971187540992\n",
      "24 Train Loss 5646.7817 Test MSE 1420.111731926096 Test RE 0.3298045944652633\n",
      "25 Train Loss 5486.417 Test MSE 1351.1560205051223 Test RE 0.32169787650303056\n",
      "26 Train Loss 5310.317 Test MSE 1271.3755343244968 Test RE 0.3120558788730992\n",
      "27 Train Loss 4990.4146 Test MSE 1209.2103055717855 Test RE 0.3043311196030339\n",
      "28 Train Loss 4869.5 Test MSE 1236.1468281744937 Test RE 0.307702109223033\n",
      "29 Train Loss 4747.4263 Test MSE 1172.67894295895 Test RE 0.2996988022301893\n",
      "30 Train Loss 4688.3853 Test MSE 1128.7959047252273 Test RE 0.2940377943793576\n",
      "31 Train Loss 4621.652 Test MSE 1151.0002292155934 Test RE 0.29691569077194846\n",
      "32 Train Loss 4529.9116 Test MSE 1138.9221853763074 Test RE 0.29535373700876094\n",
      "33 Train Loss 4422.276 Test MSE 1064.6429626850559 Test RE 0.28556004234776994\n",
      "34 Train Loss 4352.0894 Test MSE 997.1315416281885 Test RE 0.27635776449022403\n",
      "35 Train Loss 4264.049 Test MSE 973.9158835617873 Test RE 0.2731216756245944\n",
      "36 Train Loss 4182.4736 Test MSE 933.5030104507683 Test RE 0.26739501421039724\n",
      "37 Train Loss 4117.0605 Test MSE 936.3791734805778 Test RE 0.26780662519114445\n",
      "38 Train Loss 3945.2527 Test MSE 852.078442573122 Test RE 0.25546724984589836\n",
      "39 Train Loss 3861.9604 Test MSE 844.5804385927631 Test RE 0.25434075319351057\n",
      "40 Train Loss 3682.4565 Test MSE 773.249106710428 Test RE 0.24336334008227553\n",
      "41 Train Loss 3620.204 Test MSE 734.0699283342295 Test RE 0.23711780194377255\n",
      "42 Train Loss 3564.142 Test MSE 733.887495445336 Test RE 0.23708833555921385\n",
      "43 Train Loss 3531.8276 Test MSE 724.6428203563158 Test RE 0.23559031922490709\n",
      "44 Train Loss 3496.8462 Test MSE 693.3904777257409 Test RE 0.2304540688987847\n",
      "45 Train Loss 3427.1406 Test MSE 668.6781747890727 Test RE 0.22631014245138964\n",
      "46 Train Loss 3302.3386 Test MSE 621.6630685626675 Test RE 0.21820916011834943\n",
      "47 Train Loss 3252.1538 Test MSE 586.788907270742 Test RE 0.21200025898078262\n",
      "48 Train Loss 3208.8838 Test MSE 606.4364176511922 Test RE 0.21552024924109572\n",
      "49 Train Loss 3156.5823 Test MSE 586.8045168013177 Test RE 0.21200307873629962\n",
      "50 Train Loss 3121.502 Test MSE 564.1682070198914 Test RE 0.20787379657733301\n",
      "51 Train Loss 3072.0894 Test MSE 566.624047655411 Test RE 0.20832574561980194\n",
      "52 Train Loss 3032.757 Test MSE 545.3313216774726 Test RE 0.20437400945415296\n",
      "53 Train Loss 2948.6724 Test MSE 515.8409116421182 Test RE 0.1987711431892044\n",
      "54 Train Loss 2926.0864 Test MSE 513.4457020248311 Test RE 0.1983091281587317\n",
      "55 Train Loss 2882.1719 Test MSE 506.5772566809126 Test RE 0.19697825587510545\n",
      "56 Train Loss 2850.7183 Test MSE 485.67509086168906 Test RE 0.19287163357695197\n",
      "57 Train Loss 2829.3862 Test MSE 488.5896735209544 Test RE 0.19344948849055763\n",
      "58 Train Loss 2808.9277 Test MSE 467.2113855241959 Test RE 0.18916995155309044\n",
      "59 Train Loss 2783.5972 Test MSE 451.78066370256914 Test RE 0.1860198386616876\n",
      "60 Train Loss 2735.8533 Test MSE 432.24215572889915 Test RE 0.18195290931112132\n",
      "61 Train Loss 2687.9844 Test MSE 417.1110629964171 Test RE 0.17873981257188218\n",
      "62 Train Loss 2673.7383 Test MSE 415.05265385980914 Test RE 0.17829823389796437\n",
      "63 Train Loss 2651.0254 Test MSE 409.7512610768284 Test RE 0.17715588882094302\n",
      "64 Train Loss 2632.4248 Test MSE 406.0332053801918 Test RE 0.17635030682953537\n",
      "65 Train Loss 2592.3975 Test MSE 401.80331554725956 Test RE 0.17542932884401263\n",
      "66 Train Loss 2567.1887 Test MSE 406.7558841249095 Test RE 0.1765071757247063\n",
      "67 Train Loss 2548.7593 Test MSE 399.4097638612211 Test RE 0.174906030060288\n",
      "68 Train Loss 2517.5237 Test MSE 383.55220993864145 Test RE 0.17139876507552543\n",
      "69 Train Loss 2496.1057 Test MSE 372.85344717255225 Test RE 0.16899136937353973\n",
      "70 Train Loss 2460.485 Test MSE 360.4667815534041 Test RE 0.1661606063266681\n",
      "71 Train Loss 2410.8137 Test MSE 358.7097626839728 Test RE 0.16575515433478\n",
      "72 Train Loss 2387.729 Test MSE 364.616467649679 Test RE 0.1671142882122608\n",
      "73 Train Loss 2364.9277 Test MSE 342.4661557572141 Test RE 0.1619586966658837\n",
      "74 Train Loss 2324.8115 Test MSE 333.09310849131765 Test RE 0.1597269752700091\n",
      "75 Train Loss 2286.8936 Test MSE 323.3985854154432 Test RE 0.15738542160225477\n",
      "76 Train Loss 2276.27 Test MSE 321.5255686143348 Test RE 0.15692899783043943\n",
      "77 Train Loss 2267.0544 Test MSE 315.4725630568526 Test RE 0.15544481521126527\n",
      "78 Train Loss 2248.595 Test MSE 308.73625261936314 Test RE 0.15377624720219796\n",
      "79 Train Loss 2226.4177 Test MSE 302.13716810977905 Test RE 0.15212392454953988\n",
      "80 Train Loss 2188.157 Test MSE 290.3040959562593 Test RE 0.14911523783041558\n",
      "81 Train Loss 2176.4248 Test MSE 288.6611962654186 Test RE 0.14869269993359577\n",
      "82 Train Loss 2161.5254 Test MSE 285.69906868688923 Test RE 0.14792781966814186\n",
      "83 Train Loss 2146.6323 Test MSE 283.16357097958075 Test RE 0.14726994821656206\n",
      "84 Train Loss 2140.1313 Test MSE 282.5411736730935 Test RE 0.14710800852581052\n",
      "85 Train Loss 2134.9458 Test MSE 284.4281774472906 Test RE 0.14759843513388393\n",
      "86 Train Loss 2131.226 Test MSE 282.9538952514752 Test RE 0.1472154132142214\n",
      "87 Train Loss 2120.1824 Test MSE 283.4112894434658 Test RE 0.14733435182255056\n",
      "88 Train Loss 2094.3547 Test MSE 278.9027164883224 Test RE 0.14615773897250006\n",
      "89 Train Loss 2078.7302 Test MSE 276.3853108154887 Test RE 0.14549662609990385\n",
      "90 Train Loss 2052.9814 Test MSE 269.8448689588102 Test RE 0.14376478726942696\n",
      "91 Train Loss 2023.8104 Test MSE 262.7108235230584 Test RE 0.14185166140251781\n",
      "92 Train Loss 2000.6082 Test MSE 255.1154790791538 Test RE 0.1397860551153778\n",
      "93 Train Loss 1986.3567 Test MSE 253.42674165951016 Test RE 0.1393226298964667\n",
      "94 Train Loss 1967.065 Test MSE 251.28152638523724 Test RE 0.1387317052587428\n",
      "95 Train Loss 1958.9768 Test MSE 250.36684163833098 Test RE 0.13847897784007468\n",
      "96 Train Loss 1950.4985 Test MSE 248.84408726308763 Test RE 0.13805721456123024\n",
      "97 Train Loss 1937.692 Test MSE 245.34418125766567 Test RE 0.1370829131444573\n",
      "98 Train Loss 1929.7988 Test MSE 244.04493688153585 Test RE 0.13671946327260157\n",
      "99 Train Loss 1924.2295 Test MSE 244.33714976334286 Test RE 0.13680129090013932\n",
      "100 Train Loss 1917.1227 Test MSE 243.76382302077687 Test RE 0.13664069742945836\n",
      "101 Train Loss 1906.8007 Test MSE 242.39874856255094 Test RE 0.13625756716940718\n",
      "102 Train Loss 1901.1545 Test MSE 242.44134994959356 Test RE 0.13626954022283208\n",
      "103 Train Loss 1889.1638 Test MSE 242.51352596648977 Test RE 0.1362898227792875\n",
      "104 Train Loss 1875.0271 Test MSE 241.33316602552625 Test RE 0.13595774383249737\n",
      "105 Train Loss 1861.1334 Test MSE 241.44492395021044 Test RE 0.13598922022855373\n",
      "106 Train Loss 1851.8822 Test MSE 239.20028092686172 Test RE 0.1353556181317359\n",
      "107 Train Loss 1846.579 Test MSE 240.25877116270746 Test RE 0.13565477005930185\n",
      "108 Train Loss 1839.9783 Test MSE 238.99176756332326 Test RE 0.13529660982193595\n",
      "109 Train Loss 1834.5353 Test MSE 239.77336525607322 Test RE 0.13551766597206702\n",
      "110 Train Loss 1831.8267 Test MSE 239.21093741468883 Test RE 0.1353586331771753\n",
      "111 Train Loss 1822.3973 Test MSE 240.91429246544214 Test RE 0.13583970403355392\n",
      "112 Train Loss 1815.7391 Test MSE 240.31256326597872 Test RE 0.13566995524269043\n",
      "113 Train Loss 1809.3912 Test MSE 240.84756417884734 Test RE 0.13582089033355427\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 200 #200\n",
    "\n",
    "\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "alpha_full = []\n",
    "omega_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "\n",
    "n_val = 5.0\n",
    "rowdy_terms = 6\n",
    "\n",
    "N_T = 5000 #Total number of data points for 'y'\n",
    "N_f = 10000 #Total number of collocation points \n",
    "\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(label)\n",
    "    print(reps)\n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []\n",
    "\n",
    "    alpha_val = []    \n",
    "    omega_val = []\n",
    "\n",
    "    torch.manual_seed(reps*36)\n",
    "\n",
    "    layers = np.array([2,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "\n",
    "    PINN = Sequentialmodel(layers,n_val,rowdy_terms)\n",
    "\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.05, \n",
    "                              max_iter = 20, \n",
    "                              max_eval = 30, \n",
    "                              tolerance_grad = 1e-08, \n",
    "                              tolerance_change = 1e-08, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    nan_flag = train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    #elapsed_time[reps] = time.time() - start_time\n",
    "    alpha_full.append(alpha_val)\n",
    "    omega_full.append(omega_val)\n",
    "\n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time,\"alpha\": alpha_full,\"omega\": omega_full,  \"label\": label}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred = PINN.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "cmap = plt.cm.jet\n",
    "\n",
    "img3 = ax.imshow(np.transpose(np.flipud(np.transpose(u_pred.reshape(500,500)))),vmin = 0,vmax = 1000,cmap = cmap,extent=[0,1,0,1],aspect = 0.75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "\n",
    "for tune_reps in range(75):\n",
    "    label = \"MW_rowdy_tune\"+str(tune_reps)+\".mat\"\n",
    "    data = sio.loadmat(label)\n",
    "    re = np.array(data[\"test_re_loss\"])\n",
    "    print(tune_reps,\" \",np.mean(re[:,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrnr_tune[31]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
