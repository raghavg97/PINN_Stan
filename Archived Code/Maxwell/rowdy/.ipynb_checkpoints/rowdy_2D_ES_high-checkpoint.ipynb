{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def true_2D_1(xt): #True function for 2D_1 Klein Gordon Equation x \\in [-50,50] , t \\in [0,10]\n",
    "#     y = xt[:,0]*np.cos(xt[:,1])\n",
    "#     return y.reshape(-1,1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_thresh = 25000\n",
    "level = \"_high\"\n",
    "label = \"ES_rowdy\" + level\n",
    "ES_val = 1000.0\n",
    "\n",
    "\n",
    "x = np.linspace(0,1,500).reshape(-1,1)\n",
    "y = np.linspace(0,1,500).reshape(-1,1)\n",
    "\n",
    "X,Y = np.meshgrid(x,y)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "Y = Y.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xy = np.hstack((X,Y))\n",
    "\n",
    "# bound_pts_1 = (X == 0).reshape(-1,)\n",
    "# bound_pts_2 = np.logical_and(Y == 0,X != 0).reshape(-1,)\n",
    "# bound_pts_3 = np.logical_and(X == 1,Y != 0).reshape(-1,) \n",
    "# bound_pts_4 = np.logical_and(Y == 1,X != 1).reshape(-1,) \n",
    "\n",
    "# xy_bound_1 = xy[bound_pts_1,:]\n",
    "# xy_bound_2 = xy[bound_pts_2,:]\n",
    "# xy_bound_3 = xy[bound_pts_3,:]\n",
    "# xy_bound_4 = xy[bound_pts_4,:]\n",
    "\n",
    "# u_bound_1 = 1000*np.ones((np.shape(xy_bound_1)[0],1))\n",
    "# u_bound_2 = 800*np.ones((np.shape(xy_bound_2)[0],1))\n",
    "# u_bound_3 = 500*np.ones((np.shape(xy_bound_3)[0],1))\n",
    "# u_bound_4 = np.zeros((np.shape(xy_bound_4)[0],1))\n",
    "\n",
    "# xy_bound = np.vstack((xy_bound_1,xy_bound_2,xy_bound_3,xy_bound_4))\n",
    "# u_bound = np.vstack((u_bound_1,u_bound_2,u_bound_3,u_bound_4))\n",
    "\n",
    "xy_test_tensor = torch.from_numpy(xy).float().to(device)\n",
    "\n",
    "lb_xy = xy[0]\n",
    "ub_xy = xy[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_data = scipy.io.loadmat('./../ES_FEA' + level + '.mat')\n",
    "\n",
    "xy = np.array(fea_data['xy'])\n",
    "u_true = np.array(fea_data['u'])\n",
    "\n",
    "xy_test_tensor = torch.from_numpy(xy).float().to(device)\n",
    "u_true_norm = np.linalg.norm(u_true,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_T,N_f,seed):\n",
    "    '''Boundary Conditions''' \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    N_t = int(N_T/4)\n",
    "    \n",
    "    x_BC1 = np.random.uniform(size = N_t).reshape(-1,1)\n",
    "    y_BC1 = np.zeros((N_t,1))\n",
    "    u_BC1 = 0*np.ones((N_t,1))\n",
    "    \n",
    "    x_BC2 = np.ones((N_t,1))\n",
    "    y_BC2 = np.random.uniform(size = N_t).reshape(-1,1) \n",
    "    u_BC2 = ES_val*np.ones((N_t,1))\n",
    "    \n",
    "    x_BC3 = np.random.uniform(size = N_t).reshape(-1,1)\n",
    "    y_BC3 = np.ones((N_t,1)) \n",
    "    u_BC3 = 0*np.ones((N_t,1))\n",
    "    \n",
    "    x_BC4 = np.zeros((N_t,1))\n",
    "    y_BC4 = np.random.uniform(size = N_t).reshape(-1,1) \n",
    "    u_BC4 = ES_val*np.ones((N_t,1))\n",
    "    \n",
    "    XY_corners = np.array([[0,0],[1,0],[0,1],[1,1]]).reshape(-1,2)\n",
    "    U_corners = ES_val*np.ones((4,1))\n",
    "    \n",
    "    XY_1 = np.hstack((x_BC1,y_BC1))\n",
    "    XY_2 = np.hstack((x_BC2,y_BC2))\n",
    "    XY_3 = np.hstack((x_BC3,y_BC3))\n",
    "    XY_4 = np.hstack((x_BC4,y_BC4))\n",
    "    \n",
    "    xy_BC = np.vstack((XY_1,XY_2,XY_3,XY_4,XY_corners)) #choose indices from  set 'idx' (x,t)\n",
    "    u_BC = np.vstack((u_BC1,u_BC2,u_BC3,u_BC4,U_corners))\n",
    "    \n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    samples = sampling(N_f)\n",
    "    \n",
    "    xy_coll = lb_xy + (ub_xy - lb_xy)*samples\n",
    "    \n",
    "    xy_coll = np.vstack((xy_coll, xy_BC)) # append training points to collocation points \n",
    "\n",
    "    return xy_coll, xy_BC, u_BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,n_val,rowdy_terms):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        self.iter = 0\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)   \n",
    "\n",
    "        \n",
    "        self.omega1 = Parameter(torch.ones((len(layers)-2,1))) \n",
    "        self.omega1.requiresGrad = True\n",
    "        \n",
    "        \n",
    "        self.alpha = Parameter(torch.zeros(rowdy_terms,len(layers)-2))\n",
    "        self.alpha.requiresGrad = True\n",
    "        \n",
    "        \n",
    "        self.omega = Parameter((1/n_val)*torch.ones(rowdy_terms,len(layers)-2))\n",
    "        self.omega.requiresGrad = True\n",
    "        \n",
    "        self.n = torch.tensor(n_val)\n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xy):\n",
    "        if torch.is_tensor(xy) != True:         \n",
    "            xy = torch.from_numpy(xy)                \n",
    "        \n",
    "        ubxy = torch.from_numpy(ub_xy).float().to(device)\n",
    "        lbxy = torch.from_numpy(lb_xy).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xy = 2.0*(xy - lbxy)/(ubxy - lbxy) - 1.0\n",
    "        \n",
    "        #convert to float\n",
    "        a = xy.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(self.omega1[i,0]*z)\n",
    "            for j in range(rowdy_terms):\n",
    "                a = a + self.alpha[j,i]*self.n*torch.sin((j+1)*self.n*self.omega[j,i]*z)\n",
    "                \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC(self,xy,u):\n",
    "                \n",
    "        loss_bc = self.loss_function(self.forward(xy), u)\n",
    "                \n",
    "        return loss_bc\n",
    "    \n",
    "    def loss_PDE(self, xy_coll, f_hat):\n",
    "        \n",
    "        g = xy_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_y = autograd.grad(u,g,torch.ones([xy_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_xx_yy = autograd.grad(u_x_y,g,torch.ones(xy_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        #du_dx = u_x_t[:,[0]]\n",
    "        \n",
    "        d2u_dx2 = u_xx_yy[:,[0]]\n",
    "        d2u_dy2 = u_xx_yy[:,[1]]    \n",
    "        \n",
    "\n",
    "        f = d2u_dx2 + d2u_dy2\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xy_BC,u_BC,xy_coll,f_hat):\n",
    "\n",
    "        loss_BC = self.loss_BC(xy_BC,u_BC)\n",
    "        loss_f = self.loss_PDE(xy_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_BC + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "     \n",
    "    'callable for optimizer'  \n",
    "    def closure(self):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = self.loss(xy_BC, u_BC, xy_coll,f_hat)\n",
    "        self.train_loss.append(loss.cpu().detach().numpy())\n",
    "        \n",
    "        u_pred = self.test(xy_test_tensor)\n",
    "        #self.test_loss.append(np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))) #Commented because no true values yet\n",
    "        self.beta_val.append(self.beta.cpu().detach().numpy())\n",
    "        \n",
    "        #print(self.iter,\"Train Loss\",self.train_loss[-1],\"Test Loss\",self.test_loss[-1])\n",
    "        print(self.iter,\"Train Loss\",self.train_loss[-1])\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        self.iter += 1\n",
    "  \n",
    "\n",
    "        return loss        \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        u_pred = self.forward(xy_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "     \n",
    "        \n",
    "        return test_mse, test_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    alpha_val.append(PINN.alpha.cpu().detach().numpy())\n",
    "    omega_val.append(PINN.omega.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(xy_BC,u_BC,xy_coll,f_hat,seed):\n",
    "\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(xy_BC,u_BC,xy_coll,f_hat)\n",
    "        loss.backward()\n",
    "        #print(loss.cpu().detach().numpy())\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*9)\n",
    "    start_time = time.time() \n",
    "    thresh_flag = 0\n",
    "\n",
    "    xy_coll_np_array, xy_BC_np_array, u_BC_np_array = trainingdata(N_T,N_f,rep*22)\n",
    "        \n",
    "    xy_coll = torch.from_numpy(xy_coll_np_array).float().to(device)\n",
    "    xy_BC = torch.from_numpy(xy_BC_np_array).float().to(device)\n",
    "    u_BC = torch.from_numpy(u_BC_np_array).float().to(device)\n",
    "        \n",
    "    f_hat = torch.zeros(xy_coll.shape[0],1).to(device)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        train_step(xy_BC,u_BC,xy_coll,f_hat,i)\n",
    "        loss_np = PINN.loss(xy_BC,u_BC,xy_coll,f_hat).cpu().detach().numpy()\n",
    "        \n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1          \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        \n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])   \n",
    "        \n",
    "\n",
    "            \n",
    "    elapsed_time[rep] = time.time() - start_time  \n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ES_rowdy_high\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 478769.34 Test MSE 303992.4135982271 Test RE 0.9646447712793137\n",
      "1 Train Loss 429138.2 Test MSE 255096.0019929177 Test RE 0.8836654354786452\n",
      "2 Train Loss 361272.9 Test MSE 184444.0855207416 Test RE 0.7513951864511919\n",
      "3 Train Loss 322407.44 Test MSE 147401.85649680224 Test RE 0.6717188233371305\n",
      "4 Train Loss 275740.4 Test MSE 97946.22562793051 Test RE 0.547557704774788\n",
      "5 Train Loss 252163.55 Test MSE 76713.74211349247 Test RE 0.4845879671930971\n",
      "6 Train Loss 249762.53 Test MSE 74917.71602068948 Test RE 0.4788817716943218\n",
      "7 Train Loss 248334.3 Test MSE 74213.86273403474 Test RE 0.4766269116635858\n",
      "8 Train Loss 247047.47 Test MSE 73573.5391761199 Test RE 0.47456626829388443\n",
      "9 Train Loss 245730.19 Test MSE 72946.97101623648 Test RE 0.47254119313018556\n",
      "10 Train Loss 243980.12 Test MSE 72129.66760167977 Test RE 0.4698865427145621\n",
      "11 Train Loss 242731.4 Test MSE 71560.34627053238 Test RE 0.4680284556538477\n",
      "12 Train Loss 241310.61 Test MSE 70957.19552782006 Test RE 0.46605187872498494\n",
      "13 Train Loss 239419.95 Test MSE 70195.58836827948 Test RE 0.4635439862144362\n",
      "14 Train Loss 237859.14 Test MSE 69640.4555438353 Test RE 0.46170740878560657\n",
      "15 Train Loss 235649.72 Test MSE 68685.4574047329 Test RE 0.45853072208858975\n",
      "16 Train Loss 233971.98 Test MSE 67985.35758244414 Test RE 0.456187871765544\n",
      "17 Train Loss 232149.77 Test MSE 67301.38146430535 Test RE 0.45388730015955087\n",
      "18 Train Loss 228966.8 Test MSE 65841.16517216171 Test RE 0.4489363753123664\n",
      "19 Train Loss 225970.78 Test MSE 64989.05174772682 Test RE 0.4460218574351373\n",
      "20 Train Loss 224044.88 Test MSE 64110.74503511434 Test RE 0.442997682014141\n",
      "21 Train Loss 222031.22 Test MSE 63396.1754545326 Test RE 0.4405219684577115\n",
      "22 Train Loss 221165.48 Test MSE 63061.780767034405 Test RE 0.43935862563836425\n",
      "23 Train Loss 219805.05 Test MSE 62318.936355825295 Test RE 0.43676321885621555\n",
      "24 Train Loss 218380.4 Test MSE 61651.44873900723 Test RE 0.43441787321160863\n",
      "25 Train Loss 216335.19 Test MSE 60793.56775567363 Test RE 0.4313848188605232\n",
      "26 Train Loss 214036.89 Test MSE 59784.48032991053 Test RE 0.4277896481578793\n",
      "27 Train Loss 210128.0 Test MSE 58611.871496293956 Test RE 0.42357355332909086\n",
      "28 Train Loss 208186.5 Test MSE 57871.01507144751 Test RE 0.42088804669598295\n",
      "29 Train Loss 206398.4 Test MSE 57155.32460821439 Test RE 0.4182773902382952\n",
      "30 Train Loss 204258.02 Test MSE 55518.1010624168 Test RE 0.41224305075043205\n",
      "31 Train Loss 202620.45 Test MSE 54890.84771177244 Test RE 0.40990763799097835\n",
      "32 Train Loss 200956.45 Test MSE 54452.44824981472 Test RE 0.40826744161207135\n",
      "33 Train Loss 199997.44 Test MSE 54029.56586435356 Test RE 0.40667903167752495\n",
      "34 Train Loss 197926.16 Test MSE 53291.98649407289 Test RE 0.40389362317833316\n",
      "35 Train Loss 195867.3 Test MSE 53124.841056726014 Test RE 0.40325973806878984\n",
      "36 Train Loss 194852.36 Test MSE 52419.576564018564 Test RE 0.40057403587774293\n",
      "37 Train Loss 193177.1 Test MSE 51191.54343917318 Test RE 0.39585410576747876\n",
      "38 Train Loss 192405.9 Test MSE 51262.50353492604 Test RE 0.39612837096628295\n",
      "39 Train Loss 191454.4 Test MSE 51399.66418744715 Test RE 0.39665796790781993\n",
      "40 Train Loss 190023.9 Test MSE 50474.19918019384 Test RE 0.39307077990503025\n",
      "41 Train Loss 189422.12 Test MSE 50152.183774014185 Test RE 0.39181491675031743\n",
      "42 Train Loss 188829.69 Test MSE 49919.79718246397 Test RE 0.3909061003524555\n",
      "43 Train Loss 187849.56 Test MSE 49603.46150167621 Test RE 0.3896655697589209\n",
      "44 Train Loss 187469.8 Test MSE 49641.77582054483 Test RE 0.3898160319299931\n",
      "45 Train Loss 186760.9 Test MSE 49356.61549452373 Test RE 0.3886947972341886\n",
      "46 Train Loss 186123.83 Test MSE 49146.78653060309 Test RE 0.3878676913480194\n",
      "47 Train Loss 185315.88 Test MSE 48761.50125641574 Test RE 0.386344359341179\n",
      "48 Train Loss 184803.53 Test MSE 48726.078292600505 Test RE 0.3862040032377501\n",
      "49 Train Loss 183904.42 Test MSE 48186.05898545488 Test RE 0.38405793794398874\n",
      "50 Train Loss 182835.39 Test MSE 48426.677202746534 Test RE 0.3850156450163804\n",
      "51 Train Loss 182007.78 Test MSE 47815.313456565375 Test RE 0.3825776061903917\n",
      "52 Train Loss 180785.45 Test MSE 46991.3198070542 Test RE 0.37926683134137007\n",
      "53 Train Loss 180040.05 Test MSE 46583.84124413242 Test RE 0.377618871790931\n",
      "54 Train Loss 179511.53 Test MSE 46597.47821763505 Test RE 0.3776741399010083\n",
      "55 Train Loss 178667.64 Test MSE 46150.751623974495 Test RE 0.3758594129234388\n",
      "56 Train Loss 177924.5 Test MSE 46285.50435868124 Test RE 0.37640773732551086\n",
      "57 Train Loss 177142.5 Test MSE 46327.05344990234 Test RE 0.37657664432406346\n",
      "58 Train Loss 175694.92 Test MSE 45757.420134529246 Test RE 0.3742543070224967\n",
      "59 Train Loss 175097.0 Test MSE 45163.45364517321 Test RE 0.3718173183869407\n",
      "60 Train Loss 174538.97 Test MSE 45036.21748184042 Test RE 0.37129320018801204\n",
      "61 Train Loss 172979.11 Test MSE 44218.006834647065 Test RE 0.3679049431988125\n",
      "62 Train Loss 172120.1 Test MSE 44257.58539950959 Test RE 0.36806955818053644\n",
      "63 Train Loss 170856.31 Test MSE 43918.39406811553 Test RE 0.36665639782399295\n",
      "64 Train Loss 169872.69 Test MSE 43247.21382412308 Test RE 0.36384390873793676\n",
      "65 Train Loss 169270.62 Test MSE 43159.049652328045 Test RE 0.3634728517454776\n",
      "66 Train Loss 168384.78 Test MSE 42765.1256502616 Test RE 0.36181029322012764\n",
      "67 Train Loss 167320.84 Test MSE 42456.79420010483 Test RE 0.36050362922812335\n",
      "68 Train Loss 166267.94 Test MSE 41981.71667640588 Test RE 0.3584809968097288\n",
      "69 Train Loss 165414.58 Test MSE 41732.689385379876 Test RE 0.3574161960104134\n",
      "70 Train Loss 164544.7 Test MSE 41438.668309563735 Test RE 0.3561549108407652\n",
      "71 Train Loss 163794.72 Test MSE 41559.81196605091 Test RE 0.3566751305084556\n",
      "72 Train Loss 163242.28 Test MSE 41312.643430044365 Test RE 0.3556129224084574\n",
      "73 Train Loss 162612.61 Test MSE 40893.34494106723 Test RE 0.35380369138528706\n",
      "74 Train Loss 161993.16 Test MSE 40839.48742793635 Test RE 0.3535706306625061\n",
      "75 Train Loss 161344.69 Test MSE 40346.8049438833 Test RE 0.35143144336305154\n",
      "76 Train Loss 160925.61 Test MSE 40432.783382811096 Test RE 0.3518056916691068\n",
      "77 Train Loss 160100.45 Test MSE 40232.23378617517 Test RE 0.35093211594452534\n",
      "78 Train Loss 159543.14 Test MSE 40003.85183830406 Test RE 0.3499346492809783\n",
      "79 Train Loss 158794.77 Test MSE 39832.65063680865 Test RE 0.3491850531255371\n",
      "80 Train Loss 157314.95 Test MSE 39576.038523604555 Test RE 0.3480584660864623\n",
      "81 Train Loss 156603.06 Test MSE 39613.55451426426 Test RE 0.34822339751466513\n",
      "82 Train Loss 156191.81 Test MSE 39428.58797681102 Test RE 0.3474094710753193\n",
      "83 Train Loss 155758.69 Test MSE 39222.55024867245 Test RE 0.3465005719969329\n",
      "84 Train Loss 154811.05 Test MSE 38772.20814927202 Test RE 0.3445056188819512\n",
      "85 Train Loss 153933.08 Test MSE 38340.538069562965 Test RE 0.34258247575238626\n",
      "86 Train Loss 152945.72 Test MSE 38081.69443466518 Test RE 0.3414241004311485\n",
      "87 Train Loss 152249.27 Test MSE 37914.92294519182 Test RE 0.3406756794018623\n",
      "88 Train Loss 151919.53 Test MSE 37844.51733740271 Test RE 0.34035922585756256\n",
      "89 Train Loss 151510.55 Test MSE 37710.01789499069 Test RE 0.33975386904985094\n",
      "90 Train Loss 150990.44 Test MSE 37625.239054463906 Test RE 0.33937174053318675\n",
      "91 Train Loss 150145.69 Test MSE 37865.1964965205 Test RE 0.3404522034123882\n",
      "92 Train Loss 149612.0 Test MSE 37570.98690178309 Test RE 0.3391269807693564\n",
      "93 Train Loss 148733.8 Test MSE 37289.94528764404 Test RE 0.337856216998033\n",
      "94 Train Loss 148167.98 Test MSE 36620.96252256867 Test RE 0.3348119260767281\n",
      "95 Train Loss 147623.23 Test MSE 36327.7622773393 Test RE 0.3334689217929011\n",
      "96 Train Loss 146825.34 Test MSE 35943.72345763419 Test RE 0.33170160578282043\n",
      "97 Train Loss 146269.89 Test MSE 36020.11645933924 Test RE 0.3320539097355573\n",
      "98 Train Loss 146037.14 Test MSE 35744.498276644 Test RE 0.33078106763754567\n",
      "99 Train Loss 145556.1 Test MSE 35782.054379386565 Test RE 0.33095479488170376\n",
      "100 Train Loss 145033.2 Test MSE 35593.52571808323 Test RE 0.3300817753079899\n",
      "101 Train Loss 144657.2 Test MSE 35548.229974315575 Test RE 0.32987168008787066\n",
      "102 Train Loss 144082.89 Test MSE 35402.172758597764 Test RE 0.32919330973572314\n",
      "103 Train Loss 143466.11 Test MSE 35351.34789597306 Test RE 0.3289569229159107\n",
      "104 Train Loss 142794.45 Test MSE 35512.19992081385 Test RE 0.329704466313582\n",
      "105 Train Loss 142476.0 Test MSE 35393.69357106821 Test RE 0.32915388476904633\n",
      "106 Train Loss 142056.1 Test MSE 35382.69148986251 Test RE 0.32910272227549425\n",
      "107 Train Loss 141491.25 Test MSE 35368.665661851286 Test RE 0.3290374870500118\n",
      "108 Train Loss 141235.94 Test MSE 35271.531907611614 Test RE 0.32858535492194174\n",
      "109 Train Loss 140877.53 Test MSE 34755.683920900556 Test RE 0.32617371591432015\n",
      "110 Train Loss 140773.78 Test MSE 34704.00628367143 Test RE 0.32593113462389417\n",
      "111 Train Loss 140210.92 Test MSE 34152.77140193777 Test RE 0.32333224480758793\n",
      "112 Train Loss 139711.52 Test MSE 33976.19637271964 Test RE 0.3224953231799888\n",
      "113 Train Loss 139198.28 Test MSE 33982.68696391727 Test RE 0.3225261254117244\n",
      "114 Train Loss 138458.84 Test MSE 33598.53983875122 Test RE 0.3206979942571865\n",
      "115 Train Loss 138186.94 Test MSE 33512.201751980945 Test RE 0.32028568077546793\n",
      "116 Train Loss 137960.62 Test MSE 33536.70269637036 Test RE 0.320402740644211\n",
      "117 Train Loss 137615.75 Test MSE 33637.06405058216 Test RE 0.3208817983463561\n",
      "118 Train Loss 137202.48 Test MSE 33728.515426526224 Test RE 0.32131770386087766\n",
      "119 Train Loss 136722.95 Test MSE 33469.34592437313 Test RE 0.3200808225702321\n",
      "120 Train Loss 135520.8 Test MSE 33641.39517146131 Test RE 0.32090245611125906\n",
      "121 Train Loss 134466.88 Test MSE 34011.42098305857 Test RE 0.32266245238424895\n",
      "122 Train Loss 134001.62 Test MSE 33900.07715434946 Test RE 0.32213386634888014\n",
      "123 Train Loss 133585.02 Test MSE 33665.06273365857 Test RE 0.32101531771310243\n",
      "124 Train Loss 132724.0 Test MSE 33221.76726233454 Test RE 0.3188947779704251\n",
      "125 Train Loss 132131.89 Test MSE 32692.680609197865 Test RE 0.31634524261668917\n",
      "126 Train Loss 131482.19 Test MSE 31977.75863384519 Test RE 0.31286721184558447\n",
      "127 Train Loss 131128.06 Test MSE 31683.737559275058 Test RE 0.31142555386044196\n",
      "128 Train Loss 130574.18 Test MSE 31279.942420629326 Test RE 0.3094347003217262\n",
      "129 Train Loss 129818.27 Test MSE 31025.147302488076 Test RE 0.30817185170838\n",
      "130 Train Loss 128792.16 Test MSE 30565.885152909173 Test RE 0.3058824290612835\n",
      "131 Train Loss 127128.664 Test MSE 30357.008937415958 Test RE 0.3048354923969136\n",
      "132 Train Loss 126455.56 Test MSE 30038.745370896166 Test RE 0.3032333309688603\n",
      "133 Train Loss 125809.68 Test MSE 29356.359573776477 Test RE 0.2997692912581015\n",
      "134 Train Loss 125005.21 Test MSE 28961.532560426574 Test RE 0.2977466005384002\n",
      "135 Train Loss 124489.91 Test MSE 28646.94172722901 Test RE 0.2961250684779778\n",
      "136 Train Loss 123942.49 Test MSE 28461.063505751234 Test RE 0.29516278809533647\n",
      "137 Train Loss 123046.64 Test MSE 28530.59363893953 Test RE 0.2955231082402909\n",
      "138 Train Loss 122134.98 Test MSE 28724.652781974808 Test RE 0.29652644825468505\n",
      "139 Train Loss 121647.84 Test MSE 28462.890618674264 Test RE 0.2951722622158207\n",
      "140 Train Loss 121338.94 Test MSE 28308.96277114392 Test RE 0.29437303161594786\n",
      "141 Train Loss 120998.98 Test MSE 27957.615869179794 Test RE 0.2925405735915704\n",
      "142 Train Loss 120772.89 Test MSE 28024.364523864282 Test RE 0.29288958490829636\n",
      "143 Train Loss 120157.375 Test MSE 28322.789257578403 Test RE 0.2944449107572624\n",
      "144 Train Loss 119946.55 Test MSE 28306.307161368313 Test RE 0.2943592240059518\n",
      "145 Train Loss 119740.125 Test MSE 28354.354988833264 Test RE 0.294608944402356\n",
      "146 Train Loss 119454.5 Test MSE 27896.26700473376 Test RE 0.29221942874817536\n",
      "147 Train Loss 118970.586 Test MSE 27218.902375357553 Test RE 0.2886498564231403\n",
      "148 Train Loss 118570.49 Test MSE 26986.61420992479 Test RE 0.2874155375170661\n",
      "149 Train Loss 118254.234 Test MSE 26906.941891365666 Test RE 0.28699095686663634\n",
      "150 Train Loss 118009.87 Test MSE 26937.046653339716 Test RE 0.28715146152707205\n",
      "151 Train Loss 117741.95 Test MSE 27051.97275172603 Test RE 0.28776337108010414\n",
      "152 Train Loss 117416.266 Test MSE 27219.323648637714 Test RE 0.28865209016496507\n",
      "153 Train Loss 117240.53 Test MSE 27136.44621720679 Test RE 0.288212311023222\n",
      "154 Train Loss 117153.9 Test MSE 27142.06830142524 Test RE 0.2882421651528956\n",
      "155 Train Loss 117005.39 Test MSE 26924.093064028664 Test RE 0.28708240998187073\n",
      "156 Train Loss 116923.41 Test MSE 26852.246247170886 Test RE 0.28669911506935986\n",
      "157 Train Loss 116648.72 Test MSE 26809.274524735763 Test RE 0.28646962053203523\n",
      "158 Train Loss 116383.61 Test MSE 26895.26401307741 Test RE 0.28692867165420527\n",
      "159 Train Loss 116182.47 Test MSE 26697.888194110623 Test RE 0.28587389362626553\n",
      "160 Train Loss 115897.3 Test MSE 26477.15523202661 Test RE 0.28468966584789773\n",
      "161 Train Loss 115600.24 Test MSE 26356.192500428337 Test RE 0.2840386090932102\n",
      "162 Train Loss 115445.22 Test MSE 26312.935570133035 Test RE 0.28380542511192774\n",
      "163 Train Loss 115337.66 Test MSE 26249.507359368064 Test RE 0.2834631574935379\n",
      "164 Train Loss 114960.58 Test MSE 26025.057710399138 Test RE 0.2822486624093609\n",
      "165 Train Loss 114665.43 Test MSE 25829.795618085485 Test RE 0.28118783423073807\n",
      "166 Train Loss 114345.63 Test MSE 25888.31585489538 Test RE 0.2815061849676198\n",
      "167 Train Loss 114278.336 Test MSE 25812.055615133806 Test RE 0.28109125720263234\n",
      "168 Train Loss 114199.0 Test MSE 25819.172099416708 Test RE 0.2811300035094846\n",
      "169 Train Loss 114147.01 Test MSE 25757.55131698347 Test RE 0.2807943266095866\n",
      "170 Train Loss 114073.78 Test MSE 25729.810900318447 Test RE 0.28064308067205096\n",
      "171 Train Loss 114047.82 Test MSE 25736.637607661887 Test RE 0.28068030871704275\n",
      "172 Train Loss 114045.04 Test MSE 25735.982652627994 Test RE 0.2806767372685597\n",
      "173 Train Loss 114045.04 Test MSE 25735.982652627994 Test RE 0.2806767372685597\n",
      "174 Train Loss 114045.04 Test MSE 25735.982652627994 Test RE 0.2806767372685597\n",
      "175 Train Loss 114045.04 Test MSE 25735.982652627994 Test RE 0.2806767372685597\n",
      "176 Train Loss 114045.04 Test MSE 25735.982652627994 Test RE 0.2806767372685597\n",
      "177 Train Loss 114045.04 Test MSE 25735.982652627994 Test RE 0.2806767372685597\n",
      "178 Train Loss 114045.04 Test MSE 25735.982652627994 Test RE 0.2806767372685597\n",
      "179 Train Loss 114045.04 Test MSE 25735.982652627994 Test RE 0.2806767372685597\n",
      "180 Train Loss 114045.04 Test MSE 25735.982652627994 Test RE 0.2806767372685597\n",
      "181 Train Loss 114045.04 Test MSE 25735.982652627994 Test RE 0.2806767372685597\n",
      "182 Train Loss 114045.04 Test MSE 25735.982652627994 Test RE 0.2806767372685597\n",
      "183 Train Loss 114045.04 Test MSE 25735.982652627994 Test RE 0.2806767372685597\n",
      "184 Train Loss 114045.04 Test MSE 25735.982652627994 Test RE 0.2806767372685597\n",
      "185 Train Loss 114045.04 Test MSE 25735.982652627994 Test RE 0.2806767372685597\n",
      "186 Train Loss 114045.04 Test MSE 25735.982652627994 Test RE 0.2806767372685597\n",
      "187 Train Loss 114045.04 Test MSE 25735.982652627994 Test RE 0.2806767372685597\n",
      "188 Train Loss 114045.04 Test MSE 25735.982652627994 Test RE 0.2806767372685597\n",
      "189 Train Loss 114045.04 Test MSE 25735.982652627994 Test RE 0.2806767372685597\n",
      "190 Train Loss 114045.04 Test MSE 25735.982652627994 Test RE 0.2806767372685597\n",
      "191 Train Loss 114045.04 Test MSE 25735.982652627994 Test RE 0.2806767372685597\n",
      "192 Train Loss 114045.04 Test MSE 25735.982652627994 Test RE 0.2806767372685597\n",
      "193 Train Loss 114045.04 Test MSE 25735.982652627994 Test RE 0.2806767372685597\n",
      "194 Train Loss 114045.04 Test MSE 25735.982652627994 Test RE 0.2806767372685597\n",
      "195 Train Loss 114045.04 Test MSE 25735.982652627994 Test RE 0.2806767372685597\n",
      "196 Train Loss 114045.04 Test MSE 25735.982652627994 Test RE 0.2806767372685597\n",
      "197 Train Loss 114045.04 Test MSE 25735.982652627994 Test RE 0.2806767372685597\n",
      "198 Train Loss 114045.04 Test MSE 25735.982652627994 Test RE 0.2806767372685597\n",
      "199 Train Loss 114045.04 Test MSE 25735.982652627994 Test RE 0.2806767372685597\n",
      "Training time: 572.28\n",
      "Training time: 572.28\n",
      "ES_rowdy_high\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 490696.06 Test MSE 316338.1000875823 Test RE 0.9840378259221307\n",
      "1 Train Loss 404168.38 Test MSE 225795.55438408535 Test RE 0.8313688254035116\n",
      "2 Train Loss 277258.9 Test MSE 101042.25542343031 Test RE 0.5561443861906505\n",
      "3 Train Loss 250051.56 Test MSE 75138.74463465765 Test RE 0.4795876702816762\n",
      "4 Train Loss 250001.31 Test MSE 75081.51056074406 Test RE 0.47940498167471995\n",
      "5 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "6 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "7 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "8 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "9 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "10 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "11 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "12 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "13 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "14 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "15 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "16 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "17 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "18 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "19 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "20 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "21 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "22 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "23 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "24 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "25 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "26 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "27 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "28 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "29 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "30 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "31 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "32 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "33 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "34 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "35 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "36 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "37 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "38 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "39 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "40 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "41 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "42 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "43 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "44 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "45 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "46 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "47 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "48 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "49 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "50 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "51 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "52 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "53 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "54 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "55 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "56 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "57 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "58 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "59 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "60 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "61 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "62 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "63 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "64 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "65 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "66 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "67 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "68 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "69 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "70 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "71 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "72 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "73 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "74 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "75 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "76 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "77 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "78 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "79 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "80 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "81 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "82 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "83 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "84 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "85 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "86 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "87 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "88 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "89 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "90 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "91 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "92 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "93 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "94 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "95 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "96 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "97 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "98 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "99 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "100 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "101 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "102 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "103 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "104 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "105 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "106 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "107 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "108 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "109 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "110 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "111 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "112 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "113 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "114 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "115 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "116 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "117 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "118 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "119 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "120 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "121 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "122 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "123 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "124 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "125 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "126 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "127 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "128 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "129 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "130 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "131 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "132 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "133 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "134 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "135 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "136 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "137 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "138 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "139 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "140 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "141 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "142 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "143 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "144 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "145 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "146 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "147 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "148 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "149 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "150 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "151 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "152 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "153 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "154 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "155 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "156 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "157 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "158 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "159 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "160 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "161 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "162 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "163 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "164 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "165 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "166 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "167 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "168 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "169 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "170 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "171 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "172 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "173 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "174 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "175 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "176 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "177 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "178 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "179 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "180 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "181 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "182 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "183 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "184 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "185 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "186 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "187 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "188 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "189 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "190 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "191 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "192 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "193 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "194 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "195 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "196 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "197 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "198 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "199 Train Loss 250001.2 Test MSE 75081.26297910925 Test RE 0.4794041912539754\n",
      "Training time: 146.00\n",
      "Training time: 146.00\n",
      "ES_rowdy_high\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 484287.7 Test MSE 310371.4620888838 Test RE 0.9747133916090993\n",
      "1 Train Loss 433177.5 Test MSE 258113.8898343705 Test RE 0.8888771246634741\n",
      "2 Train Loss 365550.22 Test MSE 191045.30027034483 Test RE 0.7647231210537588\n",
      "3 Train Loss 263770.62 Test MSE 88740.22745486908 Test RE 0.5211902835129213\n",
      "4 Train Loss 249576.97 Test MSE 74877.46935852196 Test RE 0.47875312400539577\n",
      "5 Train Loss 244015.56 Test MSE 72636.54793577649 Test RE 0.47153468059316517\n",
      "6 Train Loss 233308.3 Test MSE 69008.9651827506 Test RE 0.45960929099448344\n",
      "7 Train Loss 223163.05 Test MSE 66748.69280244794 Test RE 0.45201976411498007\n",
      "8 Train Loss 214663.3 Test MSE 63480.70636714812 Test RE 0.4408155612419345\n",
      "9 Train Loss 204310.47 Test MSE 60740.96338300286 Test RE 0.4311981408967601\n",
      "10 Train Loss 193271.05 Test MSE 55923.6209010617 Test RE 0.4137458811145115\n",
      "11 Train Loss 185673.02 Test MSE 53460.61516270838 Test RE 0.40453212678775685\n",
      "12 Train Loss 179379.4 Test MSE 50320.46712859057 Test RE 0.3924717247391473\n",
      "13 Train Loss 173826.67 Test MSE 47076.71266013881 Test RE 0.3796112776879353\n",
      "14 Train Loss 165301.75 Test MSE 44114.5824650457 Test RE 0.36747443292473\n",
      "15 Train Loss 160058.73 Test MSE 41704.37763365764 Test RE 0.35729493860347394\n",
      "16 Train Loss 154196.47 Test MSE 38928.53966873931 Test RE 0.3451994522999512\n",
      "17 Train Loss 145629.22 Test MSE 35747.476975201986 Test RE 0.3307948498503418\n",
      "18 Train Loss 141289.58 Test MSE 34626.63939253289 Test RE 0.3255676269069221\n",
      "19 Train Loss 137956.56 Test MSE 33851.48779760601 Test RE 0.3219029245438122\n",
      "20 Train Loss 131587.25 Test MSE 31344.91065684483 Test RE 0.30975588056222264\n",
      "21 Train Loss 126909.9 Test MSE 29848.623385245945 Test RE 0.3022721917107462\n",
      "22 Train Loss 122240.5 Test MSE 27938.574827207016 Test RE 0.29244093658882464\n",
      "23 Train Loss 116974.766 Test MSE 25473.461321488357 Test RE 0.27924153854896616\n",
      "24 Train Loss 113412.44 Test MSE 24790.459733129836 Test RE 0.2754725518942506\n",
      "25 Train Loss 109912.86 Test MSE 24292.120734758337 Test RE 0.27268971455656854\n",
      "26 Train Loss 106905.91 Test MSE 22964.847974272143 Test RE 0.265135468413068\n",
      "27 Train Loss 103491.92 Test MSE 21875.681510894578 Test RE 0.2587717350546863\n",
      "28 Train Loss 96980.07 Test MSE 19702.39733060315 Test RE 0.24558146080851478\n",
      "29 Train Loss 93800.97 Test MSE 18090.039359975774 Test RE 0.23531835172834067\n",
      "30 Train Loss 91757.94 Test MSE 17392.068040314036 Test RE 0.2307340317008062\n",
      "31 Train Loss 89633.36 Test MSE 17059.388988147613 Test RE 0.22851661269914603\n",
      "32 Train Loss 87804.14 Test MSE 16715.853192010734 Test RE 0.226204018556049\n",
      "33 Train Loss 85230.62 Test MSE 16032.284338511452 Test RE 0.2215306109169688\n",
      "34 Train Loss 83199.71 Test MSE 15096.35800271382 Test RE 0.2149671679849765\n",
      "35 Train Loss 80922.63 Test MSE 14311.723867991437 Test RE 0.20930616286229187\n",
      "36 Train Loss 79297.64 Test MSE 13969.920447495482 Test RE 0.20679165469688768\n",
      "37 Train Loss 77433.98 Test MSE 13818.58241573999 Test RE 0.20566850366382636\n",
      "38 Train Loss 76374.164 Test MSE 13573.364665907771 Test RE 0.20383548939481264\n",
      "39 Train Loss 74660.34 Test MSE 13164.873756024399 Test RE 0.20074484017588734\n",
      "40 Train Loss 74029.875 Test MSE 13023.179931748646 Test RE 0.19966160702157756\n",
      "41 Train Loss 72788.68 Test MSE 12878.232409622096 Test RE 0.1985473847970717\n",
      "42 Train Loss 71031.78 Test MSE 11687.103152647609 Test RE 0.18914265478710357\n",
      "43 Train Loss 68633.53 Test MSE 11283.459544076288 Test RE 0.18584769535491763\n",
      "44 Train Loss 67369.16 Test MSE 10994.790996307182 Test RE 0.1834549907546355\n",
      "45 Train Loss 65995.055 Test MSE 10609.794602426264 Test RE 0.1802144165052543\n",
      "46 Train Loss 64499.11 Test MSE 10562.991514969806 Test RE 0.1798164863904667\n",
      "47 Train Loss 63169.355 Test MSE 10556.548952653322 Test RE 0.1797616413394347\n",
      "48 Train Loss 62248.95 Test MSE 10248.236770678162 Test RE 0.17711715073575982\n",
      "49 Train Loss 61678.434 Test MSE 9896.542248225238 Test RE 0.1740515051687579\n",
      "50 Train Loss 60887.64 Test MSE 9654.849558994749 Test RE 0.1719130309692036\n",
      "51 Train Loss 60540.13 Test MSE 9448.957664844984 Test RE 0.17007011032284547\n",
      "52 Train Loss 60194.586 Test MSE 9347.281511367686 Test RE 0.16915260992925363\n",
      "53 Train Loss 58741.12 Test MSE 8527.365880968488 Test RE 0.1615635884453916\n",
      "54 Train Loss 57164.562 Test MSE 8363.999673317065 Test RE 0.1600084968675747\n",
      "55 Train Loss 56275.418 Test MSE 8327.506420540936 Test RE 0.15965904600551503\n",
      "56 Train Loss 55538.473 Test MSE 8090.671766014328 Test RE 0.15737231484881664\n",
      "57 Train Loss 54948.34 Test MSE 8148.175301517184 Test RE 0.15793057763173413\n",
      "58 Train Loss 54172.28 Test MSE 7912.270969286362 Test RE 0.15562759920568917\n",
      "59 Train Loss 53058.414 Test MSE 7582.72291602598 Test RE 0.15235216673860472\n",
      "60 Train Loss 52191.035 Test MSE 7374.043447597859 Test RE 0.15024114609949915\n",
      "61 Train Loss 51807.73 Test MSE 7251.371682970177 Test RE 0.14898622802378833\n",
      "62 Train Loss 51495.54 Test MSE 7257.062231667483 Test RE 0.14904467538772745\n",
      "63 Train Loss 50840.125 Test MSE 7140.031902779544 Test RE 0.14783801339229918\n",
      "64 Train Loss 50365.35 Test MSE 7163.670646820136 Test RE 0.14808253731561843\n",
      "65 Train Loss 49995.37 Test MSE 7131.775754375254 Test RE 0.14775251478097032\n",
      "66 Train Loss 49613.734 Test MSE 7234.2570604002185 Test RE 0.14881030617895674\n",
      "67 Train Loss 48760.633 Test MSE 7127.998970655684 Test RE 0.14771338685475574\n",
      "68 Train Loss 48403.734 Test MSE 7087.731142352712 Test RE 0.1472955611995463\n",
      "69 Train Loss 48080.28 Test MSE 7009.856282404237 Test RE 0.14648413770031746\n",
      "70 Train Loss 47815.336 Test MSE 6937.976703041271 Test RE 0.14573117295318389\n",
      "71 Train Loss 47612.3 Test MSE 6848.949934158449 Test RE 0.14479315708171978\n",
      "72 Train Loss 47227.094 Test MSE 6773.024749624196 Test RE 0.14398835599849155\n",
      "73 Train Loss 46979.504 Test MSE 6713.131051538879 Test RE 0.14335029946878072\n",
      "74 Train Loss 46784.406 Test MSE 6733.690697671924 Test RE 0.14356964408662298\n",
      "75 Train Loss 46369.887 Test MSE 6698.561284490419 Test RE 0.1431946556330925\n",
      "76 Train Loss 45916.53 Test MSE 6651.9502029404275 Test RE 0.14269558509389946\n",
      "77 Train Loss 45788.133 Test MSE 6652.462635167537 Test RE 0.14270108125713424\n",
      "78 Train Loss 45725.23 Test MSE 6624.165909273172 Test RE 0.14239726320420384\n",
      "79 Train Loss 45309.117 Test MSE 6629.070158820962 Test RE 0.14244996587753803\n",
      "80 Train Loss 45088.54 Test MSE 6572.489967313291 Test RE 0.1418407463362853\n",
      "81 Train Loss 44905.08 Test MSE 6556.269949229349 Test RE 0.1416656163120854\n",
      "82 Train Loss 44665.004 Test MSE 6553.062332218344 Test RE 0.14163095753223626\n",
      "83 Train Loss 44387.22 Test MSE 6639.621557063912 Test RE 0.14256328861094852\n",
      "84 Train Loss 44261.375 Test MSE 6665.698389513673 Test RE 0.1428429699098707\n",
      "85 Train Loss 44184.59 Test MSE 6664.322539404929 Test RE 0.1428282272192636\n",
      "86 Train Loss 44088.266 Test MSE 6645.105608789848 Test RE 0.14262215213729915\n",
      "87 Train Loss 43856.867 Test MSE 6734.320655968798 Test RE 0.14357635962920776\n",
      "88 Train Loss 43436.023 Test MSE 6450.856506839495 Test RE 0.1405221327541659\n",
      "89 Train Loss 43316.473 Test MSE 6466.561559750804 Test RE 0.14069308415754975\n",
      "90 Train Loss 43254.934 Test MSE 6422.741463142237 Test RE 0.14021557656083317\n",
      "91 Train Loss 43152.516 Test MSE 6395.759734580537 Test RE 0.13992074604700808\n",
      "92 Train Loss 43087.688 Test MSE 6365.017873906497 Test RE 0.13958406944209206\n",
      "93 Train Loss 43025.31 Test MSE 6425.186967017872 Test RE 0.1402422680526132\n",
      "94 Train Loss 42970.082 Test MSE 6440.855779983976 Test RE 0.14041316516558067\n",
      "95 Train Loss 42918.785 Test MSE 6409.734673756558 Test RE 0.14007352828226063\n",
      "96 Train Loss 42792.47 Test MSE 6522.7196082514565 Test RE 0.14130267931376758\n",
      "97 Train Loss 42574.62 Test MSE 6610.824140794097 Test RE 0.1422537893749343\n",
      "98 Train Loss 42482.25 Test MSE 6524.600841233111 Test RE 0.1413230545646422\n",
      "99 Train Loss 42377.637 Test MSE 6509.149922981667 Test RE 0.14115562169949578\n",
      "100 Train Loss 42340.46 Test MSE 6506.355074295417 Test RE 0.14112531428874375\n",
      "101 Train Loss 42239.785 Test MSE 6450.721283951114 Test RE 0.1405206599336007\n",
      "102 Train Loss 42089.137 Test MSE 6453.192955890471 Test RE 0.14054757845116517\n",
      "103 Train Loss 41998.773 Test MSE 6453.008491346376 Test RE 0.14054556966028936\n",
      "104 Train Loss 41914.355 Test MSE 6418.321085911763 Test RE 0.14016731738476446\n",
      "105 Train Loss 41848.805 Test MSE 6446.8375225530535 Test RE 0.14047835219314747\n",
      "106 Train Loss 41720.89 Test MSE 6434.0286392297 Test RE 0.14033872827100943\n",
      "107 Train Loss 41657.906 Test MSE 6432.303317368365 Test RE 0.14031991069000882\n",
      "108 Train Loss 41639.91 Test MSE 6389.832061743967 Test RE 0.1398558908378669\n",
      "109 Train Loss 41631.523 Test MSE 6383.943781015583 Test RE 0.1397914368315495\n",
      "110 Train Loss 41616.34 Test MSE 6347.104105702015 Test RE 0.13938750766204897\n",
      "111 Train Loss 41605.074 Test MSE 6373.867416620666 Test RE 0.13968107043651726\n",
      "112 Train Loss 41574.234 Test MSE 6337.7877529419775 Test RE 0.13928517279885916\n",
      "113 Train Loss 41498.11 Test MSE 6358.843416845646 Test RE 0.13951635046824937\n",
      "114 Train Loss 41420.105 Test MSE 6291.633994836968 Test RE 0.13877708694269728\n",
      "115 Train Loss 41358.516 Test MSE 6266.644275804968 Test RE 0.13850120861727316\n",
      "116 Train Loss 41305.598 Test MSE 6274.6642973155185 Test RE 0.13858980687314312\n",
      "117 Train Loss 41268.035 Test MSE 6272.281564761303 Test RE 0.13856349042142707\n",
      "118 Train Loss 41247.633 Test MSE 6259.923790709543 Test RE 0.13842692284705865\n",
      "119 Train Loss 41234.848 Test MSE 6244.314020715581 Test RE 0.13825422416624622\n",
      "120 Train Loss 41193.02 Test MSE 6210.64620845528 Test RE 0.13788100394591526\n",
      "121 Train Loss 41174.242 Test MSE 6207.467489236062 Test RE 0.13784571445450275\n",
      "122 Train Loss 41143.08 Test MSE 6189.703721595762 Test RE 0.13764833818349384\n",
      "123 Train Loss 41094.8 Test MSE 6168.715718425656 Test RE 0.13741477152033676\n",
      "124 Train Loss 41050.04 Test MSE 6204.349383226584 Test RE 0.13781108909875886\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 200 #200\n",
    "\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "n_val = 5.0\n",
    "rowdy_terms = 6\n",
    "\n",
    "N_T = 5000 #Total number of data points for 'y'\n",
    "N_f = 10000 #Total number of collocation points \n",
    "\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(label)\n",
    "    print(reps)\n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []\n",
    "\n",
    "    alpha_val = []    \n",
    "    omega_val = []\n",
    "\n",
    "    torch.manual_seed(reps*36)\n",
    "\n",
    "    layers = np.array([2,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "\n",
    "    PINN = Sequentialmodel(layers,n_val,rowdy_terms)\n",
    "\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.05, \n",
    "                              max_iter = 20, \n",
    "                              max_eval = 30, \n",
    "                              tolerance_grad = 1e-08, \n",
    "                              tolerance_change = 1e-08, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    nan_flag = train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    #elapsed_time[reps] = time.time() - start_time\n",
    "    alpha_full.append(alpha_val)\n",
    "    omega_full.append(omega_val)\n",
    "\n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time,\"alpha\": alpha_full,\"omega\": omega_full,  \"label\": label}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred = PINN.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "cmap = plt.cm.jet\n",
    "\n",
    "img3 = ax.imshow(np.transpose(np.flipud(np.transpose(u_pred.reshape(500,500)))),vmin = 0,vmax = 1000,cmap = cmap,extent=[0,1,0,1],aspect = 0.75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "\n",
    "for tune_reps in range(75):\n",
    "    label = \"MW_rowdy_tune\"+str(tune_reps)+\".mat\"\n",
    "    data = sio.loadmat(label)\n",
    "    re = np.array(data[\"test_re_loss\"])\n",
    "    print(tune_reps,\" \",np.mean(re[:,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrnr_tune[31]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
