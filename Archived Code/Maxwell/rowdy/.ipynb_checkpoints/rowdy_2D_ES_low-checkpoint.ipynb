{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def true_2D_1(xt): #True function for 2D_1 Klein Gordon Equation x \\in [-50,50] , t \\in [0,10]\n",
    "#     y = xt[:,0]*np.cos(xt[:,1])\n",
    "#     return y.reshape(-1,1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_thresh = 25000\n",
    "level = \"_low\"\n",
    "label = \"ES_rowdy\" + level\n",
    "ES_val = 10.0\n",
    "\n",
    "\n",
    "x = np.linspace(0,1,500).reshape(-1,1)\n",
    "y = np.linspace(0,1,500).reshape(-1,1)\n",
    "\n",
    "X,Y = np.meshgrid(x,y)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "Y = Y.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xy = np.hstack((X,Y))\n",
    "\n",
    "# bound_pts_1 = (X == 0).reshape(-1,)\n",
    "# bound_pts_2 = np.logical_and(Y == 0,X != 0).reshape(-1,)\n",
    "# bound_pts_3 = np.logical_and(X == 1,Y != 0).reshape(-1,) \n",
    "# bound_pts_4 = np.logical_and(Y == 1,X != 1).reshape(-1,) \n",
    "\n",
    "# xy_bound_1 = xy[bound_pts_1,:]\n",
    "# xy_bound_2 = xy[bound_pts_2,:]\n",
    "# xy_bound_3 = xy[bound_pts_3,:]\n",
    "# xy_bound_4 = xy[bound_pts_4,:]\n",
    "\n",
    "# u_bound_1 = 1000*np.ones((np.shape(xy_bound_1)[0],1))\n",
    "# u_bound_2 = 800*np.ones((np.shape(xy_bound_2)[0],1))\n",
    "# u_bound_3 = 500*np.ones((np.shape(xy_bound_3)[0],1))\n",
    "# u_bound_4 = np.zeros((np.shape(xy_bound_4)[0],1))\n",
    "\n",
    "# xy_bound = np.vstack((xy_bound_1,xy_bound_2,xy_bound_3,xy_bound_4))\n",
    "# u_bound = np.vstack((u_bound_1,u_bound_2,u_bound_3,u_bound_4))\n",
    "\n",
    "xy_test_tensor = torch.from_numpy(xy).float().to(device)\n",
    "\n",
    "lb_xy = xy[0]\n",
    "ub_xy = xy[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "fea_data = scipy.io.loadmat('./../ES_FEA' + level + '.mat')\n",
    "\n",
    "xy = np.array(fea_data['xy'])\n",
    "u_true = np.array(fea_data['u'])\n",
    "\n",
    "xy_test_tensor = torch.from_numpy(xy).float().to(device)\n",
    "u_true_norm = np.linalg.norm(u_true,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_T,N_f,seed):\n",
    "    '''Boundary Conditions''' \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    N_t = int(N_T/4)\n",
    "    \n",
    "    x_BC1 = np.random.uniform(size = N_t).reshape(-1,1)\n",
    "    y_BC1 = np.zeros((N_t,1))\n",
    "    u_BC1 = 0*np.ones((N_t,1))\n",
    "    \n",
    "    x_BC2 = np.ones((N_t,1))\n",
    "    y_BC2 = np.random.uniform(size = N_t).reshape(-1,1) \n",
    "    u_BC2 = ES_val*np.ones((N_t,1))\n",
    "    \n",
    "    x_BC3 = np.random.uniform(size = N_t).reshape(-1,1)\n",
    "    y_BC3 = np.ones((N_t,1)) \n",
    "    u_BC3 = 0*np.ones((N_t,1))\n",
    "    \n",
    "    x_BC4 = np.zeros((N_t,1))\n",
    "    y_BC4 = np.random.uniform(size = N_t).reshape(-1,1) \n",
    "    u_BC4 = ES_val*np.ones((N_t,1))\n",
    "    \n",
    "    XY_corners = np.array([[0,0],[1,0],[0,1],[1,1]]).reshape(-1,2)\n",
    "    U_corners = ES_val*np.ones((4,1))\n",
    "    \n",
    "    XY_1 = np.hstack((x_BC1,y_BC1))\n",
    "    XY_2 = np.hstack((x_BC2,y_BC2))\n",
    "    XY_3 = np.hstack((x_BC3,y_BC3))\n",
    "    XY_4 = np.hstack((x_BC4,y_BC4))\n",
    "    \n",
    "    xy_BC = np.vstack((XY_1,XY_2,XY_3,XY_4,XY_corners)) #choose indices from  set 'idx' (x,t)\n",
    "    u_BC = np.vstack((u_BC1,u_BC2,u_BC3,u_BC4,U_corners))\n",
    "    \n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    samples = sampling(N_f)\n",
    "    \n",
    "    xy_coll = lb_xy + (ub_xy - lb_xy)*samples\n",
    "    \n",
    "    xy_coll = np.vstack((xy_coll, xy_BC)) # append training points to collocation points \n",
    "\n",
    "    return xy_coll, xy_BC, u_BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,n_val,rowdy_terms):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        self.iter = 0\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)   \n",
    "\n",
    "        \n",
    "        self.omega1 = Parameter(torch.ones((len(layers)-2,1))) \n",
    "        self.omega1.requiresGrad = True\n",
    "        \n",
    "        \n",
    "        self.alpha = Parameter(torch.zeros(rowdy_terms,len(layers)-2))\n",
    "        self.alpha.requiresGrad = True\n",
    "        \n",
    "        \n",
    "        self.omega = Parameter((1/n_val)*torch.ones(rowdy_terms,len(layers)-2))\n",
    "        self.omega.requiresGrad = True\n",
    "        \n",
    "        self.n = torch.tensor(n_val)\n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xy):\n",
    "        if torch.is_tensor(xy) != True:         \n",
    "            xy = torch.from_numpy(xy)                \n",
    "        \n",
    "        ubxy = torch.from_numpy(ub_xy).float().to(device)\n",
    "        lbxy = torch.from_numpy(lb_xy).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xy = 2.0*(xy - lbxy)/(ubxy - lbxy) - 1.0\n",
    "        \n",
    "        #convert to float\n",
    "        a = xy.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(self.omega1[i,0]*z)\n",
    "            for j in range(rowdy_terms):\n",
    "                a = a + self.alpha[j,i]*self.n*torch.sin((j+1)*self.n*self.omega[j,i]*z)\n",
    "                \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC(self,xy,u):\n",
    "                \n",
    "        loss_bc = self.loss_function(self.forward(xy), u)\n",
    "                \n",
    "        return loss_bc\n",
    "    \n",
    "    def loss_PDE(self, xy_coll, f_hat):\n",
    "        \n",
    "        g = xy_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_y = autograd.grad(u,g,torch.ones([xy_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_xx_yy = autograd.grad(u_x_y,g,torch.ones(xy_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        #du_dx = u_x_t[:,[0]]\n",
    "        \n",
    "        d2u_dx2 = u_xx_yy[:,[0]]\n",
    "        d2u_dy2 = u_xx_yy[:,[1]]    \n",
    "        \n",
    "\n",
    "        f = d2u_dx2 + d2u_dy2\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xy_BC,u_BC,xy_coll,f_hat):\n",
    "\n",
    "        loss_BC = self.loss_BC(xy_BC,u_BC)\n",
    "        loss_f = self.loss_PDE(xy_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_BC + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "     \n",
    "    'callable for optimizer'  \n",
    "    def closure(self):\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss = self.loss(xy_BC, u_BC, xy_coll,f_hat)\n",
    "        self.train_loss.append(loss.cpu().detach().numpy())\n",
    "        \n",
    "        u_pred = self.test(xy_test_tensor)\n",
    "        #self.test_loss.append(np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))) #Commented because no true values yet\n",
    "        self.beta_val.append(self.beta.cpu().detach().numpy())\n",
    "        \n",
    "        #print(self.iter,\"Train Loss\",self.train_loss[-1],\"Test Loss\",self.test_loss[-1])\n",
    "        print(self.iter,\"Train Loss\",self.train_loss[-1])\n",
    "        \n",
    "        loss.backward()\n",
    "        \n",
    "        self.iter += 1\n",
    "  \n",
    "\n",
    "        return loss        \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        u_pred = self.forward(xy_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "     \n",
    "        \n",
    "        return test_mse, test_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    alpha_val.append(PINN.alpha.cpu().detach().numpy())\n",
    "    omega_val.append(PINN.omega.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(xy_BC,u_BC,xy_coll,f_hat,seed):\n",
    "\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(xy_BC,u_BC,xy_coll,f_hat)\n",
    "        loss.backward()\n",
    "        #print(loss.cpu().detach().numpy())\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*9)\n",
    "    start_time = time.time() \n",
    "    thresh_flag = 0\n",
    "\n",
    "    xy_coll_np_array, xy_BC_np_array, u_BC_np_array = trainingdata(N_T,N_f,rep*22)\n",
    "        \n",
    "    xy_coll = torch.from_numpy(xy_coll_np_array).float().to(device)\n",
    "    xy_BC = torch.from_numpy(xy_BC_np_array).float().to(device)\n",
    "    u_BC = torch.from_numpy(u_BC_np_array).float().to(device)\n",
    "        \n",
    "    f_hat = torch.zeros(xy_coll.shape[0],1).to(device)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        train_step(xy_BC,u_BC,xy_coll,f_hat,i)\n",
    "        loss_np = PINN.loss(xy_BC,u_BC,xy_coll,f_hat).cpu().detach().numpy()\n",
    "        \n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1          \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        \n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])   \n",
    "        \n",
    "\n",
    "            \n",
    "    elapsed_time[rep] = time.time() - start_time  \n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ES_rowdy_low\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 25.169355 Test MSE 7.659955987868133 Test RE 0.4844384173519763\n",
      "1 Train Loss 25.000095 Test MSE 7.5081466675066 Test RE 0.4796139576175841\n",
      "2 Train Loss 25.000078 Test MSE 7.508115446980206 Test RE 0.4796129604463478\n",
      "3 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "4 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "5 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "6 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "7 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "8 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "9 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "10 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "11 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "12 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "13 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "14 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "15 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "16 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "17 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "18 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "19 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "20 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "21 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "22 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "23 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "24 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "25 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "26 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "27 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "28 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "29 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "30 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "31 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "32 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "33 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "34 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "35 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "36 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "37 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "38 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "39 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "40 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "41 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "42 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "43 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "44 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "45 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "46 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "47 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "48 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "49 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "50 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "51 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "52 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "53 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "54 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "55 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "56 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "57 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "58 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "59 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "60 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "61 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "62 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "63 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "64 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "65 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "66 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "67 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "68 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "69 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "70 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "71 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "72 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "73 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "74 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "75 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "76 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "77 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "78 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "79 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "80 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "81 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "82 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "83 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "84 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "85 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "86 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "87 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "88 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "89 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "90 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "91 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "92 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "93 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "94 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "95 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "96 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "97 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "98 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "99 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "100 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "101 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "102 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "103 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "104 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "105 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "106 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "107 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "108 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "109 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "110 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "111 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "112 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "113 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "114 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "115 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "116 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "117 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "118 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "119 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "120 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "121 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "122 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "123 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "124 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "125 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "126 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "127 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "128 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "129 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "130 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "131 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "132 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "133 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "134 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "135 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "136 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "137 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "138 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "139 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "140 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "141 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "142 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "143 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "144 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "145 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "146 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "147 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "148 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "149 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "150 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "151 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "152 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "153 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "154 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "155 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "156 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "157 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "158 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "159 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "160 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "161 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "162 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "163 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "164 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "165 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "166 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "167 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "168 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "169 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "170 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "171 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "172 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "173 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "174 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "175 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "176 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "177 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "178 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "179 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "180 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "181 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "182 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "183 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "184 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "185 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "186 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "187 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "188 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "189 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "190 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "191 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "192 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "193 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "194 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "195 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "196 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "197 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "198 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "199 Train Loss 25.000074 Test MSE 7.508112716287236 Test RE 0.47961287322899837\n",
      "Training time: 136.23\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'alpha_full' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_11848/1914721770.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0mtest_re_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_re_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;31m#elapsed_time[reps] = time.time() - start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     \u001b[0malpha_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0malpha_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m     \u001b[0momega_full\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0momega_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'alpha_full' is not defined"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 200 #200\n",
    "\n",
    "\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "alpha_full = []\n",
    "omega_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "n_val = 5.0\n",
    "rowdy_terms = 6\n",
    "\n",
    "N_T = 5000 #Total number of data points for 'y'\n",
    "N_f = 10000 #Total number of collocation points \n",
    "\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(label)\n",
    "    print(reps)\n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []\n",
    "\n",
    "    alpha_val = []    \n",
    "    omega_val = []\n",
    "\n",
    "    torch.manual_seed(reps*36)\n",
    "\n",
    "    layers = np.array([2,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "\n",
    "    PINN = Sequentialmodel(layers,n_val,rowdy_terms)\n",
    "\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.05, \n",
    "                              max_iter = 20, \n",
    "                              max_eval = 30, \n",
    "                              tolerance_grad = 1e-08, \n",
    "                              tolerance_change = 1e-08, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    nan_flag = train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    #elapsed_time[reps] = time.time() - start_time\n",
    "    alpha_full.append(alpha_val)\n",
    "    omega_full.append(omega_val)\n",
    "\n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time,\"alpha\": alpha_full,\"omega\": omega_full,  \"label\": label}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred = PINN.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "cmap = plt.cm.jet\n",
    "\n",
    "img3 = ax.imshow(np.transpose(np.flipud(np.transpose(u_pred.reshape(500,500)))),vmin = 0,vmax = 1000,cmap = cmap,extent=[0,1,0,1],aspect = 0.75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.io as sio\n",
    "import numpy as np\n",
    "\n",
    "for tune_reps in range(75):\n",
    "    label = \"MW_rowdy_tune\"+str(tune_reps)+\".mat\"\n",
    "    data = sio.loadmat(label)\n",
    "    re = np.array(data[\"test_re_loss\"])\n",
    "    print(tune_reps,\" \",np.mean(re[:,-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lrnr_tune[31]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
