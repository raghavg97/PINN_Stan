{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def true_2D_1(xt): #True function for 2D_1 Klein Gordon Equation x \\in [-50,50] , t \\in [0,10]\n",
    "#     y = xt[:,0]*np.cos(xt[:,1])\n",
    "#     return y.reshape(-1,1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Material Properties This link - https://www.mathworks.com/help/pde/ug/nonlinear-heat-transfer-in-a-thin-plate.html#heatTransferThinPlateExample-1\n",
    "k = 400\n",
    "rho = 8960\n",
    "cp = 386\n",
    "t_z = 0.01\n",
    "stef_bolt = 5.670373e-8\n",
    "hc = 1\n",
    "Ta = 300\n",
    "emiss = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"Navier_stan\"\n",
    "\n",
    "x = np.linspace(0,1,100).reshape(-1,1)\n",
    "y = np.linspace(0,1,100).reshape(-1,1)\n",
    "t = np.linspace(0,1,100).reshape(-1,1) #t is actually from 0 to 5000, let us scale it to 0 to 1\n",
    "\n",
    "X,Y,T = np.meshgrid(x,y,t)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "Y = Y.flatten('F').reshape(-1,1)\n",
    "T = T.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xyt = np.hstack((X,Y,T))\n",
    "\n",
    "initial_pts = (T==0).reshape(-1,)\n",
    "\n",
    "DBC_pts = np.logical_and(Y == 0,T != 0).reshape(-1,)\n",
    "\n",
    "\n",
    "NBC_pts_1 = (X == 0).reshape(-1,)\n",
    "#NBC_pts_2 = (Y == 0).reshape(-1,)\n",
    "NBC_pts_3 = (X == 1).reshape(-1,)\n",
    "NBC_pts_4 = (Y == 1).reshape(-1,)\n",
    "\n",
    "xyt_initial = xyt[initial_pts,:]\n",
    "xyt_DBC = xyt[DBC_pts,:]\n",
    "\n",
    "xyt_NBC_1 = xyt[NBC_pts_1,:]\n",
    "#xyt_NBC_2 = xyt[NBC_pts_2,:]\n",
    "xyt_NBC_3 = xyt[NBC_pts_3,:]\n",
    "xyt_NBC_4 = xyt[NBC_pts_4,:]\n",
    "\n",
    "u_initial = np.zeros((np.shape(xyt_initial)[0],1))\n",
    "u_DBC = 100*np.ones((np.shape(xyt_DBC)[0],1))\n",
    "\n",
    "xyt_I_DBC = np.vstack((xyt_initial,xyt_DBC))\n",
    "#xyt_NBC = np.vstack((xyt_NBC_1,xyt_NBC_2,xyt_NBC_3,xyt_NBC_4))\n",
    "xyt_NBC = np.vstack((xyt_NBC_1,xyt_NBC_3,xyt_NBC_4))\n",
    "\n",
    "u_I_DBC = np.vstack((u_initial,u_DBC))\n",
    "\n",
    "xyt_test_tensor = torch.from_numpy(xyt).float().to(device)\n",
    "\n",
    "lb_xyt = xyt[0]\n",
    "ub_xyt = xyt[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_D,N_N,N_f,seed):\n",
    "    '''Boundary Conditions''' \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    #choose random N_u points for training\n",
    "    idx = np.random.choice(xyt_I_DBC.shape[0], N_D, replace=False) \n",
    "    xyt_D = xyt_I_DBC[idx,:] #choose indices from  set 'idx' (x,t)\n",
    "    u_D = u_I_DBC[idx].reshape(-1,1)      #choose corresponding u\n",
    "\n",
    "    idx = np.random.choice(xyt_NBC.shape[0], N_D, replace=False) \n",
    "    xyt_N = xyt_NBC[idx,:] #choose indices from  set 'idx' (x,t)\n",
    "\n",
    "\n",
    "    '''Collocation Points'''\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    samples = sampling(N_f)\n",
    "    \n",
    "    xyt_coll = lb_xyt + (ub_xyt - lb_xyt)*samples\n",
    "    xyt_coll = np.vstack((xyt_coll, xyt_D,xyt_N)) # append training points to collocation points \n",
    "\n",
    "    return xyt_coll, xyt_D, u_D, xyt_N"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        self.iter = 0\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)   \n",
    "        \n",
    "        self.beta = Parameter(torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "        \n",
    "        self.train_loss = []\n",
    "        self.test_loss = []\n",
    "        \n",
    "        self.beta_val = []\n",
    "        \n",
    "        self.n = torch.tensor(1.0)\n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xyt):\n",
    "        if torch.is_tensor(xyt) != True:         \n",
    "            xyt = torch.from_numpy(xyt)                \n",
    "        \n",
    "        ubxyt = torch.from_numpy(ub_xyt).float().to(device)\n",
    "        lbxyt = torch.from_numpy(lb_xyt).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xyt = (xyt - lbxyt)/(ubxyt - lbxyt)\n",
    "        \n",
    "        #convert to float\n",
    "        a = xyt.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            z1 = self.activation(z) \n",
    "            a = z1 + self.beta[:,i]*z*z1\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_D(self,xyt_D,u_D):\n",
    "                \n",
    "        loss_bc = self.loss_function(self.forward(xyt_D), u_D)\n",
    "                \n",
    "        return loss_bc\n",
    "    \n",
    "    def loss_N(self,xyt_N,N_hat):\n",
    "        \n",
    "        g = xyt_N.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g)\n",
    "        \n",
    "        u_x_y_t = autograd.grad(u,g,torch.ones([xyt_N.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        du_dx = u_x_y_t[:,[0]]\n",
    "        #du_dy = u_x_y_t[:,[1]]\n",
    "        \n",
    "        loss_N1 = self.loss_function(du_dx,N_hat)\n",
    "        #loss_N2 = self.loss_function(du_dy,N_hat)\n",
    "        \n",
    "        #return loss_N1+loss_N2       \n",
    "        return loss_N1\n",
    "    \n",
    "    def loss_PDE(self, xyt_coll, f_hat):\n",
    "        \n",
    "        g = xyt_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_y_t = autograd.grad(u,g,torch.ones([xyt_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_xx_yy_tt = autograd.grad(u_x_y_t,g,torch.ones(xyt_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        du_dt = u_x_y_t[:,[2]]\n",
    "        \n",
    "        d2u_dx2 = u_xx_yy_tt[:,[0]]\n",
    "        d2u_dy2 = u_xx_yy_tt[:,[1]]    \n",
    "        \n",
    "\n",
    "        f = rho*cp*t_z*du_dt/5000 - k*t_z*(d2u_dx2+d2u_dy2) + 2*hc*(u-Ta) + 2*emiss*stef_bolt*(torch.pow(u,4)-Ta**4) \n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xyt_D,u_D,xyt_N,N_hat,xyt_coll,f_hat):\n",
    "\n",
    "        loss_D = self.loss_D(xyt_D,u_D)\n",
    "        loss_N = self.loss_N(xyt_N,N_hat)\n",
    "        loss_f = self.loss_PDE(xyt_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_D + loss_N + loss_f\n",
    "        \n",
    "        print(self.iter,\"loss_D:\",loss_D.cpu().detach().numpy(),\"loss_N:\",loss_N.cpu().detach().numpy(),\"loss_f:\",loss_f.cpu().detach().numpy())\n",
    "        \n",
    "        return loss_val\n",
    "     \n",
    "    'callable for optimizer'                                       \n",
    "    def closure(self):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = self.loss(xyt_D,u_D,xyt_N,N_hat,xyt_coll,f_hat)\n",
    "        self.train_loss.append(loss.cpu().detach().numpy())\n",
    "        \n",
    "        u_pred = self.test(xyt_test_tensor)\n",
    "        #self.test_loss.append(np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))) #Commented because no true values yet\n",
    "        self.beta_val.append(self.beta.cpu().detach().numpy())\n",
    "        \n",
    "        #print(self.iter,\"Train Loss\",self.train_loss[-1],\"Test Loss\",self.test_loss[-1])\n",
    "        print(self.iter,\"Train Loss\",self.train_loss[-1])\n",
    "        \n",
    "        loss.backward()\n",
    "        self.iter += 1\n",
    "  \n",
    "\n",
    "        return loss        \n",
    "    'test neural network'\n",
    "    def test(self,xyt_test_tensor):\n",
    "        u_pred = self.forward(xyt_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0 loss_D: 5011.9497 loss_N: 0.9682867 loss_f: 1126743.5\n",
      "0 Train Loss 1131756.4\n",
      "1 loss_D: 5006.3994 loss_N: 0.95781577 loss_f: 1124402.5\n",
      "1 Train Loss 1129409.9\n",
      "2 loss_D: 4954.4526 loss_N: 0.79594314 loss_f: 1103841.6\n",
      "2 Train Loss 1108796.9\n",
      "3 loss_D: 4749.934 loss_N: 0.43729374 loss_f: 1092676.4\n",
      "3 Train Loss 1097426.8\n",
      "4 loss_D: 4136.3774 loss_N: 0.2663266 loss_f: 1003793.2\n",
      "4 Train Loss 1007929.8\n",
      "5 loss_D: 3233.5872 loss_N: 1.9627023 loss_f: 855635.25\n",
      "5 Train Loss 858870.8\n",
      "6 loss_D: 1939.6372 loss_N: 7.105599 loss_f: 465109.8\n",
      "6 Train Loss 467056.56\n",
      "7 loss_D: 434882.75 loss_N: 13102.304 loss_f: 3239560200.0\n",
      "7 Train Loss 3240008200.0\n",
      "8 loss_D: 51014.55 loss_N: 850.2807 loss_f: 14540755.0\n",
      "8 Train Loss 14592620.0\n",
      "9 loss_D: 6609.688 loss_N: 87.09098 loss_f: 453878.06\n",
      "9 Train Loss 460574.84\n",
      "10 loss_D: 2769.1672 loss_N: 31.297762 loss_f: 176275.19\n",
      "10 Train Loss 179075.66\n",
      "11 loss_D: 40495.477 loss_N: 758.8887 loss_f: 8800972.0\n",
      "11 Train Loss 8842226.0\n",
      "12 loss_D: 8185.4053 loss_N: 110.336395 loss_f: 438088.5\n",
      "12 Train Loss 446384.25\n",
      "13 loss_D: 3818.6968 loss_N: 47.6764 loss_f: 140665.0\n",
      "13 Train Loss 144531.38\n",
      "14 loss_D: 1766.9303 loss_N: 14.7210245 loss_f: 151192.72\n",
      "14 Train Loss 152974.38\n",
      "15 loss_D: 2650.1643 loss_N: 33.15124 loss_f: 75471.195\n",
      "15 Train Loss 78154.51\n",
      "16 loss_D: 3348.4177 loss_N: 45.107986 loss_f: 47954.82\n",
      "16 Train Loss 51348.348\n",
      "17 loss_D: 3822.0566 loss_N: 42.905506 loss_f: 33329.12\n",
      "17 Train Loss 37194.082\n",
      "18 loss_D: 3601.8103 loss_N: 35.86769 loss_f: 18030.605\n",
      "18 Train Loss 21668.283\n",
      "19 loss_D: 3201.7236 loss_N: 21.18296 loss_f: 10046.888\n",
      "19 Train Loss 13269.794\n",
      "20 loss_D: 3351.102 loss_N: 19.705557 loss_f: 5311.3687\n",
      "20 Train Loss 8682.176\n",
      "21 loss_D: 3279.8647 loss_N: 17.601002 loss_f: 4426.5166\n",
      "21 Train Loss 7723.9824\n",
      "22 loss_D: 3309.6096 loss_N: 14.7667465 loss_f: 2771.8066\n",
      "22 Train Loss 6096.183\n",
      "23 loss_D: 3285.3916 loss_N: 12.806267 loss_f: 1581.7288\n",
      "23 Train Loss 4879.927\n",
      "24 loss_D: 3190.053 loss_N: 12.328567 loss_f: 1365.1857\n",
      "24 Train Loss 4567.5674\n",
      "25 loss_D: 3252.68 loss_N: 12.816407 loss_f: 1024.0892\n",
      "25 Train Loss 4289.5854\n",
      "26 loss_D: 3257.6992 loss_N: 12.744597 loss_f: 905.89856\n",
      "26 Train Loss 4176.3423\n",
      "27 loss_D: 3230.1343 loss_N: 12.500963 loss_f: 799.5653\n",
      "27 Train Loss 4042.2007\n",
      "28 loss_D: 3148.6067 loss_N: 11.886993 loss_f: 685.5167\n",
      "28 Train Loss 3846.0103\n",
      "29 loss_D: 2952.6287 loss_N: 10.978922 loss_f: 525.2232\n",
      "29 Train Loss 3488.8308\n",
      "30 loss_D: 2507.3752 loss_N: 10.564888 loss_f: 755.0985\n",
      "30 Train Loss 3273.0386\n",
      "31 loss_D: 2530.4827 loss_N: 11.093611 loss_f: 533.1541\n",
      "31 Train Loss 3074.7302\n",
      "32 loss_D: 2489.9395 loss_N: 11.521632 loss_f: 508.92007\n",
      "32 Train Loss 3010.3813\n",
      "33 loss_D: 2455.89 loss_N: 11.7451935 loss_f: 497.20963\n",
      "33 Train Loss 2964.8447\n",
      "34 loss_D: 2342.846 loss_N: 12.354505 loss_f: 493.4617\n",
      "34 Train Loss 2848.662\n",
      "35 loss_D: 2149.6511 loss_N: 13.234278 loss_f: 505.2433\n",
      "35 Train Loss 2668.129\n",
      "36 loss_D: 1937.7028 loss_N: 14.761251 loss_f: 447.1963\n",
      "36 Train Loss 2399.6602\n",
      "37 loss_D: 1823.4553 loss_N: 15.896903 loss_f: 374.3141\n",
      "37 Train Loss 2213.6663\n",
      "38 loss_D: 1576.8451 loss_N: 20.687975 loss_f: 412.78583\n",
      "38 Train Loss 2010.3188\n",
      "39 loss_D: 1394.8557 loss_N: 29.593449 loss_f: 534.5003\n",
      "39 Train Loss 1958.9495\n",
      "40 loss_D: 1335.0403 loss_N: 31.999825 loss_f: 415.0135\n",
      "40 Train Loss 1782.0537\n",
      "41 loss_D: 1327.1748 loss_N: 32.789444 loss_f: 367.01114\n",
      "41 Train Loss 1726.9753\n",
      "42 loss_D: 1240.4622 loss_N: 40.307205 loss_f: 385.2536\n",
      "42 Train Loss 1666.023\n",
      "43 loss_D: 1193.7042 loss_N: 43.867874 loss_f: 293.86392\n",
      "43 Train Loss 1531.436\n",
      "44 loss_D: 1100.6146 loss_N: 53.299572 loss_f: 282.24298\n",
      "44 Train Loss 1436.1572\n",
      "45 loss_D: 1122.9669 loss_N: 47.59633 loss_f: 223.44794\n",
      "45 Train Loss 1394.0112\n",
      "46 loss_D: 1105.2902 loss_N: 47.38221 loss_f: 210.82405\n",
      "46 Train Loss 1363.4965\n",
      "47 loss_D: 1074.5012 loss_N: 47.523266 loss_f: 200.85197\n",
      "47 Train Loss 1322.8765\n",
      "48 loss_D: 1035.0522 loss_N: 47.741646 loss_f: 170.32951\n",
      "48 Train Loss 1253.1234\n",
      "49 loss_D: 1007.71295 loss_N: 48.962757 loss_f: 160.18411\n",
      "49 Train Loss 1216.8597\n",
      "50 loss_D: 1009.8398 loss_N: 48.19298 loss_f: 136.56216\n",
      "50 Train Loss 1194.5948\n",
      "51 loss_D: 1006.6244 loss_N: 46.944454 loss_f: 125.095245\n",
      "51 Train Loss 1178.6641\n",
      "52 loss_D: 1007.54 loss_N: 39.8177 loss_f: 95.413666\n",
      "52 Train Loss 1142.7714\n",
      "53 loss_D: 982.29675 loss_N: 33.704067 loss_f: 101.481415\n",
      "53 Train Loss 1117.4822\n",
      "54 loss_D: 987.2691 loss_N: 27.550081 loss_f: 80.817825\n",
      "54 Train Loss 1095.6371\n",
      "55 loss_D: 984.3255 loss_N: 26.732706 loss_f: 79.64463\n",
      "55 Train Loss 1090.7029\n",
      "56 loss_D: 970.49396 loss_N: 23.066462 loss_f: 75.2517\n",
      "56 Train Loss 1068.8121\n",
      "57 loss_D: 945.53156 loss_N: 16.69569 loss_f: 73.959175\n",
      "57 Train Loss 1036.1864\n",
      "58 loss_D: 912.6623 loss_N: 10.17091 loss_f: 93.54696\n",
      "58 Train Loss 1016.3801\n",
      "59 loss_D: 915.43677 loss_N: 8.979189 loss_f: 76.949234\n",
      "59 Train Loss 1001.3652\n",
      "60 loss_D: 906.42896 loss_N: 8.064515 loss_f: 75.227875\n",
      "60 Train Loss 989.7213\n",
      "61 loss_D: 882.2456 loss_N: 6.8091264 loss_f: 80.17613\n",
      "61 Train Loss 969.2309\n",
      "62 loss_D: 874.67096 loss_N: 7.182084 loss_f: 79.3244\n",
      "62 Train Loss 961.1774\n",
      "63 loss_D: 866.46875 loss_N: 7.1219935 loss_f: 63.570988\n",
      "63 Train Loss 937.16174\n",
      "64 loss_D: 861.21387 loss_N: 7.9634466 loss_f: 57.49135\n",
      "64 Train Loss 926.66864\n",
      "65 loss_D: 857.6518 loss_N: 9.625796 loss_f: 49.265816\n",
      "65 Train Loss 916.5434\n",
      "66 loss_D: 847.946 loss_N: 12.151218 loss_f: 47.939148\n",
      "66 Train Loss 908.0364\n",
      "67 loss_D: 843.03296 loss_N: 14.425823 loss_f: 39.372894\n",
      "67 Train Loss 896.83167\n",
      "68 loss_D: 834.0195 loss_N: 15.865406 loss_f: 39.652744\n",
      "68 Train Loss 889.53766\n",
      "69 loss_D: 833.5539 loss_N: 13.662401 loss_f: 36.956024\n",
      "69 Train Loss 884.17236\n",
      "70 loss_D: 832.53815 loss_N: 12.631339 loss_f: 35.17926\n",
      "70 Train Loss 880.34875\n",
      "71 loss_D: 830.651 loss_N: 12.151488 loss_f: 35.226997\n",
      "71 Train Loss 878.0295\n",
      "72 loss_D: 825.0788 loss_N: 11.450928 loss_f: 36.807896\n",
      "72 Train Loss 873.33765\n",
      "73 loss_D: 814.45105 loss_N: 10.323409 loss_f: 39.30524\n",
      "73 Train Loss 864.0797\n",
      "74 loss_D: 807.0151 loss_N: 8.762248 loss_f: 41.096645\n",
      "74 Train Loss 856.87396\n",
      "75 loss_D: 804.9518 loss_N: 9.010735 loss_f: 39.275818\n",
      "75 Train Loss 853.23834\n",
      "76 loss_D: 809.97705 loss_N: 8.052069 loss_f: 29.812693\n",
      "76 Train Loss 847.8418\n",
      "77 loss_D: 802.1129 loss_N: 7.671017 loss_f: 36.342\n",
      "77 Train Loss 846.1259\n",
      "78 loss_D: 805.2784 loss_N: 7.7807703 loss_f: 30.46733\n",
      "78 Train Loss 843.5265\n",
      "79 loss_D: 804.10315 loss_N: 7.552725 loss_f: 26.600163\n",
      "79 Train Loss 838.25604\n",
      "80 loss_D: 800.41077 loss_N: 7.4258122 loss_f: 26.764828\n",
      "80 Train Loss 834.60144\n",
      "81 loss_D: 800.15875 loss_N: 7.413767 loss_f: 24.131569\n",
      "81 Train Loss 831.7041\n",
      "82 loss_D: 794.40686 loss_N: 7.634399 loss_f: 23.94289\n",
      "82 Train Loss 825.98413\n",
      "83 loss_D: 785.848 loss_N: 7.9572372 loss_f: 24.96763\n",
      "83 Train Loss 818.7729\n",
      "84 loss_D: 776.5036 loss_N: 8.213868 loss_f: 29.78899\n",
      "84 Train Loss 814.5065\n",
      "85 loss_D: 770.28613 loss_N: 8.047644 loss_f: 32.214478\n",
      "85 Train Loss 810.5483\n",
      "86 loss_D: 765.06476 loss_N: 8.092667 loss_f: 33.245373\n",
      "86 Train Loss 806.4028\n",
      "87 loss_D: 756.2636 loss_N: 8.589934 loss_f: 34.92751\n",
      "87 Train Loss 799.781\n",
      "88 loss_D: 752.885 loss_N: 8.909931 loss_f: 34.448307\n",
      "88 Train Loss 796.2432\n",
      "89 loss_D: 753.7972 loss_N: 8.925289 loss_f: 29.603302\n",
      "89 Train Loss 792.3258\n",
      "90 loss_D: 751.0867 loss_N: 9.265777 loss_f: 30.526934\n",
      "90 Train Loss 790.87933\n",
      "91 loss_D: 750.17 loss_N: 9.665866 loss_f: 29.989437\n",
      "91 Train Loss 789.8253\n",
      "92 loss_D: 747.32587 loss_N: 9.661168 loss_f: 31.082228\n",
      "92 Train Loss 788.0693\n",
      "93 loss_D: 744.2572 loss_N: 9.611467 loss_f: 31.515\n",
      "93 Train Loss 785.38367\n",
      "94 loss_D: 741.1662 loss_N: 9.495507 loss_f: 30.961042\n",
      "94 Train Loss 781.62274\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "95 loss_D: 737.65216 loss_N: 9.618099 loss_f: 30.257118\n",
      "95 Train Loss 777.5274\n",
      "96 loss_D: 739.5584 loss_N: 9.528499 loss_f: 26.352537\n",
      "96 Train Loss 775.43945\n",
      "97 loss_D: 737.0006 loss_N: 9.947508 loss_f: 26.87848\n",
      "97 Train Loss 773.8266\n",
      "98 loss_D: 737.4002 loss_N: 9.866598 loss_f: 25.813677\n",
      "98 Train Loss 773.08044\n",
      "99 loss_D: 738.0699 loss_N: 9.848575 loss_f: 24.470108\n",
      "99 Train Loss 772.38855\n",
      "100 loss_D: 737.0628 loss_N: 9.694911 loss_f: 24.226364\n",
      "100 Train Loss 770.9841\n",
      "101 loss_D: 730.82477 loss_N: 9.1466255 loss_f: 26.087482\n",
      "101 Train Loss 766.05884\n",
      "102 loss_D: 729.00464 loss_N: 8.97853 loss_f: 26.038885\n",
      "102 Train Loss 764.02203\n",
      "103 loss_D: 722.9642 loss_N: 8.994208 loss_f: 27.825663\n",
      "103 Train Loss 759.78406\n",
      "104 loss_D: 718.11786 loss_N: 9.029224 loss_f: 29.612675\n",
      "104 Train Loss 756.75977\n",
      "105 loss_D: 715.79016 loss_N: 9.113309 loss_f: 29.82219\n",
      "105 Train Loss 754.72565\n",
      "106 loss_D: 713.6181 loss_N: 9.080404 loss_f: 30.359238\n",
      "106 Train Loss 753.05774\n",
      "107 loss_D: 713.72107 loss_N: 9.053056 loss_f: 28.953543\n",
      "107 Train Loss 751.72766\n",
      "108 loss_D: 709.5686 loss_N: 9.087632 loss_f: 35.134666\n",
      "108 Train Loss 753.7909\n",
      "109 loss_D: 711.79785 loss_N: 9.03552 loss_f: 29.386324\n",
      "109 Train Loss 750.2197\n",
      "110 loss_D: 713.1041 loss_N: 8.939562 loss_f: 25.69284\n",
      "110 Train Loss 747.7366\n",
      "111 loss_D: 713.47125 loss_N: 8.838993 loss_f: 21.872356\n",
      "111 Train Loss 744.1826\n",
      "112 loss_D: 713.27686 loss_N: 8.797776 loss_f: 20.536764\n",
      "112 Train Loss 742.6114\n",
      "113 loss_D: 712.07446 loss_N: 8.773978 loss_f: 20.056374\n",
      "113 Train Loss 740.90485\n",
      "114 loss_D: 709.29767 loss_N: 8.806168 loss_f: 20.63608\n",
      "114 Train Loss 738.7399\n",
      "115 loss_D: 707.07983 loss_N: 8.878742 loss_f: 21.300282\n",
      "115 Train Loss 737.25885\n",
      "116 loss_D: 704.0964 loss_N: 8.900988 loss_f: 22.37835\n",
      "116 Train Loss 735.37573\n",
      "117 loss_D: 701.71844 loss_N: 8.937291 loss_f: 21.93717\n",
      "117 Train Loss 732.59296\n",
      "118 loss_D: 699.595 loss_N: 8.956347 loss_f: 20.990677\n",
      "118 Train Loss 729.542\n",
      "119 loss_D: 699.1372 loss_N: 8.957633 loss_f: 18.974934\n",
      "119 Train Loss 727.06976\n",
      "120 loss_D: 698.8025 loss_N: 8.929583 loss_f: 17.383125\n",
      "120 Train Loss 725.1152\n",
      "121 loss_D: 697.08527 loss_N: 8.941182 loss_f: 17.488047\n",
      "121 Train Loss 723.51447\n",
      "122 loss_D: 696.58685 loss_N: 8.924961 loss_f: 16.79518\n",
      "122 Train Loss 722.307\n",
      "123 loss_D: 694.97986 loss_N: 8.969236 loss_f: 17.454233\n",
      "123 Train Loss 721.4033\n",
      "124 loss_D: 692.52966 loss_N: 9.059196 loss_f: 18.014149\n",
      "124 Train Loss 719.603\n",
      "125 loss_D: 689.81836 loss_N: 9.15882 loss_f: 18.506945\n",
      "125 Train Loss 717.48413\n",
      "126 loss_D: 686.5439 loss_N: 9.274969 loss_f: 18.825243\n",
      "126 Train Loss 714.6441\n",
      "127 loss_D: 680.90906 loss_N: 9.577657 loss_f: 19.862238\n",
      "127 Train Loss 710.34894\n",
      "128 loss_D: 678.492 loss_N: 9.77345 loss_f: 18.86209\n",
      "128 Train Loss 707.1275\n",
      "129 loss_D: 676.4519 loss_N: 9.911639 loss_f: 19.366833\n",
      "129 Train Loss 705.73035\n",
      "130 loss_D: 678.5522 loss_N: 9.681106 loss_f: 17.071991\n",
      "130 Train Loss 705.3053\n",
      "131 loss_D: 677.1413 loss_N: 9.793508 loss_f: 17.30717\n",
      "131 Train Loss 704.242\n",
      "132 loss_D: 676.07416 loss_N: 9.898389 loss_f: 17.722633\n",
      "132 Train Loss 703.6952\n",
      "133 loss_D: 674.38995 loss_N: 10.081217 loss_f: 18.394762\n",
      "133 Train Loss 702.86597\n",
      "134 loss_D: 673.2349 loss_N: 10.215258 loss_f: 18.724012\n",
      "134 Train Loss 702.1742\n",
      "135 loss_D: 671.0887 loss_N: 10.461316 loss_f: 20.115294\n",
      "135 Train Loss 701.6653\n",
      "136 loss_D: 670.73627 loss_N: 10.444066 loss_f: 19.758186\n",
      "136 Train Loss 700.93854\n",
      "137 loss_D: 669.83856 loss_N: 10.456922 loss_f: 19.719606\n",
      "137 Train Loss 700.0151\n",
      "138 loss_D: 668.3758 loss_N: 10.553112 loss_f: 20.034546\n",
      "138 Train Loss 698.96344\n",
      "139 loss_D: 666.5487 loss_N: 10.695102 loss_f: 20.456598\n",
      "139 Train Loss 697.70044\n",
      "140 loss_D: 663.26215 loss_N: 10.9467745 loss_f: 21.010374\n",
      "140 Train Loss 695.2193\n",
      "141 loss_D: 661.19806 loss_N: 11.047739 loss_f: 20.419865\n",
      "141 Train Loss 692.66565\n",
      "142 loss_D: 658.5735 loss_N: 11.200678 loss_f: 20.763632\n",
      "142 Train Loss 690.5378\n",
      "143 loss_D: 657.3455 loss_N: 11.310106 loss_f: 20.754108\n",
      "143 Train Loss 689.4097\n",
      "144 loss_D: 655.89496 loss_N: 11.499162 loss_f: 20.786293\n",
      "144 Train Loss 688.1804\n",
      "145 loss_D: 652.81555 loss_N: 11.855723 loss_f: 21.677156\n",
      "145 Train Loss 686.34845\n",
      "146 loss_D: 648.3002 loss_N: 12.616707 loss_f: 23.38133\n",
      "146 Train Loss 684.2982\n",
      "147 loss_D: 648.2488 loss_N: 12.685854 loss_f: 22.706512\n",
      "147 Train Loss 683.6411\n",
      "148 loss_D: 646.01385 loss_N: 13.196628 loss_f: 24.114597\n",
      "148 Train Loss 683.32513\n",
      "149 loss_D: 646.9082 loss_N: 13.169906 loss_f: 22.443674\n",
      "149 Train Loss 682.5218\n",
      "150 loss_D: 647.2646 loss_N: 13.054432 loss_f: 21.681513\n",
      "150 Train Loss 682.00055\n",
      "151 loss_D: 646.7787 loss_N: 13.196756 loss_f: 21.05212\n",
      "151 Train Loss 681.0276\n",
      "152 loss_D: 643.37036 loss_N: 13.923143 loss_f: 22.365274\n",
      "152 Train Loss 679.6588\n",
      "153 loss_D: 642.2484 loss_N: 14.154672 loss_f: 22.022825\n",
      "153 Train Loss 678.4259\n",
      "154 loss_D: 639.90533 loss_N: 14.376268 loss_f: 22.221388\n",
      "154 Train Loss 676.503\n",
      "155 loss_D: 639.0143 loss_N: 14.366853 loss_f: 21.992292\n",
      "155 Train Loss 675.3735\n",
      "156 loss_D: 636.7618 loss_N: 14.776576 loss_f: 23.437967\n",
      "156 Train Loss 674.9763\n",
      "157 loss_D: 638.0209 loss_N: 14.500115 loss_f: 21.94782\n",
      "157 Train Loss 674.4688\n",
      "158 loss_D: 638.0421 loss_N: 14.476972 loss_f: 21.705357\n",
      "158 Train Loss 674.2245\n",
      "159 loss_D: 638.071 loss_N: 14.471832 loss_f: 21.222075\n",
      "159 Train Loss 673.7649\n",
      "160 loss_D: 637.51294 loss_N: 14.640162 loss_f: 20.7197\n",
      "160 Train Loss 672.8728\n",
      "161 loss_D: 637.17944 loss_N: 14.634296 loss_f: 20.184357\n",
      "161 Train Loss 671.99805\n",
      "162 loss_D: 636.3151 loss_N: 14.777418 loss_f: 19.65586\n",
      "162 Train Loss 670.7484\n",
      "163 loss_D: 635.71606 loss_N: 14.746479 loss_f: 19.165268\n",
      "163 Train Loss 669.6278\n",
      "164 loss_D: 633.6305 loss_N: 14.935192 loss_f: 19.14508\n",
      "164 Train Loss 667.71075\n",
      "165 loss_D: 630.56323 loss_N: 15.323793 loss_f: 18.936678\n",
      "165 Train Loss 664.8237\n",
      "166 loss_D: 622.80383 loss_N: 16.75682 loss_f: 24.16445\n",
      "166 Train Loss 663.7251\n",
      "167 loss_D: 625.6272 loss_N: 16.18477 loss_f: 20.347166\n",
      "167 Train Loss 662.1591\n",
      "168 loss_D: 620.0423 loss_N: 17.301804 loss_f: 30.717043\n",
      "168 Train Loss 668.06116\n",
      "169 loss_D: 623.08435 loss_N: 16.639927 loss_f: 20.07696\n",
      "169 Train Loss 659.8013\n",
      "170 loss_D: 623.24713 loss_N: 16.571316 loss_f: 18.290447\n",
      "170 Train Loss 658.1089\n",
      "171 loss_D: 619.801 loss_N: 17.253386 loss_f: 19.481297\n",
      "171 Train Loss 656.5357\n",
      "172 loss_D: 618.8567 loss_N: 17.487387 loss_f: 19.273481\n",
      "172 Train Loss 655.61755\n",
      "173 loss_D: 618.5146 loss_N: 17.507141 loss_f: 18.800365\n",
      "173 Train Loss 654.8221\n",
      "174 loss_D: 616.868 loss_N: 17.812477 loss_f: 19.65467\n",
      "174 Train Loss 654.33514\n",
      "175 loss_D: 617.7662 loss_N: 17.330063 loss_f: 18.678709\n",
      "175 Train Loss 653.77496\n",
      "176 loss_D: 617.4263 loss_N: 17.510666 loss_f: 18.091772\n",
      "176 Train Loss 653.02875\n",
      "177 loss_D: 616.33826 loss_N: 17.7775 loss_f: 18.486658\n",
      "177 Train Loss 652.6024\n",
      "178 loss_D: 615.5196 loss_N: 17.940437 loss_f: 18.346294\n",
      "178 Train Loss 651.80634\n",
      "179 loss_D: 613.73846 loss_N: 18.217182 loss_f: 18.65258\n",
      "179 Train Loss 650.6082\n",
      "180 loss_D: 612.903 loss_N: 18.527561 loss_f: 18.336706\n",
      "180 Train Loss 649.76733\n",
      "181 loss_D: 611.8226 loss_N: 19.107689 loss_f: 17.578955\n",
      "181 Train Loss 648.5092\n",
      "182 loss_D: 610.11017 loss_N: 19.930334 loss_f: 17.42453\n",
      "182 Train Loss 647.4651\n",
      "183 loss_D: 610.4812 loss_N: 20.134277 loss_f: 16.084715\n",
      "183 Train Loss 646.7002\n",
      "184 loss_D: 609.7407 loss_N: 20.686922 loss_f: 15.547037\n",
      "184 Train Loss 645.97473\n",
      "185 loss_D: 608.12726 loss_N: 21.45242 loss_f: 15.3778105\n",
      "185 Train Loss 644.95746\n",
      "186 loss_D: 605.3131 loss_N: 22.485613 loss_f: 15.604137\n",
      "186 Train Loss 643.40283\n",
      "187 loss_D: 602.7287 loss_N: 22.945402 loss_f: 15.987759\n",
      "187 Train Loss 641.6618\n",
      "188 loss_D: 599.8374 loss_N: 23.31887 loss_f: 16.952812\n",
      "188 Train Loss 640.1091\n",
      "189 loss_D: 598.6173 loss_N: 22.753511 loss_f: 17.498674\n",
      "189 Train Loss 638.8695\n",
      "190 loss_D: 596.1708 loss_N: 23.138449 loss_f: 18.520693\n",
      "190 Train Loss 637.8299\n",
      "191 loss_D: 595.8619 loss_N: 22.984943 loss_f: 17.961359\n",
      "191 Train Loss 636.80817\n",
      "192 loss_D: 594.64465 loss_N: 23.374231 loss_f: 18.245605\n",
      "192 Train Loss 636.26447\n",
      "193 loss_D: 593.9448 loss_N: 23.703957 loss_f: 17.780708\n",
      "193 Train Loss 635.4295\n",
      "194 loss_D: 592.1083 loss_N: 24.392975 loss_f: 18.002234\n",
      "194 Train Loss 634.5035\n",
      "195 loss_D: 591.7225 loss_N: 24.601334 loss_f: 17.548567\n",
      "195 Train Loss 633.8724\n",
      "196 loss_D: 590.5909 loss_N: 24.779938 loss_f: 17.71126\n",
      "196 Train Loss 633.0821\n",
      "197 loss_D: 589.58966 loss_N: 24.646969 loss_f: 18.111893\n",
      "197 Train Loss 632.3485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "198 loss_D: 589.6286 loss_N: 24.539215 loss_f: 17.754147\n",
      "198 Train Loss 631.922\n",
      "199 loss_D: 589.26855 loss_N: 24.539793 loss_f: 17.55319\n",
      "199 Train Loss 631.3615\n",
      "200 loss_D: 588.80115 loss_N: 24.694515 loss_f: 17.170868\n",
      "200 Train Loss 630.6665\n",
      "201 loss_D: 587.98376 loss_N: 25.100138 loss_f: 16.701752\n",
      "201 Train Loss 629.78564\n",
      "202 loss_D: 587.3376 loss_N: 25.6867 loss_f: 15.887297\n",
      "202 Train Loss 628.91156\n",
      "203 loss_D: 584.5998 loss_N: 26.832619 loss_f: 17.258312\n",
      "203 Train Loss 628.69073\n",
      "204 loss_D: 585.5285 loss_N: 26.451855 loss_f: 15.994996\n",
      "204 Train Loss 627.97534\n",
      "205 loss_D: 585.62756 loss_N: 26.007425 loss_f: 16.003819\n",
      "205 Train Loss 627.63885\n",
      "206 loss_D: 584.9002 loss_N: 26.0238 loss_f: 16.271923\n",
      "206 Train Loss 627.1959\n",
      "207 loss_D: 582.9868 loss_N: 26.129684 loss_f: 17.347952\n",
      "207 Train Loss 626.4645\n",
      "208 loss_D: 582.74805 loss_N: 26.138868 loss_f: 17.117647\n",
      "208 Train Loss 626.0046\n",
      "209 loss_D: 582.3172 loss_N: 25.752058 loss_f: 16.770786\n",
      "209 Train Loss 624.8401\n",
      "210 loss_D: 582.6307 loss_N: 25.522081 loss_f: 15.742734\n",
      "210 Train Loss 623.8955\n",
      "211 loss_D: 582.87213 loss_N: 25.085648 loss_f: 15.107362\n",
      "211 Train Loss 623.0651\n",
      "212 loss_D: 582.1701 loss_N: 25.160543 loss_f: 14.929226\n",
      "212 Train Loss 622.2598\n",
      "213 loss_D: 582.08246 loss_N: 25.050356 loss_f: 14.314444\n",
      "213 Train Loss 621.44727\n",
      "214 loss_D: 580.6598 loss_N: 25.443188 loss_f: 14.608034\n",
      "214 Train Loss 620.711\n",
      "215 loss_D: 579.4583 loss_N: 25.680086 loss_f: 14.639593\n",
      "215 Train Loss 619.778\n",
      "216 loss_D: 578.6294 loss_N: 25.875587 loss_f: 14.502342\n",
      "216 Train Loss 619.0073\n",
      "217 loss_D: 576.9895 loss_N: 25.947533 loss_f: 15.344287\n",
      "217 Train Loss 618.2813\n",
      "218 loss_D: 577.3906 loss_N: 25.934954 loss_f: 14.348359\n",
      "218 Train Loss 617.67395\n",
      "219 loss_D: 577.0498 loss_N: 25.932835 loss_f: 13.975621\n",
      "219 Train Loss 616.9583\n",
      "220 loss_D: 575.4183 loss_N: 26.110321 loss_f: 13.968773\n",
      "220 Train Loss 615.4973\n",
      "221 loss_D: 572.4292 loss_N: 26.737078 loss_f: 14.8042555\n",
      "221 Train Loss 613.9705\n",
      "222 loss_D: 571.5053 loss_N: 26.900606 loss_f: 14.189013\n",
      "222 Train Loss 612.595\n",
      "223 loss_D: 569.7105 loss_N: 27.411964 loss_f: 13.949023\n",
      "223 Train Loss 611.07153\n",
      "224 loss_D: 569.7084 loss_N: 27.189093 loss_f: 13.222084\n",
      "224 Train Loss 610.11957\n",
      "225 loss_D: 568.4091 loss_N: 27.013481 loss_f: 14.319349\n",
      "225 Train Loss 609.74194\n",
      "226 loss_D: 568.46564 loss_N: 27.065248 loss_f: 13.933266\n",
      "226 Train Loss 609.4642\n",
      "227 loss_D: 568.41394 loss_N: 26.96876 loss_f: 13.817968\n",
      "227 Train Loss 609.2007\n",
      "228 loss_D: 567.916 loss_N: 27.05465 loss_f: 13.928634\n",
      "228 Train Loss 608.8993\n",
      "229 loss_D: 566.8557 loss_N: 27.215502 loss_f: 14.436882\n",
      "229 Train Loss 608.5081\n",
      "230 loss_D: 565.35925 loss_N: 27.396278 loss_f: 14.990693\n",
      "230 Train Loss 607.7463\n",
      "231 loss_D: 563.9005 loss_N: 27.470018 loss_f: 15.723771\n",
      "231 Train Loss 607.0943\n",
      "232 loss_D: 563.2027 loss_N: 27.160118 loss_f: 16.183405\n",
      "232 Train Loss 606.5462\n",
      "233 loss_D: 563.1871 loss_N: 26.880577 loss_f: 16.000725\n",
      "233 Train Loss 606.06836\n",
      "234 loss_D: 562.7248 loss_N: 26.580862 loss_f: 15.9278555\n",
      "234 Train Loss 605.2335\n",
      "235 loss_D: 562.7515 loss_N: 26.391949 loss_f: 15.343845\n",
      "235 Train Loss 604.48737\n",
      "236 loss_D: 561.0724 loss_N: 26.384893 loss_f: 16.208906\n",
      "236 Train Loss 603.6662\n",
      "237 loss_D: 560.4418 loss_N: 26.54605 loss_f: 15.713966\n",
      "237 Train Loss 602.7018\n",
      "238 loss_D: 559.57086 loss_N: 26.728168 loss_f: 15.916214\n",
      "238 Train Loss 602.2152\n",
      "239 loss_D: 558.2968 loss_N: 27.093206 loss_f: 16.77774\n",
      "239 Train Loss 602.1677\n",
      "240 loss_D: 558.8854 loss_N: 26.921381 loss_f: 16.079285\n",
      "240 Train Loss 601.88605\n",
      "241 loss_D: 558.7246 loss_N: 26.769705 loss_f: 16.015987\n",
      "241 Train Loss 601.5103\n",
      "242 loss_D: 558.9331 loss_N: 26.574738 loss_f: 15.411018\n",
      "242 Train Loss 600.9188\n",
      "243 loss_D: 558.5899 loss_N: 25.985792 loss_f: 14.922262\n",
      "243 Train Loss 599.4979\n",
      "244 loss_D: 558.4895 loss_N: 25.727243 loss_f: 14.400585\n",
      "244 Train Loss 598.6173\n",
      "245 loss_D: 557.49084 loss_N: 25.132946 loss_f: 14.548473\n",
      "245 Train Loss 597.17224\n",
      "246 loss_D: 557.351 loss_N: 25.000805 loss_f: 13.961562\n",
      "246 Train Loss 596.31335\n",
      "247 loss_D: 556.6642 loss_N: 24.63273 loss_f: 13.901468\n",
      "247 Train Loss 595.1984\n",
      "248 loss_D: 555.1496 loss_N: 24.520205 loss_f: 15.033238\n",
      "248 Train Loss 594.70306\n",
      "249 loss_D: 555.40045 loss_N: 24.822638 loss_f: 13.571742\n",
      "249 Train Loss 593.7948\n",
      "250 loss_D: 554.87524 loss_N: 24.661259 loss_f: 13.424166\n",
      "250 Train Loss 592.9607\n",
      "251 loss_D: 553.94586 loss_N: 24.545746 loss_f: 13.822841\n",
      "251 Train Loss 592.3144\n",
      "252 loss_D: 553.27216 loss_N: 24.23191 loss_f: 14.34614\n",
      "252 Train Loss 591.8502\n",
      "253 loss_D: 552.6203 loss_N: 24.238897 loss_f: 14.514279\n",
      "253 Train Loss 591.3735\n",
      "254 loss_D: 552.73456 loss_N: 24.059906 loss_f: 14.173159\n",
      "254 Train Loss 590.9676\n",
      "255 loss_D: 552.62836 loss_N: 23.758333 loss_f: 14.023978\n",
      "255 Train Loss 590.4107\n",
      "256 loss_D: 552.0726 loss_N: 23.475077 loss_f: 14.318686\n",
      "256 Train Loss 589.86633\n",
      "257 loss_D: 550.3424 loss_N: 23.057186 loss_f: 15.370493\n",
      "257 Train Loss 588.7701\n",
      "258 loss_D: 549.6203 loss_N: 22.927664 loss_f: 15.550853\n",
      "258 Train Loss 588.0988\n",
      "259 loss_D: 548.8588 loss_N: 23.052277 loss_f: 15.80585\n",
      "259 Train Loss 587.717\n",
      "260 loss_D: 548.60114 loss_N: 22.984886 loss_f: 15.375003\n",
      "260 Train Loss 586.961\n",
      "261 loss_D: 547.18945 loss_N: 22.90122 loss_f: 15.163718\n",
      "261 Train Loss 585.2544\n",
      "262 loss_D: 546.0012 loss_N: 22.857449 loss_f: 15.424705\n",
      "262 Train Loss 584.2833\n",
      "263 loss_D: 545.8538 loss_N: 22.999027 loss_f: 14.816502\n",
      "263 Train Loss 583.6694\n",
      "264 loss_D: 545.25085 loss_N: 22.624187 loss_f: 14.500179\n",
      "264 Train Loss 582.37524\n",
      "265 loss_D: 544.58496 loss_N: 22.638077 loss_f: 14.22377\n",
      "265 Train Loss 581.4468\n",
      "266 loss_D: 544.1123 loss_N: 22.86138 loss_f: 13.742252\n",
      "266 Train Loss 580.71594\n",
      "267 loss_D: 543.4059 loss_N: 22.576836 loss_f: 14.30038\n",
      "267 Train Loss 580.2831\n",
      "268 loss_D: 543.03516 loss_N: 22.32909 loss_f: 14.569937\n",
      "268 Train Loss 579.9342\n",
      "269 loss_D: 542.4333 loss_N: 22.122702 loss_f: 15.128176\n",
      "269 Train Loss 579.68414\n",
      "270 loss_D: 542.738 loss_N: 21.904562 loss_f: 14.663397\n",
      "270 Train Loss 579.3059\n",
      "271 loss_D: 542.6684 loss_N: 21.831266 loss_f: 14.485818\n",
      "271 Train Loss 578.9855\n",
      "272 loss_D: 542.66254 loss_N: 21.605782 loss_f: 14.178609\n",
      "272 Train Loss 578.4469\n",
      "273 loss_D: 541.4765 loss_N: 21.204605 loss_f: 15.36909\n",
      "273 Train Loss 578.0502\n",
      "274 loss_D: 541.12177 loss_N: 21.332443 loss_f: 15.048412\n",
      "274 Train Loss 577.5026\n",
      "275 loss_D: 540.84625 loss_N: 21.325718 loss_f: 14.78049\n",
      "275 Train Loss 576.9525\n",
      "276 loss_D: 539.6688 loss_N: 21.22354 loss_f: 15.156121\n",
      "276 Train Loss 576.04846\n",
      "277 loss_D: 538.67804 loss_N: 21.099106 loss_f: 15.228661\n",
      "277 Train Loss 575.0058\n",
      "278 loss_D: 536.9866 loss_N: 21.098675 loss_f: 16.211966\n",
      "278 Train Loss 574.29724\n",
      "279 loss_D: 536.8972 loss_N: 21.216656 loss_f: 15.81274\n",
      "279 Train Loss 573.92664\n",
      "280 loss_D: 536.5469 loss_N: 21.20483 loss_f: 15.604422\n",
      "280 Train Loss 573.35614\n",
      "281 loss_D: 535.95526 loss_N: 21.210693 loss_f: 15.528402\n",
      "281 Train Loss 572.69434\n",
      "282 loss_D: 535.13226 loss_N: 21.250528 loss_f: 15.745437\n",
      "282 Train Loss 572.12823\n",
      "283 loss_D: 535.3515 loss_N: 21.229025 loss_f: 15.042537\n",
      "283 Train Loss 571.62305\n",
      "284 loss_D: 534.8665 loss_N: 21.309647 loss_f: 14.986018\n",
      "284 Train Loss 571.1622\n",
      "285 loss_D: 534.7872 loss_N: 21.315569 loss_f: 14.522782\n",
      "285 Train Loss 570.6255\n",
      "286 loss_D: 533.9408 loss_N: 21.307463 loss_f: 14.80448\n",
      "286 Train Loss 570.05273\n",
      "287 loss_D: 532.711 loss_N: 21.477098 loss_f: 15.372734\n",
      "287 Train Loss 569.56085\n",
      "288 loss_D: 531.88544 loss_N: 21.448183 loss_f: 15.710397\n",
      "288 Train Loss 569.044\n",
      "289 loss_D: 531.7321 loss_N: 21.694849 loss_f: 15.287477\n",
      "289 Train Loss 568.7144\n",
      "290 loss_D: 530.7891 loss_N: 22.00492 loss_f: 15.593193\n",
      "290 Train Loss 568.38727\n",
      "291 loss_D: 530.0594 loss_N: 22.286842 loss_f: 15.806137\n",
      "291 Train Loss 568.1524\n",
      "292 loss_D: 530.1205 loss_N: 22.373981 loss_f: 15.31339\n",
      "292 Train Loss 567.80786\n",
      "293 loss_D: 529.8988 loss_N: 22.406525 loss_f: 15.176489\n",
      "293 Train Loss 567.4818\n",
      "294 loss_D: 529.57416 loss_N: 22.445097 loss_f: 15.009747\n",
      "294 Train Loss 567.029\n",
      "295 loss_D: 528.5147 loss_N: 22.531796 loss_f: 15.056541\n",
      "295 Train Loss 566.103\n",
      "296 loss_D: 526.6864 loss_N: 22.457056 loss_f: 15.794484\n",
      "296 Train Loss 564.9379\n",
      "297 loss_D: 526.0291 loss_N: 22.263634 loss_f: 15.766615\n",
      "297 Train Loss 564.0593\n",
      "298 loss_D: 524.9536 loss_N: 22.154457 loss_f: 16.121437\n",
      "298 Train Loss 563.22955\n",
      "299 loss_D: 525.1677 loss_N: 21.825197 loss_f: 15.742009\n",
      "299 Train Loss 562.7349\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 loss_D: 524.924 loss_N: 21.650059 loss_f: 15.86992\n",
      "300 Train Loss 562.44403\n",
      "301 loss_D: 524.9981 loss_N: 21.60551 loss_f: 15.435931\n",
      "301 Train Loss 562.03955\n",
      "302 loss_D: 524.9096 loss_N: 21.571411 loss_f: 14.795371\n",
      "302 Train Loss 561.27637\n",
      "303 loss_D: 523.9839 loss_N: 21.488993 loss_f: 14.862462\n",
      "303 Train Loss 560.3354\n",
      "304 loss_D: 523.3606 loss_N: 21.481047 loss_f: 14.619806\n",
      "304 Train Loss 559.4614\n",
      "305 loss_D: 521.9624 loss_N: 21.675522 loss_f: 15.24166\n",
      "305 Train Loss 558.8796\n",
      "306 loss_D: 521.33 loss_N: 21.635395 loss_f: 15.306079\n",
      "306 Train Loss 558.2715\n",
      "307 loss_D: 519.97156 loss_N: 21.760466 loss_f: 16.195393\n",
      "307 Train Loss 557.92737\n",
      "308 loss_D: 520.16705 loss_N: 21.570305 loss_f: 15.897837\n",
      "308 Train Loss 557.6352\n",
      "309 loss_D: 519.5357 loss_N: 21.407377 loss_f: 16.534912\n",
      "309 Train Loss 557.47797\n",
      "310 loss_D: 519.4834 loss_N: 21.434303 loss_f: 16.4182\n",
      "310 Train Loss 557.33594\n",
      "311 loss_D: 519.4209 loss_N: 21.436037 loss_f: 16.310816\n",
      "311 Train Loss 557.1677\n",
      "312 loss_D: 519.0016 loss_N: 21.291882 loss_f: 16.44898\n",
      "312 Train Loss 556.74243\n",
      "313 loss_D: 517.34827 loss_N: 20.957699 loss_f: 17.333603\n",
      "313 Train Loss 555.6396\n",
      "314 loss_D: 515.71265 loss_N: 20.56463 loss_f: 18.089907\n",
      "314 Train Loss 554.3672\n",
      "315 loss_D: 513.73126 loss_N: 20.267103 loss_f: 18.993635\n",
      "315 Train Loss 552.992\n",
      "316 loss_D: 512.03296 loss_N: 20.368044 loss_f: 19.002468\n",
      "316 Train Loss 551.40344\n",
      "317 loss_D: 507.7457 loss_N: 20.294285 loss_f: 27.195368\n",
      "317 Train Loss 555.23535\n",
      "318 loss_D: 510.65002 loss_N: 20.343693 loss_f: 19.900919\n",
      "318 Train Loss 550.89465\n",
      "319 loss_D: 508.2516 loss_N: 20.463999 loss_f: 21.026058\n",
      "319 Train Loss 549.74164\n",
      "320 loss_D: 506.58774 loss_N: 20.503057 loss_f: 21.664217\n",
      "320 Train Loss 548.75507\n",
      "321 loss_D: 507.62918 loss_N: 20.789408 loss_f: 19.270416\n",
      "321 Train Loss 547.68896\n",
      "322 loss_D: 506.27972 loss_N: 20.691303 loss_f: 19.20861\n",
      "322 Train Loss 546.1796\n",
      "323 loss_D: 505.0072 loss_N: 20.77724 loss_f: 19.26707\n",
      "323 Train Loss 545.0515\n",
      "324 loss_D: 504.0478 loss_N: 20.836351 loss_f: 18.871471\n",
      "324 Train Loss 543.7556\n",
      "325 loss_D: 502.3495 loss_N: 20.73429 loss_f: 19.153917\n",
      "325 Train Loss 542.23773\n",
      "326 loss_D: 502.60718 loss_N: 21.068426 loss_f: 16.997818\n",
      "326 Train Loss 540.6734\n",
      "327 loss_D: 501.18613 loss_N: 21.220255 loss_f: 19.523539\n",
      "327 Train Loss 541.92993\n",
      "328 loss_D: 502.0604 loss_N: 21.126984 loss_f: 17.007637\n",
      "328 Train Loss 540.195\n",
      "329 loss_D: 502.20422 loss_N: 20.911568 loss_f: 17.301872\n",
      "329 Train Loss 540.41766\n",
      "330 loss_D: 502.12057 loss_N: 21.029795 loss_f: 16.650255\n",
      "330 Train Loss 539.80066\n",
      "331 loss_D: 500.85953 loss_N: 20.822947 loss_f: 18.2377\n",
      "331 Train Loss 539.92017\n",
      "332 loss_D: 501.5076 loss_N: 20.929214 loss_f: 16.9349\n",
      "332 Train Loss 539.3717\n",
      "333 loss_D: 501.1077 loss_N: 20.718246 loss_f: 16.983465\n",
      "333 Train Loss 538.8094\n",
      "334 loss_D: 501.41748 loss_N: 20.594053 loss_f: 15.960792\n",
      "334 Train Loss 537.97235\n",
      "335 loss_D: 501.0908 loss_N: 20.196991 loss_f: 15.7565155\n",
      "335 Train Loss 537.0443\n",
      "336 loss_D: 499.54138 loss_N: 19.72143 loss_f: 16.922932\n",
      "336 Train Loss 536.1857\n",
      "337 loss_D: 499.2515 loss_N: 19.199688 loss_f: 16.813314\n",
      "337 Train Loss 535.26447\n",
      "338 loss_D: 498.6291 loss_N: 18.99701 loss_f: 16.540897\n",
      "338 Train Loss 534.167\n",
      "339 loss_D: 498.29202 loss_N: 19.170027 loss_f: 16.180815\n",
      "339 Train Loss 533.6428\n",
      "340 loss_D: 497.4513 loss_N: 18.824821 loss_f: 16.873478\n",
      "340 Train Loss 533.1496\n",
      "341 loss_D: 497.5199 loss_N: 18.87239 loss_f: 16.001951\n",
      "341 Train Loss 532.3942\n",
      "342 loss_D: 496.76868 loss_N: 18.76572 loss_f: 15.858879\n",
      "342 Train Loss 531.3933\n",
      "343 loss_D: 495.6568 loss_N: 18.710543 loss_f: 15.988703\n",
      "343 Train Loss 530.3561\n",
      "344 loss_D: 493.54224 loss_N: 18.819094 loss_f: 16.931944\n",
      "344 Train Loss 529.2933\n",
      "345 loss_D: 492.0018 loss_N: 18.823805 loss_f: 17.451765\n",
      "345 Train Loss 528.27734\n",
      "346 loss_D: 491.10028 loss_N: 19.147306 loss_f: 17.235846\n",
      "346 Train Loss 527.48346\n",
      "347 loss_D: 489.77945 loss_N: 19.384844 loss_f: 17.415028\n",
      "347 Train Loss 526.57935\n",
      "348 loss_D: 487.374 loss_N: 19.91454 loss_f: 17.71739\n",
      "348 Train Loss 525.0059\n",
      "349 loss_D: 483.51678 loss_N: 20.275267 loss_f: 20.30249\n",
      "349 Train Loss 524.09454\n",
      "350 loss_D: 483.68845 loss_N: 19.933975 loss_f: 19.412327\n",
      "350 Train Loss 523.0348\n",
      "351 loss_D: 483.22214 loss_N: 19.620811 loss_f: 19.597986\n",
      "351 Train Loss 522.4409\n",
      "352 loss_D: 481.92093 loss_N: 19.476437 loss_f: 20.408056\n",
      "352 Train Loss 521.8054\n",
      "353 loss_D: 480.64822 loss_N: 19.254324 loss_f: 20.771442\n",
      "353 Train Loss 520.674\n",
      "354 loss_D: 479.5184 loss_N: 19.323185 loss_f: 21.311281\n",
      "354 Train Loss 520.1529\n",
      "355 loss_D: 478.31348 loss_N: 19.516901 loss_f: 21.774874\n",
      "355 Train Loss 519.6053\n",
      "356 loss_D: 478.1741 loss_N: 19.523119 loss_f: 21.62304\n",
      "356 Train Loss 519.32025\n",
      "357 loss_D: 477.6117 loss_N: 19.653229 loss_f: 21.577518\n",
      "357 Train Loss 518.84247\n",
      "358 loss_D: 476.00638 loss_N: 19.527613 loss_f: 22.75674\n",
      "358 Train Loss 518.2907\n",
      "359 loss_D: 475.1503 loss_N: 19.771568 loss_f: 22.815647\n",
      "359 Train Loss 517.73755\n",
      "360 loss_D: 474.8112 loss_N: 19.64629 loss_f: 22.47692\n",
      "360 Train Loss 516.9344\n",
      "361 loss_D: 473.46017 loss_N: 19.08938 loss_f: 23.447496\n",
      "361 Train Loss 515.9971\n",
      "362 loss_D: 473.86588 loss_N: 18.932083 loss_f: 22.654057\n",
      "362 Train Loss 515.452\n",
      "363 loss_D: 472.28708 loss_N: 18.824488 loss_f: 23.919596\n",
      "363 Train Loss 515.0312\n",
      "364 loss_D: 472.75894 loss_N: 18.74879 loss_f: 22.719515\n",
      "364 Train Loss 514.22723\n",
      "365 loss_D: 472.16754 loss_N: 18.839231 loss_f: 21.962887\n",
      "365 Train Loss 512.96967\n",
      "366 loss_D: 470.4194 loss_N: 18.34513 loss_f: 22.943768\n",
      "366 Train Loss 511.70828\n",
      "367 loss_D: 469.24973 loss_N: 18.016405 loss_f: 22.96778\n",
      "367 Train Loss 510.23392\n",
      "368 loss_D: 466.9325 loss_N: 17.393211 loss_f: 25.459734\n",
      "368 Train Loss 509.78546\n",
      "369 loss_D: 465.8356 loss_N: 17.089022 loss_f: 26.024328\n",
      "369 Train Loss 508.94894\n",
      "370 loss_D: 468.4978 loss_N: 17.484674 loss_f: 23.070297\n",
      "370 Train Loss 509.0528\n",
      "371 loss_D: 467.02554 loss_N: 17.26773 loss_f: 23.7973\n",
      "371 Train Loss 508.09058\n",
      "372 loss_D: 466.6793 loss_N: 17.366474 loss_f: 23.504707\n",
      "372 Train Loss 507.55048\n",
      "373 loss_D: 466.15988 loss_N: 17.231537 loss_f: 23.764788\n",
      "373 Train Loss 507.15622\n",
      "374 loss_D: 464.92505 loss_N: 16.891066 loss_f: 24.516743\n",
      "374 Train Loss 506.33286\n",
      "375 loss_D: 463.1261 loss_N: 16.476494 loss_f: 25.44499\n",
      "375 Train Loss 505.04758\n",
      "376 loss_D: 460.581 loss_N: 16.137018 loss_f: 27.408556\n",
      "376 Train Loss 504.1266\n",
      "377 loss_D: 460.43362 loss_N: 16.134365 loss_f: 26.674643\n",
      "377 Train Loss 503.24265\n",
      "378 loss_D: 459.0774 loss_N: 16.095766 loss_f: 27.18265\n",
      "378 Train Loss 502.3558\n",
      "379 loss_D: 458.1103 loss_N: 16.138968 loss_f: 27.195549\n",
      "379 Train Loss 501.44482\n",
      "380 loss_D: 456.0612 loss_N: 16.158085 loss_f: 28.05158\n",
      "380 Train Loss 500.27084\n",
      "381 loss_D: 454.23618 loss_N: 16.106413 loss_f: 28.435968\n",
      "381 Train Loss 498.77856\n",
      "382 loss_D: 452.75885 loss_N: 15.6602125 loss_f: 29.691986\n",
      "382 Train Loss 498.11105\n",
      "383 loss_D: 452.125 loss_N: 15.99392 loss_f: 28.781055\n",
      "383 Train Loss 496.9\n",
      "384 loss_D: 451.8757 loss_N: 15.306573 loss_f: 28.14025\n",
      "384 Train Loss 495.32254\n",
      "385 loss_D: 451.4712 loss_N: 15.260447 loss_f: 27.592562\n",
      "385 Train Loss 494.3242\n",
      "386 loss_D: 448.37408 loss_N: 14.902716 loss_f: 29.501143\n",
      "386 Train Loss 492.77792\n",
      "387 loss_D: 445.4069 loss_N: 14.587199 loss_f: 30.740824\n",
      "387 Train Loss 490.7349\n",
      "388 loss_D: 443.86478 loss_N: 14.39984 loss_f: 28.870031\n",
      "388 Train Loss 487.13464\n",
      "389 loss_D: 440.22388 loss_N: 14.27757 loss_f: 27.722681\n",
      "389 Train Loss 482.22412\n",
      "390 loss_D: 435.73218 loss_N: 13.627416 loss_f: 31.368917\n",
      "390 Train Loss 480.72852\n",
      "391 loss_D: 438.14938 loss_N: 13.724859 loss_f: 28.57018\n",
      "391 Train Loss 480.44443\n",
      "392 loss_D: 437.8768 loss_N: 13.59741 loss_f: 25.951681\n",
      "392 Train Loss 477.4259\n",
      "393 loss_D: 435.2118 loss_N: 13.70524 loss_f: 27.395449\n",
      "393 Train Loss 476.31247\n",
      "394 loss_D: 435.3701 loss_N: 13.282092 loss_f: 25.894394\n",
      "394 Train Loss 474.54657\n",
      "395 loss_D: 434.6293 loss_N: 13.109812 loss_f: 25.730305\n",
      "395 Train Loss 473.46942\n",
      "396 loss_D: 434.19205 loss_N: 13.124305 loss_f: 24.554491\n",
      "396 Train Loss 471.87085\n",
      "397 loss_D: 432.43228 loss_N: 12.983362 loss_f: 26.116217\n",
      "397 Train Loss 471.53186\n",
      "398 loss_D: 432.64822 loss_N: 12.970219 loss_f: 24.404917\n",
      "398 Train Loss 470.02335\n",
      "399 loss_D: 431.3772 loss_N: 12.594389 loss_f: 25.333544\n",
      "399 Train Loss 469.30515\n",
      "400 loss_D: 429.7079 loss_N: 12.113622 loss_f: 26.257563\n",
      "400 Train Loss 468.07907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "401 loss_D: 426.47443 loss_N: 11.304972 loss_f: 29.915375\n",
      "401 Train Loss 467.69476\n",
      "402 loss_D: 427.2068 loss_N: 11.453528 loss_f: 27.574062\n",
      "402 Train Loss 466.23438\n",
      "403 loss_D: 427.3183 loss_N: 11.482709 loss_f: 26.730093\n",
      "403 Train Loss 465.5311\n",
      "404 loss_D: 427.3791 loss_N: 11.459334 loss_f: 25.753704\n",
      "404 Train Loss 464.5921\n",
      "405 loss_D: 426.3801 loss_N: 11.071519 loss_f: 25.618324\n",
      "405 Train Loss 463.06995\n",
      "406 loss_D: 425.22998 loss_N: 10.71654 loss_f: 25.08373\n",
      "406 Train Loss 461.03027\n",
      "407 loss_D: 424.61215 loss_N: 10.404213 loss_f: 23.634819\n",
      "407 Train Loss 458.65118\n",
      "408 loss_D: 422.20874 loss_N: 10.105687 loss_f: 26.218863\n",
      "408 Train Loss 458.5333\n",
      "409 loss_D: 423.2924 loss_N: 10.221178 loss_f: 24.304108\n",
      "409 Train Loss 457.8177\n",
      "410 loss_D: 423.45493 loss_N: 10.280927 loss_f: 23.19353\n",
      "410 Train Loss 456.92938\n",
      "411 loss_D: 423.93524 loss_N: 10.360452 loss_f: 21.895834\n",
      "411 Train Loss 456.19153\n",
      "412 loss_D: 423.512 loss_N: 10.313881 loss_f: 21.814896\n",
      "412 Train Loss 455.64075\n",
      "413 loss_D: 422.64584 loss_N: 10.131114 loss_f: 21.928686\n",
      "413 Train Loss 454.70563\n",
      "414 loss_D: 421.56125 loss_N: 9.930505 loss_f: 22.181915\n",
      "414 Train Loss 453.67368\n",
      "415 loss_D: 419.87265 loss_N: 9.603983 loss_f: 22.520338\n",
      "415 Train Loss 451.99695\n",
      "416 loss_D: 416.81534 loss_N: 9.126422 loss_f: 23.864145\n",
      "416 Train Loss 449.8059\n",
      "417 loss_D: 415.35214 loss_N: 8.8726635 loss_f: 23.426456\n",
      "417 Train Loss 447.65125\n",
      "418 loss_D: 410.91776 loss_N: 8.245386 loss_f: 26.560946\n",
      "418 Train Loss 445.7241\n",
      "419 loss_D: 407.48804 loss_N: 7.9230905 loss_f: 27.559261\n",
      "419 Train Loss 442.9704\n",
      "420 loss_D: 407.16824 loss_N: 7.879643 loss_f: 25.90956\n",
      "420 Train Loss 440.95743\n",
      "421 loss_D: 405.44928 loss_N: 7.6799836 loss_f: 26.373\n",
      "421 Train Loss 439.50226\n",
      "422 loss_D: 402.4489 loss_N: 7.4400992 loss_f: 28.610386\n",
      "422 Train Loss 438.4994\n",
      "423 loss_D: 403.9354 loss_N: 7.4612617 loss_f: 25.45789\n",
      "423 Train Loss 436.85455\n",
      "424 loss_D: 402.91965 loss_N: 7.123111 loss_f: 25.59018\n",
      "424 Train Loss 435.63293\n",
      "425 loss_D: 401.57785 loss_N: 6.980376 loss_f: 26.206331\n",
      "425 Train Loss 434.76456\n",
      "426 loss_D: 399.2283 loss_N: 6.6812186 loss_f: 27.604881\n",
      "426 Train Loss 433.5144\n",
      "427 loss_D: 397.57263 loss_N: 6.4494047 loss_f: 28.851284\n",
      "427 Train Loss 432.87332\n",
      "428 loss_D: 397.05014 loss_N: 6.383724 loss_f: 28.088303\n",
      "428 Train Loss 431.52216\n",
      "429 loss_D: 395.3406 loss_N: 6.167875 loss_f: 29.224117\n",
      "429 Train Loss 430.7326\n",
      "430 loss_D: 392.19965 loss_N: 5.9029365 loss_f: 31.827477\n",
      "430 Train Loss 429.93005\n",
      "431 loss_D: 391.6837 loss_N: 5.982273 loss_f: 30.765734\n",
      "431 Train Loss 428.4317\n",
      "432 loss_D: 390.55606 loss_N: 5.9682 loss_f: 31.07179\n",
      "432 Train Loss 427.59604\n",
      "433 loss_D: 390.035 loss_N: 6.082537 loss_f: 31.136044\n",
      "433 Train Loss 427.2536\n",
      "434 loss_D: 387.097 loss_N: 5.967358 loss_f: 32.75235\n",
      "434 Train Loss 425.81668\n",
      "435 loss_D: 386.4008 loss_N: 5.910908 loss_f: 32.79403\n",
      "435 Train Loss 425.10574\n",
      "436 loss_D: 385.6762 loss_N: 5.7920012 loss_f: 32.922092\n",
      "436 Train Loss 424.3903\n",
      "437 loss_D: 385.87433 loss_N: 5.74176 loss_f: 31.706211\n",
      "437 Train Loss 423.3223\n",
      "438 loss_D: 383.68625 loss_N: 5.6802444 loss_f: 33.16397\n",
      "438 Train Loss 422.53046\n",
      "439 loss_D: 383.41953 loss_N: 5.6646957 loss_f: 32.52991\n",
      "439 Train Loss 421.61414\n",
      "440 loss_D: 382.51358 loss_N: 5.7002454 loss_f: 32.098114\n",
      "440 Train Loss 420.31195\n",
      "441 loss_D: 381.6958 loss_N: 5.9007173 loss_f: 31.21688\n",
      "441 Train Loss 418.81342\n",
      "442 loss_D: 381.14554 loss_N: 6.1736684 loss_f: 29.582268\n",
      "442 Train Loss 416.9015\n",
      "443 loss_D: 378.11435 loss_N: 5.9166036 loss_f: 30.514843\n",
      "443 Train Loss 414.54578\n",
      "444 loss_D: 376.8409 loss_N: 5.461935 loss_f: 30.491749\n",
      "444 Train Loss 412.79462\n",
      "445 loss_D: 374.30225 loss_N: 4.957989 loss_f: 32.129223\n",
      "445 Train Loss 411.38943\n",
      "446 loss_D: 374.70364 loss_N: 4.853412 loss_f: 30.027473\n",
      "446 Train Loss 409.58453\n",
      "447 loss_D: 373.35593 loss_N: 4.7945356 loss_f: 29.758806\n",
      "447 Train Loss 407.90927\n",
      "448 loss_D: 370.37555 loss_N: 4.7721305 loss_f: 29.951006\n",
      "448 Train Loss 405.0987\n",
      "449 loss_D: 363.81378 loss_N: 4.4803686 loss_f: 35.09083\n",
      "449 Train Loss 403.38498\n",
      "450 loss_D: 362.17935 loss_N: 4.365991 loss_f: 36.29681\n",
      "450 Train Loss 402.84216\n",
      "451 loss_D: 364.075 loss_N: 4.588524 loss_f: 32.67233\n",
      "451 Train Loss 401.33588\n",
      "452 loss_D: 363.07608 loss_N: 4.4914174 loss_f: 32.292873\n",
      "452 Train Loss 399.86038\n",
      "453 loss_D: 361.22556 loss_N: 4.384656 loss_f: 33.37297\n",
      "453 Train Loss 398.98315\n",
      "454 loss_D: 358.56235 loss_N: 4.1975045 loss_f: 34.77899\n",
      "454 Train Loss 397.53885\n",
      "455 loss_D: 357.5294 loss_N: 4.0207696 loss_f: 35.01817\n",
      "455 Train Loss 396.56833\n",
      "456 loss_D: 356.77585 loss_N: 4.0198197 loss_f: 34.902245\n",
      "456 Train Loss 395.6979\n",
      "457 loss_D: 358.11465 loss_N: 4.1166205 loss_f: 32.666737\n",
      "457 Train Loss 394.898\n",
      "458 loss_D: 357.6323 loss_N: 4.096789 loss_f: 32.52153\n",
      "458 Train Loss 394.2506\n",
      "459 loss_D: 357.47025 loss_N: 4.1187983 loss_f: 31.858511\n",
      "459 Train Loss 393.44757\n",
      "460 loss_D: 356.08344 loss_N: 4.208045 loss_f: 31.734243\n",
      "460 Train Loss 392.02573\n",
      "461 loss_D: 353.30292 loss_N: 4.618425 loss_f: 32.97081\n",
      "461 Train Loss 390.89215\n",
      "462 loss_D: 354.11325 loss_N: 4.699222 loss_f: 30.100843\n",
      "462 Train Loss 388.9133\n",
      "463 loss_D: 351.51126 loss_N: 4.422557 loss_f: 31.260334\n",
      "463 Train Loss 387.19415\n",
      "464 loss_D: 347.63138 loss_N: 4.293588 loss_f: 33.216553\n",
      "464 Train Loss 385.1415\n",
      "465 loss_D: 343.73163 loss_N: 4.167872 loss_f: 35.6529\n",
      "465 Train Loss 383.5524\n",
      "466 loss_D: 340.55295 loss_N: 4.1516323 loss_f: 38.26188\n",
      "466 Train Loss 382.96646\n",
      "467 loss_D: 342.5099 loss_N: 4.15313 loss_f: 34.920643\n",
      "467 Train Loss 381.58368\n",
      "468 loss_D: 343.22348 loss_N: 4.22237 loss_f: 33.041943\n",
      "468 Train Loss 380.4878\n",
      "469 loss_D: 340.69376 loss_N: 4.378516 loss_f: 34.256207\n",
      "469 Train Loss 379.32846\n",
      "470 loss_D: 339.4455 loss_N: 4.3681912 loss_f: 34.06451\n",
      "470 Train Loss 377.8782\n",
      "471 loss_D: 338.2876 loss_N: 4.309359 loss_f: 34.482044\n",
      "471 Train Loss 377.079\n",
      "472 loss_D: 339.1991 loss_N: 4.356694 loss_f: 31.31009\n",
      "472 Train Loss 374.86588\n",
      "473 loss_D: 337.219 loss_N: 4.4489985 loss_f: 31.168781\n",
      "473 Train Loss 372.8368\n",
      "474 loss_D: 335.266 loss_N: 4.655592 loss_f: 30.826468\n",
      "474 Train Loss 370.74805\n",
      "475 loss_D: 334.0412 loss_N: 4.8543105 loss_f: 29.514387\n",
      "475 Train Loss 368.40988\n",
      "476 loss_D: 332.83795 loss_N: 4.786773 loss_f: 28.976572\n",
      "476 Train Loss 366.6013\n",
      "477 loss_D: 331.9709 loss_N: 4.630658 loss_f: 28.563787\n",
      "477 Train Loss 365.1653\n",
      "478 loss_D: 330.29343 loss_N: 4.537319 loss_f: 28.756706\n",
      "478 Train Loss 363.58746\n",
      "479 loss_D: 328.7394 loss_N: 4.5885234 loss_f: 28.432047\n",
      "479 Train Loss 361.75998\n",
      "480 loss_D: 327.41913 loss_N: 4.7965117 loss_f: 27.575205\n",
      "480 Train Loss 359.79083\n",
      "481 loss_D: 323.99188 loss_N: 5.149706 loss_f: 29.57453\n",
      "481 Train Loss 358.71613\n",
      "482 loss_D: 322.2893 loss_N: 4.7250247 loss_f: 29.193998\n",
      "482 Train Loss 356.20834\n",
      "483 loss_D: 322.0285 loss_N: 4.77131 loss_f: 27.864393\n",
      "483 Train Loss 354.66418\n",
      "484 loss_D: 320.83035 loss_N: 4.7170305 loss_f: 27.528843\n",
      "484 Train Loss 353.07623\n",
      "485 loss_D: 321.29553 loss_N: 4.713788 loss_f: 25.973162\n",
      "485 Train Loss 351.98248\n",
      "486 loss_D: 319.949 loss_N: 4.544582 loss_f: 25.767946\n",
      "486 Train Loss 350.26154\n",
      "487 loss_D: 319.71933 loss_N: 4.383619 loss_f: 25.180408\n",
      "487 Train Loss 349.28336\n",
      "488 loss_D: 316.65808 loss_N: 4.339823 loss_f: 27.543583\n",
      "488 Train Loss 348.54147\n",
      "489 loss_D: 317.00983 loss_N: 4.3416243 loss_f: 25.852312\n",
      "489 Train Loss 347.20377\n",
      "490 loss_D: 317.07535 loss_N: 4.3790874 loss_f: 24.8175\n",
      "490 Train Loss 346.27194\n",
      "491 loss_D: 315.8888 loss_N: 4.4546638 loss_f: 25.04507\n",
      "491 Train Loss 345.38852\n",
      "492 loss_D: 314.70715 loss_N: 4.434057 loss_f: 25.726511\n",
      "492 Train Loss 344.8677\n",
      "493 loss_D: 312.44968 loss_N: 4.5179935 loss_f: 27.06865\n",
      "493 Train Loss 344.03632\n",
      "494 loss_D: 312.05298 loss_N: 4.5704103 loss_f: 26.14609\n",
      "494 Train Loss 342.76947\n",
      "495 loss_D: 310.57712 loss_N: 4.603931 loss_f: 25.952028\n",
      "495 Train Loss 341.1331\n",
      "496 loss_D: 308.16443 loss_N: 4.68264 loss_f: 26.4405\n",
      "496 Train Loss 339.28757\n",
      "497 loss_D: 306.95398 loss_N: 4.745285 loss_f: 26.39784\n",
      "497 Train Loss 338.0971\n",
      "498 loss_D: 305.0466 loss_N: 4.843776 loss_f: 26.842913\n",
      "498 Train Loss 336.7333\n",
      "499 loss_D: 304.76962 loss_N: 4.825293 loss_f: 28.100706\n",
      "499 Train Loss 337.69562\n",
      "500 loss_D: 304.93243 loss_N: 4.8321266 loss_f: 26.32513\n",
      "500 Train Loss 336.0897\n",
      "501 loss_D: 304.88516 loss_N: 4.832284 loss_f: 25.547096\n",
      "501 Train Loss 335.26453\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "502 loss_D: 305.04364 loss_N: 4.67314 loss_f: 24.910746\n",
      "502 Train Loss 334.6275\n",
      "503 loss_D: 303.26648 loss_N: 4.7741704 loss_f: 25.68111\n",
      "503 Train Loss 333.72177\n",
      "504 loss_D: 302.7338 loss_N: 4.7191067 loss_f: 25.603188\n",
      "504 Train Loss 333.0561\n",
      "505 loss_D: 300.2641 loss_N: 4.6430144 loss_f: 26.826725\n",
      "505 Train Loss 331.73383\n",
      "506 loss_D: 299.47086 loss_N: 4.538486 loss_f: 26.662727\n",
      "506 Train Loss 330.67206\n",
      "507 loss_D: 298.53986 loss_N: 4.5680184 loss_f: 26.906134\n",
      "507 Train Loss 330.014\n",
      "508 loss_D: 298.91714 loss_N: 4.6325774 loss_f: 26.013632\n",
      "508 Train Loss 329.56335\n",
      "509 loss_D: 298.22293 loss_N: 4.649736 loss_f: 25.989399\n",
      "509 Train Loss 328.8621\n",
      "510 loss_D: 296.81567 loss_N: 4.7194705 loss_f: 26.228569\n",
      "510 Train Loss 327.76373\n",
      "511 loss_D: 294.37714 loss_N: 4.8343263 loss_f: 27.477945\n",
      "511 Train Loss 326.6894\n",
      "512 loss_D: 293.5558 loss_N: 4.9667296 loss_f: 26.851696\n",
      "512 Train Loss 325.3742\n",
      "513 loss_D: 292.48895 loss_N: 5.0420575 loss_f: 26.623875\n",
      "513 Train Loss 324.15488\n",
      "514 loss_D: 289.48514 loss_N: 5.365713 loss_f: 27.75191\n",
      "514 Train Loss 322.60278\n",
      "515 loss_D: 287.44922 loss_N: 5.445324 loss_f: 28.282236\n",
      "515 Train Loss 321.17676\n",
      "516 loss_D: 286.36993 loss_N: 5.54112 loss_f: 27.974972\n",
      "516 Train Loss 319.88602\n",
      "517 loss_D: 282.95734 loss_N: 5.974143 loss_f: 29.506262\n",
      "517 Train Loss 318.43774\n",
      "518 loss_D: 283.40436 loss_N: 5.7215075 loss_f: 27.828861\n",
      "518 Train Loss 316.9547\n",
      "519 loss_D: 280.91742 loss_N: 5.733806 loss_f: 28.781042\n",
      "519 Train Loss 315.43225\n",
      "520 loss_D: 281.4233 loss_N: 5.4334507 loss_f: 27.47805\n",
      "520 Train Loss 314.3348\n",
      "521 loss_D: 280.47894 loss_N: 5.462377 loss_f: 27.259325\n",
      "521 Train Loss 313.20065\n",
      "522 loss_D: 279.89438 loss_N: 5.4181848 loss_f: 27.24965\n",
      "522 Train Loss 312.5622\n",
      "523 loss_D: 278.96603 loss_N: 5.3000827 loss_f: 27.61309\n",
      "523 Train Loss 311.8792\n",
      "524 loss_D: 277.92325 loss_N: 5.174882 loss_f: 27.997812\n",
      "524 Train Loss 311.09595\n",
      "525 loss_D: 276.037 loss_N: 5.1185384 loss_f: 28.980324\n",
      "525 Train Loss 310.13583\n",
      "526 loss_D: 274.3913 loss_N: 5.013918 loss_f: 28.922684\n",
      "526 Train Loss 308.32788\n",
      "527 loss_D: 271.97418 loss_N: 5.033388 loss_f: 29.54904\n",
      "527 Train Loss 306.5566\n",
      "528 loss_D: 269.2257 loss_N: 5.0665555 loss_f: 30.654596\n",
      "528 Train Loss 304.94687\n",
      "529 loss_D: 269.55984 loss_N: 5.0072165 loss_f: 29.552156\n",
      "529 Train Loss 304.1192\n",
      "530 loss_D: 269.4756 loss_N: 4.9621825 loss_f: 29.42423\n",
      "530 Train Loss 303.862\n",
      "531 loss_D: 269.1292 loss_N: 4.967297 loss_f: 29.06564\n",
      "531 Train Loss 303.16214\n",
      "532 loss_D: 269.05978 loss_N: 4.9727645 loss_f: 28.717499\n",
      "532 Train Loss 302.75006\n",
      "533 loss_D: 269.0671 loss_N: 4.9193583 loss_f: 28.1428\n",
      "533 Train Loss 302.12927\n",
      "534 loss_D: 267.35318 loss_N: 4.874564 loss_f: 28.532118\n",
      "534 Train Loss 300.75986\n",
      "535 loss_D: 265.19556 loss_N: 4.756117 loss_f: 28.65095\n",
      "535 Train Loss 298.6026\n",
      "536 loss_D: 263.1813 loss_N: 4.718964 loss_f: 29.64113\n",
      "536 Train Loss 297.5414\n",
      "537 loss_D: 262.91525 loss_N: 4.662671 loss_f: 28.686481\n",
      "537 Train Loss 296.2644\n",
      "538 loss_D: 262.2822 loss_N: 4.7074966 loss_f: 28.920261\n",
      "538 Train Loss 295.90994\n",
      "539 loss_D: 262.26144 loss_N: 4.76792 loss_f: 28.190075\n",
      "539 Train Loss 295.21942\n",
      "540 loss_D: 261.12595 loss_N: 4.7428865 loss_f: 28.686668\n",
      "540 Train Loss 294.5555\n",
      "541 loss_D: 261.2635 loss_N: 4.7557173 loss_f: 27.634981\n",
      "541 Train Loss 293.65417\n",
      "542 loss_D: 259.7962 loss_N: 4.7922997 loss_f: 27.59682\n",
      "542 Train Loss 292.18533\n",
      "543 loss_D: 255.87845 loss_N: 4.9492903 loss_f: 29.627796\n",
      "543 Train Loss 290.45554\n",
      "544 loss_D: 254.63347 loss_N: 4.9716525 loss_f: 29.338755\n",
      "544 Train Loss 288.94388\n",
      "545 loss_D: 252.32002 loss_N: 4.9665165 loss_f: 29.300299\n",
      "545 Train Loss 286.58682\n",
      "546 loss_D: 249.04504 loss_N: 5.0695744 loss_f: 31.085234\n",
      "546 Train Loss 285.19986\n",
      "547 loss_D: 247.84479 loss_N: 5.049953 loss_f: 30.787786\n",
      "547 Train Loss 283.68253\n",
      "548 loss_D: 245.28435 loss_N: 5.057087 loss_f: 31.4907\n",
      "548 Train Loss 281.83212\n",
      "549 loss_D: 242.14627 loss_N: 5.0123158 loss_f: 32.995\n",
      "549 Train Loss 280.1536\n",
      "550 loss_D: 237.55495 loss_N: 4.895507 loss_f: 35.77904\n",
      "550 Train Loss 278.2295\n",
      "551 loss_D: 234.68877 loss_N: 4.740522 loss_f: 38.089756\n",
      "551 Train Loss 277.51904\n",
      "552 loss_D: 237.57182 loss_N: 4.425113 loss_f: 34.215126\n",
      "552 Train Loss 276.21207\n",
      "553 loss_D: 237.43849 loss_N: 4.3397336 loss_f: 32.80561\n",
      "553 Train Loss 274.58383\n",
      "554 loss_D: 234.65395 loss_N: 4.4212685 loss_f: 35.039986\n",
      "554 Train Loss 274.1152\n",
      "555 loss_D: 235.69984 loss_N: 4.4623404 loss_f: 33.214825\n",
      "555 Train Loss 273.377\n",
      "556 loss_D: 236.17313 loss_N: 4.3155837 loss_f: 32.0561\n",
      "556 Train Loss 272.5448\n",
      "557 loss_D: 235.37141 loss_N: 4.233189 loss_f: 32.332916\n",
      "557 Train Loss 271.9375\n",
      "558 loss_D: 233.92139 loss_N: 4.143166 loss_f: 33.11859\n",
      "558 Train Loss 271.18317\n",
      "559 loss_D: 233.19809 loss_N: 4.2003593 loss_f: 33.151173\n",
      "559 Train Loss 270.54962\n",
      "560 loss_D: 232.63068 loss_N: 4.2371926 loss_f: 32.556316\n",
      "560 Train Loss 269.4242\n",
      "561 loss_D: 231.077 loss_N: 4.4299574 loss_f: 32.19953\n",
      "561 Train Loss 267.70648\n",
      "562 loss_D: 228.2332 loss_N: 4.9605875 loss_f: 32.516537\n",
      "562 Train Loss 265.71033\n",
      "563 loss_D: 223.85555 loss_N: 5.2882023 loss_f: 35.19563\n",
      "563 Train Loss 264.3394\n",
      "564 loss_D: 224.8387 loss_N: 5.1716805 loss_f: 33.437866\n",
      "564 Train Loss 263.44824\n",
      "565 loss_D: 224.4581 loss_N: 5.2698655 loss_f: 32.529774\n",
      "565 Train Loss 262.25775\n",
      "566 loss_D: 221.8237 loss_N: 5.418697 loss_f: 34.318623\n",
      "566 Train Loss 261.56104\n",
      "567 loss_D: 220.71295 loss_N: 5.5922585 loss_f: 34.265\n",
      "567 Train Loss 260.5702\n",
      "568 loss_D: 219.45372 loss_N: 5.6996274 loss_f: 34.44665\n",
      "568 Train Loss 259.6\n",
      "569 loss_D: 218.3963 loss_N: 5.770628 loss_f: 34.647594\n",
      "569 Train Loss 258.8145\n",
      "570 loss_D: 217.4132 loss_N: 5.730969 loss_f: 34.742485\n",
      "570 Train Loss 257.88666\n",
      "571 loss_D: 216.64204 loss_N: 5.6926484 loss_f: 33.95601\n",
      "571 Train Loss 256.2907\n",
      "572 loss_D: 215.02245 loss_N: 5.7936916 loss_f: 33.661854\n",
      "572 Train Loss 254.47798\n",
      "573 loss_D: 215.9738 loss_N: 5.700192 loss_f: 32.052242\n",
      "573 Train Loss 253.72624\n",
      "574 loss_D: 214.32974 loss_N: 5.7437563 loss_f: 32.956615\n",
      "574 Train Loss 253.03012\n",
      "575 loss_D: 214.52946 loss_N: 5.847378 loss_f: 32.178375\n",
      "575 Train Loss 252.55522\n",
      "576 loss_D: 215.35522 loss_N: 5.6849265 loss_f: 31.091368\n",
      "576 Train Loss 252.13152\n",
      "577 loss_D: 215.46959 loss_N: 5.587595 loss_f: 30.627937\n",
      "577 Train Loss 251.68512\n",
      "578 loss_D: 215.17929 loss_N: 5.473757 loss_f: 30.627663\n",
      "578 Train Loss 251.2807\n",
      "579 loss_D: 214.7369 loss_N: 5.369601 loss_f: 30.495855\n",
      "579 Train Loss 250.60234\n",
      "580 loss_D: 213.23239 loss_N: 5.2931333 loss_f: 31.08916\n",
      "580 Train Loss 249.61469\n",
      "581 loss_D: 210.46759 loss_N: 5.2822943 loss_f: 32.384262\n",
      "581 Train Loss 248.13414\n",
      "582 loss_D: 207.7964 loss_N: 5.4762387 loss_f: 33.58538\n",
      "582 Train Loss 246.85803\n",
      "583 loss_D: 206.35199 loss_N: 5.6242027 loss_f: 33.881863\n",
      "583 Train Loss 245.85806\n",
      "584 loss_D: 207.04263 loss_N: 5.8757696 loss_f: 32.059135\n",
      "584 Train Loss 244.97754\n",
      "585 loss_D: 207.258 loss_N: 5.9488344 loss_f: 30.574806\n",
      "585 Train Loss 243.78165\n",
      "586 loss_D: 205.95107 loss_N: 5.993421 loss_f: 31.209677\n",
      "586 Train Loss 243.15416\n",
      "587 loss_D: 205.48853 loss_N: 5.9605765 loss_f: 31.186523\n",
      "587 Train Loss 242.63562\n",
      "588 loss_D: 203.96918 loss_N: 6.1048207 loss_f: 31.78584\n",
      "588 Train Loss 241.85985\n",
      "589 loss_D: 203.08276 loss_N: 6.057111 loss_f: 32.163956\n",
      "589 Train Loss 241.30383\n",
      "590 loss_D: 202.51396 loss_N: 6.048361 loss_f: 31.53905\n",
      "590 Train Loss 240.10136\n",
      "591 loss_D: 201.6126 loss_N: 5.903992 loss_f: 31.265612\n",
      "591 Train Loss 238.7822\n",
      "592 loss_D: 197.7324 loss_N: 5.925322 loss_f: 34.972404\n",
      "592 Train Loss 238.63013\n",
      "593 loss_D: 199.4209 loss_N: 5.912932 loss_f: 32.57533\n",
      "593 Train Loss 237.90916\n",
      "594 loss_D: 198.71841 loss_N: 5.6830716 loss_f: 32.36106\n",
      "594 Train Loss 236.76254\n",
      "595 loss_D: 197.43211 loss_N: 5.343007 loss_f: 32.794613\n",
      "595 Train Loss 235.56973\n",
      "596 loss_D: 196.42682 loss_N: 5.1383266 loss_f: 33.145607\n",
      "596 Train Loss 234.71075\n",
      "597 loss_D: 196.3909 loss_N: 5.0562663 loss_f: 32.131493\n",
      "597 Train Loss 233.57864\n",
      "598 loss_D: 195.55276 loss_N: 4.988204 loss_f: 31.751812\n",
      "598 Train Loss 232.29279\n",
      "599 loss_D: 193.88054 loss_N: 4.862353 loss_f: 32.248966\n",
      "599 Train Loss 230.99185\n",
      "600 loss_D: 192.99338 loss_N: 4.859632 loss_f: 32.00378\n",
      "600 Train Loss 229.8568\n",
      "601 loss_D: 191.0758 loss_N: 4.900944 loss_f: 33.0447\n",
      "601 Train Loss 229.02145\n",
      "602 loss_D: 190.95071 loss_N: 5.0092154 loss_f: 32.24879\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "602 Train Loss 228.20872\n",
      "603 loss_D: 189.9177 loss_N: 4.9706984 loss_f: 32.19075\n",
      "603 Train Loss 227.07915\n",
      "604 loss_D: 187.41832 loss_N: 4.8866844 loss_f: 33.3367\n",
      "604 Train Loss 225.64171\n",
      "605 loss_D: 186.89366 loss_N: 4.7791066 loss_f: 33.267994\n",
      "605 Train Loss 224.94077\n",
      "606 loss_D: 185.28677 loss_N: 4.746609 loss_f: 33.814655\n",
      "606 Train Loss 223.84804\n",
      "607 loss_D: 183.60818 loss_N: 4.7678614 loss_f: 33.82804\n",
      "607 Train Loss 222.2041\n",
      "608 loss_D: 181.26842 loss_N: 4.8455296 loss_f: 34.542213\n",
      "608 Train Loss 220.65616\n",
      "609 loss_D: 181.2939 loss_N: 4.8794265 loss_f: 32.87755\n",
      "609 Train Loss 219.05087\n",
      "610 loss_D: 180.1418 loss_N: 4.861766 loss_f: 32.48283\n",
      "610 Train Loss 217.4864\n",
      "611 loss_D: 178.38524 loss_N: 4.7438045 loss_f: 32.443054\n",
      "611 Train Loss 215.5721\n",
      "612 loss_D: 175.6287 loss_N: 4.587121 loss_f: 33.463776\n",
      "612 Train Loss 213.6796\n",
      "613 loss_D: 175.72862 loss_N: 4.3893676 loss_f: 32.26433\n",
      "613 Train Loss 212.38232\n",
      "614 loss_D: 173.73361 loss_N: 4.442887 loss_f: 33.75006\n",
      "614 Train Loss 211.92656\n",
      "615 loss_D: 174.64496 loss_N: 4.4761596 loss_f: 32.055668\n",
      "615 Train Loss 211.17679\n",
      "616 loss_D: 174.39104 loss_N: 4.4272633 loss_f: 31.990917\n",
      "616 Train Loss 210.80922\n",
      "617 loss_D: 173.76617 loss_N: 4.3829765 loss_f: 32.305832\n",
      "617 Train Loss 210.45499\n",
      "618 loss_D: 172.76967 loss_N: 4.347991 loss_f: 32.90105\n",
      "618 Train Loss 210.0187\n",
      "619 loss_D: 171.48825 loss_N: 4.322505 loss_f: 33.718864\n",
      "619 Train Loss 209.52963\n",
      "620 loss_D: 171.0051 loss_N: 4.307891 loss_f: 33.474026\n",
      "620 Train Loss 208.78702\n",
      "621 loss_D: 169.07393 loss_N: 4.2809525 loss_f: 34.041904\n",
      "621 Train Loss 207.39679\n",
      "622 loss_D: 166.90105 loss_N: 4.268352 loss_f: 34.774105\n",
      "622 Train Loss 205.94351\n",
      "623 loss_D: 166.10027 loss_N: 4.3323936 loss_f: 33.92036\n",
      "623 Train Loss 204.35303\n",
      "624 loss_D: 163.40811 loss_N: 4.389811 loss_f: 34.717842\n",
      "624 Train Loss 202.51578\n",
      "625 loss_D: 161.58527 loss_N: 4.47314 loss_f: 35.824886\n",
      "625 Train Loss 201.8833\n",
      "626 loss_D: 162.65909 loss_N: 4.505376 loss_f: 33.543804\n",
      "626 Train Loss 200.70827\n",
      "627 loss_D: 161.62767 loss_N: 4.424926 loss_f: 34.613174\n",
      "627 Train Loss 200.66577\n",
      "628 loss_D: 162.11467 loss_N: 4.459741 loss_f: 33.602325\n",
      "628 Train Loss 200.17674\n",
      "629 loss_D: 161.66919 loss_N: 4.4522934 loss_f: 33.501247\n",
      "629 Train Loss 199.62274\n",
      "630 loss_D: 161.0886 loss_N: 4.462602 loss_f: 33.087944\n",
      "630 Train Loss 198.63913\n",
      "631 loss_D: 160.6627 loss_N: 4.506869 loss_f: 32.456642\n",
      "631 Train Loss 197.62622\n",
      "632 loss_D: 158.85985 loss_N: 4.5162864 loss_f: 33.508724\n",
      "632 Train Loss 196.88486\n",
      "633 loss_D: 159.58762 loss_N: 4.465659 loss_f: 31.81141\n",
      "633 Train Loss 195.86469\n",
      "634 loss_D: 159.3675 loss_N: 4.3942327 loss_f: 31.293919\n",
      "634 Train Loss 195.05563\n",
      "635 loss_D: 158.7858 loss_N: 4.328606 loss_f: 31.140955\n",
      "635 Train Loss 194.25537\n",
      "636 loss_D: 158.11642 loss_N: 4.3337483 loss_f: 31.237469\n",
      "636 Train Loss 193.68765\n",
      "637 loss_D: 157.85912 loss_N: 4.3403583 loss_f: 30.94693\n",
      "637 Train Loss 193.14641\n",
      "638 loss_D: 156.91444 loss_N: 4.3697295 loss_f: 31.242653\n",
      "638 Train Loss 192.52682\n",
      "639 loss_D: 156.03677 loss_N: 4.395101 loss_f: 31.48\n",
      "639 Train Loss 191.91187\n",
      "640 loss_D: 154.6688 loss_N: 4.382886 loss_f: 31.901768\n",
      "640 Train Loss 190.95345\n",
      "641 loss_D: 151.47842 loss_N: 4.425069 loss_f: 33.9319\n",
      "641 Train Loss 189.83539\n",
      "642 loss_D: 150.31653 loss_N: 4.372631 loss_f: 33.57358\n",
      "642 Train Loss 188.26274\n",
      "643 loss_D: 146.85141 loss_N: 4.309124 loss_f: 35.00077\n",
      "643 Train Loss 186.16132\n",
      "644 loss_D: 145.01645 loss_N: 4.2584133 loss_f: 34.99545\n",
      "644 Train Loss 184.27031\n",
      "645 loss_D: 143.39075 loss_N: 4.303842 loss_f: 35.05827\n",
      "645 Train Loss 182.75287\n",
      "646 loss_D: 143.39995 loss_N: 4.306029 loss_f: 33.78362\n",
      "646 Train Loss 181.4896\n",
      "647 loss_D: 143.01576 loss_N: 4.299132 loss_f: 33.703053\n",
      "647 Train Loss 181.01794\n",
      "648 loss_D: 142.03339 loss_N: 4.2789264 loss_f: 33.52439\n",
      "648 Train Loss 179.8367\n",
      "649 loss_D: 141.78981 loss_N: 4.222106 loss_f: 33.233456\n",
      "649 Train Loss 179.24538\n",
      "650 loss_D: 140.65652 loss_N: 4.2056403 loss_f: 33.683754\n",
      "650 Train Loss 178.54593\n",
      "651 loss_D: 140.20605 loss_N: 4.141082 loss_f: 33.624863\n",
      "651 Train Loss 177.972\n",
      "652 loss_D: 138.82469 loss_N: 4.124151 loss_f: 34.516464\n",
      "652 Train Loss 177.4653\n",
      "653 loss_D: 138.31294 loss_N: 4.111304 loss_f: 34.309322\n",
      "653 Train Loss 176.73357\n",
      "654 loss_D: 136.17819 loss_N: 4.0550256 loss_f: 36.10834\n",
      "654 Train Loss 176.34155\n",
      "655 loss_D: 136.40239 loss_N: 4.101608 loss_f: 35.114494\n",
      "655 Train Loss 175.6185\n",
      "656 loss_D: 136.13809 loss_N: 4.1056547 loss_f: 34.95762\n",
      "656 Train Loss 175.20135\n",
      "657 loss_D: 136.07303 loss_N: 4.120661 loss_f: 34.84178\n",
      "657 Train Loss 175.03548\n",
      "658 loss_D: 135.51645 loss_N: 4.082553 loss_f: 35.114986\n",
      "658 Train Loss 174.71399\n",
      "659 loss_D: 135.33762 loss_N: 4.077339 loss_f: 35.0899\n",
      "659 Train Loss 174.50485\n",
      "660 loss_D: 135.23888 loss_N: 4.1045203 loss_f: 34.808224\n",
      "660 Train Loss 174.15163\n",
      "661 loss_D: 134.40585 loss_N: 4.143147 loss_f: 34.813175\n",
      "661 Train Loss 173.36217\n",
      "662 loss_D: 133.50305 loss_N: 4.1518607 loss_f: 35.066723\n",
      "662 Train Loss 172.72163\n",
      "663 loss_D: 132.55824 loss_N: 4.1394258 loss_f: 35.01655\n",
      "663 Train Loss 171.7142\n",
      "664 loss_D: 130.49207 loss_N: 4.062924 loss_f: 35.998844\n",
      "664 Train Loss 170.55383\n",
      "665 loss_D: 130.40536 loss_N: 4.02886 loss_f: 35.179573\n",
      "665 Train Loss 169.6138\n",
      "666 loss_D: 129.27553 loss_N: 3.9885287 loss_f: 35.464207\n",
      "666 Train Loss 168.72826\n",
      "667 loss_D: 128.13947 loss_N: 3.9157422 loss_f: 35.963234\n",
      "667 Train Loss 168.01843\n",
      "668 loss_D: 128.34526 loss_N: 3.8316097 loss_f: 35.13168\n",
      "668 Train Loss 167.30855\n",
      "669 loss_D: 128.1514 loss_N: 3.7898765 loss_f: 34.55128\n",
      "669 Train Loss 166.49255\n",
      "670 loss_D: 127.60958 loss_N: 3.7181425 loss_f: 34.4242\n",
      "670 Train Loss 165.75192\n",
      "671 loss_D: 127.405846 loss_N: 3.5755172 loss_f: 33.808918\n",
      "671 Train Loss 164.79028\n",
      "672 loss_D: 127.04684 loss_N: 3.4756112 loss_f: 33.055756\n",
      "672 Train Loss 163.5782\n",
      "673 loss_D: 126.907 loss_N: 3.352696 loss_f: 32.55711\n",
      "673 Train Loss 162.8168\n",
      "674 loss_D: 126.63862 loss_N: 3.317171 loss_f: 31.905796\n",
      "674 Train Loss 161.86159\n",
      "675 loss_D: 125.82194 loss_N: 3.3017218 loss_f: 32.403423\n",
      "675 Train Loss 161.52708\n",
      "676 loss_D: 126.35742 loss_N: 3.3431742 loss_f: 31.308031\n",
      "676 Train Loss 161.00862\n",
      "677 loss_D: 125.60235 loss_N: 3.3163726 loss_f: 31.055613\n",
      "677 Train Loss 159.97433\n",
      "678 loss_D: 125.326675 loss_N: 3.3325803 loss_f: 30.709002\n",
      "678 Train Loss 159.36826\n",
      "679 loss_D: 124.53267 loss_N: 3.3690093 loss_f: 30.938969\n",
      "679 Train Loss 158.84065\n",
      "680 loss_D: 124.76818 loss_N: 3.4253452 loss_f: 30.312418\n",
      "680 Train Loss 158.50595\n",
      "681 loss_D: 124.78574 loss_N: 3.400632 loss_f: 29.940897\n",
      "681 Train Loss 158.12727\n",
      "682 loss_D: 124.22214 loss_N: 3.356057 loss_f: 29.929575\n",
      "682 Train Loss 157.50777\n",
      "683 loss_D: 123.73311 loss_N: 3.319125 loss_f: 29.774893\n",
      "683 Train Loss 156.82712\n",
      "684 loss_D: 122.60311 loss_N: 3.3145156 loss_f: 30.306572\n",
      "684 Train Loss 156.2242\n",
      "685 loss_D: 122.16873 loss_N: 3.328364 loss_f: 30.242266\n",
      "685 Train Loss 155.73936\n",
      "686 loss_D: 121.729256 loss_N: 3.3446045 loss_f: 30.358788\n",
      "686 Train Loss 155.43265\n",
      "687 loss_D: 121.11905 loss_N: 3.3507366 loss_f: 30.591227\n",
      "687 Train Loss 155.06102\n",
      "688 loss_D: 120.71707 loss_N: 3.3502624 loss_f: 30.64163\n",
      "688 Train Loss 154.70897\n",
      "689 loss_D: 119.97032 loss_N: 3.3573334 loss_f: 30.734455\n",
      "689 Train Loss 154.0621\n",
      "690 loss_D: 118.86321 loss_N: 3.33886 loss_f: 31.08605\n",
      "690 Train Loss 153.28812\n",
      "691 loss_D: 118.7231 loss_N: 3.3590941 loss_f: 30.566906\n",
      "691 Train Loss 152.6491\n",
      "692 loss_D: 118.07275 loss_N: 3.3589382 loss_f: 30.831049\n",
      "692 Train Loss 152.26274\n",
      "693 loss_D: 118.20812 loss_N: 3.3615441 loss_f: 30.346312\n",
      "693 Train Loss 151.91597\n",
      "694 loss_D: 118.29815 loss_N: 3.372325 loss_f: 30.091356\n",
      "694 Train Loss 151.76183\n",
      "695 loss_D: 118.121925 loss_N: 3.3811007 loss_f: 30.023502\n",
      "695 Train Loss 151.52654\n",
      "696 loss_D: 117.69215 loss_N: 3.3985062 loss_f: 30.121153\n",
      "696 Train Loss 151.2118\n",
      "697 loss_D: 117.13195 loss_N: 3.3953738 loss_f: 30.139177\n",
      "697 Train Loss 150.6665\n",
      "698 loss_D: 116.20947 loss_N: 3.4019663 loss_f: 30.24726\n",
      "698 Train Loss 149.8587\n",
      "699 loss_D: 116.00662 loss_N: 3.4041116 loss_f: 29.715223\n",
      "699 Train Loss 149.12596\n",
      "700 loss_D: 115.57596 loss_N: 3.4065242 loss_f: 29.559673\n",
      "700 Train Loss 148.54216\n",
      "701 loss_D: 115.49945 loss_N: 3.4191616 loss_f: 29.103168\n",
      "701 Train Loss 148.02177\n",
      "702 loss_D: 115.35744 loss_N: 3.390414 loss_f: 28.802896\n",
      "702 Train Loss 147.55075\n",
      "703 loss_D: 114.98047 loss_N: 3.391953 loss_f: 28.74694\n",
      "703 Train Loss 147.11935\n",
      "704 loss_D: 114.86926 loss_N: 3.3438375 loss_f: 28.61853\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "704 Train Loss 146.83163\n",
      "705 loss_D: 114.432594 loss_N: 3.301586 loss_f: 28.699924\n",
      "705 Train Loss 146.4341\n",
      "706 loss_D: 113.83655 loss_N: 3.2142649 loss_f: 28.563974\n",
      "706 Train Loss 145.61479\n",
      "707 loss_D: 112.65898 loss_N: 3.0354898 loss_f: 28.670794\n",
      "707 Train Loss 144.36526\n",
      "708 loss_D: 111.74855 loss_N: 2.9416792 loss_f: 29.1841\n",
      "708 Train Loss 143.87433\n",
      "709 loss_D: 112.28957 loss_N: 2.9300451 loss_f: 27.49409\n",
      "709 Train Loss 142.71371\n",
      "710 loss_D: 111.513245 loss_N: 2.9228737 loss_f: 27.741894\n",
      "710 Train Loss 142.17801\n",
      "711 loss_D: 111.680595 loss_N: 2.939539 loss_f: 27.064\n",
      "711 Train Loss 141.68413\n",
      "712 loss_D: 111.71415 loss_N: 2.9340863 loss_f: 26.381863\n",
      "712 Train Loss 141.0301\n",
      "713 loss_D: 111.62962 loss_N: 2.9295237 loss_f: 26.088781\n",
      "713 Train Loss 140.64793\n",
      "714 loss_D: 111.53352 loss_N: 2.9197729 loss_f: 25.752594\n",
      "714 Train Loss 140.20589\n",
      "715 loss_D: 111.672325 loss_N: 2.9284558 loss_f: 25.15324\n",
      "715 Train Loss 139.75403\n",
      "716 loss_D: 112.0251 loss_N: 2.9110184 loss_f: 24.382175\n",
      "716 Train Loss 139.3183\n",
      "717 loss_D: 112.146576 loss_N: 2.8895383 loss_f: 23.777937\n",
      "717 Train Loss 138.81406\n",
      "718 loss_D: 111.98412 loss_N: 2.8755739 loss_f: 23.4185\n",
      "718 Train Loss 138.2782\n",
      "719 loss_D: 111.88202 loss_N: 2.8553884 loss_f: 22.9951\n",
      "719 Train Loss 137.7325\n",
      "720 loss_D: 111.60709 loss_N: 2.8090696 loss_f: 22.64738\n",
      "720 Train Loss 137.06354\n",
      "721 loss_D: 111.1458 loss_N: 2.8015444 loss_f: 22.473755\n",
      "721 Train Loss 136.4211\n",
      "722 loss_D: 110.856575 loss_N: 2.8107448 loss_f: 22.351902\n",
      "722 Train Loss 136.01923\n",
      "723 loss_D: 110.96262 loss_N: 2.8056276 loss_f: 21.737312\n",
      "723 Train Loss 135.50555\n",
      "724 loss_D: 110.64271 loss_N: 2.811624 loss_f: 21.597103\n",
      "724 Train Loss 135.05144\n",
      "725 loss_D: 110.26428 loss_N: 2.819217 loss_f: 21.581049\n",
      "725 Train Loss 134.66455\n",
      "726 loss_D: 109.6068 loss_N: 2.8373127 loss_f: 21.821157\n",
      "726 Train Loss 134.26526\n",
      "727 loss_D: 109.423195 loss_N: 2.8664064 loss_f: 21.587568\n",
      "727 Train Loss 133.87717\n",
      "728 loss_D: 109.0122 loss_N: 2.8675575 loss_f: 21.729374\n",
      "728 Train Loss 133.60913\n",
      "729 loss_D: 108.96516 loss_N: 2.8751488 loss_f: 21.551111\n",
      "729 Train Loss 133.39142\n",
      "730 loss_D: 108.72157 loss_N: 2.8906553 loss_f: 21.533957\n",
      "730 Train Loss 133.14618\n",
      "731 loss_D: 108.414185 loss_N: 2.9033527 loss_f: 21.613855\n",
      "731 Train Loss 132.9314\n",
      "732 loss_D: 107.914024 loss_N: 2.9007802 loss_f: 21.75829\n",
      "732 Train Loss 132.57309\n",
      "733 loss_D: 107.73737 loss_N: 2.8816524 loss_f: 21.614677\n",
      "733 Train Loss 132.2337\n",
      "734 loss_D: 107.48915 loss_N: 2.8754797 loss_f: 21.603338\n",
      "734 Train Loss 131.96797\n",
      "735 loss_D: 107.53307 loss_N: 2.8531382 loss_f: 21.27647\n",
      "735 Train Loss 131.66269\n",
      "736 loss_D: 107.39091 loss_N: 2.8362942 loss_f: 21.098728\n",
      "736 Train Loss 131.32593\n",
      "737 loss_D: 107.284 loss_N: 2.8143222 loss_f: 20.770815\n",
      "737 Train Loss 130.86914\n",
      "738 loss_D: 106.9537 loss_N: 2.7899466 loss_f: 20.672668\n",
      "738 Train Loss 130.41632\n",
      "739 loss_D: 106.504196 loss_N: 2.774632 loss_f: 20.64907\n",
      "739 Train Loss 129.9279\n",
      "740 loss_D: 105.82367 loss_N: 2.7695196 loss_f: 20.750341\n",
      "740 Train Loss 129.34352\n",
      "741 loss_D: 105.18962 loss_N: 2.76624 loss_f: 20.74629\n",
      "741 Train Loss 128.70215\n",
      "742 loss_D: 104.791374 loss_N: 2.784982 loss_f: 20.57526\n",
      "742 Train Loss 128.15161\n",
      "743 loss_D: 104.87872 loss_N: 2.7954621 loss_f: 19.800358\n",
      "743 Train Loss 127.47455\n",
      "744 loss_D: 104.51161 loss_N: 2.810236 loss_f: 19.56356\n",
      "744 Train Loss 126.88541\n",
      "745 loss_D: 104.97453 loss_N: 2.7877538 loss_f: 18.516838\n",
      "745 Train Loss 126.27912\n",
      "746 loss_D: 104.94416 loss_N: 2.7632334 loss_f: 18.221664\n",
      "746 Train Loss 125.92906\n",
      "747 loss_D: 104.91013 loss_N: 2.7628694 loss_f: 17.987015\n",
      "747 Train Loss 125.66002\n",
      "748 loss_D: 104.93522 loss_N: 2.7470565 loss_f: 17.747818\n",
      "748 Train Loss 125.43009\n",
      "749 loss_D: 104.97402 loss_N: 2.7456331 loss_f: 17.49034\n",
      "749 Train Loss 125.21\n",
      "750 loss_D: 104.77263 loss_N: 2.7573464 loss_f: 17.401793\n",
      "750 Train Loss 124.93177\n",
      "751 loss_D: 104.508896 loss_N: 2.7740724 loss_f: 17.326109\n",
      "751 Train Loss 124.60908\n",
      "752 loss_D: 104.279236 loss_N: 2.7671819 loss_f: 17.296604\n",
      "752 Train Loss 124.34302\n",
      "753 loss_D: 103.81168 loss_N: 2.7368577 loss_f: 17.501282\n",
      "753 Train Loss 124.04982\n",
      "754 loss_D: 103.56012 loss_N: 2.7124228 loss_f: 17.565956\n",
      "754 Train Loss 123.8385\n",
      "755 loss_D: 103.42276 loss_N: 2.6840491 loss_f: 17.493608\n",
      "755 Train Loss 123.60042\n",
      "756 loss_D: 103.230965 loss_N: 2.656701 loss_f: 17.391926\n",
      "756 Train Loss 123.27959\n",
      "757 loss_D: 102.92563 loss_N: 2.631035 loss_f: 17.320902\n",
      "757 Train Loss 122.87756\n",
      "758 loss_D: 102.517876 loss_N: 2.5977082 loss_f: 17.181181\n",
      "758 Train Loss 122.29677\n",
      "759 loss_D: 102.12986 loss_N: 2.5820417 loss_f: 16.959482\n",
      "759 Train Loss 121.67138\n",
      "760 loss_D: 101.04658 loss_N: 2.5248542 loss_f: 18.01728\n",
      "760 Train Loss 121.588715\n",
      "761 loss_D: 101.53499 loss_N: 2.550306 loss_f: 17.190802\n",
      "761 Train Loss 121.2761\n",
      "762 loss_D: 101.55949 loss_N: 2.5495055 loss_f: 16.78648\n",
      "762 Train Loss 120.89548\n",
      "763 loss_D: 101.37669 loss_N: 2.5565796 loss_f: 16.699709\n",
      "763 Train Loss 120.63298\n",
      "764 loss_D: 101.28484 loss_N: 2.5538483 loss_f: 16.548271\n",
      "764 Train Loss 120.38696\n",
      "765 loss_D: 100.97178 loss_N: 2.5428092 loss_f: 16.553408\n",
      "765 Train Loss 120.06799\n",
      "766 loss_D: 100.74766 loss_N: 2.53017 loss_f: 16.450388\n",
      "766 Train Loss 119.72821\n",
      "767 loss_D: 100.35983 loss_N: 2.524574 loss_f: 16.545023\n",
      "767 Train Loss 119.42943\n",
      "768 loss_D: 100.15255 loss_N: 2.5129328 loss_f: 16.550203\n",
      "768 Train Loss 119.21568\n",
      "769 loss_D: 100.21405 loss_N: 2.5095615 loss_f: 16.278324\n",
      "769 Train Loss 119.00194\n",
      "770 loss_D: 100.07533 loss_N: 2.5103767 loss_f: 16.212706\n",
      "770 Train Loss 118.798416\n",
      "771 loss_D: 100.07411 loss_N: 2.500469 loss_f: 16.051697\n",
      "771 Train Loss 118.626274\n",
      "772 loss_D: 100.013245 loss_N: 2.4824424 loss_f: 15.95129\n",
      "772 Train Loss 118.446976\n",
      "773 loss_D: 99.79876 loss_N: 2.4576893 loss_f: 15.973279\n",
      "773 Train Loss 118.22973\n",
      "774 loss_D: 99.64036 loss_N: 2.433371 loss_f: 15.99562\n",
      "774 Train Loss 118.06935\n",
      "775 loss_D: 99.48241 loss_N: 2.403296 loss_f: 15.948546\n",
      "775 Train Loss 117.83425\n",
      "776 loss_D: 99.17296 loss_N: 2.3570983 loss_f: 15.998043\n",
      "776 Train Loss 117.52811\n",
      "777 loss_D: 98.922646 loss_N: 2.3063965 loss_f: 15.878573\n",
      "777 Train Loss 117.10761\n",
      "778 loss_D: 98.53096 loss_N: 2.248018 loss_f: 15.84074\n",
      "778 Train Loss 116.61972\n",
      "779 loss_D: 98.11597 loss_N: 2.1884747 loss_f: 15.805851\n",
      "779 Train Loss 116.11029\n",
      "780 loss_D: 97.882095 loss_N: 2.1438103 loss_f: 15.640543\n",
      "780 Train Loss 115.66645\n",
      "781 loss_D: 97.58889 loss_N: 2.1057148 loss_f: 15.57852\n",
      "781 Train Loss 115.273125\n",
      "782 loss_D: 97.58012 loss_N: 2.097048 loss_f: 15.316664\n",
      "782 Train Loss 114.99383\n",
      "783 loss_D: 97.311485 loss_N: 2.077504 loss_f: 15.326256\n",
      "783 Train Loss 114.71525\n",
      "784 loss_D: 97.190704 loss_N: 2.079103 loss_f: 15.23562\n",
      "784 Train Loss 114.505424\n",
      "785 loss_D: 97.08721 loss_N: 2.0765812 loss_f: 15.058587\n",
      "785 Train Loss 114.22238\n",
      "786 loss_D: 96.74586 loss_N: 2.0627887 loss_f: 15.356674\n",
      "786 Train Loss 114.16532\n",
      "787 loss_D: 96.78455 loss_N: 2.047636 loss_f: 15.028424\n",
      "787 Train Loss 113.86062\n",
      "788 loss_D: 96.82714 loss_N: 2.0267608 loss_f: 14.80044\n",
      "788 Train Loss 113.65434\n",
      "789 loss_D: 96.76753 loss_N: 1.994615 loss_f: 14.627981\n",
      "789 Train Loss 113.39013\n",
      "790 loss_D: 96.61433 loss_N: 1.9561865 loss_f: 14.541153\n",
      "790 Train Loss 113.111664\n",
      "791 loss_D: 96.52686 loss_N: 1.917582 loss_f: 14.358235\n",
      "791 Train Loss 112.80268\n",
      "792 loss_D: 96.347244 loss_N: 1.8830717 loss_f: 14.272012\n",
      "792 Train Loss 112.50233\n",
      "793 loss_D: 96.39951 loss_N: 1.8664588 loss_f: 13.813844\n",
      "793 Train Loss 112.07981\n",
      "794 loss_D: 96.16025 loss_N: 1.8073128 loss_f: 13.674394\n",
      "794 Train Loss 111.64195\n",
      "795 loss_D: 96.196014 loss_N: 1.797106 loss_f: 13.394028\n",
      "795 Train Loss 111.387146\n",
      "796 loss_D: 96.01602 loss_N: 1.7713257 loss_f: 13.448767\n",
      "796 Train Loss 111.236115\n",
      "797 loss_D: 95.92777 loss_N: 1.7575738 loss_f: 13.447526\n",
      "797 Train Loss 111.13287\n",
      "798 loss_D: 95.85407 loss_N: 1.7539402 loss_f: 13.411953\n",
      "798 Train Loss 111.01996\n",
      "799 loss_D: 95.83306 loss_N: 1.746457 loss_f: 13.320768\n",
      "799 Train Loss 110.90029\n",
      "800 loss_D: 95.72616 loss_N: 1.731784 loss_f: 13.302825\n",
      "800 Train Loss 110.760765\n",
      "801 loss_D: 95.67114 loss_N: 1.7104337 loss_f: 13.209063\n",
      "801 Train Loss 110.59064\n",
      "802 loss_D: 95.55016 loss_N: 1.6527205 loss_f: 13.622144\n",
      "802 Train Loss 110.82503\n",
      "803 loss_D: 95.611115 loss_N: 1.6890551 loss_f: 13.181748\n",
      "803 Train Loss 110.48192\n",
      "804 loss_D: 95.54747 loss_N: 1.6601609 loss_f: 13.108964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "804 Train Loss 110.3166\n",
      "805 loss_D: 95.53579 loss_N: 1.6417886 loss_f: 12.989569\n",
      "805 Train Loss 110.16715\n",
      "806 loss_D: 95.50578 loss_N: 1.619883 loss_f: 12.904638\n",
      "806 Train Loss 110.030304\n",
      "807 loss_D: 95.46008 loss_N: 1.6088676 loss_f: 12.770826\n",
      "807 Train Loss 109.83978\n",
      "808 loss_D: 95.38962 loss_N: 1.5915978 loss_f: 12.525198\n",
      "808 Train Loss 109.50642\n",
      "809 loss_D: 95.313644 loss_N: 1.5840952 loss_f: 12.2099085\n",
      "809 Train Loss 109.10765\n",
      "810 loss_D: 95.036545 loss_N: 1.5412565 loss_f: 12.233278\n",
      "810 Train Loss 108.81108\n",
      "811 loss_D: 94.891884 loss_N: 1.5261638 loss_f: 12.205064\n",
      "811 Train Loss 108.62311\n",
      "812 loss_D: 94.80196 loss_N: 1.4948463 loss_f: 12.196975\n",
      "812 Train Loss 108.49379\n",
      "813 loss_D: 94.7143 loss_N: 1.4684796 loss_f: 12.18782\n",
      "813 Train Loss 108.370605\n",
      "814 loss_D: 94.61877 loss_N: 1.4389324 loss_f: 12.19821\n",
      "814 Train Loss 108.25591\n",
      "815 loss_D: 94.586174 loss_N: 1.4313635 loss_f: 12.116069\n",
      "815 Train Loss 108.133606\n",
      "816 loss_D: 94.53709 loss_N: 1.4230207 loss_f: 12.057204\n",
      "816 Train Loss 108.01731\n",
      "817 loss_D: 94.58916 loss_N: 1.4321803 loss_f: 11.851029\n",
      "817 Train Loss 107.87237\n",
      "818 loss_D: 94.635414 loss_N: 1.4453571 loss_f: 11.639648\n",
      "818 Train Loss 107.72042\n",
      "819 loss_D: 94.641174 loss_N: 1.4451675 loss_f: 11.507912\n",
      "819 Train Loss 107.59425\n",
      "820 loss_D: 94.6544 loss_N: 1.4441143 loss_f: 11.359853\n",
      "820 Train Loss 107.458374\n",
      "821 loss_D: 94.528595 loss_N: 1.4201055 loss_f: 11.309182\n",
      "821 Train Loss 107.25788\n",
      "822 loss_D: 94.363304 loss_N: 1.3882685 loss_f: 11.277855\n",
      "822 Train Loss 107.02943\n",
      "823 loss_D: 94.1567 loss_N: 1.3610215 loss_f: 11.280612\n",
      "823 Train Loss 106.79833\n",
      "824 loss_D: 93.95747 loss_N: 1.3273753 loss_f: 11.246168\n",
      "824 Train Loss 106.53102\n",
      "825 loss_D: 93.800316 loss_N: 1.3014026 loss_f: 11.179364\n",
      "825 Train Loss 106.28108\n",
      "826 loss_D: 93.59908 loss_N: 1.265135 loss_f: 11.118672\n",
      "826 Train Loss 105.982895\n",
      "827 loss_D: 93.29265 loss_N: 1.2310048 loss_f: 11.212238\n",
      "827 Train Loss 105.735886\n",
      "828 loss_D: 93.203575 loss_N: 1.1977772 loss_f: 10.907491\n",
      "828 Train Loss 105.308846\n",
      "829 loss_D: 93.461845 loss_N: 1.222899 loss_f: 10.430946\n",
      "829 Train Loss 105.11569\n",
      "830 loss_D: 93.30465 loss_N: 1.183816 loss_f: 10.284349\n",
      "830 Train Loss 104.77281\n",
      "831 loss_D: 93.103676 loss_N: 1.1543045 loss_f: 10.29747\n",
      "831 Train Loss 104.55545\n",
      "832 loss_D: 93.038635 loss_N: 1.1362705 loss_f: 10.181558\n",
      "832 Train Loss 104.35646\n",
      "833 loss_D: 92.9878 loss_N: 1.1235396 loss_f: 10.078695\n",
      "833 Train Loss 104.19004\n",
      "834 loss_D: 92.96866 loss_N: 1.1160183 loss_f: 10.003753\n",
      "834 Train Loss 104.08843\n",
      "835 loss_D: 92.85544 loss_N: 1.0974449 loss_f: 10.048453\n",
      "835 Train Loss 104.001335\n",
      "836 loss_D: 92.7137 loss_N: 1.0665927 loss_f: 10.126835\n",
      "836 Train Loss 103.90712\n",
      "837 loss_D: 92.668976 loss_N: 1.0583631 loss_f: 10.053983\n",
      "837 Train Loss 103.78133\n",
      "838 loss_D: 92.51307 loss_N: 1.0298822 loss_f: 10.111463\n",
      "838 Train Loss 103.65442\n",
      "839 loss_D: 92.33394 loss_N: 1.003871 loss_f: 10.182535\n",
      "839 Train Loss 103.52034\n",
      "840 loss_D: 92.12524 loss_N: 0.97475797 loss_f: 10.201337\n",
      "840 Train Loss 103.30133\n",
      "841 loss_D: 91.76031 loss_N: 0.9161137 loss_f: 10.25832\n",
      "841 Train Loss 102.93474\n",
      "842 loss_D: 91.39456 loss_N: 0.8652101 loss_f: 10.345278\n",
      "842 Train Loss 102.60505\n",
      "843 loss_D: 91.24361 loss_N: 0.84143776 loss_f: 10.1746235\n",
      "843 Train Loss 102.25967\n",
      "844 loss_D: 90.83242 loss_N: 0.780702 loss_f: 10.262478\n",
      "844 Train Loss 101.875595\n",
      "845 loss_D: 90.72195 loss_N: 0.76842374 loss_f: 9.959651\n",
      "845 Train Loss 101.45002\n",
      "846 loss_D: 90.66419 loss_N: 0.7540692 loss_f: 9.7149315\n",
      "846 Train Loss 101.133194\n",
      "847 loss_D: 90.4945 loss_N: 0.72816366 loss_f: 9.627467\n",
      "847 Train Loss 100.85013\n",
      "848 loss_D: 90.51717 loss_N: 0.7274262 loss_f: 9.411343\n",
      "848 Train Loss 100.655945\n",
      "849 loss_D: 90.460075 loss_N: 0.72293663 loss_f: 9.355207\n",
      "849 Train Loss 100.53822\n",
      "850 loss_D: 90.44881 loss_N: 0.7219249 loss_f: 9.231896\n",
      "850 Train Loss 100.402626\n",
      "851 loss_D: 90.34848 loss_N: 0.71152264 loss_f: 9.195122\n",
      "851 Train Loss 100.25513\n",
      "852 loss_D: 90.33358 loss_N: 0.7139495 loss_f: 9.116467\n",
      "852 Train Loss 100.164\n",
      "853 loss_D: 90.24796 loss_N: 0.7075058 loss_f: 9.111243\n",
      "853 Train Loss 100.06671\n",
      "854 loss_D: 90.203125 loss_N: 0.7068371 loss_f: 9.08051\n",
      "854 Train Loss 99.99048\n",
      "855 loss_D: 90.12749 loss_N: 0.6985529 loss_f: 9.08327\n",
      "855 Train Loss 99.90931\n",
      "856 loss_D: 90.014626 loss_N: 0.69374585 loss_f: 9.088222\n",
      "856 Train Loss 99.79659\n",
      "857 loss_D: 89.88891 loss_N: 0.68722445 loss_f: 9.090114\n",
      "857 Train Loss 99.666245\n",
      "858 loss_D: 89.69193 loss_N: 0.68270826 loss_f: 9.085253\n",
      "858 Train Loss 99.45989\n",
      "859 loss_D: 89.63538 loss_N: 0.68846726 loss_f: 8.913336\n",
      "859 Train Loss 99.23719\n",
      "860 loss_D: 89.59007 loss_N: 0.68716496 loss_f: 8.703142\n",
      "860 Train Loss 98.98038\n",
      "861 loss_D: 89.5943 loss_N: 0.6990542 loss_f: 8.486904\n",
      "861 Train Loss 98.78026\n",
      "862 loss_D: 89.6021 loss_N: 0.6895158 loss_f: 8.333372\n",
      "862 Train Loss 98.624985\n",
      "863 loss_D: 89.484375 loss_N: 0.68134296 loss_f: 8.303617\n",
      "863 Train Loss 98.46934\n",
      "864 loss_D: 89.38659 loss_N: 0.68074954 loss_f: 8.2787075\n",
      "864 Train Loss 98.34605\n",
      "865 loss_D: 89.199394 loss_N: 0.67222583 loss_f: 8.319212\n",
      "865 Train Loss 98.190834\n",
      "866 loss_D: 88.98991 loss_N: 0.68116623 loss_f: 8.378777\n",
      "866 Train Loss 98.04985\n",
      "867 loss_D: 88.84057 loss_N: 0.6909162 loss_f: 8.375705\n",
      "867 Train Loss 97.90719\n",
      "868 loss_D: 88.82084 loss_N: 0.67338145 loss_f: 8.262561\n",
      "868 Train Loss 97.756775\n",
      "869 loss_D: 88.83328 loss_N: 0.6559615 loss_f: 8.098388\n",
      "869 Train Loss 97.58763\n",
      "870 loss_D: 88.78261 loss_N: 0.64045566 loss_f: 8.041453\n",
      "870 Train Loss 97.464516\n",
      "871 loss_D: 88.61975 loss_N: 0.61560017 loss_f: 8.052178\n",
      "871 Train Loss 97.28753\n",
      "872 loss_D: 88.36308 loss_N: 0.5914747 loss_f: 8.135116\n",
      "872 Train Loss 97.089676\n",
      "873 loss_D: 88.0362 loss_N: 0.5684869 loss_f: 8.31451\n",
      "873 Train Loss 96.919205\n",
      "874 loss_D: 87.80168 loss_N: 0.55566007 loss_f: 8.4377985\n",
      "874 Train Loss 96.795135\n",
      "875 loss_D: 87.78021 loss_N: 0.5429709 loss_f: 8.358846\n",
      "875 Train Loss 96.68203\n",
      "876 loss_D: 87.478264 loss_N: 0.5217968 loss_f: 8.582834\n",
      "876 Train Loss 96.58289\n",
      "877 loss_D: 87.53471 loss_N: 0.5144325 loss_f: 8.444465\n",
      "877 Train Loss 96.493614\n",
      "878 loss_D: 87.62856 loss_N: 0.512122 loss_f: 8.279608\n",
      "878 Train Loss 96.420296\n",
      "879 loss_D: 87.56236 loss_N: 0.50575674 loss_f: 8.294514\n",
      "879 Train Loss 96.36264\n",
      "880 loss_D: 87.4201 loss_N: 0.49561122 loss_f: 8.358878\n",
      "880 Train Loss 96.27459\n",
      "881 loss_D: 87.20136 loss_N: 0.48801756 loss_f: 8.451248\n",
      "881 Train Loss 96.140625\n",
      "882 loss_D: 86.9935 loss_N: 0.4740411 loss_f: 8.561827\n",
      "882 Train Loss 96.02937\n",
      "883 loss_D: 86.8942 loss_N: 0.4746156 loss_f: 8.498441\n",
      "883 Train Loss 95.86726\n",
      "884 loss_D: 86.79337 loss_N: 0.47600317 loss_f: 8.433579\n",
      "884 Train Loss 95.70296\n",
      "885 loss_D: 86.6371 loss_N: 0.46939802 loss_f: 8.462649\n",
      "885 Train Loss 95.569145\n",
      "886 loss_D: 86.473816 loss_N: 0.46153072 loss_f: 8.472403\n",
      "886 Train Loss 95.40775\n",
      "887 loss_D: 86.26308 loss_N: 0.45376208 loss_f: 8.511906\n",
      "887 Train Loss 95.228745\n",
      "888 loss_D: 86.0178 loss_N: 0.44988272 loss_f: 8.546678\n",
      "888 Train Loss 95.01436\n",
      "889 loss_D: 85.84171 loss_N: 0.45188704 loss_f: 8.50465\n",
      "889 Train Loss 94.79825\n",
      "890 loss_D: 85.49203 loss_N: 0.45409477 loss_f: 8.628812\n",
      "890 Train Loss 94.574936\n",
      "891 loss_D: 85.47928 loss_N: 0.45303655 loss_f: 8.414703\n",
      "891 Train Loss 94.347015\n",
      "892 loss_D: 85.248 loss_N: 0.4573128 loss_f: 8.412389\n",
      "892 Train Loss 94.11771\n",
      "893 loss_D: 85.24172 loss_N: 0.4522748 loss_f: 8.258253\n",
      "893 Train Loss 93.952255\n",
      "894 loss_D: 85.10949 loss_N: 0.46205503 loss_f: 8.260415\n",
      "894 Train Loss 93.831955\n",
      "895 loss_D: 85.069084 loss_N: 0.46219358 loss_f: 8.167947\n",
      "895 Train Loss 93.69923\n",
      "896 loss_D: 84.95203 loss_N: 0.4612491 loss_f: 8.131684\n",
      "896 Train Loss 93.54496\n",
      "897 loss_D: 84.80625 loss_N: 0.4602705 loss_f: 8.1825\n",
      "897 Train Loss 93.44903\n",
      "898 loss_D: 84.69642 loss_N: 0.45779583 loss_f: 8.214375\n",
      "898 Train Loss 93.36859\n",
      "899 loss_D: 84.66484 loss_N: 0.45524013 loss_f: 8.174452\n",
      "899 Train Loss 93.29453\n",
      "900 loss_D: 84.590485 loss_N: 0.45455146 loss_f: 8.152738\n",
      "900 Train Loss 93.19778\n",
      "901 loss_D: 84.59485 loss_N: 0.45365176 loss_f: 8.055973\n",
      "901 Train Loss 93.10448\n",
      "902 loss_D: 84.49565 loss_N: 0.45742294 loss_f: 8.013816\n",
      "902 Train Loss 92.96689\n",
      "903 loss_D: 84.31291 loss_N: 0.4674577 loss_f: 7.9972343\n",
      "903 Train Loss 92.7776\n",
      "904 loss_D: 84.08246 loss_N: 0.48557347 loss_f: 8.009752\n",
      "904 Train Loss 92.57778\n",
      "905 loss_D: 83.74534 loss_N: 0.49440077 loss_f: 8.185128\n",
      "905 Train Loss 92.424866\n",
      "906 loss_D: 83.70558 loss_N: 0.4949809 loss_f: 8.065241\n",
      "906 Train Loss 92.2658\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "907 loss_D: 83.816795 loss_N: 0.4865361 loss_f: 7.855528\n",
      "907 Train Loss 92.15886\n",
      "908 loss_D: 83.66134 loss_N: 0.4783994 loss_f: 7.8829927\n",
      "908 Train Loss 92.022736\n",
      "909 loss_D: 83.64507 loss_N: 0.48415947 loss_f: 7.7628703\n",
      "909 Train Loss 91.892105\n",
      "910 loss_D: 83.56677 loss_N: 0.49253803 loss_f: 7.750156\n",
      "910 Train Loss 91.80946\n",
      "911 loss_D: 83.54572 loss_N: 0.4987316 loss_f: 7.708739\n",
      "911 Train Loss 91.7532\n",
      "912 loss_D: 83.51428 loss_N: 0.49677217 loss_f: 7.6661496\n",
      "912 Train Loss 91.67721\n",
      "913 loss_D: 83.40395 loss_N: 0.4872539 loss_f: 7.660701\n",
      "913 Train Loss 91.5519\n",
      "914 loss_D: 83.241554 loss_N: 0.48393708 loss_f: 7.7491984\n",
      "914 Train Loss 91.47469\n",
      "915 loss_D: 83.0028 loss_N: 0.47330302 loss_f: 7.8377156\n",
      "915 Train Loss 91.31382\n",
      "916 loss_D: 82.514046 loss_N: 0.46921667 loss_f: 8.082512\n",
      "916 Train Loss 91.06577\n",
      "917 loss_D: 82.18056 loss_N: 0.46987954 loss_f: 8.267138\n",
      "917 Train Loss 90.91757\n",
      "918 loss_D: 82.08558 loss_N: 0.46893635 loss_f: 8.149242\n",
      "918 Train Loss 90.70375\n",
      "919 loss_D: 81.96472 loss_N: 0.47997078 loss_f: 8.1398535\n",
      "919 Train Loss 90.58455\n",
      "920 loss_D: 82.00717 loss_N: 0.48442 loss_f: 7.990442\n",
      "920 Train Loss 90.48203\n",
      "921 loss_D: 81.96531 loss_N: 0.49214906 loss_f: 7.9530435\n",
      "921 Train Loss 90.4105\n",
      "922 loss_D: 81.877014 loss_N: 0.49475232 loss_f: 7.9714527\n",
      "922 Train Loss 90.343216\n",
      "923 loss_D: 81.68175 loss_N: 0.50252897 loss_f: 8.042387\n",
      "923 Train Loss 90.22666\n",
      "924 loss_D: 81.512276 loss_N: 0.5081573 loss_f: 8.110037\n",
      "924 Train Loss 90.13047\n",
      "925 loss_D: 81.206566 loss_N: 0.520951 loss_f: 8.281118\n",
      "925 Train Loss 90.00864\n",
      "926 loss_D: 81.02059 loss_N: 0.5329407 loss_f: 8.302431\n",
      "926 Train Loss 89.855965\n",
      "927 loss_D: 80.78055 loss_N: 0.5416817 loss_f: 8.420895\n",
      "927 Train Loss 89.74312\n",
      "928 loss_D: 80.6711 loss_N: 0.5425448 loss_f: 8.432322\n",
      "928 Train Loss 89.64596\n",
      "929 loss_D: 80.4518 loss_N: 0.55056685 loss_f: 8.5154505\n",
      "929 Train Loss 89.517815\n",
      "930 loss_D: 80.2771 loss_N: 0.547937 loss_f: 8.539522\n",
      "930 Train Loss 89.364555\n",
      "931 loss_D: 80.11755 loss_N: 0.55036014 loss_f: 8.567794\n",
      "931 Train Loss 89.23571\n",
      "932 loss_D: 80.00927 loss_N: 0.5485214 loss_f: 8.587001\n",
      "932 Train Loss 89.14479\n",
      "933 loss_D: 79.81771 loss_N: 0.5552583 loss_f: 8.678618\n",
      "933 Train Loss 89.05159\n",
      "934 loss_D: 79.79429 loss_N: 0.5592157 loss_f: 8.58691\n",
      "934 Train Loss 88.940414\n",
      "935 loss_D: 79.64908 loss_N: 0.57218987 loss_f: 8.619285\n",
      "935 Train Loss 88.84055\n",
      "936 loss_D: 79.73822 loss_N: 0.58289176 loss_f: 8.426572\n",
      "936 Train Loss 88.74769\n",
      "937 loss_D: 79.52831 loss_N: 0.6006179 loss_f: 8.50844\n",
      "937 Train Loss 88.63737\n",
      "938 loss_D: 79.47755 loss_N: 0.60460097 loss_f: 8.470958\n",
      "938 Train Loss 88.5531\n",
      "939 loss_D: 79.40678 loss_N: 0.60536885 loss_f: 8.454698\n",
      "939 Train Loss 88.46684\n",
      "940 loss_D: 79.43011 loss_N: 0.60202605 loss_f: 8.358343\n",
      "940 Train Loss 88.39048\n",
      "941 loss_D: 79.414185 loss_N: 0.6008244 loss_f: 8.304555\n",
      "941 Train Loss 88.319565\n",
      "942 loss_D: 79.35985 loss_N: 0.5968822 loss_f: 8.31789\n",
      "942 Train Loss 88.27463\n",
      "943 loss_D: 79.36169 loss_N: 0.59598595 loss_f: 8.278789\n",
      "943 Train Loss 88.23646\n",
      "944 loss_D: 79.27158 loss_N: 0.6036824 loss_f: 8.326309\n",
      "944 Train Loss 88.20158\n",
      "945 loss_D: 79.13344 loss_N: 0.60953027 loss_f: 8.405576\n",
      "945 Train Loss 88.148544\n",
      "946 loss_D: 78.953476 loss_N: 0.6212097 loss_f: 8.518634\n",
      "946 Train Loss 88.093315\n",
      "947 loss_D: 78.78205 loss_N: 0.63363624 loss_f: 8.604363\n",
      "947 Train Loss 88.02005\n",
      "948 loss_D: 78.543884 loss_N: 0.6485558 loss_f: 8.756563\n",
      "948 Train Loss 87.949005\n",
      "949 loss_D: 78.46243 loss_N: 0.65249383 loss_f: 8.755752\n",
      "949 Train Loss 87.87068\n",
      "950 loss_D: 78.51356 loss_N: 0.6441378 loss_f: 8.593819\n",
      "950 Train Loss 87.75151\n",
      "951 loss_D: 78.324585 loss_N: 0.6419325 loss_f: 8.619503\n",
      "951 Train Loss 87.58601\n",
      "952 loss_D: 78.20655 loss_N: 0.6291625 loss_f: 8.527124\n",
      "952 Train Loss 87.36284\n",
      "953 loss_D: 77.86623 loss_N: 0.63346696 loss_f: 8.613495\n",
      "953 Train Loss 87.11319\n",
      "954 loss_D: 77.72101 loss_N: 0.6161776 loss_f: 8.46012\n",
      "954 Train Loss 86.79731\n",
      "955 loss_D: 77.38251 loss_N: 0.6128814 loss_f: 8.497304\n",
      "955 Train Loss 86.4927\n",
      "956 loss_D: 76.99683 loss_N: 0.6018163 loss_f: 8.588108\n",
      "956 Train Loss 86.18675\n",
      "957 loss_D: 77.26467 loss_N: 0.58808726 loss_f: 8.130709\n",
      "957 Train Loss 85.98347\n",
      "958 loss_D: 77.02782 loss_N: 0.6014207 loss_f: 8.198284\n",
      "958 Train Loss 85.827515\n",
      "959 loss_D: 76.95668 loss_N: 0.59916276 loss_f: 8.1692\n",
      "959 Train Loss 85.72504\n",
      "960 loss_D: 76.80571 loss_N: 0.5990817 loss_f: 8.2527485\n",
      "960 Train Loss 85.65754\n",
      "961 loss_D: 76.64865 loss_N: 0.5968692 loss_f: 8.345608\n",
      "961 Train Loss 85.591125\n",
      "962 loss_D: 76.55732 loss_N: 0.5965752 loss_f: 8.371868\n",
      "962 Train Loss 85.52576\n",
      "963 loss_D: 76.41135 loss_N: 0.5954946 loss_f: 8.438703\n",
      "963 Train Loss 85.44555\n",
      "964 loss_D: 76.25211 loss_N: 0.5980838 loss_f: 8.509061\n",
      "964 Train Loss 85.35926\n",
      "965 loss_D: 76.05035 loss_N: 0.5926855 loss_f: 8.641827\n",
      "965 Train Loss 85.28485\n",
      "966 loss_D: 76.00511 loss_N: 0.5915129 loss_f: 8.587975\n",
      "966 Train Loss 85.1846\n",
      "967 loss_D: 76.01872 loss_N: 0.59525836 loss_f: 8.450619\n",
      "967 Train Loss 85.064606\n",
      "968 loss_D: 75.92406 loss_N: 0.5914905 loss_f: 8.458877\n",
      "968 Train Loss 84.97443\n",
      "969 loss_D: 75.93633 loss_N: 0.5852883 loss_f: 8.3718815\n",
      "969 Train Loss 84.8935\n",
      "970 loss_D: 75.82347 loss_N: 0.5777192 loss_f: 8.391466\n",
      "970 Train Loss 84.792656\n",
      "971 loss_D: 75.68114 loss_N: 0.54857135 loss_f: 8.593662\n",
      "971 Train Loss 84.823364\n",
      "972 loss_D: 75.75551 loss_N: 0.56412303 loss_f: 8.40077\n",
      "972 Train Loss 84.720406\n",
      "973 loss_D: 75.74914 loss_N: 0.55927354 loss_f: 8.3106985\n",
      "973 Train Loss 84.61911\n",
      "974 loss_D: 75.709984 loss_N: 0.55133194 loss_f: 8.240853\n",
      "974 Train Loss 84.50217\n",
      "975 loss_D: 75.59524 loss_N: 0.5526958 loss_f: 8.253848\n",
      "975 Train Loss 84.40178\n",
      "976 loss_D: 75.45386 loss_N: 0.5504732 loss_f: 8.269183\n",
      "976 Train Loss 84.27351\n",
      "977 loss_D: 75.29039 loss_N: 0.5409915 loss_f: 8.308752\n",
      "977 Train Loss 84.14014\n",
      "978 loss_D: 75.09826 loss_N: 0.5231502 loss_f: 8.304642\n",
      "978 Train Loss 83.92605\n",
      "979 loss_D: 74.734 loss_N: 0.4966281 loss_f: 8.498229\n",
      "979 Train Loss 83.72886\n",
      "980 loss_D: 74.725624 loss_N: 0.4766192 loss_f: 8.350856\n",
      "980 Train Loss 83.55309\n",
      "981 loss_D: 74.536835 loss_N: 0.47305927 loss_f: 8.434937\n",
      "981 Train Loss 83.44483\n",
      "982 loss_D: 74.479225 loss_N: 0.46893397 loss_f: 8.376531\n",
      "982 Train Loss 83.32469\n",
      "983 loss_D: 74.35254 loss_N: 0.47243363 loss_f: 8.393581\n",
      "983 Train Loss 83.21855\n",
      "984 loss_D: 74.248726 loss_N: 0.46319604 loss_f: 8.395157\n",
      "984 Train Loss 83.10708\n",
      "985 loss_D: 74.080376 loss_N: 0.45235732 loss_f: 8.450084\n",
      "985 Train Loss 82.98281\n",
      "986 loss_D: 73.903496 loss_N: 0.43262455 loss_f: 8.500686\n",
      "986 Train Loss 82.83681\n",
      "987 loss_D: 73.76582 loss_N: 0.41377598 loss_f: 8.502245\n",
      "987 Train Loss 82.68184\n",
      "988 loss_D: 73.602264 loss_N: 0.40510634 loss_f: 8.554632\n",
      "988 Train Loss 82.562004\n",
      "989 loss_D: 73.669014 loss_N: 0.39221972 loss_f: 8.267921\n",
      "989 Train Loss 82.329155\n",
      "990 loss_D: 73.586945 loss_N: 0.3887283 loss_f: 8.130628\n",
      "990 Train Loss 82.1063\n",
      "991 loss_D: 73.67566 loss_N: 0.3928667 loss_f: 7.8365827\n",
      "991 Train Loss 81.90511\n",
      "992 loss_D: 73.58349 loss_N: 0.3860837 loss_f: 7.7607217\n",
      "992 Train Loss 81.73029\n",
      "993 loss_D: 73.47484 loss_N: 0.383582 loss_f: 7.718901\n",
      "993 Train Loss 81.577324\n",
      "994 loss_D: 73.27203 loss_N: 0.37842113 loss_f: 7.730692\n",
      "994 Train Loss 81.38114\n",
      "995 loss_D: 73.16268 loss_N: 0.37766168 loss_f: 7.6745896\n",
      "995 Train Loss 81.214935\n",
      "996 loss_D: 72.70503 loss_N: 0.37549382 loss_f: 7.970542\n",
      "996 Train Loss 81.05107\n",
      "997 loss_D: 72.67323 loss_N: 0.3712284 loss_f: 7.8662686\n",
      "997 Train Loss 80.910736\n",
      "998 loss_D: 72.486664 loss_N: 0.3741799 loss_f: 7.906298\n",
      "998 Train Loss 80.76714\n",
      "999 loss_D: 72.26788 loss_N: 0.37812787 loss_f: 7.9205894\n",
      "999 Train Loss 80.566605\n",
      "1000 loss_D: 71.79645 loss_N: 0.3813952 loss_f: 8.127231\n",
      "1000 Train Loss 80.30507\n",
      "1001 loss_D: 71.41836 loss_N: 0.38521585 loss_f: 8.305882\n",
      "1001 Train Loss 80.10946\n",
      "1002 loss_D: 71.03187 loss_N: 0.38956085 loss_f: 8.509858\n",
      "1002 Train Loss 79.93129\n",
      "1003 loss_D: 70.866974 loss_N: 0.38776234 loss_f: 8.571101\n",
      "1003 Train Loss 79.825836\n",
      "1004 loss_D: 70.77701 loss_N: 0.39011735 loss_f: 8.546701\n",
      "1004 Train Loss 79.71382\n",
      "1005 loss_D: 70.48792 loss_N: 0.38822818 loss_f: 8.726898\n",
      "1005 Train Loss 79.60305\n",
      "1006 loss_D: 70.231384 loss_N: 0.38731888 loss_f: 8.879897\n",
      "1006 Train Loss 79.498604\n",
      "1007 loss_D: 69.84678 loss_N: 0.3901621 loss_f: 9.216115\n",
      "1007 Train Loss 79.45306\n",
      "1008 loss_D: 70.02836 loss_N: 0.38950303 loss_f: 8.959883\n",
      "1008 Train Loss 79.37775\n",
      "1009 loss_D: 70.0301 loss_N: 0.3903976 loss_f: 8.899296\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1009 Train Loss 79.31979\n",
      "1010 loss_D: 69.999954 loss_N: 0.38744348 loss_f: 8.879469\n",
      "1010 Train Loss 79.26687\n",
      "1011 loss_D: 69.80427 loss_N: 0.39001843 loss_f: 9.013314\n",
      "1011 Train Loss 79.207596\n",
      "1012 loss_D: 69.74217 loss_N: 0.39217728 loss_f: 8.991603\n",
      "1012 Train Loss 79.125946\n",
      "1013 loss_D: 69.66102 loss_N: 0.39158416 loss_f: 8.906326\n",
      "1013 Train Loss 78.95893\n",
      "1014 loss_D: 69.46101 loss_N: 0.39893538 loss_f: 8.960612\n",
      "1014 Train Loss 78.82056\n",
      "1015 loss_D: 69.4694 loss_N: 0.40045303 loss_f: 8.7332325\n",
      "1015 Train Loss 78.60308\n",
      "1016 loss_D: 69.383385 loss_N: 0.39772436 loss_f: 8.709037\n",
      "1016 Train Loss 78.49015\n",
      "1017 loss_D: 69.12939 loss_N: 0.40110704 loss_f: 8.812605\n",
      "1017 Train Loss 78.3431\n",
      "1018 loss_D: 68.91179 loss_N: 0.40897503 loss_f: 8.878229\n",
      "1018 Train Loss 78.19899\n",
      "1019 loss_D: 68.59317 loss_N: 0.41910374 loss_f: 9.0586815\n",
      "1019 Train Loss 78.07095\n",
      "1020 loss_D: 68.497345 loss_N: 0.4222326 loss_f: 9.059023\n",
      "1020 Train Loss 77.9786\n",
      "1021 loss_D: 68.456345 loss_N: 0.4272789 loss_f: 9.005133\n",
      "1021 Train Loss 77.888756\n",
      "1022 loss_D: 68.45393 loss_N: 0.42562503 loss_f: 8.892025\n",
      "1022 Train Loss 77.77159\n",
      "1023 loss_D: 68.4846 loss_N: 0.42032245 loss_f: 8.747938\n",
      "1023 Train Loss 77.65286\n",
      "1024 loss_D: 68.40316 loss_N: 0.41728574 loss_f: 8.709258\n",
      "1024 Train Loss 77.5297\n",
      "1025 loss_D: 68.170044 loss_N: 0.41546482 loss_f: 8.789086\n",
      "1025 Train Loss 77.374596\n",
      "1026 loss_D: 67.85571 loss_N: 0.41112846 loss_f: 8.974725\n",
      "1026 Train Loss 77.24156\n",
      "1027 loss_D: 67.47892 loss_N: 0.40506536 loss_f: 9.19928\n",
      "1027 Train Loss 77.08327\n",
      "1028 loss_D: 67.0708 loss_N: 0.3942726 loss_f: 9.442163\n",
      "1028 Train Loss 76.907234\n",
      "1029 loss_D: 66.65726 loss_N: 0.39174545 loss_f: 9.780781\n",
      "1029 Train Loss 76.82979\n",
      "1030 loss_D: 66.79975 loss_N: 0.38829643 loss_f: 9.465836\n",
      "1030 Train Loss 76.653885\n",
      "1031 loss_D: 66.7887 loss_N: 0.3881893 loss_f: 9.357513\n",
      "1031 Train Loss 76.5344\n",
      "1032 loss_D: 66.91282 loss_N: 0.3871194 loss_f: 9.07165\n",
      "1032 Train Loss 76.37159\n",
      "1033 loss_D: 66.6801 loss_N: 0.38963658 loss_f: 9.173742\n",
      "1033 Train Loss 76.24348\n",
      "1034 loss_D: 66.65935 loss_N: 0.39474213 loss_f: 9.096584\n",
      "1034 Train Loss 76.15068\n",
      "1035 loss_D: 66.3519 loss_N: 0.40177846 loss_f: 9.303259\n",
      "1035 Train Loss 76.05694\n",
      "1036 loss_D: 66.35536 loss_N: 0.40478626 loss_f: 9.181245\n",
      "1036 Train Loss 75.94139\n",
      "1037 loss_D: 66.329025 loss_N: 0.40744016 loss_f: 9.104478\n",
      "1037 Train Loss 75.84094\n",
      "1038 loss_D: 66.17816 loss_N: 0.4097747 loss_f: 9.17812\n",
      "1038 Train Loss 75.76605\n",
      "1039 loss_D: 66.1102 loss_N: 0.4173784 loss_f: 9.188905\n",
      "1039 Train Loss 75.716484\n",
      "1040 loss_D: 66.06763 loss_N: 0.41623402 loss_f: 9.165005\n",
      "1040 Train Loss 75.648865\n",
      "1041 loss_D: 65.97809 loss_N: 0.40978912 loss_f: 9.167475\n",
      "1041 Train Loss 75.55535\n",
      "1042 loss_D: 65.90985 loss_N: 0.41175228 loss_f: 9.166534\n",
      "1042 Train Loss 75.48814\n",
      "1043 loss_D: 65.9442 loss_N: 0.41254926 loss_f: 9.080147\n",
      "1043 Train Loss 75.4369\n",
      "1044 loss_D: 65.99258 loss_N: 0.40978485 loss_f: 8.98393\n",
      "1044 Train Loss 75.38629\n",
      "1045 loss_D: 66.045044 loss_N: 0.41292986 loss_f: 8.897277\n",
      "1045 Train Loss 75.355255\n",
      "1046 loss_D: 66.03376 loss_N: 0.41537187 loss_f: 8.865368\n",
      "1046 Train Loss 75.3145\n",
      "1047 loss_D: 66.12667 loss_N: 0.41962442 loss_f: 8.716516\n",
      "1047 Train Loss 75.26281\n",
      "1048 loss_D: 66.08641 loss_N: 0.4285038 loss_f: 8.673796\n",
      "1048 Train Loss 75.18871\n",
      "1049 loss_D: 66.12024 loss_N: 0.43147942 loss_f: 8.533587\n",
      "1049 Train Loss 75.085304\n",
      "1050 loss_D: 66.04315 loss_N: 0.427238 loss_f: 8.477497\n",
      "1050 Train Loss 74.94789\n",
      "1051 loss_D: 65.915764 loss_N: 0.42225584 loss_f: 8.468772\n",
      "1051 Train Loss 74.80679\n",
      "1052 loss_D: 65.64899 loss_N: 0.41096824 loss_f: 8.576035\n",
      "1052 Train Loss 74.63599\n",
      "1053 loss_D: 65.42434 loss_N: 0.4014044 loss_f: 8.595275\n",
      "1053 Train Loss 74.42102\n",
      "1054 loss_D: 65.217514 loss_N: 0.40322784 loss_f: 8.619785\n",
      "1054 Train Loss 74.240524\n",
      "1055 loss_D: 65.015785 loss_N: 0.4074386 loss_f: 8.697988\n",
      "1055 Train Loss 74.121216\n",
      "1056 loss_D: 65.082214 loss_N: 0.4086193 loss_f: 8.563981\n",
      "1056 Train Loss 74.05482\n",
      "1057 loss_D: 64.91483 loss_N: 0.40925303 loss_f: 8.659051\n",
      "1057 Train Loss 73.98314\n",
      "1058 loss_D: 64.87651 loss_N: 0.40605813 loss_f: 8.638362\n",
      "1058 Train Loss 73.92093\n",
      "1059 loss_D: 64.79466 loss_N: 0.40224674 loss_f: 8.624567\n",
      "1059 Train Loss 73.82147\n",
      "1060 loss_D: 64.65851 loss_N: 0.39424214 loss_f: 8.605472\n",
      "1060 Train Loss 73.65822\n",
      "1061 loss_D: 64.294815 loss_N: 0.3942642 loss_f: 8.808636\n",
      "1061 Train Loss 73.49771\n",
      "1062 loss_D: 64.030685 loss_N: 0.39417025 loss_f: 8.895101\n",
      "1062 Train Loss 73.31996\n",
      "1063 loss_D: 63.80413 loss_N: 0.3909769 loss_f: 8.9432955\n",
      "1063 Train Loss 73.138405\n",
      "1064 loss_D: 63.50621 loss_N: 0.3950317 loss_f: 9.0843525\n",
      "1064 Train Loss 72.985596\n",
      "1065 loss_D: 63.4217 loss_N: 0.396908 loss_f: 9.03163\n",
      "1065 Train Loss 72.850235\n",
      "1066 loss_D: 63.240044 loss_N: 0.39784047 loss_f: 9.075106\n",
      "1066 Train Loss 72.71299\n",
      "1067 loss_D: 63.12411 loss_N: 0.39872554 loss_f: 9.0818405\n",
      "1067 Train Loss 72.604675\n",
      "1068 loss_D: 63.078384 loss_N: 0.40020928 loss_f: 8.993126\n",
      "1068 Train Loss 72.47172\n",
      "1069 loss_D: 62.805855 loss_N: 0.39937755 loss_f: 9.174141\n",
      "1069 Train Loss 72.37937\n",
      "1070 loss_D: 62.84244 loss_N: 0.39758396 loss_f: 9.034409\n",
      "1070 Train Loss 72.27443\n",
      "1071 loss_D: 62.796585 loss_N: 0.39803815 loss_f: 8.959254\n",
      "1071 Train Loss 72.15388\n",
      "1072 loss_D: 62.779167 loss_N: 0.39537045 loss_f: 8.902505\n",
      "1072 Train Loss 72.07704\n",
      "1073 loss_D: 62.755116 loss_N: 0.3954787 loss_f: 8.846391\n",
      "1073 Train Loss 71.99699\n",
      "1074 loss_D: 62.73766 loss_N: 0.39605522 loss_f: 8.791655\n",
      "1074 Train Loss 71.92537\n",
      "1075 loss_D: 62.68999 loss_N: 0.39718997 loss_f: 8.782875\n",
      "1075 Train Loss 71.870056\n",
      "1076 loss_D: 62.69383 loss_N: 0.40230528 loss_f: 8.728853\n",
      "1076 Train Loss 71.82499\n",
      "1077 loss_D: 62.652336 loss_N: 0.40162143 loss_f: 8.725531\n",
      "1077 Train Loss 71.77949\n",
      "1078 loss_D: 62.67256 loss_N: 0.39968204 loss_f: 8.67082\n",
      "1078 Train Loss 71.743065\n",
      "1079 loss_D: 62.640884 loss_N: 0.39778295 loss_f: 8.6656885\n",
      "1079 Train Loss 71.70435\n",
      "1080 loss_D: 62.665974 loss_N: 0.39515936 loss_f: 8.595873\n",
      "1080 Train Loss 71.657005\n",
      "1081 loss_D: 62.66146 loss_N: 0.3929439 loss_f: 8.494828\n",
      "1081 Train Loss 71.54923\n",
      "1082 loss_D: 62.52442 loss_N: 0.3887247 loss_f: 8.4618025\n",
      "1082 Train Loss 71.37495\n",
      "1083 loss_D: 62.417294 loss_N: 0.3873494 loss_f: 8.431181\n",
      "1083 Train Loss 71.235825\n",
      "1084 loss_D: 62.304718 loss_N: 0.38571918 loss_f: 8.417624\n",
      "1084 Train Loss 71.10806\n",
      "1085 loss_D: 62.184837 loss_N: 0.38282612 loss_f: 8.442628\n",
      "1085 Train Loss 71.01029\n",
      "1086 loss_D: 62.15595 loss_N: 0.3817976 loss_f: 8.386379\n",
      "1086 Train Loss 70.924126\n",
      "1087 loss_D: 62.16341 loss_N: 0.37929082 loss_f: 8.271762\n",
      "1087 Train Loss 70.81446\n",
      "1088 loss_D: 62.13951 loss_N: 0.37994778 loss_f: 8.19793\n",
      "1088 Train Loss 70.71739\n",
      "1089 loss_D: 62.090637 loss_N: 0.37945372 loss_f: 8.169716\n",
      "1089 Train Loss 70.63981\n",
      "1090 loss_D: 62.183575 loss_N: 0.3765413 loss_f: 7.997399\n",
      "1090 Train Loss 70.55752\n",
      "1091 loss_D: 62.10095 loss_N: 0.375859 loss_f: 8.012471\n",
      "1091 Train Loss 70.48928\n",
      "1092 loss_D: 62.09698 loss_N: 0.3747578 loss_f: 7.9626427\n",
      "1092 Train Loss 70.43439\n",
      "1093 loss_D: 62.06491 loss_N: 0.37325916 loss_f: 7.937875\n",
      "1093 Train Loss 70.376045\n",
      "1094 loss_D: 62.048637 loss_N: 0.37387356 loss_f: 7.8983374\n",
      "1094 Train Loss 70.32085\n",
      "1095 loss_D: 61.985973 loss_N: 0.37676442 loss_f: 7.870628\n",
      "1095 Train Loss 70.23337\n",
      "1096 loss_D: 61.912712 loss_N: 0.38102886 loss_f: 7.858656\n",
      "1096 Train Loss 70.1524\n",
      "1097 loss_D: 61.827293 loss_N: 0.38619736 loss_f: 7.864546\n",
      "1097 Train Loss 70.07803\n",
      "1098 loss_D: 61.756824 loss_N: 0.3865036 loss_f: 7.8611875\n",
      "1098 Train Loss 70.00452\n",
      "1099 loss_D: 61.710205 loss_N: 0.38323647 loss_f: 7.8456655\n",
      "1099 Train Loss 69.93911\n",
      "1100 loss_D: 61.59371 loss_N: 0.38737017 loss_f: 7.830639\n",
      "1100 Train Loss 69.81172\n",
      "1101 loss_D: 61.479736 loss_N: 0.3908528 loss_f: 7.775078\n",
      "1101 Train Loss 69.64567\n",
      "1102 loss_D: 61.40207 loss_N: 0.39015648 loss_f: 7.764142\n",
      "1102 Train Loss 69.556366\n",
      "1103 loss_D: 61.252586 loss_N: 0.40913245 loss_f: 7.7783437\n",
      "1103 Train Loss 69.44006\n",
      "1104 loss_D: 61.271473 loss_N: 0.41267645 loss_f: 7.695905\n",
      "1104 Train Loss 69.38006\n",
      "1105 loss_D: 61.164112 loss_N: 0.41323853 loss_f: 7.738939\n",
      "1105 Train Loss 69.31629\n",
      "1106 loss_D: 61.14184 loss_N: 0.41087684 loss_f: 7.6937413\n",
      "1106 Train Loss 69.24646\n",
      "1107 loss_D: 61.074425 loss_N: 0.4082327 loss_f: 7.7082872\n",
      "1107 Train Loss 69.19095\n",
      "1108 loss_D: 61.079548 loss_N: 0.40193236 loss_f: 7.6603746\n",
      "1108 Train Loss 69.14185\n",
      "1109 loss_D: 61.10141 loss_N: 0.39590707 loss_f: 7.5879607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1109 Train Loss 69.08528\n",
      "1110 loss_D: 61.14709 loss_N: 0.39299873 loss_f: 7.497056\n",
      "1110 Train Loss 69.03715\n",
      "1111 loss_D: 61.14695 loss_N: 0.389046 loss_f: 7.448417\n",
      "1111 Train Loss 68.98441\n",
      "1112 loss_D: 61.140373 loss_N: 0.3916452 loss_f: 7.394048\n",
      "1112 Train Loss 68.92606\n",
      "1113 loss_D: 61.1088 loss_N: 0.39685583 loss_f: 7.3378\n",
      "1113 Train Loss 68.84345\n",
      "1114 loss_D: 60.8947 loss_N: 0.40617368 loss_f: 7.452688\n",
      "1114 Train Loss 68.75356\n",
      "1115 loss_D: 60.83041 loss_N: 0.41155547 loss_f: 7.4187098\n",
      "1115 Train Loss 68.660675\n",
      "1116 loss_D: 60.63271 loss_N: 0.42072967 loss_f: 7.5002875\n",
      "1116 Train Loss 68.55373\n",
      "1117 loss_D: 60.439438 loss_N: 0.42775413 loss_f: 7.6083117\n",
      "1117 Train Loss 68.4755\n",
      "1118 loss_D: 60.213028 loss_N: 0.43201327 loss_f: 7.7658324\n",
      "1118 Train Loss 68.41087\n",
      "1119 loss_D: 60.118286 loss_N: 0.43421575 loss_f: 7.756573\n",
      "1119 Train Loss 68.309074\n",
      "1120 loss_D: 60.04688 loss_N: 0.43323874 loss_f: 7.691\n",
      "1120 Train Loss 68.17112\n",
      "1121 loss_D: 59.970524 loss_N: 0.42949784 loss_f: 7.6024904\n",
      "1121 Train Loss 68.00251\n",
      "1122 loss_D: 59.931107 loss_N: 0.4220504 loss_f: 7.506911\n",
      "1122 Train Loss 67.86007\n",
      "1123 loss_D: 59.873924 loss_N: 0.42181137 loss_f: 7.458347\n",
      "1123 Train Loss 67.75408\n",
      "1124 loss_D: 59.90081 loss_N: 0.41471654 loss_f: 7.3656545\n",
      "1124 Train Loss 67.68118\n",
      "1125 loss_D: 59.807472 loss_N: 0.4164813 loss_f: 7.4140654\n",
      "1125 Train Loss 67.638016\n",
      "1126 loss_D: 59.73161 loss_N: 0.40987268 loss_f: 7.4562545\n",
      "1126 Train Loss 67.59774\n",
      "1127 loss_D: 59.617653 loss_N: 0.40448007 loss_f: 7.5275717\n",
      "1127 Train Loss 67.549706\n",
      "1128 loss_D: 59.53976 loss_N: 0.3949391 loss_f: 7.5704546\n",
      "1128 Train Loss 67.50516\n",
      "1129 loss_D: 59.4049 loss_N: 0.3886796 loss_f: 7.663494\n",
      "1129 Train Loss 67.45708\n",
      "1130 loss_D: 59.28558 loss_N: 0.37891516 loss_f: 7.718929\n",
      "1130 Train Loss 67.38342\n",
      "1131 loss_D: 59.091373 loss_N: 0.37510684 loss_f: 7.846187\n",
      "1131 Train Loss 67.31267\n",
      "1132 loss_D: 59.061035 loss_N: 0.37422958 loss_f: 7.8338757\n",
      "1132 Train Loss 67.26914\n",
      "1133 loss_D: 59.113197 loss_N: 0.37410015 loss_f: 7.752742\n",
      "1133 Train Loss 67.240036\n",
      "1134 loss_D: 59.084415 loss_N: 0.37374136 loss_f: 7.7539215\n",
      "1134 Train Loss 67.21208\n",
      "1135 loss_D: 59.091072 loss_N: 0.371801 loss_f: 7.7101164\n",
      "1135 Train Loss 67.17299\n",
      "1136 loss_D: 59.026062 loss_N: 0.3659394 loss_f: 7.718853\n",
      "1136 Train Loss 67.110855\n",
      "1137 loss_D: 58.92955 loss_N: 0.36030307 loss_f: 7.7429137\n",
      "1137 Train Loss 67.03277\n",
      "1138 loss_D: 58.828293 loss_N: 0.3591486 loss_f: 7.7194858\n",
      "1138 Train Loss 66.90693\n",
      "1139 loss_D: 58.695236 loss_N: 0.36256436 loss_f: 7.678683\n",
      "1139 Train Loss 66.73648\n",
      "1140 loss_D: 58.501835 loss_N: 0.36803076 loss_f: 7.6141186\n",
      "1140 Train Loss 66.483986\n",
      "1141 loss_D: 58.40584 loss_N: 0.3607795 loss_f: 7.6211047\n",
      "1141 Train Loss 66.387726\n",
      "1142 loss_D: 58.405792 loss_N: 0.35285127 loss_f: 7.5349092\n",
      "1142 Train Loss 66.29356\n",
      "1143 loss_D: 58.34356 loss_N: 0.34593534 loss_f: 7.506665\n",
      "1143 Train Loss 66.19616\n",
      "1144 loss_D: 58.32109 loss_N: 0.34318653 loss_f: 7.4313107\n",
      "1144 Train Loss 66.09559\n",
      "1145 loss_D: 58.292324 loss_N: 0.3481205 loss_f: 7.348544\n",
      "1145 Train Loss 65.98899\n",
      "1146 loss_D: 58.129875 loss_N: 0.3573837 loss_f: 7.399362\n",
      "1146 Train Loss 65.88662\n",
      "1147 loss_D: 58.030468 loss_N: 0.3609992 loss_f: 7.4057183\n",
      "1147 Train Loss 65.79719\n",
      "1148 loss_D: 57.938156 loss_N: 0.36646378 loss_f: 7.390218\n",
      "1148 Train Loss 65.69484\n",
      "1149 loss_D: 57.6684 loss_N: 0.36330053 loss_f: 7.5644374\n",
      "1149 Train Loss 65.59614\n",
      "1150 loss_D: 57.589573 loss_N: 0.35349768 loss_f: 7.5595217\n",
      "1150 Train Loss 65.502594\n",
      "1151 loss_D: 57.43985 loss_N: 0.3567597 loss_f: 7.575307\n",
      "1151 Train Loss 65.37192\n",
      "1152 loss_D: 57.211525 loss_N: 0.36260003 loss_f: 7.711886\n",
      "1152 Train Loss 65.28601\n",
      "1153 loss_D: 57.153473 loss_N: 0.36517403 loss_f: 7.7216625\n",
      "1153 Train Loss 65.24031\n",
      "1154 loss_D: 57.063225 loss_N: 0.36676928 loss_f: 7.7686715\n",
      "1154 Train Loss 65.19866\n",
      "1155 loss_D: 56.992466 loss_N: 0.36664975 loss_f: 7.7933874\n",
      "1155 Train Loss 65.152504\n",
      "1156 loss_D: 56.970062 loss_N: 0.36421177 loss_f: 7.758383\n",
      "1156 Train Loss 65.09266\n",
      "1157 loss_D: 56.969162 loss_N: 0.36329705 loss_f: 7.708003\n",
      "1157 Train Loss 65.04046\n",
      "1158 loss_D: 56.98543 loss_N: 0.3608443 loss_f: 7.6320233\n",
      "1158 Train Loss 64.9783\n",
      "1159 loss_D: 57.02442 loss_N: 0.35685706 loss_f: 7.542803\n",
      "1159 Train Loss 64.92408\n",
      "1160 loss_D: 57.04896 loss_N: 0.35446385 loss_f: 7.477865\n",
      "1160 Train Loss 64.881294\n",
      "1161 loss_D: 57.00695 loss_N: 0.35239947 loss_f: 7.4815264\n",
      "1161 Train Loss 64.84087\n",
      "1162 loss_D: 56.867584 loss_N: 0.34860682 loss_f: 7.5649676\n",
      "1162 Train Loss 64.78116\n",
      "1163 loss_D: 56.537094 loss_N: 0.34614158 loss_f: 7.936324\n",
      "1163 Train Loss 64.81956\n",
      "1164 loss_D: 56.724403 loss_N: 0.3473915 loss_f: 7.6612043\n",
      "1164 Train Loss 64.733\n",
      "1165 loss_D: 56.696297 loss_N: 0.35158232 loss_f: 7.6331577\n",
      "1165 Train Loss 64.68104\n",
      "1166 loss_D: 56.539272 loss_N: 0.3558621 loss_f: 7.731912\n",
      "1166 Train Loss 64.627045\n",
      "1167 loss_D: 56.460262 loss_N: 0.35462025 loss_f: 7.7623777\n",
      "1167 Train Loss 64.57726\n",
      "1168 loss_D: 56.320953 loss_N: 0.35514146 loss_f: 7.83477\n",
      "1168 Train Loss 64.510864\n",
      "1169 loss_D: 56.221973 loss_N: 0.35540655 loss_f: 7.888577\n",
      "1169 Train Loss 64.46596\n",
      "1170 loss_D: 56.211075 loss_N: 0.36187813 loss_f: 7.8696094\n",
      "1170 Train Loss 64.44256\n",
      "1171 loss_D: 56.122944 loss_N: 0.36076283 loss_f: 7.9330826\n",
      "1171 Train Loss 64.416794\n",
      "1172 loss_D: 56.16759 loss_N: 0.3665971 loss_f: 7.848014\n",
      "1172 Train Loss 64.3822\n",
      "1173 loss_D: 56.156586 loss_N: 0.3736902 loss_f: 7.815075\n",
      "1173 Train Loss 64.34535\n",
      "1174 loss_D: 56.14661 loss_N: 0.37902445 loss_f: 7.770676\n",
      "1174 Train Loss 64.29631\n",
      "1175 loss_D: 56.07228 loss_N: 0.3824856 loss_f: 7.78243\n",
      "1175 Train Loss 64.2372\n",
      "1176 loss_D: 55.99221 loss_N: 0.37484384 loss_f: 7.7930584\n",
      "1176 Train Loss 64.16011\n",
      "1177 loss_D: 55.931484 loss_N: 0.35992166 loss_f: 7.778979\n",
      "1177 Train Loss 64.07038\n",
      "1178 loss_D: 55.77859 loss_N: 0.35859013 loss_f: 7.836255\n",
      "1178 Train Loss 63.973434\n",
      "1179 loss_D: 55.816456 loss_N: 0.3297224 loss_f: 7.789179\n",
      "1179 Train Loss 63.935356\n",
      "1180 loss_D: 55.842834 loss_N: 0.3388652 loss_f: 7.611803\n",
      "1180 Train Loss 63.793503\n",
      "1181 loss_D: 55.853397 loss_N: 0.3514185 loss_f: 7.505501\n",
      "1181 Train Loss 63.710316\n",
      "1182 loss_D: 55.730125 loss_N: 0.35432318 loss_f: 7.529981\n",
      "1182 Train Loss 63.61443\n",
      "1183 loss_D: 55.59351 loss_N: 0.35882115 loss_f: 7.5455904\n",
      "1183 Train Loss 63.49792\n",
      "1184 loss_D: 55.41426 loss_N: 0.35102656 loss_f: 7.649625\n",
      "1184 Train Loss 63.414913\n",
      "1185 loss_D: 55.28512 loss_N: 0.3466409 loss_f: 7.697375\n",
      "1185 Train Loss 63.329136\n",
      "1186 loss_D: 55.263794 loss_N: 0.3324981 loss_f: 7.6497664\n",
      "1186 Train Loss 63.246056\n",
      "1187 loss_D: 55.166092 loss_N: 0.32386538 loss_f: 7.680285\n",
      "1187 Train Loss 63.170242\n",
      "1188 loss_D: 55.238712 loss_N: 0.316106 loss_f: 7.552892\n",
      "1188 Train Loss 63.107708\n",
      "1189 loss_D: 55.21849 loss_N: 0.30896094 loss_f: 7.5029793\n",
      "1189 Train Loss 63.03043\n",
      "1190 loss_D: 55.185673 loss_N: 0.2983303 loss_f: 7.4499426\n",
      "1190 Train Loss 62.933945\n",
      "1191 loss_D: 55.025543 loss_N: 0.2886435 loss_f: 7.513665\n",
      "1191 Train Loss 62.82785\n",
      "1192 loss_D: 54.871162 loss_N: 0.2775713 loss_f: 7.576587\n",
      "1192 Train Loss 62.725323\n",
      "1193 loss_D: 54.676025 loss_N: 0.27216524 loss_f: 7.690046\n",
      "1193 Train Loss 62.638233\n",
      "1194 loss_D: 54.664474 loss_N: 0.2693951 loss_f: 7.607828\n",
      "1194 Train Loss 62.541695\n",
      "1195 loss_D: 54.561035 loss_N: 0.26824123 loss_f: 7.599218\n",
      "1195 Train Loss 62.428493\n",
      "1196 loss_D: 54.657955 loss_N: 0.26935697 loss_f: 7.3910794\n",
      "1196 Train Loss 62.31839\n",
      "1197 loss_D: 54.74626 loss_N: 0.26824674 loss_f: 7.201275\n",
      "1197 Train Loss 62.215782\n",
      "1198 loss_D: 54.7571 loss_N: 0.26646662 loss_f: 7.0944004\n",
      "1198 Train Loss 62.117966\n",
      "1199 loss_D: 54.708866 loss_N: 0.26196963 loss_f: 7.0731835\n",
      "1199 Train Loss 62.04402\n",
      "1200 loss_D: 54.628857 loss_N: 0.2568348 loss_f: 7.0861063\n",
      "1200 Train Loss 61.971798\n",
      "1201 loss_D: 54.530178 loss_N: 0.25331545 loss_f: 7.0804434\n",
      "1201 Train Loss 61.863937\n",
      "1202 loss_D: 54.45778 loss_N: 0.24860673 loss_f: 7.0217047\n",
      "1202 Train Loss 61.728092\n",
      "1203 loss_D: 54.411903 loss_N: 0.2482573 loss_f: 7.0572906\n",
      "1203 Train Loss 61.71745\n",
      "1204 loss_D: 54.432266 loss_N: 0.24841034 loss_f: 6.992578\n",
      "1204 Train Loss 61.673252\n",
      "1205 loss_D: 54.44378 loss_N: 0.2485016 loss_f: 6.910292\n",
      "1205 Train Loss 61.602573\n",
      "1206 loss_D: 54.48995 loss_N: 0.24808261 loss_f: 6.791071\n",
      "1206 Train Loss 61.5291\n",
      "1207 loss_D: 54.470737 loss_N: 0.24652964 loss_f: 6.735462\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1207 Train Loss 61.45273\n",
      "1208 loss_D: 54.454193 loss_N: 0.24642487 loss_f: 6.6717734\n",
      "1208 Train Loss 61.37239\n",
      "1209 loss_D: 54.42875 loss_N: 0.24757057 loss_f: 6.625541\n",
      "1209 Train Loss 61.30186\n",
      "1210 loss_D: 54.266663 loss_N: 0.2466571 loss_f: 6.7306156\n",
      "1210 Train Loss 61.243935\n",
      "1211 loss_D: 54.14989 loss_N: 0.2479048 loss_f: 6.789671\n",
      "1211 Train Loss 61.18747\n",
      "1212 loss_D: 54.067017 loss_N: 0.24757792 loss_f: 6.812366\n",
      "1212 Train Loss 61.12696\n",
      "1213 loss_D: 53.975048 loss_N: 0.24862167 loss_f: 6.8386164\n",
      "1213 Train Loss 61.062286\n",
      "1214 loss_D: 53.927563 loss_N: 0.2546919 loss_f: 6.8227606\n",
      "1214 Train Loss 61.005016\n",
      "1215 loss_D: 53.885223 loss_N: 0.25676513 loss_f: 6.7871857\n",
      "1215 Train Loss 60.929173\n",
      "1216 loss_D: 53.768085 loss_N: 0.26620844 loss_f: 6.7709656\n",
      "1216 Train Loss 60.80526\n",
      "1217 loss_D: 53.701 loss_N: 0.26927823 loss_f: 6.7454934\n",
      "1217 Train Loss 60.715775\n",
      "1218 loss_D: 53.56043 loss_N: 0.2790131 loss_f: 6.796678\n",
      "1218 Train Loss 60.63612\n",
      "1219 loss_D: 53.50066 loss_N: 0.2865682 loss_f: 6.796984\n",
      "1219 Train Loss 60.584213\n",
      "1220 loss_D: 53.395275 loss_N: 0.2884553 loss_f: 6.858598\n",
      "1220 Train Loss 60.542328\n",
      "1221 loss_D: 53.392086 loss_N: 0.2919621 loss_f: 6.8148866\n",
      "1221 Train Loss 60.498936\n",
      "1222 loss_D: 53.282173 loss_N: 0.30492154 loss_f: 6.8456163\n",
      "1222 Train Loss 60.43271\n",
      "1223 loss_D: 53.17079 loss_N: 0.31875482 loss_f: 6.8263187\n",
      "1223 Train Loss 60.315865\n",
      "1224 loss_D: 52.99976 loss_N: 0.3374419 loss_f: 6.7924623\n",
      "1224 Train Loss 60.12966\n",
      "1225 loss_D: 52.86615 loss_N: 0.3377747 loss_f: 6.6986074\n",
      "1225 Train Loss 59.902534\n",
      "1226 loss_D: 52.807262 loss_N: 0.327748 loss_f: 6.6138263\n",
      "1226 Train Loss 59.748837\n",
      "1227 loss_D: 52.698025 loss_N: 0.32344404 loss_f: 6.615528\n",
      "1227 Train Loss 59.636997\n",
      "1228 loss_D: 52.605762 loss_N: 0.3137623 loss_f: 6.6366405\n",
      "1228 Train Loss 59.556168\n",
      "1229 loss_D: 52.566074 loss_N: 0.32284927 loss_f: 6.602265\n",
      "1229 Train Loss 59.491188\n",
      "1230 loss_D: 52.433723 loss_N: 0.32727584 loss_f: 6.667178\n",
      "1230 Train Loss 59.428177\n",
      "1231 loss_D: 52.356827 loss_N: 0.33733243 loss_f: 6.6825733\n",
      "1231 Train Loss 59.37673\n",
      "1232 loss_D: 52.196068 loss_N: 0.34409314 loss_f: 6.7953444\n",
      "1232 Train Loss 59.335506\n",
      "1233 loss_D: 52.16464 loss_N: 0.34283417 loss_f: 6.7924294\n",
      "1233 Train Loss 59.299904\n",
      "1234 loss_D: 52.059044 loss_N: 0.33788002 loss_f: 6.8661575\n",
      "1234 Train Loss 59.26308\n",
      "1235 loss_D: 52.03222 loss_N: 0.3240732 loss_f: 6.86114\n",
      "1235 Train Loss 59.217434\n",
      "1236 loss_D: 51.92365 loss_N: 0.30994707 loss_f: 6.9377656\n",
      "1236 Train Loss 59.171364\n",
      "1237 loss_D: 51.93861 loss_N: 0.294902 loss_f: 6.9077625\n",
      "1237 Train Loss 59.141273\n",
      "1238 loss_D: 51.905025 loss_N: 0.2951563 loss_f: 6.917045\n",
      "1238 Train Loss 59.117226\n",
      "1239 loss_D: 51.845367 loss_N: 0.29466155 loss_f: 6.9457145\n",
      "1239 Train Loss 59.085743\n",
      "1240 loss_D: 51.68062 loss_N: 0.2957392 loss_f: 7.0412903\n",
      "1240 Train Loss 59.017647\n",
      "1241 loss_D: 51.443092 loss_N: 0.28526133 loss_f: 7.1913247\n",
      "1241 Train Loss 58.91968\n",
      "1242 loss_D: 51.15079 loss_N: 0.27352816 loss_f: 7.413347\n",
      "1242 Train Loss 58.837666\n",
      "1243 loss_D: 50.882317 loss_N: 0.2547721 loss_f: 7.650942\n",
      "1243 Train Loss 58.788033\n",
      "1244 loss_D: 50.885174 loss_N: 0.2503895 loss_f: 7.5013537\n",
      "1244 Train Loss 58.636917\n",
      "1245 loss_D: 50.859325 loss_N: 0.24683769 loss_f: 7.3947887\n",
      "1245 Train Loss 58.500954\n",
      "1246 loss_D: 50.587856 loss_N: 0.23682265 loss_f: 7.5230412\n",
      "1246 Train Loss 58.34772\n",
      "1247 loss_D: 50.607075 loss_N: 0.23780107 loss_f: 7.402528\n",
      "1247 Train Loss 58.247402\n",
      "1248 loss_D: 50.214237 loss_N: 0.23563094 loss_f: 7.68868\n",
      "1248 Train Loss 58.138546\n",
      "1249 loss_D: 50.105484 loss_N: 0.23248322 loss_f: 7.718439\n",
      "1249 Train Loss 58.056404\n",
      "1250 loss_D: 49.92861 loss_N: 0.23968583 loss_f: 7.7984047\n",
      "1250 Train Loss 57.9667\n",
      "1251 loss_D: 49.716408 loss_N: 0.24149896 loss_f: 7.926922\n",
      "1251 Train Loss 57.88483\n",
      "1252 loss_D: 49.54382 loss_N: 0.23912 loss_f: 8.038535\n",
      "1252 Train Loss 57.821476\n",
      "1253 loss_D: 49.45722 loss_N: 0.2403602 loss_f: 8.076087\n",
      "1253 Train Loss 57.773666\n",
      "1254 loss_D: 49.38119 loss_N: 0.24315825 loss_f: 8.111356\n",
      "1254 Train Loss 57.735703\n",
      "1255 loss_D: 49.43158 loss_N: 0.24414697 loss_f: 8.012764\n",
      "1255 Train Loss 57.68849\n",
      "1256 loss_D: 49.446873 loss_N: 0.24192773 loss_f: 7.956345\n",
      "1256 Train Loss 57.645145\n",
      "1257 loss_D: 49.53331 loss_N: 0.24204352 loss_f: 7.819773\n",
      "1257 Train Loss 57.595127\n",
      "1258 loss_D: 49.49556 loss_N: 0.23948589 loss_f: 7.8220434\n",
      "1258 Train Loss 57.55709\n",
      "1259 loss_D: 49.516693 loss_N: 0.23536825 loss_f: 7.7730937\n",
      "1259 Train Loss 57.525154\n",
      "1260 loss_D: 49.44176 loss_N: 0.23250721 loss_f: 7.8194575\n",
      "1260 Train Loss 57.493725\n",
      "1261 loss_D: 49.407066 loss_N: 0.23062168 loss_f: 7.8197937\n",
      "1261 Train Loss 57.45748\n",
      "1262 loss_D: 49.37668 loss_N: 0.23177953 loss_f: 7.8145566\n",
      "1262 Train Loss 57.423016\n",
      "1263 loss_D: 49.49575 loss_N: 0.23063622 loss_f: 7.6444645\n",
      "1263 Train Loss 57.37085\n",
      "1264 loss_D: 49.555847 loss_N: 0.23023908 loss_f: 7.525966\n",
      "1264 Train Loss 57.312054\n",
      "1265 loss_D: 49.73602 loss_N: 0.23019758 loss_f: 7.2763653\n",
      "1265 Train Loss 57.242584\n",
      "1266 loss_D: 49.771473 loss_N: 0.22997402 loss_f: 7.1978326\n",
      "1266 Train Loss 57.19928\n",
      "1267 loss_D: 49.805325 loss_N: 0.22943345 loss_f: 7.1267934\n",
      "1267 Train Loss 57.161552\n",
      "1268 loss_D: 49.759544 loss_N: 0.22877671 loss_f: 7.130496\n",
      "1268 Train Loss 57.118816\n",
      "1269 loss_D: 49.73882 loss_N: 0.22969095 loss_f: 7.11668\n",
      "1269 Train Loss 57.08519\n",
      "1270 loss_D: 49.67604 loss_N: 0.23141432 loss_f: 7.1362467\n",
      "1270 Train Loss 57.0437\n",
      "1271 loss_D: 49.652687 loss_N: 0.23350507 loss_f: 7.1160727\n",
      "1271 Train Loss 57.002266\n",
      "1272 loss_D: 49.551144 loss_N: 0.2380851 loss_f: 7.146421\n",
      "1272 Train Loss 56.93565\n",
      "1273 loss_D: 49.533302 loss_N: 0.23925473 loss_f: 7.0987797\n",
      "1273 Train Loss 56.871338\n",
      "1274 loss_D: 49.468475 loss_N: 0.23940352 loss_f: 7.0949388\n",
      "1274 Train Loss 56.80282\n",
      "1275 loss_D: 49.39227 loss_N: 0.24125566 loss_f: 7.093836\n",
      "1275 Train Loss 56.727364\n",
      "1276 loss_D: 49.33898 loss_N: 0.23802428 loss_f: 7.087824\n",
      "1276 Train Loss 56.664833\n",
      "1277 loss_D: 49.298035 loss_N: 0.23852172 loss_f: 7.046678\n",
      "1277 Train Loss 56.583233\n",
      "1278 loss_D: 49.219074 loss_N: 0.24100466 loss_f: 7.003528\n",
      "1278 Train Loss 56.463608\n",
      "1279 loss_D: 49.218224 loss_N: 0.24147849 loss_f: 6.9232445\n",
      "1279 Train Loss 56.382946\n",
      "1280 loss_D: 49.175728 loss_N: 0.2421752 loss_f: 6.864081\n",
      "1280 Train Loss 56.281986\n",
      "1281 loss_D: 49.217937 loss_N: 0.24031714 loss_f: 6.762073\n",
      "1281 Train Loss 56.22033\n",
      "1282 loss_D: 49.158806 loss_N: 0.2369918 loss_f: 6.7369647\n",
      "1282 Train Loss 56.132763\n",
      "1283 loss_D: 49.20483 loss_N: 0.23133755 loss_f: 6.598873\n",
      "1283 Train Loss 56.03504\n",
      "1284 loss_D: 49.12865 loss_N: 0.22493564 loss_f: 6.5726013\n",
      "1284 Train Loss 55.92619\n",
      "1285 loss_D: 49.07322 loss_N: 0.22106029 loss_f: 6.53137\n",
      "1285 Train Loss 55.825653\n",
      "1286 loss_D: 48.982506 loss_N: 0.2186999 loss_f: 6.520911\n",
      "1286 Train Loss 55.72212\n",
      "1287 loss_D: 48.811344 loss_N: 0.21892372 loss_f: 6.5663223\n",
      "1287 Train Loss 55.596592\n",
      "1288 loss_D: 48.705112 loss_N: 0.22536093 loss_f: 6.5650043\n",
      "1288 Train Loss 55.495476\n",
      "1289 loss_D: 48.553444 loss_N: 0.2301863 loss_f: 6.6238356\n",
      "1289 Train Loss 55.407467\n",
      "1290 loss_D: 48.51915 loss_N: 0.22585063 loss_f: 6.5737796\n",
      "1290 Train Loss 55.31878\n",
      "1291 loss_D: 48.378235 loss_N: 0.22486587 loss_f: 6.657736\n",
      "1291 Train Loss 55.260834\n",
      "1292 loss_D: 48.406 loss_N: 0.212646 loss_f: 6.5984654\n",
      "1292 Train Loss 55.21711\n",
      "1293 loss_D: 48.372868 loss_N: 0.20983846 loss_f: 6.5952053\n",
      "1293 Train Loss 55.17791\n",
      "1294 loss_D: 48.35595 loss_N: 0.21040285 loss_f: 6.574923\n",
      "1294 Train Loss 55.141277\n",
      "1295 loss_D: 48.280056 loss_N: 0.21017537 loss_f: 6.6024437\n",
      "1295 Train Loss 55.092674\n",
      "1296 loss_D: 48.221684 loss_N: 0.20944987 loss_f: 6.600322\n",
      "1296 Train Loss 55.031456\n",
      "1297 loss_D: 48.091454 loss_N: 0.20798472 loss_f: 6.679382\n",
      "1297 Train Loss 54.97882\n",
      "1298 loss_D: 47.820942 loss_N: 0.21216801 loss_f: 7.051255\n",
      "1298 Train Loss 55.084366\n",
      "1299 loss_D: 47.986187 loss_N: 0.20899975 loss_f: 6.721708\n",
      "1299 Train Loss 54.916893\n",
      "1300 loss_D: 48.021656 loss_N: 0.20934749 loss_f: 6.632088\n",
      "1300 Train Loss 54.86309\n",
      "1301 loss_D: 47.990192 loss_N: 0.2078732 loss_f: 6.6057935\n",
      "1301 Train Loss 54.80386\n",
      "1302 loss_D: 47.97342 loss_N: 0.20757511 loss_f: 6.5874734\n",
      "1302 Train Loss 54.76847\n",
      "1303 loss_D: 47.909847 loss_N: 0.20683114 loss_f: 6.616695\n",
      "1303 Train Loss 54.733376\n",
      "1304 loss_D: 47.87547 loss_N: 0.20782533 loss_f: 6.598672\n",
      "1304 Train Loss 54.681965\n",
      "1305 loss_D: 47.82766 loss_N: 0.20909789 loss_f: 6.5716286\n",
      "1305 Train Loss 54.608387\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1306 loss_D: 47.78753 loss_N: 0.21077873 loss_f: 6.5367465\n",
      "1306 Train Loss 54.535053\n",
      "1307 loss_D: 47.794067 loss_N: 0.21496852 loss_f: 6.4236054\n",
      "1307 Train Loss 54.432644\n",
      "1308 loss_D: 47.851856 loss_N: 0.21795048 loss_f: 6.296055\n",
      "1308 Train Loss 54.36586\n",
      "1309 loss_D: 47.86814 loss_N: 0.21745604 loss_f: 6.2212973\n",
      "1309 Train Loss 54.306896\n",
      "1310 loss_D: 47.85808 loss_N: 0.215762 loss_f: 6.1729317\n",
      "1310 Train Loss 54.246777\n",
      "1311 loss_D: 47.77845 loss_N: 0.21087521 loss_f: 6.178267\n",
      "1311 Train Loss 54.167595\n",
      "1312 loss_D: 47.714355 loss_N: 0.20946611 loss_f: 6.1755977\n",
      "1312 Train Loss 54.09942\n",
      "1313 loss_D: 47.6002 loss_N: 0.20986925 loss_f: 6.2116685\n",
      "1313 Train Loss 54.02174\n",
      "1314 loss_D: 47.525997 loss_N: 0.21149999 loss_f: 6.214134\n",
      "1314 Train Loss 53.95163\n",
      "1315 loss_D: 47.386723 loss_N: 0.21298926 loss_f: 6.2981677\n",
      "1315 Train Loss 53.89788\n",
      "1316 loss_D: 47.337547 loss_N: 0.21346533 loss_f: 6.2927766\n",
      "1316 Train Loss 53.843792\n",
      "1317 loss_D: 47.211704 loss_N: 0.2153018 loss_f: 6.345893\n",
      "1317 Train Loss 53.7729\n",
      "1318 loss_D: 47.10846 loss_N: 0.21634352 loss_f: 6.3884287\n",
      "1318 Train Loss 53.71323\n",
      "1319 loss_D: 47.003674 loss_N: 0.21923439 loss_f: 6.433975\n",
      "1319 Train Loss 53.656883\n",
      "1320 loss_D: 46.90209 loss_N: 0.22194809 loss_f: 6.4821115\n",
      "1320 Train Loss 53.606148\n",
      "1321 loss_D: 46.87578 loss_N: 0.22503398 loss_f: 6.4651804\n",
      "1321 Train Loss 53.56599\n",
      "1322 loss_D: 46.786324 loss_N: 0.22725104 loss_f: 6.5039353\n",
      "1322 Train Loss 53.51751\n",
      "1323 loss_D: 46.67432 loss_N: 0.22882074 loss_f: 6.5634108\n",
      "1323 Train Loss 53.466553\n",
      "1324 loss_D: 46.58888 loss_N: 0.23016503 loss_f: 6.6140194\n",
      "1324 Train Loss 53.43306\n",
      "1325 loss_D: 46.432835 loss_N: 0.22966596 loss_f: 6.703856\n",
      "1325 Train Loss 53.36636\n",
      "1326 loss_D: 46.40902 loss_N: 0.23086385 loss_f: 6.674466\n",
      "1326 Train Loss 53.31435\n",
      "1327 loss_D: 46.29192 loss_N: 0.23900859 loss_f: 6.6751904\n",
      "1327 Train Loss 53.20612\n",
      "1328 loss_D: 46.13231 loss_N: 0.25915453 loss_f: 6.7275085\n",
      "1328 Train Loss 53.118973\n",
      "1329 loss_D: 46.05199 loss_N: 0.2677116 loss_f: 6.719021\n",
      "1329 Train Loss 53.038723\n",
      "1330 loss_D: 45.971867 loss_N: 0.2819246 loss_f: 6.6902356\n",
      "1330 Train Loss 52.944027\n",
      "1331 loss_D: 45.930405 loss_N: 0.2882595 loss_f: 6.6150093\n",
      "1331 Train Loss 52.833675\n",
      "1332 loss_D: 45.76741 loss_N: 0.31359512 loss_f: 6.6280575\n",
      "1332 Train Loss 52.70906\n",
      "1333 loss_D: 45.82037 loss_N: 0.31316122 loss_f: 6.479446\n",
      "1333 Train Loss 52.612976\n",
      "1334 loss_D: 45.670715 loss_N: 0.31429124 loss_f: 6.5242186\n",
      "1334 Train Loss 52.509228\n",
      "1335 loss_D: 45.658817 loss_N: 0.31965837 loss_f: 6.46987\n",
      "1335 Train Loss 52.44835\n",
      "1336 loss_D: 45.59994 loss_N: 0.31480315 loss_f: 6.4955773\n",
      "1336 Train Loss 52.410324\n",
      "1337 loss_D: 45.60926 loss_N: 0.31310943 loss_f: 6.4579043\n",
      "1337 Train Loss 52.380276\n",
      "1338 loss_D: 45.5505 loss_N: 0.31526297 loss_f: 6.478939\n",
      "1338 Train Loss 52.3447\n",
      "1339 loss_D: 45.566406 loss_N: 0.3096454 loss_f: 6.4308205\n",
      "1339 Train Loss 52.306873\n",
      "1340 loss_D: 45.55001 loss_N: 0.31768396 loss_f: 6.3986387\n",
      "1340 Train Loss 52.266335\n",
      "1341 loss_D: 45.554188 loss_N: 0.31724563 loss_f: 6.3478723\n",
      "1341 Train Loss 52.219307\n",
      "1342 loss_D: 45.52832 loss_N: 0.32231116 loss_f: 6.329742\n",
      "1342 Train Loss 52.180374\n",
      "1343 loss_D: 45.4977 loss_N: 0.32977816 loss_f: 6.297601\n",
      "1343 Train Loss 52.125076\n",
      "1344 loss_D: 45.399754 loss_N: 0.33130193 loss_f: 6.685973\n",
      "1344 Train Loss 52.41703\n",
      "1345 loss_D: 45.474655 loss_N: 0.3300286 loss_f: 6.300006\n",
      "1345 Train Loss 52.10469\n",
      "1346 loss_D: 45.400085 loss_N: 0.3381149 loss_f: 6.3220973\n",
      "1346 Train Loss 52.0603\n",
      "1347 loss_D: 45.33554 loss_N: 0.34599254 loss_f: 6.3365874\n",
      "1347 Train Loss 52.01812\n",
      "1348 loss_D: 45.253765 loss_N: 0.34771386 loss_f: 6.363667\n",
      "1348 Train Loss 51.965145\n",
      "1349 loss_D: 45.174408 loss_N: 0.3568729 loss_f: 6.350283\n",
      "1349 Train Loss 51.881565\n",
      "1350 loss_D: 45.08177 loss_N: 0.3563001 loss_f: 6.363217\n",
      "1350 Train Loss 51.80129\n",
      "1351 loss_D: 44.968086 loss_N: 0.35226977 loss_f: 6.414266\n",
      "1351 Train Loss 51.73462\n",
      "1352 loss_D: 44.55955 loss_N: 0.2960262 loss_f: 7.0932913\n",
      "1352 Train Loss 51.948868\n",
      "1353 loss_D: 44.838356 loss_N: 0.33304593 loss_f: 6.509657\n",
      "1353 Train Loss 51.68106\n",
      "1354 loss_D: 44.756695 loss_N: 0.31320488 loss_f: 6.5514274\n",
      "1354 Train Loss 51.621326\n",
      "1355 loss_D: 44.56767 loss_N: 0.28764784 loss_f: 6.711022\n",
      "1355 Train Loss 51.566338\n",
      "1356 loss_D: 44.588207 loss_N: 0.27927762 loss_f: 6.675717\n",
      "1356 Train Loss 51.5432\n",
      "1357 loss_D: 44.49218 loss_N: 0.28136 loss_f: 6.745631\n",
      "1357 Train Loss 51.519173\n",
      "1358 loss_D: 44.47521 loss_N: 0.28094497 loss_f: 6.7452936\n",
      "1358 Train Loss 51.501446\n",
      "1359 loss_D: 44.416912 loss_N: 0.280787 loss_f: 6.786925\n",
      "1359 Train Loss 51.484627\n",
      "1360 loss_D: 44.399914 loss_N: 0.27702266 loss_f: 6.7873077\n",
      "1360 Train Loss 51.464245\n",
      "1361 loss_D: 44.25014 loss_N: 0.28092074 loss_f: 6.9691186\n",
      "1361 Train Loss 51.500183\n",
      "1362 loss_D: 44.353767 loss_N: 0.278201 loss_f: 6.8236313\n",
      "1362 Train Loss 51.4556\n",
      "1363 loss_D: 44.292374 loss_N: 0.28114188 loss_f: 6.8735514\n",
      "1363 Train Loss 51.447067\n",
      "1364 loss_D: 44.313004 loss_N: 0.28124103 loss_f: 6.8132563\n",
      "1364 Train Loss 51.4075\n",
      "1365 loss_D: 44.32976 loss_N: 0.28329146 loss_f: 6.764781\n",
      "1365 Train Loss 51.377834\n",
      "1366 loss_D: 44.310543 loss_N: 0.2892304 loss_f: 6.7458763\n",
      "1366 Train Loss 51.34565\n",
      "1367 loss_D: 44.286938 loss_N: 0.28997988 loss_f: 6.7356625\n",
      "1367 Train Loss 51.312576\n",
      "1368 loss_D: 44.162075 loss_N: 0.292697 loss_f: 6.8174047\n",
      "1368 Train Loss 51.27218\n",
      "1369 loss_D: 44.090702 loss_N: 0.28836122 loss_f: 6.8495216\n",
      "1369 Train Loss 51.228584\n",
      "1370 loss_D: 44.036453 loss_N: 0.28000653 loss_f: 6.8735538\n",
      "1370 Train Loss 51.190014\n",
      "1371 loss_D: 44.009186 loss_N: 0.2780704 loss_f: 6.86884\n",
      "1371 Train Loss 51.156094\n",
      "1372 loss_D: 43.980957 loss_N: 0.2719427 loss_f: 6.8502216\n",
      "1372 Train Loss 51.10312\n",
      "1373 loss_D: 43.85829 loss_N: 0.27269632 loss_f: 6.909242\n",
      "1373 Train Loss 51.04023\n",
      "1374 loss_D: 43.903362 loss_N: 0.2705202 loss_f: 6.756106\n",
      "1374 Train Loss 50.92999\n",
      "1375 loss_D: 43.886627 loss_N: 0.26657587 loss_f: 6.663224\n",
      "1375 Train Loss 50.816425\n",
      "1376 loss_D: 43.879444 loss_N: 0.26719695 loss_f: 6.6028295\n",
      "1376 Train Loss 50.74947\n",
      "1377 loss_D: 43.719887 loss_N: 0.2693099 loss_f: 6.7035327\n",
      "1377 Train Loss 50.69273\n",
      "1378 loss_D: 43.59203 loss_N: 0.27222797 loss_f: 6.741442\n",
      "1378 Train Loss 50.6057\n",
      "1379 loss_D: 43.60315 loss_N: 0.27323255 loss_f: 6.6520224\n",
      "1379 Train Loss 50.528404\n",
      "1380 loss_D: 43.498672 loss_N: 0.27445766 loss_f: 6.6672883\n",
      "1380 Train Loss 50.44042\n",
      "1381 loss_D: 43.349068 loss_N: 0.28103533 loss_f: 6.726039\n",
      "1381 Train Loss 50.356144\n",
      "1382 loss_D: 43.265377 loss_N: 0.2822957 loss_f: 6.7457395\n",
      "1382 Train Loss 50.29341\n",
      "1383 loss_D: 43.2024 loss_N: 0.283265 loss_f: 6.735922\n",
      "1383 Train Loss 50.22159\n",
      "1384 loss_D: 43.330414 loss_N: 0.28151864 loss_f: 6.5401106\n",
      "1384 Train Loss 50.152042\n",
      "1385 loss_D: 43.256268 loss_N: 0.28004903 loss_f: 6.563774\n",
      "1385 Train Loss 50.10009\n",
      "1386 loss_D: 43.30112 loss_N: 0.27774552 loss_f: 6.478803\n",
      "1386 Train Loss 50.057667\n",
      "1387 loss_D: 43.31965 loss_N: 0.2780253 loss_f: 6.4101696\n",
      "1387 Train Loss 50.007843\n",
      "1388 loss_D: 43.303654 loss_N: 0.28203034 loss_f: 6.363563\n",
      "1388 Train Loss 49.94925\n",
      "1389 loss_D: 43.30745 loss_N: 0.2839854 loss_f: 6.301767\n",
      "1389 Train Loss 49.8932\n",
      "1390 loss_D: 43.310837 loss_N: 0.2852266 loss_f: 6.2493844\n",
      "1390 Train Loss 49.845448\n",
      "1391 loss_D: 43.250767 loss_N: 0.29046786 loss_f: 6.2601156\n",
      "1391 Train Loss 49.80135\n",
      "1392 loss_D: 43.281143 loss_N: 0.28436986 loss_f: 6.2114425\n",
      "1392 Train Loss 49.776955\n",
      "1393 loss_D: 43.268543 loss_N: 0.283118 loss_f: 6.1959147\n",
      "1393 Train Loss 49.747578\n",
      "1394 loss_D: 43.23963 loss_N: 0.28161994 loss_f: 6.1899548\n",
      "1394 Train Loss 49.711205\n",
      "1395 loss_D: 43.20322 loss_N: 0.28075224 loss_f: 6.1883364\n",
      "1395 Train Loss 49.67231\n",
      "1396 loss_D: 43.159138 loss_N: 0.28174222 loss_f: 6.200748\n",
      "1396 Train Loss 49.64163\n",
      "1397 loss_D: 43.13832 loss_N: 0.28269467 loss_f: 6.194438\n",
      "1397 Train Loss 49.615456\n",
      "1398 loss_D: 43.10604 loss_N: 0.28445086 loss_f: 6.196239\n",
      "1398 Train Loss 49.58673\n",
      "1399 loss_D: 43.087654 loss_N: 0.28568965 loss_f: 6.17921\n",
      "1399 Train Loss 49.552555\n",
      "1400 loss_D: 43.018494 loss_N: 0.2880215 loss_f: 6.2127285\n",
      "1400 Train Loss 49.51924\n",
      "1401 loss_D: 43.007843 loss_N: 0.2882974 loss_f: 6.1832366\n",
      "1401 Train Loss 49.479374\n",
      "1402 loss_D: 43.014816 loss_N: 0.28921688 loss_f: 6.1395583\n",
      "1402 Train Loss 49.44359\n",
      "1403 loss_D: 42.979137 loss_N: 0.28803256 loss_f: 6.1498666\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1403 Train Loss 49.417038\n",
      "1404 loss_D: 42.953243 loss_N: 0.28722095 loss_f: 6.1453\n",
      "1404 Train Loss 49.385765\n",
      "1405 loss_D: 42.864548 loss_N: 0.2863898 loss_f: 6.18372\n",
      "1405 Train Loss 49.334656\n",
      "1406 loss_D: 42.811966 loss_N: 0.28178766 loss_f: 6.1896906\n",
      "1406 Train Loss 49.283443\n",
      "1407 loss_D: 42.679256 loss_N: 0.28368956 loss_f: 6.2793794\n",
      "1407 Train Loss 49.24233\n",
      "1408 loss_D: 42.66196 loss_N: 0.28384486 loss_f: 6.276224\n",
      "1408 Train Loss 49.22203\n",
      "1409 loss_D: 42.629467 loss_N: 0.28730676 loss_f: 6.2930226\n",
      "1409 Train Loss 49.209797\n",
      "1410 loss_D: 42.615646 loss_N: 0.2882449 loss_f: 6.296353\n",
      "1410 Train Loss 49.200245\n",
      "1411 loss_D: 42.587048 loss_N: 0.2892006 loss_f: 6.312965\n",
      "1411 Train Loss 49.189213\n",
      "1412 loss_D: 42.554676 loss_N: 0.2893341 loss_f: 6.333724\n",
      "1412 Train Loss 49.177734\n",
      "1413 loss_D: 42.501728 loss_N: 0.2884874 loss_f: 6.3746033\n",
      "1413 Train Loss 49.164818\n",
      "1414 loss_D: 42.431824 loss_N: 0.2875825 loss_f: 6.417031\n",
      "1414 Train Loss 49.136436\n",
      "1415 loss_D: 42.306103 loss_N: 0.28863662 loss_f: 6.5031586\n",
      "1415 Train Loss 49.097897\n",
      "1416 loss_D: 42.140835 loss_N: 0.29426226 loss_f: 6.60158\n",
      "1416 Train Loss 49.03668\n",
      "1417 loss_D: 42.02309 loss_N: 0.2992832 loss_f: 6.664212\n",
      "1417 Train Loss 48.986584\n",
      "1418 loss_D: 41.875275 loss_N: 0.3071227 loss_f: 6.729567\n",
      "1418 Train Loss 48.911964\n",
      "1419 loss_D: 41.698475 loss_N: 0.31463793 loss_f: 6.803428\n",
      "1419 Train Loss 48.81654\n",
      "1420 loss_D: 41.48011 loss_N: 0.3229647 loss_f: 6.9069233\n",
      "1420 Train Loss 48.71\n",
      "1421 loss_D: 41.260014 loss_N: 0.31737182 loss_f: 7.2043185\n",
      "1421 Train Loss 48.781704\n",
      "1422 loss_D: 41.386566 loss_N: 0.31992802 loss_f: 6.941824\n",
      "1422 Train Loss 48.648315\n",
      "1423 loss_D: 41.374607 loss_N: 0.32109922 loss_f: 6.90009\n",
      "1423 Train Loss 48.595795\n",
      "1424 loss_D: 41.438488 loss_N: 0.3167744 loss_f: 6.779338\n",
      "1424 Train Loss 48.534603\n",
      "1425 loss_D: 41.379883 loss_N: 0.31619468 loss_f: 6.807389\n",
      "1425 Train Loss 48.503468\n",
      "1426 loss_D: 41.425335 loss_N: 0.31367755 loss_f: 6.7313404\n",
      "1426 Train Loss 48.470352\n",
      "1427 loss_D: 41.35862 loss_N: 0.3140687 loss_f: 6.762994\n",
      "1427 Train Loss 48.43568\n",
      "1428 loss_D: 41.366238 loss_N: 0.31487098 loss_f: 6.724309\n",
      "1428 Train Loss 48.40542\n",
      "1429 loss_D: 41.37621 loss_N: 0.31655985 loss_f: 6.691607\n",
      "1429 Train Loss 48.384377\n",
      "1430 loss_D: 41.392475 loss_N: 0.31706554 loss_f: 6.650067\n",
      "1430 Train Loss 48.359608\n",
      "1431 loss_D: 41.41345 loss_N: 0.31804675 loss_f: 6.6030154\n",
      "1431 Train Loss 48.33451\n",
      "1432 loss_D: 41.40256 loss_N: 0.3208663 loss_f: 6.587284\n",
      "1432 Train Loss 48.31071\n",
      "1433 loss_D: 41.378498 loss_N: 0.32275873 loss_f: 6.583503\n",
      "1433 Train Loss 48.28476\n",
      "1434 loss_D: 41.328987 loss_N: 0.32640132 loss_f: 6.60726\n",
      "1434 Train Loss 48.26265\n",
      "1435 loss_D: 41.27442 loss_N: 0.32879722 loss_f: 6.639017\n",
      "1435 Train Loss 48.242233\n",
      "1436 loss_D: 41.224415 loss_N: 0.33110332 loss_f: 6.668181\n",
      "1436 Train Loss 48.2237\n",
      "1437 loss_D: 41.184048 loss_N: 0.33326167 loss_f: 6.685584\n",
      "1437 Train Loss 48.202896\n",
      "1438 loss_D: 41.156883 loss_N: 0.33556893 loss_f: 6.68029\n",
      "1438 Train Loss 48.17274\n",
      "1439 loss_D: 41.116257 loss_N: 0.33466646 loss_f: 6.6941094\n",
      "1439 Train Loss 48.145035\n",
      "1440 loss_D: 41.171886 loss_N: 0.35151163 loss_f: 6.6337085\n",
      "1440 Train Loss 48.15711\n",
      "1441 loss_D: 41.137245 loss_N: 0.34115824 loss_f: 6.650691\n",
      "1441 Train Loss 48.129097\n",
      "1442 loss_D: 41.10362 loss_N: 0.34090775 loss_f: 6.659563\n",
      "1442 Train Loss 48.104088\n",
      "1443 loss_D: 41.057747 loss_N: 0.3330778 loss_f: 6.659438\n",
      "1443 Train Loss 48.050262\n",
      "1444 loss_D: 41.03335 loss_N: 0.32274786 loss_f: 6.634885\n",
      "1444 Train Loss 47.990982\n",
      "1445 loss_D: 40.991 loss_N: 0.31570128 loss_f: 6.6110106\n",
      "1445 Train Loss 47.917713\n",
      "1446 loss_D: 40.85001 loss_N: 0.32163477 loss_f: 6.685197\n",
      "1446 Train Loss 47.856842\n",
      "1447 loss_D: 40.828056 loss_N: 0.3218865 loss_f: 6.6410875\n",
      "1447 Train Loss 47.79103\n",
      "1448 loss_D: 40.73695 loss_N: 0.32965204 loss_f: 6.6681123\n",
      "1448 Train Loss 47.734715\n",
      "1449 loss_D: 40.756 loss_N: 0.32555738 loss_f: 6.612769\n",
      "1449 Train Loss 47.69433\n",
      "1450 loss_D: 40.655926 loss_N: 0.32382417 loss_f: 6.670288\n",
      "1450 Train Loss 47.65004\n",
      "1451 loss_D: 40.71642 loss_N: 0.31774786 loss_f: 6.5828867\n",
      "1451 Train Loss 47.617054\n",
      "1452 loss_D: 40.73972 loss_N: 0.31472403 loss_f: 6.52714\n",
      "1452 Train Loss 47.581585\n",
      "1453 loss_D: 40.75453 loss_N: 0.3128707 loss_f: 6.474272\n",
      "1453 Train Loss 47.54167\n",
      "1454 loss_D: 40.751446 loss_N: 0.313239 loss_f: 6.448867\n",
      "1454 Train Loss 47.513554\n",
      "1455 loss_D: 40.73447 loss_N: 0.31716728 loss_f: 6.4290137\n",
      "1455 Train Loss 47.480648\n",
      "1456 loss_D: 40.673084 loss_N: 0.32092935 loss_f: 6.457375\n",
      "1456 Train Loss 47.45139\n",
      "1457 loss_D: 40.65205 loss_N: 0.32351872 loss_f: 6.4524927\n",
      "1457 Train Loss 47.42806\n",
      "1458 loss_D: 40.626263 loss_N: 0.32461327 loss_f: 6.4396276\n",
      "1458 Train Loss 47.390503\n",
      "1459 loss_D: 40.611935 loss_N: 0.32566735 loss_f: 6.409514\n",
      "1459 Train Loss 47.34712\n",
      "1460 loss_D: 40.55558 loss_N: 0.32774904 loss_f: 6.4108543\n",
      "1460 Train Loss 47.29418\n",
      "1461 loss_D: 40.50206 loss_N: 0.3317203 loss_f: 6.3819585\n",
      "1461 Train Loss 47.215736\n",
      "1462 loss_D: 40.370483 loss_N: 0.33904612 loss_f: 6.431122\n",
      "1462 Train Loss 47.14065\n",
      "1463 loss_D: 40.274044 loss_N: 0.34451288 loss_f: 6.453857\n",
      "1463 Train Loss 47.072414\n",
      "1464 loss_D: 40.271225 loss_N: 0.3437962 loss_f: 6.372388\n",
      "1464 Train Loss 46.987408\n",
      "1465 loss_D: 40.18238 loss_N: 0.3463584 loss_f: 6.378537\n",
      "1465 Train Loss 46.907276\n",
      "1466 loss_D: 40.197926 loss_N: 0.34448794 loss_f: 6.297892\n",
      "1466 Train Loss 46.840305\n",
      "1467 loss_D: 40.187695 loss_N: 0.3400014 loss_f: 6.2353296\n",
      "1467 Train Loss 46.763023\n",
      "1468 loss_D: 40.127804 loss_N: 0.33326557 loss_f: 6.1792836\n",
      "1468 Train Loss 46.640354\n",
      "1469 loss_D: 40.043034 loss_N: 0.3316796 loss_f: 6.175668\n",
      "1469 Train Loss 46.55038\n",
      "1470 loss_D: 40.01538 loss_N: 0.33071774 loss_f: 6.110392\n",
      "1470 Train Loss 46.456493\n",
      "1471 loss_D: 39.910725 loss_N: 0.33244336 loss_f: 6.142205\n",
      "1471 Train Loss 46.385372\n",
      "1472 loss_D: 39.914253 loss_N: 0.33395317 loss_f: 6.10755\n",
      "1472 Train Loss 46.35576\n",
      "1473 loss_D: 39.873302 loss_N: 0.33427754 loss_f: 6.104741\n",
      "1473 Train Loss 46.31232\n",
      "1474 loss_D: 39.841766 loss_N: 0.33427215 loss_f: 6.1163983\n",
      "1474 Train Loss 46.292435\n",
      "1475 loss_D: 39.80399 loss_N: 0.33461493 loss_f: 6.1235485\n",
      "1475 Train Loss 46.262154\n",
      "1476 loss_D: 39.71725 loss_N: 0.33589265 loss_f: 6.1697397\n",
      "1476 Train Loss 46.22288\n",
      "1477 loss_D: 39.660366 loss_N: 0.3366011 loss_f: 6.1925673\n",
      "1477 Train Loss 46.189533\n",
      "1478 loss_D: 39.59268 loss_N: 0.33410633 loss_f: 6.237343\n",
      "1478 Train Loss 46.16413\n",
      "1479 loss_D: 39.593857 loss_N: 0.33284718 loss_f: 6.21488\n",
      "1479 Train Loss 46.141586\n",
      "1480 loss_D: 39.552406 loss_N: 0.33370963 loss_f: 6.233356\n",
      "1480 Train Loss 46.119473\n",
      "1481 loss_D: 39.538933 loss_N: 0.33256423 loss_f: 6.223502\n",
      "1481 Train Loss 46.095\n",
      "1482 loss_D: 39.51458 loss_N: 0.33171996 loss_f: 6.227942\n",
      "1482 Train Loss 46.07424\n",
      "1483 loss_D: 39.45446 loss_N: 0.33261868 loss_f: 6.2589664\n",
      "1483 Train Loss 46.046043\n",
      "1484 loss_D: 39.43576 loss_N: 0.33098313 loss_f: 6.25275\n",
      "1484 Train Loss 46.019493\n",
      "1485 loss_D: 39.364803 loss_N: 0.33257967 loss_f: 6.296122\n",
      "1485 Train Loss 45.993507\n",
      "1486 loss_D: 39.363804 loss_N: 0.33290672 loss_f: 6.2694325\n",
      "1486 Train Loss 45.96614\n",
      "1487 loss_D: 39.313324 loss_N: 0.33390647 loss_f: 6.2957993\n",
      "1487 Train Loss 45.94303\n",
      "1488 loss_D: 39.32307 loss_N: 0.33431286 loss_f: 6.250782\n",
      "1488 Train Loss 45.908165\n",
      "1489 loss_D: 39.331017 loss_N: 0.33337653 loss_f: 6.1976886\n",
      "1489 Train Loss 45.862083\n",
      "1490 loss_D: 39.299667 loss_N: 0.3316678 loss_f: 6.176982\n",
      "1490 Train Loss 45.80832\n",
      "1491 loss_D: 39.270813 loss_N: 0.33054173 loss_f: 6.163761\n",
      "1491 Train Loss 45.765118\n",
      "1492 loss_D: 39.2394 loss_N: 0.33232373 loss_f: 6.150397\n",
      "1492 Train Loss 45.722122\n",
      "1493 loss_D: 39.21318 loss_N: 0.3353878 loss_f: 6.123764\n",
      "1493 Train Loss 45.672333\n",
      "1494 loss_D: 39.172855 loss_N: 0.3392376 loss_f: 6.1270056\n",
      "1494 Train Loss 45.6391\n",
      "1495 loss_D: 39.178116 loss_N: 0.34198534 loss_f: 6.0828295\n",
      "1495 Train Loss 45.60293\n",
      "1496 loss_D: 39.141438 loss_N: 0.34526074 loss_f: 6.073657\n",
      "1496 Train Loss 45.560356\n",
      "1497 loss_D: 39.123997 loss_N: 0.3484483 loss_f: 6.0408607\n",
      "1497 Train Loss 45.513306\n",
      "1498 loss_D: 39.08751 loss_N: 0.3515123 loss_f: 6.0702715\n",
      "1498 Train Loss 45.509293\n",
      "1499 loss_D: 39.10351 loss_N: 0.34983206 loss_f: 6.0336246\n",
      "1499 Train Loss 45.48697\n",
      "1500 loss_D: 39.042953 loss_N: 0.35116366 loss_f: 6.0559025\n",
      "1500 Train Loss 45.45002\n",
      "1501 loss_D: 39.039486 loss_N: 0.35028002 loss_f: 6.0265293\n",
      "1501 Train Loss 45.416298\n",
      "1502 loss_D: 39.023525 loss_N: 0.35181406 loss_f: 6.014119\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1502 Train Loss 45.389458\n",
      "1503 loss_D: 38.99859 loss_N: 0.35271934 loss_f: 6.0051055\n",
      "1503 Train Loss 45.35641\n",
      "1504 loss_D: 39.008904 loss_N: 0.35379875 loss_f: 5.964738\n",
      "1504 Train Loss 45.32744\n",
      "1505 loss_D: 39.009254 loss_N: 0.3548818 loss_f: 5.9357176\n",
      "1505 Train Loss 45.299854\n",
      "1506 loss_D: 39.01448 loss_N: 0.35431272 loss_f: 5.907793\n",
      "1506 Train Loss 45.27659\n",
      "1507 loss_D: 39.01558 loss_N: 0.35390458 loss_f: 5.8847723\n",
      "1507 Train Loss 45.254257\n",
      "1508 loss_D: 38.996593 loss_N: 0.35314983 loss_f: 5.8810906\n",
      "1508 Train Loss 45.230835\n",
      "1509 loss_D: 38.965607 loss_N: 0.35226727 loss_f: 5.882528\n",
      "1509 Train Loss 45.2004\n",
      "1510 loss_D: 38.914486 loss_N: 0.35208508 loss_f: 5.903711\n",
      "1510 Train Loss 45.17028\n",
      "1511 loss_D: 38.889347 loss_N: 0.35243538 loss_f: 5.8977776\n",
      "1511 Train Loss 45.13956\n",
      "1512 loss_D: 38.857483 loss_N: 0.35328886 loss_f: 5.895516\n",
      "1512 Train Loss 45.10629\n",
      "1513 loss_D: 38.852516 loss_N: 0.35473073 loss_f: 5.863432\n",
      "1513 Train Loss 45.07068\n",
      "1514 loss_D: 38.853745 loss_N: 0.3557073 loss_f: 5.8230505\n",
      "1514 Train Loss 45.032505\n",
      "1515 loss_D: 38.842785 loss_N: 0.3545819 loss_f: 5.79753\n",
      "1515 Train Loss 44.9949\n",
      "1516 loss_D: 38.828564 loss_N: 0.3530503 loss_f: 5.7683263\n",
      "1516 Train Loss 44.94994\n",
      "1517 loss_D: 38.769943 loss_N: 0.34980735 loss_f: 5.7728114\n",
      "1517 Train Loss 44.892563\n",
      "1518 loss_D: 38.723236 loss_N: 0.34776327 loss_f: 5.760527\n",
      "1518 Train Loss 44.831528\n",
      "1519 loss_D: 38.609386 loss_N: 0.34846395 loss_f: 5.832135\n",
      "1519 Train Loss 44.789986\n",
      "1520 loss_D: 38.634186 loss_N: 0.35013217 loss_f: 5.785327\n",
      "1520 Train Loss 44.769646\n",
      "1521 loss_D: 38.58309 loss_N: 0.35261044 loss_f: 5.8187637\n",
      "1521 Train Loss 44.754467\n",
      "1522 loss_D: 38.568592 loss_N: 0.352773 loss_f: 5.8213124\n",
      "1522 Train Loss 44.742676\n",
      "1523 loss_D: 38.552036 loss_N: 0.35253668 loss_f: 5.8232255\n",
      "1523 Train Loss 44.7278\n",
      "1524 loss_D: 38.51308 loss_N: 0.3523543 loss_f: 5.842195\n",
      "1524 Train Loss 44.70763\n",
      "1525 loss_D: 38.492504 loss_N: 0.35116202 loss_f: 5.846222\n",
      "1525 Train Loss 44.689888\n",
      "1526 loss_D: 38.464275 loss_N: 0.35185885 loss_f: 5.862076\n",
      "1526 Train Loss 44.67821\n",
      "1527 loss_D: 38.417942 loss_N: 0.35036203 loss_f: 5.9887247\n",
      "1527 Train Loss 44.757027\n",
      "1528 loss_D: 38.45253 loss_N: 0.3514694 loss_f: 5.867523\n",
      "1528 Train Loss 44.671524\n",
      "1529 loss_D: 38.45778 loss_N: 0.35097 loss_f: 5.8387556\n",
      "1529 Train Loss 44.647507\n",
      "1530 loss_D: 38.45147 loss_N: 0.35227358 loss_f: 5.821084\n",
      "1530 Train Loss 44.624825\n",
      "1531 loss_D: 38.468803 loss_N: 0.35244823 loss_f: 5.7855444\n",
      "1531 Train Loss 44.606796\n",
      "1532 loss_D: 38.46332 loss_N: 0.35302547 loss_f: 5.774227\n",
      "1532 Train Loss 44.590576\n",
      "1533 loss_D: 38.451885 loss_N: 0.35473496 loss_f: 5.7571716\n",
      "1533 Train Loss 44.563793\n",
      "1534 loss_D: 38.40066 loss_N: 0.3574154 loss_f: 5.771928\n",
      "1534 Train Loss 44.530003\n",
      "1535 loss_D: 38.341167 loss_N: 0.36216953 loss_f: 5.774341\n",
      "1535 Train Loss 44.47768\n",
      "1536 loss_D: 38.25751 loss_N: 0.3664054 loss_f: 5.80701\n",
      "1536 Train Loss 44.430927\n",
      "1537 loss_D: 38.252796 loss_N: 0.36432275 loss_f: 5.790931\n",
      "1537 Train Loss 44.40805\n",
      "1538 loss_D: 38.252205 loss_N: 0.3645813 loss_f: 5.7784843\n",
      "1538 Train Loss 44.39527\n",
      "1539 loss_D: 38.25384 loss_N: 0.36326638 loss_f: 5.763137\n",
      "1539 Train Loss 44.380245\n",
      "1540 loss_D: 38.251812 loss_N: 0.3620664 loss_f: 5.7480383\n",
      "1540 Train Loss 44.36192\n",
      "1541 loss_D: 38.229168 loss_N: 0.35663137 loss_f: 5.7623405\n",
      "1541 Train Loss 44.34814\n",
      "1542 loss_D: 38.217068 loss_N: 0.35683838 loss_f: 5.7505803\n",
      "1542 Train Loss 44.324486\n",
      "1543 loss_D: 38.205097 loss_N: 0.3564219 loss_f: 5.7418675\n",
      "1543 Train Loss 44.303387\n",
      "1544 loss_D: 38.17703 loss_N: 0.352885 loss_f: 5.7525387\n",
      "1544 Train Loss 44.282455\n",
      "1545 loss_D: 38.184956 loss_N: 0.3513246 loss_f: 5.7229743\n",
      "1545 Train Loss 44.259254\n",
      "1546 loss_D: 38.160915 loss_N: 0.35165256 loss_f: 5.7311583\n",
      "1546 Train Loss 44.24373\n",
      "1547 loss_D: 38.16058 loss_N: 0.35114878 loss_f: 5.715719\n",
      "1547 Train Loss 44.227448\n",
      "1548 loss_D: 38.139668 loss_N: 0.35149035 loss_f: 5.7201552\n",
      "1548 Train Loss 44.21131\n",
      "1549 loss_D: 38.118717 loss_N: 0.35233524 loss_f: 5.7122836\n",
      "1549 Train Loss 44.18334\n",
      "1550 loss_D: 38.079113 loss_N: 0.3533267 loss_f: 5.725267\n",
      "1550 Train Loss 44.157703\n",
      "1551 loss_D: 38.059067 loss_N: 0.35489178 loss_f: 5.7207913\n",
      "1551 Train Loss 44.13475\n",
      "1552 loss_D: 38.038586 loss_N: 0.3557704 loss_f: 5.716765\n",
      "1552 Train Loss 44.111122\n",
      "1553 loss_D: 37.9993 loss_N: 0.35697323 loss_f: 5.7179723\n",
      "1553 Train Loss 44.07424\n",
      "1554 loss_D: 37.981598 loss_N: 0.35686943 loss_f: 5.681608\n",
      "1554 Train Loss 44.020073\n",
      "1555 loss_D: 37.946648 loss_N: 0.35549685 loss_f: 5.640115\n",
      "1555 Train Loss 43.942257\n",
      "1556 loss_D: 37.950974 loss_N: 0.35199392 loss_f: 5.6029396\n",
      "1556 Train Loss 43.905907\n",
      "1557 loss_D: 37.963314 loss_N: 0.35124856 loss_f: 5.546657\n",
      "1557 Train Loss 43.86122\n",
      "1558 loss_D: 37.950317 loss_N: 0.34933227 loss_f: 5.523105\n",
      "1558 Train Loss 43.822754\n",
      "1559 loss_D: 37.96351 loss_N: 0.34605414 loss_f: 5.482111\n",
      "1559 Train Loss 43.79167\n",
      "1560 loss_D: 37.904564 loss_N: 0.33886033 loss_f: 5.656635\n",
      "1560 Train Loss 43.90006\n",
      "1561 loss_D: 37.943913 loss_N: 0.3438474 loss_f: 5.4864984\n",
      "1561 Train Loss 43.77426\n",
      "1562 loss_D: 37.934654 loss_N: 0.34112892 loss_f: 5.4611998\n",
      "1562 Train Loss 43.736984\n",
      "1563 loss_D: 37.92308 loss_N: 0.33763182 loss_f: 5.425005\n",
      "1563 Train Loss 43.685715\n",
      "1564 loss_D: 37.88068 loss_N: 0.33584663 loss_f: 5.4191484\n",
      "1564 Train Loss 43.635674\n",
      "1565 loss_D: 37.797874 loss_N: 0.33422312 loss_f: 5.438118\n",
      "1565 Train Loss 43.570217\n",
      "1566 loss_D: 37.696342 loss_N: 0.33468437 loss_f: 5.475439\n",
      "1566 Train Loss 43.50647\n",
      "1567 loss_D: 37.59113 loss_N: 0.3371878 loss_f: 5.528803\n",
      "1567 Train Loss 43.457123\n",
      "1568 loss_D: 37.526257 loss_N: 0.33837235 loss_f: 5.560584\n",
      "1568 Train Loss 43.425213\n",
      "1569 loss_D: 37.50826 loss_N: 0.33961698 loss_f: 5.5540566\n",
      "1569 Train Loss 43.401936\n",
      "1570 loss_D: 37.44916 loss_N: 0.34014338 loss_f: 5.5912223\n",
      "1570 Train Loss 43.380527\n",
      "1571 loss_D: 37.431404 loss_N: 0.3393025 loss_f: 5.5888395\n",
      "1571 Train Loss 43.359547\n",
      "1572 loss_D: 37.352386 loss_N: 0.34116364 loss_f: 5.639647\n",
      "1572 Train Loss 43.3332\n",
      "1573 loss_D: 37.283264 loss_N: 0.34026346 loss_f: 5.679371\n",
      "1573 Train Loss 43.3029\n",
      "1574 loss_D: 37.228725 loss_N: 0.33996952 loss_f: 5.7086883\n",
      "1574 Train Loss 43.27738\n",
      "1575 loss_D: 37.140057 loss_N: 0.34192562 loss_f: 5.773708\n",
      "1575 Train Loss 43.25569\n",
      "1576 loss_D: 37.100487 loss_N: 0.34156385 loss_f: 5.7971263\n",
      "1576 Train Loss 43.239178\n",
      "1577 loss_D: 37.080433 loss_N: 0.3418805 loss_f: 5.799839\n",
      "1577 Train Loss 43.222153\n",
      "1578 loss_D: 37.044025 loss_N: 0.3428989 loss_f: 5.8176656\n",
      "1578 Train Loss 43.20459\n",
      "1579 loss_D: 37.008354 loss_N: 0.34371072 loss_f: 5.84041\n",
      "1579 Train Loss 43.192474\n",
      "1580 loss_D: 37.000515 loss_N: 0.34452933 loss_f: 5.832406\n",
      "1580 Train Loss 43.17745\n",
      "1581 loss_D: 36.96951 loss_N: 0.34519374 loss_f: 5.8400645\n",
      "1581 Train Loss 43.154766\n",
      "1582 loss_D: 37.01602 loss_N: 0.3432755 loss_f: 5.7745786\n",
      "1582 Train Loss 43.133877\n",
      "1583 loss_D: 37.000298 loss_N: 0.34337407 loss_f: 5.7609105\n",
      "1583 Train Loss 43.10458\n",
      "1584 loss_D: 36.98328 loss_N: 0.342379 loss_f: 5.7551923\n",
      "1584 Train Loss 43.080853\n",
      "1585 loss_D: 37.005844 loss_N: 0.3413441 loss_f: 5.718467\n",
      "1585 Train Loss 43.065655\n",
      "1586 loss_D: 36.98646 loss_N: 0.3407752 loss_f: 5.720308\n",
      "1586 Train Loss 43.047543\n",
      "1587 loss_D: 36.948112 loss_N: 0.3404431 loss_f: 5.738048\n",
      "1587 Train Loss 43.026604\n",
      "1588 loss_D: 36.895855 loss_N: 0.34030792 loss_f: 5.7594495\n",
      "1588 Train Loss 42.995613\n",
      "1589 loss_D: 36.79064 loss_N: 0.34217077 loss_f: 5.8018756\n",
      "1589 Train Loss 42.93469\n",
      "1590 loss_D: 36.610573 loss_N: 0.3572583 loss_f: 5.980453\n",
      "1590 Train Loss 42.948284\n",
      "1591 loss_D: 36.702053 loss_N: 0.3473491 loss_f: 5.8395033\n",
      "1591 Train Loss 42.888905\n",
      "1592 loss_D: 36.62782 loss_N: 0.35014468 loss_f: 5.828234\n",
      "1592 Train Loss 42.8062\n",
      "1593 loss_D: 36.55973 loss_N: 0.35680094 loss_f: 5.8213263\n",
      "1593 Train Loss 42.737858\n",
      "1594 loss_D: 36.582863 loss_N: 0.35219344 loss_f: 5.761067\n",
      "1594 Train Loss 42.69612\n",
      "1595 loss_D: 36.520054 loss_N: 0.35175738 loss_f: 5.789851\n",
      "1595 Train Loss 42.661663\n",
      "1596 loss_D: 36.52591 loss_N: 0.34680527 loss_f: 5.754972\n",
      "1596 Train Loss 42.627686\n",
      "1597 loss_D: 36.48991 loss_N: 0.34386286 loss_f: 5.761329\n",
      "1597 Train Loss 42.595104\n",
      "1598 loss_D: 36.41958 loss_N: 0.33767793 loss_f: 5.7978516\n",
      "1598 Train Loss 42.555107\n",
      "1599 loss_D: 36.391815 loss_N: 0.33556518 loss_f: 5.7982755\n",
      "1599 Train Loss 42.525654\n",
      "1600 loss_D: 36.341423 loss_N: 0.33203167 loss_f: 5.8084526\n",
      "1600 Train Loss 42.481907\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1601 loss_D: 36.35031 loss_N: 0.32825255 loss_f: 5.768426\n",
      "1601 Train Loss 42.446987\n",
      "1602 loss_D: 36.3231 loss_N: 0.33578125 loss_f: 5.782216\n",
      "1602 Train Loss 42.441097\n",
      "1603 loss_D: 36.331406 loss_N: 0.33063754 loss_f: 5.729394\n",
      "1603 Train Loss 42.391438\n",
      "1604 loss_D: 36.326504 loss_N: 0.33003712 loss_f: 5.7137165\n",
      "1604 Train Loss 42.370255\n",
      "1605 loss_D: 36.26212 loss_N: 0.32997757 loss_f: 5.745995\n",
      "1605 Train Loss 42.338093\n",
      "1606 loss_D: 36.202038 loss_N: 0.32712236 loss_f: 5.773194\n",
      "1606 Train Loss 42.302353\n",
      "1607 loss_D: 36.03863 loss_N: 0.3289353 loss_f: 5.890614\n",
      "1607 Train Loss 42.25818\n",
      "1608 loss_D: 36.012432 loss_N: 0.32672402 loss_f: 5.8682237\n",
      "1608 Train Loss 42.207382\n",
      "1609 loss_D: 36.036892 loss_N: 0.32376555 loss_f: 5.7866187\n",
      "1609 Train Loss 42.147274\n",
      "1610 loss_D: 35.934536 loss_N: 0.32338664 loss_f: 5.8484335\n",
      "1610 Train Loss 42.106358\n",
      "1611 loss_D: 35.928406 loss_N: 0.32449803 loss_f: 5.8097544\n",
      "1611 Train Loss 42.062656\n",
      "1612 loss_D: 35.888657 loss_N: 0.32636237 loss_f: 5.823453\n",
      "1612 Train Loss 42.03847\n",
      "1613 loss_D: 35.883137 loss_N: 0.32659423 loss_f: 5.8123555\n",
      "1613 Train Loss 42.022087\n",
      "1614 loss_D: 35.86578 loss_N: 0.3274686 loss_f: 5.807308\n",
      "1614 Train Loss 42.000557\n",
      "1615 loss_D: 35.80406 loss_N: 0.32708603 loss_f: 5.830738\n",
      "1615 Train Loss 41.961887\n",
      "1616 loss_D: 35.75533 loss_N: 0.32327098 loss_f: 5.8499765\n",
      "1616 Train Loss 41.928577\n",
      "1617 loss_D: 35.729248 loss_N: 0.3228401 loss_f: 5.8508735\n",
      "1617 Train Loss 41.90296\n",
      "1618 loss_D: 35.716923 loss_N: 0.32018256 loss_f: 5.848377\n",
      "1618 Train Loss 41.885483\n",
      "1619 loss_D: 35.744762 loss_N: 0.32000735 loss_f: 5.802642\n",
      "1619 Train Loss 41.867413\n",
      "1620 loss_D: 35.76179 loss_N: 0.32057452 loss_f: 5.77363\n",
      "1620 Train Loss 41.855995\n",
      "1621 loss_D: 35.76641 loss_N: 0.32086036 loss_f: 5.7528334\n",
      "1621 Train Loss 41.840107\n",
      "1622 loss_D: 35.793667 loss_N: 0.320635 loss_f: 5.702117\n",
      "1622 Train Loss 41.81642\n",
      "1623 loss_D: 35.781704 loss_N: 0.32097358 loss_f: 5.6935654\n",
      "1623 Train Loss 41.79624\n",
      "1624 loss_D: 35.768997 loss_N: 0.32191703 loss_f: 5.685962\n",
      "1624 Train Loss 41.77688\n",
      "1625 loss_D: 35.783886 loss_N: 0.32308814 loss_f: 5.65999\n",
      "1625 Train Loss 41.766964\n",
      "1626 loss_D: 35.751186 loss_N: 0.32387817 loss_f: 5.687702\n",
      "1626 Train Loss 41.762768\n",
      "1627 loss_D: 35.74448 loss_N: 0.32249057 loss_f: 5.679571\n",
      "1627 Train Loss 41.746544\n",
      "1628 loss_D: 35.749485 loss_N: 0.32224944 loss_f: 5.6657863\n",
      "1628 Train Loss 41.737522\n",
      "1629 loss_D: 35.767838 loss_N: 0.32175124 loss_f: 5.633418\n",
      "1629 Train Loss 41.723007\n",
      "1630 loss_D: 35.734592 loss_N: 0.3217723 loss_f: 5.6493936\n",
      "1630 Train Loss 41.70576\n",
      "1631 loss_D: 35.718094 loss_N: 0.3227954 loss_f: 5.6516066\n",
      "1631 Train Loss 41.692497\n",
      "1632 loss_D: 35.633934 loss_N: 0.32523075 loss_f: 5.7290835\n",
      "1632 Train Loss 41.688248\n",
      "1633 loss_D: 35.621662 loss_N: 0.32366344 loss_f: 5.7261834\n",
      "1633 Train Loss 41.67151\n",
      "1634 loss_D: 35.651863 loss_N: 0.32336208 loss_f: 5.6831484\n",
      "1634 Train Loss 41.65837\n",
      "1635 loss_D: 35.66864 loss_N: 0.32292557 loss_f: 5.6394997\n",
      "1635 Train Loss 41.631065\n",
      "1636 loss_D: 35.657272 loss_N: 0.3222605 loss_f: 5.5992246\n",
      "1636 Train Loss 41.57876\n",
      "1637 loss_D: 35.5784 loss_N: 0.3223367 loss_f: 5.630458\n",
      "1637 Train Loss 41.531197\n",
      "1638 loss_D: 35.540695 loss_N: 0.32283434 loss_f: 5.63762\n",
      "1638 Train Loss 41.50115\n",
      "1639 loss_D: 35.483047 loss_N: 0.3226896 loss_f: 5.6708617\n",
      "1639 Train Loss 41.476597\n",
      "1640 loss_D: 35.451687 loss_N: 0.32251018 loss_f: 5.6764684\n",
      "1640 Train Loss 41.450665\n",
      "1641 loss_D: 35.372818 loss_N: 0.32053426 loss_f: 5.7163467\n",
      "1641 Train Loss 41.4097\n",
      "1642 loss_D: 35.284824 loss_N: 0.32217622 loss_f: 5.773519\n",
      "1642 Train Loss 41.38052\n",
      "1643 loss_D: 35.18686 loss_N: 0.32221884 loss_f: 5.8340364\n",
      "1643 Train Loss 41.343117\n",
      "1644 loss_D: 35.178936 loss_N: 0.32290742 loss_f: 5.819328\n",
      "1644 Train Loss 41.32117\n",
      "1645 loss_D: 35.14834 loss_N: 0.3241935 loss_f: 5.8209677\n",
      "1645 Train Loss 41.2935\n",
      "1646 loss_D: 35.08262 loss_N: 0.3256866 loss_f: 5.8493013\n",
      "1646 Train Loss 41.257607\n",
      "1647 loss_D: 34.985832 loss_N: 0.3253347 loss_f: 5.8958054\n",
      "1647 Train Loss 41.206974\n",
      "1648 loss_D: 34.913506 loss_N: 0.3244118 loss_f: 5.9304047\n",
      "1648 Train Loss 41.168324\n",
      "1649 loss_D: 34.85611 loss_N: 0.32244217 loss_f: 5.9567614\n",
      "1649 Train Loss 41.13531\n",
      "1650 loss_D: 34.850235 loss_N: 0.32194126 loss_f: 5.9227867\n",
      "1650 Train Loss 41.094963\n",
      "1651 loss_D: 34.75321 loss_N: 0.32301834 loss_f: 5.976096\n",
      "1651 Train Loss 41.052322\n",
      "1652 loss_D: 34.811657 loss_N: 0.3237621 loss_f: 5.890252\n",
      "1652 Train Loss 41.02567\n",
      "1653 loss_D: 34.77063 loss_N: 0.32659835 loss_f: 5.915553\n",
      "1653 Train Loss 41.012783\n",
      "1654 loss_D: 34.75455 loss_N: 0.32690126 loss_f: 5.9117465\n",
      "1654 Train Loss 40.9932\n",
      "1655 loss_D: 34.749 loss_N: 0.3262412 loss_f: 5.9082108\n",
      "1655 Train Loss 40.98345\n",
      "1656 loss_D: 34.728764 loss_N: 0.32636443 loss_f: 5.9061766\n",
      "1656 Train Loss 40.961304\n",
      "1657 loss_D: 34.68326 loss_N: 0.32798314 loss_f: 5.9113655\n",
      "1657 Train Loss 40.92261\n",
      "1658 loss_D: 34.664875 loss_N: 0.33101195 loss_f: 5.905684\n",
      "1658 Train Loss 40.901573\n",
      "1659 loss_D: 34.61478 loss_N: 0.33217373 loss_f: 5.933102\n",
      "1659 Train Loss 40.880054\n",
      "1660 loss_D: 34.594765 loss_N: 0.33049744 loss_f: 5.9218607\n",
      "1660 Train Loss 40.847122\n",
      "1661 loss_D: 34.585617 loss_N: 0.33193386 loss_f: 5.89622\n",
      "1661 Train Loss 40.81377\n",
      "1662 loss_D: 34.594875 loss_N: 0.33331886 loss_f: 5.861579\n",
      "1662 Train Loss 40.789776\n",
      "1663 loss_D: 34.573082 loss_N: 0.33432686 loss_f: 5.866001\n",
      "1663 Train Loss 40.77341\n",
      "1664 loss_D: 34.581448 loss_N: 0.33531478 loss_f: 5.831262\n",
      "1664 Train Loss 40.748024\n",
      "1665 loss_D: 34.56813 loss_N: 0.3350162 loss_f: 5.8257914\n",
      "1665 Train Loss 40.728935\n",
      "1666 loss_D: 34.56691 loss_N: 0.33498284 loss_f: 5.804281\n",
      "1666 Train Loss 40.706177\n",
      "1667 loss_D: 34.536285 loss_N: 0.33357543 loss_f: 5.8165903\n",
      "1667 Train Loss 40.68645\n",
      "1668 loss_D: 34.51542 loss_N: 0.3331989 loss_f: 5.8237457\n",
      "1668 Train Loss 40.672363\n",
      "1669 loss_D: 34.501087 loss_N: 0.33234495 loss_f: 5.8224516\n",
      "1669 Train Loss 40.655884\n",
      "1670 loss_D: 34.45933 loss_N: 0.33255684 loss_f: 5.8402677\n",
      "1670 Train Loss 40.632156\n",
      "1671 loss_D: 34.4018 loss_N: 0.33394238 loss_f: 5.859796\n",
      "1671 Train Loss 40.595535\n",
      "1672 loss_D: 34.345654 loss_N: 0.33686343 loss_f: 5.857394\n",
      "1672 Train Loss 40.539913\n",
      "1673 loss_D: 34.231625 loss_N: 0.34343937 loss_f: 5.901508\n",
      "1673 Train Loss 40.476574\n",
      "1674 loss_D: 34.166553 loss_N: 0.35013092 loss_f: 5.884007\n",
      "1674 Train Loss 40.400692\n",
      "1675 loss_D: 34.0999 loss_N: 0.35481608 loss_f: 5.901556\n",
      "1675 Train Loss 40.35627\n",
      "1676 loss_D: 34.02352 loss_N: 0.3532884 loss_f: 5.9306493\n",
      "1676 Train Loss 40.307457\n",
      "1677 loss_D: 34.05969 loss_N: 0.3514764 loss_f: 5.8688717\n",
      "1677 Train Loss 40.280037\n",
      "1678 loss_D: 34.017365 loss_N: 0.34863013 loss_f: 5.8926253\n",
      "1678 Train Loss 40.258617\n",
      "1679 loss_D: 33.988636 loss_N: 0.34580022 loss_f: 5.9018188\n",
      "1679 Train Loss 40.23625\n",
      "1680 loss_D: 33.94869 loss_N: 0.34574264 loss_f: 5.9215508\n",
      "1680 Train Loss 40.21598\n",
      "1681 loss_D: 33.919777 loss_N: 0.34608364 loss_f: 5.9281936\n",
      "1681 Train Loss 40.194054\n",
      "1682 loss_D: 33.9211 loss_N: 0.34737146 loss_f: 5.89731\n",
      "1682 Train Loss 40.16578\n",
      "1683 loss_D: 33.93393 loss_N: 0.3492142 loss_f: 5.848868\n",
      "1683 Train Loss 40.13201\n",
      "1684 loss_D: 33.95465 loss_N: 0.35210666 loss_f: 5.8000665\n",
      "1684 Train Loss 40.106827\n",
      "1685 loss_D: 33.971592 loss_N: 0.35041395 loss_f: 5.7590594\n",
      "1685 Train Loss 40.081066\n",
      "1686 loss_D: 33.9901 loss_N: 0.34877655 loss_f: 5.720039\n",
      "1686 Train Loss 40.058918\n",
      "1687 loss_D: 33.982784 loss_N: 0.34778076 loss_f: 5.712831\n",
      "1687 Train Loss 40.043396\n",
      "1688 loss_D: 33.969006 loss_N: 0.34745908 loss_f: 5.70576\n",
      "1688 Train Loss 40.022224\n",
      "1689 loss_D: 33.939354 loss_N: 0.34799957 loss_f: 5.7166524\n",
      "1689 Train Loss 40.004005\n",
      "1690 loss_D: 33.923344 loss_N: 0.34841326 loss_f: 5.7088575\n",
      "1690 Train Loss 39.980614\n",
      "1691 loss_D: 33.881184 loss_N: 0.34980693 loss_f: 5.725697\n",
      "1691 Train Loss 39.956688\n",
      "1692 loss_D: 33.843185 loss_N: 0.35068843 loss_f: 5.7450123\n",
      "1692 Train Loss 39.93889\n",
      "1693 loss_D: 33.776054 loss_N: 0.3519314 loss_f: 5.784871\n",
      "1693 Train Loss 39.912857\n",
      "1694 loss_D: 33.717705 loss_N: 0.35220537 loss_f: 5.8041973\n",
      "1694 Train Loss 39.874107\n",
      "1695 loss_D: 33.58124 loss_N: 0.35408643 loss_f: 5.887205\n",
      "1695 Train Loss 39.822533\n",
      "1696 loss_D: 33.512554 loss_N: 0.35540158 loss_f: 5.9105496\n",
      "1696 Train Loss 39.778503\n",
      "1697 loss_D: 33.47105 loss_N: 0.35601687 loss_f: 5.9090266\n",
      "1697 Train Loss 39.736095\n",
      "1698 loss_D: 33.38811 loss_N: 0.35913202 loss_f: 5.9637046\n",
      "1698 Train Loss 39.710945\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1699 loss_D: 33.37303 loss_N: 0.36025688 loss_f: 5.957551\n",
      "1699 Train Loss 39.690838\n",
      "1700 loss_D: 33.34966 loss_N: 0.3623961 loss_f: 5.954117\n",
      "1700 Train Loss 39.666172\n",
      "1701 loss_D: 33.290596 loss_N: 0.36596814 loss_f: 5.982007\n",
      "1701 Train Loss 39.63857\n",
      "1702 loss_D: 33.251507 loss_N: 0.3709015 loss_f: 5.982281\n",
      "1702 Train Loss 39.60469\n",
      "1703 loss_D: 33.18932 loss_N: 0.37491184 loss_f: 6.0105467\n",
      "1703 Train Loss 39.57478\n",
      "1704 loss_D: 33.214798 loss_N: 0.3718812 loss_f: 5.9569983\n",
      "1704 Train Loss 39.543674\n",
      "1705 loss_D: 33.202072 loss_N: 0.37005645 loss_f: 5.9496913\n",
      "1705 Train Loss 39.52182\n",
      "1706 loss_D: 33.20648 loss_N: 0.37120694 loss_f: 5.9196415\n",
      "1706 Train Loss 39.49733\n",
      "1707 loss_D: 33.154507 loss_N: 0.37361035 loss_f: 5.9452243\n",
      "1707 Train Loss 39.473343\n",
      "1708 loss_D: 33.087738 loss_N: 0.37913755 loss_f: 5.9762063\n",
      "1708 Train Loss 39.443085\n",
      "1709 loss_D: 33.02377 loss_N: 0.38109377 loss_f: 6.0022616\n",
      "1709 Train Loss 39.407124\n",
      "1710 loss_D: 32.955418 loss_N: 0.38819546 loss_f: 6.0484786\n",
      "1710 Train Loss 39.39209\n",
      "1711 loss_D: 32.95929 loss_N: 0.3825436 loss_f: 6.0045557\n",
      "1711 Train Loss 39.34639\n",
      "1712 loss_D: 32.951538 loss_N: 0.3762816 loss_f: 6.001285\n",
      "1712 Train Loss 39.329105\n",
      "1713 loss_D: 32.976913 loss_N: 0.3804096 loss_f: 5.9767838\n",
      "1713 Train Loss 39.334106\n",
      "1714 loss_D: 32.962692 loss_N: 0.37813073 loss_f: 5.9779263\n",
      "1714 Train Loss 39.31875\n",
      "1715 loss_D: 32.93463 loss_N: 0.37578818 loss_f: 5.9929113\n",
      "1715 Train Loss 39.303333\n",
      "1716 loss_D: 32.906773 loss_N: 0.37504822 loss_f: 6.007957\n",
      "1716 Train Loss 39.28978\n",
      "1717 loss_D: 32.892155 loss_N: 0.37336183 loss_f: 6.014808\n",
      "1717 Train Loss 39.280327\n",
      "1718 loss_D: 32.884617 loss_N: 0.3738274 loss_f: 6.012028\n",
      "1718 Train Loss 39.270473\n",
      "1719 loss_D: 32.861504 loss_N: 0.37395912 loss_f: 6.0256357\n",
      "1719 Train Loss 39.261097\n",
      "1720 loss_D: 32.865013 loss_N: 0.3738194 loss_f: 6.012954\n",
      "1720 Train Loss 39.25179\n",
      "1721 loss_D: 32.85431 loss_N: 0.37303123 loss_f: 6.0137286\n",
      "1721 Train Loss 39.24107\n",
      "1722 loss_D: 32.84567 loss_N: 0.37083113 loss_f: 6.0134506\n",
      "1722 Train Loss 39.22995\n",
      "1723 loss_D: 32.8015 loss_N: 0.3707167 loss_f: 6.0428653\n",
      "1723 Train Loss 39.21508\n",
      "1724 loss_D: 32.790714 loss_N: 0.366225 loss_f: 6.0409346\n",
      "1724 Train Loss 39.197876\n",
      "1725 loss_D: 32.76163 loss_N: 0.36276966 loss_f: 6.0564356\n",
      "1725 Train Loss 39.180836\n",
      "1726 loss_D: 32.73108 loss_N: 0.36217263 loss_f: 6.0716805\n",
      "1726 Train Loss 39.164932\n",
      "1727 loss_D: 32.68785 loss_N: 0.36311522 loss_f: 6.096894\n",
      "1727 Train Loss 39.147858\n",
      "1728 loss_D: 32.658993 loss_N: 0.36300972 loss_f: 6.1008873\n",
      "1728 Train Loss 39.12289\n",
      "1729 loss_D: 32.56867 loss_N: 0.36812764 loss_f: 6.141394\n",
      "1729 Train Loss 39.07819\n",
      "1730 loss_D: 32.534805 loss_N: 0.37108642 loss_f: 6.1269617\n",
      "1730 Train Loss 39.032852\n",
      "1731 loss_D: 32.451363 loss_N: 0.37489158 loss_f: 6.1601357\n",
      "1731 Train Loss 38.986393\n",
      "1732 loss_D: 32.430347 loss_N: 0.37478778 loss_f: 6.1255655\n",
      "1732 Train Loss 38.9307\n",
      "1733 loss_D: 32.404884 loss_N: 0.3679662 loss_f: 6.093242\n",
      "1733 Train Loss 38.866093\n",
      "1734 loss_D: 32.411648 loss_N: 0.3566776 loss_f: 6.0431733\n",
      "1734 Train Loss 38.8115\n",
      "1735 loss_D: 32.41279 loss_N: 0.34979087 loss_f: 5.996794\n",
      "1735 Train Loss 38.759377\n",
      "1736 loss_D: 32.408466 loss_N: 0.3430956 loss_f: 5.957128\n",
      "1736 Train Loss 38.708687\n",
      "1737 loss_D: 32.375195 loss_N: 0.3412706 loss_f: 5.963441\n",
      "1737 Train Loss 38.679905\n",
      "1738 loss_D: 32.341797 loss_N: 0.33981434 loss_f: 5.9637833\n",
      "1738 Train Loss 38.645393\n",
      "1739 loss_D: 32.330227 loss_N: 0.3405554 loss_f: 5.941914\n",
      "1739 Train Loss 38.612698\n",
      "1740 loss_D: 32.2551 loss_N: 0.33865947 loss_f: 5.9982824\n",
      "1740 Train Loss 38.592045\n",
      "1741 loss_D: 32.29554 loss_N: 0.3393757 loss_f: 5.9346805\n",
      "1741 Train Loss 38.569595\n",
      "1742 loss_D: 32.334167 loss_N: 0.3386705 loss_f: 5.8781266\n",
      "1742 Train Loss 38.550964\n",
      "1743 loss_D: 32.340893 loss_N: 0.3366278 loss_f: 5.85414\n",
      "1743 Train Loss 38.531662\n",
      "1744 loss_D: 32.34054 loss_N: 0.33234456 loss_f: 5.830857\n",
      "1744 Train Loss 38.503742\n",
      "1745 loss_D: 32.28124 loss_N: 0.33045298 loss_f: 5.877757\n",
      "1745 Train Loss 38.48945\n",
      "1746 loss_D: 32.259274 loss_N: 0.33078125 loss_f: 5.8689523\n",
      "1746 Train Loss 38.459007\n",
      "1747 loss_D: 32.23143 loss_N: 0.33111385 loss_f: 5.8788795\n",
      "1747 Train Loss 38.441425\n",
      "1748 loss_D: 32.186916 loss_N: 0.33246338 loss_f: 5.9004025\n",
      "1748 Train Loss 38.41978\n",
      "1749 loss_D: 32.14059 loss_N: 0.33311543 loss_f: 5.9247007\n",
      "1749 Train Loss 38.398407\n",
      "1750 loss_D: 32.11989 loss_N: 0.3337642 loss_f: 5.9242606\n",
      "1750 Train Loss 38.37791\n",
      "1751 loss_D: 32.087704 loss_N: 0.33449808 loss_f: 5.9368978\n",
      "1751 Train Loss 38.3591\n",
      "1752 loss_D: 32.09353 loss_N: 0.33597636 loss_f: 5.9132814\n",
      "1752 Train Loss 38.342785\n",
      "1753 loss_D: 32.05125 loss_N: 0.33619896 loss_f: 5.9442835\n",
      "1753 Train Loss 38.331734\n",
      "1754 loss_D: 32.048386 loss_N: 0.33569896 loss_f: 5.9394274\n",
      "1754 Train Loss 38.32351\n",
      "1755 loss_D: 31.886593 loss_N: 0.33532637 loss_f: 6.2203116\n",
      "1755 Train Loss 38.44223\n",
      "1756 loss_D: 32.0215 loss_N: 0.335611 loss_f: 5.961819\n",
      "1756 Train Loss 38.318928\n",
      "1757 loss_D: 32.00773 loss_N: 0.3343286 loss_f: 5.9661407\n",
      "1757 Train Loss 38.308197\n",
      "1758 loss_D: 31.97352 loss_N: 0.33391464 loss_f: 5.986043\n",
      "1758 Train Loss 38.293476\n",
      "1759 loss_D: 31.93641 loss_N: 0.33204365 loss_f: 6.0056553\n",
      "1759 Train Loss 38.27411\n",
      "1760 loss_D: 31.870834 loss_N: 0.3295063 loss_f: 6.043362\n",
      "1760 Train Loss 38.243702\n",
      "1761 loss_D: 31.827892 loss_N: 0.32433465 loss_f: 6.06276\n",
      "1761 Train Loss 38.214985\n",
      "1762 loss_D: 31.741798 loss_N: 0.32529986 loss_f: 6.127678\n",
      "1762 Train Loss 38.194775\n",
      "1763 loss_D: 31.770512 loss_N: 0.32531193 loss_f: 6.0842266\n",
      "1763 Train Loss 38.180054\n",
      "1764 loss_D: 31.741224 loss_N: 0.32624486 loss_f: 6.103383\n",
      "1764 Train Loss 38.170853\n",
      "1765 loss_D: 31.686306 loss_N: 0.32755396 loss_f: 6.144804\n",
      "1765 Train Loss 38.15866\n",
      "1766 loss_D: 31.621353 loss_N: 0.3298402 loss_f: 6.190801\n",
      "1766 Train Loss 38.141994\n",
      "1767 loss_D: 31.538095 loss_N: 0.33130467 loss_f: 6.25416\n",
      "1767 Train Loss 38.123558\n",
      "1768 loss_D: 31.476162 loss_N: 0.33301863 loss_f: 6.295638\n",
      "1768 Train Loss 38.10482\n",
      "1769 loss_D: 31.430655 loss_N: 0.33463553 loss_f: 6.3240504\n",
      "1769 Train Loss 38.08934\n",
      "1770 loss_D: 31.403711 loss_N: 0.3363661 loss_f: 6.3274794\n",
      "1770 Train Loss 38.06756\n",
      "1771 loss_D: 31.349155 loss_N: 0.33869746 loss_f: 6.355732\n",
      "1771 Train Loss 38.043587\n",
      "1772 loss_D: 31.323343 loss_N: 0.34218416 loss_f: 6.3437057\n",
      "1772 Train Loss 38.00923\n",
      "1773 loss_D: 31.254833 loss_N: 0.3456542 loss_f: 6.3717756\n",
      "1773 Train Loss 37.972263\n",
      "1774 loss_D: 31.201874 loss_N: 0.34871292 loss_f: 6.386611\n",
      "1774 Train Loss 37.9372\n",
      "1775 loss_D: 31.146309 loss_N: 0.35062632 loss_f: 6.4047465\n",
      "1775 Train Loss 37.90168\n",
      "1776 loss_D: 31.073761 loss_N: 0.3524132 loss_f: 6.4430637\n",
      "1776 Train Loss 37.86924\n",
      "1777 loss_D: 31.04505 loss_N: 0.3554071 loss_f: 6.439231\n",
      "1777 Train Loss 37.839687\n",
      "1778 loss_D: 30.974562 loss_N: 0.3592431 loss_f: 6.473486\n",
      "1778 Train Loss 37.80729\n",
      "1779 loss_D: 30.939177 loss_N: 0.36476338 loss_f: 6.4675217\n",
      "1779 Train Loss 37.77146\n",
      "1780 loss_D: 30.950062 loss_N: 0.3669412 loss_f: 6.4239326\n",
      "1780 Train Loss 37.740936\n",
      "1781 loss_D: 30.967865 loss_N: 0.36919534 loss_f: 6.399933\n",
      "1781 Train Loss 37.73699\n",
      "1782 loss_D: 30.945044 loss_N: 0.36969185 loss_f: 6.384867\n",
      "1782 Train Loss 37.699604\n",
      "1783 loss_D: 30.961737 loss_N: 0.36803773 loss_f: 6.3516536\n",
      "1783 Train Loss 37.681427\n",
      "1784 loss_D: 30.958467 loss_N: 0.36835912 loss_f: 6.332645\n",
      "1784 Train Loss 37.65947\n",
      "1785 loss_D: 30.972006 loss_N: 0.36936927 loss_f: 6.3057294\n",
      "1785 Train Loss 37.647106\n",
      "1786 loss_D: 30.963205 loss_N: 0.3703247 loss_f: 6.307664\n",
      "1786 Train Loss 37.641193\n",
      "1787 loss_D: 30.961355 loss_N: 0.37104192 loss_f: 6.3047957\n",
      "1787 Train Loss 37.63719\n",
      "1788 loss_D: 30.957031 loss_N: 0.3714027 loss_f: 6.302895\n",
      "1788 Train Loss 37.63133\n",
      "1789 loss_D: 30.944687 loss_N: 0.3707647 loss_f: 6.3068333\n",
      "1789 Train Loss 37.622284\n",
      "1790 loss_D: 30.909037 loss_N: 0.36926883 loss_f: 6.334914\n",
      "1790 Train Loss 37.61322\n",
      "1791 loss_D: 30.915272 loss_N: 0.3682186 loss_f: 6.309666\n",
      "1791 Train Loss 37.59316\n",
      "1792 loss_D: 30.884321 loss_N: 0.36955053 loss_f: 6.320226\n",
      "1792 Train Loss 37.574097\n",
      "1793 loss_D: 30.864325 loss_N: 0.36989492 loss_f: 6.323712\n",
      "1793 Train Loss 37.55793\n",
      "1794 loss_D: 30.817348 loss_N: 0.37209174 loss_f: 6.3486905\n",
      "1794 Train Loss 37.53813\n",
      "1795 loss_D: 30.792852 loss_N: 0.37472412 loss_f: 6.3543496\n",
      "1795 Train Loss 37.521927\n",
      "1796 loss_D: 30.753431 loss_N: 0.37708533 loss_f: 6.378291\n",
      "1796 Train Loss 37.50881\n",
      "1797 loss_D: 30.737974 loss_N: 0.37861302 loss_f: 6.3840575\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1797 Train Loss 37.500645\n",
      "1798 loss_D: 30.71983 loss_N: 0.37842008 loss_f: 6.3944116\n",
      "1798 Train Loss 37.49266\n",
      "1799 loss_D: 30.706806 loss_N: 0.37895426 loss_f: 6.393142\n",
      "1799 Train Loss 37.4789\n",
      "1800 loss_D: 30.670242 loss_N: 0.378068 loss_f: 6.4146156\n",
      "1800 Train Loss 37.46293\n",
      "1801 loss_D: 30.644081 loss_N: 0.37736088 loss_f: 6.4267054\n",
      "1801 Train Loss 37.448147\n",
      "1802 loss_D: 30.623474 loss_N: 0.37720302 loss_f: 6.429883\n",
      "1802 Train Loss 37.43056\n",
      "1803 loss_D: 30.601221 loss_N: 0.37620342 loss_f: 6.439152\n",
      "1803 Train Loss 37.416576\n",
      "1804 loss_D: 30.606268 loss_N: 0.37701702 loss_f: 6.4065037\n",
      "1804 Train Loss 37.389786\n",
      "1805 loss_D: 30.642036 loss_N: 0.3771156 loss_f: 6.3450484\n",
      "1805 Train Loss 37.3642\n",
      "1806 loss_D: 30.67503 loss_N: 0.37755075 loss_f: 6.281503\n",
      "1806 Train Loss 37.334084\n",
      "1807 loss_D: 30.707552 loss_N: 0.37914577 loss_f: 6.232456\n",
      "1807 Train Loss 37.319153\n",
      "1808 loss_D: 30.663387 loss_N: 0.3784745 loss_f: 6.2546344\n",
      "1808 Train Loss 37.296497\n",
      "1809 loss_D: 30.686043 loss_N: 0.3752354 loss_f: 6.2231874\n",
      "1809 Train Loss 37.284466\n",
      "1810 loss_D: 30.636461 loss_N: 0.37554124 loss_f: 6.25743\n",
      "1810 Train Loss 37.269432\n",
      "1811 loss_D: 30.59128 loss_N: 0.37604544 loss_f: 6.2739716\n",
      "1811 Train Loss 37.241295\n",
      "1812 loss_D: 30.524593 loss_N: 0.3780888 loss_f: 6.303016\n",
      "1812 Train Loss 37.205696\n",
      "1813 loss_D: 30.473886 loss_N: 0.38041237 loss_f: 6.3200183\n",
      "1813 Train Loss 37.174316\n",
      "1814 loss_D: 30.433218 loss_N: 0.3818492 loss_f: 6.3254356\n",
      "1814 Train Loss 37.140503\n",
      "1815 loss_D: 30.40613 loss_N: 0.38415444 loss_f: 6.318116\n",
      "1815 Train Loss 37.108402\n",
      "1816 loss_D: 30.395918 loss_N: 0.37748045 loss_f: 6.406075\n",
      "1816 Train Loss 37.179474\n",
      "1817 loss_D: 30.398905 loss_N: 0.3812044 loss_f: 6.308798\n",
      "1817 Train Loss 37.088905\n",
      "1818 loss_D: 30.39844 loss_N: 0.38139796 loss_f: 6.2644687\n",
      "1818 Train Loss 37.044304\n",
      "1819 loss_D: 30.39768 loss_N: 0.3790759 loss_f: 6.2409444\n",
      "1819 Train Loss 37.0177\n",
      "1820 loss_D: 30.376793 loss_N: 0.3786204 loss_f: 6.232384\n",
      "1820 Train Loss 36.987797\n",
      "1821 loss_D: 30.316065 loss_N: 0.37825125 loss_f: 6.2557797\n",
      "1821 Train Loss 36.950096\n",
      "1822 loss_D: 30.273046 loss_N: 0.38054815 loss_f: 6.2607265\n",
      "1822 Train Loss 36.91432\n",
      "1823 loss_D: 30.157064 loss_N: 0.38574532 loss_f: 6.330547\n",
      "1823 Train Loss 36.873356\n",
      "1824 loss_D: 30.130375 loss_N: 0.3895984 loss_f: 6.3232484\n",
      "1824 Train Loss 36.843224\n",
      "1825 loss_D: 30.066397 loss_N: 0.39300632 loss_f: 6.356049\n",
      "1825 Train Loss 36.815453\n",
      "1826 loss_D: 30.015219 loss_N: 0.3936614 loss_f: 6.373399\n",
      "1826 Train Loss 36.78228\n",
      "1827 loss_D: 29.975433 loss_N: 0.394489 loss_f: 6.37381\n",
      "1827 Train Loss 36.743732\n",
      "1828 loss_D: 29.889471 loss_N: 0.39481577 loss_f: 6.4214387\n",
      "1828 Train Loss 36.705727\n",
      "1829 loss_D: 29.934546 loss_N: 0.39045697 loss_f: 6.358\n",
      "1829 Train Loss 36.683002\n",
      "1830 loss_D: 29.905567 loss_N: 0.39190933 loss_f: 6.3649383\n",
      "1830 Train Loss 36.662415\n",
      "1831 loss_D: 29.880693 loss_N: 0.39295176 loss_f: 6.3726497\n",
      "1831 Train Loss 36.646294\n",
      "1832 loss_D: 29.882078 loss_N: 0.39441678 loss_f: 6.3583875\n",
      "1832 Train Loss 36.634884\n",
      "1833 loss_D: 29.877356 loss_N: 0.3983511 loss_f: 6.346309\n",
      "1833 Train Loss 36.622017\n",
      "1834 loss_D: 29.870611 loss_N: 0.39972177 loss_f: 6.323684\n",
      "1834 Train Loss 36.594017\n",
      "1835 loss_D: 29.874374 loss_N: 0.3996178 loss_f: 6.301878\n",
      "1835 Train Loss 36.57587\n",
      "1836 loss_D: 29.890993 loss_N: 0.39782357 loss_f: 6.270903\n",
      "1836 Train Loss 36.55972\n",
      "1837 loss_D: 29.8897 loss_N: 0.39806855 loss_f: 6.2571483\n",
      "1837 Train Loss 36.544918\n",
      "1838 loss_D: 29.886217 loss_N: 0.3968556 loss_f: 6.2405934\n",
      "1838 Train Loss 36.523666\n",
      "1839 loss_D: 29.856537 loss_N: 0.39961326 loss_f: 6.247885\n",
      "1839 Train Loss 36.504036\n",
      "1840 loss_D: 29.829042 loss_N: 0.39978772 loss_f: 6.2546797\n",
      "1840 Train Loss 36.48351\n",
      "1841 loss_D: 29.800695 loss_N: 0.40261233 loss_f: 6.2593007\n",
      "1841 Train Loss 36.46261\n",
      "1842 loss_D: 29.745022 loss_N: 0.40737566 loss_f: 6.2890577\n",
      "1842 Train Loss 36.441456\n",
      "1843 loss_D: 29.707087 loss_N: 0.4035214 loss_f: 6.3283124\n",
      "1843 Train Loss 36.43892\n",
      "1844 loss_D: 29.710377 loss_N: 0.40510818 loss_f: 6.2981553\n",
      "1844 Train Loss 36.41364\n",
      "1845 loss_D: 29.702225 loss_N: 0.40388477 loss_f: 6.29032\n",
      "1845 Train Loss 36.39643\n",
      "1846 loss_D: 29.688206 loss_N: 0.39929628 loss_f: 6.281091\n",
      "1846 Train Loss 36.36859\n",
      "1847 loss_D: 29.682161 loss_N: 0.39580113 loss_f: 6.274767\n",
      "1847 Train Loss 36.35273\n",
      "1848 loss_D: 29.654137 loss_N: 0.39045432 loss_f: 6.2919297\n",
      "1848 Train Loss 36.33652\n",
      "1849 loss_D: 29.682205 loss_N: 0.38946372 loss_f: 6.2469234\n",
      "1849 Train Loss 36.318592\n",
      "1850 loss_D: 29.67398 loss_N: 0.39048722 loss_f: 6.2382326\n",
      "1850 Train Loss 36.3027\n",
      "1851 loss_D: 29.678244 loss_N: 0.39142466 loss_f: 6.20894\n",
      "1851 Train Loss 36.278606\n",
      "1852 loss_D: 29.62437 loss_N: 0.39924416 loss_f: 6.2444396\n",
      "1852 Train Loss 36.268055\n",
      "1853 loss_D: 29.635681 loss_N: 0.39485574 loss_f: 6.2068543\n",
      "1853 Train Loss 36.237392\n",
      "1854 loss_D: 29.622911 loss_N: 0.39112234 loss_f: 6.2012568\n",
      "1854 Train Loss 36.21529\n",
      "1855 loss_D: 29.57088 loss_N: 0.39015394 loss_f: 6.2273984\n",
      "1855 Train Loss 36.188435\n",
      "1856 loss_D: 29.548536 loss_N: 0.38885486 loss_f: 6.2338195\n",
      "1856 Train Loss 36.17121\n",
      "1857 loss_D: 29.491919 loss_N: 0.3881628 loss_f: 6.272622\n",
      "1857 Train Loss 36.152702\n",
      "1858 loss_D: 29.447512 loss_N: 0.3881931 loss_f: 6.299315\n",
      "1858 Train Loss 36.13502\n",
      "1859 loss_D: 29.410969 loss_N: 0.38748476 loss_f: 6.3217387\n",
      "1859 Train Loss 36.120193\n",
      "1860 loss_D: 29.383259 loss_N: 0.3857009 loss_f: 6.3357615\n",
      "1860 Train Loss 36.10472\n",
      "1861 loss_D: 29.35273 loss_N: 0.38430488 loss_f: 6.355501\n",
      "1861 Train Loss 36.092537\n",
      "1862 loss_D: 29.342243 loss_N: 0.38287282 loss_f: 6.354987\n",
      "1862 Train Loss 36.080105\n",
      "1863 loss_D: 29.310362 loss_N: 0.38209447 loss_f: 6.370158\n",
      "1863 Train Loss 36.062614\n",
      "1864 loss_D: 29.294668 loss_N: 0.38098606 loss_f: 6.3627563\n",
      "1864 Train Loss 36.03841\n",
      "1865 loss_D: 29.231056 loss_N: 0.3844379 loss_f: 6.3990874\n",
      "1865 Train Loss 36.01458\n",
      "1866 loss_D: 29.24125 loss_N: 0.38268924 loss_f: 6.376491\n",
      "1866 Train Loss 36.000427\n",
      "1867 loss_D: 29.254225 loss_N: 0.38619593 loss_f: 6.3407836\n",
      "1867 Train Loss 35.981205\n",
      "1868 loss_D: 29.227062 loss_N: 0.38951406 loss_f: 6.344725\n",
      "1868 Train Loss 35.961304\n",
      "1869 loss_D: 29.231567 loss_N: 0.39025745 loss_f: 6.3212137\n",
      "1869 Train Loss 35.94304\n",
      "1870 loss_D: 29.240633 loss_N: 0.39161152 loss_f: 6.2857137\n",
      "1870 Train Loss 35.917957\n",
      "1871 loss_D: 29.256065 loss_N: 0.3887348 loss_f: 6.253051\n",
      "1871 Train Loss 35.89785\n",
      "1872 loss_D: 29.256065 loss_N: 0.38552883 loss_f: 6.2392263\n",
      "1872 Train Loss 35.88082\n",
      "1873 loss_D: 29.254675 loss_N: 0.3818729 loss_f: 6.2305226\n",
      "1873 Train Loss 35.86707\n",
      "1874 loss_D: 29.242624 loss_N: 0.37676996 loss_f: 6.2309475\n",
      "1874 Train Loss 35.85034\n",
      "1875 loss_D: 29.234093 loss_N: 0.37238225 loss_f: 6.225182\n",
      "1875 Train Loss 35.831657\n",
      "1876 loss_D: 29.215017 loss_N: 0.3679035 loss_f: 6.2276545\n",
      "1876 Train Loss 35.810574\n",
      "1877 loss_D: 29.20945 loss_N: 0.36589512 loss_f: 6.215535\n",
      "1877 Train Loss 35.79088\n",
      "1878 loss_D: 29.166637 loss_N: 0.3645927 loss_f: 6.234764\n",
      "1878 Train Loss 35.765995\n",
      "1879 loss_D: 29.14092 loss_N: 0.3696891 loss_f: 6.2455997\n",
      "1879 Train Loss 35.75621\n",
      "1880 loss_D: 29.1043 loss_N: 0.36496198 loss_f: 6.251197\n",
      "1880 Train Loss 35.72046\n",
      "1881 loss_D: 29.054861 loss_N: 0.36533618 loss_f: 6.2864485\n",
      "1881 Train Loss 35.706646\n",
      "1882 loss_D: 29.060034 loss_N: 0.36462846 loss_f: 6.272727\n",
      "1882 Train Loss 35.697388\n",
      "1883 loss_D: 29.038637 loss_N: 0.36477312 loss_f: 6.2889695\n",
      "1883 Train Loss 35.69238\n",
      "1884 loss_D: 29.01688 loss_N: 0.36482129 loss_f: 6.300898\n",
      "1884 Train Loss 35.6826\n",
      "1885 loss_D: 28.99615 loss_N: 0.36415312 loss_f: 6.3061996\n",
      "1885 Train Loss 35.6665\n",
      "1886 loss_D: 28.972118 loss_N: 0.36250654 loss_f: 6.3130455\n",
      "1886 Train Loss 35.64767\n",
      "1887 loss_D: 28.992287 loss_N: 0.3597693 loss_f: 6.2761116\n",
      "1887 Train Loss 35.628166\n",
      "1888 loss_D: 29.043919 loss_N: 0.3571298 loss_f: 6.219847\n",
      "1888 Train Loss 35.620895\n",
      "1889 loss_D: 29.033659 loss_N: 0.35540378 loss_f: 6.1967173\n",
      "1889 Train Loss 35.58578\n",
      "1890 loss_D: 29.014687 loss_N: 0.35613862 loss_f: 6.1933804\n",
      "1890 Train Loss 35.564205\n",
      "1891 loss_D: 28.973574 loss_N: 0.35657656 loss_f: 6.2004743\n",
      "1891 Train Loss 35.530624\n",
      "1892 loss_D: 28.948462 loss_N: 0.35562217 loss_f: 6.203863\n",
      "1892 Train Loss 35.507946\n",
      "1893 loss_D: 28.916883 loss_N: 0.3546017 loss_f: 6.2223215\n",
      "1893 Train Loss 35.493805\n",
      "1894 loss_D: 28.907156 loss_N: 0.35334662 loss_f: 6.2223687\n",
      "1894 Train Loss 35.482872\n",
      "1895 loss_D: 28.902418 loss_N: 0.35198432 loss_f: 6.221358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1895 Train Loss 35.47576\n",
      "1896 loss_D: 28.885609 loss_N: 0.34954795 loss_f: 6.224775\n",
      "1896 Train Loss 35.45993\n",
      "1897 loss_D: 28.854612 loss_N: 0.34751588 loss_f: 6.240068\n",
      "1897 Train Loss 35.442196\n",
      "1898 loss_D: 28.833593 loss_N: 0.3448228 loss_f: 6.243091\n",
      "1898 Train Loss 35.421505\n",
      "1899 loss_D: 28.745005 loss_N: 0.34180865 loss_f: 6.319418\n",
      "1899 Train Loss 35.40623\n",
      "1900 loss_D: 28.77006 loss_N: 0.34259683 loss_f: 6.2805047\n",
      "1900 Train Loss 35.39316\n",
      "1901 loss_D: 28.757898 loss_N: 0.34219512 loss_f: 6.282205\n",
      "1901 Train Loss 35.382298\n",
      "1902 loss_D: 28.730318 loss_N: 0.34162676 loss_f: 6.3022475\n",
      "1902 Train Loss 35.37419\n",
      "1903 loss_D: 28.696562 loss_N: 0.33978406 loss_f: 6.326877\n",
      "1903 Train Loss 35.363224\n",
      "1904 loss_D: 28.653692 loss_N: 0.33719432 loss_f: 6.35686\n",
      "1904 Train Loss 35.347748\n",
      "1905 loss_D: 28.553392 loss_N: 0.33525303 loss_f: 6.4658866\n",
      "1905 Train Loss 35.35453\n",
      "1906 loss_D: 28.609112 loss_N: 0.33623928 loss_f: 6.391946\n",
      "1906 Train Loss 35.337296\n",
      "1907 loss_D: 28.577106 loss_N: 0.33396977 loss_f: 6.4082813\n",
      "1907 Train Loss 35.31936\n",
      "1908 loss_D: 28.547903 loss_N: 0.3336702 loss_f: 6.4238434\n",
      "1908 Train Loss 35.305416\n",
      "1909 loss_D: 28.528887 loss_N: 0.33483133 loss_f: 6.4279366\n",
      "1909 Train Loss 35.291656\n",
      "1910 loss_D: 28.48998 loss_N: 0.33713406 loss_f: 6.450347\n",
      "1910 Train Loss 35.277462\n",
      "1911 loss_D: 28.490606 loss_N: 0.340391 loss_f: 6.43213\n",
      "1911 Train Loss 35.263126\n",
      "1912 loss_D: 28.46875 loss_N: 0.34286785 loss_f: 6.433861\n",
      "1912 Train Loss 35.24548\n",
      "1913 loss_D: 28.47588 loss_N: 0.34453905 loss_f: 6.409528\n",
      "1913 Train Loss 35.229946\n",
      "1914 loss_D: 28.443455 loss_N: 0.34459856 loss_f: 6.415741\n",
      "1914 Train Loss 35.203796\n",
      "1915 loss_D: 28.405222 loss_N: 0.34485033 loss_f: 6.4172144\n",
      "1915 Train Loss 35.167286\n",
      "1916 loss_D: 28.36113 loss_N: 0.3440554 loss_f: 6.4315505\n",
      "1916 Train Loss 35.136734\n",
      "1917 loss_D: 28.314098 loss_N: 0.34618914 loss_f: 6.448076\n",
      "1917 Train Loss 35.108364\n",
      "1918 loss_D: 28.276512 loss_N: 0.344893 loss_f: 6.4622383\n",
      "1918 Train Loss 35.08364\n",
      "1919 loss_D: 28.21323 loss_N: 0.34405825 loss_f: 6.5018735\n",
      "1919 Train Loss 35.059162\n",
      "1920 loss_D: 28.1552 loss_N: 0.34024984 loss_f: 6.566025\n",
      "1920 Train Loss 35.061474\n",
      "1921 loss_D: 28.184261 loss_N: 0.34215933 loss_f: 6.517887\n",
      "1921 Train Loss 35.044308\n",
      "1922 loss_D: 28.102825 loss_N: 0.33834004 loss_f: 6.5984106\n",
      "1922 Train Loss 35.039577\n",
      "1923 loss_D: 28.14038 loss_N: 0.33999258 loss_f: 6.5398145\n",
      "1923 Train Loss 35.020187\n",
      "1924 loss_D: 28.07328 loss_N: 0.3371299 loss_f: 6.5917106\n",
      "1924 Train Loss 35.00212\n",
      "1925 loss_D: 28.093637 loss_N: 0.3366489 loss_f: 6.5558686\n",
      "1925 Train Loss 34.986156\n",
      "1926 loss_D: 28.07848 loss_N: 0.3337414 loss_f: 6.564336\n",
      "1926 Train Loss 34.97656\n",
      "1927 loss_D: 28.068224 loss_N: 0.33171102 loss_f: 6.565292\n",
      "1927 Train Loss 34.965225\n",
      "1928 loss_D: 28.02814 loss_N: 0.32871372 loss_f: 6.593917\n",
      "1928 Train Loss 34.95077\n",
      "1929 loss_D: 28.016825 loss_N: 0.32736558 loss_f: 6.594405\n",
      "1929 Train Loss 34.938595\n",
      "1930 loss_D: 27.98984 loss_N: 0.3262619 loss_f: 6.6043887\n",
      "1930 Train Loss 34.92049\n",
      "1931 loss_D: 27.986755 loss_N: 0.3267304 loss_f: 6.589719\n",
      "1931 Train Loss 34.903206\n",
      "1932 loss_D: 27.9506 loss_N: 0.32819012 loss_f: 6.603556\n",
      "1932 Train Loss 34.882347\n",
      "1933 loss_D: 27.954231 loss_N: 0.32852632 loss_f: 6.585363\n",
      "1933 Train Loss 34.86812\n",
      "1934 loss_D: 27.931152 loss_N: 0.33029506 loss_f: 6.5943203\n",
      "1934 Train Loss 34.855766\n",
      "1935 loss_D: 27.928421 loss_N: 0.33075213 loss_f: 6.5809665\n",
      "1935 Train Loss 34.840137\n",
      "1936 loss_D: 27.845362 loss_N: 0.32441005 loss_f: 6.840697\n",
      "1936 Train Loss 35.010468\n",
      "1937 loss_D: 27.91373 loss_N: 0.32965767 loss_f: 6.5901937\n",
      "1937 Train Loss 34.833584\n",
      "1938 loss_D: 27.898405 loss_N: 0.3300515 loss_f: 6.592698\n",
      "1938 Train Loss 34.821156\n",
      "1939 loss_D: 27.894176 loss_N: 0.32915318 loss_f: 6.576638\n",
      "1939 Train Loss 34.79997\n",
      "1940 loss_D: 27.862043 loss_N: 0.3266125 loss_f: 6.602549\n",
      "1940 Train Loss 34.791206\n",
      "1941 loss_D: 27.857794 loss_N: 0.32526952 loss_f: 6.6005383\n",
      "1941 Train Loss 34.7836\n",
      "1942 loss_D: 27.839489 loss_N: 0.32417902 loss_f: 6.6148624\n",
      "1942 Train Loss 34.77853\n",
      "1943 loss_D: 27.83098 loss_N: 0.32331404 loss_f: 6.618333\n",
      "1943 Train Loss 34.77263\n",
      "1944 loss_D: 27.816912 loss_N: 0.32241508 loss_f: 6.626197\n",
      "1944 Train Loss 34.765522\n",
      "1945 loss_D: 27.812305 loss_N: 0.32009673 loss_f: 6.621673\n",
      "1945 Train Loss 34.754074\n",
      "1946 loss_D: 27.800978 loss_N: 0.31693593 loss_f: 6.6216483\n",
      "1946 Train Loss 34.739563\n",
      "1947 loss_D: 27.794733 loss_N: 0.31387177 loss_f: 6.614301\n",
      "1947 Train Loss 34.722904\n",
      "1948 loss_D: 27.809605 loss_N: 0.30762666 loss_f: 6.578818\n",
      "1948 Train Loss 34.69605\n",
      "1949 loss_D: 27.781261 loss_N: 0.3023973 loss_f: 6.5920887\n",
      "1949 Train Loss 34.675747\n",
      "1950 loss_D: 27.7845 loss_N: 0.3017908 loss_f: 6.5721617\n",
      "1950 Train Loss 34.65845\n",
      "1951 loss_D: 27.793562 loss_N: 0.30061236 loss_f: 6.553466\n",
      "1951 Train Loss 34.64764\n",
      "1952 loss_D: 27.809294 loss_N: 0.2982642 loss_f: 6.554748\n",
      "1952 Train Loss 34.662304\n",
      "1953 loss_D: 27.79948 loss_N: 0.2995969 loss_f: 6.537067\n",
      "1953 Train Loss 34.636143\n",
      "1954 loss_D: 27.782965 loss_N: 0.3008338 loss_f: 6.5366096\n",
      "1954 Train Loss 34.620407\n",
      "1955 loss_D: 27.780231 loss_N: 0.30166858 loss_f: 6.522188\n",
      "1955 Train Loss 34.604088\n",
      "1956 loss_D: 27.776655 loss_N: 0.30074954 loss_f: 6.5074573\n",
      "1956 Train Loss 34.58486\n",
      "1957 loss_D: 27.813112 loss_N: 0.30162767 loss_f: 6.457854\n",
      "1957 Train Loss 34.572594\n",
      "1958 loss_D: 27.80156 loss_N: 0.29911909 loss_f: 6.457246\n",
      "1958 Train Loss 34.557926\n",
      "1959 loss_D: 27.796219 loss_N: 0.2959 loss_f: 6.4453864\n",
      "1959 Train Loss 34.537506\n",
      "1960 loss_D: 27.805275 loss_N: 0.29337347 loss_f: 6.417949\n",
      "1960 Train Loss 34.516598\n",
      "1961 loss_D: 27.815498 loss_N: 0.2909561 loss_f: 6.384755\n",
      "1961 Train Loss 34.49121\n",
      "1962 loss_D: 27.846827 loss_N: 0.28926653 loss_f: 6.3197393\n",
      "1962 Train Loss 34.455833\n",
      "1963 loss_D: 27.870472 loss_N: 0.2899118 loss_f: 6.2703114\n",
      "1963 Train Loss 34.430695\n",
      "1964 loss_D: 27.937134 loss_N: 0.2909171 loss_f: 6.1681433\n",
      "1964 Train Loss 34.396194\n",
      "1965 loss_D: 27.982985 loss_N: 0.29311824 loss_f: 6.105714\n",
      "1965 Train Loss 34.381817\n",
      "1966 loss_D: 28.007095 loss_N: 0.2928259 loss_f: 6.0455074\n",
      "1966 Train Loss 34.34543\n",
      "1967 loss_D: 27.995058 loss_N: 0.2901349 loss_f: 6.0314684\n",
      "1967 Train Loss 34.31666\n",
      "1968 loss_D: 27.993668 loss_N: 0.28851464 loss_f: 6.0052786\n",
      "1968 Train Loss 34.28746\n",
      "1969 loss_D: 28.004774 loss_N: 0.2832461 loss_f: 5.9821286\n",
      "1969 Train Loss 34.27015\n",
      "1970 loss_D: 27.994093 loss_N: 0.28472543 loss_f: 5.952237\n",
      "1970 Train Loss 34.231056\n",
      "1971 loss_D: 27.97668 loss_N: 0.28467515 loss_f: 5.9509926\n",
      "1971 Train Loss 34.21235\n",
      "1972 loss_D: 27.942802 loss_N: 0.28476796 loss_f: 5.95868\n",
      "1972 Train Loss 34.18625\n",
      "1973 loss_D: 27.89675 loss_N: 0.282291 loss_f: 5.970777\n",
      "1973 Train Loss 34.14982\n",
      "1974 loss_D: 27.877934 loss_N: 0.27999935 loss_f: 5.9654174\n",
      "1974 Train Loss 34.12335\n",
      "1975 loss_D: 27.9026 loss_N: 0.27738637 loss_f: 5.9770203\n",
      "1975 Train Loss 34.157005\n",
      "1976 loss_D: 27.88713 loss_N: 0.27889213 loss_f: 5.93201\n",
      "1976 Train Loss 34.098034\n",
      "1977 loss_D: 27.883049 loss_N: 0.27871528 loss_f: 5.9192433\n",
      "1977 Train Loss 34.08101\n",
      "1978 loss_D: 27.867828 loss_N: 0.27914703 loss_f: 5.9115415\n",
      "1978 Train Loss 34.058517\n",
      "1979 loss_D: 27.830814 loss_N: 0.2787429 loss_f: 5.9236965\n",
      "1979 Train Loss 34.033257\n",
      "1980 loss_D: 27.775192 loss_N: 0.27762914 loss_f: 5.947048\n",
      "1980 Train Loss 33.99987\n",
      "1981 loss_D: 27.713493 loss_N: 0.27499008 loss_f: 5.9807186\n",
      "1981 Train Loss 33.9692\n",
      "1982 loss_D: 27.689877 loss_N: 0.27259633 loss_f: 5.9738417\n",
      "1982 Train Loss 33.936314\n",
      "1983 loss_D: 27.652409 loss_N: 0.26666763 loss_f: 5.9757843\n",
      "1983 Train Loss 33.89486\n",
      "1984 loss_D: 27.640244 loss_N: 0.26569593 loss_f: 5.9619727\n",
      "1984 Train Loss 33.867912\n",
      "1985 loss_D: 27.595675 loss_N: 0.26380068 loss_f: 5.9850297\n",
      "1985 Train Loss 33.844505\n",
      "1986 loss_D: 27.555552 loss_N: 0.26475972 loss_f: 6.006799\n",
      "1986 Train Loss 33.82711\n",
      "1987 loss_D: 27.476706 loss_N: 0.26201618 loss_f: 6.0696297\n",
      "1987 Train Loss 33.80835\n",
      "1988 loss_D: 27.502747 loss_N: 0.26355958 loss_f: 6.0281105\n",
      "1988 Train Loss 33.79442\n",
      "1989 loss_D: 27.482212 loss_N: 0.26391038 loss_f: 6.03803\n",
      "1989 Train Loss 33.784153\n",
      "1990 loss_D: 27.432861 loss_N: 0.26305994 loss_f: 6.072223\n",
      "1990 Train Loss 33.768143\n",
      "1991 loss_D: 27.360561 loss_N: 0.26141873 loss_f: 6.133029\n",
      "1991 Train Loss 33.75501\n",
      "1992 loss_D: 27.317087 loss_N: 0.25964347 loss_f: 6.1641803\n",
      "1992 Train Loss 33.74091\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1993 loss_D: 27.277319 loss_N: 0.25633416 loss_f: 6.1902184\n",
      "1993 Train Loss 33.723873\n",
      "1994 loss_D: 27.240175 loss_N: 0.25487503 loss_f: 6.210181\n",
      "1994 Train Loss 33.70523\n",
      "1995 loss_D: 27.210827 loss_N: 0.25348496 loss_f: 6.2187176\n",
      "1995 Train Loss 33.68303\n",
      "1996 loss_D: 27.143024 loss_N: 0.25261915 loss_f: 6.271163\n",
      "1996 Train Loss 33.666805\n",
      "1997 loss_D: 27.148884 loss_N: 0.25400588 loss_f: 6.2534137\n",
      "1997 Train Loss 33.656303\n",
      "1998 loss_D: 27.128061 loss_N: 0.25314578 loss_f: 6.2647758\n",
      "1998 Train Loss 33.64598\n",
      "1999 loss_D: 27.096027 loss_N: 0.2521002 loss_f: 6.2836075\n",
      "1999 Train Loss 33.631737\n",
      "2000 loss_D: 27.085068 loss_N: 0.2513746 loss_f: 6.285035\n",
      "2000 Train Loss 33.62148\n",
      "2001 loss_D: 27.064453 loss_N: 0.25062382 loss_f: 6.2892785\n",
      "2001 Train Loss 33.604355\n",
      "2002 loss_D: 27.03613 loss_N: 0.24949242 loss_f: 6.2992215\n",
      "2002 Train Loss 33.584846\n",
      "2003 loss_D: 26.997797 loss_N: 0.24891186 loss_f: 6.317318\n",
      "2003 Train Loss 33.564026\n",
      "2004 loss_D: 26.945406 loss_N: 0.24570334 loss_f: 6.4386535\n",
      "2004 Train Loss 33.62976\n",
      "2005 loss_D: 26.979624 loss_N: 0.24774306 loss_f: 6.322769\n",
      "2005 Train Loss 33.550137\n",
      "2006 loss_D: 27.001556 loss_N: 0.2482267 loss_f: 6.275752\n",
      "2006 Train Loss 33.525536\n",
      "2007 loss_D: 27.006687 loss_N: 0.24712825 loss_f: 6.244858\n",
      "2007 Train Loss 33.498672\n",
      "2008 loss_D: 27.03638 loss_N: 0.2480955 loss_f: 6.197846\n",
      "2008 Train Loss 33.482323\n",
      "2009 loss_D: 27.043425 loss_N: 0.24636996 loss_f: 6.174377\n",
      "2009 Train Loss 33.464172\n",
      "2010 loss_D: 27.049799 loss_N: 0.24694745 loss_f: 6.1539116\n",
      "2010 Train Loss 33.45066\n",
      "2011 loss_D: 27.046259 loss_N: 0.24634565 loss_f: 6.1460404\n",
      "2011 Train Loss 33.438644\n",
      "2012 loss_D: 27.047543 loss_N: 0.24611162 loss_f: 6.1283927\n",
      "2012 Train Loss 33.422047\n",
      "2013 loss_D: 27.030659 loss_N: 0.246435 loss_f: 6.130115\n",
      "2013 Train Loss 33.407207\n",
      "2014 loss_D: 27.044365 loss_N: 0.24584438 loss_f: 6.0910134\n",
      "2014 Train Loss 33.38122\n",
      "2015 loss_D: 27.052143 loss_N: 0.24659589 loss_f: 6.0676823\n",
      "2015 Train Loss 33.36642\n",
      "2016 loss_D: 27.04718 loss_N: 0.24750526 loss_f: 6.0491414\n",
      "2016 Train Loss 33.343826\n",
      "2017 loss_D: 27.046293 loss_N: 0.24851823 loss_f: 6.027171\n",
      "2017 Train Loss 33.321983\n",
      "2018 loss_D: 27.054274 loss_N: 0.24831876 loss_f: 6.099427\n",
      "2018 Train Loss 33.40202\n",
      "2019 loss_D: 27.04499 loss_N: 0.24812153 loss_f: 6.0157747\n",
      "2019 Train Loss 33.308887\n",
      "2020 loss_D: 27.039762 loss_N: 0.24812222 loss_f: 6.0009913\n",
      "2020 Train Loss 33.288876\n",
      "2021 loss_D: 27.045877 loss_N: 0.24755765 loss_f: 5.980282\n",
      "2021 Train Loss 33.27372\n",
      "2022 loss_D: 27.045015 loss_N: 0.24775136 loss_f: 5.962821\n",
      "2022 Train Loss 33.25559\n",
      "2023 loss_D: 27.049305 loss_N: 0.2480694 loss_f: 5.9405\n",
      "2023 Train Loss 33.237873\n",
      "2024 loss_D: 27.045109 loss_N: 0.24947888 loss_f: 5.9396195\n",
      "2024 Train Loss 33.234207\n",
      "2025 loss_D: 27.04054 loss_N: 0.24982904 loss_f: 5.9239187\n",
      "2025 Train Loss 33.214287\n",
      "2026 loss_D: 27.047722 loss_N: 0.25018492 loss_f: 5.9024873\n",
      "2026 Train Loss 33.200394\n",
      "2027 loss_D: 27.055231 loss_N: 0.251505 loss_f: 5.868661\n",
      "2027 Train Loss 33.175396\n",
      "2028 loss_D: 27.082556 loss_N: 0.25330856 loss_f: 5.8136883\n",
      "2028 Train Loss 33.14955\n",
      "2029 loss_D: 27.088055 loss_N: 0.2549495 loss_f: 5.781182\n",
      "2029 Train Loss 33.124187\n",
      "2030 loss_D: 27.09974 loss_N: 0.25427032 loss_f: 5.7689495\n",
      "2030 Train Loss 33.12296\n",
      "2031 loss_D: 27.093462 loss_N: 0.25458482 loss_f: 5.7677336\n",
      "2031 Train Loss 33.11578\n",
      "2032 loss_D: 27.10158 loss_N: 0.25614074 loss_f: 5.8120246\n",
      "2032 Train Loss 33.169746\n",
      "2033 loss_D: 27.095121 loss_N: 0.2549708 loss_f: 5.7581005\n",
      "2033 Train Loss 33.108192\n",
      "2034 loss_D: 27.080286 loss_N: 0.25441444 loss_f: 5.758451\n",
      "2034 Train Loss 33.09315\n",
      "2035 loss_D: 27.059406 loss_N: 0.25334418 loss_f: 5.7666965\n",
      "2035 Train Loss 33.079445\n",
      "2036 loss_D: 27.015043 loss_N: 0.25230452 loss_f: 5.7942924\n",
      "2036 Train Loss 33.061638\n",
      "2037 loss_D: 26.967808 loss_N: 0.25207636 loss_f: 5.8213983\n",
      "2037 Train Loss 33.041283\n",
      "2038 loss_D: 26.898462 loss_N: 0.25318074 loss_f: 5.868292\n",
      "2038 Train Loss 33.019936\n",
      "2039 loss_D: 26.85225 loss_N: 0.2550237 loss_f: 5.880701\n",
      "2039 Train Loss 32.987976\n",
      "2040 loss_D: 26.855762 loss_N: 0.2568811 loss_f: 5.846446\n",
      "2040 Train Loss 32.95909\n",
      "2041 loss_D: 26.844906 loss_N: 0.2581431 loss_f: 5.827103\n",
      "2041 Train Loss 32.930153\n",
      "2042 loss_D: 26.834978 loss_N: 0.25860205 loss_f: 5.804698\n",
      "2042 Train Loss 32.898277\n",
      "2043 loss_D: 26.809172 loss_N: 0.2584585 loss_f: 5.797835\n",
      "2043 Train Loss 32.865467\n",
      "2044 loss_D: 26.7741 loss_N: 0.25872362 loss_f: 5.798947\n",
      "2044 Train Loss 32.83177\n",
      "2045 loss_D: 26.70995 loss_N: 0.2597939 loss_f: 5.831771\n",
      "2045 Train Loss 32.801514\n",
      "2046 loss_D: 26.662506 loss_N: 0.26168382 loss_f: 5.853299\n",
      "2046 Train Loss 32.77749\n",
      "2047 loss_D: 26.585737 loss_N: 0.26280746 loss_f: 5.907249\n",
      "2047 Train Loss 32.755795\n",
      "2048 loss_D: 26.527699 loss_N: 0.2643976 loss_f: 5.9352813\n",
      "2048 Train Loss 32.727375\n",
      "2049 loss_D: 26.522297 loss_N: 0.26288578 loss_f: 5.926328\n",
      "2049 Train Loss 32.71151\n",
      "2050 loss_D: 26.49887 loss_N: 0.26201785 loss_f: 5.9330044\n",
      "2050 Train Loss 32.693893\n",
      "2051 loss_D: 26.449884 loss_N: 0.25997514 loss_f: 5.9675407\n",
      "2051 Train Loss 32.6774\n",
      "2052 loss_D: 26.451967 loss_N: 0.25898328 loss_f: 5.954265\n",
      "2052 Train Loss 32.665215\n",
      "2053 loss_D: 26.412561 loss_N: 0.25788152 loss_f: 5.9842887\n",
      "2053 Train Loss 32.65473\n",
      "2054 loss_D: 26.409002 loss_N: 0.25745955 loss_f: 5.981175\n",
      "2054 Train Loss 32.647636\n",
      "2055 loss_D: 26.405127 loss_N: 0.25743216 loss_f: 5.976162\n",
      "2055 Train Loss 32.63872\n",
      "2056 loss_D: 26.369108 loss_N: 0.25751647 loss_f: 5.995515\n",
      "2056 Train Loss 32.62214\n",
      "2057 loss_D: 26.316525 loss_N: 0.25848743 loss_f: 6.0227914\n",
      "2057 Train Loss 32.597805\n",
      "2058 loss_D: 26.251352 loss_N: 0.2600383 loss_f: 6.0600014\n",
      "2058 Train Loss 32.571392\n",
      "2059 loss_D: 26.19794 loss_N: 0.2613312 loss_f: 6.0804925\n",
      "2059 Train Loss 32.539764\n",
      "2060 loss_D: 26.16358 loss_N: 0.2648387 loss_f: 6.0787797\n",
      "2060 Train Loss 32.5072\n",
      "2061 loss_D: 26.135372 loss_N: 0.26379693 loss_f: 6.093788\n",
      "2061 Train Loss 32.492958\n",
      "2062 loss_D: 26.158031 loss_N: 0.26342443 loss_f: 6.0623536\n",
      "2062 Train Loss 32.48381\n",
      "2063 loss_D: 26.140846 loss_N: 0.26301098 loss_f: 6.072468\n",
      "2063 Train Loss 32.476326\n",
      "2064 loss_D: 26.149647 loss_N: 0.2643025 loss_f: 6.058615\n",
      "2064 Train Loss 32.472565\n",
      "2065 loss_D: 26.138742 loss_N: 0.26319024 loss_f: 6.062037\n",
      "2065 Train Loss 32.46397\n",
      "2066 loss_D: 26.107197 loss_N: 0.26379633 loss_f: 6.0831447\n",
      "2066 Train Loss 32.454136\n",
      "2067 loss_D: 26.096855 loss_N: 0.26387432 loss_f: 6.083943\n",
      "2067 Train Loss 32.44467\n",
      "2068 loss_D: 26.070236 loss_N: 0.2646753 loss_f: 6.099225\n",
      "2068 Train Loss 32.434135\n",
      "2069 loss_D: 26.048328 loss_N: 0.26432016 loss_f: 6.112282\n",
      "2069 Train Loss 32.42493\n",
      "2070 loss_D: 26.011261 loss_N: 0.262487 loss_f: 6.1373777\n",
      "2070 Train Loss 32.411125\n",
      "2071 loss_D: 25.993458 loss_N: 0.26022676 loss_f: 6.145063\n",
      "2071 Train Loss 32.398746\n",
      "2072 loss_D: 25.973412 loss_N: 0.25811782 loss_f: 6.1567826\n",
      "2072 Train Loss 32.388313\n",
      "2073 loss_D: 25.989933 loss_N: 0.25705495 loss_f: 6.1341743\n",
      "2073 Train Loss 32.381165\n",
      "2074 loss_D: 25.99817 loss_N: 0.25669712 loss_f: 6.1194587\n",
      "2074 Train Loss 32.374325\n",
      "2075 loss_D: 26.018574 loss_N: 0.25705102 loss_f: 6.092466\n",
      "2075 Train Loss 32.36809\n",
      "2076 loss_D: 26.031387 loss_N: 0.25759083 loss_f: 6.069235\n",
      "2076 Train Loss 32.358215\n",
      "2077 loss_D: 26.040604 loss_N: 0.25832865 loss_f: 6.057769\n",
      "2077 Train Loss 32.3567\n",
      "2078 loss_D: 26.023458 loss_N: 0.25816593 loss_f: 6.064615\n",
      "2078 Train Loss 32.346237\n",
      "2079 loss_D: 26.011349 loss_N: 0.2578822 loss_f: 6.073087\n",
      "2079 Train Loss 32.34232\n",
      "2080 loss_D: 25.99677 loss_N: 0.25727183 loss_f: 6.0823045\n",
      "2080 Train Loss 32.33635\n",
      "2081 loss_D: 25.983778 loss_N: 0.25720972 loss_f: 6.09073\n",
      "2081 Train Loss 32.33172\n",
      "2082 loss_D: 25.992085 loss_N: 0.2569249 loss_f: 6.0772133\n",
      "2082 Train Loss 32.326225\n",
      "2083 loss_D: 26.000622 loss_N: 0.25723377 loss_f: 6.0604377\n",
      "2083 Train Loss 32.318295\n",
      "2084 loss_D: 25.997595 loss_N: 0.25846976 loss_f: 6.0481787\n",
      "2084 Train Loss 32.304245\n",
      "2085 loss_D: 26.02508 loss_N: 0.25831008 loss_f: 6.002705\n",
      "2085 Train Loss 32.286095\n",
      "2086 loss_D: 26.030365 loss_N: 0.25874898 loss_f: 5.9800444\n",
      "2086 Train Loss 32.269157\n",
      "2087 loss_D: 26.00596 loss_N: 0.25884718 loss_f: 5.986778\n",
      "2087 Train Loss 32.251583\n",
      "2088 loss_D: 25.978094 loss_N: 0.25876185 loss_f: 5.999216\n",
      "2088 Train Loss 32.236073\n",
      "2089 loss_D: 25.932272 loss_N: 0.25812325 loss_f: 6.0329614\n",
      "2089 Train Loss 32.22336\n",
      "2090 loss_D: 25.925283 loss_N: 0.25745198 loss_f: 6.0301914\n",
      "2090 Train Loss 32.21293\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2091 loss_D: 25.915318 loss_N: 0.25661102 loss_f: 6.0328164\n",
      "2091 Train Loss 32.204746\n",
      "2092 loss_D: 25.926912 loss_N: 0.25623047 loss_f: 6.011458\n",
      "2092 Train Loss 32.194603\n",
      "2093 loss_D: 25.978464 loss_N: 0.2545283 loss_f: 5.9611826\n",
      "2093 Train Loss 32.194176\n",
      "2094 loss_D: 25.952118 loss_N: 0.25529778 loss_f: 5.979114\n",
      "2094 Train Loss 32.18653\n",
      "2095 loss_D: 25.93268 loss_N: 0.25565997 loss_f: 5.982463\n",
      "2095 Train Loss 32.170803\n",
      "2096 loss_D: 25.941793 loss_N: 0.25518206 loss_f: 5.945227\n",
      "2096 Train Loss 32.142204\n",
      "2097 loss_D: 25.933126 loss_N: 0.25644428 loss_f: 5.937911\n",
      "2097 Train Loss 32.127483\n",
      "2098 loss_D: 25.917128 loss_N: 0.25583348 loss_f: 5.9381075\n",
      "2098 Train Loss 32.11107\n",
      "2099 loss_D: 25.883156 loss_N: 0.25509775 loss_f: 5.9443545\n",
      "2099 Train Loss 32.082607\n",
      "2100 loss_D: 25.872002 loss_N: 0.25473186 loss_f: 5.935149\n",
      "2100 Train Loss 32.061882\n",
      "2101 loss_D: 25.847548 loss_N: 0.2545119 loss_f: 5.9383883\n",
      "2101 Train Loss 32.040447\n",
      "2102 loss_D: 25.867222 loss_N: 0.25390452 loss_f: 5.9045753\n",
      "2102 Train Loss 32.025703\n",
      "2103 loss_D: 25.848671 loss_N: 0.25361744 loss_f: 5.913795\n",
      "2103 Train Loss 32.016083\n",
      "2104 loss_D: 25.856064 loss_N: 0.25288647 loss_f: 5.900057\n",
      "2104 Train Loss 32.009007\n",
      "2105 loss_D: 25.85776 loss_N: 0.25181213 loss_f: 5.888188\n",
      "2105 Train Loss 31.997759\n",
      "2106 loss_D: 25.807125 loss_N: 0.25079957 loss_f: 6.2449274\n",
      "2106 Train Loss 32.302853\n",
      "2107 loss_D: 25.849363 loss_N: 0.2516101 loss_f: 5.889869\n",
      "2107 Train Loss 31.990843\n",
      "2108 loss_D: 25.829868 loss_N: 0.25160152 loss_f: 5.8978744\n",
      "2108 Train Loss 31.979345\n",
      "2109 loss_D: 25.807915 loss_N: 0.25225288 loss_f: 5.9049935\n",
      "2109 Train Loss 31.96516\n",
      "2110 loss_D: 25.72308 loss_N: 0.25289464 loss_f: 5.998475\n",
      "2110 Train Loss 31.97445\n",
      "2111 loss_D: 25.774157 loss_N: 0.2524415 loss_f: 5.9321423\n",
      "2111 Train Loss 31.95874\n",
      "2112 loss_D: 25.742733 loss_N: 0.25320292 loss_f: 5.949658\n",
      "2112 Train Loss 31.945593\n",
      "2113 loss_D: 25.703278 loss_N: 0.25303072 loss_f: 5.9693165\n",
      "2113 Train Loss 31.925625\n",
      "2114 loss_D: 25.682774 loss_N: 0.2536657 loss_f: 5.974025\n",
      "2114 Train Loss 31.910465\n",
      "2115 loss_D: 25.683023 loss_N: 0.2534933 loss_f: 5.9561887\n",
      "2115 Train Loss 31.892704\n",
      "2116 loss_D: 25.66209 loss_N: 0.2539813 loss_f: 5.957736\n",
      "2116 Train Loss 31.873806\n",
      "2117 loss_D: 25.650408 loss_N: 0.25468087 loss_f: 5.947786\n",
      "2117 Train Loss 31.852875\n",
      "2118 loss_D: 25.622053 loss_N: 0.25480765 loss_f: 5.9523973\n",
      "2118 Train Loss 31.829258\n",
      "2119 loss_D: 25.606457 loss_N: 0.25467828 loss_f: 5.948582\n",
      "2119 Train Loss 31.809717\n",
      "2120 loss_D: 25.59414 loss_N: 0.25359994 loss_f: 5.949788\n",
      "2120 Train Loss 31.797527\n",
      "2121 loss_D: 25.58311 loss_N: 0.25400195 loss_f: 5.946152\n",
      "2121 Train Loss 31.783264\n",
      "2122 loss_D: 25.58461 loss_N: 0.25289902 loss_f: 5.928056\n",
      "2122 Train Loss 31.765564\n",
      "2123 loss_D: 25.59325 loss_N: 0.25271156 loss_f: 5.9011903\n",
      "2123 Train Loss 31.747152\n",
      "2124 loss_D: 25.600197 loss_N: 0.25342673 loss_f: 5.878067\n",
      "2124 Train Loss 31.731691\n",
      "2125 loss_D: 25.607222 loss_N: 0.25392127 loss_f: 5.860169\n",
      "2125 Train Loss 31.721312\n",
      "2126 loss_D: 25.605787 loss_N: 0.25349677 loss_f: 5.847103\n",
      "2126 Train Loss 31.706387\n",
      "2127 loss_D: 25.607561 loss_N: 0.25563812 loss_f: 5.8549757\n",
      "2127 Train Loss 31.718174\n",
      "2128 loss_D: 25.60416 loss_N: 0.25435278 loss_f: 5.8367395\n",
      "2128 Train Loss 31.695251\n",
      "2129 loss_D: 25.61216 loss_N: 0.25332633 loss_f: 5.811956\n",
      "2129 Train Loss 31.677443\n",
      "2130 loss_D: 25.614992 loss_N: 0.25265923 loss_f: 5.7950435\n",
      "2130 Train Loss 31.662695\n",
      "2131 loss_D: 25.620165 loss_N: 0.2518186 loss_f: 5.759073\n",
      "2131 Train Loss 31.631056\n",
      "2132 loss_D: 25.641874 loss_N: 0.25000465 loss_f: 5.73064\n",
      "2132 Train Loss 31.622519\n",
      "2133 loss_D: 25.628517 loss_N: 0.2503051 loss_f: 5.7063713\n",
      "2133 Train Loss 31.585194\n",
      "2134 loss_D: 25.618233 loss_N: 0.25063154 loss_f: 5.6980786\n",
      "2134 Train Loss 31.566942\n",
      "2135 loss_D: 25.596043 loss_N: 0.25055277 loss_f: 5.6988144\n",
      "2135 Train Loss 31.54541\n",
      "2136 loss_D: 25.56963 loss_N: 0.24982846 loss_f: 5.7041216\n",
      "2136 Train Loss 31.523579\n",
      "2137 loss_D: 25.529453 loss_N: 0.24886826 loss_f: 5.7164297\n",
      "2137 Train Loss 31.494751\n",
      "2138 loss_D: 25.500534 loss_N: 0.24880707 loss_f: 5.7117596\n",
      "2138 Train Loss 31.461102\n",
      "2139 loss_D: 25.484303 loss_N: 0.2485752 loss_f: 5.6975455\n",
      "2139 Train Loss 31.430424\n",
      "2140 loss_D: 25.430302 loss_N: 0.24436103 loss_f: 5.7565207\n",
      "2140 Train Loss 31.431185\n",
      "2141 loss_D: 25.45744 loss_N: 0.2464435 loss_f: 5.71172\n",
      "2141 Train Loss 31.415602\n",
      "2142 loss_D: 25.460913 loss_N: 0.24572419 loss_f: 5.6914973\n",
      "2142 Train Loss 31.398134\n",
      "2143 loss_D: 25.465376 loss_N: 0.24653119 loss_f: 5.662647\n",
      "2143 Train Loss 31.374554\n",
      "2144 loss_D: 25.44989 loss_N: 0.24667226 loss_f: 5.6606517\n",
      "2144 Train Loss 31.357214\n",
      "2145 loss_D: 25.439253 loss_N: 0.24624054 loss_f: 5.6580257\n",
      "2145 Train Loss 31.34352\n",
      "2146 loss_D: 25.418547 loss_N: 0.24516416 loss_f: 5.667527\n",
      "2146 Train Loss 31.331238\n",
      "2147 loss_D: 25.39811 loss_N: 0.24432528 loss_f: 5.6784225\n",
      "2147 Train Loss 31.320858\n",
      "2148 loss_D: 25.386662 loss_N: 0.24352509 loss_f: 5.679336\n",
      "2148 Train Loss 31.309523\n",
      "2149 loss_D: 25.37702 loss_N: 0.24247995 loss_f: 5.676446\n",
      "2149 Train Loss 31.295944\n",
      "2150 loss_D: 25.369118 loss_N: 0.24207504 loss_f: 5.67276\n",
      "2150 Train Loss 31.283953\n",
      "2151 loss_D: 25.369139 loss_N: 0.24173388 loss_f: 5.6656833\n",
      "2151 Train Loss 31.276556\n",
      "2152 loss_D: 25.35139 loss_N: 0.2418466 loss_f: 5.67094\n",
      "2152 Train Loss 31.264177\n",
      "2153 loss_D: 25.316093 loss_N: 0.24179326 loss_f: 5.6874137\n",
      "2153 Train Loss 31.2453\n",
      "2154 loss_D: 25.28005 loss_N: 0.24233107 loss_f: 5.7048225\n",
      "2154 Train Loss 31.227203\n",
      "2155 loss_D: 25.198 loss_N: 0.24158247 loss_f: 5.7732973\n",
      "2155 Train Loss 31.21288\n",
      "2156 loss_D: 25.218204 loss_N: 0.24117117 loss_f: 5.7391458\n",
      "2156 Train Loss 31.19852\n",
      "2157 loss_D: 25.21486 loss_N: 0.24109025 loss_f: 5.733664\n",
      "2157 Train Loss 31.189615\n",
      "2158 loss_D: 25.209383 loss_N: 0.24004398 loss_f: 5.7305546\n",
      "2158 Train Loss 31.179981\n",
      "2159 loss_D: 25.183542 loss_N: 0.23971099 loss_f: 5.752614\n",
      "2159 Train Loss 31.175869\n",
      "2160 loss_D: 25.190119 loss_N: 0.23918699 loss_f: 5.7331166\n",
      "2160 Train Loss 31.162422\n",
      "2161 loss_D: 25.191696 loss_N: 0.23920906 loss_f: 5.7230473\n",
      "2161 Train Loss 31.153952\n",
      "2162 loss_D: 25.194756 loss_N: 0.23903315 loss_f: 5.7092867\n",
      "2162 Train Loss 31.143074\n",
      "2163 loss_D: 25.196018 loss_N: 0.23906356 loss_f: 5.6989837\n",
      "2163 Train Loss 31.134066\n",
      "2164 loss_D: 25.19667 loss_N: 0.23692235 loss_f: 5.702349\n",
      "2164 Train Loss 31.135942\n",
      "2165 loss_D: 25.19613 loss_N: 0.23797582 loss_f: 5.6841836\n",
      "2165 Train Loss 31.11829\n",
      "2166 loss_D: 25.18803 loss_N: 0.23747078 loss_f: 5.6757684\n",
      "2166 Train Loss 31.101269\n",
      "2167 loss_D: 25.161882 loss_N: 0.23480737 loss_f: 5.6659875\n",
      "2167 Train Loss 31.062677\n",
      "2168 loss_D: 25.129244 loss_N: 0.23392715 loss_f: 5.6856704\n",
      "2168 Train Loss 31.048841\n",
      "2169 loss_D: 25.125927 loss_N: 0.2336118 loss_f: 5.675024\n",
      "2169 Train Loss 31.034563\n",
      "2170 loss_D: 25.124619 loss_N: 0.2328392 loss_f: 5.6625075\n",
      "2170 Train Loss 31.019966\n",
      "2171 loss_D: 25.09795 loss_N: 0.23144934 loss_f: 5.677929\n",
      "2171 Train Loss 31.007328\n",
      "2172 loss_D: 25.076893 loss_N: 0.22979274 loss_f: 5.6780934\n",
      "2172 Train Loss 30.98478\n",
      "2173 loss_D: 25.076273 loss_N: 0.22501461 loss_f: 5.714138\n",
      "2173 Train Loss 31.015425\n",
      "2174 loss_D: 25.07613 loss_N: 0.2280422 loss_f: 5.6674137\n",
      "2174 Train Loss 30.971586\n",
      "2175 loss_D: 25.022556 loss_N: 0.22431147 loss_f: 5.680924\n",
      "2175 Train Loss 30.927792\n",
      "2176 loss_D: 25.018042 loss_N: 0.22251648 loss_f: 5.658668\n",
      "2176 Train Loss 30.899227\n",
      "2177 loss_D: 24.999647 loss_N: 0.2208321 loss_f: 5.6502285\n",
      "2177 Train Loss 30.870708\n",
      "2178 loss_D: 24.99466 loss_N: 0.22031975 loss_f: 5.643436\n",
      "2178 Train Loss 30.858416\n",
      "2179 loss_D: 24.98674 loss_N: 0.21961977 loss_f: 5.6363015\n",
      "2179 Train Loss 30.84266\n",
      "2180 loss_D: 24.966162 loss_N: 0.2190938 loss_f: 5.644513\n",
      "2180 Train Loss 30.82977\n",
      "2181 loss_D: 24.963825 loss_N: 0.21889482 loss_f: 5.6347265\n",
      "2181 Train Loss 30.817448\n",
      "2182 loss_D: 24.956367 loss_N: 0.21763925 loss_f: 5.637352\n",
      "2182 Train Loss 30.81136\n",
      "2183 loss_D: 24.94548 loss_N: 0.21755922 loss_f: 5.6312456\n",
      "2183 Train Loss 30.794285\n",
      "2184 loss_D: 24.991266 loss_N: 0.21821454 loss_f: 5.5714273\n",
      "2184 Train Loss 30.780907\n",
      "2185 loss_D: 24.99939 loss_N: 0.21870363 loss_f: 5.5459547\n",
      "2185 Train Loss 30.76405\n",
      "2186 loss_D: 24.993141 loss_N: 0.21817446 loss_f: 5.5373254\n",
      "2186 Train Loss 30.74864\n",
      "2187 loss_D: 25.006468 loss_N: 0.21852082 loss_f: 5.5106883\n",
      "2187 Train Loss 30.735678\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2188 loss_D: 25.011087 loss_N: 0.21910898 loss_f: 5.4954133\n",
      "2188 Train Loss 30.725609\n",
      "2189 loss_D: 25.013536 loss_N: 0.21923706 loss_f: 5.4835086\n",
      "2189 Train Loss 30.716282\n",
      "2190 loss_D: 25.000027 loss_N: 0.22001967 loss_f: 5.4886847\n",
      "2190 Train Loss 30.708733\n",
      "2191 loss_D: 24.979263 loss_N: 0.21863168 loss_f: 5.525579\n",
      "2191 Train Loss 30.723475\n",
      "2192 loss_D: 24.991096 loss_N: 0.21936916 loss_f: 5.4840307\n",
      "2192 Train Loss 30.694496\n",
      "2193 loss_D: 24.984413 loss_N: 0.21894711 loss_f: 5.4765162\n",
      "2193 Train Loss 30.679876\n",
      "2194 loss_D: 24.958136 loss_N: 0.2193004 loss_f: 5.484249\n",
      "2194 Train Loss 30.661686\n",
      "2195 loss_D: 24.93793 loss_N: 0.21980141 loss_f: 5.492667\n",
      "2195 Train Loss 30.650398\n",
      "2196 loss_D: 24.889778 loss_N: 0.22132175 loss_f: 5.5171194\n",
      "2196 Train Loss 30.62822\n",
      "2197 loss_D: 24.836946 loss_N: 0.22127019 loss_f: 5.5524364\n",
      "2197 Train Loss 30.610653\n",
      "2198 loss_D: 24.797705 loss_N: 0.2220914 loss_f: 5.5543613\n",
      "2198 Train Loss 30.574158\n",
      "2199 loss_D: 24.753143 loss_N: 0.21979687 loss_f: 5.566031\n",
      "2199 Train Loss 30.53897\n",
      "2200 loss_D: 24.736248 loss_N: 0.2176384 loss_f: 5.562546\n",
      "2200 Train Loss 30.516432\n",
      "2201 loss_D: 24.699503 loss_N: 0.21507217 loss_f: 5.5822787\n",
      "2201 Train Loss 30.496855\n",
      "2202 loss_D: 24.692682 loss_N: 0.21516714 loss_f: 5.5699487\n",
      "2202 Train Loss 30.477798\n",
      "2203 loss_D: 24.67725 loss_N: 0.21586491 loss_f: 5.5603547\n",
      "2203 Train Loss 30.453468\n",
      "2204 loss_D: 24.68774 loss_N: 0.21585605 loss_f: 5.533638\n",
      "2204 Train Loss 30.437235\n",
      "2205 loss_D: 24.671383 loss_N: 0.21492323 loss_f: 5.5328903\n",
      "2205 Train Loss 30.419197\n",
      "2206 loss_D: 24.684692 loss_N: 0.2142367 loss_f: 5.510048\n",
      "2206 Train Loss 30.408978\n",
      "2207 loss_D: 24.683374 loss_N: 0.21316196 loss_f: 5.49814\n",
      "2207 Train Loss 30.394676\n",
      "2208 loss_D: 24.667744 loss_N: 0.21144277 loss_f: 5.4987397\n",
      "2208 Train Loss 30.377926\n",
      "2209 loss_D: 24.654661 loss_N: 0.21143118 loss_f: 5.481115\n",
      "2209 Train Loss 30.347208\n",
      "2210 loss_D: 24.62574 loss_N: 0.21274662 loss_f: 5.912653\n",
      "2210 Train Loss 30.751139\n",
      "2211 loss_D: 24.649345 loss_N: 0.21149294 loss_f: 5.4789953\n",
      "2211 Train Loss 30.339832\n",
      "2212 loss_D: 24.564974 loss_N: 0.20957021 loss_f: 5.5484242\n",
      "2212 Train Loss 30.322968\n",
      "2213 loss_D: 24.58328 loss_N: 0.21139906 loss_f: 5.474358\n",
      "2213 Train Loss 30.269037\n",
      "2214 loss_D: 24.585005 loss_N: 0.2130976 loss_f: 5.444047\n",
      "2214 Train Loss 30.24215\n",
      "2215 loss_D: 24.56136 loss_N: 0.21565332 loss_f: 5.43849\n",
      "2215 Train Loss 30.215502\n",
      "2216 loss_D: 24.53539 loss_N: 0.21688715 loss_f: 5.4419546\n",
      "2216 Train Loss 30.194233\n",
      "2217 loss_D: 24.498919 loss_N: 0.21720374 loss_f: 5.4593577\n",
      "2217 Train Loss 30.17548\n",
      "2218 loss_D: 24.476265 loss_N: 0.21971664 loss_f: 5.4700727\n",
      "2218 Train Loss 30.166054\n",
      "2219 loss_D: 24.460903 loss_N: 0.21857578 loss_f: 5.456281\n",
      "2219 Train Loss 30.135761\n",
      "2220 loss_D: 24.458214 loss_N: 0.21649897 loss_f: 5.4456687\n",
      "2220 Train Loss 30.120382\n",
      "2221 loss_D: 24.439333 loss_N: 0.21405849 loss_f: 5.445709\n",
      "2221 Train Loss 30.0991\n",
      "2222 loss_D: 24.411247 loss_N: 0.21412016 loss_f: 5.4597716\n",
      "2222 Train Loss 30.08514\n",
      "2223 loss_D: 24.380688 loss_N: 0.21444541 loss_f: 5.476707\n",
      "2223 Train Loss 30.07184\n",
      "2224 loss_D: 24.34313 loss_N: 0.21734345 loss_f: 5.504346\n",
      "2224 Train Loss 30.06482\n",
      "2225 loss_D: 24.334387 loss_N: 0.21592426 loss_f: 5.505368\n",
      "2225 Train Loss 30.05568\n",
      "2226 loss_D: 24.334908 loss_N: 0.214956 loss_f: 5.4998703\n",
      "2226 Train Loss 30.049734\n",
      "2227 loss_D: 24.305614 loss_N: 0.21307944 loss_f: 5.523624\n",
      "2227 Train Loss 30.042318\n",
      "2228 loss_D: 24.29628 loss_N: 0.21108742 loss_f: 5.528081\n",
      "2228 Train Loss 30.03545\n",
      "2229 loss_D: 24.261246 loss_N: 0.20902775 loss_f: 5.558465\n",
      "2229 Train Loss 30.02874\n",
      "2230 loss_D: 24.233538 loss_N: 0.20608246 loss_f: 5.579955\n",
      "2230 Train Loss 30.019577\n",
      "2231 loss_D: 24.142174 loss_N: 0.19995154 loss_f: 5.6948915\n",
      "2231 Train Loss 30.037016\n",
      "2232 loss_D: 24.205612 loss_N: 0.204123 loss_f: 5.6058865\n",
      "2232 Train Loss 30.015621\n",
      "2233 loss_D: 24.177774 loss_N: 0.20181426 loss_f: 5.625534\n",
      "2233 Train Loss 30.005123\n",
      "2234 loss_D: 24.164007 loss_N: 0.20046492 loss_f: 5.631423\n",
      "2234 Train Loss 29.995895\n",
      "2235 loss_D: 24.138042 loss_N: 0.19889347 loss_f: 5.6449957\n",
      "2235 Train Loss 29.98193\n",
      "2236 loss_D: 24.13677 loss_N: 0.1982212 loss_f: 5.637677\n",
      "2236 Train Loss 29.972668\n",
      "2237 loss_D: 24.13684 loss_N: 0.1976884 loss_f: 5.6198173\n",
      "2237 Train Loss 29.954348\n",
      "2238 loss_D: 24.143658 loss_N: 0.19811061 loss_f: 5.593757\n",
      "2238 Train Loss 29.935526\n",
      "2239 loss_D: 24.169086 loss_N: 0.19825865 loss_f: 5.5529037\n",
      "2239 Train Loss 29.92025\n",
      "2240 loss_D: 24.189754 loss_N: 0.19801939 loss_f: 5.5172954\n",
      "2240 Train Loss 29.90507\n",
      "2241 loss_D: 24.21099 loss_N: 0.19742537 loss_f: 5.4792767\n",
      "2241 Train Loss 29.887693\n",
      "2242 loss_D: 24.226427 loss_N: 0.19605 loss_f: 5.4469957\n",
      "2242 Train Loss 29.869473\n",
      "2243 loss_D: 24.222027 loss_N: 0.19570942 loss_f: 5.443196\n",
      "2243 Train Loss 29.860931\n",
      "2244 loss_D: 24.2191 loss_N: 0.19535294 loss_f: 5.438692\n",
      "2244 Train Loss 29.853146\n",
      "2245 loss_D: 24.210995 loss_N: 0.19440217 loss_f: 5.432889\n",
      "2245 Train Loss 29.838287\n",
      "2246 loss_D: 24.199509 loss_N: 0.19577095 loss_f: 5.428793\n",
      "2246 Train Loss 29.824072\n",
      "2247 loss_D: 24.19861 loss_N: 0.19586384 loss_f: 5.412823\n",
      "2247 Train Loss 29.807297\n",
      "2248 loss_D: 24.188293 loss_N: 0.19627962 loss_f: 5.405381\n",
      "2248 Train Loss 29.789955\n",
      "2249 loss_D: 24.164392 loss_N: 0.19667791 loss_f: 5.4083138\n",
      "2249 Train Loss 29.769384\n",
      "2250 loss_D: 24.075148 loss_N: 0.19718543 loss_f: 5.5114546\n",
      "2250 Train Loss 29.783787\n",
      "2251 loss_D: 24.126163 loss_N: 0.19683439 loss_f: 5.432769\n",
      "2251 Train Loss 29.755768\n",
      "2252 loss_D: 24.078402 loss_N: 0.19588669 loss_f: 5.4520445\n",
      "2252 Train Loss 29.726334\n",
      "2253 loss_D: 24.035736 loss_N: 0.19457893 loss_f: 5.4758\n",
      "2253 Train Loss 29.706114\n",
      "2254 loss_D: 23.94756 loss_N: 0.19073239 loss_f: 5.570768\n",
      "2254 Train Loss 29.70906\n",
      "2255 loss_D: 23.99327 loss_N: 0.19264731 loss_f: 5.5060787\n",
      "2255 Train Loss 29.691998\n",
      "2256 loss_D: 23.970766 loss_N: 0.19248179 loss_f: 5.5077357\n",
      "2256 Train Loss 29.670984\n",
      "2257 loss_D: 23.909727 loss_N: 0.19069403 loss_f: 5.5520406\n",
      "2257 Train Loss 29.652462\n",
      "2258 loss_D: 23.913574 loss_N: 0.19108853 loss_f: 5.53076\n",
      "2258 Train Loss 29.635422\n",
      "2259 loss_D: 23.864687 loss_N: 0.19044395 loss_f: 5.565984\n",
      "2259 Train Loss 29.621113\n",
      "2260 loss_D: 23.81801 loss_N: 0.19017783 loss_f: 5.5959063\n",
      "2260 Train Loss 29.604095\n",
      "2261 loss_D: 23.777882 loss_N: 0.1905455 loss_f: 5.62228\n",
      "2261 Train Loss 29.590708\n",
      "2262 loss_D: 23.71188 loss_N: 0.19175869 loss_f: 5.675741\n",
      "2262 Train Loss 29.57938\n",
      "2263 loss_D: 23.696444 loss_N: 0.19127607 loss_f: 5.6778226\n",
      "2263 Train Loss 29.565542\n",
      "2264 loss_D: 23.707737 loss_N: 0.19151437 loss_f: 5.656371\n",
      "2264 Train Loss 29.555622\n",
      "2265 loss_D: 23.691927 loss_N: 0.19130446 loss_f: 5.659517\n",
      "2265 Train Loss 29.54275\n",
      "2266 loss_D: 23.674915 loss_N: 0.19053137 loss_f: 5.66175\n",
      "2266 Train Loss 29.527195\n",
      "2267 loss_D: 23.656939 loss_N: 0.18992129 loss_f: 5.6657186\n",
      "2267 Train Loss 29.512577\n",
      "2268 loss_D: 23.632126 loss_N: 0.18885373 loss_f: 5.673846\n",
      "2268 Train Loss 29.494825\n",
      "2269 loss_D: 23.626263 loss_N: 0.18791585 loss_f: 5.666076\n",
      "2269 Train Loss 29.480255\n",
      "2270 loss_D: 23.587217 loss_N: 0.18683378 loss_f: 5.688407\n",
      "2270 Train Loss 29.46246\n",
      "2271 loss_D: 23.58109 loss_N: 0.18454316 loss_f: 5.6759844\n",
      "2271 Train Loss 29.44162\n",
      "2272 loss_D: 23.565763 loss_N: 0.18299292 loss_f: 5.6756864\n",
      "2272 Train Loss 29.424442\n",
      "2273 loss_D: 23.562147 loss_N: 0.18208005 loss_f: 5.663623\n",
      "2273 Train Loss 29.407848\n",
      "2274 loss_D: 23.534441 loss_N: 0.18158472 loss_f: 5.67956\n",
      "2274 Train Loss 29.395586\n",
      "2275 loss_D: 23.534182 loss_N: 0.18140736 loss_f: 5.6675367\n",
      "2275 Train Loss 29.383125\n",
      "2276 loss_D: 23.50518 loss_N: 0.18093501 loss_f: 5.676244\n",
      "2276 Train Loss 29.362358\n",
      "2277 loss_D: 23.494158 loss_N: 0.18045539 loss_f: 5.664903\n",
      "2277 Train Loss 29.339518\n",
      "2278 loss_D: 23.497559 loss_N: 0.1777809 loss_f: 5.646425\n",
      "2278 Train Loss 29.321764\n",
      "2279 loss_D: 23.481375 loss_N: 0.176798 loss_f: 5.64551\n",
      "2279 Train Loss 29.303682\n",
      "2280 loss_D: 23.477774 loss_N: 0.17695938 loss_f: 5.6309695\n",
      "2280 Train Loss 29.285704\n",
      "2281 loss_D: 23.45495 loss_N: 0.17604436 loss_f: 5.631599\n",
      "2281 Train Loss 29.262594\n",
      "2282 loss_D: 23.416168 loss_N: 0.1741833 loss_f: 5.64966\n",
      "2282 Train Loss 29.240011\n",
      "2283 loss_D: 23.408838 loss_N: 0.17366634 loss_f: 5.642683\n",
      "2283 Train Loss 29.225187\n",
      "2284 loss_D: 23.368418 loss_N: 0.17298459 loss_f: 5.675905\n",
      "2284 Train Loss 29.217308\n",
      "2285 loss_D: 23.373274 loss_N: 0.17363477 loss_f: 5.66157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2285 Train Loss 29.208479\n",
      "2286 loss_D: 23.580734 loss_N: 0.17197236 loss_f: 5.973095\n",
      "2286 Train Loss 29.725801\n",
      "2287 loss_D: 23.387787 loss_N: 0.17327839 loss_f: 5.6395817\n",
      "2287 Train Loss 29.200647\n",
      "2288 loss_D: 23.392965 loss_N: 0.17366652 loss_f: 5.614026\n",
      "2288 Train Loss 29.180656\n",
      "2289 loss_D: 23.398125 loss_N: 0.17358105 loss_f: 5.595521\n",
      "2289 Train Loss 29.167225\n",
      "2290 loss_D: 23.393284 loss_N: 0.17321528 loss_f: 5.5879083\n",
      "2290 Train Loss 29.154408\n",
      "2291 loss_D: 23.37893 loss_N: 0.17260504 loss_f: 5.584683\n",
      "2291 Train Loss 29.136217\n",
      "2292 loss_D: 23.355196 loss_N: 0.172159 loss_f: 5.5869985\n",
      "2292 Train Loss 29.114353\n",
      "2293 loss_D: 23.334095 loss_N: 0.17166409 loss_f: 5.5859756\n",
      "2293 Train Loss 29.091734\n",
      "2294 loss_D: 23.29645 loss_N: 0.17245579 loss_f: 5.5963445\n",
      "2294 Train Loss 29.06525\n",
      "2295 loss_D: 23.298405 loss_N: 0.17278476 loss_f: 5.5688696\n",
      "2295 Train Loss 29.040058\n",
      "2296 loss_D: 23.292946 loss_N: 0.17326972 loss_f: 5.552986\n",
      "2296 Train Loss 29.019201\n",
      "2297 loss_D: 23.304514 loss_N: 0.17368001 loss_f: 5.528344\n",
      "2297 Train Loss 29.006538\n",
      "2298 loss_D: 23.294567 loss_N: 0.17378412 loss_f: 5.5276723\n",
      "2298 Train Loss 28.996023\n",
      "2299 loss_D: 23.300705 loss_N: 0.1738539 loss_f: 5.5155077\n",
      "2299 Train Loss 28.990067\n",
      "2300 loss_D: 23.29444 loss_N: 0.17358571 loss_f: 5.514811\n",
      "2300 Train Loss 28.982836\n",
      "2301 loss_D: 23.288202 loss_N: 0.17335176 loss_f: 5.5140357\n",
      "2301 Train Loss 28.97559\n",
      "2302 loss_D: 23.281328 loss_N: 0.17323805 loss_f: 5.5105867\n",
      "2302 Train Loss 28.965153\n",
      "2303 loss_D: 23.265532 loss_N: 0.17398748 loss_f: 5.5123014\n",
      "2303 Train Loss 28.95182\n",
      "2304 loss_D: 23.249203 loss_N: 0.17411481 loss_f: 5.5134387\n",
      "2304 Train Loss 28.936756\n",
      "2305 loss_D: 23.249014 loss_N: 0.17454234 loss_f: 5.497993\n",
      "2305 Train Loss 28.921549\n",
      "2306 loss_D: 23.220549 loss_N: 0.17702518 loss_f: 5.489645\n",
      "2306 Train Loss 28.887218\n",
      "2307 loss_D: 23.20185 loss_N: 0.17826846 loss_f: 5.486186\n",
      "2307 Train Loss 28.866306\n",
      "2308 loss_D: 23.127033 loss_N: 0.18236215 loss_f: 5.5229287\n",
      "2308 Train Loss 28.832323\n",
      "2309 loss_D: 23.09051 loss_N: 0.18093647 loss_f: 5.528716\n",
      "2309 Train Loss 28.800163\n",
      "2310 loss_D: 23.070044 loss_N: 0.1787839 loss_f: 5.519815\n",
      "2310 Train Loss 28.768642\n",
      "2311 loss_D: 22.955719 loss_N: 0.17528054 loss_f: 5.607561\n",
      "2311 Train Loss 28.73856\n",
      "2312 loss_D: 22.981707 loss_N: 0.17534472 loss_f: 5.5625515\n",
      "2312 Train Loss 28.719603\n",
      "2313 loss_D: 22.977211 loss_N: 0.17644782 loss_f: 5.5533266\n",
      "2313 Train Loss 28.706985\n",
      "2314 loss_D: 22.953005 loss_N: 0.178444 loss_f: 5.5583954\n",
      "2314 Train Loss 28.689844\n",
      "2315 loss_D: 22.931606 loss_N: 0.18042485 loss_f: 5.5603743\n",
      "2315 Train Loss 28.672405\n",
      "2316 loss_D: 22.91102 loss_N: 0.18411693 loss_f: 5.554613\n",
      "2316 Train Loss 28.64975\n",
      "2317 loss_D: 22.892727 loss_N: 0.18439217 loss_f: 5.5572925\n",
      "2317 Train Loss 28.634413\n",
      "2318 loss_D: 22.874922 loss_N: 0.18418795 loss_f: 5.5659204\n",
      "2318 Train Loss 28.62503\n",
      "2319 loss_D: 22.86151 loss_N: 0.1821595 loss_f: 5.572569\n",
      "2319 Train Loss 28.616238\n",
      "2320 loss_D: 22.8535 loss_N: 0.18027388 loss_f: 5.5729613\n",
      "2320 Train Loss 28.606735\n",
      "2321 loss_D: 22.83632 loss_N: 0.17881986 loss_f: 5.5807266\n",
      "2321 Train Loss 28.595867\n",
      "2322 loss_D: 22.845549 loss_N: 0.17715707 loss_f: 5.55964\n",
      "2322 Train Loss 28.582344\n",
      "2323 loss_D: 22.821459 loss_N: 0.17659824 loss_f: 5.5661798\n",
      "2323 Train Loss 28.564236\n",
      "2324 loss_D: 22.823149 loss_N: 0.17677148 loss_f: 5.547016\n",
      "2324 Train Loss 28.546936\n",
      "2325 loss_D: 22.81637 loss_N: 0.17714138 loss_f: 5.5508494\n",
      "2325 Train Loss 28.544361\n",
      "2326 loss_D: 22.79527 loss_N: 0.17894328 loss_f: 5.550501\n",
      "2326 Train Loss 28.524715\n",
      "2327 loss_D: 22.808483 loss_N: 0.179444 loss_f: 5.5231333\n",
      "2327 Train Loss 28.511059\n",
      "2328 loss_D: 22.804667 loss_N: 0.1800344 loss_f: 5.5087786\n",
      "2328 Train Loss 28.49348\n",
      "2329 loss_D: 22.787596 loss_N: 0.18085268 loss_f: 5.5101366\n",
      "2329 Train Loss 28.478584\n",
      "2330 loss_D: 22.774149 loss_N: 0.18104567 loss_f: 5.5128126\n",
      "2330 Train Loss 28.468006\n",
      "2331 loss_D: 22.753359 loss_N: 0.18124013 loss_f: 5.5222063\n",
      "2331 Train Loss 28.456806\n",
      "2332 loss_D: 22.739496 loss_N: 0.18249053 loss_f: 5.543226\n",
      "2332 Train Loss 28.465214\n",
      "2333 loss_D: 22.747236 loss_N: 0.18175429 loss_f: 5.5190406\n",
      "2333 Train Loss 28.448032\n",
      "2334 loss_D: 22.720135 loss_N: 0.18214116 loss_f: 5.5209045\n",
      "2334 Train Loss 28.42318\n",
      "2335 loss_D: 22.706224 loss_N: 0.18266504 loss_f: 5.514074\n",
      "2335 Train Loss 28.402964\n",
      "2336 loss_D: 22.6917 loss_N: 0.18518843 loss_f: 5.5066915\n",
      "2336 Train Loss 28.38358\n",
      "2337 loss_D: 22.7184 loss_N: 0.18401772 loss_f: 5.489628\n",
      "2337 Train Loss 28.392044\n",
      "2338 loss_D: 22.702818 loss_N: 0.18458417 loss_f: 5.4830174\n",
      "2338 Train Loss 28.370419\n",
      "2339 loss_D: 22.700836 loss_N: 0.18519409 loss_f: 5.4683757\n",
      "2339 Train Loss 28.354406\n",
      "2340 loss_D: 22.723875 loss_N: 0.18591364 loss_f: 5.4178324\n",
      "2340 Train Loss 28.327621\n",
      "2341 loss_D: 22.728983 loss_N: 0.18612799 loss_f: 5.394989\n",
      "2341 Train Loss 28.3101\n",
      "2342 loss_D: 22.733953 loss_N: 0.18609753 loss_f: 5.3753157\n",
      "2342 Train Loss 28.295368\n",
      "2343 loss_D: 22.733818 loss_N: 0.18685505 loss_f: 5.3588405\n",
      "2343 Train Loss 28.279514\n",
      "2344 loss_D: 22.733572 loss_N: 0.18691538 loss_f: 5.350448\n",
      "2344 Train Loss 28.270935\n",
      "2345 loss_D: 22.725742 loss_N: 0.18739644 loss_f: 5.3432713\n",
      "2345 Train Loss 28.25641\n",
      "2346 loss_D: 22.699127 loss_N: 0.18918703 loss_f: 5.3652296\n",
      "2346 Train Loss 28.253544\n",
      "2347 loss_D: 22.709196 loss_N: 0.1881525 loss_f: 5.342663\n",
      "2347 Train Loss 28.240011\n",
      "2348 loss_D: 22.705519 loss_N: 0.1886468 loss_f: 5.3367543\n",
      "2348 Train Loss 28.230919\n",
      "2349 loss_D: 22.705107 loss_N: 0.18884607 loss_f: 5.331405\n",
      "2349 Train Loss 28.225359\n",
      "2350 loss_D: 22.697565 loss_N: 0.1896748 loss_f: 5.330556\n",
      "2350 Train Loss 28.217796\n",
      "2351 loss_D: 22.697075 loss_N: 0.18934283 loss_f: 5.323819\n",
      "2351 Train Loss 28.210236\n",
      "2352 loss_D: 22.687824 loss_N: 0.19029449 loss_f: 5.3234053\n",
      "2352 Train Loss 28.201523\n",
      "2353 loss_D: 22.673409 loss_N: 0.18909435 loss_f: 5.329939\n",
      "2353 Train Loss 28.192442\n",
      "2354 loss_D: 22.670893 loss_N: 0.19019015 loss_f: 5.318181\n",
      "2354 Train Loss 28.179264\n",
      "2355 loss_D: 22.656208 loss_N: 0.1900547 loss_f: 5.321171\n",
      "2355 Train Loss 28.167433\n",
      "2356 loss_D: 22.634007 loss_N: 0.18926425 loss_f: 5.3299155\n",
      "2356 Train Loss 28.153187\n",
      "2357 loss_D: 22.617725 loss_N: 0.1881379 loss_f: 5.3369446\n",
      "2357 Train Loss 28.142807\n",
      "2358 loss_D: 22.601961 loss_N: 0.18596083 loss_f: 5.344662\n",
      "2358 Train Loss 28.132584\n",
      "2359 loss_D: 22.590176 loss_N: 0.18568273 loss_f: 5.3496695\n",
      "2359 Train Loss 28.125526\n",
      "2360 loss_D: 22.592712 loss_N: 0.18482557 loss_f: 5.3411756\n",
      "2360 Train Loss 28.118713\n",
      "2361 loss_D: 22.600542 loss_N: 0.18449743 loss_f: 5.3253803\n",
      "2361 Train Loss 28.11042\n",
      "2362 loss_D: 22.609442 loss_N: 0.18457028 loss_f: 5.3093286\n",
      "2362 Train Loss 28.10334\n",
      "2363 loss_D: 22.612936 loss_N: 0.18540947 loss_f: 5.2942734\n",
      "2363 Train Loss 28.092619\n",
      "2364 loss_D: 22.633434 loss_N: 0.18410338 loss_f: 5.4273615\n",
      "2364 Train Loss 28.2449\n",
      "2365 loss_D: 22.615173 loss_N: 0.18505524 loss_f: 5.2882605\n",
      "2365 Train Loss 28.08849\n",
      "2366 loss_D: 22.612112 loss_N: 0.18574674 loss_f: 5.276825\n",
      "2366 Train Loss 28.074684\n",
      "2367 loss_D: 22.60038 loss_N: 0.18529676 loss_f: 5.2738667\n",
      "2367 Train Loss 28.059544\n",
      "2368 loss_D: 22.595354 loss_N: 0.1855327 loss_f: 5.267299\n",
      "2368 Train Loss 28.048187\n",
      "2369 loss_D: 22.5824 loss_N: 0.18522438 loss_f: 5.271969\n",
      "2369 Train Loss 28.039593\n",
      "2370 loss_D: 22.559353 loss_N: 0.18671986 loss_f: 5.2968755\n",
      "2370 Train Loss 28.042948\n",
      "2371 loss_D: 22.571835 loss_N: 0.1858527 loss_f: 5.274631\n",
      "2371 Train Loss 28.032318\n",
      "2372 loss_D: 22.549988 loss_N: 0.18510695 loss_f: 5.283384\n",
      "2372 Train Loss 28.018478\n",
      "2373 loss_D: 22.535955 loss_N: 0.18563224 loss_f: 5.2807393\n",
      "2373 Train Loss 28.002327\n",
      "2374 loss_D: 22.506783 loss_N: 0.18708166 loss_f: 5.2875304\n",
      "2374 Train Loss 27.981396\n",
      "2375 loss_D: 22.559017 loss_N: 0.1949927 loss_f: 5.2968016\n",
      "2375 Train Loss 28.050812\n",
      "2376 loss_D: 22.517975 loss_N: 0.18870454 loss_f: 5.268018\n",
      "2376 Train Loss 27.974697\n",
      "2377 loss_D: 22.496433 loss_N: 0.18967952 loss_f: 5.2613044\n",
      "2377 Train Loss 27.947418\n",
      "2378 loss_D: 22.487764 loss_N: 0.19614482 loss_f: 5.24535\n",
      "2378 Train Loss 27.929258\n",
      "2379 loss_D: 22.508732 loss_N: 0.19339055 loss_f: 5.208529\n",
      "2379 Train Loss 27.91065\n",
      "2380 loss_D: 22.525797 loss_N: 0.19150639 loss_f: 5.181911\n",
      "2380 Train Loss 27.899216\n",
      "2381 loss_D: 22.556166 loss_N: 0.190005 loss_f: 5.141552\n",
      "2381 Train Loss 27.887722\n",
      "2382 loss_D: 22.561855 loss_N: 0.18931925 loss_f: 5.1235876\n",
      "2382 Train Loss 27.874763\n",
      "2383 loss_D: 22.568928 loss_N: 0.18894096 loss_f: 5.103375\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2383 Train Loss 27.861242\n",
      "2384 loss_D: 22.550854 loss_N: 0.18707924 loss_f: 5.103637\n",
      "2384 Train Loss 27.84157\n",
      "2385 loss_D: 22.539877 loss_N: 0.18553473 loss_f: 5.096792\n",
      "2385 Train Loss 27.822205\n",
      "2386 loss_D: 22.51996 loss_N: 0.18482858 loss_f: 5.09859\n",
      "2386 Train Loss 27.80338\n",
      "2387 loss_D: 22.501862 loss_N: 0.18402283 loss_f: 5.0979915\n",
      "2387 Train Loss 27.783876\n",
      "2388 loss_D: 22.472658 loss_N: 0.1840961 loss_f: 5.1224275\n",
      "2388 Train Loss 27.77918\n",
      "2389 loss_D: 22.487965 loss_N: 0.18358676 loss_f: 5.0949907\n",
      "2389 Train Loss 27.76654\n",
      "2390 loss_D: 22.480951 loss_N: 0.18321432 loss_f: 5.0959897\n",
      "2390 Train Loss 27.760155\n",
      "2391 loss_D: 22.478989 loss_N: 0.18274947 loss_f: 5.08921\n",
      "2391 Train Loss 27.750948\n",
      "2392 loss_D: 22.458372 loss_N: 0.18229035 loss_f: 5.097317\n",
      "2392 Train Loss 27.73798\n",
      "2393 loss_D: 22.438461 loss_N: 0.18154958 loss_f: 5.1000395\n",
      "2393 Train Loss 27.72005\n",
      "2394 loss_D: 22.419287 loss_N: 0.18124114 loss_f: 5.109227\n",
      "2394 Train Loss 27.709755\n",
      "2395 loss_D: 22.399584 loss_N: 0.18045451 loss_f: 5.116948\n",
      "2395 Train Loss 27.696987\n",
      "2396 loss_D: 22.417244 loss_N: 0.1804515 loss_f: 5.0890226\n",
      "2396 Train Loss 27.686718\n",
      "2397 loss_D: 22.418228 loss_N: 0.18095605 loss_f: 5.078121\n",
      "2397 Train Loss 27.677305\n",
      "2398 loss_D: 22.413353 loss_N: 0.18118654 loss_f: 5.0729346\n",
      "2398 Train Loss 27.667475\n",
      "2399 loss_D: 22.38503 loss_N: 0.18189697 loss_f: 5.0841584\n",
      "2399 Train Loss 27.651085\n",
      "2400 loss_D: 22.349894 loss_N: 0.18315935 loss_f: 5.098031\n",
      "2400 Train Loss 27.631084\n",
      "2401 loss_D: 22.306543 loss_N: 0.18359575 loss_f: 5.1203194\n",
      "2401 Train Loss 27.610458\n",
      "2402 loss_D: 22.242186 loss_N: 0.18383946 loss_f: 5.1704407\n",
      "2402 Train Loss 27.596466\n",
      "2403 loss_D: 22.264889 loss_N: 0.18378806 loss_f: 5.13246\n",
      "2403 Train Loss 27.581137\n",
      "2404 loss_D: 22.260937 loss_N: 0.18330693 loss_f: 5.125615\n",
      "2404 Train Loss 27.569859\n",
      "2405 loss_D: 22.268858 loss_N: 0.18240282 loss_f: 5.1076865\n",
      "2405 Train Loss 27.558949\n",
      "2406 loss_D: 22.257387 loss_N: 0.18198416 loss_f: 5.102883\n",
      "2406 Train Loss 27.542253\n",
      "2407 loss_D: 22.235733 loss_N: 0.1816181 loss_f: 5.10161\n",
      "2407 Train Loss 27.51896\n",
      "2408 loss_D: 22.205118 loss_N: 0.18157879 loss_f: 5.114045\n",
      "2408 Train Loss 27.500744\n",
      "2409 loss_D: 22.171547 loss_N: 0.18214157 loss_f: 5.1318297\n",
      "2409 Train Loss 27.48552\n",
      "2410 loss_D: 22.150126 loss_N: 0.18210924 loss_f: 5.139365\n",
      "2410 Train Loss 27.4716\n",
      "2411 loss_D: 22.13722 loss_N: 0.18240175 loss_f: 5.1405888\n",
      "2411 Train Loss 27.46021\n",
      "2412 loss_D: 22.132275 loss_N: 0.18182424 loss_f: 5.135846\n",
      "2412 Train Loss 27.449944\n",
      "2413 loss_D: 22.12914 loss_N: 0.18149568 loss_f: 5.127053\n",
      "2413 Train Loss 27.437689\n",
      "2414 loss_D: 22.121496 loss_N: 0.18113962 loss_f: 5.122834\n",
      "2414 Train Loss 27.425468\n",
      "2415 loss_D: 22.119602 loss_N: 0.1812549 loss_f: 5.1112876\n",
      "2415 Train Loss 27.412146\n",
      "2416 loss_D: 22.096703 loss_N: 0.18152902 loss_f: 5.123665\n",
      "2416 Train Loss 27.401896\n",
      "2417 loss_D: 22.094063 loss_N: 0.181439 loss_f: 5.1126857\n",
      "2417 Train Loss 27.388187\n",
      "2418 loss_D: 22.059149 loss_N: 0.18135506 loss_f: 5.1282744\n",
      "2418 Train Loss 27.368778\n",
      "2419 loss_D: 22.035254 loss_N: 0.18109961 loss_f: 5.132708\n",
      "2419 Train Loss 27.34906\n",
      "2420 loss_D: 22.00752 loss_N: 0.18075727 loss_f: 5.136968\n",
      "2420 Train Loss 27.325247\n",
      "2421 loss_D: 21.974459 loss_N: 0.1830628 loss_f: 5.1687627\n",
      "2421 Train Loss 27.326284\n",
      "2422 loss_D: 21.990976 loss_N: 0.18185486 loss_f: 5.1434574\n",
      "2422 Train Loss 27.316288\n",
      "2423 loss_D: 21.94288 loss_N: 0.1811237 loss_f: 5.182674\n",
      "2423 Train Loss 27.306679\n",
      "2424 loss_D: 21.980133 loss_N: 0.18085338 loss_f: 5.133887\n",
      "2424 Train Loss 27.294872\n",
      "2425 loss_D: 21.9785 loss_N: 0.18148844 loss_f: 5.1260843\n",
      "2425 Train Loss 27.286072\n",
      "2426 loss_D: 21.962269 loss_N: 0.18247609 loss_f: 5.1292243\n",
      "2426 Train Loss 27.27397\n",
      "2427 loss_D: 21.940569 loss_N: 0.18370186 loss_f: 5.129965\n",
      "2427 Train Loss 27.254236\n",
      "2428 loss_D: 21.919182 loss_N: 0.18459283 loss_f: 5.1293554\n",
      "2428 Train Loss 27.233131\n",
      "2429 loss_D: 21.907581 loss_N: 0.18415637 loss_f: 5.12018\n",
      "2429 Train Loss 27.211918\n",
      "2430 loss_D: 21.908178 loss_N: 0.18355417 loss_f: 5.098046\n",
      "2430 Train Loss 27.189777\n",
      "2431 loss_D: 21.895346 loss_N: 0.1845442 loss_f: 5.0988426\n",
      "2431 Train Loss 27.178732\n",
      "2432 loss_D: 21.901144 loss_N: 0.18425769 loss_f: 5.0748415\n",
      "2432 Train Loss 27.160244\n",
      "2433 loss_D: 21.906902 loss_N: 0.18493319 loss_f: 5.056986\n",
      "2433 Train Loss 27.14882\n",
      "2434 loss_D: 21.912971 loss_N: 0.18623951 loss_f: 5.0336485\n",
      "2434 Train Loss 27.132858\n",
      "2435 loss_D: 21.901371 loss_N: 0.18722048 loss_f: 5.03395\n",
      "2435 Train Loss 27.12254\n",
      "2436 loss_D: 21.904245 loss_N: 0.18906601 loss_f: 5.0131335\n",
      "2436 Train Loss 27.106445\n",
      "2437 loss_D: 21.895884 loss_N: 0.18886101 loss_f: 5.0066185\n",
      "2437 Train Loss 27.091364\n",
      "2438 loss_D: 21.885874 loss_N: 0.18921252 loss_f: 5.002887\n",
      "2438 Train Loss 27.077972\n",
      "2439 loss_D: 21.86888 loss_N: 0.18950379 loss_f: 5.0047617\n",
      "2439 Train Loss 27.063145\n",
      "2440 loss_D: 21.85848 loss_N: 0.19079806 loss_f: 5.001064\n",
      "2440 Train Loss 27.050343\n",
      "2441 loss_D: 21.84169 loss_N: 0.1933734 loss_f: 4.9926586\n",
      "2441 Train Loss 27.027721\n",
      "2442 loss_D: 21.834955 loss_N: 0.19903748 loss_f: 4.973676\n",
      "2442 Train Loss 27.00767\n",
      "2443 loss_D: 21.841944 loss_N: 0.19961675 loss_f: 4.946283\n",
      "2443 Train Loss 26.987844\n",
      "2444 loss_D: 21.860416 loss_N: 0.20116308 loss_f: 4.909229\n",
      "2444 Train Loss 26.970808\n",
      "2445 loss_D: 21.87761 loss_N: 0.2001805 loss_f: 4.926566\n",
      "2445 Train Loss 27.004356\n",
      "2446 loss_D: 21.865107 loss_N: 0.2008081 loss_f: 4.8938127\n",
      "2446 Train Loss 26.959726\n",
      "2447 loss_D: 21.864069 loss_N: 0.20059852 loss_f: 4.885345\n",
      "2447 Train Loss 26.950012\n",
      "2448 loss_D: 21.863293 loss_N: 0.19933495 loss_f: 4.86544\n",
      "2448 Train Loss 26.928068\n",
      "2449 loss_D: 21.850231 loss_N: 0.19873187 loss_f: 4.861636\n",
      "2449 Train Loss 26.910599\n",
      "2450 loss_D: 21.847475 loss_N: 0.196675 loss_f: 4.846307\n",
      "2450 Train Loss 26.890457\n",
      "2451 loss_D: 21.854183 loss_N: 0.19592626 loss_f: 4.8191137\n",
      "2451 Train Loss 26.869225\n",
      "2452 loss_D: 21.851082 loss_N: 0.19618072 loss_f: 4.808234\n",
      "2452 Train Loss 26.855495\n",
      "2453 loss_D: 21.873661 loss_N: 0.1959634 loss_f: 4.77966\n",
      "2453 Train Loss 26.849285\n",
      "2454 loss_D: 21.876146 loss_N: 0.1963747 loss_f: 4.770934\n",
      "2454 Train Loss 26.843456\n",
      "2455 loss_D: 21.888182 loss_N: 0.19759424 loss_f: 4.7498326\n",
      "2455 Train Loss 26.835608\n",
      "2456 loss_D: 21.90745 loss_N: 0.19826557 loss_f: 4.7142453\n",
      "2456 Train Loss 26.81996\n",
      "2457 loss_D: 21.956373 loss_N: 0.20123278 loss_f: 4.666724\n",
      "2457 Train Loss 26.82433\n",
      "2458 loss_D: 21.929308 loss_N: 0.199515 loss_f: 4.681313\n",
      "2458 Train Loss 26.810135\n",
      "2459 loss_D: 21.924482 loss_N: 0.1984035 loss_f: 4.679697\n",
      "2459 Train Loss 26.802584\n",
      "2460 loss_D: 21.914062 loss_N: 0.19662467 loss_f: 4.6855087\n",
      "2460 Train Loss 26.796196\n",
      "2461 loss_D: 21.91193 loss_N: 0.19565938 loss_f: 4.68331\n",
      "2461 Train Loss 26.7909\n",
      "2462 loss_D: 21.909275 loss_N: 0.19452581 loss_f: 4.678775\n",
      "2462 Train Loss 26.782578\n",
      "2463 loss_D: 21.904617 loss_N: 0.1934634 loss_f: 4.67526\n",
      "2463 Train Loss 26.773342\n",
      "2464 loss_D: 21.90555 loss_N: 0.19247112 loss_f: 4.660249\n",
      "2464 Train Loss 26.75827\n",
      "2465 loss_D: 21.89637 loss_N: 0.18949965 loss_f: 4.6611505\n",
      "2465 Train Loss 26.747019\n",
      "2466 loss_D: 21.900074 loss_N: 0.19060503 loss_f: 4.643597\n",
      "2466 Train Loss 26.734276\n",
      "2467 loss_D: 21.899498 loss_N: 0.19087966 loss_f: 4.6307077\n",
      "2467 Train Loss 26.721085\n",
      "2468 loss_D: 21.905447 loss_N: 0.19086112 loss_f: 4.6170225\n",
      "2468 Train Loss 26.71333\n",
      "2469 loss_D: 21.902554 loss_N: 0.18984967 loss_f: 4.609983\n",
      "2469 Train Loss 26.702387\n",
      "2470 loss_D: 21.900352 loss_N: 0.18904145 loss_f: 4.6021075\n",
      "2470 Train Loss 26.691502\n",
      "2471 loss_D: 21.902899 loss_N: 0.18811028 loss_f: 4.5921383\n",
      "2471 Train Loss 26.683147\n",
      "2472 loss_D: 21.90059 loss_N: 0.18726335 loss_f: 4.583814\n",
      "2472 Train Loss 26.671669\n",
      "2473 loss_D: 21.899685 loss_N: 0.18614966 loss_f: 4.571621\n",
      "2473 Train Loss 26.657455\n",
      "2474 loss_D: 21.88534 loss_N: 0.18551388 loss_f: 4.5704813\n",
      "2474 Train Loss 26.641335\n",
      "2475 loss_D: 21.879225 loss_N: 0.18554123 loss_f: 4.5604405\n",
      "2475 Train Loss 26.625206\n",
      "2476 loss_D: 21.865904 loss_N: 0.1859873 loss_f: 4.5579014\n",
      "2476 Train Loss 26.609793\n",
      "2477 loss_D: 21.864462 loss_N: 0.18669382 loss_f: 4.5479255\n",
      "2477 Train Loss 26.599081\n",
      "2478 loss_D: 21.880678 loss_N: 0.18689421 loss_f: 4.523408\n",
      "2478 Train Loss 26.59098\n",
      "2479 loss_D: 21.86629 loss_N: 0.18735611 loss_f: 4.5247564\n",
      "2479 Train Loss 26.578403\n",
      "2480 loss_D: 21.871576 loss_N: 0.18698767 loss_f: 4.5104804\n",
      "2480 Train Loss 26.569044\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2481 loss_D: 21.865854 loss_N: 0.18683688 loss_f: 4.5056186\n",
      "2481 Train Loss 26.55831\n",
      "2482 loss_D: 21.85547 loss_N: 0.1840164 loss_f: 4.504073\n",
      "2482 Train Loss 26.54356\n",
      "2483 loss_D: 21.835945 loss_N: 0.18539655 loss_f: 4.507074\n",
      "2483 Train Loss 26.528416\n",
      "2484 loss_D: 21.805223 loss_N: 0.18610567 loss_f: 4.516922\n",
      "2484 Train Loss 26.508251\n",
      "2485 loss_D: 21.764477 loss_N: 0.18788785 loss_f: 4.530937\n",
      "2485 Train Loss 26.483301\n",
      "2486 loss_D: 21.724892 loss_N: 0.18880902 loss_f: 4.554746\n",
      "2486 Train Loss 26.468447\n",
      "2487 loss_D: 21.716759 loss_N: 0.18839243 loss_f: 4.5437193\n",
      "2487 Train Loss 26.448872\n",
      "2488 loss_D: 21.665058 loss_N: 0.19184674 loss_f: 4.591852\n",
      "2488 Train Loss 26.448757\n",
      "2489 loss_D: 21.690336 loss_N: 0.19008802 loss_f: 4.55349\n",
      "2489 Train Loss 26.433914\n",
      "2490 loss_D: 21.685833 loss_N: 0.18851157 loss_f: 4.539435\n",
      "2490 Train Loss 26.413778\n",
      "2491 loss_D: 21.66844 loss_N: 0.18703242 loss_f: 4.541963\n",
      "2491 Train Loss 26.397436\n",
      "2492 loss_D: 21.642307 loss_N: 0.1862268 loss_f: 4.5387797\n",
      "2492 Train Loss 26.367313\n",
      "2493 loss_D: 21.615166 loss_N: 0.1850337 loss_f: 4.5416803\n",
      "2493 Train Loss 26.34188\n",
      "2494 loss_D: 21.597584 loss_N: 0.18246645 loss_f: 4.544192\n",
      "2494 Train Loss 26.324242\n",
      "2495 loss_D: 21.566675 loss_N: 0.18196765 loss_f: 4.5704885\n",
      "2495 Train Loss 26.31913\n",
      "2496 loss_D: 21.579826 loss_N: 0.1820357 loss_f: 4.5503125\n",
      "2496 Train Loss 26.312174\n",
      "2497 loss_D: 21.57307 loss_N: 0.18115407 loss_f: 4.554149\n",
      "2497 Train Loss 26.308374\n",
      "2498 loss_D: 21.566433 loss_N: 0.18090978 loss_f: 4.5576053\n",
      "2498 Train Loss 26.304949\n",
      "2499 loss_D: 21.55119 loss_N: 0.18101576 loss_f: 4.5668263\n",
      "2499 Train Loss 26.29903\n",
      "2500 loss_D: 21.534391 loss_N: 0.18081722 loss_f: 4.577892\n",
      "2500 Train Loss 26.2931\n",
      "2501 loss_D: 21.523571 loss_N: 0.18191938 loss_f: 4.582812\n",
      "2501 Train Loss 26.288301\n",
      "2502 loss_D: 21.502007 loss_N: 0.1821217 loss_f: 4.598699\n",
      "2502 Train Loss 26.282827\n",
      "2503 loss_D: 21.49798 loss_N: 0.18183526 loss_f: 4.5962873\n",
      "2503 Train Loss 26.276102\n",
      "2504 loss_D: 21.508846 loss_N: 0.181475 loss_f: 4.580798\n",
      "2504 Train Loss 26.271118\n",
      "2505 loss_D: 21.511461 loss_N: 0.18086196 loss_f: 4.570241\n",
      "2505 Train Loss 26.262566\n",
      "2506 loss_D: 21.502464 loss_N: 0.1793696 loss_f: 4.571655\n",
      "2506 Train Loss 26.253489\n",
      "2507 loss_D: 21.490955 loss_N: 0.17859802 loss_f: 4.5753922\n",
      "2507 Train Loss 26.244946\n",
      "2508 loss_D: 21.473583 loss_N: 0.17770864 loss_f: 4.579691\n",
      "2508 Train Loss 26.230984\n",
      "2509 loss_D: 21.458956 loss_N: 0.1776103 loss_f: 4.5899677\n",
      "2509 Train Loss 26.226534\n",
      "2510 loss_D: 21.454985 loss_N: 0.17853156 loss_f: 4.578117\n",
      "2510 Train Loss 26.211634\n",
      "2511 loss_D: 21.468554 loss_N: 0.1784562 loss_f: 4.5532193\n",
      "2511 Train Loss 26.200228\n",
      "2512 loss_D: 21.482801 loss_N: 0.178679 loss_f: 4.524725\n",
      "2512 Train Loss 26.186205\n",
      "2513 loss_D: 21.482384 loss_N: 0.17852624 loss_f: 4.516421\n",
      "2513 Train Loss 26.17733\n",
      "2514 loss_D: 21.483133 loss_N: 0.17741132 loss_f: 4.505106\n",
      "2514 Train Loss 26.165651\n",
      "2515 loss_D: 21.489225 loss_N: 0.17616342 loss_f: 4.4905686\n",
      "2515 Train Loss 26.155956\n",
      "2516 loss_D: 21.482681 loss_N: 0.17375252 loss_f: 4.4876366\n",
      "2516 Train Loss 26.14407\n",
      "2517 loss_D: 21.485518 loss_N: 0.17195974 loss_f: 4.47531\n",
      "2517 Train Loss 26.132786\n",
      "2518 loss_D: 21.484821 loss_N: 0.1705387 loss_f: 4.466743\n",
      "2518 Train Loss 26.122103\n",
      "2519 loss_D: 21.470888 loss_N: 0.16926116 loss_f: 4.4716673\n",
      "2519 Train Loss 26.111816\n",
      "2520 loss_D: 21.461792 loss_N: 0.1683267 loss_f: 4.469603\n",
      "2520 Train Loss 26.099722\n",
      "2521 loss_D: 21.438038 loss_N: 0.16679339 loss_f: 4.476282\n",
      "2521 Train Loss 26.081114\n",
      "2522 loss_D: 21.418406 loss_N: 0.1657988 loss_f: 4.477983\n",
      "2522 Train Loss 26.062187\n",
      "2523 loss_D: 21.376303 loss_N: 0.16600193 loss_f: 4.555347\n",
      "2523 Train Loss 26.097652\n",
      "2524 loss_D: 21.406157 loss_N: 0.16576178 loss_f: 4.4838963\n",
      "2524 Train Loss 26.055815\n",
      "2525 loss_D: 21.379446 loss_N: 0.16545916 loss_f: 4.4900665\n",
      "2525 Train Loss 26.034971\n",
      "2526 loss_D: 21.370928 loss_N: 0.16577141 loss_f: 4.4847608\n",
      "2526 Train Loss 26.02146\n",
      "2527 loss_D: 21.35491 loss_N: 0.1656469 loss_f: 4.4924583\n",
      "2527 Train Loss 26.013016\n",
      "2528 loss_D: 21.348146 loss_N: 0.16638866 loss_f: 4.488786\n",
      "2528 Train Loss 26.003323\n",
      "2529 loss_D: 21.33256 loss_N: 0.1656945 loss_f: 4.4985614\n",
      "2529 Train Loss 25.996817\n",
      "2530 loss_D: 21.299788 loss_N: 0.16543667 loss_f: 4.5252275\n",
      "2530 Train Loss 25.990452\n",
      "2531 loss_D: 21.296816 loss_N: 0.16498572 loss_f: 4.51576\n",
      "2531 Train Loss 25.977562\n",
      "2532 loss_D: 21.297827 loss_N: 0.16511026 loss_f: 4.5074034\n",
      "2532 Train Loss 25.97034\n",
      "2533 loss_D: 21.291016 loss_N: 0.16525047 loss_f: 4.5066247\n",
      "2533 Train Loss 25.96289\n",
      "2534 loss_D: 21.266077 loss_N: 0.16636132 loss_f: 4.5211306\n",
      "2534 Train Loss 25.953568\n",
      "2535 loss_D: 21.261614 loss_N: 0.16733769 loss_f: 4.51407\n",
      "2535 Train Loss 25.943022\n",
      "2536 loss_D: 21.25014 loss_N: 0.168552 loss_f: 4.5081086\n",
      "2536 Train Loss 25.9268\n",
      "2537 loss_D: 21.254349 loss_N: 0.16906998 loss_f: 4.4919443\n",
      "2537 Train Loss 25.915363\n",
      "2538 loss_D: 21.27098 loss_N: 0.16865285 loss_f: 4.4649706\n",
      "2538 Train Loss 25.904606\n",
      "2539 loss_D: 21.275341 loss_N: 0.16840553 loss_f: 4.4491587\n",
      "2539 Train Loss 25.892906\n",
      "2540 loss_D: 21.29572 loss_N: 0.16764592 loss_f: 4.4197083\n",
      "2540 Train Loss 25.883074\n",
      "2541 loss_D: 21.287992 loss_N: 0.16747269 loss_f: 4.419698\n",
      "2541 Train Loss 25.875164\n",
      "2542 loss_D: 21.276993 loss_N: 0.1673634 loss_f: 4.4250145\n",
      "2542 Train Loss 25.869371\n",
      "2543 loss_D: 21.267286 loss_N: 0.16689348 loss_f: 4.426827\n",
      "2543 Train Loss 25.861006\n",
      "2544 loss_D: 21.244314 loss_N: 0.16730669 loss_f: 4.434828\n",
      "2544 Train Loss 25.846449\n",
      "2545 loss_D: 21.221914 loss_N: 0.16223562 loss_f: 4.4758725\n",
      "2545 Train Loss 25.860022\n",
      "2546 loss_D: 21.234552 loss_N: 0.16496973 loss_f: 4.435824\n",
      "2546 Train Loss 25.835346\n",
      "2547 loss_D: 21.224497 loss_N: 0.16436227 loss_f: 4.439864\n",
      "2547 Train Loss 25.828724\n",
      "2548 loss_D: 21.23254 loss_N: 0.16439629 loss_f: 4.4087873\n",
      "2548 Train Loss 25.805723\n",
      "2549 loss_D: 21.232866 loss_N: 0.16408502 loss_f: 4.3925514\n",
      "2549 Train Loss 25.789503\n",
      "2550 loss_D: 21.236614 loss_N: 0.16393118 loss_f: 4.376485\n",
      "2550 Train Loss 25.77703\n",
      "2551 loss_D: 21.217361 loss_N: 0.16388166 loss_f: 4.3834047\n",
      "2551 Train Loss 25.764648\n",
      "2552 loss_D: 21.224312 loss_N: 0.16280562 loss_f: 4.3783975\n",
      "2552 Train Loss 25.765514\n",
      "2553 loss_D: 21.2206 loss_N: 0.16335449 loss_f: 4.3740067\n",
      "2553 Train Loss 25.757961\n",
      "2554 loss_D: 21.217688 loss_N: 0.1635529 loss_f: 4.370386\n",
      "2554 Train Loss 25.751627\n",
      "2555 loss_D: 21.201933 loss_N: 0.1637787 loss_f: 4.377522\n",
      "2555 Train Loss 25.743233\n",
      "2556 loss_D: 21.189365 loss_N: 0.16411544 loss_f: 4.384887\n",
      "2556 Train Loss 25.738369\n",
      "2557 loss_D: 21.179455 loss_N: 0.16462448 loss_f: 4.387048\n",
      "2557 Train Loss 25.731129\n",
      "2558 loss_D: 21.181684 loss_N: 0.16453052 loss_f: 4.376033\n",
      "2558 Train Loss 25.722248\n",
      "2559 loss_D: 21.175373 loss_N: 0.16479509 loss_f: 4.374985\n",
      "2559 Train Loss 25.715153\n",
      "2560 loss_D: 21.172327 loss_N: 0.16490872 loss_f: 4.3735304\n",
      "2560 Train Loss 25.710766\n",
      "2561 loss_D: 21.160563 loss_N: 0.16499445 loss_f: 4.380275\n",
      "2561 Train Loss 25.705833\n",
      "2562 loss_D: 21.147142 loss_N: 0.1652841 loss_f: 4.385695\n",
      "2562 Train Loss 25.69812\n",
      "2563 loss_D: 21.125881 loss_N: 0.16530795 loss_f: 4.3985453\n",
      "2563 Train Loss 25.689735\n",
      "2564 loss_D: 21.111906 loss_N: 0.16518332 loss_f: 4.4026823\n",
      "2564 Train Loss 25.679771\n",
      "2565 loss_D: 21.092234 loss_N: 0.16507642 loss_f: 4.409733\n",
      "2565 Train Loss 25.667044\n",
      "2566 loss_D: 21.087168 loss_N: 0.1650782 loss_f: 4.406637\n",
      "2566 Train Loss 25.658884\n",
      "2567 loss_D: 21.071451 loss_N: 0.16417678 loss_f: 4.4174047\n",
      "2567 Train Loss 25.653032\n",
      "2568 loss_D: 21.066166 loss_N: 0.16517784 loss_f: 4.408367\n",
      "2568 Train Loss 25.639711\n",
      "2569 loss_D: 21.062006 loss_N: 0.16435553 loss_f: 4.3999257\n",
      "2569 Train Loss 25.626287\n",
      "2570 loss_D: 21.043968 loss_N: 0.16435416 loss_f: 4.4009724\n",
      "2570 Train Loss 25.609295\n",
      "2571 loss_D: 21.003736 loss_N: 0.16342887 loss_f: 4.41827\n",
      "2571 Train Loss 25.585436\n",
      "2572 loss_D: 20.975315 loss_N: 0.16191837 loss_f: 4.4284134\n",
      "2572 Train Loss 25.565647\n",
      "2573 loss_D: 20.928356 loss_N: 0.16152409 loss_f: 4.4944515\n",
      "2573 Train Loss 25.584332\n",
      "2574 loss_D: 20.957502 loss_N: 0.16173121 loss_f: 4.4380646\n",
      "2574 Train Loss 25.557299\n",
      "2575 loss_D: 20.928368 loss_N: 0.16106334 loss_f: 4.453772\n",
      "2575 Train Loss 25.543203\n",
      "2576 loss_D: 20.931337 loss_N: 0.16041294 loss_f: 4.4352217\n",
      "2576 Train Loss 25.526974\n",
      "2577 loss_D: 20.912409 loss_N: 0.16055171 loss_f: 4.441934\n",
      "2577 Train Loss 25.514894\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2578 loss_D: 20.923794 loss_N: 0.16159226 loss_f: 4.4207582\n",
      "2578 Train Loss 25.506145\n",
      "2579 loss_D: 20.925644 loss_N: 0.16174285 loss_f: 4.412329\n",
      "2579 Train Loss 25.499716\n",
      "2580 loss_D: 20.9212 loss_N: 0.1617716 loss_f: 4.411468\n",
      "2580 Train Loss 25.49444\n",
      "2581 loss_D: 20.909992 loss_N: 0.16159543 loss_f: 4.416106\n",
      "2581 Train Loss 25.487694\n",
      "2582 loss_D: 20.899498 loss_N: 0.16116127 loss_f: 4.420229\n",
      "2582 Train Loss 25.480888\n",
      "2583 loss_D: 20.88934 loss_N: 0.1607548 loss_f: 4.424438\n",
      "2583 Train Loss 25.474533\n",
      "2584 loss_D: 20.8847 loss_N: 0.16026105 loss_f: 4.4205084\n",
      "2584 Train Loss 25.46547\n",
      "2585 loss_D: 20.885847 loss_N: 0.16053206 loss_f: 4.40555\n",
      "2585 Train Loss 25.45193\n",
      "2586 loss_D: 20.888975 loss_N: 0.16086331 loss_f: 4.3903203\n",
      "2586 Train Loss 25.440159\n",
      "2587 loss_D: 20.91299 loss_N: 0.16216074 loss_f: 4.350292\n",
      "2587 Train Loss 25.425444\n",
      "2588 loss_D: 20.913017 loss_N: 0.16345002 loss_f: 4.3358555\n",
      "2588 Train Loss 25.412323\n",
      "2589 loss_D: 20.90509 loss_N: 0.16403922 loss_f: 4.3293114\n",
      "2589 Train Loss 25.398441\n",
      "2590 loss_D: 20.89065 loss_N: 0.16269289 loss_f: 4.329672\n",
      "2590 Train Loss 25.383015\n",
      "2591 loss_D: 20.862736 loss_N: 0.1606533 loss_f: 4.3400364\n",
      "2591 Train Loss 25.363426\n",
      "2592 loss_D: 20.843277 loss_N: 0.15921663 loss_f: 4.3410387\n",
      "2592 Train Loss 25.343533\n",
      "2593 loss_D: 20.750822 loss_N: 0.16100042 loss_f: 4.5559106\n",
      "2593 Train Loss 25.467733\n",
      "2594 loss_D: 20.823685 loss_N: 0.15954912 loss_f: 4.352462\n",
      "2594 Train Loss 25.335697\n",
      "2595 loss_D: 20.80208 loss_N: 0.15734246 loss_f: 4.3605695\n",
      "2595 Train Loss 25.319992\n",
      "2596 loss_D: 20.809633 loss_N: 0.15661757 loss_f: 4.333992\n",
      "2596 Train Loss 25.300243\n",
      "2597 loss_D: 20.806458 loss_N: 0.15715449 loss_f: 4.3254967\n",
      "2597 Train Loss 25.289108\n",
      "2598 loss_D: 20.805408 loss_N: 0.1573182 loss_f: 4.3184743\n",
      "2598 Train Loss 25.2812\n",
      "2599 loss_D: 20.794134 loss_N: 0.15657808 loss_f: 4.3229184\n",
      "2599 Train Loss 25.27363\n",
      "2600 loss_D: 20.780933 loss_N: 0.15569304 loss_f: 4.329502\n",
      "2600 Train Loss 25.266129\n",
      "2601 loss_D: 20.776825 loss_N: 0.15560557 loss_f: 4.3288527\n",
      "2601 Train Loss 25.261284\n",
      "2602 loss_D: 20.783112 loss_N: 0.1435493 loss_f: 4.9351196\n",
      "2602 Train Loss 25.86178\n",
      "2603 loss_D: 20.774597 loss_N: 0.15446016 loss_f: 4.3275704\n",
      "2603 Train Loss 25.256628\n",
      "2604 loss_D: 20.769285 loss_N: 0.15379672 loss_f: 4.326095\n",
      "2604 Train Loss 25.249178\n",
      "2605 loss_D: 20.771387 loss_N: 0.1526851 loss_f: 4.317535\n",
      "2605 Train Loss 25.241608\n",
      "2606 loss_D: 20.774895 loss_N: 0.15255788 loss_f: 4.3066893\n",
      "2606 Train Loss 25.234142\n",
      "2607 loss_D: 20.774849 loss_N: 0.15216903 loss_f: 4.296348\n",
      "2607 Train Loss 25.223366\n",
      "2608 loss_D: 20.760715 loss_N: 0.15212604 loss_f: 4.2979603\n",
      "2608 Train Loss 25.210802\n",
      "2609 loss_D: 20.734089 loss_N: 0.15179794 loss_f: 4.3140664\n",
      "2609 Train Loss 25.199953\n",
      "2610 loss_D: 20.723663 loss_N: 0.15197045 loss_f: 4.319293\n",
      "2610 Train Loss 25.194927\n",
      "2611 loss_D: 20.690947 loss_N: 0.15189265 loss_f: 4.3549232\n",
      "2611 Train Loss 25.197762\n",
      "2612 loss_D: 20.709925 loss_N: 0.15188466 loss_f: 4.3302145\n",
      "2612 Train Loss 25.192024\n",
      "2613 loss_D: 20.67341 loss_N: 0.15241708 loss_f: 4.3610773\n",
      "2613 Train Loss 25.186905\n",
      "2614 loss_D: 20.690596 loss_N: 0.15222007 loss_f: 4.334299\n",
      "2614 Train Loss 25.177114\n",
      "2615 loss_D: 20.6919 loss_N: 0.15185659 loss_f: 4.323718\n",
      "2615 Train Loss 25.167477\n",
      "2616 loss_D: 20.687626 loss_N: 0.15131643 loss_f: 4.313157\n",
      "2616 Train Loss 25.1521\n",
      "2617 loss_D: 20.66412 loss_N: 0.15062585 loss_f: 4.3154798\n",
      "2617 Train Loss 25.130224\n",
      "2618 loss_D: 20.624868 loss_N: 0.14973974 loss_f: 4.3491945\n",
      "2618 Train Loss 25.123802\n",
      "2619 loss_D: 20.620375 loss_N: 0.1496238 loss_f: 4.3303604\n",
      "2619 Train Loss 25.100359\n",
      "2620 loss_D: 20.628565 loss_N: 0.15219189 loss_f: 4.318271\n",
      "2620 Train Loss 25.099028\n",
      "2621 loss_D: 20.62437 loss_N: 0.15093051 loss_f: 4.3157296\n",
      "2621 Train Loss 25.09103\n",
      "2622 loss_D: 20.615734 loss_N: 0.15173881 loss_f: 4.3081684\n",
      "2622 Train Loss 25.075642\n",
      "2623 loss_D: 20.579786 loss_N: 0.15156552 loss_f: 4.3145056\n",
      "2623 Train Loss 25.045856\n",
      "2624 loss_D: 20.571623 loss_N: 0.1528273 loss_f: 4.2977805\n",
      "2624 Train Loss 25.022232\n",
      "2625 loss_D: 20.528347 loss_N: 0.15183315 loss_f: 4.316435\n",
      "2625 Train Loss 24.996614\n",
      "2626 loss_D: 20.522448 loss_N: 0.15245397 loss_f: 4.297554\n",
      "2626 Train Loss 24.972456\n",
      "2627 loss_D: 20.499376 loss_N: 0.15231423 loss_f: 4.300685\n",
      "2627 Train Loss 24.952377\n",
      "2628 loss_D: 20.48628 loss_N: 0.15227725 loss_f: 4.299954\n",
      "2628 Train Loss 24.93851\n",
      "2629 loss_D: 20.471003 loss_N: 0.1522385 loss_f: 4.301588\n",
      "2629 Train Loss 24.92483\n",
      "2630 loss_D: 20.447035 loss_N: 0.15227608 loss_f: 4.3141723\n",
      "2630 Train Loss 24.913485\n",
      "2631 loss_D: 20.451214 loss_N: 0.15277307 loss_f: 4.303847\n",
      "2631 Train Loss 24.907833\n",
      "2632 loss_D: 20.439072 loss_N: 0.15270661 loss_f: 4.307364\n",
      "2632 Train Loss 24.899141\n",
      "2633 loss_D: 20.440334 loss_N: 0.1521508 loss_f: 4.3019075\n",
      "2633 Train Loss 24.894394\n",
      "2634 loss_D: 20.439587 loss_N: 0.15212043 loss_f: 4.2982826\n",
      "2634 Train Loss 24.88999\n",
      "2635 loss_D: 20.433434 loss_N: 0.1520001 loss_f: 4.2971916\n",
      "2635 Train Loss 24.882626\n",
      "2636 loss_D: 20.4216 loss_N: 0.15180661 loss_f: 4.3012094\n",
      "2636 Train Loss 24.874615\n",
      "2637 loss_D: 20.386866 loss_N: 0.15094538 loss_f: 4.334446\n",
      "2637 Train Loss 24.872257\n",
      "2638 loss_D: 20.393167 loss_N: 0.15069313 loss_f: 4.3180943\n",
      "2638 Train Loss 24.861956\n",
      "2639 loss_D: 20.39309 loss_N: 0.15059027 loss_f: 4.3133273\n",
      "2639 Train Loss 24.857008\n",
      "2640 loss_D: 20.392784 loss_N: 0.14877644 loss_f: 4.3126607\n",
      "2640 Train Loss 24.854221\n",
      "2641 loss_D: 20.3899 loss_N: 0.14976257 loss_f: 4.3079524\n",
      "2641 Train Loss 24.847616\n",
      "2642 loss_D: 20.38394 loss_N: 0.14970204 loss_f: 4.3101397\n",
      "2642 Train Loss 24.84378\n",
      "2643 loss_D: 20.379332 loss_N: 0.14957573 loss_f: 4.310485\n",
      "2643 Train Loss 24.839394\n",
      "2644 loss_D: 20.354315 loss_N: 0.14889461 loss_f: 4.336442\n",
      "2644 Train Loss 24.839653\n",
      "2645 loss_D: 20.366888 loss_N: 0.14922737 loss_f: 4.319075\n",
      "2645 Train Loss 24.83519\n",
      "2646 loss_D: 20.36572 loss_N: 0.14923908 loss_f: 4.3143477\n",
      "2646 Train Loss 24.829308\n",
      "2647 loss_D: 20.357294 loss_N: 0.14922956 loss_f: 4.3129897\n",
      "2647 Train Loss 24.819513\n",
      "2648 loss_D: 20.344763 loss_N: 0.14915502 loss_f: 4.311995\n",
      "2648 Train Loss 24.805912\n",
      "2649 loss_D: 20.28315 loss_N: 0.15072857 loss_f: 4.39429\n",
      "2649 Train Loss 24.828167\n",
      "2650 loss_D: 20.32574 loss_N: 0.14956957 loss_f: 4.32512\n",
      "2650 Train Loss 24.80043\n",
      "2651 loss_D: 20.275066 loss_N: 0.14894636 loss_f: 4.369132\n",
      "2651 Train Loss 24.793144\n",
      "2652 loss_D: 20.28749 loss_N: 0.1489835 loss_f: 4.3413086\n",
      "2652 Train Loss 24.777782\n",
      "2653 loss_D: 20.284895 loss_N: 0.14939407 loss_f: 4.332369\n",
      "2653 Train Loss 24.766659\n",
      "2654 loss_D: 20.277206 loss_N: 0.1501229 loss_f: 4.328907\n",
      "2654 Train Loss 24.756237\n",
      "2655 loss_D: 20.265017 loss_N: 0.15147361 loss_f: 4.3283305\n",
      "2655 Train Loss 24.744822\n",
      "2656 loss_D: 20.249958 loss_N: 0.15252514 loss_f: 4.3305006\n",
      "2656 Train Loss 24.732983\n",
      "2657 loss_D: 20.237362 loss_N: 0.1536803 loss_f: 4.333588\n",
      "2657 Train Loss 24.72463\n",
      "2658 loss_D: 20.22439 loss_N: 0.15351337 loss_f: 4.3377423\n",
      "2658 Train Loss 24.715645\n",
      "2659 loss_D: 20.213974 loss_N: 0.1531408 loss_f: 4.339885\n",
      "2659 Train Loss 24.707\n",
      "2660 loss_D: 20.191391 loss_N: 0.15145795 loss_f: 4.3529997\n",
      "2660 Train Loss 24.69585\n",
      "2661 loss_D: 20.180433 loss_N: 0.15088017 loss_f: 4.356215\n",
      "2661 Train Loss 24.687529\n",
      "2662 loss_D: 20.16587 loss_N: 0.14973935 loss_f: 4.364039\n",
      "2662 Train Loss 24.67965\n",
      "2663 loss_D: 20.156511 loss_N: 0.14867422 loss_f: 4.3665004\n",
      "2663 Train Loss 24.671686\n",
      "2664 loss_D: 20.124947 loss_N: 0.14898375 loss_f: 4.3879533\n",
      "2664 Train Loss 24.661882\n",
      "2665 loss_D: 20.11915 loss_N: 0.14730904 loss_f: 4.384377\n",
      "2665 Train Loss 24.650835\n",
      "2666 loss_D: 20.120249 loss_N: 0.14787364 loss_f: 4.372695\n",
      "2666 Train Loss 24.640816\n",
      "2667 loss_D: 20.121973 loss_N: 0.1484148 loss_f: 4.362082\n",
      "2667 Train Loss 24.63247\n",
      "2668 loss_D: 20.118074 loss_N: 0.14856525 loss_f: 4.3597775\n",
      "2668 Train Loss 24.626417\n",
      "2669 loss_D: 20.122303 loss_N: 0.1485232 loss_f: 4.345909\n",
      "2669 Train Loss 24.616735\n",
      "2670 loss_D: 20.127428 loss_N: 0.14861198 loss_f: 4.3274627\n",
      "2670 Train Loss 24.603502\n",
      "2671 loss_D: 20.138224 loss_N: 0.14878096 loss_f: 4.3026524\n",
      "2671 Train Loss 24.589657\n",
      "2672 loss_D: 20.128786 loss_N: 0.14939758 loss_f: 4.3046403\n",
      "2672 Train Loss 24.582823\n",
      "2673 loss_D: 20.139826 loss_N: 0.14882687 loss_f: 4.2804585\n",
      "2673 Train Loss 24.56911\n",
      "2674 loss_D: 20.126965 loss_N: 0.14834024 loss_f: 4.2857995\n",
      "2674 Train Loss 24.561104\n",
      "2675 loss_D: 20.112553 loss_N: 0.1486787 loss_f: 4.284679\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2675 Train Loss 24.54591\n",
      "2676 loss_D: 20.085793 loss_N: 0.14994217 loss_f: 4.288294\n",
      "2676 Train Loss 24.524029\n",
      "2677 loss_D: 20.065424 loss_N: 0.15048233 loss_f: 4.291407\n",
      "2677 Train Loss 24.507313\n",
      "2678 loss_D: 20.0583 loss_N: 0.15053451 loss_f: 4.282173\n",
      "2678 Train Loss 24.491007\n",
      "2679 loss_D: 20.035065 loss_N: 0.14921317 loss_f: 4.2947164\n",
      "2679 Train Loss 24.478994\n",
      "2680 loss_D: 20.034414 loss_N: 0.14775668 loss_f: 4.2853894\n",
      "2680 Train Loss 24.46756\n",
      "2681 loss_D: 20.017378 loss_N: 0.14774634 loss_f: 4.294129\n",
      "2681 Train Loss 24.459253\n",
      "2682 loss_D: 20.012146 loss_N: 0.14756115 loss_f: 4.291846\n",
      "2682 Train Loss 24.451553\n",
      "2683 loss_D: 20.007206 loss_N: 0.14729539 loss_f: 4.289639\n",
      "2683 Train Loss 24.44414\n",
      "2684 loss_D: 20.025 loss_N: 0.14750902 loss_f: 4.304258\n",
      "2684 Train Loss 24.476767\n",
      "2685 loss_D: 20.011074 loss_N: 0.14731118 loss_f: 4.2813225\n",
      "2685 Train Loss 24.439707\n",
      "2686 loss_D: 20.019888 loss_N: 0.14745627 loss_f: 4.2636476\n",
      "2686 Train Loss 24.430992\n",
      "2687 loss_D: 20.037008 loss_N: 0.14799723 loss_f: 4.237694\n",
      "2687 Train Loss 24.422699\n",
      "2688 loss_D: 20.052607 loss_N: 0.14857215 loss_f: 4.2161083\n",
      "2688 Train Loss 24.417288\n",
      "2689 loss_D: 20.079256 loss_N: 0.15178686 loss_f: 4.203873\n",
      "2689 Train Loss 24.434916\n",
      "2690 loss_D: 20.059275 loss_N: 0.14934051 loss_f: 4.206237\n",
      "2690 Train Loss 24.414852\n",
      "2691 loss_D: 20.077024 loss_N: 0.14976174 loss_f: 4.1827555\n",
      "2691 Train Loss 24.409542\n",
      "2692 loss_D: 20.075079 loss_N: 0.14954041 loss_f: 4.179995\n",
      "2692 Train Loss 24.404613\n",
      "2693 loss_D: 20.067015 loss_N: 0.14912301 loss_f: 4.1826725\n",
      "2693 Train Loss 24.39881\n",
      "2694 loss_D: 20.055328 loss_N: 0.14867786 loss_f: 4.1849923\n",
      "2694 Train Loss 24.388998\n",
      "2695 loss_D: 20.049675 loss_N: 0.14893638 loss_f: 4.1823897\n",
      "2695 Train Loss 24.381002\n",
      "2696 loss_D: 20.039042 loss_N: 0.14931472 loss_f: 4.1844025\n",
      "2696 Train Loss 24.372759\n",
      "2697 loss_D: 20.034838 loss_N: 0.14955485 loss_f: 4.1806827\n",
      "2697 Train Loss 24.365076\n",
      "2698 loss_D: 20.024925 loss_N: 0.14901851 loss_f: 4.195231\n",
      "2698 Train Loss 24.369175\n",
      "2699 loss_D: 20.030449 loss_N: 0.14932002 loss_f: 4.180786\n",
      "2699 Train Loss 24.360556\n",
      "2700 loss_D: 20.019247 loss_N: 0.1488281 loss_f: 4.1856146\n",
      "2700 Train Loss 24.353691\n",
      "2701 loss_D: 20.007261 loss_N: 0.14764872 loss_f: 4.189678\n",
      "2701 Train Loss 24.344587\n",
      "2702 loss_D: 19.993927 loss_N: 0.14661154 loss_f: 4.1961346\n",
      "2702 Train Loss 24.336674\n",
      "2703 loss_D: 19.976662 loss_N: 0.14587666 loss_f: 4.2056046\n",
      "2703 Train Loss 24.328142\n",
      "2704 loss_D: 19.953592 loss_N: 0.14543311 loss_f: 4.220695\n",
      "2704 Train Loss 24.319721\n",
      "2705 loss_D: 19.918997 loss_N: 0.14586376 loss_f: 4.245517\n",
      "2705 Train Loss 24.310379\n",
      "2706 loss_D: 19.899849 loss_N: 0.14558786 loss_f: 4.253224\n",
      "2706 Train Loss 24.29866\n",
      "2707 loss_D: 19.872028 loss_N: 0.14572656 loss_f: 4.265509\n",
      "2707 Train Loss 24.283264\n",
      "2708 loss_D: 19.84477 loss_N: 0.1461813 loss_f: 4.2763267\n",
      "2708 Train Loss 24.267279\n",
      "2709 loss_D: 19.80506 loss_N: 0.14643414 loss_f: 4.298238\n",
      "2709 Train Loss 24.249733\n",
      "2710 loss_D: 19.79592 loss_N: 0.1476873 loss_f: 4.292301\n",
      "2710 Train Loss 24.235909\n",
      "2711 loss_D: 19.75615 loss_N: 0.14716625 loss_f: 4.3119245\n",
      "2711 Train Loss 24.21524\n",
      "2712 loss_D: 19.76147 loss_N: 0.14681816 loss_f: 4.2855253\n",
      "2712 Train Loss 24.193813\n",
      "2713 loss_D: 19.747643 loss_N: 0.14644006 loss_f: 4.2739053\n",
      "2713 Train Loss 24.167988\n",
      "2714 loss_D: 19.742243 loss_N: 0.14669588 loss_f: 4.2499366\n",
      "2714 Train Loss 24.138876\n",
      "2715 loss_D: 19.69615 loss_N: 0.14819403 loss_f: 4.2540207\n",
      "2715 Train Loss 24.098364\n",
      "2716 loss_D: 19.679033 loss_N: 0.14997253 loss_f: 4.2415757\n",
      "2716 Train Loss 24.070581\n",
      "2717 loss_D: 19.651346 loss_N: 0.15075012 loss_f: 4.2407584\n",
      "2717 Train Loss 24.042854\n",
      "2718 loss_D: 19.633783 loss_N: 0.15008998 loss_f: 4.2339993\n",
      "2718 Train Loss 24.017872\n",
      "2719 loss_D: 19.633436 loss_N: 0.14972518 loss_f: 4.2209883\n",
      "2719 Train Loss 24.00415\n",
      "2720 loss_D: 19.636225 loss_N: 0.14918636 loss_f: 4.2064824\n",
      "2720 Train Loss 23.991894\n",
      "2721 loss_D: 19.636559 loss_N: 0.14896229 loss_f: 4.1917167\n",
      "2721 Train Loss 23.977238\n",
      "2722 loss_D: 19.621712 loss_N: 0.14934856 loss_f: 4.1940227\n",
      "2722 Train Loss 23.965084\n",
      "2723 loss_D: 19.597502 loss_N: 0.15031105 loss_f: 4.203145\n",
      "2723 Train Loss 23.950958\n",
      "2724 loss_D: 19.56005 loss_N: 0.15083589 loss_f: 4.2243476\n",
      "2724 Train Loss 23.935232\n",
      "2725 loss_D: 19.529213 loss_N: 0.15040407 loss_f: 4.2418094\n",
      "2725 Train Loss 23.921427\n",
      "2726 loss_D: 19.51044 loss_N: 0.1491394 loss_f: 4.251903\n",
      "2726 Train Loss 23.911484\n",
      "2727 loss_D: 19.501808 loss_N: 0.15063398 loss_f: 4.2588015\n",
      "2727 Train Loss 23.911243\n",
      "2728 loss_D: 19.505909 loss_N: 0.14988764 loss_f: 4.250458\n",
      "2728 Train Loss 23.906254\n",
      "2729 loss_D: 19.47666 loss_N: 0.1471487 loss_f: 4.283571\n",
      "2729 Train Loss 23.90738\n",
      "2730 loss_D: 19.490854 loss_N: 0.148364 loss_f: 4.2513313\n",
      "2730 Train Loss 23.890549\n",
      "2731 loss_D: 19.492887 loss_N: 0.14763843 loss_f: 4.236962\n",
      "2731 Train Loss 23.877487\n",
      "2732 loss_D: 19.501358 loss_N: 0.14762099 loss_f: 4.2197375\n",
      "2732 Train Loss 23.868717\n",
      "2733 loss_D: 19.480045 loss_N: 0.1477382 loss_f: 4.2313766\n",
      "2733 Train Loss 23.85916\n",
      "2734 loss_D: 19.476624 loss_N: 0.14822504 loss_f: 4.2219367\n",
      "2734 Train Loss 23.846786\n",
      "2735 loss_D: 19.455675 loss_N: 0.14883962 loss_f: 4.232384\n",
      "2735 Train Loss 23.836899\n",
      "2736 loss_D: 19.440544 loss_N: 0.14935918 loss_f: 4.2375298\n",
      "2736 Train Loss 23.827433\n",
      "2737 loss_D: 19.420835 loss_N: 0.14956069 loss_f: 4.24849\n",
      "2737 Train Loss 23.818886\n",
      "2738 loss_D: 19.379053 loss_N: 0.14615259 loss_f: 4.319663\n",
      "2738 Train Loss 23.844868\n",
      "2739 loss_D: 19.409258 loss_N: 0.14859805 loss_f: 4.2574396\n",
      "2739 Train Loss 23.815296\n",
      "2740 loss_D: 19.379658 loss_N: 0.14832231 loss_f: 4.291125\n",
      "2740 Train Loss 23.819105\n",
      "2741 loss_D: 19.39588 loss_N: 0.14843448 loss_f: 4.263925\n",
      "2741 Train Loss 23.808239\n",
      "2742 loss_D: 19.380476 loss_N: 0.14853804 loss_f: 4.2684484\n",
      "2742 Train Loss 23.797462\n",
      "2743 loss_D: 19.338118 loss_N: 0.14744855 loss_f: 4.2891836\n",
      "2743 Train Loss 23.77475\n",
      "2744 loss_D: 19.215698 loss_N: 0.14709009 loss_f: 4.453185\n",
      "2744 Train Loss 23.815975\n",
      "2745 loss_D: 19.297285 loss_N: 0.14711565 loss_f: 4.318421\n",
      "2745 Train Loss 23.762821\n",
      "2746 loss_D: 19.266087 loss_N: 0.14661297 loss_f: 4.330614\n",
      "2746 Train Loss 23.743313\n",
      "2747 loss_D: 19.243452 loss_N: 0.14634052 loss_f: 4.336007\n",
      "2747 Train Loss 23.7258\n",
      "2748 loss_D: 19.184856 loss_N: 0.14554714 loss_f: 4.386511\n",
      "2748 Train Loss 23.716915\n",
      "2749 loss_D: 19.216402 loss_N: 0.14561339 loss_f: 4.3456507\n",
      "2749 Train Loss 23.707664\n",
      "2750 loss_D: 19.215601 loss_N: 0.14612658 loss_f: 4.339909\n",
      "2750 Train Loss 23.701635\n",
      "2751 loss_D: 19.213484 loss_N: 0.14609092 loss_f: 4.3372335\n",
      "2751 Train Loss 23.696808\n",
      "2752 loss_D: 19.214727 loss_N: 0.1461103 loss_f: 4.331078\n",
      "2752 Train Loss 23.691916\n",
      "2753 loss_D: 19.214731 loss_N: 0.14623757 loss_f: 4.325829\n",
      "2753 Train Loss 23.686798\n",
      "2754 loss_D: 19.2219 loss_N: 0.1465008 loss_f: 4.312572\n",
      "2754 Train Loss 23.680975\n",
      "2755 loss_D: 19.225916 loss_N: 0.147179 loss_f: 4.3014545\n",
      "2755 Train Loss 23.67455\n",
      "2756 loss_D: 19.229782 loss_N: 0.14752363 loss_f: 4.2914634\n",
      "2756 Train Loss 23.66877\n",
      "2757 loss_D: 19.230215 loss_N: 0.14802529 loss_f: 4.282618\n",
      "2757 Train Loss 23.660858\n",
      "2758 loss_D: 19.22579 loss_N: 0.14883666 loss_f: 4.2737527\n",
      "2758 Train Loss 23.648378\n",
      "2759 loss_D: 19.218815 loss_N: 0.15017514 loss_f: 4.27116\n",
      "2759 Train Loss 23.64015\n",
      "2760 loss_D: 19.211767 loss_N: 0.14973639 loss_f: 4.2632613\n",
      "2760 Train Loss 23.624765\n",
      "2761 loss_D: 19.197908 loss_N: 0.14934199 loss_f: 4.262226\n",
      "2761 Train Loss 23.609476\n",
      "2762 loss_D: 19.177393 loss_N: 0.14902651 loss_f: 4.267974\n",
      "2762 Train Loss 23.594395\n",
      "2763 loss_D: 19.140514 loss_N: 0.14901505 loss_f: 4.2883673\n",
      "2763 Train Loss 23.577896\n",
      "2764 loss_D: 19.08849 loss_N: 0.14850812 loss_f: 4.327351\n",
      "2764 Train Loss 23.564348\n",
      "2765 loss_D: 19.08384 loss_N: 0.14924332 loss_f: 4.3152413\n",
      "2765 Train Loss 23.548323\n",
      "2766 loss_D: 19.066324 loss_N: 0.14964464 loss_f: 4.3242574\n",
      "2766 Train Loss 23.540226\n",
      "2767 loss_D: 19.06066 loss_N: 0.15023035 loss_f: 4.32138\n",
      "2767 Train Loss 23.53227\n",
      "2768 loss_D: 19.035145 loss_N: 0.15100698 loss_f: 4.335765\n",
      "2768 Train Loss 23.521915\n",
      "2769 loss_D: 19.021027 loss_N: 0.15179196 loss_f: 4.3328676\n",
      "2769 Train Loss 23.505688\n",
      "2770 loss_D: 18.99225 loss_N: 0.15273279 loss_f: 4.34445\n",
      "2770 Train Loss 23.489433\n",
      "2771 loss_D: 18.998009 loss_N: 0.15260959 loss_f: 4.324781\n",
      "2771 Train Loss 23.475399\n",
      "2772 loss_D: 18.978214 loss_N: 0.15300725 loss_f: 4.3279905\n",
      "2772 Train Loss 23.459213\n",
      "2773 loss_D: 18.981249 loss_N: 0.15262473 loss_f: 4.3075\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2773 Train Loss 23.441372\n",
      "2774 loss_D: 18.967745 loss_N: 0.15260202 loss_f: 4.3022356\n",
      "2774 Train Loss 23.42258\n",
      "2775 loss_D: 18.944122 loss_N: 0.15172242 loss_f: 4.3166\n",
      "2775 Train Loss 23.412445\n",
      "2776 loss_D: 18.9607 loss_N: 0.1527591 loss_f: 4.2765913\n",
      "2776 Train Loss 23.390049\n",
      "2777 loss_D: 18.95102 loss_N: 0.15368037 loss_f: 4.269642\n",
      "2777 Train Loss 23.374342\n",
      "2778 loss_D: 18.949825 loss_N: 0.15424407 loss_f: 4.25396\n",
      "2778 Train Loss 23.358028\n",
      "2779 loss_D: 18.954311 loss_N: 0.15902913 loss_f: 4.451828\n",
      "2779 Train Loss 23.565168\n",
      "2780 loss_D: 18.947227 loss_N: 0.15499352 loss_f: 4.2456794\n",
      "2780 Train Loss 23.3479\n",
      "2781 loss_D: 18.944746 loss_N: 0.15450454 loss_f: 4.236253\n",
      "2781 Train Loss 23.335503\n",
      "2782 loss_D: 18.940311 loss_N: 0.15354277 loss_f: 4.2167606\n",
      "2782 Train Loss 23.310616\n",
      "2783 loss_D: 18.9413 loss_N: 0.15324174 loss_f: 4.2010145\n",
      "2783 Train Loss 23.295555\n",
      "2784 loss_D: 18.93875 loss_N: 0.1532277 loss_f: 4.18583\n",
      "2784 Train Loss 23.277805\n",
      "2785 loss_D: 18.923761 loss_N: 0.15294695 loss_f: 4.190577\n",
      "2785 Train Loss 23.267284\n",
      "2786 loss_D: 18.920378 loss_N: 0.15225106 loss_f: 4.184679\n",
      "2786 Train Loss 23.257307\n",
      "2787 loss_D: 18.902126 loss_N: 0.1515699 loss_f: 4.1937346\n",
      "2787 Train Loss 23.24743\n",
      "2788 loss_D: 18.896097 loss_N: 0.15052198 loss_f: 4.1909547\n",
      "2788 Train Loss 23.237574\n",
      "2789 loss_D: 18.879463 loss_N: 0.1496615 loss_f: 4.1978884\n",
      "2789 Train Loss 23.227013\n",
      "2790 loss_D: 18.868093 loss_N: 0.14899789 loss_f: 4.200588\n",
      "2790 Train Loss 23.21768\n",
      "2791 loss_D: 18.847113 loss_N: 0.14829713 loss_f: 4.2077975\n",
      "2791 Train Loss 23.203207\n",
      "2792 loss_D: 18.803238 loss_N: 0.14723024 loss_f: 4.273894\n",
      "2792 Train Loss 23.224361\n",
      "2793 loss_D: 18.831898 loss_N: 0.14792643 loss_f: 4.216303\n",
      "2793 Train Loss 23.196127\n",
      "2794 loss_D: 18.821981 loss_N: 0.14782141 loss_f: 4.2132044\n",
      "2794 Train Loss 23.183006\n",
      "2795 loss_D: 18.813528 loss_N: 0.14788422 loss_f: 4.2098975\n",
      "2795 Train Loss 23.17131\n",
      "2796 loss_D: 18.815897 loss_N: 0.14749913 loss_f: 4.1966825\n",
      "2796 Train Loss 23.160078\n",
      "2797 loss_D: 18.817513 loss_N: 0.1468857 loss_f: 4.185853\n",
      "2797 Train Loss 23.15025\n",
      "2798 loss_D: 18.830864 loss_N: 0.14638679 loss_f: 4.1605153\n",
      "2798 Train Loss 23.137766\n",
      "2799 loss_D: 18.84654 loss_N: 0.14465937 loss_f: 4.143383\n",
      "2799 Train Loss 23.134583\n",
      "2800 loss_D: 18.845062 loss_N: 0.14582604 loss_f: 4.1333747\n",
      "2800 Train Loss 23.124264\n",
      "2801 loss_D: 18.855692 loss_N: 0.14598846 loss_f: 4.117294\n",
      "2801 Train Loss 23.118975\n",
      "2802 loss_D: 18.861134 loss_N: 0.14664687 loss_f: 4.100839\n",
      "2802 Train Loss 23.10862\n",
      "2803 loss_D: 18.874578 loss_N: 0.14842336 loss_f: 4.0854754\n",
      "2803 Train Loss 23.108479\n",
      "2804 loss_D: 18.867699 loss_N: 0.14752577 loss_f: 4.0870595\n",
      "2804 Train Loss 23.102283\n",
      "2805 loss_D: 18.864784 loss_N: 0.14703685 loss_f: 4.0806556\n",
      "2805 Train Loss 23.092478\n",
      "2806 loss_D: 18.865583 loss_N: 0.14653268 loss_f: 4.075677\n",
      "2806 Train Loss 23.087791\n",
      "2807 loss_D: 18.859022 loss_N: 0.14620426 loss_f: 4.0777884\n",
      "2807 Train Loss 23.083015\n",
      "2808 loss_D: 18.854347 loss_N: 0.14653309 loss_f: 4.0771484\n",
      "2808 Train Loss 23.07803\n",
      "2809 loss_D: 18.847061 loss_N: 0.14736146 loss_f: 4.078337\n",
      "2809 Train Loss 23.07276\n",
      "2810 loss_D: 18.844881 loss_N: 0.14821887 loss_f: 4.076113\n",
      "2810 Train Loss 23.069212\n",
      "2811 loss_D: 18.846012 loss_N: 0.14898625 loss_f: 4.0707493\n",
      "2811 Train Loss 23.065748\n",
      "2812 loss_D: 18.8471 loss_N: 0.14976333 loss_f: 4.0651464\n",
      "2812 Train Loss 23.062008\n",
      "2813 loss_D: 18.851002 loss_N: 0.14998317 loss_f: 4.0578566\n",
      "2813 Train Loss 23.058842\n",
      "2814 loss_D: 18.84978 loss_N: 0.15025663 loss_f: 4.054725\n",
      "2814 Train Loss 23.054762\n",
      "2815 loss_D: 18.844856 loss_N: 0.15022463 loss_f: 4.05359\n",
      "2815 Train Loss 23.048672\n",
      "2816 loss_D: 18.836956 loss_N: 0.15029147 loss_f: 4.0561886\n",
      "2816 Train Loss 23.043436\n",
      "2817 loss_D: 18.82491 loss_N: 0.15004863 loss_f: 4.061446\n",
      "2817 Train Loss 23.036404\n",
      "2818 loss_D: 18.805052 loss_N: 0.14945531 loss_f: 4.0814204\n",
      "2818 Train Loss 23.035929\n",
      "2819 loss_D: 18.814459 loss_N: 0.1497365 loss_f: 4.0678005\n",
      "2819 Train Loss 23.031996\n",
      "2820 loss_D: 18.809582 loss_N: 0.14910708 loss_f: 4.0674267\n",
      "2820 Train Loss 23.026115\n",
      "2821 loss_D: 18.808937 loss_N: 0.14830688 loss_f: 4.063382\n",
      "2821 Train Loss 23.020626\n",
      "2822 loss_D: 18.818525 loss_N: 0.14862873 loss_f: 4.206809\n",
      "2822 Train Loss 23.173962\n",
      "2823 loss_D: 18.808952 loss_N: 0.1482819 loss_f: 4.062074\n",
      "2823 Train Loss 23.019308\n",
      "2824 loss_D: 18.807327 loss_N: 0.14739545 loss_f: 4.0583673\n",
      "2824 Train Loss 23.01309\n",
      "2825 loss_D: 18.806541 loss_N: 0.14682701 loss_f: 4.054148\n",
      "2825 Train Loss 23.007517\n",
      "2826 loss_D: 18.800247 loss_N: 0.14635168 loss_f: 4.0563745\n",
      "2826 Train Loss 23.002972\n",
      "2827 loss_D: 18.798615 loss_N: 0.1458197 loss_f: 4.0522723\n",
      "2827 Train Loss 22.996708\n",
      "2828 loss_D: 18.774446 loss_N: 0.14526494 loss_f: 4.0671587\n",
      "2828 Train Loss 22.98687\n",
      "2829 loss_D: 18.79887 loss_N: 0.14591977 loss_f: 4.0522313\n",
      "2829 Train Loss 22.99702\n",
      "2830 loss_D: 18.782635 loss_N: 0.14543545 loss_f: 4.054439\n",
      "2830 Train Loss 22.98251\n",
      "2831 loss_D: 18.796 loss_N: 0.14553757 loss_f: 4.0327\n",
      "2831 Train Loss 22.974237\n",
      "2832 loss_D: 18.796724 loss_N: 0.14558971 loss_f: 4.0227637\n",
      "2832 Train Loss 22.965078\n",
      "2833 loss_D: 18.798813 loss_N: 0.14548697 loss_f: 4.0142236\n",
      "2833 Train Loss 22.958523\n",
      "2834 loss_D: 18.791008 loss_N: 0.14448166 loss_f: 4.0186267\n",
      "2834 Train Loss 22.954117\n",
      "2835 loss_D: 18.792925 loss_N: 0.14445844 loss_f: 4.00386\n",
      "2835 Train Loss 22.941244\n",
      "2836 loss_D: 18.792944 loss_N: 0.14406994 loss_f: 3.993721\n",
      "2836 Train Loss 22.930735\n",
      "2837 loss_D: 18.783121 loss_N: 0.14323756 loss_f: 3.992935\n",
      "2837 Train Loss 22.919294\n",
      "2838 loss_D: 18.7663 loss_N: 0.14277443 loss_f: 4.001492\n",
      "2838 Train Loss 22.910566\n",
      "2839 loss_D: 18.745693 loss_N: 0.1422643 loss_f: 4.0112677\n",
      "2839 Train Loss 22.899223\n",
      "2840 loss_D: 18.711956 loss_N: 0.14217089 loss_f: 4.0286217\n",
      "2840 Train Loss 22.882748\n",
      "2841 loss_D: 18.685057 loss_N: 0.14247519 loss_f: 4.03475\n",
      "2841 Train Loss 22.862282\n",
      "2842 loss_D: 18.640625 loss_N: 0.14298071 loss_f: 4.0638714\n",
      "2842 Train Loss 22.847477\n",
      "2843 loss_D: 18.659966 loss_N: 0.14361952 loss_f: 4.029246\n",
      "2843 Train Loss 22.83283\n",
      "2844 loss_D: 18.649382 loss_N: 0.14411542 loss_f: 4.020128\n",
      "2844 Train Loss 22.813625\n",
      "2845 loss_D: 18.654047 loss_N: 0.143758 loss_f: 3.9978518\n",
      "2845 Train Loss 22.795658\n",
      "2846 loss_D: 18.649843 loss_N: 0.14272702 loss_f: 3.9834476\n",
      "2846 Train Loss 22.776018\n",
      "2847 loss_D: 18.63952 loss_N: 0.1419549 loss_f: 3.9797223\n",
      "2847 Train Loss 22.761198\n",
      "2848 loss_D: 18.636293 loss_N: 0.14115179 loss_f: 3.9722013\n",
      "2848 Train Loss 22.749645\n",
      "2849 loss_D: 18.638287 loss_N: 0.14081523 loss_f: 3.9601967\n",
      "2849 Train Loss 22.7393\n",
      "2850 loss_D: 18.63737 loss_N: 0.14064467 loss_f: 3.953393\n",
      "2850 Train Loss 22.731407\n",
      "2851 loss_D: 18.640009 loss_N: 0.14077264 loss_f: 3.9450576\n",
      "2851 Train Loss 22.725838\n",
      "2852 loss_D: 18.640194 loss_N: 0.14069425 loss_f: 3.9407678\n",
      "2852 Train Loss 22.721655\n",
      "2853 loss_D: 18.637777 loss_N: 0.14060591 loss_f: 3.939421\n",
      "2853 Train Loss 22.717804\n",
      "2854 loss_D: 18.631456 loss_N: 0.14051925 loss_f: 3.9387023\n",
      "2854 Train Loss 22.710678\n",
      "2855 loss_D: 18.620121 loss_N: 0.14006089 loss_f: 3.943767\n",
      "2855 Train Loss 22.703949\n",
      "2856 loss_D: 18.631586 loss_N: 0.13959692 loss_f: 3.9412155\n",
      "2856 Train Loss 22.712399\n",
      "2857 loss_D: 18.624056 loss_N: 0.13987172 loss_f: 3.936039\n",
      "2857 Train Loss 22.699966\n",
      "2858 loss_D: 18.60714 loss_N: 0.14017878 loss_f: 3.9469507\n",
      "2858 Train Loss 22.69427\n",
      "2859 loss_D: 18.596535 loss_N: 0.14036843 loss_f: 3.967439\n",
      "2859 Train Loss 22.704342\n",
      "2860 loss_D: 18.602917 loss_N: 0.14016813 loss_f: 3.945818\n",
      "2860 Train Loss 22.688902\n",
      "2861 loss_D: 18.60054 loss_N: 0.14025089 loss_f: 3.9415011\n",
      "2861 Train Loss 22.682293\n",
      "2862 loss_D: 18.589075 loss_N: 0.14012496 loss_f: 3.944248\n",
      "2862 Train Loss 22.673449\n",
      "2863 loss_D: 18.574335 loss_N: 0.13990737 loss_f: 3.9485226\n",
      "2863 Train Loss 22.662766\n",
      "2864 loss_D: 18.545607 loss_N: 0.13953887 loss_f: 3.9650538\n",
      "2864 Train Loss 22.6502\n",
      "2865 loss_D: 18.534325 loss_N: 0.13937631 loss_f: 3.9671156\n",
      "2865 Train Loss 22.640816\n",
      "2866 loss_D: 18.509344 loss_N: 0.13985273 loss_f: 3.9883409\n",
      "2866 Train Loss 22.637537\n",
      "2867 loss_D: 18.520622 loss_N: 0.1397154 loss_f: 3.9678\n",
      "2867 Train Loss 22.628138\n",
      "2868 loss_D: 18.540548 loss_N: 0.13997702 loss_f: 3.94223\n",
      "2868 Train Loss 22.622755\n",
      "2869 loss_D: 18.547552 loss_N: 0.1404201 loss_f: 3.9300342\n",
      "2869 Train Loss 22.618008\n",
      "2870 loss_D: 18.54873 loss_N: 0.14095971 loss_f: 3.9226933\n",
      "2870 Train Loss 22.612383\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2871 loss_D: 18.54694 loss_N: 0.1418601 loss_f: 3.9123788\n",
      "2871 Train Loss 22.60118\n",
      "2872 loss_D: 18.537172 loss_N: 0.14292781 loss_f: 3.9137232\n",
      "2872 Train Loss 22.593822\n",
      "2873 loss_D: 18.557306 loss_N: 0.14343366 loss_f: 3.8804672\n",
      "2873 Train Loss 22.581207\n",
      "2874 loss_D: 18.55723 loss_N: 0.14287367 loss_f: 3.8685749\n",
      "2874 Train Loss 22.568678\n",
      "2875 loss_D: 18.58349 loss_N: 0.14198153 loss_f: 3.8358538\n",
      "2875 Train Loss 22.561325\n",
      "2876 loss_D: 18.577152 loss_N: 0.14232798 loss_f: 3.834452\n",
      "2876 Train Loss 22.553932\n",
      "2877 loss_D: 18.57013 loss_N: 0.1434683 loss_f: 3.8338835\n",
      "2877 Train Loss 22.547482\n",
      "2878 loss_D: 18.56546 loss_N: 0.14437082 loss_f: 3.8293219\n",
      "2878 Train Loss 22.539154\n",
      "2879 loss_D: 18.560362 loss_N: 0.14458767 loss_f: 3.8259602\n",
      "2879 Train Loss 22.53091\n",
      "2880 loss_D: 18.551815 loss_N: 0.14420354 loss_f: 3.8251894\n",
      "2880 Train Loss 22.521208\n",
      "2881 loss_D: 18.539816 loss_N: 0.14365822 loss_f: 3.826596\n",
      "2881 Train Loss 22.510069\n",
      "2882 loss_D: 18.51878 loss_N: 0.14251935 loss_f: 3.8253367\n",
      "2882 Train Loss 22.486635\n",
      "2883 loss_D: 18.460258 loss_N: 0.14136915 loss_f: 3.8551445\n",
      "2883 Train Loss 22.456772\n",
      "2884 loss_D: 18.440722 loss_N: 0.14096546 loss_f: 3.8447382\n",
      "2884 Train Loss 22.426426\n",
      "2885 loss_D: 18.410736 loss_N: 0.14096606 loss_f: 3.8568609\n",
      "2885 Train Loss 22.408564\n",
      "2886 loss_D: 18.415443 loss_N: 0.14154075 loss_f: 3.8434467\n",
      "2886 Train Loss 22.40043\n",
      "2887 loss_D: 18.409992 loss_N: 0.14175315 loss_f: 3.841154\n",
      "2887 Train Loss 22.392899\n",
      "2888 loss_D: 18.40519 loss_N: 0.14197631 loss_f: 3.8361027\n",
      "2888 Train Loss 22.383268\n",
      "2889 loss_D: 18.398396 loss_N: 0.14195387 loss_f: 3.836796\n",
      "2889 Train Loss 22.377146\n",
      "2890 loss_D: 18.382078 loss_N: 0.14175849 loss_f: 3.8754077\n",
      "2890 Train Loss 22.399244\n",
      "2891 loss_D: 18.39365 loss_N: 0.14184602 loss_f: 3.8385015\n",
      "2891 Train Loss 22.373997\n",
      "2892 loss_D: 18.392656 loss_N: 0.14166605 loss_f: 3.8296163\n",
      "2892 Train Loss 22.36394\n",
      "2893 loss_D: 18.379698 loss_N: 0.14160192 loss_f: 3.8326902\n",
      "2893 Train Loss 22.353989\n",
      "2894 loss_D: 18.353304 loss_N: 0.14149052 loss_f: 3.855705\n",
      "2894 Train Loss 22.3505\n",
      "2895 loss_D: 18.347721 loss_N: 0.14115727 loss_f: 3.8481085\n",
      "2895 Train Loss 22.336987\n",
      "2896 loss_D: 18.332384 loss_N: 0.14064477 loss_f: 3.8583062\n",
      "2896 Train Loss 22.331335\n",
      "2897 loss_D: 18.319113 loss_N: 0.14022984 loss_f: 3.8635733\n",
      "2897 Train Loss 22.322916\n",
      "2898 loss_D: 18.284199 loss_N: 0.13954128 loss_f: 3.887975\n",
      "2898 Train Loss 22.311716\n",
      "2899 loss_D: 18.26652 loss_N: 0.13887247 loss_f: 3.9165125\n",
      "2899 Train Loss 22.321903\n",
      "2900 loss_D: 18.277735 loss_N: 0.1392024 loss_f: 3.8905623\n",
      "2900 Train Loss 22.307499\n",
      "2901 loss_D: 18.251892 loss_N: 0.1389804 loss_f: 3.9062269\n",
      "2901 Train Loss 22.2971\n",
      "2902 loss_D: 18.239485 loss_N: 0.13929033 loss_f: 3.912091\n",
      "2902 Train Loss 22.290865\n",
      "2903 loss_D: 18.231209 loss_N: 0.13979544 loss_f: 3.9098597\n",
      "2903 Train Loss 22.280865\n",
      "2904 loss_D: 18.23185 loss_N: 0.13996445 loss_f: 3.9016616\n",
      "2904 Train Loss 22.273476\n",
      "2905 loss_D: 18.21639 loss_N: 0.1402484 loss_f: 3.9079406\n",
      "2905 Train Loss 22.26458\n",
      "2906 loss_D: 18.20003 loss_N: 0.14004868 loss_f: 3.9102414\n",
      "2906 Train Loss 22.25032\n",
      "2907 loss_D: 18.172136 loss_N: 0.13989754 loss_f: 3.9285815\n",
      "2907 Train Loss 22.240616\n",
      "2908 loss_D: 18.135437 loss_N: 0.13917173 loss_f: 3.962843\n",
      "2908 Train Loss 22.237452\n",
      "2909 loss_D: 18.12495 loss_N: 0.14038388 loss_f: 3.9508038\n",
      "2909 Train Loss 22.216139\n",
      "2910 loss_D: 18.112305 loss_N: 0.14099643 loss_f: 3.9502265\n",
      "2910 Train Loss 22.203527\n",
      "2911 loss_D: 18.113588 loss_N: 0.14244136 loss_f: 3.9305086\n",
      "2911 Train Loss 22.186539\n",
      "2912 loss_D: 18.076778 loss_N: 0.14277482 loss_f: 3.9484608\n",
      "2912 Train Loss 22.168015\n",
      "2913 loss_D: 18.06116 loss_N: 0.14278089 loss_f: 3.945104\n",
      "2913 Train Loss 22.149044\n",
      "2914 loss_D: 18.04622 loss_N: 0.14256911 loss_f: 3.9470124\n",
      "2914 Train Loss 22.135801\n",
      "2915 loss_D: 18.05061 loss_N: 0.14198886 loss_f: 3.931631\n",
      "2915 Train Loss 22.12423\n",
      "2916 loss_D: 18.047926 loss_N: 0.142067 loss_f: 3.923221\n",
      "2916 Train Loss 22.113214\n",
      "2917 loss_D: 18.05411 loss_N: 0.14194566 loss_f: 3.9091523\n",
      "2917 Train Loss 22.105207\n",
      "2918 loss_D: 18.045435 loss_N: 0.14269924 loss_f: 3.9109125\n",
      "2918 Train Loss 22.099045\n",
      "2919 loss_D: 18.041649 loss_N: 0.14336176 loss_f: 3.908089\n",
      "2919 Train Loss 22.0931\n",
      "2920 loss_D: 18.031076 loss_N: 0.1441623 loss_f: 3.9122741\n",
      "2920 Train Loss 22.087513\n",
      "2921 loss_D: 18.01819 loss_N: 0.14449656 loss_f: 3.9177096\n",
      "2921 Train Loss 22.080397\n",
      "2922 loss_D: 18.01582 loss_N: 0.14443164 loss_f: 3.9136484\n",
      "2922 Train Loss 22.0739\n",
      "2923 loss_D: 18.011868 loss_N: 0.14386328 loss_f: 3.911242\n",
      "2923 Train Loss 22.066973\n",
      "2924 loss_D: 18.011944 loss_N: 0.14344248 loss_f: 3.9045382\n",
      "2924 Train Loss 22.059925\n",
      "2925 loss_D: 18.005537 loss_N: 0.14329648 loss_f: 3.9039092\n",
      "2925 Train Loss 22.052744\n",
      "2926 loss_D: 17.99873 loss_N: 0.14307106 loss_f: 3.9035566\n",
      "2926 Train Loss 22.045357\n",
      "2927 loss_D: 17.978186 loss_N: 0.1429802 loss_f: 3.910663\n",
      "2927 Train Loss 22.03183\n",
      "2928 loss_D: 17.96289 loss_N: 0.14287917 loss_f: 3.917589\n",
      "2928 Train Loss 22.02336\n",
      "2929 loss_D: 17.933443 loss_N: 0.14319216 loss_f: 3.9229367\n",
      "2929 Train Loss 21.999573\n",
      "2930 loss_D: 17.922655 loss_N: 0.14385143 loss_f: 3.914592\n",
      "2930 Train Loss 21.9811\n",
      "2931 loss_D: 17.907877 loss_N: 0.14444023 loss_f: 3.913335\n",
      "2931 Train Loss 21.965652\n",
      "2932 loss_D: 17.904802 loss_N: 0.14490186 loss_f: 3.903062\n",
      "2932 Train Loss 21.952766\n",
      "2933 loss_D: 17.881159 loss_N: 0.14592306 loss_f: 3.9113693\n",
      "2933 Train Loss 21.938452\n",
      "2934 loss_D: 17.869211 loss_N: 0.14599939 loss_f: 3.9123492\n",
      "2934 Train Loss 21.92756\n",
      "2935 loss_D: 17.858906 loss_N: 0.14630884 loss_f: 3.9141538\n",
      "2935 Train Loss 21.919369\n",
      "2936 loss_D: 17.853638 loss_N: 0.14614108 loss_f: 3.9122584\n",
      "2936 Train Loss 21.912037\n",
      "2937 loss_D: 17.844404 loss_N: 0.14621747 loss_f: 3.9123514\n",
      "2937 Train Loss 21.902973\n",
      "2938 loss_D: 17.8394 loss_N: 0.14579591 loss_f: 3.9087932\n",
      "2938 Train Loss 21.893988\n",
      "2939 loss_D: 17.826921 loss_N: 0.1456398 loss_f: 3.911819\n",
      "2939 Train Loss 21.88438\n",
      "2940 loss_D: 17.835749 loss_N: 0.14510219 loss_f: 3.894666\n",
      "2940 Train Loss 21.875517\n",
      "2941 loss_D: 17.81375 loss_N: 0.14490424 loss_f: 3.9075406\n",
      "2941 Train Loss 21.866196\n",
      "2942 loss_D: 17.815016 loss_N: 0.14423992 loss_f: 3.8969953\n",
      "2942 Train Loss 21.85625\n",
      "2943 loss_D: 17.798246 loss_N: 0.14428452 loss_f: 3.90221\n",
      "2943 Train Loss 21.844742\n",
      "2944 loss_D: 17.780762 loss_N: 0.14449137 loss_f: 3.9114506\n",
      "2944 Train Loss 21.836704\n",
      "2945 loss_D: 17.772093 loss_N: 0.14439943 loss_f: 3.9143424\n",
      "2945 Train Loss 21.830835\n",
      "2946 loss_D: 17.768816 loss_N: 0.14429495 loss_f: 3.9091518\n",
      "2946 Train Loss 21.822262\n",
      "2947 loss_D: 17.76065 loss_N: 0.14387338 loss_f: 3.9110413\n",
      "2947 Train Loss 21.815565\n",
      "2948 loss_D: 17.765085 loss_N: 0.1438289 loss_f: 3.9020967\n",
      "2948 Train Loss 21.811012\n",
      "2949 loss_D: 17.76338 loss_N: 0.14401048 loss_f: 3.894247\n",
      "2949 Train Loss 21.801638\n",
      "2950 loss_D: 17.755157 loss_N: 0.14428203 loss_f: 3.889387\n",
      "2950 Train Loss 21.788826\n",
      "2951 loss_D: 17.734005 loss_N: 0.14572595 loss_f: 3.89769\n",
      "2951 Train Loss 21.77742\n",
      "2952 loss_D: 17.69569 loss_N: 0.14707658 loss_f: 3.9278598\n",
      "2952 Train Loss 21.770626\n",
      "2953 loss_D: 17.700481 loss_N: 0.14661981 loss_f: 3.9083738\n",
      "2953 Train Loss 21.755474\n",
      "2954 loss_D: 17.686186 loss_N: 0.1469023 loss_f: 3.9126515\n",
      "2954 Train Loss 21.745739\n",
      "2955 loss_D: 17.669804 loss_N: 0.14741282 loss_f: 3.9200397\n",
      "2955 Train Loss 21.737257\n",
      "2956 loss_D: 17.64326 loss_N: 0.14788471 loss_f: 3.9406304\n",
      "2956 Train Loss 21.731773\n",
      "2957 loss_D: 17.63536 loss_N: 0.147792 loss_f: 3.9428694\n",
      "2957 Train Loss 21.726023\n",
      "2958 loss_D: 17.643642 loss_N: 0.14766817 loss_f: 3.9315238\n",
      "2958 Train Loss 21.722836\n",
      "2959 loss_D: 17.636057 loss_N: 0.1480353 loss_f: 3.936233\n",
      "2959 Train Loss 21.720325\n",
      "2960 loss_D: 17.635502 loss_N: 0.14804567 loss_f: 3.9347303\n",
      "2960 Train Loss 21.718279\n",
      "2961 loss_D: 17.622599 loss_N: 0.14849585 loss_f: 3.9435647\n",
      "2961 Train Loss 21.71466\n",
      "2962 loss_D: 17.613672 loss_N: 0.14928289 loss_f: 3.9485116\n",
      "2962 Train Loss 21.711466\n",
      "2963 loss_D: 17.602577 loss_N: 0.15017813 loss_f: 3.955675\n",
      "2963 Train Loss 21.708431\n",
      "2964 loss_D: 17.600544 loss_N: 0.15104893 loss_f: 3.9528592\n",
      "2964 Train Loss 21.704453\n",
      "2965 loss_D: 17.601484 loss_N: 0.15255286 loss_f: 3.9433093\n",
      "2965 Train Loss 21.697348\n",
      "2966 loss_D: 17.605679 loss_N: 0.15416104 loss_f: 3.933289\n",
      "2966 Train Loss 21.693129\n",
      "2967 loss_D: 17.6069 loss_N: 0.15438716 loss_f: 3.9276068\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2967 Train Loss 21.688892\n",
      "2968 loss_D: 17.607086 loss_N: 0.15475094 loss_f: 3.9236717\n",
      "2968 Train Loss 21.685509\n",
      "2969 loss_D: 17.601091 loss_N: 0.15509516 loss_f: 3.9251513\n",
      "2969 Train Loss 21.68134\n",
      "2970 loss_D: 17.59359 loss_N: 0.15618286 loss_f: 3.9259999\n",
      "2970 Train Loss 21.675774\n",
      "2971 loss_D: 17.579584 loss_N: 0.15660252 loss_f: 3.930046\n",
      "2971 Train Loss 21.666233\n",
      "2972 loss_D: 17.562916 loss_N: 0.15678547 loss_f: 3.9360414\n",
      "2972 Train Loss 21.655743\n",
      "2973 loss_D: 17.550282 loss_N: 0.15691987 loss_f: 3.9379818\n",
      "2973 Train Loss 21.645184\n",
      "2974 loss_D: 17.536268 loss_N: 0.1568547 loss_f: 3.9432585\n",
      "2974 Train Loss 21.636381\n",
      "2975 loss_D: 17.518343 loss_N: 0.1572606 loss_f: 3.9500442\n",
      "2975 Train Loss 21.625648\n",
      "2976 loss_D: 17.51784 loss_N: 0.1575858 loss_f: 3.9375038\n",
      "2976 Train Loss 21.612928\n",
      "2977 loss_D: 17.49753 loss_N: 0.15868217 loss_f: 3.938325\n",
      "2977 Train Loss 21.594536\n",
      "2978 loss_D: 17.477879 loss_N: 0.1601686 loss_f: 3.9372628\n",
      "2978 Train Loss 21.57531\n",
      "2979 loss_D: 17.425112 loss_N: 0.16224335 loss_f: 3.9595892\n",
      "2979 Train Loss 21.546944\n",
      "2980 loss_D: 17.376724 loss_N: 0.16459103 loss_f: 3.983097\n",
      "2980 Train Loss 21.524412\n",
      "2981 loss_D: 17.360376 loss_N: 0.16662323 loss_f: 3.987538\n",
      "2981 Train Loss 21.514538\n",
      "2982 loss_D: 17.326853 loss_N: 0.16824444 loss_f: 4.0140805\n",
      "2982 Train Loss 21.509178\n",
      "2983 loss_D: 17.348 loss_N: 0.1663387 loss_f: 3.9892287\n",
      "2983 Train Loss 21.503567\n",
      "2984 loss_D: 17.353752 loss_N: 0.1666265 loss_f: 3.9782386\n",
      "2984 Train Loss 21.498617\n",
      "2985 loss_D: 17.346596 loss_N: 0.16745415 loss_f: 3.9772823\n",
      "2985 Train Loss 21.491331\n",
      "2986 loss_D: 17.33672 loss_N: 0.16768555 loss_f: 3.9826748\n",
      "2986 Train Loss 21.487082\n",
      "2987 loss_D: 17.327572 loss_N: 0.16738926 loss_f: 3.9890866\n",
      "2987 Train Loss 21.484047\n",
      "2988 loss_D: 17.321835 loss_N: 0.16631691 loss_f: 3.9925866\n",
      "2988 Train Loss 21.480738\n",
      "2989 loss_D: 17.318459 loss_N: 0.16515061 loss_f: 3.9932106\n",
      "2989 Train Loss 21.476818\n",
      "2990 loss_D: 17.316471 loss_N: 0.16443264 loss_f: 3.9920733\n",
      "2990 Train Loss 21.472977\n",
      "2991 loss_D: 17.314783 loss_N: 0.16510014 loss_f: 3.9898007\n",
      "2991 Train Loss 21.469685\n",
      "2992 loss_D: 17.308708 loss_N: 0.16662228 loss_f: 3.9913006\n",
      "2992 Train Loss 21.46663\n",
      "2993 loss_D: 17.304628 loss_N: 0.16839582 loss_f: 3.989469\n",
      "2993 Train Loss 21.462494\n",
      "2994 loss_D: 17.301783 loss_N: 0.17141336 loss_f: 3.9855585\n",
      "2994 Train Loss 21.458755\n",
      "2995 loss_D: 17.300528 loss_N: 0.17185003 loss_f: 3.9816139\n",
      "2995 Train Loss 21.45399\n",
      "2996 loss_D: 17.29694 loss_N: 0.17273434 loss_f: 3.9786808\n",
      "2996 Train Loss 21.448357\n",
      "2997 loss_D: 17.290478 loss_N: 0.17415534 loss_f: 3.97966\n",
      "2997 Train Loss 21.444294\n",
      "2998 loss_D: 17.310787 loss_N: 0.17830296 loss_f: 4.057941\n",
      "2998 Train Loss 21.547031\n",
      "2999 loss_D: 17.293917 loss_N: 0.17489426 loss_f: 3.9700062\n",
      "2999 Train Loss 21.438818\n",
      "3000 loss_D: 17.295252 loss_N: 0.17482717 loss_f: 3.9610338\n",
      "3000 Train Loss 21.431114\n",
      "3001 loss_D: 17.302769 loss_N: 0.17586814 loss_f: 3.9395945\n",
      "3001 Train Loss 21.418232\n",
      "3002 loss_D: 17.307981 loss_N: 0.17536062 loss_f: 3.9305172\n",
      "3002 Train Loss 21.413858\n",
      "3003 loss_D: 17.314787 loss_N: 0.17585976 loss_f: 3.9100733\n",
      "3003 Train Loss 21.400719\n",
      "3004 loss_D: 17.32957 loss_N: 0.17347297 loss_f: 3.8863266\n",
      "3004 Train Loss 21.38937\n",
      "3005 loss_D: 17.33448 loss_N: 0.1723177 loss_f: 3.868374\n",
      "3005 Train Loss 21.375172\n",
      "3006 loss_D: 17.343485 loss_N: 0.17843127 loss_f: 3.909889\n",
      "3006 Train Loss 21.431805\n",
      "3007 loss_D: 17.336231 loss_N: 0.17367986 loss_f: 3.8600078\n",
      "3007 Train Loss 21.369919\n",
      "3008 loss_D: 17.328815 loss_N: 0.17346086 loss_f: 3.851354\n",
      "3008 Train Loss 21.35363\n",
      "3009 loss_D: 17.32715 loss_N: 0.17340723 loss_f: 3.8456545\n",
      "3009 Train Loss 21.34621\n",
      "3010 loss_D: 17.322561 loss_N: 0.17448778 loss_f: 3.8395355\n",
      "3010 Train Loss 21.336584\n",
      "3011 loss_D: 17.326153 loss_N: 0.17405729 loss_f: 3.8283124\n",
      "3011 Train Loss 21.328522\n",
      "3012 loss_D: 17.315763 loss_N: 0.17445351 loss_f: 3.8308792\n",
      "3012 Train Loss 21.321096\n",
      "3013 loss_D: 17.320272 loss_N: 0.1736347 loss_f: 3.8201709\n",
      "3013 Train Loss 21.31408\n",
      "3014 loss_D: 17.319492 loss_N: 0.17421433 loss_f: 3.8117077\n",
      "3014 Train Loss 21.305414\n",
      "3015 loss_D: 17.345324 loss_N: 0.16889757 loss_f: 3.8076246\n",
      "3015 Train Loss 21.321846\n",
      "3016 loss_D: 17.327072 loss_N: 0.17242932 loss_f: 3.801121\n",
      "3016 Train Loss 21.300623\n",
      "3017 loss_D: 17.325483 loss_N: 0.17145534 loss_f: 3.791451\n",
      "3017 Train Loss 21.28839\n",
      "3018 loss_D: 17.323746 loss_N: 0.17405343 loss_f: 3.777421\n",
      "3018 Train Loss 21.275219\n",
      "3019 loss_D: 17.319975 loss_N: 0.1755312 loss_f: 3.7716444\n",
      "3019 Train Loss 21.26715\n",
      "3020 loss_D: 17.318373 loss_N: 0.1760913 loss_f: 3.7659159\n",
      "3020 Train Loss 21.26038\n",
      "3021 loss_D: 17.311756 loss_N: 0.17731439 loss_f: 3.7673287\n",
      "3021 Train Loss 21.2564\n",
      "3022 loss_D: 17.309923 loss_N: 0.17715968 loss_f: 3.7667565\n",
      "3022 Train Loss 21.25384\n",
      "3023 loss_D: 17.30653 loss_N: 0.17718112 loss_f: 3.7683952\n",
      "3023 Train Loss 21.252106\n",
      "3024 loss_D: 17.301794 loss_N: 0.17691843 loss_f: 3.7709136\n",
      "3024 Train Loss 21.249626\n",
      "3025 loss_D: 17.29268 loss_N: 0.17530939 loss_f: 3.7843351\n",
      "3025 Train Loss 21.252327\n",
      "3026 loss_D: 17.298256 loss_N: 0.17629938 loss_f: 3.7734632\n",
      "3026 Train Loss 21.24802\n",
      "3027 loss_D: 17.219408 loss_N: 0.18399118 loss_f: 4.020126\n",
      "3027 Train Loss 21.423525\n",
      "3028 loss_D: 17.288929 loss_N: 0.17712377 loss_f: 3.7791998\n",
      "3028 Train Loss 21.245253\n",
      "3029 loss_D: 17.278614 loss_N: 0.17679305 loss_f: 3.7852807\n",
      "3029 Train Loss 21.240686\n",
      "3030 loss_D: 17.252966 loss_N: 0.17539492 loss_f: 3.8052855\n",
      "3030 Train Loss 21.233646\n",
      "3031 loss_D: 17.222412 loss_N: 0.17513658 loss_f: 3.827821\n",
      "3031 Train Loss 21.22537\n",
      "3032 loss_D: 17.185307 loss_N: 0.17423925 loss_f: 3.8546705\n",
      "3032 Train Loss 21.214218\n",
      "3033 loss_D: 17.13557 loss_N: 0.17513332 loss_f: 3.8951707\n",
      "3033 Train Loss 21.205873\n",
      "3034 loss_D: 17.144957 loss_N: 0.17459247 loss_f: 3.8766007\n",
      "3034 Train Loss 21.19615\n",
      "3035 loss_D: 17.1371 loss_N: 0.17573075 loss_f: 3.8764937\n",
      "3035 Train Loss 21.189325\n",
      "3036 loss_D: 17.132446 loss_N: 0.17697917 loss_f: 3.8719225\n",
      "3036 Train Loss 21.181347\n",
      "3037 loss_D: 17.11368 loss_N: 0.1787357 loss_f: 3.8816922\n",
      "3037 Train Loss 21.174109\n",
      "3038 loss_D: 17.106586 loss_N: 0.17935474 loss_f: 3.8813398\n",
      "3038 Train Loss 21.167282\n",
      "3039 loss_D: 17.091171 loss_N: 0.1814195 loss_f: 3.8868897\n",
      "3039 Train Loss 21.159481\n",
      "3040 loss_D: 17.08248 loss_N: 0.18492526 loss_f: 3.8802698\n",
      "3040 Train Loss 21.147675\n",
      "3041 loss_D: 17.045815 loss_N: 0.19050416 loss_f: 3.9059963\n",
      "3041 Train Loss 21.142315\n",
      "3042 loss_D: 17.039215 loss_N: 0.19394222 loss_f: 3.8974545\n",
      "3042 Train Loss 21.130613\n",
      "3043 loss_D: 17.06418 loss_N: 0.1864057 loss_f: 3.8658233\n",
      "3043 Train Loss 21.11641\n",
      "3044 loss_D: 17.06197 loss_N: 0.18622535 loss_f: 3.8595812\n",
      "3044 Train Loss 21.107777\n",
      "3045 loss_D: 17.055296 loss_N: 0.18798727 loss_f: 3.8496108\n",
      "3045 Train Loss 21.092894\n",
      "3046 loss_D: 17.047766 loss_N: 0.18681112 loss_f: 3.8439522\n",
      "3046 Train Loss 21.07853\n",
      "3047 loss_D: 17.051434 loss_N: 0.18745773 loss_f: 3.8291514\n",
      "3047 Train Loss 21.068043\n",
      "3048 loss_D: 17.044624 loss_N: 0.18768285 loss_f: 3.8249373\n",
      "3048 Train Loss 21.057245\n",
      "3049 loss_D: 17.03716 loss_N: 0.18675137 loss_f: 3.8257828\n",
      "3049 Train Loss 21.049694\n",
      "3050 loss_D: 17.01399 loss_N: 0.1862592 loss_f: 3.859494\n",
      "3050 Train Loss 21.059742\n",
      "3051 loss_D: 17.028496 loss_N: 0.18655844 loss_f: 3.8293123\n",
      "3051 Train Loss 21.044365\n",
      "3052 loss_D: 17.020285 loss_N: 0.18779309 loss_f: 3.8251061\n",
      "3052 Train Loss 21.033184\n",
      "3053 loss_D: 17.003681 loss_N: 0.1881624 loss_f: 3.8271022\n",
      "3053 Train Loss 21.018946\n",
      "3054 loss_D: 17.00356 loss_N: 0.1886702 loss_f: 3.8180304\n",
      "3054 Train Loss 21.010262\n",
      "3055 loss_D: 17.000326 loss_N: 0.18788186 loss_f: 3.8146594\n",
      "3055 Train Loss 21.002867\n",
      "3056 loss_D: 17.009434 loss_N: 0.18675764 loss_f: 3.7995765\n",
      "3056 Train Loss 20.995768\n",
      "3057 loss_D: 17.013569 loss_N: 0.18581897 loss_f: 3.7904909\n",
      "3057 Train Loss 20.98988\n",
      "3058 loss_D: 17.01635 loss_N: 0.18526183 loss_f: 3.780932\n",
      "3058 Train Loss 20.982544\n",
      "3059 loss_D: 17.010149 loss_N: 0.18404078 loss_f: 3.7803867\n",
      "3059 Train Loss 20.974575\n",
      "3060 loss_D: 16.999517 loss_N: 0.18400234 loss_f: 3.7864678\n",
      "3060 Train Loss 20.969988\n",
      "3061 loss_D: 16.986256 loss_N: 0.1838409 loss_f: 3.7954335\n",
      "3061 Train Loss 20.96553\n",
      "3062 loss_D: 16.97303 loss_N: 0.18346755 loss_f: 3.8043008\n",
      "3062 Train Loss 20.960798\n",
      "3063 loss_D: 16.94737 loss_N: 0.18429016 loss_f: 3.823446\n",
      "3063 Train Loss 20.955107\n",
      "3064 loss_D: 16.925797 loss_N: 0.18355879 loss_f: 3.840298\n",
      "3064 Train Loss 20.949654\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3065 loss_D: 16.917877 loss_N: 0.1855885 loss_f: 3.840603\n",
      "3065 Train Loss 20.944069\n",
      "3066 loss_D: 16.880241 loss_N: 0.18941283 loss_f: 3.877433\n",
      "3066 Train Loss 20.947088\n",
      "3067 loss_D: 16.901684 loss_N: 0.18719904 loss_f: 3.8516757\n",
      "3067 Train Loss 20.940557\n",
      "3068 loss_D: 16.91873 loss_N: 0.18747096 loss_f: 3.8282561\n",
      "3068 Train Loss 20.934458\n",
      "3069 loss_D: 16.915447 loss_N: 0.18727402 loss_f: 3.824022\n",
      "3069 Train Loss 20.926744\n",
      "3070 loss_D: 16.919155 loss_N: 0.1845003 loss_f: 3.8093834\n",
      "3070 Train Loss 20.913038\n",
      "3071 loss_D: 16.914532 loss_N: 0.18317656 loss_f: 3.8072312\n",
      "3071 Train Loss 20.90494\n",
      "3072 loss_D: 16.911228 loss_N: 0.18161018 loss_f: 3.80752\n",
      "3072 Train Loss 20.900358\n",
      "3073 loss_D: 16.905945 loss_N: 0.1816141 loss_f: 3.808814\n",
      "3073 Train Loss 20.896374\n",
      "3074 loss_D: 16.897896 loss_N: 0.18173704 loss_f: 3.8090258\n",
      "3074 Train Loss 20.88866\n",
      "3075 loss_D: 16.887058 loss_N: 0.18189885 loss_f: 3.8054006\n",
      "3075 Train Loss 20.874357\n",
      "3076 loss_D: 16.87687 loss_N: 0.18133825 loss_f: 3.802903\n",
      "3076 Train Loss 20.861109\n",
      "3077 loss_D: 16.895916 loss_N: 0.18822406 loss_f: 3.9204109\n",
      "3077 Train Loss 21.00455\n",
      "3078 loss_D: 16.878881 loss_N: 0.18212341 loss_f: 3.7972531\n",
      "3078 Train Loss 20.858257\n",
      "3079 loss_D: 16.872143 loss_N: 0.18084583 loss_f: 3.7933757\n",
      "3079 Train Loss 20.846363\n",
      "3080 loss_D: 16.867786 loss_N: 0.17762733 loss_f: 3.7873297\n",
      "3080 Train Loss 20.832745\n",
      "3081 loss_D: 16.86036 loss_N: 0.17423782 loss_f: 3.7830358\n",
      "3081 Train Loss 20.817633\n",
      "3082 loss_D: 16.85138 loss_N: 0.1702147 loss_f: 3.7806664\n",
      "3082 Train Loss 20.802261\n",
      "3083 loss_D: 16.834072 loss_N: 0.16843408 loss_f: 3.8222365\n",
      "3083 Train Loss 20.824743\n",
      "3084 loss_D: 16.845549 loss_N: 0.16963845 loss_f: 3.78106\n",
      "3084 Train Loss 20.796246\n",
      "3085 loss_D: 16.83378 loss_N: 0.16698103 loss_f: 3.7811773\n",
      "3085 Train Loss 20.781939\n",
      "3086 loss_D: 16.814116 loss_N: 0.16546479 loss_f: 3.7944367\n",
      "3086 Train Loss 20.774017\n",
      "3087 loss_D: 16.805813 loss_N: 0.16448416 loss_f: 3.7914789\n",
      "3087 Train Loss 20.761776\n",
      "3088 loss_D: 16.789484 loss_N: 0.16424535 loss_f: 3.796283\n",
      "3088 Train Loss 20.750013\n",
      "3089 loss_D: 16.757496 loss_N: 0.1640103 loss_f: 3.8122866\n",
      "3089 Train Loss 20.733793\n",
      "3090 loss_D: 16.734789 loss_N: 0.16192992 loss_f: 3.8257518\n",
      "3090 Train Loss 20.722471\n",
      "3091 loss_D: 16.71289 loss_N: 0.15973157 loss_f: 3.8420167\n",
      "3091 Train Loss 20.714638\n",
      "3092 loss_D: 16.708927 loss_N: 0.15854722 loss_f: 3.842068\n",
      "3092 Train Loss 20.709541\n",
      "3093 loss_D: 16.666162 loss_N: 0.15845935 loss_f: 3.8848803\n",
      "3093 Train Loss 20.709501\n",
      "3094 loss_D: 16.687326 loss_N: 0.1584961 loss_f: 3.8606808\n",
      "3094 Train Loss 20.706505\n",
      "3095 loss_D: 16.682718 loss_N: 0.15630493 loss_f: 3.8607714\n",
      "3095 Train Loss 20.699795\n",
      "3096 loss_D: 16.674088 loss_N: 0.15663917 loss_f: 3.8616598\n",
      "3096 Train Loss 20.692387\n",
      "3097 loss_D: 16.645393 loss_N: 0.1561436 loss_f: 3.878491\n",
      "3097 Train Loss 20.680027\n",
      "3098 loss_D: 16.616184 loss_N: 0.15545067 loss_f: 3.8997898\n",
      "3098 Train Loss 20.671425\n",
      "3099 loss_D: 16.579008 loss_N: 0.15483077 loss_f: 3.9332438\n",
      "3099 Train Loss 20.667084\n",
      "3100 loss_D: 16.54648 loss_N: 0.15430565 loss_f: 3.955266\n",
      "3100 Train Loss 20.656052\n",
      "3101 loss_D: 16.563644 loss_N: 0.15458202 loss_f: 3.9280002\n",
      "3101 Train Loss 20.646227\n",
      "3102 loss_D: 16.563324 loss_N: 0.1548021 loss_f: 3.921767\n",
      "3102 Train Loss 20.639893\n",
      "3103 loss_D: 16.564484 loss_N: 0.15465668 loss_f: 3.9107769\n",
      "3103 Train Loss 20.629917\n",
      "3104 loss_D: 16.525055 loss_N: 0.15447487 loss_f: 3.937518\n",
      "3104 Train Loss 20.617046\n",
      "3105 loss_D: 16.518389 loss_N: 0.15338032 loss_f: 3.9304664\n",
      "3105 Train Loss 20.602234\n",
      "3106 loss_D: 16.49094 loss_N: 0.15231533 loss_f: 3.9447248\n",
      "3106 Train Loss 20.58798\n",
      "3107 loss_D: 16.478573 loss_N: 0.15132998 loss_f: 3.942457\n",
      "3107 Train Loss 20.57236\n",
      "3108 loss_D: 16.457087 loss_N: 0.15097842 loss_f: 3.9539952\n",
      "3108 Train Loss 20.56206\n",
      "3109 loss_D: 16.463482 loss_N: 0.15105654 loss_f: 3.9406507\n",
      "3109 Train Loss 20.55519\n",
      "3110 loss_D: 16.458954 loss_N: 0.15150051 loss_f: 3.9398584\n",
      "3110 Train Loss 20.550312\n",
      "3111 loss_D: 16.463562 loss_N: 0.15171754 loss_f: 3.9310222\n",
      "3111 Train Loss 20.546303\n",
      "3112 loss_D: 16.463142 loss_N: 0.15240909 loss_f: 3.9265904\n",
      "3112 Train Loss 20.542141\n",
      "3113 loss_D: 16.468431 loss_N: 0.15244702 loss_f: 3.916969\n",
      "3113 Train Loss 20.537848\n",
      "3114 loss_D: 16.477009 loss_N: 0.15232414 loss_f: 3.9046733\n",
      "3114 Train Loss 20.534006\n",
      "3115 loss_D: 16.48276 loss_N: 0.15198971 loss_f: 3.8960233\n",
      "3115 Train Loss 20.530771\n",
      "3116 loss_D: 16.487997 loss_N: 0.15167104 loss_f: 3.8885796\n",
      "3116 Train Loss 20.528248\n",
      "3117 loss_D: 16.492432 loss_N: 0.15137091 loss_f: 3.8806734\n",
      "3117 Train Loss 20.524475\n",
      "3118 loss_D: 16.496002 loss_N: 0.15102576 loss_f: 3.872095\n",
      "3118 Train Loss 20.519123\n",
      "3119 loss_D: 16.495682 loss_N: 0.15071179 loss_f: 3.8651836\n",
      "3119 Train Loss 20.511576\n",
      "3120 loss_D: 16.493217 loss_N: 0.15025674 loss_f: 3.8590539\n",
      "3120 Train Loss 20.50253\n",
      "3121 loss_D: 16.486588 loss_N: 0.14999902 loss_f: 3.8572967\n",
      "3121 Train Loss 20.493883\n",
      "3122 loss_D: 16.471596 loss_N: 0.1500499 loss_f: 3.865046\n",
      "3122 Train Loss 20.48669\n",
      "3123 loss_D: 16.475468 loss_N: 0.14914194 loss_f: 3.8592062\n",
      "3123 Train Loss 20.483814\n",
      "3124 loss_D: 16.472122 loss_N: 0.1500101 loss_f: 3.8498757\n",
      "3124 Train Loss 20.472008\n",
      "3125 loss_D: 16.468882 loss_N: 0.15028964 loss_f: 3.8468273\n",
      "3125 Train Loss 20.465998\n",
      "3126 loss_D: 16.465391 loss_N: 0.15066552 loss_f: 3.844721\n",
      "3126 Train Loss 20.460777\n",
      "3127 loss_D: 16.451435 loss_N: 0.1505594 loss_f: 3.858307\n",
      "3127 Train Loss 20.4603\n",
      "3128 loss_D: 16.456903 loss_N: 0.15048009 loss_f: 3.8468585\n",
      "3128 Train Loss 20.454243\n",
      "3129 loss_D: 16.451601 loss_N: 0.15071917 loss_f: 3.8486795\n",
      "3129 Train Loss 20.451\n",
      "3130 loss_D: 16.440865 loss_N: 0.15053667 loss_f: 3.8556414\n",
      "3130 Train Loss 20.447044\n",
      "3131 loss_D: 16.429127 loss_N: 0.15076245 loss_f: 3.863046\n",
      "3131 Train Loss 20.442936\n",
      "3132 loss_D: 16.415678 loss_N: 0.1504133 loss_f: 3.8735092\n",
      "3132 Train Loss 20.4396\n",
      "3133 loss_D: 16.391354 loss_N: 0.15348142 loss_f: 3.900178\n",
      "3133 Train Loss 20.445011\n",
      "3134 loss_D: 16.406897 loss_N: 0.15140405 loss_f: 3.8790476\n",
      "3134 Train Loss 20.437347\n",
      "3135 loss_D: 16.4097 loss_N: 0.15133412 loss_f: 3.8739588\n",
      "3135 Train Loss 20.434994\n",
      "3136 loss_D: 16.41656 loss_N: 0.15116467 loss_f: 3.8633742\n",
      "3136 Train Loss 20.431099\n",
      "3137 loss_D: 16.419668 loss_N: 0.1510795 loss_f: 3.858678\n",
      "3137 Train Loss 20.429426\n",
      "3138 loss_D: 16.423468 loss_N: 0.1509386 loss_f: 3.8517115\n",
      "3138 Train Loss 20.426117\n",
      "3139 loss_D: 16.419714 loss_N: 0.15095617 loss_f: 3.8525865\n",
      "3139 Train Loss 20.423258\n",
      "3140 loss_D: 16.411047 loss_N: 0.15141724 loss_f: 3.8559523\n",
      "3140 Train Loss 20.418415\n",
      "3141 loss_D: 16.39593 loss_N: 0.15130544 loss_f: 3.8657644\n",
      "3141 Train Loss 20.413\n",
      "3142 loss_D: 16.366575 loss_N: 0.15207347 loss_f: 3.8916593\n",
      "3142 Train Loss 20.410307\n",
      "3143 loss_D: 16.353487 loss_N: 0.1508838 loss_f: 3.899184\n",
      "3143 Train Loss 20.403555\n",
      "3144 loss_D: 16.35701 loss_N: 0.15086141 loss_f: 3.8902383\n",
      "3144 Train Loss 20.39811\n",
      "3145 loss_D: 16.35408 loss_N: 0.15047263 loss_f: 3.8892272\n",
      "3145 Train Loss 20.39378\n",
      "3146 loss_D: 16.34477 loss_N: 0.14962275 loss_f: 3.8942676\n",
      "3146 Train Loss 20.38866\n",
      "3147 loss_D: 16.331696 loss_N: 0.14875479 loss_f: 3.902862\n",
      "3147 Train Loss 20.383312\n",
      "3148 loss_D: 16.311085 loss_N: 0.14805435 loss_f: 3.916342\n",
      "3148 Train Loss 20.37548\n",
      "3149 loss_D: 16.29558 loss_N: 0.1467472 loss_f: 3.9293694\n",
      "3149 Train Loss 20.371696\n",
      "3150 loss_D: 16.281204 loss_N: 0.14796874 loss_f: 3.9489803\n",
      "3150 Train Loss 20.378153\n",
      "3151 loss_D: 16.289566 loss_N: 0.14719601 loss_f: 3.9294538\n",
      "3151 Train Loss 20.366215\n",
      "3152 loss_D: 16.291016 loss_N: 0.14784405 loss_f: 3.9166303\n",
      "3152 Train Loss 20.35549\n",
      "3153 loss_D: 16.297674 loss_N: 0.14820564 loss_f: 3.9019647\n",
      "3153 Train Loss 20.347843\n",
      "3154 loss_D: 16.301699 loss_N: 0.14844741 loss_f: 3.8819351\n",
      "3154 Train Loss 20.33208\n",
      "3155 loss_D: 16.31484 loss_N: 0.14785695 loss_f: 3.852653\n",
      "3155 Train Loss 20.315351\n",
      "3156 loss_D: 16.310125 loss_N: 0.14691366 loss_f: 3.8408842\n",
      "3156 Train Loss 20.297924\n",
      "3157 loss_D: 16.316725 loss_N: 0.14564548 loss_f: 3.8191426\n",
      "3157 Train Loss 20.281513\n",
      "3158 loss_D: 16.31568 loss_N: 0.14510119 loss_f: 3.805083\n",
      "3158 Train Loss 20.265863\n",
      "3159 loss_D: 16.286625 loss_N: 0.1437622 loss_f: 3.8291972\n",
      "3159 Train Loss 20.259584\n",
      "3160 loss_D: 16.305769 loss_N: 0.14437972 loss_f: 3.798395\n",
      "3160 Train Loss 20.248545\n",
      "3161 loss_D: 16.30558 loss_N: 0.1450209 loss_f: 3.7921607\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3161 Train Loss 20.242762\n",
      "3162 loss_D: 16.296635 loss_N: 0.14496543 loss_f: 3.796198\n",
      "3162 Train Loss 20.237799\n",
      "3163 loss_D: 16.282621 loss_N: 0.14443843 loss_f: 3.805193\n",
      "3163 Train Loss 20.232252\n",
      "3164 loss_D: 16.273066 loss_N: 0.1438458 loss_f: 3.8092992\n",
      "3164 Train Loss 20.226212\n",
      "3165 loss_D: 16.261436 loss_N: 0.14209378 loss_f: 3.8147676\n",
      "3165 Train Loss 20.218298\n",
      "3166 loss_D: 16.255817 loss_N: 0.14135925 loss_f: 3.812778\n",
      "3166 Train Loss 20.209955\n",
      "3167 loss_D: 16.25795 loss_N: 0.14146644 loss_f: 3.8028867\n",
      "3167 Train Loss 20.202303\n",
      "3168 loss_D: 16.268866 loss_N: 0.1415196 loss_f: 3.7787726\n",
      "3168 Train Loss 20.189157\n",
      "3169 loss_D: 16.271416 loss_N: 0.14135563 loss_f: 3.7642045\n",
      "3169 Train Loss 20.176975\n",
      "3170 loss_D: 16.273653 loss_N: 0.14139721 loss_f: 3.7513108\n",
      "3170 Train Loss 20.16636\n",
      "3171 loss_D: 16.268328 loss_N: 0.1415987 loss_f: 3.7474203\n",
      "3171 Train Loss 20.157349\n",
      "3172 loss_D: 16.256409 loss_N: 0.14174627 loss_f: 3.7523346\n",
      "3172 Train Loss 20.15049\n",
      "3173 loss_D: 16.250624 loss_N: 0.14188366 loss_f: 3.7538586\n",
      "3173 Train Loss 20.146366\n",
      "3174 loss_D: 16.241312 loss_N: 0.14184308 loss_f: 3.759786\n",
      "3174 Train Loss 20.142942\n",
      "3175 loss_D: 16.232374 loss_N: 0.141256 loss_f: 3.7615585\n",
      "3175 Train Loss 20.13519\n",
      "3176 loss_D: 16.231672 loss_N: 0.14061426 loss_f: 3.7562525\n",
      "3176 Train Loss 20.128538\n",
      "3177 loss_D: 16.220938 loss_N: 0.13966271 loss_f: 3.7604964\n",
      "3177 Train Loss 20.121096\n",
      "3178 loss_D: 16.20527 loss_N: 0.13936228 loss_f: 3.7698069\n",
      "3178 Train Loss 20.11444\n",
      "3179 loss_D: 16.192719 loss_N: 0.13906315 loss_f: 3.7757835\n",
      "3179 Train Loss 20.107565\n",
      "3180 loss_D: 16.194607 loss_N: 0.13931344 loss_f: 3.7703416\n",
      "3180 Train Loss 20.104261\n",
      "3181 loss_D: 16.194891 loss_N: 0.13933448 loss_f: 3.7669203\n",
      "3181 Train Loss 20.101145\n",
      "3182 loss_D: 16.1984 loss_N: 0.13958344 loss_f: 3.7601492\n",
      "3182 Train Loss 20.098133\n",
      "3183 loss_D: 16.203758 loss_N: 0.14014317 loss_f: 3.7494726\n",
      "3183 Train Loss 20.093372\n",
      "3184 loss_D: 16.207853 loss_N: 0.14037807 loss_f: 3.7395463\n",
      "3184 Train Loss 20.087778\n",
      "3185 loss_D: 16.205635 loss_N: 0.14020087 loss_f: 3.7379827\n",
      "3185 Train Loss 20.08382\n",
      "3186 loss_D: 16.207771 loss_N: 0.13967417 loss_f: 3.7327726\n",
      "3186 Train Loss 20.080217\n",
      "3187 loss_D: 16.205011 loss_N: 0.13950711 loss_f: 3.730246\n",
      "3187 Train Loss 20.074764\n",
      "3188 loss_D: 16.203386 loss_N: 0.13913977 loss_f: 3.724038\n",
      "3188 Train Loss 20.066563\n",
      "3189 loss_D: 16.203634 loss_N: 0.13891317 loss_f: 3.7187867\n",
      "3189 Train Loss 20.061335\n",
      "3190 loss_D: 16.212128 loss_N: 0.1385769 loss_f: 3.7020378\n",
      "3190 Train Loss 20.052742\n",
      "3191 loss_D: 16.173147 loss_N: 0.13927981 loss_f: 3.769825\n",
      "3191 Train Loss 20.082253\n",
      "3192 loss_D: 16.200098 loss_N: 0.1387382 loss_f: 3.708732\n",
      "3192 Train Loss 20.04757\n",
      "3193 loss_D: 16.197456 loss_N: 0.1385361 loss_f: 3.7037296\n",
      "3193 Train Loss 20.039722\n",
      "3194 loss_D: 16.19796 loss_N: 0.13838977 loss_f: 3.6945226\n",
      "3194 Train Loss 20.030872\n",
      "3195 loss_D: 16.1795 loss_N: 0.13835855 loss_f: 3.7076952\n",
      "3195 Train Loss 20.025555\n",
      "3196 loss_D: 16.139406 loss_N: 0.14002992 loss_f: 3.7977293\n",
      "3196 Train Loss 20.077166\n",
      "3197 loss_D: 16.170927 loss_N: 0.13867904 loss_f: 3.7124577\n",
      "3197 Train Loss 20.022064\n",
      "3198 loss_D: 16.15576 loss_N: 0.13893664 loss_f: 3.7205222\n",
      "3198 Train Loss 20.015219\n",
      "3199 loss_D: 16.14888 loss_N: 0.13912271 loss_f: 3.7204635\n",
      "3199 Train Loss 20.008465\n",
      "3200 loss_D: 16.13138 loss_N: 0.13901092 loss_f: 3.7344663\n",
      "3200 Train Loss 20.004858\n",
      "3201 loss_D: 16.128103 loss_N: 0.1392605 loss_f: 3.7302797\n",
      "3201 Train Loss 19.997644\n",
      "3202 loss_D: 16.123322 loss_N: 0.13902907 loss_f: 3.7303092\n",
      "3202 Train Loss 19.992659\n",
      "3203 loss_D: 16.110806 loss_N: 0.13881381 loss_f: 3.7384171\n",
      "3203 Train Loss 19.988035\n",
      "3204 loss_D: 16.092766 loss_N: 0.1388235 loss_f: 3.750149\n",
      "3204 Train Loss 19.981737\n",
      "3205 loss_D: 16.070408 loss_N: 0.13880657 loss_f: 3.7667437\n",
      "3205 Train Loss 19.97596\n",
      "3206 loss_D: 16.057106 loss_N: 0.1392323 loss_f: 3.7725866\n",
      "3206 Train Loss 19.968925\n",
      "3207 loss_D: 16.0395 loss_N: 0.1398242 loss_f: 3.7823694\n",
      "3207 Train Loss 19.961693\n",
      "3208 loss_D: 16.045446 loss_N: 0.14031963 loss_f: 3.7704084\n",
      "3208 Train Loss 19.956175\n",
      "3209 loss_D: 16.037487 loss_N: 0.14093263 loss_f: 3.7733188\n",
      "3209 Train Loss 19.951738\n",
      "3210 loss_D: 16.046215 loss_N: 0.14088881 loss_f: 3.761228\n",
      "3210 Train Loss 19.948332\n",
      "3211 loss_D: 16.043468 loss_N: 0.14090349 loss_f: 3.758449\n",
      "3211 Train Loss 19.942822\n",
      "3212 loss_D: 16.039402 loss_N: 0.14086483 loss_f: 3.754914\n",
      "3212 Train Loss 19.93518\n",
      "3213 loss_D: 16.02484 loss_N: 0.140365 loss_f: 3.7755072\n",
      "3213 Train Loss 19.940712\n",
      "3214 loss_D: 16.033707 loss_N: 0.14062661 loss_f: 3.7576354\n",
      "3214 Train Loss 19.931969\n",
      "3215 loss_D: 16.025246 loss_N: 0.14066157 loss_f: 3.759266\n",
      "3215 Train Loss 19.925173\n",
      "3216 loss_D: 16.024553 loss_N: 0.1406288 loss_f: 3.7543492\n",
      "3216 Train Loss 19.91953\n",
      "3217 loss_D: 16.025015 loss_N: 0.14060591 loss_f: 3.7511241\n",
      "3217 Train Loss 19.916744\n",
      "3218 loss_D: 16.029575 loss_N: 0.14040558 loss_f: 3.743121\n",
      "3218 Train Loss 19.913101\n",
      "3219 loss_D: 16.028713 loss_N: 0.13986371 loss_f: 3.736728\n",
      "3219 Train Loss 19.905306\n",
      "3220 loss_D: 16.038633 loss_N: 0.14024523 loss_f: 3.7185311\n",
      "3220 Train Loss 19.89741\n",
      "3221 loss_D: 16.026567 loss_N: 0.13949667 loss_f: 3.7285502\n",
      "3221 Train Loss 19.894613\n",
      "3222 loss_D: 16.023954 loss_N: 0.13983202 loss_f: 3.7245452\n",
      "3222 Train Loss 19.88833\n",
      "3223 loss_D: 16.027985 loss_N: 0.13995178 loss_f: 3.715487\n",
      "3223 Train Loss 19.883423\n",
      "3224 loss_D: 16.010168 loss_N: 0.1398065 loss_f: 3.7263765\n",
      "3224 Train Loss 19.87635\n",
      "3225 loss_D: 16.012938 loss_N: 0.14007655 loss_f: 3.7170296\n",
      "3225 Train Loss 19.870043\n",
      "3226 loss_D: 16.010942 loss_N: 0.14026724 loss_f: 3.7076914\n",
      "3226 Train Loss 19.8589\n",
      "3227 loss_D: 16.010164 loss_N: 0.14018157 loss_f: 3.7028408\n",
      "3227 Train Loss 19.853188\n",
      "3228 loss_D: 15.993868 loss_N: 0.14000642 loss_f: 3.705348\n",
      "3228 Train Loss 19.839222\n",
      "3229 loss_D: 15.959909 loss_N: 0.1397915 loss_f: 3.7194023\n",
      "3229 Train Loss 19.819103\n",
      "3230 loss_D: 15.925737 loss_N: 0.1395554 loss_f: 3.7398999\n",
      "3230 Train Loss 19.805193\n",
      "3231 loss_D: 15.8917055 loss_N: 0.13967904 loss_f: 3.7624788\n",
      "3231 Train Loss 19.793865\n",
      "3232 loss_D: 15.875709 loss_N: 0.13983044 loss_f: 3.7679894\n",
      "3232 Train Loss 19.78353\n",
      "3233 loss_D: 15.85379 loss_N: 0.14056987 loss_f: 3.7779975\n",
      "3233 Train Loss 19.772358\n",
      "3234 loss_D: 15.850443 loss_N: 0.14088522 loss_f: 3.7676148\n",
      "3234 Train Loss 19.758944\n",
      "3235 loss_D: 15.81603 loss_N: 0.14221402 loss_f: 3.7821858\n",
      "3235 Train Loss 19.740429\n",
      "3236 loss_D: 15.800522 loss_N: 0.14246938 loss_f: 3.777098\n",
      "3236 Train Loss 19.720089\n",
      "3237 loss_D: 15.763403 loss_N: 0.1432176 loss_f: 3.7931492\n",
      "3237 Train Loss 19.69977\n",
      "3238 loss_D: 15.719189 loss_N: 0.14279963 loss_f: 3.8443608\n",
      "3238 Train Loss 19.706348\n",
      "3239 loss_D: 15.74452 loss_N: 0.14299709 loss_f: 3.805028\n",
      "3239 Train Loss 19.692545\n",
      "3240 loss_D: 15.733487 loss_N: 0.14219518 loss_f: 3.8044972\n",
      "3240 Train Loss 19.68018\n",
      "3241 loss_D: 15.693495 loss_N: 0.14275026 loss_f: 3.829919\n",
      "3241 Train Loss 19.666164\n",
      "3242 loss_D: 15.682631 loss_N: 0.14299671 loss_f: 3.824236\n",
      "3242 Train Loss 19.649864\n",
      "3243 loss_D: 15.662524 loss_N: 0.14246175 loss_f: 3.8298383\n",
      "3243 Train Loss 19.634825\n",
      "3244 loss_D: 15.655059 loss_N: 0.14271423 loss_f: 3.8271823\n",
      "3244 Train Loss 19.624956\n",
      "3245 loss_D: 15.655703 loss_N: 0.14287198 loss_f: 3.821269\n",
      "3245 Train Loss 19.619843\n",
      "3246 loss_D: 15.619878 loss_N: 0.14378984 loss_f: 3.849378\n",
      "3246 Train Loss 19.613047\n",
      "3247 loss_D: 15.625164 loss_N: 0.143755 loss_f: 3.8348022\n",
      "3247 Train Loss 19.603722\n",
      "3248 loss_D: 15.624296 loss_N: 0.14361714 loss_f: 3.8234637\n",
      "3248 Train Loss 19.591377\n",
      "3249 loss_D: 15.615294 loss_N: 0.14378844 loss_f: 3.8245654\n",
      "3249 Train Loss 19.583649\n",
      "3250 loss_D: 15.603528 loss_N: 0.14398092 loss_f: 3.8269508\n",
      "3250 Train Loss 19.57446\n",
      "3251 loss_D: 15.593078 loss_N: 0.14401804 loss_f: 3.8277264\n",
      "3251 Train Loss 19.564823\n",
      "3252 loss_D: 15.5572195 loss_N: 0.14364858 loss_f: 3.8687615\n",
      "3252 Train Loss 19.56963\n",
      "3253 loss_D: 15.577009 loss_N: 0.14384568 loss_f: 3.8371205\n",
      "3253 Train Loss 19.557976\n",
      "3254 loss_D: 15.575096 loss_N: 0.14398533 loss_f: 3.8315156\n",
      "3254 Train Loss 19.550598\n",
      "3255 loss_D: 15.55367 loss_N: 0.14403781 loss_f: 3.8429031\n",
      "3255 Train Loss 19.540611\n",
      "3256 loss_D: 15.528296 loss_N: 0.14442407 loss_f: 3.8608522\n",
      "3256 Train Loss 19.533573\n",
      "3257 loss_D: 15.490368 loss_N: 0.14456777 loss_f: 3.8884053\n",
      "3257 Train Loss 19.52334\n",
      "3258 loss_D: 15.452978 loss_N: 0.14559162 loss_f: 3.915335\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3258 Train Loss 19.513905\n",
      "3259 loss_D: 15.425278 loss_N: 0.14605266 loss_f: 3.9267287\n",
      "3259 Train Loss 19.498058\n",
      "3260 loss_D: 15.42509 loss_N: 0.14611764 loss_f: 3.9112327\n",
      "3260 Train Loss 19.482439\n",
      "3261 loss_D: 15.39459 loss_N: 0.14644973 loss_f: 3.9250813\n",
      "3261 Train Loss 19.466122\n",
      "3262 loss_D: 15.418297 loss_N: 0.14614761 loss_f: 3.8955069\n",
      "3262 Train Loss 19.459951\n",
      "3263 loss_D: 15.396644 loss_N: 0.14673981 loss_f: 3.9077053\n",
      "3263 Train Loss 19.451088\n",
      "3264 loss_D: 15.360918 loss_N: 0.14875643 loss_f: 3.9282062\n",
      "3264 Train Loss 19.43788\n",
      "3265 loss_D: 15.348259 loss_N: 0.14954658 loss_f: 3.9301496\n",
      "3265 Train Loss 19.427956\n",
      "3266 loss_D: 15.348756 loss_N: 0.15078387 loss_f: 3.9153042\n",
      "3266 Train Loss 19.414845\n",
      "3267 loss_D: 15.342571 loss_N: 0.15115732 loss_f: 3.9075444\n",
      "3267 Train Loss 19.401274\n",
      "3268 loss_D: 15.331003 loss_N: 0.15079276 loss_f: 3.9050624\n",
      "3268 Train Loss 19.386858\n",
      "3269 loss_D: 15.303643 loss_N: 0.14898376 loss_f: 3.9606304\n",
      "3269 Train Loss 19.413258\n",
      "3270 loss_D: 15.322251 loss_N: 0.15007058 loss_f: 3.90765\n",
      "3270 Train Loss 19.37997\n",
      "3271 loss_D: 15.29314 loss_N: 0.1508819 loss_f: 3.9102256\n",
      "3271 Train Loss 19.354248\n",
      "3272 loss_D: 15.274958 loss_N: 0.15118319 loss_f: 3.9057443\n",
      "3272 Train Loss 19.331884\n",
      "3273 loss_D: 15.261015 loss_N: 0.15136915 loss_f: 3.8998322\n",
      "3273 Train Loss 19.312216\n",
      "3274 loss_D: 15.239508 loss_N: 0.15080072 loss_f: 3.9069424\n",
      "3274 Train Loss 19.29725\n",
      "3275 loss_D: 15.238593 loss_N: 0.15090245 loss_f: 3.892385\n",
      "3275 Train Loss 19.281881\n",
      "3276 loss_D: 15.241427 loss_N: 0.1501494 loss_f: 3.8692014\n",
      "3276 Train Loss 19.260778\n",
      "3277 loss_D: 15.239662 loss_N: 0.15061174 loss_f: 3.8538702\n",
      "3277 Train Loss 19.244144\n",
      "3278 loss_D: 15.247421 loss_N: 0.15077162 loss_f: 3.8289475\n",
      "3278 Train Loss 19.22714\n",
      "3279 loss_D: 15.245546 loss_N: 0.15222639 loss_f: 3.8130925\n",
      "3279 Train Loss 19.210865\n",
      "3280 loss_D: 15.258836 loss_N: 0.15215164 loss_f: 3.7911186\n",
      "3280 Train Loss 19.202106\n",
      "3281 loss_D: 15.263404 loss_N: 0.15255125 loss_f: 3.7781792\n",
      "3281 Train Loss 19.194134\n",
      "3282 loss_D: 15.256039 loss_N: 0.15393338 loss_f: 3.7740295\n",
      "3282 Train Loss 19.184002\n",
      "3283 loss_D: 15.254234 loss_N: 0.15396863 loss_f: 3.7672281\n",
      "3283 Train Loss 19.17543\n",
      "3284 loss_D: 15.246159 loss_N: 0.15451908 loss_f: 3.765603\n",
      "3284 Train Loss 19.16628\n",
      "3285 loss_D: 15.228157 loss_N: 0.1545712 loss_f: 3.771749\n",
      "3285 Train Loss 19.154478\n",
      "3286 loss_D: 15.219526 loss_N: 0.15478264 loss_f: 3.7716234\n",
      "3286 Train Loss 19.145931\n",
      "3287 loss_D: 15.211922 loss_N: 0.15319338 loss_f: 3.7794077\n",
      "3287 Train Loss 19.144524\n",
      "3288 loss_D: 15.210209 loss_N: 0.15393604 loss_f: 3.7706988\n",
      "3288 Train Loss 19.134844\n",
      "3289 loss_D: 15.209829 loss_N: 0.15372199 loss_f: 3.7669241\n",
      "3289 Train Loss 19.130476\n",
      "3290 loss_D: 15.210809 loss_N: 0.15312918 loss_f: 3.7589576\n",
      "3290 Train Loss 19.122896\n",
      "3291 loss_D: 15.2035885 loss_N: 0.15297715 loss_f: 3.7585158\n",
      "3291 Train Loss 19.115082\n",
      "3292 loss_D: 15.189077 loss_N: 0.15298292 loss_f: 3.7613375\n",
      "3292 Train Loss 19.103397\n",
      "3293 loss_D: 15.152883 loss_N: 0.15496635 loss_f: 3.7879992\n",
      "3293 Train Loss 19.095848\n",
      "3294 loss_D: 15.153328 loss_N: 0.15393266 loss_f: 3.768444\n",
      "3294 Train Loss 19.075705\n",
      "3295 loss_D: 15.124075 loss_N: 0.15437609 loss_f: 3.786368\n",
      "3295 Train Loss 19.06482\n",
      "3296 loss_D: 15.117748 loss_N: 0.15464021 loss_f: 3.7834754\n",
      "3296 Train Loss 19.055864\n",
      "3297 loss_D: 15.096778 loss_N: 0.15481561 loss_f: 3.7970836\n",
      "3297 Train Loss 19.048677\n",
      "3298 loss_D: 15.092593 loss_N: 0.154671 loss_f: 3.79406\n",
      "3298 Train Loss 19.041325\n",
      "3299 loss_D: 15.097779 loss_N: 0.15371044 loss_f: 3.7805696\n",
      "3299 Train Loss 19.032059\n",
      "3300 loss_D: 15.110357 loss_N: 0.1530892 loss_f: 3.7609162\n",
      "3300 Train Loss 19.024363\n",
      "3301 loss_D: 15.122734 loss_N: 0.15252931 loss_f: 3.7437723\n",
      "3301 Train Loss 19.019035\n",
      "3302 loss_D: 15.138643 loss_N: 0.15209906 loss_f: 3.7215128\n",
      "3302 Train Loss 19.012255\n",
      "3303 loss_D: 15.141815 loss_N: 0.15219083 loss_f: 3.7113183\n",
      "3303 Train Loss 19.005325\n",
      "3304 loss_D: 15.14277 loss_N: 0.15279497 loss_f: 3.7033336\n",
      "3304 Train Loss 18.998898\n",
      "3305 loss_D: 15.136812 loss_N: 0.15365337 loss_f: 3.702621\n",
      "3305 Train Loss 18.993086\n",
      "3306 loss_D: 15.134509 loss_N: 0.15452759 loss_f: 3.6978154\n",
      "3306 Train Loss 18.986853\n",
      "3307 loss_D: 15.139484 loss_N: 0.15415828 loss_f: 3.6856523\n",
      "3307 Train Loss 18.979296\n",
      "3308 loss_D: 15.144214 loss_N: 0.15774228 loss_f: 3.6854632\n",
      "3308 Train Loss 18.98742\n",
      "3309 loss_D: 15.140932 loss_N: 0.15523054 loss_f: 3.679075\n",
      "3309 Train Loss 18.975237\n",
      "3310 loss_D: 15.150966 loss_N: 0.15457371 loss_f: 3.661023\n",
      "3310 Train Loss 18.966562\n",
      "3311 loss_D: 15.164347 loss_N: 0.15344794 loss_f: 3.6400688\n",
      "3311 Train Loss 18.957863\n",
      "3312 loss_D: 15.1746435 loss_N: 0.15270579 loss_f: 3.6172347\n",
      "3312 Train Loss 18.944584\n",
      "3313 loss_D: 15.1796 loss_N: 0.15270615 loss_f: 3.600421\n",
      "3313 Train Loss 18.932728\n",
      "3314 loss_D: 15.1784 loss_N: 0.1527451 loss_f: 3.586481\n",
      "3314 Train Loss 18.917625\n",
      "3315 loss_D: 15.178124 loss_N: 0.15293428 loss_f: 3.572105\n",
      "3315 Train Loss 18.903164\n",
      "3316 loss_D: 15.186749 loss_N: 0.15253702 loss_f: 3.5527601\n",
      "3316 Train Loss 18.892048\n",
      "3317 loss_D: 15.169888 loss_N: 0.15200417 loss_f: 3.6014328\n",
      "3317 Train Loss 18.923325\n",
      "3318 loss_D: 15.179806 loss_N: 0.15220433 loss_f: 3.5469522\n",
      "3318 Train Loss 18.878963\n",
      "3319 loss_D: 15.177743 loss_N: 0.15202299 loss_f: 3.5408561\n",
      "3319 Train Loss 18.870623\n",
      "3320 loss_D: 15.179468 loss_N: 0.15162353 loss_f: 3.5347047\n",
      "3320 Train Loss 18.865797\n",
      "3321 loss_D: 15.177973 loss_N: 0.15154696 loss_f: 3.5331483\n",
      "3321 Train Loss 18.862667\n",
      "3322 loss_D: 15.1783695 loss_N: 0.15160725 loss_f: 3.5288663\n",
      "3322 Train Loss 18.858843\n",
      "3323 loss_D: 15.171733 loss_N: 0.15196584 loss_f: 3.5310104\n",
      "3323 Train Loss 18.85471\n",
      "3324 loss_D: 15.163589 loss_N: 0.15240146 loss_f: 3.5339797\n",
      "3324 Train Loss 18.84997\n",
      "3325 loss_D: 15.151881 loss_N: 0.15289119 loss_f: 3.5399604\n",
      "3325 Train Loss 18.844732\n",
      "3326 loss_D: 15.138015 loss_N: 0.15335622 loss_f: 3.546269\n",
      "3326 Train Loss 18.83764\n",
      "3327 loss_D: 15.126292 loss_N: 0.15362225 loss_f: 3.5478842\n",
      "3327 Train Loss 18.827799\n",
      "3328 loss_D: 15.117092 loss_N: 0.15356642 loss_f: 3.545421\n",
      "3328 Train Loss 18.81608\n",
      "3329 loss_D: 15.091509 loss_N: 0.15440752 loss_f: 3.548909\n",
      "3329 Train Loss 18.794825\n",
      "3330 loss_D: 15.068231 loss_N: 0.15619795 loss_f: 3.558296\n",
      "3330 Train Loss 18.782724\n",
      "3331 loss_D: 15.056982 loss_N: 0.15700036 loss_f: 3.5604022\n",
      "3331 Train Loss 18.774385\n",
      "3332 loss_D: 15.042917 loss_N: 0.15819004 loss_f: 3.5654514\n",
      "3332 Train Loss 18.766558\n",
      "3333 loss_D: 15.037956 loss_N: 0.15902124 loss_f: 3.5623686\n",
      "3333 Train Loss 18.759346\n",
      "3334 loss_D: 15.036048 loss_N: 0.15913059 loss_f: 3.5568633\n",
      "3334 Train Loss 18.752043\n",
      "3335 loss_D: 15.038728 loss_N: 0.15885264 loss_f: 3.5431867\n",
      "3335 Train Loss 18.740767\n",
      "3336 loss_D: 15.045331 loss_N: 0.15825392 loss_f: 3.526056\n",
      "3336 Train Loss 18.729641\n",
      "3337 loss_D: 15.050229 loss_N: 0.15739584 loss_f: 3.508679\n",
      "3337 Train Loss 18.716303\n",
      "3338 loss_D: 15.061676 loss_N: 0.15676086 loss_f: 3.4902382\n",
      "3338 Train Loss 18.708675\n",
      "3339 loss_D: 15.056906 loss_N: 0.15671743 loss_f: 3.4877038\n",
      "3339 Train Loss 18.701326\n",
      "3340 loss_D: 15.053534 loss_N: 0.15682153 loss_f: 3.4822085\n",
      "3340 Train Loss 18.692564\n",
      "3341 loss_D: 15.048445 loss_N: 0.15667018 loss_f: 3.4796228\n",
      "3341 Train Loss 18.684738\n",
      "3342 loss_D: 15.044034 loss_N: 0.15568227 loss_f: 3.4807405\n",
      "3342 Train Loss 18.680458\n",
      "3343 loss_D: 15.0461 loss_N: 0.15516266 loss_f: 3.4692843\n",
      "3343 Train Loss 18.670547\n",
      "3344 loss_D: 15.048058 loss_N: 0.15446663 loss_f: 3.4609234\n",
      "3344 Train Loss 18.663448\n",
      "3345 loss_D: 15.055474 loss_N: 0.15317786 loss_f: 3.447392\n",
      "3345 Train Loss 18.656044\n",
      "3346 loss_D: 15.054869 loss_N: 0.15255469 loss_f: 3.4429748\n",
      "3346 Train Loss 18.650398\n",
      "3347 loss_D: 15.054563 loss_N: 0.15182409 loss_f: 3.4334457\n",
      "3347 Train Loss 18.639832\n",
      "3348 loss_D: 15.024025 loss_N: 0.15465604 loss_f: 3.4712014\n",
      "3348 Train Loss 18.649883\n",
      "3349 loss_D: 15.043185 loss_N: 0.15283743 loss_f: 3.4390335\n",
      "3349 Train Loss 18.635056\n",
      "3350 loss_D: 15.010978 loss_N: 0.15267952 loss_f: 3.4795096\n",
      "3350 Train Loss 18.643167\n",
      "3351 loss_D: 15.028839 loss_N: 0.15274924 loss_f: 3.4440107\n",
      "3351 Train Loss 18.625599\n",
      "3352 loss_D: 15.022959 loss_N: 0.15315038 loss_f: 3.4380338\n",
      "3352 Train Loss 18.614143\n",
      "3353 loss_D: 15.006431 loss_N: 0.15404901 loss_f: 3.4336023\n",
      "3353 Train Loss 18.594082\n",
      "3354 loss_D: 14.986113 loss_N: 0.15554604 loss_f: 3.4416053\n",
      "3354 Train Loss 18.583263\n",
      "3355 loss_D: 14.981351 loss_N: 0.15508403 loss_f: 3.4398777\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3355 Train Loss 18.576313\n",
      "3356 loss_D: 14.956489 loss_N: 0.15499546 loss_f: 3.4578674\n",
      "3356 Train Loss 18.569351\n",
      "3357 loss_D: 14.959372 loss_N: 0.15371303 loss_f: 3.4507663\n",
      "3357 Train Loss 18.56385\n",
      "3358 loss_D: 14.95361 loss_N: 0.15272765 loss_f: 3.4506073\n",
      "3358 Train Loss 18.556946\n",
      "3359 loss_D: 14.945054 loss_N: 0.15186933 loss_f: 3.45191\n",
      "3359 Train Loss 18.548834\n",
      "3360 loss_D: 14.919084 loss_N: 0.15463166 loss_f: 3.624135\n",
      "3360 Train Loss 18.69785\n",
      "3361 loss_D: 14.942362 loss_N: 0.15196407 loss_f: 3.4529178\n",
      "3361 Train Loss 18.547243\n",
      "3362 loss_D: 14.919974 loss_N: 0.151891 loss_f: 3.4657824\n",
      "3362 Train Loss 18.537647\n",
      "3363 loss_D: 14.921875 loss_N: 0.15177937 loss_f: 3.4583952\n",
      "3363 Train Loss 18.53205\n",
      "3364 loss_D: 14.918077 loss_N: 0.1516399 loss_f: 3.4576592\n",
      "3364 Train Loss 18.527376\n",
      "3365 loss_D: 14.916634 loss_N: 0.15146017 loss_f: 3.4552991\n",
      "3365 Train Loss 18.523394\n",
      "3366 loss_D: 14.912062 loss_N: 0.1509679 loss_f: 3.4536998\n",
      "3366 Train Loss 18.51673\n",
      "3367 loss_D: 14.891928 loss_N: 0.1504833 loss_f: 3.4647381\n",
      "3367 Train Loss 18.507149\n",
      "3368 loss_D: 14.876097 loss_N: 0.14988874 loss_f: 3.469394\n",
      "3368 Train Loss 18.49538\n",
      "3369 loss_D: 14.850417 loss_N: 0.15005764 loss_f: 3.4864893\n",
      "3369 Train Loss 18.486965\n",
      "3370 loss_D: 14.843262 loss_N: 0.15029016 loss_f: 3.4869678\n",
      "3370 Train Loss 18.48052\n",
      "3371 loss_D: 14.84144 loss_N: 0.15073675 loss_f: 3.4839463\n",
      "3371 Train Loss 18.476124\n",
      "3372 loss_D: 14.838606 loss_N: 0.15128662 loss_f: 3.4807534\n",
      "3372 Train Loss 18.470646\n",
      "3373 loss_D: 14.7990465 loss_N: 0.15785444 loss_f: 3.594835\n",
      "3373 Train Loss 18.551735\n",
      "3374 loss_D: 14.834411 loss_N: 0.15176836 loss_f: 3.4834511\n",
      "3374 Train Loss 18.469631\n",
      "3375 loss_D: 14.840716 loss_N: 0.15178831 loss_f: 3.4732757\n",
      "3375 Train Loss 18.46578\n",
      "3376 loss_D: 14.834944 loss_N: 0.1517495 loss_f: 3.4766295\n",
      "3376 Train Loss 18.463324\n",
      "3377 loss_D: 14.835973 loss_N: 0.15172009 loss_f: 3.4714007\n",
      "3377 Train Loss 18.459093\n",
      "3378 loss_D: 14.82417 loss_N: 0.15066934 loss_f: 3.4796736\n",
      "3378 Train Loss 18.454514\n",
      "3379 loss_D: 14.827431 loss_N: 0.15068428 loss_f: 3.470889\n",
      "3379 Train Loss 18.449005\n",
      "3380 loss_D: 14.8309765 loss_N: 0.15042754 loss_f: 3.4639242\n",
      "3380 Train Loss 18.445328\n",
      "3381 loss_D: 14.835221 loss_N: 0.14963683 loss_f: 3.4561362\n",
      "3381 Train Loss 18.440994\n",
      "3382 loss_D: 14.846459 loss_N: 0.14810452 loss_f: 3.4470713\n",
      "3382 Train Loss 18.441635\n",
      "3383 loss_D: 14.840505 loss_N: 0.14886814 loss_f: 3.4482503\n",
      "3383 Train Loss 18.437624\n",
      "3384 loss_D: 14.837428 loss_N: 0.1474617 loss_f: 3.4428601\n",
      "3384 Train Loss 18.42775\n",
      "3385 loss_D: 14.8294 loss_N: 0.14665665 loss_f: 3.441807\n",
      "3385 Train Loss 18.417864\n",
      "3386 loss_D: 14.818906 loss_N: 0.14622022 loss_f: 3.4418867\n",
      "3386 Train Loss 18.407013\n",
      "3387 loss_D: 14.815115 loss_N: 0.14611474 loss_f: 3.438664\n",
      "3387 Train Loss 18.399893\n",
      "3388 loss_D: 14.81404 loss_N: 0.14623964 loss_f: 3.431956\n",
      "3388 Train Loss 18.392235\n",
      "3389 loss_D: 14.787918 loss_N: 0.14715247 loss_f: 3.4805717\n",
      "3389 Train Loss 18.415642\n",
      "3390 loss_D: 14.808959 loss_N: 0.14638881 loss_f: 3.4353652\n",
      "3390 Train Loss 18.390713\n",
      "3391 loss_D: 14.808266 loss_N: 0.14695674 loss_f: 3.4280941\n",
      "3391 Train Loss 18.383316\n",
      "3392 loss_D: 14.806923 loss_N: 0.14762948 loss_f: 3.4226303\n",
      "3392 Train Loss 18.377182\n",
      "3393 loss_D: 14.799754 loss_N: 0.1480874 loss_f: 3.4247065\n",
      "3393 Train Loss 18.372547\n",
      "3394 loss_D: 14.7950735 loss_N: 0.14798293 loss_f: 3.4231741\n",
      "3394 Train Loss 18.36623\n",
      "3395 loss_D: 14.789296 loss_N: 0.14783351 loss_f: 3.4234445\n",
      "3395 Train Loss 18.360575\n",
      "3396 loss_D: 14.776803 loss_N: 0.14759476 loss_f: 3.4285758\n",
      "3396 Train Loss 18.352974\n",
      "3397 loss_D: 14.772565 loss_N: 0.146949 loss_f: 3.423665\n",
      "3397 Train Loss 18.343178\n",
      "3398 loss_D: 14.758762 loss_N: 0.1469057 loss_f: 3.4294271\n",
      "3398 Train Loss 18.335094\n",
      "3399 loss_D: 14.768606 loss_N: 0.14586584 loss_f: 3.4134839\n",
      "3399 Train Loss 18.327955\n",
      "3400 loss_D: 14.776723 loss_N: 0.14774145 loss_f: 3.4246244\n",
      "3400 Train Loss 18.349089\n",
      "3401 loss_D: 14.770528 loss_N: 0.14637057 loss_f: 3.4073944\n",
      "3401 Train Loss 18.324293\n",
      "3402 loss_D: 14.762358 loss_N: 0.14631684 loss_f: 3.407125\n",
      "3402 Train Loss 18.3158\n",
      "3403 loss_D: 14.749334 loss_N: 0.14598158 loss_f: 3.4048746\n",
      "3403 Train Loss 18.30019\n",
      "3404 loss_D: 14.735247 loss_N: 0.14571078 loss_f: 3.4079351\n",
      "3404 Train Loss 18.288893\n",
      "3405 loss_D: 14.704056 loss_N: 0.14518537 loss_f: 3.4216096\n",
      "3405 Train Loss 18.270851\n",
      "3406 loss_D: 14.699303 loss_N: 0.14464767 loss_f: 3.413636\n",
      "3406 Train Loss 18.257586\n",
      "3407 loss_D: 14.69545 loss_N: 0.1436203 loss_f: 3.4048696\n",
      "3407 Train Loss 18.24394\n",
      "3408 loss_D: 14.680868 loss_N: 0.14321066 loss_f: 3.4108171\n",
      "3408 Train Loss 18.234896\n",
      "3409 loss_D: 14.679454 loss_N: 0.14284804 loss_f: 3.4032445\n",
      "3409 Train Loss 18.225546\n",
      "3410 loss_D: 14.659434 loss_N: 0.14313279 loss_f: 3.4121945\n",
      "3410 Train Loss 18.214762\n",
      "3411 loss_D: 14.646901 loss_N: 0.1440284 loss_f: 3.4149485\n",
      "3411 Train Loss 18.20588\n",
      "3412 loss_D: 14.637921 loss_N: 0.14441814 loss_f: 3.4186494\n",
      "3412 Train Loss 18.200989\n",
      "3413 loss_D: 14.639488 loss_N: 0.14465223 loss_f: 3.412213\n",
      "3413 Train Loss 18.196354\n",
      "3414 loss_D: 14.63903 loss_N: 0.14476964 loss_f: 3.4087484\n",
      "3414 Train Loss 18.192549\n",
      "3415 loss_D: 14.640452 loss_N: 0.14439623 loss_f: 3.403621\n",
      "3415 Train Loss 18.188469\n",
      "3416 loss_D: 14.640753 loss_N: 0.14407931 loss_f: 3.3990176\n",
      "3416 Train Loss 18.18385\n",
      "3417 loss_D: 14.637711 loss_N: 0.1437158 loss_f: 3.3980896\n",
      "3417 Train Loss 18.179516\n",
      "3418 loss_D: 14.634709 loss_N: 0.14355464 loss_f: 3.3960469\n",
      "3418 Train Loss 18.17431\n",
      "3419 loss_D: 14.627631 loss_N: 0.14396767 loss_f: 3.3953805\n",
      "3419 Train Loss 18.166979\n",
      "3420 loss_D: 14.620532 loss_N: 0.14403275 loss_f: 3.3952894\n",
      "3420 Train Loss 18.159855\n",
      "3421 loss_D: 14.61779 loss_N: 0.14490624 loss_f: 3.3895898\n",
      "3421 Train Loss 18.152287\n",
      "3422 loss_D: 14.609889 loss_N: 0.14484522 loss_f: 3.4730928\n",
      "3422 Train Loss 18.227827\n",
      "3423 loss_D: 14.615578 loss_N: 0.1448658 loss_f: 3.3887768\n",
      "3423 Train Loss 18.14922\n",
      "3424 loss_D: 14.608022 loss_N: 0.14600259 loss_f: 3.3872488\n",
      "3424 Train Loss 18.141273\n",
      "3425 loss_D: 14.607614 loss_N: 0.14617418 loss_f: 3.3836043\n",
      "3425 Train Loss 18.137392\n",
      "3426 loss_D: 14.597297 loss_N: 0.14592542 loss_f: 3.389739\n",
      "3426 Train Loss 18.132961\n",
      "3427 loss_D: 14.586792 loss_N: 0.146223 loss_f: 3.3957975\n",
      "3427 Train Loss 18.128813\n",
      "3428 loss_D: 14.576534 loss_N: 0.14611721 loss_f: 3.4020374\n",
      "3428 Train Loss 18.12469\n",
      "3429 loss_D: 14.565453 loss_N: 0.14644249 loss_f: 3.4067652\n",
      "3429 Train Loss 18.11866\n",
      "3430 loss_D: 14.550757 loss_N: 0.14686716 loss_f: 3.416121\n",
      "3430 Train Loss 18.113745\n",
      "3431 loss_D: 14.54565 loss_N: 0.1478425 loss_f: 3.414544\n",
      "3431 Train Loss 18.108036\n",
      "3432 loss_D: 14.532759 loss_N: 0.14883442 loss_f: 3.4198852\n",
      "3432 Train Loss 18.101479\n",
      "3433 loss_D: 14.526791 loss_N: 0.14925808 loss_f: 3.420857\n",
      "3433 Train Loss 18.096905\n",
      "3434 loss_D: 14.51195 loss_N: 0.1491354 loss_f: 3.4315617\n",
      "3434 Train Loss 18.092648\n",
      "3435 loss_D: 14.498366 loss_N: 0.14792737 loss_f: 3.4411116\n",
      "3435 Train Loss 18.087406\n",
      "3436 loss_D: 14.477155 loss_N: 0.1470469 loss_f: 3.4579346\n",
      "3436 Train Loss 18.082136\n",
      "3437 loss_D: 14.435207 loss_N: 0.14408343 loss_f: 3.5265968\n",
      "3437 Train Loss 18.105886\n",
      "3438 loss_D: 14.464501 loss_N: 0.1461556 loss_f: 3.4671428\n",
      "3438 Train Loss 18.077799\n",
      "3439 loss_D: 14.474434 loss_N: 0.14546874 loss_f: 3.4533834\n",
      "3439 Train Loss 18.073286\n",
      "3440 loss_D: 14.476653 loss_N: 0.14565678 loss_f: 3.4473624\n",
      "3440 Train Loss 18.069672\n",
      "3441 loss_D: 14.479343 loss_N: 0.14579257 loss_f: 3.4427073\n",
      "3441 Train Loss 18.067844\n",
      "3442 loss_D: 14.480041 loss_N: 0.14608023 loss_f: 3.4363296\n",
      "3442 Train Loss 18.06245\n",
      "3443 loss_D: 14.474231 loss_N: 0.14604607 loss_f: 3.4384687\n",
      "3443 Train Loss 18.058744\n",
      "3444 loss_D: 14.464524 loss_N: 0.1461842 loss_f: 3.4453292\n",
      "3444 Train Loss 18.056038\n",
      "3445 loss_D: 14.455106 loss_N: 0.14602101 loss_f: 3.4522555\n",
      "3445 Train Loss 18.053383\n",
      "3446 loss_D: 14.448856 loss_N: 0.14588894 loss_f: 3.4562187\n",
      "3446 Train Loss 18.050964\n",
      "3447 loss_D: 14.4446125 loss_N: 0.14588468 loss_f: 3.4567235\n",
      "3447 Train Loss 18.04722\n",
      "3448 loss_D: 14.443756 loss_N: 0.14607415 loss_f: 3.4532182\n",
      "3448 Train Loss 18.043049\n",
      "3449 loss_D: 14.447315 loss_N: 0.14652085 loss_f: 3.447496\n",
      "3449 Train Loss 18.041332\n",
      "3450 loss_D: 14.440888 loss_N: 0.14569911 loss_f: 3.4521804\n",
      "3450 Train Loss 18.038769\n",
      "3451 loss_D: 14.451804 loss_N: 0.1465746 loss_f: 3.433632\n",
      "3451 Train Loss 18.032011\n",
      "3452 loss_D: 14.454863 loss_N: 0.1457585 loss_f: 3.4269555\n",
      "3452 Train Loss 18.027576\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3453 loss_D: 14.455081 loss_N: 0.14544804 loss_f: 3.4221451\n",
      "3453 Train Loss 18.022675\n",
      "3454 loss_D: 14.459572 loss_N: 0.14433905 loss_f: 3.4028094\n",
      "3454 Train Loss 18.00672\n",
      "3455 loss_D: 14.451525 loss_N: 0.14354853 loss_f: 3.4025404\n",
      "3455 Train Loss 17.997614\n",
      "3456 loss_D: 14.455306 loss_N: 0.14357138 loss_f: 3.3883538\n",
      "3456 Train Loss 17.98723\n",
      "3457 loss_D: 14.44953 loss_N: 0.14383887 loss_f: 3.3825562\n",
      "3457 Train Loss 17.975925\n",
      "3458 loss_D: 14.443169 loss_N: 0.14398448 loss_f: 3.3812995\n",
      "3458 Train Loss 17.968452\n",
      "3459 loss_D: 14.426604 loss_N: 0.14453952 loss_f: 3.385457\n",
      "3459 Train Loss 17.9566\n",
      "3460 loss_D: 14.404172 loss_N: 0.1419337 loss_f: 3.4480643\n",
      "3460 Train Loss 17.99417\n",
      "3461 loss_D: 14.420539 loss_N: 0.14380626 loss_f: 3.3872917\n",
      "3461 Train Loss 17.951637\n",
      "3462 loss_D: 14.40804 loss_N: 0.1439466 loss_f: 3.391653\n",
      "3462 Train Loss 17.94364\n",
      "3463 loss_D: 14.371224 loss_N: 0.14431676 loss_f: 3.4362893\n",
      "3463 Train Loss 17.95183\n",
      "3464 loss_D: 14.3941555 loss_N: 0.14404497 loss_f: 3.4014647\n",
      "3464 Train Loss 17.939665\n",
      "3465 loss_D: 14.393208 loss_N: 0.14366981 loss_f: 3.3952498\n",
      "3465 Train Loss 17.932127\n",
      "3466 loss_D: 14.388984 loss_N: 0.14332592 loss_f: 3.3957589\n",
      "3466 Train Loss 17.928068\n",
      "3467 loss_D: 14.385712 loss_N: 0.14214003 loss_f: 3.3899257\n",
      "3467 Train Loss 17.917778\n",
      "3468 loss_D: 14.379845 loss_N: 0.14167339 loss_f: 3.388921\n",
      "3468 Train Loss 17.910439\n",
      "3469 loss_D: 14.376197 loss_N: 0.14126034 loss_f: 3.3854296\n",
      "3469 Train Loss 17.902887\n",
      "3470 loss_D: 14.3649025 loss_N: 0.14120512 loss_f: 3.3913054\n",
      "3470 Train Loss 17.897413\n",
      "3471 loss_D: 14.35535 loss_N: 0.14124782 loss_f: 3.393181\n",
      "3471 Train Loss 17.889778\n",
      "3472 loss_D: 14.338522 loss_N: 0.14125505 loss_f: 3.4015296\n",
      "3472 Train Loss 17.881308\n",
      "3473 loss_D: 14.338334 loss_N: 0.14133254 loss_f: 3.3955295\n",
      "3473 Train Loss 17.875196\n",
      "3474 loss_D: 14.332291 loss_N: 0.14126687 loss_f: 3.3958125\n",
      "3474 Train Loss 17.86937\n",
      "3475 loss_D: 14.332335 loss_N: 0.14125305 loss_f: 3.3914983\n",
      "3475 Train Loss 17.865088\n",
      "3476 loss_D: 14.325718 loss_N: 0.14122385 loss_f: 3.393451\n",
      "3476 Train Loss 17.860394\n",
      "3477 loss_D: 14.3142185 loss_N: 0.14115676 loss_f: 3.399564\n",
      "3477 Train Loss 17.85494\n",
      "3478 loss_D: 14.295397 loss_N: 0.14112209 loss_f: 3.4134007\n",
      "3478 Train Loss 17.849918\n",
      "3479 loss_D: 14.277871 loss_N: 0.14076869 loss_f: 3.430564\n",
      "3479 Train Loss 17.849205\n",
      "3480 loss_D: 14.281842 loss_N: 0.14088686 loss_f: 3.4212258\n",
      "3480 Train Loss 17.843956\n",
      "3481 loss_D: 14.281422 loss_N: 0.14105175 loss_f: 3.4179327\n",
      "3481 Train Loss 17.840406\n",
      "3482 loss_D: 14.280845 loss_N: 0.14100614 loss_f: 3.4120362\n",
      "3482 Train Loss 17.833887\n",
      "3483 loss_D: 14.245497 loss_N: 0.14388426 loss_f: 3.5926924\n",
      "3483 Train Loss 17.982075\n",
      "3484 loss_D: 14.276339 loss_N: 0.14120309 loss_f: 3.4142451\n",
      "3484 Train Loss 17.831787\n",
      "3485 loss_D: 14.256728 loss_N: 0.14146921 loss_f: 3.4212375\n",
      "3485 Train Loss 17.819435\n",
      "3486 loss_D: 14.2420435 loss_N: 0.14191525 loss_f: 3.4236546\n",
      "3486 Train Loss 17.807613\n",
      "3487 loss_D: 14.207562 loss_N: 0.14261594 loss_f: 3.4481094\n",
      "3487 Train Loss 17.798288\n",
      "3488 loss_D: 14.187547 loss_N: 0.14367886 loss_f: 3.460111\n",
      "3488 Train Loss 17.791336\n",
      "3489 loss_D: 14.160362 loss_N: 0.14562155 loss_f: 3.4826865\n",
      "3489 Train Loss 17.78867\n",
      "3490 loss_D: 14.1555805 loss_N: 0.14564659 loss_f: 3.4794133\n",
      "3490 Train Loss 17.780642\n",
      "3491 loss_D: 14.166887 loss_N: 0.14588906 loss_f: 3.462315\n",
      "3491 Train Loss 17.775091\n",
      "3492 loss_D: 14.149831 loss_N: 0.14694485 loss_f: 3.4720232\n",
      "3492 Train Loss 17.768799\n",
      "3493 loss_D: 14.141316 loss_N: 0.14867724 loss_f: 3.472678\n",
      "3493 Train Loss 17.76267\n",
      "3494 loss_D: 14.138112 loss_N: 0.14922638 loss_f: 3.4694588\n",
      "3494 Train Loss 17.756798\n",
      "3495 loss_D: 14.128836 loss_N: 0.14971513 loss_f: 3.474336\n",
      "3495 Train Loss 17.752888\n",
      "3496 loss_D: 14.132441 loss_N: 0.14941011 loss_f: 3.4663076\n",
      "3496 Train Loss 17.748158\n",
      "3497 loss_D: 14.135006 loss_N: 0.14911237 loss_f: 3.458563\n",
      "3497 Train Loss 17.742682\n",
      "3498 loss_D: 14.133794 loss_N: 0.14933218 loss_f: 3.4541655\n",
      "3498 Train Loss 17.737291\n",
      "3499 loss_D: 14.137814 loss_N: 0.14944607 loss_f: 3.4447718\n",
      "3499 Train Loss 17.732033\n",
      "3500 loss_D: 14.133743 loss_N: 0.149834 loss_f: 3.4442804\n",
      "3500 Train Loss 17.727858\n",
      "3501 loss_D: 14.131481 loss_N: 0.14983362 loss_f: 3.4416525\n",
      "3501 Train Loss 17.722967\n",
      "3502 loss_D: 14.119053 loss_N: 0.14942943 loss_f: 3.4497905\n",
      "3502 Train Loss 17.718273\n",
      "3503 loss_D: 14.115187 loss_N: 0.14945531 loss_f: 3.449247\n",
      "3503 Train Loss 17.713888\n",
      "3504 loss_D: 14.11182 loss_N: 0.14848448 loss_f: 3.4490495\n",
      "3504 Train Loss 17.709354\n",
      "3505 loss_D: 14.111965 loss_N: 0.14798717 loss_f: 3.445725\n",
      "3505 Train Loss 17.705677\n",
      "3506 loss_D: 14.111282 loss_N: 0.14723846 loss_f: 3.439435\n",
      "3506 Train Loss 17.697956\n",
      "3507 loss_D: 14.108151 loss_N: 0.14624558 loss_f: 3.4369152\n",
      "3507 Train Loss 17.691313\n",
      "3508 loss_D: 14.112129 loss_N: 0.14538626 loss_f: 3.4283826\n",
      "3508 Train Loss 17.685898\n",
      "3509 loss_D: 14.103768 loss_N: 0.14512639 loss_f: 3.4303055\n",
      "3509 Train Loss 17.6792\n",
      "3510 loss_D: 14.103068 loss_N: 0.14497384 loss_f: 3.4265397\n",
      "3510 Train Loss 17.674582\n",
      "3511 loss_D: 14.098365 loss_N: 0.14506933 loss_f: 3.4268906\n",
      "3511 Train Loss 17.670324\n",
      "3512 loss_D: 14.105162 loss_N: 0.14491346 loss_f: 3.417674\n",
      "3512 Train Loss 17.66775\n",
      "3513 loss_D: 14.109821 loss_N: 0.14481077 loss_f: 3.4100547\n",
      "3513 Train Loss 17.664686\n",
      "3514 loss_D: 14.119331 loss_N: 0.14479728 loss_f: 3.397318\n",
      "3514 Train Loss 17.661446\n",
      "3515 loss_D: 14.125299 loss_N: 0.14480174 loss_f: 3.3882139\n",
      "3515 Train Loss 17.658316\n",
      "3516 loss_D: 14.1279745 loss_N: 0.14493777 loss_f: 3.3822172\n",
      "3516 Train Loss 17.655128\n",
      "3517 loss_D: 14.128128 loss_N: 0.14496507 loss_f: 3.3778687\n",
      "3517 Train Loss 17.650963\n",
      "3518 loss_D: 14.123522 loss_N: 0.14510097 loss_f: 3.3783967\n",
      "3518 Train Loss 17.647018\n",
      "3519 loss_D: 14.130995 loss_N: 0.14515014 loss_f: 3.366943\n",
      "3519 Train Loss 17.643087\n",
      "3520 loss_D: 14.127578 loss_N: 0.14479876 loss_f: 3.3663874\n",
      "3520 Train Loss 17.638763\n",
      "3521 loss_D: 14.124295 loss_N: 0.14394599 loss_f: 3.3620875\n",
      "3521 Train Loss 17.63033\n",
      "3522 loss_D: 14.122037 loss_N: 0.14331223 loss_f: 3.360314\n",
      "3522 Train Loss 17.625664\n",
      "3523 loss_D: 14.121907 loss_N: 0.14275949 loss_f: 3.3540688\n",
      "3523 Train Loss 17.618736\n",
      "3524 loss_D: 14.115176 loss_N: 0.14315939 loss_f: 3.3578172\n",
      "3524 Train Loss 17.616152\n",
      "3525 loss_D: 14.104397 loss_N: 0.14304098 loss_f: 3.3531237\n",
      "3525 Train Loss 17.600561\n",
      "3526 loss_D: 14.102831 loss_N: 0.1427913 loss_f: 3.3492303\n",
      "3526 Train Loss 17.594852\n",
      "3527 loss_D: 14.102175 loss_N: 0.14338557 loss_f: 3.3445585\n",
      "3527 Train Loss 17.590118\n",
      "3528 loss_D: 14.096173 loss_N: 0.14450766 loss_f: 3.343531\n",
      "3528 Train Loss 17.584211\n",
      "3529 loss_D: 14.093616 loss_N: 0.14508198 loss_f: 3.3425264\n",
      "3529 Train Loss 17.581223\n",
      "3530 loss_D: 14.09389 loss_N: 0.14530857 loss_f: 3.33933\n",
      "3530 Train Loss 17.57853\n",
      "3531 loss_D: 14.095109 loss_N: 0.14522956 loss_f: 3.3343313\n",
      "3531 Train Loss 17.574669\n",
      "3532 loss_D: 14.097581 loss_N: 0.14505151 loss_f: 3.3279\n",
      "3532 Train Loss 17.570534\n",
      "3533 loss_D: 14.092786 loss_N: 0.1449234 loss_f: 3.3292117\n",
      "3533 Train Loss 17.566921\n",
      "3534 loss_D: 14.096009 loss_N: 0.1449265 loss_f: 3.3267143\n",
      "3534 Train Loss 17.56765\n",
      "3535 loss_D: 14.094295 loss_N: 0.14491484 loss_f: 3.3242238\n",
      "3535 Train Loss 17.563433\n",
      "3536 loss_D: 14.091009 loss_N: 0.14476845 loss_f: 3.3223877\n",
      "3536 Train Loss 17.558167\n",
      "3537 loss_D: 14.076963 loss_N: 0.14434144 loss_f: 3.3251436\n",
      "3537 Train Loss 17.546448\n",
      "3538 loss_D: 14.071792 loss_N: 0.14339517 loss_f: 3.3225105\n",
      "3538 Train Loss 17.537697\n",
      "3539 loss_D: 14.055484 loss_N: 0.14283173 loss_f: 3.3291714\n",
      "3539 Train Loss 17.527487\n",
      "3540 loss_D: 14.060431 loss_N: 0.14209731 loss_f: 3.3202264\n",
      "3540 Train Loss 17.522755\n",
      "3541 loss_D: 14.0571375 loss_N: 0.14247575 loss_f: 3.3179898\n",
      "3541 Train Loss 17.517603\n",
      "3542 loss_D: 14.053853 loss_N: 0.1430103 loss_f: 3.315663\n",
      "3542 Train Loss 17.512526\n",
      "3543 loss_D: 14.046595 loss_N: 0.14366259 loss_f: 3.3116937\n",
      "3543 Train Loss 17.501951\n",
      "3544 loss_D: 14.033321 loss_N: 0.14443657 loss_f: 3.3123367\n",
      "3544 Train Loss 17.490095\n",
      "3545 loss_D: 14.020149 loss_N: 0.14452878 loss_f: 3.3170998\n",
      "3545 Train Loss 17.481777\n",
      "3546 loss_D: 14.00904 loss_N: 0.14383122 loss_f: 3.3224425\n",
      "3546 Train Loss 17.475313\n",
      "3547 loss_D: 13.955079 loss_N: 0.1416402 loss_f: 3.4302733\n",
      "3547 Train Loss 17.526993\n",
      "3548 loss_D: 13.99676 loss_N: 0.14334032 loss_f: 3.330618\n",
      "3548 Train Loss 17.470718\n",
      "3549 loss_D: 13.989487 loss_N: 0.1427552 loss_f: 3.3309374\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3549 Train Loss 17.46318\n",
      "3550 loss_D: 13.975942 loss_N: 0.1421468 loss_f: 3.3444464\n",
      "3550 Train Loss 17.462536\n",
      "3551 loss_D: 13.982212 loss_N: 0.14242887 loss_f: 3.3345573\n",
      "3551 Train Loss 17.459198\n",
      "3552 loss_D: 13.984415 loss_N: 0.14262596 loss_f: 3.3268425\n",
      "3552 Train Loss 17.453884\n",
      "3553 loss_D: 13.981294 loss_N: 0.14270662 loss_f: 3.3257797\n",
      "3553 Train Loss 17.44978\n",
      "3554 loss_D: 13.980122 loss_N: 0.14273255 loss_f: 3.3240654\n",
      "3554 Train Loss 17.44692\n",
      "3555 loss_D: 13.975912 loss_N: 0.14272797 loss_f: 3.3250835\n",
      "3555 Train Loss 17.443724\n",
      "3556 loss_D: 13.968141 loss_N: 0.14284104 loss_f: 3.3271651\n",
      "3556 Train Loss 17.438147\n",
      "3557 loss_D: 13.963687 loss_N: 0.14272296 loss_f: 3.326779\n",
      "3557 Train Loss 17.43319\n",
      "3558 loss_D: 13.952022 loss_N: 0.14308403 loss_f: 3.3346984\n",
      "3558 Train Loss 17.429804\n",
      "3559 loss_D: 13.955515 loss_N: 0.14316753 loss_f: 3.3267736\n",
      "3559 Train Loss 17.425457\n",
      "3560 loss_D: 13.949227 loss_N: 0.143258 loss_f: 3.3287804\n",
      "3560 Train Loss 17.421267\n",
      "3561 loss_D: 13.946884 loss_N: 0.14302272 loss_f: 3.3254266\n",
      "3561 Train Loss 17.415333\n",
      "3562 loss_D: 13.946064 loss_N: 0.1425806 loss_f: 3.320289\n",
      "3562 Train Loss 17.408934\n",
      "3563 loss_D: 13.947332 loss_N: 0.14173418 loss_f: 3.3149765\n",
      "3563 Train Loss 17.404043\n",
      "3564 loss_D: 13.950134 loss_N: 0.14101833 loss_f: 3.3084202\n",
      "3564 Train Loss 17.399572\n",
      "3565 loss_D: 13.947822 loss_N: 0.14061616 loss_f: 3.3064075\n",
      "3565 Train Loss 17.394846\n",
      "3566 loss_D: 13.944287 loss_N: 0.14042082 loss_f: 3.3054817\n",
      "3566 Train Loss 17.39019\n",
      "3567 loss_D: 13.935501 loss_N: 0.14075327 loss_f: 3.3093112\n",
      "3567 Train Loss 17.385565\n",
      "3568 loss_D: 13.923924 loss_N: 0.14112034 loss_f: 3.3156772\n",
      "3568 Train Loss 17.380722\n",
      "3569 loss_D: 13.91324 loss_N: 0.14150146 loss_f: 3.322081\n",
      "3569 Train Loss 17.376823\n",
      "3570 loss_D: 13.906753 loss_N: 0.14161545 loss_f: 3.3250225\n",
      "3570 Train Loss 17.37339\n",
      "3571 loss_D: 13.902478 loss_N: 0.14153583 loss_f: 3.325351\n",
      "3571 Train Loss 17.369366\n",
      "3572 loss_D: 13.898881 loss_N: 0.14150591 loss_f: 3.323749\n",
      "3572 Train Loss 17.364136\n",
      "3573 loss_D: 13.90089 loss_N: 0.1411372 loss_f: 3.3162713\n",
      "3573 Train Loss 17.3583\n",
      "3574 loss_D: 13.89537 loss_N: 0.1414209 loss_f: 3.3160691\n",
      "3574 Train Loss 17.35286\n",
      "3575 loss_D: 13.889118 loss_N: 0.14192702 loss_f: 3.3148882\n",
      "3575 Train Loss 17.345934\n",
      "3576 loss_D: 13.877359 loss_N: 0.14289989 loss_f: 3.3176045\n",
      "3576 Train Loss 17.337864\n",
      "3577 loss_D: 13.866496 loss_N: 0.14332153 loss_f: 3.3239577\n",
      "3577 Train Loss 17.333776\n",
      "3578 loss_D: 13.848907 loss_N: 0.14390633 loss_f: 3.3354836\n",
      "3578 Train Loss 17.328297\n",
      "3579 loss_D: 13.841071 loss_N: 0.14379127 loss_f: 3.3389428\n",
      "3579 Train Loss 17.323805\n",
      "3580 loss_D: 13.822873 loss_N: 0.14372481 loss_f: 3.349964\n",
      "3580 Train Loss 17.31656\n",
      "3581 loss_D: 13.766034 loss_N: 0.13928258 loss_f: 3.5473187\n",
      "3581 Train Loss 17.452635\n",
      "3582 loss_D: 13.8158045 loss_N: 0.14314555 loss_f: 3.355255\n",
      "3582 Train Loss 17.314205\n",
      "3583 loss_D: 13.81545 loss_N: 0.14292024 loss_f: 3.3519297\n",
      "3583 Train Loss 17.310299\n",
      "3584 loss_D: 13.795946 loss_N: 0.1426383 loss_f: 3.363339\n",
      "3584 Train Loss 17.301924\n",
      "3585 loss_D: 13.783142 loss_N: 0.14237311 loss_f: 3.3706856\n",
      "3585 Train Loss 17.2962\n",
      "3586 loss_D: 13.734246 loss_N: 0.14211947 loss_f: 3.4092562\n",
      "3586 Train Loss 17.285622\n",
      "3587 loss_D: 13.7063055 loss_N: 0.14185278 loss_f: 3.4276905\n",
      "3587 Train Loss 17.275848\n",
      "3588 loss_D: 13.670886 loss_N: 0.1417365 loss_f: 3.4558167\n",
      "3588 Train Loss 17.268438\n",
      "3589 loss_D: 13.668302 loss_N: 0.14188571 loss_f: 3.4521496\n",
      "3589 Train Loss 17.262337\n",
      "3590 loss_D: 13.66727 loss_N: 0.14171739 loss_f: 3.448908\n",
      "3590 Train Loss 17.257895\n",
      "3591 loss_D: 13.669188 loss_N: 0.14146309 loss_f: 3.4417312\n",
      "3591 Train Loss 17.252382\n",
      "3592 loss_D: 13.67327 loss_N: 0.14130151 loss_f: 3.4341133\n",
      "3592 Train Loss 17.248684\n",
      "3593 loss_D: 13.671672 loss_N: 0.14101185 loss_f: 3.4321315\n",
      "3593 Train Loss 17.244816\n",
      "3594 loss_D: 13.678154 loss_N: 0.13962509 loss_f: 3.427828\n",
      "3594 Train Loss 17.245607\n",
      "3595 loss_D: 13.674612 loss_N: 0.14031611 loss_f: 3.4274266\n",
      "3595 Train Loss 17.242355\n",
      "3596 loss_D: 13.674062 loss_N: 0.14024514 loss_f: 3.4248092\n",
      "3596 Train Loss 17.239117\n",
      "3597 loss_D: 13.666762 loss_N: 0.14022082 loss_f: 3.4312842\n",
      "3597 Train Loss 17.238268\n",
      "3598 loss_D: 13.671267 loss_N: 0.14012161 loss_f: 3.424065\n",
      "3598 Train Loss 17.235453\n",
      "3599 loss_D: 13.673634 loss_N: 0.1399823 loss_f: 3.4207952\n",
      "3599 Train Loss 17.234411\n",
      "3600 loss_D: 13.675894 loss_N: 0.13981505 loss_f: 3.417382\n",
      "3600 Train Loss 17.233091\n",
      "3601 loss_D: 13.676756 loss_N: 0.1396112 loss_f: 3.4154198\n",
      "3601 Train Loss 17.231787\n",
      "3602 loss_D: 13.675562 loss_N: 0.13929267 loss_f: 3.4143307\n",
      "3602 Train Loss 17.229185\n",
      "3603 loss_D: 13.671784 loss_N: 0.13877836 loss_f: 3.415003\n",
      "3603 Train Loss 17.225567\n",
      "3604 loss_D: 13.681912 loss_N: 0.13824946 loss_f: 3.417305\n",
      "3604 Train Loss 17.237467\n",
      "3605 loss_D: 13.674015 loss_N: 0.1386304 loss_f: 3.411524\n",
      "3605 Train Loss 17.22417\n",
      "3606 loss_D: 13.6663885 loss_N: 0.13808087 loss_f: 3.4149272\n",
      "3606 Train Loss 17.219397\n",
      "3607 loss_D: 13.661765 loss_N: 0.137585 loss_f: 3.41462\n",
      "3607 Train Loss 17.21397\n",
      "3608 loss_D: 13.658622 loss_N: 0.13742603 loss_f: 3.4134264\n",
      "3608 Train Loss 17.209475\n",
      "3609 loss_D: 13.662188 loss_N: 0.13744858 loss_f: 3.406655\n",
      "3609 Train Loss 17.206291\n",
      "3610 loss_D: 13.666562 loss_N: 0.13756365 loss_f: 3.3993936\n",
      "3610 Train Loss 17.20352\n",
      "3611 loss_D: 13.6638155 loss_N: 0.13764855 loss_f: 3.4035015\n",
      "3611 Train Loss 17.204966\n",
      "3612 loss_D: 13.6652775 loss_N: 0.13759743 loss_f: 3.3986664\n",
      "3612 Train Loss 17.201542\n",
      "3613 loss_D: 13.664537 loss_N: 0.13774002 loss_f: 3.3954484\n",
      "3613 Train Loss 17.197725\n",
      "3614 loss_D: 13.659256 loss_N: 0.13783488 loss_f: 3.397661\n",
      "3614 Train Loss 17.194752\n",
      "3615 loss_D: 13.6542635 loss_N: 0.13781105 loss_f: 3.3992088\n",
      "3615 Train Loss 17.191282\n",
      "3616 loss_D: 13.646644 loss_N: 0.13787743 loss_f: 3.402208\n",
      "3616 Train Loss 17.18673\n",
      "3617 loss_D: 13.638795 loss_N: 0.13793366 loss_f: 3.4031315\n",
      "3617 Train Loss 17.17986\n",
      "3618 loss_D: 13.653834 loss_N: 0.13831206 loss_f: 3.3899262\n",
      "3618 Train Loss 17.182074\n",
      "3619 loss_D: 13.645068 loss_N: 0.138061 loss_f: 3.3937438\n",
      "3619 Train Loss 17.176872\n",
      "3620 loss_D: 13.643318 loss_N: 0.13805066 loss_f: 3.3910558\n",
      "3620 Train Loss 17.172424\n",
      "3621 loss_D: 13.643903 loss_N: 0.13875464 loss_f: 3.387253\n",
      "3621 Train Loss 17.16991\n",
      "3622 loss_D: 13.648109 loss_N: 0.138274 loss_f: 3.3797772\n",
      "3622 Train Loss 17.16616\n",
      "3623 loss_D: 13.647971 loss_N: 0.1382186 loss_f: 3.3771539\n",
      "3623 Train Loss 17.163343\n",
      "3624 loss_D: 13.65431 loss_N: 0.13836266 loss_f: 3.3641844\n",
      "3624 Train Loss 17.156857\n",
      "3625 loss_D: 13.656715 loss_N: 0.13899422 loss_f: 3.3641045\n",
      "3625 Train Loss 17.159815\n",
      "3626 loss_D: 13.655186 loss_N: 0.13859442 loss_f: 3.361042\n",
      "3626 Train Loss 17.154823\n",
      "3627 loss_D: 13.647249 loss_N: 0.13940135 loss_f: 3.3639133\n",
      "3627 Train Loss 17.150564\n",
      "3628 loss_D: 13.649006 loss_N: 0.14009646 loss_f: 3.3584461\n",
      "3628 Train Loss 17.147549\n",
      "3629 loss_D: 13.643347 loss_N: 0.14071819 loss_f: 3.3606803\n",
      "3629 Train Loss 17.144745\n",
      "3630 loss_D: 13.628009 loss_N: 0.14140917 loss_f: 3.3726645\n",
      "3630 Train Loss 17.142082\n",
      "3631 loss_D: 13.626102 loss_N: 0.14196381 loss_f: 3.3703022\n",
      "3631 Train Loss 17.138369\n",
      "3632 loss_D: 13.619822 loss_N: 0.14266336 loss_f: 3.3716922\n",
      "3632 Train Loss 17.134176\n",
      "3633 loss_D: 13.614432 loss_N: 0.14352909 loss_f: 3.372278\n",
      "3633 Train Loss 17.13024\n",
      "3634 loss_D: 13.600428 loss_N: 0.14510813 loss_f: 3.378068\n",
      "3634 Train Loss 17.123604\n",
      "3635 loss_D: 13.584286 loss_N: 0.14634359 loss_f: 3.387271\n",
      "3635 Train Loss 17.1179\n",
      "3636 loss_D: 13.576474 loss_N: 0.14657918 loss_f: 3.3886406\n",
      "3636 Train Loss 17.111694\n",
      "3637 loss_D: 13.568287 loss_N: 0.14560394 loss_f: 3.3918703\n",
      "3637 Train Loss 17.10576\n",
      "3638 loss_D: 13.562067 loss_N: 0.14394636 loss_f: 3.3935308\n",
      "3638 Train Loss 17.099545\n",
      "3639 loss_D: 13.557826 loss_N: 0.14313571 loss_f: 3.393714\n",
      "3639 Train Loss 17.094677\n",
      "3640 loss_D: 13.560643 loss_N: 0.14293915 loss_f: 3.3878136\n",
      "3640 Train Loss 17.091396\n",
      "3641 loss_D: 13.559796 loss_N: 0.14315215 loss_f: 3.384339\n",
      "3641 Train Loss 17.087288\n",
      "3642 loss_D: 13.558704 loss_N: 0.1436165 loss_f: 3.3813663\n",
      "3642 Train Loss 17.083687\n",
      "3643 loss_D: 13.552346 loss_N: 0.14420983 loss_f: 3.3846319\n",
      "3643 Train Loss 17.081188\n",
      "3644 loss_D: 13.547441 loss_N: 0.14467612 loss_f: 3.3861475\n",
      "3644 Train Loss 17.078264\n",
      "3645 loss_D: 13.535421 loss_N: 0.14512163 loss_f: 3.3950288\n",
      "3645 Train Loss 17.075571\n",
      "3646 loss_D: 13.528154 loss_N: 0.14521465 loss_f: 3.3992312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3646 Train Loss 17.072601\n",
      "3647 loss_D: 13.514059 loss_N: 0.14506957 loss_f: 3.4102468\n",
      "3647 Train Loss 17.069374\n",
      "3648 loss_D: 13.516259 loss_N: 0.14483261 loss_f: 3.404284\n",
      "3648 Train Loss 17.065376\n",
      "3649 loss_D: 13.509676 loss_N: 0.14511786 loss_f: 3.4070914\n",
      "3649 Train Loss 17.061886\n",
      "3650 loss_D: 13.508665 loss_N: 0.14551418 loss_f: 3.4036164\n",
      "3650 Train Loss 17.057796\n",
      "3651 loss_D: 13.499656 loss_N: 0.14590256 loss_f: 3.4087272\n",
      "3651 Train Loss 17.054285\n",
      "3652 loss_D: 13.495404 loss_N: 0.14627087 loss_f: 3.4085386\n",
      "3652 Train Loss 17.050213\n",
      "3653 loss_D: 13.481587 loss_N: 0.14627536 loss_f: 3.418093\n",
      "3653 Train Loss 17.045956\n",
      "3654 loss_D: 13.47399 loss_N: 0.1459714 loss_f: 3.4222858\n",
      "3654 Train Loss 17.042248\n",
      "3655 loss_D: 13.468386 loss_N: 0.14515896 loss_f: 3.4251106\n",
      "3655 Train Loss 17.038654\n",
      "3656 loss_D: 13.470366 loss_N: 0.14406438 loss_f: 3.4211257\n",
      "3656 Train Loss 17.035555\n",
      "3657 loss_D: 13.463939 loss_N: 0.14342535 loss_f: 3.4223726\n",
      "3657 Train Loss 17.029736\n",
      "3658 loss_D: 13.469634 loss_N: 0.14283422 loss_f: 3.4132202\n",
      "3658 Train Loss 17.025688\n",
      "3659 loss_D: 13.47212 loss_N: 0.14206803 loss_f: 3.4056883\n",
      "3659 Train Loss 17.019876\n",
      "3660 loss_D: 13.46629 loss_N: 0.14232925 loss_f: 3.4059644\n",
      "3660 Train Loss 17.014584\n",
      "3661 loss_D: 13.457118 loss_N: 0.1425114 loss_f: 3.4081912\n",
      "3661 Train Loss 17.00782\n",
      "3662 loss_D: 13.437375 loss_N: 0.14213145 loss_f: 3.420703\n",
      "3662 Train Loss 17.00021\n",
      "3663 loss_D: 13.417225 loss_N: 0.14244536 loss_f: 3.4515295\n",
      "3663 Train Loss 17.0112\n",
      "3664 loss_D: 13.430481 loss_N: 0.14222668 loss_f: 3.4237406\n",
      "3664 Train Loss 16.996449\n",
      "3665 loss_D: 13.4190855 loss_N: 0.14167248 loss_f: 3.429829\n",
      "3665 Train Loss 16.990587\n",
      "3666 loss_D: 13.400653 loss_N: 0.14155927 loss_f: 3.4424045\n",
      "3666 Train Loss 16.984617\n",
      "3667 loss_D: 13.394808 loss_N: 0.14126642 loss_f: 3.4446082\n",
      "3667 Train Loss 16.980682\n",
      "3668 loss_D: 13.369765 loss_N: 0.1419289 loss_f: 3.4664814\n",
      "3668 Train Loss 16.978176\n",
      "3669 loss_D: 13.361821 loss_N: 0.14317968 loss_f: 3.468739\n",
      "3669 Train Loss 16.97374\n",
      "3670 loss_D: 13.374387 loss_N: 0.14289805 loss_f: 3.454026\n",
      "3670 Train Loss 16.97131\n",
      "3671 loss_D: 13.372174 loss_N: 0.1427549 loss_f: 3.4554362\n",
      "3671 Train Loss 16.970366\n",
      "3672 loss_D: 13.372736 loss_N: 0.14269519 loss_f: 3.4525027\n",
      "3672 Train Loss 16.967934\n",
      "3673 loss_D: 13.368334 loss_N: 0.14241451 loss_f: 3.4534242\n",
      "3673 Train Loss 16.964172\n",
      "3674 loss_D: 13.370308 loss_N: 0.14201741 loss_f: 3.4484131\n",
      "3674 Train Loss 16.96074\n",
      "3675 loss_D: 13.3685875 loss_N: 0.1416571 loss_f: 3.4480445\n",
      "3675 Train Loss 16.958288\n",
      "3676 loss_D: 13.370511 loss_N: 0.14127196 loss_f: 3.443231\n",
      "3676 Train Loss 16.955013\n",
      "3677 loss_D: 13.368837 loss_N: 0.14093377 loss_f: 3.4403038\n",
      "3677 Train Loss 16.950075\n",
      "3678 loss_D: 13.383195 loss_N: 0.1395887 loss_f: 3.4274788\n",
      "3678 Train Loss 16.950262\n",
      "3679 loss_D: 13.3759985 loss_N: 0.14024325 loss_f: 3.4297695\n",
      "3679 Train Loss 16.94601\n",
      "3680 loss_D: 13.371601 loss_N: 0.13981833 loss_f: 3.4257185\n",
      "3680 Train Loss 16.937138\n",
      "3681 loss_D: 13.35915 loss_N: 0.14001223 loss_f: 3.4322941\n",
      "3681 Train Loss 16.931456\n",
      "3682 loss_D: 13.364062 loss_N: 0.13936085 loss_f: 3.4199812\n",
      "3682 Train Loss 16.923405\n",
      "3683 loss_D: 13.366953 loss_N: 0.13981006 loss_f: 3.4126399\n",
      "3683 Train Loss 16.919403\n",
      "3684 loss_D: 13.366906 loss_N: 0.13928701 loss_f: 3.4091852\n",
      "3684 Train Loss 16.915379\n",
      "3685 loss_D: 13.365857 loss_N: 0.1394999 loss_f: 3.406671\n",
      "3685 Train Loss 16.912027\n",
      "3686 loss_D: 13.364403 loss_N: 0.13950145 loss_f: 3.4049718\n",
      "3686 Train Loss 16.908876\n",
      "3687 loss_D: 13.359134 loss_N: 0.13881959 loss_f: 3.40625\n",
      "3687 Train Loss 16.904203\n",
      "3688 loss_D: 13.354451 loss_N: 0.13884993 loss_f: 3.4055696\n",
      "3688 Train Loss 16.89887\n",
      "3689 loss_D: 13.33725 loss_N: 0.13878578 loss_f: 3.4090006\n",
      "3689 Train Loss 16.885036\n",
      "3690 loss_D: 13.31772 loss_N: 0.13845879 loss_f: 3.415547\n",
      "3690 Train Loss 16.871727\n",
      "3691 loss_D: 13.291395 loss_N: 0.13689806 loss_f: 3.429107\n",
      "3691 Train Loss 16.8574\n",
      "3692 loss_D: 13.270124 loss_N: 0.13669728 loss_f: 3.441964\n",
      "3692 Train Loss 16.848785\n",
      "3693 loss_D: 13.26299 loss_N: 0.13638614 loss_f: 3.4402978\n",
      "3693 Train Loss 16.839674\n",
      "3694 loss_D: 13.236828 loss_N: 0.13617119 loss_f: 3.4588645\n",
      "3694 Train Loss 16.831863\n",
      "3695 loss_D: 13.241466 loss_N: 0.13598338 loss_f: 3.4485672\n",
      "3695 Train Loss 16.826015\n",
      "3696 loss_D: 13.234128 loss_N: 0.13623662 loss_f: 3.452183\n",
      "3696 Train Loss 16.822548\n",
      "3697 loss_D: 13.233412 loss_N: 0.13663097 loss_f: 3.4486506\n",
      "3697 Train Loss 16.818693\n",
      "3698 loss_D: 13.206733 loss_N: 0.13886875 loss_f: 3.4901772\n",
      "3698 Train Loss 16.835777\n",
      "3699 loss_D: 13.228556 loss_N: 0.13699263 loss_f: 3.452449\n",
      "3699 Train Loss 16.817997\n",
      "3700 loss_D: 13.22913 loss_N: 0.1372994 loss_f: 3.4492085\n",
      "3700 Train Loss 16.815638\n",
      "3701 loss_D: 13.230016 loss_N: 0.13769968 loss_f: 3.4447985\n",
      "3701 Train Loss 16.812515\n",
      "3702 loss_D: 13.228056 loss_N: 0.13822636 loss_f: 3.441982\n",
      "3702 Train Loss 16.808264\n",
      "3703 loss_D: 13.224632 loss_N: 0.13818422 loss_f: 3.4401417\n",
      "3703 Train Loss 16.80296\n",
      "3704 loss_D: 13.213136 loss_N: 0.13874522 loss_f: 3.4447865\n",
      "3704 Train Loss 16.796667\n",
      "3705 loss_D: 13.195234 loss_N: 0.1336691 loss_f: 3.5346828\n",
      "3705 Train Loss 16.863586\n",
      "3706 loss_D: 13.209653 loss_N: 0.13768847 loss_f: 3.4461734\n",
      "3706 Train Loss 16.793514\n",
      "3707 loss_D: 13.205962 loss_N: 0.13673516 loss_f: 3.456846\n",
      "3707 Train Loss 16.799543\n",
      "3708 loss_D: 13.208259 loss_N: 0.1373407 loss_f: 3.445386\n",
      "3708 Train Loss 16.790985\n",
      "3709 loss_D: 13.197537 loss_N: 0.13736576 loss_f: 3.4490976\n",
      "3709 Train Loss 16.784\n",
      "3710 loss_D: 13.183595 loss_N: 0.13777646 loss_f: 3.455733\n",
      "3710 Train Loss 16.777103\n",
      "3711 loss_D: 13.171988 loss_N: 0.13817339 loss_f: 3.463272\n",
      "3711 Train Loss 16.773434\n",
      "3712 loss_D: 13.157159 loss_N: 0.13861385 loss_f: 3.4728484\n",
      "3712 Train Loss 16.768621\n",
      "3713 loss_D: 13.131848 loss_N: 0.13749 loss_f: 3.4994318\n",
      "3713 Train Loss 16.76877\n",
      "3714 loss_D: 13.144734 loss_N: 0.13805962 loss_f: 3.4842303\n",
      "3714 Train Loss 16.767025\n",
      "3715 loss_D: 13.134904 loss_N: 0.13860683 loss_f: 3.4908898\n",
      "3715 Train Loss 16.7644\n",
      "3716 loss_D: 13.120649 loss_N: 0.13894475 loss_f: 3.5003164\n",
      "3716 Train Loss 16.75991\n",
      "3717 loss_D: 13.111674 loss_N: 0.1386043 loss_f: 3.5065968\n",
      "3717 Train Loss 16.756876\n",
      "3718 loss_D: 13.097124 loss_N: 0.13897412 loss_f: 3.5155756\n",
      "3718 Train Loss 16.751675\n",
      "3719 loss_D: 13.083728 loss_N: 0.13797484 loss_f: 3.5310538\n",
      "3719 Train Loss 16.752756\n",
      "3720 loss_D: 13.090868 loss_N: 0.13851462 loss_f: 3.5196178\n",
      "3720 Train Loss 16.749\n",
      "3721 loss_D: 13.070691 loss_N: 0.13841482 loss_f: 3.5360444\n",
      "3721 Train Loss 16.74515\n",
      "3722 loss_D: 13.078127 loss_N: 0.13905108 loss_f: 3.5219893\n",
      "3722 Train Loss 16.739168\n",
      "3723 loss_D: 13.063976 loss_N: 0.13930678 loss_f: 3.533128\n",
      "3723 Train Loss 16.736412\n",
      "3724 loss_D: 13.074956 loss_N: 0.139008 loss_f: 3.5203695\n",
      "3724 Train Loss 16.734333\n",
      "3725 loss_D: 13.074302 loss_N: 0.13863279 loss_f: 3.5191\n",
      "3725 Train Loss 16.732035\n",
      "3726 loss_D: 13.06888 loss_N: 0.13749768 loss_f: 3.522636\n",
      "3726 Train Loss 16.729013\n",
      "3727 loss_D: 13.059956 loss_N: 0.13744463 loss_f: 3.5303652\n",
      "3727 Train Loss 16.727766\n",
      "3728 loss_D: 13.050295 loss_N: 0.13738061 loss_f: 3.537557\n",
      "3728 Train Loss 16.725233\n",
      "3729 loss_D: 13.041287 loss_N: 0.13745046 loss_f: 3.5424225\n",
      "3729 Train Loss 16.72116\n",
      "3730 loss_D: 13.037946 loss_N: 0.13747557 loss_f: 3.5418787\n",
      "3730 Train Loss 16.7173\n",
      "3731 loss_D: 13.044414 loss_N: 0.13835037 loss_f: 3.5302217\n",
      "3731 Train Loss 16.712986\n",
      "3732 loss_D: 13.048738 loss_N: 0.1383147 loss_f: 3.5214596\n",
      "3732 Train Loss 16.708511\n",
      "3733 loss_D: 13.053749 loss_N: 0.13832209 loss_f: 3.5122976\n",
      "3733 Train Loss 16.704369\n",
      "3734 loss_D: 13.056257 loss_N: 0.13786131 loss_f: 3.5061452\n",
      "3734 Train Loss 16.700264\n",
      "3735 loss_D: 13.055209 loss_N: 0.13717565 loss_f: 3.505854\n",
      "3735 Train Loss 16.698238\n",
      "3736 loss_D: 13.051124 loss_N: 0.13594167 loss_f: 3.508182\n",
      "3736 Train Loss 16.695248\n",
      "3737 loss_D: 13.048973 loss_N: 0.13493612 loss_f: 3.508236\n",
      "3737 Train Loss 16.692146\n",
      "3738 loss_D: 13.046139 loss_N: 0.1338869 loss_f: 3.507253\n",
      "3738 Train Loss 16.687279\n",
      "3739 loss_D: 13.029314 loss_N: 0.1320011 loss_f: 3.532879\n",
      "3739 Train Loss 16.694195\n",
      "3740 loss_D: 13.040208 loss_N: 0.13320388 loss_f: 3.5112803\n",
      "3740 Train Loss 16.684692\n",
      "3741 loss_D: 13.04519 loss_N: 0.13322197 loss_f: 3.5026703\n",
      "3741 Train Loss 16.681082\n",
      "3742 loss_D: 13.047865 loss_N: 0.13351545 loss_f: 3.492941\n",
      "3742 Train Loss 16.67432\n",
      "3743 loss_D: 13.050468 loss_N: 0.13351013 loss_f: 3.4845328\n",
      "3743 Train Loss 16.668512\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3744 loss_D: 13.046229 loss_N: 0.13347979 loss_f: 3.4800186\n",
      "3744 Train Loss 16.659729\n",
      "3745 loss_D: 13.043972 loss_N: 0.13364792 loss_f: 3.4736273\n",
      "3745 Train Loss 16.651247\n",
      "3746 loss_D: 13.04088 loss_N: 0.13393565 loss_f: 3.4696198\n",
      "3746 Train Loss 16.644436\n",
      "3747 loss_D: 13.033795 loss_N: 0.1339098 loss_f: 3.4689975\n",
      "3747 Train Loss 16.636703\n",
      "3748 loss_D: 13.025521 loss_N: 0.13385838 loss_f: 3.4730468\n",
      "3748 Train Loss 16.632427\n",
      "3749 loss_D: 13.022129 loss_N: 0.13378482 loss_f: 3.4729874\n",
      "3749 Train Loss 16.628902\n",
      "3750 loss_D: 13.021678 loss_N: 0.13357005 loss_f: 3.4673698\n",
      "3750 Train Loss 16.622618\n",
      "3751 loss_D: 13.027167 loss_N: 0.13353132 loss_f: 3.4538007\n",
      "3751 Train Loss 16.6145\n",
      "3752 loss_D: 13.039714 loss_N: 0.13364798 loss_f: 3.433786\n",
      "3752 Train Loss 16.607147\n",
      "3753 loss_D: 13.055224 loss_N: 0.13580479 loss_f: 3.4498858\n",
      "3753 Train Loss 16.640915\n",
      "3754 loss_D: 13.042338 loss_N: 0.13407072 loss_f: 3.427505\n",
      "3754 Train Loss 16.603914\n",
      "3755 loss_D: 13.043427 loss_N: 0.1346109 loss_f: 3.421967\n",
      "3755 Train Loss 16.600006\n",
      "3756 loss_D: 13.038463 loss_N: 0.13554816 loss_f: 3.4210296\n",
      "3756 Train Loss 16.595041\n",
      "3757 loss_D: 13.031535 loss_N: 0.1362684 loss_f: 3.4232783\n",
      "3757 Train Loss 16.591082\n",
      "3758 loss_D: 13.017129 loss_N: 0.13702536 loss_f: 3.4310584\n",
      "3758 Train Loss 16.585213\n",
      "3759 loss_D: 13.005399 loss_N: 0.1371509 loss_f: 3.4381428\n",
      "3759 Train Loss 16.580692\n",
      "3760 loss_D: 12.993584 loss_N: 0.13687783 loss_f: 3.4455967\n",
      "3760 Train Loss 16.576057\n",
      "3761 loss_D: 12.982514 loss_N: 0.13632771 loss_f: 3.4522047\n",
      "3761 Train Loss 16.571047\n",
      "3762 loss_D: 12.965899 loss_N: 0.13575359 loss_f: 3.4663522\n",
      "3762 Train Loss 16.568005\n",
      "3763 loss_D: 12.952328 loss_N: 0.13540643 loss_f: 3.4755108\n",
      "3763 Train Loss 16.563246\n",
      "3764 loss_D: 12.959229 loss_N: 0.13591407 loss_f: 3.4633045\n",
      "3764 Train Loss 16.558449\n",
      "3765 loss_D: 12.933317 loss_N: 0.13746437 loss_f: 3.4814045\n",
      "3765 Train Loss 16.552187\n",
      "3766 loss_D: 12.934013 loss_N: 0.13755757 loss_f: 3.4767113\n",
      "3766 Train Loss 16.548283\n",
      "3767 loss_D: 12.914624 loss_N: 0.13836847 loss_f: 3.490486\n",
      "3767 Train Loss 16.543478\n",
      "3768 loss_D: 12.8673935 loss_N: 0.13749953 loss_f: 3.5343795\n",
      "3768 Train Loss 16.539272\n",
      "3769 loss_D: 12.875334 loss_N: 0.13792683 loss_f: 3.5200195\n",
      "3769 Train Loss 16.53328\n",
      "3770 loss_D: 12.877657 loss_N: 0.1379881 loss_f: 3.5152218\n",
      "3770 Train Loss 16.530867\n",
      "3771 loss_D: 12.872261 loss_N: 0.13800895 loss_f: 3.5182955\n",
      "3771 Train Loss 16.528566\n",
      "3772 loss_D: 12.870567 loss_N: 0.13817409 loss_f: 3.5179129\n",
      "3772 Train Loss 16.526653\n",
      "3773 loss_D: 12.864765 loss_N: 0.13857558 loss_f: 3.5212\n",
      "3773 Train Loss 16.52454\n",
      "3774 loss_D: 12.86587 loss_N: 0.13904123 loss_f: 3.5186045\n",
      "3774 Train Loss 16.523516\n",
      "3775 loss_D: 12.865482 loss_N: 0.13964514 loss_f: 3.5166354\n",
      "3775 Train Loss 16.521763\n",
      "3776 loss_D: 12.863738 loss_N: 0.14039883 loss_f: 3.5158405\n",
      "3776 Train Loss 16.519978\n",
      "3777 loss_D: 12.862111 loss_N: 0.14089444 loss_f: 3.5150015\n",
      "3777 Train Loss 16.518007\n",
      "3778 loss_D: 12.85278 loss_N: 0.14141855 loss_f: 3.5213757\n",
      "3778 Train Loss 16.515575\n",
      "3779 loss_D: 12.852095 loss_N: 0.14115945 loss_f: 3.5213413\n",
      "3779 Train Loss 16.514595\n",
      "3780 loss_D: 12.845244 loss_N: 0.14102194 loss_f: 3.5269253\n",
      "3780 Train Loss 16.513191\n",
      "3781 loss_D: 12.844997 loss_N: 0.14050706 loss_f: 3.5265725\n",
      "3781 Train Loss 16.512077\n",
      "3782 loss_D: 12.844372 loss_N: 0.1399915 loss_f: 3.525375\n",
      "3782 Train Loss 16.509739\n",
      "3783 loss_D: 12.848564 loss_N: 0.13912694 loss_f: 3.5193381\n",
      "3783 Train Loss 16.507029\n",
      "3784 loss_D: 12.855955 loss_N: 0.13775149 loss_f: 3.5211601\n",
      "3784 Train Loss 16.514866\n",
      "3785 loss_D: 12.850789 loss_N: 0.13864826 loss_f: 3.5152805\n",
      "3785 Train Loss 16.504717\n",
      "3786 loss_D: 12.854114 loss_N: 0.13857819 loss_f: 3.5093608\n",
      "3786 Train Loss 16.502052\n",
      "3787 loss_D: 12.8576765 loss_N: 0.13848464 loss_f: 3.5023737\n",
      "3787 Train Loss 16.498535\n",
      "3788 loss_D: 12.856279 loss_N: 0.13828808 loss_f: 3.5023053\n",
      "3788 Train Loss 16.496874\n",
      "3789 loss_D: 12.847566 loss_N: 0.13775428 loss_f: 3.5079968\n",
      "3789 Train Loss 16.493317\n",
      "3790 loss_D: 12.839326 loss_N: 0.13764662 loss_f: 3.5129035\n",
      "3790 Train Loss 16.489876\n",
      "3791 loss_D: 12.833562 loss_N: 0.13729227 loss_f: 3.514971\n",
      "3791 Train Loss 16.485825\n",
      "3792 loss_D: 12.820537 loss_N: 0.13740875 loss_f: 3.5244303\n",
      "3792 Train Loss 16.482376\n",
      "3793 loss_D: 12.829483 loss_N: 0.13701035 loss_f: 3.509139\n",
      "3793 Train Loss 16.475632\n",
      "3794 loss_D: 12.836522 loss_N: 0.13706814 loss_f: 3.4953644\n",
      "3794 Train Loss 16.468954\n",
      "3795 loss_D: 12.846386 loss_N: 0.13711256 loss_f: 3.478739\n",
      "3795 Train Loss 16.462238\n",
      "3796 loss_D: 12.849106 loss_N: 0.13735951 loss_f: 3.4688756\n",
      "3796 Train Loss 16.455341\n",
      "3797 loss_D: 12.846006 loss_N: 0.13745658 loss_f: 3.4638038\n",
      "3797 Train Loss 16.447268\n",
      "3798 loss_D: 12.836125 loss_N: 0.13753574 loss_f: 3.4646738\n",
      "3798 Train Loss 16.438335\n",
      "3799 loss_D: 12.825844 loss_N: 0.13739346 loss_f: 3.465385\n",
      "3799 Train Loss 16.428621\n",
      "3800 loss_D: 12.8131895 loss_N: 0.13715243 loss_f: 3.4673133\n",
      "3800 Train Loss 16.417656\n",
      "3801 loss_D: 12.792191 loss_N: 0.13713345 loss_f: 3.485088\n",
      "3801 Train Loss 16.414412\n",
      "3802 loss_D: 12.773957 loss_N: 0.1366882 loss_f: 3.5258265\n",
      "3802 Train Loss 16.436472\n",
      "3803 loss_D: 12.785536 loss_N: 0.13694437 loss_f: 3.482718\n",
      "3803 Train Loss 16.4052\n",
      "3804 loss_D: 12.784612 loss_N: 0.13691089 loss_f: 3.476904\n",
      "3804 Train Loss 16.398426\n",
      "3805 loss_D: 12.788432 loss_N: 0.13666767 loss_f: 3.4668257\n",
      "3805 Train Loss 16.391926\n",
      "3806 loss_D: 12.767125 loss_N: 0.13577029 loss_f: 3.4834366\n",
      "3806 Train Loss 16.386332\n",
      "3807 loss_D: 12.77186 loss_N: 0.13539995 loss_f: 3.4732835\n",
      "3807 Train Loss 16.380543\n",
      "3808 loss_D: 12.765809 loss_N: 0.13513474 loss_f: 3.4743967\n",
      "3808 Train Loss 16.37534\n",
      "3809 loss_D: 12.737236 loss_N: 0.13603334 loss_f: 3.62416\n",
      "3809 Train Loss 16.497429\n",
      "3810 loss_D: 12.762903 loss_N: 0.13519172 loss_f: 3.4764647\n",
      "3810 Train Loss 16.37456\n",
      "3811 loss_D: 12.764911 loss_N: 0.1350925 loss_f: 3.4702914\n",
      "3811 Train Loss 16.370295\n",
      "3812 loss_D: 12.755478 loss_N: 0.1350678 loss_f: 3.4744656\n",
      "3812 Train Loss 16.365011\n",
      "3813 loss_D: 12.757177 loss_N: 0.13582067 loss_f: 3.4722383\n",
      "3813 Train Loss 16.365236\n",
      "3814 loss_D: 12.756214 loss_N: 0.13539986 loss_f: 3.4698098\n",
      "3814 Train Loss 16.361423\n",
      "3815 loss_D: 12.757893 loss_N: 0.13558443 loss_f: 3.462181\n",
      "3815 Train Loss 16.35566\n",
      "3816 loss_D: 12.754838 loss_N: 0.13579379 loss_f: 3.4575217\n",
      "3816 Train Loss 16.348154\n",
      "3817 loss_D: 12.749498 loss_N: 0.1358555 loss_f: 3.4547796\n",
      "3817 Train Loss 16.340134\n",
      "3818 loss_D: 12.742156 loss_N: 0.13563164 loss_f: 3.4543188\n",
      "3818 Train Loss 16.332106\n",
      "3819 loss_D: 12.734742 loss_N: 0.13528647 loss_f: 3.4546223\n",
      "3819 Train Loss 16.32465\n",
      "3820 loss_D: 12.728797 loss_N: 0.13494767 loss_f: 3.4532828\n",
      "3820 Train Loss 16.317028\n",
      "3821 loss_D: 12.726388 loss_N: 0.13477717 loss_f: 3.4497757\n",
      "3821 Train Loss 16.31094\n",
      "3822 loss_D: 12.722836 loss_N: 0.13481258 loss_f: 3.4467444\n",
      "3822 Train Loss 16.304392\n",
      "3823 loss_D: 12.719641 loss_N: 0.13501057 loss_f: 3.443346\n",
      "3823 Train Loss 16.297997\n",
      "3824 loss_D: 12.705772 loss_N: 0.13501653 loss_f: 3.44871\n",
      "3824 Train Loss 16.2895\n",
      "3825 loss_D: 12.701668 loss_N: 0.13502434 loss_f: 3.447364\n",
      "3825 Train Loss 16.284056\n",
      "3826 loss_D: 12.695151 loss_N: 0.13575293 loss_f: 3.4477413\n",
      "3826 Train Loss 16.278645\n",
      "3827 loss_D: 12.687992 loss_N: 0.13573901 loss_f: 3.4509237\n",
      "3827 Train Loss 16.274654\n",
      "3828 loss_D: 12.687348 loss_N: 0.13549723 loss_f: 3.4473968\n",
      "3828 Train Loss 16.270243\n",
      "3829 loss_D: 12.679279 loss_N: 0.13522059 loss_f: 3.4491162\n",
      "3829 Train Loss 16.263617\n",
      "3830 loss_D: 12.671525 loss_N: 0.13525905 loss_f: 3.4476113\n",
      "3830 Train Loss 16.254395\n",
      "3831 loss_D: 12.656231 loss_N: 0.13506988 loss_f: 3.4608233\n",
      "3831 Train Loss 16.252125\n",
      "3832 loss_D: 12.661358 loss_N: 0.13554941 loss_f: 3.4452288\n",
      "3832 Train Loss 16.242136\n",
      "3833 loss_D: 12.655516 loss_N: 0.13561061 loss_f: 3.445021\n",
      "3833 Train Loss 16.236147\n",
      "3834 loss_D: 12.649584 loss_N: 0.13569131 loss_f: 3.4447863\n",
      "3834 Train Loss 16.230062\n",
      "3835 loss_D: 12.6419525 loss_N: 0.13774286 loss_f: 3.4672139\n",
      "3835 Train Loss 16.24691\n",
      "3836 loss_D: 12.647552 loss_N: 0.13613364 loss_f: 3.444614\n",
      "3836 Train Loss 16.228298\n",
      "3837 loss_D: 12.636683 loss_N: 0.13601995 loss_f: 3.4501936\n",
      "3837 Train Loss 16.222897\n",
      "3838 loss_D: 12.63252 loss_N: 0.13582689 loss_f: 3.451002\n",
      "3838 Train Loss 16.219349\n",
      "3839 loss_D: 12.625362 loss_N: 0.13565749 loss_f: 3.4537604\n",
      "3839 Train Loss 16.21478\n",
      "3840 loss_D: 12.630595 loss_N: 0.13552868 loss_f: 3.4450397\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3840 Train Loss 16.211163\n",
      "3841 loss_D: 12.631661 loss_N: 0.13566665 loss_f: 3.4375625\n",
      "3841 Train Loss 16.204891\n",
      "3842 loss_D: 12.646954 loss_N: 0.135776 loss_f: 3.4147522\n",
      "3842 Train Loss 16.197481\n",
      "3843 loss_D: 12.654951 loss_N: 0.13605344 loss_f: 3.4014843\n",
      "3843 Train Loss 16.192488\n",
      "3844 loss_D: 12.659371 loss_N: 0.13621016 loss_f: 3.3905811\n",
      "3844 Train Loss 16.186163\n",
      "3845 loss_D: 12.655062 loss_N: 0.13674048 loss_f: 3.3866\n",
      "3845 Train Loss 16.178402\n",
      "3846 loss_D: 12.673144 loss_N: 0.13553354 loss_f: 3.5046084\n",
      "3846 Train Loss 16.313286\n",
      "3847 loss_D: 12.656928 loss_N: 0.13657983 loss_f: 3.3825893\n",
      "3847 Train Loss 16.176098\n",
      "3848 loss_D: 12.643931 loss_N: 0.1371039 loss_f: 3.387207\n",
      "3848 Train Loss 16.168243\n",
      "3849 loss_D: 12.622409 loss_N: 0.1379473 loss_f: 3.3977733\n",
      "3849 Train Loss 16.158129\n",
      "3850 loss_D: 12.608834 loss_N: 0.13861106 loss_f: 3.4030936\n",
      "3850 Train Loss 16.15054\n",
      "3851 loss_D: 12.590956 loss_N: 0.14025487 loss_f: 3.4068198\n",
      "3851 Train Loss 16.138031\n",
      "3852 loss_D: 12.578073 loss_N: 0.14124793 loss_f: 3.4064105\n",
      "3852 Train Loss 16.12573\n",
      "3853 loss_D: 12.578575 loss_N: 0.14190085 loss_f: 3.394694\n",
      "3853 Train Loss 16.11517\n",
      "3854 loss_D: 12.580718 loss_N: 0.14242718 loss_f: 3.3774729\n",
      "3854 Train Loss 16.100618\n",
      "3855 loss_D: 12.56099 loss_N: 0.14380899 loss_f: 3.4695008\n",
      "3855 Train Loss 16.174301\n",
      "3856 loss_D: 12.576883 loss_N: 0.14263941 loss_f: 3.3769925\n",
      "3856 Train Loss 16.096516\n",
      "3857 loss_D: 12.580255 loss_N: 0.14267983 loss_f: 3.365332\n",
      "3857 Train Loss 16.088266\n",
      "3858 loss_D: 12.574156 loss_N: 0.14327753 loss_f: 3.355153\n",
      "3858 Train Loss 16.072586\n",
      "3859 loss_D: 12.577507 loss_N: 0.14353377 loss_f: 3.3386757\n",
      "3859 Train Loss 16.059717\n",
      "3860 loss_D: 12.551177 loss_N: 0.14558981 loss_f: 3.3482132\n",
      "3860 Train Loss 16.04498\n",
      "3861 loss_D: 12.543734 loss_N: 0.14650698 loss_f: 3.3458507\n",
      "3861 Train Loss 16.03609\n",
      "3862 loss_D: 12.530326 loss_N: 0.14714126 loss_f: 3.3524578\n",
      "3862 Train Loss 16.029924\n",
      "3863 loss_D: 12.516196 loss_N: 0.14766596 loss_f: 3.361095\n",
      "3863 Train Loss 16.024958\n",
      "3864 loss_D: 12.512017 loss_N: 0.14750798 loss_f: 3.3602414\n",
      "3864 Train Loss 16.019766\n",
      "3865 loss_D: 12.501085 loss_N: 0.14756767 loss_f: 3.3661253\n",
      "3865 Train Loss 16.014778\n",
      "3866 loss_D: 12.505693 loss_N: 0.14742228 loss_f: 3.3590398\n",
      "3866 Train Loss 16.012156\n",
      "3867 loss_D: 12.500653 loss_N: 0.14719597 loss_f: 3.3592288\n",
      "3867 Train Loss 16.007078\n",
      "3868 loss_D: 12.49356 loss_N: 0.14707538 loss_f: 3.3618038\n",
      "3868 Train Loss 16.00244\n",
      "3869 loss_D: 12.4933405 loss_N: 0.14672619 loss_f: 3.357745\n",
      "3869 Train Loss 15.997812\n",
      "3870 loss_D: 12.491232 loss_N: 0.14638972 loss_f: 3.3565078\n",
      "3870 Train Loss 15.994129\n",
      "3871 loss_D: 12.492468 loss_N: 0.14581154 loss_f: 3.352574\n",
      "3871 Train Loss 15.990853\n",
      "3872 loss_D: 12.490407 loss_N: 0.14517954 loss_f: 3.351573\n",
      "3872 Train Loss 15.98716\n",
      "3873 loss_D: 12.48896 loss_N: 0.14435545 loss_f: 3.35138\n",
      "3873 Train Loss 15.984696\n",
      "3874 loss_D: 12.487089 loss_N: 0.14407526 loss_f: 3.351717\n",
      "3874 Train Loss 15.982882\n",
      "3875 loss_D: 12.483859 loss_N: 0.14370611 loss_f: 3.352199\n",
      "3875 Train Loss 15.979765\n",
      "3876 loss_D: 12.472079 loss_N: 0.14257394 loss_f: 3.3627784\n",
      "3876 Train Loss 15.977432\n",
      "3877 loss_D: 12.475912 loss_N: 0.14248082 loss_f: 3.354453\n",
      "3877 Train Loss 15.972846\n",
      "3878 loss_D: 12.481761 loss_N: 0.14246374 loss_f: 3.3464663\n",
      "3878 Train Loss 15.970691\n",
      "3879 loss_D: 12.480129 loss_N: 0.14204012 loss_f: 3.3453257\n",
      "3879 Train Loss 15.967495\n",
      "3880 loss_D: 12.4799385 loss_N: 0.14170513 loss_f: 3.3430882\n",
      "3880 Train Loss 15.964732\n",
      "3881 loss_D: 12.479618 loss_N: 0.1415624 loss_f: 3.339905\n",
      "3881 Train Loss 15.961085\n",
      "3882 loss_D: 12.483382 loss_N: 0.14155579 loss_f: 3.3289137\n",
      "3882 Train Loss 15.953852\n",
      "3883 loss_D: 12.48536 loss_N: 0.14166687 loss_f: 3.3221624\n",
      "3883 Train Loss 15.949189\n",
      "3884 loss_D: 12.489541 loss_N: 0.14327158 loss_f: 3.3126261\n",
      "3884 Train Loss 15.945438\n",
      "3885 loss_D: 12.495874 loss_N: 0.14209843 loss_f: 3.300913\n",
      "3885 Train Loss 15.938886\n",
      "3886 loss_D: 12.495303 loss_N: 0.1422361 loss_f: 3.2934234\n",
      "3886 Train Loss 15.930963\n",
      "3887 loss_D: 12.501593 loss_N: 0.1422036 loss_f: 3.2820063\n",
      "3887 Train Loss 15.925802\n",
      "3888 loss_D: 12.5060625 loss_N: 0.142116 loss_f: 3.274345\n",
      "3888 Train Loss 15.9225235\n",
      "3889 loss_D: 12.513633 loss_N: 0.14178167 loss_f: 3.2646754\n",
      "3889 Train Loss 15.92009\n",
      "3890 loss_D: 12.5173 loss_N: 0.14150064 loss_f: 3.257983\n",
      "3890 Train Loss 15.916783\n",
      "3891 loss_D: 12.517686 loss_N: 0.14131643 loss_f: 3.2542033\n",
      "3891 Train Loss 15.913206\n",
      "3892 loss_D: 12.515782 loss_N: 0.14107865 loss_f: 3.2519774\n",
      "3892 Train Loss 15.908838\n",
      "3893 loss_D: 12.5115185 loss_N: 0.14107117 loss_f: 3.2535198\n",
      "3893 Train Loss 15.90611\n",
      "3894 loss_D: 12.507633 loss_N: 0.14112641 loss_f: 3.2546403\n",
      "3894 Train Loss 15.9034\n",
      "3895 loss_D: 12.505266 loss_N: 0.14119445 loss_f: 3.2541628\n",
      "3895 Train Loss 15.900623\n",
      "3896 loss_D: 12.50532 loss_N: 0.14121284 loss_f: 3.250564\n",
      "3896 Train Loss 15.897097\n",
      "3897 loss_D: 12.507312 loss_N: 0.14115945 loss_f: 3.243122\n",
      "3897 Train Loss 15.891593\n",
      "3898 loss_D: 12.508151 loss_N: 0.1412185 loss_f: 3.238788\n",
      "3898 Train Loss 15.888157\n",
      "3899 loss_D: 12.529048 loss_N: 0.14145225 loss_f: 3.2546148\n",
      "3899 Train Loss 15.925115\n",
      "3900 loss_D: 12.512881 loss_N: 0.14108437 loss_f: 3.2301376\n",
      "3900 Train Loss 15.884104\n",
      "3901 loss_D: 12.511447 loss_N: 0.1409558 loss_f: 3.228047\n",
      "3901 Train Loss 15.880449\n",
      "3902 loss_D: 12.505946 loss_N: 0.14105399 loss_f: 3.2322316\n",
      "3902 Train Loss 15.879232\n",
      "3903 loss_D: 12.504157 loss_N: 0.14058806 loss_f: 3.2269583\n",
      "3903 Train Loss 15.871703\n",
      "3904 loss_D: 12.499901 loss_N: 0.14084058 loss_f: 3.227527\n",
      "3904 Train Loss 15.868268\n",
      "3905 loss_D: 12.488301 loss_N: 0.14120957 loss_f: 3.2306256\n",
      "3905 Train Loss 15.860136\n",
      "3906 loss_D: 12.478312 loss_N: 0.1414639 loss_f: 3.2341208\n",
      "3906 Train Loss 15.853897\n",
      "3907 loss_D: 12.466878 loss_N: 0.14160024 loss_f: 3.2383187\n",
      "3907 Train Loss 15.846797\n",
      "3908 loss_D: 12.451792 loss_N: 0.14187859 loss_f: 3.244282\n",
      "3908 Train Loss 15.837952\n",
      "3909 loss_D: 12.438195 loss_N: 0.14201303 loss_f: 3.2503016\n",
      "3909 Train Loss 15.830509\n",
      "3910 loss_D: 12.42949 loss_N: 0.14209242 loss_f: 3.2545953\n",
      "3910 Train Loss 15.826178\n",
      "3911 loss_D: 12.431265 loss_N: 0.14264792 loss_f: 3.2548544\n",
      "3911 Train Loss 15.828767\n",
      "3912 loss_D: 12.430136 loss_N: 0.14231645 loss_f: 3.2511797\n",
      "3912 Train Loss 15.823632\n",
      "3913 loss_D: 12.420025 loss_N: 0.14240357 loss_f: 3.2567425\n",
      "3913 Train Loss 15.819171\n",
      "3914 loss_D: 12.41144 loss_N: 0.14261477 loss_f: 3.2611911\n",
      "3914 Train Loss 15.815246\n",
      "3915 loss_D: 12.379479 loss_N: 0.14284499 loss_f: 3.3012538\n",
      "3915 Train Loss 15.823578\n",
      "3916 loss_D: 12.401155 loss_N: 0.14267375 loss_f: 3.2689312\n",
      "3916 Train Loss 15.81276\n",
      "3917 loss_D: 12.369063 loss_N: 0.14344284 loss_f: 3.298864\n",
      "3917 Train Loss 15.811371\n",
      "3918 loss_D: 12.388474 loss_N: 0.14290884 loss_f: 3.2725875\n",
      "3918 Train Loss 15.803971\n",
      "3919 loss_D: 12.380081 loss_N: 0.14298308 loss_f: 3.279128\n",
      "3919 Train Loss 15.802193\n",
      "3920 loss_D: 12.377348 loss_N: 0.14290981 loss_f: 3.2804694\n",
      "3920 Train Loss 15.800728\n",
      "3921 loss_D: 12.368579 loss_N: 0.14286181 loss_f: 3.2861042\n",
      "3921 Train Loss 15.7975445\n",
      "3922 loss_D: 12.363078 loss_N: 0.14280099 loss_f: 3.2896912\n",
      "3922 Train Loss 15.79557\n",
      "3923 loss_D: 12.35672 loss_N: 0.14289318 loss_f: 3.2932382\n",
      "3923 Train Loss 15.7928505\n",
      "3924 loss_D: 12.350521 loss_N: 0.14304051 loss_f: 3.2956069\n",
      "3924 Train Loss 15.789168\n",
      "3925 loss_D: 12.349817 loss_N: 0.14332171 loss_f: 3.2930987\n",
      "3925 Train Loss 15.786238\n",
      "3926 loss_D: 12.347182 loss_N: 0.14379823 loss_f: 3.2916343\n",
      "3926 Train Loss 15.782615\n",
      "3927 loss_D: 12.353704 loss_N: 0.14399593 loss_f: 3.2816195\n",
      "3927 Train Loss 15.77932\n",
      "3928 loss_D: 12.347437 loss_N: 0.14496183 loss_f: 3.2967281\n",
      "3928 Train Loss 15.789127\n",
      "3929 loss_D: 12.351684 loss_N: 0.14429326 loss_f: 3.2809336\n",
      "3929 Train Loss 15.77691\n",
      "3930 loss_D: 12.352649 loss_N: 0.14419915 loss_f: 3.2768157\n",
      "3930 Train Loss 15.7736635\n",
      "3931 loss_D: 12.356732 loss_N: 0.14373042 loss_f: 3.2693315\n",
      "3931 Train Loss 15.769794\n",
      "3932 loss_D: 12.359087 loss_N: 0.14351626 loss_f: 3.2648785\n",
      "3932 Train Loss 15.767482\n",
      "3933 loss_D: 12.364241 loss_N: 0.1433674 loss_f: 3.2568426\n",
      "3933 Train Loss 15.764451\n",
      "3934 loss_D: 12.365773 loss_N: 0.14355072 loss_f: 3.2506921\n",
      "3934 Train Loss 15.760016\n",
      "3935 loss_D: 12.367263 loss_N: 0.14410019 loss_f: 3.2482333\n",
      "3935 Train Loss 15.759596\n",
      "3936 loss_D: 12.366172 loss_N: 0.14379911 loss_f: 3.2460406\n",
      "3936 Train Loss 15.756011\n",
      "3937 loss_D: 12.364222 loss_N: 0.14394864 loss_f: 3.2432737\n",
      "3937 Train Loss 15.751444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3938 loss_D: 12.355095 loss_N: 0.14431298 loss_f: 3.2460542\n",
      "3938 Train Loss 15.745462\n",
      "3939 loss_D: 12.352676 loss_N: 0.14430672 loss_f: 3.2453222\n",
      "3939 Train Loss 15.742306\n",
      "3940 loss_D: 12.34875 loss_N: 0.14395961 loss_f: 3.2446613\n",
      "3940 Train Loss 15.737371\n",
      "3941 loss_D: 12.347282 loss_N: 0.14382228 loss_f: 3.2498806\n",
      "3941 Train Loss 15.740986\n",
      "3942 loss_D: 12.348198 loss_N: 0.14390472 loss_f: 3.2437582\n",
      "3942 Train Loss 15.735861\n",
      "3943 loss_D: 12.349498 loss_N: 0.14365551 loss_f: 3.2398813\n",
      "3943 Train Loss 15.733035\n",
      "3944 loss_D: 12.346179 loss_N: 0.14349523 loss_f: 3.240961\n",
      "3944 Train Loss 15.730636\n",
      "3945 loss_D: 12.343563 loss_N: 0.14373076 loss_f: 3.2412684\n",
      "3945 Train Loss 15.728562\n",
      "3946 loss_D: 12.33895 loss_N: 0.14370313 loss_f: 3.244118\n",
      "3946 Train Loss 15.726771\n",
      "3947 loss_D: 12.33117 loss_N: 0.14425525 loss_f: 3.2493443\n",
      "3947 Train Loss 15.72477\n",
      "3948 loss_D: 12.330462 loss_N: 0.14434226 loss_f: 3.2477152\n",
      "3948 Train Loss 15.72252\n",
      "3949 loss_D: 12.325044 loss_N: 0.14427453 loss_f: 3.2486742\n",
      "3949 Train Loss 15.717993\n",
      "3950 loss_D: 12.318044 loss_N: 0.14415304 loss_f: 3.2528753\n",
      "3950 Train Loss 15.715072\n",
      "3951 loss_D: 12.308759 loss_N: 0.14408241 loss_f: 3.2576492\n",
      "3951 Train Loss 15.71049\n",
      "3952 loss_D: 12.284456 loss_N: 0.1439199 loss_f: 3.2844863\n",
      "3952 Train Loss 15.712862\n",
      "3953 loss_D: 12.298733 loss_N: 0.14400646 loss_f: 3.2654822\n",
      "3953 Train Loss 15.708221\n",
      "3954 loss_D: 12.292931 loss_N: 0.14413832 loss_f: 3.2672594\n",
      "3954 Train Loss 15.704329\n",
      "3955 loss_D: 12.289824 loss_N: 0.14435062 loss_f: 3.2672598\n",
      "3955 Train Loss 15.701434\n",
      "3956 loss_D: 12.290947 loss_N: 0.14441702 loss_f: 3.2632568\n",
      "3956 Train Loss 15.698621\n",
      "3957 loss_D: 12.291009 loss_N: 0.14457749 loss_f: 3.26052\n",
      "3957 Train Loss 15.696106\n",
      "3958 loss_D: 12.293827 loss_N: 0.14450027 loss_f: 3.2539997\n",
      "3958 Train Loss 15.6923275\n",
      "3959 loss_D: 12.301259 loss_N: 0.14409375 loss_f: 3.242985\n",
      "3959 Train Loss 15.688337\n",
      "3960 loss_D: 12.311161 loss_N: 0.14358082 loss_f: 3.229174\n",
      "3960 Train Loss 15.683915\n",
      "3961 loss_D: 12.322163 loss_N: 0.14336397 loss_f: 3.2143955\n",
      "3961 Train Loss 15.679922\n",
      "3962 loss_D: 12.322489 loss_N: 0.14366208 loss_f: 3.2616613\n",
      "3962 Train Loss 15.727813\n",
      "3963 loss_D: 12.322096 loss_N: 0.14341696 loss_f: 3.2115142\n",
      "3963 Train Loss 15.677028\n",
      "3964 loss_D: 12.318936 loss_N: 0.14342567 loss_f: 3.209637\n",
      "3964 Train Loss 15.671999\n",
      "3965 loss_D: 12.313952 loss_N: 0.14341213 loss_f: 3.2086525\n",
      "3965 Train Loss 15.666018\n",
      "3966 loss_D: 12.315208 loss_N: 0.14344443 loss_f: 3.2047195\n",
      "3966 Train Loss 15.663372\n",
      "3967 loss_D: 12.314801 loss_N: 0.14364666 loss_f: 3.2008672\n",
      "3967 Train Loss 15.659315\n",
      "3968 loss_D: 12.312801 loss_N: 0.1442612 loss_f: 3.1968803\n",
      "3968 Train Loss 15.653943\n",
      "3969 loss_D: 12.313503 loss_N: 0.14473061 loss_f: 3.1935754\n",
      "3969 Train Loss 15.65181\n",
      "3970 loss_D: 12.308339 loss_N: 0.14488558 loss_f: 3.1957057\n",
      "3970 Train Loss 15.648931\n",
      "3971 loss_D: 12.307319 loss_N: 0.1449045 loss_f: 3.1958823\n",
      "3971 Train Loss 15.648106\n",
      "3972 loss_D: 12.303223 loss_N: 0.14502645 loss_f: 3.1985447\n",
      "3972 Train Loss 15.646793\n",
      "3973 loss_D: 12.301163 loss_N: 0.14527543 loss_f: 3.1988506\n",
      "3973 Train Loss 15.645288\n",
      "3974 loss_D: 12.296828 loss_N: 0.1457862 loss_f: 3.2000585\n",
      "3974 Train Loss 15.6426735\n",
      "3975 loss_D: 12.292283 loss_N: 0.14753896 loss_f: 3.2063572\n",
      "3975 Train Loss 15.646179\n",
      "3976 loss_D: 12.295132 loss_N: 0.14636739 loss_f: 3.199784\n",
      "3976 Train Loss 15.641283\n",
      "3977 loss_D: 12.294207 loss_N: 0.14647 loss_f: 3.1985667\n",
      "3977 Train Loss 15.639243\n",
      "3978 loss_D: 12.289947 loss_N: 0.14775966 loss_f: 3.1977465\n",
      "3978 Train Loss 15.635452\n",
      "3979 loss_D: 12.28287 loss_N: 0.14730866 loss_f: 3.2010744\n",
      "3979 Train Loss 15.631253\n",
      "3980 loss_D: 12.278131 loss_N: 0.14718077 loss_f: 3.2016916\n",
      "3980 Train Loss 15.627003\n",
      "3981 loss_D: 12.2669 loss_N: 0.14733128 loss_f: 3.2078328\n",
      "3981 Train Loss 15.622065\n",
      "3982 loss_D: 12.256024 loss_N: 0.14742726 loss_f: 3.2143018\n",
      "3982 Train Loss 15.617754\n",
      "3983 loss_D: 12.257822 loss_N: 0.15063083 loss_f: 3.3269742\n",
      "3983 Train Loss 15.735427\n",
      "3984 loss_D: 12.255996 loss_N: 0.14769897 loss_f: 3.2128108\n",
      "3984 Train Loss 15.616506\n",
      "3985 loss_D: 12.24709 loss_N: 0.14726825 loss_f: 3.2167988\n",
      "3985 Train Loss 15.611157\n",
      "3986 loss_D: 12.209905 loss_N: 0.14768784 loss_f: 3.2630718\n",
      "3986 Train Loss 15.620665\n",
      "3987 loss_D: 12.23513 loss_N: 0.14739439 loss_f: 3.2259603\n",
      "3987 Train Loss 15.608484\n",
      "3988 loss_D: 12.2269745 loss_N: 0.14652045 loss_f: 3.2283456\n",
      "3988 Train Loss 15.601841\n",
      "3989 loss_D: 12.234762 loss_N: 0.14611597 loss_f: 3.2159746\n",
      "3989 Train Loss 15.596853\n",
      "3990 loss_D: 12.224121 loss_N: 0.146023 loss_f: 3.2222369\n",
      "3990 Train Loss 15.592381\n",
      "3991 loss_D: 12.230614 loss_N: 0.14558153 loss_f: 3.2119582\n",
      "3991 Train Loss 15.588153\n",
      "3992 loss_D: 12.231485 loss_N: 0.14554136 loss_f: 3.2062743\n",
      "3992 Train Loss 15.583301\n",
      "3993 loss_D: 12.227919 loss_N: 0.14575896 loss_f: 3.206475\n",
      "3993 Train Loss 15.5801525\n",
      "3994 loss_D: 12.22504 loss_N: 0.14616291 loss_f: 3.2056763\n",
      "3994 Train Loss 15.5768795\n",
      "3995 loss_D: 12.220076 loss_N: 0.14686954 loss_f: 3.2061849\n",
      "3995 Train Loss 15.57313\n",
      "3996 loss_D: 12.221138 loss_N: 0.14723507 loss_f: 3.2013721\n",
      "3996 Train Loss 15.569745\n",
      "3997 loss_D: 12.224281 loss_N: 0.14757922 loss_f: 3.1929414\n",
      "3997 Train Loss 15.564802\n",
      "3998 loss_D: 12.230698 loss_N: 0.14658691 loss_f: 3.1825192\n",
      "3998 Train Loss 15.559804\n",
      "3999 loss_D: 12.256201 loss_N: 0.14635378 loss_f: 3.1663742\n",
      "3999 Train Loss 15.568929\n",
      "4000 loss_D: 12.239645 loss_N: 0.14649497 loss_f: 3.1696174\n",
      "4000 Train Loss 15.5557575\n",
      "4001 loss_D: 12.239059 loss_N: 0.14546752 loss_f: 3.175113\n",
      "4001 Train Loss 15.55964\n",
      "4002 loss_D: 12.239172 loss_N: 0.14602932 loss_f: 3.164951\n",
      "4002 Train Loss 15.550153\n",
      "4003 loss_D: 12.236461 loss_N: 0.14543433 loss_f: 3.1627398\n",
      "4003 Train Loss 15.544635\n",
      "4004 loss_D: 12.229887 loss_N: 0.1445816 loss_f: 3.1643605\n",
      "4004 Train Loss 15.53883\n",
      "4005 loss_D: 12.226046 loss_N: 0.14419337 loss_f: 3.163946\n",
      "4005 Train Loss 15.534185\n",
      "4006 loss_D: 12.221054 loss_N: 0.14380017 loss_f: 3.163965\n",
      "4006 Train Loss 15.528819\n",
      "4007 loss_D: 12.217668 loss_N: 0.14442655 loss_f: 3.1616166\n",
      "4007 Train Loss 15.52371\n",
      "4008 loss_D: 12.23199 loss_N: 0.14486115 loss_f: 3.1396701\n",
      "4008 Train Loss 15.516521\n",
      "4009 loss_D: 12.231204 loss_N: 0.1452687 loss_f: 3.1332555\n",
      "4009 Train Loss 15.5097275\n",
      "4010 loss_D: 12.234185 loss_N: 0.14579453 loss_f: 3.1237981\n",
      "4010 Train Loss 15.503778\n",
      "4011 loss_D: 12.23327 loss_N: 0.14571537 loss_f: 3.1186783\n",
      "4011 Train Loss 15.4976635\n",
      "4012 loss_D: 12.230446 loss_N: 0.14523378 loss_f: 3.1152968\n",
      "4012 Train Loss 15.490976\n",
      "4013 loss_D: 12.226414 loss_N: 0.14502275 loss_f: 3.1142275\n",
      "4013 Train Loss 15.485663\n",
      "4014 loss_D: 12.219879 loss_N: 0.14506231 loss_f: 3.1143546\n",
      "4014 Train Loss 15.479296\n",
      "4015 loss_D: 12.214637 loss_N: 0.14528552 loss_f: 3.11333\n",
      "4015 Train Loss 15.473252\n",
      "4016 loss_D: 12.210813 loss_N: 0.14622048 loss_f: 3.1100273\n",
      "4016 Train Loss 15.46706\n",
      "4017 loss_D: 12.205333 loss_N: 0.14803036 loss_f: 3.1059053\n",
      "4017 Train Loss 15.459269\n",
      "4018 loss_D: 12.195881 loss_N: 0.15032095 loss_f: 3.1029077\n",
      "4018 Train Loss 15.44911\n",
      "4019 loss_D: 12.187884 loss_N: 0.15196115 loss_f: 3.0981975\n",
      "4019 Train Loss 15.438044\n",
      "4020 loss_D: 12.180632 loss_N: 0.1532726 loss_f: 3.0921686\n",
      "4020 Train Loss 15.426073\n",
      "4021 loss_D: 12.171043 loss_N: 0.1526573 loss_f: 3.0937293\n",
      "4021 Train Loss 15.41743\n",
      "4022 loss_D: 12.162308 loss_N: 0.1514913 loss_f: 3.0969963\n",
      "4022 Train Loss 15.410795\n",
      "4023 loss_D: 12.144848 loss_N: 0.15061627 loss_f: 3.10838\n",
      "4023 Train Loss 15.403845\n",
      "4024 loss_D: 12.136786 loss_N: 0.14997406 loss_f: 3.1087174\n",
      "4024 Train Loss 15.395477\n",
      "4025 loss_D: 12.102184 loss_N: 0.15222695 loss_f: 3.1368\n",
      "4025 Train Loss 15.3912115\n",
      "4026 loss_D: 12.107094 loss_N: 0.15254968 loss_f: 3.1220195\n",
      "4026 Train Loss 15.381663\n",
      "4027 loss_D: 12.122512 loss_N: 0.15293878 loss_f: 3.1010358\n",
      "4027 Train Loss 15.376487\n",
      "4028 loss_D: 12.115476 loss_N: 0.15323772 loss_f: 3.1043618\n",
      "4028 Train Loss 15.373075\n",
      "4029 loss_D: 12.1058235 loss_N: 0.15262797 loss_f: 3.1110613\n",
      "4029 Train Loss 15.369513\n",
      "4030 loss_D: 12.093582 loss_N: 0.15188603 loss_f: 3.1206222\n",
      "4030 Train Loss 15.366091\n",
      "4031 loss_D: 12.082262 loss_N: 0.15026022 loss_f: 3.1303346\n",
      "4031 Train Loss 15.362857\n",
      "4032 loss_D: 12.07325 loss_N: 0.1500801 loss_f: 3.1355605\n",
      "4032 Train Loss 15.358891\n",
      "4033 loss_D: 12.067169 loss_N: 0.14621389 loss_f: 3.1440523\n",
      "4033 Train Loss 15.357435\n",
      "4034 loss_D: 12.062971 loss_N: 0.14864688 loss_f: 3.134726\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4034 Train Loss 15.346344\n",
      "4035 loss_D: 12.06188 loss_N: 0.15119565 loss_f: 3.1263096\n",
      "4035 Train Loss 15.339385\n",
      "4036 loss_D: 12.055626 loss_N: 0.15263046 loss_f: 3.1228266\n",
      "4036 Train Loss 15.331083\n",
      "4037 loss_D: 12.049195 loss_N: 0.15336981 loss_f: 3.1207573\n",
      "4037 Train Loss 15.323322\n",
      "4038 loss_D: 12.048694 loss_N: 0.15242475 loss_f: 3.1155267\n",
      "4038 Train Loss 15.316645\n",
      "4039 loss_D: 12.043684 loss_N: 0.1510395 loss_f: 3.1126285\n",
      "4039 Train Loss 15.307352\n",
      "4040 loss_D: 12.0307045 loss_N: 0.14985478 loss_f: 3.1178951\n",
      "4040 Train Loss 15.298454\n",
      "4041 loss_D: 12.250968 loss_N: 0.14589137 loss_f: 4.4686704\n",
      "4041 Train Loss 16.86553\n",
      "4042 loss_D: 12.042433 loss_N: 0.14831199 loss_f: 3.101184\n",
      "4042 Train Loss 15.291928\n",
      "4043 loss_D: 12.027781 loss_N: 0.14890134 loss_f: 3.1014972\n",
      "4043 Train Loss 15.278179\n",
      "4044 loss_D: 12.005348 loss_N: 0.15117073 loss_f: 3.0966065\n",
      "4044 Train Loss 15.253125\n",
      "4045 loss_D: 11.9988575 loss_N: 0.15346201 loss_f: 3.0864198\n",
      "4045 Train Loss 15.23874\n",
      "4046 loss_D: 11.978206 loss_N: 0.1533669 loss_f: 3.0886726\n",
      "4046 Train Loss 15.220245\n",
      "4047 loss_D: 11.957719 loss_N: 0.15326291 loss_f: 3.0890229\n",
      "4047 Train Loss 15.200005\n",
      "4048 loss_D: 11.937997 loss_N: 0.15325625 loss_f: 3.1002598\n",
      "4048 Train Loss 15.191513\n",
      "4049 loss_D: 11.940499 loss_N: 0.15274347 loss_f: 3.0910673\n",
      "4049 Train Loss 15.18431\n",
      "4050 loss_D: 11.935258 loss_N: 0.15338445 loss_f: 3.0902438\n",
      "4050 Train Loss 15.178886\n",
      "4051 loss_D: 11.937785 loss_N: 0.1534991 loss_f: 3.0820308\n",
      "4051 Train Loss 15.173315\n",
      "4052 loss_D: 11.936716 loss_N: 0.15429245 loss_f: 3.0763865\n",
      "4052 Train Loss 15.167395\n",
      "4053 loss_D: 11.921517 loss_N: 0.15553284 loss_f: 3.0821674\n",
      "4053 Train Loss 15.159218\n",
      "4054 loss_D: 11.91227 loss_N: 0.15639013 loss_f: 3.0842083\n",
      "4054 Train Loss 15.152868\n",
      "4055 loss_D: 11.8946705 loss_N: 0.15755334 loss_f: 3.0940168\n",
      "4055 Train Loss 15.146241\n",
      "4056 loss_D: 11.882924 loss_N: 0.15880457 loss_f: 3.1039338\n",
      "4056 Train Loss 15.145662\n",
      "4057 loss_D: 11.877441 loss_N: 0.15786469 loss_f: 3.1041312\n",
      "4057 Train Loss 15.139437\n",
      "4058 loss_D: 11.872857 loss_N: 0.15779927 loss_f: 3.1064682\n",
      "4058 Train Loss 15.137125\n",
      "4059 loss_D: 11.8545 loss_N: 0.15819833 loss_f: 3.1191492\n",
      "4059 Train Loss 15.131847\n",
      "4060 loss_D: 11.851677 loss_N: 0.15892935 loss_f: 3.11684\n",
      "4060 Train Loss 15.127447\n",
      "4061 loss_D: 11.843085 loss_N: 0.15919967 loss_f: 3.1206481\n",
      "4061 Train Loss 15.122933\n",
      "4062 loss_D: 11.845863 loss_N: 0.15963261 loss_f: 3.1124487\n",
      "4062 Train Loss 15.117945\n",
      "4063 loss_D: 11.849922 loss_N: 0.15941364 loss_f: 3.1033285\n",
      "4063 Train Loss 15.112664\n",
      "4064 loss_D: 11.84903 loss_N: 0.15982121 loss_f: 3.098141\n",
      "4064 Train Loss 15.106992\n",
      "4065 loss_D: 11.844308 loss_N: 0.15999782 loss_f: 3.0968292\n",
      "4065 Train Loss 15.101135\n",
      "4066 loss_D: 11.814777 loss_N: 0.1615471 loss_f: 3.1268675\n",
      "4066 Train Loss 15.103191\n",
      "4067 loss_D: 11.831483 loss_N: 0.16065432 loss_f: 3.106087\n",
      "4067 Train Loss 15.098224\n",
      "4068 loss_D: 11.80209 loss_N: 0.16459763 loss_f: 3.1252432\n",
      "4068 Train Loss 15.09193\n",
      "4069 loss_D: 11.796082 loss_N: 0.16324595 loss_f: 3.1338086\n",
      "4069 Train Loss 15.093136\n",
      "4070 loss_D: 11.799217 loss_N: 0.16396691 loss_f: 3.1255565\n",
      "4070 Train Loss 15.088741\n",
      "4071 loss_D: 11.7983465 loss_N: 0.162884 loss_f: 3.1222594\n",
      "4071 Train Loss 15.083489\n",
      "4072 loss_D: 11.789853 loss_N: 0.1636104 loss_f: 3.1263995\n",
      "4072 Train Loss 15.079863\n",
      "4073 loss_D: 11.786095 loss_N: 0.16419671 loss_f: 3.1251256\n",
      "4073 Train Loss 15.0754175\n",
      "4074 loss_D: 11.77743 loss_N: 0.16528967 loss_f: 3.127803\n",
      "4074 Train Loss 15.070522\n",
      "4075 loss_D: 11.782928 loss_N: 0.16502294 loss_f: 3.1171885\n",
      "4075 Train Loss 15.06514\n",
      "4076 loss_D: 11.782206 loss_N: 0.16387312 loss_f: 3.1206017\n",
      "4076 Train Loss 15.06668\n",
      "4077 loss_D: 11.782393 loss_N: 0.16450539 loss_f: 3.1148813\n",
      "4077 Train Loss 15.06178\n",
      "4078 loss_D: 11.7907295 loss_N: 0.16343534 loss_f: 3.1030707\n",
      "4078 Train Loss 15.057236\n",
      "4079 loss_D: 11.800229 loss_N: 0.16212386 loss_f: 3.0903823\n",
      "4079 Train Loss 15.052735\n",
      "4080 loss_D: 11.802229 loss_N: 0.16165966 loss_f: 3.086279\n",
      "4080 Train Loss 15.050167\n",
      "4081 loss_D: 11.799286 loss_N: 0.16142255 loss_f: 3.086856\n",
      "4081 Train Loss 15.0475645\n",
      "4082 loss_D: 11.795461 loss_N: 0.16158375 loss_f: 3.0881047\n",
      "4082 Train Loss 15.045149\n",
      "4083 loss_D: 11.791165 loss_N: 0.16171327 loss_f: 3.0888062\n",
      "4083 Train Loss 15.041685\n",
      "4084 loss_D: 11.780097 loss_N: 0.1621896 loss_f: 3.0963871\n",
      "4084 Train Loss 15.038673\n",
      "4085 loss_D: 11.782357 loss_N: 0.16188347 loss_f: 3.0892162\n",
      "4085 Train Loss 15.033457\n",
      "4086 loss_D: 11.784057 loss_N: 0.1612903 loss_f: 3.0839963\n",
      "4086 Train Loss 15.029343\n",
      "4087 loss_D: 11.785174 loss_N: 0.16053972 loss_f: 3.0780485\n",
      "4087 Train Loss 15.023763\n",
      "4088 loss_D: 11.780145 loss_N: 0.16036844 loss_f: 3.0789983\n",
      "4088 Train Loss 15.019512\n",
      "4089 loss_D: 11.772458 loss_N: 0.16064745 loss_f: 3.0814264\n",
      "4089 Train Loss 15.014532\n",
      "4090 loss_D: 11.75548 loss_N: 0.16186972 loss_f: 3.0950913\n",
      "4090 Train Loss 15.012442\n",
      "4091 loss_D: 11.74937 loss_N: 0.16240099 loss_f: 3.0922637\n",
      "4091 Train Loss 15.004034\n",
      "4092 loss_D: 11.742895 loss_N: 0.16322906 loss_f: 3.0922604\n",
      "4092 Train Loss 14.998384\n",
      "4093 loss_D: 11.735188 loss_N: 0.16431193 loss_f: 3.0901906\n",
      "4093 Train Loss 14.989691\n",
      "4094 loss_D: 11.727421 loss_N: 0.16486958 loss_f: 3.0908895\n",
      "4094 Train Loss 14.983179\n",
      "4095 loss_D: 11.735452 loss_N: 0.16428876 loss_f: 3.078267\n",
      "4095 Train Loss 14.978007\n",
      "4096 loss_D: 11.739463 loss_N: 0.16362956 loss_f: 3.0705395\n",
      "4096 Train Loss 14.973632\n",
      "4097 loss_D: 11.74786 loss_N: 0.16302404 loss_f: 3.0590866\n",
      "4097 Train Loss 14.969971\n",
      "4098 loss_D: 11.753858 loss_N: 0.16260172 loss_f: 3.0496933\n",
      "4098 Train Loss 14.966152\n",
      "4099 loss_D: 11.76324 loss_N: 0.16264804 loss_f: 3.038723\n",
      "4099 Train Loss 14.964611\n",
      "4100 loss_D: 11.754144 loss_N: 0.1632557 loss_f: 3.043033\n",
      "4100 Train Loss 14.960432\n",
      "4101 loss_D: 11.7508545 loss_N: 0.16354243 loss_f: 3.0434523\n",
      "4101 Train Loss 14.9578495\n",
      "4102 loss_D: 11.748128 loss_N: 0.16380097 loss_f: 3.042885\n",
      "4102 Train Loss 14.954814\n",
      "4103 loss_D: 11.747423 loss_N: 0.16386658 loss_f: 3.0402274\n",
      "4103 Train Loss 14.951517\n",
      "4104 loss_D: 11.75124 loss_N: 0.16390207 loss_f: 3.033281\n",
      "4104 Train Loss 14.948423\n",
      "4105 loss_D: 11.742683 loss_N: 0.16420065 loss_f: 3.0634308\n",
      "4105 Train Loss 14.970315\n",
      "4106 loss_D: 11.747458 loss_N: 0.16399449 loss_f: 3.0309165\n",
      "4106 Train Loss 14.942369\n",
      "4107 loss_D: 11.743826 loss_N: 0.16401339 loss_f: 3.0292742\n",
      "4107 Train Loss 14.937113\n",
      "4108 loss_D: 11.734537 loss_N: 0.16387959 loss_f: 3.0338328\n",
      "4108 Train Loss 14.932249\n",
      "4109 loss_D: 11.733187 loss_N: 0.16393602 loss_f: 3.0314467\n",
      "4109 Train Loss 14.928569\n",
      "4110 loss_D: 11.730459 loss_N: 0.16400991 loss_f: 3.0309062\n",
      "4110 Train Loss 14.925375\n",
      "4111 loss_D: 11.723364 loss_N: 0.16425034 loss_f: 3.0342066\n",
      "4111 Train Loss 14.921821\n",
      "4112 loss_D: 11.715421 loss_N: 0.1646032 loss_f: 3.0373597\n",
      "4112 Train Loss 14.917383\n",
      "4113 loss_D: 11.707703 loss_N: 0.16471231 loss_f: 3.040499\n",
      "4113 Train Loss 14.912913\n",
      "4114 loss_D: 11.703101 loss_N: 0.16469751 loss_f: 3.040819\n",
      "4114 Train Loss 14.908618\n",
      "4115 loss_D: 11.700818 loss_N: 0.16459697 loss_f: 3.040779\n",
      "4115 Train Loss 14.906194\n",
      "4116 loss_D: 11.700139 loss_N: 0.16449477 loss_f: 3.038205\n",
      "4116 Train Loss 14.902839\n",
      "4117 loss_D: 11.698906 loss_N: 0.16373478 loss_f: 3.0392344\n",
      "4117 Train Loss 14.901875\n",
      "4118 loss_D: 11.695101 loss_N: 0.16430627 loss_f: 3.0390148\n",
      "4118 Train Loss 14.898422\n",
      "4119 loss_D: 11.693475 loss_N: 0.16446868 loss_f: 3.0383976\n",
      "4119 Train Loss 14.896341\n",
      "4120 loss_D: 11.686217 loss_N: 0.16489007 loss_f: 3.0421112\n",
      "4120 Train Loss 14.893219\n",
      "4121 loss_D: 11.681159 loss_N: 0.16494846 loss_f: 3.04233\n",
      "4121 Train Loss 14.888437\n",
      "4122 loss_D: 11.668134 loss_N: 0.1652816 loss_f: 3.0496972\n",
      "4122 Train Loss 14.883112\n",
      "4123 loss_D: 11.661951 loss_N: 0.16504966 loss_f: 3.050372\n",
      "4123 Train Loss 14.877373\n",
      "4124 loss_D: 11.650436 loss_N: 0.16512059 loss_f: 3.0549667\n",
      "4124 Train Loss 14.870523\n",
      "4125 loss_D: 11.6433735 loss_N: 0.16514698 loss_f: 3.0556924\n",
      "4125 Train Loss 14.864213\n",
      "4126 loss_D: 11.634927 loss_N: 0.16537601 loss_f: 3.055704\n",
      "4126 Train Loss 14.856007\n",
      "4127 loss_D: 11.630106 loss_N: 0.16582507 loss_f: 3.0527117\n",
      "4127 Train Loss 14.848642\n",
      "4128 loss_D: 11.643245 loss_N: 0.16723622 loss_f: 3.0926073\n",
      "4128 Train Loss 14.903089\n",
      "4129 loss_D: 11.631629 loss_N: 0.16600163 loss_f: 3.0490148\n",
      "4129 Train Loss 14.846645\n",
      "4130 loss_D: 11.629901 loss_N: 0.16600105 loss_f: 3.046557\n",
      "4130 Train Loss 14.842459\n",
      "4131 loss_D: 11.634183 loss_N: 0.16562289 loss_f: 3.0389996\n",
      "4131 Train Loss 14.838805\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4132 loss_D: 11.633696 loss_N: 0.16558196 loss_f: 3.0368118\n",
      "4132 Train Loss 14.836089\n",
      "4133 loss_D: 11.634466 loss_N: 0.16555467 loss_f: 3.0334163\n",
      "4133 Train Loss 14.833437\n",
      "4134 loss_D: 11.630493 loss_N: 0.16595761 loss_f: 3.0346658\n",
      "4134 Train Loss 14.831117\n",
      "4135 loss_D: 11.623701 loss_N: 0.16648647 loss_f: 3.035422\n",
      "4135 Train Loss 14.82561\n",
      "4136 loss_D: 11.614848 loss_N: 0.16728596 loss_f: 3.0397897\n",
      "4136 Train Loss 14.821924\n",
      "4137 loss_D: 11.610028 loss_N: 0.16794623 loss_f: 3.0399156\n",
      "4137 Train Loss 14.81789\n",
      "4138 loss_D: 11.6058 loss_N: 0.16819355 loss_f: 3.0407176\n",
      "4138 Train Loss 14.814711\n",
      "4139 loss_D: 11.60365 loss_N: 0.16823646 loss_f: 3.0368438\n",
      "4139 Train Loss 14.808731\n",
      "4140 loss_D: 11.602812 loss_N: 0.16800903 loss_f: 3.0327291\n",
      "4140 Train Loss 14.80355\n",
      "4141 loss_D: 11.603269 loss_N: 0.16803978 loss_f: 3.0289683\n",
      "4141 Train Loss 14.800276\n",
      "4142 loss_D: 11.592136 loss_N: 0.17008908 loss_f: 3.045762\n",
      "4142 Train Loss 14.807987\n",
      "4143 loss_D: 11.598783 loss_N: 0.16872562 loss_f: 3.0276823\n",
      "4143 Train Loss 14.795191\n",
      "4144 loss_D: 11.599151 loss_N: 0.16890807 loss_f: 3.0194135\n",
      "4144 Train Loss 14.787472\n",
      "4145 loss_D: 11.590804 loss_N: 0.16998589 loss_f: 3.0211487\n",
      "4145 Train Loss 14.781939\n",
      "4146 loss_D: 11.580473 loss_N: 0.17094965 loss_f: 3.0236568\n",
      "4146 Train Loss 14.77508\n",
      "4147 loss_D: 11.54992 loss_N: 0.17298694 loss_f: 3.044519\n",
      "4147 Train Loss 14.767426\n",
      "4148 loss_D: 11.536986 loss_N: 0.17323728 loss_f: 3.0517123\n",
      "4148 Train Loss 14.761935\n",
      "4149 loss_D: 11.511879 loss_N: 0.17381686 loss_f: 3.070921\n",
      "4149 Train Loss 14.756617\n",
      "4150 loss_D: 11.503561 loss_N: 0.17319047 loss_f: 3.0770135\n",
      "4150 Train Loss 14.753765\n",
      "4151 loss_D: 11.49795 loss_N: 0.17292939 loss_f: 3.0792022\n",
      "4151 Train Loss 14.750082\n",
      "4152 loss_D: 11.491691 loss_N: 0.1726423 loss_f: 3.0774791\n",
      "4152 Train Loss 14.741813\n",
      "4153 loss_D: 11.48855 loss_N: 0.17293395 loss_f: 3.0726674\n",
      "4153 Train Loss 14.734151\n",
      "4154 loss_D: 11.489368 loss_N: 0.17352961 loss_f: 3.063426\n",
      "4154 Train Loss 14.726324\n",
      "4155 loss_D: 11.478664 loss_N: 0.1749805 loss_f: 3.065512\n",
      "4155 Train Loss 14.719156\n",
      "4156 loss_D: 11.475971 loss_N: 0.17561512 loss_f: 3.061615\n",
      "4156 Train Loss 14.7132015\n",
      "4157 loss_D: 11.461883 loss_N: 0.17647749 loss_f: 3.0673823\n",
      "4157 Train Loss 14.705742\n",
      "4158 loss_D: 11.445717 loss_N: 0.1771567 loss_f: 3.0797808\n",
      "4158 Train Loss 14.702654\n",
      "4159 loss_D: 11.45544 loss_N: 0.1754025 loss_f: 3.0636973\n",
      "4159 Train Loss 14.69454\n",
      "4160 loss_D: 11.454055 loss_N: 0.17535083 loss_f: 3.057942\n",
      "4160 Train Loss 14.687347\n",
      "4161 loss_D: 11.452196 loss_N: 0.17543598 loss_f: 3.0533965\n",
      "4161 Train Loss 14.681028\n",
      "4162 loss_D: 11.444708 loss_N: 0.17596239 loss_f: 3.0568893\n",
      "4162 Train Loss 14.67756\n",
      "4163 loss_D: 11.441253 loss_N: 0.17617331 loss_f: 3.057623\n",
      "4163 Train Loss 14.675049\n",
      "4164 loss_D: 11.434644 loss_N: 0.17638716 loss_f: 3.062327\n",
      "4164 Train Loss 14.673357\n",
      "4165 loss_D: 11.431485 loss_N: 0.17638695 loss_f: 3.0640755\n",
      "4165 Train Loss 14.6719475\n",
      "4166 loss_D: 11.4241705 loss_N: 0.17678063 loss_f: 3.0694916\n",
      "4166 Train Loss 14.670443\n",
      "4167 loss_D: 11.422145 loss_N: 0.17682467 loss_f: 3.0705898\n",
      "4167 Train Loss 14.6695595\n",
      "4168 loss_D: 11.415848 loss_N: 0.17706583 loss_f: 3.0742848\n",
      "4168 Train Loss 14.667198\n",
      "4169 loss_D: 11.411733 loss_N: 0.17708713 loss_f: 3.0757673\n",
      "4169 Train Loss 14.664587\n",
      "4170 loss_D: 11.400403 loss_N: 0.17709921 loss_f: 3.0863543\n",
      "4170 Train Loss 14.6638565\n",
      "4171 loss_D: 11.405626 loss_N: 0.17680465 loss_f: 3.0764487\n",
      "4171 Train Loss 14.658879\n",
      "4172 loss_D: 11.409238 loss_N: 0.17652717 loss_f: 3.0700622\n",
      "4172 Train Loss 14.655827\n",
      "4173 loss_D: 11.4112835 loss_N: 0.17613097 loss_f: 3.0653393\n",
      "4173 Train Loss 14.652754\n",
      "4174 loss_D: 11.413973 loss_N: 0.17573085 loss_f: 3.0611553\n",
      "4174 Train Loss 14.650859\n",
      "4175 loss_D: 11.41375 loss_N: 0.17533611 loss_f: 3.0589807\n",
      "4175 Train Loss 14.6480665\n",
      "4176 loss_D: 11.418175 loss_N: 0.17478925 loss_f: 3.0527794\n",
      "4176 Train Loss 14.645743\n",
      "4177 loss_D: 11.417435 loss_N: 0.1748008 loss_f: 3.0507398\n",
      "4177 Train Loss 14.642975\n",
      "4178 loss_D: 11.4140415 loss_N: 0.17548493 loss_f: 3.0494452\n",
      "4178 Train Loss 14.638971\n",
      "4179 loss_D: 11.450514 loss_N: 0.17321318 loss_f: 3.123372\n",
      "4179 Train Loss 14.747099\n",
      "4180 loss_D: 11.417108 loss_N: 0.1751465 loss_f: 3.0458343\n",
      "4180 Train Loss 14.638088\n",
      "4181 loss_D: 11.4150095 loss_N: 0.17542663 loss_f: 3.0442083\n",
      "4181 Train Loss 14.6346445\n",
      "4182 loss_D: 11.410144 loss_N: 0.17558523 loss_f: 3.0453045\n",
      "4182 Train Loss 14.631033\n",
      "4183 loss_D: 11.402105 loss_N: 0.17750992 loss_f: 3.0545194\n",
      "4183 Train Loss 14.634135\n",
      "4184 loss_D: 11.407041 loss_N: 0.17629462 loss_f: 3.045587\n",
      "4184 Train Loss 14.6289215\n",
      "4185 loss_D: 11.401808 loss_N: 0.17653933 loss_f: 3.0471265\n",
      "4185 Train Loss 14.625474\n",
      "4186 loss_D: 11.395734 loss_N: 0.17643665 loss_f: 3.0501938\n",
      "4186 Train Loss 14.622364\n",
      "4187 loss_D: 11.384998 loss_N: 0.17643781 loss_f: 3.0550523\n",
      "4187 Train Loss 14.6164875\n",
      "4188 loss_D: 11.374193 loss_N: 0.17611259 loss_f: 3.0624986\n",
      "4188 Train Loss 14.612804\n",
      "4189 loss_D: 11.359706 loss_N: 0.17734697 loss_f: 3.0701187\n",
      "4189 Train Loss 14.607172\n",
      "4190 loss_D: 11.361227 loss_N: 0.17729399 loss_f: 3.0626633\n",
      "4190 Train Loss 14.601184\n",
      "4191 loss_D: 11.348049 loss_N: 0.1785694 loss_f: 3.0673285\n",
      "4191 Train Loss 14.593947\n",
      "4192 loss_D: 11.330909 loss_N: 0.17974977 loss_f: 3.078428\n",
      "4192 Train Loss 14.589087\n",
      "4193 loss_D: 11.324082 loss_N: 0.1802322 loss_f: 3.077986\n",
      "4193 Train Loss 14.5823\n",
      "4194 loss_D: 11.293293 loss_N: 0.18219042 loss_f: 3.0955262\n",
      "4194 Train Loss 14.571009\n",
      "4195 loss_D: 11.284812 loss_N: 0.1828792 loss_f: 3.0963204\n",
      "4195 Train Loss 14.564012\n",
      "4196 loss_D: 11.251959 loss_N: 0.18590675 loss_f: 3.119751\n",
      "4196 Train Loss 14.557616\n",
      "4197 loss_D: 11.2513075 loss_N: 0.18639429 loss_f: 3.115788\n",
      "4197 Train Loss 14.55349\n",
      "4198 loss_D: 11.252374 loss_N: 0.18647543 loss_f: 3.1112854\n",
      "4198 Train Loss 14.550135\n",
      "4199 loss_D: 11.249293 loss_N: 0.1867008 loss_f: 3.111955\n",
      "4199 Train Loss 14.547949\n",
      "4200 loss_D: 11.251721 loss_N: 0.18636723 loss_f: 3.1082463\n",
      "4200 Train Loss 14.546335\n",
      "4201 loss_D: 11.249391 loss_N: 0.18637916 loss_f: 3.1054344\n",
      "4201 Train Loss 14.541204\n",
      "4202 loss_D: 11.2443905 loss_N: 0.18734692 loss_f: 3.1049795\n",
      "4202 Train Loss 14.536716\n",
      "4203 loss_D: 11.193934 loss_N: 0.18778017 loss_f: 3.2572384\n",
      "4203 Train Loss 14.638953\n",
      "4204 loss_D: 11.233995 loss_N: 0.18742497 loss_f: 3.1091518\n",
      "4204 Train Loss 14.530572\n",
      "4205 loss_D: 11.229299 loss_N: 0.18730776 loss_f: 3.1073263\n",
      "4205 Train Loss 14.523932\n",
      "4206 loss_D: 11.20837 loss_N: 0.18775184 loss_f: 3.1166735\n",
      "4206 Train Loss 14.512795\n",
      "4207 loss_D: 11.205412 loss_N: 0.18745802 loss_f: 3.1162055\n",
      "4207 Train Loss 14.509075\n",
      "4208 loss_D: 11.196989 loss_N: 0.18739453 loss_f: 3.119907\n",
      "4208 Train Loss 14.50429\n",
      "4209 loss_D: 11.19903 loss_N: 0.1867391 loss_f: 3.1153035\n",
      "4209 Train Loss 14.501072\n",
      "4210 loss_D: 11.196639 loss_N: 0.18647785 loss_f: 3.114131\n",
      "4210 Train Loss 14.497248\n",
      "4211 loss_D: 11.201617 loss_N: 0.18591978 loss_f: 3.1070476\n",
      "4211 Train Loss 14.494585\n",
      "4212 loss_D: 11.20025 loss_N: 0.1859181 loss_f: 3.106351\n",
      "4212 Train Loss 14.492518\n",
      "4213 loss_D: 11.201907 loss_N: 0.18552826 loss_f: 3.102473\n",
      "4213 Train Loss 14.489908\n",
      "4214 loss_D: 11.201398 loss_N: 0.18523791 loss_f: 3.0996473\n",
      "4214 Train Loss 14.486283\n",
      "4215 loss_D: 11.198541 loss_N: 0.18479462 loss_f: 3.1010582\n",
      "4215 Train Loss 14.484393\n",
      "4216 loss_D: 11.198592 loss_N: 0.18480518 loss_f: 3.0986927\n",
      "4216 Train Loss 14.48209\n",
      "4217 loss_D: 11.195618 loss_N: 0.18505038 loss_f: 3.098882\n",
      "4217 Train Loss 14.479549\n",
      "4218 loss_D: 11.192722 loss_N: 0.18529236 loss_f: 3.0996768\n",
      "4218 Train Loss 14.477692\n",
      "4219 loss_D: 11.185539 loss_N: 0.18583576 loss_f: 3.1034126\n",
      "4219 Train Loss 14.474788\n",
      "4220 loss_D: 11.176445 loss_N: 0.18604684 loss_f: 3.1090796\n",
      "4220 Train Loss 14.471571\n",
      "4221 loss_D: 11.173038 loss_N: 0.18559743 loss_f: 3.108228\n",
      "4221 Train Loss 14.466863\n",
      "4222 loss_D: 11.170547 loss_N: 0.18545593 loss_f: 3.1070743\n",
      "4222 Train Loss 14.463078\n",
      "4223 loss_D: 11.170694 loss_N: 0.18478505 loss_f: 3.1021898\n",
      "4223 Train Loss 14.457669\n",
      "4224 loss_D: 11.1716795 loss_N: 0.18413393 loss_f: 3.097436\n",
      "4224 Train Loss 14.453249\n",
      "4225 loss_D: 11.178107 loss_N: 0.18320021 loss_f: 3.0867734\n",
      "4225 Train Loss 14.44808\n",
      "4226 loss_D: 11.182138 loss_N: 0.18272874 loss_f: 3.0790093\n",
      "4226 Train Loss 14.443876\n",
      "4227 loss_D: 11.185017 loss_N: 0.18258332 loss_f: 3.0725074\n",
      "4227 Train Loss 14.440107\n",
      "4228 loss_D: 11.18414 loss_N: 0.18276793 loss_f: 3.0695734\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4228 Train Loss 14.436481\n",
      "4229 loss_D: 11.18079 loss_N: 0.18299906 loss_f: 3.0693502\n",
      "4229 Train Loss 14.433139\n",
      "4230 loss_D: 11.176893 loss_N: 0.18300015 loss_f: 3.0680518\n",
      "4230 Train Loss 14.427946\n",
      "4231 loss_D: 11.174651 loss_N: 0.18255296 loss_f: 3.0645766\n",
      "4231 Train Loss 14.421782\n",
      "4232 loss_D: 11.178665 loss_N: 0.18074423 loss_f: 3.0631475\n",
      "4232 Train Loss 14.422557\n",
      "4233 loss_D: 11.176386 loss_N: 0.18169329 loss_f: 3.0606859\n",
      "4233 Train Loss 14.418765\n",
      "4234 loss_D: 11.17503 loss_N: 0.18088564 loss_f: 3.0556803\n",
      "4234 Train Loss 14.411595\n",
      "4235 loss_D: 11.177478 loss_N: 0.18002608 loss_f: 3.0468972\n",
      "4235 Train Loss 14.404401\n",
      "4236 loss_D: 11.177715 loss_N: 0.17943883 loss_f: 3.0393226\n",
      "4236 Train Loss 14.396477\n",
      "4237 loss_D: 11.17735 loss_N: 0.1792913 loss_f: 3.0336823\n",
      "4237 Train Loss 14.390324\n",
      "4238 loss_D: 11.1732025 loss_N: 0.1790397 loss_f: 3.0330174\n",
      "4238 Train Loss 14.38526\n",
      "4239 loss_D: 11.165837 loss_N: 0.17873657 loss_f: 3.0361438\n",
      "4239 Train Loss 14.380718\n",
      "4240 loss_D: 11.156758 loss_N: 0.17837222 loss_f: 3.0401473\n",
      "4240 Train Loss 14.375278\n",
      "4241 loss_D: 11.141797 loss_N: 0.17747422 loss_f: 3.054312\n",
      "4241 Train Loss 14.373583\n",
      "4242 loss_D: 11.118575 loss_N: 0.1791494 loss_f: 3.0779104\n",
      "4242 Train Loss 14.375635\n",
      "4243 loss_D: 11.130519 loss_N: 0.17824651 loss_f: 3.0584054\n",
      "4243 Train Loss 14.36717\n",
      "4244 loss_D: 11.137144 loss_N: 0.17435145 loss_f: 3.1833034\n",
      "4244 Train Loss 14.494799\n",
      "4245 loss_D: 11.130997 loss_N: 0.17768219 loss_f: 3.0557206\n",
      "4245 Train Loss 14.364399\n",
      "4246 loss_D: 11.135155 loss_N: 0.17767058 loss_f: 3.0451417\n",
      "4246 Train Loss 14.357967\n",
      "4247 loss_D: 11.140832 loss_N: 0.17751798 loss_f: 3.0332546\n",
      "4247 Train Loss 14.351604\n",
      "4248 loss_D: 11.141467 loss_N: 0.17717443 loss_f: 3.025498\n",
      "4248 Train Loss 14.344139\n",
      "4249 loss_D: 11.135923 loss_N: 0.17705087 loss_f: 3.0205405\n",
      "4249 Train Loss 14.333514\n",
      "4250 loss_D: 11.146206 loss_N: 0.1758572 loss_f: 3.0004187\n",
      "4250 Train Loss 14.322482\n",
      "4251 loss_D: 11.1327915 loss_N: 0.17599736 loss_f: 3.002128\n",
      "4251 Train Loss 14.310917\n",
      "4252 loss_D: 11.125434 loss_N: 0.17531589 loss_f: 3.000666\n",
      "4252 Train Loss 14.301415\n",
      "4253 loss_D: 11.121094 loss_N: 0.17522661 loss_f: 3.000375\n",
      "4253 Train Loss 14.296695\n",
      "4254 loss_D: 11.111773 loss_N: 0.17531244 loss_f: 3.0029032\n",
      "4254 Train Loss 14.289988\n",
      "4255 loss_D: 11.106492 loss_N: 0.17552955 loss_f: 2.999961\n",
      "4255 Train Loss 14.281982\n",
      "4256 loss_D: 11.085956 loss_N: 0.17728564 loss_f: 3.0102243\n",
      "4256 Train Loss 14.273465\n",
      "4257 loss_D: 11.079948 loss_N: 0.17828158 loss_f: 3.0125418\n",
      "4257 Train Loss 14.270772\n",
      "4258 loss_D: 11.080986 loss_N: 0.178581 loss_f: 3.0070405\n",
      "4258 Train Loss 14.266607\n",
      "4259 loss_D: 11.075538 loss_N: 0.1790526 loss_f: 3.0102835\n",
      "4259 Train Loss 14.2648735\n",
      "4260 loss_D: 11.07365 loss_N: 0.17920102 loss_f: 3.0103796\n",
      "4260 Train Loss 14.263231\n",
      "4261 loss_D: 11.067695 loss_N: 0.17942284 loss_f: 3.0130687\n",
      "4261 Train Loss 14.260185\n",
      "4262 loss_D: 11.059764 loss_N: 0.17966184 loss_f: 3.0179074\n",
      "4262 Train Loss 14.257333\n",
      "4263 loss_D: 11.051819 loss_N: 0.17940918 loss_f: 3.0223594\n",
      "4263 Train Loss 14.253588\n",
      "4264 loss_D: 11.040198 loss_N: 0.18012527 loss_f: 3.0319097\n",
      "4264 Train Loss 14.2522335\n",
      "4265 loss_D: 11.044427 loss_N: 0.17919911 loss_f: 3.0241387\n",
      "4265 Train Loss 14.247765\n",
      "4266 loss_D: 11.046222 loss_N: 0.17884415 loss_f: 3.0199144\n",
      "4266 Train Loss 14.244981\n",
      "4267 loss_D: 11.055233 loss_N: 0.1784533 loss_f: 3.00672\n",
      "4267 Train Loss 14.240406\n",
      "4268 loss_D: 11.089278 loss_N: 0.17170744 loss_f: 3.1218975\n",
      "4268 Train Loss 14.382883\n",
      "4269 loss_D: 11.057078 loss_N: 0.17778476 loss_f: 3.0042531\n",
      "4269 Train Loss 14.239117\n",
      "4270 loss_D: 11.05232 loss_N: 0.17828771 loss_f: 3.0048034\n",
      "4270 Train Loss 14.235411\n",
      "4271 loss_D: 11.051665 loss_N: 0.17891009 loss_f: 3.000166\n",
      "4271 Train Loss 14.2307415\n",
      "4272 loss_D: 11.049159 loss_N: 0.17923798 loss_f: 2.9979508\n",
      "4272 Train Loss 14.226348\n",
      "4273 loss_D: 11.045485 loss_N: 0.18015897 loss_f: 2.9935136\n",
      "4273 Train Loss 14.219156\n",
      "4274 loss_D: 11.04239 loss_N: 0.18034695 loss_f: 2.9907458\n",
      "4274 Train Loss 14.213482\n",
      "4275 loss_D: 11.040441 loss_N: 0.18035598 loss_f: 2.9867346\n",
      "4275 Train Loss 14.207531\n",
      "4276 loss_D: 11.042096 loss_N: 0.179961 loss_f: 2.9816377\n",
      "4276 Train Loss 14.203695\n",
      "4277 loss_D: 11.042021 loss_N: 0.1793092 loss_f: 2.9793458\n",
      "4277 Train Loss 14.200676\n",
      "4278 loss_D: 11.032773 loss_N: 0.17890015 loss_f: 2.984872\n",
      "4278 Train Loss 14.196545\n",
      "4279 loss_D: 11.030341 loss_N: 0.17870595 loss_f: 2.9846156\n",
      "4279 Train Loss 14.193663\n",
      "4280 loss_D: 11.022429 loss_N: 0.17918718 loss_f: 2.9899437\n",
      "4280 Train Loss 14.19156\n",
      "4281 loss_D: 11.017798 loss_N: 0.1792427 loss_f: 2.993218\n",
      "4281 Train Loss 14.19026\n",
      "4282 loss_D: 11.007581 loss_N: 0.17972729 loss_f: 3.0004015\n",
      "4282 Train Loss 14.18771\n",
      "4283 loss_D: 10.997642 loss_N: 0.17982936 loss_f: 3.0073133\n",
      "4283 Train Loss 14.184784\n",
      "4284 loss_D: 10.982155 loss_N: 0.18022126 loss_f: 3.0187597\n",
      "4284 Train Loss 14.181136\n",
      "4285 loss_D: 10.971568 loss_N: 0.18054208 loss_f: 3.02339\n",
      "4285 Train Loss 14.1755\n",
      "4286 loss_D: 10.960287 loss_N: 0.18089499 loss_f: 3.027425\n",
      "4286 Train Loss 14.168607\n",
      "4287 loss_D: 10.956198 loss_N: 0.1810035 loss_f: 3.0261617\n",
      "4287 Train Loss 14.1633625\n",
      "4288 loss_D: 10.949689 loss_N: 0.18113624 loss_f: 3.0285928\n",
      "4288 Train Loss 14.159418\n",
      "4289 loss_D: 10.956133 loss_N: 0.18031219 loss_f: 3.0203545\n",
      "4289 Train Loss 14.156799\n",
      "4290 loss_D: 10.949441 loss_N: 0.1802982 loss_f: 3.023237\n",
      "4290 Train Loss 14.152976\n",
      "4291 loss_D: 10.947791 loss_N: 0.17990327 loss_f: 3.0211976\n",
      "4291 Train Loss 14.148891\n",
      "4292 loss_D: 10.942723 loss_N: 0.17963156 loss_f: 3.0229979\n",
      "4292 Train Loss 14.145352\n",
      "4293 loss_D: 10.938003 loss_N: 0.17967448 loss_f: 3.0254438\n",
      "4293 Train Loss 14.143121\n",
      "4294 loss_D: 10.934111 loss_N: 0.17964403 loss_f: 3.0268726\n",
      "4294 Train Loss 14.140627\n",
      "4295 loss_D: 10.928219 loss_N: 0.17974266 loss_f: 3.0286832\n",
      "4295 Train Loss 14.136644\n",
      "4296 loss_D: 10.923146 loss_N: 0.17973308 loss_f: 3.0287302\n",
      "4296 Train Loss 14.13161\n",
      "4297 loss_D: 10.910851 loss_N: 0.18041912 loss_f: 3.037762\n",
      "4297 Train Loss 14.129031\n",
      "4298 loss_D: 10.911078 loss_N: 0.18031007 loss_f: 3.0332444\n",
      "4298 Train Loss 14.124633\n",
      "4299 loss_D: 10.9161825 loss_N: 0.18016142 loss_f: 3.0258114\n",
      "4299 Train Loss 14.122155\n",
      "4300 loss_D: 10.91144 loss_N: 0.18055445 loss_f: 3.0277467\n",
      "4300 Train Loss 14.119741\n",
      "4301 loss_D: 10.907621 loss_N: 0.18089147 loss_f: 3.0268703\n",
      "4301 Train Loss 14.115383\n",
      "4302 loss_D: 10.898049 loss_N: 0.18131588 loss_f: 3.0324135\n",
      "4302 Train Loss 14.111778\n",
      "4303 loss_D: 10.893841 loss_N: 0.18191633 loss_f: 3.0396287\n",
      "4303 Train Loss 14.115386\n",
      "4304 loss_D: 10.8965025 loss_N: 0.18152633 loss_f: 3.0321307\n",
      "4304 Train Loss 14.110159\n",
      "4305 loss_D: 10.88367 loss_N: 0.1822517 loss_f: 3.041277\n",
      "4305 Train Loss 14.107199\n",
      "4306 loss_D: 10.878035 loss_N: 0.18208894 loss_f: 3.044202\n",
      "4306 Train Loss 14.104325\n",
      "4307 loss_D: 10.870352 loss_N: 0.18194523 loss_f: 3.046973\n",
      "4307 Train Loss 14.09927\n",
      "4308 loss_D: 10.857923 loss_N: 0.18356864 loss_f: 3.0723612\n",
      "4308 Train Loss 14.1138525\n",
      "4309 loss_D: 10.866571 loss_N: 0.18236391 loss_f: 3.0471103\n",
      "4309 Train Loss 14.0960455\n",
      "4310 loss_D: 10.860388 loss_N: 0.18214096 loss_f: 3.0495129\n",
      "4310 Train Loss 14.092042\n",
      "4311 loss_D: 10.857237 loss_N: 0.18177031 loss_f: 3.048504\n",
      "4311 Train Loss 14.087511\n",
      "4312 loss_D: 10.8547735 loss_N: 0.18167138 loss_f: 3.0470362\n",
      "4312 Train Loss 14.083481\n",
      "4313 loss_D: 10.855279 loss_N: 0.18140888 loss_f: 3.0425932\n",
      "4313 Train Loss 14.079281\n",
      "4314 loss_D: 10.848898 loss_N: 0.18149734 loss_f: 3.0438848\n",
      "4314 Train Loss 14.07428\n",
      "4315 loss_D: 10.847272 loss_N: 0.1814152 loss_f: 3.0410697\n",
      "4315 Train Loss 14.069757\n",
      "4316 loss_D: 10.837894 loss_N: 0.1817967 loss_f: 3.04622\n",
      "4316 Train Loss 14.065911\n",
      "4317 loss_D: 10.836106 loss_N: 0.18200268 loss_f: 3.0446172\n",
      "4317 Train Loss 14.062727\n",
      "4318 loss_D: 10.826514 loss_N: 0.18281549 loss_f: 3.0502272\n",
      "4318 Train Loss 14.059557\n",
      "4319 loss_D: 10.82592 loss_N: 0.18311943 loss_f: 3.0482318\n",
      "4319 Train Loss 14.057272\n",
      "4320 loss_D: 10.822582 loss_N: 0.18356289 loss_f: 3.0482302\n",
      "4320 Train Loss 14.054376\n",
      "4321 loss_D: 10.821902 loss_N: 0.18386486 loss_f: 3.0454874\n",
      "4321 Train Loss 14.051254\n",
      "4322 loss_D: 10.820569 loss_N: 0.18397436 loss_f: 3.0439332\n",
      "4322 Train Loss 14.048476\n",
      "4323 loss_D: 10.79517 loss_N: 0.18597294 loss_f: 3.1110454\n",
      "4323 Train Loss 14.092188\n",
      "4324 loss_D: 10.816947 loss_N: 0.18418561 loss_f: 3.0463486\n",
      "4324 Train Loss 14.047482\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4325 loss_D: 10.816723 loss_N: 0.18399358 loss_f: 3.0441062\n",
      "4325 Train Loss 14.044823\n",
      "4326 loss_D: 10.814081 loss_N: 0.1837385 loss_f: 3.042076\n",
      "4326 Train Loss 14.039896\n",
      "4327 loss_D: 10.81162 loss_N: 0.18370172 loss_f: 3.0407858\n",
      "4327 Train Loss 14.036107\n",
      "4328 loss_D: 10.805192 loss_N: 0.18411931 loss_f: 3.0429666\n",
      "4328 Train Loss 14.032278\n",
      "4329 loss_D: 10.799225 loss_N: 0.18419807 loss_f: 3.0451245\n",
      "4329 Train Loss 14.028547\n",
      "4330 loss_D: 10.794437 loss_N: 0.18529768 loss_f: 3.0447745\n",
      "4330 Train Loss 14.024509\n",
      "4331 loss_D: 10.789478 loss_N: 0.18581393 loss_f: 3.0459447\n",
      "4331 Train Loss 14.021236\n",
      "4332 loss_D: 10.785389 loss_N: 0.1865174 loss_f: 3.0448146\n",
      "4332 Train Loss 14.016722\n",
      "4333 loss_D: 10.770293 loss_N: 0.18675837 loss_f: 3.0573378\n",
      "4333 Train Loss 14.014389\n",
      "4334 loss_D: 10.759845 loss_N: 0.18745671 loss_f: 3.058579\n",
      "4334 Train Loss 14.00588\n",
      "4335 loss_D: 10.767585 loss_N: 0.18666701 loss_f: 3.046793\n",
      "4335 Train Loss 14.001045\n",
      "4336 loss_D: 10.758387 loss_N: 0.1866204 loss_f: 3.0518363\n",
      "4336 Train Loss 13.996843\n",
      "4337 loss_D: 10.753737 loss_N: 0.18644924 loss_f: 3.0532467\n",
      "4337 Train Loss 13.993433\n",
      "4338 loss_D: 10.743172 loss_N: 0.18695112 loss_f: 3.05753\n",
      "4338 Train Loss 13.987652\n",
      "4339 loss_D: 10.724542 loss_N: 0.18727006 loss_f: 3.0669625\n",
      "4339 Train Loss 13.978774\n",
      "4340 loss_D: 10.72583 loss_N: 0.18629625 loss_f: 3.061304\n",
      "4340 Train Loss 13.973431\n",
      "4341 loss_D: 10.718504 loss_N: 0.18602498 loss_f: 3.065332\n",
      "4341 Train Loss 13.96986\n",
      "4342 loss_D: 10.716103 loss_N: 0.185498 loss_f: 3.0643897\n",
      "4342 Train Loss 13.96599\n",
      "4343 loss_D: 10.711761 loss_N: 0.1848952 loss_f: 3.0648553\n",
      "4343 Train Loss 13.961513\n",
      "4344 loss_D: 10.705308 loss_N: 0.18491681 loss_f: 3.067455\n",
      "4344 Train Loss 13.95768\n",
      "4345 loss_D: 10.689846 loss_N: 0.18480413 loss_f: 3.0781484\n",
      "4345 Train Loss 13.952799\n",
      "4346 loss_D: 10.685314 loss_N: 0.18515833 loss_f: 3.0777946\n",
      "4346 Train Loss 13.948267\n",
      "4347 loss_D: 10.678167 loss_N: 0.18487017 loss_f: 3.0815706\n",
      "4347 Train Loss 13.944608\n",
      "4348 loss_D: 10.673367 loss_N: 0.1844404 loss_f: 3.0822191\n",
      "4348 Train Loss 13.940026\n",
      "4349 loss_D: 10.664995 loss_N: 0.18397251 loss_f: 3.0858984\n",
      "4349 Train Loss 13.934866\n",
      "4350 loss_D: 10.660405 loss_N: 0.18343157 loss_f: 3.0853808\n",
      "4350 Train Loss 13.929217\n",
      "4351 loss_D: 10.64964 loss_N: 0.18369105 loss_f: 3.0861487\n",
      "4351 Train Loss 13.919479\n",
      "4352 loss_D: 10.643896 loss_N: 0.18414155 loss_f: 3.0756378\n",
      "4352 Train Loss 13.903675\n",
      "4353 loss_D: 10.631392 loss_N: 0.1838145 loss_f: 3.0734022\n",
      "4353 Train Loss 13.888608\n",
      "4354 loss_D: 10.6326 loss_N: 0.18766624 loss_f: 3.072313\n",
      "4354 Train Loss 13.892579\n",
      "4355 loss_D: 10.631477 loss_N: 0.18537492 loss_f: 3.063381\n",
      "4355 Train Loss 13.880234\n",
      "4356 loss_D: 10.653866 loss_N: 0.18671823 loss_f: 3.0336044\n",
      "4356 Train Loss 13.874188\n",
      "4357 loss_D: 10.649226 loss_N: 0.18479289 loss_f: 3.0268483\n",
      "4357 Train Loss 13.860867\n",
      "4358 loss_D: 10.639455 loss_N: 0.18450913 loss_f: 3.0293005\n",
      "4358 Train Loss 13.853265\n",
      "4359 loss_D: 10.6353655 loss_N: 0.18427853 loss_f: 3.0278525\n",
      "4359 Train Loss 13.847496\n",
      "4360 loss_D: 10.622479 loss_N: 0.18343905 loss_f: 3.0316138\n",
      "4360 Train Loss 13.837532\n",
      "4361 loss_D: 10.615132 loss_N: 0.18351746 loss_f: 3.0348725\n",
      "4361 Train Loss 13.833523\n",
      "4362 loss_D: 10.598127 loss_N: 0.18429269 loss_f: 3.0454044\n",
      "4362 Train Loss 13.827825\n",
      "4363 loss_D: 10.588027 loss_N: 0.18431042 loss_f: 3.0525982\n",
      "4363 Train Loss 13.824935\n",
      "4364 loss_D: 10.576058 loss_N: 0.18461934 loss_f: 3.061242\n",
      "4364 Train Loss 13.821919\n",
      "4365 loss_D: 10.562455 loss_N: 0.18462712 loss_f: 3.0657923\n",
      "4365 Train Loss 13.812875\n",
      "4366 loss_D: 10.579089 loss_N: 0.18375178 loss_f: 3.0462346\n",
      "4366 Train Loss 13.809076\n",
      "4367 loss_D: 10.567942 loss_N: 0.18366139 loss_f: 3.0520725\n",
      "4367 Train Loss 13.803676\n",
      "4368 loss_D: 10.551575 loss_N: 0.18308806 loss_f: 3.0629516\n",
      "4368 Train Loss 13.797615\n",
      "4369 loss_D: 10.54212 loss_N: 0.1824675 loss_f: 3.0678966\n",
      "4369 Train Loss 13.792484\n",
      "4370 loss_D: 10.535818 loss_N: 0.18150334 loss_f: 3.0681753\n",
      "4370 Train Loss 13.785497\n",
      "4371 loss_D: 10.5297985 loss_N: 0.1811943 loss_f: 3.0664241\n",
      "4371 Train Loss 13.777417\n",
      "4372 loss_D: 10.5308 loss_N: 0.18129626 loss_f: 3.0582018\n",
      "4372 Train Loss 13.770298\n",
      "4373 loss_D: 10.524698 loss_N: 0.1815801 loss_f: 3.055559\n",
      "4373 Train Loss 13.761838\n",
      "4374 loss_D: 10.523177 loss_N: 0.18210992 loss_f: 3.0505545\n",
      "4374 Train Loss 13.755841\n",
      "4375 loss_D: 10.511255 loss_N: 0.18289253 loss_f: 3.0573149\n",
      "4375 Train Loss 13.751463\n",
      "4376 loss_D: 10.505606 loss_N: 0.18326394 loss_f: 3.0591917\n",
      "4376 Train Loss 13.748061\n",
      "4377 loss_D: 10.497724 loss_N: 0.18345149 loss_f: 3.0647695\n",
      "4377 Train Loss 13.745945\n",
      "4378 loss_D: 10.492898 loss_N: 0.1835871 loss_f: 3.067848\n",
      "4378 Train Loss 13.744333\n",
      "4379 loss_D: 10.486437 loss_N: 0.18400887 loss_f: 3.0704222\n",
      "4379 Train Loss 13.740868\n",
      "4380 loss_D: 10.477904 loss_N: 0.18483977 loss_f: 3.0735428\n",
      "4380 Train Loss 13.736287\n",
      "4381 loss_D: 10.472955 loss_N: 0.18538782 loss_f: 3.0737312\n",
      "4381 Train Loss 13.732074\n",
      "4382 loss_D: 10.467307 loss_N: 0.18581878 loss_f: 3.0757334\n",
      "4382 Train Loss 13.728859\n",
      "4383 loss_D: 10.464337 loss_N: 0.18532273 loss_f: 3.0756319\n",
      "4383 Train Loss 13.725292\n",
      "4384 loss_D: 10.457179 loss_N: 0.18453214 loss_f: 3.0805275\n",
      "4384 Train Loss 13.722239\n",
      "4385 loss_D: 10.452551 loss_N: 0.18344639 loss_f: 3.0822718\n",
      "4385 Train Loss 13.718268\n",
      "4386 loss_D: 10.443656 loss_N: 0.18189211 loss_f: 3.0897758\n",
      "4386 Train Loss 13.715324\n",
      "4387 loss_D: 10.44101 loss_N: 0.181878 loss_f: 3.089339\n",
      "4387 Train Loss 13.712228\n",
      "4388 loss_D: 10.438213 loss_N: 0.1820398 loss_f: 3.0875359\n",
      "4388 Train Loss 13.707789\n",
      "4389 loss_D: 10.428746 loss_N: 0.18238659 loss_f: 3.0921428\n",
      "4389 Train Loss 13.703276\n",
      "4390 loss_D: 10.429027 loss_N: 0.18270488 loss_f: 3.0864766\n",
      "4390 Train Loss 13.698208\n",
      "4391 loss_D: 10.415032 loss_N: 0.18330276 loss_f: 3.0956714\n",
      "4391 Train Loss 13.694007\n",
      "4392 loss_D: 10.414351 loss_N: 0.18330188 loss_f: 3.0914483\n",
      "4392 Train Loss 13.689102\n",
      "4393 loss_D: 10.417823 loss_N: 0.18376161 loss_f: 3.0836766\n",
      "4393 Train Loss 13.685261\n",
      "4394 loss_D: 10.414671 loss_N: 0.1839036 loss_f: 3.0829313\n",
      "4394 Train Loss 13.681506\n",
      "4395 loss_D: 10.409103 loss_N: 0.18404067 loss_f: 3.0841238\n",
      "4395 Train Loss 13.677268\n",
      "4396 loss_D: 10.400551 loss_N: 0.18457441 loss_f: 3.0869849\n",
      "4396 Train Loss 13.67211\n",
      "4397 loss_D: 10.390483 loss_N: 0.18557425 loss_f: 3.0864637\n",
      "4397 Train Loss 13.662521\n",
      "4398 loss_D: 10.380248 loss_N: 0.18650268 loss_f: 3.0864966\n",
      "4398 Train Loss 13.653247\n",
      "4399 loss_D: 10.379717 loss_N: 0.18687578 loss_f: 3.0803623\n",
      "4399 Train Loss 13.646955\n",
      "4400 loss_D: 10.380199 loss_N: 0.18706535 loss_f: 3.075216\n",
      "4400 Train Loss 13.642481\n",
      "4401 loss_D: 10.382265 loss_N: 0.18692142 loss_f: 3.069406\n",
      "4401 Train Loss 13.638592\n",
      "4402 loss_D: 10.383959 loss_N: 0.18688251 loss_f: 3.06451\n",
      "4402 Train Loss 13.635352\n",
      "4403 loss_D: 10.389328 loss_N: 0.187134 loss_f: 3.0658984\n",
      "4403 Train Loss 13.642361\n",
      "4404 loss_D: 10.385331 loss_N: 0.18694228 loss_f: 3.062296\n",
      "4404 Train Loss 13.634569\n",
      "4405 loss_D: 10.375875 loss_N: 0.18745963 loss_f: 3.0661066\n",
      "4405 Train Loss 13.629442\n",
      "4406 loss_D: 10.372117 loss_N: 0.18777929 loss_f: 3.0664089\n",
      "4406 Train Loss 13.626306\n",
      "4407 loss_D: 10.36597 loss_N: 0.18816599 loss_f: 3.0679371\n",
      "4407 Train Loss 13.622072\n",
      "4408 loss_D: 10.35609 loss_N: 0.18846174 loss_f: 3.0728807\n",
      "4408 Train Loss 13.617432\n",
      "4409 loss_D: 10.35413 loss_N: 0.18896802 loss_f: 3.0706558\n",
      "4409 Train Loss 13.613753\n",
      "4410 loss_D: 10.350108 loss_N: 0.18907942 loss_f: 3.0690835\n",
      "4410 Train Loss 13.608271\n",
      "4411 loss_D: 10.348652 loss_N: 0.18931472 loss_f: 3.0681655\n",
      "4411 Train Loss 13.6061325\n",
      "4412 loss_D: 10.341295 loss_N: 0.18947852 loss_f: 3.0716453\n",
      "4412 Train Loss 13.602419\n",
      "4413 loss_D: 10.332673 loss_N: 0.19135432 loss_f: 3.0732417\n",
      "4413 Train Loss 13.597269\n",
      "4414 loss_D: 10.321074 loss_N: 0.19223073 loss_f: 3.081505\n",
      "4414 Train Loss 13.59481\n",
      "4415 loss_D: 10.31252 loss_N: 0.19243582 loss_f: 3.0854769\n",
      "4415 Train Loss 13.590433\n",
      "4416 loss_D: 10.317237 loss_N: 0.19237787 loss_f: 3.077492\n",
      "4416 Train Loss 13.587107\n",
      "4417 loss_D: 10.3109455 loss_N: 0.19243717 loss_f: 3.0812483\n",
      "4417 Train Loss 13.584631\n",
      "4418 loss_D: 10.303945 loss_N: 0.19204229 loss_f: 3.0859253\n",
      "4418 Train Loss 13.581912\n",
      "4419 loss_D: 10.302407 loss_N: 0.19169258 loss_f: 3.085542\n",
      "4419 Train Loss 13.579641\n",
      "4420 loss_D: 10.295478 loss_N: 0.19150828 loss_f: 3.0897832\n",
      "4420 Train Loss 13.576769\n",
      "4421 loss_D: 10.293062 loss_N: 0.1914039 loss_f: 3.0880418\n",
      "4421 Train Loss 13.572508\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4422 loss_D: 10.282977 loss_N: 0.19183257 loss_f: 3.0934665\n",
      "4422 Train Loss 13.568276\n",
      "4423 loss_D: 10.282392 loss_N: 0.19202249 loss_f: 3.090548\n",
      "4423 Train Loss 13.564962\n",
      "4424 loss_D: 10.278856 loss_N: 0.19267426 loss_f: 3.0870664\n",
      "4424 Train Loss 13.558598\n",
      "4425 loss_D: 10.275212 loss_N: 0.19249426 loss_f: 3.0872808\n",
      "4425 Train Loss 13.554987\n",
      "4426 loss_D: 10.271272 loss_N: 0.19215527 loss_f: 3.0869746\n",
      "4426 Train Loss 13.550402\n",
      "4427 loss_D: 10.268253 loss_N: 0.19148248 loss_f: 3.0866039\n",
      "4427 Train Loss 13.54634\n",
      "4428 loss_D: 10.265089 loss_N: 0.19051793 loss_f: 3.086246\n",
      "4428 Train Loss 13.541853\n",
      "4429 loss_D: 10.225513 loss_N: 0.18837106 loss_f: 3.205977\n",
      "4429 Train Loss 13.619862\n",
      "4430 loss_D: 10.260561 loss_N: 0.1902643 loss_f: 3.0898201\n",
      "4430 Train Loss 13.540646\n",
      "4431 loss_D: 10.260711 loss_N: 0.18945502 loss_f: 3.086147\n",
      "4431 Train Loss 13.536313\n",
      "4432 loss_D: 10.25435 loss_N: 0.18937498 loss_f: 3.0886772\n",
      "4432 Train Loss 13.532402\n",
      "4433 loss_D: 10.250943 loss_N: 0.18893978 loss_f: 3.0892615\n",
      "4433 Train Loss 13.529144\n",
      "4434 loss_D: 10.24597 loss_N: 0.18913588 loss_f: 3.0919304\n",
      "4434 Train Loss 13.527036\n",
      "4435 loss_D: 10.24097 loss_N: 0.18951394 loss_f: 3.0930927\n",
      "4435 Train Loss 13.523577\n",
      "4436 loss_D: 10.243103 loss_N: 0.18860789 loss_f: 3.092576\n",
      "4436 Train Loss 13.524287\n",
      "4437 loss_D: 10.241909 loss_N: 0.18907173 loss_f: 3.0906563\n",
      "4437 Train Loss 13.521637\n",
      "4438 loss_D: 10.239829 loss_N: 0.18909515 loss_f: 3.0897973\n",
      "4438 Train Loss 13.518722\n",
      "4439 loss_D: 10.241959 loss_N: 0.18882813 loss_f: 3.0844698\n",
      "4439 Train Loss 13.515257\n",
      "4440 loss_D: 10.242037 loss_N: 0.1884392 loss_f: 3.0815313\n",
      "4440 Train Loss 13.512008\n",
      "4441 loss_D: 10.2420025 loss_N: 0.18802167 loss_f: 3.0809379\n",
      "4441 Train Loss 13.510962\n",
      "4442 loss_D: 10.242188 loss_N: 0.18767133 loss_f: 3.0771866\n",
      "4442 Train Loss 13.507047\n",
      "4443 loss_D: 10.237134 loss_N: 0.18762602 loss_f: 3.0789008\n",
      "4443 Train Loss 13.50366\n",
      "4444 loss_D: 10.233888 loss_N: 0.18694592 loss_f: 3.0784886\n",
      "4444 Train Loss 13.499322\n",
      "4445 loss_D: 10.229424 loss_N: 0.1862859 loss_f: 3.0799425\n",
      "4445 Train Loss 13.495653\n",
      "4446 loss_D: 10.222084 loss_N: 0.18634719 loss_f: 3.0843308\n",
      "4446 Train Loss 13.492762\n",
      "4447 loss_D: 10.203314 loss_N: 0.18633988 loss_f: 3.1043527\n",
      "4447 Train Loss 13.494007\n",
      "4448 loss_D: 10.213694 loss_N: 0.1863428 loss_f: 3.0906303\n",
      "4448 Train Loss 13.490667\n",
      "4449 loss_D: 10.210848 loss_N: 0.18630515 loss_f: 3.090864\n",
      "4449 Train Loss 13.488017\n",
      "4450 loss_D: 10.209909 loss_N: 0.18655519 loss_f: 3.0887172\n",
      "4450 Train Loss 13.485182\n",
      "4451 loss_D: 10.211374 loss_N: 0.18694045 loss_f: 3.0850425\n",
      "4451 Train Loss 13.483356\n",
      "4452 loss_D: 10.211638 loss_N: 0.1875311 loss_f: 3.0819693\n",
      "4452 Train Loss 13.481139\n",
      "4453 loss_D: 10.210742 loss_N: 0.18821427 loss_f: 3.07895\n",
      "4453 Train Loss 13.477906\n",
      "4454 loss_D: 10.203635 loss_N: 0.18846163 loss_f: 3.0823696\n",
      "4454 Train Loss 13.474466\n",
      "4455 loss_D: 10.195225 loss_N: 0.1896032 loss_f: 3.0896876\n",
      "4455 Train Loss 13.474515\n",
      "4456 loss_D: 10.1991625 loss_N: 0.18904261 loss_f: 3.083355\n",
      "4456 Train Loss 13.4715605\n",
      "4457 loss_D: 10.195087 loss_N: 0.18884389 loss_f: 3.083535\n",
      "4457 Train Loss 13.467466\n",
      "4458 loss_D: 10.183097 loss_N: 0.18821633 loss_f: 3.0875475\n",
      "4458 Train Loss 13.45886\n",
      "4459 loss_D: 10.170046 loss_N: 0.18780807 loss_f: 3.094823\n",
      "4459 Train Loss 13.452677\n",
      "4460 loss_D: 10.166221 loss_N: 0.18829814 loss_f: 3.091341\n",
      "4460 Train Loss 13.44586\n",
      "4461 loss_D: 10.155732 loss_N: 0.18947792 loss_f: 3.0930123\n",
      "4461 Train Loss 13.438223\n",
      "4462 loss_D: 10.152123 loss_N: 0.19032209 loss_f: 3.0929253\n",
      "4462 Train Loss 13.43537\n",
      "4463 loss_D: 10.152816 loss_N: 0.19060527 loss_f: 3.0892031\n",
      "4463 Train Loss 13.432624\n",
      "4464 loss_D: 10.143621 loss_N: 0.19114289 loss_f: 3.0928013\n",
      "4464 Train Loss 13.427566\n",
      "4465 loss_D: 10.142503 loss_N: 0.1912666 loss_f: 3.0908504\n",
      "4465 Train Loss 13.42462\n",
      "4466 loss_D: 10.133358 loss_N: 0.19151811 loss_f: 3.0955305\n",
      "4466 Train Loss 13.420406\n",
      "4467 loss_D: 10.128342 loss_N: 0.19170758 loss_f: 3.096965\n",
      "4467 Train Loss 13.417014\n",
      "4468 loss_D: 10.121841 loss_N: 0.19196527 loss_f: 3.0979145\n",
      "4468 Train Loss 13.411721\n",
      "4469 loss_D: 10.103126 loss_N: 0.19284667 loss_f: 3.1210892\n",
      "4469 Train Loss 13.417061\n",
      "4470 loss_D: 10.114437 loss_N: 0.19229011 loss_f: 3.1013916\n",
      "4470 Train Loss 13.408119\n",
      "4471 loss_D: 10.112787 loss_N: 0.19196625 loss_f: 3.0966527\n",
      "4471 Train Loss 13.401406\n",
      "4472 loss_D: 10.111948 loss_N: 0.19108398 loss_f: 3.0922658\n",
      "4472 Train Loss 13.395298\n",
      "4473 loss_D: 10.108124 loss_N: 0.19013427 loss_f: 3.0913374\n",
      "4473 Train Loss 13.389595\n",
      "4474 loss_D: 10.111661 loss_N: 0.18962024 loss_f: 3.083236\n",
      "4474 Train Loss 13.384517\n",
      "4475 loss_D: 10.093885 loss_N: 0.18804863 loss_f: 3.1004155\n",
      "4475 Train Loss 13.382349\n",
      "4476 loss_D: 10.095716 loss_N: 0.18839858 loss_f: 3.0863414\n",
      "4476 Train Loss 13.370457\n",
      "4477 loss_D: 10.096946 loss_N: 0.1890502 loss_f: 3.0797238\n",
      "4477 Train Loss 13.365719\n",
      "4478 loss_D: 10.1011715 loss_N: 0.18885781 loss_f: 3.07008\n",
      "4478 Train Loss 13.360109\n",
      "4479 loss_D: 10.106317 loss_N: 0.18828975 loss_f: 3.0614834\n",
      "4479 Train Loss 13.35609\n",
      "4480 loss_D: 10.111157 loss_N: 0.18750888 loss_f: 3.0537248\n",
      "4480 Train Loss 13.35239\n",
      "4481 loss_D: 10.118992 loss_N: 0.18634918 loss_f: 3.0428581\n",
      "4481 Train Loss 13.348199\n",
      "4482 loss_D: 10.127585 loss_N: 0.18549061 loss_f: 3.0304153\n",
      "4482 Train Loss 13.343492\n",
      "4483 loss_D: 10.136378 loss_N: 0.18461351 loss_f: 3.0175686\n",
      "4483 Train Loss 13.33856\n",
      "4484 loss_D: 10.14151 loss_N: 0.18449269 loss_f: 3.0088906\n",
      "4484 Train Loss 13.334894\n",
      "4485 loss_D: 10.142945 loss_N: 0.18427156 loss_f: 3.0036917\n",
      "4485 Train Loss 13.330909\n",
      "4486 loss_D: 10.145542 loss_N: 0.18418422 loss_f: 2.997267\n",
      "4486 Train Loss 13.326993\n",
      "4487 loss_D: 10.145738 loss_N: 0.1840836 loss_f: 2.992207\n",
      "4487 Train Loss 13.322029\n",
      "4488 loss_D: 10.147053 loss_N: 0.18350196 loss_f: 2.985975\n",
      "4488 Train Loss 13.31653\n",
      "4489 loss_D: 10.143976 loss_N: 0.18321013 loss_f: 2.9835455\n",
      "4489 Train Loss 13.310732\n",
      "4490 loss_D: 10.143829 loss_N: 0.18212233 loss_f: 2.977836\n",
      "4490 Train Loss 13.303787\n",
      "4491 loss_D: 10.1419735 loss_N: 0.18182737 loss_f: 2.9759758\n",
      "4491 Train Loss 13.299777\n",
      "4492 loss_D: 10.135464 loss_N: 0.18163036 loss_f: 2.9780967\n",
      "4492 Train Loss 13.295191\n",
      "4493 loss_D: 10.130968 loss_N: 0.1813071 loss_f: 2.9816234\n",
      "4493 Train Loss 13.293899\n",
      "4494 loss_D: 10.129164 loss_N: 0.18160893 loss_f: 2.9811542\n",
      "4494 Train Loss 13.291927\n",
      "4495 loss_D: 10.130065 loss_N: 0.18186103 loss_f: 2.978211\n",
      "4495 Train Loss 13.290136\n",
      "4496 loss_D: 10.134567 loss_N: 0.182131 loss_f: 2.970386\n",
      "4496 Train Loss 13.287085\n",
      "4497 loss_D: 10.14213 loss_N: 0.18229845 loss_f: 2.9592836\n",
      "4497 Train Loss 13.283712\n",
      "4498 loss_D: 10.150413 loss_N: 0.18193744 loss_f: 2.9473593\n",
      "4498 Train Loss 13.279709\n",
      "4499 loss_D: 10.154764 loss_N: 0.18154517 loss_f: 2.9391081\n",
      "4499 Train Loss 13.275417\n",
      "4500 loss_D: 10.151888 loss_N: 0.18047282 loss_f: 2.9380465\n",
      "4500 Train Loss 13.270407\n",
      "4501 loss_D: 10.139949 loss_N: 0.17973329 loss_f: 2.9474597\n",
      "4501 Train Loss 13.267141\n",
      "4502 loss_D: 10.137131 loss_N: 0.17840666 loss_f: 2.9544675\n",
      "4502 Train Loss 13.270005\n",
      "4503 loss_D: 10.138762 loss_N: 0.17920233 loss_f: 2.9473066\n",
      "4503 Train Loss 13.265271\n",
      "4504 loss_D: 10.13051 loss_N: 0.1791529 loss_f: 2.9523666\n",
      "4504 Train Loss 13.26203\n",
      "4505 loss_D: 10.124482 loss_N: 0.17902151 loss_f: 2.956044\n",
      "4505 Train Loss 13.259548\n",
      "4506 loss_D: 10.118974 loss_N: 0.1789874 loss_f: 2.9593117\n",
      "4506 Train Loss 13.257273\n",
      "4507 loss_D: 10.11345 loss_N: 0.17809619 loss_f: 2.9604042\n",
      "4507 Train Loss 13.25195\n",
      "4508 loss_D: 10.084096 loss_N: 0.17673327 loss_f: 3.0037498\n",
      "4508 Train Loss 13.264579\n",
      "4509 loss_D: 10.105876 loss_N: 0.17774612 loss_f: 2.9665163\n",
      "4509 Train Loss 13.250138\n",
      "4510 loss_D: 10.108019 loss_N: 0.17638202 loss_f: 2.9676366\n",
      "4510 Train Loss 13.252037\n",
      "4511 loss_D: 10.106532 loss_N: 0.17714036 loss_f: 2.9634047\n",
      "4511 Train Loss 13.247077\n",
      "4512 loss_D: 10.105898 loss_N: 0.17690298 loss_f: 2.9613035\n",
      "4512 Train Loss 13.244104\n",
      "4513 loss_D: 10.101647 loss_N: 0.17660387 loss_f: 2.9629889\n",
      "4513 Train Loss 13.2412405\n",
      "4514 loss_D: 10.098229 loss_N: 0.17657492 loss_f: 2.9642644\n",
      "4514 Train Loss 13.239069\n",
      "4515 loss_D: 10.091912 loss_N: 0.17667842 loss_f: 2.9678113\n",
      "4515 Train Loss 13.2364025\n",
      "4516 loss_D: 10.086227 loss_N: 0.17674129 loss_f: 2.9722261\n",
      "4516 Train Loss 13.235195\n",
      "4517 loss_D: 10.082353 loss_N: 0.1769636 loss_f: 2.9741428\n",
      "4517 Train Loss 13.233459\n",
      "4518 loss_D: 10.076323 loss_N: 0.17701133 loss_f: 2.9775753\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4518 Train Loss 13.230909\n",
      "4519 loss_D: 10.073064 loss_N: 0.17688578 loss_f: 2.979514\n",
      "4519 Train Loss 13.229464\n",
      "4520 loss_D: 10.069791 loss_N: 0.1767982 loss_f: 2.9814963\n",
      "4520 Train Loss 13.228085\n",
      "4521 loss_D: 10.060625 loss_N: 0.17552684 loss_f: 3.006111\n",
      "4521 Train Loss 13.242263\n",
      "4522 loss_D: 10.068543 loss_N: 0.17662619 loss_f: 2.98307\n",
      "4522 Train Loss 13.228239\n",
      "4523 loss_D: 10.069501 loss_N: 0.17675912 loss_f: 2.9817176\n",
      "4523 Train Loss 13.227978\n",
      "4524 loss_D: 10.063817 loss_N: 0.17669664 loss_f: 2.9861898\n",
      "4524 Train Loss 13.226704\n",
      "4525 loss_D: 10.061671 loss_N: 0.17677094 loss_f: 2.986796\n",
      "4525 Train Loss 13.225239\n",
      "4526 loss_D: 10.058449 loss_N: 0.17706253 loss_f: 2.9887512\n",
      "4526 Train Loss 13.224263\n",
      "4527 loss_D: 10.055276 loss_N: 0.1773749 loss_f: 2.9906783\n",
      "4527 Train Loss 13.22333\n",
      "4528 loss_D: 10.048911 loss_N: 0.17753264 loss_f: 2.995393\n",
      "4528 Train Loss 13.221836\n",
      "4529 loss_D: 10.043175 loss_N: 0.17740321 loss_f: 2.9990497\n",
      "4529 Train Loss 13.219627\n",
      "4530 loss_D: 10.034144 loss_N: 0.17697285 loss_f: 3.0058136\n",
      "4530 Train Loss 13.21693\n",
      "4531 loss_D: 10.025417 loss_N: 0.17613006 loss_f: 3.0127845\n",
      "4531 Train Loss 13.214333\n",
      "4532 loss_D: 10.028303 loss_N: 0.17585231 loss_f: 3.0089667\n",
      "4532 Train Loss 13.213122\n",
      "4533 loss_D: 10.025708 loss_N: 0.17575236 loss_f: 3.010459\n",
      "4533 Train Loss 13.21192\n",
      "4534 loss_D: 10.027915 loss_N: 0.17573586 loss_f: 3.0069585\n",
      "4534 Train Loss 13.210609\n",
      "4535 loss_D: 10.027991 loss_N: 0.175653 loss_f: 3.006664\n",
      "4535 Train Loss 13.210309\n",
      "4536 loss_D: 10.022175 loss_N: 0.1755912 loss_f: 3.0118873\n",
      "4536 Train Loss 13.209654\n",
      "4537 loss_D: 10.017475 loss_N: 0.17546831 loss_f: 3.0162625\n",
      "4537 Train Loss 13.209206\n",
      "4538 loss_D: 10.009645 loss_N: 0.1752381 loss_f: 3.0232449\n",
      "4538 Train Loss 13.208128\n",
      "4539 loss_D: 10.004118 loss_N: 0.17504527 loss_f: 3.0267131\n",
      "4539 Train Loss 13.205876\n",
      "4540 loss_D: 10.000077 loss_N: 0.17490806 loss_f: 3.027955\n",
      "4540 Train Loss 13.20294\n",
      "4541 loss_D: 9.99963 loss_N: 0.17520277 loss_f: 3.034623\n",
      "4541 Train Loss 13.2094555\n",
      "4542 loss_D: 9.999945 loss_N: 0.17496799 loss_f: 3.0273204\n",
      "4542 Train Loss 13.202232\n",
      "4543 loss_D: 9.999312 loss_N: 0.17496067 loss_f: 3.0249074\n",
      "4543 Train Loss 13.199181\n",
      "4544 loss_D: 10.00373 loss_N: 0.17508483 loss_f: 3.0186722\n",
      "4544 Train Loss 13.197487\n",
      "4545 loss_D: 10.007877 loss_N: 0.17519118 loss_f: 3.00814\n",
      "4545 Train Loss 13.191208\n",
      "4546 loss_D: 10.003796 loss_N: 0.17524655 loss_f: 3.0078568\n",
      "4546 Train Loss 13.186899\n",
      "4547 loss_D: 9.98745 loss_N: 0.17496687 loss_f: 3.0172975\n",
      "4547 Train Loss 13.179714\n",
      "4548 loss_D: 9.966528 loss_N: 0.17497835 loss_f: 3.0335941\n",
      "4548 Train Loss 13.1751\n",
      "4549 loss_D: 9.954958 loss_N: 0.17525744 loss_f: 3.0412405\n",
      "4549 Train Loss 13.171456\n",
      "4550 loss_D: 9.929347 loss_N: 0.1758987 loss_f: 3.0669706\n",
      "4550 Train Loss 13.172216\n",
      "4551 loss_D: 9.942726 loss_N: 0.17555544 loss_f: 3.0498362\n",
      "4551 Train Loss 13.168118\n",
      "4552 loss_D: 9.937047 loss_N: 0.17626147 loss_f: 3.0512362\n",
      "4552 Train Loss 13.164545\n",
      "4553 loss_D: 9.938634 loss_N: 0.17800313 loss_f: 3.0424738\n",
      "4553 Train Loss 13.159111\n",
      "4554 loss_D: 9.930022 loss_N: 0.17904216 loss_f: 3.0475667\n",
      "4554 Train Loss 13.1566305\n",
      "4555 loss_D: 9.931484 loss_N: 0.1803983 loss_f: 3.0402043\n",
      "4555 Train Loss 13.152086\n",
      "4556 loss_D: 9.931597 loss_N: 0.1809831 loss_f: 3.0337827\n",
      "4556 Train Loss 13.146363\n",
      "4557 loss_D: 9.933477 loss_N: 0.18095264 loss_f: 3.0301886\n",
      "4557 Train Loss 13.144619\n",
      "4558 loss_D: 9.92148 loss_N: 0.18064491 loss_f: 3.0430434\n",
      "4558 Train Loss 13.145168\n",
      "4559 loss_D: 9.927608 loss_N: 0.18077898 loss_f: 3.0321426\n",
      "4559 Train Loss 13.14053\n",
      "4560 loss_D: 9.919387 loss_N: 0.18023418 loss_f: 3.0358906\n",
      "4560 Train Loss 13.135511\n",
      "4561 loss_D: 9.913158 loss_N: 0.17978251 loss_f: 3.0389283\n",
      "4561 Train Loss 13.131869\n",
      "4562 loss_D: 9.894232 loss_N: 0.17923231 loss_f: 3.0520046\n",
      "4562 Train Loss 13.125469\n",
      "4563 loss_D: 9.887502 loss_N: 0.17917682 loss_f: 3.0553014\n",
      "4563 Train Loss 13.121981\n",
      "4564 loss_D: 9.877725 loss_N: 0.17943077 loss_f: 3.0624342\n",
      "4564 Train Loss 13.11959\n",
      "4565 loss_D: 9.878131 loss_N: 0.17948355 loss_f: 3.0608149\n",
      "4565 Train Loss 13.118429\n",
      "4566 loss_D: 9.877145 loss_N: 0.17951931 loss_f: 3.0590153\n",
      "4566 Train Loss 13.11568\n",
      "4567 loss_D: 9.879137 loss_N: 0.17933787 loss_f: 3.0545256\n",
      "4567 Train Loss 13.113\n",
      "4568 loss_D: 9.87639 loss_N: 0.17890999 loss_f: 3.0551453\n",
      "4568 Train Loss 13.110446\n",
      "4569 loss_D: 9.870857 loss_N: 0.17841178 loss_f: 3.0567312\n",
      "4569 Train Loss 13.106\n",
      "4570 loss_D: 9.854781 loss_N: 0.17796601 loss_f: 3.0678492\n",
      "4570 Train Loss 13.100596\n",
      "4571 loss_D: 9.844232 loss_N: 0.1774009 loss_f: 3.0748098\n",
      "4571 Train Loss 13.096442\n",
      "4572 loss_D: 9.83584 loss_N: 0.17691526 loss_f: 3.0829725\n",
      "4572 Train Loss 13.095728\n",
      "4573 loss_D: 9.826579 loss_N: 0.17784257 loss_f: 3.08816\n",
      "4573 Train Loss 13.092581\n",
      "4574 loss_D: 9.84237 loss_N: 0.17855674 loss_f: 3.0889785\n",
      "4574 Train Loss 13.109905\n",
      "4575 loss_D: 9.8298235 loss_N: 0.17798655 loss_f: 3.0832622\n",
      "4575 Train Loss 13.091072\n",
      "4576 loss_D: 9.826064 loss_N: 0.17834385 loss_f: 3.0848308\n",
      "4576 Train Loss 13.089239\n",
      "4577 loss_D: 9.81344 loss_N: 0.17872493 loss_f: 3.093773\n",
      "4577 Train Loss 13.085938\n",
      "4578 loss_D: 9.8102455 loss_N: 0.17852482 loss_f: 3.095366\n",
      "4578 Train Loss 13.084137\n",
      "4579 loss_D: 9.796126 loss_N: 0.1781752 loss_f: 3.1068265\n",
      "4579 Train Loss 13.081128\n",
      "4580 loss_D: 9.798164 loss_N: 0.17771764 loss_f: 3.103763\n",
      "4580 Train Loss 13.079645\n",
      "4581 loss_D: 9.793363 loss_N: 0.17761247 loss_f: 3.106305\n",
      "4581 Train Loss 13.07728\n",
      "4582 loss_D: 9.79355 loss_N: 0.17775565 loss_f: 3.103912\n",
      "4582 Train Loss 13.075217\n",
      "4583 loss_D: 9.79317 loss_N: 0.17807682 loss_f: 3.1020787\n",
      "4583 Train Loss 13.073325\n",
      "4584 loss_D: 9.7923765 loss_N: 0.17831716 loss_f: 3.1014535\n",
      "4584 Train Loss 13.072147\n",
      "4585 loss_D: 9.792335 loss_N: 0.17862065 loss_f: 3.0978417\n",
      "4585 Train Loss 13.068796\n",
      "4586 loss_D: 9.791573 loss_N: 0.17862418 loss_f: 3.0956488\n",
      "4586 Train Loss 13.0658455\n",
      "4587 loss_D: 9.790619 loss_N: 0.17806852 loss_f: 3.0922854\n",
      "4587 Train Loss 13.060972\n",
      "4588 loss_D: 9.791536 loss_N: 0.17713775 loss_f: 3.0872066\n",
      "4588 Train Loss 13.055881\n",
      "4589 loss_D: 9.780967 loss_N: 0.17579499 loss_f: 3.095537\n",
      "4589 Train Loss 13.052299\n",
      "4590 loss_D: 9.783278 loss_N: 0.1754318 loss_f: 3.0898855\n",
      "4590 Train Loss 13.048596\n",
      "4591 loss_D: 9.786038 loss_N: 0.17636797 loss_f: 3.0886276\n",
      "4591 Train Loss 13.051034\n",
      "4592 loss_D: 9.784303 loss_N: 0.17580587 loss_f: 3.0862455\n",
      "4592 Train Loss 13.046354\n",
      "4593 loss_D: 9.780987 loss_N: 0.1757571 loss_f: 3.087192\n",
      "4593 Train Loss 13.043936\n",
      "4594 loss_D: 9.773895 loss_N: 0.17556338 loss_f: 3.0925562\n",
      "4594 Train Loss 13.042015\n",
      "4595 loss_D: 9.767398 loss_N: 0.1754852 loss_f: 3.0959764\n",
      "4595 Train Loss 13.03886\n",
      "4596 loss_D: 9.758063 loss_N: 0.17531458 loss_f: 3.102353\n",
      "4596 Train Loss 13.035731\n",
      "4597 loss_D: 9.749448 loss_N: 0.1752275 loss_f: 3.1107688\n",
      "4597 Train Loss 13.035444\n",
      "4598 loss_D: 9.753415 loss_N: 0.17526549 loss_f: 3.105035\n",
      "4598 Train Loss 13.033715\n",
      "4599 loss_D: 9.748023 loss_N: 0.17386052 loss_f: 3.1090672\n",
      "4599 Train Loss 13.030951\n",
      "4600 loss_D: 9.747526 loss_N: 0.17423762 loss_f: 3.1037502\n",
      "4600 Train Loss 13.025514\n",
      "4601 loss_D: 9.751269 loss_N: 0.17459944 loss_f: 3.0952742\n",
      "4601 Train Loss 13.021143\n",
      "4602 loss_D: 9.756929 loss_N: 0.17431737 loss_f: 3.0862412\n",
      "4602 Train Loss 13.0174885\n",
      "4603 loss_D: 9.755475 loss_N: 0.17408031 loss_f: 3.0852091\n",
      "4603 Train Loss 13.014764\n",
      "4604 loss_D: 9.755682 loss_N: 0.17384672 loss_f: 3.0834281\n",
      "4604 Train Loss 13.012958\n",
      "4605 loss_D: 9.750527 loss_N: 0.17375971 loss_f: 3.087537\n",
      "4605 Train Loss 13.011824\n",
      "4606 loss_D: 9.748422 loss_N: 0.17393778 loss_f: 3.088297\n",
      "4606 Train Loss 13.010656\n",
      "4607 loss_D: 9.742749 loss_N: 0.1745586 loss_f: 3.09256\n",
      "4607 Train Loss 13.009868\n",
      "4608 loss_D: 9.733915 loss_N: 0.17477642 loss_f: 3.1037753\n",
      "4608 Train Loss 13.012466\n",
      "4609 loss_D: 9.7395 loss_N: 0.17463474 loss_f: 3.09403\n",
      "4609 Train Loss 13.008165\n",
      "4610 loss_D: 9.742106 loss_N: 0.17478374 loss_f: 3.0895953\n",
      "4610 Train Loss 13.006485\n",
      "4611 loss_D: 9.742473 loss_N: 0.17559284 loss_f: 3.085043\n",
      "4611 Train Loss 13.003108\n",
      "4612 loss_D: 9.741717 loss_N: 0.17579933 loss_f: 3.0827842\n",
      "4612 Train Loss 13.000301\n",
      "4613 loss_D: 9.738485 loss_N: 0.17590922 loss_f: 3.0823328\n",
      "4613 Train Loss 12.996727\n",
      "4614 loss_D: 9.733598 loss_N: 0.17578912 loss_f: 3.0823488\n",
      "4614 Train Loss 12.991735\n",
      "4615 loss_D: 9.730616 loss_N: 0.17530808 loss_f: 3.0818372\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4615 Train Loss 12.987761\n",
      "4616 loss_D: 9.727766 loss_N: 0.17476757 loss_f: 3.0810356\n",
      "4616 Train Loss 12.983569\n",
      "4617 loss_D: 9.726171 loss_N: 0.17452572 loss_f: 3.0793214\n",
      "4617 Train Loss 12.980017\n",
      "4618 loss_D: 9.715842 loss_N: 0.17371745 loss_f: 3.0867062\n",
      "4618 Train Loss 12.976266\n",
      "4619 loss_D: 9.715616 loss_N: 0.174181 loss_f: 3.0836413\n",
      "4619 Train Loss 12.973438\n",
      "4620 loss_D: 9.704931 loss_N: 0.1747851 loss_f: 3.0892704\n",
      "4620 Train Loss 12.9689865\n",
      "4621 loss_D: 9.699219 loss_N: 0.17485864 loss_f: 3.0913644\n",
      "4621 Train Loss 12.965443\n",
      "4622 loss_D: 9.6799965 loss_N: 0.17492758 loss_f: 3.107319\n",
      "4622 Train Loss 12.962243\n",
      "4623 loss_D: 9.675274 loss_N: 0.17464028 loss_f: 3.1085744\n",
      "4623 Train Loss 12.958488\n",
      "4624 loss_D: 9.6687975 loss_N: 0.17477208 loss_f: 3.1109612\n",
      "4624 Train Loss 12.954531\n",
      "4625 loss_D: 9.65556 loss_N: 0.17341717 loss_f: 3.1382046\n",
      "4625 Train Loss 12.967181\n",
      "4626 loss_D: 9.665329 loss_N: 0.1744015 loss_f: 3.1131284\n",
      "4626 Train Loss 12.952859\n",
      "4627 loss_D: 9.663787 loss_N: 0.17484027 loss_f: 3.111018\n",
      "4627 Train Loss 12.949645\n",
      "4628 loss_D: 9.657449 loss_N: 0.17541535 loss_f: 3.1131198\n",
      "4628 Train Loss 12.945984\n",
      "4629 loss_D: 9.652217 loss_N: 0.17565455 loss_f: 3.1152284\n",
      "4629 Train Loss 12.9431\n",
      "4630 loss_D: 9.645941 loss_N: 0.17557 loss_f: 3.118235\n",
      "4630 Train Loss 12.939745\n",
      "4631 loss_D: 9.639545 loss_N: 0.17508699 loss_f: 3.123046\n",
      "4631 Train Loss 12.937678\n",
      "4632 loss_D: 9.6453 loss_N: 0.17439584 loss_f: 3.119831\n",
      "4632 Train Loss 12.939527\n",
      "4633 loss_D: 9.641698 loss_N: 0.17481606 loss_f: 3.1201203\n",
      "4633 Train Loss 12.936634\n",
      "4634 loss_D: 9.628427 loss_N: 0.17460415 loss_f: 3.2058554\n",
      "4634 Train Loss 13.008886\n",
      "4635 loss_D: 9.640467 loss_N: 0.1747948 loss_f: 3.120611\n",
      "4635 Train Loss 12.935873\n",
      "4636 loss_D: 9.664016 loss_N: 0.172055 loss_f: 3.1451082\n",
      "4636 Train Loss 12.981179\n",
      "4637 loss_D: 9.64331 loss_N: 0.17443004 loss_f: 3.1174684\n",
      "4637 Train Loss 12.935207\n",
      "4638 loss_D: 9.64163 loss_N: 0.17430742 loss_f: 3.1169116\n",
      "4638 Train Loss 12.93285\n",
      "4639 loss_D: 9.651017 loss_N: 0.17419216 loss_f: 3.1032913\n",
      "4639 Train Loss 12.928501\n",
      "4640 loss_D: 9.65196 loss_N: 0.17446394 loss_f: 3.0992708\n",
      "4640 Train Loss 12.925695\n",
      "4641 loss_D: 9.658154 loss_N: 0.17538725 loss_f: 3.0858357\n",
      "4641 Train Loss 12.919376\n",
      "4642 loss_D: 9.654475 loss_N: 0.17624962 loss_f: 3.0843096\n",
      "4642 Train Loss 12.915034\n",
      "4643 loss_D: 9.657481 loss_N: 0.17644538 loss_f: 3.0760918\n",
      "4643 Train Loss 12.910018\n",
      "4644 loss_D: 9.635934 loss_N: 0.17541836 loss_f: 3.096362\n",
      "4644 Train Loss 12.907714\n",
      "4645 loss_D: 9.640995 loss_N: 0.17624554 loss_f: 3.0841162\n",
      "4645 Train Loss 12.901357\n",
      "4646 loss_D: 9.638174 loss_N: 0.17625162 loss_f: 3.0834324\n",
      "4646 Train Loss 12.897858\n",
      "4647 loss_D: 9.625057 loss_N: 0.17769137 loss_f: 3.0979261\n",
      "4647 Train Loss 12.900675\n",
      "4648 loss_D: 9.632715 loss_N: 0.17675418 loss_f: 3.0855246\n",
      "4648 Train Loss 12.894994\n",
      "4649 loss_D: 9.6251955 loss_N: 0.17670119 loss_f: 3.0894122\n",
      "4649 Train Loss 12.89131\n",
      "4650 loss_D: 9.617332 loss_N: 0.17406005 loss_f: 3.123696\n",
      "4650 Train Loss 12.915089\n",
      "4651 loss_D: 9.623315 loss_N: 0.17611471 loss_f: 3.0901234\n",
      "4651 Train Loss 12.889553\n",
      "4652 loss_D: 9.619703 loss_N: 0.17628089 loss_f: 3.0894084\n",
      "4652 Train Loss 12.885393\n",
      "4653 loss_D: 9.615382 loss_N: 0.17688437 loss_f: 3.088164\n",
      "4653 Train Loss 12.880431\n",
      "4654 loss_D: 9.610581 loss_N: 0.1771885 loss_f: 3.090705\n",
      "4654 Train Loss 12.878475\n",
      "4655 loss_D: 9.605867 loss_N: 0.17796084 loss_f: 3.091364\n",
      "4655 Train Loss 12.875192\n",
      "4656 loss_D: 9.597534 loss_N: 0.17848533 loss_f: 3.0960257\n",
      "4656 Train Loss 12.872045\n",
      "4657 loss_D: 9.591755 loss_N: 0.17908461 loss_f: 3.0984213\n",
      "4657 Train Loss 12.869261\n",
      "4658 loss_D: 9.585039 loss_N: 0.17967002 loss_f: 3.101837\n",
      "4658 Train Loss 12.866547\n",
      "4659 loss_D: 9.579953 loss_N: 0.17961223 loss_f: 3.1039877\n",
      "4659 Train Loss 12.863553\n",
      "4660 loss_D: 9.5704565 loss_N: 0.17945096 loss_f: 3.1100225\n",
      "4660 Train Loss 12.85993\n",
      "4661 loss_D: 9.570361 loss_N: 0.17914928 loss_f: 3.1072273\n",
      "4661 Train Loss 12.856738\n",
      "4662 loss_D: 9.567946 loss_N: 0.1784579 loss_f: 3.107821\n",
      "4662 Train Loss 12.854225\n",
      "4663 loss_D: 9.572275 loss_N: 0.1775287 loss_f: 3.1024847\n",
      "4663 Train Loss 12.852288\n",
      "4664 loss_D: 9.575494 loss_N: 0.17691933 loss_f: 3.0983324\n",
      "4664 Train Loss 12.850745\n",
      "4665 loss_D: 9.5753355 loss_N: 0.1761576 loss_f: 3.0969706\n",
      "4665 Train Loss 12.848464\n",
      "4666 loss_D: 9.574085 loss_N: 0.17584479 loss_f: 3.0970016\n",
      "4666 Train Loss 12.846931\n",
      "4667 loss_D: 9.570809 loss_N: 0.1756933 loss_f: 3.0986354\n",
      "4667 Train Loss 12.845139\n",
      "4668 loss_D: 9.56425 loss_N: 0.17591415 loss_f: 3.1037102\n",
      "4668 Train Loss 12.843874\n",
      "4669 loss_D: 9.56376 loss_N: 0.17616333 loss_f: 3.1035213\n",
      "4669 Train Loss 12.843445\n",
      "4670 loss_D: 9.559077 loss_N: 0.17625822 loss_f: 3.1071148\n",
      "4670 Train Loss 12.84245\n",
      "4671 loss_D: 9.556237 loss_N: 0.17615052 loss_f: 3.1090028\n",
      "4671 Train Loss 12.841391\n",
      "4672 loss_D: 9.555122 loss_N: 0.17574657 loss_f: 3.1096232\n",
      "4672 Train Loss 12.840492\n",
      "4673 loss_D: 9.555578 loss_N: 0.17525591 loss_f: 3.108246\n",
      "4673 Train Loss 12.83908\n",
      "4674 loss_D: 9.557365 loss_N: 0.17487961 loss_f: 3.1059055\n",
      "4674 Train Loss 12.838151\n",
      "4675 loss_D: 9.562198 loss_N: 0.17455842 loss_f: 3.1000335\n",
      "4675 Train Loss 12.83679\n",
      "4676 loss_D: 9.567392 loss_N: 0.17455684 loss_f: 3.0935943\n",
      "4676 Train Loss 12.835544\n",
      "4677 loss_D: 9.5728245 loss_N: 0.17455323 loss_f: 3.0868447\n",
      "4677 Train Loss 12.834222\n",
      "4678 loss_D: 9.57418 loss_N: 0.17492312 loss_f: 3.0836291\n",
      "4678 Train Loss 12.832731\n",
      "4679 loss_D: 9.572331 loss_N: 0.17516163 loss_f: 3.0843873\n",
      "4679 Train Loss 12.831881\n",
      "4680 loss_D: 9.566519 loss_N: 0.17572616 loss_f: 3.0863397\n",
      "4680 Train Loss 12.828585\n",
      "4681 loss_D: 9.556137 loss_N: 0.17516388 loss_f: 3.104707\n",
      "4681 Train Loss 12.836008\n",
      "4682 loss_D: 9.562916 loss_N: 0.17551918 loss_f: 3.0881171\n",
      "4682 Train Loss 12.826551\n",
      "4683 loss_D: 9.556913 loss_N: 0.17862976 loss_f: 3.1132233\n",
      "4683 Train Loss 12.848766\n",
      "4684 loss_D: 9.561362 loss_N: 0.17617477 loss_f: 3.086564\n",
      "4684 Train Loss 12.824101\n",
      "4685 loss_D: 9.552226 loss_N: 0.17602871 loss_f: 3.091568\n",
      "4685 Train Loss 12.819822\n",
      "4686 loss_D: 9.548626 loss_N: 0.17565747 loss_f: 3.0908964\n",
      "4686 Train Loss 12.81518\n",
      "4687 loss_D: 9.5475025 loss_N: 0.17514755 loss_f: 3.0897563\n",
      "4687 Train Loss 12.812407\n",
      "4688 loss_D: 9.552917 loss_N: 0.17451043 loss_f: 3.0810218\n",
      "4688 Train Loss 12.808449\n",
      "4689 loss_D: 9.555811 loss_N: 0.17422321 loss_f: 3.0937085\n",
      "4689 Train Loss 12.823742\n",
      "4690 loss_D: 9.553461 loss_N: 0.17444922 loss_f: 3.0798807\n",
      "4690 Train Loss 12.807791\n",
      "4691 loss_D: 9.559982 loss_N: 0.1742854 loss_f: 3.0705764\n",
      "4691 Train Loss 12.804844\n",
      "4692 loss_D: 9.564692 loss_N: 0.1743828 loss_f: 3.0638168\n",
      "4692 Train Loss 12.802892\n",
      "4693 loss_D: 9.564622 loss_N: 0.17463136 loss_f: 3.062782\n",
      "4693 Train Loss 12.802035\n",
      "4694 loss_D: 9.564649 loss_N: 0.17505108 loss_f: 3.0609186\n",
      "4694 Train Loss 12.800618\n",
      "4695 loss_D: 9.562188 loss_N: 0.1755771 loss_f: 3.0626416\n",
      "4695 Train Loss 12.800407\n",
      "4696 loss_D: 9.560948 loss_N: 0.17545208 loss_f: 3.062514\n",
      "4696 Train Loss 12.798915\n",
      "4697 loss_D: 9.557668 loss_N: 0.17525274 loss_f: 3.064119\n",
      "4697 Train Loss 12.79704\n",
      "4698 loss_D: 9.557561 loss_N: 0.17517036 loss_f: 3.0634425\n",
      "4698 Train Loss 12.796173\n",
      "4699 loss_D: 9.556378 loss_N: 0.17467123 loss_f: 3.0623918\n",
      "4699 Train Loss 12.793442\n",
      "4700 loss_D: 9.555811 loss_N: 0.1742352 loss_f: 3.0620182\n",
      "4700 Train Loss 12.792065\n",
      "4701 loss_D: 9.556166 loss_N: 0.17349422 loss_f: 3.0605443\n",
      "4701 Train Loss 12.790204\n",
      "4702 loss_D: 9.552929 loss_N: 0.17280495 loss_f: 3.062537\n",
      "4702 Train Loss 12.788271\n",
      "4703 loss_D: 9.548915 loss_N: 0.17241198 loss_f: 3.0643969\n",
      "4703 Train Loss 12.785724\n",
      "4704 loss_D: 9.541116 loss_N: 0.17223528 loss_f: 3.0704448\n",
      "4704 Train Loss 12.783796\n",
      "4705 loss_D: 9.534 loss_N: 0.17233397 loss_f: 3.0746446\n",
      "4705 Train Loss 12.780979\n",
      "4706 loss_D: 9.527113 loss_N: 0.17258766 loss_f: 3.0786667\n",
      "4706 Train Loss 12.778367\n",
      "4707 loss_D: 9.525197 loss_N: 0.17301334 loss_f: 3.077285\n",
      "4707 Train Loss 12.775496\n",
      "4708 loss_D: 9.524527 loss_N: 0.17352588 loss_f: 3.0748234\n",
      "4708 Train Loss 12.772876\n",
      "4709 loss_D: 9.525599 loss_N: 0.17406195 loss_f: 3.0717874\n",
      "4709 Train Loss 12.771448\n",
      "4710 loss_D: 9.529773 loss_N: 0.17418723 loss_f: 3.0656238\n",
      "4710 Train Loss 12.769585\n",
      "4711 loss_D: 9.525981 loss_N: 0.17415255 loss_f: 3.0675688\n",
      "4711 Train Loss 12.767702\n",
      "4712 loss_D: 9.521623 loss_N: 0.17383084 loss_f: 3.068201\n",
      "4712 Train Loss 12.763655\n",
      "4713 loss_D: 9.513195 loss_N: 0.17391506 loss_f: 3.07414\n",
      "4713 Train Loss 12.76125\n",
      "4714 loss_D: 9.50739 loss_N: 0.17446424 loss_f: 3.076676\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4714 Train Loss 12.75853\n",
      "4715 loss_D: 9.499209 loss_N: 0.17432965 loss_f: 3.081613\n",
      "4715 Train Loss 12.755152\n",
      "4716 loss_D: 9.497213 loss_N: 0.17402454 loss_f: 3.08039\n",
      "4716 Train Loss 12.751628\n",
      "4717 loss_D: 9.4991455 loss_N: 0.17329921 loss_f: 3.0782695\n",
      "4717 Train Loss 12.750713\n",
      "4718 loss_D: 9.512278 loss_N: 0.17229912 loss_f: 3.080376\n",
      "4718 Train Loss 12.764953\n",
      "4719 loss_D: 9.5025625 loss_N: 0.17293243 loss_f: 3.0709596\n",
      "4719 Train Loss 12.746454\n",
      "4720 loss_D: 9.501867 loss_N: 0.17300455 loss_f: 3.0683508\n",
      "4720 Train Loss 12.743222\n",
      "4721 loss_D: 9.511411 loss_N: 0.17282248 loss_f: 3.0514967\n",
      "4721 Train Loss 12.735729\n",
      "4722 loss_D: 9.503776 loss_N: 0.17295264 loss_f: 3.0522807\n",
      "4722 Train Loss 12.729009\n",
      "4723 loss_D: 9.498118 loss_N: 0.17313883 loss_f: 3.0491815\n",
      "4723 Train Loss 12.720438\n",
      "4724 loss_D: 9.491597 loss_N: 0.17323847 loss_f: 3.049753\n",
      "4724 Train Loss 12.714589\n",
      "4725 loss_D: 9.48824 loss_N: 0.17575379 loss_f: 3.0471134\n",
      "4725 Train Loss 12.711107\n",
      "4726 loss_D: 9.486094 loss_N: 0.17561403 loss_f: 3.0434728\n",
      "4726 Train Loss 12.705181\n",
      "4727 loss_D: 9.4831705 loss_N: 0.17618574 loss_f: 3.042275\n",
      "4727 Train Loss 12.701632\n",
      "4728 loss_D: 9.474499 loss_N: 0.17747258 loss_f: 3.0454707\n",
      "4728 Train Loss 12.697441\n",
      "4729 loss_D: 9.469852 loss_N: 0.17839943 loss_f: 3.0462508\n",
      "4729 Train Loss 12.694502\n",
      "4730 loss_D: 9.464526 loss_N: 0.17867982 loss_f: 3.0483773\n",
      "4730 Train Loss 12.691583\n",
      "4731 loss_D: 9.462338 loss_N: 0.1784209 loss_f: 3.048151\n",
      "4731 Train Loss 12.6889105\n",
      "4732 loss_D: 9.45981 loss_N: 0.17789663 loss_f: 3.0494664\n",
      "4732 Train Loss 12.687173\n",
      "4733 loss_D: 9.460277 loss_N: 0.17731363 loss_f: 3.0475407\n",
      "4733 Train Loss 12.685131\n",
      "4734 loss_D: 9.458191 loss_N: 0.17707482 loss_f: 3.0472887\n",
      "4734 Train Loss 12.682554\n",
      "4735 loss_D: 9.45549 loss_N: 0.17702492 loss_f: 3.0463026\n",
      "4735 Train Loss 12.678818\n",
      "4736 loss_D: 9.452715 loss_N: 0.17759429 loss_f: 3.0444105\n",
      "4736 Train Loss 12.67472\n",
      "4737 loss_D: 9.450421 loss_N: 0.1778036 loss_f: 3.0428984\n",
      "4737 Train Loss 12.6711235\n",
      "4738 loss_D: 9.448717 loss_N: 0.17887747 loss_f: 3.045608\n",
      "4738 Train Loss 12.6732025\n",
      "4739 loss_D: 9.449726 loss_N: 0.17821907 loss_f: 3.041768\n",
      "4739 Train Loss 12.669713\n",
      "4740 loss_D: 9.442745 loss_N: 0.1787976 loss_f: 3.0468404\n",
      "4740 Train Loss 12.668384\n",
      "4741 loss_D: 9.448058 loss_N: 0.1781629 loss_f: 3.0386086\n",
      "4741 Train Loss 12.664829\n",
      "4742 loss_D: 9.446368 loss_N: 0.17782749 loss_f: 3.0372157\n",
      "4742 Train Loss 12.661411\n",
      "4743 loss_D: 9.444211 loss_N: 0.17711674 loss_f: 3.0361319\n",
      "4743 Train Loss 12.657459\n",
      "4744 loss_D: 9.4395275 loss_N: 0.17662397 loss_f: 3.0389354\n",
      "4744 Train Loss 12.655087\n",
      "4745 loss_D: 9.433921 loss_N: 0.17602542 loss_f: 3.0422528\n",
      "4745 Train Loss 12.652199\n",
      "4746 loss_D: 9.426675 loss_N: 0.17621623 loss_f: 3.0476246\n",
      "4746 Train Loss 12.650516\n",
      "4747 loss_D: 9.424572 loss_N: 0.17574888 loss_f: 3.0476203\n",
      "4747 Train Loss 12.647942\n",
      "4748 loss_D: 9.415485 loss_N: 0.17458922 loss_f: 3.051881\n",
      "4748 Train Loss 12.641955\n",
      "4749 loss_D: 9.419 loss_N: 0.17361535 loss_f: 3.046021\n",
      "4749 Train Loss 12.638636\n",
      "4750 loss_D: 9.410856 loss_N: 0.17283216 loss_f: 3.0526662\n",
      "4750 Train Loss 12.636354\n",
      "4751 loss_D: 9.4095545 loss_N: 0.17265202 loss_f: 3.0525804\n",
      "4751 Train Loss 12.634787\n",
      "4752 loss_D: 9.404434 loss_N: 0.17255436 loss_f: 3.0543895\n",
      "4752 Train Loss 12.631378\n",
      "4753 loss_D: 9.4004135 loss_N: 0.1724132 loss_f: 3.0562584\n",
      "4753 Train Loss 12.629085\n",
      "4754 loss_D: 9.394508 loss_N: 0.17233291 loss_f: 3.0605264\n",
      "4754 Train Loss 12.627367\n",
      "4755 loss_D: 9.39167 loss_N: 0.17239669 loss_f: 3.0605159\n",
      "4755 Train Loss 12.624582\n",
      "4756 loss_D: 9.388978 loss_N: 0.17245094 loss_f: 3.0602727\n",
      "4756 Train Loss 12.621702\n",
      "4757 loss_D: 9.393694 loss_N: 0.17238654 loss_f: 3.053382\n",
      "4757 Train Loss 12.619462\n",
      "4758 loss_D: 9.39337 loss_N: 0.17251858 loss_f: 3.0517614\n",
      "4758 Train Loss 12.61765\n",
      "4759 loss_D: 9.396522 loss_N: 0.1726677 loss_f: 3.0466707\n",
      "4759 Train Loss 12.61586\n",
      "4760 loss_D: 9.400387 loss_N: 0.17284094 loss_f: 3.040789\n",
      "4760 Train Loss 12.614017\n",
      "4761 loss_D: 9.402256 loss_N: 0.1730281 loss_f: 3.0363252\n",
      "4761 Train Loss 12.611609\n",
      "4762 loss_D: 9.402473 loss_N: 0.17332934 loss_f: 3.0346472\n",
      "4762 Train Loss 12.61045\n",
      "4763 loss_D: 9.40263 loss_N: 0.17338488 loss_f: 3.0337565\n",
      "4763 Train Loss 12.609771\n",
      "4764 loss_D: 9.403162 loss_N: 0.17345013 loss_f: 3.032083\n",
      "4764 Train Loss 12.608696\n",
      "4765 loss_D: 9.403708 loss_N: 0.17348394 loss_f: 3.02998\n",
      "4765 Train Loss 12.607172\n",
      "4766 loss_D: 9.405419 loss_N: 0.17345998 loss_f: 3.0270913\n",
      "4766 Train Loss 12.60597\n",
      "4767 loss_D: 9.406156 loss_N: 0.17339532 loss_f: 3.025791\n",
      "4767 Train Loss 12.605342\n",
      "4768 loss_D: 9.4064455 loss_N: 0.17323886 loss_f: 3.024198\n",
      "4768 Train Loss 12.603882\n",
      "4769 loss_D: 9.406328 loss_N: 0.17306428 loss_f: 3.0225997\n",
      "4769 Train Loss 12.601992\n",
      "4770 loss_D: 9.403815 loss_N: 0.17209905 loss_f: 3.0250566\n",
      "4770 Train Loss 12.600971\n",
      "4771 loss_D: 9.40642 loss_N: 0.17275432 loss_f: 3.0197842\n",
      "4771 Train Loss 12.598958\n",
      "4772 loss_D: 9.4062805 loss_N: 0.17320593 loss_f: 3.0179174\n",
      "4772 Train Loss 12.5974045\n",
      "4773 loss_D: 9.40737 loss_N: 0.17377879 loss_f: 3.0143514\n",
      "4773 Train Loss 12.595499\n",
      "4774 loss_D: 9.404858 loss_N: 0.1748951 loss_f: 3.0141668\n",
      "4774 Train Loss 12.59392\n",
      "4775 loss_D: 9.405409 loss_N: 0.17498317 loss_f: 3.0115175\n",
      "4775 Train Loss 12.591909\n",
      "4776 loss_D: 9.406393 loss_N: 0.17477606 loss_f: 3.0093765\n",
      "4776 Train Loss 12.590546\n",
      "4777 loss_D: 9.406102 loss_N: 0.17468376 loss_f: 3.0088613\n",
      "4777 Train Loss 12.589647\n",
      "4778 loss_D: 9.403166 loss_N: 0.1746956 loss_f: 3.009128\n",
      "4778 Train Loss 12.58699\n",
      "4779 loss_D: 9.401679 loss_N: 0.17518976 loss_f: 3.0107236\n",
      "4779 Train Loss 12.587593\n",
      "4780 loss_D: 9.402464 loss_N: 0.17491098 loss_f: 3.008418\n",
      "4780 Train Loss 12.585793\n",
      "4781 loss_D: 9.394265 loss_N: 0.175825 loss_f: 3.013214\n",
      "4781 Train Loss 12.583304\n",
      "4782 loss_D: 9.388727 loss_N: 0.17597288 loss_f: 3.0156584\n",
      "4782 Train Loss 12.5803585\n",
      "4783 loss_D: 9.384502 loss_N: 0.17641723 loss_f: 3.015693\n",
      "4783 Train Loss 12.576612\n",
      "4784 loss_D: 9.372033 loss_N: 0.17732951 loss_f: 3.024626\n",
      "4784 Train Loss 12.573988\n",
      "4785 loss_D: 9.372957 loss_N: 0.17779341 loss_f: 3.026846\n",
      "4785 Train Loss 12.577597\n",
      "4786 loss_D: 9.372273 loss_N: 0.17752485 loss_f: 3.019669\n",
      "4786 Train Loss 12.569468\n",
      "4787 loss_D: 9.37712 loss_N: 0.1764465 loss_f: 3.0124862\n",
      "4787 Train Loss 12.566053\n",
      "4788 loss_D: 9.375098 loss_N: 0.17566256 loss_f: 3.0127063\n",
      "4788 Train Loss 12.563467\n",
      "4789 loss_D: 9.372648 loss_N: 0.17557946 loss_f: 3.0120676\n",
      "4789 Train Loss 12.560295\n",
      "4790 loss_D: 9.369033 loss_N: 0.17560606 loss_f: 3.0116184\n",
      "4790 Train Loss 12.556257\n",
      "4791 loss_D: 9.3622265 loss_N: 0.17580085 loss_f: 3.0129569\n",
      "4791 Train Loss 12.550984\n",
      "4792 loss_D: 9.35982 loss_N: 0.17631869 loss_f: 3.0086823\n",
      "4792 Train Loss 12.544822\n",
      "4793 loss_D: 9.336545 loss_N: 0.1766668 loss_f: 3.029713\n",
      "4793 Train Loss 12.542925\n",
      "4794 loss_D: 9.352838 loss_N: 0.17757155 loss_f: 3.0009449\n",
      "4794 Train Loss 12.531354\n",
      "4795 loss_D: 9.358036 loss_N: 0.17716517 loss_f: 2.9911575\n",
      "4795 Train Loss 12.526359\n",
      "4796 loss_D: 9.360905 loss_N: 0.17644618 loss_f: 2.985042\n",
      "4796 Train Loss 12.522392\n",
      "4797 loss_D: 9.362769 loss_N: 0.17611186 loss_f: 2.9817343\n",
      "4797 Train Loss 12.520616\n",
      "4798 loss_D: 9.359799 loss_N: 0.17574553 loss_f: 2.982164\n",
      "4798 Train Loss 12.51771\n",
      "4799 loss_D: 9.356089 loss_N: 0.1758828 loss_f: 2.9836342\n",
      "4799 Train Loss 12.515605\n",
      "4800 loss_D: 9.349064 loss_N: 0.17613228 loss_f: 2.988143\n",
      "4800 Train Loss 12.513339\n",
      "4801 loss_D: 9.341154 loss_N: 0.1767478 loss_f: 2.9918988\n",
      "4801 Train Loss 12.509801\n",
      "4802 loss_D: 9.334649 loss_N: 0.17700872 loss_f: 2.9966502\n",
      "4802 Train Loss 12.508308\n",
      "4803 loss_D: 9.327967 loss_N: 0.17703806 loss_f: 3.0006645\n",
      "4803 Train Loss 12.50567\n",
      "4804 loss_D: 9.322569 loss_N: 0.17693181 loss_f: 3.0023637\n",
      "4804 Train Loss 12.5018635\n",
      "4805 loss_D: 9.319471 loss_N: 0.1765078 loss_f: 3.0033078\n",
      "4805 Train Loss 12.499287\n",
      "4806 loss_D: 9.317575 loss_N: 0.17598896 loss_f: 3.0014467\n",
      "4806 Train Loss 12.495011\n",
      "4807 loss_D: 9.311905 loss_N: 0.17622772 loss_f: 3.0038605\n",
      "4807 Train Loss 12.491993\n",
      "4808 loss_D: 9.316148 loss_N: 0.17483838 loss_f: 2.9997542\n",
      "4808 Train Loss 12.49074\n",
      "4809 loss_D: 9.310593 loss_N: 0.1751786 loss_f: 3.002335\n",
      "4809 Train Loss 12.488106\n",
      "4810 loss_D: 9.305773 loss_N: 0.17547275 loss_f: 3.0054884\n",
      "4810 Train Loss 12.486734\n",
      "4811 loss_D: 9.299617 loss_N: 0.1754711 loss_f: 3.0104325\n",
      "4811 Train Loss 12.48552\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4812 loss_D: 9.295126 loss_N: 0.1751536 loss_f: 3.0138507\n",
      "4812 Train Loss 12.484131\n",
      "4813 loss_D: 9.294929 loss_N: 0.1747114 loss_f: 3.0125253\n",
      "4813 Train Loss 12.482165\n",
      "4814 loss_D: 9.295121 loss_N: 0.17422588 loss_f: 3.0115688\n",
      "4814 Train Loss 12.480916\n",
      "4815 loss_D: 9.299653 loss_N: 0.17399395 loss_f: 3.0058875\n",
      "4815 Train Loss 12.479534\n",
      "4816 loss_D: 9.30201 loss_N: 0.17387553 loss_f: 3.002048\n",
      "4816 Train Loss 12.477934\n",
      "4817 loss_D: 9.30599 loss_N: 0.17359014 loss_f: 2.9968095\n",
      "4817 Train Loss 12.476389\n",
      "4818 loss_D: 9.305211 loss_N: 0.17394646 loss_f: 2.9951448\n",
      "4818 Train Loss 12.474302\n",
      "4819 loss_D: 9.303339 loss_N: 0.17418423 loss_f: 2.993713\n",
      "4819 Train Loss 12.471235\n",
      "4820 loss_D: 9.301548 loss_N: 0.17410244 loss_f: 2.9941812\n",
      "4820 Train Loss 12.469831\n",
      "4821 loss_D: 9.301611 loss_N: 0.17384908 loss_f: 2.992778\n",
      "4821 Train Loss 12.468238\n",
      "4822 loss_D: 9.302898 loss_N: 0.17341562 loss_f: 2.989512\n",
      "4822 Train Loss 12.465826\n",
      "4823 loss_D: 9.306386 loss_N: 0.17307363 loss_f: 2.9837363\n",
      "4823 Train Loss 12.463196\n",
      "4824 loss_D: 9.30977 loss_N: 0.17278755 loss_f: 2.9776855\n",
      "4824 Train Loss 12.460243\n",
      "4825 loss_D: 9.315629 loss_N: 0.1736114 loss_f: 2.9712386\n",
      "4825 Train Loss 12.46048\n",
      "4826 loss_D: 9.312582 loss_N: 0.17320149 loss_f: 2.9722385\n",
      "4826 Train Loss 12.458022\n",
      "4827 loss_D: 9.311 loss_N: 0.17319417 loss_f: 2.9707658\n",
      "4827 Train Loss 12.45496\n",
      "4828 loss_D: 9.302204 loss_N: 0.17373295 loss_f: 2.9763856\n",
      "4828 Train Loss 12.452322\n",
      "4829 loss_D: 9.301401 loss_N: 0.17399234 loss_f: 2.9727726\n",
      "4829 Train Loss 12.448166\n",
      "4830 loss_D: 9.299751 loss_N: 0.17418489 loss_f: 2.9698794\n",
      "4830 Train Loss 12.443815\n",
      "4831 loss_D: 9.29887 loss_N: 0.17407532 loss_f: 2.96642\n",
      "4831 Train Loss 12.439365\n",
      "4832 loss_D: 9.297351 loss_N: 0.17358918 loss_f: 2.9654264\n",
      "4832 Train Loss 12.436366\n",
      "4833 loss_D: 9.297952 loss_N: 0.1726654 loss_f: 2.9622548\n",
      "4833 Train Loss 12.432872\n",
      "4834 loss_D: 9.295973 loss_N: 0.17204982 loss_f: 2.9617565\n",
      "4834 Train Loss 12.429779\n",
      "4835 loss_D: 9.293831 loss_N: 0.17197323 loss_f: 2.9607441\n",
      "4835 Train Loss 12.426548\n",
      "4836 loss_D: 9.287529 loss_N: 0.17219882 loss_f: 2.96247\n",
      "4836 Train Loss 12.422198\n",
      "4837 loss_D: 9.281389 loss_N: 0.17256382 loss_f: 2.9647255\n",
      "4837 Train Loss 12.418678\n",
      "4838 loss_D: 9.275136 loss_N: 0.1731078 loss_f: 2.966453\n",
      "4838 Train Loss 12.414698\n",
      "4839 loss_D: 9.267906 loss_N: 0.17342868 loss_f: 2.9711008\n",
      "4839 Train Loss 12.412436\n",
      "4840 loss_D: 9.266116 loss_N: 0.17350318 loss_f: 2.970993\n",
      "4840 Train Loss 12.410612\n",
      "4841 loss_D: 9.262243 loss_N: 0.17351428 loss_f: 2.9726465\n",
      "4841 Train Loss 12.408404\n",
      "4842 loss_D: 9.262731 loss_N: 0.17296517 loss_f: 2.969409\n",
      "4842 Train Loss 12.405105\n",
      "4843 loss_D: 9.236641 loss_N: 0.17703383 loss_f: 3.100295\n",
      "4843 Train Loss 12.513969\n",
      "4844 loss_D: 9.258562 loss_N: 0.1733379 loss_f: 2.9704368\n",
      "4844 Train Loss 12.402337\n",
      "4845 loss_D: 9.258112 loss_N: 0.17320034 loss_f: 2.9685602\n",
      "4845 Train Loss 12.399873\n",
      "4846 loss_D: 9.259692 loss_N: 0.17317998 loss_f: 2.963486\n",
      "4846 Train Loss 12.396358\n",
      "4847 loss_D: 9.258072 loss_N: 0.17328455 loss_f: 2.9621687\n",
      "4847 Train Loss 12.393525\n",
      "4848 loss_D: 9.256309 loss_N: 0.17372954 loss_f: 2.9594223\n",
      "4848 Train Loss 12.389461\n",
      "4849 loss_D: 9.256583 loss_N: 0.17395838 loss_f: 2.9569244\n",
      "4849 Train Loss 12.387466\n",
      "4850 loss_D: 9.256574 loss_N: 0.17392133 loss_f: 2.953685\n",
      "4850 Train Loss 12.38418\n",
      "4851 loss_D: 9.255635 loss_N: 0.1738465 loss_f: 2.950875\n",
      "4851 Train Loss 12.380357\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-11-a38b6c53b355>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     49\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 50\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPINN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPINN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                     loss, flat_grad, t, ls_func_evals = _strong_wolfe(\n\u001b[0;32m--> 426\u001b[0;31m                         obj_func, x_init, t, d, loss, flat_grad, gtd)\n\u001b[0m\u001b[1;32m    427\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0mopt_cond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflat_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mtolerance_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36m_strong_wolfe\u001b[0;34m(obj_func, x, t, d, f, g, gtd, c1, c2, tolerance_change, max_ls)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# evaluate objective and gradient using initial step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mf_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mls_func_evals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mgtd_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mobj_func\u001b[0;34m(x, t, d)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m                     \u001b[0;32mdef\u001b[0m \u001b[0mobj_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m                         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_directional_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                     loss, flat_grad, t, ls_func_evals = _strong_wolfe(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36m_directional_evaluate\u001b[0;34m(self, closure, x, t, d)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_directional_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0mflat_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gather_flat_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-d2c648c82afa>\u001b[0m in \u001b[0;36mclosure\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    116\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 118\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxyt_D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu_D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxyt_N\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN_hat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxyt_coll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    119\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    120\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-10-d2c648c82afa>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, xyt_D, u_D, xyt_N, N_hat, xyt_coll, f_hat)\u001b[0m\n\u001b[1;32m    108\u001b[0m         \u001b[0mloss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_D\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_N\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 110\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"loss_D:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_D\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"loss_N:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_N\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"loss_f:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mloss_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    111\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_reps = 1\n",
    "\n",
    "train_loss_full = []\n",
    "test_loss_full = []\n",
    "beta_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(reps)\n",
    "    \n",
    "    torch.manual_seed(reps*36)\n",
    "    N_D = 5000 #Total number of data points for 'y'\n",
    "    N_N = 7000\n",
    "    N_f = 10000 #Total number of collocation points \n",
    "    \n",
    "    xyt_coll_np_array, xyt_D_np_array, u_D_np_array,xyt_N_np_array = trainingdata(N_D,N_N,N_f,(reps)*22)\n",
    "        \n",
    "    xyt_coll = torch.from_numpy(xyt_coll_np_array).float().to(device)\n",
    "    xyt_D = torch.from_numpy(xyt_D_np_array).float().to(device)\n",
    "    u_D = torch.from_numpy(u_D_np_array).float().to(device)\n",
    "    xyt_N = torch.from_numpy(xyt_N_np_array).float().to(device)\n",
    "        \n",
    "    N_hat = torch.zeros(xyt_N.shape[0],1).to(device)    \n",
    "    f_hat = torch.zeros(xyt_coll.shape[0],1).to(device)\n",
    "\n",
    "    layers = np.array([3,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "    #layers = np.array([3,100,100,100,100,100,100,100,1])\n",
    "\n",
    "    PINN = Sequentialmodel(layers)\n",
    "   \n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "    \n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=1, \n",
    "                              max_iter = 10000, \n",
    "                              max_eval = None, \n",
    "                              tolerance_grad = -1, \n",
    "                              tolerance_change = -1, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "        \n",
    "    optimizer.step(PINN.closure)\n",
    "    \n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(PINN.train_loss)\n",
    "    test_loss_full.append(PINN.test_loss)\n",
    "    elapsed_time[reps] = time.time() - start_time\n",
    "    beta_full.append(PINN.beta_val)\n",
    "\n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "    \n",
    "mdic = {\"train_loss\": train_loss_full,\"test_loss\": test_loss_full,\"Time\": elapsed_time, \"beta\": beta_full, \"label\": label}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred = PINN.test(xyt_test_tensor)\n",
    "u_pred_3d = u_pred.reshape(100,100,100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU8AAAD8CAYAAAD35CadAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAQa0lEQVR4nO3db6wcV3nH8e/j6wQKDaHCpQXbkFBMhRXakt4m0EoFRIJMXsQvqJCDUEsVYUEbVBWKREXVhvAGWkEFkgXc0jSABOHPC3QlDKlKiSJFJLURISRGIGNSckPa8CdEoEBiJ09f7KZsLvd6x+fO7pmZ/X4kS3d3z86cszPnN+fM7KwjM5EknZlttSsgSX1keEpSAcNTkgoYnpJUwPCUpAKGpyQVmBqeEXFtRNwXEXds8npExPsj4nhE3B4RF7ZfTUnqliYjz+uAfad5/ZXAnvG/g8AHtl4tSeq2qeGZmTcBPzpNkf3AR3PkFuCpEfGMtiooSV20vYVl7ATunni8Nn7u3vUFI+Igo9EpcNbvw44WVi9Jpe79QWb+esk72wjPxjJzBVgBiHhm/n+OSlIV7/jv0ne2EZ73ALsnHu8aP6feOat2BaTeaCM8V4GrIuJ64GLggcz8pSn71tipNS9znYypx6buKRHxCeClwI6IWAP+gXGaZeYHgcPAZcBx4EHgz5utehvwKwVV1rB4YFQ/TQ3PzLxiyusJ/GVrNbIzaS4cYWprKu5BwcZB6U6tefAgra2pmFRO27VVHmhVzwLsfQvQRPWMo94hcOSpjvPgp25yz2zMj0p95Ch3Vhx5qmc8iKkb3BNnwo9VQ+LodSOVv6rkyFOzZKfX7DhEmgk7rYbM2IDqI0+/JK8u8yCozVUOzz4FpR1JQ9KnvtdNHRx5nil3AvWBB9+hGcjI0x1TfeIBfwgGMPI8E+606jsHCl3hl+S1gDyIauvciwA/Bg2fI9a2OfJUR3lAU7e5h/oRaOE4Cm2Dt2dqxuyoGqaBDrvssFpEA+3OHeU5TxXyAKXFtmDf85RmxVHfohnIHUaaDQ9u0mYceaoSD5zqN0eeCy1qV0DqrcrpZeeV1E/baldAkvrIebO0COzpGztV/lY/0hr81KXeq9eNA3hCtbVrEXnQ0no/LX/r8Hen4bdQWhwd6s/1qrINeGK1tWuaDu2kUhfVnbbbQaVfsD/0St3NtSg7y6K0U1ogjjz1eG4TqZFGXSUi9gHvA5aAD2fmu9a9/izgI8BTx2XelpmHT7/Qpmufws4uqYKp0RMRS8Ah4FJgDTgSEauZeWyi2N8Bn8rMD0TEXuAwcN5pF+wFI0lNdHSA1KRaFwHHM/MEQERcD+wHJsMzgaeM/z4X+F5ra5c0f/bNqZp8RDuBuycerwEXrytzNfDvEfEm4MnAJRstKCIOAgcBOOtZbqA+cVtJj9NWl7gCuC4z3xMRLwY+FhEXZOajk4UycwVYAYgnL6cdUnPjvqaWNdml7gF2TzzeNX5u0pXAPoDM/HJEPBHYAdy36VI95ympx5qE5xFgT0Sczyg0DwCvWVfmu8DLgesi4vmMYvH7bVZ0oThKkjpvajfNzFMRcRVwA6OvIV2bmXdGxDXA0cxcBd4C/EtE/DWji0evy8w87YIDR56qw4OTWhDTMm5mKz53OfnDo1XWLUkAfCG+kpnLJW/1DqOt6Hv9JRUzPDU7bl8NWN3w9JynpJ5y5ClJBQxPLTb3QRUyPNs2xDZJ+iWG55nqY50ltc4LRtLQeICfC0eeklSg/+FpAEuqoP/hqW5y22rgPOcpqT0LdNB05ClJBQzPWha57dIAGJ5qh9tSC8ZznpJUwJGnJBUwPKWhsD/NleFZoq/1ltQaw3OI/FylmfOCkSQV6M8YpT81lbQA6kXSNhx5doEHJamI5zwlLY4WM6dufHU5PLtcN0nVOfLUfLitNTDDDE87qqQZG2Z41jK09kja1PDD00CTNAP9u2BkGErqgOGPPNu2PWvXQFIHVA7PMwii7admVhVJhbY/UrsG1VQMzzQQ27TAO7FUQ+WRZz86/Lae1FPSLyw16Lcnt7D8RuEZEfuA9wFLwIcz810blHk1cDWQwNcy8zWnX2oaSjpjTTqENA9TwzMiloBDwKXAGnAkIlYz89hEmT3A3wJ/lJn3R8TTpy837Qg9tOSpFg3IrEeeFwHHM/MEQERcD+wHjk2UeT1wKDPvB8jM+7ZQp84xMLRotjuwmapJeO4E7p54vAZcvK7M8wAi4mZGU/urM/ML6xcUEQeBgwDs3jWYUHJH0xA4EzwzbV0w2g7sAV4K7AJuiogXZOaPJwtl5gqwArB04e+loSOw06qfmoTnPcDuice7xs9NWgNuzcyTwHci4luMwvRIK7VsmZ1VW+U+pCbheQTYExHnMwrNA8D6K+mfBa4A/i0idjCaxp843UK9YDR/ft4agqWlbuzHU8MzM09FxFXADYzOZ16bmXdGxDXA0cxcHb/2iog4BjwCvDUzfzjLirfJUNFGutJJ1U2RWede7e3Lv5vn3nq4yroXiQGgeVqiXxeB/yd+6yuZuVzy3j7+NEevGF5ar28Bo411OjwNnv4xGDRv26mTE9XCM0jDsRIDTmeiVjh1XadHnl1mAA2PIaEzsZDhafD1lwGnrqgYnmmItcxg0ZL7wNws5MizFsNt2AyuxVLxgpFh0jY7r07H/aNdjjxxp1oEbmO1reLI81HO5uFaq19YnmdWV/VtJlp12j60jty3ja/hc8Q9O9XCcxuP8iR+Vmv1WmdoBzJp1qqF5xKPcA4/qbV6qZccSXZH1ZGn4dlPdmCpYng+gYc4j7tqrV7SQD3C0lzWUy08t3OKp/GDWquXpC2pFp7n8BNexo21Vq8BmteIQ4KK4Xk2D7P7cf+j8TDZodV17qNl6p3z/PFJ9qyu1Vr9sHnfmPqoZ/ttver+DLij2trVdT3rSFo89XbRnwI3V1u7VJ+z5V6rF54/B75Rbe2ShmpOqVYvPB+BfKDa2lVBOBXXgFTbnU89Aj8aYHieZUBIC8Gu3rKT/r6GtBCqhWcyGn1KUh9VDc+TtVauwTqrdgW0MJy2a1A8IGteHHlKUoGqI0+vrUiah1kEndN2SYM3i4Ga03ZJKuDIs2M8ldE9dhJtpO73PGutXDoD7qfaiNN2SSrQKDwjYh/wPkY/ovXhzHzXJuVeBXwG+IPMPHq6ZTrylNRnU8MzIpaAQ8ClwBpwJCJWM/PYunLnAH8F3DqLisqRuhZTV+8aazLyvAg4npknACLiemA/cGxduXcC7wbe2mTFTtslNdHVnGgSnjvhcf9T2xpw8WSBiLgQ2J2Zn4uITcMzIg4CBwF+A6ftkvpr21YXEBHbgPcCb5lWNjNXMnM5M5fP3eqKJamiJiPPe4DdE493jZ97zDnABcCNEQHwm8BqRFx+uotGTtsl9VmT8DwC7ImI8xmF5gHgNY+9mJkPADseexwRNwJ/49V2SUM2NTwz81REXAXcwOirStdm5p0RcQ1wNDNXS1bsyFNSn0VmVlnxcyPyvVXWLEkj++Ermblc8l7vMJKkAv6epyQV8AdjesJRutQtTtslqYA/SSdJBRx5SlKBLd+eKUmLyGm7JBXwansFnq6Q+s9znh3kiFzqPqftklTAkackFfBquyQVcNouSQWctktSAaftklTA8JSkAoanJBUwPCWpgOEpSQUMT0kqYHhKUgHDU5IKGJ6SVMDwlKQChqckFTA8JamA4SlJBQxPSSpgeEpSAcNTkgoYnpJUwPCUpAKGpyQVaBSeEbEvIr4ZEccj4m0bvP7miDgWEbdHxBcj4tntV1WSumNqeEbEEnAIeCWwF7giIvauK/ZVYDkzfwf4DPCPbVdUkrqkycjzIuB4Zp7IzIeB64H9kwUy80uZ+eD44S3ArnarKUnd0iQ8dwJ3TzxeGz+3mSuBz2/0QkQcjIijEXH0wY0KSFJPtPr/tkfEa4Fl4CUbvZ6ZK8AKwDMjss11S9I8NQnPe4DdE493jZ97nIi4BHg78JLMfKid6klSNzWZth8B9kTE+RFxNnAAWJ0sEBEvBD4EXJ6Z97VfTUnqlqnhmZmngKuAG4BvAJ/KzDsj4pqIuHxc7J+AXwU+HRG3RcTqJouTpEFodM4zMw8Dh9c99/cTf1/Scr0kqdO8w0iSChieklTA8JSkAoanJBUwPCWpgOEpSQUMT0kqYHhKUgHDU5IKGJ6SVMDwlKQChqckFTA8JamA4SlJBQxPSSpgeEpSAcNTkgoYnpJUwPCUpAKGpyQVMDwlqYDhKUkFDE9JKmB4SlIBw1OSChieklTA8JSkAoanJBUwPCWpgOEpSQUMT0kqYHhKUgHDU5IKGJ6SVMDwlKQCjcIzIvZFxDcj4nhEvG2D158QEZ8cv35rRJzXek0lqUOmhmdELAGHgFcCe4ErImLvumJXAvdn5nOBfwbe3XZFJalLmow8LwKOZ+aJzHwYuB7Yv67MfuAj478/A7w8IqK9akpSt2xvUGYncPfE4zXg4s3KZOapiHgAeBrwg8lCEXEQODh++NA74I6SSvfMDtZ9DgNlO4dlUdr526VvbBKercnMFWAFICKOZubyPNdfg+0cFts5LBFxtPS9Tabt9wC7Jx7vGj+3YZmI2A6cC/ywtFKS1HVNwvMIsCcizo+Is4EDwOq6MqvAn43//hPgPzMz26umJHXL1Gn7+BzmVcANwBJwbWbeGRHXAEczcxX4V+BjEXEc+BGjgJ1mZQv17hPbOSy2c1iK2xkOECXpzHmHkSQVMDwlqcDMw3NRbu1s0M43R8SxiLg9Ir4YEc+uUc+tmtbOiXKvioiMiF5+3aVJOyPi1eNtemdEfHzedWxDg/32WRHxpYj46njfvaxGPbciIq6NiPsiYsPvlcfI+8efwe0RcWGjBWfmzP4xusD0beA5wNnA14C968r8BfDB8d8HgE/Osk4V2/ky4Enjv9841HaOy50D3ATcAizXrveMtuce4KvAr40fP712vWfUzhXgjeO/9wJ31a53QTv/GLgQuGOT1y8DPg8E8CLg1ibLnfXIc1Fu7Zzazsz8UmY+OH54C6Pvy/ZNk+0J8E5Gv2/w83lWrkVN2vl64FBm3g+QmffNuY5taNLOBJ4y/vtc4HtzrF8rMvMmRt8C2sx+4KM5cgvw1Ih4xrTlzjo8N7q1c+dmZTLzFPDYrZ190qSdk65kdKTrm6ntHE95dmfm5+ZZsZY12Z7PA54XETdHxC0RsW9utWtPk3ZeDbw2ItaAw8Cb5lO1uTrT/gvM+fZMQUS8FlgGXlK7Lm2LiG3Ae4HXVa7KPGxnNHV/KaNZxE0R8YLM/HHNSs3AFcB1mfmeiHgxo+9zX5CZj9auWG2zHnkuyq2dTdpJRFwCvB24PDMfmlPd2jStnecAFwA3RsRdjM4frfbwolGT7bkGrGbmycz8DvAtRmHaJ03aeSXwKYDM/DLwREY/GjIkjfrverMOz0W5tXNqOyPihcCHGAVnH8+PwZR2ZuYDmbkjM8/LzPMYndu9PDOLf3yhkib77WcZjTqJiB2MpvEn5ljHNjRp53eBlwNExPMZhef351rL2VsF/nR81f1FwAOZee/Ud83hStdljI7K3wbePn7uGkadCkYb49PAceC/gOfUvjo3o3b+B/C/wG3jf6u16zyLdq4reyM9vNrecHsGo1MUx4CvAwdq13lG7dwL3MzoSvxtwCtq17mgjZ8A7gVOMpoxXAm8AXjDxLY8NP4Mvt50n/X2TEkq4B1GklTA8JSkAoanJBUwPCWpgOEpSQUMT0kqYHhKUoH/A6WqYGFaRlhfAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "cmap = plt.cm.jet\n",
    "\n",
    "img3 = ax.imshow(u_pred_3d[:,:,50],vmax =100,vmin=0,cmap = cmap,extent=[0,1,0,1],aspect = 0.75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "32.85324"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(u_pred_3d[:,:,0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + test_loss_full[i][-1]\n",
    "print(a/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + train_loss_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
