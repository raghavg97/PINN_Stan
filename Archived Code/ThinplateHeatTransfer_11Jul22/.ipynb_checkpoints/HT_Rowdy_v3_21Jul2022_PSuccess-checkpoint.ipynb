{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def true_2D_1(xt): #True function for 2D_1 Klein Gordon Equation x \\in [-50,50] , t \\in [0,10]\n",
    "#     y = xt[:,0]*np.cos(xt[:,1])\n",
    "#     return y.reshape(-1,1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Material Properties This link - https://www.mathworks.com/help/pde/ug/nonlinear-heat-transfer-in-a-thin-plate.html#heatTransferThinPlateExample-1\n",
    "k = 400\n",
    "rho = 8960\n",
    "cp = 386\n",
    "t_z = 0.01\n",
    "stef_bolt = 5.670373e-8\n",
    "hc = 1\n",
    "Ta = 300\n",
    "emiss = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"Heat_tanh\"\n",
    "\n",
    "x = np.linspace(0,1,100).reshape(-1,1)\n",
    "y = np.linspace(0,1,100).reshape(-1,1)\n",
    "t = np.linspace(0,1,100).reshape(-1,1) #t is actually from 0 to 5000, let us scale it to 0 to 1\n",
    "\n",
    "X,Y,T = np.meshgrid(x,y,t)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "Y = Y.flatten('F').reshape(-1,1)\n",
    "T = T.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xyt = np.hstack((X,Y,T))\n",
    "\n",
    "initial_pts = np.logical_and(T==0,Y!=0).reshape(-1,)\n",
    "\n",
    "DBC_pts = (Y == 0).reshape(-1,)\n",
    "\n",
    "\n",
    "NBC_pts_x0 = (X == 0).reshape(-1,)\n",
    "NBC_pts_x1 = (X == 1).reshape(-1,)\n",
    "\n",
    "NBC_pts_y0 = (Y == 0).reshape(-1,)\n",
    "NBC_pts_y1 = (Y == 1).reshape(-1,)\n",
    "\n",
    "xyt_initial = xyt[initial_pts,:]\n",
    "xyt_DBC = xyt[DBC_pts,:]\n",
    "\n",
    "xyt_NBC_x0 = xyt[NBC_pts_x0,:]\n",
    "xyt_NBC_x1 = xyt[NBC_pts_x1,:]\n",
    "\n",
    "#xyt_NBC_y0 = xyt[NBC_pts_y0,:]\n",
    "xyt_NBC_y1 = xyt[NBC_pts_y1,:]\n",
    "\n",
    "u_initial = 300*np.ones((np.shape(xyt_initial)[0],1))\n",
    "u_DBC = 1000*np.ones((np.shape(xyt_DBC)[0],1))\n",
    "\n",
    "xyt_I_DBC = np.vstack((xyt_initial,xyt_DBC))\n",
    "#xyt_NBC = np.vstack((xyt_NBC_1,xyt_NBC_2,xyt_NBC_3,xyt_NBC_4))\n",
    "xyt_NBC_x = np.vstack((xyt_NBC_x0,xyt_NBC_x1))\n",
    "#xyt_NBC_y = np.vstack((xyt_NBC_y0,xyt_NBC_y1))\n",
    "xyt_NBC_y = np.vstack((xyt_NBC_y1))\n",
    "\n",
    "u_I_DBC = np.vstack((u_initial,u_DBC))\n",
    "\n",
    "xyt_test_tensor = torch.from_numpy(xyt).float().to(device)\n",
    "\n",
    "lb_xyt = xyt[0]\n",
    "ub_xyt = xyt[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_D,N_N,N_f,seed):\n",
    "    '''Boundary Conditions''' \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    #choose random N_u points for training\n",
    "    idx = np.random.choice(xyt_I_DBC.shape[0], N_D, replace=False) \n",
    "    xyt_D = xyt_I_DBC[idx,:] #choose indices from  set 'idx' (x,t)\n",
    "    u_D = u_I_DBC[idx].reshape(-1,1)      #choose corresponding u\n",
    "\n",
    "    idx = np.random.choice(xyt_NBC_x.shape[0], N_D, replace=False) \n",
    "    xyt_Nx = xyt_NBC_x[idx,:] #choose indices from  set 'idx' (x,t)\n",
    "\n",
    "    idx = np.random.choice(xyt_NBC_y.shape[0], N_D, replace=False) \n",
    "    xyt_Ny = xyt_NBC_y[idx,:] #choose indices from  set 'idx' (x,t)\n",
    "\n",
    "    '''Collocation Points'''\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    samples = sampling(N_f)\n",
    "    \n",
    "    xyt_coll = lb_xyt + (ub_xyt - lb_xyt)*samples\n",
    "    xyt_coll = np.vstack((xyt_coll, xyt_D,xyt_Nx,xyt_Ny)) # append training points to collocation points \n",
    "\n",
    "    return xyt_coll, xyt_D, u_D, xyt_Nx,xyt_Ny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        self.iter = 0\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)   \n",
    "        \n",
    "#         self.beta = Parameter(torch.ones((50,len(layers)-2)))\n",
    "#         self.beta.requiresGrad = True\n",
    "        \n",
    "        self.train_loss = []\n",
    "        self.test_loss = []\n",
    "        \n",
    "        self.beta_val = []\n",
    "        \n",
    "        self.n = torch.tensor(1.0)\n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xyt):\n",
    "        if torch.is_tensor(xyt) != True:         \n",
    "            xyt = torch.from_numpy(xyt)                \n",
    "        \n",
    "        ubxyt = torch.from_numpy(ub_xyt).float().to(device)\n",
    "        lbxyt = torch.from_numpy(lb_xyt).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xyt = (xyt - lbxyt)/(ubxyt - lbxyt)\n",
    "        \n",
    "        #convert to float\n",
    "        a = xyt.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z) \n",
    "             \n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_D(self,xyt_D,u_D):\n",
    "                \n",
    "        loss_bc = self.loss_function(self.forward(xyt_D), u_D)\n",
    "                \n",
    "        return loss_bc\n",
    "    \n",
    "    def loss_N(self,xyt_Nx,xyt_Ny,N_hat):\n",
    "        \n",
    "        g1 = xyt_Nx.clone()             \n",
    "        g1.requires_grad = True\n",
    "        u1 = self.forward(g1)\n",
    "        \n",
    "        u1_x_y_t = autograd.grad(u1,g1,torch.ones([xyt_Nx.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        du1_dx = u1_x_y_t[:,[0]]\n",
    "        \n",
    "        g2 = xyt_Ny.clone()             \n",
    "        g2.requires_grad = True\n",
    "        u2 = self.forward(g2)\n",
    "        \n",
    "        u2_x_y_t = autograd.grad(u2,g2,torch.ones([xyt_Ny.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        du2_dy = u2_x_y_t[:,[1]]\n",
    "               \n",
    "        loss_N1 = self.loss_function(du1_dx,N_hat)\n",
    "        loss_N2 = self.loss_function(du2_dy,N_hat)\n",
    "        \n",
    "        #return loss_N1+loss_N2       \n",
    "        return loss_N1 + loss_N2\n",
    "    \n",
    "    def loss_PDE(self, xyt_coll, f_hat):\n",
    "        \n",
    "        g = xyt_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_y_t = autograd.grad(u,g,torch.ones([xyt_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_xx_yy_tt = autograd.grad(u_x_y_t,g,torch.ones(xyt_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        du_dt = u_x_y_t[:,[2]]\n",
    "        \n",
    "        d2u_dx2 = u_xx_yy_tt[:,[0]]\n",
    "        d2u_dy2 = u_xx_yy_tt[:,[1]]    \n",
    "        \n",
    "\n",
    "        f = rho*cp*t_z*du_dt/3000 - k*t_z*(d2u_dx2+d2u_dy2) + 2*hc*(u-Ta) + 2*emiss*stef_bolt*(torch.pow(u,4)-Ta**4) \n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xyt_D,u_D,xyt_Nx,xyt_Ny,N_hat,xyt_coll,f_hat):\n",
    "\n",
    "        loss_D = self.loss_D(xyt_D,u_D)\n",
    "        loss_N = self.loss_N(xyt_Nx,xyt_Ny,N_hat)\n",
    "        loss_f = self.loss_PDE(xyt_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_D + loss_N + loss_f\n",
    "        \n",
    "        #print(self.iter,\"loss_D:\",loss_D.cpu().detach().numpy(),\"loss_N:\",loss_N.cpu().detach().numpy(),\"loss_f:\",loss_f.cpu().detach().numpy())\n",
    "        \n",
    "        return loss_val\n",
    "     \n",
    "    'callable for optimizer'                                       \n",
    "    def closure(self):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = self.loss(xyt_D,u_D,xyt_Nx,xyt_Ny,N_hat,xyt_coll,f_hat)\n",
    "        self.train_loss.append(loss.cpu().detach().numpy())\n",
    "        \n",
    "        u_pred = self.test(xyt_test_tensor)\n",
    "        #self.test_loss.append(np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))) #Commented because no true values yet\n",
    "        #self.beta_val.append(self.alpha.cpu().detach().numpy())\n",
    "        \n",
    "        #print(self.iter,\"Train Loss\",self.train_loss[-1],\"Test Loss\",self.test_loss[-1])\n",
    "        print(self.iter,\"Train Loss\",self.train_loss[-1])\n",
    "        \n",
    "        loss.backward()\n",
    "        self.iter += 1\n",
    "  \n",
    "\n",
    "        return loss        \n",
    "    'test neural network'\n",
    "    def test(self,xyt_test_tensor):\n",
    "        u_pred = self.forward(xyt_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0 Train Loss 1683767.5\n",
      "1 Train Loss 1682065.1\n",
      "2 Train Loss 1667887.8\n",
      "3 Train Loss 1619635.2\n",
      "4 Train Loss 1510045.9\n",
      "5 Train Loss 1828981.5\n",
      "6 Train Loss 1481455.0\n",
      "7 Train Loss 1453051.0\n",
      "8 Train Loss 1556143.8\n",
      "9 Train Loss 1404337.2\n",
      "10 Train Loss 1347620.5\n",
      "11 Train Loss 1562894.8\n",
      "12 Train Loss 1255487.8\n",
      "13 Train Loss 1330398.5\n",
      "14 Train Loss 1213968.2\n",
      "15 Train Loss 1130228.4\n",
      "16 Train Loss 971314.8\n",
      "17 Train Loss 916470.6\n",
      "18 Train Loss 823411.5\n",
      "19 Train Loss 759469.6\n",
      "20 Train Loss 831210.06\n",
      "21 Train Loss 735840.4\n",
      "22 Train Loss 701700.44\n",
      "23 Train Loss 684294.6\n",
      "24 Train Loss 669498.2\n",
      "25 Train Loss 644226.3\n",
      "26 Train Loss 630444.25\n",
      "27 Train Loss 622613.25\n",
      "28 Train Loss 614886.9\n",
      "29 Train Loss 602490.44\n",
      "30 Train Loss 588526.6\n",
      "31 Train Loss 570365.6\n",
      "32 Train Loss 558352.3\n",
      "33 Train Loss 565671.56\n",
      "34 Train Loss 548583.0\n",
      "35 Train Loss 544138.9\n",
      "36 Train Loss 542083.3\n",
      "37 Train Loss 538073.94\n",
      "38 Train Loss 532448.56\n",
      "39 Train Loss 526395.3\n",
      "40 Train Loss 523083.25\n",
      "41 Train Loss 629752.0\n",
      "42 Train Loss 517658.22\n",
      "43 Train Loss 511266.84\n",
      "44 Train Loss 515602.2\n",
      "45 Train Loss 506376.84\n",
      "46 Train Loss 504544.44\n",
      "47 Train Loss 503295.66\n",
      "48 Train Loss 501841.06\n",
      "49 Train Loss 499947.62\n",
      "50 Train Loss 495952.97\n",
      "51 Train Loss 496336.88\n",
      "52 Train Loss 489392.03\n",
      "53 Train Loss 484237.3\n",
      "54 Train Loss 478681.7\n",
      "55 Train Loss 470650.9\n",
      "56 Train Loss 468539.28\n",
      "57 Train Loss 466463.03\n",
      "58 Train Loss 462465.7\n",
      "59 Train Loss 460380.72\n",
      "60 Train Loss 457615.06\n",
      "61 Train Loss 453822.97\n",
      "62 Train Loss 448392.6\n",
      "63 Train Loss 442983.9\n",
      "64 Train Loss 440538.47\n",
      "65 Train Loss 442445.44\n",
      "66 Train Loss 438040.2\n",
      "67 Train Loss 461346.12\n",
      "68 Train Loss 434529.2\n",
      "69 Train Loss 421434.3\n",
      "70 Train Loss 471937.7\n",
      "71 Train Loss 405460.1\n",
      "72 Train Loss 403118.5\n",
      "73 Train Loss 420012.53\n",
      "74 Train Loss 398888.78\n",
      "75 Train Loss 398261.12\n",
      "76 Train Loss 395457.38\n",
      "77 Train Loss 395481.0\n",
      "78 Train Loss 393703.72\n",
      "79 Train Loss 392790.12\n",
      "80 Train Loss 389748.94\n",
      "81 Train Loss 386323.7\n",
      "82 Train Loss 382920.34\n",
      "83 Train Loss 378571.22\n",
      "84 Train Loss 375784.12\n",
      "85 Train Loss 372375.97\n",
      "86 Train Loss 410186.84\n",
      "87 Train Loss 370952.66\n",
      "88 Train Loss 364390.47\n",
      "89 Train Loss 355521.84\n",
      "90 Train Loss 362857.34\n",
      "91 Train Loss 352431.03\n",
      "92 Train Loss 349498.9\n",
      "93 Train Loss 345824.16\n",
      "94 Train Loss 340033.84\n",
      "95 Train Loss 331896.44\n",
      "96 Train Loss 332533.25\n",
      "97 Train Loss 328490.88\n",
      "98 Train Loss 324536.75\n",
      "99 Train Loss 325549.94\n",
      "100 Train Loss 319496.4\n",
      "101 Train Loss 314753.2\n",
      "102 Train Loss 301746.4\n",
      "103 Train Loss 295278.8\n",
      "104 Train Loss 268848.03\n",
      "105 Train Loss 285976.38\n",
      "106 Train Loss 265096.78\n",
      "107 Train Loss 268799.25\n",
      "108 Train Loss 262348.88\n",
      "109 Train Loss 274943.62\n",
      "110 Train Loss 253917.81\n",
      "111 Train Loss 251631.92\n",
      "112 Train Loss 247395.36\n",
      "113 Train Loss 245547.11\n",
      "114 Train Loss 244232.3\n",
      "115 Train Loss 248547.62\n",
      "116 Train Loss 243828.48\n",
      "117 Train Loss 243684.47\n",
      "118 Train Loss 243319.58\n",
      "119 Train Loss 243143.1\n",
      "120 Train Loss 242928.92\n",
      "121 Train Loss 242559.2\n",
      "122 Train Loss 242018.12\n",
      "123 Train Loss 241787.3\n",
      "124 Train Loss 241525.22\n",
      "125 Train Loss 241133.67\n",
      "126 Train Loss 240669.89\n",
      "127 Train Loss 241656.78\n",
      "128 Train Loss 240326.73\n",
      "129 Train Loss 240300.83\n",
      "130 Train Loss 240181.39\n",
      "131 Train Loss 240098.73\n",
      "132 Train Loss 239962.02\n",
      "133 Train Loss 239586.2\n",
      "134 Train Loss 239065.11\n",
      "135 Train Loss 238923.61\n",
      "136 Train Loss 238538.52\n",
      "137 Train Loss 238192.05\n",
      "138 Train Loss 238021.81\n",
      "139 Train Loss 237727.03\n",
      "140 Train Loss 237452.64\n",
      "141 Train Loss 236953.17\n",
      "142 Train Loss 236615.58\n",
      "143 Train Loss 236384.9\n",
      "144 Train Loss 236130.14\n",
      "145 Train Loss 235650.56\n",
      "146 Train Loss 235044.53\n",
      "147 Train Loss 234651.22\n",
      "148 Train Loss 234421.94\n",
      "149 Train Loss 234235.9\n",
      "150 Train Loss 235792.33\n",
      "151 Train Loss 234081.33\n",
      "152 Train Loss 234184.06\n",
      "153 Train Loss 233987.23\n",
      "154 Train Loss 233680.28\n",
      "155 Train Loss 233330.8\n",
      "156 Train Loss 233099.86\n",
      "157 Train Loss 232852.58\n",
      "158 Train Loss 232546.61\n",
      "159 Train Loss 232277.2\n",
      "160 Train Loss 232168.36\n",
      "161 Train Loss 231513.75\n",
      "162 Train Loss 230876.5\n",
      "163 Train Loss 230214.0\n",
      "164 Train Loss 229566.39\n",
      "165 Train Loss 228443.42\n",
      "166 Train Loss 247355.34\n",
      "167 Train Loss 227002.81\n",
      "168 Train Loss 225592.67\n",
      "169 Train Loss 229400.86\n",
      "170 Train Loss 224397.75\n",
      "171 Train Loss 223370.02\n",
      "172 Train Loss 223443.5\n",
      "173 Train Loss 222191.06\n",
      "174 Train Loss 221504.3\n",
      "175 Train Loss 220862.6\n",
      "176 Train Loss 221011.23\n",
      "177 Train Loss 220541.31\n",
      "178 Train Loss 220250.64\n",
      "179 Train Loss 219564.92\n",
      "180 Train Loss 218753.95\n",
      "181 Train Loss 217704.17\n",
      "182 Train Loss 216670.61\n",
      "183 Train Loss 218142.58\n",
      "184 Train Loss 215857.97\n",
      "185 Train Loss 217038.92\n",
      "186 Train Loss 215184.94\n",
      "187 Train Loss 214940.6\n",
      "188 Train Loss 214486.83\n",
      "189 Train Loss 214182.36\n",
      "190 Train Loss 213677.75\n",
      "191 Train Loss 212507.39\n",
      "192 Train Loss 211907.66\n",
      "193 Train Loss 211518.5\n",
      "194 Train Loss 211439.55\n",
      "195 Train Loss 211283.86\n",
      "196 Train Loss 211029.27\n",
      "197 Train Loss 210688.92\n",
      "198 Train Loss 210351.72\n",
      "199 Train Loss 210066.55\n",
      "200 Train Loss 209788.61\n",
      "201 Train Loss 209399.48\n",
      "202 Train Loss 208998.34\n",
      "203 Train Loss 208303.81\n",
      "204 Train Loss 207240.25\n",
      "205 Train Loss 206505.86\n",
      "206 Train Loss 206105.84\n",
      "207 Train Loss 205735.08\n",
      "208 Train Loss 205384.88\n",
      "209 Train Loss 205125.64\n",
      "210 Train Loss 204669.66\n",
      "211 Train Loss 204347.2\n",
      "212 Train Loss 204023.22\n",
      "213 Train Loss 203835.69\n",
      "214 Train Loss 203622.53\n",
      "215 Train Loss 203350.86\n",
      "216 Train Loss 202951.0\n",
      "217 Train Loss 202516.92\n",
      "218 Train Loss 201901.98\n",
      "219 Train Loss 201627.95\n",
      "220 Train Loss 201057.64\n",
      "221 Train Loss 200607.33\n",
      "222 Train Loss 200383.39\n",
      "223 Train Loss 200189.58\n",
      "224 Train Loss 199965.2\n",
      "225 Train Loss 199804.98\n",
      "226 Train Loss 199567.19\n",
      "227 Train Loss 199335.73\n",
      "228 Train Loss 199170.53\n",
      "229 Train Loss 199000.1\n",
      "230 Train Loss 198753.36\n",
      "231 Train Loss 198453.84\n",
      "232 Train Loss 197903.06\n",
      "233 Train Loss 197417.33\n",
      "234 Train Loss 197017.31\n",
      "235 Train Loss 196586.6\n",
      "236 Train Loss 196529.47\n",
      "237 Train Loss 196321.9\n",
      "238 Train Loss 195973.36\n",
      "239 Train Loss 195737.45\n",
      "240 Train Loss 195586.06\n",
      "241 Train Loss 195306.66\n",
      "242 Train Loss 195155.52\n",
      "243 Train Loss 195062.03\n",
      "244 Train Loss 194993.6\n",
      "245 Train Loss 194894.92\n",
      "246 Train Loss 194784.11\n",
      "247 Train Loss 194540.92\n",
      "248 Train Loss 194101.88\n",
      "249 Train Loss 193647.89\n",
      "250 Train Loss 193283.75\n",
      "251 Train Loss 192885.89\n",
      "252 Train Loss 192763.28\n",
      "253 Train Loss 192465.23\n",
      "254 Train Loss 192265.75\n",
      "255 Train Loss 192154.22\n",
      "256 Train Loss 192036.3\n",
      "257 Train Loss 191927.64\n",
      "258 Train Loss 191815.1\n",
      "259 Train Loss 191625.89\n",
      "260 Train Loss 191432.67\n",
      "261 Train Loss 191812.7\n",
      "262 Train Loss 191327.8\n",
      "263 Train Loss 191134.73\n",
      "264 Train Loss 190980.8\n",
      "265 Train Loss 190608.42\n",
      "266 Train Loss 190406.23\n",
      "267 Train Loss 190395.11\n",
      "268 Train Loss 190309.23\n",
      "269 Train Loss 190226.05\n",
      "270 Train Loss 190095.86\n",
      "271 Train Loss 189863.33\n",
      "272 Train Loss 189705.58\n",
      "273 Train Loss 189691.72\n",
      "274 Train Loss 189610.11\n",
      "275 Train Loss 189495.47\n",
      "276 Train Loss 189454.92\n",
      "277 Train Loss 189429.1\n",
      "278 Train Loss 189377.48\n",
      "279 Train Loss 189256.27\n",
      "280 Train Loss 189144.22\n",
      "281 Train Loss 188932.7\n",
      "282 Train Loss 188403.89\n",
      "283 Train Loss 188886.48\n",
      "284 Train Loss 188246.44\n",
      "285 Train Loss 188150.4\n",
      "286 Train Loss 187837.98\n",
      "287 Train Loss 187684.34\n",
      "288 Train Loss 187529.88\n",
      "289 Train Loss 187426.9\n",
      "290 Train Loss 187219.88\n",
      "291 Train Loss 186920.45\n",
      "292 Train Loss 186793.89\n",
      "293 Train Loss 186560.45\n",
      "294 Train Loss 186482.73\n",
      "295 Train Loss 186421.3\n",
      "296 Train Loss 186350.69\n",
      "297 Train Loss 186309.28\n",
      "298 Train Loss 186246.9\n",
      "299 Train Loss 186179.5\n",
      "300 Train Loss 186116.1\n",
      "301 Train Loss 186036.45\n",
      "302 Train Loss 185869.94\n",
      "303 Train Loss 185742.3\n",
      "304 Train Loss 185508.28\n",
      "305 Train Loss 185275.22\n",
      "306 Train Loss 184920.11\n",
      "307 Train Loss 184871.23\n",
      "308 Train Loss 184774.3\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "309 Train Loss 184684.06\n",
      "310 Train Loss 184644.02\n",
      "311 Train Loss 184609.55\n",
      "312 Train Loss 184551.67\n",
      "313 Train Loss 184476.89\n",
      "314 Train Loss 184417.17\n",
      "315 Train Loss 184337.95\n",
      "316 Train Loss 184207.31\n",
      "317 Train Loss 183998.45\n",
      "318 Train Loss 183747.88\n",
      "319 Train Loss 183555.12\n",
      "320 Train Loss 183251.48\n",
      "321 Train Loss 183116.39\n",
      "322 Train Loss 182995.58\n",
      "323 Train Loss 182895.08\n",
      "324 Train Loss 182810.8\n",
      "325 Train Loss 182716.1\n",
      "326 Train Loss 182649.98\n",
      "327 Train Loss 182577.03\n",
      "328 Train Loss 182483.31\n",
      "329 Train Loss 182294.25\n",
      "330 Train Loss 182089.06\n",
      "331 Train Loss 181914.23\n",
      "332 Train Loss 181748.95\n",
      "333 Train Loss 181675.92\n",
      "334 Train Loss 181557.05\n",
      "335 Train Loss 181482.5\n",
      "336 Train Loss 181392.11\n",
      "337 Train Loss 181281.39\n",
      "338 Train Loss 181147.81\n",
      "339 Train Loss 180979.98\n",
      "340 Train Loss 180827.44\n",
      "341 Train Loss 180703.08\n",
      "342 Train Loss 180587.48\n",
      "343 Train Loss 180457.77\n",
      "344 Train Loss 180316.73\n",
      "345 Train Loss 180189.03\n",
      "346 Train Loss 179984.1\n",
      "347 Train Loss 179873.72\n",
      "348 Train Loss 179745.97\n",
      "349 Train Loss 179556.9\n",
      "350 Train Loss 179370.1\n",
      "351 Train Loss 179426.16\n",
      "352 Train Loss 179268.48\n",
      "353 Train Loss 179195.83\n",
      "354 Train Loss 179090.56\n",
      "355 Train Loss 178948.2\n",
      "356 Train Loss 178895.16\n",
      "357 Train Loss 178843.05\n",
      "358 Train Loss 178741.33\n",
      "359 Train Loss 178608.6\n",
      "360 Train Loss 178431.83\n",
      "361 Train Loss 178293.6\n",
      "362 Train Loss 178242.5\n",
      "363 Train Loss 178151.33\n",
      "364 Train Loss 177965.3\n",
      "365 Train Loss 177755.31\n",
      "366 Train Loss 177563.42\n",
      "367 Train Loss 177383.56\n",
      "368 Train Loss 177239.33\n",
      "369 Train Loss 177093.25\n",
      "370 Train Loss 177084.36\n",
      "371 Train Loss 176950.62\n",
      "372 Train Loss 177004.12\n",
      "373 Train Loss 176899.0\n",
      "374 Train Loss 176855.67\n",
      "375 Train Loss 176771.86\n",
      "376 Train Loss 176667.39\n",
      "377 Train Loss 176547.69\n",
      "378 Train Loss 176454.8\n",
      "379 Train Loss 176390.84\n",
      "380 Train Loss 176339.7\n",
      "381 Train Loss 176287.7\n",
      "382 Train Loss 176219.8\n",
      "383 Train Loss 176149.06\n",
      "384 Train Loss 176061.84\n",
      "385 Train Loss 175979.05\n",
      "386 Train Loss 175861.92\n",
      "387 Train Loss 175746.95\n",
      "388 Train Loss 175618.44\n",
      "389 Train Loss 175500.45\n",
      "390 Train Loss 175333.39\n",
      "391 Train Loss 175130.17\n",
      "392 Train Loss 174974.64\n",
      "393 Train Loss 174875.98\n",
      "394 Train Loss 174790.58\n",
      "395 Train Loss 174715.69\n",
      "396 Train Loss 174663.2\n",
      "397 Train Loss 174619.25\n",
      "398 Train Loss 174571.86\n",
      "399 Train Loss 174485.97\n",
      "400 Train Loss 174337.22\n",
      "401 Train Loss 174222.4\n",
      "402 Train Loss 174074.53\n",
      "403 Train Loss 174010.58\n",
      "404 Train Loss 173938.86\n",
      "405 Train Loss 173885.58\n",
      "406 Train Loss 173815.78\n",
      "407 Train Loss 173765.19\n",
      "408 Train Loss 173709.39\n",
      "409 Train Loss 173680.89\n",
      "410 Train Loss 173653.77\n",
      "411 Train Loss 173632.61\n",
      "412 Train Loss 173608.48\n",
      "413 Train Loss 173571.48\n",
      "414 Train Loss 173520.98\n",
      "415 Train Loss 173425.86\n",
      "416 Train Loss 173281.77\n",
      "417 Train Loss 173183.75\n",
      "418 Train Loss 173052.17\n",
      "419 Train Loss 172913.19\n",
      "420 Train Loss 172766.86\n",
      "421 Train Loss 172652.53\n",
      "422 Train Loss 172533.28\n",
      "423 Train Loss 172448.47\n",
      "424 Train Loss 172409.27\n",
      "425 Train Loss 172387.38\n",
      "426 Train Loss 172364.2\n",
      "427 Train Loss 172343.52\n",
      "428 Train Loss 172289.06\n",
      "429 Train Loss 172247.5\n",
      "430 Train Loss 172173.9\n",
      "431 Train Loss 172097.22\n",
      "432 Train Loss 171997.25\n",
      "433 Train Loss 171812.66\n",
      "434 Train Loss 171702.36\n",
      "435 Train Loss 171688.86\n",
      "436 Train Loss 171629.77\n",
      "437 Train Loss 171564.64\n",
      "438 Train Loss 171440.56\n",
      "439 Train Loss 171316.17\n",
      "440 Train Loss 171244.6\n",
      "441 Train Loss 171184.64\n",
      "442 Train Loss 171163.97\n",
      "443 Train Loss 171147.31\n",
      "444 Train Loss 171128.47\n",
      "445 Train Loss 171116.6\n",
      "446 Train Loss 171097.92\n",
      "447 Train Loss 171070.06\n",
      "448 Train Loss 171020.1\n",
      "449 Train Loss 170941.98\n",
      "450 Train Loss 170865.69\n",
      "451 Train Loss 170810.34\n",
      "452 Train Loss 170774.02\n",
      "453 Train Loss 170752.16\n",
      "454 Train Loss 170730.1\n",
      "455 Train Loss 170710.53\n",
      "456 Train Loss 170692.52\n",
      "457 Train Loss 170675.66\n",
      "458 Train Loss 170633.25\n",
      "459 Train Loss 170580.61\n",
      "460 Train Loss 170511.0\n",
      "461 Train Loss 170412.75\n",
      "462 Train Loss 170363.4\n",
      "463 Train Loss 170256.95\n",
      "464 Train Loss 170164.25\n",
      "465 Train Loss 170003.28\n",
      "466 Train Loss 169929.66\n",
      "467 Train Loss 169832.9\n",
      "468 Train Loss 169836.84\n",
      "469 Train Loss 169782.25\n",
      "470 Train Loss 169770.56\n",
      "471 Train Loss 169737.23\n",
      "472 Train Loss 169717.83\n",
      "473 Train Loss 169694.97\n",
      "474 Train Loss 169634.64\n",
      "475 Train Loss 169560.39\n",
      "476 Train Loss 169484.97\n",
      "477 Train Loss 169447.4\n",
      "478 Train Loss 169403.64\n",
      "479 Train Loss 169364.66\n",
      "480 Train Loss 169314.83\n",
      "481 Train Loss 169230.61\n",
      "482 Train Loss 169166.1\n",
      "483 Train Loss 169102.83\n",
      "484 Train Loss 169067.9\n",
      "485 Train Loss 168999.4\n",
      "486 Train Loss 168972.17\n",
      "487 Train Loss 168948.94\n",
      "488 Train Loss 168914.11\n",
      "489 Train Loss 168848.0\n",
      "490 Train Loss 168782.33\n",
      "491 Train Loss 168737.95\n",
      "492 Train Loss 168720.61\n",
      "493 Train Loss 168704.47\n",
      "494 Train Loss 168683.12\n",
      "495 Train Loss 168647.53\n",
      "496 Train Loss 168610.11\n",
      "497 Train Loss 168567.53\n",
      "498 Train Loss 168501.05\n",
      "499 Train Loss 168441.47\n",
      "500 Train Loss 168396.28\n",
      "501 Train Loss 168369.19\n",
      "502 Train Loss 168347.98\n",
      "503 Train Loss 168322.84\n",
      "504 Train Loss 168284.94\n",
      "505 Train Loss 168214.73\n",
      "506 Train Loss 168117.67\n",
      "507 Train Loss 168025.77\n",
      "508 Train Loss 168126.88\n",
      "509 Train Loss 167967.3\n",
      "510 Train Loss 167906.17\n",
      "511 Train Loss 167854.45\n",
      "512 Train Loss 167844.17\n",
      "513 Train Loss 167819.95\n",
      "514 Train Loss 167802.02\n",
      "515 Train Loss 167781.95\n",
      "516 Train Loss 167744.75\n",
      "517 Train Loss 167723.89\n",
      "518 Train Loss 167672.83\n",
      "519 Train Loss 167617.16\n",
      "520 Train Loss 167563.61\n",
      "521 Train Loss 167516.77\n",
      "522 Train Loss 167478.84\n",
      "523 Train Loss 167438.95\n",
      "524 Train Loss 167404.16\n",
      "525 Train Loss 167379.8\n",
      "526 Train Loss 167342.6\n",
      "527 Train Loss 167290.3\n",
      "528 Train Loss 167231.3\n",
      "529 Train Loss 167128.72\n",
      "530 Train Loss 167060.52\n",
      "531 Train Loss 167045.64\n",
      "532 Train Loss 166978.72\n",
      "533 Train Loss 166950.16\n",
      "534 Train Loss 166907.38\n",
      "535 Train Loss 166863.61\n",
      "536 Train Loss 166841.52\n",
      "537 Train Loss 166817.45\n",
      "538 Train Loss 166800.86\n",
      "539 Train Loss 166780.97\n",
      "540 Train Loss 166766.19\n",
      "541 Train Loss 166738.78\n",
      "542 Train Loss 166710.16\n",
      "543 Train Loss 166655.98\n",
      "544 Train Loss 166571.97\n",
      "545 Train Loss 166571.55\n",
      "546 Train Loss 166520.98\n",
      "547 Train Loss 166441.53\n",
      "548 Train Loss 166391.84\n",
      "549 Train Loss 166344.73\n",
      "550 Train Loss 166333.31\n",
      "551 Train Loss 166293.72\n",
      "552 Train Loss 166255.44\n",
      "553 Train Loss 166201.95\n",
      "554 Train Loss 166162.95\n",
      "555 Train Loss 166134.64\n",
      "556 Train Loss 166074.66\n",
      "557 Train Loss 166099.38\n",
      "558 Train Loss 166043.55\n",
      "559 Train Loss 166046.47\n",
      "560 Train Loss 166007.64\n",
      "561 Train Loss 165977.3\n",
      "562 Train Loss 165940.69\n",
      "563 Train Loss 165899.19\n",
      "564 Train Loss 165903.61\n",
      "565 Train Loss 165888.19\n",
      "566 Train Loss 165871.16\n",
      "567 Train Loss 165856.02\n",
      "568 Train Loss 165855.16\n",
      "569 Train Loss 165843.88\n",
      "570 Train Loss 165824.45\n",
      "571 Train Loss 165805.98\n",
      "572 Train Loss 165792.39\n",
      "573 Train Loss 165765.81\n",
      "574 Train Loss 165708.88\n",
      "575 Train Loss 165668.98\n",
      "576 Train Loss 165614.84\n",
      "577 Train Loss 165558.95\n",
      "578 Train Loss 165543.12\n",
      "579 Train Loss 165479.08\n",
      "580 Train Loss 165448.67\n",
      "581 Train Loss 165408.83\n",
      "582 Train Loss 165318.27\n",
      "583 Train Loss 165223.45\n",
      "584 Train Loss 165170.23\n",
      "585 Train Loss 165252.97\n",
      "586 Train Loss 165137.97\n",
      "587 Train Loss 165105.75\n",
      "588 Train Loss 165066.9\n",
      "589 Train Loss 165002.84\n",
      "590 Train Loss 164909.4\n",
      "591 Train Loss 164779.02\n",
      "592 Train Loss 164682.05\n",
      "593 Train Loss 164600.0\n",
      "594 Train Loss 164890.34\n",
      "595 Train Loss 164539.53\n",
      "596 Train Loss 164433.77\n",
      "597 Train Loss 164319.78\n",
      "598 Train Loss 164064.94\n",
      "599 Train Loss 163886.98\n",
      "600 Train Loss 163621.67\n",
      "601 Train Loss 163667.14\n",
      "602 Train Loss 163496.7\n",
      "603 Train Loss 163325.84\n",
      "604 Train Loss 163252.98\n",
      "605 Train Loss 163252.03\n",
      "606 Train Loss 163200.34\n",
      "607 Train Loss 163172.64\n",
      "608 Train Loss 163150.12\n",
      "609 Train Loss 163132.94\n",
      "610 Train Loss 163118.1\n",
      "611 Train Loss 163099.83\n",
      "612 Train Loss 163077.3\n",
      "613 Train Loss 163065.39\n",
      "614 Train Loss 163058.0\n",
      "615 Train Loss 163049.22\n",
      "616 Train Loss 163034.0\n",
      "617 Train Loss 162995.7\n",
      "618 Train Loss 162960.23\n",
      "619 Train Loss 162905.52\n",
      "620 Train Loss 162860.31\n",
      "621 Train Loss 162819.28\n",
      "622 Train Loss 162757.52\n",
      "623 Train Loss 162667.94\n",
      "624 Train Loss 162575.02\n",
      "625 Train Loss 162468.83\n",
      "626 Train Loss 162372.02\n",
      "627 Train Loss 162308.27\n",
      "628 Train Loss 162282.98\n",
      "629 Train Loss 162265.61\n",
      "630 Train Loss 162269.45\n",
      "631 Train Loss 162249.16\n",
      "632 Train Loss 162242.75\n",
      "633 Train Loss 162216.72\n",
      "634 Train Loss 162200.72\n",
      "635 Train Loss 162179.02\n",
      "636 Train Loss 162156.8\n",
      "637 Train Loss 162140.64\n",
      "638 Train Loss 162122.39\n",
      "639 Train Loss 162096.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "640 Train Loss 162056.6\n",
      "641 Train Loss 162004.3\n",
      "642 Train Loss 161953.61\n",
      "643 Train Loss 161920.72\n",
      "644 Train Loss 161904.33\n",
      "645 Train Loss 161885.64\n",
      "646 Train Loss 161844.03\n",
      "647 Train Loss 161800.05\n",
      "648 Train Loss 161735.39\n",
      "649 Train Loss 161703.44\n",
      "650 Train Loss 161664.66\n",
      "651 Train Loss 161627.19\n",
      "652 Train Loss 161601.08\n",
      "653 Train Loss 161565.66\n",
      "654 Train Loss 161534.12\n",
      "655 Train Loss 161525.64\n",
      "656 Train Loss 161510.31\n",
      "657 Train Loss 161503.97\n",
      "658 Train Loss 161492.62\n",
      "659 Train Loss 161473.25\n",
      "660 Train Loss 161453.81\n",
      "661 Train Loss 161423.12\n",
      "662 Train Loss 161384.33\n",
      "663 Train Loss 161332.44\n",
      "664 Train Loss 161260.69\n",
      "665 Train Loss 161191.36\n",
      "666 Train Loss 161140.47\n",
      "667 Train Loss 161092.14\n",
      "668 Train Loss 161052.75\n",
      "669 Train Loss 161009.19\n",
      "670 Train Loss 160910.1\n",
      "671 Train Loss 160822.42\n",
      "672 Train Loss 160853.97\n",
      "673 Train Loss 160767.61\n",
      "674 Train Loss 160725.19\n",
      "675 Train Loss 160702.14\n",
      "676 Train Loss 160694.02\n",
      "677 Train Loss 160684.98\n",
      "678 Train Loss 160679.02\n",
      "679 Train Loss 160672.62\n",
      "680 Train Loss 160669.34\n",
      "681 Train Loss 160664.58\n",
      "682 Train Loss 160658.86\n",
      "683 Train Loss 160652.05\n",
      "684 Train Loss 160634.55\n",
      "685 Train Loss 160631.22\n",
      "686 Train Loss 160609.03\n",
      "687 Train Loss 160597.31\n",
      "688 Train Loss 160579.1\n",
      "689 Train Loss 160548.84\n",
      "690 Train Loss 160515.55\n",
      "691 Train Loss 160460.36\n",
      "692 Train Loss 160369.78\n",
      "693 Train Loss 160264.75\n",
      "694 Train Loss 160329.11\n",
      "695 Train Loss 160203.3\n",
      "696 Train Loss 160250.61\n",
      "697 Train Loss 160177.31\n",
      "698 Train Loss 160133.98\n",
      "699 Train Loss 160126.08\n",
      "700 Train Loss 160107.27\n",
      "701 Train Loss 160093.88\n",
      "702 Train Loss 160079.67\n",
      "703 Train Loss 160038.16\n",
      "704 Train Loss 160014.31\n",
      "705 Train Loss 159988.67\n",
      "706 Train Loss 159968.72\n",
      "707 Train Loss 159943.03\n",
      "708 Train Loss 159913.08\n",
      "709 Train Loss 159873.25\n",
      "710 Train Loss 160035.9\n",
      "711 Train Loss 159849.36\n",
      "712 Train Loss 159811.22\n",
      "713 Train Loss 159776.02\n",
      "714 Train Loss 159732.95\n",
      "715 Train Loss 159674.25\n",
      "716 Train Loss 159638.62\n",
      "717 Train Loss 159610.89\n",
      "718 Train Loss 159587.03\n",
      "719 Train Loss 159544.81\n",
      "720 Train Loss 159514.53\n",
      "721 Train Loss 159491.08\n",
      "722 Train Loss 159477.4\n",
      "723 Train Loss 159468.52\n",
      "724 Train Loss 159456.31\n",
      "725 Train Loss 159430.62\n",
      "726 Train Loss 159396.11\n",
      "727 Train Loss 159353.12\n",
      "728 Train Loss 159309.08\n",
      "729 Train Loss 159268.12\n",
      "730 Train Loss 159245.2\n",
      "731 Train Loss 159227.81\n",
      "732 Train Loss 159213.95\n",
      "733 Train Loss 159204.77\n",
      "734 Train Loss 159190.16\n",
      "735 Train Loss 159168.47\n",
      "736 Train Loss 159152.39\n",
      "737 Train Loss 159142.92\n",
      "738 Train Loss 159128.33\n",
      "739 Train Loss 159121.17\n",
      "740 Train Loss 159112.5\n",
      "741 Train Loss 159103.45\n",
      "742 Train Loss 159093.39\n",
      "743 Train Loss 159080.36\n",
      "744 Train Loss 159060.38\n",
      "745 Train Loss 159028.56\n",
      "746 Train Loss 158977.89\n",
      "747 Train Loss 158903.98\n",
      "748 Train Loss 158801.19\n",
      "749 Train Loss 159135.69\n",
      "750 Train Loss 158700.1\n",
      "751 Train Loss 158655.94\n",
      "752 Train Loss 158596.34\n",
      "753 Train Loss 158567.72\n",
      "754 Train Loss 158531.84\n",
      "755 Train Loss 158485.08\n",
      "756 Train Loss 158457.69\n",
      "757 Train Loss 158429.73\n",
      "758 Train Loss 158406.36\n",
      "759 Train Loss 158394.55\n",
      "760 Train Loss 158385.28\n",
      "761 Train Loss 158377.45\n",
      "762 Train Loss 158367.61\n",
      "763 Train Loss 158357.8\n",
      "764 Train Loss 158349.38\n",
      "765 Train Loss 158335.42\n",
      "766 Train Loss 158315.23\n",
      "767 Train Loss 158281.4\n",
      "768 Train Loss 158240.48\n",
      "769 Train Loss 158200.55\n",
      "770 Train Loss 158171.95\n",
      "771 Train Loss 158154.1\n",
      "772 Train Loss 158139.8\n",
      "773 Train Loss 158112.6\n",
      "774 Train Loss 158097.17\n",
      "775 Train Loss 158072.06\n",
      "776 Train Loss 158046.64\n",
      "777 Train Loss 158014.7\n",
      "778 Train Loss 157967.64\n",
      "779 Train Loss 157908.3\n",
      "780 Train Loss 157848.67\n",
      "781 Train Loss 157801.97\n",
      "782 Train Loss 157764.81\n",
      "783 Train Loss 157709.12\n",
      "784 Train Loss 157671.36\n",
      "785 Train Loss 157627.9\n",
      "786 Train Loss 157604.5\n",
      "787 Train Loss 157589.38\n",
      "788 Train Loss 157576.17\n",
      "789 Train Loss 157558.19\n",
      "790 Train Loss 157546.12\n",
      "791 Train Loss 157534.39\n",
      "792 Train Loss 157528.7\n",
      "793 Train Loss 157521.86\n",
      "794 Train Loss 157517.48\n",
      "795 Train Loss 157512.69\n",
      "796 Train Loss 157507.39\n",
      "797 Train Loss 157499.84\n",
      "798 Train Loss 157491.84\n",
      "799 Train Loss 157485.23\n",
      "800 Train Loss 157479.22\n",
      "801 Train Loss 157473.73\n",
      "802 Train Loss 157467.22\n",
      "803 Train Loss 157456.31\n",
      "804 Train Loss 157435.81\n",
      "805 Train Loss 157443.69\n",
      "806 Train Loss 157413.11\n",
      "807 Train Loss 157367.0\n",
      "808 Train Loss 157271.16\n",
      "809 Train Loss 157186.39\n",
      "810 Train Loss 157140.38\n",
      "811 Train Loss 157094.1\n",
      "812 Train Loss 157267.52\n",
      "813 Train Loss 157059.44\n",
      "814 Train Loss 157101.47\n",
      "815 Train Loss 157026.98\n",
      "816 Train Loss 157013.73\n",
      "817 Train Loss 156977.61\n",
      "818 Train Loss 156975.36\n",
      "819 Train Loss 156952.94\n",
      "820 Train Loss 156919.86\n",
      "821 Train Loss 156908.28\n",
      "822 Train Loss 156893.0\n",
      "823 Train Loss 156891.66\n",
      "824 Train Loss 156864.8\n",
      "825 Train Loss 156835.39\n",
      "826 Train Loss 156801.12\n",
      "827 Train Loss 156757.19\n",
      "828 Train Loss 156923.05\n",
      "829 Train Loss 156730.36\n",
      "830 Train Loss 156741.69\n",
      "831 Train Loss 156705.1\n",
      "832 Train Loss 156684.0\n",
      "833 Train Loss 156666.08\n",
      "834 Train Loss 156637.6\n",
      "835 Train Loss 156617.61\n",
      "836 Train Loss 156601.97\n",
      "837 Train Loss 156578.67\n",
      "838 Train Loss 156557.34\n",
      "839 Train Loss 156547.81\n",
      "840 Train Loss 156540.19\n",
      "841 Train Loss 156528.81\n",
      "842 Train Loss 156511.17\n",
      "843 Train Loss 156475.7\n",
      "844 Train Loss 156440.4\n",
      "845 Train Loss 156419.62\n",
      "846 Train Loss 156385.88\n",
      "847 Train Loss 156373.23\n",
      "848 Train Loss 156356.8\n",
      "849 Train Loss 156339.53\n",
      "850 Train Loss 156313.86\n",
      "851 Train Loss 156287.47\n",
      "852 Train Loss 156271.81\n",
      "853 Train Loss 156261.75\n",
      "854 Train Loss 156254.14\n",
      "855 Train Loss 156243.78\n",
      "856 Train Loss 156230.83\n",
      "857 Train Loss 156217.61\n",
      "858 Train Loss 156204.1\n",
      "859 Train Loss 156191.1\n",
      "860 Train Loss 156176.02\n",
      "861 Train Loss 156146.5\n",
      "862 Train Loss 156107.58\n",
      "863 Train Loss 156056.47\n",
      "864 Train Loss 156033.75\n",
      "865 Train Loss 155931.89\n",
      "866 Train Loss 155931.55\n",
      "867 Train Loss 155854.84\n",
      "868 Train Loss 155817.89\n",
      "869 Train Loss 155765.5\n",
      "870 Train Loss 155751.62\n",
      "871 Train Loss 155655.27\n",
      "872 Train Loss 155620.03\n",
      "873 Train Loss 155584.62\n",
      "874 Train Loss 155559.33\n",
      "875 Train Loss 155508.02\n",
      "876 Train Loss 155474.78\n",
      "877 Train Loss 155425.23\n",
      "878 Train Loss 155397.95\n",
      "879 Train Loss 155377.36\n",
      "880 Train Loss 155372.27\n",
      "881 Train Loss 155354.38\n",
      "882 Train Loss 155344.03\n",
      "883 Train Loss 155331.84\n",
      "884 Train Loss 155324.7\n",
      "885 Train Loss 155316.98\n",
      "886 Train Loss 155308.12\n",
      "887 Train Loss 155297.44\n",
      "888 Train Loss 155270.23\n",
      "889 Train Loss 155254.16\n",
      "890 Train Loss 155236.47\n",
      "891 Train Loss 155222.25\n",
      "892 Train Loss 155211.81\n",
      "893 Train Loss 155201.73\n",
      "894 Train Loss 155191.33\n",
      "895 Train Loss 155183.84\n",
      "896 Train Loss 155177.52\n",
      "897 Train Loss 155168.42\n",
      "898 Train Loss 155160.16\n",
      "899 Train Loss 155150.31\n",
      "900 Train Loss 155135.7\n",
      "901 Train Loss 155114.48\n",
      "902 Train Loss 155095.6\n",
      "903 Train Loss 155076.08\n",
      "904 Train Loss 155059.1\n",
      "905 Train Loss 155037.44\n",
      "906 Train Loss 155015.55\n",
      "907 Train Loss 155000.22\n",
      "908 Train Loss 154985.55\n",
      "909 Train Loss 154974.12\n",
      "910 Train Loss 154960.5\n",
      "911 Train Loss 154937.47\n",
      "912 Train Loss 154909.61\n",
      "913 Train Loss 154883.89\n",
      "914 Train Loss 154865.44\n",
      "915 Train Loss 154850.08\n",
      "916 Train Loss 154839.98\n",
      "917 Train Loss 154830.86\n",
      "918 Train Loss 154819.14\n",
      "919 Train Loss 154808.47\n",
      "920 Train Loss 154796.14\n",
      "921 Train Loss 154778.86\n",
      "922 Train Loss 154764.31\n",
      "923 Train Loss 154745.27\n",
      "924 Train Loss 154725.4\n",
      "925 Train Loss 154700.27\n",
      "926 Train Loss 154675.97\n",
      "927 Train Loss 154654.0\n",
      "928 Train Loss 154624.6\n",
      "929 Train Loss 154612.58\n",
      "930 Train Loss 154575.92\n",
      "931 Train Loss 154558.03\n",
      "932 Train Loss 154535.64\n",
      "933 Train Loss 154516.38\n",
      "934 Train Loss 154503.34\n",
      "935 Train Loss 154493.17\n",
      "936 Train Loss 154483.95\n",
      "937 Train Loss 154474.89\n",
      "938 Train Loss 154465.0\n",
      "939 Train Loss 154454.95\n",
      "940 Train Loss 154441.56\n",
      "941 Train Loss 154424.38\n",
      "942 Train Loss 154395.98\n",
      "943 Train Loss 154364.94\n",
      "944 Train Loss 154352.67\n",
      "945 Train Loss 154337.62\n",
      "946 Train Loss 154312.67\n",
      "947 Train Loss 154287.78\n",
      "948 Train Loss 154268.38\n",
      "949 Train Loss 154248.9\n",
      "950 Train Loss 154236.05\n",
      "951 Train Loss 154223.52\n",
      "952 Train Loss 154216.38\n",
      "953 Train Loss 154207.33\n",
      "954 Train Loss 154198.42\n",
      "955 Train Loss 154185.3\n",
      "956 Train Loss 154171.45\n",
      "957 Train Loss 154149.94\n",
      "958 Train Loss 154126.8\n",
      "959 Train Loss 154127.52\n",
      "960 Train Loss 154115.86\n",
      "961 Train Loss 154156.58\n",
      "962 Train Loss 154108.22\n",
      "963 Train Loss 154092.22\n",
      "964 Train Loss 154072.69\n"
     ]
    }
   ],
   "source": [
    "max_reps = 1\n",
    "\n",
    "train_loss_full = []\n",
    "test_loss_full = []\n",
    "#alpha_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "#for reps in range(max_reps):\n",
    "for reps in range(8,9):\n",
    "    print(reps)\n",
    "    \n",
    "    torch.manual_seed(reps*36)\n",
    "    N_D = 5000 #Total number of data points for 'y'\n",
    "    N_N = 3500\n",
    "    N_f = 10000 #Total number of collocation points \n",
    "    \n",
    "    xyt_coll_np_array, xyt_D_np_array, u_D_np_array,xyt_Nx_np_array,xyt_Ny_np_array = trainingdata(N_D,N_N,N_f,(reps)*22)\n",
    "        \n",
    "    xyt_coll = torch.from_numpy(xyt_coll_np_array).float().to(device)\n",
    "    xyt_D = torch.from_numpy(xyt_D_np_array).float().to(device)\n",
    "    u_D = torch.from_numpy(u_D_np_array).float().to(device)\n",
    "    xyt_Nx = torch.from_numpy(xyt_Nx_np_array).float().to(device)\n",
    "    xyt_Ny = torch.from_numpy(xyt_Ny_np_array).float().to(device)\n",
    "        \n",
    "    N_hat = torch.zeros(xyt_Nx.shape[0],1).to(device)    \n",
    "    f_hat = torch.zeros(xyt_coll.shape[0],1).to(device)\n",
    "\n",
    "    layers = np.array([3,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "    #layers = np.array([3,100,100,100,100,100,100,100,1])\n",
    "\n",
    "    PINN = Sequentialmodel(layers)\n",
    "   \n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "    \n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=1, \n",
    "                              max_iter = 10000, \n",
    "                              max_eval = None, \n",
    "                              tolerance_grad = -1, \n",
    "                              tolerance_change = -1, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "        \n",
    "    optimizer.step(PINN.closure)\n",
    "    \n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(PINN.train_loss)\n",
    "    test_loss_full.append(PINN.test_loss)\n",
    "    elapsed_time[reps] = time.time() - start_time\n",
    "    #alpha_full.append(PINN.alpha_val)\n",
    "\n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "    \n",
    "mdic = {\"train_loss\": train_loss_full,\"test_loss\": test_loss_full,\"Time\": elapsed_time,  \"label\": label}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "del xyt_coll\n",
    "del xyt_D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred = PINN.test(xyt_test_tensor)\n",
    "u_pred_3d = u_pred.reshape(100,100,100,order = 'F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "cmap = plt.cm.jet\n",
    "\n",
    "img3 = ax.imshow(np.flipud(u_pred_3d[:,:,50]),vmax =1000,vmin=450,cmap = cmap,extent=[0,1,0,1],aspect = 0.75)\n",
    "fig.colorbar(img3, orientation='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_video():\n",
    "    for i in range(100):\n",
    "        #plt.imshow(img[i], cmap=cm.Greys_r)\n",
    "        img3 = ax.imshow(np.flipud(u_pred_3d[:,:,i]),vmax =1000,vmin=450,cmap = cmap,extent=[0,1,0,1],aspect = 0.75)\n",
    "        \n",
    "        plt.savefig(\"stan_heat_%02d.png\" % i)\n",
    "\n",
    "    #os.chdir()\n",
    "    subprocess.call(['ffmpeg', '-framerate', '8', '-i', 'stan_heat_%02d.png', '-r', '10', '-pix_fmt', 'yuv420p','Stan_transient.mp4'])\n",
    "    for file_name in glob.glob(\"*.png\"):\n",
    "        os.remove(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generate_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.animation as animation\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "# img = [] # some array of images\n",
    "frames = [] # for storing the generated images\n",
    "# fig = plt.figure()\n",
    "for i in range(100):\n",
    "#     p1 = ax.imshow(np.flipud(u_pred_3d[:,:,i]),vmax =1000,vmin=450,cmap = cmap,extent=[0,1,0,1],aspect = 0.75)\n",
    "#     fig.colorbar(p1, orientation='vertical',ax=ax)\n",
    "    frames.append([plt.imshow(np.flipud(u_pred_3d[:,:,i]),vmax =1000,vmin=450,cmap = cmap,extent=[0,1,0,1],aspect = 0.75)])\n",
    "    print(i)\n",
    "\n",
    "ani = animation.ArtistAnimation(fig, frames, interval=50, blit=True,\n",
    "                                repeat_delay=1000)\n",
    "#ani.save('movie.mp4')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writergif = animation.PillowWriter(fps=10)\n",
    "ani.save('tanh_Thinplate_movie.gif',writer=writergif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "writervideo = animation.FFMpegWriter(fps=60) \n",
    "ani.save('Stan_Thinplate_movie1.gif',writer=writervideo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(u_pred_3d[:,:,50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(u_pred_3d[50,99,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + test_loss_full[i][-1]\n",
    "print(a/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + train_loss_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
