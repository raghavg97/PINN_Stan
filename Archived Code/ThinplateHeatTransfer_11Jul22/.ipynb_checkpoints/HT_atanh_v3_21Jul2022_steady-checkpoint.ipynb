{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:2\n"
     ]
    }
   ],
   "source": [
    "####SUCCESFUL: IMPORTANT\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def true_2D_1(xt): #True function for 2D_1 Klein Gordon Equation x \\in [-50,50] , t \\in [0,10]\n",
    "#     y = xt[:,0]*np.cos(xt[:,1])\n",
    "#     return y.reshape(-1,1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Material Properties This link - https://www.mathworks.com/help/pde/ug/nonlinear-heat-transfer-in-a-thin-plate.html#heatTransferThinPlateExample-1\n",
    "k = 400\n",
    "rho = 8960\n",
    "cp = 386\n",
    "t_z = 0.01\n",
    "stef_bolt = 5.670373e-8\n",
    "hc = 1\n",
    "Ta = 300\n",
    "emiss = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"Navier_stan\"\n",
    "\n",
    "x = np.linspace(0,1,250).reshape(-1,1)\n",
    "y = np.linspace(0,1,250).reshape(-1,1)\n",
    "#t = np.linspace(0,1,100).reshape(-1,1) #t is actually from 0 to 5000, let us scale it to 0 to 1\n",
    "\n",
    "X,Y = np.meshgrid(x,y)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "Y = Y.flatten('F').reshape(-1,1)\n",
    " \n",
    "xy = np.hstack((X,Y))\n",
    "\n",
    "#initial_pts = np.logical_and(T==0,Y!=0).reshape(-1,)\n",
    "\n",
    "DBC_pts = (Y == 0).reshape(-1,)\n",
    "\n",
    "\n",
    "NBC_pts_x0 = (X == 0).reshape(-1,)\n",
    "NBC_pts_x1 = (X == 1).reshape(-1,)\n",
    "\n",
    "NBC_pts_y0 = (Y == 0).reshape(-1,)\n",
    "NBC_pts_y1 = (Y == 1).reshape(-1,)\n",
    "\n",
    "#xy_initial = xyt[initial_pts,:]\n",
    "xy_DBC = xy[DBC_pts,:]\n",
    "\n",
    "xy_NBC_x0 = xy[NBC_pts_x0,:]\n",
    "xy_NBC_x1 = xy[NBC_pts_x1,:]\n",
    "\n",
    "#xyt_NBC_y0 = xyt[NBC_pts_y0,:]\n",
    "xy_NBC_y1 = xy[NBC_pts_y1,:]\n",
    "\n",
    "#u_initial = np.zeros((np.shape(xy_initial)[0],1))\n",
    "u_DBC = 1000*np.ones((np.shape(xy_DBC)[0],1))\n",
    "\n",
    "#xy_I_DBC = np.vstack((xy_initial,xyt_DBC))\n",
    "#xyt_NBC = np.vstack((xyt_NBC_1,xyt_NBC_2,xyt_NBC_3,xyt_NBC_4))\n",
    "xy_NBC_x = np.vstack((xy_NBC_x0,xy_NBC_x1))\n",
    "#xyt_NBC_y = np.vstack((xyt_NBC_y0,xyt_NBC_y1))\n",
    "xy_NBC_y = np.vstack((xy_NBC_y1))\n",
    "\n",
    "#u_I_DBC = np.vstack((u_initial,u_DBC))\n",
    "\n",
    "xy_test_tensor = torch.from_numpy(xy).float().to(device)\n",
    "\n",
    "lb_xy = xy[0]\n",
    "ub_xy = xy[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_D,N_N,N_f,seed):\n",
    "    '''Boundary Conditions''' \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    #choose random N_u points for training\n",
    "    idx = np.random.choice(xy_DBC.shape[0], N_D, replace=False) \n",
    "    xy_D = xy_DBC[idx,:] #choose indices from  set 'idx' (x,t)\n",
    "    u_D = u_DBC[idx].reshape(-1,1)      #choose corresponding u\n",
    "\n",
    "    idx = np.random.choice(xy_NBC_x.shape[0], N_D, replace=False) \n",
    "    xy_Nx = xy_NBC_x[idx,:] #choose indices from  set 'idx' (x,t)\n",
    "\n",
    "    idx = np.random.choice(xy_NBC_y.shape[0], N_D, replace=False) \n",
    "    xy_Ny = xy_NBC_y[idx,:] #choose indices from  set 'idx' (x,t)\n",
    "\n",
    "    '''Collocation Points'''\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    samples = sampling(N_f)\n",
    "    \n",
    "    xy_coll = lb_xy + (ub_xy - lb_xy)*samples\n",
    "    xy_coll = np.vstack((xy_coll, xy_D,xy_Nx,xy_Ny)) # append training points to collocation points \n",
    "\n",
    "    return xy_coll, xy_D, u_D, xy_Nx,xy_Ny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        self.iter = 0\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)   \n",
    "        \n",
    "        self.beta = Parameter(torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "        \n",
    "        self.train_loss = []\n",
    "        self.test_loss = []\n",
    "        \n",
    "        self.beta_val = []\n",
    "        \n",
    "        self.n = torch.tensor(1.0)\n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xy):\n",
    "        if torch.is_tensor(xy) != True:         \n",
    "            xy = torch.from_numpy(xy)                \n",
    "        \n",
    "        ubxy = torch.from_numpy(ub_xy).float().to(device)\n",
    "        lbxy = torch.from_numpy(lb_xy).float().to(device)\n",
    "\n",
    "                      \n",
    "        #preprocessing input \n",
    "        xy = (xy - lbxy)/(ubxy - lbxy)\n",
    "        \n",
    "        #convert to float\n",
    "        a = xy.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            z1 = self.activation(z) \n",
    "            a = z1 + self.beta[:,i]*z*z1\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_D(self,xy_D,u_D):\n",
    "                \n",
    "        loss_bc = self.loss_function(self.forward(xy_D), u_D)\n",
    "                \n",
    "        return loss_bc\n",
    "    \n",
    "    def loss_N(self,xy_Nx,xy_Ny,N_hat):\n",
    "        \n",
    "        g1 = xy_Nx.clone()             \n",
    "        g1.requires_grad = True\n",
    "        u1 = self.forward(g1)\n",
    "        \n",
    "        u1_x_y = autograd.grad(u1,g1,torch.ones([xy_Nx.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        du1_dx = u1_x_y[:,[0]]\n",
    "        \n",
    "        g2 = xy_Ny.clone()             \n",
    "        g2.requires_grad = True\n",
    "        u2 = self.forward(g2)\n",
    "        \n",
    "        u2_x_y = autograd.grad(u2,g2,torch.ones([xy_Ny.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        du2_dy = u2_x_y[:,[1]]\n",
    "               \n",
    "        loss_N1 = self.loss_function(du1_dx,N_hat)\n",
    "        loss_N2 = self.loss_function(du2_dy,N_hat)\n",
    "        \n",
    "        #return loss_N1+loss_N2       \n",
    "        return loss_N1 + loss_N2\n",
    "    \n",
    "    def loss_PDE(self, xy_coll, f_hat):\n",
    "        \n",
    "        g = xy_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_y = autograd.grad(u,g,torch.ones([xy_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_xx_yy = autograd.grad(u_x_y,g,torch.ones(xy_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        #du_dt = u_x_y_t[:,[2]]\n",
    "        \n",
    "        d2u_dx2 = u_xx_yy[:,[0]]\n",
    "        d2u_dy2 = u_xx_yy[:,[1]]    \n",
    "        \n",
    "\n",
    "        f = - k*t_z*(d2u_dx2+d2u_dy2) + 2*hc*(u-Ta) + 2*emiss*stef_bolt*(torch.pow(u,4)-Ta**4) \n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xy_D,u_D,xy_Nx,xy_Ny,N_hat,xy_coll,f_hat):\n",
    "\n",
    "        loss_D = self.loss_D(xy_D,u_D)\n",
    "        loss_N = self.loss_N(xy_Nx,xy_Ny,N_hat)\n",
    "        loss_f = self.loss_PDE(xy_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_D + loss_N + loss_f\n",
    "        \n",
    "        #print(self.iter,\"loss_D:\",loss_D.cpu().detach().numpy(),\"loss_N:\",loss_N.cpu().detach().numpy(),\"loss_f:\",loss_f.cpu().detach().numpy())\n",
    "        \n",
    "        return loss_val\n",
    "     \n",
    "    'callable for optimizer'                                       \n",
    "    def closure(self):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = self.loss(xy_D,u_D,xy_Nx,xy_Ny,N_hat,xy_coll,f_hat)\n",
    "        self.train_loss.append(loss.cpu().detach().numpy())\n",
    "        \n",
    "        u_pred = self.test(xy_test_tensor)\n",
    "        #self.test_loss.append(np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))) #Commented because no true values yet\n",
    "        self.beta_val.append(self.beta.cpu().detach().numpy())\n",
    "        \n",
    "        #print(self.iter,\"Train Loss\",self.train_loss[-1],\"Test Loss\",self.test_loss[-1])\n",
    "        print(self.iter,\"Train Loss\",self.train_loss[-1])\n",
    "        \n",
    "        loss.backward()\n",
    "        self.iter += 1\n",
    "  \n",
    "\n",
    "        return loss        \n",
    "    'test neural network'\n",
    "    def test(self,xy_test_tensor):\n",
    "        u_pred = self.forward(xy_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0 Train Loss 2134119.0\n",
      "1 Train Loss 2130068.5\n",
      "2 Train Loss 2105508.2\n",
      "3 Train Loss 2058562.6\n",
      "4 Train Loss 2043182.8\n",
      "5 Train Loss 2025923.5\n",
      "6 Train Loss 1986442.9\n",
      "7 Train Loss 1891697.0\n",
      "8 Train Loss 1768869.8\n",
      "9 Train Loss 15808501.0\n",
      "10 Train Loss 1726310.0\n",
      "11 Train Loss 1976480.0\n",
      "12 Train Loss 1660680.5\n",
      "13 Train Loss 1591251.2\n",
      "14 Train Loss 1470817.1\n",
      "15 Train Loss 1362355.8\n",
      "16 Train Loss 1312593.1\n",
      "17 Train Loss 1276756.5\n",
      "18 Train Loss 1231463.1\n",
      "19 Train Loss 1936650.2\n",
      "20 Train Loss 1168696.6\n",
      "21 Train Loss 1150310.6\n",
      "22 Train Loss 1127648.6\n",
      "23 Train Loss 1103158.4\n",
      "24 Train Loss 1080551.4\n",
      "25 Train Loss 1040945.0\n",
      "26 Train Loss 1019205.5\n",
      "27 Train Loss 1013898.44\n",
      "28 Train Loss 1002432.25\n",
      "29 Train Loss 997019.1\n",
      "30 Train Loss 991039.75\n",
      "31 Train Loss 989087.3\n",
      "32 Train Loss 986856.7\n",
      "33 Train Loss 984520.3\n",
      "34 Train Loss 980834.9\n",
      "35 Train Loss 976053.56\n",
      "36 Train Loss 973231.56\n",
      "37 Train Loss 970178.9\n",
      "38 Train Loss 965934.94\n",
      "39 Train Loss 963942.7\n",
      "40 Train Loss 961676.25\n",
      "41 Train Loss 957500.1\n",
      "42 Train Loss 951810.44\n",
      "43 Train Loss 955082.2\n",
      "44 Train Loss 945933.1\n",
      "45 Train Loss 962301.5\n",
      "46 Train Loss 942058.44\n",
      "47 Train Loss 937216.06\n",
      "48 Train Loss 934137.44\n",
      "49 Train Loss 931641.75\n",
      "50 Train Loss 929755.6\n",
      "51 Train Loss 926415.75\n",
      "52 Train Loss 922679.25\n",
      "53 Train Loss 920043.7\n",
      "54 Train Loss 916160.0\n",
      "55 Train Loss 906812.94\n",
      "56 Train Loss 893727.2\n",
      "57 Train Loss 892728.8\n",
      "58 Train Loss 886886.75\n",
      "59 Train Loss 857413.9\n",
      "60 Train Loss 872715.7\n",
      "61 Train Loss 836641.9\n",
      "62 Train Loss 825256.25\n",
      "63 Train Loss 750710.44\n",
      "64 Train Loss 742655.2\n",
      "65 Train Loss 732625.06\n",
      "66 Train Loss 710637.6\n",
      "67 Train Loss 652076.94\n",
      "68 Train Loss 624405.1\n",
      "69 Train Loss 886839.75\n",
      "70 Train Loss 607951.44\n",
      "71 Train Loss 589243.9\n",
      "72 Train Loss 537883.2\n",
      "73 Train Loss 519515.25\n",
      "74 Train Loss 498151.22\n",
      "75 Train Loss 487267.7\n",
      "76 Train Loss 474214.28\n",
      "77 Train Loss 465939.12\n",
      "78 Train Loss 455892.06\n",
      "79 Train Loss 440384.5\n",
      "80 Train Loss 542656.1\n",
      "81 Train Loss 421728.38\n",
      "82 Train Loss 409752.88\n",
      "83 Train Loss 402279.75\n",
      "84 Train Loss 402925.6\n",
      "85 Train Loss 399157.4\n",
      "86 Train Loss 393712.2\n",
      "87 Train Loss 419588.0\n",
      "88 Train Loss 388039.4\n",
      "89 Train Loss 381528.7\n",
      "90 Train Loss 375755.72\n",
      "91 Train Loss 370301.0\n",
      "92 Train Loss 378511.12\n",
      "93 Train Loss 366580.56\n",
      "94 Train Loss 356784.7\n",
      "95 Train Loss 347672.97\n",
      "96 Train Loss 358455.84\n",
      "97 Train Loss 344698.9\n",
      "98 Train Loss 342626.97\n",
      "99 Train Loss 335080.03\n",
      "100 Train Loss 324639.34\n",
      "101 Train Loss 319051.1\n",
      "102 Train Loss 313925.1\n",
      "103 Train Loss 311766.2\n",
      "104 Train Loss 312567.66\n",
      "105 Train Loss 302398.06\n",
      "106 Train Loss 296465.38\n",
      "107 Train Loss 288515.53\n",
      "108 Train Loss 282101.06\n",
      "109 Train Loss 272038.06\n",
      "110 Train Loss 297572.34\n",
      "111 Train Loss 265276.6\n",
      "112 Train Loss 265307.62\n",
      "113 Train Loss 263179.7\n",
      "114 Train Loss 260259.28\n",
      "115 Train Loss 281442.94\n",
      "116 Train Loss 255460.97\n",
      "117 Train Loss 249912.72\n",
      "118 Train Loss 247192.16\n",
      "119 Train Loss 244514.4\n",
      "120 Train Loss 266022.0\n",
      "121 Train Loss 241164.28\n",
      "122 Train Loss 264242.78\n",
      "123 Train Loss 238735.86\n",
      "124 Train Loss 236989.66\n",
      "125 Train Loss 235819.98\n",
      "126 Train Loss 234560.73\n",
      "127 Train Loss 231499.06\n",
      "128 Train Loss 231644.64\n",
      "129 Train Loss 230261.66\n",
      "130 Train Loss 227709.7\n",
      "131 Train Loss 225047.34\n",
      "132 Train Loss 222659.36\n",
      "133 Train Loss 222178.25\n",
      "134 Train Loss 220264.38\n",
      "135 Train Loss 227481.34\n",
      "136 Train Loss 218209.64\n",
      "137 Train Loss 216263.86\n",
      "138 Train Loss 214132.03\n",
      "139 Train Loss 212950.75\n",
      "140 Train Loss 212468.62\n",
      "141 Train Loss 211549.02\n",
      "142 Train Loss 210208.31\n",
      "143 Train Loss 208655.05\n",
      "144 Train Loss 205834.55\n",
      "145 Train Loss 202965.11\n",
      "146 Train Loss 201302.45\n",
      "147 Train Loss 199123.23\n",
      "148 Train Loss 194640.39\n",
      "149 Train Loss 980352.7\n",
      "150 Train Loss 192951.56\n",
      "151 Train Loss 191572.16\n",
      "152 Train Loss 274598.03\n",
      "153 Train Loss 190115.11\n",
      "154 Train Loss 189523.36\n",
      "155 Train Loss 186949.03\n",
      "156 Train Loss 184362.03\n",
      "157 Train Loss 184116.94\n",
      "158 Train Loss 182639.34\n",
      "159 Train Loss 181035.12\n",
      "160 Train Loss 177803.33\n",
      "161 Train Loss 172585.19\n",
      "162 Train Loss 170575.03\n",
      "163 Train Loss 167566.78\n",
      "164 Train Loss 158563.16\n",
      "165 Train Loss 174961.16\n",
      "166 Train Loss 146674.45\n",
      "167 Train Loss 501849.25\n",
      "168 Train Loss 140998.84\n",
      "169 Train Loss 143451.16\n",
      "170 Train Loss 137297.31\n",
      "171 Train Loss 132068.17\n",
      "172 Train Loss 938097.9\n",
      "173 Train Loss 128128.695\n",
      "174 Train Loss 126681.86\n",
      "175 Train Loss 127099.12\n",
      "176 Train Loss 123384.63\n",
      "177 Train Loss 118965.06\n",
      "178 Train Loss 110557.72\n",
      "179 Train Loss 119623.66\n",
      "180 Train Loss 104760.09\n",
      "181 Train Loss 99191.61\n",
      "182 Train Loss 90320.945\n",
      "183 Train Loss 94902.72\n",
      "184 Train Loss 87969.42\n",
      "185 Train Loss 87316.93\n",
      "186 Train Loss 86396.39\n",
      "187 Train Loss 85247.01\n",
      "188 Train Loss 83364.125\n",
      "189 Train Loss 81235.34\n",
      "190 Train Loss 78912.41\n",
      "191 Train Loss 76912.53\n",
      "192 Train Loss 75272.14\n",
      "193 Train Loss 74014.21\n",
      "194 Train Loss 72429.25\n",
      "195 Train Loss 72484.18\n",
      "196 Train Loss 71202.88\n",
      "197 Train Loss 70231.75\n",
      "198 Train Loss 69510.59\n",
      "199 Train Loss 68217.81\n",
      "200 Train Loss 66343.016\n",
      "201 Train Loss 64738.96\n",
      "202 Train Loss 63797.684\n",
      "203 Train Loss 62838.37\n",
      "204 Train Loss 61765.05\n",
      "205 Train Loss 59745.59\n",
      "206 Train Loss 57647.254\n",
      "207 Train Loss 56233.42\n",
      "208 Train Loss 54616.582\n",
      "209 Train Loss 54704.055\n",
      "210 Train Loss 52973.86\n",
      "211 Train Loss 51183.72\n",
      "212 Train Loss 48901.117\n",
      "213 Train Loss 46046.836\n",
      "214 Train Loss 42205.383\n",
      "215 Train Loss 40757.062\n",
      "216 Train Loss 40230.08\n",
      "217 Train Loss 39218.805\n",
      "218 Train Loss 39169.312\n",
      "219 Train Loss 38729.617\n",
      "220 Train Loss 37298.89\n",
      "221 Train Loss 2230846.5\n",
      "222 Train Loss 36777.293\n",
      "223 Train Loss 35874.324\n",
      "224 Train Loss 35830.562\n",
      "225 Train Loss 34345.156\n",
      "226 Train Loss 33416.684\n",
      "227 Train Loss 32771.203\n",
      "228 Train Loss 31929.107\n",
      "229 Train Loss 30389.09\n",
      "230 Train Loss 30060.492\n",
      "231 Train Loss 29525.082\n",
      "232 Train Loss 28472.184\n",
      "233 Train Loss 27739.45\n",
      "234 Train Loss 26388.617\n",
      "235 Train Loss 24867.72\n",
      "236 Train Loss 23924.906\n",
      "237 Train Loss 23908.086\n",
      "238 Train Loss 23369.543\n",
      "239 Train Loss 22884.5\n",
      "240 Train Loss 21955.477\n",
      "241 Train Loss 20475.646\n",
      "242 Train Loss 20002.453\n",
      "243 Train Loss 18929.559\n",
      "244 Train Loss 19305.049\n",
      "245 Train Loss 18381.863\n",
      "246 Train Loss 17877.656\n",
      "247 Train Loss 16826.734\n",
      "248 Train Loss 16000.505\n",
      "249 Train Loss 15308.048\n",
      "250 Train Loss 14992.995\n",
      "251 Train Loss 14892.98\n",
      "252 Train Loss 14745.1\n",
      "253 Train Loss 14317.887\n",
      "254 Train Loss 13349.418\n",
      "255 Train Loss 13488.976\n",
      "256 Train Loss 12970.357\n",
      "257 Train Loss 12753.195\n",
      "258 Train Loss 12515.158\n",
      "259 Train Loss 12411.176\n",
      "260 Train Loss 12303.559\n",
      "261 Train Loss 12042.604\n",
      "262 Train Loss 11604.615\n",
      "263 Train Loss 11375.291\n",
      "264 Train Loss 11181.338\n",
      "265 Train Loss 11075.558\n",
      "266 Train Loss 10990.344\n",
      "267 Train Loss 10895.847\n",
      "268 Train Loss 10685.422\n",
      "269 Train Loss 10151.994\n",
      "270 Train Loss 9431.875\n",
      "271 Train Loss 9242.196\n",
      "272 Train Loss 9023.033\n",
      "273 Train Loss 8630.933\n",
      "274 Train Loss 8550.752\n",
      "275 Train Loss 8446.423\n",
      "276 Train Loss 8157.9346\n",
      "277 Train Loss 7840.6895\n",
      "278 Train Loss 7660.743\n",
      "279 Train Loss 7435.949\n",
      "280 Train Loss 7160.792\n",
      "281 Train Loss 6866.344\n",
      "282 Train Loss 6746.8936\n",
      "283 Train Loss 6843.8755\n",
      "284 Train Loss 6612.5645\n",
      "285 Train Loss 6559.309\n",
      "286 Train Loss 6356.715\n",
      "287 Train Loss 6293.763\n",
      "288 Train Loss 6109.5977\n",
      "289 Train Loss 6035.234\n",
      "290 Train Loss 5902.077\n",
      "291 Train Loss 5728.7744\n",
      "292 Train Loss 5523.2627\n",
      "293 Train Loss 5339.111\n",
      "294 Train Loss 4995.99\n",
      "295 Train Loss 4987.124\n",
      "296 Train Loss 4873.3857\n",
      "297 Train Loss 4871.5317\n",
      "298 Train Loss 4812.8804\n",
      "299 Train Loss 4770.057\n",
      "300 Train Loss 4732.936\n",
      "301 Train Loss 4707.197\n",
      "302 Train Loss 4694.52\n",
      "303 Train Loss 4685.256\n",
      "304 Train Loss 4676.5635\n",
      "305 Train Loss 4665.631\n",
      "306 Train Loss 4642.7866\n",
      "307 Train Loss 4586.8193\n",
      "308 Train Loss 4521.9917\n",
      "309 Train Loss 4430.1685\n",
      "310 Train Loss 4288.2324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311 Train Loss 4214.189\n",
      "312 Train Loss 4135.432\n",
      "313 Train Loss 4085.1638\n",
      "314 Train Loss 4024.063\n",
      "315 Train Loss 3926.8115\n",
      "316 Train Loss 3843.1387\n",
      "317 Train Loss 3757.1738\n",
      "318 Train Loss 3708.0237\n",
      "319 Train Loss 3680.9207\n",
      "320 Train Loss 3656.2686\n",
      "321 Train Loss 3615.21\n",
      "322 Train Loss 3521.1123\n",
      "323 Train Loss 3463.695\n",
      "324 Train Loss 3359.656\n",
      "325 Train Loss 3267.319\n",
      "326 Train Loss 3108.8599\n",
      "327 Train Loss 2975.3037\n",
      "328 Train Loss 3461.1855\n",
      "329 Train Loss 2874.4038\n",
      "330 Train Loss 2767.538\n",
      "331 Train Loss 2626.5034\n",
      "332 Train Loss 2582.9978\n",
      "333 Train Loss 2489.4512\n",
      "334 Train Loss 2431.7969\n",
      "335 Train Loss 2388.106\n",
      "336 Train Loss 2330.2075\n",
      "337 Train Loss 2260.6343\n",
      "338 Train Loss 2137.0513\n",
      "339 Train Loss 2041.3892\n",
      "340 Train Loss 5463.5747\n",
      "341 Train Loss 2010.4614\n",
      "342 Train Loss 1921.4926\n",
      "343 Train Loss 1827.291\n",
      "344 Train Loss 1773.4108\n",
      "345 Train Loss 1732.3405\n",
      "346 Train Loss 1687.7659\n",
      "347 Train Loss 1698.1304\n",
      "348 Train Loss 1655.7991\n",
      "349 Train Loss 1842.7776\n",
      "350 Train Loss 1643.6732\n",
      "351 Train Loss 1608.8799\n",
      "352 Train Loss 1586.4723\n",
      "353 Train Loss 1571.5406\n",
      "354 Train Loss 1561.8501\n",
      "355 Train Loss 1557.1536\n",
      "356 Train Loss 1553.2635\n",
      "357 Train Loss 1547.716\n",
      "358 Train Loss 1540.7052\n",
      "359 Train Loss 1536.6271\n",
      "360 Train Loss 1529.6721\n",
      "361 Train Loss 1529.0652\n",
      "362 Train Loss 1525.0839\n",
      "363 Train Loss 1512.7786\n",
      "364 Train Loss 1504.657\n",
      "365 Train Loss 1499.8933\n",
      "366 Train Loss 1495.5284\n",
      "367 Train Loss 1479.9583\n",
      "368 Train Loss 1445.2972\n",
      "369 Train Loss 1492.9814\n",
      "370 Train Loss 1432.8381\n",
      "371 Train Loss 1394.3298\n",
      "372 Train Loss 1353.5161\n",
      "373 Train Loss 1484.1841\n",
      "374 Train Loss 1333.2637\n",
      "375 Train Loss 1304.3038\n",
      "376 Train Loss 1266.0459\n",
      "377 Train Loss 1212.5881\n",
      "378 Train Loss 1217.1523\n",
      "379 Train Loss 1191.5673\n",
      "380 Train Loss 1193.191\n",
      "381 Train Loss 1164.8142\n",
      "382 Train Loss 1138.2848\n",
      "383 Train Loss 1108.534\n",
      "384 Train Loss 1095.1289\n",
      "385 Train Loss 1085.3744\n",
      "386 Train Loss 1081.4628\n",
      "387 Train Loss 1075.1941\n",
      "388 Train Loss 1066.386\n",
      "389 Train Loss 1055.8657\n",
      "390 Train Loss 1043.5181\n",
      "391 Train Loss 1035.7366\n",
      "392 Train Loss 1033.2977\n",
      "393 Train Loss 1029.6411\n",
      "394 Train Loss 1015.66583\n",
      "395 Train Loss 1000.80347\n",
      "396 Train Loss 984.02\n",
      "397 Train Loss 975.49414\n",
      "398 Train Loss 962.11194\n",
      "399 Train Loss 952.84955\n",
      "400 Train Loss 947.8518\n",
      "401 Train Loss 945.66315\n",
      "402 Train Loss 944.3164\n",
      "403 Train Loss 943.2423\n",
      "404 Train Loss 940.93085\n",
      "405 Train Loss 935.1073\n",
      "406 Train Loss 924.74805\n",
      "407 Train Loss 915.04407\n",
      "408 Train Loss 903.62976\n",
      "409 Train Loss 895.7449\n",
      "410 Train Loss 889.9724\n",
      "411 Train Loss 883.8575\n",
      "412 Train Loss 875.5043\n",
      "413 Train Loss 870.49554\n",
      "414 Train Loss 863.4859\n",
      "415 Train Loss 858.4048\n",
      "416 Train Loss 847.182\n",
      "417 Train Loss 837.88275\n",
      "418 Train Loss 823.75696\n",
      "419 Train Loss 801.0748\n",
      "420 Train Loss 930.62714\n",
      "421 Train Loss 794.7457\n",
      "422 Train Loss 763.68964\n",
      "423 Train Loss 756.14087\n",
      "424 Train Loss 740.27075\n",
      "425 Train Loss 734.8542\n",
      "426 Train Loss 729.03284\n",
      "427 Train Loss 724.0127\n",
      "428 Train Loss 718.56506\n",
      "429 Train Loss 709.47644\n",
      "430 Train Loss 695.6288\n",
      "431 Train Loss 693.13385\n",
      "432 Train Loss 682.24146\n",
      "433 Train Loss 726.7855\n",
      "434 Train Loss 657.22217\n",
      "435 Train Loss 638.0703\n",
      "436 Train Loss 624.73663\n",
      "437 Train Loss 611.7317\n",
      "438 Train Loss 615.5473\n",
      "439 Train Loss 601.31445\n",
      "440 Train Loss 582.2159\n",
      "441 Train Loss 566.7743\n",
      "442 Train Loss 552.68054\n",
      "443 Train Loss 533.49713\n",
      "444 Train Loss 522.20264\n",
      "445 Train Loss 513.9433\n",
      "446 Train Loss 511.86346\n",
      "447 Train Loss 508.948\n",
      "448 Train Loss 507.63782\n",
      "449 Train Loss 504.98642\n",
      "450 Train Loss 502.13992\n",
      "451 Train Loss 498.28726\n",
      "452 Train Loss 495.32574\n",
      "453 Train Loss 493.78326\n",
      "454 Train Loss 493.08615\n",
      "455 Train Loss 492.10938\n",
      "456 Train Loss 490.9536\n",
      "457 Train Loss 489.01184\n",
      "458 Train Loss 486.6198\n",
      "459 Train Loss 483.78473\n",
      "460 Train Loss 480.6836\n",
      "461 Train Loss 482.6102\n",
      "462 Train Loss 479.15045\n",
      "463 Train Loss 476.61844\n",
      "464 Train Loss 472.59436\n",
      "465 Train Loss 466.99945\n",
      "466 Train Loss 460.47083\n",
      "467 Train Loss 449.42456\n",
      "468 Train Loss 439.7215\n",
      "469 Train Loss 439.15332\n",
      "470 Train Loss 433.71783\n",
      "471 Train Loss 427.93402\n",
      "472 Train Loss 419.9408\n",
      "473 Train Loss 404.7564\n",
      "474 Train Loss 388.9276\n",
      "475 Train Loss 526.0877\n",
      "476 Train Loss 383.90115\n",
      "477 Train Loss 374.81097\n",
      "478 Train Loss 366.7281\n",
      "479 Train Loss 362.77087\n",
      "480 Train Loss 359.59308\n",
      "481 Train Loss 356.02734\n",
      "482 Train Loss 356.66412\n",
      "483 Train Loss 354.96985\n",
      "484 Train Loss 353.1568\n",
      "485 Train Loss 352.14236\n",
      "486 Train Loss 351.3135\n",
      "487 Train Loss 350.52197\n",
      "488 Train Loss 349.30908\n",
      "489 Train Loss 347.87994\n",
      "490 Train Loss 346.14575\n",
      "491 Train Loss 346.7309\n",
      "492 Train Loss 345.48547\n",
      "493 Train Loss 344.09338\n",
      "494 Train Loss 341.958\n",
      "495 Train Loss 341.1186\n",
      "496 Train Loss 340.84235\n",
      "497 Train Loss 340.53674\n",
      "498 Train Loss 340.21576\n",
      "499 Train Loss 339.31647\n",
      "500 Train Loss 342.4826\n",
      "501 Train Loss 339.07117\n",
      "502 Train Loss 337.90173\n",
      "503 Train Loss 335.61716\n",
      "504 Train Loss 328.62747\n",
      "505 Train Loss 320.17493\n",
      "506 Train Loss 312.08124\n",
      "507 Train Loss 308.60184\n",
      "508 Train Loss 302.45306\n",
      "509 Train Loss 290.6905\n",
      "510 Train Loss 285.9734\n",
      "511 Train Loss 282.21588\n",
      "512 Train Loss 279.45648\n",
      "513 Train Loss 276.08224\n",
      "514 Train Loss 271.30542\n",
      "515 Train Loss 271.66016\n",
      "516 Train Loss 269.9803\n",
      "517 Train Loss 268.02097\n",
      "518 Train Loss 265.64056\n",
      "519 Train Loss 267.8205\n",
      "520 Train Loss 264.814\n",
      "521 Train Loss 262.35028\n",
      "522 Train Loss 258.98975\n",
      "523 Train Loss 257.34848\n",
      "524 Train Loss 253.65092\n",
      "525 Train Loss 259.6659\n",
      "526 Train Loss 252.87161\n",
      "527 Train Loss 253.93478\n",
      "528 Train Loss 251.51077\n",
      "529 Train Loss 249.24136\n",
      "530 Train Loss 247.40268\n",
      "531 Train Loss 245.85399\n",
      "532 Train Loss 243.6589\n",
      "533 Train Loss 237.9321\n",
      "534 Train Loss 234.66306\n",
      "535 Train Loss 236.47379\n",
      "536 Train Loss 230.74316\n",
      "537 Train Loss 320.56168\n",
      "538 Train Loss 226.55318\n",
      "539 Train Loss 226.60327\n",
      "540 Train Loss 224.42938\n",
      "541 Train Loss 220.60837\n",
      "542 Train Loss 214.58383\n",
      "543 Train Loss 215.61093\n",
      "544 Train Loss 212.30643\n",
      "545 Train Loss 207.57855\n",
      "546 Train Loss 204.24207\n",
      "547 Train Loss 201.80121\n",
      "548 Train Loss 204.43143\n",
      "549 Train Loss 199.59554\n",
      "550 Train Loss 196.82454\n",
      "551 Train Loss 194.4964\n",
      "552 Train Loss 192.28848\n",
      "553 Train Loss 194.83176\n",
      "554 Train Loss 191.34398\n",
      "555 Train Loss 188.86893\n",
      "556 Train Loss 186.62675\n",
      "557 Train Loss 184.21231\n",
      "558 Train Loss 183.25394\n",
      "559 Train Loss 181.95425\n",
      "560 Train Loss 181.14679\n",
      "561 Train Loss 180.91846\n",
      "562 Train Loss 180.681\n",
      "563 Train Loss 180.39062\n",
      "564 Train Loss 180.04079\n",
      "565 Train Loss 179.56018\n",
      "566 Train Loss 179.44235\n",
      "567 Train Loss 179.11836\n",
      "568 Train Loss 178.90747\n",
      "569 Train Loss 178.38126\n",
      "570 Train Loss 178.05066\n",
      "571 Train Loss 177.3758\n",
      "572 Train Loss 176.94579\n",
      "573 Train Loss 176.8663\n",
      "574 Train Loss 176.74625\n",
      "575 Train Loss 176.43134\n",
      "576 Train Loss 176.18112\n",
      "577 Train Loss 175.94905\n",
      "578 Train Loss 175.6713\n",
      "579 Train Loss 175.41814\n",
      "580 Train Loss 176.63673\n",
      "581 Train Loss 175.20023\n",
      "582 Train Loss 174.97104\n",
      "583 Train Loss 174.33835\n",
      "584 Train Loss 173.63196\n",
      "585 Train Loss 178.09247\n",
      "586 Train Loss 172.94287\n",
      "587 Train Loss 171.45633\n",
      "588 Train Loss 168.10384\n",
      "589 Train Loss 166.52048\n",
      "590 Train Loss 165.63416\n",
      "591 Train Loss 164.81267\n",
      "592 Train Loss 164.18839\n",
      "593 Train Loss 163.59544\n",
      "594 Train Loss 163.26819\n",
      "595 Train Loss 163.05109\n",
      "596 Train Loss 162.58194\n",
      "597 Train Loss 162.37383\n",
      "598 Train Loss 162.11302\n",
      "599 Train Loss 161.51202\n",
      "600 Train Loss 160.39232\n",
      "601 Train Loss 158.88605\n",
      "602 Train Loss 157.3843\n",
      "603 Train Loss 155.32733\n",
      "604 Train Loss 163.77591\n",
      "605 Train Loss 154.61493\n",
      "606 Train Loss 152.78696\n",
      "607 Train Loss 151.53375\n",
      "608 Train Loss 149.976\n",
      "609 Train Loss 149.8642\n",
      "610 Train Loss 149.34674\n",
      "611 Train Loss 148.39673\n",
      "612 Train Loss 147.55232\n",
      "613 Train Loss 146.46545\n",
      "614 Train Loss 145.43988\n",
      "615 Train Loss 145.14003\n",
      "616 Train Loss 143.8893\n",
      "617 Train Loss 143.43013\n",
      "618 Train Loss 143.0127\n",
      "619 Train Loss 142.63919\n",
      "620 Train Loss 142.37762\n",
      "621 Train Loss 142.10237\n",
      "622 Train Loss 141.53143\n",
      "623 Train Loss 141.09499\n",
      "624 Train Loss 140.48494\n",
      "625 Train Loss 139.69061\n",
      "626 Train Loss 139.30376\n",
      "627 Train Loss 138.6373\n",
      "628 Train Loss 138.12334\n",
      "629 Train Loss 137.50151\n",
      "630 Train Loss 137.317\n",
      "631 Train Loss 136.96898\n",
      "632 Train Loss 136.75182\n",
      "633 Train Loss 136.63768\n",
      "634 Train Loss 136.33516\n",
      "635 Train Loss 136.11606\n",
      "636 Train Loss 135.71437\n",
      "637 Train Loss 135.44632\n",
      "638 Train Loss 134.99228\n",
      "639 Train Loss 134.63573\n",
      "640 Train Loss 134.34198\n",
      "641 Train Loss 134.06903\n",
      "642 Train Loss 133.76225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "643 Train Loss 133.34811\n",
      "644 Train Loss 133.42908\n",
      "645 Train Loss 133.11078\n",
      "646 Train Loss 132.34157\n",
      "647 Train Loss 131.87164\n",
      "648 Train Loss 131.05551\n",
      "649 Train Loss 130.43288\n",
      "650 Train Loss 129.88309\n",
      "651 Train Loss 129.23358\n",
      "652 Train Loss 128.90076\n",
      "653 Train Loss 128.54521\n",
      "654 Train Loss 128.24002\n",
      "655 Train Loss 127.91709\n",
      "656 Train Loss 127.55344\n",
      "657 Train Loss 127.3648\n",
      "658 Train Loss 126.92574\n",
      "659 Train Loss 126.82991\n",
      "660 Train Loss 126.17215\n",
      "661 Train Loss 126.04303\n",
      "662 Train Loss 125.94836\n",
      "663 Train Loss 125.61515\n",
      "664 Train Loss 125.31784\n",
      "665 Train Loss 125.11578\n",
      "666 Train Loss 124.7723\n",
      "667 Train Loss 124.64528\n",
      "668 Train Loss 124.53885\n",
      "669 Train Loss 124.38881\n",
      "670 Train Loss 124.32803\n",
      "671 Train Loss 124.285614\n",
      "672 Train Loss 124.233734\n",
      "673 Train Loss 124.19586\n",
      "674 Train Loss 124.15355\n",
      "675 Train Loss 124.1013\n",
      "676 Train Loss 124.05466\n",
      "677 Train Loss 123.95363\n",
      "678 Train Loss 123.8512\n",
      "679 Train Loss 123.70108\n",
      "680 Train Loss 123.542435\n",
      "681 Train Loss 123.41837\n",
      "682 Train Loss 123.32797\n",
      "683 Train Loss 123.3279\n",
      "684 Train Loss 123.26961\n",
      "685 Train Loss 123.21946\n",
      "686 Train Loss 123.115654\n",
      "687 Train Loss 123.07753\n",
      "688 Train Loss 122.949524\n",
      "689 Train Loss 122.94658\n",
      "690 Train Loss 122.83895\n",
      "691 Train Loss 122.49261\n",
      "692 Train Loss 122.282486\n",
      "693 Train Loss 121.881165\n",
      "694 Train Loss 121.59245\n",
      "695 Train Loss 121.33839\n",
      "696 Train Loss 121.860245\n",
      "697 Train Loss 121.1004\n",
      "698 Train Loss 120.97237\n",
      "699 Train Loss 120.44681\n",
      "700 Train Loss 120.140076\n",
      "701 Train Loss 129.00482\n",
      "702 Train Loss 119.9431\n",
      "703 Train Loss 119.525375\n",
      "704 Train Loss 119.18346\n",
      "705 Train Loss 119.00875\n",
      "706 Train Loss 119.15349\n",
      "707 Train Loss 118.77866\n",
      "708 Train Loss 118.5428\n",
      "709 Train Loss 118.36211\n",
      "710 Train Loss 118.055725\n",
      "711 Train Loss 117.6814\n",
      "712 Train Loss 120.95285\n",
      "713 Train Loss 117.43884\n",
      "714 Train Loss 116.87962\n",
      "715 Train Loss 115.77182\n",
      "716 Train Loss 116.137436\n",
      "717 Train Loss 115.45282\n",
      "718 Train Loss 114.87251\n",
      "719 Train Loss 114.51907\n",
      "720 Train Loss 113.94437\n",
      "721 Train Loss 113.28464\n",
      "722 Train Loss 112.83301\n",
      "723 Train Loss 112.40342\n",
      "724 Train Loss 111.80917\n",
      "725 Train Loss 111.40311\n",
      "726 Train Loss 110.712654\n",
      "727 Train Loss 110.014084\n",
      "728 Train Loss 151.24696\n",
      "729 Train Loss 109.78085\n",
      "730 Train Loss 109.646194\n",
      "731 Train Loss 109.09512\n",
      "732 Train Loss 108.752945\n",
      "733 Train Loss 108.347404\n",
      "734 Train Loss 108.11708\n",
      "735 Train Loss 107.76384\n",
      "736 Train Loss 107.35605\n",
      "737 Train Loss 107.161865\n",
      "738 Train Loss 111.63588\n",
      "739 Train Loss 106.93039\n",
      "740 Train Loss 106.87695\n",
      "741 Train Loss 106.679695\n",
      "742 Train Loss 106.58853\n",
      "743 Train Loss 106.43126\n",
      "744 Train Loss 106.29702\n",
      "745 Train Loss 106.15349\n",
      "746 Train Loss 106.05499\n",
      "747 Train Loss 107.483444\n",
      "748 Train Loss 105.922455\n",
      "749 Train Loss 105.83058\n",
      "750 Train Loss 105.714005\n",
      "751 Train Loss 105.63497\n",
      "752 Train Loss 105.62392\n",
      "753 Train Loss 105.59552\n",
      "754 Train Loss 105.51965\n",
      "755 Train Loss 105.432884\n",
      "756 Train Loss 105.30976\n",
      "757 Train Loss 105.28238\n",
      "758 Train Loss 105.20925\n",
      "759 Train Loss 105.15081\n",
      "760 Train Loss 105.034836\n",
      "761 Train Loss 104.964615\n",
      "762 Train Loss 104.82895\n",
      "763 Train Loss 104.72266\n",
      "764 Train Loss 104.645546\n",
      "765 Train Loss 104.61512\n",
      "766 Train Loss 104.58206\n",
      "767 Train Loss 104.52882\n",
      "768 Train Loss 104.40296\n",
      "769 Train Loss 104.327835\n",
      "770 Train Loss 104.12396\n",
      "771 Train Loss 103.95181\n",
      "772 Train Loss 103.64554\n",
      "773 Train Loss 103.332855\n",
      "774 Train Loss 102.77475\n",
      "775 Train Loss 104.07855\n",
      "776 Train Loss 102.3232\n",
      "777 Train Loss 101.32107\n",
      "778 Train Loss 99.92789\n",
      "779 Train Loss 103.39225\n",
      "780 Train Loss 99.7103\n",
      "781 Train Loss 100.23891\n",
      "782 Train Loss 99.062775\n",
      "783 Train Loss 99.75982\n",
      "784 Train Loss 98.737366\n",
      "785 Train Loss 98.30466\n",
      "786 Train Loss 97.80162\n",
      "787 Train Loss 97.297005\n",
      "788 Train Loss 97.18003\n",
      "789 Train Loss 96.90692\n",
      "790 Train Loss 96.406906\n",
      "791 Train Loss 96.17998\n",
      "792 Train Loss 95.70407\n",
      "793 Train Loss 95.50534\n",
      "794 Train Loss 95.09186\n",
      "795 Train Loss 94.71003\n",
      "796 Train Loss 93.87778\n",
      "797 Train Loss 93.339264\n",
      "798 Train Loss 92.7155\n",
      "799 Train Loss 92.18994\n",
      "800 Train Loss 91.68286\n",
      "801 Train Loss 91.83862\n",
      "802 Train Loss 91.21153\n",
      "803 Train Loss 91.06639\n",
      "804 Train Loss 90.828896\n",
      "805 Train Loss 90.57628\n",
      "806 Train Loss 90.13637\n",
      "807 Train Loss 89.67964\n",
      "808 Train Loss 89.4157\n",
      "809 Train Loss 89.2135\n",
      "810 Train Loss 92.789536\n",
      "811 Train Loss 88.95668\n",
      "812 Train Loss 89.9247\n",
      "813 Train Loss 88.7251\n",
      "814 Train Loss 88.811615\n",
      "815 Train Loss 88.60394\n",
      "816 Train Loss 88.44167\n",
      "817 Train Loss 88.264755\n",
      "818 Train Loss 88.12784\n",
      "819 Train Loss 87.938965\n",
      "820 Train Loss 87.8537\n",
      "821 Train Loss 87.8\n",
      "822 Train Loss 87.73718\n",
      "823 Train Loss 87.668\n",
      "824 Train Loss 87.40475\n",
      "825 Train Loss 87.30504\n",
      "826 Train Loss 87.0961\n",
      "827 Train Loss 86.94799\n",
      "828 Train Loss 86.85427\n",
      "829 Train Loss 86.75891\n",
      "830 Train Loss 86.70886\n",
      "831 Train Loss 86.67062\n",
      "832 Train Loss 86.64157\n",
      "833 Train Loss 86.604904\n",
      "834 Train Loss 86.56326\n",
      "835 Train Loss 86.449234\n",
      "836 Train Loss 86.322365\n",
      "837 Train Loss 86.15342\n",
      "838 Train Loss 85.89183\n",
      "839 Train Loss 85.638275\n",
      "840 Train Loss 85.3835\n",
      "841 Train Loss 85.2961\n",
      "842 Train Loss 85.20447\n",
      "843 Train Loss 85.132645\n",
      "844 Train Loss 85.00912\n",
      "845 Train Loss 84.75963\n",
      "846 Train Loss 84.45302\n",
      "847 Train Loss 84.67215\n",
      "848 Train Loss 84.12463\n",
      "849 Train Loss 83.79205\n",
      "850 Train Loss 83.54248\n",
      "851 Train Loss 83.830826\n",
      "852 Train Loss 83.27156\n",
      "853 Train Loss 82.675735\n",
      "854 Train Loss 81.91234\n",
      "855 Train Loss 82.748535\n",
      "856 Train Loss 81.64975\n",
      "857 Train Loss 81.62096\n",
      "858 Train Loss 81.43811\n",
      "859 Train Loss 81.13133\n",
      "860 Train Loss 80.91412\n",
      "861 Train Loss 80.57214\n",
      "862 Train Loss 80.38014\n",
      "863 Train Loss 80.04993\n",
      "864 Train Loss 79.62502\n",
      "865 Train Loss 79.28206\n",
      "866 Train Loss 78.76697\n",
      "867 Train Loss 77.90982\n",
      "868 Train Loss 77.54068\n",
      "869 Train Loss 77.00248\n",
      "870 Train Loss 77.00333\n",
      "871 Train Loss 76.68201\n",
      "872 Train Loss 76.12958\n",
      "873 Train Loss 76.0715\n",
      "874 Train Loss 75.55372\n",
      "875 Train Loss 74.8363\n",
      "876 Train Loss 74.44037\n",
      "877 Train Loss 74.46789\n",
      "878 Train Loss 74.263504\n",
      "879 Train Loss 74.11957\n",
      "880 Train Loss 73.9572\n",
      "881 Train Loss 73.84222\n",
      "882 Train Loss 73.69998\n",
      "883 Train Loss 73.634766\n",
      "884 Train Loss 73.48846\n",
      "885 Train Loss 73.371124\n",
      "886 Train Loss 73.242035\n",
      "887 Train Loss 73.09756\n",
      "888 Train Loss 72.8727\n",
      "889 Train Loss 72.76287\n",
      "890 Train Loss 72.62613\n",
      "891 Train Loss 72.59202\n",
      "892 Train Loss 72.52682\n",
      "893 Train Loss 72.39737\n",
      "894 Train Loss 72.138824\n",
      "895 Train Loss 75.08373\n",
      "896 Train Loss 72.10462\n",
      "897 Train Loss 71.770035\n",
      "898 Train Loss 72.56102\n",
      "899 Train Loss 71.67406\n",
      "900 Train Loss 71.68642\n",
      "901 Train Loss 71.5267\n",
      "902 Train Loss 71.14102\n",
      "903 Train Loss 70.55777\n",
      "904 Train Loss 73.677666\n",
      "905 Train Loss 70.30675\n",
      "906 Train Loss 71.090256\n",
      "907 Train Loss 69.687\n",
      "908 Train Loss 68.72782\n",
      "909 Train Loss 73.17765\n",
      "910 Train Loss 68.28433\n",
      "911 Train Loss 67.358406\n",
      "912 Train Loss 66.42233\n",
      "913 Train Loss 67.26328\n",
      "914 Train Loss 65.981834\n",
      "915 Train Loss 65.70027\n",
      "916 Train Loss 65.21062\n",
      "917 Train Loss 64.76604\n",
      "918 Train Loss 63.96682\n",
      "919 Train Loss 68.94842\n",
      "920 Train Loss 63.59706\n",
      "921 Train Loss 63.009747\n",
      "922 Train Loss 62.450066\n",
      "923 Train Loss 62.022797\n",
      "924 Train Loss 62.212715\n",
      "925 Train Loss 61.47873\n",
      "926 Train Loss 61.070076\n",
      "927 Train Loss 60.52211\n",
      "928 Train Loss 60.305923\n",
      "929 Train Loss 59.958828\n",
      "930 Train Loss 59.780083\n",
      "931 Train Loss 59.73046\n",
      "932 Train Loss 59.579372\n",
      "933 Train Loss 59.405956\n",
      "934 Train Loss 59.14263\n",
      "935 Train Loss 61.848755\n",
      "936 Train Loss 59.05114\n",
      "937 Train Loss 58.856815\n",
      "938 Train Loss 58.6789\n",
      "939 Train Loss 58.48182\n",
      "940 Train Loss 58.65117\n",
      "941 Train Loss 58.3226\n",
      "942 Train Loss 58.072872\n",
      "943 Train Loss 57.954765\n",
      "944 Train Loss 57.81626\n",
      "945 Train Loss 57.57206\n",
      "946 Train Loss 57.59349\n",
      "947 Train Loss 57.422062\n",
      "948 Train Loss 57.1828\n",
      "949 Train Loss 56.9441\n",
      "950 Train Loss 56.84478\n",
      "951 Train Loss 56.62999\n",
      "952 Train Loss 56.524673\n",
      "953 Train Loss 56.73447\n",
      "954 Train Loss 56.410744\n",
      "955 Train Loss 56.219604\n",
      "956 Train Loss 55.927856\n",
      "957 Train Loss 55.92128\n",
      "958 Train Loss 55.799206\n"
     ]
    }
   ],
   "source": [
    "max_reps = 1\n",
    "\n",
    "train_loss_full = []\n",
    "test_loss_full = []\n",
    "beta_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(reps)\n",
    "    \n",
    "    torch.manual_seed(reps*36)\n",
    "    N_D = 100 #Total number of data points for 'y'\n",
    "    N_N = 3500\n",
    "    N_f = 10000 #Total number of collocation points \n",
    "    \n",
    "    xy_coll_np_array, xy_D_np_array, u_D_np_array,xy_Nx_np_array,xy_Ny_np_array = trainingdata(N_D,N_N,N_f,(reps)*22)\n",
    "        \n",
    "    xy_coll = torch.from_numpy(xy_coll_np_array).float().to(device)\n",
    "    xy_D = torch.from_numpy(xy_D_np_array).float().to(device)\n",
    "    u_D = torch.from_numpy(u_D_np_array).float().to(device)\n",
    "    xy_Nx = torch.from_numpy(xy_Nx_np_array).float().to(device)\n",
    "    xy_Ny = torch.from_numpy(xy_Ny_np_array).float().to(device)\n",
    "        \n",
    "    N_hat = torch.zeros(xy_Nx.shape[0],1).to(device)    \n",
    "    f_hat = torch.zeros(xy_coll.shape[0],1).to(device)\n",
    "\n",
    "    layers = np.array([2,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "    #layers = np.array([3,100,100,100,100,100,100,100,1])\n",
    "\n",
    "    PINN = Sequentialmodel(layers)\n",
    "   \n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "    \n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=1, \n",
    "                              max_iter = 10000, \n",
    "                              max_eval = None, \n",
    "                              tolerance_grad = -1, \n",
    "                              tolerance_change = -1, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "        \n",
    "    optimizer.step(PINN.closure)\n",
    "    \n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(PINN.train_loss)\n",
    "    test_loss_full.append(PINN.test_loss)\n",
    "    elapsed_time[reps] = time.time() - start_time\n",
    "    beta_full.append(PINN.beta_val)\n",
    "\n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "    \n",
    "mdic = {\"train_loss\": train_loss_full,\"test_loss\": test_loss_full,\"Time\": elapsed_time, \"beta\": beta_full, \"label\": label}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred = PINN.test(xy_test_tensor)\n",
    "u_pred_3d = u_pred.reshape(250,250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "cmap = plt.cm.jet\n",
    "\n",
    "img3 = ax.imshow(np.transpose(u_pred_3d),vmax =1000,vmin=450,cmap = cmap,extent=[0,1,0,1],aspect = 0.75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = scipy.io.loadmat('Thinplate_steady.mat')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(u_pred_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + test_loss_full[i][-1]\n",
    "print(a/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + train_loss_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
