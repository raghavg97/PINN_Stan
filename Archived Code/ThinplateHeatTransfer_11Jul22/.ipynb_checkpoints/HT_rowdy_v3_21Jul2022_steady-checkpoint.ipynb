{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####SUCCESFUL: IMPORTANT\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def true_2D_1(xt): #True function for 2D_1 Klein Gordon Equation x \\in [-50,50] , t \\in [0,10]\n",
    "#     y = xt[:,0]*np.cos(xt[:,1])\n",
    "#     return y.reshape(-1,1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Material Properties This link - https://www.mathworks.com/help/pde/ug/nonlinear-heat-transfer-in-a-thin-plate.html#heatTransferThinPlateExample-1\n",
    "k = 400\n",
    "rho = 8960\n",
    "cp = 386\n",
    "t_z = 0.01\n",
    "stef_bolt = 5.670373e-8\n",
    "hc = 1\n",
    "Ta = 300\n",
    "emiss = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"Thinplate_rowdy\"\n",
    "\n",
    "x = np.linspace(0,1,250).reshape(-1,1)\n",
    "y = np.linspace(0,1,250).reshape(-1,1)\n",
    "#t = np.linspace(0,1,100).reshape(-1,1) #t is actually from 0 to 5000, let us scale it to 0 to 1\n",
    "\n",
    "X,Y = np.meshgrid(x,y)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "Y = Y.flatten('F').reshape(-1,1)\n",
    " \n",
    "xy = np.hstack((X,Y))\n",
    "\n",
    "#initial_pts = np.logical_and(T==0,Y!=0).reshape(-1,)\n",
    "\n",
    "DBC_pts = (Y == 0).reshape(-1,)\n",
    "\n",
    "\n",
    "NBC_pts_x0 = (X == 0).reshape(-1,)\n",
    "NBC_pts_x1 = (X == 1).reshape(-1,)\n",
    "\n",
    "NBC_pts_y0 = (Y == 0).reshape(-1,)\n",
    "NBC_pts_y1 = (Y == 1).reshape(-1,)\n",
    "\n",
    "#xy_initial = xyt[initial_pts,:]\n",
    "xy_DBC = xy[DBC_pts,:]\n",
    "\n",
    "xy_NBC_x0 = xy[NBC_pts_x0,:]\n",
    "xy_NBC_x1 = xy[NBC_pts_x1,:]\n",
    "\n",
    "#xyt_NBC_y0 = xyt[NBC_pts_y0,:]\n",
    "xy_NBC_y1 = xy[NBC_pts_y1,:]\n",
    "\n",
    "#u_initial = np.zeros((np.shape(xy_initial)[0],1))\n",
    "u_DBC = 1000*np.ones((np.shape(xy_DBC)[0],1))\n",
    "\n",
    "#xy_I_DBC = np.vstack((xy_initial,xyt_DBC))\n",
    "#xyt_NBC = np.vstack((xyt_NBC_1,xyt_NBC_2,xyt_NBC_3,xyt_NBC_4))\n",
    "xy_NBC_x = np.vstack((xy_NBC_x0,xy_NBC_x1))\n",
    "#xyt_NBC_y = np.vstack((xyt_NBC_y0,xyt_NBC_y1))\n",
    "xy_NBC_y = np.vstack((xy_NBC_y1))\n",
    "\n",
    "#u_I_DBC = np.vstack((u_initial,u_DBC))\n",
    "\n",
    "xy_test_tensor = torch.from_numpy(xy).float().to(device)\n",
    "\n",
    "lb_xy = xy[0]\n",
    "ub_xy = xy[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_D,N_N,N_f,seed):\n",
    "    '''Boundary Conditions''' \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    #choose random N_u points for training\n",
    "    idx = np.random.choice(xy_DBC.shape[0], N_D, replace=False) \n",
    "    xy_D = xy_DBC[idx,:] #choose indices from  set 'idx' (x,t)\n",
    "    u_D = u_DBC[idx].reshape(-1,1)      #choose corresponding u\n",
    "\n",
    "    idx = np.random.choice(xy_NBC_x.shape[0], N_D, replace=False) \n",
    "    xy_Nx = xy_NBC_x[idx,:] #choose indices from  set 'idx' (x,t)\n",
    "\n",
    "    idx = np.random.choice(xy_NBC_y.shape[0], N_D, replace=False) \n",
    "    xy_Ny = xy_NBC_y[idx,:] #choose indices from  set 'idx' (x,t)\n",
    "\n",
    "    '''Collocation Points'''\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    samples = sampling(N_f)\n",
    "    \n",
    "    xy_coll = lb_xy + (ub_xy - lb_xy)*samples\n",
    "    xy_coll = np.vstack((xy_coll, xy_D,xy_Nx,xy_Ny)) # append training points to collocation points \n",
    "\n",
    "    return xy_coll, xy_D, u_D, xy_Nx,xy_Ny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        self.iter = 0\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)   \n",
    "        \n",
    "        #self.beta = Parameter(torch.ones((50,len(layers)-2)))\n",
    "        #self.beta.requiresGrad = True\n",
    "        \n",
    "        self.train_loss = []\n",
    "        self.test_loss = []\n",
    "        \n",
    "        self.beta_val = []\n",
    "        \n",
    "        self.n = torch.tensor(1.0)\n",
    "        \n",
    "        self.alpha = Parameter(torch.zeros(5,len(layers)-2))\n",
    "        self.omega = Parameter(torch.ones(5,len(layers)-2))\n",
    "        \n",
    "        \n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xy):\n",
    "        if torch.is_tensor(xy) != True:         \n",
    "            xy = torch.from_numpy(xy)                \n",
    "        \n",
    "        ubxy = torch.from_numpy(ub_xy).float().to(device)\n",
    "        lbxy = torch.from_numpy(lb_xy).float().to(device)\n",
    "\n",
    "                      \n",
    "        #preprocessing input \n",
    "        xy = (xy - lbxy)/(ubxy - lbxy)\n",
    "        \n",
    "        #convert to float\n",
    "        a = xy.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            z1 = self.activation(z)\n",
    "            z2 = torch.zeros(z.shape).to(device)\n",
    "            for j in range(5):\n",
    "                z2 = z2 + self.alpha[j,i]*self.n*torch.sin(torch.tensor(j+1)*self.n*self.omega[j,i]*z)\n",
    "                \n",
    "            a = z1+z2\n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_D(self,xy_D,u_D):\n",
    "                \n",
    "        loss_bc = self.loss_function(self.forward(xy_D), u_D)\n",
    "                \n",
    "        return loss_bc\n",
    "    \n",
    "    def loss_N(self,xy_Nx,xy_Ny,N_hat):\n",
    "        \n",
    "        g1 = xy_Nx.clone()             \n",
    "        g1.requires_grad = True\n",
    "        u1 = self.forward(g1)\n",
    "        \n",
    "        u1_x_y = autograd.grad(u1,g1,torch.ones([xy_Nx.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        du1_dx = u1_x_y[:,[0]]\n",
    "        \n",
    "        g2 = xy_Ny.clone()             \n",
    "        g2.requires_grad = True\n",
    "        u2 = self.forward(g2)\n",
    "        \n",
    "        u2_x_y = autograd.grad(u2,g2,torch.ones([xy_Ny.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        du2_dy = u2_x_y[:,[1]]\n",
    "               \n",
    "        loss_N1 = self.loss_function(du1_dx,N_hat)\n",
    "        loss_N2 = self.loss_function(du2_dy,N_hat)\n",
    "        \n",
    "        #return loss_N1+loss_N2       \n",
    "        return loss_N1 + loss_N2\n",
    "    \n",
    "    def loss_PDE(self, xy_coll, f_hat):\n",
    "        \n",
    "        g = xy_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_y = autograd.grad(u,g,torch.ones([xy_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_xx_yy = autograd.grad(u_x_y,g,torch.ones(xy_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        #du_dt = u_x_y_t[:,[2]]\n",
    "        \n",
    "        d2u_dx2 = u_xx_yy[:,[0]]\n",
    "        d2u_dy2 = u_xx_yy[:,[1]]    \n",
    "        \n",
    "\n",
    "        f = - k*t_z*(d2u_dx2+d2u_dy2) + 2*hc*(u-Ta) + 2*emiss*stef_bolt*(torch.pow(u,4)-Ta**4) \n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xy_D,u_D,xy_Nx,xy_Ny,N_hat,xy_coll,f_hat):\n",
    "\n",
    "        loss_D = self.loss_D(xy_D,u_D)\n",
    "        loss_N = self.loss_N(xy_Nx,xy_Ny,N_hat)\n",
    "        loss_f = self.loss_PDE(xy_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_D + loss_N + loss_f\n",
    "        \n",
    "        #print(self.iter,\"loss_D:\",loss_D.cpu().detach().numpy(),\"loss_N:\",loss_N.cpu().detach().numpy(),\"loss_f:\",loss_f.cpu().detach().numpy())\n",
    "        \n",
    "        return loss_val\n",
    "     \n",
    "    'callable for optimizer'                                       \n",
    "    def closure(self):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = self.loss(xy_D,u_D,xy_Nx,xy_Ny,N_hat,xy_coll,f_hat)\n",
    "        self.train_loss.append(loss.cpu().detach().numpy())\n",
    "        \n",
    "        u_pred = self.test(xy_test_tensor)\n",
    "        #self.test_loss.append(np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))) #Commented because no true values yet\n",
    "        self.beta_val.append(self.beta.cpu().detach().numpy())\n",
    "        \n",
    "        #print(self.iter,\"Train Loss\",self.train_loss[-1],\"Test Loss\",self.test_loss[-1])\n",
    "        print(self.iter,\"Train Loss\",self.train_loss[-1])\n",
    "        \n",
    "        loss.backward()\n",
    "        self.iter += 1\n",
    "  \n",
    "\n",
    "        return loss        \n",
    "    'test neural network'\n",
    "    def test(self,xy_test_tensor):\n",
    "        u_pred = self.forward(xy_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0 Train Loss 2122402.5\n",
      "1 Train Loss 2121532.2\n",
      "2 Train Loss 2116285.5\n",
      "3 Train Loss 2092505.6\n",
      "4 Train Loss 2080129.9\n",
      "5 Train Loss 2046470.0\n",
      "6 Train Loss 2046045.2\n",
      "7 Train Loss 2038034.8\n",
      "8 Train Loss 2143573.2\n",
      "9 Train Loss 1986930.1\n",
      "10 Train Loss 2358787.5\n",
      "11 Train Loss 1983480.5\n",
      "12 Train Loss 252927730.0\n",
      "13 Train Loss 21141078.0\n",
      "14 Train Loss 2239888.5\n",
      "15 Train Loss 1967614.8\n",
      "16 Train Loss 1925796.2\n",
      "17 Train Loss 1963429.0\n",
      "18 Train Loss 1889739.8\n",
      "19 Train Loss 1988729.5\n",
      "20 Train Loss 1858799.2\n",
      "21 Train Loss 1830010.5\n",
      "22 Train Loss 3023977.0\n",
      "23 Train Loss 1836281.1\n",
      "24 Train Loss 1808493.9\n",
      "25 Train Loss 1791194.2\n",
      "26 Train Loss 1761030.2\n",
      "27 Train Loss 1712555.5\n",
      "28 Train Loss 1667021.2\n",
      "29 Train Loss 1639650.5\n",
      "30 Train Loss 1624061.5\n",
      "31 Train Loss 1607319.5\n",
      "32 Train Loss 1583613.1\n",
      "33 Train Loss 1553432.0\n",
      "34 Train Loss 1540513.5\n",
      "35 Train Loss 1529452.6\n",
      "36 Train Loss 1515608.2\n",
      "37 Train Loss 1483664.8\n",
      "38 Train Loss 1471693.9\n",
      "39 Train Loss 1447464.2\n",
      "40 Train Loss 1423975.8\n",
      "41 Train Loss 1402967.6\n",
      "42 Train Loss 1383440.8\n",
      "43 Train Loss 1388585.5\n",
      "44 Train Loss 1360414.6\n",
      "45 Train Loss 1349262.1\n",
      "46 Train Loss 1329722.6\n",
      "47 Train Loss 1324238.9\n",
      "48 Train Loss 1295463.0\n",
      "49 Train Loss 1280286.1\n",
      "50 Train Loss 1267840.8\n",
      "51 Train Loss 1237166.6\n",
      "52 Train Loss 1195672.9\n",
      "53 Train Loss 1176563.5\n",
      "54 Train Loss 1159788.4\n",
      "55 Train Loss 1157477.1\n",
      "56 Train Loss 1136278.9\n",
      "57 Train Loss 1196871.0\n",
      "58 Train Loss 1121232.5\n",
      "59 Train Loss 1096430.5\n",
      "60 Train Loss 1074339.4\n",
      "61 Train Loss 1074899.4\n",
      "62 Train Loss 1056232.1\n",
      "63 Train Loss 1040908.5\n",
      "64 Train Loss 1033048.1\n",
      "65 Train Loss 1023889.9\n",
      "66 Train Loss 1012195.1\n",
      "67 Train Loss 1004256.7\n",
      "68 Train Loss 996523.5\n",
      "69 Train Loss 988405.4\n",
      "70 Train Loss 981839.25\n",
      "71 Train Loss 975915.2\n",
      "72 Train Loss 970021.8\n",
      "73 Train Loss 971979.3\n",
      "74 Train Loss 964431.7\n",
      "75 Train Loss 959997.94\n",
      "76 Train Loss 954034.8\n",
      "77 Train Loss 965832.6\n",
      "78 Train Loss 950511.6\n",
      "79 Train Loss 948498.0\n",
      "80 Train Loss 943044.25\n",
      "81 Train Loss 939377.06\n",
      "82 Train Loss 935941.06\n",
      "83 Train Loss 929647.8\n",
      "84 Train Loss 922045.94\n",
      "85 Train Loss 917510.8\n",
      "86 Train Loss 910582.5\n",
      "87 Train Loss 913234.2\n",
      "88 Train Loss 907235.56\n",
      "89 Train Loss 902656.0\n",
      "90 Train Loss 899844.56\n",
      "91 Train Loss 903794.8\n",
      "92 Train Loss 897222.4\n",
      "93 Train Loss 894447.6\n",
      "94 Train Loss 891851.06\n",
      "95 Train Loss 884311.25\n",
      "96 Train Loss 878524.9\n",
      "97 Train Loss 868124.75\n",
      "98 Train Loss 867496.3\n",
      "99 Train Loss 864831.4\n",
      "100 Train Loss 863865.6\n",
      "101 Train Loss 858913.6\n",
      "102 Train Loss 854200.6\n",
      "103 Train Loss 848257.56\n",
      "104 Train Loss 1004930.3\n",
      "105 Train Loss 846849.0\n",
      "106 Train Loss 845186.06\n",
      "107 Train Loss 840974.94\n",
      "108 Train Loss 839266.56\n",
      "109 Train Loss 837307.75\n",
      "110 Train Loss 834052.6\n",
      "111 Train Loss 832223.56\n",
      "112 Train Loss 827157.1\n",
      "113 Train Loss 833622.8\n",
      "114 Train Loss 822775.6\n",
      "115 Train Loss 820054.1\n",
      "116 Train Loss 824003.94\n",
      "117 Train Loss 817767.8\n",
      "118 Train Loss 822270.56\n",
      "119 Train Loss 815469.06\n",
      "120 Train Loss 826171.5\n",
      "121 Train Loss 814600.7\n",
      "122 Train Loss 813172.75\n",
      "123 Train Loss 808337.8\n",
      "124 Train Loss 806877.7\n",
      "125 Train Loss 804914.7\n",
      "126 Train Loss 799927.06\n",
      "127 Train Loss 807324.6\n",
      "128 Train Loss 796614.5\n",
      "129 Train Loss 791727.7\n",
      "130 Train Loss 787909.2\n",
      "131 Train Loss 785559.2\n",
      "132 Train Loss 790636.4\n",
      "133 Train Loss 783399.25\n",
      "134 Train Loss 781058.4\n",
      "135 Train Loss 778071.75\n",
      "136 Train Loss 797101.2\n",
      "137 Train Loss 772981.94\n",
      "138 Train Loss 772219.94\n",
      "139 Train Loss 770609.2\n",
      "140 Train Loss 769945.6\n",
      "141 Train Loss 768398.4\n",
      "142 Train Loss 765439.44\n",
      "143 Train Loss 764547.44\n",
      "144 Train Loss 764847.56\n",
      "145 Train Loss 763145.0\n",
      "146 Train Loss 761622.56\n",
      "147 Train Loss 760035.4\n",
      "148 Train Loss 755687.1\n",
      "149 Train Loss 752084.44\n",
      "150 Train Loss 746798.5\n",
      "151 Train Loss 747042.5\n",
      "152 Train Loss 744772.3\n",
      "153 Train Loss 741933.56\n",
      "154 Train Loss 740983.94\n",
      "155 Train Loss 739784.25\n",
      "156 Train Loss 736810.44\n",
      "157 Train Loss 734123.6\n",
      "158 Train Loss 737859.1\n",
      "159 Train Loss 730220.2\n",
      "160 Train Loss 726274.4\n",
      "161 Train Loss 723215.94\n",
      "162 Train Loss 730571.75\n",
      "163 Train Loss 721054.0\n",
      "164 Train Loss 718779.8\n",
      "165 Train Loss 715617.9\n",
      "166 Train Loss 719157.7\n",
      "167 Train Loss 712512.1\n",
      "168 Train Loss 707236.44\n",
      "169 Train Loss 708416.5\n",
      "170 Train Loss 701854.9\n",
      "171 Train Loss 693372.0\n",
      "172 Train Loss 688650.3\n",
      "173 Train Loss 685042.7\n",
      "174 Train Loss 678775.9\n",
      "175 Train Loss 671474.1\n",
      "176 Train Loss 669260.44\n",
      "177 Train Loss 667298.8\n",
      "178 Train Loss 662215.7\n",
      "179 Train Loss 655876.06\n",
      "180 Train Loss 652599.06\n",
      "181 Train Loss 648028.06\n",
      "182 Train Loss 638066.44\n",
      "183 Train Loss 627889.6\n",
      "184 Train Loss 622133.3\n",
      "185 Train Loss 618741.1\n",
      "186 Train Loss 612422.4\n",
      "187 Train Loss 608303.1\n",
      "188 Train Loss 605407.2\n",
      "189 Train Loss 602041.6\n",
      "190 Train Loss 599669.6\n",
      "191 Train Loss 595551.8\n",
      "192 Train Loss 593278.56\n",
      "193 Train Loss 587109.0\n",
      "194 Train Loss 593324.0\n",
      "195 Train Loss 583244.4\n",
      "196 Train Loss 581905.2\n",
      "197 Train Loss 577730.7\n",
      "198 Train Loss 566595.94\n",
      "199 Train Loss 557845.8\n",
      "200 Train Loss 546073.9\n",
      "201 Train Loss 529047.75\n",
      "202 Train Loss 545274.94\n",
      "203 Train Loss 526998.4\n",
      "204 Train Loss 525059.5\n",
      "205 Train Loss 523132.5\n",
      "206 Train Loss 524583.8\n",
      "207 Train Loss 515103.16\n",
      "208 Train Loss 594776.5\n",
      "209 Train Loss 512727.56\n",
      "210 Train Loss 505967.12\n",
      "211 Train Loss 501992.34\n",
      "212 Train Loss 496617.88\n",
      "213 Train Loss 491389.97\n",
      "214 Train Loss 500835.62\n",
      "215 Train Loss 487647.8\n",
      "216 Train Loss 484198.3\n",
      "217 Train Loss 477557.8\n",
      "218 Train Loss 473578.7\n",
      "219 Train Loss 468744.56\n",
      "220 Train Loss 462405.25\n",
      "221 Train Loss 455350.7\n",
      "222 Train Loss 450043.2\n",
      "223 Train Loss 447037.94\n",
      "224 Train Loss 443027.28\n",
      "225 Train Loss 450133.03\n",
      "226 Train Loss 440996.84\n",
      "227 Train Loss 439529.66\n",
      "228 Train Loss 440677.03\n",
      "229 Train Loss 437607.0\n",
      "230 Train Loss 435572.75\n",
      "231 Train Loss 431311.66\n",
      "232 Train Loss 432887.03\n",
      "233 Train Loss 428953.22\n",
      "234 Train Loss 449949.84\n",
      "235 Train Loss 427081.7\n",
      "236 Train Loss 425636.2\n",
      "237 Train Loss 422425.03\n",
      "238 Train Loss 419965.16\n",
      "239 Train Loss 419432.5\n",
      "240 Train Loss 419784.28\n",
      "241 Train Loss 418992.66\n",
      "242 Train Loss 417477.84\n",
      "243 Train Loss 415869.4\n",
      "244 Train Loss 413554.4\n",
      "245 Train Loss 413601.8\n",
      "246 Train Loss 412531.44\n",
      "247 Train Loss 410560.5\n",
      "248 Train Loss 408691.78\n",
      "249 Train Loss 407457.25\n",
      "250 Train Loss 406391.75\n",
      "251 Train Loss 404332.72\n",
      "252 Train Loss 403118.9\n",
      "253 Train Loss 401876.6\n",
      "254 Train Loss 399484.6\n",
      "255 Train Loss 395345.1\n",
      "256 Train Loss 389982.8\n",
      "257 Train Loss 489507.44\n",
      "258 Train Loss 387336.0\n",
      "259 Train Loss 382352.56\n",
      "260 Train Loss 378955.28\n",
      "261 Train Loss 374603.38\n",
      "262 Train Loss 368568.06\n",
      "263 Train Loss 367967.28\n",
      "264 Train Loss 362390.34\n",
      "265 Train Loss 360507.66\n",
      "266 Train Loss 358754.72\n",
      "267 Train Loss 356541.62\n",
      "268 Train Loss 353503.3\n",
      "269 Train Loss 351500.44\n",
      "270 Train Loss 352878.47\n",
      "271 Train Loss 345524.12\n",
      "272 Train Loss 355977.72\n",
      "273 Train Loss 342749.84\n",
      "274 Train Loss 347512.3\n",
      "275 Train Loss 341024.94\n",
      "276 Train Loss 339539.78\n",
      "277 Train Loss 336824.66\n",
      "278 Train Loss 340600.28\n",
      "279 Train Loss 335063.4\n",
      "280 Train Loss 333654.0\n",
      "281 Train Loss 330418.38\n",
      "282 Train Loss 331293.06\n",
      "283 Train Loss 326367.12\n",
      "284 Train Loss 323413.22\n",
      "285 Train Loss 337771.7\n",
      "286 Train Loss 319060.2\n",
      "287 Train Loss 311778.44\n",
      "288 Train Loss 358819.1\n",
      "289 Train Loss 310049.22\n",
      "290 Train Loss 392124.94\n",
      "291 Train Loss 309635.34\n",
      "292 Train Loss 308700.4\n",
      "293 Train Loss 306770.2\n",
      "294 Train Loss 305368.66\n",
      "295 Train Loss 304509.56\n",
      "296 Train Loss 301731.1\n",
      "297 Train Loss 330264.3\n",
      "298 Train Loss 299711.16\n",
      "299 Train Loss 296926.47\n",
      "300 Train Loss 294545.16\n",
      "301 Train Loss 292028.06\n",
      "302 Train Loss 290727.8\n",
      "303 Train Loss 289369.16\n",
      "304 Train Loss 288138.28\n",
      "305 Train Loss 287231.62\n",
      "306 Train Loss 286456.03\n",
      "307 Train Loss 285469.7\n",
      "308 Train Loss 283556.56\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "309 Train Loss 283433.66\n",
      "310 Train Loss 282002.38\n",
      "311 Train Loss 287082.9\n",
      "312 Train Loss 280173.22\n",
      "313 Train Loss 277863.84\n",
      "314 Train Loss 281693.4\n",
      "315 Train Loss 277394.38\n",
      "316 Train Loss 276972.84\n",
      "317 Train Loss 276616.47\n",
      "318 Train Loss 275813.06\n",
      "319 Train Loss 274497.47\n",
      "320 Train Loss 272769.6\n",
      "321 Train Loss 271593.4\n",
      "322 Train Loss 270167.4\n",
      "323 Train Loss 269200.38\n",
      "324 Train Loss 268173.66\n",
      "325 Train Loss 266478.84\n",
      "326 Train Loss 265259.1\n",
      "327 Train Loss 263414.8\n",
      "328 Train Loss 260439.44\n",
      "329 Train Loss 259774.67\n",
      "330 Train Loss 269530.4\n",
      "331 Train Loss 258038.4\n",
      "332 Train Loss 257351.89\n",
      "333 Train Loss 256722.31\n",
      "334 Train Loss 256354.1\n",
      "335 Train Loss 254930.73\n",
      "336 Train Loss 254651.47\n",
      "337 Train Loss 254129.03\n",
      "338 Train Loss 253124.08\n",
      "339 Train Loss 251560.03\n",
      "340 Train Loss 251037.55\n",
      "341 Train Loss 250332.64\n",
      "342 Train Loss 249595.77\n",
      "343 Train Loss 249159.4\n",
      "344 Train Loss 248539.86\n",
      "345 Train Loss 246680.77\n",
      "346 Train Loss 245125.56\n",
      "347 Train Loss 242211.7\n",
      "348 Train Loss 240063.45\n",
      "349 Train Loss 241065.48\n",
      "350 Train Loss 238814.03\n",
      "351 Train Loss 237288.31\n",
      "352 Train Loss 236693.52\n",
      "353 Train Loss 243257.56\n",
      "354 Train Loss 235959.23\n",
      "355 Train Loss 236104.08\n",
      "356 Train Loss 235161.31\n",
      "357 Train Loss 234930.84\n",
      "358 Train Loss 234340.1\n",
      "359 Train Loss 234182.28\n",
      "360 Train Loss 233967.4\n",
      "361 Train Loss 233617.47\n",
      "362 Train Loss 233276.14\n",
      "363 Train Loss 232689.77\n",
      "364 Train Loss 232061.78\n",
      "365 Train Loss 231255.48\n",
      "366 Train Loss 229407.02\n",
      "367 Train Loss 227412.17\n",
      "368 Train Loss 226015.72\n",
      "369 Train Loss 223686.12\n",
      "370 Train Loss 223293.4\n",
      "371 Train Loss 222412.53\n",
      "372 Train Loss 221929.42\n",
      "373 Train Loss 221306.89\n",
      "374 Train Loss 220411.62\n",
      "375 Train Loss 219855.98\n",
      "376 Train Loss 219306.62\n",
      "377 Train Loss 219204.48\n",
      "378 Train Loss 218907.17\n",
      "379 Train Loss 217903.55\n",
      "380 Train Loss 219065.62\n",
      "381 Train Loss 216760.81\n",
      "382 Train Loss 217333.95\n",
      "383 Train Loss 215749.92\n",
      "384 Train Loss 214259.78\n",
      "385 Train Loss 213348.72\n",
      "386 Train Loss 212659.14\n",
      "387 Train Loss 211657.66\n",
      "388 Train Loss 210579.1\n",
      "389 Train Loss 210804.5\n",
      "390 Train Loss 210052.78\n",
      "391 Train Loss 209683.92\n",
      "392 Train Loss 209289.84\n",
      "393 Train Loss 208950.81\n",
      "394 Train Loss 209607.72\n",
      "395 Train Loss 208333.48\n",
      "396 Train Loss 208459.8\n",
      "397 Train Loss 207511.05\n",
      "398 Train Loss 206860.0\n",
      "399 Train Loss 205215.05\n",
      "400 Train Loss 204339.06\n",
      "401 Train Loss 204210.17\n",
      "402 Train Loss 204007.47\n",
      "403 Train Loss 203560.33\n",
      "404 Train Loss 202633.22\n",
      "405 Train Loss 201923.03\n",
      "406 Train Loss 201630.97\n",
      "407 Train Loss 201122.08\n",
      "408 Train Loss 200875.7\n",
      "409 Train Loss 200374.77\n",
      "410 Train Loss 200180.05\n",
      "411 Train Loss 199751.06\n",
      "412 Train Loss 199423.11\n",
      "413 Train Loss 198730.06\n",
      "414 Train Loss 197919.39\n",
      "415 Train Loss 198393.11\n",
      "416 Train Loss 197447.97\n",
      "417 Train Loss 196469.97\n",
      "418 Train Loss 195872.44\n",
      "419 Train Loss 194914.98\n",
      "420 Train Loss 194836.5\n",
      "421 Train Loss 193850.78\n",
      "422 Train Loss 192377.92\n",
      "423 Train Loss 192813.23\n",
      "424 Train Loss 191667.14\n",
      "425 Train Loss 193358.44\n",
      "426 Train Loss 190293.67\n",
      "427 Train Loss 189034.14\n",
      "428 Train Loss 188945.6\n",
      "429 Train Loss 188160.72\n",
      "430 Train Loss 187104.69\n",
      "431 Train Loss 186271.05\n",
      "432 Train Loss 185515.45\n",
      "433 Train Loss 184860.36\n",
      "434 Train Loss 184388.97\n",
      "435 Train Loss 184035.61\n",
      "436 Train Loss 183786.08\n",
      "437 Train Loss 182963.48\n",
      "438 Train Loss 181454.7\n",
      "439 Train Loss 180883.53\n",
      "440 Train Loss 180556.4\n",
      "441 Train Loss 180448.1\n",
      "442 Train Loss 179575.84\n",
      "443 Train Loss 179169.06\n",
      "444 Train Loss 178844.72\n",
      "445 Train Loss 178472.42\n",
      "446 Train Loss 177821.98\n",
      "447 Train Loss 176909.81\n",
      "448 Train Loss 175429.28\n",
      "449 Train Loss 173928.08\n",
      "450 Train Loss 180135.17\n",
      "451 Train Loss 172380.72\n",
      "452 Train Loss 171000.27\n",
      "453 Train Loss 170627.06\n",
      "454 Train Loss 169112.9\n",
      "455 Train Loss 173830.28\n",
      "456 Train Loss 168243.28\n",
      "457 Train Loss 167686.0\n",
      "458 Train Loss 167196.25\n",
      "459 Train Loss 166980.2\n",
      "460 Train Loss 166686.33\n",
      "461 Train Loss 166449.73\n",
      "462 Train Loss 166080.78\n",
      "463 Train Loss 165822.19\n",
      "464 Train Loss 165338.94\n",
      "465 Train Loss 164929.1\n",
      "466 Train Loss 164639.48\n",
      "467 Train Loss 164465.45\n",
      "468 Train Loss 164241.4\n",
      "469 Train Loss 163975.62\n",
      "470 Train Loss 163329.4\n",
      "471 Train Loss 162840.08\n",
      "472 Train Loss 162287.44\n",
      "473 Train Loss 162680.97\n",
      "474 Train Loss 162049.88\n",
      "475 Train Loss 161539.5\n",
      "476 Train Loss 160714.12\n",
      "477 Train Loss 160314.55\n",
      "478 Train Loss 159740.89\n",
      "479 Train Loss 159105.7\n",
      "480 Train Loss 158332.83\n",
      "481 Train Loss 157699.44\n",
      "482 Train Loss 158111.42\n",
      "483 Train Loss 157373.36\n",
      "484 Train Loss 158183.02\n",
      "485 Train Loss 157137.67\n",
      "486 Train Loss 156577.92\n",
      "487 Train Loss 156099.19\n",
      "488 Train Loss 155780.72\n",
      "489 Train Loss 155195.69\n",
      "490 Train Loss 153959.02\n",
      "491 Train Loss 153331.44\n",
      "492 Train Loss 152942.23\n",
      "493 Train Loss 152807.16\n",
      "494 Train Loss 152439.45\n",
      "495 Train Loss 152256.05\n",
      "496 Train Loss 152006.86\n",
      "497 Train Loss 151675.97\n",
      "498 Train Loss 151318.78\n",
      "499 Train Loss 150900.17\n",
      "500 Train Loss 150397.06\n",
      "501 Train Loss 149561.9\n",
      "502 Train Loss 148308.23\n",
      "503 Train Loss 148859.19\n",
      "504 Train Loss 147561.27\n",
      "505 Train Loss 146435.0\n",
      "506 Train Loss 145249.0\n",
      "507 Train Loss 147205.42\n",
      "508 Train Loss 144730.67\n",
      "509 Train Loss 144302.75\n",
      "510 Train Loss 144191.17\n",
      "511 Train Loss 144489.94\n",
      "512 Train Loss 144028.47\n",
      "513 Train Loss 144021.69\n",
      "514 Train Loss 143890.64\n",
      "515 Train Loss 143880.66\n",
      "516 Train Loss 143788.5\n",
      "517 Train Loss 143621.36\n",
      "518 Train Loss 143500.31\n",
      "519 Train Loss 143381.4\n",
      "520 Train Loss 143321.72\n",
      "521 Train Loss 143255.08\n",
      "522 Train Loss 143189.33\n",
      "523 Train Loss 143064.55\n",
      "524 Train Loss 142579.47\n",
      "525 Train Loss 142059.48\n",
      "526 Train Loss 140625.97\n",
      "527 Train Loss 143725.73\n",
      "528 Train Loss 140413.75\n",
      "529 Train Loss 141367.23\n",
      "530 Train Loss 139647.48\n",
      "531 Train Loss 147044.48\n",
      "532 Train Loss 138889.6\n",
      "533 Train Loss 138127.12\n",
      "534 Train Loss 137706.75\n",
      "535 Train Loss 137368.22\n",
      "536 Train Loss 137238.84\n",
      "537 Train Loss 137114.19\n",
      "538 Train Loss 136817.9\n",
      "539 Train Loss 136626.64\n",
      "540 Train Loss 135972.67\n",
      "541 Train Loss 135266.81\n",
      "542 Train Loss 138379.14\n",
      "543 Train Loss 135049.19\n",
      "544 Train Loss 134934.11\n",
      "545 Train Loss 134701.4\n",
      "546 Train Loss 134825.6\n",
      "547 Train Loss 134460.61\n",
      "548 Train Loss 134141.45\n",
      "549 Train Loss 133958.44\n",
      "550 Train Loss 133857.81\n",
      "551 Train Loss 133660.06\n",
      "552 Train Loss 133489.2\n",
      "553 Train Loss 133204.56\n",
      "554 Train Loss 132230.0\n",
      "555 Train Loss 131872.77\n",
      "556 Train Loss 131302.69\n",
      "557 Train Loss 132262.67\n",
      "558 Train Loss 130381.86\n",
      "559 Train Loss 129767.45\n",
      "560 Train Loss 129750.266\n",
      "561 Train Loss 129284.766\n",
      "562 Train Loss 128772.055\n",
      "563 Train Loss 128454.34\n",
      "564 Train Loss 128120.11\n",
      "565 Train Loss 127767.414\n",
      "566 Train Loss 127440.09\n",
      "567 Train Loss 127094.88\n",
      "568 Train Loss 126179.14\n",
      "569 Train Loss 125508.34\n",
      "570 Train Loss 125468.33\n",
      "571 Train Loss 124959.0\n",
      "572 Train Loss 124580.84\n",
      "573 Train Loss 124200.695\n",
      "574 Train Loss 123508.51\n",
      "575 Train Loss 122641.984\n",
      "576 Train Loss 121494.055\n",
      "577 Train Loss 120128.81\n",
      "578 Train Loss 119488.46\n",
      "579 Train Loss 126282.58\n",
      "580 Train Loss 118997.91\n",
      "581 Train Loss 118962.78\n",
      "582 Train Loss 118374.33\n",
      "583 Train Loss 118269.914\n",
      "584 Train Loss 117875.22\n",
      "585 Train Loss 116722.82\n",
      "586 Train Loss 131612.72\n",
      "587 Train Loss 113507.055\n",
      "588 Train Loss 111827.89\n",
      "589 Train Loss 111006.92\n",
      "590 Train Loss 109237.95\n",
      "591 Train Loss 138091.38\n",
      "592 Train Loss 104869.45\n",
      "593 Train Loss 103374.96\n",
      "594 Train Loss 102116.53\n",
      "595 Train Loss 115006.63\n",
      "596 Train Loss 97878.02\n",
      "597 Train Loss 105127.48\n",
      "598 Train Loss 95799.3\n",
      "599 Train Loss 94889.61\n",
      "600 Train Loss 96844.08\n",
      "601 Train Loss 93037.96\n",
      "602 Train Loss 129564.34\n",
      "603 Train Loss 91590.805\n",
      "604 Train Loss 102028.99\n",
      "605 Train Loss 90812.914\n",
      "606 Train Loss 90653.69\n",
      "607 Train Loss 90338.91\n",
      "608 Train Loss 90252.16\n",
      "609 Train Loss 89976.25\n",
      "610 Train Loss 89400.29\n",
      "611 Train Loss 88489.25\n",
      "612 Train Loss 87652.92\n",
      "613 Train Loss 87194.945\n",
      "614 Train Loss 86812.86\n",
      "615 Train Loss 86679.0\n",
      "616 Train Loss 86544.67\n",
      "617 Train Loss 86275.86\n",
      "618 Train Loss 86001.59\n",
      "619 Train Loss 85752.32\n",
      "620 Train Loss 85472.13\n",
      "621 Train Loss 85146.805\n",
      "622 Train Loss 84775.22\n",
      "623 Train Loss 84507.75\n",
      "624 Train Loss 84224.4\n",
      "625 Train Loss 84362.43\n",
      "626 Train Loss 84113.19\n",
      "627 Train Loss 83851.05\n",
      "628 Train Loss 83591.414\n",
      "629 Train Loss 83270.65\n",
      "630 Train Loss 82893.1\n",
      "631 Train Loss 82600.74\n",
      "632 Train Loss 82504.44\n",
      "633 Train Loss 82419.71\n",
      "634 Train Loss 82194.15\n",
      "635 Train Loss 81839.31\n",
      "636 Train Loss 81673.016\n",
      "637 Train Loss 81509.59\n",
      "638 Train Loss 81187.914\n",
      "639 Train Loss 81009.625\n",
      "640 Train Loss 80860.42\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "641 Train Loss 80743.63\n",
      "642 Train Loss 80556.33\n",
      "643 Train Loss 80356.055\n",
      "644 Train Loss 79807.28\n",
      "645 Train Loss 79307.51\n",
      "646 Train Loss 78786.27\n",
      "647 Train Loss 78580.51\n",
      "648 Train Loss 78148.16\n",
      "649 Train Loss 78114.19\n",
      "650 Train Loss 78165.39\n",
      "651 Train Loss 78014.695\n",
      "652 Train Loss 77953.04\n",
      "653 Train Loss 77834.16\n",
      "654 Train Loss 77789.59\n",
      "655 Train Loss 77695.93\n",
      "656 Train Loss 77537.58\n",
      "657 Train Loss 77250.67\n",
      "658 Train Loss 76540.95\n",
      "659 Train Loss 75255.836\n",
      "660 Train Loss 75386.43\n",
      "661 Train Loss 74332.94\n",
      "662 Train Loss 73496.09\n",
      "663 Train Loss 74345.64\n",
      "664 Train Loss 73146.195\n",
      "665 Train Loss 73113.43\n",
      "666 Train Loss 72946.35\n",
      "667 Train Loss 72760.47\n",
      "668 Train Loss 72544.64\n",
      "669 Train Loss 72204.53\n",
      "670 Train Loss 71747.22\n",
      "671 Train Loss 71426.3\n",
      "672 Train Loss 72984.41\n",
      "673 Train Loss 71208.25\n",
      "674 Train Loss 70995.5\n",
      "675 Train Loss 70701.17\n",
      "676 Train Loss 70432.55\n",
      "677 Train Loss 70018.86\n",
      "678 Train Loss 69432.86\n",
      "679 Train Loss 68737.0\n",
      "680 Train Loss 107648.984\n",
      "681 Train Loss 68380.984\n",
      "682 Train Loss 68054.27\n",
      "683 Train Loss 66958.414\n",
      "684 Train Loss 66614.47\n",
      "685 Train Loss 65148.234\n",
      "686 Train Loss 65882.5\n",
      "687 Train Loss 63362.562\n",
      "688 Train Loss 69876.875\n",
      "689 Train Loss 62398.14\n",
      "690 Train Loss 62909.242\n",
      "691 Train Loss 61004.133\n",
      "692 Train Loss 70056.41\n",
      "693 Train Loss 59617.582\n",
      "694 Train Loss 59065.484\n",
      "695 Train Loss 57759.016\n",
      "696 Train Loss 60788.36\n",
      "697 Train Loss 55754.016\n",
      "698 Train Loss 90250.125\n",
      "699 Train Loss 54816.047\n",
      "700 Train Loss 59961.414\n",
      "701 Train Loss 53907.14\n",
      "702 Train Loss 53163.516\n",
      "703 Train Loss 52508.875\n",
      "704 Train Loss 52364.13\n",
      "705 Train Loss 52196.336\n",
      "706 Train Loss 52367.875\n",
      "707 Train Loss 51760.824\n",
      "708 Train Loss 51613.44\n",
      "709 Train Loss 51359.727\n",
      "710 Train Loss 51197.562\n",
      "711 Train Loss 50796.434\n",
      "712 Train Loss 49190.863\n",
      "713 Train Loss 48329.523\n",
      "714 Train Loss 47504.008\n",
      "715 Train Loss 46499.094\n",
      "716 Train Loss 46056.664\n",
      "717 Train Loss 45818.9\n",
      "718 Train Loss 45424.812\n",
      "719 Train Loss 45166.695\n",
      "720 Train Loss 44410.64\n",
      "721 Train Loss 43162.09\n",
      "722 Train Loss 42252.535\n",
      "723 Train Loss 41559.992\n",
      "724 Train Loss 41220.82\n",
      "725 Train Loss 41527.977\n",
      "726 Train Loss 41041.234\n",
      "727 Train Loss 40890.83\n",
      "728 Train Loss 40688.43\n",
      "729 Train Loss 40549.812\n",
      "730 Train Loss 40406.594\n",
      "731 Train Loss 40312.63\n",
      "732 Train Loss 40278.47\n",
      "733 Train Loss 40234.047\n",
      "734 Train Loss 40211.457\n",
      "735 Train Loss 40160.926\n",
      "736 Train Loss 40117.45\n",
      "737 Train Loss 40033.18\n",
      "738 Train Loss 39769.855\n",
      "739 Train Loss 39582.637\n",
      "740 Train Loss 39496.23\n",
      "741 Train Loss 39352.96\n",
      "742 Train Loss 39235.258\n",
      "743 Train Loss 38905.78\n",
      "744 Train Loss 38961.727\n",
      "745 Train Loss 38726.676\n",
      "746 Train Loss 38504.35\n",
      "747 Train Loss 38235.61\n",
      "748 Train Loss 38218.18\n",
      "749 Train Loss 38046.574\n",
      "750 Train Loss 37852.91\n",
      "751 Train Loss 37761.145\n",
      "752 Train Loss 37642.375\n",
      "753 Train Loss 37631.426\n",
      "754 Train Loss 37570.33\n",
      "755 Train Loss 37439.273\n",
      "756 Train Loss 37322.113\n",
      "757 Train Loss 37262.273\n",
      "758 Train Loss 37052.418\n",
      "759 Train Loss 36961.5\n",
      "760 Train Loss 36809.453\n",
      "761 Train Loss 36708.484\n",
      "762 Train Loss 36548.47\n",
      "763 Train Loss 36365.4\n",
      "764 Train Loss 36155.832\n",
      "765 Train Loss 36150.586\n",
      "766 Train Loss 36041.566\n",
      "767 Train Loss 35926.14\n",
      "768 Train Loss 35759.43\n",
      "769 Train Loss 35616.047\n",
      "770 Train Loss 35524.785\n",
      "771 Train Loss 35445.273\n",
      "772 Train Loss 35304.402\n",
      "773 Train Loss 34885.92\n",
      "774 Train Loss 34400.28\n",
      "775 Train Loss 34098.64\n",
      "776 Train Loss 33899.9\n",
      "777 Train Loss 33726.812\n",
      "778 Train Loss 33552.797\n",
      "779 Train Loss 33451.293\n",
      "780 Train Loss 33384.16\n",
      "781 Train Loss 33230.043\n",
      "782 Train Loss 33037.02\n",
      "783 Train Loss 32895.773\n",
      "784 Train Loss 32765.355\n",
      "785 Train Loss 32578.348\n",
      "786 Train Loss 32325.623\n",
      "787 Train Loss 31975.059\n",
      "788 Train Loss 31415.855\n",
      "789 Train Loss 31057.06\n",
      "790 Train Loss 32455.8\n",
      "791 Train Loss 30637.219\n",
      "792 Train Loss 32209.822\n",
      "793 Train Loss 30336.48\n",
      "794 Train Loss 30076.299\n",
      "795 Train Loss 29841.61\n",
      "796 Train Loss 29772.953\n",
      "797 Train Loss 29644.746\n",
      "798 Train Loss 29642.031\n",
      "799 Train Loss 29613.367\n",
      "800 Train Loss 29605.02\n",
      "801 Train Loss 29521.705\n",
      "802 Train Loss 29471.398\n",
      "803 Train Loss 29388.898\n",
      "804 Train Loss 29145.855\n",
      "805 Train Loss 28803.975\n",
      "806 Train Loss 28568.377\n",
      "807 Train Loss 28421.504\n",
      "808 Train Loss 28124.287\n",
      "809 Train Loss 27894.033\n",
      "810 Train Loss 27529.83\n",
      "811 Train Loss 28332.064\n",
      "812 Train Loss 27369.574\n",
      "813 Train Loss 27071.514\n",
      "814 Train Loss 26898.938\n",
      "815 Train Loss 26836.924\n",
      "816 Train Loss 26809.832\n",
      "817 Train Loss 26757.121\n",
      "818 Train Loss 26693.527\n",
      "819 Train Loss 26612.559\n",
      "820 Train Loss 26485.797\n",
      "821 Train Loss 26348.309\n",
      "822 Train Loss 26180.27\n",
      "823 Train Loss 25994.906\n",
      "824 Train Loss 25714.102\n",
      "825 Train Loss 26642.453\n",
      "826 Train Loss 25591.25\n",
      "827 Train Loss 25593.932\n",
      "828 Train Loss 25494.227\n",
      "829 Train Loss 25349.22\n",
      "830 Train Loss 25238.902\n",
      "831 Train Loss 25183.104\n",
      "832 Train Loss 25130.793\n",
      "833 Train Loss 25073.6\n",
      "834 Train Loss 25023.227\n",
      "835 Train Loss 24959.635\n",
      "836 Train Loss 24886.562\n",
      "837 Train Loss 24804.203\n",
      "838 Train Loss 24719.176\n",
      "839 Train Loss 24703.904\n",
      "840 Train Loss 24681.98\n",
      "841 Train Loss 24621.295\n",
      "842 Train Loss 24593.838\n",
      "843 Train Loss 24556.684\n",
      "844 Train Loss 24528.91\n",
      "845 Train Loss 24496.082\n",
      "846 Train Loss 24459.201\n",
      "847 Train Loss 24413.873\n",
      "848 Train Loss 24379.816\n",
      "849 Train Loss 24306.53\n",
      "850 Train Loss 24239.396\n",
      "851 Train Loss 24188.69\n",
      "852 Train Loss 24116.21\n",
      "853 Train Loss 24036.709\n",
      "854 Train Loss 23969.309\n",
      "855 Train Loss 23788.904\n",
      "856 Train Loss 23590.324\n",
      "857 Train Loss 23523.23\n",
      "858 Train Loss 23474.87\n",
      "859 Train Loss 23447.473\n",
      "860 Train Loss 23417.514\n",
      "861 Train Loss 23396.984\n",
      "862 Train Loss 23356.453\n",
      "863 Train Loss 23322.043\n",
      "864 Train Loss 23262.883\n",
      "865 Train Loss 23207.277\n",
      "866 Train Loss 23168.984\n",
      "867 Train Loss 23127.809\n",
      "868 Train Loss 23086.887\n",
      "869 Train Loss 23044.035\n",
      "870 Train Loss 22961.537\n",
      "871 Train Loss 22745.45\n",
      "872 Train Loss 22649.23\n",
      "873 Train Loss 22249.156\n",
      "874 Train Loss 26175.719\n",
      "875 Train Loss 21820.203\n",
      "876 Train Loss 21659.125\n",
      "877 Train Loss 21429.871\n",
      "878 Train Loss 22050.584\n",
      "879 Train Loss 21187.072\n",
      "880 Train Loss 21121.16\n",
      "881 Train Loss 21112.293\n",
      "882 Train Loss 20920.098\n",
      "883 Train Loss 20983.0\n",
      "884 Train Loss 20735.508\n",
      "885 Train Loss 20596.078\n",
      "886 Train Loss 20389.037\n",
      "887 Train Loss 20284.273\n",
      "888 Train Loss 20253.371\n",
      "889 Train Loss 20319.646\n",
      "890 Train Loss 20202.197\n",
      "891 Train Loss 20143.05\n",
      "892 Train Loss 20085.621\n",
      "893 Train Loss 20006.684\n",
      "894 Train Loss 19910.215\n",
      "895 Train Loss 19890.27\n",
      "896 Train Loss 19862.273\n",
      "897 Train Loss 19818.254\n",
      "898 Train Loss 19773.484\n",
      "899 Train Loss 19704.316\n",
      "900 Train Loss 19649.465\n",
      "901 Train Loss 19567.549\n",
      "902 Train Loss 19479.906\n",
      "903 Train Loss 19393.184\n",
      "904 Train Loss 19294.867\n",
      "905 Train Loss 19316.0\n",
      "906 Train Loss 19183.217\n",
      "907 Train Loss 19007.186\n",
      "908 Train Loss 18870.762\n",
      "909 Train Loss 18787.092\n",
      "910 Train Loss 18688.047\n",
      "911 Train Loss 18593.387\n",
      "912 Train Loss 18675.611\n",
      "913 Train Loss 18542.82\n",
      "914 Train Loss 18705.637\n",
      "915 Train Loss 18506.123\n",
      "916 Train Loss 18440.117\n",
      "917 Train Loss 18404.379\n",
      "918 Train Loss 18354.336\n",
      "919 Train Loss 18298.387\n",
      "920 Train Loss 18215.229\n",
      "921 Train Loss 18231.723\n",
      "922 Train Loss 18155.883\n",
      "923 Train Loss 18060.688\n",
      "924 Train Loss 17844.434\n",
      "925 Train Loss 17524.523\n",
      "926 Train Loss 17217.432\n",
      "927 Train Loss 23568.527\n",
      "928 Train Loss 17008.236\n",
      "929 Train Loss 20177.586\n",
      "930 Train Loss 16810.43\n",
      "931 Train Loss 17261.492\n",
      "932 Train Loss 16615.334\n",
      "933 Train Loss 17350.893\n",
      "934 Train Loss 16446.06\n",
      "935 Train Loss 16286.162\n",
      "936 Train Loss 16047.683\n",
      "937 Train Loss 16018.205\n",
      "938 Train Loss 15957.51\n",
      "939 Train Loss 15916.73\n",
      "940 Train Loss 15857.658\n",
      "941 Train Loss 15818.133\n",
      "942 Train Loss 15777.373\n",
      "943 Train Loss 15777.984\n",
      "944 Train Loss 15717.955\n",
      "945 Train Loss 15617.913\n",
      "946 Train Loss 15483.3955\n",
      "947 Train Loss 15190.751\n",
      "948 Train Loss 15392.584\n",
      "949 Train Loss 15083.385\n",
      "950 Train Loss 14961.106\n",
      "951 Train Loss 14924.294\n",
      "952 Train Loss 14733.834\n",
      "953 Train Loss 14651.1875\n",
      "954 Train Loss 14544.857\n",
      "955 Train Loss 15151.24\n",
      "956 Train Loss 14400.291\n",
      "957 Train Loss 14244.057\n",
      "958 Train Loss 14202.697\n",
      "959 Train Loss 14213.29\n",
      "960 Train Loss 14038.551\n",
      "961 Train Loss 14005.85\n",
      "962 Train Loss 13941.641\n",
      "963 Train Loss 13893.716\n",
      "964 Train Loss 13796.803\n",
      "965 Train Loss 13602.461\n",
      "966 Train Loss 13528.787\n",
      "967 Train Loss 13438.391\n",
      "968 Train Loss 13298.657\n",
      "969 Train Loss 13134.3955\n",
      "970 Train Loss 12907.73\n",
      "971 Train Loss 12639.807\n",
      "972 Train Loss 15281.595\n",
      "973 Train Loss 12431.078\n",
      "974 Train Loss 12801.3125\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "975 Train Loss 12167.774\n",
      "976 Train Loss 11934.668\n",
      "977 Train Loss 11792.612\n",
      "978 Train Loss 11710.603\n",
      "979 Train Loss 11632.209\n",
      "980 Train Loss 11601.719\n",
      "981 Train Loss 11552.837\n",
      "982 Train Loss 11499.717\n",
      "983 Train Loss 11446.669\n",
      "984 Train Loss 11416.141\n",
      "985 Train Loss 11386.574\n",
      "986 Train Loss 11361.981\n",
      "987 Train Loss 11345.031\n",
      "988 Train Loss 11324.001\n",
      "989 Train Loss 11289.994\n",
      "990 Train Loss 11246.973\n",
      "991 Train Loss 11191.944\n",
      "992 Train Loss 11143.817\n",
      "993 Train Loss 11118.046\n",
      "994 Train Loss 11096.322\n",
      "995 Train Loss 11080.056\n",
      "996 Train Loss 11066.244\n",
      "997 Train Loss 11050.482\n",
      "998 Train Loss 11018.786\n",
      "999 Train Loss 10965.013\n",
      "1000 Train Loss 10948.203\n",
      "1001 Train Loss 10927.255\n",
      "1002 Train Loss 10912.703\n",
      "1003 Train Loss 10903.236\n",
      "1004 Train Loss 10896.437\n",
      "1005 Train Loss 10890.793\n",
      "1006 Train Loss 10882.156\n",
      "1007 Train Loss 10872.358\n",
      "1008 Train Loss 10855.533\n",
      "1009 Train Loss 10832.708\n",
      "1010 Train Loss 10802.453\n",
      "1011 Train Loss 10806.618\n",
      "1012 Train Loss 10785.936\n",
      "1013 Train Loss 10759.273\n",
      "1014 Train Loss 10740.668\n",
      "1015 Train Loss 10695.579\n",
      "1016 Train Loss 10653.303\n",
      "1017 Train Loss 10568.625\n",
      "1018 Train Loss 10774.457\n",
      "1019 Train Loss 10533.644\n",
      "1020 Train Loss 10509.101\n",
      "1021 Train Loss 10532.402\n",
      "1022 Train Loss 10487.939\n",
      "1023 Train Loss 10471.943\n",
      "1024 Train Loss 10461.722\n",
      "1025 Train Loss 10451.682\n",
      "1026 Train Loss 10441.053\n",
      "1027 Train Loss 10413.258\n",
      "1028 Train Loss 10391.934\n",
      "1029 Train Loss 10358.406\n",
      "1030 Train Loss 10291.984\n",
      "1031 Train Loss 10237.338\n",
      "1032 Train Loss 12937.508\n",
      "1033 Train Loss 10162.058\n",
      "1034 Train Loss 10077.467\n",
      "1035 Train Loss 10063.834\n",
      "1036 Train Loss 10029.189\n",
      "1037 Train Loss 9985.984\n",
      "1038 Train Loss 9935.818\n",
      "1039 Train Loss 9864.573\n",
      "1040 Train Loss 9836.047\n",
      "1041 Train Loss 9787.969\n",
      "1042 Train Loss 9775.925\n",
      "1043 Train Loss 9756.113\n",
      "1044 Train Loss 9746.798\n",
      "1045 Train Loss 9730.975\n",
      "1046 Train Loss 9711.551\n",
      "1047 Train Loss 9700.736\n",
      "1048 Train Loss 9685.979\n",
      "1049 Train Loss 9655.507\n",
      "1050 Train Loss 9611.092\n",
      "1051 Train Loss 9913.15\n",
      "1052 Train Loss 9584.612\n",
      "1053 Train Loss 9537.869\n",
      "1054 Train Loss 9499.663\n",
      "1055 Train Loss 9463.906\n",
      "1056 Train Loss 9489.474\n",
      "1057 Train Loss 9438.154\n",
      "1058 Train Loss 9404.963\n",
      "1059 Train Loss 9326.377\n",
      "1060 Train Loss 9264.146\n",
      "1061 Train Loss 9188.198\n",
      "1062 Train Loss 9125.637\n",
      "1063 Train Loss 9010.07\n",
      "1064 Train Loss 8938.641\n",
      "1065 Train Loss 8855.537\n",
      "1066 Train Loss 8831.961\n",
      "1067 Train Loss 8835.837\n",
      "1068 Train Loss 8799.641\n",
      "1069 Train Loss 8837.701\n",
      "1070 Train Loss 8786.162\n",
      "1071 Train Loss 8785.14\n",
      "1072 Train Loss 8774.909\n",
      "1073 Train Loss 8767.973\n",
      "1074 Train Loss 8755.635\n",
      "1075 Train Loss 8745.168\n",
      "1076 Train Loss 8739.109\n",
      "1077 Train Loss 8732.056\n",
      "1078 Train Loss 8723.046\n",
      "1079 Train Loss 8715.545\n",
      "1080 Train Loss 8693.479\n",
      "1081 Train Loss 8674.283\n",
      "1082 Train Loss 8644.6\n",
      "1083 Train Loss 8604.002\n",
      "1084 Train Loss 8618.289\n",
      "1085 Train Loss 8582.781\n",
      "1086 Train Loss 8550.972\n",
      "1087 Train Loss 8542.829\n",
      "1088 Train Loss 8515.255\n",
      "1089 Train Loss 8501.773\n",
      "1090 Train Loss 8494.882\n",
      "1091 Train Loss 8485.908\n",
      "1092 Train Loss 8478.615\n",
      "1093 Train Loss 8468.903\n",
      "1094 Train Loss 8467.779\n",
      "1095 Train Loss 8464.49\n",
      "1096 Train Loss 8462.257\n",
      "1097 Train Loss 8458.775\n",
      "1098 Train Loss 8449.328\n",
      "1099 Train Loss 8437.246\n",
      "1100 Train Loss 8412.697\n",
      "1101 Train Loss 8395.267\n",
      "1102 Train Loss 8366.797\n",
      "1103 Train Loss 8348.541\n",
      "1104 Train Loss 8330.519\n",
      "1105 Train Loss 8316.604\n",
      "1106 Train Loss 8292.949\n",
      "1107 Train Loss 8273.939\n",
      "1108 Train Loss 8257.538\n",
      "1109 Train Loss 8252.105\n",
      "1110 Train Loss 8247.333\n",
      "1111 Train Loss 8246.993\n",
      "1112 Train Loss 8244.613\n",
      "1113 Train Loss 8243.007\n",
      "1114 Train Loss 8240.123\n",
      "1115 Train Loss 8235.588\n",
      "1116 Train Loss 8230.471\n",
      "1117 Train Loss 8225.636\n",
      "1118 Train Loss 8218.894\n",
      "1119 Train Loss 8207.634\n",
      "1120 Train Loss 8194.583\n",
      "1121 Train Loss 8183.9663\n",
      "1122 Train Loss 8173.509\n",
      "1123 Train Loss 8171.0254\n",
      "1124 Train Loss 8162.714\n",
      "1125 Train Loss 8158.912\n",
      "1126 Train Loss 8155.4746\n",
      "1127 Train Loss 8151.4863\n",
      "1128 Train Loss 8148.84\n",
      "1129 Train Loss 8144.5845\n",
      "1130 Train Loss 8137.066\n",
      "1131 Train Loss 8126.38\n",
      "1132 Train Loss 8113.2397\n",
      "1133 Train Loss 8102.99\n",
      "1134 Train Loss 8096.6416\n",
      "1135 Train Loss 8075.63\n",
      "1136 Train Loss 8063.212\n",
      "1137 Train Loss 8047.466\n",
      "1138 Train Loss 8065.047\n",
      "1139 Train Loss 8036.4014\n",
      "1140 Train Loss 8016.949\n",
      "1141 Train Loss 8008.3906\n",
      "1142 Train Loss 8000.078\n",
      "1143 Train Loss 7990.0176\n",
      "1144 Train Loss 7983.172\n",
      "1145 Train Loss 7979.014\n",
      "1146 Train Loss 7975.3574\n",
      "1147 Train Loss 7970.356\n",
      "1148 Train Loss 7965.436\n",
      "1149 Train Loss 7958.2607\n",
      "1150 Train Loss 7949.8027\n",
      "1151 Train Loss 7940.3486\n",
      "1152 Train Loss 7931.6846\n",
      "1153 Train Loss 7922.233\n",
      "1154 Train Loss 7911.3936\n",
      "1155 Train Loss 7893.341\n",
      "1156 Train Loss 7874.22\n",
      "1157 Train Loss 7849.5615\n",
      "1158 Train Loss 7831.7295\n",
      "1159 Train Loss 7823.831\n",
      "1160 Train Loss 7818.759\n",
      "1161 Train Loss 7799.574\n",
      "1162 Train Loss 7770.4297\n",
      "1163 Train Loss 7771.968\n",
      "1164 Train Loss 7746.8633\n",
      "1165 Train Loss 7720.913\n",
      "1166 Train Loss 7707.7783\n",
      "1167 Train Loss 7705.0464\n",
      "1168 Train Loss 7702.885\n",
      "1169 Train Loss 7697.786\n",
      "1170 Train Loss 7695.573\n",
      "1171 Train Loss 7688.242\n",
      "1172 Train Loss 7678.6543\n",
      "1173 Train Loss 7672.862\n",
      "1174 Train Loss 7661.1943\n",
      "1175 Train Loss 7649.0464\n",
      "1176 Train Loss 7636.497\n",
      "1177 Train Loss 7626.068\n",
      "1178 Train Loss 7614.944\n",
      "1179 Train Loss 7621.294\n",
      "1180 Train Loss 7607.7837\n",
      "1181 Train Loss 7590.381\n",
      "1182 Train Loss 7575.839\n",
      "1183 Train Loss 7558.449\n",
      "1184 Train Loss 7603.8027\n",
      "1185 Train Loss 7540.199\n",
      "1186 Train Loss 7507.1616\n",
      "1187 Train Loss 7487.932\n",
      "1188 Train Loss 7478.998\n",
      "1189 Train Loss 7503.104\n",
      "1190 Train Loss 7464.5293\n",
      "1191 Train Loss 7441.9688\n",
      "1192 Train Loss 7425.5874\n",
      "1193 Train Loss 7391.1025\n",
      "1194 Train Loss 7362.7305\n",
      "1195 Train Loss 7331.753\n",
      "1196 Train Loss 7286.6406\n",
      "1197 Train Loss 7243.368\n",
      "1198 Train Loss 7213.258\n",
      "1199 Train Loss 7208.218\n",
      "1200 Train Loss 7179.981\n",
      "1201 Train Loss 7135.297\n",
      "1202 Train Loss 7098.1543\n",
      "1203 Train Loss 7060.162\n",
      "1204 Train Loss 7084.1772\n",
      "1205 Train Loss 7006.514\n",
      "1206 Train Loss 6952.6895\n",
      "1207 Train Loss 6890.701\n",
      "1208 Train Loss 6863.445\n",
      "1209 Train Loss 6810.0327\n",
      "1210 Train Loss 6753.6055\n",
      "1211 Train Loss 6762.55\n",
      "1212 Train Loss 6715.823\n",
      "1213 Train Loss 6691.692\n",
      "1214 Train Loss 6672.139\n",
      "1215 Train Loss 6611.278\n",
      "1216 Train Loss 6591.125\n",
      "1217 Train Loss 6572.721\n",
      "1218 Train Loss 6560.418\n",
      "1219 Train Loss 6536.4683\n",
      "1220 Train Loss 6491.0156\n",
      "1221 Train Loss 6430.001\n",
      "1222 Train Loss 6475.8438\n",
      "1223 Train Loss 6398.552\n",
      "1224 Train Loss 6391.0186\n",
      "1225 Train Loss 6368.5947\n",
      "1226 Train Loss 6359.639\n",
      "1227 Train Loss 6343.994\n",
      "1228 Train Loss 6319.1143\n",
      "1229 Train Loss 6597.4106\n",
      "1230 Train Loss 6310.8945\n",
      "1231 Train Loss 6261.2373\n",
      "1232 Train Loss 6209.2803\n",
      "1233 Train Loss 6180.118\n",
      "1234 Train Loss 6135.1113\n",
      "1235 Train Loss 6129.635\n",
      "1236 Train Loss 6163.7285\n",
      "1237 Train Loss 6092.908\n",
      "1238 Train Loss 6065.4453\n",
      "1239 Train Loss 6041.2812\n",
      "1240 Train Loss 6015.6006\n",
      "1241 Train Loss 5971.204\n",
      "1242 Train Loss 5895.854\n",
      "1243 Train Loss 5823.037\n",
      "1244 Train Loss 6182.2495\n",
      "1245 Train Loss 5795.163\n",
      "1246 Train Loss 5790.8574\n",
      "1247 Train Loss 5769.061\n",
      "1248 Train Loss 5729.2314\n",
      "1249 Train Loss 5690.5176\n",
      "1250 Train Loss 5641.6787\n",
      "1251 Train Loss 5576.547\n",
      "1252 Train Loss 5570.729\n",
      "1253 Train Loss 5526.1475\n",
      "1254 Train Loss 5535.146\n",
      "1255 Train Loss 5481.334\n",
      "1256 Train Loss 5461.519\n",
      "1257 Train Loss 5444.0493\n",
      "1258 Train Loss 5429.247\n",
      "1259 Train Loss 5385.9307\n",
      "1260 Train Loss 5387.6914\n",
      "1261 Train Loss 5366.2646\n",
      "1262 Train Loss 5354.4473\n",
      "1263 Train Loss 5326.2886\n",
      "1264 Train Loss 5307.237\n",
      "1265 Train Loss 5278.3066\n",
      "1266 Train Loss 5251.8076\n",
      "1267 Train Loss 5226.9043\n",
      "1268 Train Loss 5220.67\n",
      "1269 Train Loss 5202.4385\n",
      "1270 Train Loss 5190.271\n",
      "1271 Train Loss 5176.754\n",
      "1272 Train Loss 5149.699\n",
      "1273 Train Loss 5120.352\n",
      "1274 Train Loss 5065.2056\n",
      "1275 Train Loss 5026.3984\n",
      "1276 Train Loss 5018.615\n",
      "1277 Train Loss 4962.24\n",
      "1278 Train Loss 4905.7495\n",
      "1279 Train Loss 5300.8877\n",
      "1280 Train Loss 4859.4355\n",
      "1281 Train Loss 4812.8853\n",
      "1282 Train Loss 4769.9214\n",
      "1283 Train Loss 4786.526\n",
      "1284 Train Loss 4751.9746\n",
      "1285 Train Loss 4742.5107\n",
      "1286 Train Loss 4711.462\n",
      "1287 Train Loss 4700.983\n",
      "1288 Train Loss 4688.8623\n",
      "1289 Train Loss 4677.619\n",
      "1290 Train Loss 4655.666\n",
      "1291 Train Loss 4627.269\n",
      "1292 Train Loss 4661.667\n",
      "1293 Train Loss 4615.892\n",
      "1294 Train Loss 4596.378\n",
      "1295 Train Loss 4598.5938\n",
      "1296 Train Loss 4586.456\n",
      "1297 Train Loss 4568.6562\n",
      "1298 Train Loss 4548.0273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1299 Train Loss 4503.747\n",
      "1300 Train Loss 4530.0273\n",
      "1301 Train Loss 4489.702\n",
      "1302 Train Loss 4452.343\n",
      "1303 Train Loss 4412.745\n",
      "1304 Train Loss 4410.017\n",
      "1305 Train Loss 4394.8135\n",
      "1306 Train Loss 4376.8164\n",
      "1307 Train Loss 4347.8213\n",
      "1308 Train Loss 4337.7153\n",
      "1309 Train Loss 4328.667\n",
      "1310 Train Loss 4315.027\n",
      "1311 Train Loss 4300.288\n",
      "1312 Train Loss 4277.675\n",
      "1313 Train Loss 4254.539\n",
      "1314 Train Loss 4278.0415\n",
      "1315 Train Loss 4240.254\n",
      "1316 Train Loss 4226.683\n",
      "1317 Train Loss 4203.492\n",
      "1318 Train Loss 4168.118\n",
      "1319 Train Loss 4132.08\n",
      "1320 Train Loss 4119.3726\n",
      "1321 Train Loss 4102.219\n",
      "1322 Train Loss 4085.9165\n",
      "1323 Train Loss 4074.7483\n",
      "1324 Train Loss 4063.727\n",
      "1325 Train Loss 4046.507\n",
      "1326 Train Loss 4036.6584\n",
      "1327 Train Loss 4026.2393\n",
      "1328 Train Loss 4010.5867\n",
      "1329 Train Loss 3992.0322\n",
      "1330 Train Loss 3987.941\n",
      "1331 Train Loss 3978.489\n",
      "1332 Train Loss 3971.554\n",
      "1333 Train Loss 3957.45\n",
      "1334 Train Loss 3951.2715\n",
      "1335 Train Loss 3944.4404\n",
      "1336 Train Loss 3933.4277\n",
      "1337 Train Loss 3931.1301\n",
      "1338 Train Loss 3923.1423\n",
      "1339 Train Loss 3914.9792\n",
      "1340 Train Loss 3905.231\n",
      "1341 Train Loss 3889.1978\n",
      "1342 Train Loss 3859.6118\n",
      "1343 Train Loss 3830.9478\n",
      "1344 Train Loss 4731.8105\n",
      "1345 Train Loss 3779.5872\n",
      "1346 Train Loss 3736.122\n",
      "1347 Train Loss 3797.9268\n",
      "1348 Train Loss 3701.8716\n",
      "1349 Train Loss 3684.359\n",
      "1350 Train Loss 3668.7742\n",
      "1351 Train Loss 3615.6514\n",
      "1352 Train Loss 3634.6333\n",
      "1353 Train Loss 3550.2996\n",
      "1354 Train Loss 3468.5554\n",
      "1355 Train Loss 3663.4695\n",
      "1356 Train Loss 3418.4946\n",
      "1357 Train Loss 3372.0127\n",
      "1358 Train Loss 3315.6323\n",
      "1359 Train Loss 3284.85\n",
      "1360 Train Loss 3366.2996\n",
      "1361 Train Loss 3247.3086\n",
      "1362 Train Loss 3209.5718\n",
      "1363 Train Loss 3187.9094\n",
      "1364 Train Loss 3184.2617\n",
      "1365 Train Loss 3168.043\n",
      "1366 Train Loss 3153.477\n",
      "1367 Train Loss 3139.0945\n",
      "1368 Train Loss 3084.1436\n",
      "1369 Train Loss 3052.2153\n",
      "1370 Train Loss 3011.4377\n",
      "1371 Train Loss 2984.7183\n",
      "1372 Train Loss 2987.3762\n",
      "1373 Train Loss 2969.629\n",
      "1374 Train Loss 2947.899\n",
      "1375 Train Loss 2924.324\n",
      "1376 Train Loss 2908.56\n",
      "1377 Train Loss 2893.4946\n",
      "1378 Train Loss 2880.5317\n",
      "1379 Train Loss 2873.1997\n",
      "1380 Train Loss 2864.393\n",
      "1381 Train Loss 2862.105\n",
      "1382 Train Loss 2858.9468\n",
      "1383 Train Loss 2857.6028\n",
      "1384 Train Loss 2855.4104\n",
      "1385 Train Loss 2852.1436\n",
      "1386 Train Loss 2846.2446\n",
      "1387 Train Loss 2838.233\n",
      "1388 Train Loss 2830.304\n",
      "1389 Train Loss 2831.685\n",
      "1390 Train Loss 2827.7432\n",
      "1391 Train Loss 2825.03\n",
      "1392 Train Loss 2822.6018\n",
      "1393 Train Loss 2817.8413\n",
      "1394 Train Loss 2810.7212\n",
      "1395 Train Loss 2802.4497\n",
      "1396 Train Loss 2856.2737\n",
      "1397 Train Loss 2799.306\n",
      "1398 Train Loss 2791.4805\n",
      "1399 Train Loss 2787.338\n",
      "1400 Train Loss 2784.4448\n",
      "1401 Train Loss 2783.9434\n",
      "1402 Train Loss 2782.809\n",
      "1403 Train Loss 2781.5386\n",
      "1404 Train Loss 2779.8838\n",
      "1405 Train Loss 2776.4124\n",
      "1406 Train Loss 2772.735\n",
      "1407 Train Loss 2767.7766\n",
      "1408 Train Loss 2762.7148\n",
      "1409 Train Loss 2750.691\n",
      "1410 Train Loss 2736.835\n",
      "1411 Train Loss 2723.3145\n",
      "1412 Train Loss 2721.9385\n",
      "1413 Train Loss 2714.234\n",
      "1414 Train Loss 2699.5317\n",
      "1415 Train Loss 2690.5183\n",
      "1416 Train Loss 2694.124\n",
      "1417 Train Loss 2683.304\n",
      "1418 Train Loss 2680.6746\n",
      "1419 Train Loss 2675.4312\n",
      "1420 Train Loss 2670.1099\n",
      "1421 Train Loss 2659.439\n",
      "1422 Train Loss 2649.662\n",
      "1423 Train Loss 2650.6147\n",
      "1424 Train Loss 2638.2324\n",
      "1425 Train Loss 2630.4668\n",
      "1426 Train Loss 2624.3896\n",
      "1427 Train Loss 2620.9207\n",
      "1428 Train Loss 2615.0876\n",
      "1429 Train Loss 2605.151\n",
      "1430 Train Loss 2591.0278\n",
      "1431 Train Loss 2716.8057\n",
      "1432 Train Loss 2587.2175\n",
      "1433 Train Loss 2580.3276\n",
      "1434 Train Loss 2573.9788\n",
      "1435 Train Loss 2569.4458\n",
      "1436 Train Loss 2563.9958\n",
      "1437 Train Loss 2561.2163\n",
      "1438 Train Loss 2558.8699\n",
      "1439 Train Loss 2559.4084\n",
      "1440 Train Loss 2547.347\n",
      "1441 Train Loss 2539.3562\n",
      "1442 Train Loss 2531.7773\n",
      "1443 Train Loss 2521.479\n",
      "1444 Train Loss 2508.4075\n",
      "1445 Train Loss 2495.702\n",
      "1446 Train Loss 2485.2925\n",
      "1447 Train Loss 2476.2031\n",
      "1448 Train Loss 2471.8174\n",
      "1449 Train Loss 2468.002\n",
      "1450 Train Loss 2466.0688\n",
      "1451 Train Loss 2463.9966\n",
      "1452 Train Loss 2462.4946\n",
      "1453 Train Loss 2460.19\n",
      "1454 Train Loss 2455.9092\n",
      "1455 Train Loss 2451.6855\n",
      "1456 Train Loss 2448.3525\n",
      "1457 Train Loss 2449.5657\n",
      "1458 Train Loss 2447.106\n",
      "1459 Train Loss 2445.9766\n",
      "1460 Train Loss 2444.7012\n",
      "1461 Train Loss 2443.248\n",
      "1462 Train Loss 2441.0696\n",
      "1463 Train Loss 2440.5244\n",
      "1464 Train Loss 2438.9692\n",
      "1465 Train Loss 2435.6738\n",
      "1466 Train Loss 2433.148\n",
      "1467 Train Loss 2430.732\n",
      "1468 Train Loss 2429.5703\n",
      "1469 Train Loss 2427.5957\n",
      "1470 Train Loss 2424.0762\n",
      "1471 Train Loss 2421.2026\n",
      "1472 Train Loss 2419.0127\n",
      "1473 Train Loss 2415.307\n",
      "1474 Train Loss 2410.4783\n",
      "1475 Train Loss 2403.1812\n",
      "1476 Train Loss 2392.2722\n",
      "1477 Train Loss 2404.9377\n",
      "1478 Train Loss 2382.0405\n",
      "1479 Train Loss 2365.568\n",
      "1480 Train Loss 2356.1387\n",
      "1481 Train Loss 2361.987\n",
      "1482 Train Loss 2351.1775\n",
      "1483 Train Loss 2355.7256\n",
      "1484 Train Loss 2350.324\n",
      "1485 Train Loss 2347.4976\n",
      "1486 Train Loss 2344.4146\n",
      "1487 Train Loss 2342.8333\n",
      "1488 Train Loss 2337.6406\n",
      "1489 Train Loss 2332.0347\n",
      "1490 Train Loss 2327.3464\n",
      "1491 Train Loss 2323.7737\n",
      "1492 Train Loss 2319.7117\n",
      "1493 Train Loss 2316.3381\n",
      "1494 Train Loss 2314.1172\n",
      "1495 Train Loss 2312.9302\n",
      "1496 Train Loss 2310.6255\n",
      "1497 Train Loss 2308.2458\n",
      "1498 Train Loss 2304.5955\n",
      "1499 Train Loss 2297.644\n",
      "1500 Train Loss 2290.74\n",
      "1501 Train Loss 2284.129\n",
      "1502 Train Loss 2271.8477\n",
      "1503 Train Loss 2253.7534\n",
      "1504 Train Loss 2273.067\n",
      "1505 Train Loss 2246.9404\n",
      "1506 Train Loss 2235.2422\n",
      "1507 Train Loss 2226.7512\n",
      "1508 Train Loss 2208.771\n",
      "1509 Train Loss 2198.9517\n",
      "1510 Train Loss 2188.2117\n",
      "1511 Train Loss 2190.2874\n",
      "1512 Train Loss 2181.636\n",
      "1513 Train Loss 2179.677\n",
      "1514 Train Loss 2176.497\n",
      "1515 Train Loss 2172.0015\n",
      "1516 Train Loss 2166.4702\n",
      "1517 Train Loss 2162.363\n",
      "1518 Train Loss 2159.2466\n",
      "1519 Train Loss 2155.4778\n",
      "1520 Train Loss 2149.928\n",
      "1521 Train Loss 2147.564\n",
      "1522 Train Loss 2144.744\n",
      "1523 Train Loss 2142.501\n",
      "1524 Train Loss 2139.067\n",
      "1525 Train Loss 2132.6687\n",
      "1526 Train Loss 2136.3003\n",
      "1527 Train Loss 2130.4058\n",
      "1528 Train Loss 2125.7314\n",
      "1529 Train Loss 2121.6099\n",
      "1530 Train Loss 2119.6584\n",
      "1531 Train Loss 2115.2761\n",
      "1532 Train Loss 2112.2014\n",
      "1533 Train Loss 2109.2466\n",
      "1534 Train Loss 2104.708\n",
      "1535 Train Loss 2099.8066\n",
      "1536 Train Loss 2089.3276\n",
      "1537 Train Loss 2077.2612\n",
      "1538 Train Loss 2267.9265\n",
      "1539 Train Loss 2071.636\n",
      "1540 Train Loss 2063.422\n",
      "1541 Train Loss 2054.7808\n",
      "1542 Train Loss 2049.0332\n",
      "1543 Train Loss 2045.6805\n",
      "1544 Train Loss 2042.127\n",
      "1545 Train Loss 2041.4558\n",
      "1546 Train Loss 2038.1392\n",
      "1547 Train Loss 2036.1342\n",
      "1548 Train Loss 2033.101\n",
      "1549 Train Loss 2029.8794\n",
      "1550 Train Loss 2026.7146\n",
      "1551 Train Loss 2022.371\n",
      "1552 Train Loss 2019.6294\n",
      "1553 Train Loss 2014.6917\n",
      "1554 Train Loss 2008.6165\n",
      "1555 Train Loss 2005.7832\n",
      "1556 Train Loss 1999.4727\n",
      "1557 Train Loss 2007.1913\n",
      "1558 Train Loss 1998.0365\n",
      "1559 Train Loss 1995.1135\n",
      "1560 Train Loss 1992.4594\n",
      "1561 Train Loss 1989.6683\n",
      "1562 Train Loss 1987.1422\n",
      "1563 Train Loss 1984.9275\n",
      "1564 Train Loss 1976.5607\n",
      "1565 Train Loss 1970.1825\n",
      "1566 Train Loss 1964.542\n",
      "1567 Train Loss 1990.7848\n",
      "1568 Train Loss 1963.586\n",
      "1569 Train Loss 1964.4962\n",
      "1570 Train Loss 1961.8708\n",
      "1571 Train Loss 1962.818\n",
      "1572 Train Loss 1961.0771\n",
      "1573 Train Loss 1959.6107\n",
      "1574 Train Loss 1958.069\n",
      "1575 Train Loss 1954.487\n",
      "1576 Train Loss 1951.1218\n",
      "1577 Train Loss 1948.1035\n",
      "1578 Train Loss 1945.3555\n",
      "1579 Train Loss 1942.1895\n",
      "1580 Train Loss 1939.1605\n",
      "1581 Train Loss 1935.0759\n",
      "1582 Train Loss 1931.6704\n",
      "1583 Train Loss 1927.9409\n",
      "1584 Train Loss 1926.3705\n",
      "1585 Train Loss 1923.8793\n",
      "1586 Train Loss 1922.8059\n",
      "1587 Train Loss 1921.1534\n",
      "1588 Train Loss 1929.2241\n",
      "1589 Train Loss 1919.8956\n",
      "1590 Train Loss 1923.8936\n",
      "1591 Train Loss 1918.4923\n",
      "1592 Train Loss 1918.8947\n",
      "1593 Train Loss 1917.0923\n",
      "1594 Train Loss 1922.531\n",
      "1595 Train Loss 1914.471\n",
      "1596 Train Loss 1910.4656\n",
      "1597 Train Loss 1905.6624\n",
      "1598 Train Loss 1902.0999\n",
      "1599 Train Loss 1894.0592\n",
      "1600 Train Loss 1889.345\n",
      "1601 Train Loss 1882.3971\n",
      "1602 Train Loss 1878.0308\n",
      "1603 Train Loss 1867.6495\n",
      "1604 Train Loss 1861.5532\n",
      "1605 Train Loss 1851.5514\n",
      "1606 Train Loss 1846.8888\n",
      "1607 Train Loss 1846.7917\n",
      "1608 Train Loss 1837.6394\n",
      "1609 Train Loss 1839.1696\n",
      "1610 Train Loss 1832.0873\n",
      "1611 Train Loss 1831.8823\n",
      "1612 Train Loss 1824.8541\n",
      "1613 Train Loss 1811.4512\n",
      "1614 Train Loss 1802.8217\n",
      "1615 Train Loss 1796.6304\n",
      "1616 Train Loss 1774.3679\n",
      "1617 Train Loss 1785.0674\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1618 Train Loss 1757.7804\n",
      "1619 Train Loss 1801.0085\n",
      "1620 Train Loss 1743.5076\n",
      "1621 Train Loss 1734.6136\n",
      "1622 Train Loss 1727.9968\n",
      "1623 Train Loss 1771.2646\n",
      "1624 Train Loss 1718.747\n",
      "1625 Train Loss 1779.7467\n",
      "1626 Train Loss 1705.4159\n",
      "1627 Train Loss 1682.3967\n",
      "1628 Train Loss 1666.519\n",
      "1629 Train Loss 1726.7596\n",
      "1630 Train Loss 1658.6455\n",
      "1631 Train Loss 1667.8875\n",
      "1632 Train Loss 1648.8423\n",
      "1633 Train Loss 1643.8167\n",
      "1634 Train Loss 1663.6079\n",
      "1635 Train Loss 1637.0369\n",
      "1636 Train Loss 1636.2067\n",
      "1637 Train Loss 1628.2773\n",
      "1638 Train Loss 1618.1052\n",
      "1639 Train Loss 1630.3677\n",
      "1640 Train Loss 1609.2622\n",
      "1641 Train Loss 1590.1476\n",
      "1642 Train Loss 1575.2649\n",
      "1643 Train Loss 1762.5123\n",
      "1644 Train Loss 1568.9443\n",
      "1645 Train Loss 1555.1682\n",
      "1646 Train Loss 1531.5905\n",
      "1647 Train Loss 1639.8354\n",
      "1648 Train Loss 1524.8557\n",
      "1649 Train Loss 1523.6108\n",
      "1650 Train Loss 1514.4045\n",
      "1651 Train Loss 1502.0613\n",
      "1652 Train Loss 1500.519\n",
      "1653 Train Loss 1496.2302\n",
      "1654 Train Loss 1486.7933\n",
      "1655 Train Loss 1482.7815\n",
      "1656 Train Loss 1480.407\n",
      "1657 Train Loss 1477.1011\n",
      "1658 Train Loss 1475.1515\n",
      "1659 Train Loss 1472.6616\n",
      "1660 Train Loss 1469.7764\n",
      "1661 Train Loss 1466.1846\n",
      "1662 Train Loss 1459.646\n",
      "1663 Train Loss 1456.2104\n",
      "1664 Train Loss 1453.1841\n",
      "1665 Train Loss 1450.3401\n",
      "1666 Train Loss 1446.929\n",
      "1667 Train Loss 1444.355\n",
      "1668 Train Loss 1441.1296\n",
      "1669 Train Loss 1434.969\n",
      "1670 Train Loss 1465.9666\n",
      "1671 Train Loss 1434.3547\n",
      "1672 Train Loss 1432.4634\n",
      "1673 Train Loss 1431.3293\n",
      "1674 Train Loss 1429.385\n",
      "1675 Train Loss 1428.2446\n",
      "1676 Train Loss 1425.6002\n",
      "1677 Train Loss 1428.4104\n",
      "1678 Train Loss 1423.7139\n",
      "1679 Train Loss 1420.0586\n",
      "1680 Train Loss 1415.8535\n",
      "1681 Train Loss 1413.6886\n",
      "1682 Train Loss 1410.926\n",
      "1683 Train Loss 1408.5244\n",
      "1684 Train Loss 1406.1833\n",
      "1685 Train Loss 1401.7415\n",
      "1686 Train Loss 1397.501\n",
      "1687 Train Loss 1389.476\n",
      "1688 Train Loss 1384.751\n",
      "1689 Train Loss 1379.4607\n",
      "1690 Train Loss 1377.0503\n",
      "1691 Train Loss 1374.9781\n",
      "1692 Train Loss 1372.9602\n",
      "1693 Train Loss 1369.771\n",
      "1694 Train Loss 1365.1365\n",
      "1695 Train Loss 1358.5559\n",
      "1696 Train Loss 1354.2274\n",
      "1697 Train Loss 1362.2205\n",
      "1698 Train Loss 1352.5674\n",
      "1699 Train Loss 1350.3221\n",
      "1700 Train Loss 1349.0265\n",
      "1701 Train Loss 1347.917\n",
      "1702 Train Loss 1347.0598\n",
      "1703 Train Loss 1346.1753\n",
      "1704 Train Loss 1344.113\n",
      "1705 Train Loss 1341.4801\n",
      "1706 Train Loss 1338.098\n",
      "1707 Train Loss 1333.4462\n",
      "1708 Train Loss 1328.71\n",
      "1709 Train Loss 1323.9613\n",
      "1710 Train Loss 1317.4363\n",
      "1711 Train Loss 1320.8159\n",
      "1712 Train Loss 1314.52\n",
      "1713 Train Loss 1309.8245\n",
      "1714 Train Loss 1306.7074\n",
      "1715 Train Loss 1305.3141\n",
      "1716 Train Loss 1302.6448\n",
      "1717 Train Loss 1300.7393\n",
      "1718 Train Loss 1298.9037\n",
      "1719 Train Loss 1296.901\n",
      "1720 Train Loss 1292.8898\n",
      "1721 Train Loss 1287.7279\n",
      "1722 Train Loss 1281.7649\n",
      "1723 Train Loss 1275.1239\n",
      "1724 Train Loss 1270.3467\n",
      "1725 Train Loss 1265.7825\n",
      "1726 Train Loss 1262.7603\n",
      "1727 Train Loss 1259.3275\n",
      "1728 Train Loss 1257.4648\n",
      "1729 Train Loss 1255.0122\n",
      "1730 Train Loss 1252.0505\n",
      "1731 Train Loss 1251.9929\n",
      "1732 Train Loss 1250.7493\n",
      "1733 Train Loss 1248.6772\n",
      "1734 Train Loss 1247.0667\n",
      "1735 Train Loss 1245.4801\n",
      "1736 Train Loss 1243.9751\n",
      "1737 Train Loss 1242.8894\n",
      "1738 Train Loss 1241.989\n",
      "1739 Train Loss 1241.4434\n",
      "1740 Train Loss 1241.0801\n",
      "1741 Train Loss 1239.3213\n",
      "1742 Train Loss 1235.9998\n",
      "1743 Train Loss 1233.3811\n",
      "1744 Train Loss 1230.3026\n",
      "1745 Train Loss 1229.8589\n",
      "1746 Train Loss 1228.9792\n",
      "1747 Train Loss 1227.4412\n",
      "1748 Train Loss 1226.55\n",
      "1749 Train Loss 1226.1082\n",
      "1750 Train Loss 1225.5957\n",
      "1751 Train Loss 1224.9543\n",
      "1752 Train Loss 1224.2356\n",
      "1753 Train Loss 1221.688\n",
      "1754 Train Loss 1219.9365\n",
      "1755 Train Loss 1217.8224\n",
      "1756 Train Loss 1215.3109\n",
      "1757 Train Loss 1214.3445\n",
      "1758 Train Loss 1213.2498\n",
      "1759 Train Loss 1212.3655\n",
      "1760 Train Loss 1214.4395\n",
      "1761 Train Loss 1211.8032\n",
      "1762 Train Loss 1210.5804\n",
      "1763 Train Loss 1209.7341\n",
      "1764 Train Loss 1208.1348\n",
      "1765 Train Loss 1206.1456\n",
      "1766 Train Loss 1205.8372\n",
      "1767 Train Loss 1197.0618\n",
      "1768 Train Loss 1196.3488\n",
      "1769 Train Loss 1192.8181\n",
      "1770 Train Loss 1195.6627\n",
      "1771 Train Loss 1183.0625\n",
      "1772 Train Loss 1198.3335\n",
      "1773 Train Loss 1176.0679\n",
      "1774 Train Loss 1203.1646\n",
      "1775 Train Loss 1158.8794\n",
      "1776 Train Loss 1856.7806\n",
      "1777 Train Loss 1152.2725\n",
      "1778 Train Loss 1142.0873\n",
      "1779 Train Loss 1131.7657\n",
      "1780 Train Loss 1126.4559\n",
      "1781 Train Loss 1117.537\n",
      "1782 Train Loss 1140.362\n",
      "1783 Train Loss 1114.648\n",
      "1784 Train Loss 1107.7258\n",
      "1785 Train Loss 1108.1616\n",
      "1786 Train Loss 1105.4119\n",
      "1787 Train Loss 1110.4476\n",
      "1788 Train Loss 1103.1256\n",
      "1789 Train Loss 1103.2502\n",
      "1790 Train Loss 1101.1039\n",
      "1791 Train Loss 1098.4839\n",
      "1792 Train Loss 1096.3975\n",
      "1793 Train Loss 1107.2971\n",
      "1794 Train Loss 1095.2449\n",
      "1795 Train Loss 1093.4266\n",
      "1796 Train Loss 1092.147\n",
      "1797 Train Loss 1090.6976\n",
      "1798 Train Loss 1089.4595\n",
      "1799 Train Loss 1089.1155\n",
      "1800 Train Loss 1087.8103\n",
      "1801 Train Loss 1086.2773\n",
      "1802 Train Loss 1083.6531\n",
      "1803 Train Loss 1079.6815\n",
      "1804 Train Loss 1077.6228\n",
      "1805 Train Loss 1074.8342\n",
      "1806 Train Loss 1073.0212\n",
      "1807 Train Loss 1070.4991\n",
      "1808 Train Loss 1067.4735\n",
      "1809 Train Loss 1063.0428\n",
      "1810 Train Loss 1059.758\n",
      "1811 Train Loss 1053.5947\n",
      "1812 Train Loss 1051.6393\n",
      "1813 Train Loss 1049.5477\n",
      "1814 Train Loss 1053.1622\n",
      "1815 Train Loss 1048.5479\n",
      "1816 Train Loss 1051.1036\n",
      "1817 Train Loss 1047.731\n",
      "1818 Train Loss 1046.9729\n",
      "1819 Train Loss 1045.9448\n",
      "1820 Train Loss 1044.8765\n",
      "1821 Train Loss 1043.9247\n",
      "1822 Train Loss 1042.4852\n",
      "1823 Train Loss 1040.6661\n",
      "1824 Train Loss 1038.7122\n",
      "1825 Train Loss 1038.5522\n",
      "1826 Train Loss 1037.9277\n",
      "1827 Train Loss 1037.7579\n",
      "1828 Train Loss 1037.3704\n",
      "1829 Train Loss 1037.2825\n",
      "1830 Train Loss 1036.9725\n",
      "1831 Train Loss 1036.5043\n",
      "1832 Train Loss 1035.8345\n",
      "1833 Train Loss 1035.083\n",
      "1834 Train Loss 1034.0161\n",
      "1835 Train Loss 1032.5657\n",
      "1836 Train Loss 1031.532\n",
      "1837 Train Loss 1029.7109\n",
      "1838 Train Loss 1029.2585\n",
      "1839 Train Loss 1028.6958\n",
      "1840 Train Loss 1028.3864\n",
      "1841 Train Loss 1028.1592\n",
      "1842 Train Loss 1027.9592\n",
      "1843 Train Loss 1027.523\n",
      "1844 Train Loss 1026.9236\n",
      "1845 Train Loss 1026.447\n",
      "1846 Train Loss 1026.138\n",
      "1847 Train Loss 1025.7041\n",
      "1848 Train Loss 1025.053\n",
      "1849 Train Loss 1024.3353\n",
      "1850 Train Loss 1023.06256\n",
      "1851 Train Loss 1022.0146\n",
      "1852 Train Loss 1021.25085\n",
      "1853 Train Loss 1019.9963\n",
      "1854 Train Loss 1019.8869\n",
      "1855 Train Loss 1018.72687\n",
      "1856 Train Loss 1018.09155\n",
      "1857 Train Loss 1016.6351\n",
      "1858 Train Loss 1015.5145\n",
      "1859 Train Loss 1014.2743\n",
      "1860 Train Loss 1013.00024\n",
      "1861 Train Loss 1012.30725\n",
      "1862 Train Loss 1010.7695\n",
      "1863 Train Loss 1009.78394\n",
      "1864 Train Loss 1008.90894\n",
      "1865 Train Loss 1007.68994\n",
      "1866 Train Loss 1006.8791\n",
      "1867 Train Loss 1005.7501\n",
      "1868 Train Loss 1004.8721\n",
      "1869 Train Loss 1004.40894\n",
      "1870 Train Loss 1002.6572\n",
      "1871 Train Loss 1000.4281\n",
      "1872 Train Loss 997.4309\n",
      "1873 Train Loss 995.81415\n",
      "1874 Train Loss 1008.55743\n",
      "1875 Train Loss 994.0808\n",
      "1876 Train Loss 994.05994\n",
      "1877 Train Loss 993.03467\n",
      "1878 Train Loss 991.1015\n",
      "1879 Train Loss 990.03064\n",
      "1880 Train Loss 989.1835\n",
      "1881 Train Loss 987.9857\n",
      "1882 Train Loss 987.9307\n",
      "1883 Train Loss 987.22473\n",
      "1884 Train Loss 986.7324\n",
      "1885 Train Loss 986.50006\n",
      "1886 Train Loss 986.4595\n",
      "1887 Train Loss 986.3921\n",
      "1888 Train Loss 986.2152\n",
      "1889 Train Loss 985.7843\n",
      "1890 Train Loss 985.5616\n",
      "1891 Train Loss 985.0824\n",
      "1892 Train Loss 984.87103\n",
      "1893 Train Loss 984.1753\n",
      "1894 Train Loss 983.38354\n",
      "1895 Train Loss 983.3043\n",
      "1896 Train Loss 983.0848\n",
      "1897 Train Loss 982.6669\n",
      "1898 Train Loss 988.43506\n",
      "1899 Train Loss 982.6022\n",
      "1900 Train Loss 982.06104\n",
      "1901 Train Loss 981.3967\n",
      "1902 Train Loss 981.0233\n",
      "1903 Train Loss 980.4748\n",
      "1904 Train Loss 979.7489\n",
      "1905 Train Loss 1005.891\n",
      "1906 Train Loss 979.20435\n",
      "1907 Train Loss 977.04407\n",
      "1908 Train Loss 975.4232\n",
      "1909 Train Loss 973.3833\n",
      "1910 Train Loss 993.1145\n",
      "1911 Train Loss 971.4979\n",
      "1912 Train Loss 968.3121\n",
      "1913 Train Loss 964.756\n",
      "1914 Train Loss 961.0935\n",
      "1915 Train Loss 954.9176\n",
      "1916 Train Loss 973.77124\n",
      "1917 Train Loss 949.98413\n",
      "1918 Train Loss 941.7041\n",
      "1919 Train Loss 929.00415\n",
      "1920 Train Loss 915.18994\n",
      "1921 Train Loss 1017.4368\n",
      "1922 Train Loss 908.7494\n",
      "1923 Train Loss 911.32434\n",
      "1924 Train Loss 900.4372\n",
      "1925 Train Loss 919.61755\n",
      "1926 Train Loss 893.8473\n",
      "1927 Train Loss 889.5615\n",
      "1928 Train Loss 907.54016\n",
      "1929 Train Loss 882.37823\n",
      "1930 Train Loss 881.70874\n",
      "1931 Train Loss 875.6885\n",
      "1932 Train Loss 871.3967\n",
      "1933 Train Loss 885.8257\n",
      "1934 Train Loss 864.8707\n",
      "1935 Train Loss 865.4447\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1936 Train Loss 860.99097\n",
      "1937 Train Loss 856.7916\n",
      "1938 Train Loss 851.5283\n",
      "1939 Train Loss 850.3093\n",
      "1940 Train Loss 846.9949\n",
      "1941 Train Loss 841.82495\n",
      "1942 Train Loss 836.44415\n",
      "1943 Train Loss 833.725\n",
      "1944 Train Loss 833.2222\n",
      "1945 Train Loss 830.67065\n",
      "1946 Train Loss 827.7924\n",
      "1947 Train Loss 826.3598\n",
      "1948 Train Loss 822.03125\n",
      "1949 Train Loss 819.8266\n",
      "1950 Train Loss 816.54083\n",
      "1951 Train Loss 812.67737\n",
      "1952 Train Loss 810.6492\n",
      "1953 Train Loss 808.7107\n",
      "1954 Train Loss 807.4446\n",
      "1955 Train Loss 805.7292\n",
      "1956 Train Loss 804.16125\n",
      "1957 Train Loss 802.5356\n",
      "1958 Train Loss 800.8655\n",
      "1959 Train Loss 803.54175\n",
      "1960 Train Loss 800.1803\n",
      "1961 Train Loss 799.8125\n",
      "1962 Train Loss 798.5359\n",
      "1963 Train Loss 798.28296\n",
      "1964 Train Loss 797.4164\n",
      "1965 Train Loss 796.4952\n",
      "1966 Train Loss 796.09033\n",
      "1967 Train Loss 795.6293\n",
      "1968 Train Loss 795.0603\n",
      "1969 Train Loss 794.24786\n",
      "1970 Train Loss 793.6173\n",
      "1971 Train Loss 792.63763\n",
      "1972 Train Loss 791.7179\n",
      "1973 Train Loss 791.0017\n",
      "1974 Train Loss 790.30896\n",
      "1975 Train Loss 790.2196\n",
      "1976 Train Loss 790.0056\n",
      "1977 Train Loss 789.871\n",
      "1978 Train Loss 789.6583\n",
      "1979 Train Loss 789.4049\n",
      "1980 Train Loss 789.277\n",
      "1981 Train Loss 789.082\n",
      "1982 Train Loss 789.12244\n",
      "1983 Train Loss 789.12286\n",
      "1984 Train Loss 789.1727\n",
      "1985 Train Loss 789.0727\n",
      "1986 Train Loss 789.16895\n",
      "1987 Train Loss 789.1388\n",
      "1988 Train Loss 789.0777\n",
      "1989 Train Loss 788.98834\n",
      "1990 Train Loss 788.9223\n",
      "1991 Train Loss 788.6741\n",
      "1992 Train Loss 788.33514\n",
      "1993 Train Loss 788.0879\n",
      "1994 Train Loss 787.6006\n",
      "1995 Train Loss 787.2218\n",
      "1996 Train Loss 786.79144\n",
      "1997 Train Loss 786.3412\n",
      "1998 Train Loss 785.9067\n",
      "1999 Train Loss 785.4715\n",
      "2000 Train Loss 785.0615\n",
      "2001 Train Loss 784.8191\n",
      "2002 Train Loss 784.56104\n",
      "2003 Train Loss 784.0925\n",
      "2004 Train Loss 783.66626\n",
      "2005 Train Loss 783.3867\n",
      "2006 Train Loss 783.193\n",
      "2007 Train Loss 782.91016\n",
      "2008 Train Loss 782.7362\n",
      "2009 Train Loss 782.35547\n",
      "2010 Train Loss 782.5303\n",
      "2011 Train Loss 782.4203\n",
      "2012 Train Loss 782.3609\n",
      "2013 Train Loss 782.3391\n",
      "2014 Train Loss 782.3374\n",
      "2015 Train Loss 782.405\n",
      "2016 Train Loss 782.2162\n",
      "2017 Train Loss 782.2213\n",
      "2018 Train Loss 782.213\n",
      "2019 Train Loss 782.2224\n",
      "2020 Train Loss 782.57635\n",
      "2021 Train Loss 782.30853\n",
      "2022 Train Loss 782.449\n",
      "2023 Train Loss 782.28064\n",
      "2024 Train Loss 782.29535\n",
      "2025 Train Loss 782.213\n",
      "2026 Train Loss 782.57635\n",
      "2027 Train Loss 782.30853\n",
      "2028 Train Loss 782.449\n",
      "2029 Train Loss 782.28064\n",
      "2030 Train Loss 782.29535\n",
      "2031 Train Loss 782.213\n",
      "2032 Train Loss 782.57635\n",
      "2033 Train Loss 782.30853\n",
      "2034 Train Loss 782.449\n",
      "2035 Train Loss 782.28064\n",
      "2036 Train Loss 782.29535\n",
      "2037 Train Loss 782.213\n",
      "2038 Train Loss 782.57635\n",
      "2039 Train Loss 782.30853\n",
      "2040 Train Loss 782.449\n",
      "2041 Train Loss 782.28064\n",
      "2042 Train Loss 782.29535\n",
      "2043 Train Loss 782.213\n",
      "2044 Train Loss 782.57635\n",
      "2045 Train Loss 782.30853\n",
      "2046 Train Loss 782.449\n",
      "2047 Train Loss 782.28064\n",
      "2048 Train Loss 782.29535\n",
      "2049 Train Loss 782.213\n",
      "2050 Train Loss 782.57635\n",
      "2051 Train Loss 782.30853\n",
      "2052 Train Loss 782.449\n",
      "2053 Train Loss 782.28064\n",
      "2054 Train Loss 782.29535\n",
      "2055 Train Loss 782.213\n",
      "2056 Train Loss 782.57635\n",
      "2057 Train Loss 782.30853\n",
      "2058 Train Loss 782.449\n",
      "2059 Train Loss 782.28064\n",
      "2060 Train Loss 782.29535\n",
      "2061 Train Loss 782.213\n",
      "2062 Train Loss 782.57635\n",
      "2063 Train Loss 782.30853\n",
      "2064 Train Loss 782.449\n",
      "2065 Train Loss 782.28064\n",
      "2066 Train Loss 782.29535\n",
      "2067 Train Loss 782.213\n",
      "2068 Train Loss 782.57635\n",
      "2069 Train Loss 782.30853\n",
      "2070 Train Loss 782.449\n",
      "2071 Train Loss 782.28064\n",
      "2072 Train Loss 782.29535\n",
      "2073 Train Loss 782.213\n",
      "2074 Train Loss 782.57635\n",
      "2075 Train Loss 782.30853\n",
      "2076 Train Loss 782.449\n",
      "2077 Train Loss 782.28064\n",
      "2078 Train Loss 782.29535\n",
      "2079 Train Loss 782.213\n",
      "2080 Train Loss 782.57635\n",
      "2081 Train Loss 782.30853\n",
      "2082 Train Loss 782.449\n",
      "2083 Train Loss 782.28064\n",
      "2084 Train Loss 782.29535\n",
      "2085 Train Loss 782.213\n",
      "2086 Train Loss 782.57635\n",
      "2087 Train Loss 782.30853\n",
      "2088 Train Loss 782.449\n",
      "2089 Train Loss 782.28064\n",
      "2090 Train Loss 782.29535\n",
      "2091 Train Loss 782.213\n",
      "2092 Train Loss 782.57635\n",
      "2093 Train Loss 782.30853\n",
      "2094 Train Loss 782.449\n",
      "2095 Train Loss 782.28064\n",
      "2096 Train Loss 782.29535\n",
      "2097 Train Loss 782.213\n",
      "2098 Train Loss 782.57635\n",
      "2099 Train Loss 782.30853\n",
      "2100 Train Loss 782.449\n",
      "2101 Train Loss 782.28064\n",
      "2102 Train Loss 782.29535\n",
      "2103 Train Loss 782.213\n",
      "2104 Train Loss 782.57635\n",
      "2105 Train Loss 782.30853\n",
      "2106 Train Loss 782.449\n",
      "2107 Train Loss 782.28064\n",
      "2108 Train Loss 782.29535\n",
      "2109 Train Loss 782.213\n",
      "2110 Train Loss 782.57635\n",
      "2111 Train Loss 782.30853\n",
      "2112 Train Loss 782.449\n",
      "2113 Train Loss 782.28064\n",
      "2114 Train Loss 782.29535\n",
      "2115 Train Loss 782.213\n",
      "2116 Train Loss 782.57635\n",
      "2117 Train Loss 782.30853\n",
      "2118 Train Loss 782.449\n",
      "2119 Train Loss 782.28064\n",
      "2120 Train Loss 782.29535\n",
      "2121 Train Loss 782.213\n",
      "2122 Train Loss 782.57635\n",
      "2123 Train Loss 782.30853\n",
      "2124 Train Loss 782.449\n",
      "2125 Train Loss 782.28064\n",
      "2126 Train Loss 782.29535\n",
      "2127 Train Loss 782.213\n",
      "2128 Train Loss 782.57635\n",
      "2129 Train Loss 782.30853\n",
      "2130 Train Loss 782.449\n",
      "2131 Train Loss 782.28064\n",
      "2132 Train Loss 782.29535\n",
      "2133 Train Loss 782.213\n",
      "2134 Train Loss 782.57635\n",
      "2135 Train Loss 782.30853\n",
      "2136 Train Loss 782.449\n",
      "2137 Train Loss 782.28064\n",
      "2138 Train Loss 782.29535\n",
      "2139 Train Loss 782.213\n",
      "2140 Train Loss 782.57635\n",
      "2141 Train Loss 782.30853\n",
      "2142 Train Loss 782.449\n",
      "2143 Train Loss 782.28064\n",
      "2144 Train Loss 782.29535\n",
      "2145 Train Loss 782.213\n",
      "2146 Train Loss 782.57635\n",
      "2147 Train Loss 782.30853\n",
      "2148 Train Loss 782.449\n",
      "2149 Train Loss 782.28064\n",
      "2150 Train Loss 782.29535\n",
      "2151 Train Loss 782.213\n",
      "2152 Train Loss 782.57635\n",
      "2153 Train Loss 782.30853\n",
      "2154 Train Loss 782.449\n",
      "2155 Train Loss 782.28064\n",
      "2156 Train Loss 782.29535\n",
      "2157 Train Loss 782.213\n",
      "2158 Train Loss 782.57635\n",
      "2159 Train Loss 782.30853\n",
      "2160 Train Loss 782.449\n",
      "2161 Train Loss 782.28064\n",
      "2162 Train Loss 782.29535\n",
      "2163 Train Loss 782.213\n",
      "2164 Train Loss 782.57635\n",
      "2165 Train Loss 782.30853\n",
      "2166 Train Loss 782.449\n",
      "2167 Train Loss 782.28064\n",
      "2168 Train Loss 782.29535\n",
      "2169 Train Loss 782.213\n",
      "2170 Train Loss 782.57635\n",
      "2171 Train Loss 782.30853\n",
      "2172 Train Loss 782.449\n",
      "2173 Train Loss 782.28064\n",
      "2174 Train Loss 782.29535\n",
      "2175 Train Loss 782.213\n",
      "2176 Train Loss 782.57635\n",
      "2177 Train Loss 782.30853\n",
      "2178 Train Loss 782.449\n",
      "2179 Train Loss 782.28064\n",
      "2180 Train Loss 782.29535\n",
      "2181 Train Loss 782.213\n",
      "2182 Train Loss 782.57635\n",
      "2183 Train Loss 782.30853\n",
      "2184 Train Loss 782.449\n",
      "2185 Train Loss 782.28064\n",
      "2186 Train Loss 782.29535\n",
      "2187 Train Loss 782.213\n",
      "2188 Train Loss 782.57635\n",
      "2189 Train Loss 782.30853\n",
      "2190 Train Loss 782.449\n",
      "2191 Train Loss 782.28064\n",
      "2192 Train Loss 782.29535\n",
      "2193 Train Loss 782.213\n",
      "2194 Train Loss 782.57635\n",
      "2195 Train Loss 782.30853\n",
      "2196 Train Loss 782.449\n",
      "2197 Train Loss 782.28064\n",
      "2198 Train Loss 782.29535\n",
      "2199 Train Loss 782.213\n",
      "2200 Train Loss 782.57635\n",
      "2201 Train Loss 782.30853\n",
      "2202 Train Loss 782.449\n",
      "2203 Train Loss 782.28064\n",
      "2204 Train Loss 782.29535\n",
      "2205 Train Loss 782.213\n",
      "2206 Train Loss 782.57635\n",
      "2207 Train Loss 782.30853\n",
      "2208 Train Loss 782.449\n",
      "2209 Train Loss 782.28064\n",
      "2210 Train Loss 782.29535\n",
      "2211 Train Loss 782.213\n",
      "2212 Train Loss 782.57635\n",
      "2213 Train Loss 782.30853\n",
      "2214 Train Loss 782.449\n",
      "2215 Train Loss 782.28064\n",
      "2216 Train Loss 782.29535\n",
      "2217 Train Loss 782.213\n",
      "2218 Train Loss 782.57635\n",
      "2219 Train Loss 782.30853\n",
      "2220 Train Loss 782.449\n",
      "2221 Train Loss 782.28064\n",
      "2222 Train Loss 782.29535\n",
      "2223 Train Loss 782.213\n",
      "2224 Train Loss 782.57635\n",
      "2225 Train Loss 782.30853\n",
      "2226 Train Loss 782.449\n",
      "2227 Train Loss 782.28064\n",
      "2228 Train Loss 782.29535\n",
      "2229 Train Loss 782.213\n",
      "2230 Train Loss 782.57635\n",
      "2231 Train Loss 782.30853\n",
      "2232 Train Loss 782.449\n",
      "2233 Train Loss 782.28064\n",
      "2234 Train Loss 782.29535\n",
      "2235 Train Loss 782.213\n",
      "2236 Train Loss 782.57635\n",
      "2237 Train Loss 782.30853\n",
      "2238 Train Loss 782.449\n",
      "2239 Train Loss 782.28064\n",
      "2240 Train Loss 782.29535\n",
      "2241 Train Loss 782.213\n",
      "2242 Train Loss 782.57635\n",
      "2243 Train Loss 782.30853\n",
      "2244 Train Loss 782.449\n",
      "2245 Train Loss 782.28064\n",
      "2246 Train Loss 782.29535\n",
      "2247 Train Loss 782.213\n",
      "2248 Train Loss 782.57635\n",
      "2249 Train Loss 782.30853\n",
      "2250 Train Loss 782.449\n",
      "2251 Train Loss 782.28064\n",
      "2252 Train Loss 782.29535\n",
      "2253 Train Loss 782.213\n",
      "2254 Train Loss 782.57635\n",
      "2255 Train Loss 782.30853\n",
      "2256 Train Loss 782.449\n",
      "2257 Train Loss 782.28064\n",
      "2258 Train Loss 782.29535\n",
      "2259 Train Loss 782.213\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2260 Train Loss 782.57635\n",
      "2261 Train Loss 782.30853\n",
      "2262 Train Loss 782.449\n",
      "2263 Train Loss 782.28064\n",
      "2264 Train Loss 782.29535\n",
      "2265 Train Loss 782.213\n",
      "2266 Train Loss 782.57635\n",
      "2267 Train Loss 782.30853\n",
      "2268 Train Loss 782.449\n",
      "2269 Train Loss 782.28064\n",
      "2270 Train Loss 782.29535\n",
      "2271 Train Loss 782.213\n",
      "2272 Train Loss 782.57635\n",
      "2273 Train Loss 782.30853\n",
      "2274 Train Loss 782.449\n",
      "2275 Train Loss 782.28064\n",
      "2276 Train Loss 782.29535\n",
      "2277 Train Loss 782.213\n",
      "2278 Train Loss 782.57635\n",
      "2279 Train Loss 782.30853\n",
      "2280 Train Loss 782.449\n",
      "2281 Train Loss 782.28064\n",
      "2282 Train Loss 782.29535\n",
      "2283 Train Loss 782.213\n",
      "2284 Train Loss 782.57635\n",
      "2285 Train Loss 782.30853\n",
      "2286 Train Loss 782.449\n",
      "2287 Train Loss 782.28064\n",
      "2288 Train Loss 782.29535\n",
      "2289 Train Loss 782.213\n",
      "2290 Train Loss 782.57635\n",
      "2291 Train Loss 782.30853\n",
      "2292 Train Loss 782.449\n",
      "2293 Train Loss 782.28064\n",
      "2294 Train Loss 782.29535\n",
      "2295 Train Loss 782.213\n",
      "2296 Train Loss 782.57635\n",
      "2297 Train Loss 782.30853\n",
      "2298 Train Loss 782.449\n",
      "2299 Train Loss 782.28064\n",
      "2300 Train Loss 782.29535\n",
      "2301 Train Loss 782.213\n",
      "2302 Train Loss 782.57635\n",
      "2303 Train Loss 782.30853\n",
      "2304 Train Loss 782.449\n",
      "2305 Train Loss 782.28064\n",
      "2306 Train Loss 782.29535\n",
      "2307 Train Loss 782.213\n",
      "2308 Train Loss 782.57635\n",
      "2309 Train Loss 782.30853\n",
      "2310 Train Loss 782.449\n",
      "2311 Train Loss 782.28064\n",
      "2312 Train Loss 782.29535\n",
      "2313 Train Loss 782.213\n",
      "2314 Train Loss 782.57635\n",
      "2315 Train Loss 782.30853\n",
      "2316 Train Loss 782.449\n",
      "2317 Train Loss 782.28064\n",
      "2318 Train Loss 782.29535\n",
      "2319 Train Loss 782.213\n",
      "2320 Train Loss 782.57635\n",
      "2321 Train Loss 782.30853\n",
      "2322 Train Loss 782.449\n",
      "2323 Train Loss 782.28064\n",
      "2324 Train Loss 782.29535\n",
      "2325 Train Loss 782.213\n",
      "2326 Train Loss 782.57635\n",
      "2327 Train Loss 782.30853\n",
      "2328 Train Loss 782.449\n",
      "2329 Train Loss 782.28064\n",
      "2330 Train Loss 782.29535\n",
      "2331 Train Loss 782.213\n",
      "2332 Train Loss 782.57635\n",
      "2333 Train Loss 782.30853\n",
      "2334 Train Loss 782.449\n",
      "2335 Train Loss 782.28064\n",
      "2336 Train Loss 782.29535\n",
      "2337 Train Loss 782.213\n",
      "2338 Train Loss 782.57635\n",
      "2339 Train Loss 782.30853\n",
      "2340 Train Loss 782.449\n",
      "2341 Train Loss 782.28064\n",
      "2342 Train Loss 782.29535\n",
      "2343 Train Loss 782.213\n",
      "2344 Train Loss 782.57635\n",
      "2345 Train Loss 782.30853\n",
      "2346 Train Loss 782.449\n",
      "2347 Train Loss 782.28064\n",
      "2348 Train Loss 782.29535\n",
      "2349 Train Loss 782.213\n",
      "2350 Train Loss 782.57635\n",
      "2351 Train Loss 782.30853\n",
      "2352 Train Loss 782.449\n",
      "2353 Train Loss 782.28064\n",
      "2354 Train Loss 782.29535\n",
      "2355 Train Loss 782.213\n",
      "2356 Train Loss 782.57635\n",
      "2357 Train Loss 782.30853\n",
      "2358 Train Loss 782.449\n",
      "2359 Train Loss 782.28064\n",
      "2360 Train Loss 782.29535\n",
      "2361 Train Loss 782.213\n",
      "2362 Train Loss 782.57635\n",
      "2363 Train Loss 782.30853\n",
      "2364 Train Loss 782.449\n",
      "2365 Train Loss 782.28064\n",
      "2366 Train Loss 782.29535\n",
      "2367 Train Loss 782.213\n",
      "2368 Train Loss 782.57635\n",
      "2369 Train Loss 782.30853\n",
      "2370 Train Loss 782.449\n",
      "2371 Train Loss 782.28064\n",
      "2372 Train Loss 782.29535\n",
      "2373 Train Loss 782.213\n",
      "2374 Train Loss 782.57635\n",
      "2375 Train Loss 782.30853\n",
      "2376 Train Loss 782.449\n",
      "2377 Train Loss 782.28064\n",
      "2378 Train Loss 782.29535\n",
      "2379 Train Loss 782.213\n",
      "2380 Train Loss 782.57635\n",
      "2381 Train Loss 782.30853\n",
      "2382 Train Loss 782.449\n",
      "2383 Train Loss 782.28064\n",
      "2384 Train Loss 782.29535\n",
      "2385 Train Loss 782.213\n",
      "2386 Train Loss 782.57635\n",
      "2387 Train Loss 782.30853\n",
      "2388 Train Loss 782.449\n",
      "2389 Train Loss 782.28064\n",
      "2390 Train Loss 782.29535\n",
      "2391 Train Loss 782.213\n",
      "2392 Train Loss 782.57635\n",
      "2393 Train Loss 782.30853\n",
      "2394 Train Loss 782.449\n",
      "2395 Train Loss 782.28064\n",
      "2396 Train Loss 782.29535\n",
      "2397 Train Loss 782.213\n",
      "2398 Train Loss 782.57635\n",
      "2399 Train Loss 782.30853\n",
      "2400 Train Loss 782.449\n",
      "2401 Train Loss 782.28064\n",
      "2402 Train Loss 782.29535\n",
      "2403 Train Loss 782.213\n",
      "2404 Train Loss 782.57635\n",
      "2405 Train Loss 782.30853\n",
      "2406 Train Loss 782.449\n",
      "2407 Train Loss 782.28064\n",
      "2408 Train Loss 782.29535\n",
      "2409 Train Loss 782.213\n",
      "2410 Train Loss 782.57635\n",
      "2411 Train Loss 782.30853\n",
      "2412 Train Loss 782.449\n",
      "2413 Train Loss 782.28064\n",
      "2414 Train Loss 782.29535\n",
      "2415 Train Loss 782.213\n",
      "2416 Train Loss 782.57635\n",
      "2417 Train Loss 782.30853\n",
      "2418 Train Loss 782.449\n",
      "2419 Train Loss 782.28064\n",
      "2420 Train Loss 782.29535\n",
      "2421 Train Loss 782.213\n",
      "2422 Train Loss 782.57635\n",
      "2423 Train Loss 782.30853\n",
      "2424 Train Loss 782.449\n",
      "2425 Train Loss 782.28064\n",
      "2426 Train Loss 782.29535\n",
      "2427 Train Loss 782.213\n",
      "2428 Train Loss 782.57635\n",
      "2429 Train Loss 782.30853\n",
      "2430 Train Loss 782.449\n",
      "2431 Train Loss 782.28064\n",
      "2432 Train Loss 782.29535\n",
      "2433 Train Loss 782.213\n",
      "2434 Train Loss 782.57635\n",
      "2435 Train Loss 782.30853\n",
      "2436 Train Loss 782.449\n",
      "2437 Train Loss 782.28064\n",
      "2438 Train Loss 782.29535\n",
      "2439 Train Loss 782.213\n",
      "2440 Train Loss 782.57635\n",
      "2441 Train Loss 782.30853\n",
      "2442 Train Loss 782.449\n",
      "2443 Train Loss 782.28064\n",
      "2444 Train Loss 782.29535\n",
      "2445 Train Loss 782.213\n",
      "2446 Train Loss 782.57635\n",
      "2447 Train Loss 782.30853\n",
      "2448 Train Loss 782.449\n",
      "2449 Train Loss 782.28064\n",
      "2450 Train Loss 782.29535\n",
      "2451 Train Loss 782.213\n",
      "2452 Train Loss 782.57635\n",
      "2453 Train Loss 782.30853\n",
      "2454 Train Loss 782.449\n",
      "2455 Train Loss 782.28064\n",
      "2456 Train Loss 782.29535\n",
      "2457 Train Loss 782.213\n",
      "2458 Train Loss 782.57635\n",
      "2459 Train Loss 782.30853\n",
      "2460 Train Loss 782.449\n",
      "2461 Train Loss 782.28064\n",
      "2462 Train Loss 782.29535\n",
      "2463 Train Loss 782.213\n",
      "2464 Train Loss 782.57635\n",
      "2465 Train Loss 782.30853\n",
      "2466 Train Loss 782.449\n",
      "2467 Train Loss 782.28064\n",
      "2468 Train Loss 782.29535\n",
      "2469 Train Loss 782.213\n",
      "2470 Train Loss 782.57635\n",
      "2471 Train Loss 782.30853\n",
      "2472 Train Loss 782.449\n",
      "2473 Train Loss 782.28064\n",
      "2474 Train Loss 782.29535\n",
      "2475 Train Loss 782.213\n",
      "2476 Train Loss 782.57635\n",
      "2477 Train Loss 782.30853\n",
      "2478 Train Loss 782.449\n",
      "2479 Train Loss 782.28064\n",
      "2480 Train Loss 782.29535\n",
      "2481 Train Loss 782.213\n",
      "2482 Train Loss 782.57635\n",
      "2483 Train Loss 782.30853\n",
      "2484 Train Loss 782.449\n",
      "2485 Train Loss 782.28064\n",
      "2486 Train Loss 782.29535\n",
      "2487 Train Loss 782.213\n",
      "2488 Train Loss 782.57635\n",
      "2489 Train Loss 782.30853\n",
      "2490 Train Loss 782.449\n",
      "2491 Train Loss 782.28064\n",
      "2492 Train Loss 782.29535\n",
      "2493 Train Loss 782.213\n",
      "2494 Train Loss 782.57635\n",
      "2495 Train Loss 782.30853\n",
      "2496 Train Loss 782.449\n",
      "2497 Train Loss 782.28064\n",
      "2498 Train Loss 782.29535\n",
      "2499 Train Loss 782.213\n",
      "2500 Train Loss 782.57635\n",
      "2501 Train Loss 782.30853\n",
      "2502 Train Loss 782.449\n",
      "2503 Train Loss 782.28064\n",
      "2504 Train Loss 782.29535\n",
      "2505 Train Loss 782.213\n",
      "2506 Train Loss 782.57635\n",
      "2507 Train Loss 782.30853\n",
      "2508 Train Loss 782.449\n",
      "2509 Train Loss 782.28064\n",
      "2510 Train Loss 782.29535\n",
      "2511 Train Loss 782.213\n",
      "2512 Train Loss 782.57635\n",
      "2513 Train Loss 782.30853\n",
      "2514 Train Loss 782.449\n",
      "2515 Train Loss 782.28064\n",
      "2516 Train Loss 782.29535\n",
      "2517 Train Loss 782.213\n",
      "2518 Train Loss 782.57635\n",
      "2519 Train Loss 782.30853\n",
      "2520 Train Loss 782.449\n",
      "2521 Train Loss 782.28064\n",
      "2522 Train Loss 782.29535\n",
      "2523 Train Loss 782.213\n",
      "2524 Train Loss 782.57635\n",
      "2525 Train Loss 782.30853\n",
      "2526 Train Loss 782.449\n",
      "2527 Train Loss 782.28064\n",
      "2528 Train Loss 782.29535\n",
      "2529 Train Loss 782.213\n",
      "2530 Train Loss 782.57635\n",
      "2531 Train Loss 782.30853\n",
      "2532 Train Loss 782.449\n",
      "2533 Train Loss 782.28064\n",
      "2534 Train Loss 782.29535\n",
      "2535 Train Loss 782.213\n",
      "2536 Train Loss 782.57635\n",
      "2537 Train Loss 782.30853\n",
      "2538 Train Loss 782.449\n",
      "2539 Train Loss 782.28064\n",
      "2540 Train Loss 782.29535\n",
      "2541 Train Loss 782.213\n",
      "2542 Train Loss 782.57635\n",
      "2543 Train Loss 782.30853\n",
      "2544 Train Loss 782.449\n",
      "2545 Train Loss 782.28064\n",
      "2546 Train Loss 782.29535\n",
      "2547 Train Loss 782.213\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-46bc9707a98c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPINN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPINN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                     loss, flat_grad, t, ls_func_evals = _strong_wolfe(\n\u001b[0;32m--> 426\u001b[0;31m                         obj_func, x_init, t, d, loss, flat_grad, gtd)\n\u001b[0m\u001b[1;32m    427\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0mopt_cond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflat_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mtolerance_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36m_strong_wolfe\u001b[0;34m(obj_func, x, t, d, f, g, gtd, c1, c2, tolerance_change, max_ls)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# evaluate objective and gradient using initial step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mf_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mls_func_evals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mgtd_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mobj_func\u001b[0;34m(x, t, d)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m                     \u001b[0;32mdef\u001b[0m \u001b[0mobj_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m                         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_directional_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                     loss, flat_grad, t, ls_func_evals = _strong_wolfe(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36m_directional_evaluate\u001b[0;34m(self, closure, x, t, d)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_directional_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0mflat_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gather_flat_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-17e282a4c97c>\u001b[0m in \u001b[0;36mclosure\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    131\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy_D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu_D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxy_Nx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxy_Ny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN_hat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxy_coll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-17e282a4c97c>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, xy_D, u_D, xy_Nx, xy_Ny, N_hat, xy_coll, f_hat)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mloss_D\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy_D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu_D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mloss_N\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_N\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy_Nx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxy_Ny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m         \u001b[0mloss_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_PDE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy_coll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-17e282a4c97c>\u001b[0m in \u001b[0;36mloss_N\u001b[0;34m(self, xy_Nx, xy_Ny, N_hat)\u001b[0m\n\u001b[1;32m     83\u001b[0m         \u001b[0mu2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     84\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 85\u001b[0;31m         \u001b[0mu2_x_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg2\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mxy_Ny\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mallow_unused\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mdu2_dy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu2_x_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    226\u001b[0m     return Variable._execution_engine.run_backward(\n\u001b[1;32m    227\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         inputs, allow_unused, accumulate_grad=False)\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_reps = 1\n",
    "\n",
    "train_loss_full = []\n",
    "test_loss_full = []\n",
    "beta_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(reps)\n",
    "    \n",
    "    torch.manual_seed(reps*36)\n",
    "    N_D = 100 #Total number of data points for 'y'\n",
    "    N_N = 3500\n",
    "    N_f = 10000 #Total number of collocation points \n",
    "    \n",
    "    xy_coll_np_array, xy_D_np_array, u_D_np_array,xy_Nx_np_array,xy_Ny_np_array = trainingdata(N_D,N_N,N_f,(reps)*22)\n",
    "        \n",
    "    xy_coll = torch.from_numpy(xy_coll_np_array).float().to(device)\n",
    "    xy_D = torch.from_numpy(xy_D_np_array).float().to(device)\n",
    "    u_D = torch.from_numpy(u_D_np_array).float().to(device)\n",
    "    xy_Nx = torch.from_numpy(xy_Nx_np_array).float().to(device)\n",
    "    xy_Ny = torch.from_numpy(xy_Ny_np_array).float().to(device)\n",
    "        \n",
    "    N_hat = torch.zeros(xy_Nx.shape[0],1).to(device)    \n",
    "    f_hat = torch.zeros(xy_coll.shape[0],1).to(device)\n",
    "\n",
    "    layers = np.array([2,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "    #layers = np.array([3,100,100,100,100,100,100,100,1])\n",
    "\n",
    "    PINN = Sequentialmodel(layers)\n",
    "   \n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "    \n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=1, \n",
    "                              max_iter = 10000, \n",
    "                              max_eval = None, \n",
    "                              tolerance_grad = -1, \n",
    "                              tolerance_change = -1, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "        \n",
    "    optimizer.step(PINN.closure)\n",
    "    \n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(PINN.train_loss)\n",
    "    test_loss_full.append(PINN.test_loss)\n",
    "    elapsed_time[reps] = time.time() - start_time\n",
    "    beta_full.append(PINN.beta_val)\n",
    "\n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "    \n",
    "mdic = {\"train_loss\": train_loss_full,\"test_loss\": test_loss_full,\"Time\": elapsed_time, \"beta\": beta_full, \"label\": label}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred = PINN.test(xy_test_tensor)\n",
    "u_pred_3d = u_pred.reshape(250,250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7fe050405450>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "cmap = plt.cm.jet\n",
    "\n",
    "img3 = ax.imshow(np.flipud(np.transpose(u_pred_3d)),vmax =1000,vmin=450,cmap = cmap,extent=[0,1,0,1],aspect = 1)\n",
    "fig.colorbar(img3, orientation='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/smartlab/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py:226: MatReadWarning: Duplicate variable name \"None\" in stream - replacing previous with new\n",
      "Consider mio5.varmats_from_mat to split file into single variable files\n",
      "  matfile_dict = MR.get_variables(variable_names)\n"
     ]
    }
   ],
   "source": [
    "mat = scipy.io.loadmat('Thinplate_steady.mat')\n",
    "xy_test = np.array(mat['xy_test'])\n",
    "\n",
    "u_test = np.array(mat['u_test'])\n",
    "xy_test_tensor = torch.from_numpy(xy_test).float().to(device)\n",
    "u_pred = PINN.test(xy_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fe05027e310>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(u_test,'bo')\n",
    "plt.plot(u_pred,'ro',markersize = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.004639238035505407"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(u_pred-u_test)/np.linalg.norm(u_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + test_loss_full[i][-1]\n",
    "print(a/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + train_loss_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
