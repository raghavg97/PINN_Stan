{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####SUCCESFUL: IMPORTANT\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def true_2D_1(xt): #True function for 2D_1 Klein Gordon Equation x \\in [-50,50] , t \\in [0,10]\n",
    "#     y = xt[:,0]*np.cos(xt[:,1])\n",
    "#     return y.reshape(-1,1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Material Properties This link - https://www.mathworks.com/help/pde/ug/nonlinear-heat-transfer-in-a-thin-plate.html#heatTransferThinPlateExample-1\n",
    "k = 400\n",
    "rho = 8960\n",
    "cp = 386\n",
    "t_z = 0.01\n",
    "stef_bolt = 5.670373e-8\n",
    "hc = 1\n",
    "Ta = 300\n",
    "emiss = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"Navier_stan\"\n",
    "\n",
    "x = np.linspace(0,1,250).reshape(-1,1)\n",
    "y = np.linspace(0,1,250).reshape(-1,1)\n",
    "#t = np.linspace(0,1,100).reshape(-1,1) #t is actually from 0 to 5000, let us scale it to 0 to 1\n",
    "\n",
    "X,Y = np.meshgrid(x,y)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "Y = Y.flatten('F').reshape(-1,1)\n",
    " \n",
    "xy = np.hstack((X,Y))\n",
    "\n",
    "#initial_pts = np.logical_and(T==0,Y!=0).reshape(-1,)\n",
    "\n",
    "DBC_pts = (Y == 0).reshape(-1,)\n",
    "\n",
    "\n",
    "NBC_pts_x0 = (X == 0).reshape(-1,)\n",
    "NBC_pts_x1 = (X == 1).reshape(-1,)\n",
    "\n",
    "NBC_pts_y0 = (Y == 0).reshape(-1,)\n",
    "NBC_pts_y1 = (Y == 1).reshape(-1,)\n",
    "\n",
    "#xy_initial = xyt[initial_pts,:]\n",
    "xy_DBC = xy[DBC_pts,:]\n",
    "\n",
    "xy_NBC_x0 = xy[NBC_pts_x0,:]\n",
    "xy_NBC_x1 = xy[NBC_pts_x1,:]\n",
    "\n",
    "#xyt_NBC_y0 = xyt[NBC_pts_y0,:]\n",
    "xy_NBC_y1 = xy[NBC_pts_y1,:]\n",
    "\n",
    "#u_initial = np.zeros((np.shape(xy_initial)[0],1))\n",
    "u_DBC = 1000*np.ones((np.shape(xy_DBC)[0],1))\n",
    "\n",
    "#xy_I_DBC = np.vstack((xy_initial,xyt_DBC))\n",
    "#xyt_NBC = np.vstack((xyt_NBC_1,xyt_NBC_2,xyt_NBC_3,xyt_NBC_4))\n",
    "xy_NBC_x = np.vstack((xy_NBC_x0,xy_NBC_x1))\n",
    "#xyt_NBC_y = np.vstack((xyt_NBC_y0,xyt_NBC_y1))\n",
    "xy_NBC_y = np.vstack((xy_NBC_y1))\n",
    "\n",
    "#u_I_DBC = np.vstack((u_initial,u_DBC))\n",
    "\n",
    "xy_test_tensor = torch.from_numpy(xy).float().to(device)\n",
    "\n",
    "lb_xy = xy[0]\n",
    "ub_xy = xy[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_D,N_N,N_f,seed):\n",
    "    '''Boundary Conditions''' \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    #choose random N_u points for training\n",
    "    idx = np.random.choice(xy_DBC.shape[0], N_D, replace=False) \n",
    "    xy_D = xy_DBC[idx,:] #choose indices from  set 'idx' (x,t)\n",
    "    u_D = u_DBC[idx].reshape(-1,1)      #choose corresponding u\n",
    "\n",
    "    idx = np.random.choice(xy_NBC_x.shape[0], N_D, replace=False) \n",
    "    xy_Nx = xy_NBC_x[idx,:] #choose indices from  set 'idx' (x,t)\n",
    "\n",
    "    idx = np.random.choice(xy_NBC_y.shape[0], N_D, replace=False) \n",
    "    xy_Ny = xy_NBC_y[idx,:] #choose indices from  set 'idx' (x,t)\n",
    "\n",
    "    '''Collocation Points'''\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    samples = sampling(N_f)\n",
    "    \n",
    "    xy_coll = lb_xy + (ub_xy - lb_xy)*samples\n",
    "    xy_coll = np.vstack((xy_coll, xy_D,xy_Nx,xy_Ny)) # append training points to collocation points \n",
    "\n",
    "    return xy_coll, xy_D, u_D, xy_Nx,xy_Ny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        self.iter = 0\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)   \n",
    "        \n",
    "        self.beta = Parameter(torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "        \n",
    "        self.train_loss = []\n",
    "        self.test_loss = []\n",
    "        \n",
    "        self.beta_val = []\n",
    "        \n",
    "        self.n = torch.tensor(1.0)\n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xy):\n",
    "        if torch.is_tensor(xy) != True:         \n",
    "            xy = torch.from_numpy(xy)                \n",
    "        \n",
    "        ubxy = torch.from_numpy(ub_xy).float().to(device)\n",
    "        lbxy = torch.from_numpy(lb_xy).float().to(device)\n",
    "\n",
    "                      \n",
    "        #preprocessing input \n",
    "        xy = (xy - lbxy)/(ubxy - lbxy)\n",
    "        \n",
    "        #convert to float\n",
    "        a = xy.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            z1 = self.activation(z) \n",
    "            a = z1 + self.beta[:,i]*z*z1\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_D(self,xy_D,u_D):\n",
    "                \n",
    "        loss_bc = self.loss_function(self.forward(xy_D), u_D)\n",
    "                \n",
    "        return loss_bc\n",
    "    \n",
    "    def loss_N(self,xy_Nx,xy_Ny,N_hat):\n",
    "        \n",
    "        g1 = xy_Nx.clone()             \n",
    "        g1.requires_grad = True\n",
    "        u1 = self.forward(g1)\n",
    "        \n",
    "        u1_x_y = autograd.grad(u1,g1,torch.ones([xy_Nx.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        du1_dx = u1_x_y[:,[0]]\n",
    "        \n",
    "        g2 = xy_Ny.clone()             \n",
    "        g2.requires_grad = True\n",
    "        u2 = self.forward(g2)\n",
    "        \n",
    "        u2_x_y = autograd.grad(u2,g2,torch.ones([xy_Ny.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        du2_dy = u2_x_y[:,[1]]\n",
    "               \n",
    "        loss_N1 = self.loss_function(du1_dx,N_hat)\n",
    "        loss_N2 = self.loss_function(du2_dy,N_hat)\n",
    "        \n",
    "        #return loss_N1+loss_N2       \n",
    "        return loss_N1 + loss_N2\n",
    "    \n",
    "    def loss_PDE(self, xy_coll, f_hat):\n",
    "        \n",
    "        g = xy_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_y = autograd.grad(u,g,torch.ones([xy_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_xx_yy = autograd.grad(u_x_y,g,torch.ones(xy_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        #du_dt = u_x_y_t[:,[2]]\n",
    "        \n",
    "        d2u_dx2 = u_xx_yy[:,[0]]\n",
    "        d2u_dy2 = u_xx_yy[:,[1]]    \n",
    "        \n",
    "\n",
    "        f = - k*t_z*(d2u_dx2+d2u_dy2) + 2*hc*(u-Ta) + 2*emiss*stef_bolt*(torch.pow(u,4)-Ta**4) \n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xy_D,u_D,xy_Nx,xy_Ny,N_hat,xy_coll,f_hat):\n",
    "\n",
    "        loss_D = self.loss_D(xy_D,u_D)\n",
    "        loss_N = self.loss_N(xy_Nx,xy_Ny,N_hat)\n",
    "        loss_f = self.loss_PDE(xy_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_D + loss_N + loss_f\n",
    "        \n",
    "        #print(self.iter,\"loss_D:\",loss_D.cpu().detach().numpy(),\"loss_N:\",loss_N.cpu().detach().numpy(),\"loss_f:\",loss_f.cpu().detach().numpy())\n",
    "        \n",
    "        return loss_val\n",
    "     \n",
    "    'callable for optimizer'                                       \n",
    "    def closure(self):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = self.loss(xy_D,u_D,xy_Nx,xy_Ny,N_hat,xy_coll,f_hat)\n",
    "        self.train_loss.append(loss.cpu().detach().numpy())\n",
    "        \n",
    "        u_pred = self.test(xy_test_tensor)\n",
    "        #self.test_loss.append(np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))) #Commented because no true values yet\n",
    "        self.beta_val.append(self.beta.cpu().detach().numpy())\n",
    "        \n",
    "        #print(self.iter,\"Train Loss\",self.train_loss[-1],\"Test Loss\",self.test_loss[-1])\n",
    "        print(self.iter,\"Train Loss\",self.train_loss[-1])\n",
    "        \n",
    "        loss.backward()\n",
    "        self.iter += 1\n",
    "  \n",
    "\n",
    "        return loss        \n",
    "    'test neural network'\n",
    "    def test(self,xy_test_tensor):\n",
    "        u_pred = self.forward(xy_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0 Train Loss 2134119.0\n",
      "1 Train Loss 2130068.5\n",
      "2 Train Loss 2105508.2\n",
      "3 Train Loss 2058562.6\n",
      "4 Train Loss 2043182.8\n",
      "5 Train Loss 2025923.5\n",
      "6 Train Loss 1986442.9\n",
      "7 Train Loss 1891697.0\n",
      "8 Train Loss 1768869.8\n",
      "9 Train Loss 15808501.0\n",
      "10 Train Loss 1726310.0\n",
      "11 Train Loss 1976480.0\n",
      "12 Train Loss 1660680.5\n",
      "13 Train Loss 1591251.2\n",
      "14 Train Loss 1470817.1\n",
      "15 Train Loss 1362355.8\n",
      "16 Train Loss 1312593.1\n",
      "17 Train Loss 1276756.5\n",
      "18 Train Loss 1231463.1\n",
      "19 Train Loss 1936650.2\n",
      "20 Train Loss 1168696.6\n",
      "21 Train Loss 1150310.6\n",
      "22 Train Loss 1127648.6\n",
      "23 Train Loss 1103158.4\n",
      "24 Train Loss 1080551.4\n",
      "25 Train Loss 1040945.0\n",
      "26 Train Loss 1019205.5\n",
      "27 Train Loss 1013898.44\n",
      "28 Train Loss 1002432.25\n",
      "29 Train Loss 997019.1\n",
      "30 Train Loss 991039.75\n",
      "31 Train Loss 989087.3\n",
      "32 Train Loss 986856.7\n",
      "33 Train Loss 984520.3\n",
      "34 Train Loss 980834.9\n",
      "35 Train Loss 976053.56\n",
      "36 Train Loss 973231.56\n",
      "37 Train Loss 970178.9\n",
      "38 Train Loss 965934.94\n",
      "39 Train Loss 963942.7\n",
      "40 Train Loss 961676.25\n",
      "41 Train Loss 957500.1\n",
      "42 Train Loss 951810.44\n",
      "43 Train Loss 955082.2\n",
      "44 Train Loss 945933.1\n",
      "45 Train Loss 962301.5\n",
      "46 Train Loss 942058.44\n",
      "47 Train Loss 937216.06\n",
      "48 Train Loss 934137.44\n",
      "49 Train Loss 931641.75\n",
      "50 Train Loss 929755.6\n",
      "51 Train Loss 926415.75\n",
      "52 Train Loss 922679.25\n",
      "53 Train Loss 920043.7\n",
      "54 Train Loss 916160.0\n",
      "55 Train Loss 906812.94\n",
      "56 Train Loss 893727.2\n",
      "57 Train Loss 892728.8\n",
      "58 Train Loss 886886.75\n",
      "59 Train Loss 857413.9\n",
      "60 Train Loss 872715.7\n",
      "61 Train Loss 836641.9\n",
      "62 Train Loss 825256.25\n",
      "63 Train Loss 750710.44\n",
      "64 Train Loss 742655.2\n",
      "65 Train Loss 732625.06\n",
      "66 Train Loss 710637.6\n",
      "67 Train Loss 652076.94\n",
      "68 Train Loss 624405.1\n",
      "69 Train Loss 886839.75\n",
      "70 Train Loss 607951.44\n",
      "71 Train Loss 589243.9\n",
      "72 Train Loss 537883.2\n",
      "73 Train Loss 519515.25\n",
      "74 Train Loss 498151.22\n",
      "75 Train Loss 487267.7\n",
      "76 Train Loss 474214.28\n",
      "77 Train Loss 465939.12\n",
      "78 Train Loss 455892.06\n",
      "79 Train Loss 440384.5\n",
      "80 Train Loss 542656.1\n",
      "81 Train Loss 421728.38\n",
      "82 Train Loss 409752.88\n",
      "83 Train Loss 402279.75\n",
      "84 Train Loss 402925.6\n",
      "85 Train Loss 399157.4\n",
      "86 Train Loss 393712.2\n",
      "87 Train Loss 419588.0\n",
      "88 Train Loss 388039.4\n",
      "89 Train Loss 381528.7\n",
      "90 Train Loss 375755.72\n",
      "91 Train Loss 370301.0\n",
      "92 Train Loss 378511.12\n",
      "93 Train Loss 366580.56\n",
      "94 Train Loss 356784.7\n",
      "95 Train Loss 347672.97\n",
      "96 Train Loss 358455.84\n",
      "97 Train Loss 344698.9\n",
      "98 Train Loss 342626.97\n",
      "99 Train Loss 335080.03\n",
      "100 Train Loss 324639.34\n",
      "101 Train Loss 319051.1\n",
      "102 Train Loss 313925.1\n",
      "103 Train Loss 311766.2\n",
      "104 Train Loss 312567.66\n",
      "105 Train Loss 302398.06\n",
      "106 Train Loss 296465.38\n",
      "107 Train Loss 288515.53\n",
      "108 Train Loss 282101.06\n",
      "109 Train Loss 272038.06\n",
      "110 Train Loss 297572.34\n",
      "111 Train Loss 265276.6\n",
      "112 Train Loss 265307.62\n",
      "113 Train Loss 263179.7\n",
      "114 Train Loss 260259.28\n",
      "115 Train Loss 281442.94\n",
      "116 Train Loss 255460.97\n",
      "117 Train Loss 249912.72\n",
      "118 Train Loss 247192.16\n",
      "119 Train Loss 244514.4\n",
      "120 Train Loss 266022.0\n",
      "121 Train Loss 241164.28\n",
      "122 Train Loss 264242.78\n",
      "123 Train Loss 238735.86\n",
      "124 Train Loss 236989.66\n",
      "125 Train Loss 235819.98\n",
      "126 Train Loss 234560.73\n",
      "127 Train Loss 231499.06\n",
      "128 Train Loss 231644.64\n",
      "129 Train Loss 230261.66\n",
      "130 Train Loss 227709.7\n",
      "131 Train Loss 225047.34\n",
      "132 Train Loss 222659.36\n",
      "133 Train Loss 222178.25\n",
      "134 Train Loss 220264.38\n",
      "135 Train Loss 227481.34\n",
      "136 Train Loss 218209.64\n",
      "137 Train Loss 216263.86\n",
      "138 Train Loss 214132.03\n",
      "139 Train Loss 212950.75\n",
      "140 Train Loss 212468.62\n",
      "141 Train Loss 211549.02\n",
      "142 Train Loss 210208.31\n",
      "143 Train Loss 208655.05\n",
      "144 Train Loss 205834.55\n",
      "145 Train Loss 202965.11\n",
      "146 Train Loss 201302.45\n",
      "147 Train Loss 199123.23\n",
      "148 Train Loss 194640.39\n",
      "149 Train Loss 980352.7\n",
      "150 Train Loss 192951.56\n",
      "151 Train Loss 191572.16\n",
      "152 Train Loss 274598.03\n",
      "153 Train Loss 190115.11\n",
      "154 Train Loss 189523.36\n",
      "155 Train Loss 186949.03\n",
      "156 Train Loss 184362.03\n",
      "157 Train Loss 184116.94\n",
      "158 Train Loss 182639.34\n",
      "159 Train Loss 181035.12\n",
      "160 Train Loss 177803.33\n",
      "161 Train Loss 172585.19\n",
      "162 Train Loss 170575.03\n",
      "163 Train Loss 167566.78\n",
      "164 Train Loss 158563.16\n",
      "165 Train Loss 174961.16\n",
      "166 Train Loss 146674.45\n",
      "167 Train Loss 501849.25\n",
      "168 Train Loss 140998.84\n",
      "169 Train Loss 143451.16\n",
      "170 Train Loss 137297.31\n",
      "171 Train Loss 132068.17\n",
      "172 Train Loss 938097.9\n",
      "173 Train Loss 128128.695\n",
      "174 Train Loss 126681.86\n",
      "175 Train Loss 127099.12\n",
      "176 Train Loss 123384.63\n",
      "177 Train Loss 118965.06\n",
      "178 Train Loss 110557.72\n",
      "179 Train Loss 119623.66\n",
      "180 Train Loss 104760.09\n",
      "181 Train Loss 99191.61\n",
      "182 Train Loss 90320.945\n",
      "183 Train Loss 94902.72\n",
      "184 Train Loss 87969.42\n",
      "185 Train Loss 87316.93\n",
      "186 Train Loss 86396.39\n",
      "187 Train Loss 85247.01\n",
      "188 Train Loss 83364.125\n",
      "189 Train Loss 81235.34\n",
      "190 Train Loss 78912.41\n",
      "191 Train Loss 76912.53\n",
      "192 Train Loss 75272.14\n",
      "193 Train Loss 74014.21\n",
      "194 Train Loss 72429.25\n",
      "195 Train Loss 72484.18\n",
      "196 Train Loss 71202.88\n",
      "197 Train Loss 70231.75\n",
      "198 Train Loss 69510.59\n",
      "199 Train Loss 68217.81\n",
      "200 Train Loss 66343.016\n",
      "201 Train Loss 64738.96\n",
      "202 Train Loss 63797.684\n",
      "203 Train Loss 62838.37\n",
      "204 Train Loss 61765.05\n",
      "205 Train Loss 59745.59\n",
      "206 Train Loss 57647.254\n",
      "207 Train Loss 56233.42\n",
      "208 Train Loss 54616.582\n",
      "209 Train Loss 54704.055\n",
      "210 Train Loss 52973.86\n",
      "211 Train Loss 51183.72\n",
      "212 Train Loss 48901.117\n",
      "213 Train Loss 46046.836\n",
      "214 Train Loss 42205.383\n",
      "215 Train Loss 40757.062\n",
      "216 Train Loss 40230.08\n",
      "217 Train Loss 39218.805\n",
      "218 Train Loss 39169.312\n",
      "219 Train Loss 38729.617\n",
      "220 Train Loss 37298.89\n",
      "221 Train Loss 2230846.5\n",
      "222 Train Loss 36777.293\n",
      "223 Train Loss 35874.324\n",
      "224 Train Loss 35830.562\n",
      "225 Train Loss 34345.156\n",
      "226 Train Loss 33416.684\n",
      "227 Train Loss 32771.203\n",
      "228 Train Loss 31929.107\n",
      "229 Train Loss 30389.09\n",
      "230 Train Loss 30060.492\n",
      "231 Train Loss 29525.082\n",
      "232 Train Loss 28472.184\n",
      "233 Train Loss 27739.45\n",
      "234 Train Loss 26388.617\n",
      "235 Train Loss 24867.72\n",
      "236 Train Loss 23924.906\n",
      "237 Train Loss 23908.086\n",
      "238 Train Loss 23369.543\n",
      "239 Train Loss 22884.5\n",
      "240 Train Loss 21955.477\n",
      "241 Train Loss 20475.646\n",
      "242 Train Loss 20002.453\n",
      "243 Train Loss 18929.559\n",
      "244 Train Loss 19305.049\n",
      "245 Train Loss 18381.863\n",
      "246 Train Loss 17877.656\n",
      "247 Train Loss 16826.734\n",
      "248 Train Loss 16000.505\n",
      "249 Train Loss 15308.048\n",
      "250 Train Loss 14992.995\n",
      "251 Train Loss 14892.98\n",
      "252 Train Loss 14745.1\n",
      "253 Train Loss 14317.887\n",
      "254 Train Loss 13349.418\n",
      "255 Train Loss 13488.976\n",
      "256 Train Loss 12970.357\n",
      "257 Train Loss 12753.195\n",
      "258 Train Loss 12515.158\n",
      "259 Train Loss 12411.176\n",
      "260 Train Loss 12303.559\n",
      "261 Train Loss 12042.604\n",
      "262 Train Loss 11604.615\n",
      "263 Train Loss 11375.291\n",
      "264 Train Loss 11181.338\n",
      "265 Train Loss 11075.558\n",
      "266 Train Loss 10990.344\n",
      "267 Train Loss 10895.847\n",
      "268 Train Loss 10685.422\n",
      "269 Train Loss 10151.994\n",
      "270 Train Loss 9431.875\n",
      "271 Train Loss 9242.196\n",
      "272 Train Loss 9023.033\n",
      "273 Train Loss 8630.933\n",
      "274 Train Loss 8550.752\n",
      "275 Train Loss 8446.423\n",
      "276 Train Loss 8157.9346\n",
      "277 Train Loss 7840.6895\n",
      "278 Train Loss 7660.743\n",
      "279 Train Loss 7435.949\n",
      "280 Train Loss 7160.792\n",
      "281 Train Loss 6866.344\n",
      "282 Train Loss 6746.8936\n",
      "283 Train Loss 6843.8755\n",
      "284 Train Loss 6612.5645\n",
      "285 Train Loss 6559.309\n",
      "286 Train Loss 6356.715\n",
      "287 Train Loss 6293.763\n",
      "288 Train Loss 6109.5977\n",
      "289 Train Loss 6035.234\n",
      "290 Train Loss 5902.077\n",
      "291 Train Loss 5728.7744\n",
      "292 Train Loss 5523.2627\n",
      "293 Train Loss 5339.111\n",
      "294 Train Loss 4995.99\n",
      "295 Train Loss 4987.124\n",
      "296 Train Loss 4873.3857\n",
      "297 Train Loss 4871.5317\n",
      "298 Train Loss 4812.8804\n",
      "299 Train Loss 4770.057\n",
      "300 Train Loss 4732.936\n",
      "301 Train Loss 4707.197\n",
      "302 Train Loss 4694.52\n",
      "303 Train Loss 4685.256\n",
      "304 Train Loss 4676.5635\n",
      "305 Train Loss 4665.631\n",
      "306 Train Loss 4642.7866\n",
      "307 Train Loss 4586.8193\n",
      "308 Train Loss 4521.9917\n",
      "309 Train Loss 4430.1685\n",
      "310 Train Loss 4288.2324\n",
      "311 Train Loss 4214.189\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "312 Train Loss 4135.432\n",
      "313 Train Loss 4085.1638\n",
      "314 Train Loss 4024.063\n",
      "315 Train Loss 3926.8115\n",
      "316 Train Loss 3843.1387\n",
      "317 Train Loss 3757.1738\n",
      "318 Train Loss 3708.0237\n",
      "319 Train Loss 3680.9207\n",
      "320 Train Loss 3656.2686\n",
      "321 Train Loss 3615.21\n",
      "322 Train Loss 3521.1123\n",
      "323 Train Loss 3463.695\n",
      "324 Train Loss 3359.656\n",
      "325 Train Loss 3267.319\n",
      "326 Train Loss 3108.8599\n",
      "327 Train Loss 2975.3037\n",
      "328 Train Loss 3461.1855\n",
      "329 Train Loss 2874.4038\n",
      "330 Train Loss 2767.538\n",
      "331 Train Loss 2626.5034\n",
      "332 Train Loss 2582.9978\n",
      "333 Train Loss 2489.4512\n",
      "334 Train Loss 2431.7969\n",
      "335 Train Loss 2388.106\n",
      "336 Train Loss 2330.2075\n",
      "337 Train Loss 2260.6343\n",
      "338 Train Loss 2137.0513\n",
      "339 Train Loss 2041.3892\n",
      "340 Train Loss 5463.5747\n",
      "341 Train Loss 2010.4614\n",
      "342 Train Loss 1921.4926\n",
      "343 Train Loss 1827.291\n",
      "344 Train Loss 1773.4108\n",
      "345 Train Loss 1732.3405\n",
      "346 Train Loss 1687.7659\n",
      "347 Train Loss 1698.1304\n",
      "348 Train Loss 1655.7991\n",
      "349 Train Loss 1842.7776\n",
      "350 Train Loss 1643.6732\n",
      "351 Train Loss 1608.8799\n",
      "352 Train Loss 1586.4723\n",
      "353 Train Loss 1571.5406\n",
      "354 Train Loss 1561.8501\n",
      "355 Train Loss 1557.1536\n",
      "356 Train Loss 1553.2635\n",
      "357 Train Loss 1547.716\n",
      "358 Train Loss 1540.7052\n",
      "359 Train Loss 1536.6271\n",
      "360 Train Loss 1529.6721\n",
      "361 Train Loss 1529.0652\n",
      "362 Train Loss 1525.0839\n",
      "363 Train Loss 1512.7786\n",
      "364 Train Loss 1504.657\n",
      "365 Train Loss 1499.8933\n",
      "366 Train Loss 1495.5284\n",
      "367 Train Loss 1479.9583\n",
      "368 Train Loss 1445.2972\n",
      "369 Train Loss 1492.9814\n",
      "370 Train Loss 1432.8381\n",
      "371 Train Loss 1394.3298\n",
      "372 Train Loss 1353.5161\n",
      "373 Train Loss 1484.1841\n",
      "374 Train Loss 1333.2637\n",
      "375 Train Loss 1304.3038\n",
      "376 Train Loss 1266.0459\n",
      "377 Train Loss 1212.5881\n",
      "378 Train Loss 1217.1523\n",
      "379 Train Loss 1191.5673\n",
      "380 Train Loss 1193.191\n",
      "381 Train Loss 1164.8142\n",
      "382 Train Loss 1138.2848\n",
      "383 Train Loss 1108.534\n",
      "384 Train Loss 1095.1289\n",
      "385 Train Loss 1085.3744\n",
      "386 Train Loss 1081.4628\n",
      "387 Train Loss 1075.1941\n",
      "388 Train Loss 1066.386\n",
      "389 Train Loss 1055.8657\n",
      "390 Train Loss 1043.5181\n",
      "391 Train Loss 1035.7366\n",
      "392 Train Loss 1033.2977\n",
      "393 Train Loss 1029.6411\n",
      "394 Train Loss 1015.66583\n",
      "395 Train Loss 1000.80347\n",
      "396 Train Loss 984.02\n",
      "397 Train Loss 975.49414\n",
      "398 Train Loss 962.11194\n",
      "399 Train Loss 952.84955\n",
      "400 Train Loss 947.8518\n",
      "401 Train Loss 945.66315\n",
      "402 Train Loss 944.3164\n",
      "403 Train Loss 943.2423\n",
      "404 Train Loss 940.93085\n",
      "405 Train Loss 935.1073\n",
      "406 Train Loss 924.74805\n",
      "407 Train Loss 915.04407\n",
      "408 Train Loss 903.62976\n",
      "409 Train Loss 895.7449\n",
      "410 Train Loss 889.9724\n",
      "411 Train Loss 883.8575\n",
      "412 Train Loss 875.5043\n",
      "413 Train Loss 870.49554\n",
      "414 Train Loss 863.4859\n",
      "415 Train Loss 858.4048\n",
      "416 Train Loss 847.182\n",
      "417 Train Loss 837.88275\n",
      "418 Train Loss 823.75696\n",
      "419 Train Loss 801.0748\n",
      "420 Train Loss 930.62714\n",
      "421 Train Loss 794.7457\n",
      "422 Train Loss 763.68964\n",
      "423 Train Loss 756.14087\n",
      "424 Train Loss 740.27075\n",
      "425 Train Loss 734.8542\n",
      "426 Train Loss 729.03284\n",
      "427 Train Loss 724.0127\n",
      "428 Train Loss 718.56506\n",
      "429 Train Loss 709.47644\n",
      "430 Train Loss 695.6288\n",
      "431 Train Loss 693.13385\n",
      "432 Train Loss 682.24146\n",
      "433 Train Loss 726.7855\n",
      "434 Train Loss 657.22217\n",
      "435 Train Loss 638.0703\n",
      "436 Train Loss 624.73663\n",
      "437 Train Loss 611.7317\n",
      "438 Train Loss 615.5473\n",
      "439 Train Loss 601.31445\n",
      "440 Train Loss 582.2159\n",
      "441 Train Loss 566.7743\n",
      "442 Train Loss 552.68054\n",
      "443 Train Loss 533.49713\n",
      "444 Train Loss 522.20264\n",
      "445 Train Loss 513.9433\n",
      "446 Train Loss 511.86346\n",
      "447 Train Loss 508.948\n",
      "448 Train Loss 507.63782\n",
      "449 Train Loss 504.98642\n",
      "450 Train Loss 502.13992\n",
      "451 Train Loss 498.28726\n",
      "452 Train Loss 495.32574\n",
      "453 Train Loss 493.78326\n",
      "454 Train Loss 493.08615\n",
      "455 Train Loss 492.10938\n",
      "456 Train Loss 490.9536\n",
      "457 Train Loss 489.01184\n",
      "458 Train Loss 486.6198\n",
      "459 Train Loss 483.78473\n",
      "460 Train Loss 480.6836\n",
      "461 Train Loss 482.6102\n",
      "462 Train Loss 479.15045\n",
      "463 Train Loss 476.61844\n",
      "464 Train Loss 472.59436\n",
      "465 Train Loss 466.99945\n",
      "466 Train Loss 460.47083\n",
      "467 Train Loss 449.42456\n",
      "468 Train Loss 439.7215\n",
      "469 Train Loss 439.15332\n",
      "470 Train Loss 433.71783\n",
      "471 Train Loss 427.93402\n",
      "472 Train Loss 419.9408\n",
      "473 Train Loss 404.7564\n",
      "474 Train Loss 388.9276\n",
      "475 Train Loss 526.0877\n",
      "476 Train Loss 383.90115\n",
      "477 Train Loss 374.81097\n",
      "478 Train Loss 366.7281\n",
      "479 Train Loss 362.77087\n",
      "480 Train Loss 359.59308\n",
      "481 Train Loss 356.02734\n",
      "482 Train Loss 356.66412\n",
      "483 Train Loss 354.96985\n",
      "484 Train Loss 353.1568\n",
      "485 Train Loss 352.14236\n",
      "486 Train Loss 351.3135\n",
      "487 Train Loss 350.52197\n",
      "488 Train Loss 349.30908\n",
      "489 Train Loss 347.87994\n",
      "490 Train Loss 346.14575\n",
      "491 Train Loss 346.7309\n",
      "492 Train Loss 345.48547\n",
      "493 Train Loss 344.09338\n",
      "494 Train Loss 341.958\n",
      "495 Train Loss 341.1186\n",
      "496 Train Loss 340.84235\n",
      "497 Train Loss 340.53674\n",
      "498 Train Loss 340.21576\n",
      "499 Train Loss 339.31647\n",
      "500 Train Loss 342.4826\n",
      "501 Train Loss 339.07117\n",
      "502 Train Loss 337.90173\n",
      "503 Train Loss 335.61716\n",
      "504 Train Loss 328.62747\n",
      "505 Train Loss 320.17493\n",
      "506 Train Loss 312.08124\n",
      "507 Train Loss 308.60184\n",
      "508 Train Loss 302.45306\n",
      "509 Train Loss 290.6905\n",
      "510 Train Loss 285.9734\n",
      "511 Train Loss 282.21588\n",
      "512 Train Loss 279.45648\n",
      "513 Train Loss 276.08224\n",
      "514 Train Loss 271.30542\n",
      "515 Train Loss 271.66016\n",
      "516 Train Loss 269.9803\n",
      "517 Train Loss 268.02097\n",
      "518 Train Loss 265.64056\n",
      "519 Train Loss 267.8205\n",
      "520 Train Loss 264.814\n",
      "521 Train Loss 262.35028\n",
      "522 Train Loss 258.98975\n",
      "523 Train Loss 257.34848\n",
      "524 Train Loss 253.65092\n",
      "525 Train Loss 259.6659\n",
      "526 Train Loss 252.87161\n",
      "527 Train Loss 253.93478\n",
      "528 Train Loss 251.51077\n",
      "529 Train Loss 249.24136\n",
      "530 Train Loss 247.40268\n",
      "531 Train Loss 245.85399\n",
      "532 Train Loss 243.6589\n",
      "533 Train Loss 237.9321\n",
      "534 Train Loss 234.66306\n",
      "535 Train Loss 236.47379\n",
      "536 Train Loss 230.74316\n",
      "537 Train Loss 320.56168\n",
      "538 Train Loss 226.55318\n",
      "539 Train Loss 226.60327\n",
      "540 Train Loss 224.42938\n",
      "541 Train Loss 220.60837\n",
      "542 Train Loss 214.58383\n",
      "543 Train Loss 215.61093\n",
      "544 Train Loss 212.30643\n",
      "545 Train Loss 207.57855\n",
      "546 Train Loss 204.24207\n",
      "547 Train Loss 201.80121\n",
      "548 Train Loss 204.43143\n",
      "549 Train Loss 199.59554\n",
      "550 Train Loss 196.82454\n",
      "551 Train Loss 194.4964\n",
      "552 Train Loss 192.28848\n",
      "553 Train Loss 194.83176\n",
      "554 Train Loss 191.34398\n",
      "555 Train Loss 188.86893\n",
      "556 Train Loss 186.62675\n",
      "557 Train Loss 184.21231\n",
      "558 Train Loss 183.25394\n",
      "559 Train Loss 181.95425\n",
      "560 Train Loss 181.14679\n",
      "561 Train Loss 180.91846\n",
      "562 Train Loss 180.681\n",
      "563 Train Loss 180.39062\n",
      "564 Train Loss 180.04079\n",
      "565 Train Loss 179.56018\n",
      "566 Train Loss 179.44235\n",
      "567 Train Loss 179.11836\n",
      "568 Train Loss 178.90747\n",
      "569 Train Loss 178.38126\n",
      "570 Train Loss 178.05066\n",
      "571 Train Loss 177.3758\n",
      "572 Train Loss 176.94579\n",
      "573 Train Loss 176.8663\n",
      "574 Train Loss 176.74625\n",
      "575 Train Loss 176.43134\n",
      "576 Train Loss 176.18112\n",
      "577 Train Loss 175.94905\n",
      "578 Train Loss 175.6713\n",
      "579 Train Loss 175.41814\n",
      "580 Train Loss 176.63673\n",
      "581 Train Loss 175.20023\n",
      "582 Train Loss 174.97104\n",
      "583 Train Loss 174.33835\n",
      "584 Train Loss 173.63196\n",
      "585 Train Loss 178.09247\n",
      "586 Train Loss 172.94287\n",
      "587 Train Loss 171.45633\n",
      "588 Train Loss 168.10384\n",
      "589 Train Loss 166.52048\n",
      "590 Train Loss 165.63416\n",
      "591 Train Loss 164.81267\n",
      "592 Train Loss 164.18839\n",
      "593 Train Loss 163.59544\n",
      "594 Train Loss 163.26819\n",
      "595 Train Loss 163.05109\n",
      "596 Train Loss 162.58194\n",
      "597 Train Loss 162.37383\n",
      "598 Train Loss 162.11302\n",
      "599 Train Loss 161.51202\n",
      "600 Train Loss 160.39232\n",
      "601 Train Loss 158.88605\n",
      "602 Train Loss 157.3843\n",
      "603 Train Loss 155.32733\n",
      "604 Train Loss 163.77591\n",
      "605 Train Loss 154.61493\n",
      "606 Train Loss 152.78696\n",
      "607 Train Loss 151.53375\n",
      "608 Train Loss 149.976\n",
      "609 Train Loss 149.8642\n",
      "610 Train Loss 149.34674\n",
      "611 Train Loss 148.39673\n",
      "612 Train Loss 147.55232\n",
      "613 Train Loss 146.46545\n",
      "614 Train Loss 145.43988\n",
      "615 Train Loss 145.14003\n",
      "616 Train Loss 143.8893\n",
      "617 Train Loss 143.43013\n",
      "618 Train Loss 143.0127\n",
      "619 Train Loss 142.63919\n",
      "620 Train Loss 142.37762\n",
      "621 Train Loss 142.10237\n",
      "622 Train Loss 141.53143\n",
      "623 Train Loss 141.09499\n",
      "624 Train Loss 140.48494\n",
      "625 Train Loss 139.69061\n",
      "626 Train Loss 139.30376\n",
      "627 Train Loss 138.6373\n",
      "628 Train Loss 138.12334\n",
      "629 Train Loss 137.50151\n",
      "630 Train Loss 137.317\n",
      "631 Train Loss 136.96898\n",
      "632 Train Loss 136.75182\n",
      "633 Train Loss 136.63768\n",
      "634 Train Loss 136.33516\n",
      "635 Train Loss 136.11606\n",
      "636 Train Loss 135.71437\n",
      "637 Train Loss 135.44632\n",
      "638 Train Loss 134.99228\n",
      "639 Train Loss 134.63573\n",
      "640 Train Loss 134.34198\n",
      "641 Train Loss 134.06903\n",
      "642 Train Loss 133.76225\n",
      "643 Train Loss 133.34811\n",
      "644 Train Loss 133.42908\n",
      "645 Train Loss 133.11078\n",
      "646 Train Loss 132.34157\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "647 Train Loss 131.87164\n",
      "648 Train Loss 131.05551\n",
      "649 Train Loss 130.43288\n",
      "650 Train Loss 129.88309\n",
      "651 Train Loss 129.23358\n",
      "652 Train Loss 128.90076\n",
      "653 Train Loss 128.54521\n",
      "654 Train Loss 128.24002\n",
      "655 Train Loss 127.91709\n",
      "656 Train Loss 127.55344\n",
      "657 Train Loss 127.3648\n",
      "658 Train Loss 126.92574\n",
      "659 Train Loss 126.82991\n",
      "660 Train Loss 126.17215\n",
      "661 Train Loss 126.04303\n",
      "662 Train Loss 125.94836\n",
      "663 Train Loss 125.61515\n",
      "664 Train Loss 125.31784\n",
      "665 Train Loss 125.11578\n",
      "666 Train Loss 124.7723\n",
      "667 Train Loss 124.64528\n",
      "668 Train Loss 124.53885\n",
      "669 Train Loss 124.38881\n",
      "670 Train Loss 124.32803\n",
      "671 Train Loss 124.285614\n",
      "672 Train Loss 124.233734\n",
      "673 Train Loss 124.19586\n",
      "674 Train Loss 124.15355\n",
      "675 Train Loss 124.1013\n",
      "676 Train Loss 124.05466\n",
      "677 Train Loss 123.95363\n",
      "678 Train Loss 123.8512\n",
      "679 Train Loss 123.70108\n",
      "680 Train Loss 123.542435\n",
      "681 Train Loss 123.41837\n",
      "682 Train Loss 123.32797\n",
      "683 Train Loss 123.3279\n",
      "684 Train Loss 123.26961\n",
      "685 Train Loss 123.21946\n",
      "686 Train Loss 123.115654\n",
      "687 Train Loss 123.07753\n",
      "688 Train Loss 122.949524\n",
      "689 Train Loss 122.94658\n",
      "690 Train Loss 122.83895\n",
      "691 Train Loss 122.49261\n",
      "692 Train Loss 122.282486\n",
      "693 Train Loss 121.881165\n",
      "694 Train Loss 121.59245\n",
      "695 Train Loss 121.33839\n",
      "696 Train Loss 121.860245\n",
      "697 Train Loss 121.1004\n",
      "698 Train Loss 120.97237\n",
      "699 Train Loss 120.44681\n",
      "700 Train Loss 120.140076\n",
      "701 Train Loss 129.00482\n",
      "702 Train Loss 119.9431\n",
      "703 Train Loss 119.525375\n",
      "704 Train Loss 119.18346\n",
      "705 Train Loss 119.00875\n",
      "706 Train Loss 119.15349\n",
      "707 Train Loss 118.77866\n",
      "708 Train Loss 118.5428\n",
      "709 Train Loss 118.36211\n",
      "710 Train Loss 118.055725\n",
      "711 Train Loss 117.6814\n",
      "712 Train Loss 120.95285\n",
      "713 Train Loss 117.43884\n",
      "714 Train Loss 116.87962\n",
      "715 Train Loss 115.77182\n",
      "716 Train Loss 116.137436\n",
      "717 Train Loss 115.45282\n",
      "718 Train Loss 114.87251\n",
      "719 Train Loss 114.51907\n",
      "720 Train Loss 113.94437\n",
      "721 Train Loss 113.28464\n",
      "722 Train Loss 112.83301\n",
      "723 Train Loss 112.40342\n",
      "724 Train Loss 111.80917\n",
      "725 Train Loss 111.40311\n",
      "726 Train Loss 110.712654\n",
      "727 Train Loss 110.014084\n",
      "728 Train Loss 151.24696\n",
      "729 Train Loss 109.78085\n",
      "730 Train Loss 109.646194\n",
      "731 Train Loss 109.09512\n",
      "732 Train Loss 108.752945\n",
      "733 Train Loss 108.347404\n",
      "734 Train Loss 108.11708\n",
      "735 Train Loss 107.76384\n",
      "736 Train Loss 107.35605\n",
      "737 Train Loss 107.161865\n",
      "738 Train Loss 111.63588\n",
      "739 Train Loss 106.93039\n",
      "740 Train Loss 106.87695\n",
      "741 Train Loss 106.679695\n",
      "742 Train Loss 106.58853\n",
      "743 Train Loss 106.43126\n",
      "744 Train Loss 106.29702\n",
      "745 Train Loss 106.15349\n",
      "746 Train Loss 106.05499\n",
      "747 Train Loss 107.483444\n",
      "748 Train Loss 105.922455\n",
      "749 Train Loss 105.83058\n",
      "750 Train Loss 105.714005\n",
      "751 Train Loss 105.63497\n",
      "752 Train Loss 105.62392\n",
      "753 Train Loss 105.59552\n",
      "754 Train Loss 105.51965\n",
      "755 Train Loss 105.432884\n",
      "756 Train Loss 105.30976\n",
      "757 Train Loss 105.28238\n",
      "758 Train Loss 105.20925\n",
      "759 Train Loss 105.15081\n",
      "760 Train Loss 105.034836\n",
      "761 Train Loss 104.964615\n",
      "762 Train Loss 104.82895\n",
      "763 Train Loss 104.72266\n",
      "764 Train Loss 104.645546\n",
      "765 Train Loss 104.61512\n",
      "766 Train Loss 104.58206\n",
      "767 Train Loss 104.52882\n",
      "768 Train Loss 104.40296\n",
      "769 Train Loss 104.327835\n",
      "770 Train Loss 104.12396\n",
      "771 Train Loss 103.95181\n",
      "772 Train Loss 103.64554\n",
      "773 Train Loss 103.332855\n",
      "774 Train Loss 102.77475\n",
      "775 Train Loss 104.07855\n",
      "776 Train Loss 102.3232\n",
      "777 Train Loss 101.32107\n",
      "778 Train Loss 99.92789\n",
      "779 Train Loss 103.39225\n",
      "780 Train Loss 99.7103\n",
      "781 Train Loss 100.23891\n",
      "782 Train Loss 99.062775\n",
      "783 Train Loss 99.75982\n",
      "784 Train Loss 98.737366\n",
      "785 Train Loss 98.30466\n",
      "786 Train Loss 97.80162\n",
      "787 Train Loss 97.297005\n",
      "788 Train Loss 97.18003\n",
      "789 Train Loss 96.90692\n",
      "790 Train Loss 96.406906\n",
      "791 Train Loss 96.17998\n",
      "792 Train Loss 95.70407\n",
      "793 Train Loss 95.50534\n",
      "794 Train Loss 95.09186\n",
      "795 Train Loss 94.71003\n",
      "796 Train Loss 93.87778\n",
      "797 Train Loss 93.339264\n",
      "798 Train Loss 92.7155\n",
      "799 Train Loss 92.18994\n",
      "800 Train Loss 91.68286\n",
      "801 Train Loss 91.83862\n",
      "802 Train Loss 91.21153\n",
      "803 Train Loss 91.06639\n",
      "804 Train Loss 90.828896\n",
      "805 Train Loss 90.57628\n",
      "806 Train Loss 90.13637\n",
      "807 Train Loss 89.67964\n",
      "808 Train Loss 89.4157\n",
      "809 Train Loss 89.2135\n",
      "810 Train Loss 92.789536\n",
      "811 Train Loss 88.95668\n",
      "812 Train Loss 89.9247\n",
      "813 Train Loss 88.7251\n",
      "814 Train Loss 88.811615\n",
      "815 Train Loss 88.60394\n",
      "816 Train Loss 88.44167\n",
      "817 Train Loss 88.264755\n",
      "818 Train Loss 88.12784\n",
      "819 Train Loss 87.938965\n",
      "820 Train Loss 87.8537\n",
      "821 Train Loss 87.8\n",
      "822 Train Loss 87.73718\n",
      "823 Train Loss 87.668\n",
      "824 Train Loss 87.40475\n",
      "825 Train Loss 87.30504\n",
      "826 Train Loss 87.0961\n",
      "827 Train Loss 86.94799\n",
      "828 Train Loss 86.85427\n",
      "829 Train Loss 86.75891\n",
      "830 Train Loss 86.70886\n",
      "831 Train Loss 86.67062\n",
      "832 Train Loss 86.64157\n",
      "833 Train Loss 86.604904\n",
      "834 Train Loss 86.56326\n",
      "835 Train Loss 86.449234\n",
      "836 Train Loss 86.322365\n",
      "837 Train Loss 86.15342\n",
      "838 Train Loss 85.89183\n",
      "839 Train Loss 85.638275\n",
      "840 Train Loss 85.3835\n",
      "841 Train Loss 85.2961\n",
      "842 Train Loss 85.20447\n",
      "843 Train Loss 85.132645\n",
      "844 Train Loss 85.00912\n",
      "845 Train Loss 84.75963\n",
      "846 Train Loss 84.45302\n",
      "847 Train Loss 84.67215\n",
      "848 Train Loss 84.12463\n",
      "849 Train Loss 83.79205\n",
      "850 Train Loss 83.54248\n",
      "851 Train Loss 83.830826\n",
      "852 Train Loss 83.27156\n",
      "853 Train Loss 82.675735\n",
      "854 Train Loss 81.91234\n",
      "855 Train Loss 82.748535\n",
      "856 Train Loss 81.64975\n",
      "857 Train Loss 81.62096\n",
      "858 Train Loss 81.43811\n",
      "859 Train Loss 81.13133\n",
      "860 Train Loss 80.91412\n",
      "861 Train Loss 80.57214\n",
      "862 Train Loss 80.38014\n",
      "863 Train Loss 80.04993\n",
      "864 Train Loss 79.62502\n",
      "865 Train Loss 79.28206\n",
      "866 Train Loss 78.76697\n",
      "867 Train Loss 77.90982\n",
      "868 Train Loss 77.54068\n",
      "869 Train Loss 77.00248\n",
      "870 Train Loss 77.00333\n",
      "871 Train Loss 76.68201\n",
      "872 Train Loss 76.12958\n",
      "873 Train Loss 76.0715\n",
      "874 Train Loss 75.55372\n",
      "875 Train Loss 74.8363\n",
      "876 Train Loss 74.44037\n",
      "877 Train Loss 74.46789\n",
      "878 Train Loss 74.263504\n",
      "879 Train Loss 74.11957\n",
      "880 Train Loss 73.9572\n",
      "881 Train Loss 73.84222\n",
      "882 Train Loss 73.69998\n",
      "883 Train Loss 73.634766\n",
      "884 Train Loss 73.48846\n",
      "885 Train Loss 73.371124\n",
      "886 Train Loss 73.242035\n",
      "887 Train Loss 73.09756\n",
      "888 Train Loss 72.8727\n",
      "889 Train Loss 72.76287\n",
      "890 Train Loss 72.62613\n",
      "891 Train Loss 72.59202\n",
      "892 Train Loss 72.52682\n",
      "893 Train Loss 72.39737\n",
      "894 Train Loss 72.138824\n",
      "895 Train Loss 75.08373\n",
      "896 Train Loss 72.10462\n",
      "897 Train Loss 71.770035\n",
      "898 Train Loss 72.56102\n",
      "899 Train Loss 71.67406\n",
      "900 Train Loss 71.68642\n",
      "901 Train Loss 71.5267\n",
      "902 Train Loss 71.14102\n",
      "903 Train Loss 70.55777\n",
      "904 Train Loss 73.677666\n",
      "905 Train Loss 70.30675\n",
      "906 Train Loss 71.090256\n",
      "907 Train Loss 69.687\n",
      "908 Train Loss 68.72782\n",
      "909 Train Loss 73.17765\n",
      "910 Train Loss 68.28433\n",
      "911 Train Loss 67.358406\n",
      "912 Train Loss 66.42233\n",
      "913 Train Loss 67.26328\n",
      "914 Train Loss 65.981834\n",
      "915 Train Loss 65.70027\n",
      "916 Train Loss 65.21062\n",
      "917 Train Loss 64.76604\n",
      "918 Train Loss 63.96682\n",
      "919 Train Loss 68.94842\n",
      "920 Train Loss 63.59706\n",
      "921 Train Loss 63.009747\n",
      "922 Train Loss 62.450066\n",
      "923 Train Loss 62.022797\n",
      "924 Train Loss 62.212715\n",
      "925 Train Loss 61.47873\n",
      "926 Train Loss 61.070076\n",
      "927 Train Loss 60.52211\n",
      "928 Train Loss 60.305923\n",
      "929 Train Loss 59.958828\n",
      "930 Train Loss 59.780083\n",
      "931 Train Loss 59.73046\n",
      "932 Train Loss 59.579372\n",
      "933 Train Loss 59.405956\n",
      "934 Train Loss 59.14263\n",
      "935 Train Loss 61.848755\n",
      "936 Train Loss 59.05114\n",
      "937 Train Loss 58.856815\n",
      "938 Train Loss 58.6789\n",
      "939 Train Loss 58.48182\n",
      "940 Train Loss 58.65117\n",
      "941 Train Loss 58.3226\n",
      "942 Train Loss 58.072872\n",
      "943 Train Loss 57.954765\n",
      "944 Train Loss 57.81626\n",
      "945 Train Loss 57.57206\n",
      "946 Train Loss 57.59349\n",
      "947 Train Loss 57.422062\n",
      "948 Train Loss 57.1828\n",
      "949 Train Loss 56.9441\n",
      "950 Train Loss 56.84478\n",
      "951 Train Loss 56.62999\n",
      "952 Train Loss 56.524673\n",
      "953 Train Loss 56.73447\n",
      "954 Train Loss 56.410744\n",
      "955 Train Loss 56.219604\n",
      "956 Train Loss 55.927856\n",
      "957 Train Loss 55.92128\n",
      "958 Train Loss 55.799206\n",
      "959 Train Loss 55.618538\n",
      "960 Train Loss 55.557228\n",
      "961 Train Loss 55.512493\n",
      "962 Train Loss 55.45498\n",
      "963 Train Loss 55.408123\n",
      "964 Train Loss 55.353806\n",
      "965 Train Loss 55.293213\n",
      "966 Train Loss 55.212532\n",
      "967 Train Loss 55.0663\n",
      "968 Train Loss 54.94246\n",
      "969 Train Loss 54.72193\n",
      "970 Train Loss 54.63252\n",
      "971 Train Loss 54.543312\n",
      "972 Train Loss 54.450542\n",
      "973 Train Loss 54.384823\n",
      "974 Train Loss 54.33632\n",
      "975 Train Loss 54.264336\n",
      "976 Train Loss 54.180443\n",
      "977 Train Loss 54.098022\n",
      "978 Train Loss 54.041977\n",
      "979 Train Loss 53.960213\n",
      "980 Train Loss 53.858242\n",
      "981 Train Loss 53.828445\n",
      "982 Train Loss 53.7529\n",
      "983 Train Loss 53.64719\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "984 Train Loss 53.556694\n",
      "985 Train Loss 53.443172\n",
      "986 Train Loss 53.40487\n",
      "987 Train Loss 53.174507\n",
      "988 Train Loss 53.150738\n",
      "989 Train Loss 52.91992\n",
      "990 Train Loss 52.822903\n",
      "991 Train Loss 52.664566\n",
      "992 Train Loss 52.429474\n",
      "993 Train Loss 52.35521\n",
      "994 Train Loss 52.287212\n",
      "995 Train Loss 52.208908\n",
      "996 Train Loss 52.164467\n",
      "997 Train Loss 52.110596\n",
      "998 Train Loss 51.98934\n",
      "999 Train Loss 51.839367\n",
      "1000 Train Loss 51.60943\n",
      "1001 Train Loss 53.337883\n",
      "1002 Train Loss 51.516785\n",
      "1003 Train Loss 51.33195\n",
      "1004 Train Loss 51.264133\n",
      "1005 Train Loss 51.037746\n",
      "1006 Train Loss 50.929966\n",
      "1007 Train Loss 51.143692\n",
      "1008 Train Loss 50.862373\n",
      "1009 Train Loss 50.763824\n",
      "1010 Train Loss 50.68469\n",
      "1011 Train Loss 50.615395\n",
      "1012 Train Loss 50.582054\n",
      "1013 Train Loss 50.537712\n",
      "1014 Train Loss 50.440605\n",
      "1015 Train Loss 50.33881\n",
      "1016 Train Loss 50.44463\n",
      "1017 Train Loss 50.25114\n",
      "1018 Train Loss 50.149475\n",
      "1019 Train Loss 50.032223\n",
      "1020 Train Loss 49.856728\n",
      "1021 Train Loss 50.34797\n",
      "1022 Train Loss 49.76951\n",
      "1023 Train Loss 49.662548\n",
      "1024 Train Loss 49.49279\n",
      "1025 Train Loss 49.37151\n",
      "1026 Train Loss 49.21051\n",
      "1027 Train Loss 49.102707\n",
      "1028 Train Loss 48.87249\n",
      "1029 Train Loss 48.534218\n",
      "1030 Train Loss 48.453117\n",
      "1031 Train Loss 48.12094\n",
      "1032 Train Loss 48.417618\n",
      "1033 Train Loss 47.535767\n",
      "1034 Train Loss 47.198997\n",
      "1035 Train Loss 48.86924\n",
      "1036 Train Loss 46.958122\n",
      "1037 Train Loss 46.914013\n",
      "1038 Train Loss 46.66889\n",
      "1039 Train Loss 46.58288\n",
      "1040 Train Loss 46.504974\n",
      "1041 Train Loss 46.46965\n",
      "1042 Train Loss 46.437393\n",
      "1043 Train Loss 46.394154\n",
      "1044 Train Loss 46.33397\n",
      "1045 Train Loss 46.27942\n",
      "1046 Train Loss 46.17568\n",
      "1047 Train Loss 46.054955\n",
      "1048 Train Loss 45.93222\n",
      "1049 Train Loss 45.822792\n",
      "1050 Train Loss 45.846046\n",
      "1051 Train Loss 45.750717\n",
      "1052 Train Loss 45.656033\n",
      "1053 Train Loss 45.448425\n",
      "1054 Train Loss 45.22145\n",
      "1055 Train Loss 45.82512\n",
      "1056 Train Loss 45.078476\n",
      "1057 Train Loss 44.81732\n",
      "1058 Train Loss 44.59665\n",
      "1059 Train Loss 44.468597\n",
      "1060 Train Loss 45.133877\n",
      "1061 Train Loss 44.364693\n",
      "1062 Train Loss 44.09928\n",
      "1063 Train Loss 43.7873\n",
      "1064 Train Loss 43.424385\n",
      "1065 Train Loss 43.16909\n",
      "1066 Train Loss 44.625908\n",
      "1067 Train Loss 42.80006\n",
      "1068 Train Loss 42.56658\n",
      "1069 Train Loss 42.400505\n",
      "1070 Train Loss 42.36189\n",
      "1071 Train Loss 42.296738\n",
      "1072 Train Loss 42.058167\n",
      "1073 Train Loss 41.917118\n",
      "1074 Train Loss 41.738544\n",
      "1075 Train Loss 41.61328\n",
      "1076 Train Loss 41.3422\n",
      "1077 Train Loss 41.131393\n",
      "1078 Train Loss 41.069523\n",
      "1079 Train Loss 40.927525\n",
      "1080 Train Loss 40.99247\n",
      "1081 Train Loss 40.855503\n",
      "1082 Train Loss 40.808716\n",
      "1083 Train Loss 40.75766\n",
      "1084 Train Loss 40.68556\n",
      "1085 Train Loss 40.61809\n",
      "1086 Train Loss 40.475243\n",
      "1087 Train Loss 40.25715\n",
      "1088 Train Loss 41.27709\n",
      "1089 Train Loss 40.160927\n",
      "1090 Train Loss 39.90955\n",
      "1091 Train Loss 39.783947\n",
      "1092 Train Loss 39.646107\n",
      "1093 Train Loss 39.67889\n",
      "1094 Train Loss 39.583267\n",
      "1095 Train Loss 39.744064\n",
      "1096 Train Loss 39.543716\n",
      "1097 Train Loss 39.5231\n",
      "1098 Train Loss 39.442528\n",
      "1099 Train Loss 39.376312\n",
      "1100 Train Loss 39.307114\n",
      "1101 Train Loss 41.824482\n",
      "1102 Train Loss 39.287292\n",
      "1103 Train Loss 39.157684\n",
      "1104 Train Loss 39.361744\n",
      "1105 Train Loss 39.09891\n",
      "1106 Train Loss 39.033077\n",
      "1107 Train Loss 39.04332\n",
      "1108 Train Loss 38.989307\n",
      "1109 Train Loss 38.92482\n",
      "1110 Train Loss 38.87864\n",
      "1111 Train Loss 38.813644\n",
      "1112 Train Loss 38.795162\n",
      "1113 Train Loss 38.762417\n",
      "1114 Train Loss 38.741074\n",
      "1115 Train Loss 38.72815\n",
      "1116 Train Loss 38.680344\n",
      "1117 Train Loss 38.639248\n",
      "1118 Train Loss 38.62245\n",
      "1119 Train Loss 38.589996\n",
      "1120 Train Loss 38.56964\n",
      "1121 Train Loss 38.53389\n",
      "1122 Train Loss 38.48486\n",
      "1123 Train Loss 38.447426\n",
      "1124 Train Loss 38.406807\n",
      "1125 Train Loss 38.36644\n",
      "1126 Train Loss 38.30305\n",
      "1127 Train Loss 38.24356\n",
      "1128 Train Loss 38.19428\n",
      "1129 Train Loss 38.148354\n",
      "1130 Train Loss 38.093246\n",
      "1131 Train Loss 38.502388\n",
      "1132 Train Loss 38.048393\n",
      "1133 Train Loss 37.970764\n",
      "1134 Train Loss 37.893036\n",
      "1135 Train Loss 37.870697\n",
      "1136 Train Loss 37.849518\n",
      "1137 Train Loss 37.82431\n",
      "1138 Train Loss 37.701424\n",
      "1139 Train Loss 37.58066\n",
      "1140 Train Loss 39.760727\n",
      "1141 Train Loss 37.47619\n",
      "1142 Train Loss 37.342438\n",
      "1143 Train Loss 36.92298\n",
      "1144 Train Loss 36.74292\n",
      "1145 Train Loss 36.57495\n",
      "1146 Train Loss 36.559074\n",
      "1147 Train Loss 36.449734\n",
      "1148 Train Loss 36.47538\n",
      "1149 Train Loss 36.29607\n",
      "1150 Train Loss 36.11748\n",
      "1151 Train Loss 36.09397\n",
      "1152 Train Loss 35.991768\n",
      "1153 Train Loss 35.89617\n",
      "1154 Train Loss 35.821537\n",
      "1155 Train Loss 35.77715\n",
      "1156 Train Loss 35.7327\n",
      "1157 Train Loss 35.659916\n",
      "1158 Train Loss 35.599514\n",
      "1159 Train Loss 35.521793\n",
      "1160 Train Loss 35.431908\n",
      "1161 Train Loss 35.331287\n",
      "1162 Train Loss 35.21006\n",
      "1163 Train Loss 35.003815\n",
      "1164 Train Loss 34.87014\n",
      "1165 Train Loss 34.82364\n",
      "1166 Train Loss 34.651848\n",
      "1167 Train Loss 34.57238\n",
      "1168 Train Loss 34.47465\n",
      "1169 Train Loss 34.371056\n",
      "1170 Train Loss 34.34307\n",
      "1171 Train Loss 34.316994\n",
      "1172 Train Loss 34.311504\n",
      "1173 Train Loss 34.29698\n",
      "1174 Train Loss 34.283127\n",
      "1175 Train Loss 34.309464\n",
      "1176 Train Loss 34.278595\n",
      "1177 Train Loss 34.265675\n",
      "1178 Train Loss 34.239395\n",
      "1179 Train Loss 34.199677\n",
      "1180 Train Loss 34.145657\n",
      "1181 Train Loss 34.068638\n",
      "1182 Train Loss 39.653286\n",
      "1183 Train Loss 34.051098\n",
      "1184 Train Loss 34.029198\n",
      "1185 Train Loss 33.86111\n",
      "1186 Train Loss 33.787037\n",
      "1187 Train Loss 33.559807\n",
      "1188 Train Loss 33.36895\n",
      "1189 Train Loss 33.544163\n",
      "1190 Train Loss 33.269985\n",
      "1191 Train Loss 33.15558\n",
      "1192 Train Loss 33.07513\n",
      "1193 Train Loss 33.04021\n",
      "1194 Train Loss 32.991684\n",
      "1195 Train Loss 32.934692\n",
      "1196 Train Loss 32.932632\n",
      "1197 Train Loss 32.874187\n",
      "1198 Train Loss 32.775433\n",
      "1199 Train Loss 32.731987\n",
      "1200 Train Loss 32.683907\n",
      "1201 Train Loss 32.68721\n",
      "1202 Train Loss 32.649597\n",
      "1203 Train Loss 32.57887\n",
      "1204 Train Loss 32.658466\n",
      "1205 Train Loss 32.514572\n",
      "1206 Train Loss 32.431305\n",
      "1207 Train Loss 32.326103\n",
      "1208 Train Loss 32.105255\n",
      "1209 Train Loss 32.066704\n",
      "1210 Train Loss 32.15309\n",
      "1211 Train Loss 31.957066\n",
      "1212 Train Loss 31.891306\n",
      "1213 Train Loss 31.88041\n",
      "1214 Train Loss 31.817003\n",
      "1215 Train Loss 31.758781\n",
      "1216 Train Loss 31.711323\n",
      "1217 Train Loss 31.676128\n",
      "1218 Train Loss 31.637642\n",
      "1219 Train Loss 31.585735\n",
      "1220 Train Loss 31.524994\n",
      "1221 Train Loss 31.468124\n",
      "1222 Train Loss 31.389837\n",
      "1223 Train Loss 31.411665\n",
      "1224 Train Loss 31.353413\n",
      "1225 Train Loss 31.268223\n",
      "1226 Train Loss 31.202587\n",
      "1227 Train Loss 31.030342\n",
      "1228 Train Loss 30.95425\n",
      "1229 Train Loss 30.81831\n",
      "1230 Train Loss 31.932564\n",
      "1231 Train Loss 30.697792\n",
      "1232 Train Loss 30.535564\n",
      "1233 Train Loss 30.457146\n",
      "1234 Train Loss 30.340216\n",
      "1235 Train Loss 30.266483\n",
      "1236 Train Loss 30.106302\n",
      "1237 Train Loss 30.29903\n",
      "1238 Train Loss 30.064877\n",
      "1239 Train Loss 30.07058\n",
      "1240 Train Loss 30.009237\n",
      "1241 Train Loss 29.902573\n",
      "1242 Train Loss 29.770245\n",
      "1243 Train Loss 29.679024\n",
      "1244 Train Loss 29.822243\n",
      "1245 Train Loss 29.620983\n",
      "1246 Train Loss 29.52982\n",
      "1247 Train Loss 29.470266\n",
      "1248 Train Loss 29.37378\n",
      "1249 Train Loss 29.359428\n",
      "1250 Train Loss 29.310196\n",
      "1251 Train Loss 29.384632\n",
      "1252 Train Loss 29.272928\n",
      "1253 Train Loss 29.247034\n",
      "1254 Train Loss 29.21952\n",
      "1255 Train Loss 29.18718\n",
      "1256 Train Loss 29.144762\n",
      "1257 Train Loss 29.153965\n",
      "1258 Train Loss 29.127665\n",
      "1259 Train Loss 29.091873\n",
      "1260 Train Loss 29.208666\n",
      "1261 Train Loss 29.084784\n",
      "1262 Train Loss 29.072748\n",
      "1263 Train Loss 29.04451\n",
      "1264 Train Loss 29.029385\n",
      "1265 Train Loss 29.152256\n",
      "1266 Train Loss 29.028133\n",
      "1267 Train Loss 29.012318\n",
      "1268 Train Loss 28.996134\n",
      "1269 Train Loss 28.982578\n",
      "1270 Train Loss 28.978645\n",
      "1271 Train Loss 28.980268\n",
      "1272 Train Loss 28.971422\n",
      "1273 Train Loss 28.963398\n",
      "1274 Train Loss 28.95869\n",
      "1275 Train Loss 28.960066\n",
      "1276 Train Loss 28.961199\n",
      "1277 Train Loss 28.96155\n",
      "1278 Train Loss 28.961285\n",
      "1279 Train Loss 28.960821\n",
      "1280 Train Loss 28.958746\n",
      "1281 Train Loss 28.947128\n",
      "1282 Train Loss 28.930527\n",
      "1283 Train Loss 28.910877\n",
      "1284 Train Loss 28.88986\n",
      "1285 Train Loss 28.840696\n",
      "1286 Train Loss 28.800251\n",
      "1287 Train Loss 28.935696\n",
      "1288 Train Loss 28.768208\n",
      "1289 Train Loss 28.715296\n",
      "1290 Train Loss 28.66068\n",
      "1291 Train Loss 28.616259\n",
      "1292 Train Loss 28.60863\n",
      "1293 Train Loss 28.579865\n",
      "1294 Train Loss 28.568079\n",
      "1295 Train Loss 28.531033\n",
      "1296 Train Loss 28.50232\n",
      "1297 Train Loss 28.444355\n",
      "1298 Train Loss 28.521854\n",
      "1299 Train Loss 28.434301\n",
      "1300 Train Loss 28.419275\n",
      "1301 Train Loss 28.394104\n",
      "1302 Train Loss 28.41839\n",
      "1303 Train Loss 28.385502\n",
      "1304 Train Loss 28.365654\n",
      "1305 Train Loss 28.351448\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1306 Train Loss 28.354443\n",
      "1307 Train Loss 28.33342\n",
      "1308 Train Loss 28.330936\n",
      "1309 Train Loss 28.331192\n",
      "1310 Train Loss 28.329891\n",
      "1311 Train Loss 28.328918\n",
      "1312 Train Loss 28.32341\n",
      "1313 Train Loss 28.31662\n",
      "1314 Train Loss 28.311104\n",
      "1315 Train Loss 28.313906\n",
      "1316 Train Loss 28.30343\n",
      "1317 Train Loss 28.30563\n",
      "1318 Train Loss 28.313692\n",
      "1319 Train Loss 28.302486\n",
      "1320 Train Loss 28.306347\n",
      "1321 Train Loss 28.302502\n",
      "1322 Train Loss 28.56991\n",
      "1323 Train Loss 28.313808\n",
      "1324 Train Loss 28.31231\n",
      "1325 Train Loss 28.310932\n",
      "1326 Train Loss 28.30312\n",
      "1327 Train Loss 28.56991\n",
      "1328 Train Loss 28.313808\n",
      "1329 Train Loss 28.31231\n",
      "1330 Train Loss 28.310932\n",
      "1331 Train Loss 28.30312\n",
      "1332 Train Loss 28.56991\n",
      "1333 Train Loss 28.313808\n",
      "1334 Train Loss 28.31231\n",
      "1335 Train Loss 28.310932\n",
      "1336 Train Loss 28.30312\n",
      "1337 Train Loss 28.56991\n",
      "1338 Train Loss 28.313808\n",
      "1339 Train Loss 28.31231\n",
      "1340 Train Loss 28.310932\n",
      "1341 Train Loss 28.30312\n",
      "1342 Train Loss 28.56991\n",
      "1343 Train Loss 28.313808\n",
      "1344 Train Loss 28.31231\n",
      "1345 Train Loss 28.310932\n",
      "1346 Train Loss 28.30312\n",
      "1347 Train Loss 28.56991\n",
      "1348 Train Loss 28.313808\n",
      "1349 Train Loss 28.31231\n",
      "1350 Train Loss 28.310932\n",
      "1351 Train Loss 28.30312\n",
      "1352 Train Loss 28.56991\n",
      "1353 Train Loss 28.313808\n",
      "1354 Train Loss 28.31231\n",
      "1355 Train Loss 28.310932\n",
      "1356 Train Loss 28.30312\n",
      "1357 Train Loss 28.56991\n",
      "1358 Train Loss 28.313808\n",
      "1359 Train Loss 28.31231\n",
      "1360 Train Loss 28.310932\n",
      "1361 Train Loss 28.30312\n",
      "1362 Train Loss 28.56991\n",
      "1363 Train Loss 28.313808\n",
      "1364 Train Loss 28.31231\n",
      "1365 Train Loss 28.310932\n",
      "1366 Train Loss 28.30312\n",
      "1367 Train Loss 28.56991\n",
      "1368 Train Loss 28.313808\n",
      "1369 Train Loss 28.31231\n",
      "1370 Train Loss 28.310932\n",
      "1371 Train Loss 28.30312\n",
      "1372 Train Loss 28.56991\n",
      "1373 Train Loss 28.313808\n",
      "1374 Train Loss 28.31231\n",
      "1375 Train Loss 28.310932\n",
      "1376 Train Loss 28.30312\n",
      "1377 Train Loss 28.56991\n",
      "1378 Train Loss 28.313808\n",
      "1379 Train Loss 28.31231\n",
      "1380 Train Loss 28.310932\n",
      "1381 Train Loss 28.30312\n",
      "1382 Train Loss 28.56991\n",
      "1383 Train Loss 28.313808\n",
      "1384 Train Loss 28.31231\n",
      "1385 Train Loss 28.310932\n",
      "1386 Train Loss 28.30312\n",
      "1387 Train Loss 28.56991\n",
      "1388 Train Loss 28.313808\n",
      "1389 Train Loss 28.31231\n",
      "1390 Train Loss 28.310932\n",
      "1391 Train Loss 28.30312\n",
      "1392 Train Loss 28.56991\n",
      "1393 Train Loss 28.313808\n",
      "1394 Train Loss 28.31231\n",
      "1395 Train Loss 28.310932\n",
      "1396 Train Loss 28.30312\n",
      "1397 Train Loss 28.56991\n",
      "1398 Train Loss 28.313808\n",
      "1399 Train Loss 28.31231\n",
      "1400 Train Loss 28.310932\n",
      "1401 Train Loss 28.30312\n",
      "1402 Train Loss 28.56991\n",
      "1403 Train Loss 28.313808\n",
      "1404 Train Loss 28.31231\n",
      "1405 Train Loss 28.310932\n",
      "1406 Train Loss 28.30312\n",
      "1407 Train Loss 28.56991\n",
      "1408 Train Loss 28.313808\n",
      "1409 Train Loss 28.31231\n",
      "1410 Train Loss 28.310932\n",
      "1411 Train Loss 28.30312\n",
      "1412 Train Loss 28.56991\n",
      "1413 Train Loss 28.313808\n",
      "1414 Train Loss 28.31231\n",
      "1415 Train Loss 28.310932\n",
      "1416 Train Loss 28.30312\n",
      "1417 Train Loss 28.56991\n",
      "1418 Train Loss 28.313808\n",
      "1419 Train Loss 28.31231\n",
      "1420 Train Loss 28.310932\n",
      "1421 Train Loss 28.30312\n",
      "1422 Train Loss 28.56991\n",
      "1423 Train Loss 28.313808\n",
      "1424 Train Loss 28.31231\n",
      "1425 Train Loss 28.310932\n",
      "1426 Train Loss 28.30312\n",
      "1427 Train Loss 28.56991\n",
      "1428 Train Loss 28.313808\n",
      "1429 Train Loss 28.31231\n",
      "1430 Train Loss 28.310932\n",
      "1431 Train Loss 28.30312\n",
      "1432 Train Loss 28.56991\n",
      "1433 Train Loss 28.313808\n",
      "1434 Train Loss 28.31231\n",
      "1435 Train Loss 28.310932\n",
      "1436 Train Loss 28.30312\n",
      "1437 Train Loss 28.56991\n",
      "1438 Train Loss 28.313808\n",
      "1439 Train Loss 28.31231\n",
      "1440 Train Loss 28.310932\n",
      "1441 Train Loss 28.30312\n",
      "1442 Train Loss 28.56991\n",
      "1443 Train Loss 28.313808\n",
      "1444 Train Loss 28.31231\n",
      "1445 Train Loss 28.310932\n",
      "1446 Train Loss 28.30312\n",
      "1447 Train Loss 28.56991\n",
      "1448 Train Loss 28.313808\n",
      "1449 Train Loss 28.31231\n",
      "1450 Train Loss 28.310932\n",
      "1451 Train Loss 28.30312\n",
      "1452 Train Loss 28.56991\n",
      "1453 Train Loss 28.313808\n",
      "1454 Train Loss 28.31231\n",
      "1455 Train Loss 28.310932\n",
      "1456 Train Loss 28.30312\n",
      "1457 Train Loss 28.56991\n",
      "1458 Train Loss 28.313808\n",
      "1459 Train Loss 28.31231\n",
      "1460 Train Loss 28.310932\n",
      "1461 Train Loss 28.30312\n",
      "1462 Train Loss 28.56991\n",
      "1463 Train Loss 28.313808\n",
      "1464 Train Loss 28.31231\n",
      "1465 Train Loss 28.310932\n",
      "1466 Train Loss 28.30312\n",
      "1467 Train Loss 28.56991\n",
      "1468 Train Loss 28.313808\n",
      "1469 Train Loss 28.31231\n",
      "1470 Train Loss 28.310932\n",
      "1471 Train Loss 28.30312\n",
      "1472 Train Loss 28.56991\n",
      "1473 Train Loss 28.313808\n",
      "1474 Train Loss 28.31231\n",
      "1475 Train Loss 28.310932\n",
      "1476 Train Loss 28.30312\n",
      "1477 Train Loss 28.56991\n",
      "1478 Train Loss 28.313808\n",
      "1479 Train Loss 28.31231\n",
      "1480 Train Loss 28.310932\n",
      "1481 Train Loss 28.30312\n",
      "1482 Train Loss 28.56991\n",
      "1483 Train Loss 28.313808\n",
      "1484 Train Loss 28.31231\n",
      "1485 Train Loss 28.310932\n",
      "1486 Train Loss 28.30312\n",
      "1487 Train Loss 28.56991\n",
      "1488 Train Loss 28.313808\n",
      "1489 Train Loss 28.31231\n",
      "1490 Train Loss 28.310932\n",
      "1491 Train Loss 28.30312\n",
      "1492 Train Loss 28.56991\n",
      "1493 Train Loss 28.313808\n",
      "1494 Train Loss 28.31231\n",
      "1495 Train Loss 28.310932\n",
      "1496 Train Loss 28.30312\n",
      "1497 Train Loss 28.56991\n",
      "1498 Train Loss 28.313808\n",
      "1499 Train Loss 28.31231\n",
      "1500 Train Loss 28.310932\n",
      "1501 Train Loss 28.30312\n",
      "1502 Train Loss 28.56991\n",
      "1503 Train Loss 28.313808\n",
      "1504 Train Loss 28.31231\n",
      "1505 Train Loss 28.310932\n",
      "1506 Train Loss 28.30312\n",
      "1507 Train Loss 28.56991\n",
      "1508 Train Loss 28.313808\n",
      "1509 Train Loss 28.31231\n",
      "1510 Train Loss 28.310932\n",
      "1511 Train Loss 28.30312\n",
      "1512 Train Loss 28.56991\n",
      "1513 Train Loss 28.313808\n",
      "1514 Train Loss 28.31231\n",
      "1515 Train Loss 28.310932\n",
      "1516 Train Loss 28.30312\n",
      "1517 Train Loss 28.56991\n",
      "1518 Train Loss 28.313808\n",
      "1519 Train Loss 28.31231\n",
      "1520 Train Loss 28.310932\n",
      "1521 Train Loss 28.30312\n",
      "1522 Train Loss 28.56991\n",
      "1523 Train Loss 28.313808\n",
      "1524 Train Loss 28.31231\n",
      "1525 Train Loss 28.310932\n",
      "1526 Train Loss 28.30312\n",
      "1527 Train Loss 28.56991\n",
      "1528 Train Loss 28.313808\n",
      "1529 Train Loss 28.31231\n",
      "1530 Train Loss 28.310932\n",
      "1531 Train Loss 28.30312\n",
      "1532 Train Loss 28.56991\n",
      "1533 Train Loss 28.313808\n",
      "1534 Train Loss 28.31231\n",
      "1535 Train Loss 28.310932\n",
      "1536 Train Loss 28.30312\n",
      "1537 Train Loss 28.56991\n",
      "1538 Train Loss 28.313808\n",
      "1539 Train Loss 28.31231\n",
      "1540 Train Loss 28.310932\n",
      "1541 Train Loss 28.30312\n",
      "1542 Train Loss 28.56991\n",
      "1543 Train Loss 28.313808\n",
      "1544 Train Loss 28.31231\n",
      "1545 Train Loss 28.310932\n",
      "1546 Train Loss 28.30312\n",
      "1547 Train Loss 28.56991\n",
      "1548 Train Loss 28.313808\n",
      "1549 Train Loss 28.31231\n",
      "1550 Train Loss 28.310932\n",
      "1551 Train Loss 28.30312\n",
      "1552 Train Loss 28.56991\n",
      "1553 Train Loss 28.313808\n",
      "1554 Train Loss 28.31231\n",
      "1555 Train Loss 28.310932\n",
      "1556 Train Loss 28.30312\n",
      "1557 Train Loss 28.56991\n",
      "1558 Train Loss 28.313808\n",
      "1559 Train Loss 28.31231\n",
      "1560 Train Loss 28.310932\n",
      "1561 Train Loss 28.30312\n",
      "1562 Train Loss 28.56991\n",
      "1563 Train Loss 28.313808\n",
      "1564 Train Loss 28.31231\n",
      "1565 Train Loss 28.310932\n",
      "1566 Train Loss 28.30312\n",
      "1567 Train Loss 28.56991\n",
      "1568 Train Loss 28.313808\n",
      "1569 Train Loss 28.31231\n",
      "1570 Train Loss 28.310932\n",
      "1571 Train Loss 28.30312\n",
      "1572 Train Loss 28.56991\n",
      "1573 Train Loss 28.313808\n",
      "1574 Train Loss 28.31231\n",
      "1575 Train Loss 28.310932\n",
      "1576 Train Loss 28.30312\n",
      "1577 Train Loss 28.56991\n",
      "1578 Train Loss 28.313808\n",
      "1579 Train Loss 28.31231\n",
      "1580 Train Loss 28.310932\n",
      "1581 Train Loss 28.30312\n",
      "1582 Train Loss 28.56991\n",
      "1583 Train Loss 28.313808\n",
      "1584 Train Loss 28.31231\n",
      "1585 Train Loss 28.310932\n",
      "1586 Train Loss 28.30312\n",
      "1587 Train Loss 28.56991\n",
      "1588 Train Loss 28.313808\n",
      "1589 Train Loss 28.31231\n",
      "1590 Train Loss 28.310932\n",
      "1591 Train Loss 28.30312\n",
      "1592 Train Loss 28.56991\n",
      "1593 Train Loss 28.313808\n",
      "1594 Train Loss 28.31231\n",
      "1595 Train Loss 28.310932\n",
      "1596 Train Loss 28.30312\n",
      "1597 Train Loss 28.56991\n",
      "1598 Train Loss 28.313808\n",
      "1599 Train Loss 28.31231\n",
      "1600 Train Loss 28.310932\n",
      "1601 Train Loss 28.30312\n",
      "1602 Train Loss 28.56991\n",
      "1603 Train Loss 28.313808\n",
      "1604 Train Loss 28.31231\n",
      "1605 Train Loss 28.310932\n",
      "1606 Train Loss 28.30312\n",
      "1607 Train Loss 28.56991\n",
      "1608 Train Loss 28.313808\n",
      "1609 Train Loss 28.31231\n",
      "1610 Train Loss 28.310932\n",
      "1611 Train Loss 28.30312\n",
      "1612 Train Loss 28.56991\n",
      "1613 Train Loss 28.313808\n",
      "1614 Train Loss 28.31231\n",
      "1615 Train Loss 28.310932\n",
      "1616 Train Loss 28.30312\n",
      "1617 Train Loss 28.56991\n",
      "1618 Train Loss 28.313808\n",
      "1619 Train Loss 28.31231\n",
      "1620 Train Loss 28.310932\n",
      "1621 Train Loss 28.30312\n",
      "1622 Train Loss 28.56991\n",
      "1623 Train Loss 28.313808\n",
      "1624 Train Loss 28.31231\n",
      "1625 Train Loss 28.310932\n",
      "1626 Train Loss 28.30312\n",
      "1627 Train Loss 28.56991\n",
      "1628 Train Loss 28.313808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1629 Train Loss 28.31231\n",
      "1630 Train Loss 28.310932\n",
      "1631 Train Loss 28.30312\n",
      "1632 Train Loss 28.56991\n",
      "1633 Train Loss 28.313808\n",
      "1634 Train Loss 28.31231\n",
      "1635 Train Loss 28.310932\n",
      "1636 Train Loss 28.30312\n",
      "1637 Train Loss 28.56991\n",
      "1638 Train Loss 28.313808\n",
      "1639 Train Loss 28.31231\n",
      "1640 Train Loss 28.310932\n",
      "1641 Train Loss 28.30312\n",
      "1642 Train Loss 28.56991\n",
      "1643 Train Loss 28.313808\n",
      "1644 Train Loss 28.31231\n",
      "1645 Train Loss 28.310932\n",
      "1646 Train Loss 28.30312\n",
      "1647 Train Loss 28.56991\n",
      "1648 Train Loss 28.313808\n",
      "1649 Train Loss 28.31231\n",
      "1650 Train Loss 28.310932\n",
      "1651 Train Loss 28.30312\n",
      "1652 Train Loss 28.56991\n",
      "1653 Train Loss 28.313808\n",
      "1654 Train Loss 28.31231\n",
      "1655 Train Loss 28.310932\n",
      "1656 Train Loss 28.30312\n",
      "1657 Train Loss 28.56991\n",
      "1658 Train Loss 28.313808\n",
      "1659 Train Loss 28.31231\n",
      "1660 Train Loss 28.310932\n",
      "1661 Train Loss 28.30312\n",
      "1662 Train Loss 28.56991\n",
      "1663 Train Loss 28.313808\n",
      "1664 Train Loss 28.31231\n",
      "1665 Train Loss 28.310932\n",
      "1666 Train Loss 28.30312\n",
      "1667 Train Loss 28.56991\n",
      "1668 Train Loss 28.313808\n",
      "1669 Train Loss 28.31231\n",
      "1670 Train Loss 28.310932\n",
      "1671 Train Loss 28.30312\n",
      "1672 Train Loss 28.56991\n",
      "1673 Train Loss 28.313808\n",
      "1674 Train Loss 28.31231\n",
      "1675 Train Loss 28.310932\n",
      "1676 Train Loss 28.30312\n",
      "1677 Train Loss 28.56991\n",
      "1678 Train Loss 28.313808\n",
      "1679 Train Loss 28.31231\n",
      "1680 Train Loss 28.310932\n",
      "1681 Train Loss 28.30312\n",
      "1682 Train Loss 28.56991\n",
      "1683 Train Loss 28.313808\n",
      "1684 Train Loss 28.31231\n",
      "1685 Train Loss 28.310932\n",
      "1686 Train Loss 28.30312\n",
      "1687 Train Loss 28.56991\n",
      "1688 Train Loss 28.313808\n",
      "1689 Train Loss 28.31231\n",
      "1690 Train Loss 28.310932\n",
      "1691 Train Loss 28.30312\n",
      "1692 Train Loss 28.56991\n",
      "1693 Train Loss 28.313808\n",
      "1694 Train Loss 28.31231\n",
      "1695 Train Loss 28.310932\n",
      "1696 Train Loss 28.30312\n",
      "1697 Train Loss 28.56991\n",
      "1698 Train Loss 28.313808\n",
      "1699 Train Loss 28.31231\n",
      "1700 Train Loss 28.310932\n",
      "1701 Train Loss 28.30312\n",
      "1702 Train Loss 28.56991\n",
      "1703 Train Loss 28.313808\n",
      "1704 Train Loss 28.31231\n",
      "1705 Train Loss 28.310932\n",
      "1706 Train Loss 28.30312\n",
      "1707 Train Loss 28.56991\n",
      "1708 Train Loss 28.313808\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-46bc9707a98c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPINN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPINN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                     loss, flat_grad, t, ls_func_evals = _strong_wolfe(\n\u001b[0;32m--> 426\u001b[0;31m                         obj_func, x_init, t, d, loss, flat_grad, gtd)\n\u001b[0m\u001b[1;32m    427\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0mopt_cond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflat_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mtolerance_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36m_strong_wolfe\u001b[0;34m(obj_func, x, t, d, f, g, gtd, c1, c2, tolerance_change, max_ls)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;31m# Evaluate new point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0mf_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mls_func_evals\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mgtd_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mobj_func\u001b[0;34m(x, t, d)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m                     \u001b[0;32mdef\u001b[0m \u001b[0mobj_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m                         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_directional_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                     loss, flat_grad, t, ls_func_evals = _strong_wolfe(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36m_directional_evaluate\u001b[0;34m(self, closure, x, t, d)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_directional_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0mflat_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gather_flat_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-2139e3529a74>\u001b[0m in \u001b[0;36mclosure\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Train Loss\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_reps = 1\n",
    "\n",
    "train_loss_full = []\n",
    "test_loss_full = []\n",
    "beta_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(reps)\n",
    "    \n",
    "    torch.manual_seed(reps*36)\n",
    "    N_D = 100 #Total number of data points for 'y'\n",
    "    N_N = 3500\n",
    "    N_f = 10000 #Total number of collocation points \n",
    "    \n",
    "    xy_coll_np_array, xy_D_np_array, u_D_np_array,xy_Nx_np_array,xy_Ny_np_array = trainingdata(N_D,N_N,N_f,(reps)*22)\n",
    "        \n",
    "    xy_coll = torch.from_numpy(xy_coll_np_array).float().to(device)\n",
    "    xy_D = torch.from_numpy(xy_D_np_array).float().to(device)\n",
    "    u_D = torch.from_numpy(u_D_np_array).float().to(device)\n",
    "    xy_Nx = torch.from_numpy(xy_Nx_np_array).float().to(device)\n",
    "    xy_Ny = torch.from_numpy(xy_Ny_np_array).float().to(device)\n",
    "        \n",
    "    N_hat = torch.zeros(xy_Nx.shape[0],1).to(device)    \n",
    "    f_hat = torch.zeros(xy_coll.shape[0],1).to(device)\n",
    "\n",
    "    layers = np.array([2,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "    #layers = np.array([3,100,100,100,100,100,100,100,1])\n",
    "\n",
    "    PINN = Sequentialmodel(layers)\n",
    "   \n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "    \n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=1, \n",
    "                              max_iter = 10000, \n",
    "                              max_eval = None, \n",
    "                              tolerance_grad = -1, \n",
    "                              tolerance_change = -1, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "        \n",
    "    optimizer.step(PINN.closure)\n",
    "    \n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(PINN.train_loss)\n",
    "    test_loss_full.append(PINN.test_loss)\n",
    "    elapsed_time[reps] = time.time() - start_time\n",
    "    beta_full.append(PINN.beta_val)\n",
    "\n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "    \n",
    "mdic = {\"train_loss\": train_loss_full,\"test_loss\": test_loss_full,\"Time\": elapsed_time, \"beta\": beta_full, \"label\": label}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred = PINN.test(xy_test_tensor)\n",
    "u_pred_3d = u_pred.reshape(250,250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "cmap = plt.cm.jet\n",
    "\n",
    "img3 = ax.imshow(np.transpose(u_pred_3d),vmax =1000,vmin=450,cmap = cmap,extent=[0,1,0,1],aspect = 0.75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1001.5523"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.max(u_pred_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + test_loss_full[i][-1]\n",
    "print(a/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + train_loss_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
