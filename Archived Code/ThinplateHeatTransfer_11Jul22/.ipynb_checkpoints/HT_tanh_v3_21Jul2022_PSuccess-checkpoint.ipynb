{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def true_2D_1(xt): #True function for 2D_1 Klein Gordon Equation x \\in [-50,50] , t \\in [0,10]\n",
    "#     y = xt[:,0]*np.cos(xt[:,1])\n",
    "#     return y.reshape(-1,1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Material Properties This link - https://www.mathworks.com/help/pde/ug/nonlinear-heat-transfer-in-a-thin-plate.html#heatTransferThinPlateExample-1\n",
    "k = 400\n",
    "rho = 8960\n",
    "cp = 386\n",
    "t_z = 0.01\n",
    "stef_bolt = 5.670373e-8\n",
    "hc = 1\n",
    "Ta = 300\n",
    "emiss = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"Navier_stan\"\n",
    "\n",
    "x = np.linspace(0,1,100).reshape(-1,1)\n",
    "y = np.linspace(0,1,100).reshape(-1,1)\n",
    "t = np.linspace(0,1,100).reshape(-1,1) #t is actually from 0 to 5000, let us scale it to 0 to 1\n",
    "\n",
    "X,Y,T = np.meshgrid(x,y,t)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "Y = Y.flatten('F').reshape(-1,1)\n",
    "T = T.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xyt = np.hstack((X,Y,T))\n",
    "\n",
    "initial_pts = np.logical_and(T==0,Y!=0).reshape(-1,)\n",
    "\n",
    "DBC_pts = (Y == 0).reshape(-1,)\n",
    "\n",
    "\n",
    "NBC_pts_x0 = (X == 0).reshape(-1,)\n",
    "NBC_pts_x1 = (X == 1).reshape(-1,)\n",
    "\n",
    "NBC_pts_y0 = (Y == 0).reshape(-1,)\n",
    "NBC_pts_y1 = (Y == 1).reshape(-1,)\n",
    "\n",
    "xyt_initial = xyt[initial_pts,:]\n",
    "xyt_DBC = xyt[DBC_pts,:]\n",
    "\n",
    "xyt_NBC_x0 = xyt[NBC_pts_x0,:]\n",
    "xyt_NBC_x1 = xyt[NBC_pts_x1,:]\n",
    "\n",
    "#xyt_NBC_y0 = xyt[NBC_pts_y0,:]\n",
    "xyt_NBC_y1 = xyt[NBC_pts_y1,:]\n",
    "\n",
    "u_initial = 300*np.ones((np.shape(xyt_initial)[0],1))\n",
    "u_DBC = 1000*np.ones((np.shape(xyt_DBC)[0],1))\n",
    "\n",
    "xyt_I_DBC = np.vstack((xyt_initial,xyt_DBC))\n",
    "#xyt_NBC = np.vstack((xyt_NBC_1,xyt_NBC_2,xyt_NBC_3,xyt_NBC_4))\n",
    "xyt_NBC_x = np.vstack((xyt_NBC_x0,xyt_NBC_x1))\n",
    "#xyt_NBC_y = np.vstack((xyt_NBC_y0,xyt_NBC_y1))\n",
    "xyt_NBC_y = np.vstack((xyt_NBC_y1))\n",
    "\n",
    "u_I_DBC = np.vstack((u_initial,u_DBC))\n",
    "\n",
    "xyt_test_tensor = torch.from_numpy(xyt).float().to(device)\n",
    "\n",
    "lb_xyt = xyt[0]\n",
    "ub_xyt = xyt[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_D,N_N,N_f,seed):\n",
    "    '''Boundary Conditions''' \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    #choose random N_u points for training\n",
    "    idx = np.random.choice(xyt_I_DBC.shape[0], N_D, replace=False) \n",
    "    xyt_D = xyt_I_DBC[idx,:] #choose indices from  set 'idx' (x,t)\n",
    "    u_D = u_I_DBC[idx].reshape(-1,1)      #choose corresponding u\n",
    "\n",
    "    idx = np.random.choice(xyt_NBC_x.shape[0], N_D, replace=False) \n",
    "    xyt_Nx = xyt_NBC_x[idx,:] #choose indices from  set 'idx' (x,t)\n",
    "\n",
    "    idx = np.random.choice(xyt_NBC_y.shape[0], N_D, replace=False) \n",
    "    xyt_Ny = xyt_NBC_y[idx,:] #choose indices from  set 'idx' (x,t)\n",
    "\n",
    "    '''Collocation Points'''\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    samples = sampling(N_f)\n",
    "    \n",
    "    xyt_coll = lb_xyt + (ub_xyt - lb_xyt)*samples\n",
    "    xyt_coll = np.vstack((xyt_coll, xyt_D,xyt_Nx,xyt_Ny)) # append training points to collocation points \n",
    "\n",
    "    return xyt_coll, xyt_D, u_D, xyt_Nx,xyt_Ny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        self.iter = 0\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)   \n",
    "        \n",
    "#         self.beta = Parameter(torch.ones((50,len(layers)-2)))\n",
    "#         self.beta.requiresGrad = True\n",
    "        \n",
    "        self.alpha = Parameter(torch.ones((50,len(layers)-2)))\n",
    "        self.alpha.requiresGrad = True\n",
    "        \n",
    "        self.train_loss = []\n",
    "        self.test_loss = []\n",
    "        \n",
    "        self.beta_val = []\n",
    "        \n",
    "        self.n = torch.tensor(1.0)\n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xyt):\n",
    "        if torch.is_tensor(xyt) != True:         \n",
    "            xyt = torch.from_numpy(xyt)                \n",
    "        \n",
    "        ubxyt = torch.from_numpy(ub_xyt).float().to(device)\n",
    "        lbxyt = torch.from_numpy(lb_xyt).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xyt = (xyt - lbxyt)/(ubxyt - lbxyt)\n",
    "        \n",
    "        #convert to float\n",
    "        a = xyt.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(self.n*self.alpha[:,i]*z) \n",
    "             \n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_D(self,xyt_D,u_D):\n",
    "                \n",
    "        loss_bc = self.loss_function(self.forward(xyt_D), u_D)\n",
    "                \n",
    "        return loss_bc\n",
    "    \n",
    "    def loss_N(self,xyt_Nx,xyt_Ny,N_hat):\n",
    "        \n",
    "        g1 = xyt_Nx.clone()             \n",
    "        g1.requires_grad = True\n",
    "        u1 = self.forward(g1)\n",
    "        \n",
    "        u1_x_y_t = autograd.grad(u1,g1,torch.ones([xyt_Nx.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        du1_dx = u1_x_y_t[:,[0]]\n",
    "        \n",
    "        g2 = xyt_Ny.clone()             \n",
    "        g2.requires_grad = True\n",
    "        u2 = self.forward(g2)\n",
    "        \n",
    "        u2_x_y_t = autograd.grad(u2,g2,torch.ones([xyt_Ny.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        du2_dy = u2_x_y_t[:,[1]]\n",
    "               \n",
    "        loss_N1 = self.loss_function(du1_dx,N_hat)\n",
    "        loss_N2 = self.loss_function(du2_dy,N_hat)\n",
    "        \n",
    "        #return loss_N1+loss_N2       \n",
    "        return loss_N1 + loss_N2\n",
    "    \n",
    "    def loss_PDE(self, xyt_coll, f_hat):\n",
    "        \n",
    "        g = xyt_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_y_t = autograd.grad(u,g,torch.ones([xyt_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_xx_yy_tt = autograd.grad(u_x_y_t,g,torch.ones(xyt_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        du_dt = u_x_y_t[:,[2]]\n",
    "        \n",
    "        d2u_dx2 = u_xx_yy_tt[:,[0]]\n",
    "        d2u_dy2 = u_xx_yy_tt[:,[1]]    \n",
    "        \n",
    "\n",
    "        f = rho*cp*t_z*du_dt/3000 - k*t_z*(d2u_dx2+d2u_dy2) + 2*hc*(u-Ta) + 2*emiss*stef_bolt*(torch.pow(u,4)-Ta**4) \n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xyt_D,u_D,xyt_Nx,xyt_Ny,N_hat,xyt_coll,f_hat):\n",
    "\n",
    "        loss_D = self.loss_D(xyt_D,u_D)\n",
    "        loss_N = self.loss_N(xyt_Nx,xyt_Ny,N_hat)\n",
    "        loss_f = self.loss_PDE(xyt_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_D + loss_N + loss_f\n",
    "        \n",
    "        #print(self.iter,\"loss_D:\",loss_D.cpu().detach().numpy(),\"loss_N:\",loss_N.cpu().detach().numpy(),\"loss_f:\",loss_f.cpu().detach().numpy())\n",
    "        \n",
    "        return loss_val\n",
    "     \n",
    "    'callable for optimizer'                                       \n",
    "    def closure(self):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = self.loss(xyt_D,u_D,xyt_Nx,xyt_Ny,N_hat,xyt_coll,f_hat)\n",
    "        self.train_loss.append(loss.cpu().detach().numpy())\n",
    "        \n",
    "        u_pred = self.test(xyt_test_tensor)\n",
    "        #self.test_loss.append(np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))) #Commented because no true values yet\n",
    "        self.beta_val.append(self.alpha.cpu().detach().numpy())\n",
    "        \n",
    "        #print(self.iter,\"Train Loss\",self.train_loss[-1],\"Test Loss\",self.test_loss[-1])\n",
    "        print(self.iter,\"Train Loss\",self.train_loss[-1])\n",
    "        \n",
    "        loss.backward()\n",
    "        self.iter += 1\n",
    "  \n",
    "\n",
    "        return loss        \n",
    "    'test neural network'\n",
    "    def test(self,xyt_test_tensor):\n",
    "        u_pred = self.forward(xyt_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0 Train Loss 1683767.5\n",
      "1 Train Loss 1682067.8\n",
      "2 Train Loss 1667935.2\n",
      "3 Train Loss 1618318.5\n",
      "4 Train Loss 1509106.6\n",
      "5 Train Loss 1964733.5\n",
      "6 Train Loss 1488893.6\n",
      "7 Train Loss 1462808.5\n",
      "8 Train Loss 1412429.2\n",
      "9 Train Loss 1344291.0\n",
      "10 Train Loss 1171534.8\n",
      "11 Train Loss 9780679.0\n",
      "12 Train Loss 1067398.9\n",
      "13 Train Loss 1028675.4\n",
      "14 Train Loss 1129940.6\n",
      "15 Train Loss 1020228.44\n",
      "16 Train Loss 1010459.75\n",
      "17 Train Loss 994273.0\n",
      "18 Train Loss 952693.25\n",
      "19 Train Loss 885659.2\n",
      "20 Train Loss 921311.4\n",
      "21 Train Loss 842994.1\n",
      "22 Train Loss 798671.8\n",
      "23 Train Loss 720432.3\n",
      "24 Train Loss 700574.8\n",
      "25 Train Loss 653911.2\n",
      "26 Train Loss 8639259.0\n",
      "27 Train Loss 756131.7\n",
      "28 Train Loss 631801.3\n",
      "29 Train Loss 624431.5\n",
      "30 Train Loss 619725.8\n",
      "31 Train Loss 616064.94\n",
      "32 Train Loss 606468.2\n",
      "33 Train Loss 594916.8\n",
      "34 Train Loss 611691.75\n",
      "35 Train Loss 588233.7\n",
      "36 Train Loss 578636.6\n",
      "37 Train Loss 576587.06\n",
      "38 Train Loss 573833.75\n",
      "39 Train Loss 567159.5\n",
      "40 Train Loss 559373.0\n",
      "41 Train Loss 536660.4\n",
      "42 Train Loss 503542.12\n",
      "43 Train Loss 491308.12\n",
      "44 Train Loss 494294.34\n",
      "45 Train Loss 485362.03\n",
      "46 Train Loss 478461.7\n",
      "47 Train Loss 933660.4\n",
      "48 Train Loss 475098.97\n",
      "49 Train Loss 696707.5\n",
      "50 Train Loss 467832.44\n",
      "51 Train Loss 453924.94\n",
      "52 Train Loss 448326.84\n",
      "53 Train Loss 443987.03\n",
      "54 Train Loss 439761.5\n",
      "55 Train Loss 431262.84\n",
      "56 Train Loss 418694.28\n",
      "57 Train Loss 416834.94\n",
      "58 Train Loss 412303.44\n",
      "59 Train Loss 409392.97\n",
      "60 Train Loss 406873.12\n",
      "61 Train Loss 402713.25\n",
      "62 Train Loss 427665.03\n",
      "63 Train Loss 398976.8\n",
      "64 Train Loss 395965.3\n",
      "65 Train Loss 394305.62\n",
      "66 Train Loss 393438.34\n",
      "67 Train Loss 391214.72\n",
      "68 Train Loss 403491.03\n",
      "69 Train Loss 387274.22\n",
      "70 Train Loss 385990.0\n",
      "71 Train Loss 378952.25\n",
      "72 Train Loss 378239.3\n",
      "73 Train Loss 375551.94\n",
      "74 Train Loss 371111.47\n",
      "75 Train Loss 368078.0\n",
      "76 Train Loss 365729.3\n",
      "77 Train Loss 361743.28\n",
      "78 Train Loss 359763.47\n",
      "79 Train Loss 357687.62\n",
      "80 Train Loss 355686.62\n",
      "81 Train Loss 372616.03\n",
      "82 Train Loss 352742.34\n",
      "83 Train Loss 350647.2\n",
      "84 Train Loss 342567.16\n",
      "85 Train Loss 339265.3\n",
      "86 Train Loss 335380.34\n",
      "87 Train Loss 332066.4\n",
      "88 Train Loss 336642.75\n",
      "89 Train Loss 330131.94\n",
      "90 Train Loss 326310.9\n",
      "91 Train Loss 319195.78\n",
      "92 Train Loss 310782.28\n",
      "93 Train Loss 1070803.4\n",
      "94 Train Loss 310680.47\n",
      "95 Train Loss 314635.12\n",
      "96 Train Loss 309800.16\n",
      "97 Train Loss 305318.6\n",
      "98 Train Loss 293513.4\n",
      "99 Train Loss 290500.3\n",
      "100 Train Loss 287047.66\n",
      "101 Train Loss 281010.94\n",
      "102 Train Loss 278288.94\n",
      "103 Train Loss 276301.25\n",
      "104 Train Loss 271061.25\n",
      "105 Train Loss 264866.78\n",
      "106 Train Loss 260473.58\n",
      "107 Train Loss 261774.05\n",
      "108 Train Loss 251677.08\n",
      "109 Train Loss 254967.52\n",
      "110 Train Loss 249035.19\n",
      "111 Train Loss 244740.44\n",
      "112 Train Loss 244404.7\n",
      "113 Train Loss 244222.03\n",
      "114 Train Loss 244121.61\n",
      "115 Train Loss 244008.28\n",
      "116 Train Loss 243963.89\n",
      "117 Train Loss 243944.08\n",
      "118 Train Loss 243927.27\n",
      "119 Train Loss 243908.5\n",
      "120 Train Loss 243890.97\n",
      "121 Train Loss 243879.38\n",
      "122 Train Loss 243852.52\n",
      "123 Train Loss 246081.11\n",
      "124 Train Loss 243754.27\n",
      "125 Train Loss 243735.95\n",
      "126 Train Loss 257801.17\n",
      "127 Train Loss 244169.69\n",
      "128 Train Loss 243722.8\n",
      "129 Train Loss 243581.36\n",
      "130 Train Loss 243282.23\n",
      "131 Train Loss 244140.2\n",
      "132 Train Loss 242812.75\n",
      "133 Train Loss 243673.44\n",
      "134 Train Loss 242499.1\n",
      "135 Train Loss 242214.23\n",
      "136 Train Loss 242011.19\n",
      "137 Train Loss 241306.27\n",
      "138 Train Loss 247536.34\n",
      "139 Train Loss 241152.02\n",
      "140 Train Loss 240500.34\n",
      "141 Train Loss 257753.05\n",
      "142 Train Loss 240101.83\n",
      "143 Train Loss 239564.03\n",
      "144 Train Loss 239228.38\n",
      "145 Train Loss 238787.55\n",
      "146 Train Loss 474490.0\n",
      "147 Train Loss 245707.72\n",
      "148 Train Loss 238638.53\n",
      "149 Train Loss 238540.58\n",
      "150 Train Loss 238219.7\n",
      "151 Train Loss 237260.86\n",
      "152 Train Loss 236771.52\n",
      "153 Train Loss 235886.27\n",
      "154 Train Loss 235895.06\n",
      "155 Train Loss 235785.56\n",
      "156 Train Loss 235680.58\n",
      "157 Train Loss 235416.98\n",
      "158 Train Loss 235937.5\n",
      "159 Train Loss 234992.61\n",
      "160 Train Loss 234277.19\n",
      "161 Train Loss 233262.95\n",
      "162 Train Loss 232386.4\n",
      "163 Train Loss 236021.53\n",
      "164 Train Loss 231531.14\n",
      "165 Train Loss 233597.1\n",
      "166 Train Loss 231350.16\n",
      "167 Train Loss 231097.33\n",
      "168 Train Loss 230509.47\n",
      "169 Train Loss 229972.1\n",
      "170 Train Loss 229940.5\n",
      "171 Train Loss 229256.1\n",
      "172 Train Loss 229069.23\n",
      "173 Train Loss 228741.0\n",
      "174 Train Loss 228502.92\n",
      "175 Train Loss 227947.33\n",
      "176 Train Loss 227367.83\n",
      "177 Train Loss 306717.78\n",
      "178 Train Loss 226028.33\n",
      "179 Train Loss 230297.88\n",
      "180 Train Loss 225419.84\n",
      "181 Train Loss 227521.7\n",
      "182 Train Loss 225043.28\n",
      "183 Train Loss 224624.34\n",
      "184 Train Loss 225007.89\n",
      "185 Train Loss 224268.11\n",
      "186 Train Loss 224297.98\n",
      "187 Train Loss 223865.94\n",
      "188 Train Loss 223633.67\n",
      "189 Train Loss 223415.17\n",
      "190 Train Loss 223048.48\n",
      "191 Train Loss 223028.98\n",
      "192 Train Loss 222898.06\n",
      "193 Train Loss 222865.81\n",
      "194 Train Loss 222682.9\n",
      "195 Train Loss 222524.8\n",
      "196 Train Loss 222238.86\n",
      "197 Train Loss 222096.14\n",
      "198 Train Loss 221858.5\n",
      "199 Train Loss 220837.0\n",
      "200 Train Loss 219762.62\n",
      "201 Train Loss 218532.92\n",
      "202 Train Loss 218225.5\n",
      "203 Train Loss 217545.19\n",
      "204 Train Loss 217781.12\n",
      "205 Train Loss 217439.72\n",
      "206 Train Loss 217268.23\n",
      "207 Train Loss 217172.9\n",
      "208 Train Loss 217078.61\n",
      "209 Train Loss 217084.3\n",
      "210 Train Loss 217000.66\n",
      "211 Train Loss 216871.17\n",
      "212 Train Loss 216582.53\n",
      "213 Train Loss 216364.97\n",
      "214 Train Loss 215871.61\n",
      "215 Train Loss 214908.33\n",
      "216 Train Loss 214859.94\n",
      "217 Train Loss 214646.02\n",
      "218 Train Loss 214602.9\n",
      "219 Train Loss 214344.98\n",
      "220 Train Loss 214195.11\n",
      "221 Train Loss 213935.3\n",
      "222 Train Loss 213691.2\n",
      "223 Train Loss 213342.62\n",
      "224 Train Loss 212954.6\n",
      "225 Train Loss 212640.4\n",
      "226 Train Loss 212755.12\n",
      "227 Train Loss 212440.5\n",
      "228 Train Loss 212109.97\n",
      "229 Train Loss 211676.03\n",
      "230 Train Loss 210860.3\n",
      "231 Train Loss 211421.69\n",
      "232 Train Loss 210649.31\n",
      "233 Train Loss 210445.75\n",
      "234 Train Loss 210219.44\n",
      "235 Train Loss 210090.31\n",
      "236 Train Loss 209958.6\n",
      "237 Train Loss 209827.61\n",
      "238 Train Loss 209685.4\n",
      "239 Train Loss 209559.4\n",
      "240 Train Loss 209415.53\n",
      "241 Train Loss 209278.7\n",
      "242 Train Loss 209137.02\n",
      "243 Train Loss 208894.27\n",
      "244 Train Loss 208507.31\n",
      "245 Train Loss 208091.88\n",
      "246 Train Loss 207702.77\n",
      "247 Train Loss 210050.56\n",
      "248 Train Loss 207345.61\n",
      "249 Train Loss 207141.03\n",
      "250 Train Loss 206954.6\n",
      "251 Train Loss 207020.16\n",
      "252 Train Loss 206815.55\n",
      "253 Train Loss 206728.77\n",
      "254 Train Loss 206653.75\n",
      "255 Train Loss 206577.61\n",
      "256 Train Loss 206481.0\n",
      "257 Train Loss 206638.38\n",
      "258 Train Loss 206411.69\n",
      "259 Train Loss 206283.52\n",
      "260 Train Loss 206155.38\n",
      "261 Train Loss 206031.95\n",
      "262 Train Loss 205912.61\n",
      "263 Train Loss 205826.5\n",
      "264 Train Loss 205680.5\n",
      "265 Train Loss 205625.7\n",
      "266 Train Loss 205574.4\n",
      "267 Train Loss 205437.83\n",
      "268 Train Loss 205250.62\n",
      "269 Train Loss 205066.9\n",
      "270 Train Loss 204759.98\n",
      "271 Train Loss 204291.19\n",
      "272 Train Loss 204564.84\n",
      "273 Train Loss 203933.73\n",
      "274 Train Loss 203975.52\n",
      "275 Train Loss 203779.39\n",
      "276 Train Loss 204114.53\n",
      "277 Train Loss 203677.1\n",
      "278 Train Loss 203657.22\n",
      "279 Train Loss 203532.39\n",
      "280 Train Loss 203425.6\n",
      "281 Train Loss 203275.95\n",
      "282 Train Loss 203182.8\n",
      "283 Train Loss 203034.06\n",
      "284 Train Loss 203009.55\n",
      "285 Train Loss 202874.75\n",
      "286 Train Loss 202781.83\n",
      "287 Train Loss 202629.94\n",
      "288 Train Loss 202488.5\n",
      "289 Train Loss 202459.36\n",
      "290 Train Loss 202390.88\n",
      "291 Train Loss 202345.2\n",
      "292 Train Loss 202282.19\n",
      "293 Train Loss 202212.61\n",
      "294 Train Loss 202093.22\n",
      "295 Train Loss 202048.39\n",
      "296 Train Loss 201989.48\n",
      "297 Train Loss 201956.25\n",
      "298 Train Loss 201857.75\n",
      "299 Train Loss 201741.34\n",
      "300 Train Loss 201580.64\n",
      "301 Train Loss 201392.42\n",
      "302 Train Loss 201376.06\n",
      "303 Train Loss 201222.52\n",
      "304 Train Loss 200985.27\n",
      "305 Train Loss 201077.7\n",
      "306 Train Loss 200865.22\n",
      "307 Train Loss 200976.12\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "308 Train Loss 200731.72\n",
      "309 Train Loss 200578.73\n",
      "310 Train Loss 200987.83\n",
      "311 Train Loss 200443.77\n",
      "312 Train Loss 200170.48\n",
      "313 Train Loss 200038.25\n",
      "314 Train Loss 213821.44\n",
      "315 Train Loss 199872.9\n",
      "316 Train Loss 199811.38\n",
      "317 Train Loss 200108.06\n",
      "318 Train Loss 199633.03\n",
      "319 Train Loss 199531.3\n",
      "320 Train Loss 199395.83\n",
      "321 Train Loss 199242.27\n",
      "322 Train Loss 198997.45\n"
     ]
    }
   ],
   "source": [
    "max_reps = 1\n",
    "\n",
    "train_loss_full = []\n",
    "test_loss_full = []\n",
    "alpha_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "#for reps in range(max_reps):\n",
    "for reps in range(8,9):\n",
    "    print(reps)\n",
    "    \n",
    "    torch.manual_seed(reps*36)\n",
    "    N_D = 5000 #Total number of data points for 'y'\n",
    "    N_N = 3500\n",
    "    N_f = 10000 #Total number of collocation points \n",
    "    \n",
    "    xyt_coll_np_array, xyt_D_np_array, u_D_np_array,xyt_Nx_np_array,xyt_Ny_np_array = trainingdata(N_D,N_N,N_f,(reps)*22)\n",
    "        \n",
    "    xyt_coll = torch.from_numpy(xyt_coll_np_array).float().to(device)\n",
    "    xyt_D = torch.from_numpy(xyt_D_np_array).float().to(device)\n",
    "    u_D = torch.from_numpy(u_D_np_array).float().to(device)\n",
    "    xyt_Nx = torch.from_numpy(xyt_Nx_np_array).float().to(device)\n",
    "    xyt_Ny = torch.from_numpy(xyt_Ny_np_array).float().to(device)\n",
    "        \n",
    "    N_hat = torch.zeros(xyt_Nx.shape[0],1).to(device)    \n",
    "    f_hat = torch.zeros(xyt_coll.shape[0],1).to(device)\n",
    "\n",
    "    layers = np.array([3,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "    #layers = np.array([3,100,100,100,100,100,100,100,1])\n",
    "\n",
    "    PINN = Sequentialmodel(layers)\n",
    "   \n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "    \n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=1, \n",
    "                              max_iter = 10000, \n",
    "                              max_eval = None, \n",
    "                              tolerance_grad = -1, \n",
    "                              tolerance_change = -1, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "        \n",
    "    optimizer.step(PINN.closure)\n",
    "    \n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(PINN.train_loss)\n",
    "    test_loss_full.append(PINN.test_loss)\n",
    "    elapsed_time[reps] = time.time() - start_time\n",
    "    alpha_full.append(PINN.alpha_val)\n",
    "\n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "    \n",
    "mdic = {\"train_loss\": train_loss_full,\"test_loss\": test_loss_full,\"Time\": elapsed_time, \"alpha\": alpha_full, \"label\": label}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "del xyt_coll\n",
    "del xyt_D"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred = PINN.test(xyt_test_tensor)\n",
    "u_pred_3d = u_pred.reshape(100,100,100,order = 'F')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x7f7f80263890>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "cmap = plt.cm.jet\n",
    "\n",
    "img3 = ax.imshow(np.flipud(u_pred_3d[:,:,50]),vmax =1000,vmin=450,cmap = cmap,extent=[0,1,0,1],aspect = 0.75)\n",
    "fig.colorbar(img3, orientation='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_video():\n",
    "    for i in range(100):\n",
    "        #plt.imshow(img[i], cmap=cm.Greys_r)\n",
    "        img3 = ax.imshow(np.flipud(u_pred_3d[:,:,i]),vmax =1000,vmin=450,cmap = cmap,extent=[0,1,0,1],aspect = 0.75)\n",
    "        \n",
    "        plt.savefig(\"stan_heat_%02d.png\" % i)\n",
    "\n",
    "    #os.chdir()\n",
    "    subprocess.call(['ffmpeg', '-framerate', '8', '-i', 'stan_heat_%02d.png', '-r', '10', '-pix_fmt', 'yuv420p','Stan_transient.mp4'])\n",
    "    for file_name in glob.glob(\"*.png\"):\n",
    "        os.remove(file_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ffmpeg': 'ffmpeg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-21-509721d7df78>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mgenerate_video\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-20-716314f7f0e9>\u001b[0m in \u001b[0;36mgenerate_video\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[0;31m#os.chdir()\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0msubprocess\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'ffmpeg'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-framerate'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'8'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-i'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'stan_heat_%02d.png'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-r'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'10'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'-pix_fmt'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'yuv420p'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'Stan_transient.mp4'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mfile_name\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mglob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mglob\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"*.png\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m         \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mremove\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(timeout, *popenargs, **kwargs)\u001b[0m\n\u001b[1;32m    321\u001b[0m     \u001b[0mretcode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcall\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"ls\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"-l\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    322\u001b[0m     \"\"\"\n\u001b[0;32m--> 323\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0mPopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mpopenargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    324\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[1;32m    773\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 775\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    776\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# Cleanup if the child failed starting.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1520\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0merrno_num\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mENOENT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1521\u001b[0m                             \u001b[0merr_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m': '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1522\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1523\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ffmpeg': 'ffmpeg'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 432x288 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "generate_video()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n",
      "15\n",
      "16\n",
      "17\n",
      "18\n",
      "19\n",
      "20\n",
      "21\n",
      "22\n",
      "23\n",
      "24\n",
      "25\n",
      "26\n",
      "27\n",
      "28\n",
      "29\n",
      "30\n",
      "31\n",
      "32\n",
      "33\n",
      "34\n",
      "35\n",
      "36\n",
      "37\n",
      "38\n",
      "39\n",
      "40\n",
      "41\n",
      "42\n",
      "43\n",
      "44\n",
      "45\n",
      "46\n",
      "47\n",
      "48\n",
      "49\n",
      "50\n",
      "51\n",
      "52\n",
      "53\n",
      "54\n",
      "55\n",
      "56\n",
      "57\n",
      "58\n",
      "59\n",
      "60\n",
      "61\n",
      "62\n",
      "63\n",
      "64\n",
      "65\n",
      "66\n",
      "67\n",
      "68\n",
      "69\n",
      "70\n",
      "71\n",
      "72\n",
      "73\n",
      "74\n",
      "75\n",
      "76\n",
      "77\n",
      "78\n",
      "79\n",
      "80\n",
      "81\n",
      "82\n",
      "83\n",
      "84\n",
      "85\n",
      "86\n",
      "87\n",
      "88\n",
      "89\n",
      "90\n",
      "91\n",
      "92\n",
      "93\n",
      "94\n",
      "95\n",
      "96\n",
      "97\n",
      "98\n",
      "99\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.animation as animation\n",
    "\n",
    "fig, ax = plt.subplots(1,1)\n",
    "# img = [] # some array of images\n",
    "frames = [] # for storing the generated images\n",
    "# fig = plt.figure()\n",
    "for i in range(100):\n",
    "#     p1 = ax.imshow(np.flipud(u_pred_3d[:,:,i]),vmax =1000,vmin=450,cmap = cmap,extent=[0,1,0,1],aspect = 0.75)\n",
    "#     fig.colorbar(p1, orientation='vertical',ax=ax)\n",
    "    frames.append([plt.imshow(np.flipud(u_pred_3d[:,:,i]),vmax =1000,vmin=450,cmap = cmap,extent=[0,1,0,1],aspect = 0.75)])\n",
    "    print(i)\n",
    "\n",
    "ani = animation.ArtistAnimation(fig, frames, interval=50, blit=True,\n",
    "                                repeat_delay=1000)\n",
    "#ani.save('movie.mp4')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "writergif = animation.PillowWriter(fps=10)\n",
    "ani.save('Stan_Thinplate_movie.gif',writer=writergif)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'ffmpeg': 'ffmpeg'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-35-346946443c28>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mwritervideo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0manimation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mFFMpegWriter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m60\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mani\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Stan_Thinplate_movie1.gif'\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mwriter\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mwritervideo\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36msave\u001b[0;34m(self, filename, writer, fps, dpi, codec, bitrate, extra_args, metadata, extra_anim, savefig_kwargs, progress_callback)\u001b[0m\n\u001b[1;32m   1154\u001b[0m         \u001b[0;31m# widget (both are likewise done in savefig()).\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1155\u001b[0m         \u001b[0;32mwith\u001b[0m \u001b[0mmpl\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrc_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m{\u001b[0m\u001b[0;34m'savefig.bbox'\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1156\u001b[0;31m              \u001b[0mwriter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msaving\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_fig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1157\u001b[0m              cbook._setattr_cm(self._fig.canvas,\n\u001b[1;32m   1158\u001b[0m                                _is_saving=True, manager=None):\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36msaving\u001b[0;34m(self, fig, outfile, dpi, *args, **kwargs)\u001b[0m\n\u001b[1;32m    232\u001b[0m         \"\"\"\n\u001b[1;32m    233\u001b[0m         \u001b[0;31m# This particular sequence is what contextlib.contextmanager wants\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 234\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msetup\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutfile\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdpi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    235\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    236\u001b[0m             \u001b[0;32myield\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36msetup\u001b[0;34m(self, fig, outfile, dpi)\u001b[0m\n\u001b[1;32m    324\u001b[0m         \u001b[0;31m# Run here so that grab_frame() can write the data to a pipe. This\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    325\u001b[0m         \u001b[0;31m# eliminates the need for temp files.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 326\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    327\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    328\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_run\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/matplotlib/animation.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    336\u001b[0m         self._proc = subprocess.Popen(\n\u001b[1;32m    337\u001b[0m             \u001b[0mcommand\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstdout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstderr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mPIPE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 338\u001b[0;31m             creationflags=subprocess_creation_flags)\n\u001b[0m\u001b[1;32m    339\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    340\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfinish\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, args, bufsize, executable, stdin, stdout, stderr, preexec_fn, close_fds, shell, cwd, env, universal_newlines, startupinfo, creationflags, restore_signals, start_new_session, pass_fds, encoding, errors, text)\u001b[0m\n\u001b[1;32m    773\u001b[0m                                 \u001b[0mc2pread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mc2pwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    774\u001b[0m                                 \u001b[0merrread\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrwrite\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 775\u001b[0;31m                                 restore_signals, start_new_session)\n\u001b[0m\u001b[1;32m    776\u001b[0m         \u001b[0;32mexcept\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    777\u001b[0m             \u001b[0;31m# Cleanup if the child failed starting.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/subprocess.py\u001b[0m in \u001b[0;36m_execute_child\u001b[0;34m(self, args, executable, preexec_fn, close_fds, pass_fds, cwd, env, startupinfo, creationflags, shell, p2cread, p2cwrite, c2pread, c2pwrite, errread, errwrite, restore_signals, start_new_session)\u001b[0m\n\u001b[1;32m   1520\u001b[0m                         \u001b[0;32mif\u001b[0m \u001b[0merrno_num\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0merrno\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mENOENT\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1521\u001b[0m                             \u001b[0merr_msg\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m': '\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mrepr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1522\u001b[0;31m                     \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merrno_num\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_msg\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merr_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1523\u001b[0m                 \u001b[0;32mraise\u001b[0m \u001b[0mchild_exception_type\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0merr_msg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1524\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'ffmpeg': 'ffmpeg'"
     ]
    }
   ],
   "source": [
    "writervideo = animation.FFMpegWriter(fps=60) \n",
    "ani.save('Stan_Thinplate_movie1.gif',writer=writervideo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(u_pred_3d[:,:,50])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(u_pred_3d[50,99,:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + test_loss_full[i][-1]\n",
    "print(a/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + train_loss_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
