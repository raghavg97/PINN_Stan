{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:2\n"
     ]
    }
   ],
   "source": [
    "####SUCCESFUL: IMPORTANT\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def true_2D_1(xt): #True function for 2D_1 Klein Gordon Equation x \\in [-50,50] , t \\in [0,10]\n",
    "#     y = xt[:,0]*np.cos(xt[:,1])\n",
    "#     return y.reshape(-1,1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Material Properties This link - https://www.mathworks.com/help/pde/ug/nonlinear-heat-transfer-in-a-thin-plate.html#heatTransferThinPlateExample-1\n",
    "k = 400\n",
    "rho = 8960\n",
    "cp = 386\n",
    "t_z = 0.01\n",
    "stef_bolt = 5.670373e-8\n",
    "hc = 1\n",
    "Ta = 300\n",
    "emiss = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"Navier_stan\"\n",
    "\n",
    "x = np.linspace(0,1,250).reshape(-1,1)\n",
    "y = np.linspace(0,1,250).reshape(-1,1)\n",
    "#t = np.linspace(0,1,100).reshape(-1,1) #t is actually from 0 to 5000, let us scale it to 0 to 1\n",
    "\n",
    "X,Y = np.meshgrid(x,y)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "Y = Y.flatten('F').reshape(-1,1)\n",
    " \n",
    "xy = np.hstack((X,Y))\n",
    "\n",
    "#initial_pts = np.logical_and(T==0,Y!=0).reshape(-1,)\n",
    "\n",
    "DBC_pts = (Y == 0).reshape(-1,)\n",
    "\n",
    "\n",
    "NBC_pts_x0 = (X == 0).reshape(-1,)\n",
    "NBC_pts_x1 = (X == 1).reshape(-1,)\n",
    "\n",
    "NBC_pts_y0 = (Y == 0).reshape(-1,)\n",
    "NBC_pts_y1 = (Y == 1).reshape(-1,)\n",
    "\n",
    "#xy_initial = xyt[initial_pts,:]\n",
    "xy_DBC = xy[DBC_pts,:]\n",
    "\n",
    "xy_NBC_x0 = xy[NBC_pts_x0,:]\n",
    "xy_NBC_x1 = xy[NBC_pts_x1,:]\n",
    "\n",
    "#xyt_NBC_y0 = xyt[NBC_pts_y0,:]\n",
    "xy_NBC_y1 = xy[NBC_pts_y1,:]\n",
    "\n",
    "#u_initial = np.zeros((np.shape(xy_initial)[0],1))\n",
    "u_DBC = 1000*np.ones((np.shape(xy_DBC)[0],1))\n",
    "\n",
    "#xy_I_DBC = np.vstack((xy_initial,xyt_DBC))\n",
    "#xyt_NBC = np.vstack((xyt_NBC_1,xyt_NBC_2,xyt_NBC_3,xyt_NBC_4))\n",
    "xy_NBC_x = np.vstack((xy_NBC_x0,xy_NBC_x1))\n",
    "#xyt_NBC_y = np.vstack((xyt_NBC_y0,xyt_NBC_y1))\n",
    "xy_NBC_y = np.vstack((xy_NBC_y1))\n",
    "\n",
    "#u_I_DBC = np.vstack((u_initial,u_DBC))\n",
    "\n",
    "xy_test_tensor = torch.from_numpy(xy).float().to(device)\n",
    "\n",
    "lb_xy = xy[0]\n",
    "ub_xy = xy[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_D,N_N,N_f,seed):\n",
    "    '''Boundary Conditions''' \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    #choose random N_u points for training\n",
    "    idx = np.random.choice(xy_DBC.shape[0], N_D, replace=False) \n",
    "    xy_D = xy_DBC[idx,:] #choose indices from  set 'idx' (x,t)\n",
    "    u_D = u_DBC[idx].reshape(-1,1)      #choose corresponding u\n",
    "\n",
    "    idx = np.random.choice(xy_NBC_x.shape[0], N_D, replace=False) \n",
    "    xy_Nx = xy_NBC_x[idx,:] #choose indices from  set 'idx' (x,t)\n",
    "\n",
    "    idx = np.random.choice(xy_NBC_y.shape[0], N_D, replace=False) \n",
    "    xy_Ny = xy_NBC_y[idx,:] #choose indices from  set 'idx' (x,t)\n",
    "\n",
    "    '''Collocation Points'''\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    samples = sampling(N_f)\n",
    "    \n",
    "    xy_coll = lb_xy + (ub_xy - lb_xy)*samples\n",
    "    xy_coll = np.vstack((xy_coll, xy_D,xy_Nx,xy_Ny)) # append training points to collocation points \n",
    "\n",
    "    return xy_coll, xy_D, u_D, xy_Nx,xy_Ny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        self.iter = 0\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)   \n",
    "        \n",
    "        self.beta = Parameter(torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "        \n",
    "        self.train_loss = []\n",
    "        self.test_loss = []\n",
    "        \n",
    "        self.beta_val = []\n",
    "        \n",
    "        self.n = torch.tensor(1.0)\n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xy):\n",
    "        if torch.is_tensor(xy) != True:         \n",
    "            xy = torch.from_numpy(xy)                \n",
    "        \n",
    "        ubxy = torch.from_numpy(ub_xy).float().to(device)\n",
    "        lbxy = torch.from_numpy(lb_xy).float().to(device)\n",
    "\n",
    "                      \n",
    "        #preprocessing input \n",
    "        xy = (xy - lbxy)/(ubxy - lbxy)\n",
    "        \n",
    "        #convert to float\n",
    "        a = xy.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            z1 = self.activation(z) \n",
    "            a = z1 + self.beta[:,i]*z*z1\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_D(self,xy_D,u_D):\n",
    "                \n",
    "        loss_bc = self.loss_function(self.forward(xy_D), u_D)\n",
    "                \n",
    "        return loss_bc\n",
    "    \n",
    "    def loss_N(self,xy_Nx,xy_Ny,N_hat):\n",
    "        \n",
    "        g1 = xy_Nx.clone()             \n",
    "        g1.requires_grad = True\n",
    "        u1 = self.forward(g1)\n",
    "        \n",
    "        u1_x_y = autograd.grad(u1,g1,torch.ones([xy_Nx.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        du1_dx = u1_x_y[:,[0]]\n",
    "        \n",
    "        g2 = xy_Ny.clone()             \n",
    "        g2.requires_grad = True\n",
    "        u2 = self.forward(g2)\n",
    "        \n",
    "        u2_x_y = autograd.grad(u2,g2,torch.ones([xy_Ny.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        du2_dy = u2_x_y[:,[1]]\n",
    "               \n",
    "        loss_N1 = self.loss_function(du1_dx,N_hat)\n",
    "        loss_N2 = self.loss_function(du2_dy,N_hat)\n",
    "        \n",
    "        #return loss_N1+loss_N2       \n",
    "        return loss_N1 + loss_N2\n",
    "    \n",
    "    def loss_PDE(self, xy_coll, f_hat):\n",
    "        \n",
    "        g = xy_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_y = autograd.grad(u,g,torch.ones([xy_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_xx_yy = autograd.grad(u_x_y,g,torch.ones(xy_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        #du_dt = u_x_y_t[:,[2]]\n",
    "        \n",
    "        d2u_dx2 = u_xx_yy[:,[0]]\n",
    "        d2u_dy2 = u_xx_yy[:,[1]]    \n",
    "        \n",
    "\n",
    "        f = - k*t_z*(d2u_dx2+d2u_dy2) + 2*hc*(u-Ta) + 2*emiss*stef_bolt*(torch.pow(u,4)-Ta**4) \n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xy_D,u_D,xy_Nx,xy_Ny,N_hat,xy_coll,f_hat):\n",
    "\n",
    "        loss_D = self.loss_D(xy_D,u_D)\n",
    "        loss_N = self.loss_N(xy_Nx,xy_Ny,N_hat)\n",
    "        loss_f = self.loss_PDE(xy_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_D + loss_N + loss_f\n",
    "        \n",
    "        #print(self.iter,\"loss_D:\",loss_D.cpu().detach().numpy(),\"loss_N:\",loss_N.cpu().detach().numpy(),\"loss_f:\",loss_f.cpu().detach().numpy())\n",
    "        \n",
    "        return loss_val\n",
    "     \n",
    "    'callable for optimizer'                                       \n",
    "    def closure(self):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = self.loss(xy_D,u_D,xy_Nx,xy_Ny,N_hat,xy_coll,f_hat)\n",
    "        self.train_loss.append(loss.cpu().detach().numpy())\n",
    "        \n",
    "        u_pred = self.test(xy_test_tensor)\n",
    "        #self.test_loss.append(np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))) #Commented because no true values yet\n",
    "        self.beta_val.append(self.beta.cpu().detach().numpy())\n",
    "        \n",
    "        #print(self.iter,\"Train Loss\",self.train_loss[-1],\"Test Loss\",self.test_loss[-1])\n",
    "        print(self.iter,\"Train Loss\",self.train_loss[-1])\n",
    "        \n",
    "        loss.backward()\n",
    "        self.iter += 1\n",
    "  \n",
    "\n",
    "        return loss        \n",
    "    'test neural network'\n",
    "    def test(self,xy_test_tensor):\n",
    "        u_pred = self.forward(xy_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0 Train Loss 2134119.0\n",
      "1 Train Loss 2130068.5\n",
      "2 Train Loss 2105508.2\n",
      "3 Train Loss 2058562.6\n",
      "4 Train Loss 2043182.8\n",
      "5 Train Loss 2025923.5\n",
      "6 Train Loss 1986442.9\n",
      "7 Train Loss 1891697.0\n",
      "8 Train Loss 1768869.8\n",
      "9 Train Loss 15808501.0\n",
      "10 Train Loss 1726310.0\n",
      "11 Train Loss 1976480.0\n",
      "12 Train Loss 1660680.5\n",
      "13 Train Loss 1591251.2\n",
      "14 Train Loss 1470817.1\n",
      "15 Train Loss 1362355.8\n",
      "16 Train Loss 1312593.1\n",
      "17 Train Loss 1276756.5\n",
      "18 Train Loss 1231463.1\n",
      "19 Train Loss 1936650.2\n",
      "20 Train Loss 1168696.6\n",
      "21 Train Loss 1150310.6\n",
      "22 Train Loss 1127648.6\n",
      "23 Train Loss 1103158.4\n",
      "24 Train Loss 1080551.4\n",
      "25 Train Loss 1040945.0\n",
      "26 Train Loss 1019205.5\n",
      "27 Train Loss 1013898.44\n",
      "28 Train Loss 1002432.25\n",
      "29 Train Loss 997019.1\n",
      "30 Train Loss 991039.75\n",
      "31 Train Loss 989087.3\n",
      "32 Train Loss 986856.7\n",
      "33 Train Loss 984520.3\n",
      "34 Train Loss 980834.9\n",
      "35 Train Loss 976053.56\n",
      "36 Train Loss 973231.56\n",
      "37 Train Loss 970178.9\n",
      "38 Train Loss 965934.94\n",
      "39 Train Loss 963942.7\n",
      "40 Train Loss 961676.25\n",
      "41 Train Loss 957500.1\n",
      "42 Train Loss 951810.44\n",
      "43 Train Loss 955082.2\n",
      "44 Train Loss 945933.1\n",
      "45 Train Loss 962301.5\n",
      "46 Train Loss 942058.44\n",
      "47 Train Loss 937216.06\n",
      "48 Train Loss 934137.44\n",
      "49 Train Loss 931641.75\n",
      "50 Train Loss 929755.6\n",
      "51 Train Loss 926415.75\n",
      "52 Train Loss 922679.25\n",
      "53 Train Loss 920043.7\n",
      "54 Train Loss 916160.0\n",
      "55 Train Loss 906812.94\n",
      "56 Train Loss 893727.2\n",
      "57 Train Loss 892728.8\n",
      "58 Train Loss 886886.75\n",
      "59 Train Loss 857413.9\n",
      "60 Train Loss 872715.7\n",
      "61 Train Loss 836641.9\n",
      "62 Train Loss 825256.25\n",
      "63 Train Loss 750710.44\n",
      "64 Train Loss 742655.2\n",
      "65 Train Loss 732625.06\n",
      "66 Train Loss 710637.6\n",
      "67 Train Loss 652076.94\n",
      "68 Train Loss 624405.1\n",
      "69 Train Loss 886839.75\n",
      "70 Train Loss 607951.44\n",
      "71 Train Loss 589243.9\n",
      "72 Train Loss 537883.2\n",
      "73 Train Loss 519515.25\n",
      "74 Train Loss 498151.22\n",
      "75 Train Loss 487267.7\n",
      "76 Train Loss 474214.28\n",
      "77 Train Loss 465939.12\n",
      "78 Train Loss 455892.06\n",
      "79 Train Loss 440384.5\n",
      "80 Train Loss 542656.1\n",
      "81 Train Loss 421728.38\n",
      "82 Train Loss 409752.88\n",
      "83 Train Loss 402279.75\n",
      "84 Train Loss 402925.6\n",
      "85 Train Loss 399157.4\n",
      "86 Train Loss 393712.2\n",
      "87 Train Loss 419588.0\n",
      "88 Train Loss 388039.4\n",
      "89 Train Loss 381528.7\n",
      "90 Train Loss 375755.72\n",
      "91 Train Loss 370301.0\n",
      "92 Train Loss 378511.12\n",
      "93 Train Loss 366580.56\n",
      "94 Train Loss 356784.7\n",
      "95 Train Loss 347672.97\n",
      "96 Train Loss 358455.84\n",
      "97 Train Loss 344698.9\n",
      "98 Train Loss 342626.97\n",
      "99 Train Loss 335080.03\n",
      "100 Train Loss 324639.34\n",
      "101 Train Loss 319051.1\n",
      "102 Train Loss 313925.1\n",
      "103 Train Loss 311766.2\n",
      "104 Train Loss 312567.66\n",
      "105 Train Loss 302398.06\n",
      "106 Train Loss 296465.38\n",
      "107 Train Loss 288515.53\n",
      "108 Train Loss 282101.06\n",
      "109 Train Loss 272038.06\n",
      "110 Train Loss 297572.34\n",
      "111 Train Loss 265276.6\n",
      "112 Train Loss 265307.62\n",
      "113 Train Loss 263179.7\n",
      "114 Train Loss 260259.28\n",
      "115 Train Loss 281442.94\n",
      "116 Train Loss 255460.97\n",
      "117 Train Loss 249912.72\n",
      "118 Train Loss 247192.16\n",
      "119 Train Loss 244514.4\n",
      "120 Train Loss 266022.0\n",
      "121 Train Loss 241164.28\n",
      "122 Train Loss 264242.78\n",
      "123 Train Loss 238735.86\n",
      "124 Train Loss 236989.66\n",
      "125 Train Loss 235819.98\n",
      "126 Train Loss 234560.73\n",
      "127 Train Loss 231499.06\n",
      "128 Train Loss 231644.64\n",
      "129 Train Loss 230261.66\n",
      "130 Train Loss 227709.7\n",
      "131 Train Loss 225047.34\n",
      "132 Train Loss 222659.36\n",
      "133 Train Loss 222178.25\n",
      "134 Train Loss 220264.38\n",
      "135 Train Loss 227481.34\n",
      "136 Train Loss 218209.64\n",
      "137 Train Loss 216263.86\n",
      "138 Train Loss 214132.03\n",
      "139 Train Loss 212950.75\n",
      "140 Train Loss 212468.62\n",
      "141 Train Loss 211549.02\n",
      "142 Train Loss 210208.31\n",
      "143 Train Loss 208655.05\n",
      "144 Train Loss 205834.55\n",
      "145 Train Loss 202965.11\n",
      "146 Train Loss 201302.45\n",
      "147 Train Loss 199123.23\n",
      "148 Train Loss 194640.39\n",
      "149 Train Loss 980352.7\n",
      "150 Train Loss 192951.56\n",
      "151 Train Loss 191572.16\n",
      "152 Train Loss 274598.03\n",
      "153 Train Loss 190115.11\n",
      "154 Train Loss 189523.36\n",
      "155 Train Loss 186949.03\n",
      "156 Train Loss 184362.03\n",
      "157 Train Loss 184116.94\n",
      "158 Train Loss 182639.34\n",
      "159 Train Loss 181035.12\n",
      "160 Train Loss 177803.33\n",
      "161 Train Loss 172585.19\n",
      "162 Train Loss 170575.03\n",
      "163 Train Loss 167566.78\n",
      "164 Train Loss 158563.16\n",
      "165 Train Loss 174961.16\n",
      "166 Train Loss 146674.45\n",
      "167 Train Loss 501849.25\n",
      "168 Train Loss 140998.84\n",
      "169 Train Loss 143451.16\n",
      "170 Train Loss 137297.31\n",
      "171 Train Loss 132068.17\n",
      "172 Train Loss 938097.9\n",
      "173 Train Loss 128128.695\n",
      "174 Train Loss 126681.86\n",
      "175 Train Loss 127099.12\n",
      "176 Train Loss 123384.63\n",
      "177 Train Loss 118965.06\n",
      "178 Train Loss 110557.72\n",
      "179 Train Loss 119623.66\n",
      "180 Train Loss 104760.09\n",
      "181 Train Loss 99191.61\n",
      "182 Train Loss 90320.945\n",
      "183 Train Loss 94902.72\n",
      "184 Train Loss 87969.42\n",
      "185 Train Loss 87316.93\n",
      "186 Train Loss 86396.39\n",
      "187 Train Loss 85247.01\n",
      "188 Train Loss 83364.125\n",
      "189 Train Loss 81235.34\n",
      "190 Train Loss 78912.41\n",
      "191 Train Loss 76912.53\n",
      "192 Train Loss 75272.14\n",
      "193 Train Loss 74014.21\n",
      "194 Train Loss 72429.25\n",
      "195 Train Loss 72484.18\n",
      "196 Train Loss 71202.88\n",
      "197 Train Loss 70231.75\n",
      "198 Train Loss 69510.59\n",
      "199 Train Loss 68217.81\n",
      "200 Train Loss 66343.016\n",
      "201 Train Loss 64738.96\n",
      "202 Train Loss 63797.684\n",
      "203 Train Loss 62838.37\n",
      "204 Train Loss 61765.05\n",
      "205 Train Loss 59745.59\n",
      "206 Train Loss 57647.254\n",
      "207 Train Loss 56233.42\n",
      "208 Train Loss 54616.582\n",
      "209 Train Loss 54704.055\n",
      "210 Train Loss 52973.86\n",
      "211 Train Loss 51183.72\n",
      "212 Train Loss 48901.117\n",
      "213 Train Loss 46046.836\n",
      "214 Train Loss 42205.383\n",
      "215 Train Loss 40757.062\n",
      "216 Train Loss 40230.08\n",
      "217 Train Loss 39218.805\n",
      "218 Train Loss 39169.312\n",
      "219 Train Loss 38729.617\n",
      "220 Train Loss 37298.89\n",
      "221 Train Loss 2230846.5\n",
      "222 Train Loss 36777.293\n",
      "223 Train Loss 35874.324\n",
      "224 Train Loss 35830.562\n",
      "225 Train Loss 34345.156\n",
      "226 Train Loss 33416.684\n",
      "227 Train Loss 32771.203\n",
      "228 Train Loss 31929.107\n",
      "229 Train Loss 30389.09\n",
      "230 Train Loss 30060.492\n",
      "231 Train Loss 29525.082\n",
      "232 Train Loss 28472.184\n",
      "233 Train Loss 27739.45\n",
      "234 Train Loss 26388.617\n",
      "235 Train Loss 24867.72\n",
      "236 Train Loss 23924.906\n",
      "237 Train Loss 23908.086\n",
      "238 Train Loss 23369.543\n",
      "239 Train Loss 22884.5\n",
      "240 Train Loss 21955.477\n",
      "241 Train Loss 20475.646\n",
      "242 Train Loss 20002.453\n",
      "243 Train Loss 18929.559\n",
      "244 Train Loss 19305.049\n",
      "245 Train Loss 18381.863\n",
      "246 Train Loss 17877.656\n",
      "247 Train Loss 16826.734\n",
      "248 Train Loss 16000.505\n",
      "249 Train Loss 15308.048\n",
      "250 Train Loss 14992.995\n",
      "251 Train Loss 14892.98\n",
      "252 Train Loss 14745.1\n",
      "253 Train Loss 14317.887\n",
      "254 Train Loss 13349.418\n",
      "255 Train Loss 13488.976\n",
      "256 Train Loss 12970.357\n",
      "257 Train Loss 12753.195\n",
      "258 Train Loss 12515.158\n",
      "259 Train Loss 12411.176\n",
      "260 Train Loss 12303.559\n",
      "261 Train Loss 12042.604\n",
      "262 Train Loss 11604.615\n",
      "263 Train Loss 11375.291\n",
      "264 Train Loss 11181.338\n",
      "265 Train Loss 11075.558\n",
      "266 Train Loss 10990.344\n",
      "267 Train Loss 10895.847\n",
      "268 Train Loss 10685.422\n",
      "269 Train Loss 10151.994\n",
      "270 Train Loss 9431.875\n",
      "271 Train Loss 9242.196\n",
      "272 Train Loss 9023.033\n",
      "273 Train Loss 8630.933\n",
      "274 Train Loss 8550.752\n",
      "275 Train Loss 8446.423\n",
      "276 Train Loss 8157.9346\n",
      "277 Train Loss 7840.6895\n",
      "278 Train Loss 7660.743\n",
      "279 Train Loss 7435.949\n",
      "280 Train Loss 7160.792\n",
      "281 Train Loss 6866.344\n",
      "282 Train Loss 6746.8936\n",
      "283 Train Loss 6843.8755\n",
      "284 Train Loss 6612.5645\n",
      "285 Train Loss 6559.309\n",
      "286 Train Loss 6356.715\n",
      "287 Train Loss 6293.763\n",
      "288 Train Loss 6109.5977\n",
      "289 Train Loss 6035.234\n",
      "290 Train Loss 5902.077\n",
      "291 Train Loss 5728.7744\n",
      "292 Train Loss 5523.2627\n",
      "293 Train Loss 5339.111\n",
      "294 Train Loss 4995.99\n",
      "295 Train Loss 4987.124\n",
      "296 Train Loss 4873.3857\n",
      "297 Train Loss 4871.5317\n",
      "298 Train Loss 4812.8804\n",
      "299 Train Loss 4770.057\n",
      "300 Train Loss 4732.936\n",
      "301 Train Loss 4707.197\n",
      "302 Train Loss 4694.52\n",
      "303 Train Loss 4685.256\n",
      "304 Train Loss 4676.5635\n",
      "305 Train Loss 4665.631\n",
      "306 Train Loss 4642.7866\n",
      "307 Train Loss 4586.8193\n",
      "308 Train Loss 4521.9917\n",
      "309 Train Loss 4430.1685\n",
      "310 Train Loss 4288.2324\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "311 Train Loss 4214.189\n",
      "312 Train Loss 4135.432\n",
      "313 Train Loss 4085.1638\n",
      "314 Train Loss 4024.063\n",
      "315 Train Loss 3926.8115\n",
      "316 Train Loss 3843.1387\n",
      "317 Train Loss 3757.1738\n",
      "318 Train Loss 3708.0237\n",
      "319 Train Loss 3680.9207\n",
      "320 Train Loss 3656.2686\n",
      "321 Train Loss 3615.21\n",
      "322 Train Loss 3521.1123\n",
      "323 Train Loss 3463.695\n",
      "324 Train Loss 3359.656\n",
      "325 Train Loss 3267.319\n",
      "326 Train Loss 3108.8599\n",
      "327 Train Loss 2975.3037\n",
      "328 Train Loss 3461.1855\n",
      "329 Train Loss 2874.4038\n",
      "330 Train Loss 2767.538\n",
      "331 Train Loss 2626.5034\n",
      "332 Train Loss 2582.9978\n",
      "333 Train Loss 2489.4512\n",
      "334 Train Loss 2431.7969\n",
      "335 Train Loss 2388.106\n",
      "336 Train Loss 2330.2075\n",
      "337 Train Loss 2260.6343\n",
      "338 Train Loss 2137.0513\n",
      "339 Train Loss 2041.3892\n",
      "340 Train Loss 5463.5747\n",
      "341 Train Loss 2010.4614\n",
      "342 Train Loss 1921.4926\n",
      "343 Train Loss 1827.291\n",
      "344 Train Loss 1773.4108\n",
      "345 Train Loss 1732.3405\n",
      "346 Train Loss 1687.7659\n",
      "347 Train Loss 1698.1304\n",
      "348 Train Loss 1655.7991\n",
      "349 Train Loss 1842.7776\n",
      "350 Train Loss 1643.6732\n",
      "351 Train Loss 1608.8799\n",
      "352 Train Loss 1586.4723\n",
      "353 Train Loss 1571.5406\n",
      "354 Train Loss 1561.8501\n",
      "355 Train Loss 1557.1536\n",
      "356 Train Loss 1553.2635\n",
      "357 Train Loss 1547.716\n",
      "358 Train Loss 1540.7052\n",
      "359 Train Loss 1536.6271\n",
      "360 Train Loss 1529.6721\n",
      "361 Train Loss 1529.0652\n",
      "362 Train Loss 1525.0839\n",
      "363 Train Loss 1512.7786\n",
      "364 Train Loss 1504.657\n",
      "365 Train Loss 1499.8933\n",
      "366 Train Loss 1495.5284\n",
      "367 Train Loss 1479.9583\n",
      "368 Train Loss 1445.2972\n",
      "369 Train Loss 1492.9814\n",
      "370 Train Loss 1432.8381\n",
      "371 Train Loss 1394.3298\n",
      "372 Train Loss 1353.5161\n",
      "373 Train Loss 1484.1841\n",
      "374 Train Loss 1333.2637\n",
      "375 Train Loss 1304.3038\n",
      "376 Train Loss 1266.0459\n",
      "377 Train Loss 1212.5881\n",
      "378 Train Loss 1217.1523\n",
      "379 Train Loss 1191.5673\n",
      "380 Train Loss 1193.191\n",
      "381 Train Loss 1164.8142\n",
      "382 Train Loss 1138.2848\n",
      "383 Train Loss 1108.534\n",
      "384 Train Loss 1095.1289\n",
      "385 Train Loss 1085.3744\n",
      "386 Train Loss 1081.4628\n",
      "387 Train Loss 1075.1941\n",
      "388 Train Loss 1066.386\n",
      "389 Train Loss 1055.8657\n",
      "390 Train Loss 1043.5181\n",
      "391 Train Loss 1035.7366\n",
      "392 Train Loss 1033.2977\n",
      "393 Train Loss 1029.6411\n",
      "394 Train Loss 1015.66583\n",
      "395 Train Loss 1000.80347\n",
      "396 Train Loss 984.02\n",
      "397 Train Loss 975.49414\n",
      "398 Train Loss 962.11194\n",
      "399 Train Loss 952.84955\n",
      "400 Train Loss 947.8518\n",
      "401 Train Loss 945.66315\n",
      "402 Train Loss 944.3164\n",
      "403 Train Loss 943.2423\n",
      "404 Train Loss 940.93085\n",
      "405 Train Loss 935.1073\n",
      "406 Train Loss 924.74805\n",
      "407 Train Loss 915.04407\n",
      "408 Train Loss 903.62976\n",
      "409 Train Loss 895.7449\n",
      "410 Train Loss 889.9724\n",
      "411 Train Loss 883.8575\n",
      "412 Train Loss 875.5043\n",
      "413 Train Loss 870.49554\n",
      "414 Train Loss 863.4859\n",
      "415 Train Loss 858.4048\n",
      "416 Train Loss 847.182\n",
      "417 Train Loss 837.88275\n",
      "418 Train Loss 823.75696\n",
      "419 Train Loss 801.0748\n",
      "420 Train Loss 930.62714\n",
      "421 Train Loss 794.7457\n",
      "422 Train Loss 763.68964\n",
      "423 Train Loss 756.14087\n",
      "424 Train Loss 740.27075\n",
      "425 Train Loss 734.8542\n",
      "426 Train Loss 729.03284\n",
      "427 Train Loss 724.0127\n",
      "428 Train Loss 718.56506\n",
      "429 Train Loss 709.47644\n",
      "430 Train Loss 695.6288\n",
      "431 Train Loss 693.13385\n",
      "432 Train Loss 682.24146\n",
      "433 Train Loss 726.7855\n",
      "434 Train Loss 657.22217\n",
      "435 Train Loss 638.0703\n",
      "436 Train Loss 624.73663\n",
      "437 Train Loss 611.7317\n",
      "438 Train Loss 615.5473\n",
      "439 Train Loss 601.31445\n",
      "440 Train Loss 582.2159\n",
      "441 Train Loss 566.7743\n",
      "442 Train Loss 552.68054\n",
      "443 Train Loss 533.49713\n",
      "444 Train Loss 522.20264\n",
      "445 Train Loss 513.9433\n",
      "446 Train Loss 511.86346\n",
      "447 Train Loss 508.948\n",
      "448 Train Loss 507.63782\n",
      "449 Train Loss 504.98642\n",
      "450 Train Loss 502.13992\n",
      "451 Train Loss 498.28726\n",
      "452 Train Loss 495.32574\n",
      "453 Train Loss 493.78326\n",
      "454 Train Loss 493.08615\n",
      "455 Train Loss 492.10938\n",
      "456 Train Loss 490.9536\n",
      "457 Train Loss 489.01184\n",
      "458 Train Loss 486.6198\n",
      "459 Train Loss 483.78473\n",
      "460 Train Loss 480.6836\n",
      "461 Train Loss 482.6102\n",
      "462 Train Loss 479.15045\n",
      "463 Train Loss 476.61844\n",
      "464 Train Loss 472.59436\n",
      "465 Train Loss 466.99945\n",
      "466 Train Loss 460.47083\n",
      "467 Train Loss 449.42456\n",
      "468 Train Loss 439.7215\n",
      "469 Train Loss 439.15332\n",
      "470 Train Loss 433.71783\n",
      "471 Train Loss 427.93402\n",
      "472 Train Loss 419.9408\n",
      "473 Train Loss 404.7564\n",
      "474 Train Loss 388.9276\n",
      "475 Train Loss 526.0877\n",
      "476 Train Loss 383.90115\n",
      "477 Train Loss 374.81097\n",
      "478 Train Loss 366.7281\n",
      "479 Train Loss 362.77087\n",
      "480 Train Loss 359.59308\n",
      "481 Train Loss 356.02734\n",
      "482 Train Loss 356.66412\n",
      "483 Train Loss 354.96985\n",
      "484 Train Loss 353.1568\n",
      "485 Train Loss 352.14236\n",
      "486 Train Loss 351.3135\n",
      "487 Train Loss 350.52197\n",
      "488 Train Loss 349.30908\n",
      "489 Train Loss 347.87994\n",
      "490 Train Loss 346.14575\n",
      "491 Train Loss 346.7309\n",
      "492 Train Loss 345.48547\n",
      "493 Train Loss 344.09338\n",
      "494 Train Loss 341.958\n",
      "495 Train Loss 341.1186\n",
      "496 Train Loss 340.84235\n",
      "497 Train Loss 340.53674\n",
      "498 Train Loss 340.21576\n",
      "499 Train Loss 339.31647\n",
      "500 Train Loss 342.4826\n",
      "501 Train Loss 339.07117\n",
      "502 Train Loss 337.90173\n",
      "503 Train Loss 335.61716\n",
      "504 Train Loss 328.62747\n",
      "505 Train Loss 320.17493\n",
      "506 Train Loss 312.08124\n",
      "507 Train Loss 308.60184\n",
      "508 Train Loss 302.45306\n",
      "509 Train Loss 290.6905\n",
      "510 Train Loss 285.9734\n",
      "511 Train Loss 282.21588\n",
      "512 Train Loss 279.45648\n",
      "513 Train Loss 276.08224\n",
      "514 Train Loss 271.30542\n",
      "515 Train Loss 271.66016\n",
      "516 Train Loss 269.9803\n",
      "517 Train Loss 268.02097\n",
      "518 Train Loss 265.64056\n",
      "519 Train Loss 267.8205\n",
      "520 Train Loss 264.814\n",
      "521 Train Loss 262.35028\n",
      "522 Train Loss 258.98975\n",
      "523 Train Loss 257.34848\n",
      "524 Train Loss 253.65092\n",
      "525 Train Loss 259.6659\n",
      "526 Train Loss 252.87161\n",
      "527 Train Loss 253.93478\n",
      "528 Train Loss 251.51077\n",
      "529 Train Loss 249.24136\n",
      "530 Train Loss 247.40268\n",
      "531 Train Loss 245.85399\n",
      "532 Train Loss 243.6589\n",
      "533 Train Loss 237.9321\n",
      "534 Train Loss 234.66306\n",
      "535 Train Loss 236.47379\n",
      "536 Train Loss 230.74316\n",
      "537 Train Loss 320.56168\n",
      "538 Train Loss 226.55318\n",
      "539 Train Loss 226.60327\n",
      "540 Train Loss 224.42938\n",
      "541 Train Loss 220.60837\n",
      "542 Train Loss 214.58383\n",
      "543 Train Loss 215.61093\n",
      "544 Train Loss 212.30643\n",
      "545 Train Loss 207.57855\n",
      "546 Train Loss 204.24207\n",
      "547 Train Loss 201.80121\n",
      "548 Train Loss 204.43143\n",
      "549 Train Loss 199.59554\n",
      "550 Train Loss 196.82454\n",
      "551 Train Loss 194.4964\n",
      "552 Train Loss 192.28848\n",
      "553 Train Loss 194.83176\n",
      "554 Train Loss 191.34398\n",
      "555 Train Loss 188.86893\n",
      "556 Train Loss 186.62675\n",
      "557 Train Loss 184.21231\n",
      "558 Train Loss 183.25394\n",
      "559 Train Loss 181.95425\n",
      "560 Train Loss 181.14679\n",
      "561 Train Loss 180.91846\n",
      "562 Train Loss 180.681\n",
      "563 Train Loss 180.39062\n",
      "564 Train Loss 180.04079\n",
      "565 Train Loss 179.56018\n",
      "566 Train Loss 179.44235\n",
      "567 Train Loss 179.11836\n",
      "568 Train Loss 178.90747\n",
      "569 Train Loss 178.38126\n",
      "570 Train Loss 178.05066\n",
      "571 Train Loss 177.3758\n",
      "572 Train Loss 176.94579\n",
      "573 Train Loss 176.8663\n",
      "574 Train Loss 176.74625\n",
      "575 Train Loss 176.43134\n",
      "576 Train Loss 176.18112\n",
      "577 Train Loss 175.94905\n",
      "578 Train Loss 175.6713\n",
      "579 Train Loss 175.41814\n",
      "580 Train Loss 176.63673\n",
      "581 Train Loss 175.20023\n",
      "582 Train Loss 174.97104\n",
      "583 Train Loss 174.33835\n",
      "584 Train Loss 173.63196\n",
      "585 Train Loss 178.09247\n",
      "586 Train Loss 172.94287\n",
      "587 Train Loss 171.45633\n",
      "588 Train Loss 168.10384\n",
      "589 Train Loss 166.52048\n",
      "590 Train Loss 165.63416\n",
      "591 Train Loss 164.81267\n",
      "592 Train Loss 164.18839\n",
      "593 Train Loss 163.59544\n",
      "594 Train Loss 163.26819\n",
      "595 Train Loss 163.05109\n",
      "596 Train Loss 162.58194\n",
      "597 Train Loss 162.37383\n",
      "598 Train Loss 162.11302\n",
      "599 Train Loss 161.51202\n",
      "600 Train Loss 160.39232\n",
      "601 Train Loss 158.88605\n",
      "602 Train Loss 157.3843\n",
      "603 Train Loss 155.32733\n",
      "604 Train Loss 163.77591\n",
      "605 Train Loss 154.61493\n",
      "606 Train Loss 152.78696\n",
      "607 Train Loss 151.53375\n",
      "608 Train Loss 149.976\n",
      "609 Train Loss 149.8642\n",
      "610 Train Loss 149.34674\n",
      "611 Train Loss 148.39673\n",
      "612 Train Loss 147.55232\n",
      "613 Train Loss 146.46545\n",
      "614 Train Loss 145.43988\n",
      "615 Train Loss 145.14003\n",
      "616 Train Loss 143.8893\n",
      "617 Train Loss 143.43013\n",
      "618 Train Loss 143.0127\n",
      "619 Train Loss 142.63919\n",
      "620 Train Loss 142.37762\n",
      "621 Train Loss 142.10237\n",
      "622 Train Loss 141.53143\n",
      "623 Train Loss 141.09499\n",
      "624 Train Loss 140.48494\n",
      "625 Train Loss 139.69061\n",
      "626 Train Loss 139.30376\n",
      "627 Train Loss 138.6373\n",
      "628 Train Loss 138.12334\n",
      "629 Train Loss 137.50151\n",
      "630 Train Loss 137.317\n",
      "631 Train Loss 136.96898\n",
      "632 Train Loss 136.75182\n",
      "633 Train Loss 136.63768\n",
      "634 Train Loss 136.33516\n",
      "635 Train Loss 136.11606\n",
      "636 Train Loss 135.71437\n",
      "637 Train Loss 135.44632\n",
      "638 Train Loss 134.99228\n",
      "639 Train Loss 134.63573\n",
      "640 Train Loss 134.34198\n",
      "641 Train Loss 134.06903\n",
      "642 Train Loss 133.76225\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "643 Train Loss 133.34811\n",
      "644 Train Loss 133.42908\n",
      "645 Train Loss 133.11078\n",
      "646 Train Loss 132.34157\n",
      "647 Train Loss 131.87164\n",
      "648 Train Loss 131.05551\n",
      "649 Train Loss 130.43288\n",
      "650 Train Loss 129.88309\n",
      "651 Train Loss 129.23358\n",
      "652 Train Loss 128.90076\n",
      "653 Train Loss 128.54521\n",
      "654 Train Loss 128.24002\n",
      "655 Train Loss 127.91709\n",
      "656 Train Loss 127.55344\n",
      "657 Train Loss 127.3648\n",
      "658 Train Loss 126.92574\n",
      "659 Train Loss 126.82991\n",
      "660 Train Loss 126.17215\n",
      "661 Train Loss 126.04303\n",
      "662 Train Loss 125.94836\n",
      "663 Train Loss 125.61515\n",
      "664 Train Loss 125.31784\n",
      "665 Train Loss 125.11578\n",
      "666 Train Loss 124.7723\n",
      "667 Train Loss 124.64528\n",
      "668 Train Loss 124.53885\n",
      "669 Train Loss 124.38881\n",
      "670 Train Loss 124.32803\n",
      "671 Train Loss 124.285614\n",
      "672 Train Loss 124.233734\n",
      "673 Train Loss 124.19586\n",
      "674 Train Loss 124.15355\n",
      "675 Train Loss 124.1013\n",
      "676 Train Loss 124.05466\n",
      "677 Train Loss 123.95363\n",
      "678 Train Loss 123.8512\n",
      "679 Train Loss 123.70108\n",
      "680 Train Loss 123.542435\n",
      "681 Train Loss 123.41837\n",
      "682 Train Loss 123.32797\n",
      "683 Train Loss 123.3279\n",
      "684 Train Loss 123.26961\n",
      "685 Train Loss 123.21946\n",
      "686 Train Loss 123.115654\n",
      "687 Train Loss 123.07753\n",
      "688 Train Loss 122.949524\n",
      "689 Train Loss 122.94658\n",
      "690 Train Loss 122.83895\n",
      "691 Train Loss 122.49261\n",
      "692 Train Loss 122.282486\n",
      "693 Train Loss 121.881165\n",
      "694 Train Loss 121.59245\n",
      "695 Train Loss 121.33839\n",
      "696 Train Loss 121.860245\n",
      "697 Train Loss 121.1004\n",
      "698 Train Loss 120.97237\n",
      "699 Train Loss 120.44681\n",
      "700 Train Loss 120.140076\n",
      "701 Train Loss 129.00482\n",
      "702 Train Loss 119.9431\n",
      "703 Train Loss 119.525375\n",
      "704 Train Loss 119.18346\n",
      "705 Train Loss 119.00875\n",
      "706 Train Loss 119.15349\n",
      "707 Train Loss 118.77866\n",
      "708 Train Loss 118.5428\n",
      "709 Train Loss 118.36211\n",
      "710 Train Loss 118.055725\n",
      "711 Train Loss 117.6814\n",
      "712 Train Loss 120.95285\n",
      "713 Train Loss 117.43884\n",
      "714 Train Loss 116.87962\n",
      "715 Train Loss 115.77182\n",
      "716 Train Loss 116.137436\n",
      "717 Train Loss 115.45282\n",
      "718 Train Loss 114.87251\n",
      "719 Train Loss 114.51907\n",
      "720 Train Loss 113.94437\n",
      "721 Train Loss 113.28464\n",
      "722 Train Loss 112.83301\n",
      "723 Train Loss 112.40342\n",
      "724 Train Loss 111.80917\n",
      "725 Train Loss 111.40311\n",
      "726 Train Loss 110.712654\n",
      "727 Train Loss 110.014084\n",
      "728 Train Loss 151.24696\n",
      "729 Train Loss 109.78085\n",
      "730 Train Loss 109.646194\n",
      "731 Train Loss 109.09512\n",
      "732 Train Loss 108.752945\n",
      "733 Train Loss 108.347404\n",
      "734 Train Loss 108.11708\n",
      "735 Train Loss 107.76384\n",
      "736 Train Loss 107.35605\n",
      "737 Train Loss 107.161865\n",
      "738 Train Loss 111.63588\n",
      "739 Train Loss 106.93039\n",
      "740 Train Loss 106.87695\n",
      "741 Train Loss 106.679695\n",
      "742 Train Loss 106.58853\n",
      "743 Train Loss 106.43126\n",
      "744 Train Loss 106.29702\n",
      "745 Train Loss 106.15349\n",
      "746 Train Loss 106.05499\n",
      "747 Train Loss 107.483444\n",
      "748 Train Loss 105.922455\n",
      "749 Train Loss 105.83058\n",
      "750 Train Loss 105.714005\n",
      "751 Train Loss 105.63497\n",
      "752 Train Loss 105.62392\n",
      "753 Train Loss 105.59552\n",
      "754 Train Loss 105.51965\n",
      "755 Train Loss 105.432884\n",
      "756 Train Loss 105.30976\n",
      "757 Train Loss 105.28238\n",
      "758 Train Loss 105.20925\n",
      "759 Train Loss 105.15081\n",
      "760 Train Loss 105.034836\n",
      "761 Train Loss 104.964615\n",
      "762 Train Loss 104.82895\n",
      "763 Train Loss 104.72266\n",
      "764 Train Loss 104.645546\n",
      "765 Train Loss 104.61512\n",
      "766 Train Loss 104.58206\n",
      "767 Train Loss 104.52882\n",
      "768 Train Loss 104.40296\n",
      "769 Train Loss 104.327835\n",
      "770 Train Loss 104.12396\n",
      "771 Train Loss 103.95181\n",
      "772 Train Loss 103.64554\n",
      "773 Train Loss 103.332855\n",
      "774 Train Loss 102.77475\n",
      "775 Train Loss 104.07855\n",
      "776 Train Loss 102.3232\n",
      "777 Train Loss 101.32107\n",
      "778 Train Loss 99.92789\n",
      "779 Train Loss 103.39225\n",
      "780 Train Loss 99.7103\n",
      "781 Train Loss 100.23891\n",
      "782 Train Loss 99.062775\n",
      "783 Train Loss 99.75982\n",
      "784 Train Loss 98.737366\n",
      "785 Train Loss 98.30466\n",
      "786 Train Loss 97.80162\n",
      "787 Train Loss 97.297005\n",
      "788 Train Loss 97.18003\n",
      "789 Train Loss 96.90692\n",
      "790 Train Loss 96.406906\n",
      "791 Train Loss 96.17998\n",
      "792 Train Loss 95.70407\n",
      "793 Train Loss 95.50534\n",
      "794 Train Loss 95.09186\n",
      "795 Train Loss 94.71003\n",
      "796 Train Loss 93.87778\n",
      "797 Train Loss 93.339264\n",
      "798 Train Loss 92.7155\n",
      "799 Train Loss 92.18994\n",
      "800 Train Loss 91.68286\n",
      "801 Train Loss 91.83862\n",
      "802 Train Loss 91.21153\n",
      "803 Train Loss 91.06639\n",
      "804 Train Loss 90.828896\n",
      "805 Train Loss 90.57628\n",
      "806 Train Loss 90.13637\n",
      "807 Train Loss 89.67964\n",
      "808 Train Loss 89.4157\n",
      "809 Train Loss 89.2135\n",
      "810 Train Loss 92.789536\n",
      "811 Train Loss 88.95668\n",
      "812 Train Loss 89.9247\n",
      "813 Train Loss 88.7251\n",
      "814 Train Loss 88.811615\n",
      "815 Train Loss 88.60394\n",
      "816 Train Loss 88.44167\n",
      "817 Train Loss 88.264755\n",
      "818 Train Loss 88.12784\n",
      "819 Train Loss 87.938965\n",
      "820 Train Loss 87.8537\n",
      "821 Train Loss 87.8\n",
      "822 Train Loss 87.73718\n",
      "823 Train Loss 87.668\n",
      "824 Train Loss 87.40475\n",
      "825 Train Loss 87.30504\n",
      "826 Train Loss 87.0961\n",
      "827 Train Loss 86.94799\n",
      "828 Train Loss 86.85427\n",
      "829 Train Loss 86.75891\n",
      "830 Train Loss 86.70886\n",
      "831 Train Loss 86.67062\n",
      "832 Train Loss 86.64157\n",
      "833 Train Loss 86.604904\n",
      "834 Train Loss 86.56326\n",
      "835 Train Loss 86.449234\n",
      "836 Train Loss 86.322365\n",
      "837 Train Loss 86.15342\n",
      "838 Train Loss 85.89183\n",
      "839 Train Loss 85.638275\n",
      "840 Train Loss 85.3835\n",
      "841 Train Loss 85.2961\n",
      "842 Train Loss 85.20447\n",
      "843 Train Loss 85.132645\n",
      "844 Train Loss 85.00912\n",
      "845 Train Loss 84.75963\n",
      "846 Train Loss 84.45302\n",
      "847 Train Loss 84.67215\n",
      "848 Train Loss 84.12463\n",
      "849 Train Loss 83.79205\n",
      "850 Train Loss 83.54248\n",
      "851 Train Loss 83.830826\n",
      "852 Train Loss 83.27156\n",
      "853 Train Loss 82.675735\n",
      "854 Train Loss 81.91234\n",
      "855 Train Loss 82.748535\n",
      "856 Train Loss 81.64975\n",
      "857 Train Loss 81.62096\n",
      "858 Train Loss 81.43811\n",
      "859 Train Loss 81.13133\n",
      "860 Train Loss 80.91412\n",
      "861 Train Loss 80.57214\n",
      "862 Train Loss 80.38014\n",
      "863 Train Loss 80.04993\n",
      "864 Train Loss 79.62502\n",
      "865 Train Loss 79.28206\n",
      "866 Train Loss 78.76697\n",
      "867 Train Loss 77.90982\n",
      "868 Train Loss 77.54068\n",
      "869 Train Loss 77.00248\n",
      "870 Train Loss 77.00333\n",
      "871 Train Loss 76.68201\n",
      "872 Train Loss 76.12958\n",
      "873 Train Loss 76.0715\n",
      "874 Train Loss 75.55372\n",
      "875 Train Loss 74.8363\n",
      "876 Train Loss 74.44037\n",
      "877 Train Loss 74.46789\n",
      "878 Train Loss 74.263504\n",
      "879 Train Loss 74.11957\n",
      "880 Train Loss 73.9572\n",
      "881 Train Loss 73.84222\n",
      "882 Train Loss 73.69998\n",
      "883 Train Loss 73.634766\n",
      "884 Train Loss 73.48846\n",
      "885 Train Loss 73.371124\n",
      "886 Train Loss 73.242035\n",
      "887 Train Loss 73.09756\n",
      "888 Train Loss 72.8727\n",
      "889 Train Loss 72.76287\n",
      "890 Train Loss 72.62613\n",
      "891 Train Loss 72.59202\n",
      "892 Train Loss 72.52682\n",
      "893 Train Loss 72.39737\n",
      "894 Train Loss 72.138824\n",
      "895 Train Loss 75.08373\n",
      "896 Train Loss 72.10462\n",
      "897 Train Loss 71.770035\n",
      "898 Train Loss 72.56102\n",
      "899 Train Loss 71.67406\n",
      "900 Train Loss 71.68642\n",
      "901 Train Loss 71.5267\n",
      "902 Train Loss 71.14102\n",
      "903 Train Loss 70.55777\n",
      "904 Train Loss 73.677666\n",
      "905 Train Loss 70.30675\n",
      "906 Train Loss 71.090256\n",
      "907 Train Loss 69.687\n",
      "908 Train Loss 68.72782\n",
      "909 Train Loss 73.17765\n",
      "910 Train Loss 68.28433\n",
      "911 Train Loss 67.358406\n",
      "912 Train Loss 66.42233\n",
      "913 Train Loss 67.26328\n",
      "914 Train Loss 65.981834\n",
      "915 Train Loss 65.70027\n",
      "916 Train Loss 65.21062\n",
      "917 Train Loss 64.76604\n",
      "918 Train Loss 63.96682\n",
      "919 Train Loss 68.94842\n",
      "920 Train Loss 63.59706\n",
      "921 Train Loss 63.009747\n",
      "922 Train Loss 62.450066\n",
      "923 Train Loss 62.022797\n",
      "924 Train Loss 62.212715\n",
      "925 Train Loss 61.47873\n",
      "926 Train Loss 61.070076\n",
      "927 Train Loss 60.52211\n",
      "928 Train Loss 60.305923\n",
      "929 Train Loss 59.958828\n",
      "930 Train Loss 59.780083\n",
      "931 Train Loss 59.73046\n",
      "932 Train Loss 59.579372\n",
      "933 Train Loss 59.405956\n",
      "934 Train Loss 59.14263\n",
      "935 Train Loss 61.848755\n",
      "936 Train Loss 59.05114\n",
      "937 Train Loss 58.856815\n",
      "938 Train Loss 58.6789\n",
      "939 Train Loss 58.48182\n",
      "940 Train Loss 58.65117\n",
      "941 Train Loss 58.3226\n",
      "942 Train Loss 58.072872\n",
      "943 Train Loss 57.954765\n",
      "944 Train Loss 57.81626\n",
      "945 Train Loss 57.57206\n",
      "946 Train Loss 57.59349\n",
      "947 Train Loss 57.422062\n",
      "948 Train Loss 57.1828\n",
      "949 Train Loss 56.9441\n",
      "950 Train Loss 56.84478\n",
      "951 Train Loss 56.62999\n",
      "952 Train Loss 56.524673\n",
      "953 Train Loss 56.73447\n",
      "954 Train Loss 56.410744\n",
      "955 Train Loss 56.219604\n",
      "956 Train Loss 55.927856\n",
      "957 Train Loss 55.92128\n",
      "958 Train Loss 55.799206\n",
      "959 Train Loss 55.618538\n",
      "960 Train Loss 55.557228\n",
      "961 Train Loss 55.512493\n",
      "962 Train Loss 55.45498\n",
      "963 Train Loss 55.408123\n",
      "964 Train Loss 55.353806\n",
      "965 Train Loss 55.293213\n",
      "966 Train Loss 55.212532\n",
      "967 Train Loss 55.0663\n",
      "968 Train Loss 54.94246\n",
      "969 Train Loss 54.72193\n",
      "970 Train Loss 54.63252\n",
      "971 Train Loss 54.543312\n",
      "972 Train Loss 54.450542\n",
      "973 Train Loss 54.384823\n",
      "974 Train Loss 54.33632\n",
      "975 Train Loss 54.264336\n",
      "976 Train Loss 54.180443\n",
      "977 Train Loss 54.098022\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "978 Train Loss 54.041977\n",
      "979 Train Loss 53.960213\n",
      "980 Train Loss 53.858242\n",
      "981 Train Loss 53.828445\n",
      "982 Train Loss 53.7529\n",
      "983 Train Loss 53.64719\n",
      "984 Train Loss 53.556694\n",
      "985 Train Loss 53.443172\n",
      "986 Train Loss 53.40487\n",
      "987 Train Loss 53.174507\n",
      "988 Train Loss 53.150738\n",
      "989 Train Loss 52.91992\n",
      "990 Train Loss 52.822903\n",
      "991 Train Loss 52.664566\n",
      "992 Train Loss 52.429474\n",
      "993 Train Loss 52.35521\n",
      "994 Train Loss 52.287212\n",
      "995 Train Loss 52.208908\n",
      "996 Train Loss 52.164467\n",
      "997 Train Loss 52.110596\n",
      "998 Train Loss 51.98934\n",
      "999 Train Loss 51.839367\n",
      "1000 Train Loss 51.60943\n",
      "1001 Train Loss 53.337883\n",
      "1002 Train Loss 51.516785\n",
      "1003 Train Loss 51.33195\n",
      "1004 Train Loss 51.264133\n",
      "1005 Train Loss 51.037746\n",
      "1006 Train Loss 50.929966\n",
      "1007 Train Loss 51.143692\n",
      "1008 Train Loss 50.862373\n",
      "1009 Train Loss 50.763824\n",
      "1010 Train Loss 50.68469\n",
      "1011 Train Loss 50.615395\n",
      "1012 Train Loss 50.582054\n",
      "1013 Train Loss 50.537712\n",
      "1014 Train Loss 50.440605\n",
      "1015 Train Loss 50.33881\n",
      "1016 Train Loss 50.44463\n",
      "1017 Train Loss 50.25114\n",
      "1018 Train Loss 50.149475\n",
      "1019 Train Loss 50.032223\n",
      "1020 Train Loss 49.856728\n",
      "1021 Train Loss 50.34797\n",
      "1022 Train Loss 49.76951\n",
      "1023 Train Loss 49.662548\n",
      "1024 Train Loss 49.49279\n",
      "1025 Train Loss 49.37151\n",
      "1026 Train Loss 49.21051\n",
      "1027 Train Loss 49.102707\n",
      "1028 Train Loss 48.87249\n",
      "1029 Train Loss 48.534218\n",
      "1030 Train Loss 48.453117\n",
      "1031 Train Loss 48.12094\n",
      "1032 Train Loss 48.417618\n",
      "1033 Train Loss 47.535767\n",
      "1034 Train Loss 47.198997\n",
      "1035 Train Loss 48.86924\n",
      "1036 Train Loss 46.958122\n",
      "1037 Train Loss 46.914013\n",
      "1038 Train Loss 46.66889\n",
      "1039 Train Loss 46.58288\n",
      "1040 Train Loss 46.504974\n",
      "1041 Train Loss 46.46965\n",
      "1042 Train Loss 46.437393\n",
      "1043 Train Loss 46.394154\n",
      "1044 Train Loss 46.33397\n",
      "1045 Train Loss 46.27942\n",
      "1046 Train Loss 46.17568\n",
      "1047 Train Loss 46.054955\n",
      "1048 Train Loss 45.93222\n",
      "1049 Train Loss 45.822792\n",
      "1050 Train Loss 45.846046\n",
      "1051 Train Loss 45.750717\n",
      "1052 Train Loss 45.656033\n",
      "1053 Train Loss 45.448425\n",
      "1054 Train Loss 45.22145\n",
      "1055 Train Loss 45.82512\n",
      "1056 Train Loss 45.078476\n",
      "1057 Train Loss 44.81732\n",
      "1058 Train Loss 44.59665\n",
      "1059 Train Loss 44.468597\n",
      "1060 Train Loss 45.133877\n",
      "1061 Train Loss 44.364693\n",
      "1062 Train Loss 44.09928\n",
      "1063 Train Loss 43.7873\n",
      "1064 Train Loss 43.424385\n",
      "1065 Train Loss 43.16909\n",
      "1066 Train Loss 44.625908\n",
      "1067 Train Loss 42.80006\n",
      "1068 Train Loss 42.56658\n",
      "1069 Train Loss 42.400505\n",
      "1070 Train Loss 42.36189\n",
      "1071 Train Loss 42.296738\n",
      "1072 Train Loss 42.058167\n",
      "1073 Train Loss 41.917118\n",
      "1074 Train Loss 41.738544\n",
      "1075 Train Loss 41.61328\n",
      "1076 Train Loss 41.3422\n",
      "1077 Train Loss 41.131393\n",
      "1078 Train Loss 41.069523\n",
      "1079 Train Loss 40.927525\n",
      "1080 Train Loss 40.99247\n",
      "1081 Train Loss 40.855503\n",
      "1082 Train Loss 40.808716\n",
      "1083 Train Loss 40.75766\n",
      "1084 Train Loss 40.68556\n",
      "1085 Train Loss 40.61809\n",
      "1086 Train Loss 40.475243\n",
      "1087 Train Loss 40.25715\n",
      "1088 Train Loss 41.27709\n",
      "1089 Train Loss 40.160927\n",
      "1090 Train Loss 39.90955\n",
      "1091 Train Loss 39.783947\n",
      "1092 Train Loss 39.646107\n",
      "1093 Train Loss 39.67889\n",
      "1094 Train Loss 39.583267\n",
      "1095 Train Loss 39.744064\n",
      "1096 Train Loss 39.543716\n",
      "1097 Train Loss 39.5231\n",
      "1098 Train Loss 39.442528\n",
      "1099 Train Loss 39.376312\n",
      "1100 Train Loss 39.307114\n",
      "1101 Train Loss 41.824482\n",
      "1102 Train Loss 39.287292\n",
      "1103 Train Loss 39.157684\n",
      "1104 Train Loss 39.361744\n",
      "1105 Train Loss 39.09891\n",
      "1106 Train Loss 39.033077\n",
      "1107 Train Loss 39.04332\n",
      "1108 Train Loss 38.989307\n",
      "1109 Train Loss 38.92482\n",
      "1110 Train Loss 38.87864\n",
      "1111 Train Loss 38.813644\n",
      "1112 Train Loss 38.795162\n",
      "1113 Train Loss 38.762417\n",
      "1114 Train Loss 38.741074\n",
      "1115 Train Loss 38.72815\n",
      "1116 Train Loss 38.680344\n",
      "1117 Train Loss 38.639248\n",
      "1118 Train Loss 38.62245\n",
      "1119 Train Loss 38.589996\n",
      "1120 Train Loss 38.56964\n",
      "1121 Train Loss 38.53389\n",
      "1122 Train Loss 38.48486\n",
      "1123 Train Loss 38.447426\n",
      "1124 Train Loss 38.406807\n",
      "1125 Train Loss 38.36644\n",
      "1126 Train Loss 38.30305\n",
      "1127 Train Loss 38.24356\n",
      "1128 Train Loss 38.19428\n",
      "1129 Train Loss 38.148354\n",
      "1130 Train Loss 38.093246\n",
      "1131 Train Loss 38.502388\n",
      "1132 Train Loss 38.048393\n",
      "1133 Train Loss 37.970764\n",
      "1134 Train Loss 37.893036\n",
      "1135 Train Loss 37.870697\n",
      "1136 Train Loss 37.849518\n",
      "1137 Train Loss 37.82431\n",
      "1138 Train Loss 37.701424\n",
      "1139 Train Loss 37.58066\n",
      "1140 Train Loss 39.760727\n",
      "1141 Train Loss 37.47619\n",
      "1142 Train Loss 37.342438\n",
      "1143 Train Loss 36.92298\n",
      "1144 Train Loss 36.74292\n",
      "1145 Train Loss 36.57495\n",
      "1146 Train Loss 36.559074\n",
      "1147 Train Loss 36.449734\n",
      "1148 Train Loss 36.47538\n",
      "1149 Train Loss 36.29607\n",
      "1150 Train Loss 36.11748\n",
      "1151 Train Loss 36.09397\n",
      "1152 Train Loss 35.991768\n",
      "1153 Train Loss 35.89617\n",
      "1154 Train Loss 35.821537\n",
      "1155 Train Loss 35.77715\n",
      "1156 Train Loss 35.7327\n",
      "1157 Train Loss 35.659916\n",
      "1158 Train Loss 35.599514\n",
      "1159 Train Loss 35.521793\n",
      "1160 Train Loss 35.431908\n",
      "1161 Train Loss 35.331287\n",
      "1162 Train Loss 35.21006\n",
      "1163 Train Loss 35.003815\n",
      "1164 Train Loss 34.87014\n",
      "1165 Train Loss 34.82364\n",
      "1166 Train Loss 34.651848\n",
      "1167 Train Loss 34.57238\n",
      "1168 Train Loss 34.47465\n",
      "1169 Train Loss 34.371056\n",
      "1170 Train Loss 34.34307\n",
      "1171 Train Loss 34.316994\n",
      "1172 Train Loss 34.311504\n",
      "1173 Train Loss 34.29698\n",
      "1174 Train Loss 34.283127\n",
      "1175 Train Loss 34.309464\n",
      "1176 Train Loss 34.278595\n",
      "1177 Train Loss 34.265675\n",
      "1178 Train Loss 34.239395\n",
      "1179 Train Loss 34.199677\n",
      "1180 Train Loss 34.145657\n",
      "1181 Train Loss 34.068638\n",
      "1182 Train Loss 39.653286\n",
      "1183 Train Loss 34.051098\n",
      "1184 Train Loss 34.029198\n",
      "1185 Train Loss 33.86111\n",
      "1186 Train Loss 33.787037\n",
      "1187 Train Loss 33.559807\n",
      "1188 Train Loss 33.36895\n",
      "1189 Train Loss 33.544163\n",
      "1190 Train Loss 33.269985\n",
      "1191 Train Loss 33.15558\n",
      "1192 Train Loss 33.07513\n",
      "1193 Train Loss 33.04021\n",
      "1194 Train Loss 32.991684\n",
      "1195 Train Loss 32.934692\n",
      "1196 Train Loss 32.932632\n",
      "1197 Train Loss 32.874187\n",
      "1198 Train Loss 32.775433\n",
      "1199 Train Loss 32.731987\n",
      "1200 Train Loss 32.683907\n",
      "1201 Train Loss 32.68721\n",
      "1202 Train Loss 32.649597\n",
      "1203 Train Loss 32.57887\n",
      "1204 Train Loss 32.658466\n",
      "1205 Train Loss 32.514572\n",
      "1206 Train Loss 32.431305\n",
      "1207 Train Loss 32.326103\n",
      "1208 Train Loss 32.105255\n",
      "1209 Train Loss 32.066704\n",
      "1210 Train Loss 32.15309\n",
      "1211 Train Loss 31.957066\n",
      "1212 Train Loss 31.891306\n",
      "1213 Train Loss 31.88041\n",
      "1214 Train Loss 31.817003\n",
      "1215 Train Loss 31.758781\n",
      "1216 Train Loss 31.711323\n",
      "1217 Train Loss 31.676128\n",
      "1218 Train Loss 31.637642\n",
      "1219 Train Loss 31.585735\n",
      "1220 Train Loss 31.524994\n",
      "1221 Train Loss 31.468124\n",
      "1222 Train Loss 31.389837\n",
      "1223 Train Loss 31.411665\n",
      "1224 Train Loss 31.353413\n",
      "1225 Train Loss 31.268223\n",
      "1226 Train Loss 31.202587\n",
      "1227 Train Loss 31.030342\n",
      "1228 Train Loss 30.95425\n",
      "1229 Train Loss 30.81831\n",
      "1230 Train Loss 31.932564\n",
      "1231 Train Loss 30.697792\n",
      "1232 Train Loss 30.535564\n",
      "1233 Train Loss 30.457146\n",
      "1234 Train Loss 30.340216\n",
      "1235 Train Loss 30.266483\n",
      "1236 Train Loss 30.106302\n",
      "1237 Train Loss 30.29903\n",
      "1238 Train Loss 30.064877\n",
      "1239 Train Loss 30.07058\n",
      "1240 Train Loss 30.009237\n",
      "1241 Train Loss 29.902573\n",
      "1242 Train Loss 29.770245\n",
      "1243 Train Loss 29.679024\n",
      "1244 Train Loss 29.822243\n",
      "1245 Train Loss 29.620983\n",
      "1246 Train Loss 29.52982\n",
      "1247 Train Loss 29.470266\n",
      "1248 Train Loss 29.37378\n",
      "1249 Train Loss 29.359428\n",
      "1250 Train Loss 29.310196\n",
      "1251 Train Loss 29.384632\n",
      "1252 Train Loss 29.272928\n",
      "1253 Train Loss 29.247034\n",
      "1254 Train Loss 29.21952\n",
      "1255 Train Loss 29.18718\n",
      "1256 Train Loss 29.144762\n",
      "1257 Train Loss 29.153965\n",
      "1258 Train Loss 29.127665\n",
      "1259 Train Loss 29.091873\n",
      "1260 Train Loss 29.208666\n",
      "1261 Train Loss 29.084784\n",
      "1262 Train Loss 29.072748\n",
      "1263 Train Loss 29.04451\n",
      "1264 Train Loss 29.029385\n",
      "1265 Train Loss 29.152256\n",
      "1266 Train Loss 29.028133\n",
      "1267 Train Loss 29.012318\n",
      "1268 Train Loss 28.996134\n",
      "1269 Train Loss 28.982578\n",
      "1270 Train Loss 28.978645\n",
      "1271 Train Loss 28.980268\n",
      "1272 Train Loss 28.971422\n",
      "1273 Train Loss 28.963398\n",
      "1274 Train Loss 28.95869\n",
      "1275 Train Loss 28.960066\n",
      "1276 Train Loss 28.961199\n",
      "1277 Train Loss 28.96155\n",
      "1278 Train Loss 28.961285\n",
      "1279 Train Loss 28.960821\n",
      "1280 Train Loss 28.958746\n",
      "1281 Train Loss 28.947128\n",
      "1282 Train Loss 28.930527\n",
      "1283 Train Loss 28.910877\n",
      "1284 Train Loss 28.88986\n",
      "1285 Train Loss 28.840696\n",
      "1286 Train Loss 28.800251\n",
      "1287 Train Loss 28.935696\n",
      "1288 Train Loss 28.768208\n",
      "1289 Train Loss 28.715296\n",
      "1290 Train Loss 28.66068\n",
      "1291 Train Loss 28.616259\n",
      "1292 Train Loss 28.60863\n",
      "1293 Train Loss 28.579865\n",
      "1294 Train Loss 28.568079\n",
      "1295 Train Loss 28.531033\n",
      "1296 Train Loss 28.50232\n",
      "1297 Train Loss 28.444355\n",
      "1298 Train Loss 28.521854\n",
      "1299 Train Loss 28.434301\n",
      "1300 Train Loss 28.419275\n",
      "1301 Train Loss 28.394104\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1302 Train Loss 28.41839\n",
      "1303 Train Loss 28.385502\n",
      "1304 Train Loss 28.365654\n",
      "1305 Train Loss 28.351448\n",
      "1306 Train Loss 28.354443\n",
      "1307 Train Loss 28.33342\n",
      "1308 Train Loss 28.330936\n",
      "1309 Train Loss 28.331192\n",
      "1310 Train Loss 28.329891\n",
      "1311 Train Loss 28.328918\n",
      "1312 Train Loss 28.32341\n",
      "1313 Train Loss 28.31662\n",
      "1314 Train Loss 28.311104\n",
      "1315 Train Loss 28.313906\n",
      "1316 Train Loss 28.30343\n",
      "1317 Train Loss 28.30563\n",
      "1318 Train Loss 28.313692\n",
      "1319 Train Loss 28.302486\n",
      "1320 Train Loss 28.306347\n",
      "1321 Train Loss 28.302502\n",
      "1322 Train Loss 28.56991\n",
      "1323 Train Loss 28.313808\n",
      "1324 Train Loss 28.31231\n",
      "1325 Train Loss 28.310932\n",
      "1326 Train Loss 28.30312\n",
      "1327 Train Loss 28.56991\n",
      "1328 Train Loss 28.313808\n",
      "1329 Train Loss 28.31231\n",
      "1330 Train Loss 28.310932\n",
      "1331 Train Loss 28.30312\n",
      "1332 Train Loss 28.56991\n",
      "1333 Train Loss 28.313808\n",
      "1334 Train Loss 28.31231\n",
      "1335 Train Loss 28.310932\n",
      "1336 Train Loss 28.30312\n",
      "1337 Train Loss 28.56991\n",
      "1338 Train Loss 28.313808\n",
      "1339 Train Loss 28.31231\n",
      "1340 Train Loss 28.310932\n",
      "1341 Train Loss 28.30312\n",
      "1342 Train Loss 28.56991\n",
      "1343 Train Loss 28.313808\n",
      "1344 Train Loss 28.31231\n",
      "1345 Train Loss 28.310932\n",
      "1346 Train Loss 28.30312\n",
      "1347 Train Loss 28.56991\n",
      "1348 Train Loss 28.313808\n",
      "1349 Train Loss 28.31231\n",
      "1350 Train Loss 28.310932\n",
      "1351 Train Loss 28.30312\n",
      "1352 Train Loss 28.56991\n",
      "1353 Train Loss 28.313808\n",
      "1354 Train Loss 28.31231\n",
      "1355 Train Loss 28.310932\n",
      "1356 Train Loss 28.30312\n",
      "1357 Train Loss 28.56991\n",
      "1358 Train Loss 28.313808\n",
      "1359 Train Loss 28.31231\n",
      "1360 Train Loss 28.310932\n",
      "1361 Train Loss 28.30312\n",
      "1362 Train Loss 28.56991\n",
      "1363 Train Loss 28.313808\n",
      "1364 Train Loss 28.31231\n",
      "1365 Train Loss 28.310932\n",
      "1366 Train Loss 28.30312\n",
      "1367 Train Loss 28.56991\n",
      "1368 Train Loss 28.313808\n",
      "1369 Train Loss 28.31231\n",
      "1370 Train Loss 28.310932\n",
      "1371 Train Loss 28.30312\n",
      "1372 Train Loss 28.56991\n",
      "1373 Train Loss 28.313808\n",
      "1374 Train Loss 28.31231\n",
      "1375 Train Loss 28.310932\n",
      "1376 Train Loss 28.30312\n",
      "1377 Train Loss 28.56991\n",
      "1378 Train Loss 28.313808\n",
      "1379 Train Loss 28.31231\n",
      "1380 Train Loss 28.310932\n",
      "1381 Train Loss 28.30312\n",
      "1382 Train Loss 28.56991\n",
      "1383 Train Loss 28.313808\n",
      "1384 Train Loss 28.31231\n",
      "1385 Train Loss 28.310932\n",
      "1386 Train Loss 28.30312\n",
      "1387 Train Loss 28.56991\n",
      "1388 Train Loss 28.313808\n",
      "1389 Train Loss 28.31231\n",
      "1390 Train Loss 28.310932\n",
      "1391 Train Loss 28.30312\n",
      "1392 Train Loss 28.56991\n",
      "1393 Train Loss 28.313808\n",
      "1394 Train Loss 28.31231\n",
      "1395 Train Loss 28.310932\n",
      "1396 Train Loss 28.30312\n",
      "1397 Train Loss 28.56991\n",
      "1398 Train Loss 28.313808\n",
      "1399 Train Loss 28.31231\n",
      "1400 Train Loss 28.310932\n",
      "1401 Train Loss 28.30312\n",
      "1402 Train Loss 28.56991\n",
      "1403 Train Loss 28.313808\n",
      "1404 Train Loss 28.31231\n",
      "1405 Train Loss 28.310932\n",
      "1406 Train Loss 28.30312\n",
      "1407 Train Loss 28.56991\n",
      "1408 Train Loss 28.313808\n",
      "1409 Train Loss 28.31231\n",
      "1410 Train Loss 28.310932\n",
      "1411 Train Loss 28.30312\n",
      "1412 Train Loss 28.56991\n",
      "1413 Train Loss 28.313808\n",
      "1414 Train Loss 28.31231\n",
      "1415 Train Loss 28.310932\n",
      "1416 Train Loss 28.30312\n",
      "1417 Train Loss 28.56991\n",
      "1418 Train Loss 28.313808\n",
      "1419 Train Loss 28.31231\n",
      "1420 Train Loss 28.310932\n",
      "1421 Train Loss 28.30312\n",
      "1422 Train Loss 28.56991\n",
      "1423 Train Loss 28.313808\n",
      "1424 Train Loss 28.31231\n",
      "1425 Train Loss 28.310932\n",
      "1426 Train Loss 28.30312\n",
      "1427 Train Loss 28.56991\n",
      "1428 Train Loss 28.313808\n",
      "1429 Train Loss 28.31231\n",
      "1430 Train Loss 28.310932\n",
      "1431 Train Loss 28.30312\n",
      "1432 Train Loss 28.56991\n",
      "1433 Train Loss 28.313808\n",
      "1434 Train Loss 28.31231\n",
      "1435 Train Loss 28.310932\n",
      "1436 Train Loss 28.30312\n",
      "1437 Train Loss 28.56991\n",
      "1438 Train Loss 28.313808\n",
      "1439 Train Loss 28.31231\n",
      "1440 Train Loss 28.310932\n",
      "1441 Train Loss 28.30312\n",
      "1442 Train Loss 28.56991\n",
      "1443 Train Loss 28.313808\n",
      "1444 Train Loss 28.31231\n",
      "1445 Train Loss 28.310932\n",
      "1446 Train Loss 28.30312\n",
      "1447 Train Loss 28.56991\n",
      "1448 Train Loss 28.313808\n",
      "1449 Train Loss 28.31231\n",
      "1450 Train Loss 28.310932\n",
      "1451 Train Loss 28.30312\n",
      "1452 Train Loss 28.56991\n",
      "1453 Train Loss 28.313808\n",
      "1454 Train Loss 28.31231\n",
      "1455 Train Loss 28.310932\n",
      "1456 Train Loss 28.30312\n",
      "1457 Train Loss 28.56991\n",
      "1458 Train Loss 28.313808\n",
      "1459 Train Loss 28.31231\n",
      "1460 Train Loss 28.310932\n",
      "1461 Train Loss 28.30312\n",
      "1462 Train Loss 28.56991\n",
      "1463 Train Loss 28.313808\n",
      "1464 Train Loss 28.31231\n",
      "1465 Train Loss 28.310932\n",
      "1466 Train Loss 28.30312\n",
      "1467 Train Loss 28.56991\n",
      "1468 Train Loss 28.313808\n",
      "1469 Train Loss 28.31231\n",
      "1470 Train Loss 28.310932\n",
      "1471 Train Loss 28.30312\n",
      "1472 Train Loss 28.56991\n",
      "1473 Train Loss 28.313808\n",
      "1474 Train Loss 28.31231\n",
      "1475 Train Loss 28.310932\n",
      "1476 Train Loss 28.30312\n",
      "1477 Train Loss 28.56991\n",
      "1478 Train Loss 28.313808\n",
      "1479 Train Loss 28.31231\n",
      "1480 Train Loss 28.310932\n",
      "1481 Train Loss 28.30312\n",
      "1482 Train Loss 28.56991\n",
      "1483 Train Loss 28.313808\n",
      "1484 Train Loss 28.31231\n",
      "1485 Train Loss 28.310932\n",
      "1486 Train Loss 28.30312\n",
      "1487 Train Loss 28.56991\n",
      "1488 Train Loss 28.313808\n",
      "1489 Train Loss 28.31231\n",
      "1490 Train Loss 28.310932\n",
      "1491 Train Loss 28.30312\n",
      "1492 Train Loss 28.56991\n",
      "1493 Train Loss 28.313808\n",
      "1494 Train Loss 28.31231\n",
      "1495 Train Loss 28.310932\n",
      "1496 Train Loss 28.30312\n",
      "1497 Train Loss 28.56991\n",
      "1498 Train Loss 28.313808\n",
      "1499 Train Loss 28.31231\n",
      "1500 Train Loss 28.310932\n",
      "1501 Train Loss 28.30312\n",
      "1502 Train Loss 28.56991\n",
      "1503 Train Loss 28.313808\n",
      "1504 Train Loss 28.31231\n",
      "1505 Train Loss 28.310932\n",
      "1506 Train Loss 28.30312\n",
      "1507 Train Loss 28.56991\n",
      "1508 Train Loss 28.313808\n",
      "1509 Train Loss 28.31231\n",
      "1510 Train Loss 28.310932\n",
      "1511 Train Loss 28.30312\n",
      "1512 Train Loss 28.56991\n",
      "1513 Train Loss 28.313808\n",
      "1514 Train Loss 28.31231\n",
      "1515 Train Loss 28.310932\n",
      "1516 Train Loss 28.30312\n",
      "1517 Train Loss 28.56991\n",
      "1518 Train Loss 28.313808\n",
      "1519 Train Loss 28.31231\n",
      "1520 Train Loss 28.310932\n",
      "1521 Train Loss 28.30312\n",
      "1522 Train Loss 28.56991\n",
      "1523 Train Loss 28.313808\n",
      "1524 Train Loss 28.31231\n",
      "1525 Train Loss 28.310932\n",
      "1526 Train Loss 28.30312\n",
      "1527 Train Loss 28.56991\n",
      "1528 Train Loss 28.313808\n",
      "1529 Train Loss 28.31231\n",
      "1530 Train Loss 28.310932\n",
      "1531 Train Loss 28.30312\n",
      "1532 Train Loss 28.56991\n",
      "1533 Train Loss 28.313808\n",
      "1534 Train Loss 28.31231\n",
      "1535 Train Loss 28.310932\n",
      "1536 Train Loss 28.30312\n",
      "1537 Train Loss 28.56991\n",
      "1538 Train Loss 28.313808\n",
      "1539 Train Loss 28.31231\n",
      "1540 Train Loss 28.310932\n",
      "1541 Train Loss 28.30312\n",
      "1542 Train Loss 28.56991\n",
      "1543 Train Loss 28.313808\n",
      "1544 Train Loss 28.31231\n",
      "1545 Train Loss 28.310932\n",
      "1546 Train Loss 28.30312\n",
      "1547 Train Loss 28.56991\n",
      "1548 Train Loss 28.313808\n",
      "1549 Train Loss 28.31231\n",
      "1550 Train Loss 28.310932\n",
      "1551 Train Loss 28.30312\n",
      "1552 Train Loss 28.56991\n",
      "1553 Train Loss 28.313808\n",
      "1554 Train Loss 28.31231\n",
      "1555 Train Loss 28.310932\n",
      "1556 Train Loss 28.30312\n",
      "1557 Train Loss 28.56991\n",
      "1558 Train Loss 28.313808\n",
      "1559 Train Loss 28.31231\n",
      "1560 Train Loss 28.310932\n",
      "1561 Train Loss 28.30312\n",
      "1562 Train Loss 28.56991\n",
      "1563 Train Loss 28.313808\n",
      "1564 Train Loss 28.31231\n",
      "1565 Train Loss 28.310932\n",
      "1566 Train Loss 28.30312\n",
      "1567 Train Loss 28.56991\n",
      "1568 Train Loss 28.313808\n",
      "1569 Train Loss 28.31231\n",
      "1570 Train Loss 28.310932\n",
      "1571 Train Loss 28.30312\n",
      "1572 Train Loss 28.56991\n",
      "1573 Train Loss 28.313808\n",
      "1574 Train Loss 28.31231\n",
      "1575 Train Loss 28.310932\n",
      "1576 Train Loss 28.30312\n",
      "1577 Train Loss 28.56991\n",
      "1578 Train Loss 28.313808\n",
      "1579 Train Loss 28.31231\n",
      "1580 Train Loss 28.310932\n",
      "1581 Train Loss 28.30312\n",
      "1582 Train Loss 28.56991\n",
      "1583 Train Loss 28.313808\n",
      "1584 Train Loss 28.31231\n",
      "1585 Train Loss 28.310932\n",
      "1586 Train Loss 28.30312\n",
      "1587 Train Loss 28.56991\n",
      "1588 Train Loss 28.313808\n",
      "1589 Train Loss 28.31231\n",
      "1590 Train Loss 28.310932\n",
      "1591 Train Loss 28.30312\n",
      "1592 Train Loss 28.56991\n",
      "1593 Train Loss 28.313808\n",
      "1594 Train Loss 28.31231\n",
      "1595 Train Loss 28.310932\n",
      "1596 Train Loss 28.30312\n",
      "1597 Train Loss 28.56991\n",
      "1598 Train Loss 28.313808\n",
      "1599 Train Loss 28.31231\n",
      "1600 Train Loss 28.310932\n",
      "1601 Train Loss 28.30312\n",
      "1602 Train Loss 28.56991\n",
      "1603 Train Loss 28.313808\n",
      "1604 Train Loss 28.31231\n",
      "1605 Train Loss 28.310932\n",
      "1606 Train Loss 28.30312\n",
      "1607 Train Loss 28.56991\n",
      "1608 Train Loss 28.313808\n",
      "1609 Train Loss 28.31231\n",
      "1610 Train Loss 28.310932\n",
      "1611 Train Loss 28.30312\n",
      "1612 Train Loss 28.56991\n",
      "1613 Train Loss 28.313808\n",
      "1614 Train Loss 28.31231\n",
      "1615 Train Loss 28.310932\n",
      "1616 Train Loss 28.30312\n",
      "1617 Train Loss 28.56991\n",
      "1618 Train Loss 28.313808\n",
      "1619 Train Loss 28.31231\n",
      "1620 Train Loss 28.310932\n",
      "1621 Train Loss 28.30312\n",
      "1622 Train Loss 28.56991\n",
      "1623 Train Loss 28.313808\n",
      "1624 Train Loss 28.31231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1625 Train Loss 28.310932\n",
      "1626 Train Loss 28.30312\n",
      "1627 Train Loss 28.56991\n",
      "1628 Train Loss 28.313808\n",
      "1629 Train Loss 28.31231\n",
      "1630 Train Loss 28.310932\n",
      "1631 Train Loss 28.30312\n",
      "1632 Train Loss 28.56991\n",
      "1633 Train Loss 28.313808\n",
      "1634 Train Loss 28.31231\n",
      "1635 Train Loss 28.310932\n",
      "1636 Train Loss 28.30312\n",
      "1637 Train Loss 28.56991\n",
      "1638 Train Loss 28.313808\n",
      "1639 Train Loss 28.31231\n",
      "1640 Train Loss 28.310932\n",
      "1641 Train Loss 28.30312\n",
      "1642 Train Loss 28.56991\n",
      "1643 Train Loss 28.313808\n",
      "1644 Train Loss 28.31231\n",
      "1645 Train Loss 28.310932\n",
      "1646 Train Loss 28.30312\n",
      "1647 Train Loss 28.56991\n",
      "1648 Train Loss 28.313808\n",
      "1649 Train Loss 28.31231\n",
      "1650 Train Loss 28.310932\n",
      "1651 Train Loss 28.30312\n",
      "1652 Train Loss 28.56991\n",
      "1653 Train Loss 28.313808\n",
      "1654 Train Loss 28.31231\n",
      "1655 Train Loss 28.310932\n",
      "1656 Train Loss 28.30312\n",
      "1657 Train Loss 28.56991\n",
      "1658 Train Loss 28.313808\n",
      "1659 Train Loss 28.31231\n",
      "1660 Train Loss 28.310932\n",
      "1661 Train Loss 28.30312\n",
      "1662 Train Loss 28.56991\n",
      "1663 Train Loss 28.313808\n",
      "1664 Train Loss 28.31231\n",
      "1665 Train Loss 28.310932\n",
      "1666 Train Loss 28.30312\n",
      "1667 Train Loss 28.56991\n",
      "1668 Train Loss 28.313808\n",
      "1669 Train Loss 28.31231\n",
      "1670 Train Loss 28.310932\n",
      "1671 Train Loss 28.30312\n",
      "1672 Train Loss 28.56991\n",
      "1673 Train Loss 28.313808\n",
      "1674 Train Loss 28.31231\n",
      "1675 Train Loss 28.310932\n",
      "1676 Train Loss 28.30312\n",
      "1677 Train Loss 28.56991\n",
      "1678 Train Loss 28.313808\n",
      "1679 Train Loss 28.31231\n",
      "1680 Train Loss 28.310932\n",
      "1681 Train Loss 28.30312\n",
      "1682 Train Loss 28.56991\n",
      "1683 Train Loss 28.313808\n",
      "1684 Train Loss 28.31231\n",
      "1685 Train Loss 28.310932\n",
      "1686 Train Loss 28.30312\n",
      "1687 Train Loss 28.56991\n",
      "1688 Train Loss 28.313808\n",
      "1689 Train Loss 28.31231\n",
      "1690 Train Loss 28.310932\n",
      "1691 Train Loss 28.30312\n",
      "1692 Train Loss 28.56991\n",
      "1693 Train Loss 28.313808\n",
      "1694 Train Loss 28.31231\n",
      "1695 Train Loss 28.310932\n",
      "1696 Train Loss 28.30312\n",
      "1697 Train Loss 28.56991\n",
      "1698 Train Loss 28.313808\n",
      "1699 Train Loss 28.31231\n",
      "1700 Train Loss 28.310932\n",
      "1701 Train Loss 28.30312\n",
      "1702 Train Loss 28.56991\n",
      "1703 Train Loss 28.313808\n",
      "1704 Train Loss 28.31231\n",
      "1705 Train Loss 28.310932\n",
      "1706 Train Loss 28.30312\n",
      "1707 Train Loss 28.56991\n",
      "1708 Train Loss 28.313808\n",
      "1709 Train Loss 28.31231\n",
      "1710 Train Loss 28.310932\n",
      "1711 Train Loss 28.30312\n",
      "1712 Train Loss 28.56991\n",
      "1713 Train Loss 28.313808\n",
      "1714 Train Loss 28.31231\n",
      "1715 Train Loss 28.310932\n",
      "1716 Train Loss 28.30312\n",
      "1717 Train Loss 28.56991\n",
      "1718 Train Loss 28.313808\n",
      "1719 Train Loss 28.31231\n",
      "1720 Train Loss 28.310932\n",
      "1721 Train Loss 28.30312\n",
      "1722 Train Loss 28.56991\n",
      "1723 Train Loss 28.313808\n",
      "1724 Train Loss 28.31231\n",
      "1725 Train Loss 28.310932\n",
      "1726 Train Loss 28.30312\n",
      "1727 Train Loss 28.56991\n",
      "1728 Train Loss 28.313808\n",
      "1729 Train Loss 28.31231\n",
      "1730 Train Loss 28.310932\n",
      "1731 Train Loss 28.30312\n",
      "1732 Train Loss 28.56991\n",
      "1733 Train Loss 28.313808\n",
      "1734 Train Loss 28.31231\n",
      "1735 Train Loss 28.310932\n",
      "1736 Train Loss 28.30312\n",
      "1737 Train Loss 28.56991\n",
      "1738 Train Loss 28.313808\n",
      "1739 Train Loss 28.31231\n",
      "1740 Train Loss 28.310932\n",
      "1741 Train Loss 28.30312\n",
      "1742 Train Loss 28.56991\n",
      "1743 Train Loss 28.313808\n",
      "1744 Train Loss 28.31231\n",
      "1745 Train Loss 28.310932\n",
      "1746 Train Loss 28.30312\n",
      "1747 Train Loss 28.56991\n",
      "1748 Train Loss 28.313808\n",
      "1749 Train Loss 28.31231\n",
      "1750 Train Loss 28.310932\n",
      "1751 Train Loss 28.30312\n",
      "1752 Train Loss 28.56991\n",
      "1753 Train Loss 28.313808\n",
      "1754 Train Loss 28.31231\n",
      "1755 Train Loss 28.310932\n",
      "1756 Train Loss 28.30312\n",
      "1757 Train Loss 28.56991\n",
      "1758 Train Loss 28.313808\n",
      "1759 Train Loss 28.31231\n",
      "1760 Train Loss 28.310932\n",
      "1761 Train Loss 28.30312\n",
      "1762 Train Loss 28.56991\n",
      "1763 Train Loss 28.313808\n",
      "1764 Train Loss 28.31231\n",
      "1765 Train Loss 28.310932\n",
      "1766 Train Loss 28.30312\n",
      "1767 Train Loss 28.56991\n",
      "1768 Train Loss 28.313808\n",
      "1769 Train Loss 28.31231\n",
      "1770 Train Loss 28.310932\n",
      "1771 Train Loss 28.30312\n",
      "1772 Train Loss 28.56991\n",
      "1773 Train Loss 28.313808\n",
      "1774 Train Loss 28.31231\n",
      "1775 Train Loss 28.310932\n",
      "1776 Train Loss 28.30312\n",
      "1777 Train Loss 28.56991\n",
      "1778 Train Loss 28.313808\n",
      "1779 Train Loss 28.31231\n",
      "1780 Train Loss 28.310932\n",
      "1781 Train Loss 28.30312\n",
      "1782 Train Loss 28.56991\n",
      "1783 Train Loss 28.313808\n",
      "1784 Train Loss 28.31231\n",
      "1785 Train Loss 28.310932\n",
      "1786 Train Loss 28.30312\n",
      "1787 Train Loss 28.56991\n",
      "1788 Train Loss 28.313808\n",
      "1789 Train Loss 28.31231\n",
      "1790 Train Loss 28.310932\n",
      "1791 Train Loss 28.30312\n",
      "1792 Train Loss 28.56991\n",
      "1793 Train Loss 28.313808\n",
      "1794 Train Loss 28.31231\n",
      "1795 Train Loss 28.310932\n",
      "1796 Train Loss 28.30312\n",
      "1797 Train Loss 28.56991\n",
      "1798 Train Loss 28.313808\n",
      "1799 Train Loss 28.31231\n",
      "1800 Train Loss 28.310932\n",
      "1801 Train Loss 28.30312\n",
      "1802 Train Loss 28.56991\n",
      "1803 Train Loss 28.313808\n",
      "1804 Train Loss 28.31231\n",
      "1805 Train Loss 28.310932\n",
      "1806 Train Loss 28.30312\n",
      "1807 Train Loss 28.56991\n",
      "1808 Train Loss 28.313808\n",
      "1809 Train Loss 28.31231\n",
      "1810 Train Loss 28.310932\n",
      "1811 Train Loss 28.30312\n",
      "1812 Train Loss 28.56991\n",
      "1813 Train Loss 28.313808\n",
      "1814 Train Loss 28.31231\n",
      "1815 Train Loss 28.310932\n",
      "1816 Train Loss 28.30312\n",
      "1817 Train Loss 28.56991\n",
      "1818 Train Loss 28.313808\n",
      "1819 Train Loss 28.31231\n",
      "1820 Train Loss 28.310932\n",
      "1821 Train Loss 28.30312\n",
      "1822 Train Loss 28.56991\n",
      "1823 Train Loss 28.313808\n",
      "1824 Train Loss 28.31231\n",
      "1825 Train Loss 28.310932\n",
      "1826 Train Loss 28.30312\n",
      "1827 Train Loss 28.56991\n",
      "1828 Train Loss 28.313808\n",
      "1829 Train Loss 28.31231\n",
      "1830 Train Loss 28.310932\n",
      "1831 Train Loss 28.30312\n",
      "1832 Train Loss 28.56991\n",
      "1833 Train Loss 28.313808\n",
      "1834 Train Loss 28.31231\n",
      "1835 Train Loss 28.310932\n",
      "1836 Train Loss 28.30312\n",
      "1837 Train Loss 28.56991\n",
      "1838 Train Loss 28.313808\n",
      "1839 Train Loss 28.31231\n",
      "1840 Train Loss 28.310932\n",
      "1841 Train Loss 28.30312\n",
      "1842 Train Loss 28.56991\n",
      "1843 Train Loss 28.313808\n",
      "1844 Train Loss 28.31231\n",
      "1845 Train Loss 28.310932\n",
      "1846 Train Loss 28.30312\n",
      "1847 Train Loss 28.56991\n",
      "1848 Train Loss 28.313808\n",
      "1849 Train Loss 28.31231\n",
      "1850 Train Loss 28.310932\n",
      "1851 Train Loss 28.30312\n",
      "1852 Train Loss 28.56991\n",
      "1853 Train Loss 28.313808\n",
      "1854 Train Loss 28.31231\n",
      "1855 Train Loss 28.310932\n",
      "1856 Train Loss 28.30312\n",
      "1857 Train Loss 28.56991\n",
      "1858 Train Loss 28.313808\n",
      "1859 Train Loss 28.31231\n",
      "1860 Train Loss 28.310932\n",
      "1861 Train Loss 28.30312\n",
      "1862 Train Loss 28.56991\n",
      "1863 Train Loss 28.313808\n",
      "1864 Train Loss 28.31231\n",
      "1865 Train Loss 28.310932\n",
      "1866 Train Loss 28.30312\n",
      "1867 Train Loss 28.56991\n",
      "1868 Train Loss 28.313808\n",
      "1869 Train Loss 28.31231\n",
      "1870 Train Loss 28.310932\n",
      "1871 Train Loss 28.30312\n",
      "1872 Train Loss 28.56991\n",
      "1873 Train Loss 28.313808\n",
      "1874 Train Loss 28.31231\n",
      "1875 Train Loss 28.310932\n",
      "1876 Train Loss 28.30312\n",
      "1877 Train Loss 28.56991\n",
      "1878 Train Loss 28.313808\n",
      "1879 Train Loss 28.31231\n",
      "1880 Train Loss 28.310932\n",
      "1881 Train Loss 28.30312\n",
      "1882 Train Loss 28.56991\n",
      "1883 Train Loss 28.313808\n",
      "1884 Train Loss 28.31231\n",
      "1885 Train Loss 28.310932\n",
      "1886 Train Loss 28.30312\n",
      "1887 Train Loss 28.56991\n",
      "1888 Train Loss 28.313808\n",
      "1889 Train Loss 28.31231\n",
      "1890 Train Loss 28.310932\n",
      "1891 Train Loss 28.30312\n",
      "1892 Train Loss 28.56991\n",
      "1893 Train Loss 28.313808\n",
      "1894 Train Loss 28.31231\n",
      "1895 Train Loss 28.310932\n",
      "1896 Train Loss 28.30312\n",
      "1897 Train Loss 28.56991\n",
      "1898 Train Loss 28.313808\n",
      "1899 Train Loss 28.31231\n",
      "1900 Train Loss 28.310932\n",
      "1901 Train Loss 28.30312\n",
      "1902 Train Loss 28.56991\n",
      "1903 Train Loss 28.313808\n",
      "1904 Train Loss 28.31231\n",
      "1905 Train Loss 28.310932\n",
      "1906 Train Loss 28.30312\n",
      "1907 Train Loss 28.56991\n",
      "1908 Train Loss 28.313808\n",
      "1909 Train Loss 28.31231\n",
      "1910 Train Loss 28.310932\n",
      "1911 Train Loss 28.30312\n",
      "1912 Train Loss 28.56991\n",
      "1913 Train Loss 28.313808\n",
      "1914 Train Loss 28.31231\n",
      "1915 Train Loss 28.310932\n",
      "1916 Train Loss 28.30312\n",
      "1917 Train Loss 28.56991\n",
      "1918 Train Loss 28.313808\n",
      "1919 Train Loss 28.31231\n",
      "1920 Train Loss 28.310932\n",
      "1921 Train Loss 28.30312\n",
      "1922 Train Loss 28.56991\n",
      "1923 Train Loss 28.313808\n",
      "1924 Train Loss 28.31231\n",
      "1925 Train Loss 28.310932\n",
      "1926 Train Loss 28.30312\n",
      "1927 Train Loss 28.56991\n",
      "1928 Train Loss 28.313808\n",
      "1929 Train Loss 28.31231\n",
      "1930 Train Loss 28.310932\n",
      "1931 Train Loss 28.30312\n",
      "1932 Train Loss 28.56991\n",
      "1933 Train Loss 28.313808\n",
      "1934 Train Loss 28.31231\n",
      "1935 Train Loss 28.310932\n",
      "1936 Train Loss 28.30312\n",
      "1937 Train Loss 28.56991\n",
      "1938 Train Loss 28.313808\n",
      "1939 Train Loss 28.31231\n",
      "1940 Train Loss 28.310932\n",
      "1941 Train Loss 28.30312\n",
      "1942 Train Loss 28.56991\n",
      "1943 Train Loss 28.313808\n",
      "1944 Train Loss 28.31231\n",
      "1945 Train Loss 28.310932\n",
      "1946 Train Loss 28.30312\n",
      "1947 Train Loss 28.56991\n",
      "1948 Train Loss 28.313808\n",
      "1949 Train Loss 28.31231\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1950 Train Loss 28.310932\n",
      "1951 Train Loss 28.30312\n",
      "1952 Train Loss 28.56991\n",
      "1953 Train Loss 28.313808\n",
      "1954 Train Loss 28.31231\n",
      "1955 Train Loss 28.310932\n",
      "1956 Train Loss 28.30312\n",
      "1957 Train Loss 28.56991\n",
      "1958 Train Loss 28.313808\n",
      "1959 Train Loss 28.31231\n",
      "1960 Train Loss 28.310932\n",
      "1961 Train Loss 28.30312\n",
      "1962 Train Loss 28.56991\n",
      "1963 Train Loss 28.313808\n",
      "1964 Train Loss 28.31231\n",
      "1965 Train Loss 28.310932\n",
      "1966 Train Loss 28.30312\n",
      "1967 Train Loss 28.56991\n",
      "1968 Train Loss 28.313808\n",
      "1969 Train Loss 28.31231\n",
      "1970 Train Loss 28.310932\n",
      "1971 Train Loss 28.30312\n",
      "1972 Train Loss 28.56991\n",
      "1973 Train Loss 28.313808\n",
      "1974 Train Loss 28.31231\n",
      "1975 Train Loss 28.310932\n",
      "1976 Train Loss 28.30312\n",
      "1977 Train Loss 28.56991\n",
      "1978 Train Loss 28.313808\n",
      "1979 Train Loss 28.31231\n",
      "1980 Train Loss 28.310932\n",
      "1981 Train Loss 28.30312\n",
      "1982 Train Loss 28.56991\n",
      "1983 Train Loss 28.313808\n",
      "1984 Train Loss 28.31231\n",
      "1985 Train Loss 28.310932\n",
      "1986 Train Loss 28.30312\n",
      "1987 Train Loss 28.56991\n",
      "1988 Train Loss 28.313808\n",
      "1989 Train Loss 28.31231\n",
      "1990 Train Loss 28.310932\n",
      "1991 Train Loss 28.30312\n",
      "1992 Train Loss 28.56991\n",
      "1993 Train Loss 28.313808\n",
      "1994 Train Loss 28.31231\n",
      "1995 Train Loss 28.310932\n",
      "1996 Train Loss 28.30312\n",
      "1997 Train Loss 28.56991\n",
      "1998 Train Loss 28.313808\n",
      "1999 Train Loss 28.31231\n",
      "2000 Train Loss 28.310932\n",
      "2001 Train Loss 28.30312\n",
      "2002 Train Loss 28.56991\n",
      "2003 Train Loss 28.313808\n",
      "2004 Train Loss 28.31231\n",
      "2005 Train Loss 28.310932\n",
      "2006 Train Loss 28.30312\n",
      "2007 Train Loss 28.56991\n",
      "2008 Train Loss 28.313808\n",
      "2009 Train Loss 28.31231\n",
      "2010 Train Loss 28.310932\n",
      "2011 Train Loss 28.30312\n",
      "2012 Train Loss 28.56991\n",
      "2013 Train Loss 28.313808\n",
      "2014 Train Loss 28.31231\n",
      "2015 Train Loss 28.310932\n",
      "2016 Train Loss 28.30312\n",
      "2017 Train Loss 28.56991\n",
      "2018 Train Loss 28.313808\n",
      "2019 Train Loss 28.31231\n",
      "2020 Train Loss 28.310932\n",
      "2021 Train Loss 28.30312\n",
      "2022 Train Loss 28.56991\n",
      "2023 Train Loss 28.313808\n",
      "2024 Train Loss 28.31231\n",
      "2025 Train Loss 28.310932\n",
      "2026 Train Loss 28.30312\n",
      "2027 Train Loss 28.56991\n",
      "2028 Train Loss 28.313808\n",
      "2029 Train Loss 28.31231\n",
      "2030 Train Loss 28.310932\n",
      "2031 Train Loss 28.30312\n",
      "2032 Train Loss 28.56991\n",
      "2033 Train Loss 28.313808\n",
      "2034 Train Loss 28.31231\n",
      "2035 Train Loss 28.310932\n",
      "2036 Train Loss 28.30312\n",
      "2037 Train Loss 28.56991\n",
      "2038 Train Loss 28.313808\n",
      "2039 Train Loss 28.31231\n",
      "2040 Train Loss 28.310932\n",
      "2041 Train Loss 28.30312\n",
      "2042 Train Loss 28.56991\n",
      "2043 Train Loss 28.313808\n",
      "2044 Train Loss 28.31231\n",
      "2045 Train Loss 28.310932\n",
      "2046 Train Loss 28.30312\n",
      "2047 Train Loss 28.56991\n",
      "2048 Train Loss 28.313808\n",
      "2049 Train Loss 28.31231\n",
      "2050 Train Loss 28.310932\n",
      "2051 Train Loss 28.30312\n",
      "2052 Train Loss 28.56991\n",
      "2053 Train Loss 28.313808\n",
      "2054 Train Loss 28.31231\n",
      "2055 Train Loss 28.310932\n",
      "2056 Train Loss 28.30312\n",
      "2057 Train Loss 28.56991\n",
      "2058 Train Loss 28.313808\n",
      "2059 Train Loss 28.31231\n",
      "2060 Train Loss 28.310932\n",
      "2061 Train Loss 28.30312\n",
      "2062 Train Loss 28.56991\n",
      "2063 Train Loss 28.313808\n",
      "2064 Train Loss 28.31231\n",
      "2065 Train Loss 28.310932\n",
      "2066 Train Loss 28.30312\n",
      "2067 Train Loss 28.56991\n",
      "2068 Train Loss 28.313808\n",
      "2069 Train Loss 28.31231\n",
      "2070 Train Loss 28.310932\n",
      "2071 Train Loss 28.30312\n",
      "2072 Train Loss 28.56991\n",
      "2073 Train Loss 28.313808\n",
      "2074 Train Loss 28.31231\n",
      "2075 Train Loss 28.310932\n",
      "2076 Train Loss 28.30312\n",
      "2077 Train Loss 28.56991\n",
      "2078 Train Loss 28.313808\n",
      "2079 Train Loss 28.31231\n",
      "2080 Train Loss 28.310932\n",
      "2081 Train Loss 28.30312\n",
      "2082 Train Loss 28.56991\n",
      "2083 Train Loss 28.313808\n",
      "2084 Train Loss 28.31231\n",
      "2085 Train Loss 28.310932\n",
      "2086 Train Loss 28.30312\n",
      "2087 Train Loss 28.56991\n",
      "2088 Train Loss 28.313808\n",
      "2089 Train Loss 28.31231\n",
      "2090 Train Loss 28.310932\n",
      "2091 Train Loss 28.30312\n",
      "2092 Train Loss 28.56991\n",
      "2093 Train Loss 28.313808\n",
      "2094 Train Loss 28.31231\n",
      "2095 Train Loss 28.310932\n",
      "2096 Train Loss 28.30312\n",
      "2097 Train Loss 28.56991\n",
      "2098 Train Loss 28.313808\n",
      "2099 Train Loss 28.31231\n",
      "2100 Train Loss 28.310932\n",
      "2101 Train Loss 28.30312\n",
      "2102 Train Loss 28.56991\n",
      "2103 Train Loss 28.313808\n",
      "2104 Train Loss 28.31231\n",
      "2105 Train Loss 28.310932\n",
      "2106 Train Loss 28.30312\n",
      "2107 Train Loss 28.56991\n",
      "2108 Train Loss 28.313808\n",
      "2109 Train Loss 28.31231\n",
      "2110 Train Loss 28.310932\n",
      "2111 Train Loss 28.30312\n",
      "2112 Train Loss 28.56991\n",
      "2113 Train Loss 28.313808\n",
      "2114 Train Loss 28.31231\n",
      "2115 Train Loss 28.310932\n",
      "2116 Train Loss 28.30312\n",
      "2117 Train Loss 28.56991\n",
      "2118 Train Loss 28.313808\n",
      "2119 Train Loss 28.31231\n",
      "2120 Train Loss 28.310932\n",
      "2121 Train Loss 28.30312\n",
      "2122 Train Loss 28.56991\n",
      "2123 Train Loss 28.313808\n",
      "2124 Train Loss 28.31231\n",
      "2125 Train Loss 28.310932\n",
      "2126 Train Loss 28.30312\n",
      "2127 Train Loss 28.56991\n",
      "2128 Train Loss 28.313808\n",
      "2129 Train Loss 28.31231\n",
      "2130 Train Loss 28.310932\n",
      "2131 Train Loss 28.30312\n",
      "2132 Train Loss 28.56991\n",
      "2133 Train Loss 28.313808\n",
      "2134 Train Loss 28.31231\n",
      "2135 Train Loss 28.310932\n",
      "2136 Train Loss 28.30312\n",
      "2137 Train Loss 28.56991\n",
      "2138 Train Loss 28.313808\n",
      "2139 Train Loss 28.31231\n",
      "2140 Train Loss 28.310932\n",
      "2141 Train Loss 28.30312\n",
      "2142 Train Loss 28.56991\n",
      "2143 Train Loss 28.313808\n",
      "2144 Train Loss 28.31231\n",
      "2145 Train Loss 28.310932\n",
      "2146 Train Loss 28.30312\n",
      "2147 Train Loss 28.56991\n",
      "2148 Train Loss 28.313808\n",
      "2149 Train Loss 28.31231\n",
      "2150 Train Loss 28.310932\n",
      "2151 Train Loss 28.30312\n",
      "2152 Train Loss 28.56991\n",
      "2153 Train Loss 28.313808\n",
      "2154 Train Loss 28.31231\n",
      "2155 Train Loss 28.310932\n",
      "2156 Train Loss 28.30312\n",
      "2157 Train Loss 28.56991\n",
      "2158 Train Loss 28.313808\n",
      "2159 Train Loss 28.31231\n",
      "2160 Train Loss 28.310932\n",
      "2161 Train Loss 28.30312\n",
      "2162 Train Loss 28.56991\n",
      "2163 Train Loss 28.313808\n",
      "2164 Train Loss 28.31231\n",
      "2165 Train Loss 28.310932\n",
      "2166 Train Loss 28.30312\n",
      "2167 Train Loss 28.56991\n",
      "2168 Train Loss 28.313808\n",
      "2169 Train Loss 28.31231\n",
      "2170 Train Loss 28.310932\n",
      "2171 Train Loss 28.30312\n",
      "2172 Train Loss 28.56991\n",
      "2173 Train Loss 28.313808\n",
      "2174 Train Loss 28.31231\n",
      "2175 Train Loss 28.310932\n",
      "2176 Train Loss 28.30312\n",
      "2177 Train Loss 28.56991\n",
      "2178 Train Loss 28.313808\n",
      "2179 Train Loss 28.31231\n",
      "2180 Train Loss 28.310932\n",
      "2181 Train Loss 28.30312\n",
      "2182 Train Loss 28.56991\n",
      "2183 Train Loss 28.313808\n",
      "2184 Train Loss 28.31231\n",
      "2185 Train Loss 28.310932\n",
      "2186 Train Loss 28.30312\n",
      "2187 Train Loss 28.56991\n",
      "2188 Train Loss 28.313808\n",
      "2189 Train Loss 28.31231\n",
      "2190 Train Loss 28.310932\n",
      "2191 Train Loss 28.30312\n",
      "2192 Train Loss 28.56991\n",
      "2193 Train Loss 28.313808\n",
      "2194 Train Loss 28.31231\n",
      "2195 Train Loss 28.310932\n",
      "2196 Train Loss 28.30312\n",
      "2197 Train Loss 28.56991\n",
      "2198 Train Loss 28.313808\n",
      "2199 Train Loss 28.31231\n",
      "2200 Train Loss 28.310932\n",
      "2201 Train Loss 28.30312\n",
      "2202 Train Loss 28.56991\n",
      "2203 Train Loss 28.313808\n",
      "2204 Train Loss 28.31231\n",
      "2205 Train Loss 28.310932\n",
      "2206 Train Loss 28.30312\n",
      "2207 Train Loss 28.56991\n",
      "2208 Train Loss 28.313808\n",
      "2209 Train Loss 28.31231\n",
      "2210 Train Loss 28.310932\n",
      "2211 Train Loss 28.30312\n",
      "2212 Train Loss 28.56991\n",
      "2213 Train Loss 28.313808\n",
      "2214 Train Loss 28.31231\n",
      "2215 Train Loss 28.310932\n",
      "2216 Train Loss 28.30312\n",
      "2217 Train Loss 28.56991\n",
      "2218 Train Loss 28.313808\n",
      "2219 Train Loss 28.31231\n",
      "2220 Train Loss 28.310932\n",
      "2221 Train Loss 28.30312\n",
      "2222 Train Loss 28.56991\n",
      "2223 Train Loss 28.313808\n",
      "2224 Train Loss 28.31231\n",
      "2225 Train Loss 28.310932\n",
      "2226 Train Loss 28.30312\n",
      "2227 Train Loss 28.56991\n",
      "2228 Train Loss 28.313808\n",
      "2229 Train Loss 28.31231\n",
      "2230 Train Loss 28.310932\n",
      "2231 Train Loss 28.30312\n",
      "2232 Train Loss 28.56991\n",
      "2233 Train Loss 28.313808\n",
      "2234 Train Loss 28.31231\n",
      "2235 Train Loss 28.310932\n",
      "2236 Train Loss 28.30312\n",
      "2237 Train Loss 28.56991\n",
      "2238 Train Loss 28.313808\n",
      "2239 Train Loss 28.31231\n",
      "2240 Train Loss 28.310932\n",
      "2241 Train Loss 28.30312\n",
      "2242 Train Loss 28.56991\n",
      "2243 Train Loss 28.313808\n",
      "2244 Train Loss 28.31231\n",
      "2245 Train Loss 28.310932\n",
      "2246 Train Loss 28.30312\n",
      "2247 Train Loss 28.56991\n",
      "2248 Train Loss 28.313808\n",
      "2249 Train Loss 28.31231\n",
      "2250 Train Loss 28.310932\n",
      "2251 Train Loss 28.30312\n",
      "2252 Train Loss 28.56991\n",
      "2253 Train Loss 28.313808\n",
      "2254 Train Loss 28.31231\n",
      "2255 Train Loss 28.310932\n",
      "2256 Train Loss 28.30312\n",
      "2257 Train Loss 28.56991\n",
      "2258 Train Loss 28.313808\n",
      "2259 Train Loss 28.31231\n",
      "2260 Train Loss 28.310932\n",
      "2261 Train Loss 28.30312\n",
      "2262 Train Loss 28.56991\n",
      "2263 Train Loss 28.313808\n",
      "2264 Train Loss 28.31231\n",
      "2265 Train Loss 28.310932\n",
      "2266 Train Loss 28.30312\n",
      "2267 Train Loss 28.56991\n",
      "2268 Train Loss 28.313808\n",
      "2269 Train Loss 28.31231\n",
      "2270 Train Loss 28.310932\n",
      "2271 Train Loss 28.30312\n",
      "2272 Train Loss 28.56991\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2273 Train Loss 28.313808\n",
      "2274 Train Loss 28.31231\n",
      "2275 Train Loss 28.310932\n",
      "2276 Train Loss 28.30312\n",
      "2277 Train Loss 28.56991\n",
      "2278 Train Loss 28.313808\n",
      "2279 Train Loss 28.31231\n",
      "2280 Train Loss 28.310932\n",
      "2281 Train Loss 28.30312\n",
      "2282 Train Loss 28.56991\n",
      "2283 Train Loss 28.313808\n",
      "2284 Train Loss 28.31231\n",
      "2285 Train Loss 28.310932\n",
      "2286 Train Loss 28.30312\n",
      "2287 Train Loss 28.56991\n",
      "2288 Train Loss 28.313808\n",
      "2289 Train Loss 28.31231\n",
      "2290 Train Loss 28.310932\n",
      "2291 Train Loss 28.30312\n",
      "2292 Train Loss 28.56991\n",
      "2293 Train Loss 28.313808\n",
      "2294 Train Loss 28.31231\n",
      "2295 Train Loss 28.310932\n",
      "2296 Train Loss 28.30312\n",
      "2297 Train Loss 28.56991\n",
      "2298 Train Loss 28.313808\n",
      "2299 Train Loss 28.31231\n",
      "2300 Train Loss 28.310932\n",
      "2301 Train Loss 28.30312\n",
      "2302 Train Loss 28.56991\n",
      "2303 Train Loss 28.313808\n",
      "2304 Train Loss 28.31231\n",
      "2305 Train Loss 28.310932\n",
      "2306 Train Loss 28.30312\n",
      "2307 Train Loss 28.56991\n",
      "2308 Train Loss 28.313808\n",
      "2309 Train Loss 28.31231\n",
      "2310 Train Loss 28.310932\n",
      "2311 Train Loss 28.30312\n",
      "2312 Train Loss 28.56991\n",
      "2313 Train Loss 28.313808\n",
      "2314 Train Loss 28.31231\n",
      "2315 Train Loss 28.310932\n",
      "2316 Train Loss 28.30312\n",
      "2317 Train Loss 28.56991\n",
      "2318 Train Loss 28.313808\n",
      "2319 Train Loss 28.31231\n",
      "2320 Train Loss 28.310932\n",
      "2321 Train Loss 28.30312\n",
      "2322 Train Loss 28.56991\n",
      "2323 Train Loss 28.313808\n",
      "2324 Train Loss 28.31231\n",
      "2325 Train Loss 28.310932\n",
      "2326 Train Loss 28.30312\n",
      "2327 Train Loss 28.56991\n",
      "2328 Train Loss 28.313808\n",
      "2329 Train Loss 28.31231\n",
      "2330 Train Loss 28.310932\n",
      "2331 Train Loss 28.30312\n",
      "2332 Train Loss 28.56991\n",
      "2333 Train Loss 28.313808\n",
      "2334 Train Loss 28.31231\n",
      "2335 Train Loss 28.310932\n",
      "2336 Train Loss 28.30312\n",
      "2337 Train Loss 28.56991\n",
      "2338 Train Loss 28.313808\n",
      "2339 Train Loss 28.31231\n",
      "2340 Train Loss 28.310932\n",
      "2341 Train Loss 28.30312\n",
      "2342 Train Loss 28.56991\n",
      "2343 Train Loss 28.313808\n",
      "2344 Train Loss 28.31231\n",
      "2345 Train Loss 28.310932\n",
      "2346 Train Loss 28.30312\n",
      "2347 Train Loss 28.56991\n",
      "2348 Train Loss 28.313808\n",
      "2349 Train Loss 28.31231\n",
      "2350 Train Loss 28.310932\n",
      "2351 Train Loss 28.30312\n",
      "2352 Train Loss 28.56991\n",
      "2353 Train Loss 28.313808\n",
      "2354 Train Loss 28.31231\n",
      "2355 Train Loss 28.310932\n",
      "2356 Train Loss 28.30312\n",
      "2357 Train Loss 28.56991\n",
      "2358 Train Loss 28.313808\n",
      "2359 Train Loss 28.31231\n",
      "2360 Train Loss 28.310932\n",
      "2361 Train Loss 28.30312\n",
      "2362 Train Loss 28.56991\n",
      "2363 Train Loss 28.313808\n",
      "2364 Train Loss 28.31231\n",
      "2365 Train Loss 28.310932\n",
      "2366 Train Loss 28.30312\n",
      "2367 Train Loss 28.56991\n",
      "2368 Train Loss 28.313808\n",
      "2369 Train Loss 28.31231\n",
      "2370 Train Loss 28.310932\n",
      "2371 Train Loss 28.30312\n",
      "2372 Train Loss 28.56991\n",
      "2373 Train Loss 28.313808\n",
      "2374 Train Loss 28.31231\n",
      "2375 Train Loss 28.310932\n",
      "2376 Train Loss 28.30312\n",
      "2377 Train Loss 28.56991\n",
      "2378 Train Loss 28.313808\n",
      "2379 Train Loss 28.31231\n",
      "2380 Train Loss 28.310932\n",
      "2381 Train Loss 28.30312\n",
      "2382 Train Loss 28.56991\n",
      "2383 Train Loss 28.313808\n",
      "2384 Train Loss 28.31231\n",
      "2385 Train Loss 28.310932\n",
      "2386 Train Loss 28.30312\n",
      "2387 Train Loss 28.56991\n",
      "2388 Train Loss 28.313808\n",
      "2389 Train Loss 28.31231\n",
      "2390 Train Loss 28.310932\n",
      "2391 Train Loss 28.30312\n",
      "2392 Train Loss 28.56991\n",
      "2393 Train Loss 28.313808\n",
      "2394 Train Loss 28.31231\n",
      "2395 Train Loss 28.310932\n",
      "2396 Train Loss 28.30312\n",
      "2397 Train Loss 28.56991\n",
      "2398 Train Loss 28.313808\n",
      "2399 Train Loss 28.31231\n",
      "2400 Train Loss 28.310932\n",
      "2401 Train Loss 28.30312\n",
      "2402 Train Loss 28.56991\n",
      "2403 Train Loss 28.313808\n",
      "2404 Train Loss 28.31231\n",
      "2405 Train Loss 28.310932\n",
      "2406 Train Loss 28.30312\n",
      "2407 Train Loss 28.56991\n",
      "2408 Train Loss 28.313808\n",
      "2409 Train Loss 28.31231\n",
      "2410 Train Loss 28.310932\n",
      "2411 Train Loss 28.30312\n",
      "2412 Train Loss 28.56991\n",
      "2413 Train Loss 28.313808\n",
      "2414 Train Loss 28.31231\n",
      "2415 Train Loss 28.310932\n",
      "2416 Train Loss 28.30312\n",
      "2417 Train Loss 28.56991\n",
      "2418 Train Loss 28.313808\n",
      "2419 Train Loss 28.31231\n",
      "2420 Train Loss 28.310932\n",
      "2421 Train Loss 28.30312\n",
      "2422 Train Loss 28.56991\n",
      "2423 Train Loss 28.313808\n",
      "2424 Train Loss 28.31231\n",
      "2425 Train Loss 28.310932\n",
      "2426 Train Loss 28.30312\n",
      "2427 Train Loss 28.56991\n",
      "2428 Train Loss 28.313808\n",
      "2429 Train Loss 28.31231\n",
      "2430 Train Loss 28.310932\n",
      "2431 Train Loss 28.30312\n",
      "2432 Train Loss 28.56991\n",
      "2433 Train Loss 28.313808\n",
      "2434 Train Loss 28.31231\n",
      "2435 Train Loss 28.310932\n",
      "2436 Train Loss 28.30312\n",
      "2437 Train Loss 28.56991\n",
      "2438 Train Loss 28.313808\n",
      "2439 Train Loss 28.31231\n",
      "2440 Train Loss 28.310932\n",
      "2441 Train Loss 28.30312\n",
      "2442 Train Loss 28.56991\n",
      "2443 Train Loss 28.313808\n",
      "2444 Train Loss 28.31231\n",
      "2445 Train Loss 28.310932\n",
      "2446 Train Loss 28.30312\n",
      "2447 Train Loss 28.56991\n",
      "2448 Train Loss 28.313808\n",
      "2449 Train Loss 28.31231\n",
      "2450 Train Loss 28.310932\n",
      "2451 Train Loss 28.30312\n",
      "2452 Train Loss 28.56991\n",
      "2453 Train Loss 28.313808\n",
      "2454 Train Loss 28.31231\n",
      "2455 Train Loss 28.310932\n",
      "2456 Train Loss 28.30312\n",
      "2457 Train Loss 28.56991\n",
      "2458 Train Loss 28.313808\n",
      "2459 Train Loss 28.31231\n",
      "2460 Train Loss 28.310932\n",
      "2461 Train Loss 28.30312\n",
      "2462 Train Loss 28.56991\n",
      "2463 Train Loss 28.313808\n",
      "2464 Train Loss 28.31231\n",
      "2465 Train Loss 28.310932\n",
      "2466 Train Loss 28.30312\n",
      "2467 Train Loss 28.56991\n",
      "2468 Train Loss 28.313808\n",
      "2469 Train Loss 28.31231\n",
      "2470 Train Loss 28.310932\n",
      "2471 Train Loss 28.30312\n",
      "2472 Train Loss 28.56991\n",
      "2473 Train Loss 28.313808\n",
      "2474 Train Loss 28.31231\n",
      "2475 Train Loss 28.310932\n",
      "2476 Train Loss 28.30312\n",
      "2477 Train Loss 28.56991\n",
      "2478 Train Loss 28.313808\n",
      "2479 Train Loss 28.31231\n",
      "2480 Train Loss 28.310932\n",
      "2481 Train Loss 28.30312\n",
      "2482 Train Loss 28.56991\n",
      "2483 Train Loss 28.313808\n",
      "2484 Train Loss 28.31231\n",
      "2485 Train Loss 28.310932\n",
      "2486 Train Loss 28.30312\n",
      "2487 Train Loss 28.56991\n",
      "2488 Train Loss 28.313808\n",
      "2489 Train Loss 28.31231\n",
      "2490 Train Loss 28.310932\n",
      "2491 Train Loss 28.30312\n",
      "2492 Train Loss 28.56991\n",
      "2493 Train Loss 28.313808\n",
      "2494 Train Loss 28.31231\n",
      "2495 Train Loss 28.310932\n",
      "2496 Train Loss 28.30312\n",
      "2497 Train Loss 28.56991\n",
      "2498 Train Loss 28.313808\n",
      "2499 Train Loss 28.31231\n",
      "2500 Train Loss 28.310932\n",
      "2501 Train Loss 28.30312\n",
      "2502 Train Loss 28.56991\n",
      "2503 Train Loss 28.313808\n",
      "2504 Train Loss 28.31231\n",
      "2505 Train Loss 28.310932\n",
      "2506 Train Loss 28.30312\n",
      "2507 Train Loss 28.56991\n",
      "2508 Train Loss 28.313808\n",
      "2509 Train Loss 28.31231\n",
      "2510 Train Loss 28.310932\n",
      "2511 Train Loss 28.30312\n",
      "2512 Train Loss 28.56991\n",
      "2513 Train Loss 28.313808\n",
      "2514 Train Loss 28.31231\n",
      "2515 Train Loss 28.310932\n",
      "2516 Train Loss 28.30312\n",
      "2517 Train Loss 28.56991\n",
      "2518 Train Loss 28.313808\n",
      "2519 Train Loss 28.31231\n",
      "2520 Train Loss 28.310932\n",
      "2521 Train Loss 28.30312\n",
      "2522 Train Loss 28.56991\n",
      "2523 Train Loss 28.313808\n",
      "2524 Train Loss 28.31231\n",
      "2525 Train Loss 28.310932\n",
      "2526 Train Loss 28.30312\n",
      "2527 Train Loss 28.56991\n",
      "2528 Train Loss 28.313808\n",
      "2529 Train Loss 28.31231\n",
      "2530 Train Loss 28.310932\n",
      "2531 Train Loss 28.30312\n",
      "2532 Train Loss 28.56991\n",
      "2533 Train Loss 28.313808\n",
      "2534 Train Loss 28.31231\n",
      "2535 Train Loss 28.310932\n",
      "2536 Train Loss 28.30312\n",
      "2537 Train Loss 28.56991\n",
      "2538 Train Loss 28.313808\n",
      "2539 Train Loss 28.31231\n",
      "2540 Train Loss 28.310932\n",
      "2541 Train Loss 28.30312\n",
      "2542 Train Loss 28.56991\n",
      "2543 Train Loss 28.313808\n",
      "2544 Train Loss 28.31231\n",
      "2545 Train Loss 28.310932\n",
      "2546 Train Loss 28.30312\n",
      "2547 Train Loss 28.56991\n",
      "2548 Train Loss 28.313808\n",
      "2549 Train Loss 28.31231\n",
      "2550 Train Loss 28.310932\n",
      "2551 Train Loss 28.30312\n",
      "2552 Train Loss 28.56991\n",
      "2553 Train Loss 28.313808\n",
      "2554 Train Loss 28.31231\n",
      "2555 Train Loss 28.310932\n",
      "2556 Train Loss 28.30312\n",
      "2557 Train Loss 28.56991\n",
      "2558 Train Loss 28.313808\n",
      "2559 Train Loss 28.31231\n",
      "2560 Train Loss 28.310932\n",
      "2561 Train Loss 28.30312\n",
      "2562 Train Loss 28.56991\n",
      "2563 Train Loss 28.313808\n",
      "2564 Train Loss 28.31231\n",
      "2565 Train Loss 28.310932\n",
      "2566 Train Loss 28.30312\n",
      "2567 Train Loss 28.56991\n",
      "2568 Train Loss 28.313808\n",
      "2569 Train Loss 28.31231\n",
      "2570 Train Loss 28.310932\n",
      "2571 Train Loss 28.30312\n",
      "2572 Train Loss 28.56991\n",
      "2573 Train Loss 28.313808\n",
      "2574 Train Loss 28.31231\n",
      "2575 Train Loss 28.310932\n",
      "2576 Train Loss 28.30312\n",
      "2577 Train Loss 28.56991\n",
      "2578 Train Loss 28.313808\n",
      "2579 Train Loss 28.31231\n",
      "2580 Train Loss 28.310932\n",
      "2581 Train Loss 28.30312\n",
      "2582 Train Loss 28.56991\n",
      "2583 Train Loss 28.313808\n",
      "2584 Train Loss 28.31231\n",
      "2585 Train Loss 28.310932\n",
      "2586 Train Loss 28.30312\n",
      "2587 Train Loss 28.56991\n",
      "2588 Train Loss 28.313808\n",
      "2589 Train Loss 28.31231\n",
      "2590 Train Loss 28.310932\n",
      "2591 Train Loss 28.30312\n",
      "2592 Train Loss 28.56991\n",
      "2593 Train Loss 28.313808\n",
      "2594 Train Loss 28.31231\n",
      "2595 Train Loss 28.310932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2596 Train Loss 28.30312\n",
      "2597 Train Loss 28.56991\n",
      "2598 Train Loss 28.313808\n",
      "2599 Train Loss 28.31231\n",
      "2600 Train Loss 28.310932\n",
      "2601 Train Loss 28.30312\n",
      "2602 Train Loss 28.56991\n",
      "2603 Train Loss 28.313808\n",
      "2604 Train Loss 28.31231\n",
      "2605 Train Loss 28.310932\n",
      "2606 Train Loss 28.30312\n",
      "2607 Train Loss 28.56991\n",
      "2608 Train Loss 28.313808\n",
      "2609 Train Loss 28.31231\n",
      "2610 Train Loss 28.310932\n",
      "2611 Train Loss 28.30312\n",
      "2612 Train Loss 28.56991\n",
      "2613 Train Loss 28.313808\n",
      "2614 Train Loss 28.31231\n",
      "2615 Train Loss 28.310932\n",
      "2616 Train Loss 28.30312\n",
      "2617 Train Loss 28.56991\n",
      "2618 Train Loss 28.313808\n",
      "2619 Train Loss 28.31231\n",
      "2620 Train Loss 28.310932\n",
      "2621 Train Loss 28.30312\n",
      "2622 Train Loss 28.56991\n",
      "2623 Train Loss 28.313808\n",
      "2624 Train Loss 28.31231\n",
      "2625 Train Loss 28.310932\n",
      "2626 Train Loss 28.30312\n",
      "2627 Train Loss 28.56991\n",
      "2628 Train Loss 28.313808\n",
      "2629 Train Loss 28.31231\n",
      "2630 Train Loss 28.310932\n",
      "2631 Train Loss 28.30312\n",
      "2632 Train Loss 28.56991\n",
      "2633 Train Loss 28.313808\n",
      "2634 Train Loss 28.31231\n",
      "2635 Train Loss 28.310932\n",
      "2636 Train Loss 28.30312\n",
      "2637 Train Loss 28.56991\n",
      "2638 Train Loss 28.313808\n",
      "2639 Train Loss 28.31231\n",
      "2640 Train Loss 28.310932\n",
      "2641 Train Loss 28.30312\n",
      "2642 Train Loss 28.56991\n",
      "2643 Train Loss 28.313808\n",
      "2644 Train Loss 28.31231\n",
      "2645 Train Loss 28.310932\n",
      "2646 Train Loss 28.30312\n",
      "2647 Train Loss 28.56991\n",
      "2648 Train Loss 28.313808\n",
      "2649 Train Loss 28.31231\n",
      "2650 Train Loss 28.310932\n",
      "2651 Train Loss 28.30312\n",
      "2652 Train Loss 28.56991\n",
      "2653 Train Loss 28.313808\n",
      "2654 Train Loss 28.31231\n",
      "2655 Train Loss 28.310932\n",
      "2656 Train Loss 28.30312\n",
      "2657 Train Loss 28.56991\n",
      "2658 Train Loss 28.313808\n",
      "2659 Train Loss 28.31231\n",
      "2660 Train Loss 28.310932\n",
      "2661 Train Loss 28.30312\n",
      "2662 Train Loss 28.56991\n",
      "2663 Train Loss 28.313808\n",
      "2664 Train Loss 28.31231\n",
      "2665 Train Loss 28.310932\n",
      "2666 Train Loss 28.30312\n",
      "2667 Train Loss 28.56991\n",
      "2668 Train Loss 28.313808\n",
      "2669 Train Loss 28.31231\n",
      "2670 Train Loss 28.310932\n",
      "2671 Train Loss 28.30312\n",
      "2672 Train Loss 28.56991\n",
      "2673 Train Loss 28.313808\n",
      "2674 Train Loss 28.31231\n",
      "2675 Train Loss 28.310932\n",
      "2676 Train Loss 28.30312\n",
      "2677 Train Loss 28.56991\n",
      "2678 Train Loss 28.313808\n",
      "2679 Train Loss 28.31231\n",
      "2680 Train Loss 28.310932\n",
      "2681 Train Loss 28.30312\n",
      "2682 Train Loss 28.56991\n",
      "2683 Train Loss 28.313808\n",
      "2684 Train Loss 28.31231\n",
      "2685 Train Loss 28.310932\n",
      "2686 Train Loss 28.30312\n",
      "2687 Train Loss 28.56991\n",
      "2688 Train Loss 28.313808\n",
      "2689 Train Loss 28.31231\n",
      "2690 Train Loss 28.310932\n",
      "2691 Train Loss 28.30312\n",
      "2692 Train Loss 28.56991\n",
      "2693 Train Loss 28.313808\n",
      "2694 Train Loss 28.31231\n",
      "2695 Train Loss 28.310932\n",
      "2696 Train Loss 28.30312\n",
      "2697 Train Loss 28.56991\n",
      "2698 Train Loss 28.313808\n",
      "2699 Train Loss 28.31231\n",
      "2700 Train Loss 28.310932\n",
      "2701 Train Loss 28.30312\n",
      "2702 Train Loss 28.56991\n",
      "2703 Train Loss 28.313808\n",
      "2704 Train Loss 28.31231\n",
      "2705 Train Loss 28.310932\n",
      "2706 Train Loss 28.30312\n",
      "2707 Train Loss 28.56991\n",
      "2708 Train Loss 28.313808\n",
      "2709 Train Loss 28.31231\n",
      "2710 Train Loss 28.310932\n",
      "2711 Train Loss 28.30312\n",
      "2712 Train Loss 28.56991\n",
      "2713 Train Loss 28.313808\n",
      "2714 Train Loss 28.31231\n",
      "2715 Train Loss 28.310932\n",
      "2716 Train Loss 28.30312\n",
      "2717 Train Loss 28.56991\n",
      "2718 Train Loss 28.313808\n",
      "2719 Train Loss 28.31231\n",
      "2720 Train Loss 28.310932\n",
      "2721 Train Loss 28.30312\n",
      "2722 Train Loss 28.56991\n",
      "2723 Train Loss 28.313808\n",
      "2724 Train Loss 28.31231\n",
      "2725 Train Loss 28.310932\n",
      "2726 Train Loss 28.30312\n",
      "2727 Train Loss 28.56991\n",
      "2728 Train Loss 28.313808\n",
      "2729 Train Loss 28.31231\n",
      "2730 Train Loss 28.310932\n",
      "2731 Train Loss 28.30312\n",
      "2732 Train Loss 28.56991\n",
      "2733 Train Loss 28.313808\n",
      "2734 Train Loss 28.31231\n",
      "2735 Train Loss 28.310932\n",
      "2736 Train Loss 28.30312\n",
      "2737 Train Loss 28.56991\n",
      "2738 Train Loss 28.313808\n",
      "2739 Train Loss 28.31231\n",
      "2740 Train Loss 28.310932\n",
      "2741 Train Loss 28.30312\n",
      "2742 Train Loss 28.56991\n",
      "2743 Train Loss 28.313808\n",
      "2744 Train Loss 28.31231\n",
      "2745 Train Loss 28.310932\n",
      "2746 Train Loss 28.30312\n",
      "2747 Train Loss 28.56991\n",
      "2748 Train Loss 28.313808\n",
      "2749 Train Loss 28.31231\n",
      "2750 Train Loss 28.310932\n",
      "2751 Train Loss 28.30312\n",
      "2752 Train Loss 28.56991\n",
      "2753 Train Loss 28.313808\n",
      "2754 Train Loss 28.31231\n",
      "2755 Train Loss 28.310932\n",
      "2756 Train Loss 28.30312\n",
      "2757 Train Loss 28.56991\n",
      "2758 Train Loss 28.313808\n",
      "2759 Train Loss 28.31231\n",
      "2760 Train Loss 28.310932\n",
      "2761 Train Loss 28.30312\n",
      "2762 Train Loss 28.56991\n",
      "2763 Train Loss 28.313808\n",
      "2764 Train Loss 28.31231\n",
      "2765 Train Loss 28.310932\n",
      "2766 Train Loss 28.30312\n",
      "2767 Train Loss 28.56991\n",
      "2768 Train Loss 28.313808\n",
      "2769 Train Loss 28.31231\n",
      "2770 Train Loss 28.310932\n",
      "2771 Train Loss 28.30312\n",
      "2772 Train Loss 28.56991\n",
      "2773 Train Loss 28.313808\n",
      "2774 Train Loss 28.31231\n",
      "2775 Train Loss 28.310932\n",
      "2776 Train Loss 28.30312\n",
      "2777 Train Loss 28.56991\n",
      "2778 Train Loss 28.313808\n",
      "2779 Train Loss 28.31231\n",
      "2780 Train Loss 28.310932\n",
      "2781 Train Loss 28.30312\n",
      "2782 Train Loss 28.56991\n",
      "2783 Train Loss 28.313808\n",
      "2784 Train Loss 28.31231\n",
      "2785 Train Loss 28.310932\n",
      "2786 Train Loss 28.30312\n",
      "2787 Train Loss 28.56991\n",
      "2788 Train Loss 28.313808\n",
      "2789 Train Loss 28.31231\n",
      "2790 Train Loss 28.310932\n",
      "2791 Train Loss 28.30312\n",
      "2792 Train Loss 28.56991\n",
      "2793 Train Loss 28.313808\n",
      "2794 Train Loss 28.31231\n",
      "2795 Train Loss 28.310932\n",
      "2796 Train Loss 28.30312\n",
      "2797 Train Loss 28.56991\n",
      "2798 Train Loss 28.313808\n",
      "2799 Train Loss 28.31231\n",
      "2800 Train Loss 28.310932\n",
      "2801 Train Loss 28.30312\n",
      "2802 Train Loss 28.56991\n",
      "2803 Train Loss 28.313808\n",
      "2804 Train Loss 28.31231\n",
      "2805 Train Loss 28.310932\n",
      "2806 Train Loss 28.30312\n",
      "2807 Train Loss 28.56991\n",
      "2808 Train Loss 28.313808\n",
      "2809 Train Loss 28.31231\n",
      "2810 Train Loss 28.310932\n",
      "2811 Train Loss 28.30312\n",
      "2812 Train Loss 28.56991\n",
      "2813 Train Loss 28.313808\n",
      "2814 Train Loss 28.31231\n",
      "2815 Train Loss 28.310932\n",
      "2816 Train Loss 28.30312\n",
      "2817 Train Loss 28.56991\n",
      "2818 Train Loss 28.313808\n",
      "2819 Train Loss 28.31231\n",
      "2820 Train Loss 28.310932\n",
      "2821 Train Loss 28.30312\n",
      "2822 Train Loss 28.56991\n",
      "2823 Train Loss 28.313808\n",
      "2824 Train Loss 28.31231\n",
      "2825 Train Loss 28.310932\n",
      "2826 Train Loss 28.30312\n",
      "2827 Train Loss 28.56991\n",
      "2828 Train Loss 28.313808\n",
      "2829 Train Loss 28.31231\n",
      "2830 Train Loss 28.310932\n",
      "2831 Train Loss 28.30312\n",
      "2832 Train Loss 28.56991\n",
      "2833 Train Loss 28.313808\n",
      "2834 Train Loss 28.31231\n",
      "2835 Train Loss 28.310932\n",
      "2836 Train Loss 28.30312\n",
      "2837 Train Loss 28.56991\n",
      "2838 Train Loss 28.313808\n",
      "2839 Train Loss 28.31231\n",
      "2840 Train Loss 28.310932\n",
      "2841 Train Loss 28.30312\n",
      "2842 Train Loss 28.56991\n",
      "2843 Train Loss 28.313808\n",
      "2844 Train Loss 28.31231\n",
      "2845 Train Loss 28.310932\n",
      "2846 Train Loss 28.30312\n",
      "2847 Train Loss 28.56991\n",
      "2848 Train Loss 28.313808\n",
      "2849 Train Loss 28.31231\n",
      "2850 Train Loss 28.310932\n",
      "2851 Train Loss 28.30312\n",
      "2852 Train Loss 28.56991\n",
      "2853 Train Loss 28.313808\n",
      "2854 Train Loss 28.31231\n",
      "2855 Train Loss 28.310932\n",
      "2856 Train Loss 28.30312\n",
      "2857 Train Loss 28.56991\n",
      "2858 Train Loss 28.313808\n",
      "2859 Train Loss 28.31231\n",
      "2860 Train Loss 28.310932\n",
      "2861 Train Loss 28.30312\n",
      "2862 Train Loss 28.56991\n",
      "2863 Train Loss 28.313808\n",
      "2864 Train Loss 28.31231\n",
      "2865 Train Loss 28.310932\n",
      "2866 Train Loss 28.30312\n",
      "2867 Train Loss 28.56991\n",
      "2868 Train Loss 28.313808\n",
      "2869 Train Loss 28.31231\n",
      "2870 Train Loss 28.310932\n",
      "2871 Train Loss 28.30312\n",
      "2872 Train Loss 28.56991\n",
      "2873 Train Loss 28.313808\n",
      "2874 Train Loss 28.31231\n",
      "2875 Train Loss 28.310932\n",
      "2876 Train Loss 28.30312\n",
      "2877 Train Loss 28.56991\n",
      "2878 Train Loss 28.313808\n",
      "2879 Train Loss 28.31231\n",
      "2880 Train Loss 28.310932\n",
      "2881 Train Loss 28.30312\n",
      "2882 Train Loss 28.56991\n",
      "2883 Train Loss 28.313808\n",
      "2884 Train Loss 28.31231\n",
      "2885 Train Loss 28.310932\n",
      "2886 Train Loss 28.30312\n",
      "2887 Train Loss 28.56991\n",
      "2888 Train Loss 28.313808\n",
      "2889 Train Loss 28.31231\n",
      "2890 Train Loss 28.310932\n",
      "2891 Train Loss 28.30312\n",
      "2892 Train Loss 28.56991\n",
      "2893 Train Loss 28.313808\n",
      "2894 Train Loss 28.31231\n",
      "2895 Train Loss 28.310932\n",
      "2896 Train Loss 28.30312\n",
      "2897 Train Loss 28.56991\n",
      "2898 Train Loss 28.313808\n",
      "2899 Train Loss 28.31231\n",
      "2900 Train Loss 28.310932\n",
      "2901 Train Loss 28.30312\n",
      "2902 Train Loss 28.56991\n",
      "2903 Train Loss 28.313808\n",
      "2904 Train Loss 28.31231\n",
      "2905 Train Loss 28.310932\n",
      "2906 Train Loss 28.30312\n",
      "2907 Train Loss 28.56991\n",
      "2908 Train Loss 28.313808\n",
      "2909 Train Loss 28.31231\n",
      "2910 Train Loss 28.310932\n",
      "2911 Train Loss 28.30312\n",
      "2912 Train Loss 28.56991\n",
      "2913 Train Loss 28.313808\n",
      "2914 Train Loss 28.31231\n",
      "2915 Train Loss 28.310932\n",
      "2916 Train Loss 28.30312\n",
      "2917 Train Loss 28.56991\n",
      "2918 Train Loss 28.313808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2919 Train Loss 28.31231\n",
      "2920 Train Loss 28.310932\n",
      "2921 Train Loss 28.30312\n",
      "2922 Train Loss 28.56991\n",
      "2923 Train Loss 28.313808\n",
      "2924 Train Loss 28.31231\n",
      "2925 Train Loss 28.310932\n",
      "2926 Train Loss 28.30312\n",
      "2927 Train Loss 28.56991\n",
      "2928 Train Loss 28.313808\n",
      "2929 Train Loss 28.31231\n",
      "2930 Train Loss 28.310932\n",
      "2931 Train Loss 28.30312\n",
      "2932 Train Loss 28.56991\n",
      "2933 Train Loss 28.313808\n",
      "2934 Train Loss 28.31231\n",
      "2935 Train Loss 28.310932\n",
      "2936 Train Loss 28.30312\n",
      "2937 Train Loss 28.56991\n",
      "2938 Train Loss 28.313808\n",
      "2939 Train Loss 28.31231\n",
      "2940 Train Loss 28.310932\n",
      "2941 Train Loss 28.30312\n",
      "2942 Train Loss 28.56991\n",
      "2943 Train Loss 28.313808\n",
      "2944 Train Loss 28.31231\n",
      "2945 Train Loss 28.310932\n",
      "2946 Train Loss 28.30312\n",
      "2947 Train Loss 28.56991\n",
      "2948 Train Loss 28.313808\n",
      "2949 Train Loss 28.31231\n",
      "2950 Train Loss 28.310932\n",
      "2951 Train Loss 28.30312\n",
      "2952 Train Loss 28.56991\n",
      "2953 Train Loss 28.313808\n",
      "2954 Train Loss 28.31231\n",
      "2955 Train Loss 28.310932\n",
      "2956 Train Loss 28.30312\n",
      "2957 Train Loss 28.56991\n",
      "2958 Train Loss 28.313808\n",
      "2959 Train Loss 28.31231\n",
      "2960 Train Loss 28.310932\n",
      "2961 Train Loss 28.30312\n",
      "2962 Train Loss 28.56991\n",
      "2963 Train Loss 28.313808\n",
      "2964 Train Loss 28.31231\n",
      "2965 Train Loss 28.310932\n",
      "2966 Train Loss 28.30312\n",
      "2967 Train Loss 28.56991\n",
      "2968 Train Loss 28.313808\n",
      "2969 Train Loss 28.31231\n",
      "2970 Train Loss 28.310932\n",
      "2971 Train Loss 28.30312\n",
      "2972 Train Loss 28.56991\n",
      "2973 Train Loss 28.313808\n",
      "2974 Train Loss 28.31231\n",
      "2975 Train Loss 28.310932\n",
      "2976 Train Loss 28.30312\n",
      "2977 Train Loss 28.56991\n",
      "2978 Train Loss 28.313808\n",
      "2979 Train Loss 28.31231\n",
      "2980 Train Loss 28.310932\n",
      "2981 Train Loss 28.30312\n",
      "2982 Train Loss 28.56991\n",
      "2983 Train Loss 28.313808\n",
      "2984 Train Loss 28.31231\n",
      "2985 Train Loss 28.310932\n",
      "2986 Train Loss 28.30312\n",
      "2987 Train Loss 28.56991\n",
      "2988 Train Loss 28.313808\n",
      "2989 Train Loss 28.31231\n",
      "2990 Train Loss 28.310932\n",
      "2991 Train Loss 28.30312\n",
      "2992 Train Loss 28.56991\n",
      "2993 Train Loss 28.313808\n",
      "2994 Train Loss 28.31231\n",
      "2995 Train Loss 28.310932\n",
      "2996 Train Loss 28.30312\n",
      "2997 Train Loss 28.56991\n",
      "2998 Train Loss 28.313808\n",
      "2999 Train Loss 28.31231\n",
      "3000 Train Loss 28.310932\n",
      "3001 Train Loss 28.30312\n",
      "3002 Train Loss 28.56991\n",
      "3003 Train Loss 28.313808\n",
      "3004 Train Loss 28.31231\n",
      "3005 Train Loss 28.310932\n",
      "3006 Train Loss 28.30312\n",
      "3007 Train Loss 28.56991\n",
      "3008 Train Loss 28.313808\n",
      "3009 Train Loss 28.31231\n",
      "3010 Train Loss 28.310932\n",
      "3011 Train Loss 28.30312\n",
      "3012 Train Loss 28.56991\n",
      "3013 Train Loss 28.313808\n",
      "3014 Train Loss 28.31231\n",
      "3015 Train Loss 28.310932\n",
      "3016 Train Loss 28.30312\n",
      "3017 Train Loss 28.56991\n",
      "3018 Train Loss 28.313808\n",
      "3019 Train Loss 28.31231\n",
      "3020 Train Loss 28.310932\n",
      "3021 Train Loss 28.30312\n",
      "3022 Train Loss 28.56991\n",
      "3023 Train Loss 28.313808\n",
      "3024 Train Loss 28.31231\n",
      "3025 Train Loss 28.310932\n",
      "3026 Train Loss 28.30312\n",
      "3027 Train Loss 28.56991\n",
      "3028 Train Loss 28.313808\n",
      "3029 Train Loss 28.31231\n",
      "3030 Train Loss 28.310932\n",
      "3031 Train Loss 28.30312\n",
      "3032 Train Loss 28.56991\n",
      "3033 Train Loss 28.313808\n",
      "3034 Train Loss 28.31231\n",
      "3035 Train Loss 28.310932\n",
      "3036 Train Loss 28.30312\n",
      "3037 Train Loss 28.56991\n",
      "3038 Train Loss 28.313808\n",
      "3039 Train Loss 28.31231\n",
      "3040 Train Loss 28.310932\n",
      "3041 Train Loss 28.30312\n",
      "3042 Train Loss 28.56991\n",
      "3043 Train Loss 28.313808\n",
      "3044 Train Loss 28.31231\n",
      "3045 Train Loss 28.310932\n",
      "3046 Train Loss 28.30312\n",
      "3047 Train Loss 28.56991\n",
      "3048 Train Loss 28.313808\n",
      "3049 Train Loss 28.31231\n",
      "3050 Train Loss 28.310932\n",
      "3051 Train Loss 28.30312\n",
      "3052 Train Loss 28.56991\n",
      "3053 Train Loss 28.313808\n",
      "3054 Train Loss 28.31231\n",
      "3055 Train Loss 28.310932\n",
      "3056 Train Loss 28.30312\n",
      "3057 Train Loss 28.56991\n",
      "3058 Train Loss 28.313808\n",
      "3059 Train Loss 28.31231\n",
      "3060 Train Loss 28.310932\n",
      "3061 Train Loss 28.30312\n",
      "3062 Train Loss 28.56991\n",
      "3063 Train Loss 28.313808\n",
      "3064 Train Loss 28.31231\n",
      "3065 Train Loss 28.310932\n",
      "3066 Train Loss 28.30312\n",
      "3067 Train Loss 28.56991\n",
      "3068 Train Loss 28.313808\n",
      "3069 Train Loss 28.31231\n",
      "3070 Train Loss 28.310932\n",
      "3071 Train Loss 28.30312\n",
      "3072 Train Loss 28.56991\n",
      "3073 Train Loss 28.313808\n",
      "3074 Train Loss 28.31231\n",
      "3075 Train Loss 28.310932\n",
      "3076 Train Loss 28.30312\n",
      "3077 Train Loss 28.56991\n",
      "3078 Train Loss 28.313808\n",
      "3079 Train Loss 28.31231\n",
      "3080 Train Loss 28.310932\n",
      "3081 Train Loss 28.30312\n",
      "3082 Train Loss 28.56991\n",
      "3083 Train Loss 28.313808\n",
      "3084 Train Loss 28.31231\n",
      "3085 Train Loss 28.310932\n",
      "3086 Train Loss 28.30312\n",
      "3087 Train Loss 28.56991\n",
      "3088 Train Loss 28.313808\n",
      "3089 Train Loss 28.31231\n",
      "3090 Train Loss 28.310932\n",
      "3091 Train Loss 28.30312\n",
      "3092 Train Loss 28.56991\n",
      "3093 Train Loss 28.313808\n",
      "3094 Train Loss 28.31231\n",
      "3095 Train Loss 28.310932\n",
      "3096 Train Loss 28.30312\n",
      "3097 Train Loss 28.56991\n",
      "3098 Train Loss 28.313808\n",
      "3099 Train Loss 28.31231\n",
      "3100 Train Loss 28.310932\n",
      "3101 Train Loss 28.30312\n",
      "3102 Train Loss 28.56991\n",
      "3103 Train Loss 28.313808\n",
      "3104 Train Loss 28.31231\n",
      "3105 Train Loss 28.310932\n",
      "3106 Train Loss 28.30312\n",
      "3107 Train Loss 28.56991\n",
      "3108 Train Loss 28.313808\n",
      "3109 Train Loss 28.31231\n",
      "3110 Train Loss 28.310932\n",
      "3111 Train Loss 28.30312\n",
      "3112 Train Loss 28.56991\n",
      "3113 Train Loss 28.313808\n",
      "3114 Train Loss 28.31231\n",
      "3115 Train Loss 28.310932\n",
      "3116 Train Loss 28.30312\n",
      "3117 Train Loss 28.56991\n",
      "3118 Train Loss 28.313808\n",
      "3119 Train Loss 28.31231\n",
      "3120 Train Loss 28.310932\n",
      "3121 Train Loss 28.30312\n",
      "3122 Train Loss 28.56991\n",
      "3123 Train Loss 28.313808\n",
      "3124 Train Loss 28.31231\n",
      "3125 Train Loss 28.310932\n",
      "3126 Train Loss 28.30312\n",
      "3127 Train Loss 28.56991\n",
      "3128 Train Loss 28.313808\n",
      "3129 Train Loss 28.31231\n",
      "3130 Train Loss 28.310932\n",
      "3131 Train Loss 28.30312\n",
      "3132 Train Loss 28.56991\n",
      "3133 Train Loss 28.313808\n",
      "3134 Train Loss 28.31231\n",
      "3135 Train Loss 28.310932\n",
      "3136 Train Loss 28.30312\n",
      "3137 Train Loss 28.56991\n",
      "3138 Train Loss 28.313808\n",
      "3139 Train Loss 28.31231\n",
      "3140 Train Loss 28.310932\n",
      "3141 Train Loss 28.30312\n",
      "3142 Train Loss 28.56991\n",
      "3143 Train Loss 28.313808\n",
      "3144 Train Loss 28.31231\n",
      "3145 Train Loss 28.310932\n",
      "3146 Train Loss 28.30312\n",
      "3147 Train Loss 28.56991\n",
      "3148 Train Loss 28.313808\n",
      "3149 Train Loss 28.31231\n",
      "3150 Train Loss 28.310932\n",
      "3151 Train Loss 28.30312\n",
      "3152 Train Loss 28.56991\n",
      "3153 Train Loss 28.313808\n",
      "3154 Train Loss 28.31231\n",
      "3155 Train Loss 28.310932\n",
      "3156 Train Loss 28.30312\n",
      "3157 Train Loss 28.56991\n",
      "3158 Train Loss 28.313808\n",
      "3159 Train Loss 28.31231\n",
      "3160 Train Loss 28.310932\n",
      "3161 Train Loss 28.30312\n",
      "3162 Train Loss 28.56991\n",
      "3163 Train Loss 28.313808\n",
      "3164 Train Loss 28.31231\n",
      "3165 Train Loss 28.310932\n",
      "3166 Train Loss 28.30312\n",
      "3167 Train Loss 28.56991\n",
      "3168 Train Loss 28.313808\n",
      "3169 Train Loss 28.31231\n",
      "3170 Train Loss 28.310932\n",
      "3171 Train Loss 28.30312\n",
      "3172 Train Loss 28.56991\n",
      "3173 Train Loss 28.313808\n",
      "3174 Train Loss 28.31231\n",
      "3175 Train Loss 28.310932\n",
      "3176 Train Loss 28.30312\n",
      "3177 Train Loss 28.56991\n",
      "3178 Train Loss 28.313808\n",
      "3179 Train Loss 28.31231\n",
      "3180 Train Loss 28.310932\n",
      "3181 Train Loss 28.30312\n",
      "3182 Train Loss 28.56991\n",
      "3183 Train Loss 28.313808\n",
      "3184 Train Loss 28.31231\n",
      "3185 Train Loss 28.310932\n",
      "3186 Train Loss 28.30312\n",
      "3187 Train Loss 28.56991\n",
      "3188 Train Loss 28.313808\n",
      "3189 Train Loss 28.31231\n",
      "3190 Train Loss 28.310932\n",
      "3191 Train Loss 28.30312\n",
      "3192 Train Loss 28.56991\n",
      "3193 Train Loss 28.313808\n",
      "3194 Train Loss 28.31231\n",
      "3195 Train Loss 28.310932\n",
      "3196 Train Loss 28.30312\n",
      "3197 Train Loss 28.56991\n",
      "3198 Train Loss 28.313808\n",
      "3199 Train Loss 28.31231\n",
      "3200 Train Loss 28.310932\n",
      "3201 Train Loss 28.30312\n",
      "3202 Train Loss 28.56991\n",
      "3203 Train Loss 28.313808\n",
      "3204 Train Loss 28.31231\n",
      "3205 Train Loss 28.310932\n",
      "3206 Train Loss 28.30312\n",
      "3207 Train Loss 28.56991\n",
      "3208 Train Loss 28.313808\n",
      "3209 Train Loss 28.31231\n",
      "3210 Train Loss 28.310932\n",
      "3211 Train Loss 28.30312\n",
      "3212 Train Loss 28.56991\n",
      "3213 Train Loss 28.313808\n",
      "3214 Train Loss 28.31231\n",
      "3215 Train Loss 28.310932\n",
      "3216 Train Loss 28.30312\n",
      "3217 Train Loss 28.56991\n",
      "3218 Train Loss 28.313808\n",
      "3219 Train Loss 28.31231\n",
      "3220 Train Loss 28.310932\n",
      "3221 Train Loss 28.30312\n",
      "3222 Train Loss 28.56991\n",
      "3223 Train Loss 28.313808\n",
      "3224 Train Loss 28.31231\n",
      "3225 Train Loss 28.310932\n",
      "3226 Train Loss 28.30312\n",
      "3227 Train Loss 28.56991\n",
      "3228 Train Loss 28.313808\n",
      "3229 Train Loss 28.31231\n",
      "3230 Train Loss 28.310932\n",
      "3231 Train Loss 28.30312\n",
      "3232 Train Loss 28.56991\n",
      "3233 Train Loss 28.313808\n",
      "3234 Train Loss 28.31231\n",
      "3235 Train Loss 28.310932\n",
      "3236 Train Loss 28.30312\n",
      "3237 Train Loss 28.56991\n",
      "3238 Train Loss 28.313808\n",
      "3239 Train Loss 28.31231\n",
      "3240 Train Loss 28.310932\n",
      "3241 Train Loss 28.30312\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3242 Train Loss 28.56991\n",
      "3243 Train Loss 28.313808\n",
      "3244 Train Loss 28.31231\n",
      "3245 Train Loss 28.310932\n",
      "3246 Train Loss 28.30312\n",
      "3247 Train Loss 28.56991\n",
      "3248 Train Loss 28.313808\n",
      "3249 Train Loss 28.31231\n",
      "3250 Train Loss 28.310932\n",
      "3251 Train Loss 28.30312\n",
      "3252 Train Loss 28.56991\n",
      "3253 Train Loss 28.313808\n",
      "3254 Train Loss 28.31231\n",
      "3255 Train Loss 28.310932\n",
      "3256 Train Loss 28.30312\n",
      "3257 Train Loss 28.56991\n",
      "3258 Train Loss 28.313808\n",
      "3259 Train Loss 28.31231\n",
      "3260 Train Loss 28.310932\n",
      "3261 Train Loss 28.30312\n",
      "3262 Train Loss 28.56991\n",
      "3263 Train Loss 28.313808\n",
      "3264 Train Loss 28.31231\n",
      "3265 Train Loss 28.310932\n",
      "3266 Train Loss 28.30312\n",
      "3267 Train Loss 28.56991\n",
      "3268 Train Loss 28.313808\n",
      "3269 Train Loss 28.31231\n",
      "3270 Train Loss 28.310932\n",
      "3271 Train Loss 28.30312\n",
      "3272 Train Loss 28.56991\n",
      "3273 Train Loss 28.313808\n",
      "3274 Train Loss 28.31231\n",
      "3275 Train Loss 28.310932\n",
      "3276 Train Loss 28.30312\n",
      "3277 Train Loss 28.56991\n",
      "3278 Train Loss 28.313808\n",
      "3279 Train Loss 28.31231\n",
      "3280 Train Loss 28.310932\n",
      "3281 Train Loss 28.30312\n",
      "3282 Train Loss 28.56991\n",
      "3283 Train Loss 28.313808\n",
      "3284 Train Loss 28.31231\n",
      "3285 Train Loss 28.310932\n",
      "3286 Train Loss 28.30312\n",
      "3287 Train Loss 28.56991\n",
      "3288 Train Loss 28.313808\n",
      "3289 Train Loss 28.31231\n",
      "3290 Train Loss 28.310932\n",
      "3291 Train Loss 28.30312\n",
      "3292 Train Loss 28.56991\n",
      "3293 Train Loss 28.313808\n",
      "3294 Train Loss 28.31231\n",
      "3295 Train Loss 28.310932\n",
      "3296 Train Loss 28.30312\n",
      "3297 Train Loss 28.56991\n",
      "3298 Train Loss 28.313808\n",
      "3299 Train Loss 28.31231\n",
      "3300 Train Loss 28.310932\n",
      "3301 Train Loss 28.30312\n",
      "3302 Train Loss 28.56991\n",
      "3303 Train Loss 28.313808\n",
      "3304 Train Loss 28.31231\n",
      "3305 Train Loss 28.310932\n",
      "3306 Train Loss 28.30312\n",
      "3307 Train Loss 28.56991\n",
      "3308 Train Loss 28.313808\n",
      "3309 Train Loss 28.31231\n",
      "3310 Train Loss 28.310932\n",
      "3311 Train Loss 28.30312\n",
      "3312 Train Loss 28.56991\n",
      "3313 Train Loss 28.313808\n",
      "3314 Train Loss 28.31231\n",
      "3315 Train Loss 28.310932\n",
      "3316 Train Loss 28.30312\n",
      "3317 Train Loss 28.56991\n",
      "3318 Train Loss 28.313808\n",
      "3319 Train Loss 28.31231\n",
      "3320 Train Loss 28.310932\n",
      "3321 Train Loss 28.30312\n",
      "3322 Train Loss 28.56991\n",
      "3323 Train Loss 28.313808\n",
      "3324 Train Loss 28.31231\n",
      "3325 Train Loss 28.310932\n",
      "3326 Train Loss 28.30312\n",
      "3327 Train Loss 28.56991\n",
      "3328 Train Loss 28.313808\n",
      "3329 Train Loss 28.31231\n",
      "3330 Train Loss 28.310932\n",
      "3331 Train Loss 28.30312\n",
      "3332 Train Loss 28.56991\n",
      "3333 Train Loss 28.313808\n",
      "3334 Train Loss 28.31231\n",
      "3335 Train Loss 28.310932\n",
      "3336 Train Loss 28.30312\n",
      "3337 Train Loss 28.56991\n",
      "3338 Train Loss 28.313808\n",
      "3339 Train Loss 28.31231\n",
      "3340 Train Loss 28.310932\n",
      "3341 Train Loss 28.30312\n",
      "3342 Train Loss 28.56991\n",
      "3343 Train Loss 28.313808\n",
      "3344 Train Loss 28.31231\n",
      "3345 Train Loss 28.310932\n",
      "3346 Train Loss 28.30312\n",
      "3347 Train Loss 28.56991\n",
      "3348 Train Loss 28.313808\n",
      "3349 Train Loss 28.31231\n",
      "3350 Train Loss 28.310932\n",
      "3351 Train Loss 28.30312\n",
      "3352 Train Loss 28.56991\n",
      "3353 Train Loss 28.313808\n",
      "3354 Train Loss 28.31231\n",
      "3355 Train Loss 28.310932\n",
      "3356 Train Loss 28.30312\n",
      "3357 Train Loss 28.56991\n",
      "3358 Train Loss 28.313808\n",
      "3359 Train Loss 28.31231\n",
      "3360 Train Loss 28.310932\n",
      "3361 Train Loss 28.30312\n",
      "3362 Train Loss 28.56991\n",
      "3363 Train Loss 28.313808\n",
      "3364 Train Loss 28.31231\n",
      "3365 Train Loss 28.310932\n",
      "3366 Train Loss 28.30312\n",
      "3367 Train Loss 28.56991\n",
      "3368 Train Loss 28.313808\n",
      "3369 Train Loss 28.31231\n",
      "3370 Train Loss 28.310932\n",
      "3371 Train Loss 28.30312\n",
      "3372 Train Loss 28.56991\n",
      "3373 Train Loss 28.313808\n",
      "3374 Train Loss 28.31231\n",
      "3375 Train Loss 28.310932\n",
      "3376 Train Loss 28.30312\n",
      "3377 Train Loss 28.56991\n",
      "3378 Train Loss 28.313808\n",
      "3379 Train Loss 28.31231\n",
      "3380 Train Loss 28.310932\n",
      "3381 Train Loss 28.30312\n",
      "3382 Train Loss 28.56991\n",
      "3383 Train Loss 28.313808\n",
      "3384 Train Loss 28.31231\n",
      "3385 Train Loss 28.310932\n",
      "3386 Train Loss 28.30312\n",
      "3387 Train Loss 28.56991\n",
      "3388 Train Loss 28.313808\n",
      "3389 Train Loss 28.31231\n",
      "3390 Train Loss 28.310932\n",
      "3391 Train Loss 28.30312\n",
      "3392 Train Loss 28.56991\n",
      "3393 Train Loss 28.313808\n",
      "3394 Train Loss 28.31231\n",
      "3395 Train Loss 28.310932\n",
      "3396 Train Loss 28.30312\n",
      "3397 Train Loss 28.56991\n",
      "3398 Train Loss 28.313808\n",
      "3399 Train Loss 28.31231\n",
      "3400 Train Loss 28.310932\n",
      "3401 Train Loss 28.30312\n",
      "3402 Train Loss 28.56991\n",
      "3403 Train Loss 28.313808\n",
      "3404 Train Loss 28.31231\n",
      "3405 Train Loss 28.310932\n",
      "3406 Train Loss 28.30312\n",
      "3407 Train Loss 28.56991\n",
      "3408 Train Loss 28.313808\n",
      "3409 Train Loss 28.31231\n",
      "3410 Train Loss 28.310932\n",
      "3411 Train Loss 28.30312\n",
      "3412 Train Loss 28.56991\n",
      "3413 Train Loss 28.313808\n",
      "3414 Train Loss 28.31231\n",
      "3415 Train Loss 28.310932\n",
      "3416 Train Loss 28.30312\n",
      "3417 Train Loss 28.56991\n",
      "3418 Train Loss 28.313808\n",
      "3419 Train Loss 28.31231\n",
      "3420 Train Loss 28.310932\n",
      "3421 Train Loss 28.30312\n",
      "3422 Train Loss 28.56991\n",
      "3423 Train Loss 28.313808\n",
      "3424 Train Loss 28.31231\n",
      "3425 Train Loss 28.310932\n",
      "3426 Train Loss 28.30312\n",
      "3427 Train Loss 28.56991\n",
      "3428 Train Loss 28.313808\n",
      "3429 Train Loss 28.31231\n",
      "3430 Train Loss 28.310932\n",
      "3431 Train Loss 28.30312\n",
      "3432 Train Loss 28.56991\n",
      "3433 Train Loss 28.313808\n",
      "3434 Train Loss 28.31231\n",
      "3435 Train Loss 28.310932\n",
      "3436 Train Loss 28.30312\n",
      "3437 Train Loss 28.56991\n",
      "3438 Train Loss 28.313808\n",
      "3439 Train Loss 28.31231\n",
      "3440 Train Loss 28.310932\n",
      "3441 Train Loss 28.30312\n",
      "3442 Train Loss 28.56991\n",
      "3443 Train Loss 28.313808\n",
      "3444 Train Loss 28.31231\n",
      "3445 Train Loss 28.310932\n",
      "3446 Train Loss 28.30312\n",
      "3447 Train Loss 28.56991\n",
      "3448 Train Loss 28.313808\n",
      "3449 Train Loss 28.31231\n",
      "3450 Train Loss 28.310932\n",
      "3451 Train Loss 28.30312\n",
      "3452 Train Loss 28.56991\n",
      "3453 Train Loss 28.313808\n",
      "3454 Train Loss 28.31231\n",
      "3455 Train Loss 28.310932\n",
      "3456 Train Loss 28.30312\n",
      "3457 Train Loss 28.56991\n",
      "3458 Train Loss 28.313808\n",
      "3459 Train Loss 28.31231\n",
      "3460 Train Loss 28.310932\n",
      "3461 Train Loss 28.30312\n",
      "3462 Train Loss 28.56991\n",
      "3463 Train Loss 28.313808\n",
      "3464 Train Loss 28.31231\n",
      "3465 Train Loss 28.310932\n",
      "3466 Train Loss 28.30312\n",
      "3467 Train Loss 28.56991\n",
      "3468 Train Loss 28.313808\n",
      "3469 Train Loss 28.31231\n",
      "3470 Train Loss 28.310932\n",
      "3471 Train Loss 28.30312\n",
      "3472 Train Loss 28.56991\n",
      "3473 Train Loss 28.313808\n",
      "3474 Train Loss 28.31231\n",
      "3475 Train Loss 28.310932\n",
      "3476 Train Loss 28.30312\n",
      "3477 Train Loss 28.56991\n",
      "3478 Train Loss 28.313808\n",
      "3479 Train Loss 28.31231\n",
      "3480 Train Loss 28.310932\n",
      "3481 Train Loss 28.30312\n",
      "3482 Train Loss 28.56991\n",
      "3483 Train Loss 28.313808\n",
      "3484 Train Loss 28.31231\n",
      "3485 Train Loss 28.310932\n",
      "3486 Train Loss 28.30312\n",
      "3487 Train Loss 28.56991\n",
      "3488 Train Loss 28.313808\n",
      "3489 Train Loss 28.31231\n",
      "3490 Train Loss 28.310932\n",
      "3491 Train Loss 28.30312\n",
      "3492 Train Loss 28.56991\n",
      "3493 Train Loss 28.313808\n",
      "3494 Train Loss 28.31231\n",
      "3495 Train Loss 28.310932\n",
      "3496 Train Loss 28.30312\n",
      "3497 Train Loss 28.56991\n",
      "3498 Train Loss 28.313808\n",
      "3499 Train Loss 28.31231\n",
      "3500 Train Loss 28.310932\n",
      "3501 Train Loss 28.30312\n",
      "3502 Train Loss 28.56991\n",
      "3503 Train Loss 28.313808\n",
      "3504 Train Loss 28.31231\n",
      "3505 Train Loss 28.310932\n",
      "3506 Train Loss 28.30312\n",
      "3507 Train Loss 28.56991\n",
      "3508 Train Loss 28.313808\n",
      "3509 Train Loss 28.31231\n",
      "3510 Train Loss 28.310932\n",
      "3511 Train Loss 28.30312\n",
      "3512 Train Loss 28.56991\n",
      "3513 Train Loss 28.313808\n",
      "3514 Train Loss 28.31231\n",
      "3515 Train Loss 28.310932\n",
      "3516 Train Loss 28.30312\n",
      "3517 Train Loss 28.56991\n",
      "3518 Train Loss 28.313808\n",
      "3519 Train Loss 28.31231\n",
      "3520 Train Loss 28.310932\n",
      "3521 Train Loss 28.30312\n",
      "3522 Train Loss 28.56991\n",
      "3523 Train Loss 28.313808\n",
      "3524 Train Loss 28.31231\n",
      "3525 Train Loss 28.310932\n",
      "3526 Train Loss 28.30312\n",
      "3527 Train Loss 28.56991\n",
      "3528 Train Loss 28.313808\n",
      "3529 Train Loss 28.31231\n",
      "3530 Train Loss 28.310932\n",
      "3531 Train Loss 28.30312\n",
      "3532 Train Loss 28.56991\n",
      "3533 Train Loss 28.313808\n",
      "3534 Train Loss 28.31231\n",
      "3535 Train Loss 28.310932\n",
      "3536 Train Loss 28.30312\n",
      "3537 Train Loss 28.56991\n",
      "3538 Train Loss 28.313808\n",
      "3539 Train Loss 28.31231\n",
      "3540 Train Loss 28.310932\n",
      "3541 Train Loss 28.30312\n",
      "3542 Train Loss 28.56991\n",
      "3543 Train Loss 28.313808\n",
      "3544 Train Loss 28.31231\n",
      "3545 Train Loss 28.310932\n",
      "3546 Train Loss 28.30312\n",
      "3547 Train Loss 28.56991\n",
      "3548 Train Loss 28.313808\n",
      "3549 Train Loss 28.31231\n",
      "3550 Train Loss 28.310932\n",
      "3551 Train Loss 28.30312\n",
      "3552 Train Loss 28.56991\n",
      "3553 Train Loss 28.313808\n",
      "3554 Train Loss 28.31231\n",
      "3555 Train Loss 28.310932\n",
      "3556 Train Loss 28.30312\n",
      "3557 Train Loss 28.56991\n",
      "3558 Train Loss 28.313808\n",
      "3559 Train Loss 28.31231\n",
      "3560 Train Loss 28.310932\n",
      "3561 Train Loss 28.30312\n",
      "3562 Train Loss 28.56991\n",
      "3563 Train Loss 28.313808\n",
      "3564 Train Loss 28.31231\n",
      "3565 Train Loss 28.310932\n",
      "3566 Train Loss 28.30312\n",
      "3567 Train Loss 28.56991\n",
      "3568 Train Loss 28.313808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3569 Train Loss 28.31231\n",
      "3570 Train Loss 28.310932\n",
      "3571 Train Loss 28.30312\n",
      "3572 Train Loss 28.56991\n",
      "3573 Train Loss 28.313808\n",
      "3574 Train Loss 28.31231\n",
      "3575 Train Loss 28.310932\n",
      "3576 Train Loss 28.30312\n",
      "3577 Train Loss 28.56991\n",
      "3578 Train Loss 28.313808\n",
      "3579 Train Loss 28.31231\n",
      "3580 Train Loss 28.310932\n",
      "3581 Train Loss 28.30312\n",
      "3582 Train Loss 28.56991\n",
      "3583 Train Loss 28.313808\n",
      "3584 Train Loss 28.31231\n",
      "3585 Train Loss 28.310932\n",
      "3586 Train Loss 28.30312\n",
      "3587 Train Loss 28.56991\n",
      "3588 Train Loss 28.313808\n",
      "3589 Train Loss 28.31231\n",
      "3590 Train Loss 28.310932\n",
      "3591 Train Loss 28.30312\n",
      "3592 Train Loss 28.56991\n",
      "3593 Train Loss 28.313808\n",
      "3594 Train Loss 28.31231\n",
      "3595 Train Loss 28.310932\n",
      "3596 Train Loss 28.30312\n",
      "3597 Train Loss 28.56991\n",
      "3598 Train Loss 28.313808\n",
      "3599 Train Loss 28.31231\n",
      "3600 Train Loss 28.310932\n",
      "3601 Train Loss 28.30312\n",
      "3602 Train Loss 28.56991\n",
      "3603 Train Loss 28.313808\n",
      "3604 Train Loss 28.31231\n",
      "3605 Train Loss 28.310932\n",
      "3606 Train Loss 28.30312\n",
      "3607 Train Loss 28.56991\n",
      "3608 Train Loss 28.313808\n",
      "3609 Train Loss 28.31231\n",
      "3610 Train Loss 28.310932\n",
      "3611 Train Loss 28.30312\n",
      "3612 Train Loss 28.56991\n",
      "3613 Train Loss 28.313808\n",
      "3614 Train Loss 28.31231\n",
      "3615 Train Loss 28.310932\n",
      "3616 Train Loss 28.30312\n",
      "3617 Train Loss 28.56991\n",
      "3618 Train Loss 28.313808\n",
      "3619 Train Loss 28.31231\n",
      "3620 Train Loss 28.310932\n",
      "3621 Train Loss 28.30312\n",
      "3622 Train Loss 28.56991\n",
      "3623 Train Loss 28.313808\n",
      "3624 Train Loss 28.31231\n",
      "3625 Train Loss 28.310932\n",
      "3626 Train Loss 28.30312\n",
      "3627 Train Loss 28.56991\n",
      "3628 Train Loss 28.313808\n",
      "3629 Train Loss 28.31231\n",
      "3630 Train Loss 28.310932\n",
      "3631 Train Loss 28.30312\n",
      "3632 Train Loss 28.56991\n",
      "3633 Train Loss 28.313808\n",
      "3634 Train Loss 28.31231\n",
      "3635 Train Loss 28.310932\n",
      "3636 Train Loss 28.30312\n",
      "3637 Train Loss 28.56991\n",
      "3638 Train Loss 28.313808\n",
      "3639 Train Loss 28.31231\n",
      "3640 Train Loss 28.310932\n",
      "3641 Train Loss 28.30312\n",
      "3642 Train Loss 28.56991\n",
      "3643 Train Loss 28.313808\n",
      "3644 Train Loss 28.31231\n",
      "3645 Train Loss 28.310932\n",
      "3646 Train Loss 28.30312\n",
      "3647 Train Loss 28.56991\n",
      "3648 Train Loss 28.313808\n",
      "3649 Train Loss 28.31231\n",
      "3650 Train Loss 28.310932\n",
      "3651 Train Loss 28.30312\n",
      "3652 Train Loss 28.56991\n",
      "3653 Train Loss 28.313808\n",
      "3654 Train Loss 28.31231\n",
      "3655 Train Loss 28.310932\n",
      "3656 Train Loss 28.30312\n",
      "3657 Train Loss 28.56991\n",
      "3658 Train Loss 28.313808\n",
      "3659 Train Loss 28.31231\n",
      "3660 Train Loss 28.310932\n",
      "3661 Train Loss 28.30312\n",
      "3662 Train Loss 28.56991\n",
      "3663 Train Loss 28.313808\n",
      "3664 Train Loss 28.31231\n",
      "3665 Train Loss 28.310932\n",
      "3666 Train Loss 28.30312\n",
      "3667 Train Loss 28.56991\n",
      "3668 Train Loss 28.313808\n",
      "3669 Train Loss 28.31231\n",
      "3670 Train Loss 28.310932\n",
      "3671 Train Loss 28.30312\n",
      "3672 Train Loss 28.56991\n",
      "3673 Train Loss 28.313808\n",
      "3674 Train Loss 28.31231\n",
      "3675 Train Loss 28.310932\n",
      "3676 Train Loss 28.30312\n",
      "3677 Train Loss 28.56991\n",
      "3678 Train Loss 28.313808\n",
      "3679 Train Loss 28.31231\n",
      "3680 Train Loss 28.310932\n",
      "3681 Train Loss 28.30312\n",
      "3682 Train Loss 28.56991\n",
      "3683 Train Loss 28.313808\n",
      "3684 Train Loss 28.31231\n",
      "3685 Train Loss 28.310932\n",
      "3686 Train Loss 28.30312\n",
      "3687 Train Loss 28.56991\n",
      "3688 Train Loss 28.313808\n",
      "3689 Train Loss 28.31231\n",
      "3690 Train Loss 28.310932\n",
      "3691 Train Loss 28.30312\n",
      "3692 Train Loss 28.56991\n",
      "3693 Train Loss 28.313808\n",
      "3694 Train Loss 28.31231\n",
      "3695 Train Loss 28.310932\n",
      "3696 Train Loss 28.30312\n",
      "3697 Train Loss 28.56991\n",
      "3698 Train Loss 28.313808\n",
      "3699 Train Loss 28.31231\n",
      "3700 Train Loss 28.310932\n",
      "3701 Train Loss 28.30312\n",
      "3702 Train Loss 28.56991\n",
      "3703 Train Loss 28.313808\n",
      "3704 Train Loss 28.31231\n",
      "3705 Train Loss 28.310932\n",
      "3706 Train Loss 28.30312\n",
      "3707 Train Loss 28.56991\n",
      "3708 Train Loss 28.313808\n",
      "3709 Train Loss 28.31231\n",
      "3710 Train Loss 28.310932\n",
      "3711 Train Loss 28.30312\n",
      "3712 Train Loss 28.56991\n",
      "3713 Train Loss 28.313808\n",
      "3714 Train Loss 28.31231\n",
      "3715 Train Loss 28.310932\n",
      "3716 Train Loss 28.30312\n",
      "3717 Train Loss 28.56991\n",
      "3718 Train Loss 28.313808\n",
      "3719 Train Loss 28.31231\n",
      "3720 Train Loss 28.310932\n",
      "3721 Train Loss 28.30312\n",
      "3722 Train Loss 28.56991\n",
      "3723 Train Loss 28.313808\n",
      "3724 Train Loss 28.31231\n",
      "3725 Train Loss 28.310932\n",
      "3726 Train Loss 28.30312\n",
      "3727 Train Loss 28.56991\n",
      "3728 Train Loss 28.313808\n",
      "3729 Train Loss 28.31231\n",
      "3730 Train Loss 28.310932\n",
      "3731 Train Loss 28.30312\n",
      "3732 Train Loss 28.56991\n",
      "3733 Train Loss 28.313808\n",
      "3734 Train Loss 28.31231\n",
      "3735 Train Loss 28.310932\n",
      "3736 Train Loss 28.30312\n",
      "3737 Train Loss 28.56991\n",
      "3738 Train Loss 28.313808\n",
      "3739 Train Loss 28.31231\n",
      "3740 Train Loss 28.310932\n",
      "3741 Train Loss 28.30312\n",
      "3742 Train Loss 28.56991\n",
      "3743 Train Loss 28.313808\n",
      "3744 Train Loss 28.31231\n",
      "3745 Train Loss 28.310932\n",
      "3746 Train Loss 28.30312\n",
      "3747 Train Loss 28.56991\n",
      "3748 Train Loss 28.313808\n",
      "3749 Train Loss 28.31231\n",
      "3750 Train Loss 28.310932\n",
      "3751 Train Loss 28.30312\n",
      "3752 Train Loss 28.56991\n",
      "3753 Train Loss 28.313808\n",
      "3754 Train Loss 28.31231\n",
      "3755 Train Loss 28.310932\n",
      "3756 Train Loss 28.30312\n",
      "3757 Train Loss 28.56991\n",
      "3758 Train Loss 28.313808\n",
      "3759 Train Loss 28.31231\n",
      "3760 Train Loss 28.310932\n",
      "3761 Train Loss 28.30312\n",
      "3762 Train Loss 28.56991\n",
      "3763 Train Loss 28.313808\n",
      "3764 Train Loss 28.31231\n",
      "3765 Train Loss 28.310932\n",
      "3766 Train Loss 28.30312\n",
      "3767 Train Loss 28.56991\n",
      "3768 Train Loss 28.313808\n",
      "3769 Train Loss 28.31231\n",
      "3770 Train Loss 28.310932\n",
      "3771 Train Loss 28.30312\n",
      "3772 Train Loss 28.56991\n",
      "3773 Train Loss 28.313808\n",
      "3774 Train Loss 28.31231\n",
      "3775 Train Loss 28.310932\n",
      "3776 Train Loss 28.30312\n",
      "3777 Train Loss 28.56991\n",
      "3778 Train Loss 28.313808\n",
      "3779 Train Loss 28.31231\n",
      "3780 Train Loss 28.310932\n",
      "3781 Train Loss 28.30312\n",
      "3782 Train Loss 28.56991\n",
      "3783 Train Loss 28.313808\n",
      "3784 Train Loss 28.31231\n",
      "3785 Train Loss 28.310932\n",
      "3786 Train Loss 28.30312\n",
      "3787 Train Loss 28.56991\n",
      "3788 Train Loss 28.313808\n",
      "3789 Train Loss 28.31231\n",
      "3790 Train Loss 28.310932\n",
      "3791 Train Loss 28.30312\n",
      "3792 Train Loss 28.56991\n",
      "3793 Train Loss 28.313808\n",
      "3794 Train Loss 28.31231\n",
      "3795 Train Loss 28.310932\n",
      "3796 Train Loss 28.30312\n",
      "3797 Train Loss 28.56991\n",
      "3798 Train Loss 28.313808\n",
      "3799 Train Loss 28.31231\n",
      "3800 Train Loss 28.310932\n",
      "3801 Train Loss 28.30312\n",
      "3802 Train Loss 28.56991\n",
      "3803 Train Loss 28.313808\n",
      "3804 Train Loss 28.31231\n",
      "3805 Train Loss 28.310932\n",
      "3806 Train Loss 28.30312\n",
      "3807 Train Loss 28.56991\n",
      "3808 Train Loss 28.313808\n",
      "3809 Train Loss 28.31231\n",
      "3810 Train Loss 28.310932\n",
      "3811 Train Loss 28.30312\n",
      "3812 Train Loss 28.56991\n",
      "3813 Train Loss 28.313808\n",
      "3814 Train Loss 28.31231\n",
      "3815 Train Loss 28.310932\n",
      "3816 Train Loss 28.30312\n",
      "3817 Train Loss 28.56991\n",
      "3818 Train Loss 28.313808\n",
      "3819 Train Loss 28.31231\n",
      "3820 Train Loss 28.310932\n",
      "3821 Train Loss 28.30312\n",
      "3822 Train Loss 28.56991\n",
      "3823 Train Loss 28.313808\n",
      "3824 Train Loss 28.31231\n",
      "3825 Train Loss 28.310932\n",
      "3826 Train Loss 28.30312\n",
      "3827 Train Loss 28.56991\n",
      "3828 Train Loss 28.313808\n",
      "3829 Train Loss 28.31231\n",
      "3830 Train Loss 28.310932\n",
      "3831 Train Loss 28.30312\n",
      "3832 Train Loss 28.56991\n",
      "3833 Train Loss 28.313808\n",
      "3834 Train Loss 28.31231\n",
      "3835 Train Loss 28.310932\n",
      "3836 Train Loss 28.30312\n",
      "3837 Train Loss 28.56991\n",
      "3838 Train Loss 28.313808\n",
      "3839 Train Loss 28.31231\n",
      "3840 Train Loss 28.310932\n",
      "3841 Train Loss 28.30312\n",
      "3842 Train Loss 28.56991\n",
      "3843 Train Loss 28.313808\n",
      "3844 Train Loss 28.31231\n",
      "3845 Train Loss 28.310932\n",
      "3846 Train Loss 28.30312\n",
      "3847 Train Loss 28.56991\n",
      "3848 Train Loss 28.313808\n",
      "3849 Train Loss 28.31231\n",
      "3850 Train Loss 28.310932\n",
      "3851 Train Loss 28.30312\n",
      "3852 Train Loss 28.56991\n",
      "3853 Train Loss 28.313808\n",
      "3854 Train Loss 28.31231\n",
      "3855 Train Loss 28.310932\n",
      "3856 Train Loss 28.30312\n",
      "3857 Train Loss 28.56991\n",
      "3858 Train Loss 28.313808\n",
      "3859 Train Loss 28.31231\n",
      "3860 Train Loss 28.310932\n",
      "3861 Train Loss 28.30312\n",
      "3862 Train Loss 28.56991\n",
      "3863 Train Loss 28.313808\n",
      "3864 Train Loss 28.31231\n",
      "3865 Train Loss 28.310932\n",
      "3866 Train Loss 28.30312\n",
      "3867 Train Loss 28.56991\n",
      "3868 Train Loss 28.313808\n",
      "3869 Train Loss 28.31231\n",
      "3870 Train Loss 28.310932\n",
      "3871 Train Loss 28.30312\n",
      "3872 Train Loss 28.56991\n",
      "3873 Train Loss 28.313808\n",
      "3874 Train Loss 28.31231\n",
      "3875 Train Loss 28.310932\n",
      "3876 Train Loss 28.30312\n",
      "3877 Train Loss 28.56991\n",
      "3878 Train Loss 28.313808\n",
      "3879 Train Loss 28.31231\n",
      "3880 Train Loss 28.310932\n",
      "3881 Train Loss 28.30312\n",
      "3882 Train Loss 28.56991\n",
      "3883 Train Loss 28.313808\n",
      "3884 Train Loss 28.31231\n",
      "3885 Train Loss 28.310932\n",
      "3886 Train Loss 28.30312\n",
      "3887 Train Loss 28.56991\n",
      "3888 Train Loss 28.313808\n",
      "3889 Train Loss 28.31231\n",
      "3890 Train Loss 28.310932\n",
      "3891 Train Loss 28.30312\n",
      "3892 Train Loss 28.56991\n",
      "3893 Train Loss 28.313808\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3894 Train Loss 28.31231\n",
      "3895 Train Loss 28.310932\n",
      "3896 Train Loss 28.30312\n",
      "3897 Train Loss 28.56991\n",
      "3898 Train Loss 28.313808\n",
      "3899 Train Loss 28.31231\n",
      "3900 Train Loss 28.310932\n",
      "3901 Train Loss 28.30312\n",
      "3902 Train Loss 28.56991\n",
      "3903 Train Loss 28.313808\n",
      "3904 Train Loss 28.31231\n",
      "3905 Train Loss 28.310932\n",
      "3906 Train Loss 28.30312\n",
      "3907 Train Loss 28.56991\n",
      "3908 Train Loss 28.313808\n",
      "3909 Train Loss 28.31231\n",
      "3910 Train Loss 28.310932\n",
      "3911 Train Loss 28.30312\n",
      "3912 Train Loss 28.56991\n",
      "3913 Train Loss 28.313808\n",
      "3914 Train Loss 28.31231\n",
      "3915 Train Loss 28.310932\n",
      "3916 Train Loss 28.30312\n",
      "3917 Train Loss 28.56991\n",
      "3918 Train Loss 28.313808\n",
      "3919 Train Loss 28.31231\n",
      "3920 Train Loss 28.310932\n",
      "3921 Train Loss 28.30312\n",
      "3922 Train Loss 28.56991\n",
      "3923 Train Loss 28.313808\n",
      "3924 Train Loss 28.31231\n",
      "3925 Train Loss 28.310932\n",
      "3926 Train Loss 28.30312\n",
      "3927 Train Loss 28.56991\n",
      "3928 Train Loss 28.313808\n",
      "3929 Train Loss 28.31231\n",
      "3930 Train Loss 28.310932\n",
      "3931 Train Loss 28.30312\n",
      "3932 Train Loss 28.56991\n",
      "3933 Train Loss 28.313808\n",
      "3934 Train Loss 28.31231\n",
      "3935 Train Loss 28.310932\n",
      "3936 Train Loss 28.30312\n",
      "3937 Train Loss 28.56991\n",
      "3938 Train Loss 28.313808\n",
      "3939 Train Loss 28.31231\n",
      "3940 Train Loss 28.310932\n",
      "3941 Train Loss 28.30312\n",
      "3942 Train Loss 28.56991\n",
      "3943 Train Loss 28.313808\n",
      "3944 Train Loss 28.31231\n",
      "3945 Train Loss 28.310932\n",
      "3946 Train Loss 28.30312\n",
      "3947 Train Loss 28.56991\n",
      "3948 Train Loss 28.313808\n",
      "3949 Train Loss 28.31231\n",
      "3950 Train Loss 28.310932\n",
      "3951 Train Loss 28.30312\n",
      "3952 Train Loss 28.56991\n",
      "3953 Train Loss 28.313808\n",
      "3954 Train Loss 28.31231\n",
      "3955 Train Loss 28.310932\n",
      "3956 Train Loss 28.30312\n",
      "3957 Train Loss 28.56991\n",
      "3958 Train Loss 28.313808\n",
      "3959 Train Loss 28.31231\n",
      "3960 Train Loss 28.310932\n",
      "3961 Train Loss 28.30312\n",
      "3962 Train Loss 28.56991\n",
      "3963 Train Loss 28.313808\n",
      "3964 Train Loss 28.31231\n",
      "3965 Train Loss 28.310932\n",
      "3966 Train Loss 28.30312\n",
      "3967 Train Loss 28.56991\n",
      "3968 Train Loss 28.313808\n",
      "3969 Train Loss 28.31231\n",
      "3970 Train Loss 28.310932\n",
      "3971 Train Loss 28.30312\n",
      "3972 Train Loss 28.56991\n",
      "3973 Train Loss 28.313808\n",
      "3974 Train Loss 28.31231\n",
      "3975 Train Loss 28.310932\n",
      "3976 Train Loss 28.30312\n",
      "3977 Train Loss 28.56991\n",
      "3978 Train Loss 28.313808\n",
      "3979 Train Loss 28.31231\n",
      "3980 Train Loss 28.310932\n",
      "3981 Train Loss 28.30312\n",
      "3982 Train Loss 28.56991\n",
      "3983 Train Loss 28.313808\n",
      "3984 Train Loss 28.31231\n",
      "3985 Train Loss 28.310932\n",
      "3986 Train Loss 28.30312\n",
      "3987 Train Loss 28.56991\n",
      "3988 Train Loss 28.313808\n",
      "3989 Train Loss 28.31231\n",
      "3990 Train Loss 28.310932\n",
      "3991 Train Loss 28.30312\n",
      "3992 Train Loss 28.56991\n",
      "3993 Train Loss 28.313808\n",
      "3994 Train Loss 28.31231\n",
      "3995 Train Loss 28.310932\n",
      "3996 Train Loss 28.30312\n",
      "3997 Train Loss 28.56991\n",
      "3998 Train Loss 28.313808\n",
      "3999 Train Loss 28.31231\n",
      "4000 Train Loss 28.310932\n",
      "4001 Train Loss 28.30312\n",
      "4002 Train Loss 28.56991\n",
      "4003 Train Loss 28.313808\n",
      "4004 Train Loss 28.31231\n",
      "4005 Train Loss 28.310932\n",
      "4006 Train Loss 28.30312\n",
      "4007 Train Loss 28.56991\n",
      "4008 Train Loss 28.313808\n",
      "4009 Train Loss 28.31231\n",
      "4010 Train Loss 28.310932\n",
      "4011 Train Loss 28.30312\n",
      "4012 Train Loss 28.56991\n",
      "4013 Train Loss 28.313808\n",
      "4014 Train Loss 28.31231\n",
      "4015 Train Loss 28.310932\n",
      "4016 Train Loss 28.30312\n",
      "4017 Train Loss 28.56991\n",
      "4018 Train Loss 28.313808\n",
      "4019 Train Loss 28.31231\n",
      "4020 Train Loss 28.310932\n",
      "4021 Train Loss 28.30312\n",
      "4022 Train Loss 28.56991\n",
      "4023 Train Loss 28.313808\n",
      "4024 Train Loss 28.31231\n",
      "4025 Train Loss 28.310932\n",
      "4026 Train Loss 28.30312\n",
      "4027 Train Loss 28.56991\n",
      "4028 Train Loss 28.313808\n",
      "4029 Train Loss 28.31231\n",
      "4030 Train Loss 28.310932\n",
      "4031 Train Loss 28.30312\n",
      "4032 Train Loss 28.56991\n",
      "4033 Train Loss 28.313808\n",
      "4034 Train Loss 28.31231\n",
      "4035 Train Loss 28.310932\n",
      "4036 Train Loss 28.30312\n",
      "4037 Train Loss 28.56991\n",
      "4038 Train Loss 28.313808\n",
      "4039 Train Loss 28.31231\n",
      "4040 Train Loss 28.310932\n",
      "4041 Train Loss 28.30312\n",
      "4042 Train Loss 28.56991\n",
      "4043 Train Loss 28.313808\n",
      "4044 Train Loss 28.31231\n",
      "4045 Train Loss 28.310932\n",
      "4046 Train Loss 28.30312\n",
      "4047 Train Loss 28.56991\n",
      "4048 Train Loss 28.313808\n",
      "4049 Train Loss 28.31231\n",
      "4050 Train Loss 28.310932\n",
      "4051 Train Loss 28.30312\n",
      "4052 Train Loss 28.56991\n",
      "4053 Train Loss 28.313808\n",
      "4054 Train Loss 28.31231\n",
      "4055 Train Loss 28.310932\n",
      "4056 Train Loss 28.30312\n",
      "4057 Train Loss 28.56991\n",
      "4058 Train Loss 28.313808\n",
      "4059 Train Loss 28.31231\n",
      "4060 Train Loss 28.310932\n",
      "4061 Train Loss 28.30312\n",
      "4062 Train Loss 28.56991\n",
      "4063 Train Loss 28.313808\n",
      "4064 Train Loss 28.31231\n",
      "4065 Train Loss 28.310932\n",
      "4066 Train Loss 28.30312\n",
      "4067 Train Loss 28.56991\n",
      "4068 Train Loss 28.313808\n",
      "4069 Train Loss 28.31231\n",
      "4070 Train Loss 28.310932\n",
      "4071 Train Loss 28.30312\n",
      "4072 Train Loss 28.56991\n",
      "4073 Train Loss 28.313808\n",
      "4074 Train Loss 28.31231\n",
      "4075 Train Loss 28.310932\n",
      "4076 Train Loss 28.30312\n",
      "4077 Train Loss 28.56991\n",
      "4078 Train Loss 28.313808\n",
      "4079 Train Loss 28.31231\n",
      "4080 Train Loss 28.310932\n",
      "4081 Train Loss 28.30312\n",
      "4082 Train Loss 28.56991\n",
      "4083 Train Loss 28.313808\n",
      "4084 Train Loss 28.31231\n",
      "4085 Train Loss 28.310932\n",
      "4086 Train Loss 28.30312\n",
      "4087 Train Loss 28.56991\n",
      "4088 Train Loss 28.313808\n",
      "4089 Train Loss 28.31231\n",
      "4090 Train Loss 28.310932\n",
      "4091 Train Loss 28.30312\n",
      "4092 Train Loss 28.56991\n",
      "4093 Train Loss 28.313808\n",
      "4094 Train Loss 28.31231\n",
      "4095 Train Loss 28.310932\n",
      "4096 Train Loss 28.30312\n",
      "4097 Train Loss 28.56991\n",
      "4098 Train Loss 28.313808\n",
      "4099 Train Loss 28.31231\n",
      "4100 Train Loss 28.310932\n",
      "4101 Train Loss 28.30312\n",
      "4102 Train Loss 28.56991\n",
      "4103 Train Loss 28.313808\n",
      "4104 Train Loss 28.31231\n",
      "4105 Train Loss 28.310932\n",
      "4106 Train Loss 28.30312\n",
      "4107 Train Loss 28.56991\n",
      "4108 Train Loss 28.313808\n",
      "4109 Train Loss 28.31231\n",
      "4110 Train Loss 28.310932\n",
      "4111 Train Loss 28.30312\n",
      "4112 Train Loss 28.56991\n",
      "4113 Train Loss 28.313808\n",
      "4114 Train Loss 28.31231\n",
      "4115 Train Loss 28.310932\n",
      "4116 Train Loss 28.30312\n",
      "4117 Train Loss 28.56991\n",
      "4118 Train Loss 28.313808\n",
      "4119 Train Loss 28.31231\n",
      "4120 Train Loss 28.310932\n",
      "4121 Train Loss 28.30312\n",
      "4122 Train Loss 28.56991\n",
      "4123 Train Loss 28.313808\n",
      "4124 Train Loss 28.31231\n",
      "4125 Train Loss 28.310932\n",
      "4126 Train Loss 28.30312\n",
      "4127 Train Loss 28.56991\n",
      "4128 Train Loss 28.313808\n",
      "4129 Train Loss 28.31231\n",
      "4130 Train Loss 28.310932\n",
      "4131 Train Loss 28.30312\n",
      "4132 Train Loss 28.56991\n",
      "4133 Train Loss 28.313808\n",
      "4134 Train Loss 28.31231\n",
      "4135 Train Loss 28.310932\n",
      "4136 Train Loss 28.30312\n",
      "4137 Train Loss 28.56991\n",
      "4138 Train Loss 28.313808\n",
      "4139 Train Loss 28.31231\n",
      "4140 Train Loss 28.310932\n",
      "4141 Train Loss 28.30312\n",
      "4142 Train Loss 28.56991\n",
      "4143 Train Loss 28.313808\n",
      "4144 Train Loss 28.31231\n",
      "4145 Train Loss 28.310932\n",
      "4146 Train Loss 28.30312\n",
      "4147 Train Loss 28.56991\n",
      "4148 Train Loss 28.313808\n",
      "4149 Train Loss 28.31231\n",
      "4150 Train Loss 28.310932\n",
      "4151 Train Loss 28.30312\n",
      "4152 Train Loss 28.56991\n",
      "4153 Train Loss 28.313808\n",
      "4154 Train Loss 28.31231\n",
      "4155 Train Loss 28.310932\n",
      "4156 Train Loss 28.30312\n",
      "4157 Train Loss 28.56991\n",
      "4158 Train Loss 28.313808\n",
      "4159 Train Loss 28.31231\n",
      "4160 Train Loss 28.310932\n",
      "4161 Train Loss 28.30312\n",
      "4162 Train Loss 28.56991\n",
      "4163 Train Loss 28.313808\n",
      "4164 Train Loss 28.31231\n",
      "4165 Train Loss 28.310932\n",
      "4166 Train Loss 28.30312\n",
      "4167 Train Loss 28.56991\n",
      "4168 Train Loss 28.313808\n",
      "4169 Train Loss 28.31231\n",
      "4170 Train Loss 28.310932\n",
      "4171 Train Loss 28.30312\n",
      "4172 Train Loss 28.56991\n",
      "4173 Train Loss 28.313808\n",
      "4174 Train Loss 28.31231\n",
      "4175 Train Loss 28.310932\n",
      "4176 Train Loss 28.30312\n",
      "4177 Train Loss 28.56991\n",
      "4178 Train Loss 28.313808\n",
      "4179 Train Loss 28.31231\n",
      "4180 Train Loss 28.310932\n",
      "4181 Train Loss 28.30312\n",
      "4182 Train Loss 28.56991\n",
      "4183 Train Loss 28.313808\n",
      "4184 Train Loss 28.31231\n",
      "4185 Train Loss 28.310932\n",
      "4186 Train Loss 28.30312\n",
      "4187 Train Loss 28.56991\n",
      "4188 Train Loss 28.313808\n",
      "4189 Train Loss 28.31231\n",
      "4190 Train Loss 28.310932\n",
      "4191 Train Loss 28.30312\n",
      "4192 Train Loss 28.56991\n",
      "4193 Train Loss 28.313808\n",
      "4194 Train Loss 28.31231\n",
      "4195 Train Loss 28.310932\n",
      "4196 Train Loss 28.30312\n",
      "4197 Train Loss 28.56991\n",
      "4198 Train Loss 28.313808\n",
      "4199 Train Loss 28.31231\n",
      "4200 Train Loss 28.310932\n",
      "4201 Train Loss 28.30312\n",
      "4202 Train Loss 28.56991\n",
      "4203 Train Loss 28.313808\n",
      "4204 Train Loss 28.31231\n",
      "4205 Train Loss 28.310932\n",
      "4206 Train Loss 28.30312\n",
      "4207 Train Loss 28.56991\n",
      "4208 Train Loss 28.313808\n",
      "4209 Train Loss 28.31231\n",
      "4210 Train Loss 28.310932\n",
      "4211 Train Loss 28.30312\n",
      "4212 Train Loss 28.56991\n",
      "4213 Train Loss 28.313808\n",
      "4214 Train Loss 28.31231\n",
      "4215 Train Loss 28.310932\n",
      "4216 Train Loss 28.30312\n",
      "4217 Train Loss 28.56991\n",
      "4218 Train Loss 28.313808\n",
      "4219 Train Loss 28.31231\n",
      "4220 Train Loss 28.310932\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4221 Train Loss 28.30312\n",
      "4222 Train Loss 28.56991\n",
      "4223 Train Loss 28.313808\n",
      "4224 Train Loss 28.31231\n",
      "4225 Train Loss 28.310932\n",
      "4226 Train Loss 28.30312\n",
      "4227 Train Loss 28.56991\n",
      "4228 Train Loss 28.313808\n",
      "4229 Train Loss 28.31231\n",
      "4230 Train Loss 28.310932\n",
      "4231 Train Loss 28.30312\n",
      "4232 Train Loss 28.56991\n",
      "4233 Train Loss 28.313808\n",
      "4234 Train Loss 28.31231\n",
      "4235 Train Loss 28.310932\n",
      "4236 Train Loss 28.30312\n",
      "4237 Train Loss 28.56991\n",
      "4238 Train Loss 28.313808\n",
      "4239 Train Loss 28.31231\n",
      "4240 Train Loss 28.310932\n",
      "4241 Train Loss 28.30312\n",
      "4242 Train Loss 28.56991\n",
      "4243 Train Loss 28.313808\n",
      "4244 Train Loss 28.31231\n",
      "4245 Train Loss 28.310932\n",
      "4246 Train Loss 28.30312\n",
      "4247 Train Loss 28.56991\n",
      "4248 Train Loss 28.313808\n",
      "4249 Train Loss 28.31231\n",
      "4250 Train Loss 28.310932\n",
      "4251 Train Loss 28.30312\n",
      "4252 Train Loss 28.56991\n",
      "4253 Train Loss 28.313808\n",
      "4254 Train Loss 28.31231\n",
      "4255 Train Loss 28.310932\n",
      "4256 Train Loss 28.30312\n",
      "4257 Train Loss 28.56991\n",
      "4258 Train Loss 28.313808\n",
      "4259 Train Loss 28.31231\n",
      "4260 Train Loss 28.310932\n",
      "4261 Train Loss 28.30312\n",
      "4262 Train Loss 28.56991\n",
      "4263 Train Loss 28.313808\n",
      "4264 Train Loss 28.31231\n",
      "4265 Train Loss 28.310932\n",
      "4266 Train Loss 28.30312\n",
      "4267 Train Loss 28.56991\n",
      "4268 Train Loss 28.313808\n",
      "4269 Train Loss 28.31231\n",
      "4270 Train Loss 28.310932\n",
      "4271 Train Loss 28.30312\n",
      "4272 Train Loss 28.56991\n",
      "4273 Train Loss 28.313808\n",
      "4274 Train Loss 28.31231\n",
      "4275 Train Loss 28.310932\n",
      "4276 Train Loss 28.30312\n",
      "4277 Train Loss 28.56991\n",
      "4278 Train Loss 28.313808\n",
      "4279 Train Loss 28.31231\n",
      "4280 Train Loss 28.310932\n",
      "4281 Train Loss 28.30312\n",
      "4282 Train Loss 28.56991\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-46bc9707a98c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPINN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPINN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                     loss, flat_grad, t, ls_func_evals = _strong_wolfe(\n\u001b[0;32m--> 426\u001b[0;31m                         obj_func, x_init, t, d, loss, flat_grad, gtd)\n\u001b[0m\u001b[1;32m    427\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0mopt_cond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflat_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mtolerance_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36m_strong_wolfe\u001b[0;34m(obj_func, x, t, d, f, g, gtd, c1, c2, tolerance_change, max_ls)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# evaluate objective and gradient using initial step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mf_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mls_func_evals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mgtd_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mobj_func\u001b[0;34m(x, t, d)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m                     \u001b[0;32mdef\u001b[0m \u001b[0mobj_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m                         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_directional_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                     loss, flat_grad, t, ls_func_evals = _strong_wolfe(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36m_directional_evaluate\u001b[0;34m(self, closure, x, t, d)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_directional_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0mflat_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gather_flat_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-2139e3529a74>\u001b[0m in \u001b[0;36mclosure\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Train Loss\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 135\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    136\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    137\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_reps = 1\n",
    "\n",
    "train_loss_full = []\n",
    "test_loss_full = []\n",
    "beta_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(reps)\n",
    "    \n",
    "    torch.manual_seed(reps*36)\n",
    "    N_D = 100 #Total number of data points for 'y'\n",
    "    N_N = 3500\n",
    "    N_f = 10000 #Total number of collocation points \n",
    "    \n",
    "    xy_coll_np_array, xy_D_np_array, u_D_np_array,xy_Nx_np_array,xy_Ny_np_array = trainingdata(N_D,N_N,N_f,(reps)*22)\n",
    "        \n",
    "    xy_coll = torch.from_numpy(xy_coll_np_array).float().to(device)\n",
    "    xy_D = torch.from_numpy(xy_D_np_array).float().to(device)\n",
    "    u_D = torch.from_numpy(u_D_np_array).float().to(device)\n",
    "    xy_Nx = torch.from_numpy(xy_Nx_np_array).float().to(device)\n",
    "    xy_Ny = torch.from_numpy(xy_Ny_np_array).float().to(device)\n",
    "        \n",
    "    N_hat = torch.zeros(xy_Nx.shape[0],1).to(device)    \n",
    "    f_hat = torch.zeros(xy_coll.shape[0],1).to(device)\n",
    "\n",
    "    layers = np.array([2,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "    #layers = np.array([3,100,100,100,100,100,100,100,1])\n",
    "\n",
    "    PINN = Sequentialmodel(layers)\n",
    "   \n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "    \n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=1, \n",
    "                              max_iter = 10000, \n",
    "                              max_eval = None, \n",
    "                              tolerance_grad = -1, \n",
    "                              tolerance_change = -1, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "        \n",
    "    optimizer.step(PINN.closure)\n",
    "    \n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(PINN.train_loss)\n",
    "    test_loss_full.append(PINN.test_loss)\n",
    "    elapsed_time[reps] = time.time() - start_time\n",
    "    beta_full.append(PINN.beta_val)\n",
    "\n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "    \n",
    "mdic = {\"train_loss\": train_loss_full,\"test_loss\": test_loss_full,\"Time\": elapsed_time, \"beta\": beta_full, \"label\": label}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred = PINN.test(xy_test_tensor)\n",
    "u_pred_3d = u_pred.reshape(250,250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-7-debfad42fa7b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0max\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msubplots\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mcmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcm\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mjet\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mimg3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimshow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflipud\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranspose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_pred_3d\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvmax\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0;36m1000\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvmin\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m450\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcmap\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcmap\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mextent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maspect\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolorbar\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morientation\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'vertical'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "cmap = plt.cm.jet\n",
    "\n",
    "img3 = ax.imshow(np.flipud(np.transpose(u_pred_3d)),vmax =1000,vmin=450,cmap = cmap,extent=[0,1,0,1],aspect = 1)\n",
    "fig.colorbar(img3, orientation='vertical')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "mat = scipy.io.loadmat('Thinplate_steady.mat')\n",
    "xy_test = np.array(mat['xy_test'])\n",
    "\n",
    "u_test = np.array(mat['u_test'])\n",
    "xy_test_tensor = torch.from_numpy(xy_test).float().to(device)\n",
    "u_pred = PINN.test(xy_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f99d021abd0>]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(u_test,'bo')\n",
    "plt.plot(u_pred,'ro',markersize = 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0016613590458503691"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.linalg.norm(u_pred-u_test)/np.linalg.norm(u_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-12-af732ad11e54>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-12-af732ad11e54>\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    mat'u'\u001b[0m\n\u001b[0m         ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + test_loss_full[i][-1]\n",
    "print(a/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + train_loss_full[i][-1]\n",
    "print(a/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "l1 =np.array([1,2,3,4,5])\n",
    "l2 =np.array([1,2,3,4,5])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "L1,L2 = np.meshgrid(l1,l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2, 3, 4, 5],\n",
       "       [1, 2, 3, 4, 5],\n",
       "       [1, 2, 3, 4, 5],\n",
       "       [1, 2, 3, 4, 5],\n",
       "       [1, 2, 3, 4, 5]])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "L1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
