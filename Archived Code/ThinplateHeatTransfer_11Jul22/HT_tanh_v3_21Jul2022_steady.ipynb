{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "####SUCCESFUL: IMPORTANT\n",
    "\n",
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def true_2D_1(xt): #True function for 2D_1 Klein Gordon Equation x \\in [-50,50] , t \\in [0,10]\n",
    "#     y = xt[:,0]*np.cos(xt[:,1])\n",
    "#     return y.reshape(-1,1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Material Properties This link - https://www.mathworks.com/help/pde/ug/nonlinear-heat-transfer-in-a-thin-plate.html#heatTransferThinPlateExample-1\n",
    "k = 400\n",
    "rho = 8960\n",
    "cp = 386\n",
    "t_z = 0.01\n",
    "stef_bolt = 5.670373e-8\n",
    "hc = 1\n",
    "Ta = 300/1000\n",
    "emiss = 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"Navier_stan\"\n",
    "\n",
    "x = np.linspace(0,1,250).reshape(-1,1)\n",
    "y = np.linspace(0,1,250).reshape(-1,1)\n",
    "#t = np.linspace(0,1,100).reshape(-1,1) #t is actually from 0 to 5000, let us scale it to 0 to 1\n",
    "\n",
    "X,Y = np.meshgrid(x,y)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "Y = Y.flatten('F').reshape(-1,1)\n",
    " \n",
    "xy = np.hstack((X,Y))\n",
    "\n",
    "#initial_pts = np.logical_and(T==0,Y!=0).reshape(-1,)\n",
    "\n",
    "DBC_pts = (Y == 0).reshape(-1,)\n",
    "\n",
    "\n",
    "NBC_pts_x0 = (X == 0).reshape(-1,)\n",
    "NBC_pts_x1 = (X == 1).reshape(-1,)\n",
    "\n",
    "NBC_pts_y0 = (Y == 0).reshape(-1,)\n",
    "NBC_pts_y1 = (Y == 1).reshape(-1,)\n",
    "\n",
    "#xy_initial = xyt[initial_pts,:]\n",
    "xy_DBC = xy[DBC_pts,:]\n",
    "\n",
    "xy_NBC_x0 = xy[NBC_pts_x0,:]\n",
    "xy_NBC_x1 = xy[NBC_pts_x1,:]\n",
    "\n",
    "#xyt_NBC_y0 = xyt[NBC_pts_y0,:]\n",
    "xy_NBC_y1 = xy[NBC_pts_y1,:]\n",
    "\n",
    "#u_initial = np.zeros((np.shape(xy_initial)[0],1))\n",
    "u_DBC = 1*np.ones((np.shape(xy_DBC)[0],1)) #Scaling by 1000\n",
    "\n",
    "#xy_I_DBC = np.vstack((xy_initial,xyt_DBC))\n",
    "#xyt_NBC = np.vstack((xyt_NBC_1,xyt_NBC_2,xyt_NBC_3,xyt_NBC_4))\n",
    "xy_NBC_x = np.vstack((xy_NBC_x0,xy_NBC_x1))\n",
    "#xyt_NBC_y = np.vstack((xyt_NBC_y0,xyt_NBC_y1))\n",
    "xy_NBC_y = np.vstack((xy_NBC_y1))\n",
    "\n",
    "#u_I_DBC = np.vstack((u_initial,u_DBC))\n",
    "\n",
    "xy_test_tensor = torch.from_numpy(xy).float().to(device)\n",
    "\n",
    "lb_xy = xy[0]\n",
    "ub_xy = xy[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_D,N_N,N_f,seed):\n",
    "    '''Boundary Conditions''' \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    #choose random N_u points for training\n",
    "    idx = np.random.choice(xy_DBC.shape[0], N_D, replace=False) \n",
    "    xy_D = xy_DBC[idx,:] #choose indices from  set 'idx' (x,t)\n",
    "    u_D = u_DBC[idx].reshape(-1,1)      #choose corresponding u\n",
    "\n",
    "    idx = np.random.choice(xy_NBC_x.shape[0], N_D, replace=False) \n",
    "    xy_Nx = xy_NBC_x[idx,:] #choose indices from  set 'idx' (x,t)\n",
    "\n",
    "    idx = np.random.choice(xy_NBC_y.shape[0], N_D, replace=False) \n",
    "    xy_Ny = xy_NBC_y[idx,:] #choose indices from  set 'idx' (x,t)\n",
    "\n",
    "    '''Collocation Points'''\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    samples = sampling(N_f)\n",
    "    \n",
    "    xy_coll = lb_xy + (ub_xy - lb_xy)*samples\n",
    "    xy_coll = np.vstack((xy_coll, xy_D,xy_Nx,xy_Ny)) # append training points to collocation points \n",
    "\n",
    "    return xy_coll, xy_D, u_D, xy_Nx,xy_Ny"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        self.iter = 0\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)   \n",
    "        \n",
    "        self.beta = Parameter(torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "        \n",
    "        self.train_loss = []\n",
    "        self.test_loss = []\n",
    "        \n",
    "        self.beta_val = []\n",
    "        \n",
    "        self.n = torch.tensor(1.0)\n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xy):\n",
    "        if torch.is_tensor(xy) != True:         \n",
    "            xy = torch.from_numpy(xy)                \n",
    "        \n",
    "        ubxy = torch.from_numpy(ub_xy).float().to(device)\n",
    "        lbxy = torch.from_numpy(lb_xy).float().to(device)\n",
    "\n",
    "                      \n",
    "        #preprocessing input \n",
    "        xy = (xy - lbxy)/(ubxy - lbxy)\n",
    "        \n",
    "        #convert to float\n",
    "        a = xy.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z) \n",
    "            #a = z1 + self.beta[:,i]*z*z1\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_D(self,xy_D,u_D):\n",
    "                \n",
    "        loss_bc = self.loss_function(self.forward(xy_D), u_D)\n",
    "                \n",
    "        return loss_bc\n",
    "    \n",
    "    def loss_N(self,xy_Nx,xy_Ny,N_hat):\n",
    "        \n",
    "        g1 = xy_Nx.clone()             \n",
    "        g1.requires_grad = True\n",
    "        u1 = self.forward(g1)\n",
    "        \n",
    "        u1_x_y = autograd.grad(u1,g1,torch.ones([xy_Nx.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        du1_dx = u1_x_y[:,[0]]\n",
    "        \n",
    "        g2 = xy_Ny.clone()             \n",
    "        g2.requires_grad = True\n",
    "        u2 = self.forward(g2)\n",
    "        \n",
    "        u2_x_y = autograd.grad(u2,g2,torch.ones([xy_Ny.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        du2_dy = u2_x_y[:,[1]]\n",
    "               \n",
    "        loss_N1 = self.loss_function(du1_dx,N_hat)\n",
    "        loss_N2 = self.loss_function(du2_dy,N_hat)\n",
    "        \n",
    "        #return loss_N1+loss_N2       \n",
    "        return loss_N1 + loss_N2\n",
    "    \n",
    "    def loss_PDE(self, xy_coll, f_hat):\n",
    "        \n",
    "        g = xy_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_y = autograd.grad(u,g,torch.ones([xy_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_xx_yy = autograd.grad(u_x_y,g,torch.ones(xy_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        #du_dt = u_x_y_t[:,[2]]\n",
    "        \n",
    "        d2u_dx2 = u_xx_yy[:,[0]]\n",
    "        d2u_dy2 = u_xx_yy[:,[1]]    \n",
    "        \n",
    "\n",
    "        f = - 1000*k*t_z*(d2u_dx2+d2u_dy2) + 1000*2*hc*(u-Ta) + 2*emiss*stef_bolt*(torch.pow(1000*u,4)-(1000*Ta)**4) \n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xy_D,u_D,xy_Nx,xy_Ny,N_hat,xy_coll,f_hat):\n",
    "\n",
    "        loss_D = self.loss_D(xy_D,u_D)\n",
    "        loss_N = self.loss_N(xy_Nx,xy_Ny,N_hat)\n",
    "        loss_f = self.loss_PDE(xy_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_D + loss_N + loss_f\n",
    "        \n",
    "        #print(self.iter,\"loss_D:\",loss_D.cpu().detach().numpy(),\"loss_N:\",loss_N.cpu().detach().numpy(),\"loss_f:\",loss_f.cpu().detach().numpy())\n",
    "        \n",
    "        return loss_val\n",
    "     \n",
    "    'callable for optimizer'                                       \n",
    "    def closure(self):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = self.loss(xy_D,u_D,xy_Nx,xy_Ny,N_hat,xy_coll,f_hat)\n",
    "        self.train_loss.append(loss.cpu().detach().numpy())\n",
    "        \n",
    "        u_pred = self.test(xy_test_tensor)\n",
    "        #self.test_loss.append(np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))) #Commented because no true values yet\n",
    "        self.beta_val.append(self.beta.cpu().detach().numpy())\n",
    "        \n",
    "        #print(self.iter,\"Train Loss\",self.train_loss[-1],\"Test Loss\",self.test_loss[-1])\n",
    "        print(self.iter,\"Train Loss\",self.train_loss[-1])\n",
    "        \n",
    "        loss.backward()\n",
    "        self.iter += 1\n",
    "  \n",
    "\n",
    "        return loss        \n",
    "    'test neural network'\n",
    "    def test(self,xy_test_tensor):\n",
    "        u_pred = self.forward(xy_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0 Train Loss 1544169.9\n",
      "1 Train Loss 760948.44\n",
      "2 Train Loss 56458.312\n",
      "3 Train Loss 50984.734\n",
      "4 Train Loss 49212.754\n",
      "5 Train Loss 36747.734\n",
      "6 Train Loss 12836.262\n",
      "7 Train Loss 4465.0576\n",
      "8 Train Loss 4189.278\n",
      "9 Train Loss 4144.6367\n",
      "10 Train Loss 3988.931\n",
      "11 Train Loss 3699.8643\n",
      "12 Train Loss 3179.586\n",
      "13 Train Loss 2444.6746\n",
      "14 Train Loss 1836.5422\n",
      "15 Train Loss 1560.7633\n",
      "16 Train Loss 1170.4177\n",
      "17 Train Loss 980.0194\n",
      "18 Train Loss 897.4101\n",
      "19 Train Loss 893.33124\n",
      "20 Train Loss 889.6283\n",
      "21 Train Loss 865.8682\n",
      "22 Train Loss 821.6742\n",
      "23 Train Loss 709.98456\n",
      "24 Train Loss 518.83527\n",
      "25 Train Loss 398.67636\n",
      "26 Train Loss 372.7787\n",
      "27 Train Loss 367.02505\n",
      "28 Train Loss 365.28778\n",
      "29 Train Loss 364.99268\n",
      "30 Train Loss 360.2054\n",
      "31 Train Loss 345.30347\n",
      "32 Train Loss 310.5156\n",
      "33 Train Loss 244.68568\n",
      "34 Train Loss 170.18878\n",
      "35 Train Loss 109.952354\n",
      "36 Train Loss 76.43324\n",
      "37 Train Loss 72.46092\n",
      "38 Train Loss 69.74372\n",
      "39 Train Loss 69.36155\n",
      "40 Train Loss 69.3501\n",
      "41 Train Loss 69.26107\n",
      "42 Train Loss 68.27298\n",
      "43 Train Loss 64.46124\n",
      "44 Train Loss 57.467205\n",
      "45 Train Loss 46.86983\n",
      "46 Train Loss 34.039745\n",
      "47 Train Loss 26.52363\n",
      "48 Train Loss 25.136572\n",
      "49 Train Loss 24.93067\n",
      "50 Train Loss 24.895737\n",
      "51 Train Loss 24.846296\n",
      "52 Train Loss 24.572645\n",
      "53 Train Loss 24.324667\n",
      "54 Train Loss 23.644386\n",
      "55 Train Loss 22.113005\n",
      "56 Train Loss 18.457312\n",
      "57 Train Loss 14.858303\n",
      "58 Train Loss 107.19574\n",
      "59 Train Loss 13.526224\n",
      "60 Train Loss 10.822919\n",
      "61 Train Loss 9.300573\n",
      "62 Train Loss 8.949412\n",
      "63 Train Loss 8.566899\n",
      "64 Train Loss 8.170134\n",
      "65 Train Loss 7.915468\n",
      "66 Train Loss 7.813447\n",
      "67 Train Loss 7.73857\n",
      "68 Train Loss 7.72241\n",
      "69 Train Loss 7.7191734\n",
      "70 Train Loss 7.7175245\n",
      "71 Train Loss 7.6994977\n",
      "72 Train Loss 7.6431\n",
      "73 Train Loss 7.538479\n",
      "74 Train Loss 7.231191\n",
      "75 Train Loss 6.497003\n",
      "76 Train Loss 4.9785767\n",
      "77 Train Loss 4.412938\n",
      "78 Train Loss 4.9323125\n",
      "79 Train Loss 3.120152\n",
      "80 Train Loss 4.2604384\n",
      "81 Train Loss 2.793955\n",
      "82 Train Loss 2.3641462\n",
      "83 Train Loss 1.70573\n",
      "84 Train Loss 1.4711192\n",
      "85 Train Loss 1.2276869\n",
      "86 Train Loss 1.071805\n",
      "87 Train Loss 0.94489753\n",
      "88 Train Loss 0.89342594\n",
      "89 Train Loss 0.8788378\n",
      "90 Train Loss 0.87147695\n",
      "91 Train Loss 0.8659189\n",
      "92 Train Loss 0.86289823\n",
      "93 Train Loss 0.8605787\n",
      "94 Train Loss 0.858382\n",
      "95 Train Loss 0.8559027\n",
      "96 Train Loss 0.85291505\n",
      "97 Train Loss 0.84915394\n",
      "98 Train Loss 0.8461275\n",
      "99 Train Loss 0.8452288\n",
      "100 Train Loss 0.8451564\n",
      "101 Train Loss 0.8451165\n",
      "102 Train Loss 0.8450611\n",
      "103 Train Loss 0.8447885\n",
      "104 Train Loss 0.8442784\n",
      "105 Train Loss 0.84270376\n",
      "106 Train Loss 0.83906686\n",
      "107 Train Loss 0.83056796\n",
      "108 Train Loss 0.8153534\n",
      "109 Train Loss 0.7993795\n",
      "110 Train Loss 0.7903118\n",
      "111 Train Loss 0.8022002\n",
      "112 Train Loss 0.78817785\n",
      "113 Train Loss 0.7875137\n",
      "114 Train Loss 0.7873064\n",
      "115 Train Loss 0.7872044\n",
      "116 Train Loss 0.7870967\n",
      "117 Train Loss 0.7870514\n",
      "118 Train Loss 0.78704417\n",
      "119 Train Loss 0.7870457\n",
      "120 Train Loss 0.78704923\n",
      "121 Train Loss 0.7870536\n",
      "122 Train Loss 0.7870444\n",
      "123 Train Loss 0.7870457\n",
      "124 Train Loss 0.78704923\n",
      "125 Train Loss 0.7870536\n",
      "126 Train Loss 0.7870444\n",
      "127 Train Loss 0.7870457\n",
      "128 Train Loss 0.78704923\n",
      "129 Train Loss 0.7870536\n",
      "130 Train Loss 0.7870444\n",
      "131 Train Loss 0.7870457\n",
      "132 Train Loss 0.78704923\n",
      "133 Train Loss 0.7870536\n",
      "134 Train Loss 0.7870444\n",
      "135 Train Loss 0.7870457\n",
      "136 Train Loss 0.78704923\n",
      "137 Train Loss 0.7870536\n",
      "138 Train Loss 0.7870444\n",
      "139 Train Loss 0.7870457\n",
      "140 Train Loss 0.78704923\n",
      "141 Train Loss 0.7870536\n",
      "142 Train Loss 0.7870444\n",
      "143 Train Loss 0.7870457\n",
      "144 Train Loss 0.78704923\n",
      "145 Train Loss 0.7870536\n",
      "146 Train Loss 0.7870444\n",
      "147 Train Loss 0.7870457\n",
      "148 Train Loss 0.78704923\n",
      "149 Train Loss 0.7870536\n",
      "150 Train Loss 0.7870444\n",
      "151 Train Loss 0.7870457\n",
      "152 Train Loss 0.78704923\n",
      "153 Train Loss 0.7870536\n",
      "154 Train Loss 0.7870444\n",
      "155 Train Loss 0.7870457\n",
      "156 Train Loss 0.78704923\n",
      "157 Train Loss 0.7870536\n",
      "158 Train Loss 0.7870444\n",
      "159 Train Loss 0.7870457\n",
      "160 Train Loss 0.78704923\n",
      "161 Train Loss 0.7870536\n",
      "162 Train Loss 0.7870444\n",
      "163 Train Loss 0.7870457\n",
      "164 Train Loss 0.78704923\n",
      "165 Train Loss 0.7870536\n",
      "166 Train Loss 0.7870444\n",
      "167 Train Loss 0.7870457\n",
      "168 Train Loss 0.78704923\n",
      "169 Train Loss 0.7870536\n",
      "170 Train Loss 0.7870444\n",
      "171 Train Loss 0.7870457\n",
      "172 Train Loss 0.78704923\n",
      "173 Train Loss 0.7870536\n",
      "174 Train Loss 0.7870444\n",
      "175 Train Loss 0.7870457\n",
      "176 Train Loss 0.78704923\n",
      "177 Train Loss 0.7870536\n",
      "178 Train Loss 0.7870444\n",
      "179 Train Loss 0.7870457\n",
      "180 Train Loss 0.78704923\n",
      "181 Train Loss 0.7870536\n",
      "182 Train Loss 0.7870444\n",
      "183 Train Loss 0.7870457\n",
      "184 Train Loss 0.78704923\n",
      "185 Train Loss 0.7870536\n",
      "186 Train Loss 0.7870444\n",
      "187 Train Loss 0.7870457\n",
      "188 Train Loss 0.78704923\n",
      "189 Train Loss 0.7870536\n",
      "190 Train Loss 0.7870444\n",
      "191 Train Loss 0.7870457\n",
      "192 Train Loss 0.78704923\n",
      "193 Train Loss 0.7870536\n",
      "194 Train Loss 0.7870444\n",
      "195 Train Loss 0.7870457\n",
      "196 Train Loss 0.78704923\n",
      "197 Train Loss 0.7870536\n",
      "198 Train Loss 0.7870444\n",
      "199 Train Loss 0.7870457\n",
      "200 Train Loss 0.78704923\n",
      "201 Train Loss 0.7870536\n",
      "202 Train Loss 0.7870444\n",
      "203 Train Loss 0.7870457\n",
      "204 Train Loss 0.78704923\n",
      "205 Train Loss 0.7870536\n",
      "206 Train Loss 0.7870444\n",
      "207 Train Loss 0.7870457\n",
      "208 Train Loss 0.78704923\n",
      "209 Train Loss 0.7870536\n",
      "210 Train Loss 0.7870444\n",
      "211 Train Loss 0.7870457\n",
      "212 Train Loss 0.78704923\n",
      "213 Train Loss 0.7870536\n",
      "214 Train Loss 0.7870444\n",
      "215 Train Loss 0.7870457\n",
      "216 Train Loss 0.78704923\n",
      "217 Train Loss 0.7870536\n",
      "218 Train Loss 0.7870444\n",
      "219 Train Loss 0.7870457\n",
      "220 Train Loss 0.78704923\n",
      "221 Train Loss 0.7870536\n",
      "222 Train Loss 0.7870444\n",
      "223 Train Loss 0.7870457\n",
      "224 Train Loss 0.78704923\n",
      "225 Train Loss 0.7870536\n",
      "226 Train Loss 0.7870444\n",
      "227 Train Loss 0.7870457\n",
      "228 Train Loss 0.78704923\n",
      "229 Train Loss 0.7870536\n",
      "230 Train Loss 0.7870444\n",
      "231 Train Loss 0.7870457\n",
      "232 Train Loss 0.78704923\n",
      "233 Train Loss 0.7870536\n",
      "234 Train Loss 0.7870444\n",
      "235 Train Loss 0.7870457\n",
      "236 Train Loss 0.78704923\n",
      "237 Train Loss 0.7870536\n",
      "238 Train Loss 0.7870444\n",
      "239 Train Loss 0.7870457\n",
      "240 Train Loss 0.78704923\n",
      "241 Train Loss 0.7870536\n",
      "242 Train Loss 0.7870444\n",
      "243 Train Loss 0.7870457\n",
      "244 Train Loss 0.78704923\n",
      "245 Train Loss 0.7870536\n",
      "246 Train Loss 0.7870444\n",
      "247 Train Loss 0.7870457\n",
      "248 Train Loss 0.78704923\n",
      "249 Train Loss 0.7870536\n",
      "250 Train Loss 0.7870444\n",
      "251 Train Loss 0.7870457\n",
      "252 Train Loss 0.78704923\n",
      "253 Train Loss 0.7870536\n",
      "254 Train Loss 0.7870444\n",
      "255 Train Loss 0.7870457\n",
      "256 Train Loss 0.78704923\n",
      "257 Train Loss 0.7870536\n",
      "258 Train Loss 0.7870444\n",
      "259 Train Loss 0.7870457\n",
      "260 Train Loss 0.78704923\n",
      "261 Train Loss 0.7870536\n",
      "262 Train Loss 0.7870444\n",
      "263 Train Loss 0.7870457\n",
      "264 Train Loss 0.78704923\n",
      "265 Train Loss 0.7870536\n",
      "266 Train Loss 0.7870444\n",
      "267 Train Loss 0.7870457\n",
      "268 Train Loss 0.78704923\n",
      "269 Train Loss 0.7870536\n",
      "270 Train Loss 0.7870444\n",
      "271 Train Loss 0.7870457\n",
      "272 Train Loss 0.78704923\n",
      "273 Train Loss 0.7870536\n",
      "274 Train Loss 0.7870444\n",
      "275 Train Loss 0.7870457\n",
      "276 Train Loss 0.78704923\n",
      "277 Train Loss 0.7870536\n",
      "278 Train Loss 0.7870444\n",
      "279 Train Loss 0.7870457\n",
      "280 Train Loss 0.78704923\n",
      "281 Train Loss 0.7870536\n",
      "282 Train Loss 0.7870444\n",
      "283 Train Loss 0.7870457\n",
      "284 Train Loss 0.78704923\n",
      "285 Train Loss 0.7870536\n",
      "286 Train Loss 0.7870444\n",
      "287 Train Loss 0.7870457\n",
      "288 Train Loss 0.78704923\n",
      "289 Train Loss 0.7870536\n",
      "290 Train Loss 0.7870444\n",
      "291 Train Loss 0.7870457\n",
      "292 Train Loss 0.78704923\n",
      "293 Train Loss 0.7870536\n",
      "294 Train Loss 0.7870444\n",
      "295 Train Loss 0.7870457\n",
      "296 Train Loss 0.78704923\n",
      "297 Train Loss 0.7870536\n",
      "298 Train Loss 0.7870444\n",
      "299 Train Loss 0.7870457\n",
      "300 Train Loss 0.78704923\n",
      "301 Train Loss 0.7870536\n",
      "302 Train Loss 0.7870444\n",
      "303 Train Loss 0.7870457\n",
      "304 Train Loss 0.78704923\n",
      "305 Train Loss 0.7870536\n",
      "306 Train Loss 0.7870444\n",
      "307 Train Loss 0.7870457\n",
      "308 Train Loss 0.78704923\n",
      "309 Train Loss 0.7870536\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "310 Train Loss 0.7870444\n",
      "311 Train Loss 0.7870457\n",
      "312 Train Loss 0.78704923\n",
      "313 Train Loss 0.7870536\n",
      "314 Train Loss 0.7870444\n",
      "315 Train Loss 0.7870457\n",
      "316 Train Loss 0.78704923\n",
      "317 Train Loss 0.7870536\n",
      "318 Train Loss 0.7870444\n",
      "319 Train Loss 0.7870457\n",
      "320 Train Loss 0.78704923\n",
      "321 Train Loss 0.7870536\n",
      "322 Train Loss 0.7870444\n",
      "323 Train Loss 0.7870457\n",
      "324 Train Loss 0.78704923\n",
      "325 Train Loss 0.7870536\n",
      "326 Train Loss 0.7870444\n",
      "327 Train Loss 0.7870457\n",
      "328 Train Loss 0.78704923\n",
      "329 Train Loss 0.7870536\n",
      "330 Train Loss 0.7870444\n",
      "331 Train Loss 0.7870457\n",
      "332 Train Loss 0.78704923\n",
      "333 Train Loss 0.7870536\n",
      "334 Train Loss 0.7870444\n",
      "335 Train Loss 0.7870457\n",
      "336 Train Loss 0.78704923\n",
      "337 Train Loss 0.7870536\n",
      "338 Train Loss 0.7870444\n",
      "339 Train Loss 0.7870457\n",
      "340 Train Loss 0.78704923\n",
      "341 Train Loss 0.7870536\n",
      "342 Train Loss 0.7870444\n",
      "343 Train Loss 0.7870457\n",
      "344 Train Loss 0.78704923\n",
      "345 Train Loss 0.7870536\n",
      "346 Train Loss 0.7870444\n",
      "347 Train Loss 0.7870457\n",
      "348 Train Loss 0.78704923\n",
      "349 Train Loss 0.7870536\n",
      "350 Train Loss 0.7870444\n",
      "351 Train Loss 0.7870457\n",
      "352 Train Loss 0.78704923\n",
      "353 Train Loss 0.7870536\n",
      "354 Train Loss 0.7870444\n",
      "355 Train Loss 0.7870457\n",
      "356 Train Loss 0.78704923\n",
      "357 Train Loss 0.7870536\n",
      "358 Train Loss 0.7870444\n",
      "359 Train Loss 0.7870457\n",
      "360 Train Loss 0.78704923\n",
      "361 Train Loss 0.7870536\n",
      "362 Train Loss 0.7870444\n",
      "363 Train Loss 0.7870457\n",
      "364 Train Loss 0.78704923\n",
      "365 Train Loss 0.7870536\n",
      "366 Train Loss 0.7870444\n",
      "367 Train Loss 0.7870457\n",
      "368 Train Loss 0.78704923\n",
      "369 Train Loss 0.7870536\n",
      "370 Train Loss 0.7870444\n",
      "371 Train Loss 0.7870457\n",
      "372 Train Loss 0.78704923\n",
      "373 Train Loss 0.7870536\n",
      "374 Train Loss 0.7870444\n",
      "375 Train Loss 0.7870457\n",
      "376 Train Loss 0.78704923\n",
      "377 Train Loss 0.7870536\n",
      "378 Train Loss 0.7870444\n",
      "379 Train Loss 0.7870457\n",
      "380 Train Loss 0.78704923\n",
      "381 Train Loss 0.7870536\n",
      "382 Train Loss 0.7870444\n",
      "383 Train Loss 0.7870457\n",
      "384 Train Loss 0.78704923\n",
      "385 Train Loss 0.7870536\n",
      "386 Train Loss 0.7870444\n",
      "387 Train Loss 0.7870457\n",
      "388 Train Loss 0.78704923\n",
      "389 Train Loss 0.7870536\n",
      "390 Train Loss 0.7870444\n",
      "391 Train Loss 0.7870457\n",
      "392 Train Loss 0.78704923\n",
      "393 Train Loss 0.7870536\n",
      "394 Train Loss 0.7870444\n",
      "395 Train Loss 0.7870457\n",
      "396 Train Loss 0.78704923\n",
      "397 Train Loss 0.7870536\n",
      "398 Train Loss 0.7870444\n",
      "399 Train Loss 0.7870457\n",
      "400 Train Loss 0.78704923\n",
      "401 Train Loss 0.7870536\n",
      "402 Train Loss 0.7870444\n",
      "403 Train Loss 0.7870457\n",
      "404 Train Loss 0.78704923\n",
      "405 Train Loss 0.7870536\n",
      "406 Train Loss 0.7870444\n",
      "407 Train Loss 0.7870457\n",
      "408 Train Loss 0.78704923\n",
      "409 Train Loss 0.7870536\n",
      "410 Train Loss 0.7870444\n",
      "411 Train Loss 0.7870457\n",
      "412 Train Loss 0.78704923\n",
      "413 Train Loss 0.7870536\n",
      "414 Train Loss 0.7870444\n",
      "415 Train Loss 0.7870457\n",
      "416 Train Loss 0.78704923\n",
      "417 Train Loss 0.7870536\n",
      "418 Train Loss 0.7870444\n",
      "419 Train Loss 0.7870457\n",
      "420 Train Loss 0.78704923\n",
      "421 Train Loss 0.7870536\n",
      "422 Train Loss 0.7870444\n",
      "423 Train Loss 0.7870457\n",
      "424 Train Loss 0.78704923\n",
      "425 Train Loss 0.7870536\n",
      "426 Train Loss 0.7870444\n",
      "427 Train Loss 0.7870457\n",
      "428 Train Loss 0.78704923\n",
      "429 Train Loss 0.7870536\n",
      "430 Train Loss 0.7870444\n",
      "431 Train Loss 0.7870457\n",
      "432 Train Loss 0.78704923\n",
      "433 Train Loss 0.7870536\n",
      "434 Train Loss 0.7870444\n",
      "435 Train Loss 0.7870457\n",
      "436 Train Loss 0.78704923\n",
      "437 Train Loss 0.7870536\n",
      "438 Train Loss 0.7870444\n",
      "439 Train Loss 0.7870457\n",
      "440 Train Loss 0.78704923\n",
      "441 Train Loss 0.7870536\n",
      "442 Train Loss 0.7870444\n",
      "443 Train Loss 0.7870457\n",
      "444 Train Loss 0.78704923\n",
      "445 Train Loss 0.7870536\n",
      "446 Train Loss 0.7870444\n",
      "447 Train Loss 0.7870457\n",
      "448 Train Loss 0.78704923\n",
      "449 Train Loss 0.7870536\n",
      "450 Train Loss 0.7870444\n",
      "451 Train Loss 0.7870457\n",
      "452 Train Loss 0.78704923\n",
      "453 Train Loss 0.7870536\n",
      "454 Train Loss 0.7870444\n",
      "455 Train Loss 0.7870457\n",
      "456 Train Loss 0.78704923\n",
      "457 Train Loss 0.7870536\n",
      "458 Train Loss 0.7870444\n",
      "459 Train Loss 0.7870457\n",
      "460 Train Loss 0.78704923\n",
      "461 Train Loss 0.7870536\n",
      "462 Train Loss 0.7870444\n",
      "463 Train Loss 0.7870457\n",
      "464 Train Loss 0.78704923\n",
      "465 Train Loss 0.7870536\n",
      "466 Train Loss 0.7870444\n",
      "467 Train Loss 0.7870457\n",
      "468 Train Loss 0.78704923\n",
      "469 Train Loss 0.7870536\n",
      "470 Train Loss 0.7870444\n",
      "471 Train Loss 0.7870457\n",
      "472 Train Loss 0.78704923\n",
      "473 Train Loss 0.7870536\n",
      "474 Train Loss 0.7870444\n",
      "475 Train Loss 0.7870457\n",
      "476 Train Loss 0.78704923\n",
      "477 Train Loss 0.7870536\n",
      "478 Train Loss 0.7870444\n",
      "479 Train Loss 0.7870457\n",
      "480 Train Loss 0.78704923\n",
      "481 Train Loss 0.7870536\n",
      "482 Train Loss 0.7870444\n",
      "483 Train Loss 0.7870457\n",
      "484 Train Loss 0.78704923\n",
      "485 Train Loss 0.7870536\n",
      "486 Train Loss 0.7870444\n",
      "487 Train Loss 0.7870457\n",
      "488 Train Loss 0.78704923\n",
      "489 Train Loss 0.7870536\n",
      "490 Train Loss 0.7870444\n",
      "491 Train Loss 0.7870457\n",
      "492 Train Loss 0.78704923\n",
      "493 Train Loss 0.7870536\n",
      "494 Train Loss 0.7870444\n",
      "495 Train Loss 0.7870457\n",
      "496 Train Loss 0.78704923\n",
      "497 Train Loss 0.7870536\n",
      "498 Train Loss 0.7870444\n",
      "499 Train Loss 0.7870457\n",
      "500 Train Loss 0.78704923\n",
      "501 Train Loss 0.7870536\n",
      "502 Train Loss 0.7870444\n",
      "503 Train Loss 0.7870457\n",
      "504 Train Loss 0.78704923\n",
      "505 Train Loss 0.7870536\n",
      "506 Train Loss 0.7870444\n",
      "507 Train Loss 0.7870457\n",
      "508 Train Loss 0.78704923\n",
      "509 Train Loss 0.7870536\n",
      "510 Train Loss 0.7870444\n",
      "511 Train Loss 0.7870457\n",
      "512 Train Loss 0.78704923\n",
      "513 Train Loss 0.7870536\n",
      "514 Train Loss 0.7870444\n",
      "515 Train Loss 0.7870457\n",
      "516 Train Loss 0.78704923\n",
      "517 Train Loss 0.7870536\n",
      "518 Train Loss 0.7870444\n",
      "519 Train Loss 0.7870457\n",
      "520 Train Loss 0.78704923\n",
      "521 Train Loss 0.7870536\n",
      "522 Train Loss 0.7870444\n",
      "523 Train Loss 0.7870457\n",
      "524 Train Loss 0.78704923\n",
      "525 Train Loss 0.7870536\n",
      "526 Train Loss 0.7870444\n",
      "527 Train Loss 0.7870457\n",
      "528 Train Loss 0.78704923\n",
      "529 Train Loss 0.7870536\n",
      "530 Train Loss 0.7870444\n",
      "531 Train Loss 0.7870457\n",
      "532 Train Loss 0.78704923\n",
      "533 Train Loss 0.7870536\n",
      "534 Train Loss 0.7870444\n",
      "535 Train Loss 0.7870457\n",
      "536 Train Loss 0.78704923\n",
      "537 Train Loss 0.7870536\n",
      "538 Train Loss 0.7870444\n",
      "539 Train Loss 0.7870457\n",
      "540 Train Loss 0.78704923\n",
      "541 Train Loss 0.7870536\n",
      "542 Train Loss 0.7870444\n",
      "543 Train Loss 0.7870457\n",
      "544 Train Loss 0.78704923\n",
      "545 Train Loss 0.7870536\n",
      "546 Train Loss 0.7870444\n",
      "547 Train Loss 0.7870457\n",
      "548 Train Loss 0.78704923\n",
      "549 Train Loss 0.7870536\n",
      "550 Train Loss 0.7870444\n",
      "551 Train Loss 0.7870457\n",
      "552 Train Loss 0.78704923\n",
      "553 Train Loss 0.7870536\n",
      "554 Train Loss 0.7870444\n",
      "555 Train Loss 0.7870457\n",
      "556 Train Loss 0.78704923\n",
      "557 Train Loss 0.7870536\n",
      "558 Train Loss 0.7870444\n",
      "559 Train Loss 0.7870457\n",
      "560 Train Loss 0.78704923\n",
      "561 Train Loss 0.7870536\n",
      "562 Train Loss 0.7870444\n",
      "563 Train Loss 0.7870457\n",
      "564 Train Loss 0.78704923\n",
      "565 Train Loss 0.7870536\n",
      "566 Train Loss 0.7870444\n",
      "567 Train Loss 0.7870457\n",
      "568 Train Loss 0.78704923\n",
      "569 Train Loss 0.7870536\n",
      "570 Train Loss 0.7870444\n",
      "571 Train Loss 0.7870457\n",
      "572 Train Loss 0.78704923\n",
      "573 Train Loss 0.7870536\n",
      "574 Train Loss 0.7870444\n",
      "575 Train Loss 0.7870457\n",
      "576 Train Loss 0.78704923\n",
      "577 Train Loss 0.7870536\n",
      "578 Train Loss 0.7870444\n",
      "579 Train Loss 0.7870457\n",
      "580 Train Loss 0.78704923\n",
      "581 Train Loss 0.7870536\n",
      "582 Train Loss 0.7870444\n",
      "583 Train Loss 0.7870457\n",
      "584 Train Loss 0.78704923\n",
      "585 Train Loss 0.7870536\n",
      "586 Train Loss 0.7870444\n",
      "587 Train Loss 0.7870457\n",
      "588 Train Loss 0.78704923\n",
      "589 Train Loss 0.7870536\n",
      "590 Train Loss 0.7870444\n",
      "591 Train Loss 0.7870457\n",
      "592 Train Loss 0.78704923\n",
      "593 Train Loss 0.7870536\n",
      "594 Train Loss 0.7870444\n",
      "595 Train Loss 0.7870457\n",
      "596 Train Loss 0.78704923\n",
      "597 Train Loss 0.7870536\n",
      "598 Train Loss 0.7870444\n",
      "599 Train Loss 0.7870457\n",
      "600 Train Loss 0.78704923\n",
      "601 Train Loss 0.7870536\n",
      "602 Train Loss 0.7870444\n",
      "603 Train Loss 0.7870457\n",
      "604 Train Loss 0.78704923\n",
      "605 Train Loss 0.7870536\n",
      "606 Train Loss 0.7870444\n",
      "607 Train Loss 0.7870457\n",
      "608 Train Loss 0.78704923\n",
      "609 Train Loss 0.7870536\n",
      "610 Train Loss 0.7870444\n",
      "611 Train Loss 0.7870457\n",
      "612 Train Loss 0.78704923\n",
      "613 Train Loss 0.7870536\n",
      "614 Train Loss 0.7870444\n",
      "615 Train Loss 0.7870457\n",
      "616 Train Loss 0.78704923\n",
      "617 Train Loss 0.7870536\n",
      "618 Train Loss 0.7870444\n",
      "619 Train Loss 0.7870457\n",
      "620 Train Loss 0.78704923\n",
      "621 Train Loss 0.7870536\n",
      "622 Train Loss 0.7870444\n",
      "623 Train Loss 0.7870457\n",
      "624 Train Loss 0.78704923\n",
      "625 Train Loss 0.7870536\n",
      "626 Train Loss 0.7870444\n",
      "627 Train Loss 0.7870457\n",
      "628 Train Loss 0.78704923\n",
      "629 Train Loss 0.7870536\n",
      "630 Train Loss 0.7870444\n",
      "631 Train Loss 0.7870457\n",
      "632 Train Loss 0.78704923\n",
      "633 Train Loss 0.7870536\n",
      "634 Train Loss 0.7870444\n",
      "635 Train Loss 0.7870457\n",
      "636 Train Loss 0.78704923\n",
      "637 Train Loss 0.7870536\n",
      "638 Train Loss 0.7870444\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "639 Train Loss 0.7870457\n",
      "640 Train Loss 0.78704923\n",
      "641 Train Loss 0.7870536\n",
      "642 Train Loss 0.7870444\n",
      "643 Train Loss 0.7870457\n",
      "644 Train Loss 0.78704923\n",
      "645 Train Loss 0.7870536\n",
      "646 Train Loss 0.7870444\n",
      "647 Train Loss 0.7870457\n",
      "648 Train Loss 0.78704923\n",
      "649 Train Loss 0.7870536\n",
      "650 Train Loss 0.7870444\n",
      "651 Train Loss 0.7870457\n",
      "652 Train Loss 0.78704923\n",
      "653 Train Loss 0.7870536\n",
      "654 Train Loss 0.7870444\n",
      "655 Train Loss 0.7870457\n",
      "656 Train Loss 0.78704923\n",
      "657 Train Loss 0.7870536\n",
      "658 Train Loss 0.7870444\n",
      "659 Train Loss 0.7870457\n",
      "660 Train Loss 0.78704923\n",
      "661 Train Loss 0.7870536\n",
      "662 Train Loss 0.7870444\n",
      "663 Train Loss 0.7870457\n",
      "664 Train Loss 0.78704923\n",
      "665 Train Loss 0.7870536\n",
      "666 Train Loss 0.7870444\n",
      "667 Train Loss 0.7870457\n",
      "668 Train Loss 0.78704923\n",
      "669 Train Loss 0.7870536\n",
      "670 Train Loss 0.7870444\n",
      "671 Train Loss 0.7870457\n",
      "672 Train Loss 0.78704923\n",
      "673 Train Loss 0.7870536\n",
      "674 Train Loss 0.7870444\n",
      "675 Train Loss 0.7870457\n",
      "676 Train Loss 0.78704923\n",
      "677 Train Loss 0.7870536\n",
      "678 Train Loss 0.7870444\n",
      "679 Train Loss 0.7870457\n",
      "680 Train Loss 0.78704923\n",
      "681 Train Loss 0.7870536\n",
      "682 Train Loss 0.7870444\n",
      "683 Train Loss 0.7870457\n",
      "684 Train Loss 0.78704923\n",
      "685 Train Loss 0.7870536\n",
      "686 Train Loss 0.7870444\n",
      "687 Train Loss 0.7870457\n",
      "688 Train Loss 0.78704923\n",
      "689 Train Loss 0.7870536\n",
      "690 Train Loss 0.7870444\n",
      "691 Train Loss 0.7870457\n",
      "692 Train Loss 0.78704923\n",
      "693 Train Loss 0.7870536\n",
      "694 Train Loss 0.7870444\n",
      "695 Train Loss 0.7870457\n",
      "696 Train Loss 0.78704923\n",
      "697 Train Loss 0.7870536\n",
      "698 Train Loss 0.7870444\n",
      "699 Train Loss 0.7870457\n",
      "700 Train Loss 0.78704923\n",
      "701 Train Loss 0.7870536\n",
      "702 Train Loss 0.7870444\n",
      "703 Train Loss 0.7870457\n",
      "704 Train Loss 0.78704923\n",
      "705 Train Loss 0.7870536\n",
      "706 Train Loss 0.7870444\n",
      "707 Train Loss 0.7870457\n",
      "708 Train Loss 0.78704923\n",
      "709 Train Loss 0.7870536\n",
      "710 Train Loss 0.7870444\n",
      "711 Train Loss 0.7870457\n",
      "712 Train Loss 0.78704923\n",
      "713 Train Loss 0.7870536\n",
      "714 Train Loss 0.7870444\n",
      "715 Train Loss 0.7870457\n",
      "716 Train Loss 0.78704923\n",
      "717 Train Loss 0.7870536\n",
      "718 Train Loss 0.7870444\n",
      "719 Train Loss 0.7870457\n",
      "720 Train Loss 0.78704923\n",
      "721 Train Loss 0.7870536\n",
      "722 Train Loss 0.7870444\n",
      "723 Train Loss 0.7870457\n",
      "724 Train Loss 0.78704923\n",
      "725 Train Loss 0.7870536\n",
      "726 Train Loss 0.7870444\n",
      "727 Train Loss 0.7870457\n",
      "728 Train Loss 0.78704923\n",
      "729 Train Loss 0.7870536\n",
      "730 Train Loss 0.7870444\n",
      "731 Train Loss 0.7870457\n",
      "732 Train Loss 0.78704923\n",
      "733 Train Loss 0.7870536\n",
      "734 Train Loss 0.7870444\n",
      "735 Train Loss 0.7870457\n",
      "736 Train Loss 0.78704923\n",
      "737 Train Loss 0.7870536\n",
      "738 Train Loss 0.7870444\n",
      "739 Train Loss 0.7870457\n",
      "740 Train Loss 0.78704923\n",
      "741 Train Loss 0.7870536\n",
      "742 Train Loss 0.7870444\n",
      "743 Train Loss 0.7870457\n",
      "744 Train Loss 0.78704923\n",
      "745 Train Loss 0.7870536\n",
      "746 Train Loss 0.7870444\n",
      "747 Train Loss 0.7870457\n",
      "748 Train Loss 0.78704923\n",
      "749 Train Loss 0.7870536\n",
      "750 Train Loss 0.7870444\n",
      "751 Train Loss 0.7870457\n",
      "752 Train Loss 0.78704923\n",
      "753 Train Loss 0.7870536\n",
      "754 Train Loss 0.7870444\n",
      "755 Train Loss 0.7870457\n",
      "756 Train Loss 0.78704923\n",
      "757 Train Loss 0.7870536\n",
      "758 Train Loss 0.7870444\n",
      "759 Train Loss 0.7870457\n",
      "760 Train Loss 0.78704923\n",
      "761 Train Loss 0.7870536\n",
      "762 Train Loss 0.7870444\n",
      "763 Train Loss 0.7870457\n",
      "764 Train Loss 0.78704923\n",
      "765 Train Loss 0.7870536\n",
      "766 Train Loss 0.7870444\n",
      "767 Train Loss 0.7870457\n",
      "768 Train Loss 0.78704923\n",
      "769 Train Loss 0.7870536\n",
      "770 Train Loss 0.7870444\n",
      "771 Train Loss 0.7870457\n",
      "772 Train Loss 0.78704923\n",
      "773 Train Loss 0.7870536\n",
      "774 Train Loss 0.7870444\n",
      "775 Train Loss 0.7870457\n",
      "776 Train Loss 0.78704923\n",
      "777 Train Loss 0.7870536\n",
      "778 Train Loss 0.7870444\n",
      "779 Train Loss 0.7870457\n",
      "780 Train Loss 0.78704923\n",
      "781 Train Loss 0.7870536\n",
      "782 Train Loss 0.7870444\n",
      "783 Train Loss 0.7870457\n",
      "784 Train Loss 0.78704923\n",
      "785 Train Loss 0.7870536\n",
      "786 Train Loss 0.7870444\n",
      "787 Train Loss 0.7870457\n",
      "788 Train Loss 0.78704923\n",
      "789 Train Loss 0.7870536\n",
      "790 Train Loss 0.7870444\n",
      "791 Train Loss 0.7870457\n",
      "792 Train Loss 0.78704923\n",
      "793 Train Loss 0.7870536\n",
      "794 Train Loss 0.7870444\n",
      "795 Train Loss 0.7870457\n",
      "796 Train Loss 0.78704923\n",
      "797 Train Loss 0.7870536\n",
      "798 Train Loss 0.7870444\n",
      "799 Train Loss 0.7870457\n",
      "800 Train Loss 0.78704923\n",
      "801 Train Loss 0.7870536\n",
      "802 Train Loss 0.7870444\n",
      "803 Train Loss 0.7870457\n",
      "804 Train Loss 0.78704923\n",
      "805 Train Loss 0.7870536\n",
      "806 Train Loss 0.7870444\n",
      "807 Train Loss 0.7870457\n",
      "808 Train Loss 0.78704923\n",
      "809 Train Loss 0.7870536\n",
      "810 Train Loss 0.7870444\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-46bc9707a98c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPINN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPINN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                     loss, flat_grad, t, ls_func_evals = _strong_wolfe(\n\u001b[0;32m--> 426\u001b[0;31m                         obj_func, x_init, t, d, loss, flat_grad, gtd)\n\u001b[0m\u001b[1;32m    427\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0mopt_cond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflat_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mtolerance_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36m_strong_wolfe\u001b[0;34m(obj_func, x, t, d, f, g, gtd, c1, c2, tolerance_change, max_ls)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# evaluate objective and gradient using initial step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mf_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mls_func_evals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mgtd_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mobj_func\u001b[0;34m(x, t, d)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m                     \u001b[0;32mdef\u001b[0m \u001b[0mobj_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m                         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_directional_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                     loss, flat_grad, t, ls_func_evals = _strong_wolfe(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36m_directional_evaluate\u001b[0;34m(self, closure, x, t, d)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_directional_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0mflat_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gather_flat_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-cc42467177bc>\u001b[0m in \u001b[0;36mclosure\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    123\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    124\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 125\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy_D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu_D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxy_Nx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxy_Ny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN_hat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxy_coll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    126\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    127\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-cc42467177bc>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, xy_D, u_D, xy_Nx, xy_Ny, N_hat, xy_coll, f_hat)\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0mloss_D\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_D\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy_D\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu_D\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    112\u001b[0m         \u001b[0mloss_N\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_N\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy_Nx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxy_Ny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 113\u001b[0;31m         \u001b[0mloss_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_PDE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy_coll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    114\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    115\u001b[0m         \u001b[0mloss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_D\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_N\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-cc42467177bc>\u001b[0m in \u001b[0;36mloss_PDE\u001b[0;34m(self, xy_coll, f_hat)\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxy_coll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m         \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m         \u001b[0mu_x_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mxy_coll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mallow_unused\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-cc42467177bc>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, xy)\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinears\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m             \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;31m#a = z1 + self.beta[:,i]*z*z1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_reps = 1\n",
    "\n",
    "train_loss_full = []\n",
    "test_loss_full = []\n",
    "beta_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(reps)\n",
    "    \n",
    "    torch.manual_seed(reps*36)\n",
    "    N_D = 100 #Total number of data points for 'y'\n",
    "    N_N = 3500\n",
    "    N_f = 10000 #Total number of collocation points \n",
    "    \n",
    "    xy_coll_np_array, xy_D_np_array, u_D_np_array,xy_Nx_np_array,xy_Ny_np_array = trainingdata(N_D,N_N,N_f,(reps)*22)\n",
    "        \n",
    "    xy_coll = torch.from_numpy(xy_coll_np_array).float().to(device)\n",
    "    xy_D = torch.from_numpy(xy_D_np_array).float().to(device)\n",
    "    u_D = torch.from_numpy(u_D_np_array).float().to(device)\n",
    "    xy_Nx = torch.from_numpy(xy_Nx_np_array).float().to(device)\n",
    "    xy_Ny = torch.from_numpy(xy_Ny_np_array).float().to(device)\n",
    "        \n",
    "    N_hat = torch.zeros(xy_Nx.shape[0],1).to(device)    \n",
    "    f_hat = torch.zeros(xy_coll.shape[0],1).to(device)\n",
    "\n",
    "    layers = np.array([2,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "    #layers = np.array([3,100,100,100,100,100,100,100,1])\n",
    "\n",
    "    PINN = Sequentialmodel(layers)\n",
    "   \n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "    \n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=1, \n",
    "                              max_iter = 10000, \n",
    "                              max_eval = None, \n",
    "                              tolerance_grad = -1, \n",
    "                              tolerance_change = -1, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "        \n",
    "    optimizer.step(PINN.closure)\n",
    "    \n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(PINN.train_loss)\n",
    "    test_loss_full.append(PINN.test_loss)\n",
    "    elapsed_time[reps] = time.time() - start_time\n",
    "    beta_full.append(PINN.beta_val)\n",
    "\n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "    \n",
    "mdic = {\"train_loss\": train_loss_full,\"test_loss\": test_loss_full,\"Time\": elapsed_time, \"beta\": beta_full, \"label\": label}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred = PINN.test(xy_test_tensor)\n",
    "u_pred_3d = u_pred.reshape(250,250)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAU8AAAD8CAYAAAD35CadAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAM8UlEQVR4nO3cf6jd9X3H8edLU1e6WTuWFWqSVsviaLADXbCOwXToRvSP5I+OkoB0DjG0m2XQMXA4upr+1Y11UMjWXpi4Fqq1/aNcqMWxThGkcaZorVEst6lrYmW21vpPaFX23h/nKHe3N55v3jn3nnOvzwcEzveczz33/clJnjk/7jepKiRJZ+acWQ8gSRuR8ZSkBuMpSQ3GU5IajKckNRhPSWqYGM8kdyR5PskTp7k9ST6bZCnJ40kun/6YkjRfhjzzvBPY8wa3XwfsHP86CPzL2Y8lSfNtYjyr6kHgp2+wZB/whRo5ArwjybumNaAkzaMtU7iPbcCJZccnx9c9t3JhkoOMnp0Cb/ld2DqFby9JXc/9pKp+s/OV04jnYFW1ACwAJBfW6x2VpJm4/b+7XzmNT9ufBXYsO94+vk6SNq1pxHMR+PD4U/crgZeq6pdeskvSZjLxZXuSu4Crga1JTgJ/B7wFoKo+B9wLXA8sAaeAP1urYSVpXkyMZ1UdmHB7AX8xtYkkaQPwDCNJajCektRgPCWpwXhKUoPxlKQG4ylJDcZTkhqMpyQ1GE9JajCektRgPCWpwXhKUoPxlKQG4ylJDcZTkhqMpyQ1GE9JajCektRgPCWpwXhKUoPxlKQG4ylJDcZTkhqMpyQ1GE9JajCektRgPCWpwXhKUoPxlKQG4ylJDcZTkhqMpyQ1GE9JajCektRgPCWpYVA8k+xJ8nSSpSS3rnL7u5Pcn+TRJI8nuX76o0rS/JgYzyTnAoeB64BdwIEku1Ys+1vgnqq6DNgP/PO0B5WkeTLkmecVwFJVHa+ql4G7gX0r1hTw9vHlC4AfTW9ESZo/Wwas2QacWHZ8EvjAijWfBP49yceAXwWuXe2OkhwEDo6OLjizSSVpjkzrA6MDwJ1VtR24Hvhikl+676paqKrdVbUb3jalby1J629IPJ8Fdiw73j6+brmbgHsAqupbwFuBrdMYUJLm0ZB4PgLsTHJxkvMYfSC0uGLND4FrAJK8j1E8fzzNQSVpnkyMZ1W9CtwC3Ac8xehT9WNJDiXZO172V8DNSb4D3AXcWFW1VkNL0qxlVo1LLqzXPzuSpJm4/dujz2DOnGcYSVKD8ZSkBuMpSQ3GU5IajKckNRhPSWownpLUYDwlqcF4SlKD8ZSkBuMpSQ3GU5IajKckNRhPSWownpLUYDwlqcF4SlKD8ZSkBuMpSQ3GU5IajKckNRhPSWownpLUYDwlqcF4SlKD8ZSkBuMpSQ3GU5IajKckNRhPSWownpLUYDwlqcF4SlKD8ZSkhkHxTLInydNJlpLcepo1H0ryZJJjSb403TElab5smbQgybnAYeCPgJPAI0kWq+rJZWt2An8D/H5VvZjknWs1sCTNgyHPPK8AlqrqeFW9DNwN7Fux5mbgcFW9CFBVz093TEmaL0PiuQ04sez45Pi65S4BLknyUJIjSfasdkdJDiY5muQonOpNLElzYOLL9jO4n53A1cB24MEk76+qny1fVFULwAJAcmFN6XtL0rob8szzWWDHsuPt4+uWOwksVtUrVfUD4HuMYipJm9KQeD4C7ExycZLzgP3A4oo1X2P0rJMkWxm9jD8+vTElab5MjGdVvQrcAtwHPAXcU1XHkhxKsne87D7ghSRPAvcDf11VL6zV0JI0a6mazVuPo/c8D87ke0vSyO3frqrdna/0DCNJajCektRgPCWpwXhKUoPxlKQG4ylJDcZTkhqMpyQ1GE9JajCektRgPCWpwXhKUoPxlKQG4ylJDcZTkhqMpyQ1GE9JajCektRgPCWpwXhKUoPxlKQG4ylJDcZTkhqMpyQ1GE9JajCektRgPCWpwXhKUoPxlKQG4ylJDcZTkhqMpyQ1GE9JajCektRgPCWpYVA8k+xJ8nSSpSS3vsG6DyapJLunN6IkzZ+J8UxyLnAYuA7YBRxIsmuVdecDfwk8PO0hJWneDHnmeQWwVFXHq+pl4G5g3yrrPgV8Gvj5FOeTpLk0JJ7bgBPLjk+Or3tdksuBHVX19Te6oyQHkxxNchROnfGwkjQvtpztHSQ5B/gMcOOktVW1ACyMvu7COtvvLUmzMuSZ57PAjmXH28fXveZ84FLggSTPAFcCi35oJGkzGxLPR4CdSS5Och6wH1h87caqeqmqtlbVRVV1EXAE2FtVR9dkYkmaAxPjWVWvArcA9wFPAfdU1bEkh5LsXesBJWkeDXrPs6ruBe5dcd0nTrP26rMfS5Lmm2cYSVKD8ZSkBuMpSQ3GU5IajKckNRhPSWownpLUYDwlqcF4SlKD8ZSkBuMpSQ3GU5IajKckNRhPSWownpLUYDwlqcF4SlKD8ZSkBuMpSQ3GU5IajKckNRhPSWownpLUYDwlqcF4SlKD8ZSkBuMpSQ3GU5IajKckNRhPSWownpLUYDwlqcF4SlKD8ZSkhkHxTLInydNJlpLcusrtH0/yZJLHk3wzyXumP6okzY+J8UxyLnAYuA7YBRxIsmvFskeB3VX1O8BXgb+f9qCSNE+GPPO8AliqquNV9TJwN7Bv+YKqur+qTo0PjwDbpzumJM2XIfHcBpxYdnxyfN3p3AR8Y7UbkhxMcjTJUTi12hJJ2hC2TPPOktwA7AauWu32qloAFkZrL6xpfm9JWk9D4vkssGPZ8fbxdf9PkmuB24CrquoX0xlPkubTkJftjwA7k1yc5DxgP7C4fEGSy4DPA3ur6vnpjylJ82ViPKvqVeAW4D7gKeCeqjqW5FCSveNl/wD8GvCVJI8lWTzN3UnSpjDoPc+quhe4d8V1n1h2+dopzyVJc80zjCSpwXhKUoPxlKQG4ylJDcZTkhqMpyQ1GE9JajCektRgPCWpwXhKUoPxlKQG4ylJDcZTkhqMpyQ1GE9JajCektRgPCWpwXhKUoPxlKQG4ylJDcZTkhqMpyQ1GE9JajCektRgPCWpwXhKUoPxlKQG4ylJDcZTkhqMpyQ1GE9JajCektRgPCWpwXhKUoPxlKSGQfFMsifJ00mWkty6yu2/kuTL49sfTnLR1CeVpDkyMZ5JzgUOA9cBu4ADSXatWHYT8GJV/RbwT8Cnpz2oJM2TIc88rwCWqup4Vb0M3A3sW7FmH/Bv48tfBa5JkumNKUnzZcuANduAE8uOTwIfON2aqno1yUvAbwA/Wb4oyUHg4PjwF3D7E52hN5itrPh92KTc5+byZtnnb3e/cEg8p6aqFoAFgCRHq2r3en7/WXCfm4v73FySHO1+7ZCX7c8CO5Ydbx9ft+qaJFuAC4AXukNJ0rwbEs9HgJ1JLk5yHrAfWFyxZhH40/HlPwH+s6pqemNK0nyZ+LJ9/B7mLcB9wLnAHVV1LMkh4GhVLQL/CnwxyRLwU0aBnWThLObeSNzn5uI+N5f2PuMTREk6c55hJEkNxlOSGtY8nm+WUzsH7PPjSZ5M8niSbyZ5zyzmPFuT9rls3QeTVJIN+eMuQ/aZ5EPjx/RYki+t94zTMODP7buT3J/k0fGf3etnMefZSHJHkueTrPpz5Rn57Pj34PEklw+646pas1+MPmD6PvBe4DzgO8CuFWv+HPjc+PJ+4MtrOdMM9/mHwNvGlz+6Wfc5Xnc+8CBwBNg967nX6PHcCTwK/Pr4+J2znnuN9rkAfHR8eRfwzKznbuzzD4DLgSdOc/v1wDeAAFcCDw+537V+5vlmObVz4j6r6v6qOjU+PMLo52U3miGPJ8CnGP3/Bj9fz+GmaMg+bwYOV9WLAFX1/DrPOA1D9lnA28eXLwB+tI7zTUVVPcjop4BOZx/whRo5Arwjybsm3e9ax3O1Uzu3nW5NVb0KvHZq50YyZJ/L3cToX7qNZuI+xy95dlTV19dzsCkb8nheAlyS5KEkR5LsWbfppmfIPj8J3JDkJHAv8LH1GW1dnenfX2CdT88UJLkB2A1cNetZpi3JOcBngBtnPMp62MLopfvVjF5FPJjk/VX1s1kOtQYOAHdW1T8m+T1GP899aVX976wHm7W1fub5Zjm1c8g+SXItcBuwt6p+sU6zTdOkfZ4PXAo8kOQZRu8fLW7AD42GPJ4ngcWqeqWqfgB8j1FMN5Ih+7wJuAegqr4FvJXRfxqymQz6+7vSWsfzzXJq58R9JrkM+DyjcG7E98dgwj6r6qWq2lpVF1XVRYze291bVe3/fGFGhvy5/RqjZ50k2croZfzxdZxxGobs84fANQBJ3sconj9e1ynX3iLw4fGn7lcCL1XVcxO/ah0+6bqe0b/K3wduG193iNFfKhg9GF8BloD/At4760/n1mif/wH8D/DY+NfirGdei32uWPsAG/DT9oGPZxi9RfEk8F1g/6xnXqN97gIeYvRJ/GPAH8965sYe7wKeA15h9IrhJuAjwEeWPZaHx78H3x36Z9bTMyWpwTOMJKnBeEpSg/GUpAbjKUkNxlOSGoynJDUYT0lq+D8zRJ2heMQjwQAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "cmap = plt.cm.jet\n",
    "\n",
    "img3 = ax.imshow(np.transpose(1000*u_pred_3d),vmax =1000,vmin=450,cmap = cmap,extent=[0,1,0,1],aspect = 0.75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.24514565"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(u_pred_3d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred_3d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + test_loss_full[i][-1]\n",
    "print(a/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + train_loss_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
