{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def true_2D_1(xt): #True function for 2D_1 Klein Gordon Equation x \\in [-50,50] , t \\in [0,10]\n",
    "#     y = xt[:,0]*np.cos(xt[:,1])\n",
    "#     return y.reshape(-1,1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"QCRE_2D_1_atanh_NW_new\"\n",
    "\n",
    "x = np.linspace(0,1,500).reshape(-1,1)\n",
    "y = np.linspace(0,1,500).reshape(-1,1)\n",
    "\n",
    "X,Y = np.meshgrid(x,y)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "Y = Y.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xy = np.hstack((X,Y))\n",
    "\n",
    "bound_pts_1 = (X == 0).reshape(-1,)\n",
    "bound_pts_2 = (Y == 0).reshape(-1,)\n",
    "bound_pts_3 = (X == 1).reshape(-1,)\n",
    "bound_pts_4 = (Y == 1).reshape(-1,)\n",
    "\n",
    "xy_bound_1 = xy[bound_pts_1,:]\n",
    "xy_bound_2 = xy[bound_pts_2,:]\n",
    "xy_bound_3 = xy[bound_pts_3,:]\n",
    "xy_bound_4 = xy[bound_pts_4,:]\n",
    "\n",
    "\n",
    "u_bound_1 = 1000*np.ones((np.shape(xy_bound_1)[0],1))\n",
    "u_bound_2 = 800*np.ones((np.shape(xy_bound_2)[0],1))\n",
    "u_bound_3 = 500*np.ones((np.shape(xy_bound_3)[0],1))\n",
    "u_bound_4 = np.zeros((np.shape(xy_bound_4)[0],1))\n",
    "\n",
    "xy_bound = np.vstack((xy_bound_1,xy_bound_2,xy_bound_3,xy_bound_4))\n",
    "u_bound = np.vstack((u_bound_1,u_bound_2,u_bound_3,u_bound_4))\n",
    "\n",
    "xy_test_tensor = torch.from_numpy(xy).float().to(device)\n",
    "\n",
    "lb_xy = xy[0]\n",
    "ub_xy = xy[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1., 1.])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ub_xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_T,N_f,seed):\n",
    "    '''Boundary Conditions''' \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    #choose random N_u points for training\n",
    "    idx = np.random.choice(xy_bound.shape[0], N_T, replace=False) \n",
    "    xy_BC = xy_bound[idx,:] #choose indices from  set 'idx' (x,t)\n",
    "    u_BC = u_bound[idx].reshape(-1,1)      #choose corresponding u\n",
    "\n",
    "\n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    samples = sampling(N_f)\n",
    "    \n",
    "    xy_coll = lb_xy + (ub_xy - lb_xy)*samples\n",
    "    \n",
    "    xy_coll = np.vstack((xy_coll, xy_BC)) # append training points to collocation points \n",
    "\n",
    "    return xy_coll, xy_BC, u_BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        self.iter = 0\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "            \n",
    "        \n",
    "        self.alpha = Parameter(torch.ones((50,len(layers)-2)))\n",
    "        self.alpha.requiresGrad = True\n",
    "        \n",
    "        self.train_loss = []\n",
    "        self.test_loss = []\n",
    "        \n",
    "        self.alpha_val = []\n",
    "        \n",
    "        self.n = torch.tensor(1.0)\n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xy):\n",
    "        if torch.is_tensor(xy) != True:         \n",
    "            xy = torch.from_numpy(xy)                \n",
    "        \n",
    "        ubxy = torch.from_numpy(ub_xy).float().to(device)\n",
    "        lbxy = torch.from_numpy(lb_xy).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xy = (xy - lbxy)/(ubxy - lbxy)\n",
    "        \n",
    "        #convert to float\n",
    "        a = xy.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(self.n*self.alpha[:,i]*z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC(self,xy,u):\n",
    "                \n",
    "        loss_bc = self.loss_function(self.forward(xy), u)\n",
    "                \n",
    "        return loss_bc\n",
    "    \n",
    "    def loss_PDE(self, xy_coll, f_hat):\n",
    "        \n",
    "        g = xy_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_y = autograd.grad(u,g,torch.ones([xy_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_xx_yy = autograd.grad(u_x_y,g,torch.ones(xy_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        #du_dx = u_x_t[:,[0]]\n",
    "        \n",
    "        d2u_dx2 = u_xx_yy[:,[0]]\n",
    "        d2u_dy2 = u_xx_yy[:,[1]]    \n",
    "        \n",
    "\n",
    "        f = d2u_dx2 + d2u_dy2\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xy_BC,u_BC,xy_coll,f_hat):\n",
    "\n",
    "        loss_BC = self.loss_BC(xy_BC,u_BC)\n",
    "        loss_f = self.loss_PDE(xy_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_BC + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "     \n",
    "    'callable for optimizer'                                       \n",
    "    def closure(self):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = self.loss(xy_BC, u_BC, xy_coll,f_hat)\n",
    "        self.train_loss.append(loss.cpu().detach().numpy())\n",
    "        \n",
    "        u_pred = self.test(xy_test_tensor)\n",
    "        #self.test_loss.append(np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))) #Commented because no true values yet\n",
    "        self.alpha_val.append(self.alpha.cpu().detach().numpy())\n",
    "        \n",
    "        #print(self.iter,\"Train Loss\",self.train_loss[-1],\"Test Loss\",self.test_loss[-1])\n",
    "        print(self.iter,\"Train Loss\",self.train_loss[-1])\n",
    "        \n",
    "        loss.backward()\n",
    "        self.iter += 1\n",
    "  \n",
    "\n",
    "        return loss        \n",
    "    'test neural network'\n",
    "    def test(self,xy_test_tensor):\n",
    "        u_pred = self.forward(xy_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0 Train Loss 520012.8\n",
      "1 Train Loss 519937.5\n",
      "2 Train Loss 519287.62\n",
      "3 Train Loss 514351.16\n",
      "4 Train Loss 505620.4\n",
      "5 Train Loss 475983.5\n",
      "6 Train Loss 255324.78\n",
      "7 Train Loss 249605.4\n",
      "8 Train Loss 249687.34\n",
      "9 Train Loss 249781.75\n",
      "10 Train Loss 249632.42\n",
      "11 Train Loss 249600.05\n",
      "12 Train Loss 249600.02\n",
      "13 Train Loss 249600.02\n",
      "14 Train Loss 249600.02\n",
      "15 Train Loss 249600.02\n",
      "16 Train Loss 249600.02\n",
      "17 Train Loss 249600.02\n",
      "18 Train Loss 249600.02\n",
      "19 Train Loss 249600.02\n",
      "20 Train Loss 249600.02\n",
      "21 Train Loss 249600.02\n",
      "22 Train Loss 249600.02\n",
      "23 Train Loss 249600.02\n",
      "24 Train Loss 249600.02\n",
      "25 Train Loss 249600.02\n",
      "26 Train Loss 249600.02\n",
      "27 Train Loss 249600.02\n",
      "28 Train Loss 249600.02\n",
      "29 Train Loss 249600.02\n",
      "30 Train Loss 249600.02\n",
      "31 Train Loss 249600.02\n",
      "32 Train Loss 249600.02\n",
      "33 Train Loss 249600.02\n",
      "34 Train Loss 249600.02\n",
      "35 Train Loss 249600.02\n",
      "36 Train Loss 249600.02\n",
      "37 Train Loss 249600.02\n",
      "38 Train Loss 249600.02\n",
      "39 Train Loss 249600.02\n",
      "40 Train Loss 249600.02\n",
      "41 Train Loss 249600.36\n",
      "42 Train Loss 249600.0\n",
      "43 Train Loss 249600.0\n",
      "44 Train Loss 249600.02\n",
      "45 Train Loss 249600.0\n",
      "46 Train Loss 249600.0\n",
      "47 Train Loss 249600.0\n",
      "48 Train Loss 249600.0\n",
      "49 Train Loss 249600.0\n",
      "50 Train Loss 249600.02\n",
      "51 Train Loss 249600.03\n",
      "52 Train Loss 249600.02\n",
      "53 Train Loss 249600.0\n",
      "54 Train Loss 249600.0\n",
      "55 Train Loss 249600.0\n",
      "56 Train Loss 249600.0\n",
      "57 Train Loss 249600.02\n",
      "58 Train Loss 249600.03\n",
      "59 Train Loss 249600.02\n",
      "60 Train Loss 249600.0\n",
      "61 Train Loss 249600.0\n",
      "62 Train Loss 249600.0\n",
      "63 Train Loss 249600.0\n",
      "64 Train Loss 249600.02\n",
      "65 Train Loss 249600.03\n",
      "66 Train Loss 249600.02\n",
      "67 Train Loss 249600.0\n",
      "68 Train Loss 249600.0\n",
      "69 Train Loss 249600.0\n",
      "70 Train Loss 249600.0\n",
      "71 Train Loss 249600.02\n",
      "72 Train Loss 249600.03\n",
      "73 Train Loss 249600.02\n",
      "74 Train Loss 249600.0\n",
      "75 Train Loss 249600.0\n",
      "76 Train Loss 249600.0\n",
      "77 Train Loss 249600.0\n",
      "78 Train Loss 249600.02\n",
      "79 Train Loss 249600.03\n",
      "80 Train Loss 249600.02\n",
      "81 Train Loss 249600.0\n",
      "82 Train Loss 249600.0\n",
      "83 Train Loss 249600.0\n",
      "84 Train Loss 249600.0\n",
      "85 Train Loss 249600.02\n",
      "86 Train Loss 249600.03\n",
      "87 Train Loss 249600.02\n",
      "88 Train Loss 249600.0\n",
      "89 Train Loss 249600.0\n",
      "90 Train Loss 249600.0\n",
      "91 Train Loss 249600.0\n",
      "92 Train Loss 249600.02\n",
      "93 Train Loss 249600.03\n",
      "94 Train Loss 249600.02\n",
      "95 Train Loss 249600.0\n",
      "96 Train Loss 249600.0\n",
      "97 Train Loss 249600.0\n",
      "98 Train Loss 249600.0\n",
      "99 Train Loss 249600.02\n",
      "100 Train Loss 249600.03\n",
      "101 Train Loss 249600.02\n",
      "102 Train Loss 249600.0\n",
      "103 Train Loss 249600.0\n",
      "104 Train Loss 249600.0\n",
      "105 Train Loss 249600.0\n",
      "106 Train Loss 249600.02\n",
      "107 Train Loss 249600.03\n",
      "108 Train Loss 249600.02\n",
      "109 Train Loss 249600.0\n",
      "110 Train Loss 249600.0\n",
      "111 Train Loss 249600.0\n",
      "112 Train Loss 249600.0\n",
      "113 Train Loss 249600.02\n",
      "114 Train Loss 249600.03\n",
      "115 Train Loss 249600.02\n",
      "116 Train Loss 249600.0\n",
      "117 Train Loss 249600.0\n",
      "118 Train Loss 249600.0\n",
      "119 Train Loss 249600.0\n",
      "120 Train Loss 249600.02\n",
      "121 Train Loss 249600.03\n",
      "122 Train Loss 249600.02\n",
      "123 Train Loss 249600.0\n",
      "124 Train Loss 249600.0\n",
      "125 Train Loss 249600.0\n",
      "126 Train Loss 249600.0\n",
      "127 Train Loss 249600.02\n",
      "128 Train Loss 249600.03\n",
      "129 Train Loss 249600.02\n",
      "130 Train Loss 249600.0\n",
      "131 Train Loss 249600.0\n",
      "132 Train Loss 249600.0\n",
      "133 Train Loss 249600.0\n",
      "134 Train Loss 249600.02\n",
      "135 Train Loss 249600.03\n",
      "136 Train Loss 249600.02\n",
      "137 Train Loss 249600.0\n",
      "138 Train Loss 249600.0\n",
      "139 Train Loss 249600.0\n",
      "140 Train Loss 249600.0\n",
      "141 Train Loss 249600.02\n",
      "142 Train Loss 249600.03\n",
      "143 Train Loss 249600.02\n",
      "144 Train Loss 249600.0\n",
      "145 Train Loss 249600.0\n",
      "146 Train Loss 249600.0\n",
      "147 Train Loss 249600.0\n",
      "148 Train Loss 249600.02\n",
      "149 Train Loss 249600.03\n",
      "150 Train Loss 249600.02\n",
      "151 Train Loss 249600.0\n",
      "152 Train Loss 249600.0\n",
      "153 Train Loss 249600.0\n",
      "154 Train Loss 249600.0\n",
      "155 Train Loss 249600.02\n",
      "156 Train Loss 249600.03\n",
      "157 Train Loss 249600.02\n",
      "158 Train Loss 249600.0\n",
      "159 Train Loss 249600.0\n",
      "160 Train Loss 249600.0\n",
      "161 Train Loss 249600.0\n",
      "162 Train Loss 249600.02\n",
      "163 Train Loss 249600.03\n",
      "164 Train Loss 249600.02\n",
      "165 Train Loss 249600.0\n",
      "166 Train Loss 249600.0\n",
      "167 Train Loss 249600.0\n",
      "168 Train Loss 249600.0\n",
      "169 Train Loss 249600.02\n",
      "170 Train Loss 249600.03\n",
      "171 Train Loss 249600.02\n",
      "172 Train Loss 249600.0\n",
      "173 Train Loss 249600.0\n",
      "174 Train Loss 249600.0\n",
      "175 Train Loss 249600.0\n",
      "176 Train Loss 249600.02\n",
      "177 Train Loss 249600.03\n",
      "178 Train Loss 249600.02\n",
      "179 Train Loss 249600.0\n",
      "180 Train Loss 249600.0\n",
      "181 Train Loss 249600.0\n",
      "182 Train Loss 249600.0\n",
      "183 Train Loss 249600.02\n",
      "184 Train Loss 249600.03\n",
      "185 Train Loss 249600.02\n",
      "186 Train Loss 249600.0\n",
      "187 Train Loss 249600.0\n",
      "188 Train Loss 249600.0\n",
      "189 Train Loss 249600.0\n",
      "190 Train Loss 249600.02\n",
      "191 Train Loss 249600.03\n",
      "192 Train Loss 249600.02\n",
      "193 Train Loss 249600.0\n",
      "194 Train Loss 249600.0\n",
      "195 Train Loss 249600.0\n",
      "196 Train Loss 249600.0\n",
      "197 Train Loss 249600.02\n",
      "198 Train Loss 249600.03\n",
      "199 Train Loss 249600.02\n",
      "200 Train Loss 249600.0\n",
      "201 Train Loss 249600.0\n",
      "202 Train Loss 249600.0\n",
      "203 Train Loss 249600.0\n",
      "204 Train Loss 249600.02\n",
      "205 Train Loss 249600.03\n",
      "206 Train Loss 249600.02\n",
      "207 Train Loss 249600.0\n",
      "208 Train Loss 249600.0\n",
      "209 Train Loss 249600.0\n",
      "210 Train Loss 249600.0\n",
      "211 Train Loss 249600.02\n",
      "212 Train Loss 249600.03\n",
      "213 Train Loss 249600.02\n",
      "214 Train Loss 249600.0\n",
      "215 Train Loss 249600.0\n",
      "216 Train Loss 249600.0\n",
      "217 Train Loss 249600.0\n",
      "218 Train Loss 249600.02\n",
      "219 Train Loss 249600.03\n",
      "220 Train Loss 249600.02\n",
      "221 Train Loss 249600.0\n",
      "222 Train Loss 249600.0\n",
      "223 Train Loss 249600.0\n",
      "224 Train Loss 249600.0\n",
      "225 Train Loss 249600.02\n",
      "226 Train Loss 249600.03\n",
      "227 Train Loss 249600.02\n",
      "228 Train Loss 249600.0\n",
      "229 Train Loss 249600.0\n",
      "230 Train Loss 249600.0\n",
      "231 Train Loss 249600.0\n",
      "232 Train Loss 249600.02\n",
      "233 Train Loss 249600.03\n",
      "234 Train Loss 249600.02\n",
      "235 Train Loss 249600.0\n",
      "236 Train Loss 249600.0\n",
      "237 Train Loss 249600.0\n",
      "238 Train Loss 249600.0\n",
      "239 Train Loss 249600.02\n",
      "240 Train Loss 249600.03\n",
      "241 Train Loss 249600.02\n",
      "242 Train Loss 249600.0\n",
      "243 Train Loss 249600.0\n",
      "244 Train Loss 249600.0\n",
      "245 Train Loss 249600.0\n",
      "246 Train Loss 249600.02\n",
      "247 Train Loss 249600.03\n",
      "248 Train Loss 249600.02\n",
      "249 Train Loss 249600.0\n",
      "250 Train Loss 249600.0\n",
      "251 Train Loss 249600.0\n",
      "252 Train Loss 249600.0\n",
      "253 Train Loss 249600.02\n",
      "254 Train Loss 249600.03\n",
      "255 Train Loss 249600.02\n",
      "256 Train Loss 249600.0\n",
      "257 Train Loss 249600.0\n",
      "258 Train Loss 249600.0\n",
      "259 Train Loss 249600.0\n",
      "260 Train Loss 249600.02\n",
      "261 Train Loss 249600.03\n",
      "262 Train Loss 249600.02\n",
      "263 Train Loss 249600.0\n",
      "264 Train Loss 249600.0\n",
      "265 Train Loss 249600.0\n",
      "266 Train Loss 249600.0\n",
      "267 Train Loss 249600.02\n",
      "268 Train Loss 249600.03\n",
      "269 Train Loss 249600.02\n",
      "270 Train Loss 249600.0\n",
      "271 Train Loss 249600.0\n",
      "272 Train Loss 249600.0\n",
      "273 Train Loss 249600.0\n",
      "274 Train Loss 249600.02\n",
      "275 Train Loss 249600.03\n",
      "276 Train Loss 249600.02\n",
      "277 Train Loss 249600.0\n",
      "278 Train Loss 249600.0\n",
      "279 Train Loss 249600.0\n",
      "280 Train Loss 249600.0\n",
      "281 Train Loss 249600.02\n",
      "282 Train Loss 249600.03\n",
      "283 Train Loss 249600.02\n",
      "284 Train Loss 249600.0\n",
      "285 Train Loss 249600.0\n",
      "286 Train Loss 249600.0\n",
      "287 Train Loss 249600.0\n",
      "288 Train Loss 249600.02\n",
      "289 Train Loss 249600.03\n",
      "290 Train Loss 249600.02\n",
      "291 Train Loss 249600.0\n",
      "292 Train Loss 249600.0\n",
      "293 Train Loss 249600.0\n",
      "294 Train Loss 249600.0\n",
      "295 Train Loss 249600.02\n",
      "296 Train Loss 249600.03\n",
      "297 Train Loss 249600.02\n",
      "298 Train Loss 249600.0\n",
      "299 Train Loss 249600.0\n",
      "300 Train Loss 249600.0\n",
      "301 Train Loss 249600.0\n",
      "302 Train Loss 249600.02\n",
      "303 Train Loss 249600.03\n",
      "304 Train Loss 249600.02\n",
      "305 Train Loss 249600.0\n",
      "306 Train Loss 249600.0\n",
      "307 Train Loss 249600.0\n",
      "308 Train Loss 249600.0\n",
      "309 Train Loss 249600.02\n",
      "310 Train Loss 249600.03\n",
      "311 Train Loss 249600.02\n",
      "312 Train Loss 249600.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "313 Train Loss 249600.0\n",
      "314 Train Loss 249600.0\n",
      "315 Train Loss 249600.0\n",
      "316 Train Loss 249600.02\n",
      "317 Train Loss 249600.03\n",
      "318 Train Loss 249600.02\n",
      "319 Train Loss 249600.0\n",
      "320 Train Loss 249600.0\n",
      "321 Train Loss 249600.0\n",
      "322 Train Loss 249600.0\n",
      "323 Train Loss 249600.02\n",
      "324 Train Loss 249600.03\n",
      "325 Train Loss 249600.02\n",
      "326 Train Loss 249600.0\n",
      "327 Train Loss 249600.0\n",
      "328 Train Loss 249600.0\n",
      "329 Train Loss 249600.0\n",
      "330 Train Loss 249600.02\n",
      "331 Train Loss 249600.03\n",
      "332 Train Loss 249600.02\n",
      "333 Train Loss 249600.0\n",
      "334 Train Loss 249600.0\n",
      "335 Train Loss 249600.0\n",
      "336 Train Loss 249600.0\n",
      "337 Train Loss 249600.02\n",
      "338 Train Loss 249600.03\n",
      "339 Train Loss 249600.02\n",
      "340 Train Loss 249600.0\n",
      "341 Train Loss 249600.0\n",
      "342 Train Loss 249600.0\n",
      "343 Train Loss 249600.0\n",
      "344 Train Loss 249600.02\n",
      "345 Train Loss 249600.03\n",
      "346 Train Loss 249600.02\n",
      "347 Train Loss 249600.0\n",
      "348 Train Loss 249600.0\n",
      "349 Train Loss 249600.0\n",
      "350 Train Loss 249600.0\n",
      "351 Train Loss 249600.02\n",
      "352 Train Loss 249600.03\n",
      "353 Train Loss 249600.02\n",
      "354 Train Loss 249600.0\n",
      "355 Train Loss 249600.0\n",
      "356 Train Loss 249600.0\n",
      "357 Train Loss 249600.0\n",
      "358 Train Loss 249600.02\n",
      "359 Train Loss 249600.03\n",
      "360 Train Loss 249600.02\n",
      "361 Train Loss 249600.0\n",
      "362 Train Loss 249600.0\n",
      "363 Train Loss 249600.0\n",
      "364 Train Loss 249600.0\n",
      "365 Train Loss 249600.02\n",
      "366 Train Loss 249600.03\n",
      "367 Train Loss 249600.02\n",
      "368 Train Loss 249600.0\n",
      "369 Train Loss 249600.0\n",
      "370 Train Loss 249600.0\n",
      "371 Train Loss 249600.0\n",
      "372 Train Loss 249600.02\n",
      "373 Train Loss 249600.03\n",
      "374 Train Loss 249600.02\n",
      "375 Train Loss 249600.0\n",
      "376 Train Loss 249600.0\n",
      "377 Train Loss 249600.0\n",
      "378 Train Loss 249600.0\n",
      "379 Train Loss 249600.02\n",
      "380 Train Loss 249600.03\n",
      "381 Train Loss 249600.02\n",
      "382 Train Loss 249600.0\n",
      "383 Train Loss 249600.0\n",
      "384 Train Loss 249600.0\n",
      "385 Train Loss 249600.0\n",
      "386 Train Loss 249600.02\n",
      "387 Train Loss 249600.03\n",
      "388 Train Loss 249600.02\n",
      "389 Train Loss 249600.0\n",
      "390 Train Loss 249600.0\n",
      "391 Train Loss 249600.0\n",
      "392 Train Loss 249600.0\n",
      "393 Train Loss 249600.02\n",
      "394 Train Loss 249600.03\n",
      "395 Train Loss 249600.02\n",
      "396 Train Loss 249600.0\n",
      "397 Train Loss 249600.0\n",
      "398 Train Loss 249600.0\n",
      "399 Train Loss 249600.0\n",
      "400 Train Loss 249600.02\n",
      "401 Train Loss 249600.03\n",
      "402 Train Loss 249600.02\n",
      "403 Train Loss 249600.0\n",
      "404 Train Loss 249600.0\n",
      "405 Train Loss 249600.0\n",
      "406 Train Loss 249600.0\n",
      "407 Train Loss 249600.02\n",
      "408 Train Loss 249600.03\n",
      "409 Train Loss 249600.02\n",
      "410 Train Loss 249600.0\n",
      "411 Train Loss 249600.0\n",
      "412 Train Loss 249600.0\n",
      "413 Train Loss 249600.0\n",
      "414 Train Loss 249600.02\n",
      "415 Train Loss 249600.03\n",
      "416 Train Loss 249600.02\n",
      "417 Train Loss 249600.0\n",
      "418 Train Loss 249600.0\n",
      "419 Train Loss 249600.0\n",
      "420 Train Loss 249600.0\n",
      "421 Train Loss 249600.02\n",
      "422 Train Loss 249600.03\n",
      "423 Train Loss 249600.02\n",
      "424 Train Loss 249600.0\n",
      "425 Train Loss 249600.0\n",
      "426 Train Loss 249600.0\n",
      "427 Train Loss 249600.0\n",
      "428 Train Loss 249600.02\n",
      "429 Train Loss 249600.03\n",
      "430 Train Loss 249600.02\n",
      "431 Train Loss 249600.0\n",
      "432 Train Loss 249600.0\n",
      "433 Train Loss 249600.0\n",
      "434 Train Loss 249600.0\n",
      "435 Train Loss 249600.02\n",
      "436 Train Loss 249600.03\n",
      "437 Train Loss 249600.02\n",
      "438 Train Loss 249600.0\n",
      "439 Train Loss 249600.0\n",
      "440 Train Loss 249600.0\n",
      "441 Train Loss 249600.0\n",
      "442 Train Loss 249600.02\n",
      "443 Train Loss 249600.03\n",
      "444 Train Loss 249600.02\n",
      "445 Train Loss 249600.0\n",
      "446 Train Loss 249600.0\n",
      "447 Train Loss 249600.0\n",
      "448 Train Loss 249600.0\n",
      "449 Train Loss 249600.02\n",
      "450 Train Loss 249600.03\n",
      "451 Train Loss 249600.02\n",
      "452 Train Loss 249600.0\n",
      "453 Train Loss 249600.0\n",
      "454 Train Loss 249600.0\n",
      "455 Train Loss 249600.0\n",
      "456 Train Loss 249600.02\n",
      "457 Train Loss 249600.03\n",
      "458 Train Loss 249600.02\n",
      "459 Train Loss 249600.0\n",
      "460 Train Loss 249600.0\n",
      "461 Train Loss 249600.0\n",
      "462 Train Loss 249600.0\n",
      "463 Train Loss 249600.02\n",
      "464 Train Loss 249600.03\n",
      "465 Train Loss 249600.02\n",
      "466 Train Loss 249600.0\n",
      "467 Train Loss 249600.0\n",
      "468 Train Loss 249600.0\n",
      "469 Train Loss 249600.0\n",
      "470 Train Loss 249600.02\n",
      "471 Train Loss 249600.03\n",
      "472 Train Loss 249600.02\n",
      "473 Train Loss 249600.0\n",
      "474 Train Loss 249600.0\n",
      "475 Train Loss 249600.0\n",
      "476 Train Loss 249600.0\n",
      "477 Train Loss 249600.02\n",
      "478 Train Loss 249600.03\n",
      "479 Train Loss 249600.02\n",
      "480 Train Loss 249600.0\n",
      "481 Train Loss 249600.0\n",
      "482 Train Loss 249600.0\n",
      "483 Train Loss 249600.0\n",
      "484 Train Loss 249600.02\n",
      "485 Train Loss 249600.03\n",
      "486 Train Loss 249600.02\n",
      "487 Train Loss 249600.0\n",
      "488 Train Loss 249600.0\n",
      "489 Train Loss 249600.0\n",
      "490 Train Loss 249600.0\n",
      "491 Train Loss 249600.02\n",
      "492 Train Loss 249600.03\n",
      "493 Train Loss 249600.02\n",
      "494 Train Loss 249600.0\n",
      "495 Train Loss 249600.0\n",
      "496 Train Loss 249600.0\n",
      "497 Train Loss 249600.0\n",
      "498 Train Loss 249600.02\n",
      "499 Train Loss 249600.03\n",
      "500 Train Loss 249600.02\n",
      "501 Train Loss 249600.0\n",
      "502 Train Loss 249600.0\n",
      "503 Train Loss 249600.0\n",
      "504 Train Loss 249600.0\n",
      "505 Train Loss 249600.02\n",
      "506 Train Loss 249600.03\n",
      "507 Train Loss 249600.02\n",
      "508 Train Loss 249600.0\n",
      "509 Train Loss 249600.0\n",
      "510 Train Loss 249600.0\n",
      "511 Train Loss 249600.0\n",
      "512 Train Loss 249600.02\n",
      "513 Train Loss 249600.03\n",
      "514 Train Loss 249600.02\n",
      "515 Train Loss 249600.0\n",
      "516 Train Loss 249600.0\n",
      "517 Train Loss 249600.0\n",
      "518 Train Loss 249600.0\n",
      "519 Train Loss 249600.02\n",
      "520 Train Loss 249600.03\n",
      "521 Train Loss 249600.02\n",
      "522 Train Loss 249600.0\n",
      "523 Train Loss 249600.0\n",
      "524 Train Loss 249600.0\n",
      "525 Train Loss 249600.0\n",
      "526 Train Loss 249600.02\n",
      "527 Train Loss 249600.03\n",
      "528 Train Loss 249600.02\n",
      "529 Train Loss 249600.0\n",
      "530 Train Loss 249600.0\n",
      "531 Train Loss 249600.0\n",
      "532 Train Loss 249600.0\n",
      "533 Train Loss 249600.02\n",
      "534 Train Loss 249600.03\n",
      "535 Train Loss 249600.02\n",
      "536 Train Loss 249600.0\n",
      "537 Train Loss 249600.0\n",
      "538 Train Loss 249600.0\n",
      "539 Train Loss 249600.0\n",
      "540 Train Loss 249600.02\n",
      "541 Train Loss 249600.03\n",
      "542 Train Loss 249600.02\n",
      "543 Train Loss 249600.0\n",
      "544 Train Loss 249600.0\n",
      "545 Train Loss 249600.0\n",
      "546 Train Loss 249600.0\n",
      "547 Train Loss 249600.02\n",
      "548 Train Loss 249600.03\n",
      "549 Train Loss 249600.02\n",
      "550 Train Loss 249600.0\n",
      "551 Train Loss 249600.0\n",
      "552 Train Loss 249600.0\n",
      "553 Train Loss 249600.0\n",
      "554 Train Loss 249600.02\n",
      "555 Train Loss 249600.03\n",
      "556 Train Loss 249600.02\n",
      "557 Train Loss 249600.0\n",
      "558 Train Loss 249600.0\n",
      "559 Train Loss 249600.0\n",
      "560 Train Loss 249600.0\n",
      "561 Train Loss 249600.02\n",
      "562 Train Loss 249600.03\n",
      "563 Train Loss 249600.02\n",
      "564 Train Loss 249600.0\n",
      "565 Train Loss 249600.0\n",
      "566 Train Loss 249600.0\n",
      "567 Train Loss 249600.0\n",
      "568 Train Loss 249600.02\n",
      "569 Train Loss 249600.03\n",
      "570 Train Loss 249600.02\n",
      "571 Train Loss 249600.0\n",
      "572 Train Loss 249600.0\n",
      "573 Train Loss 249600.0\n",
      "574 Train Loss 249600.0\n",
      "575 Train Loss 249600.02\n",
      "576 Train Loss 249600.03\n",
      "577 Train Loss 249600.02\n",
      "578 Train Loss 249600.0\n",
      "579 Train Loss 249600.0\n",
      "580 Train Loss 249600.0\n",
      "581 Train Loss 249600.0\n",
      "582 Train Loss 249600.02\n",
      "583 Train Loss 249600.03\n",
      "584 Train Loss 249600.02\n",
      "585 Train Loss 249600.0\n",
      "586 Train Loss 249600.0\n",
      "587 Train Loss 249600.0\n",
      "588 Train Loss 249600.0\n",
      "589 Train Loss 249600.02\n",
      "590 Train Loss 249600.03\n",
      "591 Train Loss 249600.02\n",
      "592 Train Loss 249600.0\n",
      "593 Train Loss 249600.0\n",
      "594 Train Loss 249600.0\n",
      "595 Train Loss 249600.0\n",
      "596 Train Loss 249600.02\n",
      "597 Train Loss 249600.03\n",
      "598 Train Loss 249600.02\n",
      "599 Train Loss 249600.0\n",
      "600 Train Loss 249600.0\n",
      "601 Train Loss 249600.0\n",
      "602 Train Loss 249600.0\n",
      "603 Train Loss 249600.02\n",
      "604 Train Loss 249600.03\n",
      "605 Train Loss 249600.02\n",
      "606 Train Loss 249600.0\n",
      "607 Train Loss 249600.0\n",
      "608 Train Loss 249600.0\n",
      "609 Train Loss 249600.0\n",
      "610 Train Loss 249600.02\n",
      "611 Train Loss 249600.03\n",
      "612 Train Loss 249600.02\n",
      "613 Train Loss 249600.0\n",
      "614 Train Loss 249600.0\n",
      "615 Train Loss 249600.0\n",
      "616 Train Loss 249600.0\n",
      "617 Train Loss 249600.02\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-26-de506b19c61b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPINN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPINN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                     loss, flat_grad, t, ls_func_evals = _strong_wolfe(\n\u001b[0;32m--> 426\u001b[0;31m                         obj_func, x_init, t, d, loss, flat_grad, gtd)\n\u001b[0m\u001b[1;32m    427\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0mopt_cond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflat_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mtolerance_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36m_strong_wolfe\u001b[0;34m(obj_func, x, t, d, f, g, gtd, c1, c2, tolerance_change, max_ls)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# evaluate objective and gradient using initial step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mf_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mls_func_evals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mgtd_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mobj_func\u001b[0;34m(x, t, d)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m                     \u001b[0;32mdef\u001b[0m \u001b[0mobj_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m                         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_directional_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                     loss, flat_grad, t, ls_func_evals = _strong_wolfe(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36m_directional_evaluate\u001b[0;34m(self, closure, x, t, d)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_directional_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0mflat_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gather_flat_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-25-7698bcd113d3>\u001b[0m in \u001b[0;36mclosure\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    106\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Train Loss\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 108\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    109\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    110\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "\n",
    "train_loss_full = []\n",
    "test_loss_full = []\n",
    "alpha_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(reps)\n",
    "    \n",
    "    torch.manual_seed(reps*36)\n",
    "    N_T = 500 #Total number of data points for 'y'\n",
    "    N_f = 10000 #Total number of collocation points \n",
    "    \n",
    "    xy_coll_np_array, xy_BC_np_array, u_BC_np_array = trainingdata(N_T,N_f,(reps)*22)\n",
    "        \n",
    "    xy_coll = torch.from_numpy(xy_coll_np_array).float().to(device)\n",
    "    xy_BC = torch.from_numpy(xy_BC_np_array).float().to(device)\n",
    "    u_BC = torch.from_numpy(u_BC_np_array).float().to(device)\n",
    "        \n",
    "    f_hat = torch.zeros(xy_coll.shape[0],1).to(device)\n",
    "\n",
    "    layers = np.array([2,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "\n",
    "    PINN = Sequentialmodel(layers)\n",
    "   \n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "    \n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=1, \n",
    "                              max_iter = 1000, \n",
    "                              max_eval = None, \n",
    "                              tolerance_grad = -1, \n",
    "                              tolerance_change = -1, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "        \n",
    "    optimizer.step(PINN.closure)\n",
    "    \n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(PINN.train_loss)\n",
    "    test_loss_full.append(PINN.test_loss)\n",
    "    elapsed_time[reps] = time.time() - start_time\n",
    "    alpha_full.append(PINN.alpha_val)\n",
    "\n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "    \n",
    "mdic = {\"train_loss\": train_loss_full,\"test_loss\": test_loss_full,\"Time\": elapsed_time, \"alpha\": alpha_full, \"label\": label}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.343263611470872\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + test_loss_full[i][-1]\n",
    "print(a/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.422675666003488\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + train_loss_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
