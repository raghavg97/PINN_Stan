{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def true_2D_1(xt): #True function for 2D_1 Klein Gordon Equation x \\in [-50,50] , t \\in [0,10]\n",
    "#     y = xt[:,0]*np.cos(xt[:,1])\n",
    "#     return y.reshape(-1,1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "label = \"QCRE_2D_1_atanh_NW_new\"\n",
    "\n",
    "x_full = np.hstack((np.linspace(-0.25,-0.1 - 0.5/500,150),np.linspace(-0.1,0.1,200),np.linspace(0.1+0.5/500,0.25,200))).reshape(-1,1)\n",
    "y_full = np.hstack((np.linspace(-0.25,-0.1 - 0.5/500,150),np.linspace(-0.1,0.1,200),np.linspace(0.1+0.5/500,0.25,200))).reshape(-1,1)\n",
    "\n",
    "X_full,Y_full = np.meshgrid(x_full,y_full)\n",
    "\n",
    "X_full = X_full.flatten('F').reshape(-1,1)\n",
    "Y_full = Y_full.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xy_full = np.hstack((X_full,Y_full))\n",
    "\n",
    "inside_square_pts = np.logical_and(np.logical_and(X_full < 0.1, X_full > -0.1),np.logical_and(Y_full < 0.1, Y_full > -0.1)).reshape(-1,)  \n",
    "\n",
    "xy = xy_full[~inside_square_pts,:]\n",
    "\n",
    "X = xy[:,0].reshape(-1,1)\n",
    "Y = xy[:,1].reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.25      ],\n",
       "       [-0.24899329],\n",
       "       [-0.24798658],\n",
       "       [-0.24697987],\n",
       "       [-0.24597315],\n",
       "       [-0.24496644],\n",
       "       [-0.24395973],\n",
       "       [-0.24295302],\n",
       "       [-0.24194631],\n",
       "       [-0.2409396 ],\n",
       "       [-0.23993289],\n",
       "       [-0.23892617],\n",
       "       [-0.23791946],\n",
       "       [-0.23691275],\n",
       "       [-0.23590604],\n",
       "       [-0.23489933],\n",
       "       [-0.23389262],\n",
       "       [-0.23288591],\n",
       "       [-0.23187919],\n",
       "       [-0.23087248],\n",
       "       [-0.22986577],\n",
       "       [-0.22885906],\n",
       "       [-0.22785235],\n",
       "       [-0.22684564],\n",
       "       [-0.22583893],\n",
       "       [-0.22483221],\n",
       "       [-0.2238255 ],\n",
       "       [-0.22281879],\n",
       "       [-0.22181208],\n",
       "       [-0.22080537],\n",
       "       [-0.21979866],\n",
       "       [-0.21879195],\n",
       "       [-0.21778523],\n",
       "       [-0.21677852],\n",
       "       [-0.21577181],\n",
       "       [-0.2147651 ],\n",
       "       [-0.21375839],\n",
       "       [-0.21275168],\n",
       "       [-0.21174497],\n",
       "       [-0.21073826],\n",
       "       [-0.20973154],\n",
       "       [-0.20872483],\n",
       "       [-0.20771812],\n",
       "       [-0.20671141],\n",
       "       [-0.2057047 ],\n",
       "       [-0.20469799],\n",
       "       [-0.20369128],\n",
       "       [-0.20268456],\n",
       "       [-0.20167785],\n",
       "       [-0.20067114],\n",
       "       [-0.19966443],\n",
       "       [-0.19865772],\n",
       "       [-0.19765101],\n",
       "       [-0.1966443 ],\n",
       "       [-0.19563758],\n",
       "       [-0.19463087],\n",
       "       [-0.19362416],\n",
       "       [-0.19261745],\n",
       "       [-0.19161074],\n",
       "       [-0.19060403],\n",
       "       [-0.18959732],\n",
       "       [-0.1885906 ],\n",
       "       [-0.18758389],\n",
       "       [-0.18657718],\n",
       "       [-0.18557047],\n",
       "       [-0.18456376],\n",
       "       [-0.18355705],\n",
       "       [-0.18255034],\n",
       "       [-0.18154362],\n",
       "       [-0.18053691],\n",
       "       [-0.1795302 ],\n",
       "       [-0.17852349],\n",
       "       [-0.17751678],\n",
       "       [-0.17651007],\n",
       "       [-0.17550336],\n",
       "       [-0.17449664],\n",
       "       [-0.17348993],\n",
       "       [-0.17248322],\n",
       "       [-0.17147651],\n",
       "       [-0.1704698 ],\n",
       "       [-0.16946309],\n",
       "       [-0.16845638],\n",
       "       [-0.16744966],\n",
       "       [-0.16644295],\n",
       "       [-0.16543624],\n",
       "       [-0.16442953],\n",
       "       [-0.16342282],\n",
       "       [-0.16241611],\n",
       "       [-0.1614094 ],\n",
       "       [-0.16040268],\n",
       "       [-0.15939597],\n",
       "       [-0.15838926],\n",
       "       [-0.15738255],\n",
       "       [-0.15637584],\n",
       "       [-0.15536913],\n",
       "       [-0.15436242],\n",
       "       [-0.1533557 ],\n",
       "       [-0.15234899],\n",
       "       [-0.15134228],\n",
       "       [-0.15033557],\n",
       "       [-0.14932886],\n",
       "       [-0.14832215],\n",
       "       [-0.14731544],\n",
       "       [-0.14630872],\n",
       "       [-0.14530201],\n",
       "       [-0.1442953 ],\n",
       "       [-0.14328859],\n",
       "       [-0.14228188],\n",
       "       [-0.14127517],\n",
       "       [-0.14026846],\n",
       "       [-0.13926174],\n",
       "       [-0.13825503],\n",
       "       [-0.13724832],\n",
       "       [-0.13624161],\n",
       "       [-0.1352349 ],\n",
       "       [-0.13422819],\n",
       "       [-0.13322148],\n",
       "       [-0.13221477],\n",
       "       [-0.13120805],\n",
       "       [-0.13020134],\n",
       "       [-0.12919463],\n",
       "       [-0.12818792],\n",
       "       [-0.12718121],\n",
       "       [-0.1261745 ],\n",
       "       [-0.12516779],\n",
       "       [-0.12416107],\n",
       "       [-0.12315436],\n",
       "       [-0.12214765],\n",
       "       [-0.12114094],\n",
       "       [-0.12013423],\n",
       "       [-0.11912752],\n",
       "       [-0.11812081],\n",
       "       [-0.11711409],\n",
       "       [-0.11610738],\n",
       "       [-0.11510067],\n",
       "       [-0.11409396],\n",
       "       [-0.11308725],\n",
       "       [-0.11208054],\n",
       "       [-0.11107383],\n",
       "       [-0.11006711],\n",
       "       [-0.1090604 ],\n",
       "       [-0.10805369],\n",
       "       [-0.10704698],\n",
       "       [-0.10604027],\n",
       "       [-0.10503356],\n",
       "       [-0.10402685],\n",
       "       [-0.10302013],\n",
       "       [-0.10201342],\n",
       "       [-0.10100671],\n",
       "       [-0.1       ],\n",
       "       [-0.1       ],\n",
       "       [-0.09899497],\n",
       "       [-0.09798995],\n",
       "       [-0.09698492],\n",
       "       [-0.0959799 ],\n",
       "       [-0.09497487],\n",
       "       [-0.09396985],\n",
       "       [-0.09296482],\n",
       "       [-0.0919598 ],\n",
       "       [-0.09095477],\n",
       "       [-0.08994975],\n",
       "       [-0.08894472],\n",
       "       [-0.0879397 ],\n",
       "       [-0.08693467],\n",
       "       [-0.08592965],\n",
       "       [-0.08492462],\n",
       "       [-0.0839196 ],\n",
       "       [-0.08291457],\n",
       "       [-0.08190955],\n",
       "       [-0.08090452],\n",
       "       [-0.0798995 ],\n",
       "       [-0.07889447],\n",
       "       [-0.07788945],\n",
       "       [-0.07688442],\n",
       "       [-0.0758794 ],\n",
       "       [-0.07487437],\n",
       "       [-0.07386935],\n",
       "       [-0.07286432],\n",
       "       [-0.0718593 ],\n",
       "       [-0.07085427],\n",
       "       [-0.06984925],\n",
       "       [-0.06884422],\n",
       "       [-0.0678392 ],\n",
       "       [-0.06683417],\n",
       "       [-0.06582915],\n",
       "       [-0.06482412],\n",
       "       [-0.0638191 ],\n",
       "       [-0.06281407],\n",
       "       [-0.06180905],\n",
       "       [-0.06080402],\n",
       "       [-0.05979899],\n",
       "       [-0.05879397],\n",
       "       [-0.05778894],\n",
       "       [-0.05678392],\n",
       "       [-0.05577889],\n",
       "       [-0.05477387],\n",
       "       [-0.05376884],\n",
       "       [-0.05276382],\n",
       "       [-0.05175879],\n",
       "       [-0.05075377],\n",
       "       [-0.04974874],\n",
       "       [-0.04874372],\n",
       "       [-0.04773869],\n",
       "       [-0.04673367],\n",
       "       [-0.04572864],\n",
       "       [-0.04472362],\n",
       "       [-0.04371859],\n",
       "       [-0.04271357],\n",
       "       [-0.04170854],\n",
       "       [-0.04070352],\n",
       "       [-0.03969849],\n",
       "       [-0.03869347],\n",
       "       [-0.03768844],\n",
       "       [-0.03668342],\n",
       "       [-0.03567839],\n",
       "       [-0.03467337],\n",
       "       [-0.03366834],\n",
       "       [-0.03266332],\n",
       "       [-0.03165829],\n",
       "       [-0.03065327],\n",
       "       [-0.02964824],\n",
       "       [-0.02864322],\n",
       "       [-0.02763819],\n",
       "       [-0.02663317],\n",
       "       [-0.02562814],\n",
       "       [-0.02462312],\n",
       "       [-0.02361809],\n",
       "       [-0.02261307],\n",
       "       [-0.02160804],\n",
       "       [-0.02060302],\n",
       "       [-0.01959799],\n",
       "       [-0.01859296],\n",
       "       [-0.01758794],\n",
       "       [-0.01658291],\n",
       "       [-0.01557789],\n",
       "       [-0.01457286],\n",
       "       [-0.01356784],\n",
       "       [-0.01256281],\n",
       "       [-0.01155779],\n",
       "       [-0.01055276],\n",
       "       [-0.00954774],\n",
       "       [-0.00854271],\n",
       "       [-0.00753769],\n",
       "       [-0.00653266],\n",
       "       [-0.00552764],\n",
       "       [-0.00452261],\n",
       "       [-0.00351759],\n",
       "       [-0.00251256],\n",
       "       [-0.00150754],\n",
       "       [-0.00050251],\n",
       "       [ 0.00050251],\n",
       "       [ 0.00150754],\n",
       "       [ 0.00251256],\n",
       "       [ 0.00351759],\n",
       "       [ 0.00452261],\n",
       "       [ 0.00552764],\n",
       "       [ 0.00653266],\n",
       "       [ 0.00753769],\n",
       "       [ 0.00854271],\n",
       "       [ 0.00954774],\n",
       "       [ 0.01055276],\n",
       "       [ 0.01155779],\n",
       "       [ 0.01256281],\n",
       "       [ 0.01356784],\n",
       "       [ 0.01457286],\n",
       "       [ 0.01557789],\n",
       "       [ 0.01658291],\n",
       "       [ 0.01758794],\n",
       "       [ 0.01859296],\n",
       "       [ 0.01959799],\n",
       "       [ 0.02060302],\n",
       "       [ 0.02160804],\n",
       "       [ 0.02261307],\n",
       "       [ 0.02361809],\n",
       "       [ 0.02462312],\n",
       "       [ 0.02562814],\n",
       "       [ 0.02663317],\n",
       "       [ 0.02763819],\n",
       "       [ 0.02864322],\n",
       "       [ 0.02964824],\n",
       "       [ 0.03065327],\n",
       "       [ 0.03165829],\n",
       "       [ 0.03266332],\n",
       "       [ 0.03366834],\n",
       "       [ 0.03467337],\n",
       "       [ 0.03567839],\n",
       "       [ 0.03668342],\n",
       "       [ 0.03768844],\n",
       "       [ 0.03869347],\n",
       "       [ 0.03969849],\n",
       "       [ 0.04070352],\n",
       "       [ 0.04170854],\n",
       "       [ 0.04271357],\n",
       "       [ 0.04371859],\n",
       "       [ 0.04472362],\n",
       "       [ 0.04572864],\n",
       "       [ 0.04673367],\n",
       "       [ 0.04773869],\n",
       "       [ 0.04874372],\n",
       "       [ 0.04974874],\n",
       "       [ 0.05075377],\n",
       "       [ 0.05175879],\n",
       "       [ 0.05276382],\n",
       "       [ 0.05376884],\n",
       "       [ 0.05477387],\n",
       "       [ 0.05577889],\n",
       "       [ 0.05678392],\n",
       "       [ 0.05778894],\n",
       "       [ 0.05879397],\n",
       "       [ 0.05979899],\n",
       "       [ 0.06080402],\n",
       "       [ 0.06180905],\n",
       "       [ 0.06281407],\n",
       "       [ 0.0638191 ],\n",
       "       [ 0.06482412],\n",
       "       [ 0.06582915],\n",
       "       [ 0.06683417],\n",
       "       [ 0.0678392 ],\n",
       "       [ 0.06884422],\n",
       "       [ 0.06984925],\n",
       "       [ 0.07085427],\n",
       "       [ 0.0718593 ],\n",
       "       [ 0.07286432],\n",
       "       [ 0.07386935],\n",
       "       [ 0.07487437],\n",
       "       [ 0.0758794 ],\n",
       "       [ 0.07688442],\n",
       "       [ 0.07788945],\n",
       "       [ 0.07889447],\n",
       "       [ 0.0798995 ],\n",
       "       [ 0.08090452],\n",
       "       [ 0.08190955],\n",
       "       [ 0.08291457],\n",
       "       [ 0.0839196 ],\n",
       "       [ 0.08492462],\n",
       "       [ 0.08592965],\n",
       "       [ 0.08693467],\n",
       "       [ 0.0879397 ],\n",
       "       [ 0.08894472],\n",
       "       [ 0.08994975],\n",
       "       [ 0.09095477],\n",
       "       [ 0.0919598 ],\n",
       "       [ 0.09296482],\n",
       "       [ 0.09396985],\n",
       "       [ 0.09497487],\n",
       "       [ 0.0959799 ],\n",
       "       [ 0.09698492],\n",
       "       [ 0.09798995],\n",
       "       [ 0.09899497],\n",
       "       [ 0.1       ],\n",
       "       [ 0.1       ],\n",
       "       [ 0.10075377],\n",
       "       [ 0.10150754],\n",
       "       [ 0.10226131],\n",
       "       [ 0.10301508],\n",
       "       [ 0.10376884],\n",
       "       [ 0.10452261],\n",
       "       [ 0.10527638],\n",
       "       [ 0.10603015],\n",
       "       [ 0.10678392],\n",
       "       [ 0.10753769],\n",
       "       [ 0.10829146],\n",
       "       [ 0.10904523],\n",
       "       [ 0.10979899],\n",
       "       [ 0.11055276],\n",
       "       [ 0.11130653],\n",
       "       [ 0.1120603 ],\n",
       "       [ 0.11281407],\n",
       "       [ 0.11356784],\n",
       "       [ 0.11432161],\n",
       "       [ 0.11507538],\n",
       "       [ 0.11582915],\n",
       "       [ 0.11658291],\n",
       "       [ 0.11733668],\n",
       "       [ 0.11809045],\n",
       "       [ 0.11884422],\n",
       "       [ 0.11959799],\n",
       "       [ 0.12035176],\n",
       "       [ 0.12110553],\n",
       "       [ 0.1218593 ],\n",
       "       [ 0.12261307],\n",
       "       [ 0.12336683],\n",
       "       [ 0.1241206 ],\n",
       "       [ 0.12487437],\n",
       "       [ 0.12562814],\n",
       "       [ 0.12638191],\n",
       "       [ 0.12713568],\n",
       "       [ 0.12788945],\n",
       "       [ 0.12864322],\n",
       "       [ 0.12939698],\n",
       "       [ 0.13015075],\n",
       "       [ 0.13090452],\n",
       "       [ 0.13165829],\n",
       "       [ 0.13241206],\n",
       "       [ 0.13316583],\n",
       "       [ 0.1339196 ],\n",
       "       [ 0.13467337],\n",
       "       [ 0.13542714],\n",
       "       [ 0.1361809 ],\n",
       "       [ 0.13693467],\n",
       "       [ 0.13768844],\n",
       "       [ 0.13844221],\n",
       "       [ 0.13919598],\n",
       "       [ 0.13994975],\n",
       "       [ 0.14070352],\n",
       "       [ 0.14145729],\n",
       "       [ 0.14221106],\n",
       "       [ 0.14296482],\n",
       "       [ 0.14371859],\n",
       "       [ 0.14447236],\n",
       "       [ 0.14522613],\n",
       "       [ 0.1459799 ],\n",
       "       [ 0.14673367],\n",
       "       [ 0.14748744],\n",
       "       [ 0.14824121],\n",
       "       [ 0.14899497],\n",
       "       [ 0.14974874],\n",
       "       [ 0.15050251],\n",
       "       [ 0.15125628],\n",
       "       [ 0.15201005],\n",
       "       [ 0.15276382],\n",
       "       [ 0.15351759],\n",
       "       [ 0.15427136],\n",
       "       [ 0.15502513],\n",
       "       [ 0.15577889],\n",
       "       [ 0.15653266],\n",
       "       [ 0.15728643],\n",
       "       [ 0.1580402 ],\n",
       "       [ 0.15879397],\n",
       "       [ 0.15954774],\n",
       "       [ 0.16030151],\n",
       "       [ 0.16105528],\n",
       "       [ 0.16180905],\n",
       "       [ 0.16256281],\n",
       "       [ 0.16331658],\n",
       "       [ 0.16407035],\n",
       "       [ 0.16482412],\n",
       "       [ 0.16557789],\n",
       "       [ 0.16633166],\n",
       "       [ 0.16708543],\n",
       "       [ 0.1678392 ],\n",
       "       [ 0.16859296],\n",
       "       [ 0.16934673],\n",
       "       [ 0.1701005 ],\n",
       "       [ 0.17085427],\n",
       "       [ 0.17160804],\n",
       "       [ 0.17236181],\n",
       "       [ 0.17311558],\n",
       "       [ 0.17386935],\n",
       "       [ 0.17462312],\n",
       "       [ 0.17537688],\n",
       "       [ 0.17613065],\n",
       "       [ 0.17688442],\n",
       "       [ 0.17763819],\n",
       "       [ 0.17839196],\n",
       "       [ 0.17914573],\n",
       "       [ 0.1798995 ],\n",
       "       [ 0.18065327],\n",
       "       [ 0.18140704],\n",
       "       [ 0.1821608 ],\n",
       "       [ 0.18291457],\n",
       "       [ 0.18366834],\n",
       "       [ 0.18442211],\n",
       "       [ 0.18517588],\n",
       "       [ 0.18592965],\n",
       "       [ 0.18668342],\n",
       "       [ 0.18743719],\n",
       "       [ 0.18819095],\n",
       "       [ 0.18894472],\n",
       "       [ 0.18969849],\n",
       "       [ 0.19045226],\n",
       "       [ 0.19120603],\n",
       "       [ 0.1919598 ],\n",
       "       [ 0.19271357],\n",
       "       [ 0.19346734],\n",
       "       [ 0.19422111],\n",
       "       [ 0.19497487],\n",
       "       [ 0.19572864],\n",
       "       [ 0.19648241],\n",
       "       [ 0.19723618],\n",
       "       [ 0.19798995],\n",
       "       [ 0.19874372],\n",
       "       [ 0.19949749],\n",
       "       [ 0.20025126],\n",
       "       [ 0.20100503],\n",
       "       [ 0.20175879],\n",
       "       [ 0.20251256],\n",
       "       [ 0.20326633],\n",
       "       [ 0.2040201 ],\n",
       "       [ 0.20477387],\n",
       "       [ 0.20552764],\n",
       "       [ 0.20628141],\n",
       "       [ 0.20703518],\n",
       "       [ 0.20778894],\n",
       "       [ 0.20854271],\n",
       "       [ 0.20929648],\n",
       "       [ 0.21005025],\n",
       "       [ 0.21080402],\n",
       "       [ 0.21155779],\n",
       "       [ 0.21231156],\n",
       "       [ 0.21306533],\n",
       "       [ 0.2138191 ],\n",
       "       [ 0.21457286],\n",
       "       [ 0.21532663],\n",
       "       [ 0.2160804 ],\n",
       "       [ 0.21683417],\n",
       "       [ 0.21758794],\n",
       "       [ 0.21834171],\n",
       "       [ 0.21909548],\n",
       "       [ 0.21984925],\n",
       "       [ 0.22060302],\n",
       "       [ 0.22135678],\n",
       "       [ 0.22211055],\n",
       "       [ 0.22286432],\n",
       "       [ 0.22361809],\n",
       "       [ 0.22437186],\n",
       "       [ 0.22512563],\n",
       "       [ 0.2258794 ],\n",
       "       [ 0.22663317],\n",
       "       [ 0.22738693],\n",
       "       [ 0.2281407 ],\n",
       "       [ 0.22889447],\n",
       "       [ 0.22964824],\n",
       "       [ 0.23040201],\n",
       "       [ 0.23115578],\n",
       "       [ 0.23190955],\n",
       "       [ 0.23266332],\n",
       "       [ 0.23341709],\n",
       "       [ 0.23417085],\n",
       "       [ 0.23492462],\n",
       "       [ 0.23567839],\n",
       "       [ 0.23643216],\n",
       "       [ 0.23718593],\n",
       "       [ 0.2379397 ],\n",
       "       [ 0.23869347],\n",
       "       [ 0.23944724],\n",
       "       [ 0.24020101],\n",
       "       [ 0.24095477],\n",
       "       [ 0.24170854],\n",
       "       [ 0.24246231],\n",
       "       [ 0.24321608],\n",
       "       [ 0.24396985],\n",
       "       [ 0.24472362],\n",
       "       [ 0.24547739],\n",
       "       [ 0.24623116],\n",
       "       [ 0.24698492],\n",
       "       [ 0.24773869],\n",
       "       [ 0.24849246],\n",
       "       [ 0.24924623],\n",
       "       [ 0.25      ]])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.hstack((np.linspace(-0.25,-0.1,150),np.linspace(-0.1,0.1,200),np.linspace(0.1,0.25,200))).reshape(-1,1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_bound_pts_1 = (X == -0.25).reshape(-1,)\n",
    "out_bound_pts_2 = (Y == -0.25).reshape(-1,)\n",
    "out_bound_pts_3 = (X == 0.25).reshape(-1,)\n",
    "out_bound_pts_4 = (Y == 0.25).reshape(-1,)\n",
    "\n",
    "in_bound_pts_5 = (X == -0.1).reshape(-1,)\n",
    "in_bound_pts_6 = (Y == -0.1).reshape(-1,)\n",
    "in_bound_pts_7 = (X == 0.1).reshape(-1,)\n",
    "in_bound_pts_8 = (Y == 0.1).reshape(-1,)\n",
    "\n",
    "xy_bound_1 = xy[out_bound_pts_1,:]\n",
    "xy_bound_2 = xy[out_bound_pts_2,:]\n",
    "xy_bound_3 = xy[out_bound_pts_3,:]\n",
    "xy_bound_4 = xy[out_bound_pts_4,:]\n",
    "\n",
    "xy_bound_5 = xy[in_bound_pts_5,:]\n",
    "xy_bound_6 = xy[in_bound_pts_6,:]\n",
    "xy_bound_7 = xy[in_bound_pts_7,:]\n",
    "xy_bound_8 = xy[in_bound_pts_8,:]\n",
    "\n",
    "xy_out_bound = np.vstack((xy_bound_1,xy_bound_2,xy_bound_3,xy_bound_4))\n",
    "xy_in_bound = np.vstack((xy_bound_5,xy_bound_6,xy_bound_7,xy_bound_8))\n",
    "\n",
    "u_bound_out = np.zeros((np.shape(xy_out_bound)[0],1))\n",
    "u_bound_in = 1000*np.ones((np.shape(xy_in_bound)[0],1))\n",
    "\n",
    "xy_bound = np.vstack((xy_out_bound,xy_in_bound)) \n",
    "u_bound = np.vstack((u_bound_out,u_bound_in))\n",
    "\n",
    "xy_test_tensor = torch.from_numpy(xy).float().to(device)\n",
    "\n",
    "lb_xy = xy[0]\n",
    "ub_xy = xy[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([False, False, False, ..., False, False, False])"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "in_bound_pts_5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_T,N_f,seed):\n",
    "    '''Boundary Conditions''' \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    #choose random N_u points for training\n",
    "    idx = np.random.choice(xy_bound.shape[0], N_T, replace=False) \n",
    "    xy_BC = xy_bound[idx,:] #choose indices from  set 'idx' (x,t)\n",
    "    u_BC = u_bound[idx].reshape(-1,1)      #choose corresponding u\n",
    "\n",
    "\n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "#     x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "#     sampling = LHS(xlimits=x01,random_state =seed)\n",
    "#     samples = sampling(N_f)\n",
    "    \n",
    "#     xy_coll = lb_xy + (ub_xy - lb_xy)*samples\n",
    "#     xy_coll = np.vstack((xy_coll, xy_BC))\n",
    "    idx_coll = np.random.choice(xy.shape[0], N_f, replace=False)\n",
    "    xy_coll = xy[idx_coll,:]\n",
    "     # append training points to collocation points \n",
    "\n",
    "    return xy_coll, xy_BC, u_BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        self.iter = 0\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)   \n",
    "        \n",
    "        self.beta = Parameter(torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "        \n",
    "        self.train_loss = []\n",
    "        self.test_loss = []\n",
    "        \n",
    "        self.beta_val = []\n",
    "        \n",
    "        self.n = torch.tensor(1.0)\n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xy):\n",
    "        if torch.is_tensor(xy) != True:         \n",
    "            xy = torch.from_numpy(xy)                \n",
    "        \n",
    "        ubxy = torch.from_numpy(ub_xy).float().to(device)\n",
    "        lbxy = torch.from_numpy(lb_xy).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xy = (xy - lbxy)/(ubxy - lbxy)\n",
    "        \n",
    "        #convert to float\n",
    "        a = xy.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z) + self.beta[:,i]*z*self.activation(z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC(self,xy,u):\n",
    "                \n",
    "        loss_bc = self.loss_function(self.forward(xy), u)\n",
    "                \n",
    "        return loss_bc\n",
    "    \n",
    "    def loss_PDE(self, xy_coll, f_hat):\n",
    "        \n",
    "        g = xy_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_y = autograd.grad(u,g,torch.ones([xy_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_xx_yy = autograd.grad(u_x_y,g,torch.ones(xy_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        #du_dx = u_x_t[:,[0]]\n",
    "        \n",
    "        d2u_dx2 = u_xx_yy[:,[0]]\n",
    "        d2u_dy2 = u_xx_yy[:,[1]]    \n",
    "        \n",
    "\n",
    "        f = d2u_dx2 + d2u_dy2\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xy_BC,u_BC,xy_coll,f_hat):\n",
    "\n",
    "        loss_BC = self.loss_BC(xy_BC,u_BC)\n",
    "        loss_f = self.loss_PDE(xy_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_BC + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "     \n",
    "    'callable for optimizer'                                       \n",
    "    def closure(self):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = self.loss(xy_BC, u_BC, xy_coll,f_hat)\n",
    "        self.train_loss.append(loss.cpu().detach().numpy())\n",
    "        \n",
    "        u_pred = self.test(xy_test_tensor)\n",
    "        #self.test_loss.append(np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))) #Commented because no true values yet\n",
    "        self.beta_val.append(self.beta.cpu().detach().numpy())\n",
    "        \n",
    "        #print(self.iter,\"Train Loss\",self.train_loss[-1],\"Test Loss\",self.test_loss[-1])\n",
    "        print(self.iter,\"Train Loss\",self.train_loss[-1])\n",
    "        \n",
    "        loss.backward()\n",
    "        self.iter += 1\n",
    "  \n",
    "\n",
    "        return loss        \n",
    "    'test neural network'\n",
    "    def test(self,xy_test_tensor):\n",
    "        u_pred = self.forward(xy_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0 Train Loss 45.894398\n",
      "1 Train Loss 23.153412\n",
      "2 Train Loss 5.081561\n",
      "3 Train Loss 2.3020113\n",
      "4 Train Loss 1.1274108\n",
      "5 Train Loss 0.40467605\n",
      "6 Train Loss 0.3745988\n",
      "7 Train Loss 0.27051425\n",
      "8 Train Loss 0.25267962\n",
      "9 Train Loss 0.21570987\n",
      "10 Train Loss 0.17023428\n",
      "11 Train Loss 0.1099682\n",
      "12 Train Loss 0.081767224\n",
      "13 Train Loss 0.07431487\n",
      "14 Train Loss 0.07120624\n",
      "15 Train Loss 0.06739916\n",
      "16 Train Loss 0.05640261\n",
      "17 Train Loss 0.040248174\n",
      "18 Train Loss 0.025628764\n",
      "19 Train Loss 0.01954413\n",
      "20 Train Loss 0.018144907\n",
      "21 Train Loss 0.017552517\n",
      "22 Train Loss 0.016601443\n",
      "23 Train Loss 0.014653152\n",
      "24 Train Loss 0.011685845\n",
      "25 Train Loss 0.009102444\n",
      "26 Train Loss 0.007935863\n",
      "27 Train Loss 0.006252707\n",
      "28 Train Loss 0.005320689\n",
      "29 Train Loss 0.004202315\n",
      "30 Train Loss 0.0033395207\n",
      "31 Train Loss 0.002979538\n",
      "32 Train Loss 0.0027916015\n",
      "33 Train Loss 0.0024861062\n",
      "34 Train Loss 0.0021808038\n",
      "35 Train Loss 0.0019192661\n",
      "36 Train Loss 0.0016338902\n",
      "37 Train Loss 0.0014938382\n",
      "38 Train Loss 0.0013448935\n",
      "39 Train Loss 0.0012103488\n",
      "40 Train Loss 0.0010035171\n",
      "41 Train Loss 0.00075641274\n",
      "42 Train Loss 0.000717829\n",
      "43 Train Loss 0.00053735974\n",
      "44 Train Loss 0.00051646284\n",
      "45 Train Loss 0.000499176\n",
      "46 Train Loss 0.0004806091\n",
      "47 Train Loss 0.0004490002\n",
      "48 Train Loss 0.00039528913\n",
      "49 Train Loss 0.00034732997\n",
      "50 Train Loss 0.0003203872\n",
      "51 Train Loss 0.0003075644\n",
      "52 Train Loss 0.0003003265\n",
      "53 Train Loss 0.00029359595\n",
      "54 Train Loss 0.00028708638\n",
      "55 Train Loss 0.00028167182\n",
      "56 Train Loss 0.0002747871\n",
      "57 Train Loss 0.0002651098\n",
      "58 Train Loss 0.0002455019\n",
      "59 Train Loss 0.00021815105\n",
      "60 Train Loss 0.00018700492\n",
      "61 Train Loss 0.00016456116\n",
      "62 Train Loss 0.00015592077\n",
      "63 Train Loss 0.00015352314\n",
      "64 Train Loss 0.00015136112\n",
      "65 Train Loss 0.0001490128\n",
      "66 Train Loss 0.00014488443\n",
      "67 Train Loss 0.00014124016\n",
      "68 Train Loss 0.00013184542\n",
      "69 Train Loss 0.00011823133\n",
      "70 Train Loss 0.00010602488\n",
      "71 Train Loss 0.00012819645\n",
      "72 Train Loss 9.9928475e-05\n",
      "73 Train Loss 9.249368e-05\n",
      "74 Train Loss 8.302825e-05\n",
      "75 Train Loss 7.780308e-05\n",
      "76 Train Loss 7.4585354e-05\n",
      "77 Train Loss 7.2055445e-05\n",
      "78 Train Loss 6.776169e-05\n",
      "79 Train Loss 6.451879e-05\n",
      "80 Train Loss 6.623975e-05\n",
      "81 Train Loss 6.191541e-05\n",
      "82 Train Loss 5.999496e-05\n",
      "83 Train Loss 5.766606e-05\n",
      "84 Train Loss 5.6621095e-05\n",
      "85 Train Loss 5.3832577e-05\n",
      "86 Train Loss 5.2124244e-05\n",
      "87 Train Loss 4.8768714e-05\n",
      "88 Train Loss 4.6103476e-05\n",
      "89 Train Loss 4.3991393e-05\n",
      "90 Train Loss 4.2824955e-05\n",
      "91 Train Loss 4.1518906e-05\n",
      "92 Train Loss 3.902096e-05\n",
      "93 Train Loss 3.669635e-05\n",
      "94 Train Loss 3.4651028e-05\n",
      "95 Train Loss 3.3040968e-05\n",
      "96 Train Loss 3.155184e-05\n",
      "97 Train Loss 3.0445917e-05\n",
      "98 Train Loss 2.9741208e-05\n",
      "99 Train Loss 2.928049e-05\n",
      "100 Train Loss 2.8844748e-05\n",
      "101 Train Loss 2.8021856e-05\n",
      "102 Train Loss 2.6942274e-05\n",
      "103 Train Loss 2.5266738e-05\n",
      "104 Train Loss 2.3482768e-05\n",
      "105 Train Loss 2.217797e-05\n",
      "106 Train Loss 2.1698565e-05\n",
      "107 Train Loss 2.1430744e-05\n",
      "108 Train Loss 2.1170159e-05\n",
      "109 Train Loss 2.0808871e-05\n",
      "110 Train Loss 2.0402946e-05\n",
      "111 Train Loss 1.974862e-05\n",
      "112 Train Loss 1.8751109e-05\n",
      "113 Train Loss 1.7099168e-05\n",
      "114 Train Loss 1.4830524e-05\n",
      "115 Train Loss 1.4072121e-05\n",
      "116 Train Loss 1.2352849e-05\n",
      "117 Train Loss 1.138729e-05\n",
      "118 Train Loss 9.000044e-06\n",
      "119 Train Loss 7.805913e-06\n",
      "120 Train Loss 7.004714e-06\n",
      "121 Train Loss 6.495905e-06\n",
      "122 Train Loss 6.2446998e-06\n",
      "123 Train Loss 6.0868306e-06\n",
      "124 Train Loss 5.965195e-06\n",
      "125 Train Loss 5.8524984e-06\n",
      "126 Train Loss 5.6772947e-06\n",
      "127 Train Loss 5.2643104e-06\n",
      "128 Train Loss 4.9014093e-06\n",
      "129 Train Loss 4.571046e-06\n",
      "130 Train Loss 4.3990894e-06\n",
      "131 Train Loss 4.357169e-06\n",
      "132 Train Loss 4.1811554e-06\n",
      "133 Train Loss 4.1073768e-06\n",
      "134 Train Loss 4.0113086e-06\n",
      "135 Train Loss 3.9523093e-06\n",
      "136 Train Loss 3.8708295e-06\n",
      "137 Train Loss 3.8716935e-06\n",
      "138 Train Loss 3.8259177e-06\n",
      "139 Train Loss 3.7784605e-06\n",
      "140 Train Loss 3.7536602e-06\n",
      "141 Train Loss 3.7413881e-06\n",
      "142 Train Loss 3.7267564e-06\n",
      "143 Train Loss 3.719666e-06\n",
      "144 Train Loss 3.7136947e-06\n",
      "145 Train Loss 3.706176e-06\n",
      "146 Train Loss 3.6914441e-06\n",
      "147 Train Loss 3.6744475e-06\n",
      "148 Train Loss 3.6423805e-06\n",
      "149 Train Loss 3.575008e-06\n",
      "150 Train Loss 3.4990178e-06\n",
      "151 Train Loss 3.4287143e-06\n",
      "152 Train Loss 3.3501572e-06\n",
      "153 Train Loss 3.2811242e-06\n",
      "154 Train Loss 3.2342346e-06\n",
      "155 Train Loss 3.2077571e-06\n",
      "156 Train Loss 3.1901454e-06\n",
      "157 Train Loss 3.155495e-06\n",
      "158 Train Loss 3.1200034e-06\n",
      "159 Train Loss 3.076038e-06\n",
      "160 Train Loss 3.0150725e-06\n",
      "161 Train Loss 2.976513e-06\n",
      "162 Train Loss 2.9275354e-06\n",
      "163 Train Loss 2.8475695e-06\n",
      "164 Train Loss 2.6965813e-06\n",
      "165 Train Loss 2.4998837e-06\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-88eaf22fc2f3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPINN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPINN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                     loss, flat_grad, t, ls_func_evals = _strong_wolfe(\n\u001b[0;32m--> 426\u001b[0;31m                         obj_func, x_init, t, d, loss, flat_grad, gtd)\n\u001b[0m\u001b[1;32m    427\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0mopt_cond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflat_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mtolerance_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36m_strong_wolfe\u001b[0;34m(obj_func, x, t, d, f, g, gtd, c1, c2, tolerance_change, max_ls)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# evaluate objective and gradient using initial step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mf_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mls_func_evals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mgtd_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mobj_func\u001b[0;34m(x, t, d)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m                     \u001b[0;32mdef\u001b[0m \u001b[0mobj_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m                         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_directional_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                     loss, flat_grad, t, ls_func_evals = _strong_wolfe(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36m_directional_evaluate\u001b[0;34m(self, closure, x, t, d)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_directional_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0mflat_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gather_flat_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-b72583e830d1>\u001b[0m in \u001b[0;36mclosure\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     95\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     96\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 97\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy_BC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_BC\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxy_coll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     98\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-b72583e830d1>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, xy_BC, u_BC, xy_coll, f_hat)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0mloss_BC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_BC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy_BC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu_BC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 87\u001b[0;31m         \u001b[0mloss_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_PDE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy_coll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     88\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     89\u001b[0m         \u001b[0mloss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_BC\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-b72583e830d1>\u001b[0m in \u001b[0;36mloss_PDE\u001b[0;34m(self, xy_coll, f_hat)\u001b[0m\n\u001b[1;32m     68\u001b[0m         \u001b[0mu_x_y\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mxy_coll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mallow_unused\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m         \u001b[0mu_xx_yy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_x_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy_coll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mallow_unused\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m#du_dx = u_x_t[:,[0]]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    226\u001b[0m     return Variable._execution_engine.run_backward(\n\u001b[1;32m    227\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         inputs, allow_unused, accumulate_grad=False)\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "\n",
    "train_loss_full = []\n",
    "test_loss_full = []\n",
    "beta_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(reps)\n",
    "    \n",
    "    torch.manual_seed(reps*36)\n",
    "    N_T = 100 #Total number of data points for 'y'\n",
    "    N_f = 10000 #Total number of collocation points \n",
    "    \n",
    "    xy_coll_np_array, xy_BC_np_array, u_BC_np_array = trainingdata(N_T,N_f,(reps)*22)\n",
    "        \n",
    "    xy_coll = torch.from_numpy(xy_coll_np_array).float().to(device)\n",
    "    xy_BC = torch.from_numpy(xy_BC_np_array).float().to(device)\n",
    "    u_BC = torch.from_numpy(u_BC_np_array).float().to(device)\n",
    "        \n",
    "    f_hat = torch.zeros(xy_coll.shape[0],1).to(device)\n",
    "\n",
    "    layers = np.array([2,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "\n",
    "    PINN = Sequentialmodel(layers)\n",
    "   \n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "    \n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=1, \n",
    "                              max_iter = 10000, \n",
    "                              max_eval = None, \n",
    "                              tolerance_grad = -1, \n",
    "                              tolerance_change = -1, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "        \n",
    "    optimizer.step(PINN.closure)\n",
    "    \n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(PINN.train_loss)\n",
    "    test_loss_full.append(PINN.test_loss)\n",
    "    elapsed_time[reps] = time.time() - start_time\n",
    "    beta_full.append(PINN.beta_val)\n",
    "\n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "    \n",
    "mdic = {\"train_loss\": train_loss_full,\"test_loss\": test_loss_full,\"Time\": elapsed_time, \"beta\": beta_full, \"label\": label}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred = PINN.test(xy_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "cmap = plt.cm.jet\n",
    "\n",
    "img3 = ax.imshow(u_pred.reshape(210,1000),vmin = 0,vmax = 1000,cmap = cmap,extent=[0,1,0,1],aspect = 0.75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(projection='3d')\n",
    "\n",
    "ax.scatter(xy[:,0], xy[:,1], u_pred, marker='*')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import cm\n",
    "fig, ax = plt.subplots(subplot_kw={\"projection\": \"3d\"})\n",
    "\n",
    "surf = ax.plot_surface(xy[:,0], xy[:,1], u_pred, cmap=cm.jet, linewidth=0, antialiased=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-0.000986978"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(u_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + test_loss_full[i][-1]\n",
    "print(a/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + train_loss_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
