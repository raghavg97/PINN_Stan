{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def true_2D_1(xt): #True function for 2D_1 Klein Gordon Equation x \\in [-50,50] , t \\in [0,10]\n",
    "#     y = xt[:,0]*np.cos(xt[:,1])\n",
    "#     return y.reshape(-1,1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-25-186bb4d70e1c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0mbound_pts_1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 14\u001b[0;31m \u001b[0mbound_pts_2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0mX\u001b[0m\u001b[0;34m!=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     15\u001b[0m \u001b[0mbound_pts_3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0mbound_pts_4\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mY\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: The truth value of an array with more than one element is ambiguous. Use a.any() or a.all()"
     ]
    }
   ],
   "source": [
    "label = \"QCRE_2D_1_atanh_NW_new\"\n",
    "\n",
    "x = np.linspace(0,1,500).reshape(-1,1)\n",
    "y = np.linspace(0,1,500).reshape(-1,1)\n",
    "\n",
    "X,Y = np.meshgrid(x,y)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "Y = Y.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xy = np.hstack((X,Y))\n",
    "\n",
    "bound_pts_1 = (X == 0).reshape(-1,)\n",
    "bound_pts_2 = (Y == 0 and X != 0).reshape(-1,) \n",
    "bound_pts_3 = (X == 1).reshape(-1,) \n",
    "bound_pts_4 = (Y == 1).reshape(-1,) \n",
    "\n",
    "xy_bound_1 = xy[bound_pts_1,:]\n",
    "xy_bound_2 = xy[bound_pts_2,:]\n",
    "xy_bound_3 = xy[bound_pts_3,:]\n",
    "xy_bound_4 = xy[bound_pts_4,:]\n",
    "\n",
    "u_bound_1 = 1000*np.ones((np.shape(xy_bound_1)[0],1))\n",
    "u_bound_2 = 800*np.ones((np.shape(xy_bound_2)[0],1))\n",
    "u_bound_3 = 500*np.ones((np.shape(xy_bound_3)[0],1))\n",
    "u_bound_4 = np.zeros((np.shape(xy_bound_4)[0],1))\n",
    "\n",
    "xy_bound = np.vstack((xy_bound_1,xy_bound_2,xy_bound_3,xy_bound_4))\n",
    "u_bound = np.vstack((u_bound_1,u_bound_2,u_bound_3,u_bound_4))\n",
    "\n",
    "xy_test_tensor = torch.from_numpy(xy).float().to(device)\n",
    "\n",
    "lb_xy = xy[0]\n",
    "ub_xy = xy[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ub_xy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_T,N_f,seed):\n",
    "    '''Boundary Conditions''' \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    #choose random N_u points for training\n",
    "    idx = np.random.choice(xy_bound.shape[0], N_T, replace=False) \n",
    "    xy_BC = xy_bound[idx,:] #choose indices from  set 'idx' (x,t)\n",
    "    u_BC = u_bound[idx].reshape(-1,1)      #choose corresponding u\n",
    "\n",
    "\n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    samples = sampling(N_f)\n",
    "    \n",
    "    xy_coll = lb_xy + (ub_xy - lb_xy)*samples\n",
    "    \n",
    "    xy_coll = np.vstack((xy_coll, xy_BC)) # append training points to collocation points \n",
    "\n",
    "    return xy_coll, xy_BC, u_BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        self.iter = 0\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)   \n",
    "        \n",
    "        self.beta = Parameter(torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "        \n",
    "        self.train_loss = []\n",
    "        self.test_loss = []\n",
    "        \n",
    "        self.beta_val = []\n",
    "        \n",
    "        self.n = torch.tensor(1.0)\n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xy):\n",
    "        if torch.is_tensor(xy) != True:         \n",
    "            xy = torch.from_numpy(xy)                \n",
    "        \n",
    "        ubxy = torch.from_numpy(ub_xy).float().to(device)\n",
    "        lbxy = torch.from_numpy(lb_xy).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xy = (xy - lbxy)/(ubxy - lbxy)\n",
    "        \n",
    "        #convert to float\n",
    "        a = xy.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z) + self.beta[:,i]*z*self.activation(z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC(self,xy,u):\n",
    "                \n",
    "        loss_bc = self.loss_function(self.forward(xy), u)\n",
    "                \n",
    "        return loss_bc\n",
    "    \n",
    "    def loss_PDE(self, xy_coll, f_hat):\n",
    "        \n",
    "        g = xy_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_y = autograd.grad(u,g,torch.ones([xy_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_xx_yy = autograd.grad(u_x_y,g,torch.ones(xy_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        #du_dx = u_x_t[:,[0]]\n",
    "        \n",
    "        d2u_dx2 = u_xx_yy[:,[0]]\n",
    "        d2u_dy2 = u_xx_yy[:,[1]]    \n",
    "        \n",
    "\n",
    "        f = d2u_dx2 + d2u_dy2\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xy_BC,u_BC,xy_coll,f_hat):\n",
    "\n",
    "        loss_BC = self.loss_BC(xy_BC,u_BC)\n",
    "        loss_f = self.loss_PDE(xy_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_BC + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "     \n",
    "    'callable for optimizer'                                       \n",
    "    def closure(self):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        loss = self.loss(xy_BC, u_BC, xy_coll,f_hat)\n",
    "        self.train_loss.append(loss.cpu().detach().numpy())\n",
    "        \n",
    "        u_pred = self.test(xy_test_tensor)\n",
    "        #self.test_loss.append(np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))) #Commented because no true values yet\n",
    "        self.beta_val.append(self.beta.cpu().detach().numpy())\n",
    "        \n",
    "        #print(self.iter,\"Train Loss\",self.train_loss[-1],\"Test Loss\",self.test_loss[-1])\n",
    "        print(self.iter,\"Train Loss\",self.train_loss[-1])\n",
    "        \n",
    "        loss.backward()\n",
    "        self.iter += 1\n",
    "  \n",
    "\n",
    "        return loss        \n",
    "    'test neural network'\n",
    "    def test(self,xy_test_tensor):\n",
    "        u_pred = self.forward(xy_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0 Train Loss 317763.2\n",
      "1 Train Loss 317689.47\n",
      "2 Train Loss 317102.94\n",
      "3 Train Loss 314723.3\n",
      "4 Train Loss 276887.25\n",
      "5 Train Loss 3755282000.0\n",
      "6 Train Loss 20897520.0\n",
      "7 Train Loss 371714.84\n",
      "8 Train Loss 156635.25\n",
      "9 Train Loss 150782.22\n",
      "10 Train Loss 150189.72\n",
      "11 Train Loss 69559060.0\n",
      "12 Train Loss 2385378.5\n",
      "13 Train Loss 216960.77\n",
      "14 Train Loss 150023.67\n",
      "15 Train Loss 151224.38\n",
      "16 Train Loss 143053.89\n",
      "17 Train Loss 140043.03\n",
      "18 Train Loss 138498.53\n",
      "19 Train Loss 137072.3\n",
      "20 Train Loss 134393.95\n",
      "21 Train Loss 133977.81\n",
      "22 Train Loss 132570.98\n",
      "23 Train Loss 135460.31\n",
      "24 Train Loss 129706.77\n",
      "25 Train Loss 128179.05\n",
      "26 Train Loss 126705.13\n",
      "27 Train Loss 837151.0\n",
      "28 Train Loss 125374.96\n",
      "29 Train Loss 125083.96\n",
      "30 Train Loss 6257838.0\n",
      "31 Train Loss 125075.87\n",
      "32 Train Loss 132352.08\n",
      "33 Train Loss 124615.59\n",
      "34 Train Loss 122933.805\n",
      "35 Train Loss 137637.86\n",
      "36 Train Loss 122418.47\n",
      "37 Train Loss 143749.3\n",
      "38 Train Loss 122395.09\n",
      "39 Train Loss 122327.76\n",
      "40 Train Loss 121968.82\n",
      "41 Train Loss 121632.555\n",
      "42 Train Loss 122256.02\n",
      "43 Train Loss 121293.4\n",
      "44 Train Loss 120980.33\n",
      "45 Train Loss 120485.34\n",
      "46 Train Loss 120275.65\n",
      "47 Train Loss 120096.57\n",
      "48 Train Loss 120010.2\n",
      "49 Train Loss 119939.4\n",
      "50 Train Loss 119855.86\n",
      "51 Train Loss 119745.586\n",
      "52 Train Loss 119591.98\n",
      "53 Train Loss 119342.02\n",
      "54 Train Loss 120221.73\n",
      "55 Train Loss 119166.266\n",
      "56 Train Loss 118981.47\n",
      "57 Train Loss 118697.695\n",
      "58 Train Loss 118530.24\n",
      "59 Train Loss 118202.8\n",
      "60 Train Loss 139508.34\n",
      "61 Train Loss 118095.79\n",
      "62 Train Loss 117963.47\n",
      "63 Train Loss 117847.266\n",
      "64 Train Loss 117610.17\n",
      "65 Train Loss 117382.3\n",
      "66 Train Loss 117200.38\n",
      "67 Train Loss 117110.805\n",
      "68 Train Loss 116934.51\n",
      "69 Train Loss 117163.42\n",
      "70 Train Loss 116857.984\n",
      "71 Train Loss 116726.29\n",
      "72 Train Loss 117091.08\n",
      "73 Train Loss 116587.18\n",
      "74 Train Loss 116569.21\n",
      "75 Train Loss 116451.41\n",
      "76 Train Loss 116352.91\n",
      "77 Train Loss 116173.875\n",
      "78 Train Loss 169140.67\n",
      "79 Train Loss 117117.47\n",
      "80 Train Loss 116074.77\n",
      "81 Train Loss 117260.65\n",
      "82 Train Loss 116022.4\n",
      "83 Train Loss 115945.055\n",
      "84 Train Loss 115900.84\n",
      "85 Train Loss 115879.8\n",
      "86 Train Loss 115825.016\n",
      "87 Train Loss 115797.086\n",
      "88 Train Loss 115754.984\n",
      "89 Train Loss 115804.43\n",
      "90 Train Loss 115711.24\n",
      "91 Train Loss 115702.33\n",
      "92 Train Loss 115677.8\n",
      "93 Train Loss 115639.62\n",
      "94 Train Loss 115614.805\n",
      "95 Train Loss 115604.04\n",
      "96 Train Loss 115585.766\n",
      "97 Train Loss 115526.59\n",
      "98 Train Loss 115514.41\n",
      "99 Train Loss 115499.29\n",
      "100 Train Loss 115475.14\n",
      "101 Train Loss 115453.01\n",
      "102 Train Loss 115415.39\n",
      "103 Train Loss 115416.016\n",
      "104 Train Loss 115393.08\n",
      "105 Train Loss 115358.42\n",
      "106 Train Loss 115290.984\n",
      "107 Train Loss 115229.3\n",
      "108 Train Loss 115163.77\n",
      "109 Train Loss 115143.06\n",
      "110 Train Loss 115128.375\n",
      "111 Train Loss 115075.914\n",
      "112 Train Loss 115088.18\n",
      "113 Train Loss 115038.4\n",
      "114 Train Loss 115006.34\n",
      "115 Train Loss 114958.01\n",
      "116 Train Loss 114911.32\n",
      "117 Train Loss 114878.1\n",
      "118 Train Loss 114848.45\n",
      "119 Train Loss 114818.27\n",
      "120 Train Loss 114776.48\n",
      "121 Train Loss 114688.54\n",
      "122 Train Loss 114586.336\n",
      "123 Train Loss 114504.414\n",
      "124 Train Loss 114715.39\n",
      "125 Train Loss 114458.39\n",
      "126 Train Loss 114431.76\n",
      "127 Train Loss 114385.836\n",
      "128 Train Loss 114372.47\n",
      "129 Train Loss 114348.586\n",
      "130 Train Loss 114319.65\n",
      "131 Train Loss 114262.57\n",
      "132 Train Loss 114189.766\n",
      "133 Train Loss 114372.73\n",
      "134 Train Loss 114095.44\n",
      "135 Train Loss 114232.27\n",
      "136 Train Loss 114034.055\n",
      "137 Train Loss 113971.08\n",
      "138 Train Loss 113950.76\n",
      "139 Train Loss 113934.02\n",
      "140 Train Loss 113900.52\n",
      "141 Train Loss 113871.195\n",
      "142 Train Loss 113815.58\n",
      "143 Train Loss 115560.31\n",
      "144 Train Loss 113799.945\n",
      "145 Train Loss 113788.16\n",
      "146 Train Loss 113754.164\n",
      "147 Train Loss 113716.1\n",
      "148 Train Loss 113685.56\n",
      "149 Train Loss 113657.125\n",
      "150 Train Loss 113594.76\n",
      "151 Train Loss 113546.4\n",
      "152 Train Loss 113514.78\n",
      "153 Train Loss 113490.25\n",
      "154 Train Loss 113466.02\n",
      "155 Train Loss 113433.99\n",
      "156 Train Loss 113354.7\n",
      "157 Train Loss 113756.875\n",
      "158 Train Loss 113265.95\n",
      "159 Train Loss 113154.76\n",
      "160 Train Loss 113021.11\n",
      "161 Train Loss 113053.09\n",
      "162 Train Loss 112956.414\n",
      "163 Train Loss 112894.35\n",
      "164 Train Loss 112843.06\n",
      "165 Train Loss 112767.94\n",
      "166 Train Loss 112677.78\n",
      "167 Train Loss 112597.516\n",
      "168 Train Loss 112507.03\n",
      "169 Train Loss 112500.414\n",
      "170 Train Loss 112452.586\n",
      "171 Train Loss 112413.63\n",
      "172 Train Loss 112386.51\n",
      "173 Train Loss 112365.414\n",
      "174 Train Loss 112319.92\n",
      "175 Train Loss 112310.94\n",
      "176 Train Loss 112289.26\n",
      "177 Train Loss 112252.41\n",
      "178 Train Loss 112206.234\n",
      "179 Train Loss 112138.84\n",
      "180 Train Loss 112043.53\n",
      "181 Train Loss 112061.21\n",
      "182 Train Loss 112009.3\n",
      "183 Train Loss 111970.414\n",
      "184 Train Loss 111929.98\n",
      "185 Train Loss 111951.38\n",
      "186 Train Loss 111883.7\n",
      "187 Train Loss 111809.08\n",
      "188 Train Loss 111696.03\n",
      "189 Train Loss 111819.85\n",
      "190 Train Loss 111631.22\n",
      "191 Train Loss 111577.31\n",
      "192 Train Loss 111525.42\n",
      "193 Train Loss 111468.2\n",
      "194 Train Loss 111407.15\n",
      "195 Train Loss 111345.63\n",
      "196 Train Loss 111306.2\n",
      "197 Train Loss 111214.02\n",
      "198 Train Loss 111263.98\n",
      "199 Train Loss 111136.0\n",
      "200 Train Loss 111088.65\n",
      "201 Train Loss 110977.64\n",
      "202 Train Loss 110929.3\n",
      "203 Train Loss 110870.61\n",
      "204 Train Loss 111201.23\n",
      "205 Train Loss 110811.64\n",
      "206 Train Loss 110723.94\n",
      "207 Train Loss 110616.16\n",
      "208 Train Loss 110531.78\n",
      "209 Train Loss 110428.734\n",
      "210 Train Loss 110370.64\n",
      "211 Train Loss 110287.4\n",
      "212 Train Loss 110199.09\n",
      "213 Train Loss 110149.82\n",
      "214 Train Loss 110119.98\n",
      "215 Train Loss 110080.54\n",
      "216 Train Loss 110042.88\n",
      "217 Train Loss 110011.47\n",
      "218 Train Loss 109968.44\n",
      "219 Train Loss 109883.6\n",
      "220 Train Loss 109821.7\n",
      "221 Train Loss 109703.52\n",
      "222 Train Loss 109605.86\n",
      "223 Train Loss 109546.34\n",
      "224 Train Loss 109477.16\n",
      "225 Train Loss 109406.836\n",
      "226 Train Loss 109325.22\n",
      "227 Train Loss 109299.35\n",
      "228 Train Loss 109242.64\n",
      "229 Train Loss 109182.8\n",
      "230 Train Loss 109110.09\n",
      "231 Train Loss 109034.13\n",
      "232 Train Loss 108933.99\n",
      "233 Train Loss 108840.22\n",
      "234 Train Loss 108761.4\n",
      "235 Train Loss 108737.984\n",
      "236 Train Loss 108713.24\n",
      "237 Train Loss 108683.76\n",
      "238 Train Loss 108657.375\n",
      "239 Train Loss 108632.95\n",
      "240 Train Loss 108596.39\n",
      "241 Train Loss 108529.39\n",
      "242 Train Loss 108511.9\n",
      "243 Train Loss 108480.41\n",
      "244 Train Loss 108433.29\n",
      "245 Train Loss 108407.76\n",
      "246 Train Loss 108381.734\n",
      "247 Train Loss 108358.92\n",
      "248 Train Loss 108335.68\n",
      "249 Train Loss 108313.28\n",
      "250 Train Loss 108274.766\n",
      "251 Train Loss 108221.07\n",
      "252 Train Loss 108156.34\n",
      "253 Train Loss 108083.6\n",
      "254 Train Loss 108026.96\n",
      "255 Train Loss 107986.86\n",
      "256 Train Loss 107959.32\n",
      "257 Train Loss 107915.69\n",
      "258 Train Loss 107874.54\n",
      "259 Train Loss 107846.055\n",
      "260 Train Loss 107822.94\n",
      "261 Train Loss 107794.05\n",
      "262 Train Loss 107755.31\n",
      "263 Train Loss 107679.57\n",
      "264 Train Loss 107654.63\n",
      "265 Train Loss 107597.95\n",
      "266 Train Loss 107567.55\n",
      "267 Train Loss 107518.9\n",
      "268 Train Loss 107490.42\n",
      "269 Train Loss 107479.9\n",
      "270 Train Loss 107471.34\n",
      "271 Train Loss 107456.58\n",
      "272 Train Loss 107443.94\n",
      "273 Train Loss 107414.555\n",
      "274 Train Loss 107386.06\n",
      "275 Train Loss 107348.96\n",
      "276 Train Loss 107307.16\n",
      "277 Train Loss 107350.58\n",
      "278 Train Loss 107279.555\n",
      "279 Train Loss 107222.484\n",
      "280 Train Loss 107222.73\n",
      "281 Train Loss 107198.79\n",
      "282 Train Loss 107168.336\n",
      "283 Train Loss 107152.664\n",
      "284 Train Loss 107135.71\n",
      "285 Train Loss 107112.98\n",
      "286 Train Loss 107084.13\n",
      "287 Train Loss 107063.98\n",
      "288 Train Loss 107019.3\n",
      "289 Train Loss 106937.62\n",
      "290 Train Loss 107022.484\n",
      "291 Train Loss 106895.625\n",
      "292 Train Loss 106802.62\n",
      "293 Train Loss 106772.63\n",
      "294 Train Loss 106674.05\n",
      "295 Train Loss 106646.4\n",
      "296 Train Loss 106627.61\n",
      "297 Train Loss 106626.75\n",
      "298 Train Loss 106608.09\n",
      "299 Train Loss 106648.68\n",
      "300 Train Loss 106596.34\n",
      "301 Train Loss 106582.69\n",
      "302 Train Loss 106559.86\n",
      "303 Train Loss 106516.94\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "304 Train Loss 106473.21\n",
      "305 Train Loss 106436.766\n",
      "306 Train Loss 106422.09\n",
      "307 Train Loss 106403.805\n",
      "308 Train Loss 106524.02\n",
      "309 Train Loss 106349.73\n",
      "310 Train Loss 106313.984\n",
      "311 Train Loss 106278.28\n",
      "312 Train Loss 106253.52\n",
      "313 Train Loss 106227.195\n",
      "314 Train Loss 106196.266\n",
      "315 Train Loss 106166.33\n",
      "316 Train Loss 106137.07\n",
      "317 Train Loss 106110.18\n",
      "318 Train Loss 106075.51\n",
      "319 Train Loss 106036.5\n",
      "320 Train Loss 106008.19\n",
      "321 Train Loss 105991.79\n",
      "322 Train Loss 105974.555\n",
      "323 Train Loss 105970.24\n",
      "324 Train Loss 105953.19\n",
      "325 Train Loss 105933.69\n",
      "326 Train Loss 105902.664\n",
      "327 Train Loss 105874.47\n",
      "328 Train Loss 105854.555\n",
      "329 Train Loss 105809.82\n",
      "330 Train Loss 105812.87\n",
      "331 Train Loss 105792.625\n",
      "332 Train Loss 105767.35\n",
      "333 Train Loss 105753.38\n",
      "334 Train Loss 105771.52\n",
      "335 Train Loss 105741.57\n",
      "336 Train Loss 105730.63\n",
      "337 Train Loss 105723.87\n",
      "338 Train Loss 105710.445\n",
      "339 Train Loss 105704.06\n",
      "340 Train Loss 105696.96\n",
      "341 Train Loss 105688.01\n",
      "342 Train Loss 105677.38\n",
      "343 Train Loss 105661.94\n",
      "344 Train Loss 105639.58\n",
      "345 Train Loss 105625.13\n",
      "346 Train Loss 105616.08\n",
      "347 Train Loss 105604.56\n",
      "348 Train Loss 105596.52\n",
      "349 Train Loss 105587.664\n",
      "350 Train Loss 105575.336\n",
      "351 Train Loss 105561.33\n",
      "352 Train Loss 105541.36\n",
      "353 Train Loss 105528.086\n",
      "354 Train Loss 105516.72\n",
      "355 Train Loss 105510.12\n",
      "356 Train Loss 105500.945\n",
      "357 Train Loss 105492.484\n",
      "358 Train Loss 105476.305\n",
      "359 Train Loss 105461.64\n",
      "360 Train Loss 105448.086\n",
      "361 Train Loss 105446.69\n",
      "362 Train Loss 105440.75\n",
      "363 Train Loss 105436.43\n",
      "364 Train Loss 105436.31\n",
      "365 Train Loss 105433.07\n",
      "366 Train Loss 105431.09\n",
      "367 Train Loss 105428.68\n",
      "368 Train Loss 105420.13\n",
      "369 Train Loss 105407.07\n",
      "370 Train Loss 105399.3\n",
      "371 Train Loss 105390.65\n",
      "372 Train Loss 105385.1\n",
      "373 Train Loss 105381.945\n",
      "374 Train Loss 105377.42\n",
      "375 Train Loss 105370.3\n",
      "376 Train Loss 105362.31\n",
      "377 Train Loss 105349.56\n",
      "378 Train Loss 105336.56\n",
      "379 Train Loss 105323.3\n",
      "380 Train Loss 105312.76\n",
      "381 Train Loss 105299.56\n",
      "382 Train Loss 105290.38\n",
      "383 Train Loss 105279.98\n",
      "384 Train Loss 105274.29\n",
      "385 Train Loss 105268.89\n",
      "386 Train Loss 105263.5\n",
      "387 Train Loss 105257.27\n",
      "388 Train Loss 105254.37\n",
      "389 Train Loss 105248.4\n",
      "390 Train Loss 105244.07\n",
      "391 Train Loss 105237.61\n",
      "392 Train Loss 105230.68\n",
      "393 Train Loss 105224.484\n",
      "394 Train Loss 105219.44\n",
      "395 Train Loss 105216.836\n",
      "396 Train Loss 105211.45\n",
      "397 Train Loss 105207.414\n",
      "398 Train Loss 105204.11\n",
      "399 Train Loss 105199.64\n",
      "400 Train Loss 105194.46\n",
      "401 Train Loss 105184.79\n",
      "402 Train Loss 105175.836\n",
      "403 Train Loss 105171.61\n",
      "404 Train Loss 105168.55\n",
      "405 Train Loss 105166.164\n",
      "406 Train Loss 105164.78\n",
      "407 Train Loss 105163.555\n",
      "408 Train Loss 105161.86\n",
      "409 Train Loss 105159.055\n",
      "410 Train Loss 105154.68\n",
      "411 Train Loss 105148.16\n",
      "412 Train Loss 105139.75\n",
      "413 Train Loss 105128.62\n",
      "414 Train Loss 105120.03\n",
      "415 Train Loss 105113.67\n",
      "416 Train Loss 105109.56\n",
      "417 Train Loss 105105.445\n",
      "418 Train Loss 105098.3\n",
      "419 Train Loss 105088.7\n",
      "420 Train Loss 105075.93\n",
      "421 Train Loss 105071.97\n",
      "422 Train Loss 105067.07\n",
      "423 Train Loss 105065.695\n",
      "424 Train Loss 105063.89\n",
      "425 Train Loss 105062.734\n",
      "426 Train Loss 105061.37\n",
      "427 Train Loss 105058.35\n",
      "428 Train Loss 105054.555\n",
      "429 Train Loss 105042.37\n",
      "430 Train Loss 105020.51\n",
      "431 Train Loss 105002.836\n",
      "432 Train Loss 104986.3\n",
      "433 Train Loss 104973.56\n",
      "434 Train Loss 104957.125\n",
      "435 Train Loss 104949.11\n",
      "436 Train Loss 104936.77\n",
      "437 Train Loss 104938.16\n",
      "438 Train Loss 104923.41\n",
      "439 Train Loss 104917.016\n",
      "440 Train Loss 105102.45\n",
      "441 Train Loss 104904.3\n",
      "442 Train Loss 104894.05\n",
      "443 Train Loss 104890.695\n",
      "444 Train Loss 104881.68\n",
      "445 Train Loss 104878.08\n",
      "446 Train Loss 104871.08\n",
      "447 Train Loss 104859.48\n",
      "448 Train Loss 104850.33\n",
      "449 Train Loss 104840.83\n",
      "450 Train Loss 104924.82\n",
      "451 Train Loss 104834.086\n",
      "452 Train Loss 104822.55\n",
      "453 Train Loss 104810.08\n",
      "454 Train Loss 104804.28\n",
      "455 Train Loss 104799.21\n",
      "456 Train Loss 104787.96\n",
      "457 Train Loss 104803.08\n",
      "458 Train Loss 104779.46\n",
      "459 Train Loss 104771.9\n",
      "460 Train Loss 104762.78\n",
      "461 Train Loss 104753.28\n",
      "462 Train Loss 104741.14\n",
      "463 Train Loss 104728.99\n",
      "464 Train Loss 104720.46\n",
      "465 Train Loss 104716.58\n",
      "466 Train Loss 104721.91\n",
      "467 Train Loss 104712.05\n",
      "468 Train Loss 104706.23\n",
      "469 Train Loss 104699.234\n",
      "470 Train Loss 104685.41\n",
      "471 Train Loss 104670.484\n",
      "472 Train Loss 104663.64\n",
      "473 Train Loss 104652.79\n",
      "474 Train Loss 104648.72\n",
      "475 Train Loss 104644.195\n",
      "476 Train Loss 104638.06\n",
      "477 Train Loss 104633.49\n",
      "478 Train Loss 104624.44\n",
      "479 Train Loss 104617.36\n",
      "480 Train Loss 104618.16\n",
      "481 Train Loss 104612.72\n",
      "482 Train Loss 104606.66\n",
      "483 Train Loss 104599.37\n",
      "484 Train Loss 104590.4\n",
      "485 Train Loss 104589.23\n",
      "486 Train Loss 104582.91\n",
      "487 Train Loss 104578.61\n",
      "488 Train Loss 104572.39\n",
      "489 Train Loss 104561.164\n",
      "490 Train Loss 104549.33\n",
      "491 Train Loss 104539.64\n",
      "492 Train Loss 104535.25\n",
      "493 Train Loss 104534.1\n",
      "494 Train Loss 104531.12\n",
      "495 Train Loss 104527.9\n",
      "496 Train Loss 104522.35\n",
      "497 Train Loss 104518.086\n",
      "498 Train Loss 104515.12\n",
      "499 Train Loss 104511.484\n",
      "500 Train Loss 104503.93\n",
      "501 Train Loss 104498.13\n",
      "502 Train Loss 104492.914\n",
      "503 Train Loss 104490.13\n",
      "504 Train Loss 104491.695\n",
      "505 Train Loss 104487.88\n",
      "506 Train Loss 104485.41\n",
      "507 Train Loss 104474.91\n",
      "508 Train Loss 104466.51\n",
      "509 Train Loss 104456.81\n",
      "510 Train Loss 104452.96\n",
      "511 Train Loss 104449.73\n",
      "512 Train Loss 104443.164\n",
      "513 Train Loss 104439.48\n",
      "514 Train Loss 104435.98\n",
      "515 Train Loss 104432.04\n",
      "516 Train Loss 104430.336\n",
      "517 Train Loss 104427.71\n",
      "518 Train Loss 104423.47\n",
      "519 Train Loss 104419.76\n",
      "520 Train Loss 104418.664\n",
      "521 Train Loss 104414.78\n",
      "522 Train Loss 104411.62\n",
      "523 Train Loss 104407.96\n",
      "524 Train Loss 104403.5\n",
      "525 Train Loss 104396.04\n",
      "526 Train Loss 104388.86\n",
      "527 Train Loss 104392.5\n",
      "528 Train Loss 104381.56\n",
      "529 Train Loss 104373.73\n",
      "530 Train Loss 104368.445\n",
      "531 Train Loss 104360.97\n",
      "532 Train Loss 104359.91\n",
      "533 Train Loss 104351.82\n",
      "534 Train Loss 104347.99\n",
      "535 Train Loss 104344.266\n",
      "536 Train Loss 104339.25\n",
      "537 Train Loss 104335.8\n",
      "538 Train Loss 104333.68\n",
      "539 Train Loss 104333.42\n",
      "540 Train Loss 104331.3\n",
      "541 Train Loss 104329.46\n",
      "542 Train Loss 104326.3\n",
      "543 Train Loss 104322.64\n",
      "544 Train Loss 104317.79\n",
      "545 Train Loss 104312.71\n",
      "546 Train Loss 104307.19\n",
      "547 Train Loss 104305.0\n",
      "548 Train Loss 104336.3\n",
      "549 Train Loss 104297.484\n",
      "550 Train Loss 104292.65\n",
      "551 Train Loss 104288.39\n",
      "552 Train Loss 104283.53\n",
      "553 Train Loss 104281.016\n",
      "554 Train Loss 104273.27\n",
      "555 Train Loss 104273.63\n",
      "556 Train Loss 104270.33\n",
      "557 Train Loss 104267.16\n",
      "558 Train Loss 104264.37\n",
      "559 Train Loss 104262.46\n",
      "560 Train Loss 104261.836\n",
      "561 Train Loss 104260.56\n",
      "562 Train Loss 104259.266\n",
      "563 Train Loss 104256.69\n",
      "564 Train Loss 104252.64\n",
      "565 Train Loss 104249.984\n",
      "566 Train Loss 104250.38\n",
      "567 Train Loss 104247.88\n",
      "568 Train Loss 104245.66\n",
      "569 Train Loss 104243.11\n",
      "570 Train Loss 104239.4\n",
      "571 Train Loss 104237.44\n",
      "572 Train Loss 104235.8\n",
      "573 Train Loss 104233.72\n",
      "574 Train Loss 104231.45\n",
      "575 Train Loss 104226.445\n",
      "576 Train Loss 104220.49\n",
      "577 Train Loss 104215.36\n",
      "578 Train Loss 104208.93\n",
      "579 Train Loss 104208.34\n",
      "580 Train Loss 104206.36\n",
      "581 Train Loss 104204.03\n",
      "582 Train Loss 104201.11\n",
      "583 Train Loss 104199.516\n",
      "584 Train Loss 104197.664\n",
      "585 Train Loss 104194.516\n",
      "586 Train Loss 104191.19\n",
      "587 Train Loss 104188.28\n",
      "588 Train Loss 104186.58\n",
      "589 Train Loss 104185.31\n",
      "590 Train Loss 104183.98\n",
      "591 Train Loss 104183.17\n",
      "592 Train Loss 104182.01\n",
      "593 Train Loss 104180.6\n",
      "594 Train Loss 104178.27\n",
      "595 Train Loss 104175.11\n",
      "596 Train Loss 104189.82\n",
      "597 Train Loss 104173.65\n",
      "598 Train Loss 104170.49\n",
      "599 Train Loss 104168.55\n",
      "600 Train Loss 104166.69\n",
      "601 Train Loss 104163.97\n",
      "602 Train Loss 104162.89\n",
      "603 Train Loss 104161.375\n",
      "604 Train Loss 104159.164\n",
      "605 Train Loss 104156.78\n",
      "606 Train Loss 104153.18\n",
      "607 Train Loss 104149.3\n",
      "608 Train Loss 104148.055\n",
      "609 Train Loss 104145.65\n",
      "610 Train Loss 104143.96\n",
      "611 Train Loss 104141.516\n",
      "612 Train Loss 104137.74\n",
      "613 Train Loss 104134.3\n",
      "614 Train Loss 104131.57\n",
      "615 Train Loss 104129.1\n",
      "616 Train Loss 104127.1\n",
      "617 Train Loss 104125.01\n",
      "618 Train Loss 104122.89\n",
      "619 Train Loss 104120.35\n",
      "620 Train Loss 104116.33\n",
      "621 Train Loss 104113.45\n",
      "622 Train Loss 104110.4\n",
      "623 Train Loss 104104.16\n",
      "624 Train Loss 104098.88\n",
      "625 Train Loss 104096.625\n",
      "626 Train Loss 104094.13\n",
      "627 Train Loss 104091.11\n",
      "628 Train Loss 104088.92\n",
      "629 Train Loss 104086.984\n",
      "630 Train Loss 104084.01\n",
      "631 Train Loss 104080.0\n",
      "632 Train Loss 104083.0\n",
      "633 Train Loss 104077.57\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "634 Train Loss 104075.04\n",
      "635 Train Loss 104073.56\n",
      "636 Train Loss 104071.87\n",
      "637 Train Loss 104070.19\n",
      "638 Train Loss 104069.5\n",
      "639 Train Loss 104068.58\n",
      "640 Train Loss 104067.44\n",
      "641 Train Loss 104066.125\n",
      "642 Train Loss 104065.19\n",
      "643 Train Loss 104064.02\n",
      "644 Train Loss 104062.18\n",
      "645 Train Loss 104060.195\n",
      "646 Train Loss 104058.28\n",
      "647 Train Loss 104055.836\n",
      "648 Train Loss 104052.66\n",
      "649 Train Loss 104051.61\n",
      "650 Train Loss 104049.68\n",
      "651 Train Loss 104047.8\n",
      "652 Train Loss 104046.234\n",
      "653 Train Loss 104044.19\n",
      "654 Train Loss 104042.13\n",
      "655 Train Loss 104041.15\n",
      "656 Train Loss 104040.15\n",
      "657 Train Loss 104038.94\n",
      "658 Train Loss 104036.95\n",
      "659 Train Loss 104034.55\n",
      "660 Train Loss 104032.68\n",
      "661 Train Loss 104030.19\n",
      "662 Train Loss 104030.14\n",
      "663 Train Loss 104028.734\n",
      "664 Train Loss 104027.09\n",
      "665 Train Loss 104025.95\n",
      "666 Train Loss 104024.94\n",
      "667 Train Loss 104024.055\n",
      "668 Train Loss 104023.06\n",
      "669 Train Loss 104022.5\n",
      "670 Train Loss 104021.76\n",
      "671 Train Loss 104021.195\n",
      "672 Train Loss 104020.5\n",
      "673 Train Loss 104019.42\n",
      "674 Train Loss 104017.86\n",
      "675 Train Loss 104015.85\n",
      "676 Train Loss 104013.47\n",
      "677 Train Loss 104012.484\n",
      "678 Train Loss 104009.9\n",
      "679 Train Loss 104006.69\n",
      "680 Train Loss 104003.086\n",
      "681 Train Loss 103999.87\n",
      "682 Train Loss 104006.07\n",
      "683 Train Loss 103998.484\n",
      "684 Train Loss 103996.46\n",
      "685 Train Loss 103995.18\n",
      "686 Train Loss 103994.57\n",
      "687 Train Loss 103993.695\n",
      "688 Train Loss 103992.75\n",
      "689 Train Loss 103990.82\n",
      "690 Train Loss 103987.32\n",
      "691 Train Loss 103984.29\n",
      "692 Train Loss 103981.04\n",
      "693 Train Loss 103980.02\n",
      "694 Train Loss 103978.43\n",
      "695 Train Loss 103977.68\n",
      "696 Train Loss 103976.72\n",
      "697 Train Loss 103975.23\n",
      "698 Train Loss 103973.51\n",
      "699 Train Loss 103969.92\n",
      "700 Train Loss 103965.26\n",
      "701 Train Loss 103964.16\n",
      "702 Train Loss 103962.6\n",
      "703 Train Loss 103960.74\n",
      "704 Train Loss 103961.76\n",
      "705 Train Loss 103958.086\n",
      "706 Train Loss 103956.66\n",
      "707 Train Loss 103955.22\n",
      "708 Train Loss 103954.6\n",
      "709 Train Loss 103953.75\n",
      "710 Train Loss 103952.84\n",
      "711 Train Loss 103951.85\n",
      "712 Train Loss 103949.68\n",
      "713 Train Loss 103947.94\n",
      "714 Train Loss 103945.32\n",
      "715 Train Loss 103941.66\n",
      "716 Train Loss 103938.586\n",
      "717 Train Loss 103935.64\n",
      "718 Train Loss 103932.56\n",
      "719 Train Loss 103928.516\n",
      "720 Train Loss 103928.28\n",
      "721 Train Loss 103933.67\n",
      "722 Train Loss 103923.14\n",
      "723 Train Loss 103926.51\n",
      "724 Train Loss 103919.1\n",
      "725 Train Loss 103917.05\n",
      "726 Train Loss 103914.16\n",
      "727 Train Loss 103911.58\n",
      "728 Train Loss 103910.11\n",
      "729 Train Loss 103909.19\n",
      "730 Train Loss 103908.93\n",
      "731 Train Loss 103908.19\n",
      "732 Train Loss 103907.625\n",
      "733 Train Loss 103906.73\n",
      "734 Train Loss 103904.71\n",
      "735 Train Loss 103902.47\n",
      "736 Train Loss 103900.54\n",
      "737 Train Loss 103896.664\n",
      "738 Train Loss 103894.83\n",
      "739 Train Loss 103893.44\n",
      "740 Train Loss 103892.27\n",
      "741 Train Loss 103890.41\n",
      "742 Train Loss 103889.34\n",
      "743 Train Loss 103888.766\n",
      "744 Train Loss 103888.164\n",
      "745 Train Loss 103886.734\n",
      "746 Train Loss 103885.72\n",
      "747 Train Loss 103884.21\n",
      "748 Train Loss 103883.27\n",
      "749 Train Loss 103882.38\n",
      "750 Train Loss 103881.664\n",
      "751 Train Loss 103880.99\n",
      "752 Train Loss 103879.914\n",
      "753 Train Loss 103878.47\n",
      "754 Train Loss 103876.86\n",
      "755 Train Loss 103899.48\n",
      "756 Train Loss 103876.11\n",
      "757 Train Loss 103874.336\n",
      "758 Train Loss 103872.266\n",
      "759 Train Loss 103869.52\n",
      "760 Train Loss 103867.7\n",
      "761 Train Loss 103885.14\n",
      "762 Train Loss 103866.18\n",
      "763 Train Loss 103865.21\n",
      "764 Train Loss 103864.03\n",
      "765 Train Loss 103863.61\n",
      "766 Train Loss 103862.15\n",
      "767 Train Loss 103860.81\n",
      "768 Train Loss 103859.25\n",
      "769 Train Loss 103856.9\n",
      "770 Train Loss 103856.3\n",
      "771 Train Loss 103854.09\n",
      "772 Train Loss 103852.99\n",
      "773 Train Loss 103851.86\n",
      "774 Train Loss 103850.91\n",
      "775 Train Loss 103850.164\n",
      "776 Train Loss 103849.4\n",
      "777 Train Loss 103848.77\n",
      "778 Train Loss 103848.05\n",
      "779 Train Loss 103847.02\n",
      "780 Train Loss 103846.12\n",
      "781 Train Loss 103845.375\n",
      "782 Train Loss 103844.19\n",
      "783 Train Loss 103843.22\n",
      "784 Train Loss 103841.83\n",
      "785 Train Loss 103839.164\n",
      "786 Train Loss 103835.164\n",
      "787 Train Loss 103827.9\n",
      "788 Train Loss 104018.77\n",
      "789 Train Loss 103825.79\n",
      "790 Train Loss 103820.67\n",
      "791 Train Loss 103812.67\n",
      "792 Train Loss 103801.83\n",
      "793 Train Loss 103794.766\n",
      "794 Train Loss 103831.734\n",
      "795 Train Loss 103792.234\n",
      "796 Train Loss 103790.38\n",
      "797 Train Loss 103787.93\n",
      "798 Train Loss 103787.4\n",
      "799 Train Loss 103781.63\n",
      "800 Train Loss 103777.78\n",
      "801 Train Loss 103773.78\n",
      "802 Train Loss 103771.086\n",
      "803 Train Loss 103766.47\n",
      "804 Train Loss 103766.06\n",
      "805 Train Loss 103761.67\n",
      "806 Train Loss 103757.82\n",
      "807 Train Loss 103753.72\n",
      "808 Train Loss 103749.29\n",
      "809 Train Loss 103749.08\n",
      "810 Train Loss 103745.9\n",
      "811 Train Loss 103741.836\n",
      "812 Train Loss 103748.266\n",
      "813 Train Loss 103738.78\n",
      "814 Train Loss 103735.4\n",
      "815 Train Loss 103728.445\n",
      "816 Train Loss 103723.6\n",
      "817 Train Loss 103718.24\n",
      "818 Train Loss 103713.33\n",
      "819 Train Loss 103721.98\n",
      "820 Train Loss 103708.38\n",
      "821 Train Loss 103711.4\n",
      "822 Train Loss 103703.47\n",
      "823 Train Loss 103696.7\n",
      "824 Train Loss 103691.74\n",
      "825 Train Loss 103690.46\n",
      "826 Train Loss 103688.99\n",
      "827 Train Loss 103685.38\n",
      "828 Train Loss 103684.086\n",
      "829 Train Loss 103680.266\n",
      "830 Train Loss 103678.68\n",
      "831 Train Loss 103676.83\n",
      "832 Train Loss 103675.84\n",
      "833 Train Loss 103674.19\n",
      "834 Train Loss 103671.484\n",
      "835 Train Loss 103669.8\n",
      "836 Train Loss 103667.27\n",
      "837 Train Loss 103665.72\n",
      "838 Train Loss 103664.47\n",
      "839 Train Loss 103663.17\n",
      "840 Train Loss 103661.52\n",
      "841 Train Loss 103659.95\n",
      "842 Train Loss 103658.36\n",
      "843 Train Loss 103656.6\n",
      "844 Train Loss 103653.5\n",
      "845 Train Loss 103651.77\n",
      "846 Train Loss 103650.27\n",
      "847 Train Loss 103648.93\n",
      "848 Train Loss 103648.88\n",
      "849 Train Loss 103648.34\n",
      "850 Train Loss 103647.805\n",
      "851 Train Loss 103647.53\n",
      "852 Train Loss 103647.3\n",
      "853 Train Loss 103646.586\n",
      "854 Train Loss 103645.67\n",
      "855 Train Loss 103644.65\n",
      "856 Train Loss 103643.29\n",
      "857 Train Loss 103642.41\n",
      "858 Train Loss 103640.79\n",
      "859 Train Loss 103638.664\n",
      "860 Train Loss 103637.41\n",
      "861 Train Loss 103635.29\n",
      "862 Train Loss 103633.74\n",
      "863 Train Loss 103632.33\n",
      "864 Train Loss 103630.84\n",
      "865 Train Loss 103629.2\n",
      "866 Train Loss 103627.01\n",
      "867 Train Loss 103626.62\n",
      "868 Train Loss 103625.43\n",
      "869 Train Loss 103623.22\n",
      "870 Train Loss 103621.39\n",
      "871 Train Loss 103619.7\n",
      "872 Train Loss 103618.14\n",
      "873 Train Loss 103615.29\n",
      "874 Train Loss 103612.02\n",
      "875 Train Loss 103608.1\n",
      "876 Train Loss 103605.64\n",
      "877 Train Loss 103603.47\n",
      "878 Train Loss 103604.74\n",
      "879 Train Loss 103601.95\n",
      "880 Train Loss 103599.836\n",
      "881 Train Loss 103597.95\n",
      "882 Train Loss 103595.84\n",
      "883 Train Loss 103594.89\n",
      "884 Train Loss 103593.59\n",
      "885 Train Loss 103592.734\n",
      "886 Train Loss 103591.92\n",
      "887 Train Loss 103590.98\n",
      "888 Train Loss 103590.27\n",
      "889 Train Loss 103589.766\n",
      "890 Train Loss 103589.17\n",
      "891 Train Loss 103588.46\n",
      "892 Train Loss 103587.64\n",
      "893 Train Loss 103586.69\n",
      "894 Train Loss 103585.28\n",
      "895 Train Loss 103584.25\n",
      "896 Train Loss 103582.86\n",
      "897 Train Loss 103581.195\n",
      "898 Train Loss 103580.18\n",
      "899 Train Loss 103579.34\n",
      "900 Train Loss 103578.47\n",
      "901 Train Loss 103577.83\n",
      "902 Train Loss 103577.25\n",
      "903 Train Loss 103576.47\n",
      "904 Train Loss 103575.695\n",
      "905 Train Loss 103574.87\n",
      "906 Train Loss 103574.125\n",
      "907 Train Loss 103572.73\n",
      "908 Train Loss 103571.586\n",
      "909 Train Loss 103570.53\n",
      "910 Train Loss 103569.55\n",
      "911 Train Loss 103568.53\n",
      "912 Train Loss 103567.7\n",
      "913 Train Loss 103566.61\n",
      "914 Train Loss 103565.64\n",
      "915 Train Loss 103564.71\n",
      "916 Train Loss 103564.07\n",
      "917 Train Loss 103563.84\n",
      "918 Train Loss 103563.49\n",
      "919 Train Loss 103563.28\n",
      "920 Train Loss 103562.99\n",
      "921 Train Loss 103562.79\n",
      "922 Train Loss 103562.52\n",
      "923 Train Loss 103562.18\n",
      "924 Train Loss 103561.87\n",
      "925 Train Loss 103561.336\n",
      "926 Train Loss 103560.35\n",
      "927 Train Loss 103559.086\n",
      "928 Train Loss 103557.6\n",
      "929 Train Loss 103556.32\n",
      "930 Train Loss 103555.27\n",
      "931 Train Loss 103554.29\n",
      "932 Train Loss 103553.02\n",
      "933 Train Loss 103551.02\n",
      "934 Train Loss 103548.586\n",
      "935 Train Loss 103546.57\n",
      "936 Train Loss 103545.15\n",
      "937 Train Loss 103543.95\n",
      "938 Train Loss 103542.85\n",
      "939 Train Loss 103542.49\n",
      "940 Train Loss 103541.86\n",
      "941 Train Loss 103541.44\n",
      "942 Train Loss 103541.1\n",
      "943 Train Loss 103540.766\n",
      "944 Train Loss 103540.48\n",
      "945 Train Loss 103539.99\n",
      "946 Train Loss 103539.26\n",
      "947 Train Loss 103538.07\n",
      "948 Train Loss 103536.1\n",
      "949 Train Loss 103534.92\n",
      "950 Train Loss 103530.336\n",
      "951 Train Loss 103526.28\n",
      "952 Train Loss 103532.14\n",
      "953 Train Loss 103523.0\n",
      "954 Train Loss 103532.94\n",
      "955 Train Loss 103520.86\n",
      "956 Train Loss 103517.695\n",
      "957 Train Loss 103516.17\n",
      "958 Train Loss 103514.74\n",
      "959 Train Loss 103515.086\n",
      "960 Train Loss 103513.12\n",
      "961 Train Loss 103511.77\n",
      "962 Train Loss 103509.52\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "963 Train Loss 103508.58\n",
      "964 Train Loss 103508.01\n",
      "965 Train Loss 103507.49\n",
      "966 Train Loss 103507.15\n",
      "967 Train Loss 103506.83\n",
      "968 Train Loss 103506.234\n",
      "969 Train Loss 103505.734\n",
      "970 Train Loss 103505.35\n",
      "971 Train Loss 103504.99\n",
      "972 Train Loss 103504.6\n",
      "973 Train Loss 103504.38\n",
      "974 Train Loss 103504.12\n",
      "975 Train Loss 103503.66\n",
      "976 Train Loss 103502.78\n",
      "977 Train Loss 103502.07\n",
      "978 Train Loss 103501.49\n",
      "979 Train Loss 103500.75\n",
      "980 Train Loss 103500.07\n",
      "981 Train Loss 103499.766\n",
      "982 Train Loss 103499.414\n",
      "983 Train Loss 103498.97\n",
      "984 Train Loss 103498.45\n",
      "985 Train Loss 103497.8\n",
      "986 Train Loss 103497.06\n",
      "987 Train Loss 103496.734\n",
      "988 Train Loss 103496.49\n",
      "989 Train Loss 103496.195\n",
      "990 Train Loss 103495.914\n",
      "991 Train Loss 103495.46\n",
      "992 Train Loss 103495.78\n",
      "993 Train Loss 103495.14\n",
      "994 Train Loss 103494.414\n",
      "995 Train Loss 103493.51\n",
      "996 Train Loss 103492.25\n",
      "997 Train Loss 103491.07\n",
      "998 Train Loss 103490.39\n",
      "999 Train Loss 103489.14\n",
      "1000 Train Loss 103488.516\n",
      "1001 Train Loss 103487.695\n",
      "1002 Train Loss 103486.69\n",
      "1003 Train Loss 103486.26\n",
      "1004 Train Loss 103485.94\n",
      "1005 Train Loss 103485.664\n",
      "1006 Train Loss 103485.445\n",
      "1007 Train Loss 103485.13\n",
      "1008 Train Loss 103484.8\n",
      "1009 Train Loss 103484.42\n",
      "1010 Train Loss 103483.98\n",
      "1011 Train Loss 103483.38\n",
      "1012 Train Loss 103482.516\n",
      "1013 Train Loss 103481.47\n",
      "1014 Train Loss 103482.28\n",
      "1015 Train Loss 103480.664\n",
      "1016 Train Loss 103479.05\n",
      "1017 Train Loss 103476.88\n",
      "1018 Train Loss 103475.35\n",
      "1019 Train Loss 103472.875\n",
      "1020 Train Loss 103470.7\n",
      "1021 Train Loss 103469.03\n",
      "1022 Train Loss 103466.21\n",
      "1023 Train Loss 103464.44\n",
      "1024 Train Loss 103465.164\n",
      "1025 Train Loss 103463.01\n",
      "1026 Train Loss 103463.125\n",
      "1027 Train Loss 103462.086\n",
      "1028 Train Loss 103461.64\n",
      "1029 Train Loss 103460.984\n",
      "1030 Train Loss 103460.57\n",
      "1031 Train Loss 103459.95\n",
      "1032 Train Loss 103458.41\n",
      "1033 Train Loss 103457.04\n",
      "1034 Train Loss 103455.625\n",
      "1035 Train Loss 103454.4\n",
      "1036 Train Loss 103453.47\n",
      "1037 Train Loss 103452.195\n",
      "1038 Train Loss 103451.33\n",
      "1039 Train Loss 103450.32\n",
      "1040 Train Loss 103449.14\n",
      "1041 Train Loss 103447.445\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-f150601943b1>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPINN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPINN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                     loss, flat_grad, t, ls_func_evals = _strong_wolfe(\n\u001b[0;32m--> 426\u001b[0;31m                         obj_func, x_init, t, d, loss, flat_grad, gtd)\n\u001b[0m\u001b[1;32m    427\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0mopt_cond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflat_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mtolerance_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36m_strong_wolfe\u001b[0;34m(obj_func, x, t, d, f, g, gtd, c1, c2, tolerance_change, max_ls)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# evaluate objective and gradient using initial step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mf_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mls_func_evals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mgtd_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mobj_func\u001b[0;34m(x, t, d)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m                     \u001b[0;32mdef\u001b[0m \u001b[0mobj_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m                         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_directional_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                     loss, flat_grad, t, ls_func_evals = _strong_wolfe(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36m_directional_evaluate\u001b[0;34m(self, closure, x, t, d)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_directional_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0mflat_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gather_flat_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-23-b72583e830d1>\u001b[0m in \u001b[0;36mclosure\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    105\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Train Loss\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 107\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    108\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    109\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "\n",
    "train_loss_full = []\n",
    "test_loss_full = []\n",
    "beta_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(reps)\n",
    "    \n",
    "    torch.manual_seed(reps*36)\n",
    "    N_T = 500 #Total number of data points for 'y'\n",
    "    N_f = 10000 #Total number of collocation points \n",
    "    \n",
    "    xy_coll_np_array, xy_BC_np_array, u_BC_np_array = trainingdata(N_T,N_f,(reps)*22)\n",
    "        \n",
    "    xy_coll = torch.from_numpy(xy_coll_np_array).float().to(device)\n",
    "    xy_BC = torch.from_numpy(xy_BC_np_array).float().to(device)\n",
    "    u_BC = torch.from_numpy(u_BC_np_array).float().to(device)\n",
    "        \n",
    "    f_hat = torch.zeros(xy_coll.shape[0],1).to(device)\n",
    "\n",
    "    layers = np.array([2,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "\n",
    "    PINN = Sequentialmodel(layers)\n",
    "   \n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "    \n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=1, \n",
    "                              max_iter = 10000, \n",
    "                              max_eval = None, \n",
    "                              tolerance_grad = -1, \n",
    "                              tolerance_change = -1, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "    start_time = time.time()\n",
    "        \n",
    "    optimizer.step(PINN.closure)\n",
    "    \n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(PINN.train_loss)\n",
    "    test_loss_full.append(PINN.test_loss)\n",
    "    elapsed_time[reps] = time.time() - start_time\n",
    "    beta_full.append(PINN.beta_val)\n",
    "\n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "    \n",
    "mdic = {\"train_loss\": train_loss_full,\"test_loss\": test_loss_full,\"Time\": elapsed_time, \"beta\": beta_full, \"label\": label}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred = PINN.test(xy_test_tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "cmap = plt.cm.jet\n",
    "\n",
    "img3 = ax.imshow(np.transpose(u_pred.reshape(500,500)),vmin = 0,vmax = 1000,cmap = cmap,extent=[0,1,0,1],aspect = 0.75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + test_loss_full[i][-1]\n",
    "print(a/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + train_loss_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
