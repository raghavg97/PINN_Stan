{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 286,
     "status": "ok",
     "timestamp": 1660687093981,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "iAtv2UvNSq_u",
    "outputId": "68a82578-1b95-4343-a8ec-7635a4df93ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 280,
     "status": "ok",
     "timestamp": 1660687410736,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "mTLFQRt5Sq_y"
   },
   "outputs": [],
   "source": [
    "def true_2D_1(xt): #True function for 2D_1 Klein Gordon Equation x \\in [-50,50] , t \\in [0,10]\n",
    "    x = xt[:,0].reshape(-1,1)\n",
    "    t = xt[:,1].reshape(-1,1)\n",
    "    y = x*np.cos(5*np.pi*t) + np.power(x*t,3)\n",
    "    return y.reshape(-1,1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 4312,
     "status": "ok",
     "timestamp": 1660687098957,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "81bNHCY3Sq_y"
   },
   "outputs": [],
   "source": [
    "pi = np.pi\n",
    "\n",
    "loss_thresh = 0.1\n",
    "level = \"low\"\n",
    "label = \"KG_stanALR_\" + level\n",
    "\n",
    "x = np.linspace(0,1,500).reshape(-1,1)\n",
    "t = np.linspace(0,1,500).reshape(-1,1)\n",
    "\n",
    "X,T = np.meshgrid(x,t)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "T = T.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xt = np.hstack((X,T))\n",
    "\n",
    "y_true = true_2D_1(xt)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "#bound_pts_idx = ((X == -5) + (X == 5) + (T == 0)).reshape(-1,)\n",
    "\n",
    "#xt_bound = xt[bound_pts_idx,:]\n",
    "#y_bound = y_true[bound_pts_idx,:]\n",
    "\n",
    "\n",
    "xt_test_tensor = torch.from_numpy(xt).float().to(device)\n",
    "\n",
    "\n",
    "lb_xt = xt[0]\n",
    "ub_xt = xt[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1660687098958,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "YQgCA-PuSq_z"
   },
   "outputs": [],
   "source": [
    "def trainingdata(N_I,N_B,N_f,seed):\n",
    "    '''Boundary Conditions''' \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    x_BC1 = np.random.uniform(size = N_I).reshape(-1,1)\n",
    "    t_BC1 = np.zeros((N_I,1))\n",
    "    samples = np.hstack((x_BC1,t_BC1))\n",
    "    xt_BC1 = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    y_BC1 = true_2D_1(xt_BC1)\n",
    "    \n",
    "    x_BC2 = np.zeros((int(N_B/2),1))\n",
    "    t_BC2 = np.random.uniform(size = int(N_B/2)).reshape(-1,1)\n",
    "    samples = np.hstack((x_BC2,t_BC2))\n",
    "    xt_BC2 = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    y_BC2 = true_2D_1(xt_BC2)\n",
    "    \n",
    "    x_BC3 = np.ones((int(N_B/2),1))\n",
    "    t_BC3 = np.random.uniform(size = int(N_B/2)).reshape(-1,1)\n",
    "    samples = np.hstack((x_BC3,t_BC3))\n",
    "    xt_BC3 = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    y_BC3 = true_2D_1(xt_BC3)\n",
    "    \n",
    "    x_NBC = np.random.uniform(size = N_I).reshape(-1,1)\n",
    "    t_NBC = np.zeros((N_I,1))\n",
    "    samples = np.hstack((x_NBC,t_NBC))\n",
    "    xt_NBC = lb_xt + (ub_xt - lb_xt)*samples\n",
    "\n",
    "    \n",
    "    xt_BC = np.vstack((xt_BC1,xt_BC2,xt_BC3))\n",
    "    y_BC = np.vstack((y_BC1,y_BC2,y_BC3))\n",
    "\n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    samples = sampling(N_f)\n",
    "    xt_coll = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    \n",
    "    xt_coll = np.vstack((xt_coll, xt_BC,xt_NBC)) # append training points to collocation points \n",
    "\n",
    "    return xt_coll, xt_BC, y_BC,xt_NBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1660687098958,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "gTJxct8bSq_0"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,beta_init):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "            \n",
    "        \n",
    "        beta_mean = 1.0*torch.ones((50,len(layers)-2))\n",
    "        beta_std = 0.01*torch.ones((50,len(layers)-2))\n",
    "        \n",
    "        self.beta = Parameter(torch.normal(beta_mean,beta_std))\n",
    "        self.beta.requiresGrad = True\n",
    "        \n",
    "        self.lambdas = torch.ones((2,),device = device)\n",
    "        self.lambda_alpha = 0.1\n",
    "\n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xt):\n",
    "        if torch.is_tensor(xt) != True:         \n",
    "            xt = torch.from_numpy(xt)                \n",
    "        \n",
    "        ubxt = torch.from_numpy(ub_xt).float().to(device)\n",
    "        lbxt = torch.from_numpy(lb_xt).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xt = 2.0*(xt - lbxt)/(ubxt - lbxt) - 1.0\n",
    "        \n",
    "        #convert to float\n",
    "        a = xt.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            z1 = self.activation(z)\n",
    "            a = z1 + self.beta[:,i]*z*z1            \n",
    "        \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC(self,xt,y):\n",
    "                \n",
    "        loss_bc = self.loss_function(self.forward(xt), y)\n",
    "                \n",
    "        return loss_bc\n",
    "    \n",
    "    def loss_NBC(self,xt_NBC,N_hat):\n",
    "        g = xt_NBC.clone()             \n",
    "        g.requires_grad = True\n",
    "        y = self.forward(g) \n",
    "        \n",
    "        y_x_t = autograd.grad(y,g,torch.ones([xt_NBC.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]        \n",
    "        dy_dt = y_x_t[:,[1]]\n",
    "        \n",
    "        loss_nbc = self.loss_function(dy_dt, N_hat)\n",
    "                \n",
    "        return loss_nbc\n",
    "    \n",
    "    def loss_PDE(self, xt_coll, f_hat):\n",
    "        \n",
    "        g = xt_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        y = self.forward(g) \n",
    "        \n",
    "        \n",
    "        y_x_t = autograd.grad(y,g,torch.ones([xt_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        y_xx_tt = autograd.grad(y_x_t,g,torch.ones(xt_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        #du_dx = u_x_t[:,[0]]\n",
    "        \n",
    "        d2y_dx2 = y_xx_tt[:,[0]]\n",
    "        d2y_dt2 = y_xx_tt[:,[1]]    \n",
    "        \n",
    "\n",
    "        term1 = -25.0*(pi**2)*(g[:,0].reshape(-1,1))*torch.cos(5.0*pi*g[:,1].reshape(-1,1)) + 6*torch.pow(g[:,0].reshape(-1,1),3)*g[:,1].reshape(-1,1)  \n",
    "        term1 = term1.reshape(-1,1)\n",
    "        term2 = 6*torch.pow(g[:,1].reshape(-1,1),3)*g[:,0].reshape(-1,1)\n",
    "        term2 = term2.reshape(-1,1)\n",
    "        term3 = torch.pow(g[:,0]*torch.cos(5.0*pi*g[:,1]) + torch.pow(g[:,0]*g[:,1],3),3)\n",
    "        term3 = term3.reshape(-1,1)\n",
    "        \n",
    "        \n",
    "        f = d2y_dt2 - d2y_dx2 + torch.pow(y,3) - (term1 - term2 + term3)\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xt_BC,y_BC,xt_coll,f_hat,xt_NBC,N_hat):\n",
    "\n",
    "        loss_BC = self.lambdas[0]*self.loss_BC(xt_BC,y_BC)\n",
    "        loss_NBC = self.lambdas[1]*self.loss_NBC(xt_NBC,N_hat)\n",
    "        loss_f = self.loss_PDE(xt_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_BC + loss_f + loss_NBC\n",
    "        \n",
    "        return loss_val\n",
    "    \n",
    "    def lambda_update(self,xt_BC,y_BC,xt_coll,f_hat,xt_NBC,N_hat):\n",
    "        loss_bc1 = self.lambdas[0]*self.loss_BC(xt_BC,y_BC)\n",
    "        loss_bc1.backward()\n",
    "        bc1_grads = []\n",
    "        for param in self.parameters():\n",
    "            bc1_grads.append(param.grad.view(-1))\n",
    "        bc1_grads = torch.cat(bc1_grads)\n",
    "        bc1_grads = torch.mean(torch.abs(bc1_grads))      \n",
    "        \n",
    "        loss_bc2 = self.lambdas[1]*self.loss_NBC(xt_NBC,N_hat)\n",
    "        loss_bc2.backward()\n",
    "        bc2_grads = []\n",
    "        for param in self.parameters():\n",
    "            bc2_grads.append(param.grad.view(-1))\n",
    "        bc2_grads = torch.cat(bc2_grads)\n",
    "        bc2_grads = torch.mean(torch.abs(bc2_grads))    \n",
    "    \n",
    "        loss_f = self.loss_PDE(xt_coll,f_hat)\n",
    "        loss_f.backward()\n",
    "        f_grads = []\n",
    "        for param in self.parameters():\n",
    "            f_grads.append(param.grad.view(-1))   \n",
    "        f_grads = torch.cat(f_grads)\n",
    "        f_grads = torch.max(torch.abs(f_grads))\n",
    "    \n",
    "        self.lambdas[0] = (1.0-self.lambda_alpha)*self.lambdas[0] + self.lambda_alpha*f_grads/bc1_grads\n",
    "        self.lambdas[1] = (1.0-self.lambda_alpha)*self.lambdas[1] + self.lambda_alpha*f_grads/bc2_grads\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        y_pred = self.forward(xt_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return y_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1660687098958,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "VoQzfzYsYKVs"
   },
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1660687098959,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "_IUDZDkxXmyF"
   },
   "outputs": [],
   "source": [
    "def train_step(xt_BC,y_BC,xt_coll,f_hat,xt_NBC,N_hat,seed):\n",
    "    # x_coll_np_array = colloc_pts(N_f,seed*123)\n",
    "    # x_coll_train = torch.from_numpy(x_coll_np_array).float().to(device)        \n",
    "    \n",
    "    # f_hat = torch.zeros(x_coll_train.shape[0],1).to(device)\n",
    "#     xt_coll, xt_BC, y_BC = trainingdata(N_I,N_B,N_f,seed*123)\n",
    "#     xt_coll = torch.from_numpy(xt_coll).float().to(device)\n",
    "#     xt_BC = torch.from_numpy(xt_BC).float().to(device)\n",
    "#     y_BC = torch.from_numpy(y_BC).float().to(device)\n",
    "\n",
    "#     f_hat = torch.zeros(xt_coll.shape[0],1).to(device)\n",
    "    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(xt_BC,y_BC,xt_coll,f_hat,xt_NBC,N_hat)\n",
    "        loss.backward()\n",
    "        #print(loss.cpu().detach().numpy())\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)\n",
    "    PINN.lambda_update(xt_BC,y_BC,xt_coll,f_hat,xt_NBC,N_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 467,
     "status": "ok",
     "timestamp": 1660690085956,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "Vt9Dlr8MYIwW"
   },
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*9)\n",
    "    start_time = time.time() \n",
    "    thresh_flag = 0\n",
    "\n",
    "    xt_coll, xt_BC, y_BC, xt_NBC = trainingdata(N_I,N_B,N_f,rep*11)\n",
    "    xt_coll = torch.from_numpy(xt_coll).float().to(device)\n",
    "    xt_BC = torch.from_numpy(xt_BC).float().to(device)\n",
    "    y_BC = torch.from_numpy(y_BC).float().to(device)\n",
    "    xt_NBC = torch.from_numpy(xt_NBC).float().to(device)\n",
    "    \n",
    "    f_hat = torch.zeros(xt_coll.shape[0],1).to(device)\n",
    "    N_hat = torch.zeros(xt_NBC.shape[0],1).to(device)\n",
    "    \n",
    "    loss_np = PINN.loss(xt_BC,y_BC,xt_coll,f_hat,xt_NBC,N_hat).cpu().detach().numpy()\n",
    "    data_update(loss_np)\n",
    "    for i in range(max_iter):\n",
    "        if(np.isnan(loss_np) == False):\n",
    "            train_step(xt_BC,y_BC,xt_coll,f_hat,xt_NBC,N_hat,i)\n",
    "            loss_np = PINN.loss(xt_BC,y_BC,xt_coll,f_hat,xt_NBC,N_hat).cpu().detach().numpy()\n",
    "        \n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1          \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        \n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])   \n",
    "        \n",
    "      \n",
    "         \n",
    "\n",
    "    elapsed_time[rep] = time.time() - start_time  \n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "sP4Re5lSSq_1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KG_stan_low\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 6475.4526 Test MSE 2.4670306726108975 Test RE 3.6757768528895207\n",
      "1 Train Loss 1744.4894 Test MSE 3.79777917661905 Test RE 4.5606497293917885\n",
      "2 Train Loss 169.25607 Test MSE 2.952071478300228 Test RE 4.0209187589939255\n",
      "3 Train Loss 26.016972 Test MSE 0.38098759468121973 Test RE 1.4444991195600743\n",
      "4 Train Loss 6.3268785 Test MSE 0.01953187100163041 Test RE 0.3270647860985713\n",
      "5 Train Loss 2.416866 Test MSE 0.02816055769778252 Test RE 0.3927195213308721\n",
      "6 Train Loss 1.2726898 Test MSE 0.014312509966423494 Test RE 0.27997532992333984\n",
      "7 Train Loss 0.6766213 Test MSE 0.010111957210892714 Test RE 0.23533118172711776\n",
      "8 Train Loss 0.42972007 Test MSE 0.004413516444927915 Test RE 0.1554727350276509\n",
      "9 Train Loss 0.28832522 Test MSE 0.0029903269966274426 Test RE 0.12797384095101352\n",
      "10 Train Loss 0.20535664 Test MSE 0.0013877434938449828 Test RE 0.08717991956621185\n",
      "11 Train Loss 0.1681942 Test MSE 0.001018972859550521 Test RE 0.07470388230828817\n",
      "12 Train Loss 0.12226024 Test MSE 0.0004133560555985998 Test RE 0.0475799538234964\n",
      "13 Train Loss 0.102608494 Test MSE 0.00030315680984856474 Test RE 0.04074698957496757\n",
      "14 Train Loss 0.08899376 Test MSE 0.00024658466401127033 Test RE 0.036748946665369986\n",
      "15 Train Loss 0.07756223 Test MSE 0.0002473468178885688 Test RE 0.03680569541603594\n",
      "16 Train Loss 0.064672224 Test MSE 0.00017196261967732766 Test RE 0.03068872816485574\n",
      "17 Train Loss 0.0490411 Test MSE 0.00010166435272195675 Test RE 0.02359642523192774\n",
      "18 Train Loss 0.038487416 Test MSE 7.30610804468409e-05 Test RE 0.020003450194942186\n",
      "19 Train Loss 0.03485118 Test MSE 7.618518556046608e-05 Test RE 0.020426649121674672\n",
      "20 Train Loss 0.030990027 Test MSE 6.750027105160881e-05 Test RE 0.01922713707141453\n",
      "21 Train Loss 0.026319994 Test MSE 5.714754816332634e-05 Test RE 0.017691337399511967\n",
      "22 Train Loss 0.023534233 Test MSE 5.7524232674809534e-05 Test RE 0.017749547310702274\n",
      "23 Train Loss 0.020545993 Test MSE 5.080719624054285e-05 Test RE 0.016681092042574834\n",
      "24 Train Loss 0.018341225 Test MSE 4.061699718349531e-05 Test RE 0.014914742867662294\n",
      "25 Train Loss 0.01584314 Test MSE 5.3886253762463664e-05 Test RE 0.017179117890993738\n",
      "26 Train Loss 0.014807211 Test MSE 5.4438223088580144e-05 Test RE 0.017266878558840154\n",
      "27 Train Loss 0.013165707 Test MSE 3.692685170227276e-05 Test RE 0.01422109393811049\n",
      "28 Train Loss 0.012896122 Test MSE 3.5008761387464274e-05 Test RE 0.013846826063910779\n",
      "29 Train Loss 0.011463077 Test MSE 2.528164908123034e-05 Test RE 0.01176696775806767\n",
      "30 Train Loss 0.009665315 Test MSE 2.183574795173903e-05 Test RE 0.010935682692876496\n",
      "31 Train Loss 0.009116451 Test MSE 1.8234876243729568e-05 Test RE 0.009993400071948079\n",
      "32 Train Loss 0.007845148 Test MSE 1.6306111204736547e-05 Test RE 0.009450114355962849\n",
      "33 Train Loss 0.007581287 Test MSE 1.9335905974850455e-05 Test RE 0.010290681262224596\n",
      "34 Train Loss 0.00650012 Test MSE 1.4942328721072276e-05 Test RE 0.00904630044121992\n",
      "35 Train Loss 0.005703946 Test MSE 7.054158246989046e-06 Test RE 0.006215620108915508\n",
      "36 Train Loss 0.0053958944 Test MSE 4.748315045466014e-06 Test RE 0.005099547332580921\n",
      "37 Train Loss 0.00494141 Test MSE 5.644920366159249e-06 Test RE 0.00556020461484934\n",
      "38 Train Loss 0.00471125 Test MSE 5.714149622764881e-06 Test RE 0.005594195866992338\n",
      "39 Train Loss 0.0043041077 Test MSE 8.146075223094922e-06 Test RE 0.006679378774017669\n",
      "40 Train Loss 0.0038841753 Test MSE 6.949015337483152e-06 Test RE 0.006169123992365466\n",
      "41 Train Loss 0.0038163231 Test MSE 6.792177012872445e-06 Test RE 0.006099108536050168\n",
      "42 Train Loss 0.0035316786 Test MSE 5.97263657249017e-06 Test RE 0.005719326747469913\n",
      "43 Train Loss 0.0031553016 Test MSE 5.157870049012145e-06 Test RE 0.005314924040131597\n",
      "44 Train Loss 0.0030946168 Test MSE 5.501231128763776e-06 Test RE 0.005488982042077088\n",
      "45 Train Loss 0.002793981 Test MSE 6.3856242091826935e-06 Test RE 0.005913757917615356\n",
      "46 Train Loss 0.0027491413 Test MSE 6.020242927767213e-06 Test RE 0.005742075150811629\n",
      "47 Train Loss 0.002641547 Test MSE 5.068329993689663e-06 Test RE 0.005268588818502403\n",
      "48 Train Loss 0.002489502 Test MSE 4.200772972126597e-06 Test RE 0.004796522317898148\n",
      "49 Train Loss 0.0023722227 Test MSE 3.7120976014729223e-06 Test RE 0.004508909902743304\n",
      "50 Train Loss 0.0022777398 Test MSE 3.952280449108614e-06 Test RE 0.004652493135808985\n",
      "51 Train Loss 0.0022251983 Test MSE 3.7374215381041507e-06 Test RE 0.004524263657220703\n",
      "52 Train Loss 0.0020905738 Test MSE 4.260164301163939e-06 Test RE 0.004830310384947586\n",
      "53 Train Loss 0.0020191043 Test MSE 3.981711973215075e-06 Test RE 0.004669783911431438\n",
      "54 Train Loss 0.0019044591 Test MSE 3.9664428921205125e-06 Test RE 0.004660821460149179\n",
      "55 Train Loss 0.0018275277 Test MSE 4.633770513169273e-06 Test RE 0.005037663154505838\n",
      "56 Train Loss 0.0017687852 Test MSE 4.38803157953092e-06 Test RE 0.004902264460402216\n",
      "57 Train Loss 0.0015951663 Test MSE 4.934997663638911e-06 Test RE 0.005198826688392836\n",
      "58 Train Loss 0.00147428 Test MSE 5.198584807593081e-06 Test RE 0.005335860054659483\n",
      "59 Train Loss 0.001442579 Test MSE 5.566100712885644e-06 Test RE 0.005521249769450314\n",
      "60 Train Loss 0.0014189612 Test MSE 6.2161919262328235e-06 Test RE 0.005834774436029005\n",
      "61 Train Loss 0.0013153468 Test MSE 5.79642368781336e-06 Test RE 0.005634325401113711\n",
      "62 Train Loss 0.0012317125 Test MSE 6.675367180749095e-06 Test RE 0.006046435767586941\n",
      "63 Train Loss 0.0012045472 Test MSE 7.036981241865339e-06 Test RE 0.006208047921837805\n",
      "64 Train Loss 0.0011778532 Test MSE 6.784285875194111e-06 Test RE 0.006095564540345653\n",
      "65 Train Loss 0.0011221174 Test MSE 6.1212198368967134e-06 Test RE 0.005790030514389997\n",
      "66 Train Loss 0.0010453053 Test MSE 5.663960874763988e-06 Test RE 0.00556957410034772\n",
      "67 Train Loss 0.0010035327 Test MSE 5.044025706992819e-06 Test RE 0.005255941341726102\n",
      "68 Train Loss 0.0009860913 Test MSE 5.104091738466973e-06 Test RE 0.005287143523678509\n",
      "69 Train Loss 0.00097373134 Test MSE 5.078691203479342e-06 Test RE 0.005273971368916613\n",
      "70 Train Loss 0.000956223 Test MSE 4.554373336877704e-06 Test RE 0.004994317847397449\n",
      "71 Train Loss 0.0009014702 Test MSE 5.427211193379833e-06 Test RE 0.005451929421769693\n",
      "72 Train Loss 0.0008796309 Test MSE 5.292244125538906e-06 Test RE 0.005383711744748275\n",
      "73 Train Loss 0.0008695581 Test MSE 5.201634648583438e-06 Test RE 0.005337425013126874\n",
      "74 Train Loss 0.00085813337 Test MSE 5.153753342028943e-06 Test RE 0.005312802587645002\n",
      "75 Train Loss 0.0008129186 Test MSE 4.792341234221242e-06 Test RE 0.00512313418493561\n",
      "76 Train Loss 0.00079344347 Test MSE 4.91519109612036e-06 Test RE 0.005188383477990127\n",
      "77 Train Loss 0.000773433 Test MSE 4.914440978192425e-06 Test RE 0.005187987557680328\n",
      "78 Train Loss 0.0007651611 Test MSE 4.9442713694538945e-06 Test RE 0.005203709138598296\n",
      "79 Train Loss 0.0007595353 Test MSE 4.943109076377107e-06 Test RE 0.0052030974619579895\n",
      "80 Train Loss 0.00073571905 Test MSE 5.110757053617753e-06 Test RE 0.005290594576501698\n",
      "81 Train Loss 0.00071247545 Test MSE 5.060918188274003e-06 Test RE 0.00526473507950027\n",
      "82 Train Loss 0.00068731717 Test MSE 4.939850686942504e-06 Test RE 0.005201382295214094\n",
      "83 Train Loss 0.00067842676 Test MSE 4.849723232360487e-06 Test RE 0.005153714322907616\n",
      "84 Train Loss 0.0006738191 Test MSE 4.834344672272277e-06 Test RE 0.005145536574145398\n",
      "85 Train Loss 0.0006677949 Test MSE 4.867916617318996e-06 Test RE 0.00516337216645482\n",
      "86 Train Loss 0.00065876637 Test MSE 4.8162278428650245e-06 Test RE 0.005135886009744487\n",
      "87 Train Loss 0.0006595667 Test MSE 4.672057583487782e-06 Test RE 0.005058432480747653\n",
      "88 Train Loss 0.0006538478 Test MSE 4.678006219622675e-06 Test RE 0.005061651747807732\n",
      "89 Train Loss 0.00064631895 Test MSE 4.613404526988392e-06 Test RE 0.0050265803917891975\n",
      "90 Train Loss 0.0006257036 Test MSE 4.58757684917437e-06 Test RE 0.005012490243028151\n",
      "91 Train Loss 0.0006082954 Test MSE 4.474502892320338e-06 Test RE 0.004950331250187585\n",
      "92 Train Loss 0.0005970657 Test MSE 4.2856406199651485e-06 Test RE 0.004844731788709477\n",
      "93 Train Loss 0.0005936673 Test MSE 4.281430862959502e-06 Test RE 0.004842351729768144\n",
      "94 Train Loss 0.0005854615 Test MSE 4.251836564054291e-06 Test RE 0.004825586948292429\n",
      "95 Train Loss 0.00057895435 Test MSE 4.290754887413582e-06 Test RE 0.004847621656141879\n",
      "96 Train Loss 0.0005768429 Test MSE 4.284855913821384e-06 Test RE 0.004844288230187408\n",
      "97 Train Loss 0.0005724705 Test MSE 4.400854670221391e-06 Test RE 0.004909422148566266\n",
      "98 Train Loss 0.000571494 Test MSE 4.422653707130825e-06 Test RE 0.004921566207135267\n",
      "99 Train Loss 0.0005709182 Test MSE 4.331074711313468e-06 Test RE 0.004870344723844847\n",
      "100 Train Loss 0.00056451035 Test MSE 4.143838558203045e-06 Test RE 0.004763907032283433\n",
      "101 Train Loss 0.000559097 Test MSE 4.164108434805925e-06 Test RE 0.004775544310994421\n",
      "102 Train Loss 0.000547473 Test MSE 3.9492227672465894e-06 Test RE 0.004650693086877903\n",
      "103 Train Loss 0.00053773355 Test MSE 4.011878607375331e-06 Test RE 0.0046874403678004144\n",
      "104 Train Loss 0.00053742673 Test MSE 4.11530419571476e-06 Test RE 0.0047474766298103085\n",
      "105 Train Loss 0.0005253217 Test MSE 4.126489156073101e-06 Test RE 0.004753923821082118\n",
      "106 Train Loss 0.00051805447 Test MSE 4.1110954992270076e-06 Test RE 0.004745048401128665\n",
      "107 Train Loss 0.00051301596 Test MSE 4.090093583728542e-06 Test RE 0.00473291262052369\n",
      "108 Train Loss 0.0005070108 Test MSE 4.103915777211505e-06 Test RE 0.004740903153731544\n",
      "109 Train Loss 0.0005042529 Test MSE 4.154454013898925e-06 Test RE 0.004770005085721685\n",
      "110 Train Loss 0.0005009219 Test MSE 4.158786685719089e-06 Test RE 0.004772491751959562\n",
      "111 Train Loss 0.00049859897 Test MSE 4.168172675673963e-06 Test RE 0.004777874248975123\n",
      "112 Train Loss 0.000499682 Test MSE 4.165802439506728e-06 Test RE 0.004776515583963357\n",
      "113 Train Loss 0.00050026 Test MSE 4.166447317203848e-06 Test RE 0.004776885278543873\n",
      "114 Train Loss 0.0005005068 Test MSE 4.167010243548897e-06 Test RE 0.00477720796878132\n",
      "115 Train Loss 0.0005000153 Test MSE 4.1650496412581815e-06 Test RE 0.004776083984640185\n",
      "116 Train Loss 0.00049775594 Test MSE 4.272902227288901e-06 Test RE 0.004837526328831456\n",
      "117 Train Loss 0.00049260707 Test MSE 4.248683951969917e-06 Test RE 0.004823797600962141\n",
      "118 Train Loss 0.0004887642 Test MSE 4.323581058026994e-06 Test RE 0.0048661295489117184\n",
      "119 Train Loss 0.00048298767 Test MSE 4.304092169209361e-06 Test RE 0.004855149927201214\n",
      "120 Train Loss 0.00047877178 Test MSE 4.217062631833596e-06 Test RE 0.004805813240912226\n",
      "121 Train Loss 0.00047582405 Test MSE 4.262748016296169e-06 Test RE 0.004831774912308368\n",
      "122 Train Loss 0.00045890093 Test MSE 4.11301875546977e-06 Test RE 0.004746158187666614\n",
      "123 Train Loss 0.00045173545 Test MSE 4.081121050839677e-06 Test RE 0.004727718420362037\n",
      "124 Train Loss 0.00044500755 Test MSE 3.828012035534421e-06 Test RE 0.0045787666541905565\n",
      "125 Train Loss 0.00043545713 Test MSE 3.550662960467902e-06 Test RE 0.00440977660313623\n",
      "126 Train Loss 0.00042842506 Test MSE 3.4562064059108343e-06 Test RE 0.004350725662251308\n",
      "127 Train Loss 0.00042501857 Test MSE 3.4471042319774123e-06 Test RE 0.004344992907240849\n",
      "128 Train Loss 0.00042187775 Test MSE 3.2848217692587833e-06 Test RE 0.004241483375688479\n",
      "129 Train Loss 0.00041805 Test MSE 3.3020283851412676e-06 Test RE 0.004252577775395519\n",
      "130 Train Loss 0.00041301013 Test MSE 3.1129519880489386e-06 Test RE 0.004129030351534339\n",
      "131 Train Loss 0.00040921304 Test MSE 3.139644327032141e-06 Test RE 0.004146694971234585\n",
      "132 Train Loss 0.0004001088 Test MSE 3.0708901409586286e-06 Test RE 0.004101039989146208\n",
      "133 Train Loss 0.00039721755 Test MSE 2.8405066118280794e-06 Test RE 0.003944207606446004\n",
      "134 Train Loss 0.00039554897 Test MSE 2.533986308334826e-06 Test RE 0.003725323534285209\n",
      "135 Train Loss 0.00038720033 Test MSE 2.4632401943269436e-06 Test RE 0.003672951937001461\n",
      "136 Train Loss 0.00037885594 Test MSE 2.4037516535570335e-06 Test RE 0.0036283290172384546\n",
      "137 Train Loss 0.0003676864 Test MSE 2.3678915980284866e-06 Test RE 0.003601162941522509\n",
      "138 Train Loss 0.00036006872 Test MSE 2.393518063459111e-06 Test RE 0.003620597262618866\n",
      "139 Train Loss 0.0003533031 Test MSE 2.495475465194365e-06 Test RE 0.0036969069207907364\n",
      "140 Train Loss 0.00034783967 Test MSE 2.3987772762059505e-06 Test RE 0.003624572800385251\n",
      "141 Train Loss 0.00034399735 Test MSE 2.3521507640068853e-06 Test RE 0.0035891734086902793\n",
      "142 Train Loss 0.0003433211 Test MSE 2.2954281853227002e-06 Test RE 0.0035456324957062395\n",
      "143 Train Loss 0.00034172946 Test MSE 2.2638639411113853e-06 Test RE 0.0035211702596215596\n",
      "144 Train Loss 0.0003379948 Test MSE 2.2843454485280696e-06 Test RE 0.0035370626658993956\n",
      "145 Train Loss 0.00033596953 Test MSE 2.4206488270747824e-06 Test RE 0.0036410593548092964\n",
      "146 Train Loss 0.00032833358 Test MSE 2.4098230813611647e-06 Test RE 0.003632908368190333\n",
      "147 Train Loss 0.00032547236 Test MSE 2.595746061965773e-06 Test RE 0.003770448089757645\n",
      "148 Train Loss 0.00031939434 Test MSE 2.5902780225778735e-06 Test RE 0.00376647469882716\n",
      "149 Train Loss 0.00031403956 Test MSE 2.529194794790227e-06 Test RE 0.003721799761443379\n",
      "150 Train Loss 0.00031001293 Test MSE 2.4800232595574647e-06 Test RE 0.0036854433594485187\n",
      "151 Train Loss 0.0003051443 Test MSE 2.556389106935633e-06 Test RE 0.0037417549614454106\n",
      "152 Train Loss 0.00030202014 Test MSE 2.5585666789948364e-06 Test RE 0.003743348264796055\n",
      "153 Train Loss 0.0003023846 Test MSE 2.5531653411364853e-06 Test RE 0.0037393949242068513\n",
      "154 Train Loss 0.00030322175 Test MSE 2.5379223726206504e-06 Test RE 0.0037282157013026804\n",
      "155 Train Loss 0.0002994864 Test MSE 2.501243740366657e-06 Test RE 0.0037011771426190563\n",
      "156 Train Loss 0.00029839293 Test MSE 2.447712083448841e-06 Test RE 0.0036613566053348722\n",
      "157 Train Loss 0.0002955557 Test MSE 2.416625130391361e-06 Test RE 0.003638031940936436\n",
      "158 Train Loss 0.0002900059 Test MSE 2.359924627980092e-06 Test RE 0.003595099629591078\n",
      "159 Train Loss 0.00028711208 Test MSE 2.2332709537462282e-06 Test RE 0.0034972974624338355\n",
      "160 Train Loss 0.00028420342 Test MSE 2.1855963189763625e-06 Test RE 0.0034597668992421685\n",
      "161 Train Loss 0.0002790749 Test MSE 2.1293346558439766e-06 Test RE 0.0034149458792217457\n",
      "162 Train Loss 0.00027400322 Test MSE 2.0352581804306774e-06 Test RE 0.0033386555765691907\n",
      "163 Train Loss 0.0002698703 Test MSE 2.0018297057942727e-06 Test RE 0.003311123875444931\n",
      "164 Train Loss 0.00026571308 Test MSE 1.782876024849176e-06 Test RE 0.003124801514765499\n",
      "165 Train Loss 0.00026015646 Test MSE 1.8016400155185257e-06 Test RE 0.0031412020604414735\n",
      "166 Train Loss 0.00025891716 Test MSE 1.7674235142203442e-06 Test RE 0.003111230433610762\n",
      "167 Train Loss 0.00025572802 Test MSE 1.8075919554107074e-06 Test RE 0.0031463864562717805\n",
      "168 Train Loss 0.00025245838 Test MSE 1.8115399307324054e-06 Test RE 0.0031498206054374994\n",
      "169 Train Loss 0.0002499064 Test MSE 1.8241293575564527e-06 Test RE 0.0031607466077777225\n",
      "170 Train Loss 0.00024785413 Test MSE 1.8241293575564527e-06 Test RE 0.0031607466077777225\n",
      "171 Train Loss 0.00024780162 Test MSE 1.8407507486314483e-06 Test RE 0.0031751142486804622\n",
      "172 Train Loss 0.00024671323 Test MSE 1.8153348280868757e-06 Test RE 0.00315311807412027\n",
      "173 Train Loss 0.00024565269 Test MSE 1.825950624243968e-06 Test RE 0.003162324107288741\n",
      "174 Train Loss 0.00024458006 Test MSE 1.8341454577552067e-06 Test RE 0.0031694123899864593\n",
      "175 Train Loss 0.00024313417 Test MSE 1.8375820786983014e-06 Test RE 0.003172380249387521\n",
      "176 Train Loss 0.00024414217 Test MSE 1.8288967556027433e-06 Test RE 0.003164874249412698\n",
      "177 Train Loss 0.00024306843 Test MSE 1.7936593765352769e-06 Test RE 0.0031342371222250927\n",
      "178 Train Loss 0.00024242236 Test MSE 1.7936593765352769e-06 Test RE 0.0031342371222250927\n",
      "179 Train Loss 0.00024259496 Test MSE 1.793456135311064e-06 Test RE 0.003134059545522406\n",
      "180 Train Loss 0.00024272522 Test MSE 1.793456135311064e-06 Test RE 0.003134059545522406\n",
      "181 Train Loss 0.00024282206 Test MSE 1.793456131930326e-06 Test RE 0.003134059542568491\n",
      "182 Train Loss 0.00024276803 Test MSE 1.788538387924011e-06 Test RE 0.003129759721060535\n",
      "183 Train Loss 0.00024263309 Test MSE 1.7885416997047823e-06 Test RE 0.0031297626186984418\n",
      "184 Train Loss 0.00024159158 Test MSE 1.7930113755178517e-06 Test RE 0.0031336709131836184\n",
      "185 Train Loss 0.00024121001 Test MSE 1.8241388107906095e-06 Test RE 0.003160754797777779\n",
      "186 Train Loss 0.00024124594 Test MSE 1.80369477932446e-06 Test RE 0.0031429928146934605\n",
      "187 Train Loss 0.00024027815 Test MSE 1.8027817336220888e-06 Test RE 0.003142197209091822\n",
      "188 Train Loss 0.00023949382 Test MSE 1.8118610667340152e-06 Test RE 0.0031500997811689186\n",
      "189 Train Loss 0.0002396533 Test MSE 1.8114521188773052e-06 Test RE 0.0031497442629462758\n",
      "190 Train Loss 0.00023792141 Test MSE 1.8328528282964423e-06 Test RE 0.003168295358104842\n",
      "191 Train Loss 0.00023618448 Test MSE 1.8310314375800606e-06 Test RE 0.0031667207259168237\n",
      "192 Train Loss 0.00023569728 Test MSE 1.7541587997197788e-06 Test RE 0.003099533376202939\n",
      "193 Train Loss 0.00023478035 Test MSE 1.7412100570654156e-06 Test RE 0.0030880722127902107\n",
      "194 Train Loss 0.00023564267 Test MSE 1.6789822940903713e-06 Test RE 0.003032389067053649\n",
      "195 Train Loss 0.00023377665 Test MSE 1.6164137852018702e-06 Test RE 0.0029753505226945707\n",
      "196 Train Loss 0.0002315625 Test MSE 1.7085310285578713e-06 Test RE 0.0030589564867360385\n",
      "197 Train Loss 0.00022912264 Test MSE 1.6916434282497636e-06 Test RE 0.003043801151522318\n",
      "198 Train Loss 0.00022916884 Test MSE 1.71201147629317e-06 Test RE 0.003062070600995567\n",
      "199 Train Loss 0.00022741138 Test MSE 1.6456318065797696e-06 Test RE 0.003002121055247507\n",
      "200 Train Loss 0.00022614097 Test MSE 1.6465085437077403e-06 Test RE 0.0030029206631868293\n",
      "201 Train Loss 0.00022512562 Test MSE 1.6567381971644528e-06 Test RE 0.003012234696799995\n",
      "202 Train Loss 0.0002250759 Test MSE 1.6504899213799348e-06 Test RE 0.003006549111570832\n",
      "203 Train Loss 0.00022603963 Test MSE 1.637926724216515e-06 Test RE 0.00299508463014149\n",
      "204 Train Loss 0.00022724592 Test MSE 1.637926724216515e-06 Test RE 0.00299508463014149\n",
      "205 Train Loss 0.00022841233 Test MSE 1.637926724216515e-06 Test RE 0.00299508463014149\n",
      "206 Train Loss 0.00022790888 Test MSE 1.6293268523419258e-06 Test RE 0.002987211493257531\n",
      "207 Train Loss 0.00022690308 Test MSE 1.6219955037758377e-06 Test RE 0.0029804832606803795\n",
      "208 Train Loss 0.00022704188 Test MSE 1.6349866358720614e-06 Test RE 0.0029923953252861215\n",
      "209 Train Loss 0.00022483927 Test MSE 1.6512173675165192e-06 Test RE 0.0030072116001966286\n",
      "210 Train Loss 0.00022572403 Test MSE 1.6500045322522343e-06 Test RE 0.003006106984193516\n",
      "211 Train Loss 0.00022383887 Test MSE 1.6541644854633694e-06 Test RE 0.00300989406237488\n",
      "212 Train Loss 0.00022366607 Test MSE 1.6608162610890656e-06 Test RE 0.003015939728661807\n",
      "213 Train Loss 0.00022474088 Test MSE 1.6800897613057137e-06 Test RE 0.003033388993582385\n",
      "214 Train Loss 0.000223226 Test MSE 1.6714147967304256e-06 Test RE 0.0030255475678643952\n",
      "215 Train Loss 0.0002217203 Test MSE 1.6822293206016992e-06 Test RE 0.003035319857881085\n",
      "216 Train Loss 0.00022263046 Test MSE 1.6898438048797445e-06 Test RE 0.0030421816750989634\n",
      "217 Train Loss 0.00022175489 Test MSE 1.687029161285402e-06 Test RE 0.0030396470545460985\n",
      "218 Train Loss 0.00022174417 Test MSE 1.68625204019029e-06 Test RE 0.003038946875202608\n",
      "219 Train Loss 0.00022325524 Test MSE 1.6896907075128708e-06 Test RE 0.00304204386338163\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_28500/3105483073.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m     \u001b[0mnan_flag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_28500/102566860.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(max_iter, rep)\u001b[0m\n\u001b[1;32m     18\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32mif\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misnan\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss_np\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m             \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxt_BC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_BC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxt_coll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_hat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxt_NBC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN_hat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m             \u001b[0mloss_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPINN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxt_BC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_BC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxt_coll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_hat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxt_NBC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_28500/2714484387.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(xt_BC, y_BC, xt_coll, f_hat, xt_NBC, N_hat, seed)\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 20\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     21\u001b[0m     \u001b[0mPINN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlambda_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxt_BC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_BC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxt_coll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_hat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxt_NBC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                     loss, flat_grad, t, ls_func_evals = _strong_wolfe(\n\u001b[0;32m--> 426\u001b[0;31m                         obj_func, x_init, t, d, loss, flat_grad, gtd)\n\u001b[0m\u001b[1;32m    427\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0mopt_cond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflat_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mtolerance_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36m_strong_wolfe\u001b[0;34m(obj_func, x, t, d, f, g, gtd, c1, c2, tolerance_change, max_ls)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;31m# Evaluate new point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0mf_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mls_func_evals\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mgtd_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mobj_func\u001b[0;34m(x, t, d)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m                     \u001b[0;32mdef\u001b[0m \u001b[0mobj_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m                         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_directional_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                     loss, flat_grad, t, ls_func_evals = _strong_wolfe(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36m_directional_evaluate\u001b[0;34m(self, closure, x, t, d)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_directional_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0mflat_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gather_flat_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_28500/2714484387.py\u001b[0m in \u001b[0;36mclosure\u001b[0;34m()\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     15\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPINN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxt_BC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my_BC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxt_coll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_hat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxt_NBC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mN_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 16\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     17\u001b[0m         \u001b[0;31m#print(loss.cpu().detach().numpy())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_reps = 10 #10\n",
    "max_iter = 300 #200\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "beta_init = 0\n",
    "\n",
    "N_I = 1000  #Total number of data points for 'y'\n",
    "N_B = 5000\n",
    "N_f = 10000 #Total number of collocation points\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(label)\n",
    "    print(reps)\n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []\n",
    "    beta_val = []\n",
    "\n",
    "    torch.manual_seed(reps*36)\n",
    "\n",
    "    layers = np.array([2,50,50,50,50,1]) #9 hidden layers\n",
    "    # layers = np.array([2,50,50,50,50,50,50,50,1])\n",
    "\n",
    "    PINN = Sequentialmodel(layers,beta_init)\n",
    "\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=1, \n",
    "                            max_iter = 20, \n",
    "                            max_eval = 30, \n",
    "                            tolerance_grad = 1e-8, \n",
    "                            tolerance_change = 1e-8, \n",
    "                            history_size = 100, \n",
    "                            line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    nan_flag = train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "\n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    #elapsed_time[reps] = time.time() - start_time\n",
    "    beta_full.append(beta_val)\n",
    "\n",
    "\n",
    "  #print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"beta\": beta_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred = PINN.test()\n",
    "plt.imshow(u_pred.reshape(500,500),cmap = 'jet')\n",
    "plt.figure()\n",
    "plt.imshow(y_true.reshape(500,500),cmap = 'jet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1660688534316,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "06syezgfv_qO",
    "outputId": "9f4852d5-694a-4977-8893-a6183a2ce493"
   },
   "outputs": [],
   "source": [
    "a = np.ones((10,1))\n",
    "for i in range(10):\n",
    "    a[i] = test_re_full[i][-1]    \n",
    "print(\"a = \",np.nanmean(a))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "stan_2D_KG_16Aug2022_tune.ipynb",
   "version": ""
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
