{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 286,
     "status": "ok",
     "timestamp": 1660687093981,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "iAtv2UvNSq_u",
    "outputId": "68a82578-1b95-4343-a8ec-7635a4df93ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 280,
     "status": "ok",
     "timestamp": 1660687410736,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "mTLFQRt5Sq_y"
   },
   "outputs": [],
   "source": [
    "def true_2D_1(xt): #True function for 2D_1 Klein Gordon Equation x \\in [-50,50] , t \\in [0,10]\n",
    "    x = xt[:,0].reshape(-1,1)\n",
    "    t = xt[:,1].reshape(-1,1)\n",
    "    y = x*np.cos(5*np.pi*t) + np.power(x*t,3)\n",
    "    return y.reshape(-1,1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 4312,
     "status": "ok",
     "timestamp": 1660687098957,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "81bNHCY3Sq_y"
   },
   "outputs": [],
   "source": [
    "pi = np.pi\n",
    "\n",
    "loss_thresh = 0.1\n",
    "level = \"medium\"\n",
    "label = \"KG_swish_\" + level\n",
    "\n",
    "x = np.linspace(0,2,500).reshape(-1,1)\n",
    "t = np.linspace(0,1,500).reshape(-1,1)\n",
    "\n",
    "X,T = np.meshgrid(x,t)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "T = T.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xt = np.hstack((X,T))\n",
    "\n",
    "y_true = true_2D_1(xt)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "#bound_pts_idx = ((X == -5) + (X == 5) + (T == 0)).reshape(-1,)\n",
    "\n",
    "#xt_bound = xt[bound_pts_idx,:]\n",
    "#y_bound = y_true[bound_pts_idx,:]\n",
    "\n",
    "\n",
    "xt_test_tensor = torch.from_numpy(xt).float().to(device)\n",
    "\n",
    "\n",
    "lb_xt = xt[0]\n",
    "ub_xt = xt[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1660687098958,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "YQgCA-PuSq_z"
   },
   "outputs": [],
   "source": [
    "def trainingdata(N_I,N_B,N_f,seed):\n",
    "    '''Boundary Conditions''' \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    x_BC1 = np.random.uniform(size = N_I).reshape(-1,1)\n",
    "    t_BC1 = np.zeros((N_I,1))\n",
    "    samples = np.hstack((x_BC1,t_BC1))\n",
    "    xt_BC1 = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    y_BC1 = true_2D_1(xt_BC1)\n",
    "    \n",
    "    x_BC2 = np.zeros((int(N_B/2),1))\n",
    "    t_BC2 = np.random.uniform(size = int(N_B/2)).reshape(-1,1)\n",
    "    samples = np.hstack((x_BC2,t_BC2))\n",
    "    xt_BC2 = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    y_BC2 = true_2D_1(xt_BC2)\n",
    "    \n",
    "    x_BC3 = np.ones((int(N_B/2),1))\n",
    "    t_BC3 = np.random.uniform(size = int(N_B/2)).reshape(-1,1)\n",
    "    samples = np.hstack((x_BC3,t_BC3))\n",
    "    xt_BC3 = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    y_BC3 = true_2D_1(xt_BC3)\n",
    "    \n",
    "    x_NBC = np.random.uniform(size = N_I).reshape(-1,1)\n",
    "    t_NBC = np.zeros((N_I,1))\n",
    "    samples = np.hstack((x_NBC,t_NBC))\n",
    "    xt_NBC = lb_xt + (ub_xt - lb_xt)*samples\n",
    "\n",
    "    \n",
    "    xt_BC = np.vstack((xt_BC1,xt_BC2,xt_BC3))\n",
    "    y_BC = np.vstack((y_BC1,y_BC2,y_BC3))\n",
    "\n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    samples = sampling(N_f)\n",
    "    xt_coll = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    \n",
    "    xt_coll = np.vstack((xt_coll, xt_BC,xt_NBC)) # append training points to collocation points \n",
    "\n",
    "    return xt_coll, xt_BC, y_BC,xt_NBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1660687098958,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "gTJxct8bSq_0"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,beta_init):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Sigmoid()\n",
    "\n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "            \n",
    "        self.beta = Parameter(torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "\n",
    "    'foward pass'\n",
    "    def forward(self,xt):\n",
    "        if torch.is_tensor(xt) != True:         \n",
    "            xt = torch.from_numpy(xt)                \n",
    "        \n",
    "        ubxt = torch.from_numpy(ub_xt).float().to(device)\n",
    "        lbxt = torch.from_numpy(lb_xt).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xt = 2.0*(xt - lbxt)/(ubxt - lbxt) - 1.0\n",
    "        \n",
    "        #convert to float\n",
    "        a = xt.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = z*self.activation(self.beta[:,i]*z)\n",
    "                    \n",
    "        \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC(self,xt,y):\n",
    "                \n",
    "        loss_bc = self.loss_function(self.forward(xt), y)\n",
    "                \n",
    "        return loss_bc\n",
    "    \n",
    "    def loss_NBC(self,xt_NBC,N_hat):\n",
    "        g = xt_NBC.clone()             \n",
    "        g.requires_grad = True\n",
    "        y = self.forward(g) \n",
    "        \n",
    "        y_x_t = autograd.grad(y,g,torch.ones([xt_NBC.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]        \n",
    "        dy_dt = y_x_t[:,[1]]\n",
    "        \n",
    "        loss_nbc = self.loss_function(dy_dt, N_hat)\n",
    "                \n",
    "        return loss_nbc\n",
    "    \n",
    "    def loss_PDE(self, xt_coll, f_hat):\n",
    "        \n",
    "        g = xt_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        y = self.forward(g) \n",
    "        \n",
    "        \n",
    "        y_x_t = autograd.grad(y,g,torch.ones([xt_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        y_xx_tt = autograd.grad(y_x_t,g,torch.ones(xt_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        #du_dx = u_x_t[:,[0]]\n",
    "        \n",
    "        d2y_dx2 = y_xx_tt[:,[0]]\n",
    "        d2y_dt2 = y_xx_tt[:,[1]]    \n",
    "        \n",
    "\n",
    "        term1 = -25.0*(pi**2)*(g[:,0].reshape(-1,1))*torch.cos(5.0*pi*g[:,1].reshape(-1,1)) + 6*torch.pow(g[:,0].reshape(-1,1),3)*g[:,1].reshape(-1,1)  \n",
    "        term1 = term1.reshape(-1,1)\n",
    "        term2 = 6*torch.pow(g[:,1].reshape(-1,1),3)*g[:,0].reshape(-1,1)\n",
    "        term2 = term2.reshape(-1,1)\n",
    "        term3 = torch.pow(g[:,0]*torch.cos(5.0*pi*g[:,1]) + torch.pow(g[:,0]*g[:,1],3),3)\n",
    "        term3 = term3.reshape(-1,1)\n",
    "        \n",
    "        \n",
    "        f = d2y_dt2 - d2y_dx2 + torch.pow(y,3) - (term1 - term2 + term3)\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xt_BC,y_BC,xt_coll,f_hat,xt_NBC,N_hat):\n",
    "\n",
    "        loss_BC = self.loss_BC(xt_BC,y_BC)\n",
    "        loss_NBC = self.loss_NBC(xt_NBC,N_hat)\n",
    "        loss_f = self.loss_PDE(xt_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_BC + loss_f + loss_NBC\n",
    "        \n",
    "        return loss_val\n",
    "         \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        y_pred = self.forward(xt_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return y_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1660687098958,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "VoQzfzYsYKVs"
   },
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    # beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1660687098959,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "_IUDZDkxXmyF"
   },
   "outputs": [],
   "source": [
    "def train_step(xt_BC,y_BC,xt_coll,f_hat,xt_NBC,N_hat,seed):\n",
    "    # x_coll_np_array = colloc_pts(N_f,seed*123)\n",
    "    # x_coll_train = torch.from_numpy(x_coll_np_array).float().to(device)        \n",
    "    \n",
    "    # f_hat = torch.zeros(x_coll_train.shape[0],1).to(device)\n",
    "#     xt_coll, xt_BC, y_BC = trainingdata(N_I,N_B,N_f,seed*123)\n",
    "#     xt_coll = torch.from_numpy(xt_coll).float().to(device)\n",
    "#     xt_BC = torch.from_numpy(xt_BC).float().to(device)\n",
    "#     y_BC = torch.from_numpy(y_BC).float().to(device)\n",
    "\n",
    "#     f_hat = torch.zeros(xt_coll.shape[0],1).to(device)\n",
    "    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(xt_BC,y_BC,xt_coll,f_hat,xt_NBC,N_hat)\n",
    "        loss.backward()\n",
    "        #print(loss.cpu().detach().numpy())\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 467,
     "status": "ok",
     "timestamp": 1660690085956,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "Vt9Dlr8MYIwW"
   },
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*9)\n",
    "    start_time = time.time() \n",
    "    thresh_flag = 0\n",
    "\n",
    "    xt_coll, xt_BC, y_BC, xt_NBC = trainingdata(N_I,N_B,N_f,rep*11)\n",
    "    xt_coll = torch.from_numpy(xt_coll).float().to(device)\n",
    "    xt_BC = torch.from_numpy(xt_BC).float().to(device)\n",
    "    y_BC = torch.from_numpy(y_BC).float().to(device)\n",
    "    xt_NBC = torch.from_numpy(xt_NBC).float().to(device)\n",
    "    \n",
    "    f_hat = torch.zeros(xt_coll.shape[0],1).to(device)\n",
    "    N_hat = torch.zeros(xt_NBC.shape[0],1).to(device)\n",
    "    \n",
    "    loss_np = PINN.loss(xt_BC,y_BC,xt_coll,f_hat,xt_NBC,N_hat).cpu().detach().numpy()\n",
    "    data_update(loss_np)\n",
    "    for i in range(max_iter):\n",
    "        if(np.isnan(loss_np) == False):\n",
    "            train_step(xt_BC,y_BC,xt_coll,f_hat,xt_NBC,N_hat,i)\n",
    "            loss_np = PINN.loss(xt_BC,y_BC,xt_coll,f_hat,xt_NBC,N_hat).cpu().detach().numpy()\n",
    "        \n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1          \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        \n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])   \n",
    "        \n",
    "      \n",
    "         \n",
    "\n",
    "    elapsed_time[rep] = time.time() - start_time  \n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "sP4Re5lSSq_1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KG_swish_low\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 12197.505 Test MSE 1.3469241886151566 Test RE 2.7160229800391\n",
      "1 Train Loss 9644.228 Test MSE 6.089167335156723 Test RE 5.774851469153808\n",
      "2 Train Loss 7748.52 Test MSE 1.0666001266342775 Test RE 2.4169222482582082\n",
      "3 Train Loss 5869.546 Test MSE 2.5279039312345932 Test RE 3.7208498644929278\n",
      "4 Train Loss 4792.2256 Test MSE 2.70258933125671 Test RE 3.847263152629272\n",
      "5 Train Loss 3613.234 Test MSE 3.0985207385683893 Test RE 4.119448403691627\n",
      "6 Train Loss 2891.6028 Test MSE 5.661116161539833 Test RE 5.568175270859771\n",
      "7 Train Loss 1801.4724 Test MSE 3.9773664295224136 Test RE 4.667234971435788\n",
      "8 Train Loss 868.50793 Test MSE 8.452167394066201 Test RE 6.803711798369758\n",
      "9 Train Loss 467.21442 Test MSE 9.69906134861963 Test RE 7.288307996767058\n",
      "10 Train Loss 355.7721 Test MSE 11.324706027166439 Test RE 7.875449166758051\n",
      "11 Train Loss 279.39606 Test MSE 12.3850548795447 Test RE 8.235895579843621\n",
      "12 Train Loss 224.23767 Test MSE 12.265814836782154 Test RE 8.196153175615061\n",
      "13 Train Loss 168.76224 Test MSE 12.034537861162347 Test RE 8.118514539995331\n",
      "14 Train Loss 101.81874 Test MSE 12.896730359023284 Test RE 8.404302464714613\n",
      "15 Train Loss 72.89216 Test MSE 13.692798330947666 Test RE 8.659802152871018\n",
      "16 Train Loss 62.007607 Test MSE 13.482519830431253 Test RE 8.59305116929736\n",
      "17 Train Loss 53.477043 Test MSE 13.759522295471248 Test RE 8.680875790040657\n",
      "18 Train Loss 48.918247 Test MSE 13.767433720161248 Test RE 8.683371088362712\n",
      "19 Train Loss 46.726997 Test MSE 13.784769093151779 Test RE 8.688836236203796\n",
      "20 Train Loss 43.863712 Test MSE 14.077021575652903 Test RE 8.780459664086319\n",
      "21 Train Loss 41.046925 Test MSE 14.411148893057456 Test RE 8.884053529562587\n",
      "22 Train Loss 39.095303 Test MSE 14.50824328204362 Test RE 8.913931220523404\n",
      "23 Train Loss 36.961147 Test MSE 14.521325036374668 Test RE 8.917949059961696\n",
      "24 Train Loss 35.631798 Test MSE 14.707723969269113 Test RE 8.975002935850966\n",
      "25 Train Loss 35.058697 Test MSE 14.580569192168376 Test RE 8.93612228387356\n",
      "26 Train Loss 34.28423 Test MSE 14.696924191465541 Test RE 8.97170719010454\n",
      "27 Train Loss 33.50374 Test MSE 14.404236789027298 Test RE 8.881922718628003\n",
      "28 Train Loss 32.79399 Test MSE 14.343500959058469 Test RE 8.863177511570722\n",
      "29 Train Loss 31.848524 Test MSE 14.133624010989461 Test RE 8.798094673244888\n",
      "30 Train Loss 31.302786 Test MSE 14.132813379003828 Test RE 8.797842362895608\n",
      "31 Train Loss 30.806507 Test MSE 14.128318042947228 Test RE 8.796443051860702\n",
      "32 Train Loss 30.382364 Test MSE 14.150841968094982 Test RE 8.80345207870671\n",
      "33 Train Loss 29.941437 Test MSE 14.094286562352131 Test RE 8.785842481239959\n",
      "34 Train Loss 29.598024 Test MSE 14.085138413076391 Test RE 8.782990714160842\n",
      "35 Train Loss 28.688395 Test MSE 13.952628499583287 Test RE 8.741578854007292\n",
      "36 Train Loss 27.838034 Test MSE 13.628933101416598 Test RE 8.63958325254975\n",
      "37 Train Loss 27.326342 Test MSE 13.489821538950347 Test RE 8.595377717412195\n",
      "38 Train Loss 26.770206 Test MSE 13.23743821701431 Test RE 8.514591896554183\n",
      "39 Train Loss 25.762 Test MSE 13.058581913666925 Test RE 8.456874249740984\n",
      "40 Train Loss 24.763552 Test MSE 12.971721155450854 Test RE 8.428701354885389\n",
      "41 Train Loss 23.979666 Test MSE 12.979157491773906 Test RE 8.431116981804523\n",
      "42 Train Loss 23.496376 Test MSE 12.799910158499314 Test RE 8.372696035016727\n",
      "43 Train Loss 22.895842 Test MSE 12.533444288745512 Test RE 8.28508715807198\n",
      "44 Train Loss 21.806366 Test MSE 12.260948337537675 Test RE 8.194527089992851\n",
      "45 Train Loss 20.022367 Test MSE 11.986838065346141 Test RE 8.102409394213755\n",
      "46 Train Loss 18.749178 Test MSE 11.749813243106267 Test RE 8.02190188618891\n",
      "47 Train Loss 18.378002 Test MSE 11.713351049722187 Test RE 8.009445372873325\n",
      "48 Train Loss 17.870323 Test MSE 11.393789028319585 Test RE 7.89943356269466\n",
      "49 Train Loss 17.188179 Test MSE 11.33875159808103 Test RE 7.880331452570573\n",
      "50 Train Loss 16.236023 Test MSE 11.293740628574913 Test RE 7.864674787599252\n",
      "51 Train Loss 15.818053 Test MSE 11.357035336046849 Test RE 7.886682410957557\n",
      "52 Train Loss 15.3887005 Test MSE 11.133344101297997 Test RE 7.8086270363739345\n",
      "53 Train Loss 14.673916 Test MSE 11.053287841321144 Test RE 7.7805017352637105\n",
      "54 Train Loss 14.221655 Test MSE 11.007789075169873 Test RE 7.764471741174782\n",
      "55 Train Loss 13.834342 Test MSE 10.782186021105183 Test RE 7.684493970678213\n",
      "56 Train Loss 13.076874 Test MSE 10.441671896952837 Test RE 7.56217781943235\n",
      "57 Train Loss 12.843428 Test MSE 10.537668064775625 Test RE 7.596859967566366\n",
      "58 Train Loss 12.593095 Test MSE 10.57435996596314 Test RE 7.6100745142195505\n",
      "59 Train Loss 12.335496 Test MSE 10.464389606287321 Test RE 7.57039977938409\n",
      "60 Train Loss 12.2376995 Test MSE 10.220308539444835 Test RE 7.481589359226332\n",
      "61 Train Loss 12.109335 Test MSE 9.961979832467904 Test RE 7.386431814592811\n",
      "62 Train Loss 12.00223 Test MSE 9.761118943270969 Test RE 7.311587243687354\n",
      "63 Train Loss 11.881977 Test MSE 9.705712277423446 Test RE 7.290806471035928\n",
      "64 Train Loss 11.6828985 Test MSE 9.522693794184772 Test RE 7.221738755166204\n",
      "65 Train Loss 11.471743 Test MSE 9.413360965383873 Test RE 7.180161624875514\n",
      "66 Train Loss 11.270546 Test MSE 9.356299892947213 Test RE 7.1583665138893435\n",
      "67 Train Loss 11.044444 Test MSE 9.388499097799977 Test RE 7.17067350196097\n",
      "68 Train Loss 10.599253 Test MSE 9.252866561570615 Test RE 7.118688895541372\n",
      "69 Train Loss 10.166513 Test MSE 8.926808097249221 Test RE 6.992137553114883\n",
      "70 Train Loss 10.038267 Test MSE 8.855772106216458 Test RE 6.964261660157642\n",
      "71 Train Loss 9.925915 Test MSE 8.72985570997276 Test RE 6.9145734908218595\n",
      "72 Train Loss 9.733912 Test MSE 8.520198986281615 Test RE 6.831038497465647\n",
      "73 Train Loss 9.391525 Test MSE 8.191307773018654 Test RE 6.69789732955726\n",
      "74 Train Loss 8.894728 Test MSE 7.325080523922891 Test RE 6.333854252341119\n",
      "75 Train Loss 8.472278 Test MSE 6.746051401643891 Test RE 6.078363761851616\n",
      "76 Train Loss 8.017328 Test MSE 6.498304737722591 Test RE 5.965706743168242\n",
      "77 Train Loss 7.3836503 Test MSE 6.288157046222675 Test RE 5.868451961667182\n",
      "78 Train Loss 6.9815655 Test MSE 5.981687306598239 Test RE 5.723658545452716\n",
      "79 Train Loss 6.7355556 Test MSE 5.9533992517169425 Test RE 5.710108602381878\n",
      "80 Train Loss 6.5670443 Test MSE 6.020709114045858 Test RE 5.742297469483684\n",
      "81 Train Loss 6.370143 Test MSE 6.057289650254927 Test RE 5.759715535850301\n",
      "82 Train Loss 6.1491838 Test MSE 6.0469574867774 Test RE 5.754801149640089\n",
      "83 Train Loss 5.9842143 Test MSE 5.840674369027998 Test RE 5.655791111474264\n",
      "84 Train Loss 5.8358583 Test MSE 5.715375546654323 Test RE 5.594795929207429\n",
      "85 Train Loss 5.6760626 Test MSE 5.5683831547635805 Test RE 5.522381678547576\n",
      "86 Train Loss 5.4709506 Test MSE 5.455587433355822 Test RE 5.466163579926586\n",
      "87 Train Loss 5.371079 Test MSE 5.387267060402292 Test RE 5.43182934020137\n",
      "88 Train Loss 5.2708077 Test MSE 5.1886273563437015 Test RE 5.330747410065762\n",
      "89 Train Loss 5.16427 Test MSE 5.150608685670817 Test RE 5.311181488747152\n",
      "90 Train Loss 5.101511 Test MSE 5.045083636510497 Test RE 5.256492501086004\n",
      "91 Train Loss 4.8631496 Test MSE 4.874750123782623 Test RE 5.166995026681085\n",
      "92 Train Loss 4.7042866 Test MSE 4.692546281004907 Test RE 5.069511894255202\n",
      "93 Train Loss 4.6195717 Test MSE 4.807919556054801 Test RE 5.131454239327604\n",
      "94 Train Loss 4.460236 Test MSE 4.8229064891442786 Test RE 5.13944573367761\n",
      "95 Train Loss 4.284393 Test MSE 4.548771795422007 Test RE 4.991245582153349\n",
      "96 Train Loss 4.197207 Test MSE 4.3122904742619275 Test RE 4.859771699026773\n",
      "97 Train Loss 3.928789 Test MSE 3.7292242117765437 Test RE 4.519299375924365\n",
      "98 Train Loss 3.8267984 Test MSE 3.4527733299445873 Test RE 4.348564321043896\n",
      "99 Train Loss 3.7231321 Test MSE 3.2916201317526537 Test RE 4.24587025562164\n",
      "100 Train Loss 3.5855918 Test MSE 3.177476527559597 Test RE 4.171603657551634\n",
      "101 Train Loss 3.425067 Test MSE 2.865447375031292 Test RE 3.9614856080448484\n",
      "102 Train Loss 3.1875648 Test MSE 2.8183878852974376 Test RE 3.9288210293878945\n",
      "103 Train Loss 3.0903146 Test MSE 2.835093128389172 Test RE 3.9404473466785843\n",
      "104 Train Loss 2.9589 Test MSE 2.5904337178453805 Test RE 3.76658789391063\n",
      "105 Train Loss 2.7310452 Test MSE 2.380929147205955 Test RE 3.6110632869305745\n",
      "106 Train Loss 2.5053 Test MSE 2.45177560175801 Test RE 3.6643945075409143\n",
      "107 Train Loss 2.3056407 Test MSE 1.958014018720149 Test RE 3.2746867493172673\n",
      "108 Train Loss 2.1709204 Test MSE 1.657407742472132 Test RE 3.0128433095198583\n",
      "109 Train Loss 1.9710969 Test MSE 1.3798961867976196 Test RE 2.7490653968989176\n",
      "110 Train Loss 1.8029667 Test MSE 1.4150707876827662 Test RE 2.783882794162958\n",
      "111 Train Loss 1.6644448 Test MSE 1.3793739671278573 Test RE 2.748545157807796\n",
      "112 Train Loss 1.5546331 Test MSE 1.246632947826749 Test RE 2.6129503916459194\n",
      "113 Train Loss 1.4194822 Test MSE 1.113306849246646 Test RE 2.469274119041921\n",
      "114 Train Loss 1.2751827 Test MSE 0.9583409107204852 Test RE 2.290983061035762\n",
      "115 Train Loss 1.1635897 Test MSE 0.6858104258448938 Test RE 1.9380452175096725\n",
      "116 Train Loss 0.959144 Test MSE 0.4911833051381965 Test RE 1.6401503561810862\n",
      "117 Train Loss 0.7686927 Test MSE 0.41065427643531455 Test RE 1.4996849699774268\n",
      "118 Train Loss 0.66046 Test MSE 0.3313782452919162 Test RE 1.347174522722519\n",
      "119 Train Loss 0.5859899 Test MSE 0.31843584315349005 Test RE 1.3206046953457355\n",
      "120 Train Loss 0.5102904 Test MSE 0.23385271566035834 Test RE 1.1317045783438051\n",
      "121 Train Loss 0.443345 Test MSE 0.13777444556341678 Test RE 0.8686527522990327\n",
      "122 Train Loss 0.37830752 Test MSE 0.08504775044438452 Test RE 0.6824852652079159\n",
      "123 Train Loss 0.33567387 Test MSE 0.09743825333002917 Test RE 0.7305107464309777\n",
      "124 Train Loss 0.30294654 Test MSE 0.11623457879157265 Test RE 0.7978652257092393\n",
      "125 Train Loss 0.25935823 Test MSE 0.0967248834503402 Test RE 0.7278317077905286\n",
      "126 Train Loss 0.24137262 Test MSE 0.08116496062632528 Test RE 0.6667241021379161\n",
      "127 Train Loss 0.22284381 Test MSE 0.06322904991163202 Test RE 0.5884643964531403\n",
      "128 Train Loss 0.20325364 Test MSE 0.06478738958659884 Test RE 0.5956718884155685\n",
      "129 Train Loss 0.18946683 Test MSE 0.05059099191710877 Test RE 0.5263788868228523\n",
      "130 Train Loss 0.16752967 Test MSE 0.04263239437668591 Test RE 0.48320534144647925\n",
      "131 Train Loss 0.14543061 Test MSE 0.036186024173123034 Test RE 0.4451765695183643\n",
      "132 Train Loss 0.12834686 Test MSE 0.01908366170209497 Test RE 0.3232903330438485\n",
      "133 Train Loss 0.11730625 Test MSE 0.014203709005875366 Test RE 0.27890914038812437\n",
      "134 Train Loss 0.10568254 Test MSE 0.01388684825548889 Test RE 0.2757805980051844\n",
      "135 Train Loss 0.10092477 Test MSE 0.011722539504280297 Test RE 0.25338022445237235\n",
      "136 Train Loss 0.09497086 Test MSE 0.00897371196078589 Test RE 0.22169093074841364\n",
      "137 Train Loss 0.09071348 Test MSE 0.008855270092205271 Test RE 0.22022304843219895\n",
      "138 Train Loss 0.087158464 Test MSE 0.009954552107417247 Test RE 0.23349238763282454\n",
      "139 Train Loss 0.083140805 Test MSE 0.008983572496294775 Test RE 0.2218126970324573\n",
      "140 Train Loss 0.080307685 Test MSE 0.007601029068145306 Test RE 0.20403189383582285\n",
      "141 Train Loss 0.0757278 Test MSE 0.006970823133964023 Test RE 0.19539070322211982\n",
      "142 Train Loss 0.07395643 Test MSE 0.00549273821570348 Test RE 0.17344281538814219\n",
      "143 Train Loss 0.06779629 Test MSE 0.0019670079134364235 Test RE 0.10379224783693031\n",
      "144 Train Loss 0.06640417 Test MSE 0.002235592755267066 Test RE 0.11065173062801964\n",
      "145 Train Loss 0.06275668 Test MSE 0.0015828220915656435 Test RE 0.09310605287067467\n",
      "146 Train Loss 0.060689032 Test MSE 0.0010916319806752554 Test RE 0.07732144991399856\n",
      "147 Train Loss 0.059740108 Test MSE 0.0012478272475710628 Test RE 0.08266831704192422\n",
      "148 Train Loss 0.05668481 Test MSE 0.0019104968585094452 Test RE 0.10229043558858499\n",
      "149 Train Loss 0.05595998 Test MSE 0.0020425934933269584 Test RE 0.10576764537842578\n",
      "150 Train Loss 0.05301129 Test MSE 0.0025423064400064963 Test RE 0.11799831694283294\n",
      "151 Train Loss 0.052321937 Test MSE 0.001997024545197434 Test RE 0.10458118667374768\n",
      "152 Train Loss 0.051259667 Test MSE 0.0012413819224273376 Test RE 0.08245453985155689\n",
      "153 Train Loss 0.04972509 Test MSE 0.0008937281978179609 Test RE 0.06996238310421213\n",
      "154 Train Loss 0.048416376 Test MSE 0.0007521516671835556 Test RE 0.06418219641955723\n",
      "155 Train Loss 0.047085915 Test MSE 0.0008368328445679871 Test RE 0.06769883929365893\n",
      "156 Train Loss 0.045518275 Test MSE 0.0007920660236926618 Test RE 0.06586315887919529\n",
      "157 Train Loss 0.043523226 Test MSE 0.0014280027457304859 Test RE 0.08843544903012247\n",
      "158 Train Loss 0.042599596 Test MSE 0.0014329001707057264 Test RE 0.08858696669087276\n",
      "159 Train Loss 0.040874038 Test MSE 0.0011089075735572034 Test RE 0.07793087255277305\n",
      "160 Train Loss 0.03937768 Test MSE 0.0009059265987084983 Test RE 0.07043821950105637\n",
      "161 Train Loss 0.037333604 Test MSE 0.0012609023405688437 Test RE 0.08310029958952525\n",
      "162 Train Loss 0.036279432 Test MSE 0.0012618581566033074 Test RE 0.08313179035190409\n",
      "163 Train Loss 0.035683025 Test MSE 0.0013530749534491127 Test RE 0.08608406991030869\n",
      "164 Train Loss 0.03538842 Test MSE 0.001673811619369748 Test RE 0.09574479050224038\n",
      "165 Train Loss 0.03249025 Test MSE 0.0010841596011174242 Test RE 0.07705635720446669\n",
      "166 Train Loss 0.030968778 Test MSE 0.0012809556761344703 Test RE 0.08375850465809212\n",
      "167 Train Loss 0.030215276 Test MSE 0.0008996923637676586 Test RE 0.07019543686376517\n",
      "168 Train Loss 0.02999529 Test MSE 0.0008792333838869315 Test RE 0.06939272605361362\n",
      "169 Train Loss 0.02899921 Test MSE 0.0006199663282036715 Test RE 0.05827012038614642\n",
      "170 Train Loss 0.027647078 Test MSE 0.0005765197938802169 Test RE 0.05619128650006187\n",
      "171 Train Loss 0.027124317 Test MSE 0.0008742825978488764 Test RE 0.06919708202716955\n",
      "172 Train Loss 0.026755735 Test MSE 0.0008196242646616777 Test RE 0.0669991460816846\n",
      "173 Train Loss 0.026473658 Test MSE 0.0007307419486158658 Test RE 0.06326214066933258\n",
      "174 Train Loss 0.026015263 Test MSE 0.00044584551770669105 Test RE 0.04941446144390581\n",
      "175 Train Loss 0.025174519 Test MSE 0.0007453108314653371 Test RE 0.06388966051124281\n",
      "176 Train Loss 0.02455299 Test MSE 0.0005503754818914105 Test RE 0.05490240933155046\n",
      "177 Train Loss 0.023726279 Test MSE 0.0006157589161295611 Test RE 0.05807205819238885\n",
      "178 Train Loss 0.022829274 Test MSE 0.000636568521341169 Test RE 0.05904517898110508\n",
      "179 Train Loss 0.022056501 Test MSE 0.00037285237677249045 Test RE 0.0451887493576725\n",
      "180 Train Loss 0.021620292 Test MSE 0.00028461026381902124 Test RE 0.03948090897809827\n",
      "181 Train Loss 0.021342918 Test MSE 0.0001889052167943911 Test RE 0.03216502103914021\n",
      "182 Train Loss 0.020746218 Test MSE 0.00016535003517255647 Test RE 0.030092897775849715\n",
      "183 Train Loss 0.019904085 Test MSE 0.00013594104204559017 Test RE 0.027285829520210282\n",
      "184 Train Loss 0.0189634 Test MSE 0.00013387029203699362 Test RE 0.027077213507191947\n",
      "185 Train Loss 0.01861189 Test MSE 0.00014761201770602684 Test RE 0.028433002443292554\n",
      "186 Train Loss 0.018042872 Test MSE 0.0001428793983830384 Test RE 0.027973491141644838\n",
      "187 Train Loss 0.017633148 Test MSE 0.0002815464910647031 Test RE 0.03926783193613309\n",
      "188 Train Loss 0.017265983 Test MSE 0.00030929462893774373 Test RE 0.0411574115185202\n",
      "189 Train Loss 0.017052094 Test MSE 0.00022704436340054376 Test RE 0.03526283510667651\n",
      "190 Train Loss 0.016798502 Test MSE 0.00019757272482398542 Test RE 0.03289465673497782\n",
      "191 Train Loss 0.016613457 Test MSE 0.0001573819957513141 Test RE 0.029358873533225388\n",
      "192 Train Loss 0.01626235 Test MSE 0.00019080060681512292 Test RE 0.032325982972415596\n",
      "193 Train Loss 0.015662175 Test MSE 0.00013572137973287342 Test RE 0.02726377550501423\n",
      "194 Train Loss 0.014555926 Test MSE 0.00010432562674081767 Test RE 0.02390327265107739\n",
      "195 Train Loss 0.013788124 Test MSE 0.000330912079892145 Test RE 0.042571423731781625\n",
      "196 Train Loss 0.013426031 Test MSE 0.0002710516661858796 Test RE 0.03852901485185549\n",
      "197 Train Loss 0.01319415 Test MSE 0.00031331405404092324 Test RE 0.04142397796511815\n",
      "198 Train Loss 0.012893371 Test MSE 0.00024195290272918633 Test RE 0.03640217074616349\n",
      "199 Train Loss 0.0121710785 Test MSE 8.35022653211065e-05 Test RE 0.021385085591780154\n",
      "200 Train Loss 0.011848598 Test MSE 8.124186751018461e-05 Test RE 0.02109365375970912\n",
      "201 Train Loss 0.011624753 Test MSE 3.73203958206277e-05 Test RE 0.014296673024604266\n",
      "202 Train Loss 0.011501312 Test MSE 2.6085786977340996e-05 Test RE 0.011952639903707566\n",
      "203 Train Loss 0.011229735 Test MSE 4.555941308154888e-05 Test RE 0.015796138187406526\n",
      "204 Train Loss 0.010899741 Test MSE 3.891790491383451e-05 Test RE 0.014599453203462119\n",
      "205 Train Loss 0.010377646 Test MSE 7.552970259470517e-05 Test RE 0.02033858577438816\n",
      "206 Train Loss 0.010208129 Test MSE 6.085065033572172e-05 Test RE 0.018255531262776493\n",
      "207 Train Loss 0.010074214 Test MSE 8.65844456604705e-05 Test RE 0.02177618535166219\n",
      "208 Train Loss 0.0098245945 Test MSE 7.186536916693548e-05 Test RE 0.019839087572718954\n",
      "209 Train Loss 0.009415292 Test MSE 0.00020466851597752806 Test RE 0.033480149170773126\n",
      "210 Train Loss 0.009255282 Test MSE 0.00023496173211071916 Test RE 0.03587239983376236\n",
      "211 Train Loss 0.0090547195 Test MSE 0.0003265515870472527 Test RE 0.0422900076230522\n",
      "212 Train Loss 0.008938862 Test MSE 0.00029387912967440303 Test RE 0.040118643167789556\n",
      "213 Train Loss 0.0086893095 Test MSE 0.0002703572439283433 Test RE 0.03847962839285328\n",
      "214 Train Loss 0.008466951 Test MSE 0.00041122135736283065 Test RE 0.04745693600989022\n",
      "215 Train Loss 0.008299089 Test MSE 0.0003487370379445128 Test RE 0.043702964886849396\n",
      "216 Train Loss 0.008093074 Test MSE 0.0002781657838711663 Test RE 0.03903136305921043\n",
      "217 Train Loss 0.007795612 Test MSE 0.0002428817915239189 Test RE 0.036471980152457895\n",
      "218 Train Loss 0.0075511946 Test MSE 0.00011096266932607854 Test RE 0.024651895146274756\n",
      "219 Train Loss 0.0073593277 Test MSE 0.00013090829134245884 Test RE 0.026775984084829587\n",
      "220 Train Loss 0.007134508 Test MSE 0.00010183020303067541 Test RE 0.02361566442217463\n",
      "221 Train Loss 0.0070809624 Test MSE 7.564946512958728e-05 Test RE 0.020354704173486487\n",
      "222 Train Loss 0.0070306584 Test MSE 5.5122069163661437e-05 Test RE 0.017374992263385534\n",
      "223 Train Loss 0.0068980283 Test MSE 5.998459972112027e-05 Test RE 0.0181251557268784\n",
      "224 Train Loss 0.006636967 Test MSE 4.293088787814398e-05 Test RE 0.015333694249589728\n",
      "225 Train Loss 0.006326268 Test MSE 4.529625612082012e-05 Test RE 0.015750451868408663\n",
      "226 Train Loss 0.0061842334 Test MSE 4.4361619819418566e-05 Test RE 0.015587108620096105\n",
      "227 Train Loss 0.0060459077 Test MSE 4.614397032696113e-05 Test RE 0.015897152622689086\n",
      "228 Train Loss 0.0059980378 Test MSE 6.125619140415252e-05 Test RE 0.018316262524902364\n",
      "229 Train Loss 0.0058901687 Test MSE 5.65151428495629e-05 Test RE 0.017593177277346696\n",
      "230 Train Loss 0.0057734307 Test MSE 6.257546617776363e-05 Test RE 0.018512450526410207\n",
      "231 Train Loss 0.0056873136 Test MSE 5.955271254970707e-05 Test RE 0.018059787585673556\n",
      "232 Train Loss 0.005643818 Test MSE 6.746250455113092e-05 Test RE 0.019221757513170998\n",
      "233 Train Loss 0.005600622 Test MSE 6.676719105370815e-05 Test RE 0.019122444838083848\n",
      "234 Train Loss 0.0055507794 Test MSE 5.499029871966427e-05 Test RE 0.01735421219835765\n",
      "235 Train Loss 0.0054542916 Test MSE 5.245013710129122e-05 Test RE 0.01694865259830719\n",
      "236 Train Loss 0.00537513 Test MSE 5.8186606939085445e-05 Test RE 0.017851445172936607\n",
      "237 Train Loss 0.0051951353 Test MSE 6.96797004137164e-05 Test RE 0.019535071333781715\n",
      "238 Train Loss 0.0050641927 Test MSE 7.085896279100716e-05 Test RE 0.019699683989947793\n",
      "239 Train Loss 0.0050029606 Test MSE 7.26094956770023e-05 Test RE 0.0199415344994792\n",
      "240 Train Loss 0.004951271 Test MSE 8.193078677896088e-05 Test RE 0.021182900523462996\n",
      "241 Train Loss 0.004913286 Test MSE 6.979977876007421e-05 Test RE 0.019551896386992073\n",
      "242 Train Loss 0.004873521 Test MSE 9.442311393748163e-05 Test RE 0.02274055309662058\n",
      "243 Train Loss 0.004848247 Test MSE 8.36585711385206e-05 Test RE 0.02140509133949645\n",
      "244 Train Loss 0.004834801 Test MSE 9.285853043154481e-05 Test RE 0.02255136152030458\n",
      "245 Train Loss 0.0048032654 Test MSE 0.0001056218490705646 Test RE 0.024051310615757365\n",
      "246 Train Loss 0.00476902 Test MSE 0.00011539642202402662 Test RE 0.025139581037591573\n",
      "247 Train Loss 0.0047037234 Test MSE 0.0001045337605358813 Test RE 0.023927104762651988\n",
      "248 Train Loss 0.0046178824 Test MSE 0.00012734802793095489 Test RE 0.02640936604034753\n",
      "249 Train Loss 0.0045624836 Test MSE 0.00013608966629690246 Test RE 0.027300741235260932\n",
      "250 Train Loss 0.004475901 Test MSE 0.0002118394697261141 Test RE 0.034061620378274475\n",
      "251 Train Loss 0.004419676 Test MSE 0.00021352226273151223 Test RE 0.03419664072098027\n",
      "252 Train Loss 0.0044091674 Test MSE 0.00020977669527846488 Test RE 0.03389537817663002\n",
      "253 Train Loss 0.0044091674 Test MSE 0.00020977669527846488 Test RE 0.03389537817663002\n",
      "254 Train Loss 0.0044091674 Test MSE 0.00020977669527846488 Test RE 0.03389537817663002\n",
      "255 Train Loss 0.0044091674 Test MSE 0.00020977669527846488 Test RE 0.03389537817663002\n",
      "256 Train Loss 0.0044091674 Test MSE 0.00020977669527846488 Test RE 0.03389537817663002\n",
      "257 Train Loss 0.0044091674 Test MSE 0.00020977669527846488 Test RE 0.03389537817663002\n",
      "258 Train Loss 0.0044091674 Test MSE 0.00020977669527846488 Test RE 0.03389537817663002\n",
      "259 Train Loss 0.0044091674 Test MSE 0.00020977669527846488 Test RE 0.03389537817663002\n",
      "260 Train Loss 0.0044091674 Test MSE 0.00020977669527846488 Test RE 0.03389537817663002\n",
      "261 Train Loss 0.0044091674 Test MSE 0.00020977669527846488 Test RE 0.03389537817663002\n",
      "262 Train Loss 0.0044091674 Test MSE 0.00020977669527846488 Test RE 0.03389537817663002\n",
      "263 Train Loss 0.0044091674 Test MSE 0.00020977669527846488 Test RE 0.03389537817663002\n",
      "264 Train Loss 0.0044091674 Test MSE 0.00020977669527846488 Test RE 0.03389537817663002\n",
      "265 Train Loss 0.0044091674 Test MSE 0.00020977669527846488 Test RE 0.03389537817663002\n",
      "266 Train Loss 0.0044091674 Test MSE 0.00020977669527846488 Test RE 0.03389537817663002\n",
      "267 Train Loss 0.0044091674 Test MSE 0.00020977669527846488 Test RE 0.03389537817663002\n",
      "268 Train Loss 0.0044091674 Test MSE 0.00020977669527846488 Test RE 0.03389537817663002\n",
      "269 Train Loss 0.0044091674 Test MSE 0.00020977669527846488 Test RE 0.03389537817663002\n",
      "270 Train Loss 0.0044091674 Test MSE 0.00020977669527846488 Test RE 0.03389537817663002\n",
      "271 Train Loss 0.0044091674 Test MSE 0.00020977669527846488 Test RE 0.03389537817663002\n",
      "272 Train Loss 0.0044091674 Test MSE 0.00020977669527846488 Test RE 0.03389537817663002\n",
      "273 Train Loss 0.0044091674 Test MSE 0.00020977669527846488 Test RE 0.03389537817663002\n",
      "274 Train Loss 0.0044091674 Test MSE 0.00020977669527846488 Test RE 0.03389537817663002\n",
      "275 Train Loss 0.0044091674 Test MSE 0.00020977669527846488 Test RE 0.03389537817663002\n",
      "276 Train Loss 0.0044091674 Test MSE 0.00020977669527846488 Test RE 0.03389537817663002\n",
      "277 Train Loss 0.0044091674 Test MSE 0.00020977669527846488 Test RE 0.03389537817663002\n",
      "278 Train Loss 0.0044091674 Test MSE 0.00020977669527846488 Test RE 0.03389537817663002\n",
      "279 Train Loss 0.0044091674 Test MSE 0.00020977669527846488 Test RE 0.03389537817663002\n",
      "280 Train Loss 0.0044091674 Test MSE 0.00020977669527846488 Test RE 0.03389537817663002\n",
      "281 Train Loss 0.0044091674 Test MSE 0.00020977669527846488 Test RE 0.03389537817663002\n",
      "282 Train Loss 0.0044091674 Test MSE 0.00020977669527846488 Test RE 0.03389537817663002\n",
      "283 Train Loss 0.0044091674 Test MSE 0.00020977669527846488 Test RE 0.03389537817663002\n",
      "284 Train Loss 0.0044091674 Test MSE 0.00020977669527846488 Test RE 0.03389537817663002\n",
      "285 Train Loss 0.0044091674 Test MSE 0.00020977669527846488 Test RE 0.03389537817663002\n",
      "286 Train Loss 0.0044091674 Test MSE 0.00020977669527846488 Test RE 0.03389537817663002\n",
      "287 Train Loss 0.0044091674 Test MSE 0.00020977669527846488 Test RE 0.03389537817663002\n",
      "288 Train Loss 0.0044091674 Test MSE 0.00020977669527846488 Test RE 0.03389537817663002\n",
      "289 Train Loss 0.0044091674 Test MSE 0.00020977669527846488 Test RE 0.03389537817663002\n",
      "290 Train Loss 0.0044091674 Test MSE 0.00020977669527846488 Test RE 0.03389537817663002\n",
      "291 Train Loss 0.0044091674 Test MSE 0.00020977669527846488 Test RE 0.03389537817663002\n",
      "292 Train Loss 0.0044091674 Test MSE 0.00020977669527846488 Test RE 0.03389537817663002\n",
      "293 Train Loss 0.0044091674 Test MSE 0.00020977669527846488 Test RE 0.03389537817663002\n",
      "294 Train Loss 0.0044091674 Test MSE 0.00020977669527846488 Test RE 0.03389537817663002\n",
      "295 Train Loss 0.0044091674 Test MSE 0.00020977669527846488 Test RE 0.03389537817663002\n",
      "296 Train Loss 0.0044091674 Test MSE 0.00020977669527846488 Test RE 0.03389537817663002\n",
      "297 Train Loss 0.0044091674 Test MSE 0.00020977669527846488 Test RE 0.03389537817663002\n",
      "298 Train Loss 0.0044091674 Test MSE 0.00020977669527846488 Test RE 0.03389537817663002\n",
      "299 Train Loss 0.0044091674 Test MSE 0.00020977669527846488 Test RE 0.03389537817663002\n",
      "Training time: 259.67\n",
      "KG_swish_low\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 12096.362 Test MSE 1.654574199100151 Test RE 3.01026679383587\n",
      "1 Train Loss 9731.538 Test MSE 2.7412717915466134 Test RE 3.874698481903651\n",
      "2 Train Loss 8621.015 Test MSE 4.742180366775071 Test RE 5.0962520378175045\n",
      "3 Train Loss 6279.913 Test MSE 9.984766900204493 Test RE 7.394874864252346\n",
      "4 Train Loss 4820.1753 Test MSE 11.832969784163343 Test RE 8.050238400725124\n",
      "5 Train Loss 3285.689 Test MSE 15.877361980673399 Test RE 9.32504716567586\n",
      "6 Train Loss 2338.7747 Test MSE 19.317530681007504 Test RE 10.285790381014941\n",
      "7 Train Loss 1681.272 Test MSE 19.003367822010343 Test RE 10.201808129468919\n",
      "8 Train Loss 1229.8499 Test MSE 19.998228300553784 Test RE 10.46544320194301\n",
      "9 Train Loss 1066.9955 Test MSE 20.291289077364716 Test RE 10.5418463762428\n",
      "10 Train Loss 946.1676 Test MSE 19.446029565440128 Test RE 10.319943863313917\n",
      "11 Train Loss 840.47815 Test MSE 19.035319248449575 Test RE 10.210380963122299\n",
      "12 Train Loss 428.1865 Test MSE 10.490342064189482 Test RE 7.579781540980587\n",
      "13 Train Loss 103.74037 Test MSE 1.8870710505109205 Test RE 3.2148150303426064\n",
      "14 Train Loss 43.04339 Test MSE 0.9695079180495211 Test RE 2.304292170618311\n",
      "15 Train Loss 17.961254 Test MSE 0.11117662109714303 Test RE 0.7803125635626282\n",
      "16 Train Loss 8.277495 Test MSE 0.13243749335608782 Test RE 0.8516621375180815\n",
      "17 Train Loss 4.9914174 Test MSE 0.08759212836194694 Test RE 0.6926190042246565\n",
      "18 Train Loss 2.82301 Test MSE 0.029469085742170744 Test RE 0.4017401110006181\n",
      "19 Train Loss 1.8158731 Test MSE 0.030210269069023746 Test RE 0.4067608635966884\n",
      "20 Train Loss 1.1421874 Test MSE 0.04869765198028814 Test RE 0.5164352463052123\n",
      "21 Train Loss 0.828794 Test MSE 0.023749086710985728 Test RE 0.3606494885357727\n",
      "22 Train Loss 0.5769576 Test MSE 0.02691748402093758 Test RE 0.3839539112431404\n",
      "23 Train Loss 0.44396013 Test MSE 0.0159569119806921 Test RE 0.29562168546712414\n",
      "24 Train Loss 0.3464284 Test MSE 0.006951105844195562 Test RE 0.19511417180000243\n",
      "25 Train Loss 0.29593977 Test MSE 0.008220374172094081 Test RE 0.2121815692649948\n",
      "26 Train Loss 0.24942216 Test MSE 0.020409634796863484 Test RE 0.3343331813478333\n",
      "27 Train Loss 0.21068047 Test MSE 0.005085166936619144 Test RE 0.16688391186051416\n",
      "28 Train Loss 0.18073946 Test MSE 0.00981048894031142 Test RE 0.2317966687818525\n",
      "29 Train Loss 0.12555103 Test MSE 0.017538477751804806 Test RE 0.309925857962333\n",
      "30 Train Loss 0.10975823 Test MSE 0.014294318659879011 Test RE 0.2797973479886487\n",
      "31 Train Loss 0.09688636 Test MSE 0.016362060381097446 Test RE 0.29935110082431804\n",
      "32 Train Loss 0.08361382 Test MSE 0.01837745283063448 Test RE 0.3172521123315071\n",
      "33 Train Loss 0.073641956 Test MSE 0.019450048699110586 Test RE 0.32637900234756706\n",
      "34 Train Loss 0.06013928 Test MSE 0.012796160259285611 Test RE 0.2647291098385429\n",
      "35 Train Loss 0.056812458 Test MSE 0.013235110294555987 Test RE 0.2692313608993749\n",
      "36 Train Loss 0.046215396 Test MSE 0.010061829548130336 Test RE 0.23474715739125918\n",
      "37 Train Loss 0.04415636 Test MSE 0.01093162461329109 Test RE 0.24468323792977195\n",
      "38 Train Loss 0.040568076 Test MSE 0.009283565326971555 Test RE 0.22548583407598088\n",
      "39 Train Loss 0.035880204 Test MSE 0.007403722772744623 Test RE 0.20136636866813165\n",
      "40 Train Loss 0.032723233 Test MSE 0.006597331111489872 Test RE 0.19008419358532747\n",
      "41 Train Loss 0.027879337 Test MSE 0.005933257499499416 Test RE 0.18026377535018506\n",
      "42 Train Loss 0.026519252 Test MSE 0.005008463352505738 Test RE 0.16562050869065154\n",
      "43 Train Loss 0.025107583 Test MSE 0.004317059437349569 Test RE 0.1537644283767706\n",
      "44 Train Loss 0.023315407 Test MSE 0.003937360893510712 Test RE 0.14684679618572613\n",
      "45 Train Loss 0.022514703 Test MSE 0.0037793296431917764 Test RE 0.14386967117248034\n",
      "46 Train Loss 0.021687029 Test MSE 0.004105384724311975 Test RE 0.1499473500403409\n",
      "47 Train Loss 0.017752647 Test MSE 0.0035458776503271977 Test RE 0.13935537913300725\n",
      "48 Train Loss 0.016538434 Test MSE 0.0027848833613051496 Test RE 0.12349954832491375\n",
      "49 Train Loss 0.016217906 Test MSE 0.0025320354533317534 Test RE 0.1177597175183656\n",
      "50 Train Loss 0.015084526 Test MSE 0.0021484870450625988 Test RE 0.10847464404911576\n",
      "51 Train Loss 0.013972786 Test MSE 0.0026842103143800973 Test RE 0.12124675788199582\n",
      "52 Train Loss 0.013651469 Test MSE 0.002357521185936924 Test RE 0.11362912597961157\n",
      "53 Train Loss 0.0112717375 Test MSE 0.0022648905021612578 Test RE 0.1113744235719057\n",
      "54 Train Loss 0.011026819 Test MSE 0.0022371322151616504 Test RE 0.11068982222273688\n",
      "55 Train Loss 0.010726776 Test MSE 0.002208468489885911 Test RE 0.10997841785135444\n",
      "56 Train Loss 0.009536759 Test MSE 0.002046000938012428 Test RE 0.10585582915660993\n",
      "57 Train Loss 0.009204148 Test MSE 0.0019692958766296485 Test RE 0.103852594271234\n",
      "58 Train Loss 0.009104104 Test MSE 0.0019593232692622317 Test RE 0.10358930329649473\n",
      "59 Train Loss 0.008477609 Test MSE 0.002045417226506333 Test RE 0.10584072807039555\n",
      "60 Train Loss 0.008365545 Test MSE 0.0018803867094973495 Test RE 0.1014811664610457\n",
      "61 Train Loss 0.008279897 Test MSE 0.0018343642927704153 Test RE 0.10023159883344053\n",
      "62 Train Loss 0.008063455 Test MSE 0.0016537732610909077 Test RE 0.09516995126948277\n",
      "63 Train Loss 0.00775848 Test MSE 0.00158368424144224 Test RE 0.09313140646050994\n",
      "64 Train Loss 0.007571392 Test MSE 0.0016063832766260772 Test RE 0.09379646071388416\n",
      "65 Train Loss 0.0075239553 Test MSE 0.0015983129324670217 Test RE 0.0935605509951928\n",
      "66 Train Loss 0.0072471504 Test MSE 0.0015133179856089344 Test RE 0.09103889145787943\n",
      "67 Train Loss 0.006777985 Test MSE 0.001269421160987407 Test RE 0.08338054526529469\n",
      "68 Train Loss 0.006681434 Test MSE 0.0012276406905194874 Test RE 0.08199691280601591\n",
      "69 Train Loss 0.006634444 Test MSE 0.0010821806461500016 Test RE 0.07698599823189406\n",
      "70 Train Loss 0.0065437234 Test MSE 0.0010245876087028182 Test RE 0.07490941640932341\n",
      "71 Train Loss 0.006276807 Test MSE 0.0009492645462617855 Test RE 0.07210335854909032\n",
      "72 Train Loss 0.0060128896 Test MSE 0.0010006054431982469 Test RE 0.0740275360696092\n",
      "73 Train Loss 0.0057696686 Test MSE 0.0009611103198251634 Test RE 0.07255184887786095\n",
      "74 Train Loss 0.005692858 Test MSE 0.0009457649077439824 Test RE 0.07197032466932062\n",
      "75 Train Loss 0.0055563953 Test MSE 0.0010670358236120805 Test RE 0.07644540118284143\n",
      "76 Train Loss 0.0054577733 Test MSE 0.001162625478354728 Test RE 0.07979612134329209\n",
      "77 Train Loss 0.0053367224 Test MSE 0.0012840148472590106 Test RE 0.08385846080932542\n",
      "78 Train Loss 0.005263929 Test MSE 0.0013843862786946958 Test RE 0.08707440331782267\n",
      "79 Train Loss 0.0051311227 Test MSE 0.0013443676199651846 Test RE 0.08580663794016644\n",
      "80 Train Loss 0.0048779463 Test MSE 0.001403458436079501 Test RE 0.08767214692367843\n",
      "81 Train Loss 0.004703482 Test MSE 0.0014080758532276588 Test RE 0.08781625039140653\n",
      "82 Train Loss 0.0046577337 Test MSE 0.0013577244991234932 Test RE 0.08623184759215534\n",
      "83 Train Loss 0.004623464 Test MSE 0.0013227808283010295 Test RE 0.08511494247504492\n",
      "84 Train Loss 0.004522941 Test MSE 0.0011554243405202283 Test RE 0.07954861454567042\n",
      "85 Train Loss 0.004389409 Test MSE 0.0010908289104006265 Test RE 0.07729300352206248\n",
      "86 Train Loss 0.004239464 Test MSE 0.0009631740930276868 Test RE 0.07262970168322232\n",
      "87 Train Loss 0.004024026 Test MSE 0.0008618245745131424 Test RE 0.0687023038502632\n",
      "88 Train Loss 0.0039516757 Test MSE 0.0007266331246198096 Test RE 0.06308403441613097\n",
      "89 Train Loss 0.003840183 Test MSE 0.0007782493738502949 Test RE 0.06528617938880456\n",
      "90 Train Loss 0.0037408066 Test MSE 0.0008252658277790052 Test RE 0.06722933187372179\n",
      "91 Train Loss 0.0037022883 Test MSE 0.0008653023139093895 Test RE 0.06884078223805389\n",
      "92 Train Loss 0.0036577797 Test MSE 0.0009008762191888317 Test RE 0.07024160482778546\n",
      "93 Train Loss 0.0035827444 Test MSE 0.0008961315803575173 Test RE 0.07005639014057889\n",
      "94 Train Loss 0.0035019428 Test MSE 0.0008775718179575452 Test RE 0.06932712622741136\n",
      "95 Train Loss 0.0034652478 Test MSE 0.0008781083775018949 Test RE 0.0693483167706656\n",
      "96 Train Loss 0.0033922065 Test MSE 0.0008647628027061444 Test RE 0.06881931796847418\n",
      "97 Train Loss 0.0033112795 Test MSE 0.0007670198611066177 Test RE 0.06481345439334407\n",
      "98 Train Loss 0.003254223 Test MSE 0.000734890652348858 Test RE 0.06344146828119962\n",
      "99 Train Loss 0.003147011 Test MSE 0.0006203655174646563 Test RE 0.058288877100905995\n",
      "100 Train Loss 0.0030879108 Test MSE 0.0006292615104706045 Test RE 0.05870531845054334\n",
      "101 Train Loss 0.0030641449 Test MSE 0.0006413996356726549 Test RE 0.05926881149259411\n",
      "102 Train Loss 0.0030080304 Test MSE 0.0006525430166805892 Test RE 0.05978144897599035\n",
      "103 Train Loss 0.0029395951 Test MSE 0.0005843520316663347 Test RE 0.05657168874044267\n",
      "104 Train Loss 0.0028678156 Test MSE 0.0005086836237547151 Test RE 0.05278198783562543\n",
      "105 Train Loss 0.002827972 Test MSE 0.00046556918224701017 Test RE 0.05049565111296108\n",
      "106 Train Loss 0.0028049743 Test MSE 0.000439488476882608 Test RE 0.04906091123725513\n",
      "107 Train Loss 0.0027634217 Test MSE 0.0003931600958831496 Test RE 0.04640305568476891\n",
      "108 Train Loss 0.0027119864 Test MSE 0.00034865277532466907 Test RE 0.04369768476402399\n",
      "109 Train Loss 0.002670094 Test MSE 0.0003016229593687552 Test RE 0.04064377723732121\n",
      "110 Train Loss 0.0026088615 Test MSE 0.0002741693382188887 Test RE 0.03874996422776298\n",
      "111 Train Loss 0.002547591 Test MSE 0.00026991184861526417 Test RE 0.03844791903231028\n",
      "112 Train Loss 0.0024672202 Test MSE 0.0002822481217373529 Test RE 0.039316730381963656\n",
      "113 Train Loss 0.0023983659 Test MSE 0.00025348754366110506 Test RE 0.03725977050047575\n",
      "114 Train Loss 0.0023562443 Test MSE 0.00024887817817880793 Test RE 0.036919454332266774\n",
      "115 Train Loss 0.002307928 Test MSE 0.00021292779449442006 Test RE 0.03414900403527624\n",
      "116 Train Loss 0.0022893897 Test MSE 0.00020843630932581715 Test RE 0.03378691594773605\n",
      "117 Train Loss 0.002274681 Test MSE 0.00020811808384623535 Test RE 0.03376111438635957\n",
      "118 Train Loss 0.0022382601 Test MSE 0.00019573884797270317 Test RE 0.032741636151962086\n",
      "119 Train Loss 0.0021640374 Test MSE 0.00020540721265976518 Test RE 0.033540513608291966\n",
      "120 Train Loss 0.0021044647 Test MSE 0.00020681392612975465 Test RE 0.0336551672926237\n",
      "121 Train Loss 0.0020419685 Test MSE 0.00019786191240864836 Test RE 0.03291872191890982\n",
      "122 Train Loss 0.001978027 Test MSE 0.00021378567557713967 Test RE 0.03421772765097979\n",
      "123 Train Loss 0.0019502182 Test MSE 0.00021114887163332385 Test RE 0.03400605449952485\n",
      "124 Train Loss 0.0019051313 Test MSE 0.00020319108411129727 Test RE 0.03335908944217055\n",
      "125 Train Loss 0.0018671389 Test MSE 0.00022585129260710622 Test RE 0.03517006364919475\n",
      "126 Train Loss 0.0018400007 Test MSE 0.0002477528201589682 Test RE 0.03683589000072538\n",
      "127 Train Loss 0.0018258516 Test MSE 0.000255839482276003 Test RE 0.03743222544300524\n",
      "128 Train Loss 0.0018031913 Test MSE 0.00025817437103721507 Test RE 0.03760264787914853\n",
      "129 Train Loss 0.0017823876 Test MSE 0.0002451131524775893 Test RE 0.03663913161047318\n",
      "130 Train Loss 0.0017677171 Test MSE 0.00023669766528827313 Test RE 0.036004671358323456\n",
      "131 Train Loss 0.0017450593 Test MSE 0.0002298712271105048 Test RE 0.035481679702362545\n",
      "132 Train Loss 0.0015984653 Test MSE 0.00019696008798539147 Test RE 0.032843616984726286\n",
      "133 Train Loss 0.0014997439 Test MSE 0.00019349827846857837 Test RE 0.03255370448898681\n",
      "134 Train Loss 0.0014624234 Test MSE 0.00018919447483078394 Test RE 0.03218963770156564\n",
      "135 Train Loss 0.0014444563 Test MSE 0.00016983283414274283 Test RE 0.030498093574248875\n",
      "136 Train Loss 0.0014079753 Test MSE 0.00015974681889763983 Test RE 0.029578624452416064\n",
      "137 Train Loss 0.0013775806 Test MSE 0.0001761925562902013 Test RE 0.031063875884864894\n",
      "138 Train Loss 0.001357084 Test MSE 0.00017933706966718818 Test RE 0.031339848927426237\n",
      "139 Train Loss 0.0013433731 Test MSE 0.00017785482077446938 Test RE 0.031210065828324057\n",
      "140 Train Loss 0.0013243965 Test MSE 0.00017808686508736382 Test RE 0.03123041882640415\n",
      "141 Train Loss 0.0013029096 Test MSE 0.00017544179151777618 Test RE 0.0309976229143963\n",
      "142 Train Loss 0.0012773816 Test MSE 0.0001754522165283863 Test RE 0.030998543863004383\n",
      "143 Train Loss 0.0012662164 Test MSE 0.0001621350403621622 Test RE 0.02979890502741236\n",
      "144 Train Loss 0.0012257539 Test MSE 0.00015501780834137517 Test RE 0.029137525076068795\n",
      "145 Train Loss 0.0012022805 Test MSE 0.00013798900596739288 Test RE 0.027490592935838346\n",
      "146 Train Loss 0.0011656948 Test MSE 0.00012002841072134089 Test RE 0.025639165864467763\n",
      "147 Train Loss 0.0011413343 Test MSE 0.00011987076124737723 Test RE 0.025622322647576537\n",
      "148 Train Loss 0.0011055181 Test MSE 0.00011054086454131145 Test RE 0.024604995643835833\n",
      "149 Train Loss 0.0010657007 Test MSE 9.356364588917873e-05 Test RE 0.022636820773132296\n",
      "150 Train Loss 0.0010339417 Test MSE 9.229957782288828e-05 Test RE 0.022483386244120693\n",
      "151 Train Loss 0.0010114989 Test MSE 9.508682775403225e-05 Test RE 0.022820336469350014\n",
      "152 Train Loss 0.0009995499 Test MSE 9.4888917882829e-05 Test RE 0.02279657543717374\n",
      "153 Train Loss 0.0009922761 Test MSE 0.00010133770641474477 Test RE 0.02355848722268828\n",
      "154 Train Loss 0.0009824251 Test MSE 9.702940716304026e-05 Test RE 0.023052262323335522\n",
      "155 Train Loss 0.0009704104 Test MSE 8.879488184405823e-05 Test RE 0.02205239845118084\n",
      "156 Train Loss 0.0009574393 Test MSE 9.151675868412785e-05 Test RE 0.022387839205166973\n",
      "157 Train Loss 0.00094265706 Test MSE 0.00010060983418006342 Test RE 0.023473728687932103\n",
      "158 Train Loss 0.00093410857 Test MSE 9.233289905363416e-05 Test RE 0.02248744426104613\n",
      "159 Train Loss 0.00093052106 Test MSE 9.080391599052733e-05 Test RE 0.022300477029928024\n",
      "160 Train Loss 0.0009260492 Test MSE 9.028089849574246e-05 Test RE 0.022236160512725152\n",
      "161 Train Loss 0.0009080892 Test MSE 8.50225823828904e-05 Test RE 0.021578885462128935\n",
      "162 Train Loss 0.00089044054 Test MSE 8.904490881965556e-05 Test RE 0.022083423992801686\n",
      "163 Train Loss 0.0008702204 Test MSE 7.008205106231308e-05 Test RE 0.019591390712412702\n",
      "164 Train Loss 0.00084220216 Test MSE 5.129362186552129e-05 Test RE 0.01676075380912958\n",
      "165 Train Loss 0.00082319486 Test MSE 5.370429484113415e-05 Test RE 0.01715008880819443\n",
      "166 Train Loss 0.00080081157 Test MSE 4.346607759583425e-05 Test RE 0.015428975503930312\n",
      "167 Train Loss 0.00075482373 Test MSE 4.051435723652912e-05 Test RE 0.014895886023630837\n",
      "168 Train Loss 0.00071371684 Test MSE 3.88979048498069e-05 Test RE 0.014595701363238088\n",
      "169 Train Loss 0.0006991106 Test MSE 4.032380602097906e-05 Test RE 0.01486081481985526\n",
      "170 Train Loss 0.0006947738 Test MSE 4.164254884904352e-05 Test RE 0.015101862646159191\n",
      "171 Train Loss 0.00069009466 Test MSE 4.3208175520455175e-05 Test RE 0.015383134182324147\n",
      "172 Train Loss 0.00068972213 Test MSE 4.330891262626279e-05 Test RE 0.015401056140605824\n",
      "173 Train Loss 0.00068653066 Test MSE 4.468262266955957e-05 Test RE 0.01564340150919298\n",
      "174 Train Loss 0.0006828459 Test MSE 4.44882884750593e-05 Test RE 0.015609346206365832\n",
      "175 Train Loss 0.00067826384 Test MSE 4.498024695293494e-05 Test RE 0.01569541422251378\n",
      "176 Train Loss 0.0006725557 Test MSE 4.722933511119119e-05 Test RE 0.016083026586859613\n",
      "177 Train Loss 0.00066471 Test MSE 5.200269842249496e-05 Test RE 0.016876205454337767\n",
      "178 Train Loss 0.00065585173 Test MSE 5.5115329549122695e-05 Test RE 0.017373930036115627\n",
      "179 Train Loss 0.0006515075 Test MSE 5.643611693051377e-05 Test RE 0.01758087258077981\n",
      "180 Train Loss 0.0006422112 Test MSE 5.2081637894495734e-05 Test RE 0.01688900953572459\n",
      "181 Train Loss 0.0006328335 Test MSE 5.0811572692803064e-05 Test RE 0.016681810468667287\n",
      "182 Train Loss 0.0006245124 Test MSE 4.995938319903362e-05 Test RE 0.016541328936491986\n",
      "183 Train Loss 0.00061092945 Test MSE 5.8581470161380443e-05 Test RE 0.017911914078463067\n",
      "184 Train Loss 0.000601622 Test MSE 6.142261004445355e-05 Test RE 0.01834112613423066\n",
      "185 Train Loss 0.000592798 Test MSE 5.611803794176714e-05 Test RE 0.017531258892226457\n",
      "186 Train Loss 0.0005873002 Test MSE 5.693900910140984e-05 Test RE 0.017659028867488987\n",
      "187 Train Loss 0.00057567644 Test MSE 5.430452838932139e-05 Test RE 0.017245662682366367\n",
      "188 Train Loss 0.0005656586 Test MSE 5.022762076813998e-05 Test RE 0.01658567562192493\n",
      "189 Train Loss 0.0005597054 Test MSE 5.1754857582121174e-05 Test RE 0.01683594207875705\n",
      "190 Train Loss 0.00055234245 Test MSE 4.8505983757924925e-05 Test RE 0.01629894606190426\n",
      "191 Train Loss 0.0005428769 Test MSE 4.887568670194603e-05 Test RE 0.01636094181608557\n",
      "192 Train Loss 0.0005319406 Test MSE 5.5148145578690414e-05 Test RE 0.017379101542526903\n",
      "193 Train Loss 0.0005214887 Test MSE 5.773704318043009e-05 Test RE 0.017782349165631064\n",
      "194 Train Loss 0.00051467016 Test MSE 5.17816523241428e-05 Test RE 0.01684029970210509\n",
      "195 Train Loss 0.0005119804 Test MSE 4.987092342139007e-05 Test RE 0.0165266781294105\n",
      "196 Train Loss 0.000511809 Test MSE 4.982997446508213e-05 Test RE 0.016519891718101434\n",
      "197 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "198 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "199 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "200 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "201 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "202 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "203 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "204 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "205 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "206 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "207 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "208 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "209 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "210 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "211 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "212 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "213 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "214 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "215 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "216 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "217 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "218 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "219 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "220 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "221 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "222 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "223 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "224 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "225 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "226 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "227 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "228 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "229 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "230 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "231 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "232 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "233 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "234 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "235 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "236 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "237 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "238 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "239 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "240 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "241 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "242 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "243 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "244 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "245 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "246 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "247 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "248 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "249 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "250 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "251 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "252 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "253 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "254 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "255 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "256 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "257 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "258 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "259 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "260 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "261 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "262 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "263 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "264 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "265 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "266 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "267 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "268 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "269 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "270 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "271 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "272 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "273 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "274 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "275 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "276 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "277 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "278 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "279 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "280 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "281 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "282 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "283 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "284 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "285 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "286 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "287 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "288 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "289 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "290 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "291 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "292 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "293 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "294 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "295 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "296 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "297 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "298 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "299 Train Loss 0.000511807 Test MSE 4.9829972376866445e-05 Test RE 0.016519891371953376\n",
      "Training time: 207.00\n",
      "KG_swish_low\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 12068.097 Test MSE 1.9286937571930152 Test RE 3.2500758858693404\n",
      "1 Train Loss 8057.8506 Test MSE 1.4109706267822164 Test RE 2.7798467248182344\n",
      "2 Train Loss 5754.641 Test MSE 3.237344456756713 Test RE 4.210719578568012\n",
      "3 Train Loss 3791.0425 Test MSE 5.268925573063202 Test RE 5.371837863314244\n",
      "4 Train Loss 2676.5195 Test MSE 13.815021070926946 Test RE 8.698365246890079\n",
      "5 Train Loss 1862.0549 Test MSE 14.662361146039796 Test RE 8.961151510402113\n",
      "6 Train Loss 527.5086 Test MSE 16.12478858238258 Test RE 9.39742522258055\n",
      "7 Train Loss 142.45808 Test MSE 9.738648457705297 Test RE 7.303166611845326\n",
      "8 Train Loss 87.605606 Test MSE 10.453503654164182 Test RE 7.566461066446621\n",
      "9 Train Loss 58.96375 Test MSE 10.506024974652673 Test RE 7.585445257154957\n",
      "10 Train Loss 45.766438 Test MSE 11.103668641633416 Test RE 7.7982133092082435\n",
      "11 Train Loss 35.63741 Test MSE 10.946835090631366 Test RE 7.742944598869566\n",
      "12 Train Loss 27.773888 Test MSE 9.99350645172202 Test RE 7.398110480800105\n",
      "13 Train Loss 21.869522 Test MSE 6.935943040040156 Test RE 6.163318667279646\n",
      "14 Train Loss 15.443396 Test MSE 4.446564286072587 Test RE 4.934852220586124\n",
      "15 Train Loss 13.38592 Test MSE 2.5819138176918797 Test RE 3.7603886650498963\n",
      "16 Train Loss 11.521378 Test MSE 2.274992510694054 Test RE 3.529814230882812\n",
      "17 Train Loss 10.122178 Test MSE 2.4342015736146965 Test RE 3.6512379227451035\n",
      "18 Train Loss 8.648585 Test MSE 2.493531331578456 Test RE 3.6954665777574527\n",
      "19 Train Loss 7.42847 Test MSE 2.327390635522655 Test RE 3.5702325513465376\n",
      "20 Train Loss 6.039398 Test MSE 2.191568904730919 Test RE 3.464490931900162\n",
      "21 Train Loss 5.3077316 Test MSE 2.1037801252410206 Test RE 3.394392334590785\n",
      "22 Train Loss 4.296453 Test MSE 2.144554385124739 Test RE 3.4271285598937347\n",
      "23 Train Loss 3.9665747 Test MSE 1.9607154114065082 Test RE 3.276944947078374\n",
      "24 Train Loss 3.7610462 Test MSE 1.9158075700601351 Test RE 3.2392003190158842\n",
      "25 Train Loss 3.5429068 Test MSE 1.8017063312500983 Test RE 3.1412598714342326\n",
      "26 Train Loss 3.162951 Test MSE 1.7351267036301512 Test RE 3.082673015947677\n",
      "27 Train Loss 2.6647558 Test MSE 0.8717198481775665 Test RE 2.184994415461359\n",
      "28 Train Loss 2.0299313 Test MSE 0.5565248969738363 Test RE 1.7458388878605196\n",
      "29 Train Loss 1.6697676 Test MSE 0.5487154990284068 Test RE 1.733546429226424\n",
      "30 Train Loss 1.4406636 Test MSE 0.5560506084242063 Test RE 1.74509479905827\n",
      "31 Train Loss 1.2814403 Test MSE 0.5075337054455643 Test RE 1.6672253635959364\n",
      "32 Train Loss 1.174767 Test MSE 0.37965667157108773 Test RE 1.4419738415283385\n",
      "33 Train Loss 0.95808184 Test MSE 0.2620799547264432 Test RE 1.1980605464435563\n",
      "34 Train Loss 0.89230573 Test MSE 0.20078080386593417 Test RE 1.04863164238995\n",
      "35 Train Loss 0.7790778 Test MSE 0.16553836122417073 Test RE 0.9521627556134322\n",
      "36 Train Loss 0.6952599 Test MSE 0.11454318628904145 Test RE 0.7920388672319324\n",
      "37 Train Loss 0.62121207 Test MSE 0.02985673650000564 Test RE 0.40437382091347135\n",
      "38 Train Loss 0.5528108 Test MSE 0.014089690950347384 Test RE 0.27778743499320446\n",
      "39 Train Loss 0.46165675 Test MSE 0.012244105319009364 Test RE 0.2589556507485584\n",
      "40 Train Loss 0.3996884 Test MSE 0.010145241903560155 Test RE 0.23571817361868286\n",
      "41 Train Loss 0.37400904 Test MSE 0.009220518982652743 Test RE 0.22471887250022468\n",
      "42 Train Loss 0.33924946 Test MSE 0.008395456445316158 Test RE 0.21442924699442303\n",
      "43 Train Loss 0.3112915 Test MSE 0.005049219257256081 Test RE 0.16629300411381212\n",
      "44 Train Loss 0.29068187 Test MSE 0.004951205412161471 Test RE 0.16467108091119276\n",
      "45 Train Loss 0.27341273 Test MSE 0.00597823487377056 Test RE 0.1809457349542368\n",
      "46 Train Loss 0.25674316 Test MSE 0.005521509081919583 Test RE 0.1738964673178275\n",
      "47 Train Loss 0.22237955 Test MSE 0.005609778857189278 Test RE 0.17528095658190387\n",
      "48 Train Loss 0.20868956 Test MSE 0.005280854057337716 Test RE 0.17006460971644607\n",
      "49 Train Loss 0.1994633 Test MSE 0.005064812404041591 Test RE 0.1665495816367035\n",
      "50 Train Loss 0.18326862 Test MSE 0.0051721542597022415 Test RE 0.16830522496162545\n",
      "51 Train Loss 0.17196777 Test MSE 0.004682718413365751 Test RE 0.160144078789601\n",
      "52 Train Loss 0.16386853 Test MSE 0.0037806986108505775 Test RE 0.14389572540873666\n",
      "53 Train Loss 0.15101746 Test MSE 0.0030523452502367225 Test RE 0.12929409527358063\n",
      "54 Train Loss 0.13626128 Test MSE 0.0031857849223009566 Test RE 0.13209004565669227\n",
      "55 Train Loss 0.12642072 Test MSE 0.004061796822393514 Test RE 0.14914921151791086\n",
      "56 Train Loss 0.11960548 Test MSE 0.0050568840188962155 Test RE 0.1664191734113414\n",
      "57 Train Loss 0.110182814 Test MSE 0.004494002365087682 Test RE 0.15688394890341215\n",
      "58 Train Loss 0.10352947 Test MSE 0.0034366915260072464 Test RE 0.1371930591175676\n",
      "59 Train Loss 0.10071092 Test MSE 0.00355803129996325 Test RE 0.1395939981455932\n",
      "60 Train Loss 0.098016515 Test MSE 0.003670120618107978 Test RE 0.1417757759210549\n",
      "61 Train Loss 0.086409606 Test MSE 0.0038489770976476738 Test RE 0.14518927195875808\n",
      "62 Train Loss 0.080517486 Test MSE 0.004540791120664681 Test RE 0.1576985231993355\n",
      "63 Train Loss 0.07585632 Test MSE 0.004579630084195578 Test RE 0.15837151230592236\n",
      "64 Train Loss 0.07075217 Test MSE 0.004685025399945451 Test RE 0.16018352219788146\n",
      "65 Train Loss 0.06753123 Test MSE 0.005118936266746304 Test RE 0.1674371122603309\n",
      "66 Train Loss 0.06350609 Test MSE 0.005751036084464698 Test RE 0.17747407051372777\n",
      "67 Train Loss 0.05950786 Test MSE 0.006983963742567819 Test RE 0.19557478075719445\n",
      "68 Train Loss 0.057608288 Test MSE 0.00653768469307809 Test RE 0.1892229677359647\n",
      "69 Train Loss 0.056573182 Test MSE 0.005623306395342122 Test RE 0.17549216741274584\n",
      "70 Train Loss 0.05268067 Test MSE 0.004930137844824496 Test RE 0.16432036658169064\n",
      "71 Train Loss 0.051514845 Test MSE 0.004275564958405628 Test RE 0.15302367177015122\n",
      "72 Train Loss 0.049811564 Test MSE 0.003948381485115792 Test RE 0.1470521631524621\n",
      "73 Train Loss 0.04920096 Test MSE 0.0038329565967937496 Test RE 0.14488679805276894\n",
      "74 Train Loss 0.047322515 Test MSE 0.0035651496503365868 Test RE 0.1397335672503026\n",
      "75 Train Loss 0.04583971 Test MSE 0.004102139493750785 Test RE 0.14988807302266605\n",
      "76 Train Loss 0.04290795 Test MSE 0.003169213113668072 Test RE 0.13174604487095143\n",
      "77 Train Loss 0.042252693 Test MSE 0.00288220071147726 Test RE 0.1256388563681712\n",
      "78 Train Loss 0.041921362 Test MSE 0.002888749295813403 Test RE 0.12578150601639504\n",
      "79 Train Loss 0.040786363 Test MSE 0.0022593408222732745 Test RE 0.1112378890481043\n",
      "80 Train Loss 0.037799295 Test MSE 0.002160250771779869 Test RE 0.10877120718377614\n",
      "81 Train Loss 0.03717187 Test MSE 0.002256494889013245 Test RE 0.11116780768632875\n",
      "82 Train Loss 0.036125373 Test MSE 0.0020118814888994293 Test RE 0.10496948377735985\n",
      "83 Train Loss 0.035594914 Test MSE 0.0023196565964483397 Test RE 0.11271292291960265\n",
      "84 Train Loss 0.035065614 Test MSE 0.002673262251149253 Test RE 0.12099924123565683\n",
      "85 Train Loss 0.034691244 Test MSE 0.002704331385329232 Test RE 0.12170034748529117\n",
      "86 Train Loss 0.033723902 Test MSE 0.0030395679569830223 Test RE 0.12902319520568256\n",
      "87 Train Loss 0.033415705 Test MSE 0.0029516011075816253 Test RE 0.12714248526273414\n",
      "88 Train Loss 0.032359086 Test MSE 0.0028766758695699095 Test RE 0.12551838110440466\n",
      "89 Train Loss 0.03150914 Test MSE 0.002548144982566118 Test RE 0.11813373395871693\n",
      "90 Train Loss 0.031041417 Test MSE 0.002506691415099572 Test RE 0.1171688860123079\n",
      "91 Train Loss 0.03065176 Test MSE 0.0028567349155436215 Test RE 0.12508258140574544\n",
      "92 Train Loss 0.029992936 Test MSE 0.0034596457821706654 Test RE 0.13765046468241787\n",
      "93 Train Loss 0.028950779 Test MSE 0.0037367303376649105 Test RE 0.14305654861267952\n",
      "94 Train Loss 0.02820311 Test MSE 0.004024368824447516 Test RE 0.14846044296647257\n",
      "95 Train Loss 0.027431399 Test MSE 0.00374754415568256 Test RE 0.14326339654433012\n",
      "96 Train Loss 0.0269988 Test MSE 0.0036287029070950164 Test RE 0.14097352859864298\n",
      "97 Train Loss 0.026512602 Test MSE 0.00404234021940714 Test RE 0.14879155939427915\n",
      "98 Train Loss 0.02631054 Test MSE 0.003795053620003852 Test RE 0.14416864684850364\n",
      "99 Train Loss 0.026119879 Test MSE 0.003750125888403904 Test RE 0.14331273607223097\n",
      "100 Train Loss 0.025179423 Test MSE 0.0032038905128451327 Test RE 0.13246486379341327\n",
      "101 Train Loss 0.024763204 Test MSE 0.003011428716404601 Test RE 0.12842458108711446\n",
      "102 Train Loss 0.023903554 Test MSE 0.0029261227259817194 Test RE 0.12659254548246932\n",
      "103 Train Loss 0.023087045 Test MSE 0.003017615223330361 Test RE 0.12855642746471366\n",
      "104 Train Loss 0.022583026 Test MSE 0.003128801787238445 Test RE 0.13090338877865385\n",
      "105 Train Loss 0.022418102 Test MSE 0.0029874439887736283 Test RE 0.12791213556578884\n",
      "106 Train Loss 0.022015627 Test MSE 0.002591384053580215 Test RE 0.1191318140695891\n",
      "107 Train Loss 0.02101839 Test MSE 0.0024131557965308675 Test RE 0.11496206197508979\n",
      "108 Train Loss 0.020475995 Test MSE 0.002716885343504987 Test RE 0.12198249709240253\n",
      "109 Train Loss 0.020369114 Test MSE 0.0027635712755083087 Test RE 0.12302608362685516\n",
      "110 Train Loss 0.020127269 Test MSE 0.0028416590353826233 Test RE 0.124752094933261\n",
      "111 Train Loss 0.019781405 Test MSE 0.002829444545413282 Test RE 0.12448369113001108\n",
      "112 Train Loss 0.01943352 Test MSE 0.0032805992736303787 Test RE 0.1340412460032943\n",
      "113 Train Loss 0.019260375 Test MSE 0.003282962088747945 Test RE 0.13408950817264098\n",
      "114 Train Loss 0.019002838 Test MSE 0.0034201618228661794 Test RE 0.13686272775767383\n",
      "115 Train Loss 0.018414391 Test MSE 0.0033088742855401708 Test RE 0.1346176477688807\n",
      "116 Train Loss 0.017657423 Test MSE 0.002826772629507173 Test RE 0.12442490070151299\n",
      "117 Train Loss 0.017254667 Test MSE 0.0026638775814212972 Test RE 0.12078666645649846\n",
      "118 Train Loss 0.016979994 Test MSE 0.0024625899507798696 Test RE 0.11613360712354025\n",
      "119 Train Loss 0.016892267 Test MSE 0.0023644952790750043 Test RE 0.11379707248793604\n",
      "120 Train Loss 0.01656689 Test MSE 0.0025042892219227625 Test RE 0.11711273036400155\n",
      "121 Train Loss 0.01626209 Test MSE 0.0021752829123028714 Test RE 0.1091489942307931\n",
      "122 Train Loss 0.015992245 Test MSE 0.0020514257086966456 Test RE 0.10599606943075685\n",
      "123 Train Loss 0.01558599 Test MSE 0.0017593606714502688 Test RE 0.0981610745377591\n",
      "124 Train Loss 0.015291313 Test MSE 0.0017141003715986842 Test RE 0.09689023038580198\n",
      "125 Train Loss 0.014907343 Test MSE 0.0015157726429883207 Test RE 0.09111269575526537\n",
      "126 Train Loss 0.014622836 Test MSE 0.0014693207869837062 Test RE 0.08970572807935918\n",
      "127 Train Loss 0.014371396 Test MSE 0.00162643954206793 Test RE 0.09438018541712874\n",
      "128 Train Loss 0.014031092 Test MSE 0.001519992633824679 Test RE 0.09123943887251915\n",
      "129 Train Loss 0.013767464 Test MSE 0.0015791900530368103 Test RE 0.09299916815214443\n",
      "130 Train Loss 0.01358341 Test MSE 0.001692270170215192 Test RE 0.0962712727945304\n",
      "131 Train Loss 0.013401614 Test MSE 0.0018472203060915144 Test RE 0.10058221864311906\n",
      "132 Train Loss 0.013140768 Test MSE 0.0019977753033852456 Test RE 0.10460084286789519\n",
      "133 Train Loss 0.013006895 Test MSE 0.0018961585869558945 Test RE 0.10190586802290116\n",
      "134 Train Loss 0.012919836 Test MSE 0.0018631970116673871 Test RE 0.10101625261741556\n",
      "135 Train Loss 0.012856493 Test MSE 0.001800438882391212 Test RE 0.09930041322361144\n",
      "136 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "137 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "138 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "139 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "140 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "141 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "142 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "143 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "144 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "145 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "146 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "147 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "148 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "149 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "150 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "151 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "152 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "153 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "154 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "155 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "156 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "157 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "158 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "159 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "160 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "161 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "162 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "163 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "164 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "165 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "166 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "167 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "168 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "169 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "170 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "171 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "172 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "173 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "174 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "175 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "176 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "177 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "178 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "179 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "180 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "181 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "182 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "183 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "184 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "185 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "186 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "187 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "188 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "189 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "190 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "191 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "192 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "193 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "194 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "195 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "196 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "197 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "198 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "199 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "200 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "201 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "202 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "203 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "204 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "205 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "206 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "207 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "208 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "209 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "210 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "211 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "212 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "213 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "214 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "215 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "216 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "217 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "218 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "219 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "220 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "221 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "222 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "223 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "224 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "225 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "226 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "227 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "228 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "229 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "230 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "231 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "232 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "233 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "234 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "235 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "236 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "237 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "238 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "239 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "240 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "241 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "242 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "243 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "244 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "245 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "246 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "247 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "248 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "249 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "250 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "251 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "252 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "253 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "254 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "255 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "256 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "257 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "258 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "259 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "260 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "261 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "262 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "263 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "264 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "265 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "266 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "267 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "268 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "269 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "270 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "271 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "272 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "273 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "274 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "275 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "276 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "277 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "278 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "279 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "280 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "281 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "282 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "283 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "284 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "285 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "286 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "287 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "288 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "289 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "290 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "291 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "292 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "293 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "294 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "295 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "296 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "297 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "298 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "299 Train Loss 0.0128526045 Test MSE 0.0017953621376875833 Test RE 0.09916031440497149\n",
      "Training time: 181.30\n",
      "KG_swish_low\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 10874.797 Test MSE 3.9780817880063335 Test RE 4.6676546707621425\n",
      "1 Train Loss 4068.0781 Test MSE 10.605623250700859 Test RE 7.62131587237158\n",
      "2 Train Loss 1446.5032 Test MSE 17.018365962220535 Test RE 9.654300071455149\n",
      "3 Train Loss 747.03986 Test MSE 18.14856622196319 Test RE 9.969721445627808\n",
      "4 Train Loss 587.75183 Test MSE 17.43409376465065 Test RE 9.771507118365403\n",
      "5 Train Loss 512.8042 Test MSE 17.514017059378933 Test RE 9.79387931752423\n",
      "6 Train Loss 389.83044 Test MSE 17.09883702669372 Test RE 9.677098252845\n",
      "7 Train Loss 250.3133 Test MSE 13.91914017879738 Test RE 8.731082026725034\n",
      "8 Train Loss 124.90106 Test MSE 11.14054356815697 Test RE 7.811151384204216\n",
      "9 Train Loss 67.11735 Test MSE 10.006876304039514 Test RE 7.40305762246712\n",
      "10 Train Loss 44.148464 Test MSE 8.753179367682435 Test RE 6.923804202959723\n",
      "11 Train Loss 28.470797 Test MSE 6.829895419183695 Test RE 6.116019915117865\n",
      "12 Train Loss 20.7259 Test MSE 5.737432730854924 Test RE 5.605581450543413\n",
      "13 Train Loss 13.864879 Test MSE 5.230048066384786 Test RE 5.351982739821341\n",
      "14 Train Loss 9.34356 Test MSE 4.361374990184037 Test RE 4.887351541616239\n",
      "15 Train Loss 7.439843 Test MSE 3.7769361582467713 Test RE 4.548117604033714\n",
      "16 Train Loss 6.347324 Test MSE 3.2009237841756075 Test RE 4.186966930633573\n",
      "17 Train Loss 5.0075054 Test MSE 2.571286367489146 Test RE 3.7526415916826457\n",
      "18 Train Loss 4.2914863 Test MSE 1.9753407889644161 Test RE 3.289143942172531\n",
      "19 Train Loss 3.785866 Test MSE 1.53146523359322 Test RE 2.896112546830885\n",
      "20 Train Loss 3.2004519 Test MSE 1.2723735441296022 Test RE 2.6397887838447804\n",
      "21 Train Loss 2.5054765 Test MSE 0.9590817036322081 Test RE 2.2918683494280083\n",
      "22 Train Loss 1.7315494 Test MSE 0.5189786634244535 Test RE 1.6859186527535912\n",
      "23 Train Loss 1.2125825 Test MSE 0.2667565657055044 Test RE 1.2087025068074853\n",
      "24 Train Loss 0.93873817 Test MSE 0.24247289754433352 Test RE 1.1523740373489355\n",
      "25 Train Loss 0.61970216 Test MSE 0.12530211592707122 Test RE 0.8284018621161638\n",
      "26 Train Loss 0.46303785 Test MSE 0.07435060836474257 Test RE 0.6381226017912851\n",
      "27 Train Loss 0.36874014 Test MSE 0.06367943891679036 Test RE 0.5905565328434927\n",
      "28 Train Loss 0.29912826 Test MSE 0.06824233286296247 Test RE 0.6113484232244922\n",
      "29 Train Loss 0.22227038 Test MSE 0.03781354782271969 Test RE 0.455077723959884\n",
      "30 Train Loss 0.19613637 Test MSE 0.026900337231289782 Test RE 0.3838315999280178\n",
      "31 Train Loss 0.14994048 Test MSE 0.031505477204294 Test RE 0.41538890760087477\n",
      "32 Train Loss 0.13401076 Test MSE 0.02542688040250216 Test RE 0.37317144497374877\n",
      "33 Train Loss 0.11538671 Test MSE 0.022577861583544277 Test RE 0.3516440449994524\n",
      "34 Train Loss 0.10692913 Test MSE 0.02082361431632806 Test RE 0.3377068887631918\n",
      "35 Train Loss 0.09191182 Test MSE 0.01471694637299837 Test RE 0.28390348112077896\n",
      "36 Train Loss 0.082190126 Test MSE 0.010134838194638267 Test RE 0.23559728087324533\n",
      "37 Train Loss 0.073827356 Test MSE 0.01113394374422217 Test RE 0.2469371180963058\n",
      "38 Train Loss 0.065374866 Test MSE 0.010527348599146866 Test RE 0.2401161471618873\n",
      "39 Train Loss 0.05780036 Test MSE 0.008098413432863698 Test RE 0.21060168235225896\n",
      "40 Train Loss 0.053954735 Test MSE 0.007367901332893891 Test RE 0.20087864245224993\n",
      "41 Train Loss 0.048551198 Test MSE 0.00641065544013394 Test RE 0.1873756194341268\n",
      "42 Train Loss 0.0459097 Test MSE 0.0061343816298445215 Test RE 0.18329358238009927\n",
      "43 Train Loss 0.04080114 Test MSE 0.007516699010340574 Test RE 0.2028969151946679\n",
      "44 Train Loss 0.033813287 Test MSE 0.006855520700129569 Test RE 0.19376801367012966\n",
      "45 Train Loss 0.030954652 Test MSE 0.0058213783625398335 Test RE 0.1785561354188437\n",
      "46 Train Loss 0.03017213 Test MSE 0.005237851190709108 Test RE 0.16937076220029637\n",
      "47 Train Loss 0.028315745 Test MSE 0.005623205670144651 Test RE 0.17549059568953437\n",
      "48 Train Loss 0.026948359 Test MSE 0.004944308403461116 Test RE 0.1645563478736366\n",
      "49 Train Loss 0.025924042 Test MSE 0.005422812244169614 Test RE 0.17233526171353541\n",
      "50 Train Loss 0.025337715 Test MSE 0.0059603440408824504 Test RE 0.18067477742091753\n",
      "51 Train Loss 0.02140193 Test MSE 0.004834680442930635 Test RE 0.16272180422812368\n",
      "52 Train Loss 0.020148398 Test MSE 0.004334226285178389 Test RE 0.15406984826203496\n",
      "53 Train Loss 0.019794079 Test MSE 0.004079018377177921 Test RE 0.14946506489057776\n",
      "54 Train Loss 0.019458879 Test MSE 0.0036876143271130485 Test RE 0.14211326288478227\n",
      "55 Train Loss 0.016814144 Test MSE 0.003326925366682571 Test RE 0.13498434181090457\n",
      "56 Train Loss 0.015947232 Test MSE 0.002522953461015107 Test RE 0.11754833548665235\n",
      "57 Train Loss 0.015472296 Test MSE 0.0021477358733416134 Test RE 0.10845567949247978\n",
      "58 Train Loss 0.014647721 Test MSE 0.0019812931737763374 Test RE 0.10416845807027059\n",
      "59 Train Loss 0.014096905 Test MSE 0.002339002367994693 Test RE 0.11318195591741041\n",
      "60 Train Loss 0.013391096 Test MSE 0.0020837888824271566 Test RE 0.10682889158627111\n",
      "61 Train Loss 0.013042016 Test MSE 0.002075270409377656 Test RE 0.1066103111483094\n",
      "62 Train Loss 0.012443966 Test MSE 0.0020088136141837177 Test RE 0.10488942039240136\n",
      "63 Train Loss 0.011851981 Test MSE 0.0017857087821113622 Test RE 0.09889337105687913\n",
      "64 Train Loss 0.011640255 Test MSE 0.0016818680331307072 Test RE 0.09597493399213552\n",
      "65 Train Loss 0.011484946 Test MSE 0.0016820293946143063 Test RE 0.09597953788673236\n",
      "66 Train Loss 0.011362724 Test MSE 0.001570860116901701 Test RE 0.09275356712306758\n",
      "67 Train Loss 0.0110105695 Test MSE 0.001517242108575224 Test RE 0.09115684965199398\n",
      "68 Train Loss 0.010718442 Test MSE 0.0013467902702450959 Test RE 0.08588391810634485\n",
      "69 Train Loss 0.0104105 Test MSE 0.0011191842059541845 Test RE 0.0782911460411684\n",
      "70 Train Loss 0.010070077 Test MSE 0.0010108997155609774 Test RE 0.07440736091276964\n",
      "71 Train Loss 0.009822191 Test MSE 0.000820791966244152 Test RE 0.06704685523767322\n",
      "72 Train Loss 0.009252102 Test MSE 0.0007935613485553595 Test RE 0.06592530040344179\n",
      "73 Train Loss 0.008959623 Test MSE 0.0008523807860608541 Test RE 0.06832485045226516\n",
      "74 Train Loss 0.008628866 Test MSE 0.0008178361181719251 Test RE 0.06692602129288473\n",
      "75 Train Loss 0.008515885 Test MSE 0.00087372358893166 Test RE 0.06917495647464618\n",
      "76 Train Loss 0.008331065 Test MSE 0.0007522666659981613 Test RE 0.06418710274013678\n",
      "77 Train Loss 0.007968508 Test MSE 0.0006627406854323004 Test RE 0.060246757824134925\n",
      "78 Train Loss 0.007488882 Test MSE 0.0006656236621761318 Test RE 0.06037765481279147\n",
      "79 Train Loss 0.007364106 Test MSE 0.000674673917104931 Test RE 0.06078673602811511\n",
      "80 Train Loss 0.007271554 Test MSE 0.0006783887203695264 Test RE 0.06095385437961232\n",
      "81 Train Loss 0.007188589 Test MSE 0.0006963556676981532 Test RE 0.06175575298142257\n",
      "82 Train Loss 0.0071015484 Test MSE 0.0007289634649522939 Test RE 0.06318510990311957\n",
      "83 Train Loss 0.0068492955 Test MSE 0.0007758138627593236 Test RE 0.06518394364340116\n",
      "84 Train Loss 0.006545916 Test MSE 0.0009472971533970094 Test RE 0.07202860109054905\n",
      "85 Train Loss 0.0063546253 Test MSE 0.0009066780043284501 Test RE 0.07046742534600245\n",
      "86 Train Loss 0.006266754 Test MSE 0.000837923683626373 Test RE 0.06774294875247315\n",
      "87 Train Loss 0.006110353 Test MSE 0.0007571575479130316 Test RE 0.06439542177616729\n",
      "88 Train Loss 0.0060158647 Test MSE 0.0007313760334077248 Test RE 0.06328958186334056\n",
      "89 Train Loss 0.0059712604 Test MSE 0.0007138623275679847 Test RE 0.06252721651269627\n",
      "90 Train Loss 0.005804324 Test MSE 0.0006633644951092407 Test RE 0.060275105013175116\n",
      "91 Train Loss 0.005727888 Test MSE 0.0006560020643747942 Test RE 0.05993968648993618\n",
      "92 Train Loss 0.005631039 Test MSE 0.0006848227150969534 Test RE 0.06124222244636241\n",
      "93 Train Loss 0.0052914014 Test MSE 0.0007192738401463008 Test RE 0.06276376631221968\n",
      "94 Train Loss 0.005097208 Test MSE 0.0006838877135261709 Test RE 0.06120040057673261\n",
      "95 Train Loss 0.005041716 Test MSE 0.0006778795011670655 Test RE 0.06093097317666073\n",
      "96 Train Loss 0.0050141956 Test MSE 0.0006639725506759958 Test RE 0.0603027234782518\n",
      "97 Train Loss 0.004959599 Test MSE 0.0006567757202449603 Test RE 0.059975021002186685\n",
      "98 Train Loss 0.0048385644 Test MSE 0.0006437925007079254 Test RE 0.05937926543671559\n",
      "99 Train Loss 0.0047307247 Test MSE 0.0006662568318978988 Test RE 0.06040636489181564\n",
      "100 Train Loss 0.0045764544 Test MSE 0.0006213040650650036 Test RE 0.05833295289847256\n",
      "101 Train Loss 0.0045006853 Test MSE 0.000633338795365528 Test RE 0.058895201214833943\n",
      "102 Train Loss 0.0044534695 Test MSE 0.0006307571486854703 Test RE 0.05877504288618014\n",
      "103 Train Loss 0.0044016605 Test MSE 0.0006264267783356841 Test RE 0.05857293970907071\n",
      "104 Train Loss 0.00432719 Test MSE 0.0006286499326741531 Test RE 0.05867678373451436\n",
      "105 Train Loss 0.0042736535 Test MSE 0.0006217994998877035 Test RE 0.0583562059389453\n",
      "106 Train Loss 0.0042247116 Test MSE 0.0006208603002562054 Test RE 0.05831211709755894\n",
      "107 Train Loss 0.0041787904 Test MSE 0.0006269878156016527 Test RE 0.058599163242727674\n",
      "108 Train Loss 0.004018543 Test MSE 0.000650009582515581 Test RE 0.059665288319008795\n",
      "109 Train Loss 0.0038796116 Test MSE 0.0005933638264346251 Test RE 0.05700624006305964\n",
      "110 Train Loss 0.0037832644 Test MSE 0.0005858411299363309 Test RE 0.056643723404523595\n",
      "111 Train Loss 0.0037212996 Test MSE 0.0005486551515160189 Test RE 0.054816536860950095\n",
      "112 Train Loss 0.0037019483 Test MSE 0.0005613696272743454 Test RE 0.055448055408621896\n",
      "113 Train Loss 0.0036904279 Test MSE 0.0005548207295554258 Test RE 0.05512368008419632\n",
      "114 Train Loss 0.003663141 Test MSE 0.0005262048542581921 Test RE 0.05368331042577473\n",
      "115 Train Loss 0.0035908865 Test MSE 0.0005205002861137066 Test RE 0.05339152803892203\n",
      "116 Train Loss 0.0034563332 Test MSE 0.0005263887273798392 Test RE 0.053692688955480426\n",
      "117 Train Loss 0.0033542395 Test MSE 0.0005497515186157859 Test RE 0.05487127894588048\n",
      "118 Train Loss 0.0033049863 Test MSE 0.0005803688549573119 Test RE 0.05637855144608848\n",
      "119 Train Loss 0.0032590965 Test MSE 0.0006095302802583716 Test RE 0.05777760117448653\n",
      "120 Train Loss 0.003184883 Test MSE 0.0005905398800745002 Test RE 0.05687042578539707\n",
      "121 Train Loss 0.003142854 Test MSE 0.0005673275595729968 Test RE 0.055741519671244806\n",
      "122 Train Loss 0.003113477 Test MSE 0.000552127697346817 Test RE 0.05498973553608407\n",
      "123 Train Loss 0.0030945179 Test MSE 0.0005702105841749948 Test RE 0.05588297280693029\n",
      "124 Train Loss 0.003083758 Test MSE 0.0005781495731214751 Test RE 0.0562706547737194\n",
      "125 Train Loss 0.0030622107 Test MSE 0.0005920680496516874 Test RE 0.056943961465429606\n",
      "126 Train Loss 0.003022124 Test MSE 0.0005876193436386878 Test RE 0.05672962410603875\n",
      "127 Train Loss 0.0029882581 Test MSE 0.0005866219913340587 Test RE 0.05668146074137337\n",
      "128 Train Loss 0.0029141584 Test MSE 0.0005963349960882641 Test RE 0.05714878642561679\n",
      "129 Train Loss 0.0027950702 Test MSE 0.0005505489066961898 Test RE 0.05491105859918116\n",
      "130 Train Loss 0.002728924 Test MSE 0.0005489678592127875 Test RE 0.054832156062888514\n",
      "131 Train Loss 0.0026739594 Test MSE 0.0005031861976949689 Test RE 0.05249600132841104\n",
      "132 Train Loss 0.0026523909 Test MSE 0.0004970369480499164 Test RE 0.05217424833743581\n",
      "133 Train Loss 0.002639266 Test MSE 0.0005008203123859876 Test RE 0.05237244283881743\n",
      "134 Train Loss 0.0026155673 Test MSE 0.0004833232398971915 Test RE 0.05144944604336611\n",
      "135 Train Loss 0.002575048 Test MSE 0.0004798543264123191 Test RE 0.05126448176619346\n",
      "136 Train Loss 0.0025354887 Test MSE 0.0004635222522913938 Test RE 0.05038452378209788\n",
      "137 Train Loss 0.0024821819 Test MSE 0.0004014355343199213 Test RE 0.04688887041712952\n",
      "138 Train Loss 0.0024243107 Test MSE 0.0003685952873820953 Test RE 0.044930034616552685\n",
      "139 Train Loss 0.0023609828 Test MSE 0.0003732208405378411 Test RE 0.04521107227446127\n",
      "140 Train Loss 0.0022614275 Test MSE 0.00036587524131641205 Test RE 0.04476394721793647\n",
      "141 Train Loss 0.002192941 Test MSE 0.00034583674991303025 Test RE 0.04352085658344549\n",
      "142 Train Loss 0.0021651632 Test MSE 0.000341993288754287 Test RE 0.04327834611603978\n",
      "143 Train Loss 0.0021385655 Test MSE 0.00034226253913455305 Test RE 0.0432953792258955\n",
      "144 Train Loss 0.0021077078 Test MSE 0.0003422386644002821 Test RE 0.0432938691516358\n",
      "145 Train Loss 0.0020771697 Test MSE 0.00033785763750761587 Test RE 0.043015872329521324\n",
      "146 Train Loss 0.0020481434 Test MSE 0.0003405425460435839 Test RE 0.04318645473477533\n",
      "147 Train Loss 0.0020308227 Test MSE 0.0003324965267444406 Test RE 0.042673220572825354\n",
      "148 Train Loss 0.0020176482 Test MSE 0.0003332443193910535 Test RE 0.04272118016960373\n",
      "149 Train Loss 0.0020053347 Test MSE 0.0003228969678945034 Test RE 0.042052696369302735\n",
      "150 Train Loss 0.0019767312 Test MSE 0.00034475068906845676 Test RE 0.04345246670692773\n",
      "151 Train Loss 0.0019286547 Test MSE 0.0003415458176401542 Test RE 0.04325002370563614\n",
      "152 Train Loss 0.0019102443 Test MSE 0.00034650799167252786 Test RE 0.04356307137851391\n",
      "153 Train Loss 0.0018739484 Test MSE 0.00034875524224993644 Test RE 0.04370410553388706\n",
      "154 Train Loss 0.0018476516 Test MSE 0.0003647767221994737 Test RE 0.04469669612755982\n",
      "155 Train Loss 0.0018123746 Test MSE 0.0003495408191835065 Test RE 0.0437532999558202\n",
      "156 Train Loss 0.0017631747 Test MSE 0.00031764334184150096 Test RE 0.041709188696281624\n",
      "157 Train Loss 0.0016763653 Test MSE 0.0003394575500446251 Test RE 0.043117602085722975\n",
      "158 Train Loss 0.0016190322 Test MSE 0.0002953898375899753 Test RE 0.040221627449104215\n",
      "159 Train Loss 0.0015749787 Test MSE 0.0002748145045984979 Test RE 0.03879553001599593\n",
      "160 Train Loss 0.0015152154 Test MSE 0.000291647301756359 Test RE 0.039966014853175226\n",
      "161 Train Loss 0.0014718419 Test MSE 0.0002737527155336559 Test RE 0.038720511167852634\n",
      "162 Train Loss 0.0014591852 Test MSE 0.00026856768129792475 Test RE 0.038352063775107956\n",
      "163 Train Loss 0.0014509159 Test MSE 0.00026199656767786534 Test RE 0.037879973364211096\n",
      "164 Train Loss 0.0014458584 Test MSE 0.0002565994131588379 Test RE 0.03748777748616345\n",
      "165 Train Loss 0.001439946 Test MSE 0.00026123023261044994 Test RE 0.03782453368719563\n",
      "166 Train Loss 0.0014323457 Test MSE 0.00025501146457500946 Test RE 0.03737160214925237\n",
      "167 Train Loss 0.0014247113 Test MSE 0.000250447664989491 Test RE 0.03703568294412168\n",
      "168 Train Loss 0.0014160314 Test MSE 0.0002529459927557749 Test RE 0.03721994832462378\n",
      "169 Train Loss 0.0013984803 Test MSE 0.00025879042132449407 Test RE 0.037647484474209666\n",
      "170 Train Loss 0.0013851158 Test MSE 0.0002464664730939432 Test RE 0.03674013850952014\n",
      "171 Train Loss 0.0013648093 Test MSE 0.00023494653312640967 Test RE 0.03587123957490905\n",
      "172 Train Loss 0.0013529056 Test MSE 0.00022290653333985062 Test RE 0.034940029144930856\n",
      "173 Train Loss 0.0013302043 Test MSE 0.00023298109698545103 Test RE 0.03572088473359072\n",
      "174 Train Loss 0.0013018036 Test MSE 0.00024161561488341917 Test RE 0.03637678917075166\n",
      "175 Train Loss 0.0012715374 Test MSE 0.00026451551026254635 Test RE 0.03806163457657898\n",
      "176 Train Loss 0.0012583762 Test MSE 0.0002685868035893583 Test RE 0.03835342910373418\n",
      "177 Train Loss 0.0012384845 Test MSE 0.0002553880673624244 Test RE 0.03739918729558659\n",
      "178 Train Loss 0.0012213446 Test MSE 0.000276554708366721 Test RE 0.038918168353748846\n",
      "179 Train Loss 0.001211827 Test MSE 0.0002683307684327707 Test RE 0.038335144200227815\n",
      "180 Train Loss 0.0012046095 Test MSE 0.0002717475497623137 Test RE 0.03857844181599541\n",
      "181 Train Loss 0.0011973656 Test MSE 0.00027114082372056974 Test RE 0.038535351041538914\n",
      "182 Train Loss 0.0011943855 Test MSE 0.00027150231320153807 Test RE 0.03856103047258094\n",
      "183 Train Loss 0.0011925364 Test MSE 0.00026889543199606344 Test RE 0.038375458405061985\n",
      "184 Train Loss 0.0011886982 Test MSE 0.00027273301617261326 Test RE 0.03864832902200807\n",
      "185 Train Loss 0.0011840957 Test MSE 0.00027555031946270597 Test RE 0.03884743274408837\n",
      "186 Train Loss 0.0011798033 Test MSE 0.00028675640032422856 Test RE 0.03962948458906911\n",
      "187 Train Loss 0.0011768643 Test MSE 0.0002806355385962108 Test RE 0.03920425431906805\n",
      "188 Train Loss 0.0011661806 Test MSE 0.000281900170189546 Test RE 0.03929248834992853\n",
      "189 Train Loss 0.0011338574 Test MSE 0.00026350184738681515 Test RE 0.037988635644325294\n",
      "190 Train Loss 0.0010992723 Test MSE 0.00024657694361137055 Test RE 0.03674837136847131\n",
      "191 Train Loss 0.0010843764 Test MSE 0.00023784157005352565 Test RE 0.03609156759889113\n",
      "192 Train Loss 0.0010683152 Test MSE 0.0002497464363413087 Test RE 0.03698379847940836\n",
      "193 Train Loss 0.0010630088 Test MSE 0.0002422117293757573 Test RE 0.036421635967060384\n",
      "194 Train Loss 0.0010582906 Test MSE 0.00023415162087019118 Test RE 0.035810505229748914\n",
      "195 Train Loss 0.0010552028 Test MSE 0.00022820002312974198 Test RE 0.035352465416207436\n",
      "196 Train Loss 0.0010502754 Test MSE 0.0002254081313028699 Test RE 0.035135541676231494\n",
      "197 Train Loss 0.0010420264 Test MSE 0.00022595263617080667 Test RE 0.03517795348634682\n",
      "198 Train Loss 0.0010355995 Test MSE 0.00022743691130970518 Test RE 0.03529330574680771\n",
      "199 Train Loss 0.0010307241 Test MSE 0.00021917020162893393 Test RE 0.034645961434642956\n",
      "200 Train Loss 0.0010263812 Test MSE 0.00022654372309283756 Test RE 0.03522393579420424\n",
      "201 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "202 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "203 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "204 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "205 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "206 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "207 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "208 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "209 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "210 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "211 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "212 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "213 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "214 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "215 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "216 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "217 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "218 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "219 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "220 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "221 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "222 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "223 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "224 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "225 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "226 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "227 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "228 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "229 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "230 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "231 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "232 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "233 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "234 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "235 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "236 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "237 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "238 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "239 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "240 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "241 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "242 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "243 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "244 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "245 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "246 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "247 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "248 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "249 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "250 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "251 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "252 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "253 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "254 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "255 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "256 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "257 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "258 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "259 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "260 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "261 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "262 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "263 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "264 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "265 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "266 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "267 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "268 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "269 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "270 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "271 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "272 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "273 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "274 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "275 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "276 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "277 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "278 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "279 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "280 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "281 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "282 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "283 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "284 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "285 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "286 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "287 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "288 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "289 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "290 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "291 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "292 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "293 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "294 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "295 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "296 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "297 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "298 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "299 Train Loss 0.0010250221 Test MSE 0.00022546167679859207 Test RE 0.03513971463634316\n",
      "Training time: 215.20\n",
      "KG_swish_low\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss nan Test MSE nan Test RE nan\n",
      "1 Train Loss nan Test MSE nan Test RE nan\n",
      "2 Train Loss nan Test MSE nan Test RE nan\n",
      "3 Train Loss nan Test MSE nan Test RE nan\n",
      "4 Train Loss nan Test MSE nan Test RE nan\n",
      "5 Train Loss nan Test MSE nan Test RE nan\n",
      "6 Train Loss nan Test MSE nan Test RE nan\n",
      "7 Train Loss nan Test MSE nan Test RE nan\n",
      "8 Train Loss nan Test MSE nan Test RE nan\n",
      "9 Train Loss nan Test MSE nan Test RE nan\n",
      "10 Train Loss nan Test MSE nan Test RE nan\n",
      "11 Train Loss nan Test MSE nan Test RE nan\n",
      "12 Train Loss nan Test MSE nan Test RE nan\n",
      "13 Train Loss nan Test MSE nan Test RE nan\n",
      "14 Train Loss nan Test MSE nan Test RE nan\n",
      "15 Train Loss nan Test MSE nan Test RE nan\n",
      "16 Train Loss nan Test MSE nan Test RE nan\n",
      "17 Train Loss nan Test MSE nan Test RE nan\n",
      "18 Train Loss nan Test MSE nan Test RE nan\n",
      "19 Train Loss nan Test MSE nan Test RE nan\n",
      "20 Train Loss nan Test MSE nan Test RE nan\n",
      "21 Train Loss nan Test MSE nan Test RE nan\n",
      "22 Train Loss nan Test MSE nan Test RE nan\n",
      "23 Train Loss nan Test MSE nan Test RE nan\n",
      "24 Train Loss nan Test MSE nan Test RE nan\n",
      "25 Train Loss nan Test MSE nan Test RE nan\n",
      "26 Train Loss nan Test MSE nan Test RE nan\n",
      "27 Train Loss nan Test MSE nan Test RE nan\n",
      "28 Train Loss nan Test MSE nan Test RE nan\n",
      "29 Train Loss nan Test MSE nan Test RE nan\n",
      "30 Train Loss nan Test MSE nan Test RE nan\n",
      "31 Train Loss nan Test MSE nan Test RE nan\n",
      "32 Train Loss nan Test MSE nan Test RE nan\n",
      "33 Train Loss nan Test MSE nan Test RE nan\n",
      "34 Train Loss nan Test MSE nan Test RE nan\n",
      "35 Train Loss nan Test MSE nan Test RE nan\n",
      "36 Train Loss nan Test MSE nan Test RE nan\n",
      "37 Train Loss nan Test MSE nan Test RE nan\n",
      "38 Train Loss nan Test MSE nan Test RE nan\n",
      "39 Train Loss nan Test MSE nan Test RE nan\n",
      "40 Train Loss nan Test MSE nan Test RE nan\n",
      "41 Train Loss nan Test MSE nan Test RE nan\n",
      "42 Train Loss nan Test MSE nan Test RE nan\n",
      "43 Train Loss nan Test MSE nan Test RE nan\n",
      "44 Train Loss nan Test MSE nan Test RE nan\n",
      "45 Train Loss nan Test MSE nan Test RE nan\n",
      "46 Train Loss nan Test MSE nan Test RE nan\n",
      "47 Train Loss nan Test MSE nan Test RE nan\n",
      "48 Train Loss nan Test MSE nan Test RE nan\n",
      "49 Train Loss nan Test MSE nan Test RE nan\n",
      "50 Train Loss nan Test MSE nan Test RE nan\n",
      "51 Train Loss nan Test MSE nan Test RE nan\n",
      "52 Train Loss nan Test MSE nan Test RE nan\n",
      "53 Train Loss nan Test MSE nan Test RE nan\n",
      "54 Train Loss nan Test MSE nan Test RE nan\n",
      "55 Train Loss nan Test MSE nan Test RE nan\n",
      "56 Train Loss nan Test MSE nan Test RE nan\n",
      "57 Train Loss nan Test MSE nan Test RE nan\n",
      "58 Train Loss nan Test MSE nan Test RE nan\n",
      "59 Train Loss nan Test MSE nan Test RE nan\n",
      "60 Train Loss nan Test MSE nan Test RE nan\n",
      "61 Train Loss nan Test MSE nan Test RE nan\n",
      "62 Train Loss nan Test MSE nan Test RE nan\n",
      "63 Train Loss nan Test MSE nan Test RE nan\n",
      "64 Train Loss nan Test MSE nan Test RE nan\n",
      "65 Train Loss nan Test MSE nan Test RE nan\n",
      "66 Train Loss nan Test MSE nan Test RE nan\n",
      "67 Train Loss nan Test MSE nan Test RE nan\n",
      "68 Train Loss nan Test MSE nan Test RE nan\n",
      "69 Train Loss nan Test MSE nan Test RE nan\n",
      "70 Train Loss nan Test MSE nan Test RE nan\n",
      "71 Train Loss nan Test MSE nan Test RE nan\n",
      "72 Train Loss nan Test MSE nan Test RE nan\n",
      "73 Train Loss nan Test MSE nan Test RE nan\n",
      "74 Train Loss nan Test MSE nan Test RE nan\n",
      "75 Train Loss nan Test MSE nan Test RE nan\n",
      "76 Train Loss nan Test MSE nan Test RE nan\n",
      "77 Train Loss nan Test MSE nan Test RE nan\n",
      "78 Train Loss nan Test MSE nan Test RE nan\n",
      "79 Train Loss nan Test MSE nan Test RE nan\n",
      "80 Train Loss nan Test MSE nan Test RE nan\n",
      "81 Train Loss nan Test MSE nan Test RE nan\n",
      "82 Train Loss nan Test MSE nan Test RE nan\n",
      "83 Train Loss nan Test MSE nan Test RE nan\n",
      "84 Train Loss nan Test MSE nan Test RE nan\n",
      "85 Train Loss nan Test MSE nan Test RE nan\n",
      "86 Train Loss nan Test MSE nan Test RE nan\n",
      "87 Train Loss nan Test MSE nan Test RE nan\n",
      "88 Train Loss nan Test MSE nan Test RE nan\n",
      "89 Train Loss nan Test MSE nan Test RE nan\n",
      "90 Train Loss nan Test MSE nan Test RE nan\n",
      "91 Train Loss nan Test MSE nan Test RE nan\n",
      "92 Train Loss nan Test MSE nan Test RE nan\n",
      "93 Train Loss nan Test MSE nan Test RE nan\n",
      "94 Train Loss nan Test MSE nan Test RE nan\n",
      "95 Train Loss nan Test MSE nan Test RE nan\n",
      "96 Train Loss nan Test MSE nan Test RE nan\n",
      "97 Train Loss nan Test MSE nan Test RE nan\n",
      "98 Train Loss nan Test MSE nan Test RE nan\n",
      "99 Train Loss nan Test MSE nan Test RE nan\n",
      "100 Train Loss nan Test MSE nan Test RE nan\n",
      "101 Train Loss nan Test MSE nan Test RE nan\n",
      "102 Train Loss nan Test MSE nan Test RE nan\n",
      "103 Train Loss nan Test MSE nan Test RE nan\n",
      "104 Train Loss nan Test MSE nan Test RE nan\n",
      "105 Train Loss nan Test MSE nan Test RE nan\n",
      "106 Train Loss nan Test MSE nan Test RE nan\n",
      "107 Train Loss nan Test MSE nan Test RE nan\n",
      "108 Train Loss nan Test MSE nan Test RE nan\n",
      "109 Train Loss nan Test MSE nan Test RE nan\n",
      "110 Train Loss nan Test MSE nan Test RE nan\n",
      "111 Train Loss nan Test MSE nan Test RE nan\n",
      "112 Train Loss nan Test MSE nan Test RE nan\n",
      "113 Train Loss nan Test MSE nan Test RE nan\n",
      "114 Train Loss nan Test MSE nan Test RE nan\n",
      "115 Train Loss nan Test MSE nan Test RE nan\n",
      "116 Train Loss nan Test MSE nan Test RE nan\n",
      "117 Train Loss nan Test MSE nan Test RE nan\n",
      "118 Train Loss nan Test MSE nan Test RE nan\n",
      "119 Train Loss nan Test MSE nan Test RE nan\n",
      "120 Train Loss nan Test MSE nan Test RE nan\n",
      "121 Train Loss nan Test MSE nan Test RE nan\n",
      "122 Train Loss nan Test MSE nan Test RE nan\n",
      "123 Train Loss nan Test MSE nan Test RE nan\n",
      "124 Train Loss nan Test MSE nan Test RE nan\n",
      "125 Train Loss nan Test MSE nan Test RE nan\n",
      "126 Train Loss nan Test MSE nan Test RE nan\n",
      "127 Train Loss nan Test MSE nan Test RE nan\n",
      "128 Train Loss nan Test MSE nan Test RE nan\n",
      "129 Train Loss nan Test MSE nan Test RE nan\n",
      "130 Train Loss nan Test MSE nan Test RE nan\n",
      "131 Train Loss nan Test MSE nan Test RE nan\n",
      "132 Train Loss nan Test MSE nan Test RE nan\n",
      "133 Train Loss nan Test MSE nan Test RE nan\n",
      "134 Train Loss nan Test MSE nan Test RE nan\n",
      "135 Train Loss nan Test MSE nan Test RE nan\n",
      "136 Train Loss nan Test MSE nan Test RE nan\n",
      "137 Train Loss nan Test MSE nan Test RE nan\n",
      "138 Train Loss nan Test MSE nan Test RE nan\n",
      "139 Train Loss nan Test MSE nan Test RE nan\n",
      "140 Train Loss nan Test MSE nan Test RE nan\n",
      "141 Train Loss nan Test MSE nan Test RE nan\n",
      "142 Train Loss nan Test MSE nan Test RE nan\n",
      "143 Train Loss nan Test MSE nan Test RE nan\n",
      "144 Train Loss nan Test MSE nan Test RE nan\n",
      "145 Train Loss nan Test MSE nan Test RE nan\n",
      "146 Train Loss nan Test MSE nan Test RE nan\n",
      "147 Train Loss nan Test MSE nan Test RE nan\n",
      "148 Train Loss nan Test MSE nan Test RE nan\n",
      "149 Train Loss nan Test MSE nan Test RE nan\n",
      "150 Train Loss nan Test MSE nan Test RE nan\n",
      "151 Train Loss nan Test MSE nan Test RE nan\n",
      "152 Train Loss nan Test MSE nan Test RE nan\n",
      "153 Train Loss nan Test MSE nan Test RE nan\n",
      "154 Train Loss nan Test MSE nan Test RE nan\n",
      "155 Train Loss nan Test MSE nan Test RE nan\n",
      "156 Train Loss nan Test MSE nan Test RE nan\n",
      "157 Train Loss nan Test MSE nan Test RE nan\n",
      "158 Train Loss nan Test MSE nan Test RE nan\n",
      "159 Train Loss nan Test MSE nan Test RE nan\n",
      "160 Train Loss nan Test MSE nan Test RE nan\n",
      "161 Train Loss nan Test MSE nan Test RE nan\n",
      "162 Train Loss nan Test MSE nan Test RE nan\n",
      "163 Train Loss nan Test MSE nan Test RE nan\n",
      "164 Train Loss nan Test MSE nan Test RE nan\n",
      "165 Train Loss nan Test MSE nan Test RE nan\n",
      "166 Train Loss nan Test MSE nan Test RE nan\n",
      "167 Train Loss nan Test MSE nan Test RE nan\n",
      "168 Train Loss nan Test MSE nan Test RE nan\n",
      "169 Train Loss nan Test MSE nan Test RE nan\n",
      "170 Train Loss nan Test MSE nan Test RE nan\n",
      "171 Train Loss nan Test MSE nan Test RE nan\n",
      "172 Train Loss nan Test MSE nan Test RE nan\n",
      "173 Train Loss nan Test MSE nan Test RE nan\n",
      "174 Train Loss nan Test MSE nan Test RE nan\n",
      "175 Train Loss nan Test MSE nan Test RE nan\n",
      "176 Train Loss nan Test MSE nan Test RE nan\n",
      "177 Train Loss nan Test MSE nan Test RE nan\n",
      "178 Train Loss nan Test MSE nan Test RE nan\n",
      "179 Train Loss nan Test MSE nan Test RE nan\n",
      "180 Train Loss nan Test MSE nan Test RE nan\n",
      "181 Train Loss nan Test MSE nan Test RE nan\n",
      "182 Train Loss nan Test MSE nan Test RE nan\n",
      "183 Train Loss nan Test MSE nan Test RE nan\n",
      "184 Train Loss nan Test MSE nan Test RE nan\n",
      "185 Train Loss nan Test MSE nan Test RE nan\n",
      "186 Train Loss nan Test MSE nan Test RE nan\n",
      "187 Train Loss nan Test MSE nan Test RE nan\n",
      "188 Train Loss nan Test MSE nan Test RE nan\n",
      "189 Train Loss nan Test MSE nan Test RE nan\n",
      "190 Train Loss nan Test MSE nan Test RE nan\n",
      "191 Train Loss nan Test MSE nan Test RE nan\n",
      "192 Train Loss nan Test MSE nan Test RE nan\n",
      "193 Train Loss nan Test MSE nan Test RE nan\n",
      "194 Train Loss nan Test MSE nan Test RE nan\n",
      "195 Train Loss nan Test MSE nan Test RE nan\n",
      "196 Train Loss nan Test MSE nan Test RE nan\n",
      "197 Train Loss nan Test MSE nan Test RE nan\n",
      "198 Train Loss nan Test MSE nan Test RE nan\n",
      "199 Train Loss nan Test MSE nan Test RE nan\n",
      "200 Train Loss nan Test MSE nan Test RE nan\n",
      "201 Train Loss nan Test MSE nan Test RE nan\n",
      "202 Train Loss nan Test MSE nan Test RE nan\n",
      "203 Train Loss nan Test MSE nan Test RE nan\n",
      "204 Train Loss nan Test MSE nan Test RE nan\n",
      "205 Train Loss nan Test MSE nan Test RE nan\n",
      "206 Train Loss nan Test MSE nan Test RE nan\n",
      "207 Train Loss nan Test MSE nan Test RE nan\n",
      "208 Train Loss nan Test MSE nan Test RE nan\n",
      "209 Train Loss nan Test MSE nan Test RE nan\n",
      "210 Train Loss nan Test MSE nan Test RE nan\n",
      "211 Train Loss nan Test MSE nan Test RE nan\n",
      "212 Train Loss nan Test MSE nan Test RE nan\n",
      "213 Train Loss nan Test MSE nan Test RE nan\n",
      "214 Train Loss nan Test MSE nan Test RE nan\n",
      "215 Train Loss nan Test MSE nan Test RE nan\n",
      "216 Train Loss nan Test MSE nan Test RE nan\n",
      "217 Train Loss nan Test MSE nan Test RE nan\n",
      "218 Train Loss nan Test MSE nan Test RE nan\n",
      "219 Train Loss nan Test MSE nan Test RE nan\n",
      "220 Train Loss nan Test MSE nan Test RE nan\n",
      "221 Train Loss nan Test MSE nan Test RE nan\n",
      "222 Train Loss nan Test MSE nan Test RE nan\n",
      "223 Train Loss nan Test MSE nan Test RE nan\n",
      "224 Train Loss nan Test MSE nan Test RE nan\n",
      "225 Train Loss nan Test MSE nan Test RE nan\n",
      "226 Train Loss nan Test MSE nan Test RE nan\n",
      "227 Train Loss nan Test MSE nan Test RE nan\n",
      "228 Train Loss nan Test MSE nan Test RE nan\n",
      "229 Train Loss nan Test MSE nan Test RE nan\n",
      "230 Train Loss nan Test MSE nan Test RE nan\n",
      "231 Train Loss nan Test MSE nan Test RE nan\n",
      "232 Train Loss nan Test MSE nan Test RE nan\n",
      "233 Train Loss nan Test MSE nan Test RE nan\n",
      "234 Train Loss nan Test MSE nan Test RE nan\n",
      "235 Train Loss nan Test MSE nan Test RE nan\n",
      "236 Train Loss nan Test MSE nan Test RE nan\n",
      "237 Train Loss nan Test MSE nan Test RE nan\n",
      "238 Train Loss nan Test MSE nan Test RE nan\n",
      "239 Train Loss nan Test MSE nan Test RE nan\n",
      "240 Train Loss nan Test MSE nan Test RE nan\n",
      "241 Train Loss nan Test MSE nan Test RE nan\n",
      "242 Train Loss nan Test MSE nan Test RE nan\n",
      "243 Train Loss nan Test MSE nan Test RE nan\n",
      "244 Train Loss nan Test MSE nan Test RE nan\n",
      "245 Train Loss nan Test MSE nan Test RE nan\n",
      "246 Train Loss nan Test MSE nan Test RE nan\n",
      "247 Train Loss nan Test MSE nan Test RE nan\n",
      "248 Train Loss nan Test MSE nan Test RE nan\n",
      "249 Train Loss nan Test MSE nan Test RE nan\n",
      "250 Train Loss nan Test MSE nan Test RE nan\n",
      "251 Train Loss nan Test MSE nan Test RE nan\n",
      "252 Train Loss nan Test MSE nan Test RE nan\n",
      "253 Train Loss nan Test MSE nan Test RE nan\n",
      "254 Train Loss nan Test MSE nan Test RE nan\n",
      "255 Train Loss nan Test MSE nan Test RE nan\n",
      "256 Train Loss nan Test MSE nan Test RE nan\n",
      "257 Train Loss nan Test MSE nan Test RE nan\n",
      "258 Train Loss nan Test MSE nan Test RE nan\n",
      "259 Train Loss nan Test MSE nan Test RE nan\n",
      "260 Train Loss nan Test MSE nan Test RE nan\n",
      "261 Train Loss nan Test MSE nan Test RE nan\n",
      "262 Train Loss nan Test MSE nan Test RE nan\n",
      "263 Train Loss nan Test MSE nan Test RE nan\n",
      "264 Train Loss nan Test MSE nan Test RE nan\n",
      "265 Train Loss nan Test MSE nan Test RE nan\n",
      "266 Train Loss nan Test MSE nan Test RE nan\n",
      "267 Train Loss nan Test MSE nan Test RE nan\n",
      "268 Train Loss nan Test MSE nan Test RE nan\n",
      "269 Train Loss nan Test MSE nan Test RE nan\n",
      "270 Train Loss nan Test MSE nan Test RE nan\n",
      "271 Train Loss nan Test MSE nan Test RE nan\n",
      "272 Train Loss nan Test MSE nan Test RE nan\n",
      "273 Train Loss nan Test MSE nan Test RE nan\n",
      "274 Train Loss nan Test MSE nan Test RE nan\n",
      "275 Train Loss nan Test MSE nan Test RE nan\n",
      "276 Train Loss nan Test MSE nan Test RE nan\n",
      "277 Train Loss nan Test MSE nan Test RE nan\n",
      "278 Train Loss nan Test MSE nan Test RE nan\n",
      "279 Train Loss nan Test MSE nan Test RE nan\n",
      "280 Train Loss nan Test MSE nan Test RE nan\n",
      "281 Train Loss nan Test MSE nan Test RE nan\n",
      "282 Train Loss nan Test MSE nan Test RE nan\n",
      "283 Train Loss nan Test MSE nan Test RE nan\n",
      "284 Train Loss nan Test MSE nan Test RE nan\n",
      "285 Train Loss nan Test MSE nan Test RE nan\n",
      "286 Train Loss nan Test MSE nan Test RE nan\n",
      "287 Train Loss nan Test MSE nan Test RE nan\n",
      "288 Train Loss nan Test MSE nan Test RE nan\n",
      "289 Train Loss nan Test MSE nan Test RE nan\n",
      "290 Train Loss nan Test MSE nan Test RE nan\n",
      "291 Train Loss nan Test MSE nan Test RE nan\n",
      "292 Train Loss nan Test MSE nan Test RE nan\n",
      "293 Train Loss nan Test MSE nan Test RE nan\n",
      "294 Train Loss nan Test MSE nan Test RE nan\n",
      "295 Train Loss nan Test MSE nan Test RE nan\n",
      "296 Train Loss nan Test MSE nan Test RE nan\n",
      "297 Train Loss nan Test MSE nan Test RE nan\n",
      "298 Train Loss nan Test MSE nan Test RE nan\n",
      "299 Train Loss nan Test MSE nan Test RE nan\n",
      "Training time: 30.00\n",
      "KG_swish_low\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 9762.898 Test MSE 0.7356278174991968 Test RE 2.0072013288288413\n",
      "1 Train Loss 8735.263 Test MSE 0.47501958691273777 Test RE 1.6129378108736896\n",
      "2 Train Loss 7720.1074 Test MSE 0.6399694938519687 Test RE 1.8721536987915417\n",
      "3 Train Loss 5797.5723 Test MSE 4.176167403765771 Test RE 4.782454134424806\n",
      "4 Train Loss 4847.814 Test MSE 4.881664129981605 Test RE 5.170657981333016\n",
      "5 Train Loss 3848.9832 Test MSE 4.8468956826185705 Test RE 5.15221171053463\n",
      "6 Train Loss 3007.8625 Test MSE 3.621669803440265 Test RE 4.453652112948144\n",
      "7 Train Loss 1824.8372 Test MSE 2.943418626292423 Test RE 4.0150215529537086\n",
      "8 Train Loss 1167.593 Test MSE 2.3379859771013733 Test RE 3.5783499851284355\n",
      "9 Train Loss 776.1809 Test MSE 0.7553969627441939 Test RE 2.033993127544308\n",
      "10 Train Loss 395.66107 Test MSE 2.9124301936930834 Test RE 3.993830474050465\n",
      "11 Train Loss 202.88809 Test MSE 5.1316606595039005 Test RE 5.301403117126219\n",
      "12 Train Loss 135.0337 Test MSE 4.288381652299131 Test RE 4.846280850446673\n",
      "13 Train Loss 82.76453 Test MSE 3.4977685973384154 Test RE 4.376807049119891\n",
      "14 Train Loss 62.575935 Test MSE 3.426770488439861 Test RE 4.33215885281852\n",
      "15 Train Loss 42.95519 Test MSE 3.112605959193596 Test RE 4.128800858217631\n",
      "16 Train Loss 25.14046 Test MSE 2.3463206238877308 Test RE 3.5847225106155047\n",
      "17 Train Loss 19.180094 Test MSE 1.448963708584303 Test RE 2.817024460824786\n",
      "18 Train Loss 16.661274 Test MSE 1.352910102281542 Test RE 2.722051476736729\n",
      "19 Train Loss 13.840397 Test MSE 1.134573797040487 Test RE 2.4927472091743157\n",
      "20 Train Loss 11.882465 Test MSE 0.6572115344589227 Test RE 1.897205838639321\n",
      "21 Train Loss 10.38115 Test MSE 0.6463333703276322 Test RE 1.8814390496332836\n",
      "22 Train Loss 8.735926 Test MSE 0.6234496178947595 Test RE 1.8478322630215398\n",
      "23 Train Loss 7.545125 Test MSE 0.49939979839303594 Test RE 1.653811644161277\n",
      "24 Train Loss 6.365411 Test MSE 0.4321613506363014 Test RE 1.5384551010052054\n",
      "25 Train Loss 5.651184 Test MSE 0.29925492889546806 Test RE 1.2802138465893507\n",
      "26 Train Loss 5.1399693 Test MSE 0.31128161014184347 Test RE 1.3056855450451361\n",
      "27 Train Loss 4.695026 Test MSE 0.30367189625759533 Test RE 1.289627141534386\n",
      "28 Train Loss 3.7302828 Test MSE 0.21997601102925265 Test RE 1.0976137155697514\n",
      "29 Train Loss 3.078431 Test MSE 0.14124727398013556 Test RE 0.8795325193718427\n",
      "30 Train Loss 2.7570736 Test MSE 0.10208456113754655 Test RE 0.7477249925370147\n",
      "31 Train Loss 2.4288492 Test MSE 0.06668583992645201 Test RE 0.6043362937696144\n",
      "32 Train Loss 2.0609524 Test MSE 0.06623144346276034 Test RE 0.6022738044166531\n",
      "33 Train Loss 1.706313 Test MSE 0.054255288273845334 Test RE 0.545108434210881\n",
      "34 Train Loss 1.598383 Test MSE 0.03722127017537954 Test RE 0.45149969233748943\n",
      "35 Train Loss 1.4246461 Test MSE 0.04107694404507454 Test RE 0.4743085214157294\n",
      "36 Train Loss 1.2956799 Test MSE 0.06339953550692512 Test RE 0.5892572058286005\n",
      "37 Train Loss 1.1610004 Test MSE 0.07114231406222189 Test RE 0.6242030088656397\n",
      "38 Train Loss 1.0702738 Test MSE 0.05741375596401632 Test RE 0.5607507247243834\n",
      "39 Train Loss 0.9617475 Test MSE 0.04872523852999805 Test RE 0.5165815023252772\n",
      "40 Train Loss 0.89661396 Test MSE 0.05318389544923501 Test RE 0.5396994009965609\n",
      "41 Train Loss 0.8416087 Test MSE 0.05589459473841509 Test RE 0.5532822909660506\n",
      "42 Train Loss 0.7900543 Test MSE 0.043006884663673396 Test RE 0.48532298061439844\n",
      "43 Train Loss 0.7436905 Test MSE 0.04824977664978967 Test RE 0.5140549171329493\n",
      "44 Train Loss 0.7113476 Test MSE 0.04183433835731403 Test RE 0.4786613004356555\n",
      "45 Train Loss 0.6377508 Test MSE 0.02924977652674458 Test RE 0.4002424425243057\n",
      "46 Train Loss 0.5924451 Test MSE 0.024965331799741187 Test RE 0.36976903103464676\n",
      "47 Train Loss 0.55954003 Test MSE 0.026812859472145022 Test RE 0.3832069967929459\n",
      "48 Train Loss 0.516999 Test MSE 0.022408550644966822 Test RE 0.35032307798486384\n",
      "49 Train Loss 0.46217155 Test MSE 0.029947210341981653 Test RE 0.40498603751201984\n",
      "50 Train Loss 0.3728985 Test MSE 0.03913384205195139 Test RE 0.46295428475190664\n",
      "51 Train Loss 0.33605322 Test MSE 0.04874032057105568 Test RE 0.5166614455010957\n",
      "52 Train Loss 0.29240832 Test MSE 0.06644324657272786 Test RE 0.6032360486042119\n",
      "53 Train Loss 0.26713708 Test MSE 0.056777347242973716 Test RE 0.5576342146119895\n",
      "54 Train Loss 0.25598842 Test MSE 0.05367329676113802 Test RE 0.5421768872584373\n",
      "55 Train Loss 0.24082252 Test MSE 0.04807180976548489 Test RE 0.5131060084178208\n",
      "56 Train Loss 0.22355843 Test MSE 0.05574155803804443 Test RE 0.5525243417103238\n",
      "57 Train Loss 0.20483777 Test MSE 0.06051614050371176 Test RE 0.5757016515018392\n",
      "58 Train Loss 0.19271128 Test MSE 0.05591530478648729 Test RE 0.5533847824653931\n",
      "59 Train Loss 0.18185592 Test MSE 0.04889752411064039 Test RE 0.5174939761589361\n",
      "60 Train Loss 0.16674325 Test MSE 0.04159612130316997 Test RE 0.47729653554857654\n",
      "61 Train Loss 0.1583406 Test MSE 0.03948432734784719 Test RE 0.4650227883659776\n",
      "62 Train Loss 0.15329704 Test MSE 0.0364501944661992 Test RE 0.4467985848265673\n",
      "63 Train Loss 0.14624923 Test MSE 0.03869231528703175 Test RE 0.46033524038885226\n",
      "64 Train Loss 0.1370542 Test MSE 0.031688384144272344 Test RE 0.41659294522637547\n",
      "65 Train Loss 0.12635122 Test MSE 0.02812463776741897 Test RE 0.39246897659343066\n",
      "66 Train Loss 0.12133526 Test MSE 0.028518094432115084 Test RE 0.39520471328345586\n",
      "67 Train Loss 0.11816623 Test MSE 0.02654170559706369 Test RE 0.38126441957255947\n",
      "68 Train Loss 0.114186525 Test MSE 0.027227528614283274 Test RE 0.3861588350110004\n",
      "69 Train Loss 0.107736446 Test MSE 0.022905809984231366 Test RE 0.3541886914368173\n",
      "70 Train Loss 0.102461115 Test MSE 0.020121786641896514 Test RE 0.3319671682907217\n",
      "71 Train Loss 0.09776073 Test MSE 0.014913011803156106 Test RE 0.28578836565319893\n",
      "72 Train Loss 0.09504446 Test MSE 0.013362665220826526 Test RE 0.27052562436667676\n",
      "73 Train Loss 0.09082106 Test MSE 0.012270176258871987 Test RE 0.2592311966974455\n",
      "74 Train Loss 0.08674133 Test MSE 0.013663628564506085 Test RE 0.27355514528175856\n",
      "75 Train Loss 0.0848853 Test MSE 0.01347819938739436 Test RE 0.2716925953491653\n",
      "76 Train Loss 0.079742454 Test MSE 0.010493933890920992 Test RE 0.23973476965760254\n",
      "77 Train Loss 0.07581881 Test MSE 0.008282796325851122 Test RE 0.2129856556317352\n",
      "78 Train Loss 0.07375559 Test MSE 0.007049396387951519 Test RE 0.19648881336646257\n",
      "79 Train Loss 0.07131401 Test MSE 0.0062380013265681215 Test RE 0.1848351632555857\n",
      "80 Train Loss 0.06531576 Test MSE 0.005865435522456158 Test RE 0.17923053310459813\n",
      "81 Train Loss 0.062755406 Test MSE 0.007733335056275133 Test RE 0.20579995596724796\n",
      "82 Train Loss 0.06163428 Test MSE 0.007427031810481314 Test RE 0.2016830990476139\n",
      "83 Train Loss 0.058042776 Test MSE 0.009066666386542776 Test RE 0.22283616824524113\n",
      "84 Train Loss 0.0512503 Test MSE 0.007807137939166734 Test RE 0.20677964737843568\n",
      "85 Train Loss 0.048949517 Test MSE 0.006528575063376045 Test RE 0.18909108985129378\n",
      "86 Train Loss 0.047225885 Test MSE 0.005390379030961221 Test RE 0.17181913018539108\n",
      "87 Train Loss 0.043043252 Test MSE 0.003366514770121383 Test RE 0.13578510302819985\n",
      "88 Train Loss 0.040847708 Test MSE 0.002057366440388819 Test RE 0.10614943569249206\n",
      "89 Train Loss 0.038411967 Test MSE 0.0017664842203086717 Test RE 0.09835959802422298\n",
      "90 Train Loss 0.037244238 Test MSE 0.0012280092761352377 Test RE 0.08200922121785338\n",
      "91 Train Loss 0.036644965 Test MSE 0.001167858959439553 Test RE 0.07997551814461475\n",
      "92 Train Loss 0.035951477 Test MSE 0.0016169040870473918 Test RE 0.09410311364958526\n",
      "93 Train Loss 0.034859635 Test MSE 0.0014449718809550615 Test RE 0.0889593420056775\n",
      "94 Train Loss 0.031985544 Test MSE 0.0017725911761222461 Test RE 0.09852947201831877\n",
      "95 Train Loss 0.030222876 Test MSE 0.0024803053643461886 Test RE 0.11655058033313502\n",
      "96 Train Loss 0.029516133 Test MSE 0.002027141809399932 Test RE 0.105366833685411\n",
      "97 Train Loss 0.029105082 Test MSE 0.0014015811422545504 Test RE 0.08761349130155606\n",
      "98 Train Loss 0.028543148 Test MSE 0.001314759585373891 Test RE 0.08485648477445455\n",
      "99 Train Loss 0.02696836 Test MSE 0.0009623426989522928 Test RE 0.07259834860784967\n",
      "100 Train Loss 0.025265647 Test MSE 0.0006812801865366365 Test RE 0.0610836167121709\n",
      "101 Train Loss 0.02445954 Test MSE 0.0005670040731251217 Test RE 0.05572562568107538\n",
      "102 Train Loss 0.023795536 Test MSE 0.0002918912189401162 Test RE 0.03998272400894504\n",
      "103 Train Loss 0.023477674 Test MSE 0.0002780818459211291 Test RE 0.039025473658027554\n",
      "104 Train Loss 0.022855286 Test MSE 0.00033809542687490593 Test RE 0.04303100728102243\n",
      "105 Train Loss 0.022221461 Test MSE 0.000502182470950792 Test RE 0.05244361719750966\n",
      "106 Train Loss 0.021827366 Test MSE 0.00031706079032053057 Test RE 0.041670924236215996\n",
      "107 Train Loss 0.021036992 Test MSE 0.00021610938736027458 Test RE 0.03440318732145141\n",
      "108 Train Loss 0.020507725 Test MSE 0.00022081820684166287 Test RE 0.034775974078917374\n",
      "109 Train Loss 0.020160282 Test MSE 0.0002408572308038733 Test RE 0.03631965449461101\n",
      "110 Train Loss 0.019933665 Test MSE 0.00025053332003200995 Test RE 0.03704201564801532\n",
      "111 Train Loss 0.019714452 Test MSE 0.00024587344331978694 Test RE 0.0366959111593762\n",
      "112 Train Loss 0.01940435 Test MSE 0.00019008563351849878 Test RE 0.03226535971931717\n",
      "113 Train Loss 0.019054884 Test MSE 0.0001701220079676068 Test RE 0.03052404703454331\n",
      "114 Train Loss 0.018419938 Test MSE 0.00020709640232636348 Test RE 0.0336781433557998\n",
      "115 Train Loss 0.01787733 Test MSE 0.00020149414432782652 Test RE 0.03321949903434866\n",
      "116 Train Loss 0.017495895 Test MSE 0.000166914648723089 Test RE 0.030234938561464513\n",
      "117 Train Loss 0.016690586 Test MSE 0.00011151584299915847 Test RE 0.024713266353834112\n",
      "118 Train Loss 0.016227618 Test MSE 0.00010983175671745097 Test RE 0.02452594946388764\n",
      "119 Train Loss 0.014868575 Test MSE 0.0002354264143387818 Test RE 0.035907854613987086\n",
      "120 Train Loss 0.01387741 Test MSE 0.0003430264895551439 Test RE 0.04334367122885136\n",
      "121 Train Loss 0.013429509 Test MSE 0.0005744790865604075 Test RE 0.056091748179596286\n",
      "122 Train Loss 0.013132824 Test MSE 0.00042817534267321255 Test RE 0.048425341381584365\n",
      "123 Train Loss 0.012981664 Test MSE 0.00045392297244899703 Test RE 0.04986007704243898\n",
      "124 Train Loss 0.012737661 Test MSE 0.00044924616709735574 Test RE 0.04960255579229628\n",
      "125 Train Loss 0.01247953 Test MSE 0.0003777938709745572 Test RE 0.04548721191092925\n",
      "126 Train Loss 0.012322105 Test MSE 0.0003652237726415407 Test RE 0.04472407665677433\n",
      "127 Train Loss 0.011997711 Test MSE 0.00021427670169722046 Test RE 0.034257001005964266\n",
      "128 Train Loss 0.011565339 Test MSE 0.0002135753840108706 Test RE 0.03420089427354094\n",
      "129 Train Loss 0.010914618 Test MSE 0.00021652265181660893 Test RE 0.03443606609665967\n",
      "130 Train Loss 0.010750758 Test MSE 0.00030155792683251816 Test RE 0.04063939542499268\n",
      "131 Train Loss 0.010717983 Test MSE 0.0003228471414133986 Test RE 0.042049451652057084\n",
      "132 Train Loss 0.010666021 Test MSE 0.0003401727254256192 Test RE 0.04316299866443886\n",
      "133 Train Loss 0.0105388975 Test MSE 0.00037478432530017983 Test RE 0.04530567169099325\n",
      "134 Train Loss 0.010216591 Test MSE 0.0003359972572013697 Test RE 0.04289727748234194\n",
      "135 Train Loss 0.009835312 Test MSE 0.0004472497229858268 Test RE 0.04949221651348633\n",
      "136 Train Loss 0.009586845 Test MSE 0.0005796447503156962 Test RE 0.056343369757729134\n",
      "137 Train Loss 0.009373949 Test MSE 0.0005637364698533502 Test RE 0.0555648223060538\n",
      "138 Train Loss 0.0089612715 Test MSE 0.0004676873354151226 Test RE 0.05061038824542435\n",
      "139 Train Loss 0.008811346 Test MSE 0.00045515313034163405 Test RE 0.04992759319755072\n",
      "140 Train Loss 0.008766893 Test MSE 0.0003907969182414224 Test RE 0.04626338746309832\n",
      "141 Train Loss 0.008719813 Test MSE 0.00037227647468981534 Test RE 0.045153836945360216\n",
      "142 Train Loss 0.008627368 Test MSE 0.0004047871081913648 Test RE 0.04708420048677766\n",
      "143 Train Loss 0.008468769 Test MSE 0.00036536155866977535 Test RE 0.044732512268378315\n",
      "144 Train Loss 0.008244476 Test MSE 0.0004131505498591912 Test RE 0.047568124833606396\n",
      "145 Train Loss 0.008066989 Test MSE 0.00044850467316496123 Test RE 0.049561603652915176\n",
      "146 Train Loss 0.0079167085 Test MSE 0.0005144257492104285 Test RE 0.05307905881049192\n",
      "147 Train Loss 0.007840512 Test MSE 0.0004774000796089124 Test RE 0.051133215911680914\n",
      "148 Train Loss 0.007803604 Test MSE 0.00046352341587170423 Test RE 0.05038458702221498\n",
      "149 Train Loss 0.007687204 Test MSE 0.0005219154024260226 Test RE 0.05346405819866403\n",
      "150 Train Loss 0.0072269416 Test MSE 0.0005582654593984786 Test RE 0.05529453920081044\n",
      "151 Train Loss 0.006966915 Test MSE 0.0005692861034916961 Test RE 0.05583765298345782\n",
      "152 Train Loss 0.006817848 Test MSE 0.0006308138616362092 Test RE 0.058777685132245634\n",
      "153 Train Loss 0.0066972165 Test MSE 0.0006600112768507643 Test RE 0.06012257073638049\n",
      "154 Train Loss 0.0066253683 Test MSE 0.0005975127768852143 Test RE 0.057205193933316426\n",
      "155 Train Loss 0.0065480405 Test MSE 0.0005755721373971647 Test RE 0.05614508519180115\n",
      "156 Train Loss 0.0064693964 Test MSE 0.0005859516939586792 Test RE 0.05664906825154817\n",
      "157 Train Loss 0.0064100646 Test MSE 0.00046568096186469775 Test RE 0.050501712559936865\n",
      "158 Train Loss 0.006307875 Test MSE 0.00042295669388095876 Test RE 0.04812932993266003\n",
      "159 Train Loss 0.006269413 Test MSE 0.00045925085838755274 Test RE 0.05015183781769639\n",
      "160 Train Loss 0.0062434627 Test MSE 0.0004625611249353158 Test RE 0.05033225976120801\n",
      "161 Train Loss 0.0061634923 Test MSE 0.00043189812632910246 Test RE 0.048635403580024716\n",
      "162 Train Loss 0.0060609966 Test MSE 0.0003843011645578854 Test RE 0.04587728560112713\n",
      "163 Train Loss 0.0059730504 Test MSE 0.0003756110109964341 Test RE 0.04535561097279421\n",
      "164 Train Loss 0.0058684438 Test MSE 0.0003876788258106438 Test RE 0.04607845456332675\n",
      "165 Train Loss 0.0057806754 Test MSE 0.0004251543789751695 Test RE 0.04825420804538565\n",
      "166 Train Loss 0.0056984494 Test MSE 0.0004951095985939594 Test RE 0.05207299260392146\n",
      "167 Train Loss 0.005617128 Test MSE 0.0005368141943844017 Test RE 0.054221791102217566\n",
      "168 Train Loss 0.0055335187 Test MSE 0.0006504562746636204 Test RE 0.05968578604644138\n",
      "169 Train Loss 0.0054495875 Test MSE 0.0006896751717606431 Test RE 0.06145881183231477\n",
      "170 Train Loss 0.005294495 Test MSE 0.0008031802858863338 Test RE 0.06632364470312624\n",
      "171 Train Loss 0.0051078247 Test MSE 0.0007793845594683167 Test RE 0.06533377655265944\n",
      "172 Train Loss 0.004947566 Test MSE 0.0007516877058933712 Test RE 0.06416239811990045\n",
      "173 Train Loss 0.004853612 Test MSE 0.0007764940902977837 Test RE 0.06521251376872092\n",
      "174 Train Loss 0.0048048175 Test MSE 0.0007455580341888353 Test RE 0.06390025500955755\n",
      "175 Train Loss 0.004785524 Test MSE 0.0007026120268669669 Test RE 0.062032552787858936\n",
      "176 Train Loss 0.004751296 Test MSE 0.0007001289853253655 Test RE 0.06192284378333851\n",
      "177 Train Loss 0.0047107777 Test MSE 0.0006752867474146646 Test RE 0.06081433713869954\n",
      "178 Train Loss 0.00464593 Test MSE 0.0005823059034585592 Test RE 0.05647255805568857\n",
      "179 Train Loss 0.0045338185 Test MSE 0.0005871817950165539 Test RE 0.05670849938342893\n",
      "180 Train Loss 0.0044747866 Test MSE 0.0005712939930826612 Test RE 0.055936036868531304\n",
      "181 Train Loss 0.004404717 Test MSE 0.000574352152718863 Test RE 0.056085550970175195\n",
      "182 Train Loss 0.004346326 Test MSE 0.0005620852916698552 Test RE 0.05548338824275664\n",
      "183 Train Loss 0.0042056493 Test MSE 0.00044201844645087103 Test RE 0.04920192118345229\n",
      "184 Train Loss 0.0040588127 Test MSE 0.0005157300276028499 Test RE 0.053146304709125534\n",
      "185 Train Loss 0.0037981404 Test MSE 0.00041735633253573465 Test RE 0.04780962835540105\n",
      "186 Train Loss 0.0036703465 Test MSE 0.0003432370192302343 Test RE 0.043356970098778605\n",
      "187 Train Loss 0.003561918 Test MSE 0.0003656890381051554 Test RE 0.04475255501296236\n",
      "188 Train Loss 0.0034768893 Test MSE 0.00040276999582478676 Test RE 0.04696674030302457\n",
      "189 Train Loss 0.0034477634 Test MSE 0.00038328912936726694 Test RE 0.04581683817162362\n",
      "190 Train Loss 0.003435084 Test MSE 0.0003678655396882593 Test RE 0.044885536172553316\n",
      "191 Train Loss 0.003416439 Test MSE 0.0003670155801288409 Test RE 0.04483365178954364\n",
      "192 Train Loss 0.003402386 Test MSE 0.00036200712138073713 Test RE 0.04452669092919778\n",
      "193 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "194 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "195 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "196 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "197 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "198 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "199 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "200 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "201 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "202 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "203 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "204 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "205 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "206 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "207 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "208 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "209 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "210 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "211 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "212 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "213 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "214 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "215 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "216 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "217 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "218 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "219 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "220 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "221 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "222 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "223 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "224 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "225 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "226 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "227 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "228 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "229 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "230 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "231 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "232 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "233 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "234 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "235 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "236 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "237 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "238 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "239 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "240 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "241 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "242 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "243 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "244 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "245 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "246 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "247 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "248 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "249 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "250 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "251 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "252 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "253 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "254 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "255 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "256 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "257 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "258 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "259 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "260 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "261 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "262 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "263 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "264 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "265 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "266 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "267 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "268 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "269 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "270 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "271 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "272 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "273 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "274 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "275 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "276 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "277 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "278 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "279 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "280 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "281 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "282 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "283 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "284 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "285 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "286 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "287 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "288 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "289 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "290 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "291 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "292 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "293 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "294 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "295 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "296 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "297 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "298 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "299 Train Loss 0.003399201 Test MSE 0.00035369842782438513 Test RE 0.044012742276270035\n",
      "Training time: 210.01\n",
      "KG_swish_low\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 7546.8906 Test MSE 6.596515946644189 Test RE 6.010618619458159\n",
      "1 Train Loss 4799.2812 Test MSE 6.6221133212250285 Test RE 6.022269246028124\n",
      "2 Train Loss 1059.0309 Test MSE 15.185338394168005 Test RE 9.119564539261923\n",
      "3 Train Loss 258.56552 Test MSE 15.633071603243941 Test RE 9.253031115184422\n",
      "4 Train Loss 141.84895 Test MSE 15.895498082654639 Test RE 9.330371467638624\n",
      "5 Train Loss 90.05572 Test MSE 15.525391353924789 Test RE 9.221108713794562\n",
      "6 Train Loss 63.353706 Test MSE 14.717121291747102 Test RE 8.977869712829593\n",
      "7 Train Loss 42.637966 Test MSE 13.787328949040143 Test RE 8.689642964845579\n",
      "8 Train Loss 32.08832 Test MSE 13.282706812619844 Test RE 8.529138315733611\n",
      "9 Train Loss 25.35379 Test MSE 12.618578477664848 Test RE 8.313178017874817\n",
      "10 Train Loss 20.805195 Test MSE 10.76072368045929 Test RE 7.6768420259708705\n",
      "11 Train Loss 16.23175 Test MSE 9.949792238664239 Test RE 7.3819121115463044\n",
      "12 Train Loss 13.152082 Test MSE 7.683167814927927 Test RE 6.486822637000084\n",
      "13 Train Loss 9.826398 Test MSE 6.2882019553483754 Test RE 5.868472917455524\n",
      "14 Train Loss 7.7878375 Test MSE 5.250385916963203 Test RE 5.362378649306949\n",
      "15 Train Loss 5.9615073 Test MSE 4.76149405637727 Test RE 5.106619359229114\n",
      "16 Train Loss 5.2475443 Test MSE 4.127617782725799 Test RE 4.7545738940747615\n",
      "17 Train Loss 4.3861585 Test MSE 3.642021179651792 Test RE 4.46614786488473\n",
      "18 Train Loss 3.5209851 Test MSE 2.0443042850952 Test RE 3.3460670054876913\n",
      "19 Train Loss 2.6365747 Test MSE 1.4375661037701604 Test RE 2.805923175022743\n",
      "20 Train Loss 1.5857689 Test MSE 0.820568730663664 Test RE 2.119919382786177\n",
      "21 Train Loss 1.2303654 Test MSE 0.7492122587723875 Test RE 2.025649501309308\n",
      "22 Train Loss 0.8875298 Test MSE 0.6196554265588913 Test RE 1.8422009108815023\n",
      "23 Train Loss 0.7733234 Test MSE 0.4018006560839588 Test RE 1.4834304342807916\n",
      "24 Train Loss 0.6566954 Test MSE 0.3045555519279091 Test RE 1.2915021232710882\n",
      "25 Train Loss 0.56153846 Test MSE 0.2665944490963482 Test RE 1.2083351671474116\n",
      "26 Train Loss 0.49697554 Test MSE 0.2052375466077159 Test RE 1.0602060331944407\n",
      "27 Train Loss 0.40506086 Test MSE 0.17020890119911633 Test RE 0.9655016006001954\n",
      "28 Train Loss 0.3396069 Test MSE 0.09936367483110294 Test RE 0.7376930408732889\n",
      "29 Train Loss 0.3148488 Test MSE 0.07881938065891707 Test RE 0.6570196719212047\n",
      "30 Train Loss 0.25310662 Test MSE 0.06504893517050168 Test RE 0.5968730359973047\n",
      "31 Train Loss 0.23925772 Test MSE 0.045004378128927366 Test RE 0.49646569698343646\n",
      "32 Train Loss 0.21141534 Test MSE 0.03555275660428585 Test RE 0.441264006477552\n",
      "33 Train Loss 0.17497493 Test MSE 0.01814326332688932 Test RE 0.31522421071131307\n",
      "34 Train Loss 0.14971022 Test MSE 0.012708377454467268 Test RE 0.2638195144822708\n",
      "35 Train Loss 0.13307476 Test MSE 0.010278664296653828 Test RE 0.23726310254719768\n",
      "36 Train Loss 0.09815411 Test MSE 0.005427812782512188 Test RE 0.17241470116957108\n",
      "37 Train Loss 0.091519535 Test MSE 0.005161281246478664 Test RE 0.1681282244686707\n",
      "38 Train Loss 0.082958795 Test MSE 0.008008859674266585 Test RE 0.20943400904851167\n",
      "39 Train Loss 0.07462195 Test MSE 0.005603948579481574 Test RE 0.1751898476145005\n",
      "40 Train Loss 0.06945579 Test MSE 0.005084706821269859 Test RE 0.16687636170657005\n",
      "41 Train Loss 0.0666386 Test MSE 0.004740248950865745 Test RE 0.1611248178621798\n",
      "42 Train Loss 0.060724933 Test MSE 0.0030759878757172223 Test RE 0.12979386758842565\n",
      "43 Train Loss 0.057700377 Test MSE 0.0028160872770839375 Test RE 0.12418951164004811\n",
      "44 Train Loss 0.05427042 Test MSE 0.00354066024823221 Test RE 0.13925281768427544\n",
      "45 Train Loss 0.045422815 Test MSE 0.0028186281962879834 Test RE 0.12424552630431505\n",
      "46 Train Loss 0.0443018 Test MSE 0.0029322159863315023 Test RE 0.1267242829834976\n",
      "47 Train Loss 0.04152789 Test MSE 0.0028057639806495786 Test RE 0.12396167384335606\n",
      "48 Train Loss 0.035782002 Test MSE 0.0022677275322596393 Test RE 0.1114441562379402\n",
      "49 Train Loss 0.03476361 Test MSE 0.0023448384231748855 Test RE 0.11332306834972994\n",
      "50 Train Loss 0.034337413 Test MSE 0.0026156165850422836 Test RE 0.11968753021128832\n",
      "51 Train Loss 0.031197784 Test MSE 0.002816602237603182 Test RE 0.12420086600618012\n",
      "52 Train Loss 0.028553996 Test MSE 0.002776331591130872 Test RE 0.12330978242490348\n",
      "53 Train Loss 0.0283436 Test MSE 0.00276840683124352 Test RE 0.12313366896141432\n",
      "54 Train Loss 0.024313247 Test MSE 0.0021213014498210744 Test RE 0.10778617450653642\n",
      "55 Train Loss 0.023982162 Test MSE 0.0022141476539072973 Test RE 0.11011973398412823\n",
      "56 Train Loss 0.02323431 Test MSE 0.002376505060703762 Test RE 0.11408570638433027\n",
      "57 Train Loss 0.022677714 Test MSE 0.00237925838103174 Test RE 0.11415177474083545\n",
      "58 Train Loss 0.02245046 Test MSE 0.002459784546386794 Test RE 0.116067438053563\n",
      "59 Train Loss 0.021623977 Test MSE 0.0022273434524846284 Test RE 0.11044739036110109\n",
      "60 Train Loss 0.021148346 Test MSE 0.002317654622983017 Test RE 0.11266427412638524\n",
      "61 Train Loss 0.021045867 Test MSE 0.0023953437382959472 Test RE 0.11453699542689806\n",
      "62 Train Loss 0.020085376 Test MSE 0.0022346272866740225 Test RE 0.11062783488337283\n",
      "63 Train Loss 0.018956464 Test MSE 0.0023685057719841232 Test RE 0.11389353895146621\n",
      "64 Train Loss 0.018700868 Test MSE 0.002324266516024338 Test RE 0.11282486613009753\n",
      "65 Train Loss 0.018635742 Test MSE 0.002340750765967542 Test RE 0.11322424961894574\n",
      "66 Train Loss 0.018316211 Test MSE 0.002317639644331773 Test RE 0.11266391005971636\n",
      "67 Train Loss 0.01753358 Test MSE 0.0024827457681241664 Test RE 0.11660790403032396\n",
      "68 Train Loss 0.016660163 Test MSE 0.0021385497289963663 Test RE 0.10822349146171895\n",
      "69 Train Loss 0.016443595 Test MSE 0.0019922263998060448 Test RE 0.10445547527169295\n",
      "70 Train Loss 0.016362041 Test MSE 0.0019742706316966904 Test RE 0.10398368563177238\n",
      "71 Train Loss 0.015598057 Test MSE 0.0017536694400805027 Test RE 0.09800217877570769\n",
      "72 Train Loss 0.014698991 Test MSE 0.0018306219801407944 Test RE 0.1001293046801595\n",
      "73 Train Loss 0.014476566 Test MSE 0.001848063689497581 Test RE 0.10060517738103048\n",
      "74 Train Loss 0.014391937 Test MSE 0.0018132839303747657 Test RE 0.09965400801934593\n",
      "75 Train Loss 0.014060263 Test MSE 0.0017572774986224908 Test RE 0.09810294346406508\n",
      "76 Train Loss 0.013304102 Test MSE 0.0015221454380021581 Test RE 0.0913040283781218\n",
      "77 Train Loss 0.012656033 Test MSE 0.0014123112543530001 Test RE 0.08794822402552904\n",
      "78 Train Loss 0.012542337 Test MSE 0.001432320225663117 Test RE 0.08856903774765344\n",
      "79 Train Loss 0.012514655 Test MSE 0.0014162369472920122 Test RE 0.08807037066008921\n",
      "80 Train Loss 0.01242219 Test MSE 0.0013266591803426315 Test RE 0.08523962832360843\n",
      "81 Train Loss 0.012088542 Test MSE 0.001219061855709038 Test RE 0.08170991058870215\n",
      "82 Train Loss 0.011686688 Test MSE 0.0013193586591803842 Test RE 0.08500477064725422\n",
      "83 Train Loss 0.011381877 Test MSE 0.0011872370456886213 Test RE 0.08063629849260566\n",
      "84 Train Loss 0.011190055 Test MSE 0.001212198768641722 Test RE 0.081479580301528\n",
      "85 Train Loss 0.010987606 Test MSE 0.0012092604367968682 Test RE 0.08138076841075377\n",
      "86 Train Loss 0.010873918 Test MSE 0.0012448589839763181 Test RE 0.08256993505228341\n",
      "87 Train Loss 0.0108438 Test MSE 0.0012869232036825454 Test RE 0.08395337884863381\n",
      "88 Train Loss 0.010775002 Test MSE 0.0012622961146356389 Test RE 0.08314621553772915\n",
      "89 Train Loss 0.010352502 Test MSE 0.001224193000842863 Test RE 0.0818816923372584\n",
      "90 Train Loss 0.010010108 Test MSE 0.001213982023774659 Test RE 0.08153949022960547\n",
      "91 Train Loss 0.009869825 Test MSE 0.0011751466293318073 Test RE 0.08022466156444552\n",
      "92 Train Loss 0.009757443 Test MSE 0.0011663369598650295 Test RE 0.07992338753932313\n",
      "93 Train Loss 0.0096776085 Test MSE 0.0011705439949100521 Test RE 0.08006740159377419\n",
      "94 Train Loss 0.009597851 Test MSE 0.0012069955420917354 Test RE 0.08130452128805692\n",
      "95 Train Loss 0.00948655 Test MSE 0.0012433855188860568 Test RE 0.08252105403794043\n",
      "96 Train Loss 0.009301767 Test MSE 0.001165437685122419 Test RE 0.0798925701402532\n",
      "97 Train Loss 0.008418473 Test MSE 0.0010076383865513185 Test RE 0.07428723874925203\n",
      "98 Train Loss 0.0075092833 Test MSE 0.0008560026107939508 Test RE 0.06846985508221953\n",
      "99 Train Loss 0.0070965053 Test MSE 0.0009165434276406548 Test RE 0.0708497607119177\n",
      "100 Train Loss 0.0069391173 Test MSE 0.0008100202183228225 Test RE 0.0666054541058451\n",
      "101 Train Loss 0.0068489183 Test MSE 0.0008081618377995696 Test RE 0.06652900579525571\n",
      "102 Train Loss 0.006790789 Test MSE 0.0008555389268610911 Test RE 0.06845130801714233\n",
      "103 Train Loss 0.0067633516 Test MSE 0.000880071752945511 Test RE 0.06942580194329319\n",
      "104 Train Loss 0.006711232 Test MSE 0.0008330935344096199 Test RE 0.06754741694949011\n",
      "105 Train Loss 0.006625617 Test MSE 0.0007711314158351004 Test RE 0.06498693614731862\n",
      "106 Train Loss 0.006441397 Test MSE 0.0007443663275995176 Test RE 0.06384916521798158\n",
      "107 Train Loss 0.0062900432 Test MSE 0.0007051325904054743 Test RE 0.06214372154547591\n",
      "108 Train Loss 0.0060736346 Test MSE 0.0007371236792845451 Test RE 0.06353778128630873\n",
      "109 Train Loss 0.0059016817 Test MSE 0.0007545884848270786 Test RE 0.06428608094443732\n",
      "110 Train Loss 0.00581566 Test MSE 0.00079026945168161 Test RE 0.06578842073957165\n",
      "111 Train Loss 0.005743134 Test MSE 0.0007701255703786155 Test RE 0.06494453861120858\n",
      "112 Train Loss 0.005698835 Test MSE 0.000757372587905112 Test RE 0.06440456558511062\n",
      "113 Train Loss 0.0056562745 Test MSE 0.0007489641088321148 Test RE 0.06404605252808993\n",
      "114 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "115 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "116 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "117 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "118 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "119 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "120 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "121 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "122 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "123 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "124 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "125 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "126 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "127 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "128 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "129 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "130 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "131 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "132 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "133 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "134 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "135 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "136 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "137 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "138 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "139 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "140 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "141 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "142 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "143 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "144 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "145 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "146 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "147 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "148 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "149 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "150 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "151 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "152 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "153 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "154 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "155 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "156 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "157 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "158 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "159 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "160 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "161 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "162 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "163 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "164 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "165 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "166 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "167 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "168 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "169 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "170 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "171 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "172 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "173 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "174 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "175 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "176 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "177 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "178 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "179 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "180 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "181 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "182 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "183 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "184 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "185 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "186 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "187 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "188 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "189 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "190 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "191 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "192 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "193 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "194 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "195 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "196 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "197 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "198 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "199 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "200 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "201 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "202 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "203 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "204 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "205 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "206 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "207 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "208 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "209 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "210 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "211 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "212 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "213 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "214 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "215 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "216 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "217 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "218 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "219 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "220 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "221 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "222 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "223 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "224 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "225 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "226 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "227 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "228 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "229 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "230 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "231 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "232 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "233 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "234 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "235 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "236 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "237 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "238 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "239 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "240 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "241 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "242 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "243 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "244 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "245 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "246 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "247 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "248 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "249 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "250 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "251 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "252 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "253 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "254 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "255 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "256 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "257 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "258 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "259 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "260 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "261 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "262 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "263 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "264 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "265 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "266 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "267 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "268 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "269 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "270 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "271 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "272 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "273 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "274 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "275 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "276 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "277 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "278 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "279 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "280 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "281 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "282 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "283 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "284 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "285 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "286 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "287 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "288 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "289 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "290 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "291 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "292 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "293 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "294 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "295 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "296 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "297 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "298 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "299 Train Loss 0.005634557 Test MSE 0.0007432772213660869 Test RE 0.06380243823968142\n",
      "Training time: 158.06\n",
      "KG_swish_low\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 12108.203 Test MSE 1.3487827762466695 Test RE 2.7178962207787523\n",
      "1 Train Loss 9749.802 Test MSE 4.837276454952657 Test RE 5.147096589986711\n",
      "2 Train Loss 5431.3237 Test MSE 7.20058324163404 Test RE 6.279798398484077\n",
      "3 Train Loss 3570.16 Test MSE 11.95315591300812 Test RE 8.091017792532716\n",
      "4 Train Loss 768.6966 Test MSE 14.218615921673647 Test RE 8.824508495980279\n",
      "5 Train Loss 204.07379 Test MSE 14.892453201933563 Test RE 9.031190143783972\n",
      "6 Train Loss 120.97861 Test MSE 15.017265140085135 Test RE 9.068955864496866\n",
      "7 Train Loss 76.38252 Test MSE 15.287690426466765 Test RE 9.150246713922323\n",
      "8 Train Loss 55.508934 Test MSE 14.970072280947514 Test RE 9.054694721422686\n",
      "9 Train Loss 45.0975 Test MSE 14.944699428879044 Test RE 9.04701804303756\n",
      "10 Train Loss 37.840923 Test MSE 14.96298319732038 Test RE 9.052550540421665\n",
      "11 Train Loss 34.239407 Test MSE 15.174769165965323 Test RE 9.116390308509647\n",
      "12 Train Loss 31.617008 Test MSE 15.02121689974915 Test RE 9.070149023712661\n",
      "13 Train Loss 28.848995 Test MSE 15.029508158368971 Test RE 9.072651902713767\n",
      "14 Train Loss 27.312918 Test MSE 14.98653551821411 Test RE 9.059672273391968\n",
      "15 Train Loss 25.918995 Test MSE 14.896644318155689 Test RE 9.032460858028804\n",
      "16 Train Loss 25.081205 Test MSE 14.692309065897497 Test RE 8.970298432538861\n",
      "17 Train Loss 23.800528 Test MSE 14.693674901481902 Test RE 8.970715374083476\n",
      "18 Train Loss 22.929272 Test MSE 14.726661558335367 Test RE 8.980779160694828\n",
      "19 Train Loss 22.067787 Test MSE 14.795876997360155 Test RE 9.001859291106232\n",
      "20 Train Loss 21.813675 Test MSE 14.957170349100286 Test RE 9.050791993573695\n",
      "21 Train Loss 21.041067 Test MSE 15.140400681267645 Test RE 9.106060855658434\n",
      "22 Train Loss 20.391296 Test MSE 15.292379932380411 Test RE 9.151650027461123\n",
      "23 Train Loss 20.192354 Test MSE 15.224988282114058 Test RE 9.13146266022936\n",
      "24 Train Loss 19.849628 Test MSE 15.219304402683965 Test RE 9.129757996264312\n",
      "25 Train Loss 19.627367 Test MSE 15.153501481019715 Test RE 9.109999684193687\n",
      "26 Train Loss 19.28879 Test MSE 15.244364077166932 Test RE 9.137271304876517\n",
      "27 Train Loss 19.07626 Test MSE 15.112899435327929 Test RE 9.097786905447707\n",
      "28 Train Loss 18.326412 Test MSE 14.797865510611746 Test RE 9.002464179721423\n",
      "29 Train Loss 17.87637 Test MSE 14.463823209518146 Test RE 8.900274811354498\n",
      "30 Train Loss 17.595587 Test MSE 14.175615231479679 Test RE 8.811154619330035\n",
      "31 Train Loss 17.347338 Test MSE 14.10666149189194 Test RE 8.78969866531867\n",
      "32 Train Loss 16.731934 Test MSE 13.943023406533664 Test RE 8.738569452102244\n",
      "33 Train Loss 16.27413 Test MSE 13.804516848683049 Test RE 8.695057726184448\n",
      "34 Train Loss 15.586035 Test MSE 13.237344053316455 Test RE 8.514561612488967\n",
      "35 Train Loss 15.251005 Test MSE 12.849402509625737 Test RE 8.388867422667316\n",
      "36 Train Loss 14.713369 Test MSE 12.797154724835854 Test RE 8.371794792353805\n",
      "37 Train Loss 14.293516 Test MSE 12.64507250884405 Test RE 8.32190063706323\n",
      "38 Train Loss 13.926488 Test MSE 12.410819137347755 Test RE 8.244457572488388\n",
      "39 Train Loss 13.694074 Test MSE 12.330121284183361 Test RE 8.217610228756278\n",
      "40 Train Loss 13.470813 Test MSE 12.219252227938542 Test RE 8.180581558413639\n",
      "41 Train Loss 13.093555 Test MSE 12.141988848459276 Test RE 8.154677285456692\n",
      "42 Train Loss 12.698675 Test MSE 11.724286251038015 Test RE 8.01318317878157\n",
      "43 Train Loss 12.03863 Test MSE 11.531043512554703 Test RE 7.946871114594719\n",
      "44 Train Loss 11.706271 Test MSE 11.268082119624909 Test RE 7.855735739785204\n",
      "45 Train Loss 11.18604 Test MSE 10.853557905732655 Test RE 7.709885491017901\n",
      "46 Train Loss 10.462504 Test MSE 10.135064779393335 Test RE 7.450323463128428\n",
      "47 Train Loss 9.910133 Test MSE 10.182290683430018 Test RE 7.467661257600211\n",
      "48 Train Loss 9.143672 Test MSE 9.428747556519752 Test RE 7.1860273886016195\n",
      "49 Train Loss 8.518501 Test MSE 8.90663558583073 Test RE 6.984232782018176\n",
      "50 Train Loss 7.214286 Test MSE 6.526092495331597 Test RE 5.978448279546932\n",
      "51 Train Loss 5.970717 Test MSE 5.836529037507693 Test RE 5.653783698535308\n",
      "52 Train Loss 5.4301314 Test MSE 5.450621528233661 Test RE 5.463675247563699\n",
      "53 Train Loss 4.947954 Test MSE 5.568373073479616 Test RE 5.5223766795450375\n",
      "54 Train Loss 4.0747824 Test MSE 4.62786669195058 Test RE 5.034452924103497\n",
      "55 Train Loss 3.6603172 Test MSE 3.8905335099893845 Test RE 4.616006845564684\n",
      "56 Train Loss 3.2524126 Test MSE 3.610916639375429 Test RE 4.447035489163308\n",
      "57 Train Loss 2.8148975 Test MSE 2.9362343217645575 Test RE 4.0101186213588935\n",
      "58 Train Loss 2.4734573 Test MSE 2.523475368871146 Test RE 3.717589210748264\n",
      "59 Train Loss 2.1126184 Test MSE 2.264827102192292 Test RE 3.521919221042086\n",
      "60 Train Loss 1.7979763 Test MSE 2.0435176017777286 Test RE 3.3454231315794987\n",
      "61 Train Loss 1.4155767 Test MSE 1.3612026340717054 Test RE 2.7303810082684876\n",
      "62 Train Loss 1.1515453 Test MSE 1.152339254149538 Test RE 2.5121874527583863\n",
      "63 Train Loss 0.9312795 Test MSE 0.9831204359556343 Test RE 2.3204126581842694\n",
      "64 Train Loss 0.7234221 Test MSE 0.6251292680326584 Test RE 1.8503197327320762\n",
      "65 Train Loss 0.6067566 Test MSE 0.3997151174663707 Test RE 1.4795755669762212\n",
      "66 Train Loss 0.5385487 Test MSE 0.2548614621446461 Test RE 1.181446198460637\n",
      "67 Train Loss 0.43668187 Test MSE 0.20399644118176258 Test RE 1.0569955515444005\n",
      "68 Train Loss 0.36573356 Test MSE 0.11208049401003577 Test RE 0.7834781375860851\n",
      "69 Train Loss 0.31742492 Test MSE 0.0754374993859359 Test RE 0.6427698627310886\n",
      "70 Train Loss 0.28556964 Test MSE 0.06275506921773856 Test RE 0.5862546096016499\n",
      "71 Train Loss 0.22809608 Test MSE 0.05053566262833197 Test RE 0.5260909686019866\n",
      "72 Train Loss 0.20769829 Test MSE 0.03594931879430745 Test RE 0.44371815315961405\n",
      "73 Train Loss 0.18387543 Test MSE 0.023377583884650416 Test RE 0.35781758156463706\n",
      "74 Train Loss 0.16402215 Test MSE 0.01859820643771431 Test RE 0.3191518719270407\n",
      "75 Train Loss 0.14601626 Test MSE 0.019155592019145157 Test RE 0.32389903448343965\n",
      "76 Train Loss 0.13772258 Test MSE 0.017867631621051212 Test RE 0.3128206103409744\n",
      "77 Train Loss 0.12264487 Test MSE 0.013437122398013718 Test RE 0.27127826589276494\n",
      "78 Train Loss 0.11645593 Test MSE 0.013431636908082117 Test RE 0.27122288773753955\n",
      "79 Train Loss 0.11167372 Test MSE 0.013381999366132774 Test RE 0.2707212624029764\n",
      "80 Train Loss 0.10194178 Test MSE 0.011525027177138436 Test RE 0.25123656279579604\n",
      "81 Train Loss 0.0969141 Test MSE 0.011952982372773239 Test RE 0.2558585907895695\n",
      "82 Train Loss 0.09356892 Test MSE 0.011529636506745277 Test RE 0.2512867976606023\n",
      "83 Train Loss 0.08770646 Test MSE 0.013953446931520995 Test RE 0.27644110261738275\n",
      "84 Train Loss 0.08194887 Test MSE 0.012614126533562154 Test RE 0.2628393930263734\n",
      "85 Train Loss 0.07915671 Test MSE 0.012355049829213639 Test RE 0.26012621078191295\n",
      "86 Train Loss 0.074266374 Test MSE 0.010988614476477039 Test RE 0.2453202126608565\n",
      "87 Train Loss 0.06650106 Test MSE 0.010647792122210948 Test RE 0.24148582655067818\n",
      "88 Train Loss 0.062378217 Test MSE 0.00935849005061262 Test RE 0.22639391801806436\n",
      "89 Train Loss 0.057278894 Test MSE 0.007772526080596022 Test RE 0.20632077384371092\n",
      "90 Train Loss 0.054732986 Test MSE 0.0067238495728393285 Test RE 0.19189818114921312\n",
      "91 Train Loss 0.05228913 Test MSE 0.005950059270594431 Test RE 0.18051882997409743\n",
      "92 Train Loss 0.049549103 Test MSE 0.0049756827852401805 Test RE 0.16507762292889977\n",
      "93 Train Loss 0.045625724 Test MSE 0.00452753604854174 Test RE 0.15746818525471046\n",
      "94 Train Loss 0.042844646 Test MSE 0.004235995200995957 Test RE 0.1523139194176104\n",
      "95 Train Loss 0.041108035 Test MSE 0.0044177590322658855 Test RE 0.1555474428288042\n",
      "96 Train Loss 0.034620102 Test MSE 0.002250353302321073 Test RE 0.11101641981276479\n",
      "97 Train Loss 0.031633336 Test MSE 0.0014838598005964174 Test RE 0.09014845723458277\n",
      "98 Train Loss 0.030578207 Test MSE 0.001470112989953304 Test RE 0.08972990781208551\n",
      "99 Train Loss 0.029216446 Test MSE 0.002006510161908464 Test RE 0.10482926621150547\n",
      "100 Train Loss 0.02802185 Test MSE 0.0019066341092306462 Test RE 0.1021869750079556\n",
      "101 Train Loss 0.026599377 Test MSE 0.0016573504011578924 Test RE 0.0952728227937498\n",
      "102 Train Loss 0.026009427 Test MSE 0.0018653698852466745 Test RE 0.10107513838955585\n",
      "103 Train Loss 0.025016047 Test MSE 0.0011940768604934658 Test RE 0.08086824259428174\n",
      "104 Train Loss 0.024402551 Test MSE 0.0013934076848687498 Test RE 0.0873576544539821\n",
      "105 Train Loss 0.020261439 Test MSE 0.0019169540430753328 Test RE 0.10246315271611649\n",
      "106 Train Loss 0.019197349 Test MSE 0.0016237767959180194 Test RE 0.09430289591076738\n",
      "107 Train Loss 0.018725831 Test MSE 0.001492619221427398 Test RE 0.09041414485253005\n",
      "108 Train Loss 0.018176617 Test MSE 0.0011953684726202839 Test RE 0.08091196765754403\n",
      "109 Train Loss 0.017550914 Test MSE 0.001009489528382993 Test RE 0.0743554443262287\n",
      "110 Train Loss 0.016818987 Test MSE 0.0015859223360283241 Test RE 0.09319719081995555\n",
      "111 Train Loss 0.016397232 Test MSE 0.001280060217628921 Test RE 0.08372922363804322\n",
      "112 Train Loss 0.016093455 Test MSE 0.0012412073775094749 Test RE 0.08244874287378556\n",
      "113 Train Loss 0.015608455 Test MSE 0.0008341992489787829 Test RE 0.067592227882543\n",
      "114 Train Loss 0.014914804 Test MSE 0.000746177350749948 Test RE 0.06392678967848879\n",
      "115 Train Loss 0.013871674 Test MSE 0.0009324111805397401 Test RE 0.07146042590792609\n",
      "116 Train Loss 0.012784897 Test MSE 0.0015328206211216706 Test RE 0.09162363786939698\n",
      "117 Train Loss 0.012182044 Test MSE 0.0016225727869219992 Test RE 0.09426792725297342\n",
      "118 Train Loss 0.011890337 Test MSE 0.0014842146357860405 Test RE 0.09015923518436636\n",
      "119 Train Loss 0.011701999 Test MSE 0.0016639065517455717 Test RE 0.09546107703082554\n",
      "120 Train Loss 0.011504334 Test MSE 0.0017512050857281362 Test RE 0.0979332954968728\n",
      "121 Train Loss 0.010927487 Test MSE 0.001461760929210603 Test RE 0.08947465631035964\n",
      "122 Train Loss 0.010691079 Test MSE 0.0012032732534302348 Test RE 0.08117905579259216\n",
      "123 Train Loss 0.010477347 Test MSE 0.0010650510961452539 Test RE 0.07637427240602195\n",
      "124 Train Loss 0.01009792 Test MSE 0.0008071222006579853 Test RE 0.06648619983692275\n",
      "125 Train Loss 0.009916069 Test MSE 0.0008035266936178105 Test RE 0.06633794569319584\n",
      "126 Train Loss 0.009827377 Test MSE 0.0006699601483753654 Test RE 0.06057401337078059\n",
      "127 Train Loss 0.009663306 Test MSE 0.0005480197551356664 Test RE 0.054784786207506014\n",
      "128 Train Loss 0.0092852935 Test MSE 0.000513137378243086 Test RE 0.05301254931931801\n",
      "129 Train Loss 0.008643951 Test MSE 0.0002336219958625006 Test RE 0.03576998260596875\n",
      "130 Train Loss 0.008007037 Test MSE 0.00012158835394782017 Test RE 0.0258052370895609\n",
      "131 Train Loss 0.007194174 Test MSE 0.0002971937566580907 Test RE 0.04034425543821177\n",
      "132 Train Loss 0.006905071 Test MSE 0.00020398526501003874 Test RE 0.03342421856636051\n",
      "133 Train Loss 0.0067133876 Test MSE 0.00016687760078810607 Test RE 0.03023158294166286\n",
      "134 Train Loss 0.0066155717 Test MSE 0.00019984882794320488 Test RE 0.03308359280297663\n",
      "135 Train Loss 0.006564326 Test MSE 0.00020203072891517368 Test RE 0.03326370185683991\n",
      "136 Train Loss 0.006443516 Test MSE 0.00015422471458146047 Test RE 0.029062893579203778\n",
      "137 Train Loss 0.005952812 Test MSE 6.577268913062987e-05 Test RE 0.018979495462347624\n",
      "138 Train Loss 0.0057011344 Test MSE 7.502206876025734e-05 Test RE 0.02027012290309532\n",
      "139 Train Loss 0.005614262 Test MSE 9.707206275090772e-05 Test RE 0.023057328827317893\n",
      "140 Train Loss 0.005569548 Test MSE 8.616159898672752e-05 Test RE 0.021722946821830968\n",
      "141 Train Loss 0.005513575 Test MSE 9.548037745467582e-05 Test RE 0.02286751263087194\n",
      "142 Train Loss 0.0053911307 Test MSE 7.186074039134423e-05 Test RE 0.019838448654669112\n",
      "143 Train Loss 0.0051642717 Test MSE 5.402521211981139e-05 Test RE 0.017201253824001117\n",
      "144 Train Loss 0.0050053485 Test MSE 5.7839289832203295e-05 Test RE 0.017798087600762933\n",
      "145 Train Loss 0.004872571 Test MSE 0.0001092010734471056 Test RE 0.024455430808980914\n",
      "146 Train Loss 0.0047657625 Test MSE 0.00015440122896975603 Test RE 0.029079520459901708\n",
      "147 Train Loss 0.0047328738 Test MSE 0.0001493864500349699 Test RE 0.028603387357951742\n",
      "148 Train Loss 0.0047131777 Test MSE 0.000133479049345055 Test RE 0.027037617293297882\n",
      "149 Train Loss 0.004605835 Test MSE 0.00019155500675325528 Test RE 0.032389826222304116\n",
      "150 Train Loss 0.0043770215 Test MSE 0.00017237422432042027 Test RE 0.030725434029739897\n",
      "151 Train Loss 0.004315157 Test MSE 0.00017234183136815326 Test RE 0.030722546897729897\n",
      "152 Train Loss 0.0043007 Test MSE 0.0002100302250784061 Test RE 0.03391585445867506\n",
      "153 Train Loss 0.0041987137 Test MSE 0.0003116739495123381 Test RE 0.04131541469234614\n",
      "154 Train Loss 0.003977059 Test MSE 0.000292765006351734 Test RE 0.04004252418767191\n",
      "155 Train Loss 0.0038334285 Test MSE 0.0002897182764721368 Test RE 0.03983362316292941\n",
      "156 Train Loss 0.003806335 Test MSE 0.00025545040048352986 Test RE 0.03740375106762696\n",
      "157 Train Loss 0.0037840442 Test MSE 0.0002453215929936744 Test RE 0.036654706981481204\n",
      "158 Train Loss 0.003711106 Test MSE 0.00024665916155771603 Test RE 0.03675449749641209\n",
      "159 Train Loss 0.003607263 Test MSE 0.0003781175978565966 Test RE 0.04550669645164867\n",
      "160 Train Loss 0.0035318318 Test MSE 0.00035445708787021607 Test RE 0.044059919221940705\n",
      "161 Train Loss 0.0033611539 Test MSE 0.00029212772814105753 Test RE 0.03999891902740686\n",
      "162 Train Loss 0.0032799134 Test MSE 0.0003378631020675945 Test RE 0.043016220200692214\n",
      "163 Train Loss 0.003232785 Test MSE 0.0002927336857838305 Test RE 0.04004038221695237\n",
      "164 Train Loss 0.0032168522 Test MSE 0.00027642352713038804 Test RE 0.03890893701751254\n",
      "165 Train Loss 0.0032082577 Test MSE 0.0002812267919080208 Test RE 0.03924553107473172\n",
      "166 Train Loss 0.0031968413 Test MSE 0.000285459654272466 Test RE 0.039539778471704416\n",
      "167 Train Loss 0.0031569381 Test MSE 0.0003111535257503922 Test RE 0.041280906664446206\n",
      "168 Train Loss 0.003113119 Test MSE 0.00026272368938708585 Test RE 0.03793250127967132\n",
      "169 Train Loss 0.0030589236 Test MSE 0.00025265415521705156 Test RE 0.03719847078929677\n",
      "170 Train Loss 0.0029980747 Test MSE 0.00023872623039006548 Test RE 0.0361586272456925\n",
      "171 Train Loss 0.002931095 Test MSE 0.00014062541680196535 Test RE 0.02775196731712671\n",
      "172 Train Loss 0.0028717343 Test MSE 9.718797713949717e-05 Test RE 0.02307109117411866\n",
      "173 Train Loss 0.0028102833 Test MSE 8.655302763105048e-05 Test RE 0.021772234140205647\n",
      "174 Train Loss 0.0027785485 Test MSE 6.526189986731324e-05 Test RE 0.018905654648342225\n",
      "175 Train Loss 0.0027452256 Test MSE 5.9477950833470634e-05 Test RE 0.01804844801210038\n",
      "176 Train Loss 0.0027189527 Test MSE 6.785812276070347e-05 Test RE 0.01927803589506262\n",
      "177 Train Loss 0.002688169 Test MSE 8.021890078522706e-05 Test RE 0.020960431423502193\n",
      "178 Train Loss 0.0026476083 Test MSE 8.350700491277262e-05 Test RE 0.021385692492292673\n",
      "179 Train Loss 0.0026300391 Test MSE 9.617042453348337e-05 Test RE 0.022949996869730743\n",
      "180 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "181 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "182 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "183 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "184 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "185 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "186 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "187 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "188 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "189 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "190 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "191 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "192 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "193 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "194 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "195 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "196 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "197 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "198 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "199 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "200 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "201 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "202 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "203 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "204 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "205 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "206 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "207 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "208 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "209 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "210 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "211 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "212 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "213 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "214 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "215 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "216 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "217 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "218 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "219 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "220 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "221 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "222 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "223 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "224 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "225 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "226 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "227 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "228 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "229 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "230 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "231 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "232 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "233 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "234 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "235 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "236 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "237 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "238 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "239 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "240 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "241 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "242 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "243 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "244 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "245 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "246 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "247 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "248 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "249 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "250 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "251 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "252 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "253 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "254 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "255 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "256 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "257 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "258 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "259 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "260 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "261 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "262 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "263 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "264 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "265 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "266 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "267 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "268 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "269 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "270 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "271 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "272 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "273 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "274 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "275 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "276 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "277 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "278 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "279 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "280 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "281 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "282 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "283 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "284 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "285 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "286 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "287 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "288 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "289 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "290 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "291 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "292 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "293 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "294 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "295 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "296 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "297 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "298 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "299 Train Loss 0.002626336 Test MSE 8.90562706026137e-05 Test RE 0.022084832827234363\n",
      "Training time: 183.83\n",
      "KG_swish_low\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 10635.932 Test MSE 4.955257230975539 Test RE 5.209487088821493\n",
      "1 Train Loss 2062.6238 Test MSE 6.856329784939228 Test RE 6.127844179250581\n",
      "2 Train Loss 268.44867 Test MSE 8.354669536026039 Test RE 6.764356720629004\n",
      "3 Train Loss 99.02504 Test MSE 9.002921770506626 Test RE 7.021883216081147\n",
      "4 Train Loss 53.929977 Test MSE 9.801725187355991 Test RE 7.326779557535362\n",
      "5 Train Loss 28.593311 Test MSE 9.37093049535367 Test RE 7.1639611562712116\n",
      "6 Train Loss 18.535757 Test MSE 9.51888080659461 Test RE 7.220292780011131\n",
      "7 Train Loss 15.393704 Test MSE 9.178958031236759 Test RE 7.090201146086408\n",
      "8 Train Loss 12.9918995 Test MSE 8.619733285602893 Test RE 6.870823272784168\n",
      "9 Train Loss 10.854469 Test MSE 7.739035640987926 Test RE 6.510364245705461\n",
      "10 Train Loss 9.726839 Test MSE 7.386672852429625 Test RE 6.360427352395109\n",
      "11 Train Loss 8.512316 Test MSE 6.845768353416019 Test RE 6.123122721050457\n",
      "12 Train Loss 7.075233 Test MSE 5.829114056910675 Test RE 5.650191150659308\n",
      "13 Train Loss 6.0649266 Test MSE 4.713439552306274 Test RE 5.08078520342264\n",
      "14 Train Loss 5.3788333 Test MSE 3.9397748803368002 Test RE 4.64512673422969\n",
      "15 Train Loss 4.4710774 Test MSE 3.316370268929313 Test RE 4.261802998090319\n",
      "16 Train Loss 3.559582 Test MSE 2.6803966645282613 Test RE 3.83143443778675\n",
      "17 Train Loss 2.6710112 Test MSE 1.9766953145867416 Test RE 3.2902714605922996\n",
      "18 Train Loss 1.9209431 Test MSE 1.1590630663627666 Test RE 2.5195060041832424\n",
      "19 Train Loss 1.3464649 Test MSE 1.010417246960083 Test RE 2.3524057883702567\n",
      "20 Train Loss 1.0256124 Test MSE 0.8532261472870067 Test RE 2.1616926308028144\n",
      "21 Train Loss 0.79263234 Test MSE 0.7382389924188784 Test RE 2.0107605410401663\n",
      "22 Train Loss 0.6123477 Test MSE 0.6702670938083466 Test RE 1.915957244403806\n",
      "23 Train Loss 0.5197965 Test MSE 0.5405273686216361 Test RE 1.7205635109810669\n",
      "24 Train Loss 0.44913688 Test MSE 0.5244933675853775 Test RE 1.6948523287081478\n",
      "25 Train Loss 0.37171292 Test MSE 0.39930889792267404 Test RE 1.478823549769113\n",
      "26 Train Loss 0.30580512 Test MSE 0.41704327047587136 Test RE 1.5113060569066752\n",
      "27 Train Loss 0.23723811 Test MSE 0.3372521802433996 Test RE 1.359061929422082\n",
      "28 Train Loss 0.20939925 Test MSE 0.3200372104312284 Test RE 1.3239210950764533\n",
      "29 Train Loss 0.17700976 Test MSE 0.2100999396604269 Test RE 1.0726914718185476\n",
      "30 Train Loss 0.1583102 Test MSE 0.1673736879183222 Test RE 0.9574265284729061\n",
      "31 Train Loss 0.1325958 Test MSE 0.10183090933542421 Test RE 0.7467954702343359\n",
      "32 Train Loss 0.112290576 Test MSE 0.08912943566048585 Test RE 0.6986705579069562\n",
      "33 Train Loss 0.098425925 Test MSE 0.06613702993044926 Test RE 0.6018443778981731\n",
      "34 Train Loss 0.08485707 Test MSE 0.052032648342101126 Test RE 0.5338261318684107\n",
      "35 Train Loss 0.07493102 Test MSE 0.04298355389847459 Test RE 0.4851913215479885\n",
      "36 Train Loss 0.06619901 Test MSE 0.04951918547806686 Test RE 0.5207731805824913\n",
      "37 Train Loss 0.06010758 Test MSE 0.03501882119619612 Test RE 0.43793799470310396\n",
      "38 Train Loss 0.054385863 Test MSE 0.02377652443690354 Test RE 0.3608577606581348\n",
      "39 Train Loss 0.04907218 Test MSE 0.02160736355158031 Test RE 0.3440034147497593\n",
      "40 Train Loss 0.045192495 Test MSE 0.022463351348878554 Test RE 0.3507511785931886\n",
      "41 Train Loss 0.039449476 Test MSE 0.01722858000784715 Test RE 0.3071755227787992\n",
      "42 Train Loss 0.03570373 Test MSE 0.012185902387378603 Test RE 0.25833943863340897\n",
      "43 Train Loss 0.031749405 Test MSE 0.012698048746368581 Test RE 0.26371228330118807\n",
      "44 Train Loss 0.028198905 Test MSE 0.009897624291576503 Test RE 0.2328237854789309\n",
      "45 Train Loss 0.026005287 Test MSE 0.008779377293501386 Test RE 0.21927732303897346\n",
      "46 Train Loss 0.023281641 Test MSE 0.012262616403005275 Test RE 0.25915132610743\n",
      "47 Train Loss 0.02132754 Test MSE 0.014033376350949273 Test RE 0.2772317396724098\n",
      "48 Train Loss 0.019242203 Test MSE 0.011946040008812227 Test RE 0.2557842778944502\n",
      "49 Train Loss 0.018303065 Test MSE 0.009465133054981483 Test RE 0.22768017981508923\n",
      "50 Train Loss 0.018100306 Test MSE 0.009091141841710367 Test RE 0.2231367385207498\n",
      "51 Train Loss 0.016808929 Test MSE 0.007564336158118287 Test RE 0.20353883028036418\n",
      "52 Train Loss 0.0153529635 Test MSE 0.006557684870924919 Test RE 0.1895121835065402\n",
      "53 Train Loss 0.015038949 Test MSE 0.006054083049880503 Test RE 0.18208998125243844\n",
      "54 Train Loss 0.014640822 Test MSE 0.005241401232501979 Test RE 0.169428149420375\n",
      "55 Train Loss 0.013868178 Test MSE 0.005639144117254344 Test RE 0.17573912549206258\n",
      "56 Train Loss 0.013644835 Test MSE 0.0057438346909397045 Test RE 0.17736292002292456\n",
      "57 Train Loss 0.0134716295 Test MSE 0.005165994476079213 Test RE 0.16820497344435328\n",
      "58 Train Loss 0.012872964 Test MSE 0.0043790157565912384 Test RE 0.15486387357680326\n",
      "59 Train Loss 0.011927315 Test MSE 0.004315014727355586 Test RE 0.15372800996454578\n",
      "60 Train Loss 0.01166018 Test MSE 0.004627882944214745 Test RE 0.15920365967768538\n",
      "61 Train Loss 0.011398745 Test MSE 0.004513698068337245 Test RE 0.15722735788587777\n",
      "62 Train Loss 0.010718869 Test MSE 0.003731205839803701 Test RE 0.14295075988185155\n",
      "63 Train Loss 0.01032264 Test MSE 0.0038184839937939158 Test RE 0.1446130052016754\n",
      "64 Train Loss 0.010216798 Test MSE 0.004002804556568775 Test RE 0.14806215181844692\n",
      "65 Train Loss 0.010099458 Test MSE 0.0037988772267953286 Test RE 0.14424125523488981\n",
      "66 Train Loss 0.009862418 Test MSE 0.003669486444660277 Test RE 0.14176352641670675\n",
      "67 Train Loss 0.009477375 Test MSE 0.0034657949277105277 Test RE 0.13777273976742463\n",
      "68 Train Loss 0.009064151 Test MSE 0.003095630098836639 Test RE 0.1302076180949386\n",
      "69 Train Loss 0.008845283 Test MSE 0.0029092460258125052 Test RE 0.12622695009958196\n",
      "70 Train Loss 0.008704337 Test MSE 0.0028142342287284777 Test RE 0.12414864517930703\n",
      "71 Train Loss 0.008544341 Test MSE 0.002845587848987046 Test RE 0.12483830486006463\n",
      "72 Train Loss 0.008274494 Test MSE 0.0029971677250906583 Test RE 0.12812013501921599\n",
      "73 Train Loss 0.00805936 Test MSE 0.002810624839597632 Test RE 0.1240690063674791\n",
      "74 Train Loss 0.0077792667 Test MSE 0.002663285263818674 Test RE 0.12077323715204563\n",
      "75 Train Loss 0.0075790575 Test MSE 0.0025470158260006846 Test RE 0.11810755682537288\n",
      "76 Train Loss 0.0071960012 Test MSE 0.0020505370298002316 Test RE 0.10597310816208667\n",
      "77 Train Loss 0.0068884906 Test MSE 0.0019756328242783133 Test RE 0.1040195523914757\n",
      "78 Train Loss 0.0066273715 Test MSE 0.0018770555030748943 Test RE 0.10139123693615909\n",
      "79 Train Loss 0.006520091 Test MSE 0.0017941129487461943 Test RE 0.09912581119254314\n",
      "80 Train Loss 0.0063901115 Test MSE 0.001853757757861241 Test RE 0.10076004546954939\n",
      "81 Train Loss 0.0060699023 Test MSE 0.0017722135751143857 Test RE 0.09851897698426565\n",
      "82 Train Loss 0.0057732104 Test MSE 0.0017847334213600616 Test RE 0.09886635940511632\n",
      "83 Train Loss 0.0056162803 Test MSE 0.0016554315398688557 Test RE 0.09521765392980387\n",
      "84 Train Loss 0.0055098883 Test MSE 0.0016149540382639953 Test RE 0.09404635053485404\n",
      "85 Train Loss 0.00539469 Test MSE 0.0015612258587062134 Test RE 0.09246869574039349\n",
      "86 Train Loss 0.0053639016 Test MSE 0.0015294014222900294 Test RE 0.09152139030983504\n",
      "87 Train Loss 0.0053075664 Test MSE 0.0015518532510840387 Test RE 0.09219071627479324\n",
      "88 Train Loss 0.0050479406 Test MSE 0.0016678506680236484 Test RE 0.09557415030779369\n",
      "89 Train Loss 0.0048485785 Test MSE 0.0015321317293475805 Test RE 0.09160304646326868\n",
      "90 Train Loss 0.004764674 Test MSE 0.001439740259065013 Test RE 0.08879815421156513\n",
      "91 Train Loss 0.0047098068 Test MSE 0.001474599210237792 Test RE 0.08986671413153685\n",
      "92 Train Loss 0.004624018 Test MSE 0.0013674422228886897 Test RE 0.08653989358783232\n",
      "93 Train Loss 0.0044439184 Test MSE 0.0013182341963053781 Test RE 0.08496853899673178\n",
      "94 Train Loss 0.0043438515 Test MSE 0.0012249940125799153 Test RE 0.08190847629664551\n",
      "95 Train Loss 0.0042741545 Test MSE 0.0011338545892412618 Test RE 0.0788025997044192\n",
      "96 Train Loss 0.004154141 Test MSE 0.0012002057537005846 Test RE 0.081075515039092\n",
      "97 Train Loss 0.004080772 Test MSE 0.0010965405635985114 Test RE 0.07749509499664697\n",
      "98 Train Loss 0.0040193936 Test MSE 0.001035527240393151 Test RE 0.07530826253838485\n",
      "99 Train Loss 0.003893173 Test MSE 0.0009322517087028865 Test RE 0.07145431464879094\n",
      "100 Train Loss 0.003817267 Test MSE 0.000937654803833415 Test RE 0.07166108105837134\n",
      "101 Train Loss 0.0037307108 Test MSE 0.0009179106874969023 Test RE 0.07090258632334807\n",
      "102 Train Loss 0.0035540485 Test MSE 0.0008482627473565102 Test RE 0.06815960444959829\n",
      "103 Train Loss 0.0032666884 Test MSE 0.000737288554756862 Test RE 0.0635448867668861\n",
      "104 Train Loss 0.003034903 Test MSE 0.0008130453622637301 Test RE 0.06672971230770695\n",
      "105 Train Loss 0.0028973573 Test MSE 0.0008030488252628755 Test RE 0.06631821671601437\n",
      "106 Train Loss 0.002810291 Test MSE 0.0007277093726112748 Test RE 0.063130735386277\n",
      "107 Train Loss 0.002753643 Test MSE 0.0006985425134044254 Test RE 0.061852646312782984\n",
      "108 Train Loss 0.0027195541 Test MSE 0.0006829674640233948 Test RE 0.06115921062991812\n",
      "109 Train Loss 0.0026857713 Test MSE 0.0007000472849108885 Test RE 0.06191923068510316\n",
      "110 Train Loss 0.0026257471 Test MSE 0.000674052942408696 Test RE 0.06075875531503259\n",
      "111 Train Loss 0.0025558064 Test MSE 0.0006364437148650672 Test RE 0.05903939045896442\n",
      "112 Train Loss 0.00242887 Test MSE 0.0006456451690279302 Test RE 0.05946464314176138\n",
      "113 Train Loss 0.0023041056 Test MSE 0.0005803091544370838 Test RE 0.05637565163902223\n",
      "114 Train Loss 0.002139659 Test MSE 0.000515953482444779 Test RE 0.05315781704346542\n",
      "115 Train Loss 0.0020865062 Test MSE 0.0004952814277605013 Test RE 0.05208202785869238\n",
      "116 Train Loss 0.002064495 Test MSE 0.000512129152879095 Test RE 0.05296044350982569\n",
      "117 Train Loss 0.0020473965 Test MSE 0.0005118630962571946 Test RE 0.05294668496168452\n",
      "118 Train Loss 0.0020294045 Test MSE 0.0005270281681264496 Test RE 0.05372529116645161\n",
      "119 Train Loss 0.0019808304 Test MSE 0.0004944566101753498 Test RE 0.05203864235091858\n",
      "120 Train Loss 0.0019204328 Test MSE 0.0004517377074167575 Test RE 0.04973991465673099\n",
      "121 Train Loss 0.0018528153 Test MSE 0.00042180746652613034 Test RE 0.048063898688053684\n",
      "122 Train Loss 0.0017926631 Test MSE 0.00040195032493394475 Test RE 0.04691892532609487\n",
      "123 Train Loss 0.0017537499 Test MSE 0.00034934219180620806 Test RE 0.0437408667324224\n",
      "124 Train Loss 0.0017149391 Test MSE 0.0003399294662865392 Test RE 0.043147562871244785\n",
      "125 Train Loss 0.0016620159 Test MSE 0.0003342029916818236 Test RE 0.0427825858657325\n",
      "126 Train Loss 0.0016255064 Test MSE 0.0003418807562514786 Test RE 0.04327122518491763\n",
      "127 Train Loss 0.0015882667 Test MSE 0.0003098588567133973 Test RE 0.04119493492101655\n",
      "128 Train Loss 0.0015425109 Test MSE 0.000279735002611586 Test RE 0.039141302188973125\n",
      "129 Train Loss 0.0015101766 Test MSE 0.0002753985462672124 Test RE 0.038836732681902\n",
      "130 Train Loss 0.0014921101 Test MSE 0.0002796428942120829 Test RE 0.03913485762628155\n",
      "131 Train Loss 0.0014677824 Test MSE 0.00026488225210553727 Test RE 0.03808801102422058\n",
      "132 Train Loss 0.0014511195 Test MSE 0.00023846354839132235 Test RE 0.03613872822844302\n",
      "133 Train Loss 0.001427693 Test MSE 0.00022663146653274744 Test RE 0.0352307564865744\n",
      "134 Train Loss 0.0014077342 Test MSE 0.0002236010593236203 Test RE 0.03499441939702312\n",
      "135 Train Loss 0.0013888464 Test MSE 0.0002471652493539862 Test RE 0.03679218405760812\n",
      "136 Train Loss 0.0013523687 Test MSE 0.00025043578465427165 Test RE 0.037034804513998244\n",
      "137 Train Loss 0.0012999707 Test MSE 0.00022749636254931275 Test RE 0.035297918220800445\n",
      "138 Train Loss 0.0012782327 Test MSE 0.00020854647406342048 Test RE 0.033795843459114315\n",
      "139 Train Loss 0.0012577195 Test MSE 0.00019124964393417435 Test RE 0.03236399919305412\n",
      "140 Train Loss 0.0012402462 Test MSE 0.00018431447065378598 Test RE 0.03177178248953962\n",
      "141 Train Loss 0.0012200774 Test MSE 0.00017342597647786786 Test RE 0.030819028062268546\n",
      "142 Train Loss 0.0011838825 Test MSE 0.00016198268469739177 Test RE 0.02978490096319303\n",
      "143 Train Loss 0.0011486973 Test MSE 0.0001534844668829549 Test RE 0.028993061653456203\n",
      "144 Train Loss 0.0011132947 Test MSE 0.0001470908425687873 Test RE 0.02838276372512272\n",
      "145 Train Loss 0.0010664638 Test MSE 0.00013622281644187726 Test RE 0.02731409349426046\n",
      "146 Train Loss 0.001032151 Test MSE 0.00010976974405478214 Test RE 0.024519024626207135\n",
      "147 Train Loss 0.0010196201 Test MSE 0.00010071825724687047 Test RE 0.023486373616522314\n",
      "148 Train Loss 0.0010120323 Test MSE 9.548464900025708e-05 Test RE 0.022868024141889574\n",
      "149 Train Loss 0.0010075476 Test MSE 9.658942648075865e-05 Test RE 0.02299993759820862\n",
      "150 Train Loss 0.0010006196 Test MSE 9.36744912909044e-05 Test RE 0.022650225791428356\n",
      "151 Train Loss 0.0009759988 Test MSE 9.597301054563951e-05 Test RE 0.022926429448200572\n",
      "152 Train Loss 0.00092703337 Test MSE 8.75657322007301e-05 Test RE 0.021899235593216396\n",
      "153 Train Loss 0.0009045399 Test MSE 9.103752719410019e-05 Test RE 0.022329144818178683\n",
      "154 Train Loss 0.00086833147 Test MSE 7.856415676862701e-05 Test RE 0.020743120528009702\n",
      "155 Train Loss 0.00085328514 Test MSE 8.146090802114158e-05 Test RE 0.021122070478376578\n",
      "156 Train Loss 0.00083694427 Test MSE 7.945767319709228e-05 Test RE 0.020860743622392503\n",
      "157 Train Loss 0.0008265096 Test MSE 8.78654397582931e-05 Test RE 0.021936680377825632\n",
      "158 Train Loss 0.00080542883 Test MSE 7.856174988308051e-05 Test RE 0.020742802783242666\n",
      "159 Train Loss 0.0007863543 Test MSE 6.976558513616284e-05 Test RE 0.019547106743662602\n",
      "160 Train Loss 0.0007769631 Test MSE 6.694522035056678e-05 Test RE 0.019147922089831224\n",
      "161 Train Loss 0.00077227387 Test MSE 6.508218468955449e-05 Test RE 0.018879605947800703\n",
      "162 Train Loss 0.0007683097 Test MSE 6.45650296357527e-05 Test RE 0.018804445924120828\n",
      "163 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "164 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "165 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "166 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "167 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "168 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "169 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "170 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "171 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "172 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "173 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "174 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "175 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "176 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "177 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "178 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "179 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "180 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "181 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "182 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "183 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "184 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "185 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "186 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "187 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "188 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "189 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "190 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "191 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "192 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "193 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "194 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "195 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "196 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "197 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "198 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "199 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "200 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "201 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "202 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "203 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "204 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "205 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "206 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "207 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "208 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "209 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "210 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "211 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "212 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "213 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "214 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "215 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "216 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "217 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "218 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "219 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "220 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "221 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "222 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "223 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "224 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "225 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "226 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "227 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "228 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "229 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "230 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "231 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "232 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "233 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "234 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "235 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "236 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "237 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "238 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "239 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "240 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "241 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "242 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "243 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "244 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "245 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "246 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "247 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "248 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "249 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "250 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "251 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "252 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "253 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "254 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "255 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "256 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "257 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "258 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "259 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "260 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "261 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "262 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "263 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "264 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "265 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "266 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "267 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "268 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "269 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "270 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "271 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "272 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "273 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "274 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "275 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "276 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "277 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "278 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "279 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "280 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "281 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "282 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "283 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "284 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "285 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "286 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "287 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "288 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "289 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "290 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "291 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "292 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "293 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "294 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "295 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "296 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "297 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "298 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "299 Train Loss 0.0007679798 Test MSE 6.44073313269813e-05 Test RE 0.018781467213385545\n",
      "Training time: 130.06\n",
      "KG_swish_low\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 12281.725 Test MSE 2.2452215540421014 Test RE 3.5066422847451872\n",
      "1 Train Loss nan Test MSE nan Test RE nan\n",
      "2 Train Loss nan Test MSE nan Test RE nan\n",
      "3 Train Loss nan Test MSE nan Test RE nan\n",
      "4 Train Loss nan Test MSE nan Test RE nan\n",
      "5 Train Loss nan Test MSE nan Test RE nan\n",
      "6 Train Loss nan Test MSE nan Test RE nan\n",
      "7 Train Loss nan Test MSE nan Test RE nan\n",
      "8 Train Loss nan Test MSE nan Test RE nan\n",
      "9 Train Loss nan Test MSE nan Test RE nan\n",
      "10 Train Loss nan Test MSE nan Test RE nan\n",
      "11 Train Loss nan Test MSE nan Test RE nan\n",
      "12 Train Loss nan Test MSE nan Test RE nan\n",
      "13 Train Loss nan Test MSE nan Test RE nan\n",
      "14 Train Loss nan Test MSE nan Test RE nan\n",
      "15 Train Loss nan Test MSE nan Test RE nan\n",
      "16 Train Loss nan Test MSE nan Test RE nan\n",
      "17 Train Loss nan Test MSE nan Test RE nan\n",
      "18 Train Loss nan Test MSE nan Test RE nan\n",
      "19 Train Loss nan Test MSE nan Test RE nan\n",
      "20 Train Loss nan Test MSE nan Test RE nan\n",
      "21 Train Loss nan Test MSE nan Test RE nan\n",
      "22 Train Loss nan Test MSE nan Test RE nan\n",
      "23 Train Loss nan Test MSE nan Test RE nan\n",
      "24 Train Loss nan Test MSE nan Test RE nan\n",
      "25 Train Loss nan Test MSE nan Test RE nan\n",
      "26 Train Loss nan Test MSE nan Test RE nan\n",
      "27 Train Loss nan Test MSE nan Test RE nan\n",
      "28 Train Loss nan Test MSE nan Test RE nan\n",
      "29 Train Loss nan Test MSE nan Test RE nan\n",
      "30 Train Loss nan Test MSE nan Test RE nan\n",
      "31 Train Loss nan Test MSE nan Test RE nan\n",
      "32 Train Loss nan Test MSE nan Test RE nan\n",
      "33 Train Loss nan Test MSE nan Test RE nan\n",
      "34 Train Loss nan Test MSE nan Test RE nan\n",
      "35 Train Loss nan Test MSE nan Test RE nan\n",
      "36 Train Loss nan Test MSE nan Test RE nan\n",
      "37 Train Loss nan Test MSE nan Test RE nan\n",
      "38 Train Loss nan Test MSE nan Test RE nan\n",
      "39 Train Loss nan Test MSE nan Test RE nan\n",
      "40 Train Loss nan Test MSE nan Test RE nan\n",
      "41 Train Loss nan Test MSE nan Test RE nan\n",
      "42 Train Loss nan Test MSE nan Test RE nan\n",
      "43 Train Loss nan Test MSE nan Test RE nan\n",
      "44 Train Loss nan Test MSE nan Test RE nan\n",
      "45 Train Loss nan Test MSE nan Test RE nan\n",
      "46 Train Loss nan Test MSE nan Test RE nan\n",
      "47 Train Loss nan Test MSE nan Test RE nan\n",
      "48 Train Loss nan Test MSE nan Test RE nan\n",
      "49 Train Loss nan Test MSE nan Test RE nan\n",
      "50 Train Loss nan Test MSE nan Test RE nan\n",
      "51 Train Loss nan Test MSE nan Test RE nan\n",
      "52 Train Loss nan Test MSE nan Test RE nan\n",
      "53 Train Loss nan Test MSE nan Test RE nan\n",
      "54 Train Loss nan Test MSE nan Test RE nan\n",
      "55 Train Loss nan Test MSE nan Test RE nan\n",
      "56 Train Loss nan Test MSE nan Test RE nan\n",
      "57 Train Loss nan Test MSE nan Test RE nan\n",
      "58 Train Loss nan Test MSE nan Test RE nan\n",
      "59 Train Loss nan Test MSE nan Test RE nan\n",
      "60 Train Loss nan Test MSE nan Test RE nan\n",
      "61 Train Loss nan Test MSE nan Test RE nan\n",
      "62 Train Loss nan Test MSE nan Test RE nan\n",
      "63 Train Loss nan Test MSE nan Test RE nan\n",
      "64 Train Loss nan Test MSE nan Test RE nan\n",
      "65 Train Loss nan Test MSE nan Test RE nan\n",
      "66 Train Loss nan Test MSE nan Test RE nan\n",
      "67 Train Loss nan Test MSE nan Test RE nan\n",
      "68 Train Loss nan Test MSE nan Test RE nan\n",
      "69 Train Loss nan Test MSE nan Test RE nan\n",
      "70 Train Loss nan Test MSE nan Test RE nan\n",
      "71 Train Loss nan Test MSE nan Test RE nan\n",
      "72 Train Loss nan Test MSE nan Test RE nan\n",
      "73 Train Loss nan Test MSE nan Test RE nan\n",
      "74 Train Loss nan Test MSE nan Test RE nan\n",
      "75 Train Loss nan Test MSE nan Test RE nan\n",
      "76 Train Loss nan Test MSE nan Test RE nan\n",
      "77 Train Loss nan Test MSE nan Test RE nan\n",
      "78 Train Loss nan Test MSE nan Test RE nan\n",
      "79 Train Loss nan Test MSE nan Test RE nan\n",
      "80 Train Loss nan Test MSE nan Test RE nan\n",
      "81 Train Loss nan Test MSE nan Test RE nan\n",
      "82 Train Loss nan Test MSE nan Test RE nan\n",
      "83 Train Loss nan Test MSE nan Test RE nan\n",
      "84 Train Loss nan Test MSE nan Test RE nan\n",
      "85 Train Loss nan Test MSE nan Test RE nan\n",
      "86 Train Loss nan Test MSE nan Test RE nan\n",
      "87 Train Loss nan Test MSE nan Test RE nan\n",
      "88 Train Loss nan Test MSE nan Test RE nan\n",
      "89 Train Loss nan Test MSE nan Test RE nan\n",
      "90 Train Loss nan Test MSE nan Test RE nan\n",
      "91 Train Loss nan Test MSE nan Test RE nan\n",
      "92 Train Loss nan Test MSE nan Test RE nan\n",
      "93 Train Loss nan Test MSE nan Test RE nan\n",
      "94 Train Loss nan Test MSE nan Test RE nan\n",
      "95 Train Loss nan Test MSE nan Test RE nan\n",
      "96 Train Loss nan Test MSE nan Test RE nan\n",
      "97 Train Loss nan Test MSE nan Test RE nan\n",
      "98 Train Loss nan Test MSE nan Test RE nan\n",
      "99 Train Loss nan Test MSE nan Test RE nan\n",
      "100 Train Loss nan Test MSE nan Test RE nan\n",
      "101 Train Loss nan Test MSE nan Test RE nan\n",
      "102 Train Loss nan Test MSE nan Test RE nan\n",
      "103 Train Loss nan Test MSE nan Test RE nan\n",
      "104 Train Loss nan Test MSE nan Test RE nan\n",
      "105 Train Loss nan Test MSE nan Test RE nan\n",
      "106 Train Loss nan Test MSE nan Test RE nan\n",
      "107 Train Loss nan Test MSE nan Test RE nan\n",
      "108 Train Loss nan Test MSE nan Test RE nan\n",
      "109 Train Loss nan Test MSE nan Test RE nan\n",
      "110 Train Loss nan Test MSE nan Test RE nan\n",
      "111 Train Loss nan Test MSE nan Test RE nan\n",
      "112 Train Loss nan Test MSE nan Test RE nan\n",
      "113 Train Loss nan Test MSE nan Test RE nan\n",
      "114 Train Loss nan Test MSE nan Test RE nan\n",
      "115 Train Loss nan Test MSE nan Test RE nan\n",
      "116 Train Loss nan Test MSE nan Test RE nan\n",
      "117 Train Loss nan Test MSE nan Test RE nan\n",
      "118 Train Loss nan Test MSE nan Test RE nan\n",
      "119 Train Loss nan Test MSE nan Test RE nan\n",
      "120 Train Loss nan Test MSE nan Test RE nan\n",
      "121 Train Loss nan Test MSE nan Test RE nan\n",
      "122 Train Loss nan Test MSE nan Test RE nan\n",
      "123 Train Loss nan Test MSE nan Test RE nan\n",
      "124 Train Loss nan Test MSE nan Test RE nan\n",
      "125 Train Loss nan Test MSE nan Test RE nan\n",
      "126 Train Loss nan Test MSE nan Test RE nan\n",
      "127 Train Loss nan Test MSE nan Test RE nan\n",
      "128 Train Loss nan Test MSE nan Test RE nan\n",
      "129 Train Loss nan Test MSE nan Test RE nan\n",
      "130 Train Loss nan Test MSE nan Test RE nan\n",
      "131 Train Loss nan Test MSE nan Test RE nan\n",
      "132 Train Loss nan Test MSE nan Test RE nan\n",
      "133 Train Loss nan Test MSE nan Test RE nan\n",
      "134 Train Loss nan Test MSE nan Test RE nan\n",
      "135 Train Loss nan Test MSE nan Test RE nan\n",
      "136 Train Loss nan Test MSE nan Test RE nan\n",
      "137 Train Loss nan Test MSE nan Test RE nan\n",
      "138 Train Loss nan Test MSE nan Test RE nan\n",
      "139 Train Loss nan Test MSE nan Test RE nan\n",
      "140 Train Loss nan Test MSE nan Test RE nan\n",
      "141 Train Loss nan Test MSE nan Test RE nan\n",
      "142 Train Loss nan Test MSE nan Test RE nan\n",
      "143 Train Loss nan Test MSE nan Test RE nan\n",
      "144 Train Loss nan Test MSE nan Test RE nan\n",
      "145 Train Loss nan Test MSE nan Test RE nan\n",
      "146 Train Loss nan Test MSE nan Test RE nan\n",
      "147 Train Loss nan Test MSE nan Test RE nan\n",
      "148 Train Loss nan Test MSE nan Test RE nan\n",
      "149 Train Loss nan Test MSE nan Test RE nan\n",
      "150 Train Loss nan Test MSE nan Test RE nan\n",
      "151 Train Loss nan Test MSE nan Test RE nan\n",
      "152 Train Loss nan Test MSE nan Test RE nan\n",
      "153 Train Loss nan Test MSE nan Test RE nan\n",
      "154 Train Loss nan Test MSE nan Test RE nan\n",
      "155 Train Loss nan Test MSE nan Test RE nan\n",
      "156 Train Loss nan Test MSE nan Test RE nan\n",
      "157 Train Loss nan Test MSE nan Test RE nan\n",
      "158 Train Loss nan Test MSE nan Test RE nan\n",
      "159 Train Loss nan Test MSE nan Test RE nan\n",
      "160 Train Loss nan Test MSE nan Test RE nan\n",
      "161 Train Loss nan Test MSE nan Test RE nan\n",
      "162 Train Loss nan Test MSE nan Test RE nan\n",
      "163 Train Loss nan Test MSE nan Test RE nan\n",
      "164 Train Loss nan Test MSE nan Test RE nan\n",
      "165 Train Loss nan Test MSE nan Test RE nan\n",
      "166 Train Loss nan Test MSE nan Test RE nan\n",
      "167 Train Loss nan Test MSE nan Test RE nan\n",
      "168 Train Loss nan Test MSE nan Test RE nan\n",
      "169 Train Loss nan Test MSE nan Test RE nan\n",
      "170 Train Loss nan Test MSE nan Test RE nan\n",
      "171 Train Loss nan Test MSE nan Test RE nan\n",
      "172 Train Loss nan Test MSE nan Test RE nan\n",
      "173 Train Loss nan Test MSE nan Test RE nan\n",
      "174 Train Loss nan Test MSE nan Test RE nan\n",
      "175 Train Loss nan Test MSE nan Test RE nan\n",
      "176 Train Loss nan Test MSE nan Test RE nan\n",
      "177 Train Loss nan Test MSE nan Test RE nan\n",
      "178 Train Loss nan Test MSE nan Test RE nan\n",
      "179 Train Loss nan Test MSE nan Test RE nan\n",
      "180 Train Loss nan Test MSE nan Test RE nan\n",
      "181 Train Loss nan Test MSE nan Test RE nan\n",
      "182 Train Loss nan Test MSE nan Test RE nan\n",
      "183 Train Loss nan Test MSE nan Test RE nan\n",
      "184 Train Loss nan Test MSE nan Test RE nan\n",
      "185 Train Loss nan Test MSE nan Test RE nan\n",
      "186 Train Loss nan Test MSE nan Test RE nan\n",
      "187 Train Loss nan Test MSE nan Test RE nan\n",
      "188 Train Loss nan Test MSE nan Test RE nan\n",
      "189 Train Loss nan Test MSE nan Test RE nan\n",
      "190 Train Loss nan Test MSE nan Test RE nan\n",
      "191 Train Loss nan Test MSE nan Test RE nan\n",
      "192 Train Loss nan Test MSE nan Test RE nan\n",
      "193 Train Loss nan Test MSE nan Test RE nan\n",
      "194 Train Loss nan Test MSE nan Test RE nan\n",
      "195 Train Loss nan Test MSE nan Test RE nan\n",
      "196 Train Loss nan Test MSE nan Test RE nan\n",
      "197 Train Loss nan Test MSE nan Test RE nan\n",
      "198 Train Loss nan Test MSE nan Test RE nan\n",
      "199 Train Loss nan Test MSE nan Test RE nan\n",
      "200 Train Loss nan Test MSE nan Test RE nan\n",
      "201 Train Loss nan Test MSE nan Test RE nan\n",
      "202 Train Loss nan Test MSE nan Test RE nan\n",
      "203 Train Loss nan Test MSE nan Test RE nan\n",
      "204 Train Loss nan Test MSE nan Test RE nan\n",
      "205 Train Loss nan Test MSE nan Test RE nan\n",
      "206 Train Loss nan Test MSE nan Test RE nan\n",
      "207 Train Loss nan Test MSE nan Test RE nan\n",
      "208 Train Loss nan Test MSE nan Test RE nan\n",
      "209 Train Loss nan Test MSE nan Test RE nan\n",
      "210 Train Loss nan Test MSE nan Test RE nan\n",
      "211 Train Loss nan Test MSE nan Test RE nan\n",
      "212 Train Loss nan Test MSE nan Test RE nan\n",
      "213 Train Loss nan Test MSE nan Test RE nan\n",
      "214 Train Loss nan Test MSE nan Test RE nan\n",
      "215 Train Loss nan Test MSE nan Test RE nan\n",
      "216 Train Loss nan Test MSE nan Test RE nan\n",
      "217 Train Loss nan Test MSE nan Test RE nan\n",
      "218 Train Loss nan Test MSE nan Test RE nan\n",
      "219 Train Loss nan Test MSE nan Test RE nan\n",
      "220 Train Loss nan Test MSE nan Test RE nan\n",
      "221 Train Loss nan Test MSE nan Test RE nan\n",
      "222 Train Loss nan Test MSE nan Test RE nan\n",
      "223 Train Loss nan Test MSE nan Test RE nan\n",
      "224 Train Loss nan Test MSE nan Test RE nan\n",
      "225 Train Loss nan Test MSE nan Test RE nan\n",
      "226 Train Loss nan Test MSE nan Test RE nan\n",
      "227 Train Loss nan Test MSE nan Test RE nan\n",
      "228 Train Loss nan Test MSE nan Test RE nan\n",
      "229 Train Loss nan Test MSE nan Test RE nan\n",
      "230 Train Loss nan Test MSE nan Test RE nan\n",
      "231 Train Loss nan Test MSE nan Test RE nan\n",
      "232 Train Loss nan Test MSE nan Test RE nan\n",
      "233 Train Loss nan Test MSE nan Test RE nan\n",
      "234 Train Loss nan Test MSE nan Test RE nan\n",
      "235 Train Loss nan Test MSE nan Test RE nan\n",
      "236 Train Loss nan Test MSE nan Test RE nan\n",
      "237 Train Loss nan Test MSE nan Test RE nan\n",
      "238 Train Loss nan Test MSE nan Test RE nan\n",
      "239 Train Loss nan Test MSE nan Test RE nan\n",
      "240 Train Loss nan Test MSE nan Test RE nan\n",
      "241 Train Loss nan Test MSE nan Test RE nan\n",
      "242 Train Loss nan Test MSE nan Test RE nan\n",
      "243 Train Loss nan Test MSE nan Test RE nan\n",
      "244 Train Loss nan Test MSE nan Test RE nan\n",
      "245 Train Loss nan Test MSE nan Test RE nan\n",
      "246 Train Loss nan Test MSE nan Test RE nan\n",
      "247 Train Loss nan Test MSE nan Test RE nan\n",
      "248 Train Loss nan Test MSE nan Test RE nan\n",
      "249 Train Loss nan Test MSE nan Test RE nan\n",
      "250 Train Loss nan Test MSE nan Test RE nan\n",
      "251 Train Loss nan Test MSE nan Test RE nan\n",
      "252 Train Loss nan Test MSE nan Test RE nan\n",
      "253 Train Loss nan Test MSE nan Test RE nan\n",
      "254 Train Loss nan Test MSE nan Test RE nan\n",
      "255 Train Loss nan Test MSE nan Test RE nan\n",
      "256 Train Loss nan Test MSE nan Test RE nan\n",
      "257 Train Loss nan Test MSE nan Test RE nan\n",
      "258 Train Loss nan Test MSE nan Test RE nan\n",
      "259 Train Loss nan Test MSE nan Test RE nan\n",
      "260 Train Loss nan Test MSE nan Test RE nan\n",
      "261 Train Loss nan Test MSE nan Test RE nan\n",
      "262 Train Loss nan Test MSE nan Test RE nan\n",
      "263 Train Loss nan Test MSE nan Test RE nan\n",
      "264 Train Loss nan Test MSE nan Test RE nan\n",
      "265 Train Loss nan Test MSE nan Test RE nan\n",
      "266 Train Loss nan Test MSE nan Test RE nan\n",
      "267 Train Loss nan Test MSE nan Test RE nan\n",
      "268 Train Loss nan Test MSE nan Test RE nan\n",
      "269 Train Loss nan Test MSE nan Test RE nan\n",
      "270 Train Loss nan Test MSE nan Test RE nan\n",
      "271 Train Loss nan Test MSE nan Test RE nan\n",
      "272 Train Loss nan Test MSE nan Test RE nan\n",
      "273 Train Loss nan Test MSE nan Test RE nan\n",
      "274 Train Loss nan Test MSE nan Test RE nan\n",
      "275 Train Loss nan Test MSE nan Test RE nan\n",
      "276 Train Loss nan Test MSE nan Test RE nan\n",
      "277 Train Loss nan Test MSE nan Test RE nan\n",
      "278 Train Loss nan Test MSE nan Test RE nan\n",
      "279 Train Loss nan Test MSE nan Test RE nan\n",
      "280 Train Loss nan Test MSE nan Test RE nan\n",
      "281 Train Loss nan Test MSE nan Test RE nan\n",
      "282 Train Loss nan Test MSE nan Test RE nan\n",
      "283 Train Loss nan Test MSE nan Test RE nan\n",
      "284 Train Loss nan Test MSE nan Test RE nan\n",
      "285 Train Loss nan Test MSE nan Test RE nan\n",
      "286 Train Loss nan Test MSE nan Test RE nan\n",
      "287 Train Loss nan Test MSE nan Test RE nan\n",
      "288 Train Loss nan Test MSE nan Test RE nan\n",
      "289 Train Loss nan Test MSE nan Test RE nan\n",
      "290 Train Loss nan Test MSE nan Test RE nan\n",
      "291 Train Loss nan Test MSE nan Test RE nan\n",
      "292 Train Loss nan Test MSE nan Test RE nan\n",
      "293 Train Loss nan Test MSE nan Test RE nan\n",
      "294 Train Loss nan Test MSE nan Test RE nan\n",
      "295 Train Loss nan Test MSE nan Test RE nan\n",
      "296 Train Loss nan Test MSE nan Test RE nan\n",
      "297 Train Loss nan Test MSE nan Test RE nan\n",
      "298 Train Loss nan Test MSE nan Test RE nan\n",
      "299 Train Loss nan Test MSE nan Test RE nan\n",
      "Training time: 20.64\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10 #10\n",
    "max_iter = 300 #200\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "beta_init = 0\n",
    "\n",
    "N_I = 1000  #Total number of data points for 'y'\n",
    "N_B = 5000\n",
    "N_f = 10000 #Total number of collocation points\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(label)\n",
    "    print(reps)\n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []\n",
    "    beta_val = []\n",
    "\n",
    "    torch.manual_seed(reps*36)\n",
    "\n",
    "    layers = np.array([2,50,50,50,50,1]) #9 hidden layers\n",
    "    # layers = np.array([2,50,50,50,50,50,50,50,1])\n",
    "\n",
    "    PINN = Sequentialmodel(layers,beta_init)\n",
    "\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=1, \n",
    "                            max_iter = 20, \n",
    "                            max_eval = 30, \n",
    "                            tolerance_grad = 1e-8, \n",
    "                            tolerance_change = 1e-8, \n",
    "                            history_size = 100, \n",
    "                            line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    nan_flag = train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "\n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    #elapsed_time[reps] = time.time() - start_time\n",
    "    beta_full.append(beta_val)\n",
    "\n",
    "\n",
    "  #print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"beta\": beta_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f5afc782690>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "u_pred = PINN.test()\n",
    "plt.imshow(u_pred.reshape(500,500),cmap = 'jet')\n",
    "plt.figure()\n",
    "plt.imshow(y_true.reshape(500,500),cmap = 'jet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1660688534316,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "06syezgfv_qO",
    "outputId": "9f4852d5-694a-4977-8893-a6183a2ce493"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a =  0.04167459739330868\n"
     ]
    }
   ],
   "source": [
    "a = np.ones((10,1))\n",
    "for i in range(10):\n",
    "    a[i] = test_re_full[i][-1]    \n",
    "print(\"a = \",np.nanmean(a))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "stan_2D_KG_16Aug2022_tune.ipynb",
   "version": ""
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
