{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 286,
     "status": "ok",
     "timestamp": 1660687093981,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "iAtv2UvNSq_u",
    "outputId": "68a82578-1b95-4343-a8ec-7635a4df93ef"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "executionInfo": {
     "elapsed": 280,
     "status": "ok",
     "timestamp": 1660687410736,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "mTLFQRt5Sq_y"
   },
   "outputs": [],
   "source": [
    "def true_2D_1(xt): #True function for 2D_1 Klein Gordon Equation x \\in [-50,50] , t \\in [0,10]\n",
    "    x = xt[:,0].reshape(-1,1)\n",
    "    t = xt[:,1].reshape(-1,1)\n",
    "    y = x*np.cos(5*np.pi*t) + np.power(x*t,3)\n",
    "    return y.reshape(-1,1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "executionInfo": {
     "elapsed": 4312,
     "status": "ok",
     "timestamp": 1660687098957,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "81bNHCY3Sq_y"
   },
   "outputs": [],
   "source": [
    "pi = np.pi\n",
    "\n",
    "loss_thresh = 0.1\n",
    "level = \"high\"\n",
    "label = \"KG_rowdy_\" + level\n",
    "\n",
    "x = np.linspace(-2,2,500).reshape(-1,1)\n",
    "t = np.linspace(0,1,500).reshape(-1,1)\n",
    "\n",
    "X,T = np.meshgrid(x,t)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "T = T.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xt = np.hstack((X,T))\n",
    "\n",
    "y_true = true_2D_1(xt)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "#bound_pts_idx = ((X == -5) + (X == 5) + (T == 0)).reshape(-1,)\n",
    "\n",
    "#xt_bound = xt[bound_pts_idx,:]\n",
    "#y_bound = y_true[bound_pts_idx,:]\n",
    "\n",
    "\n",
    "xt_test_tensor = torch.from_numpy(xt).float().to(device)\n",
    "\n",
    "\n",
    "lb_xt = xt[0]\n",
    "ub_xt = xt[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1660687098958,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "YQgCA-PuSq_z"
   },
   "outputs": [],
   "source": [
    "def trainingdata(N_I,N_B,N_f,seed):\n",
    "    '''Boundary Conditions''' \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    x_BC1 = np.random.uniform(size = N_I).reshape(-1,1)\n",
    "    t_BC1 = np.zeros((N_I,1))\n",
    "    samples = np.hstack((x_BC1,t_BC1))\n",
    "    xt_BC1 = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    y_BC1 = true_2D_1(xt_BC1)\n",
    "    \n",
    "    x_BC2 = np.zeros((int(N_B/2),1))\n",
    "    t_BC2 = np.random.uniform(size = int(N_B/2)).reshape(-1,1)\n",
    "    samples = np.hstack((x_BC2,t_BC2))\n",
    "    xt_BC2 = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    y_BC2 = true_2D_1(xt_BC2)\n",
    "    \n",
    "    x_BC3 = np.ones((int(N_B/2),1))\n",
    "    t_BC3 = np.random.uniform(size = int(N_B/2)).reshape(-1,1)\n",
    "    samples = np.hstack((x_BC3,t_BC3))\n",
    "    xt_BC3 = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    y_BC3 = true_2D_1(xt_BC3)\n",
    "    \n",
    "    x_NBC = np.random.uniform(size = N_I).reshape(-1,1)\n",
    "    t_NBC = np.zeros((N_I,1))\n",
    "    samples = np.hstack((x_NBC,t_NBC))\n",
    "    xt_NBC = lb_xt + (ub_xt - lb_xt)*samples\n",
    "\n",
    "    \n",
    "    xt_BC = np.vstack((xt_BC1,xt_BC2,xt_BC3))\n",
    "    y_BC = np.vstack((y_BC1,y_BC2,y_BC3))\n",
    "\n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    samples = sampling(N_f)\n",
    "    xt_coll = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    \n",
    "    xt_coll = np.vstack((xt_coll, xt_BC,xt_NBC)) # append training points to collocation points \n",
    "\n",
    "    return xt_coll, xt_BC, y_BC,xt_NBC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1660687098958,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "gTJxct8bSq_0"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,rowdy_terms,n_val):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "        \n",
    "        self.omega1 = Parameter(torch.ones((len(layers)-2,1))) \n",
    "        self.omega1.requiresGrad = True\n",
    "        \n",
    "        \n",
    "        self.alpha = Parameter(torch.zeros(rowdy_terms,len(layers)-2))\n",
    "        self.alpha.requiresGrad = True\n",
    "        \n",
    "        \n",
    "        self.omega = Parameter((1/n_val)*torch.ones(rowdy_terms,len(layers)-2))\n",
    "        self.omega.requiresGrad = True\n",
    "        \n",
    "        self.n = torch.tensor(n_val)\n",
    "            \n",
    "\n",
    "    'foward pass'\n",
    "    def forward(self,xt):\n",
    "        if torch.is_tensor(xt) != True:         \n",
    "            xt = torch.from_numpy(xt)                \n",
    "        \n",
    "        ubxt = torch.from_numpy(ub_xt).float().to(device)\n",
    "        lbxt = torch.from_numpy(lb_xt).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xt = 2.0*(xt - lbxt)/(ubxt - lbxt) - 1.0\n",
    "        \n",
    "        #convert to float\n",
    "        a = xt.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(self.omega1[i,0]*z)\n",
    "            for j in range(rowdy_terms):\n",
    "                a = a + self.alpha[j,i]*self.n*torch.sin((j+1)*self.n*self.omega[j,i]*z)\n",
    "                    \n",
    "        \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC(self,xt,y):\n",
    "                \n",
    "        loss_bc = self.loss_function(self.forward(xt), y)\n",
    "                \n",
    "        return loss_bc\n",
    "    \n",
    "    def loss_NBC(self,xt_NBC,N_hat):\n",
    "        g = xt_NBC.clone()             \n",
    "        g.requires_grad = True\n",
    "        y = self.forward(g) \n",
    "        \n",
    "        y_x_t = autograd.grad(y,g,torch.ones([xt_NBC.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]        \n",
    "        dy_dt = y_x_t[:,[1]]\n",
    "        \n",
    "        loss_nbc = self.loss_function(dy_dt, N_hat)\n",
    "                \n",
    "        return loss_nbc\n",
    "    \n",
    "    def loss_PDE(self, xt_coll, f_hat):\n",
    "        \n",
    "        g = xt_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        y = self.forward(g) \n",
    "        \n",
    "        \n",
    "        y_x_t = autograd.grad(y,g,torch.ones([xt_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        y_xx_tt = autograd.grad(y_x_t,g,torch.ones(xt_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        #du_dx = u_x_t[:,[0]]\n",
    "        \n",
    "        d2y_dx2 = y_xx_tt[:,[0]]\n",
    "        d2y_dt2 = y_xx_tt[:,[1]]    \n",
    "        \n",
    "\n",
    "        term1 = -25.0*(pi**2)*(g[:,0].reshape(-1,1))*torch.cos(5.0*pi*g[:,1].reshape(-1,1)) + 6*torch.pow(g[:,0].reshape(-1,1),3)*g[:,1].reshape(-1,1)  \n",
    "        term1 = term1.reshape(-1,1)\n",
    "        term2 = 6*torch.pow(g[:,1].reshape(-1,1),3)*g[:,0].reshape(-1,1)\n",
    "        term2 = term2.reshape(-1,1)\n",
    "        term3 = torch.pow(g[:,0]*torch.cos(5.0*pi*g[:,1]) + torch.pow(g[:,0]*g[:,1],3),3)\n",
    "        term3 = term3.reshape(-1,1)\n",
    "        \n",
    "        \n",
    "        f = d2y_dt2 - d2y_dx2 + torch.pow(y,3) - (term1 - term2 + term3)\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xt_BC,y_BC,xt_coll,f_hat,xt_NBC,N_hat):\n",
    "\n",
    "        loss_BC = self.loss_BC(xt_BC,y_BC)\n",
    "        loss_NBC = self.loss_NBC(xt_NBC,N_hat)\n",
    "        loss_f = self.loss_PDE(xt_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_BC + loss_f + loss_NBC\n",
    "        \n",
    "        return loss_val\n",
    "         \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        y_pred = self.forward(xt_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return y_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "executionInfo": {
     "elapsed": 5,
     "status": "ok",
     "timestamp": 1660687098958,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "VoQzfzYsYKVs"
   },
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    # beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1660687098959,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "_IUDZDkxXmyF"
   },
   "outputs": [],
   "source": [
    "def train_step(xt_BC,y_BC,xt_coll,f_hat,xt_NBC,N_hat,seed):\n",
    "    # x_coll_np_array = colloc_pts(N_f,seed*123)\n",
    "    # x_coll_train = torch.from_numpy(x_coll_np_array).float().to(device)        \n",
    "    \n",
    "    # f_hat = torch.zeros(x_coll_train.shape[0],1).to(device)\n",
    "#     xt_coll, xt_BC, y_BC = trainingdata(N_I,N_B,N_f,seed*123)\n",
    "#     xt_coll = torch.from_numpy(xt_coll).float().to(device)\n",
    "#     xt_BC = torch.from_numpy(xt_BC).float().to(device)\n",
    "#     y_BC = torch.from_numpy(y_BC).float().to(device)\n",
    "\n",
    "#     f_hat = torch.zeros(xt_coll.shape[0],1).to(device)\n",
    "    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(xt_BC,y_BC,xt_coll,f_hat,xt_NBC,N_hat)\n",
    "        loss.backward()\n",
    "        #print(loss.cpu().detach().numpy())\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "executionInfo": {
     "elapsed": 467,
     "status": "ok",
     "timestamp": 1660690085956,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "Vt9Dlr8MYIwW"
   },
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*9)\n",
    "    start_time = time.time() \n",
    "    thresh_flag = 0\n",
    "\n",
    "    xt_coll, xt_BC, y_BC, xt_NBC = trainingdata(N_I,N_B,N_f,rep*11)\n",
    "    xt_coll = torch.from_numpy(xt_coll).float().to(device)\n",
    "    xt_BC = torch.from_numpy(xt_BC).float().to(device)\n",
    "    y_BC = torch.from_numpy(y_BC).float().to(device)\n",
    "    xt_NBC = torch.from_numpy(xt_NBC).float().to(device)\n",
    "    \n",
    "    f_hat = torch.zeros(xt_coll.shape[0],1).to(device)\n",
    "    N_hat = torch.zeros(xt_NBC.shape[0],1).to(device)\n",
    "    \n",
    "    loss_np = PINN.loss(xt_BC,y_BC,xt_coll,f_hat,xt_NBC,N_hat).cpu().detach().numpy()\n",
    "    data_update(loss_np)\n",
    "    for i in range(max_iter):\n",
    "        if(np.isnan(loss_np) == False):\n",
    "            train_step(xt_BC,y_BC,xt_coll,f_hat,xt_NBC,N_hat,i)\n",
    "            loss_np = PINN.loss(xt_BC,y_BC,xt_coll,f_hat,xt_NBC,N_hat).cpu().detach().numpy()\n",
    "        \n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1          \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        \n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])   \n",
    "        \n",
    "      \n",
    "         \n",
    "\n",
    "    elapsed_time[rep] = time.time() - start_time  \n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "background_save": true,
     "base_uri": "https://localhost:8080/"
    },
    "id": "sP4Re5lSSq_1",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KG_rowdy_medium\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 27277.727 Test MSE 2.6953268203578835 Test RE 1.1895244799457358\n",
      "1 Train Loss 3232.7952 Test MSE 2.455255913407958 Test RE 1.135314149418008\n",
      "2 Train Loss 511.49222 Test MSE 3.1922280863634667 Test RE 1.2945374680607984\n",
      "3 Train Loss 147.30666 Test MSE 3.1790098429177025 Test RE 1.291854504725403\n",
      "4 Train Loss 62.119507 Test MSE 2.2309809055807857 Test RE 1.0822200962506083\n",
      "5 Train Loss 29.840477 Test MSE 1.147433685505059 Test RE 0.7761246291938039\n",
      "6 Train Loss 16.568604 Test MSE 0.6818931452932969 Test RE 0.5983094054405874\n",
      "7 Train Loss 10.526774 Test MSE 0.4392068490367819 Test RE 0.4801779571237327\n",
      "8 Train Loss 7.1587725 Test MSE 0.36583864516807885 Test RE 0.43824044677503804\n",
      "9 Train Loss 5.178589 Test MSE 0.3332538898682598 Test RE 0.4182686179268932\n",
      "10 Train Loss 3.8085072 Test MSE 0.3127708171168608 Test RE 0.40521058353474415\n",
      "11 Train Loss 3.077721 Test MSE 0.2910468484318733 Test RE 0.3908851018679013\n",
      "12 Train Loss 2.377777 Test MSE 0.2840109368124824 Test RE 0.3861314712829256\n",
      "13 Train Loss 1.8890021 Test MSE 0.28752293159844333 Test RE 0.3885115295894674\n",
      "14 Train Loss 1.4989719 Test MSE 0.2573963265458441 Test RE 0.36759436045197663\n",
      "15 Train Loss 1.2323219 Test MSE 0.18811052924216898 Test RE 0.3142492286137492\n",
      "16 Train Loss 1.0435897 Test MSE 0.15971913611308636 Test RE 0.2895650512089736\n",
      "17 Train Loss 0.8717374 Test MSE 0.13429740236054805 Test RE 0.26552258889040864\n",
      "18 Train Loss 0.7309941 Test MSE 0.11111183803255394 Test RE 0.24151707123789581\n",
      "19 Train Loss 0.6296318 Test MSE 0.10422178236699735 Test RE 0.23390899106706914\n",
      "20 Train Loss 0.56468236 Test MSE 0.09702264737934309 Test RE 0.2256857966634579\n",
      "21 Train Loss 0.51615024 Test MSE 0.08998293511742077 Test RE 0.217344044871822\n",
      "22 Train Loss 0.46583784 Test MSE 0.08574267762640445 Test RE 0.21216131012127384\n",
      "23 Train Loss 0.41694105 Test MSE 0.07479415080140035 Test RE 0.19815338034797123\n",
      "24 Train Loss 0.37560955 Test MSE 0.06576192135550239 Test RE 0.1858039380018753\n",
      "25 Train Loss 0.34475282 Test MSE 0.056100793648565815 Test RE 0.1716137851247513\n",
      "26 Train Loss 0.31289577 Test MSE 0.045688588097395295 Test RE 0.15487151127823648\n",
      "27 Train Loss 0.2859211 Test MSE 0.03769584766654741 Test RE 0.14067419043605336\n",
      "28 Train Loss 0.26607254 Test MSE 0.03182559745656486 Test RE 0.12925755906193895\n",
      "29 Train Loss 0.2513213 Test MSE 0.02711350800751871 Test RE 0.1193055159592675\n",
      "30 Train Loss 0.21896109 Test MSE 0.02172111926633124 Test RE 0.10678463505603455\n",
      "31 Train Loss 0.20339109 Test MSE 0.02049280747509506 Test RE 0.10372140657453312\n",
      "32 Train Loss 0.18563683 Test MSE 0.018930273980787192 Test RE 0.09968874243822674\n",
      "33 Train Loss 0.1772304 Test MSE 0.016505318207612335 Test RE 0.09308498185408921\n",
      "34 Train Loss 0.15621792 Test MSE 0.017336354396262296 Test RE 0.09539960046049871\n",
      "35 Train Loss 0.1444769 Test MSE 0.01506007147207555 Test RE 0.08891625939090861\n",
      "36 Train Loss 0.12583463 Test MSE 0.013532712695993186 Test RE 0.0842869033665989\n",
      "37 Train Loss 0.11734319 Test MSE 0.011823001413766994 Test RE 0.07878282294893502\n",
      "38 Train Loss 0.10875216 Test MSE 0.010975529418743508 Test RE 0.07590675166550544\n",
      "39 Train Loss 0.103283785 Test MSE 0.010858171940885153 Test RE 0.07549983889225946\n",
      "40 Train Loss 0.098422915 Test MSE 0.010716090199964902 Test RE 0.07500424571542319\n",
      "41 Train Loss 0.0960554 Test MSE 0.010661551886322602 Test RE 0.07481313949319293\n",
      "42 Train Loss 0.09205647 Test MSE 0.010304458319821644 Test RE 0.07354958915293484\n",
      "43 Train Loss 0.08875342 Test MSE 0.009826233066042316 Test RE 0.07182261247028675\n",
      "44 Train Loss 0.08493318 Test MSE 0.009811871276313862 Test RE 0.07177010616236294\n",
      "45 Train Loss 0.08208803 Test MSE 0.009923824301485256 Test RE 0.07217839173279057\n",
      "46 Train Loss 0.08041247 Test MSE 0.009622215979963896 Test RE 0.0710730933820795\n",
      "47 Train Loss 0.076491885 Test MSE 0.009091499892478944 Test RE 0.06908526639973468\n",
      "48 Train Loss 0.073217034 Test MSE 0.008538197782404828 Test RE 0.06695002954948803\n",
      "49 Train Loss 0.06908601 Test MSE 0.008763524741014538 Test RE 0.067827697745831\n",
      "50 Train Loss 0.06795911 Test MSE 0.008864008831348715 Test RE 0.0682154514576981\n",
      "51 Train Loss 0.06679256 Test MSE 0.00896212397806689 Test RE 0.06859194870313023\n",
      "52 Train Loss 0.06422347 Test MSE 0.00885880895727067 Test RE 0.06819543998203757\n",
      "53 Train Loss 0.0628515 Test MSE 0.009703223661528664 Test RE 0.0713716420482666\n",
      "54 Train Loss 0.061311882 Test MSE 0.01019317726296823 Test RE 0.07315136863988289\n",
      "55 Train Loss 0.060525775 Test MSE 0.01063381823984885 Test RE 0.07471577129336401\n",
      "56 Train Loss 0.059407383 Test MSE 0.01058001453188361 Test RE 0.0745265126793627\n",
      "57 Train Loss 0.05709789 Test MSE 0.010386487063811503 Test RE 0.07384175497997943\n",
      "58 Train Loss 0.05592517 Test MSE 0.010705358945143347 Test RE 0.07496668111552805\n",
      "59 Train Loss 0.054762688 Test MSE 0.010465465718447264 Test RE 0.07412196896967277\n",
      "60 Train Loss 0.05245763 Test MSE 0.0101259610557573 Test RE 0.07290978104871995\n",
      "61 Train Loss 0.049956 Test MSE 0.009211405484693064 Test RE 0.06953934854011493\n",
      "62 Train Loss 0.04853277 Test MSE 0.008450904562316234 Test RE 0.06660690688882424\n",
      "63 Train Loss 0.047343124 Test MSE 0.007982141176015123 Test RE 0.06473324377067223\n",
      "64 Train Loss 0.04601166 Test MSE 0.0076026512066302825 Test RE 0.06317572010031\n",
      "65 Train Loss 0.04384471 Test MSE 0.0070574601673300385 Test RE 0.06086839993379294\n",
      "66 Train Loss 0.042115394 Test MSE 0.007386631678593394 Test RE 0.06227172405478041\n",
      "67 Train Loss 0.041247204 Test MSE 0.007179584137032588 Test RE 0.06139278179288936\n",
      "68 Train Loss 0.040450335 Test MSE 0.007208322936475242 Test RE 0.0615155321306835\n",
      "69 Train Loss 0.039001383 Test MSE 0.00702461773889046 Test RE 0.060726606912623576\n",
      "70 Train Loss 0.037667114 Test MSE 0.007100969902077911 Test RE 0.06105574060336726\n",
      "71 Train Loss 0.03710332 Test MSE 0.007290804945774553 Test RE 0.06186648008763604\n",
      "72 Train Loss 0.036212694 Test MSE 0.0071548657369742445 Test RE 0.061287006880825466\n",
      "73 Train Loss 0.035501167 Test MSE 0.007168206032183907 Test RE 0.0613441152913962\n",
      "74 Train Loss 0.034593705 Test MSE 0.007047018921706569 Test RE 0.06082335701902973\n",
      "75 Train Loss 0.034166258 Test MSE 0.007086158991184553 Test RE 0.060992033590678375\n",
      "76 Train Loss 0.03348971 Test MSE 0.007111913617483761 Test RE 0.06110277075664461\n",
      "77 Train Loss 0.032611176 Test MSE 0.006843359430146041 Test RE 0.05993801320252774\n",
      "78 Train Loss 0.032008894 Test MSE 0.006845847137532387 Test RE 0.05994890658692056\n",
      "79 Train Loss 0.031404953 Test MSE 0.007069500723604849 Test RE 0.060920300833882564\n",
      "80 Train Loss 0.030855834 Test MSE 0.007327884549111257 Test RE 0.062023600958587694\n",
      "81 Train Loss 0.030143574 Test MSE 0.007203431635581477 Test RE 0.0614946575083451\n",
      "82 Train Loss 0.029445978 Test MSE 0.0070463818154129 Test RE 0.06082060750041222\n",
      "83 Train Loss 0.028825998 Test MSE 0.006840506830340229 Test RE 0.05992551955901041\n",
      "84 Train Loss 0.02834475 Test MSE 0.006708438357383351 Test RE 0.059344214314151475\n",
      "85 Train Loss 0.027854009 Test MSE 0.006778806166776736 Test RE 0.05965464637153943\n",
      "86 Train Loss 0.027226582 Test MSE 0.006695171460775538 Test RE 0.05928550443598802\n",
      "87 Train Loss 0.026243621 Test MSE 0.007085494616402022 Test RE 0.060989174318191015\n",
      "88 Train Loss 0.025653152 Test MSE 0.007201395338910761 Test RE 0.061485965108426265\n",
      "89 Train Loss 0.025107775 Test MSE 0.007164309435782795 Test RE 0.06132743986724433\n",
      "90 Train Loss 0.02476638 Test MSE 0.007163109335681786 Test RE 0.06132230314402914\n",
      "91 Train Loss 0.02425127 Test MSE 0.00721856399739196 Test RE 0.0615592150176228\n",
      "92 Train Loss 0.023688229 Test MSE 0.007046516668524112 Test RE 0.06082118948767323\n",
      "93 Train Loss 0.023245532 Test MSE 0.006954921271867237 Test RE 0.060424598974790064\n",
      "94 Train Loss 0.02292639 Test MSE 0.00696429700104792 Test RE 0.060465313589189246\n",
      "95 Train Loss 0.021950524 Test MSE 0.006873166435329793 Test RE 0.06006840467985588\n",
      "96 Train Loss 0.020890774 Test MSE 0.006587535078388788 Test RE 0.05880701534722819\n",
      "97 Train Loss 0.02030544 Test MSE 0.0065112347781453375 Test RE 0.05846545652850686\n",
      "98 Train Loss 0.019543033 Test MSE 0.006238444713710728 Test RE 0.057227639536024784\n",
      "99 Train Loss 0.019290296 Test MSE 0.005971880809851791 Test RE 0.05599164581037023\n",
      "100 Train Loss 0.018876543 Test MSE 0.005791482360738934 Test RE 0.05513946355826929\n",
      "101 Train Loss 0.018460177 Test MSE 0.005357460375718087 Test RE 0.05303311677030065\n",
      "102 Train Loss 0.018131543 Test MSE 0.005136906560030468 Test RE 0.051930021291066615\n",
      "103 Train Loss 0.017870642 Test MSE 0.00507373496540475 Test RE 0.05160972635459964\n",
      "104 Train Loss 0.017554874 Test MSE 0.004850093513896665 Test RE 0.050459474551048905\n",
      "105 Train Loss 0.017081898 Test MSE 0.004535449720785941 Test RE 0.048795283551064154\n",
      "106 Train Loss 0.016761098 Test MSE 0.004348207452704511 Test RE 0.04777743118981471\n",
      "107 Train Loss 0.01629856 Test MSE 0.00417941940477393 Test RE 0.04684094465600819\n",
      "108 Train Loss 0.016022647 Test MSE 0.004044791611725345 Test RE 0.04608034728078142\n",
      "109 Train Loss 0.015503007 Test MSE 0.0035874766136490066 Test RE 0.04339724969874844\n",
      "110 Train Loss 0.015036798 Test MSE 0.0032827423536995293 Test RE 0.04151318614699297\n",
      "111 Train Loss 0.014663039 Test MSE 0.0031267725242297498 Test RE 0.0405149970079165\n",
      "112 Train Loss 0.014453645 Test MSE 0.00293518908820164 Test RE 0.03925416193763178\n",
      "113 Train Loss 0.014229023 Test MSE 0.0026794314444947166 Test RE 0.037504984690068036\n",
      "114 Train Loss 0.014002827 Test MSE 0.0025820897222884774 Test RE 0.0368174183335432\n",
      "115 Train Loss 0.013865773 Test MSE 0.0025809254615621617 Test RE 0.036809116935865076\n",
      "116 Train Loss 0.013738504 Test MSE 0.0024614875950940587 Test RE 0.03594731798343307\n",
      "117 Train Loss 0.013621182 Test MSE 0.0024549570716165627 Test RE 0.03589960075803993\n",
      "118 Train Loss 0.01339826 Test MSE 0.002496501218187151 Test RE 0.03620208292168436\n",
      "119 Train Loss 0.013242097 Test MSE 0.002438333287012296 Test RE 0.03577784690718538\n",
      "120 Train Loss 0.0131854685 Test MSE 0.002433643197858876 Test RE 0.035743421330301256\n",
      "121 Train Loss 0.013126143 Test MSE 0.00235273229975384 Test RE 0.03514422130506758\n",
      "122 Train Loss 0.013033427 Test MSE 0.0023029029409647 Test RE 0.03477006358194152\n",
      "123 Train Loss 0.012917718 Test MSE 0.002269396540740388 Test RE 0.034516190869833936\n",
      "124 Train Loss 0.012707354 Test MSE 0.002194072125466602 Test RE 0.03393853704176045\n",
      "125 Train Loss 0.012478204 Test MSE 0.002120576819644157 Test RE 0.03336527222088632\n",
      "126 Train Loss 0.012339914 Test MSE 0.00201533045785198 Test RE 0.03252675974884341\n",
      "127 Train Loss 0.012226695 Test MSE 0.0019267520164376315 Test RE 0.0318039146084117\n",
      "128 Train Loss 0.012113257 Test MSE 0.001817902218816317 Test RE 0.030892490906320874\n",
      "129 Train Loss 0.011959789 Test MSE 0.0017217100719383012 Test RE 0.030064063336189627\n",
      "130 Train Loss 0.011864745 Test MSE 0.0016440187646495608 Test RE 0.029377920602438715\n",
      "131 Train Loss 0.011696847 Test MSE 0.001618836253193722 Test RE 0.029152051900806747\n",
      "132 Train Loss 0.011306971 Test MSE 0.0016104690985978942 Test RE 0.029076616314281364\n",
      "133 Train Loss 0.011065922 Test MSE 0.001529830186521226 Test RE 0.02833931057412381\n",
      "134 Train Loss 0.010898671 Test MSE 0.0015153238705781266 Test RE 0.02820462955334117\n",
      "135 Train Loss 0.010659123 Test MSE 0.001740681253531332 Test RE 0.030229244531786423\n",
      "136 Train Loss 0.010532611 Test MSE 0.0016652127865245117 Test RE 0.029566678311039228\n",
      "137 Train Loss 0.010360258 Test MSE 0.0015843642972821165 Test RE 0.02883999569932889\n",
      "138 Train Loss 0.010114735 Test MSE 0.001377499682055298 Test RE 0.026891400530230775\n",
      "139 Train Loss 0.009999578 Test MSE 0.0013194350884631028 Test RE 0.026318533263836246\n",
      "140 Train Loss 0.009886886 Test MSE 0.0011640632876061489 Test RE 0.02472042676272331\n",
      "141 Train Loss 0.00981643 Test MSE 0.0011440522138933757 Test RE 0.024507024817737623\n",
      "142 Train Loss 0.009722896 Test MSE 0.0010410444363367847 Test RE 0.02337772800481321\n",
      "143 Train Loss 0.009641511 Test MSE 0.0010390841769569814 Test RE 0.0233557078096053\n",
      "144 Train Loss 0.0095215915 Test MSE 0.0010893141355849394 Test RE 0.02391356017500697\n",
      "145 Train Loss 0.009423391 Test MSE 0.0011307289146387383 Test RE 0.024363906076969827\n",
      "146 Train Loss 0.009302866 Test MSE 0.0010911523931285167 Test RE 0.023933729174406084\n",
      "147 Train Loss 0.009233766 Test MSE 0.0010113523670695424 Test RE 0.023041933308136045\n",
      "148 Train Loss 0.009088458 Test MSE 0.0009238715304971056 Test RE 0.02202284690900814\n",
      "149 Train Loss 0.008956142 Test MSE 0.0008434474853806464 Test RE 0.021042468781847908\n",
      "150 Train Loss 0.008864491 Test MSE 0.0007935209171664144 Test RE 0.020410181053974776\n",
      "151 Train Loss 0.008820069 Test MSE 0.0007238054506791517 Test RE 0.019492995937817927\n",
      "152 Train Loss 0.008777342 Test MSE 0.0007142246539708677 Test RE 0.019363554682611108\n",
      "153 Train Loss 0.008696381 Test MSE 0.0007152938666963691 Test RE 0.019378043132597857\n",
      "154 Train Loss 0.008643286 Test MSE 0.0007160421047214198 Test RE 0.019388175750521486\n",
      "155 Train Loss 0.008588831 Test MSE 0.0007179016608268172 Test RE 0.019413334901842973\n",
      "156 Train Loss 0.0084983185 Test MSE 0.0007218712652432856 Test RE 0.019466933485208636\n",
      "157 Train Loss 0.008429871 Test MSE 0.0006815941007011407 Test RE 0.018916055479197346\n",
      "158 Train Loss 0.008286338 Test MSE 0.0006928370134619782 Test RE 0.01907142780045908\n",
      "159 Train Loss 0.008166319 Test MSE 0.0006648540541548616 Test RE 0.018682320933661636\n",
      "160 Train Loss 0.0080679245 Test MSE 0.0006984392063545987 Test RE 0.01914837714541837\n",
      "161 Train Loss 0.007942559 Test MSE 0.0007271721624017196 Test RE 0.01953827824404325\n",
      "162 Train Loss 0.007810461 Test MSE 0.0007407220494401345 Test RE 0.019719472986187023\n",
      "163 Train Loss 0.0074953716 Test MSE 0.0007269079074440009 Test RE 0.019534727808320468\n",
      "164 Train Loss 0.0073498692 Test MSE 0.0007234629382579227 Test RE 0.019488383245778205\n",
      "165 Train Loss 0.0072233733 Test MSE 0.0006565503493843476 Test RE 0.018565287798708627\n",
      "166 Train Loss 0.0071081095 Test MSE 0.0006402666641128036 Test RE 0.018333615248284203\n",
      "167 Train Loss 0.00704738 Test MSE 0.0006315604044534587 Test RE 0.018208539589965534\n",
      "168 Train Loss 0.0070044897 Test MSE 0.0006235194248835028 Test RE 0.01809225338917703\n",
      "169 Train Loss 0.006961243 Test MSE 0.0006110699073461652 Test RE 0.017910722952848655\n",
      "170 Train Loss 0.006901756 Test MSE 0.0006095461448209427 Test RE 0.017888377945366234\n",
      "171 Train Loss 0.0068622944 Test MSE 0.0005940807852416155 Test RE 0.017659988669826158\n",
      "172 Train Loss 0.006819437 Test MSE 0.000596958174929004 Test RE 0.01770270448369463\n",
      "173 Train Loss 0.0067566494 Test MSE 0.0005747674353187039 Test RE 0.01737055700542736\n",
      "174 Train Loss 0.0067068157 Test MSE 0.0005502636811170406 Test RE 0.01699624928257987\n",
      "175 Train Loss 0.006628003 Test MSE 0.0005407460536680227 Test RE 0.01684862044860584\n",
      "176 Train Loss 0.006550775 Test MSE 0.000526917757844648 Test RE 0.016631793540209917\n",
      "177 Train Loss 0.0064630103 Test MSE 0.0005367900892940858 Test RE 0.016786877146803967\n",
      "178 Train Loss 0.0063977772 Test MSE 0.0005553382049419797 Test RE 0.01707443901445646\n",
      "179 Train Loss 0.0063374406 Test MSE 0.0005503987209154164 Test RE 0.016998334673055657\n",
      "180 Train Loss 0.0062713786 Test MSE 0.000548486090151065 Test RE 0.016968774437801745\n",
      "181 Train Loss 0.0061769728 Test MSE 0.0005008397464958191 Test RE 0.016215004038544072\n",
      "182 Train Loss 0.0060847094 Test MSE 0.0004984927785077728 Test RE 0.016176967137739286\n",
      "183 Train Loss 0.006041992 Test MSE 0.0004963276705591047 Test RE 0.01614179812885156\n",
      "184 Train Loss 0.0059748 Test MSE 0.0005373618740330044 Test RE 0.01679581539441823\n",
      "185 Train Loss 0.0058948635 Test MSE 0.0006104571823849989 Test RE 0.0179017410840874\n",
      "186 Train Loss 0.0058191577 Test MSE 0.0006353069037202306 Test RE 0.018262467455583565\n",
      "187 Train Loss 0.005769846 Test MSE 0.0006333331841223011 Test RE 0.01823407721718729\n",
      "188 Train Loss 0.0057236804 Test MSE 0.0006548039898258469 Test RE 0.018540580435313418\n",
      "189 Train Loss 0.0056872456 Test MSE 0.0006861923665345976 Test RE 0.018979755299478124\n",
      "190 Train Loss 0.0056504137 Test MSE 0.0006819416032528233 Test RE 0.01892087692638577\n",
      "191 Train Loss 0.005633221 Test MSE 0.0006900325112382293 Test RE 0.019032789494346756\n",
      "192 Train Loss 0.0056042317 Test MSE 0.0007268041856096957 Test RE 0.019533334062246367\n",
      "193 Train Loss 0.005583808 Test MSE 0.0007093436703792444 Test RE 0.01929727636167035\n",
      "194 Train Loss 0.0055500986 Test MSE 0.000717928898151315 Test RE 0.019413703171144932\n",
      "195 Train Loss 0.005515354 Test MSE 0.0007022597849921104 Test RE 0.019200678123269702\n",
      "196 Train Loss 0.005467554 Test MSE 0.0007165112094804114 Test RE 0.01939452565390824\n",
      "197 Train Loss 0.005398914 Test MSE 0.000701441761470416 Test RE 0.019189491961625612\n",
      "198 Train Loss 0.0053818775 Test MSE 0.0007107733691690376 Test RE 0.01931671362888482\n",
      "199 Train Loss 0.0053431746 Test MSE 0.0006717750237977937 Test RE 0.018779308400686458\n",
      "200 Train Loss 0.0052913222 Test MSE 0.0006401429545074469 Test RE 0.018331843991087553\n",
      "201 Train Loss 0.0052190293 Test MSE 0.0006217872568707693 Test RE 0.01806710532094137\n",
      "202 Train Loss 0.0051578847 Test MSE 0.0005981481767805206 Test RE 0.017720340361598096\n",
      "203 Train Loss 0.005130898 Test MSE 0.0006113623035390813 Test RE 0.01791500756972548\n",
      "204 Train Loss 0.005090943 Test MSE 0.000566653506834653 Test RE 0.017247512090481695\n",
      "205 Train Loss 0.005032881 Test MSE 0.0005556974319492304 Test RE 0.01707996052188753\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10 #10\n",
    "max_iter = 300 #200\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "n_val = 5.0\n",
    "rowdy_terms = 6\n",
    "\n",
    "N_I = 1000  #Total number of data points for 'y'\n",
    "N_B = 5000\n",
    "N_f = 10000 #Total number of collocation points\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(label)\n",
    "    print(reps)\n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []\n",
    "    beta_val = []\n",
    "\n",
    "    torch.manual_seed(reps*36)\n",
    "\n",
    "    layers = np.array([2,50,50,50,50,1]) #9 hidden layers\n",
    "    # layers = np.array([2,50,50,50,50,50,50,50,1])\n",
    "\n",
    "    PINN = Sequentialmodel(layers,rowdy_terms,n_val)\n",
    "\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=1, \n",
    "                            max_iter = 20, \n",
    "                            max_eval = 30, \n",
    "                            tolerance_grad = 1e-8, \n",
    "                            tolerance_change = 1e-8, \n",
    "                            history_size = 100, \n",
    "                            line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    nan_flag = train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "\n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    #elapsed_time[reps] = time.time() - start_time\n",
    "    beta_full.append(beta_val)\n",
    "\n",
    "\n",
    "  #print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"beta\": beta_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred = PINN.test()\n",
    "plt.imshow(u_pred.reshape(500,500),cmap = 'jet')\n",
    "plt.figure()\n",
    "plt.imshow(y_true.reshape(500,500),cmap = 'jet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "executionInfo": {
     "elapsed": 6,
     "status": "ok",
     "timestamp": 1660688534316,
     "user": {
      "displayName": "Raghav Gnanasambandam",
      "userId": "17884362014649498321"
     },
     "user_tz": 240
    },
    "id": "06syezgfv_qO",
    "outputId": "9f4852d5-694a-4977-8893-a6183a2ce493"
   },
   "outputs": [],
   "source": [
    "a = np.ones((10,1))\n",
    "for i in range(10):\n",
    "    a[i] = test_re_full[i][-1]    \n",
    "print(\"a = \",np.nanmean(a))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "stan_2D_KG_16Aug2022_tune.ipynb",
   "version": ""
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
