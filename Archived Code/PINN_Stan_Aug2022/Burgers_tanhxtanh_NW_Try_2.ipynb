{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "\n",
    "from torch.nn.parameter import Parameter\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat('burgers_shock_10sin.mat')\n",
    "label = \"QCRE_2D_5_tanhxtanh_NW\"\n",
    "                     \n",
    "x_test = data['x']                                   # 256 points between -1 and 1 [256x1]\n",
    "t_test = data['t']   \n",
    "usol = data['usol']\n",
    "X_test, T_test = np.meshgrid(x_test,t_test)  \n",
    "\n",
    "xt_test_tensor = torch.from_numpy(np.hstack((X_test.flatten()[:,None], T_test.flatten()[:,None]))).float().to(device)\n",
    "\n",
    "u_true = usol.flatten('F')[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = scipy.io.loadmat('burgers_shock_10sin.mat')  \t# Load data from file\n",
    "\n",
    "pi = torch.from_numpy(np.array(np.pi)).double().to(device)\n",
    "\n",
    "label = \"QCRE_2D_5_tanhxtanh_NW\"\n",
    "# x = data['x']                                   # 256 points between -1 and 1 [256x1]\n",
    "# t = data['t']                                   # 100 time points between 0 and 0.2 [100x1] \n",
    "# usol = data['usol']   \n",
    "\n",
    "#usol = usol/1000# solution of 256x100 grid points\n",
    "\n",
    "x = np.linspace(-1,1,500).reshape(-1,1)\n",
    "t = np.linspace(0,0.2,500).reshape(-1,1)\n",
    "\n",
    "X,T = np.meshgrid(x,t)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "T = T.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xt = np.hstack((X,T))\n",
    "\n",
    "#y_true = true_2D_1(xt)\n",
    "\n",
    "bound_pts_1 = (X==-1).reshape(-1,)\n",
    "xt_bound_1 = xt[bound_pts_1,:]\n",
    "u_bound_1 = np.zeros((np.shape(xt_bound_1)[0],1))\n",
    "\n",
    "bound_pts_2 = (X==1).reshape(-1,)\n",
    "xt_bound_2 = xt[bound_pts_2,:]\n",
    "u_bound_2 = np.zeros((np.shape(xt_bound_2)[0],1))\n",
    "\n",
    "bound_pts_3 = (T==0).reshape(-1,)\n",
    "xt_bound_3 = xt[bound_pts_3,:]\n",
    "u_bound_3 = -10*np.sin(np.pi*xt_bound_3[:,0].reshape(-1,1))\n",
    "#u_bound_3 = -10*np.ones((np.shape(bound_pts_3)[0],1))\n",
    "\n",
    "\n",
    "xt_bound = np.vstack((xt_bound_1,xt_bound_2,xt_bound_3))\n",
    "u_bound = np.vstack((u_bound_1,u_bound_2,u_bound_3))\n",
    "\n",
    "\n",
    "lb_xt = xt[0]\n",
    "ub_xt = xt[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fea90155450>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(xt_bound_3[:,0],u_bound_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_u,N_f,seed):\n",
    "\n",
    "    '''Boundary Conditions'''\n",
    "    #Initial Condition -1 =< x =<1 and t = 0  \n",
    "#     leftedge_x = np.hstack((X[0,:][:,None], T[0,:][:,None])) #L1\n",
    "#     leftedge_u = usol[:,0][:,None]\n",
    "\n",
    "#     #Boundary Condition x = -1 and 0 =< t =<1\n",
    "#     bottomedge_x = np.hstack((X[:,0][:,None], T[:,0][:,None])) #L2\n",
    "#     bottomedge_u = usol[-1,:][:,None]\n",
    "\n",
    "#     #Boundary Condition x = 1 and 0 =< t =<1\n",
    "#     topedge_x = np.hstack((X[:,-1][:,None], T[:,0][:,None])) #L3\n",
    "#     topedge_u = usol[0,:][:,None]\n",
    "\n",
    "#     all_X_u_train = np.vstack([leftedge_x, bottomedge_x, topedge_x]) # X_u_train [456,2] (456 = 256(L1)+100(L2)+100(L3))\n",
    "#     all_u_train = np.vstack([leftedge_u, bottomedge_u, topedge_u])   #corresponding u [456x1]\n",
    "\n",
    "    #choose random N_u points for training\n",
    "    np.random.seed(seed)\n",
    "    idx = np.random.choice(xt_bound.shape[0], N_u, replace=True) \n",
    "\n",
    "    X_u_train = xt_bound[idx, :] #choose indices from  set 'idx' (x,t)\n",
    "    u_train = u_bound[idx,:]      #choose corresponding u\n",
    "\n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    samples = sampling(N_f)\n",
    "    X_f_train = lb_xt + (ub_xt-lb_xt)*samples \n",
    "    \n",
    "    X_f_train = np.vstack((X_f_train, X_u_train)) # append training points to collocation points \n",
    "\n",
    "    return X_f_train, X_u_train, u_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n   Fortran Style ('F') flatten,stacked column wise!\\n   u = [c1 \\n        c2\\n        .\\n        .\\n        cn]\\n\\n   u =  [25600x1] \\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X_u_test = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "X_u_test = xt_test_tensor\n",
    "# Domain bounds\n",
    "lb = X_u_test[0]  # [-1. 0.]\n",
    "ub = X_u_test[-1] # [1.  0.99]\n",
    "\n",
    "'''\n",
    "   Fortran Style ('F') flatten,stacked column wise!\n",
    "   u = [c1 \n",
    "        c2\n",
    "        .\n",
    "        .\n",
    "        cn]\n",
    "\n",
    "   u =  [25600x1] \n",
    "'''\n",
    "#u_true = usol.flatten('F')[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        self.iter = 0\n",
    "        \n",
    "        '''\n",
    "        Alternatively:\n",
    "        \n",
    "        *all layers are callable \n",
    "    \n",
    "        Simple linear Layers\n",
    "        self.fc1 = nn.Linear(2,50)\n",
    "        self.fc2 = nn.Linear(50,50)\n",
    "        self.fc3 = nn.Linear(50,50)\n",
    "        self.fc4 = nn.Linear(50,1)\n",
    "        \n",
    "        '''\n",
    "    \n",
    "        'Xavier Normal Initialization'\n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            # weights from a normal distribution with \n",
    "            # Recommended gain value for tanh = 5/3?\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "            \n",
    "        self.beta = Parameter(1*torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "        \n",
    "        self.train_loss = []\n",
    "        self.test_loss = []\n",
    "        self.beta_val = []\n",
    "        \n",
    "        self.W1 = Parameter(torch.tensor(0.0))\n",
    "        self.W1.requiresGrad = True\n",
    "        \n",
    "        self.W2 = Parameter(torch.tensor(0.0))\n",
    "        self.W2.requiresGrad = True\n",
    "        \n",
    "    'foward pass'\n",
    "    def forward(self,x):\n",
    "         if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "         u_b = torch.from_numpy(ub_xt).float().to(device)\n",
    "         l_b = torch.from_numpy(lb_xt).float().to(device)\n",
    "        \n",
    "            \n",
    "         #preprocessing input \n",
    "         x = (x - l_b)/(u_b - l_b) #feature scaling\n",
    "        \n",
    "         #convert to float\n",
    "         a = x.float()\n",
    "                        \n",
    "         '''     \n",
    "         Alternatively:\n",
    "        \n",
    "         a = self.activation(self.fc1(a))\n",
    "         a = self.activation(self.fc2(a))\n",
    "         a = self.activation(self.fc3(a))\n",
    "         a = self.fc4(a)\n",
    "         \n",
    "         '''\n",
    "        \n",
    "         for i in range(len(layers)-2):\n",
    "                z = self.linears[i](a)\n",
    "                a = self.activation(z) + self.beta[:,i]*z*self.activation(z)\n",
    "         \n",
    "         a = self.linears[-1](a)\n",
    "        \n",
    "         return a\n",
    "                        \n",
    "    def loss_BC(self,x,y):\n",
    "                \n",
    "        loss_u = self.loss_function(torch.sum(self.forward(x)), y)\n",
    "                \n",
    "        return loss_u\n",
    "    \n",
    "    def loss_BC_fake(self,x):\n",
    "        \n",
    "        loss_u = self.loss_function(self.forward(x)[:,0],self.forward(x)[:,1])\n",
    "        \n",
    "        return loss_u\n",
    "    \n",
    "    def loss_PDE(self, x_to_train_f,f_hat):\n",
    "        \n",
    "        nu = 0.01/pi\n",
    "                \n",
    "        x_1_f = x_to_train_f[:,[0]]\n",
    "        x_2_f = x_to_train_f[:,[1]]\n",
    "                        \n",
    "        g = x_to_train_f.clone()\n",
    "                        \n",
    "        g.requires_grad = True\n",
    "        \n",
    "        u = self.forward(g)[:,[0]]\n",
    "        \n",
    "                \n",
    "        #print(u.shape)\n",
    "        u_x_t = autograd.grad(u,g,torch.ones([x_to_train_f.shape[0], 1]).to(device), retain_graph=True, create_graph=True)[0]\n",
    "                                \n",
    "        u_xx_tt = autograd.grad(u_x_t,g,torch.ones(x_to_train_f.shape).to(device), create_graph=True)[0]\n",
    "                                                            \n",
    "        u_x = u_x_t[:,[0]]\n",
    "        \n",
    "        u_t = u_x_t[:,[1]]\n",
    "        \n",
    "        u_xx = u_xx_tt[:,[0]]\n",
    "                                        \n",
    "        f = u_t + (self.forward(g))*(u_x) - (nu)*u_xx \n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss_PDE_fake(self, x_to_train_f,f_hat):\n",
    "        \n",
    "        nu = 0.01/pi\n",
    "                \n",
    "        x_1_f = x_to_train_f[:,[0]]\n",
    "        x_2_f = x_to_train_f[:,[1]]\n",
    "                        \n",
    "        g = x_to_train_f.clone()\n",
    "                        \n",
    "        g.requires_grad = True\n",
    "        \n",
    "        u = self.forward(g)[:,[1]]\n",
    "                \n",
    "        u_x_t = autograd.grad(u,g,torch.ones([x_to_train_f.shape[0], 1]).to(device), retain_graph=True, create_graph=True)[0]\n",
    "                                \n",
    "        u_xx_tt = autograd.grad(u_x_t,g,torch.ones(x_to_train_f.shape).to(device), create_graph=True)[0]\n",
    "                                                            \n",
    "        u_x = u_x_t[:,[0]]\n",
    "        \n",
    "        u_t = u_x_t[:,[1]]\n",
    "        \n",
    "        u_xx = u_xx_tt[:,[0]]\n",
    "                                        \n",
    "        f = u_t + (self.forward(g))*(u_x) - (nu)*u_xx \n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f_fake\n",
    "    \n",
    "    def loss(self,x,y,x_to_train_f,f_hat):\n",
    "\n",
    "        loss_u = self.loss_BC(x,y)\n",
    "        loss_f = self.loss_PDE(x_to_train_f,f_hat)\n",
    "        \n",
    "        loss_u_fake = self.loss_BC_fake(x)\n",
    "        loss_f_fake = self.loss_PDE(x_to_train_f,f_hat)\n",
    "     \n",
    "        \n",
    "        loss_val = loss_u + loss_f + loss_u_fake + loss_f_fake\n",
    "        \n",
    "        return loss_val\n",
    "     \n",
    "    'callable for optimizer'                                       \n",
    "    def closure(self):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "#         X_f_train_np_array, X_u_train_np_array, u_train_np_array = trainingdata(N_u,N_f,self.iter*32)\n",
    "        \n",
    "#         X_f_train = torch.from_numpy(X_f_train_np_array).float().to(device)\n",
    "#         X_u_train = torch.from_numpy(X_u_train_np_array).float().to(device)\n",
    "#         u_train = torch.from_numpy(u_train_np_array).float().to(device)\n",
    "        \n",
    "#     #u = torch.from_numpy(u_true).float().to(device)\n",
    "#         f_hat = torch.zeros(X_f_train.shape[0],1).to(device)\n",
    "        \n",
    "\n",
    "        loss = self.loss(X_u_train, u_train, X_f_train,f_hat)\n",
    "        \n",
    "        self.train_loss.append(loss.cpu().detach().numpy())\n",
    "        \n",
    "        \n",
    "        u_pred = self.test(xt_test_tensor)\n",
    "        self.test_loss.append(np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1))))\n",
    "        self.beta_val.append(self.beta.cpu().detach().numpy())\n",
    "        \n",
    "        print(self.iter,\"Train Loss\",self.train_loss[-1],\"Test Loss\",self.test_loss[-1])\n",
    "#         print(self.iter,\"Train Loss\",self.train_loss[-1])\n",
    "        loss.backward()\n",
    "        self.iter += 1\n",
    "     \n",
    "        return loss        \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self,xt_test_tensor):\n",
    "        u_pred = self.forward(xt_test_tensor)[:,[0]]\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.0000,  0.0000],\n",
       "        [-0.9922,  0.0000],\n",
       "        [-0.9843,  0.0000],\n",
       "        ...,\n",
       "        [ 0.9843,  0.2000],\n",
       "        [ 0.9922,  0.2000],\n",
       "        [ 1.0000,  0.2000]], device='cuda:2')"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xt_test_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 65163.992 Test Loss 24.83290042880723\n",
      "1 Train Loss 15019.376 Test Loss 24.77008874393443\n",
      "2 Train Loss 229.10286 Test Loss 24.726353861248366\n",
      "3 Train Loss 36.496555 Test Loss 24.721491459576672\n",
      "4 Train Loss 36.08051 Test Loss 24.721241415301137\n",
      "5 Train Loss 36.07933 Test Loss 24.721222299285955\n",
      "6 Train Loss 36.074383 Test Loss 24.72113583762751\n",
      "7 Train Loss 36.012444 Test Loss 24.719821595862705\n",
      "8 Train Loss 35.878403 Test Loss 24.716520402965585\n",
      "9 Train Loss 35.490715 Test Loss 24.706338911206153\n",
      "10 Train Loss 34.345352 Test Loss 24.677057539651607\n",
      "11 Train Loss 25.653044 Test Loss 24.533120490514804\n",
      "12 Train Loss 32.831146 Test Loss 24.404078003862118\n",
      "13 Train Loss 23.225048 Test Loss 24.475461733133155\n",
      "14 Train Loss 23.032907 Test Loss 24.470645127465158\n",
      "15 Train Loss 21.851913 Test Loss 24.429072487587174\n",
      "16 Train Loss 23.740904 Test Loss 24.327782008323794\n",
      "17 Train Loss 20.011562 Test Loss 24.391794248849173\n",
      "18 Train Loss 19.07823 Test Loss 24.322825623185715\n",
      "19 Train Loss 18.660816 Test Loss 24.343317077413495\n",
      "20 Train Loss 18.364706 Test Loss 24.325339824623804\n",
      "21 Train Loss 18.125624 Test Loss 24.283415120191616\n",
      "22 Train Loss 17.958708 Test Loss 24.28952485419796\n",
      "23 Train Loss 17.892551 Test Loss 24.279899967453897\n",
      "24 Train Loss 17.858953 Test Loss 24.267259810066516\n",
      "25 Train Loss 17.855616 Test Loss 24.264908992831973\n",
      "26 Train Loss 17.855259 Test Loss 24.263917203848077\n",
      "27 Train Loss 17.85519 Test Loss 24.26377708800897\n",
      "28 Train Loss 17.854767 Test Loss 24.263303479363753\n",
      "29 Train Loss 17.85392 Test Loss 24.26297740331383\n",
      "30 Train Loss 17.851519 Test Loss 24.263110212717876\n",
      "31 Train Loss 17.845633 Test Loss 24.26508527543157\n",
      "32 Train Loss 17.826399 Test Loss 24.274133287940753\n",
      "33 Train Loss 18.935406 Test Loss 24.373255542646316\n",
      "34 Train Loss 17.75902 Test Loss 24.312786380833487\n",
      "35 Train Loss 17.747332 Test Loss 24.3123897214017\n",
      "36 Train Loss 17.857857 Test Loss 24.343527926652236\n",
      "37 Train Loss 17.734993 Test Loss 24.32218855844374\n",
      "38 Train Loss 17.726494 Test Loss 24.321472886765616\n",
      "39 Train Loss 17.71724 Test Loss 24.32565147821292\n",
      "40 Train Loss 17.705004 Test Loss 24.334675378483617\n",
      "41 Train Loss 17.689049 Test Loss 24.345744630016394\n",
      "42 Train Loss 17.675325 Test Loss 24.357660918672863\n",
      "43 Train Loss 17.652985 Test Loss 24.37570379138638\n",
      "44 Train Loss 17.95923 Test Loss 24.432101357967262\n",
      "45 Train Loss 17.6368 Test Loss 24.39441504055317\n",
      "46 Train Loss 17.621084 Test Loss 24.395909446008964\n",
      "47 Train Loss 17.609892 Test Loss 24.414987481294975\n",
      "48 Train Loss 17.60334 Test Loss 24.41211676724228\n",
      "49 Train Loss 17.591818 Test Loss 24.42127055209015\n",
      "50 Train Loss 17.583563 Test Loss 24.443013610788565\n",
      "51 Train Loss 17.57768 Test Loss 24.449411925889855\n",
      "52 Train Loss 17.57305 Test Loss 24.45789166494696\n",
      "53 Train Loss 17.569494 Test Loss 24.482599059605104\n",
      "54 Train Loss 17.567276 Test Loss 24.47428431446972\n",
      "55 Train Loss 17.562452 Test Loss 24.478644107206666\n",
      "56 Train Loss 17.558826 Test Loss 24.492798117956227\n",
      "57 Train Loss 17.557234 Test Loss 24.50133033174018\n",
      "58 Train Loss 17.556538 Test Loss 24.51446320993164\n",
      "59 Train Loss 17.556297 Test Loss 24.510488234227097\n",
      "60 Train Loss 17.556238 Test Loss 24.512569026239433\n",
      "61 Train Loss 17.556211 Test Loss 24.514469410128175\n",
      "62 Train Loss 17.555656 Test Loss 24.515030933493275\n",
      "63 Train Loss 17.553211 Test Loss 24.51755769848338\n",
      "64 Train Loss 17.541885 Test Loss 24.531361342359983\n",
      "65 Train Loss 17.559055 Test Loss 24.53829635888021\n",
      "66 Train Loss 17.525799 Test Loss 24.534899347003922\n",
      "67 Train Loss 17.518726 Test Loss 24.493897165442668\n",
      "68 Train Loss 17.513136 Test Loss 24.51284895945054\n",
      "69 Train Loss 17.510988 Test Loss 24.534001925065894\n",
      "70 Train Loss 17.508247 Test Loss 24.53779601939155\n",
      "71 Train Loss 17.505365 Test Loss 24.544496838919407\n",
      "72 Train Loss 17.503778 Test Loss 24.55496666561354\n",
      "73 Train Loss 17.503195 Test Loss 24.556729830104825\n",
      "74 Train Loss 17.503098 Test Loss 24.558644683233048\n",
      "75 Train Loss 17.503077 Test Loss 24.559689917647948\n",
      "76 Train Loss 17.503069 Test Loss 24.560578929212205\n",
      "77 Train Loss 17.503069 Test Loss 24.560640663616105\n",
      "78 Train Loss 17.502617 Test Loss 24.560654885331722\n",
      "79 Train Loss 17.50067 Test Loss 24.560720880278218\n",
      "80 Train Loss 17.495394 Test Loss 24.561276840166233\n",
      "81 Train Loss 17.493713 Test Loss 24.546013708044256\n",
      "82 Train Loss 17.490934 Test Loss 24.5532555247052\n",
      "83 Train Loss 17.490374 Test Loss 24.56447575966313\n",
      "84 Train Loss 17.489256 Test Loss 24.560087676539307\n",
      "85 Train Loss 17.48807 Test Loss 24.569281736005777\n",
      "86 Train Loss 17.486965 Test Loss 24.569448930467253\n",
      "87 Train Loss 17.486277 Test Loss 24.565873081565265\n",
      "88 Train Loss 17.486294 Test Loss 24.564426874176224\n",
      "89 Train Loss 17.486092 Test Loss 24.565115062606832\n",
      "90 Train Loss 17.486 Test Loss 24.56551557216103\n",
      "91 Train Loss 17.485987 Test Loss 24.566547972115412\n",
      "92 Train Loss 17.485983 Test Loss 24.56676094148525\n",
      "93 Train Loss 17.485985 Test Loss 24.56672595984084\n",
      "94 Train Loss 17.485981 Test Loss 24.56676069307496\n",
      "95 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "96 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "97 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "98 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "99 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "100 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "101 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "102 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "103 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "104 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "105 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "106 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "107 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "108 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "109 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "110 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "111 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "112 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "113 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "114 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "115 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "116 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "117 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "118 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "119 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "120 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "121 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "122 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "123 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "124 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "125 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "126 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "127 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "128 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "129 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "130 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "131 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "132 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "133 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "134 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "135 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "136 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "137 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "138 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "139 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "140 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "141 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "142 Train Loss 17.485981 Test Loss 24.566760695642692\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "143 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "144 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "145 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "146 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "147 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "148 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "149 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "150 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "151 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "152 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "153 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "154 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "155 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "156 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "157 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "158 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "159 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "160 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "161 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "162 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "163 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "164 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "165 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "166 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "167 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "168 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "169 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "170 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "171 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "172 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "173 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "174 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "175 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "176 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "177 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "178 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "179 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "180 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "181 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "182 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "183 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "184 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "185 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "186 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "187 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "188 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "189 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "190 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "191 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "192 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "193 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "194 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "195 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "196 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "197 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "198 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "199 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "200 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "201 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "202 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "203 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "204 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "205 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "206 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "207 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "208 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "209 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "210 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "211 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "212 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "213 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "214 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "215 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "216 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "217 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "218 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "219 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "220 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "221 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "222 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "223 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "224 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "225 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "226 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "227 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "228 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "229 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "230 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "231 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "232 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "233 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "234 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "235 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "236 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "237 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "238 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "239 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "240 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "241 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "242 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "243 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "244 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "245 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "246 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "247 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "248 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "249 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "250 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "251 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "252 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "253 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "254 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "255 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "256 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "257 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "258 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "259 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "260 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "261 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "262 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "263 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "264 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "265 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "266 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "267 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "268 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "269 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "270 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "271 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "272 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "273 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "274 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "275 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "276 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "277 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "278 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "279 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "280 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "281 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "282 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "283 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "284 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "285 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "286 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "287 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "288 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "289 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "290 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "291 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "292 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "293 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "294 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "295 Train Loss 17.485994 Test Loss 24.566217788818204\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "296 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "297 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "298 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "299 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "300 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "301 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "302 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "303 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "304 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "305 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "306 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "307 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "308 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "309 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "310 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "311 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "312 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "313 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "314 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "315 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "316 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "317 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "318 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "319 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "320 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "321 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "322 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "323 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "324 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "325 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "326 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "327 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "328 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "329 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "330 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "331 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "332 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "333 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "334 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "335 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "336 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "337 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "338 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "339 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "340 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "341 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "342 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "343 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "344 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "345 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "346 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "347 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "348 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "349 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "350 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "351 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "352 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "353 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "354 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "355 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "356 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "357 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "358 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "359 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "360 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "361 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "362 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "363 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "364 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "365 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "366 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "367 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "368 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "369 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "370 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "371 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "372 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "373 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "374 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "375 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "376 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "377 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "378 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "379 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "380 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "381 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "382 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "383 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "384 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "385 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "386 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "387 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "388 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "389 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "390 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "391 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "392 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "393 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "394 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "395 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "396 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "397 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "398 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "399 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "400 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "401 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "402 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "403 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "404 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "405 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "406 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "407 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "408 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "409 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "410 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "411 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "412 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "413 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "414 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "415 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "416 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "417 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "418 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "419 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "420 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "421 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "422 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "423 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "424 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "425 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "426 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "427 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "428 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "429 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "430 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "431 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "432 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "433 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "434 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "435 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "436 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "437 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "438 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "439 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "440 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "441 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "442 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "443 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "444 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "445 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "446 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "447 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "448 Train Loss 17.485987 Test Loss 24.566731359246273\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "449 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "450 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "451 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "452 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "453 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "454 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "455 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "456 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "457 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "458 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "459 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "460 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "461 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "462 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "463 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "464 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "465 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "466 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "467 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "468 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "469 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "470 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "471 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "472 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "473 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "474 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "475 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "476 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "477 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "478 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "479 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "480 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "481 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "482 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "483 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "484 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "485 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "486 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "487 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "488 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "489 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "490 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "491 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "492 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "493 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "494 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "495 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "496 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "497 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "498 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "499 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "500 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "501 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "502 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "503 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "504 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "505 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "506 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "507 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "508 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "509 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "510 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "511 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "512 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "513 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "514 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "515 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "516 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "517 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "518 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "519 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "520 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "521 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "522 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "523 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "524 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "525 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "526 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "527 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "528 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "529 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "530 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "531 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "532 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "533 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "534 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "535 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "536 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "537 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "538 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "539 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "540 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "541 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "542 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "543 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "544 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "545 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "546 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "547 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "548 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "549 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "550 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "551 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "552 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "553 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "554 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "555 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "556 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "557 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "558 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "559 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "560 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "561 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "562 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "563 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "564 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "565 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "566 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "567 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "568 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "569 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "570 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "571 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "572 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "573 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "574 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "575 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "576 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "577 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "578 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "579 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "580 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "581 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "582 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "583 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "584 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "585 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "586 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "587 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "588 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "589 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "590 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "591 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "592 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "593 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "594 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "595 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "596 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "597 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "598 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "599 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "600 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "601 Train Loss 17.485983 Test Loss 24.566757729965843\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "602 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "603 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "604 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "605 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "606 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "607 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "608 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "609 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "610 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "611 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "612 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "613 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "614 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "615 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "616 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "617 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "618 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "619 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "620 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "621 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "622 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "623 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "624 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "625 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "626 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "1\n",
      "627 Train Loss 17.485981 Test Loss 24.56676069307496\n",
      "628 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "629 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "630 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "631 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "632 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "633 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "634 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "635 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "636 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "637 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "638 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "639 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "640 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "641 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "642 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "643 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "644 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "645 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "646 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "647 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "648 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "649 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "650 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "651 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "652 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "653 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "654 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "655 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "656 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "657 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "658 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "659 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "660 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "661 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "662 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "663 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "664 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "665 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "666 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "667 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "668 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "669 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "670 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "671 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "672 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "673 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "674 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "675 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "676 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "677 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "678 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "679 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "680 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "681 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "682 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "683 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "684 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "685 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "686 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "687 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "688 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "689 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "690 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "691 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "692 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "693 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "694 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "695 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "696 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "697 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "698 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "699 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "700 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "701 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "702 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "703 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "704 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "705 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "706 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "707 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "708 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "709 Train Loss 17.485987 Test Loss 24.566731359246273\n",
      "710 Train Loss 17.485983 Test Loss 24.566757729965843\n",
      "711 Train Loss 17.485981 Test Loss 24.566760695642692\n",
      "712 Train Loss 17.485994 Test Loss 24.566217788818204\n",
      "713 Train Loss 17.485987 Test Loss 24.566731359246273\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-eb8883fc2eac>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mopt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mopt\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPINN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPINN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                     loss, flat_grad, t, ls_func_evals = _strong_wolfe(\n\u001b[0;32m--> 426\u001b[0;31m                         obj_func, x_init, t, d, loss, flat_grad, gtd)\n\u001b[0m\u001b[1;32m    427\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0mopt_cond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflat_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mtolerance_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36m_strong_wolfe\u001b[0;34m(obj_func, x, t, d, f, g, gtd, c1, c2, tolerance_change, max_ls)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;31m# Evaluate new point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0mf_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mls_func_evals\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mgtd_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mobj_func\u001b[0;34m(x, t, d)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m                     \u001b[0;32mdef\u001b[0m \u001b[0mobj_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m                         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_directional_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                     loss, flat_grad, t, ls_func_evals = _strong_wolfe(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36m_directional_evaluate\u001b[0;34m(self, closure, x, t, d)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_directional_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0mflat_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gather_flat_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-acf282fc5e32>\u001b[0m in \u001b[0;36mclosure\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    179\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    180\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 181\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_u_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_f_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    182\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    183\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-acf282fc5e32>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, x, y, x_to_train_f, f_hat)\u001b[0m\n\u001b[1;32m    158\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    159\u001b[0m         \u001b[0mloss_u_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_BC_fake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 160\u001b[0;31m         \u001b[0mloss_f_fake\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_PDE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_to_train_f\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-acf282fc5e32>\u001b[0m in \u001b[0;36mloss_PDE\u001b[0;34m(self, x_to_train_f, f_hat)\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrequires_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 105\u001b[0;31m         \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    106\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    107\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-acf282fc5e32>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     76\u001b[0m                 \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 78\u001b[0;31m          \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinears\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     79\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     80\u001b[0m          \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/modules/linear.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     94\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     95\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 96\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     97\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     98\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mextra_repr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/nn/functional.py\u001b[0m in \u001b[0;36mlinear\u001b[0;34m(input, weight, bias)\u001b[0m\n\u001b[1;32m   1845\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhas_torch_function_variadic\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1846\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mhandle_torch_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1847\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_C\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_nn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinear\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mweight\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbias\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1848\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1849\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_reps = 1\n",
    "\n",
    "train_loss_full = []\n",
    "test_loss_full = []\n",
    "beta_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(reps)\n",
    "    N_u = 1000 #Total number of data points for 'u'\n",
    "    N_f = 10000 #Total number of collocation points \n",
    "    \n",
    "    X_f_train_np_array, X_u_train_np_array, u_train_np_array = trainingdata(N_u,N_f,reps*32)\n",
    "        \n",
    "    X_f_train = torch.from_numpy(X_f_train_np_array).float().to(device)\n",
    "    X_u_train = torch.from_numpy(X_u_train_np_array).float().to(device)\n",
    "    u_train = torch.from_numpy(u_train_np_array).float().to(device)\n",
    "        \n",
    "    #u = torch.from_numpy(u_true).float().to(device)\n",
    "    f_hat = torch.zeros(X_f_train.shape[0],1).to(device)\n",
    "        \n",
    "\n",
    "    #X_u_test_tensor = torch.from_numpy(X_u_test).float().to(device)\n",
    "    'Convert to tensor and send to GPU'\n",
    "\n",
    "\n",
    "    layers = np.array([2,50,50,50,50,50,50,50,50,50,2]) #9 hidden layers\n",
    "\n",
    "    #layers = np.array([2,512,512,1])\n",
    "    PINN = Sequentialmodel(layers)\n",
    "       \n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "    \n",
    "    \n",
    "    'L-BFGS Optimizer'\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=1, \n",
    "                                  max_iter = 500, \n",
    "                                  max_eval = None, \n",
    "                                  tolerance_grad = -1, \n",
    "                                  tolerance_change = -1, \n",
    "                                  history_size = 100, \n",
    "                                  line_search_fn = 'strong_wolfe')\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    for opt in range(10):\n",
    "        print(opt)\n",
    "        optimizer.step(PINN.closure)\n",
    "    \n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(PINN.train_loss)\n",
    "    test_loss_full.append(PINN.test_loss)\n",
    "    elapsed_time[reps] = time.time() - start_time\n",
    "    beta_full.append(PINN.beta_val)\n",
    "    \n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_loss\": test_loss_full,\"Time\": elapsed_time, \"beta\": beta_full, \"label\": label}\n",
    "savemat(label+'.mat', mdic) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINN.W2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred = PINN.test(xt_test_tensor)\n",
    "\n",
    "cmap = plt.cm.rainbow\n",
    "plt.imshow(u_pred.reshape(100,256),cmap = cmap,aspect =1,vmin=-10,vmax=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_u_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(u_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(u_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.cm.rainbow\n",
    "plt.imshow(u_true.reshape(100,256),cmap = cmap,aspect = 1,vmin=-10,vmax=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(u_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.cm.rainbow\n",
    "plt.imshow(np.transpose(np.abs(u_pred - u_true).reshape(100,256)),cmap = cmap,aspect = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0 \n",
    "for i in range(10):\n",
    "    print(test_loss_full[i][-1])\n",
    "    a = a + test_loss_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
