{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "\n",
    "from torch.nn.parameter import Parameter\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat('burgers_shock_10sin.mat')\n",
    "label = \"QCRE_2D_5_tanhxtanh_NW\"\n",
    "                     \n",
    "x_test = data['x']                                   # 256 points between -1 and 1 [256x1]\n",
    "t_test = data['t']   \n",
    "usol = data['usol']\n",
    "X_test, T_test = np.meshgrid(x_test,t_test)  \n",
    "\n",
    "xt_test_tensor = torch.from_numpy(np.hstack((X_test.flatten()[:,None], T_test.flatten()[:,None]))).float().to(device)\n",
    "\n",
    "u_true = usol.flatten('F')[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = scipy.io.loadmat('burgers_shock_10sin.mat')  \t# Load data from file\n",
    "\n",
    "pi = torch.from_numpy(np.array(np.pi)).double().to(device)\n",
    "\n",
    "label = \"QCRE_2D_5_tanhxtanh_NW\"\n",
    "# x = data['x']                                   # 256 points between -1 and 1 [256x1]\n",
    "# t = data['t']                                   # 100 time points between 0 and 0.2 [100x1] \n",
    "# usol = data['usol']   \n",
    "\n",
    "#usol = usol/1000# solution of 256x100 grid points\n",
    "\n",
    "x = np.linspace(-1,1,500).reshape(-1,1)\n",
    "t = np.linspace(0,0.2,500).reshape(-1,1)\n",
    "\n",
    "X,T = np.meshgrid(x,t)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "T = T.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xt = np.hstack((X,T))\n",
    "\n",
    "#y_true = true_2D_1(xt)\n",
    "\n",
    "bound_pts_1 = (X==-1).reshape(-1,)\n",
    "xt_bound_1 = xt[bound_pts_1,:]\n",
    "u_bound_1 = np.zeros((np.shape(xt_bound_1)[0],1))\n",
    "\n",
    "bound_pts_2 = (X==1).reshape(-1,)\n",
    "xt_bound_2 = xt[bound_pts_2,:]\n",
    "u_bound_2 = np.zeros((np.shape(xt_bound_2)[0],1))\n",
    "\n",
    "bound_pts_3 = (T==0).reshape(-1,)\n",
    "xt_bound_3 = xt[bound_pts_3,:]\n",
    "u_bound_3 = -10*np.sin(np.pi*xt_bound_3[:,0].reshape(-1,1))\n",
    "#u_bound_3 = -10*np.ones((np.shape(bound_pts_3)[0],1))\n",
    "\n",
    "\n",
    "xt_bound = np.vstack((xt_bound_1,xt_bound_2,xt_bound_3))\n",
    "u_bound = np.vstack((u_bound_1,u_bound_2,u_bound_3))\n",
    "\n",
    "\n",
    "lb_xt = xt[0]\n",
    "ub_xt = xt[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa178147150>]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAD4CAYAAADhNOGaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAAwDElEQVR4nO3dd3yV9fn/8deVTQaBkBBGgJCA7DIMKBsVBaxfcVJcgKKIq9a2rlpt9Vc7bK2KaJ11tHUvUFC2VRSEgGzIYAcChL0JSa7fHzl8v0dMIAnnPvcZ1/PxOI+cc9+fc99v7hzOlc+9PqKqGGOMCV8RbgcwxhjjLisExhgT5qwQGGNMmLNCYIwxYc4KgTHGhLkotwPURWpqqmZmZrodwxhjgsqiRYt2qmraydODshBkZmaSm5vrdgxjjAkqIrKxqum2a8gYY8KcFQJjjAlzVgiMMSbMWSEwxpgwZ4XAGGPCnE8KgYj8U0R2iMgKr2kpIjJDRAo8PxtW897RnjYFIjLaF3mMMcbUnK96BK8DQ0+a9gAwS1XbArM8r39ARFKA3wHnAL2A31VXMIwxxjjDJ9cRqOpXIpJ50uThwCDP8zeAL4H7T2ozBJihqrsBRGQGlQXlbV/kMrWz7/Bx8rYfYMvew+w9fJz9R8qIihRioyJISYihZUo8makJpCbGuh3VGONDTl5Qlq6qxZ7n24D0Kto0BzZ7vS7yTPsRERkHjANo2bKlD2OGrwNHj/NlXglfF5TwTeEutuw9UqP3tUipR8/MFAaelcbgDukkxAbldYnGGA+//A9WVRWRMxoBR1VfAl4CyMnJsdF06khVmb9uN+/lbubzFcUcPV5Bcr1o+rZpxKjerTirSRKZjRJoUC+apLgoylU5eryCnQePsWn3YQq3H2TRxj38N6+EjxZvITYqgsEd0hndJ5OemQ0REbf/icaYWnKyEGwXkaaqWiwiTYEdVbTZwv/tPgLIoHIXkvExVWXm6h1MnFPI0s17SYqL4soeGVzevTndWzYkMqLqL/AoIDYqkuR60WSnJXJeu8bcAlRUKIs27eGzpVuZtHQrU5YX06V5Mnec14YhndKtIBgTRMRXQ1V6jhF8pqqdPa//CuxS1T+LyANAiqred9J7UoBFQA/PpMXA2SeOGVQnJydH7V5DNbdk814e/XQl32/aS4uUeowfmM2VPTKIi470yfKPlJbz0fdFvPr1etbtPET3lg34zcUd6JmZ4pPlG2N8Q0QWqWrOj6b7ohCIyNtU/mWfCmyn8kygT4D3gJbARmCEqu4WkRxgvKre7HnvTcBvPIt6XFVfO936rBDUzL4jx3l8yireyy0iLSmWey9qxxU9mhMV6czlI2XlFXy4uIi/z8hn+/5jjOzZggeHdSA5PtqR9RljasfRQuBvVghO78u8HTzw4XJKDh7j5v6tuev8tiT66aDukdJynp6Vzytfr6dhfAxPXNWF89tXda6AMcafqisEdmVxiDleXsEfPlvFmNcWkhQXxce39+HBYR38VgQA6sVE8uCwDky6oy+piTHc9Hou/++zVRwrK/dbBmNMzdl5fyFk+/6j3PnWYhZu2MPo3q148OIOPjsOUBedmyfzyR19+dPU1bw6dz25G3bz0qgc0uvHuZbJGPNj1iMIEau27ufSiXNZuXU/E67pzqPDO7taBE6Ii47k0eGdeeH6synYcZBLJ85lWdFet2MZY7xYIQgBX+WXMOLFeUSI8NHtfbi0azO3I/3I0M5N+PC2PkRFRDDixXnMXLXd7UjGGA8rBEHug0VF3PT6QjIa1uPj2/vSvkl9tyNVq0PT+ky6sy/t0pO49d+L+OT7LW5HMsZghSCovfXdJn79/lLOzWrE++N70yQ58Pe9pybG8p9bzqVXZgq/eHcJb87b4HYkY8KeFYIg9e/5G/nNx8s5r10ar4zOISkueM7VT4yN4rUbe3Jhx3QembSSV+eudzuSMWHNCkEQ+tf8jfz2kxVc0L4xL9xwdkAcFK6tuOhI/nFdD4Z1bsL/+2wVb323ye1IxoQtKwRBZtKSLTz8yQoGd2jM89f3IDYq+IrACVGRETwzsjvntUvjoU+W8/H3RW5HMiYsWSEIIl/ll/Dr95fSq3UKE68N7iJwQkxUBP+4/mx6ZzXi1+8vY4adTWSM31khCBJLN+9l/L8XkZ2WyMujcoJyd1B14qIjeXlUDp2b1efnb39v1xkY42dWCIJA0Z7DjH1jIY0SY3jzpl4k1wueA8M1lRAbxSuje9LIc0uKoj2H3Y5kTNiwQhDgDh0r4+Y3cjlWVsFrY3rROIRvz5CWFMvrN/aktKycm15fyL4jx92OZExYsEIQwCoqlHveXUL+9gNMvLYHbRonuh3JcW0aJ/HCDWezfuchfvHO91RUBN/dcY0JNlYIAtjfZ+QzfdV2fvvTjgw8K83tOH7TJzuVR/6nE3PySnh6Zr7bcYwJeVYIAtSMVduZOKeQn+W04Ma+mW7H8bvrz2nJiJwMJswuZNrKbW7HMSakOVoIRKSdiCzxeuwXkV+c1GaQiOzzavOIk5mCwebdh/nVe0vo3Lw+jw7vFJbj/4oIjw3vTNeMZH713lIKdxx0O5IxIcvRQqCqearaTVW7AWcDh4GPq2j69Yl2qvqYk5kC3bGycu58azEKPH9tcF417Ctx0ZH84/qziY2K4M63FnP0uA1sY4wT/Llr6AJgrapu9OM6g84fp6xmadE+/npVV1o2inc7juuaNajH30Z0Zc22A/xx6mq34xgTkvxZCEYCb1czr7eILBWRz0WkU1UNRGSciOSKSG5JSYlzKV00ZVkxb8zbyM39WjO0cxO34wSM89o15pb+rXlz3ka+WGHHC4zxNb8UAhGJAS4F3q9i9mKglap2BZ4FPqlqGar6kqrmqGpOWlronUFTvO8ID360jG4tGnD/sPZuxwk49w5pz08ykrn/w2Vs2XvE7TjGhBR/9QiGAYtV9Uc3klHV/ap60PN8KhAtIql+yhUQKiqUX7+/lLIK5emfdSM60k7mOllMVAQTRnanrLyCu9/+nrLyCrcjGRMy/PWNcw3V7BYSkSbiOS1GRHp5Mu3yU66A8Nq3G/imcBcPX9KRzNQEt+MErMzUBB6/vAu5G/fw0tfr3I5jTMhwvBCISAJwIfCR17TxIjLe8/IqYIWILAUmACNVNWwuJ83bdoC/fLGGwR3SGdmzhdtxAt7wbs24uEsTnpqRz5pt+92OY0xIkGD8zs3JydHc3Fy3Y5yx0rIKhj/3DSUHjvLFLwaQmhjrdqSgsOvgMYY8/RXp9eP4+Pa+xETZrjRjakJEFqlqzsnT7X+QiybOKWR18X7+fMVPrAjUQqPEWB6/vAsrt+5n4pxCt+MYE/SsELhkdfF+np9TyOXdmzO4Y7rbcYLOkE5NuKJ7c56bU8jyon1uxzEmqFkhcEFZeQX3fbCMBvHRPHJJR7fjBK3f/U8nUhNj+NX7Sygts7OIjKkrKwQueHXuepZv2cejl3amYUKM23GCVnJ8NH+8vAv52w/y0ldr3Y5jTNCyQuBn60oO8vcZ+VzUMZ2Lu9jVw2fqgg7p/LRLUybMLmRdid2Yzpi6sELgR6rKAx8tJzYqgj9c1jks7yrqhN/9T0dioyL4zcfLCcaz4IxxmxUCP/pw8RYWrN/Nby7uENJDTvpb4/pxPDisA/PX7eb93CK34xgTdKwQ+Mnew6X8aepqerRswIgcu3DM10b2bEHPzIY8PnU1JQeOuR3HmKBihcBP/jotjz2HS/nDZV2IiLBdQr4WESH86YouHC4t40+f2+2qjakNKwR+sHTzXt5asIkxfVrTsVl9t+OErDaNk7i5fxYfLd5C7obdbscxJmhYIXBYeYXy209WkJYYyz0XtnU7Tsi76/w2NE2O4+FJKymvsAPHxtSEFQKHvfXdRpZv2cfDl3QkKS7a7TghLz4mit/+tCOri/fz1nc2GJ4xNWGFwEE7Dx7jiWl59GuTyiU/aep2nLBxcZcm9MluxF+n5bHroB04NuZ0rBA46Mnp+RwpLef3l3ayawb8SER49NJOHC4t56/T8tyOY0zAs0LgkFVb9/Puwk2M6p1Jm8aJbscJO23Tk7ixbybv5m5myea9bscxJqBZIXCAqvLYZytJrhfN3RfYAWK33D34LNISY/nd5JVU2IFjY6rljxHKNojIchFZIiI/Gk1GKk0QkUIRWSYiPZzO5LRpK7czf91ufnnhWSTH2wFityTGRnHvkHYs3byXT5dtdTuOMQHLXz2C81S1W1Uj41A5sH1bz2Mc8A8/ZXLEsbJy/jh1NWelJ3JNr5Zuxwl7V/bIoFOz+jzxRR5Hj5e7HceYgBQIu4aGA29qpflAAxEJ2lNsXvtmA5t2H+bhSzoSFRkImze8RUQID/20A1v2HuHVuevdjmNMQPLHN5UC00VkkYiMq2J+c2Cz1+siz7QfEJFxIpIrIrklJSUORT0zJQeOMXF2IRe0b0z/tmluxzEefbJTubBjOs/PKbT7EBlTBX8Ugn6q2oPKXUB3iMiAuixEVV9S1RxVzUlLC8wv2SenV+5+eOinHdyOYk7y4LD2HCur4KmZ+W5HMSbgOF4IVHWL5+cO4GOg10lNtgDet+PM8EwLKmu27efd3M2M6p1JVpqdLhpostISuaF3K95ZsIm8bQfcjmNMQHG0EIhIgogknXgOXASsOKnZZGCU5+yhc4F9qlrsZC4n/OXzNSTFRvHzC9q4HcVU4+4L2pIUF83jU+3upMZ4c7pHkA7MFZGlwAJgiqp+ISLjRWS8p81UYB1QCLwM3O5wJp+bt3YXc/JKuP28NjSItzGIA1WD+Bh+fkFbvsovYU7eDrfjGBMwopxcuKquA7pWMf0Fr+cK3OFkDiepKn/+Yg1Nk+MY0yfT7TjmNG44txVvztvAXz5fw4C2aUTa2BDGBMTpo0Ht8xXbWLp5L/dceBZx0ZFuxzGnERMVwa8uaseabQeYvDToDkUZ4wgrBGfgeHkFf52Wx1npiVzZI8PtOKaGLunSlE7N6vPk9HxKyyrcjmOM66wQnIF3F25m/c5D3Dekve1iCCIREcJ9Q9tTtOeIjVlgDFYI6uzQsTKenllAr8wULujQ2O04ppYGtE2ld1Yjnp1dyMFjZW7HMcZVVgjq6NW569l58Bj3D2tvYw0EIRHhvqHt2HWolFe/tltPmPBmhaAOdh08xov/XcvQTk04u1VDt+OYOuresiFDOqXz8tfrbCQzE9asENTBxDmFHC2r4N6h7dyOYs7QvUPacbi0jOfmrHU7ijGusUJQS1v3HuE/8zdxVY8Msu1WEkGvTeMkrjo7g3/P30jRnsNuxzHGFVYIamninEIU5S67lUTI+MXgs0DgqRkFbkcxxhVWCGph067DvLdwM9f0aklGw3i34xgfadagHqN7t+Kj74so2G43pDPhxwpBLUyYXUBkhHDHedYbCDW3DWpDfHQkT8+0XoEJP1YIamhtyUE+WlzEqN6tSK8f53Yc42MpCTHc2Lc1U5YXs7p4v9txjPErKwQ19PTMAuKiIxk/MNvtKMYht/TPIik2iqdm2OA1JrxYIaiB1cX7+XTpVm7sm0mjxFi34xiHJMdHM7Z/a6av2s7yon1uxzHGb6wQ1MBTM/JJiotiXH/rDYS6m/q1JrleNH+fked2FGP8xrFCICItRGSOiKwSkZUicncVbQaJyD4RWeJ5POJUnrpaVrSX6au2c0v/LJLjo92OYxxWPy6acQOymJNXwuJNe9yOY4xfONkjKAN+paodgXOpHLi+YxXtvlbVbp7HYw7mqZMnp+fTMD6aG/tmuh3F+MmYPpk0SoixYwUmbDhWCFS1WFUXe54fAFYDzZ1anxNyN+zmv/kl3Dowm6Q46w2Ei4TYKMYPzObrgp0sWL/b7TjGOM4vxwhEJBPoDnxXxezeIrJURD4XkU6nWMY4EckVkdySkhKnov7Ak9PzSU2MZVTvVn5Znwkc15/birSkWJ6cnkflaKrGhC7HC4GIJAIfAr9Q1ZNP0F4MtFLVrsCzwCfVLUdVX1LVHFXNSUtLcyzvCd+u3cm8dbu4fVA28TGODu1sAlC9mEhuH5TNd+t3M2/tLrfjGOMoRwuBiERTWQT+o6ofnTxfVfer6kHP86lAtIikOpmppp6ZWUB6/ViuPael21GMS67p1ZKmyXE8OSPfegUmpDl51pAArwKrVfXv1bRp4mmHiPTy5HH9z6/563bx3frdjB+YbQPSh7G46EjuOK8Nizbu4auCnW7HMcYxTvYI+gI3AOd7nR56sYiMF5HxnjZXAStEZCkwARipAfCn1zMzC0hLiuWaXtYbCHcjclrQLDmOCbMKrFdgQpZjO79VdS5wyjEcVXUiMNGpDHWxYP1u5q3bxW9/2sF6A4aYqAhuO68ND3+ygm8Kd9GvbUDsuTTGp+zK4pNMmFVAamIM151jZwqZSiNyMmhSP45nZtmxAhOarBB4WbRxN3MLdzJuQBb1Yqw3YCrFRkVy26BsFm7Yw7x1rh/CMsbnrBB4eWZWIY0SYrj+XOsNmB/6Wc8WpNeP5Rkbr8CEICsEHt9v2sNX+SXcMiDLrhswP3LiFuTfrd/NfOsVmBBjhcBjwqwCGsZHc4P1Bkw1runVkrSkWCbMsl6BCS1WCIClm/cyJ6+Em/tnkRBrvQFTtbjoSG4dkMW3a3excIPdg8iEDisEVPYGGsRHM7pPpttRTIC77pxWpCbasQITWsK+EKzYso9Za3Ywtm9rEq03YE6jXkxlr2Bu4U4WbbRegQkNYV8InplVQP24KEbbeAOmhq47tyWNEmJ4Zlah21GM8YmwLgQrt+5jxqrt3NSvNfVtvAFTQ/ExUdwyIIuv8kv43kYxMyEgrAvBs7MKSYqL4sa+rd2OYoLMDee2omF8tJ1BZEJC2BaC1cX7+WLlNm7sWzlYuTG1kRAbxc39K8c2Xrp5r9txjDkjYVsInp1dQGJsFDfZsQFTR6P7ZNLAegUmBIRlIcjbdoCpy7cxpk8mDeJj3I5jglRibBQ392vNrDU7WF60z+04xtRZWBaCZ2cXkBATydh+dmzAnJlRfTKpHxfFM9YrMEEs7ApBwfYDTFlezKg+mTRMsN6AOTP146IZ2y+Lmau3s3Kr9QpMcPLH4PVDRSRPRApF5IEq5seKyLue+d+JSKaTeSbOKaRedCS39M9ycjUmjIzpm0lSXBTP2nUFJkg5PXh9JPAcMAzoCFwjIh1PajYW2KOqbYCngL84lWdtyUE+XbqVG3q3IsV6A8ZHkutFc2Pf1nyxchuri/e7HceYWnO6R9ALKFTVdapaCrwDDD+pzXDgDc/zD4ALTgxo72sTZxcSG2W9AeN7J25RMnG29QqMM+YW7OTnb3/P7kOlPl+204WgObDZ63WRZ1qVbVS1DNgHNDp5QSIyTkRyRSS3pKSk1kHKK5QDR49z/bktSU2MrfX7jTmV5PhoxvTJZOqKYvK3H3A7jgkxqsrTM/NZuGE3CbG+Hz0xaA4Wq+pLqpqjqjlpaWm1fn9khPDK6J48MKyDA+mMgbH9WhMfHWnXFRifm7duF7kb93DboGxio4KvEGwBWni9zvBMq7KNiEQByYBjQ0BFRjiy18kYGibEMKpPJlOWF1O4w3oFxneenVVI46RYRuS0OH3jOnC6ECwE2opIaxGJAUYCk09qMxkY7Xl+FTBbVdXhXMY44pb+WdSLjuRZO1ZgfGTB+t3MW7eLWwdmExft+94AOFwIPPv87wSmAauB91R1pYg8JiKXepq9CjQSkULgl8CPTjE1JlikJMRwQ+9WfLp0K2tLDrodx4SAZ2cXkJoYw7W9Wjq2DsePEajqVFU9S1WzVfVxz7RHVHWy5/lRVb1aVduoai9VXed0JmOcdEv/LGKjInnOegXmDC3etIevC3ZW9jRjnOkNQBAdLDYmWKQmxnL9uS35ZMkWNuw85HYcE8SenVVAw/horj+3laPrsUJgjANuGZBFdGQEE+dYr8DUzbKivczJK+Hm/lkkODyMrhUCYxzQOCmO685pxcffb2HjLusVmNqbMKuA5HrRjOrtbG8ArBAY45hbB2YRGSE8P2et21FMkFmxZR8zV+/g5n6tSfLDMLpWCIxxSHr9OK7t1ZIPFxexefdht+OYIPL0zALqx0Ux2k8DZ1khMMZBtw7MIkKE57+0XoGpmcrewHbG9suivh96A2CFwBhHNU2ux896tuCDRZvZsveI23FMEJgwq7I3MMaPw+haITDGYbcNygbgH1/aGUTm1FZt3c/0Vdu5qV9rkuv5pzcAVgiMcVyzBvW4OqcF7y0sonif9QpM9SbMKiApLoob+/p3GF0rBMb4we2DsqlQ5R92rMBUY3Xxfr5YuY0b+/q3NwBWCIzxi4yG8Vx1dgbvLNjMtn1H3Y5jAtCEWQUkxUYx1s+9AbBCYIzf3HFeG8pVeeG/1iswP7Rm234+X7GNMX0zSY73b28ArBAY4zctUuK5ontz3l6wiR37rVdg/s+zswpJjI1ibD//9wbACoExfnXn+W0oq1Be/Mpusmsq5W8/wNQVxYzu04oG8TGuZLBCYIwftWqUwPBuzfjPdxspOXDM7TgmAEyYVUB8dCQ398tyLYMVAmP87K7z21JaVsHLX1uvINwVbD/AlOXFjO6TScMEd3oD4FAhEJG/isgaEVkmIh+LSINq2m0QkeUiskREcp3IYkygaZ2awPBuzfnXvI3sPGi9gnD27OxC6kVHcnN/93oD4FyPYAbQWVV/AuQDD56i7Xmq2k1VcxzKYkzAueO8NhwtK7deQRgr2H6AT5dtZVTvTFJc7A2AQ4VAVad7xisGmA9kOLEeY4JVm8aJXNq1GW9+a72CcPX0zMpjA+MGuNsbAP8cI7gJ+LyaeQpMF5FFIjLuVAsRkXEikisiuSUlJT4PaYy/3X1BW46VldvVxmFo5dZ9TFlezNh+rV3vDcAZFAIRmSkiK6p4DPdq8xBQBvynmsX0U9UewDDgDhEZUN36VPUlVc1R1Zy0tLS6xjYmYGSlJXJljwz+NX+jXW0cZp6aUXmH0bEuHxs4oc6FQFUHq2rnKh6TAERkDHAJcJ2qajXL2OL5uQP4GOhV1zzGBKOfX9CWigpl4pwCt6MYP1myeS8zV29n3IAsv99TqDpOnTU0FLgPuFRVqxyaSUQSRCTpxHPgImCFE3mMCVQtUuL5Wc8WvLtws41iFib+PiOfhvHRjHHhnkLVceoYwUQgCZjhOTX0BQARaSYiUz1t0oG5IrIUWABMUdUvHMpjTMC68/w2iAjPzrZeQahbuGE3X+WXcNugbBJjo9yO878cSaKqbaqZvhW42PN8HdDVifUbE0yaJtfj+nNa8ca8DYwfmE1WWqLbkYwDVJW/TcsjLSmWG87NdDvOD9iVxcYEgNsGZRMTGcEzs6xXEKq+XbuL79bv5o5B2dSLiXQ7zg9YITAmAKQlxTK6TyaTl24lf/sBt+MYH1NVnpyeR9PkOEb2aul2nB+xQmBMgLh1QBYJMVE8NSPf7SjGx77MK2Hxpr3cdX5b4qIDqzcAVgiMCRgNE2IY2681n6/Yxoot+9yOY3xEVXlyRh4tUupxdU5g3mTBCoExAWRs/8rxap+cnud2FOMjlYV9P3dfcBbRkYH5lRuYqYwJU/Xjorl1YBZz8kr4bt0ut+OYM1RWXsHfpuVxVnoil3dv7nacalkhMCbA3NinNen1Y/nzF2uo5qJ8EyTeyy1i3c5D3DukPZER4nacalkhMCbA1IuJ5J7BZ/H9pr1MW7nd7Timjo6UlvP0zHxyWjVkcIfGbsc5JSsExgSgq87OIDstgSemraGsvMLtOKYO/vnNenYcOMb9w9ojEri9AbBCYExAioqM4L6h7VlXcoj3FxW5HcfU0p5Dpbzw37UM7tCYnpkpbsc5LSsExgSoizqm06NlA56akc+R0nK345haeP7LQg4eK+PeIe3djlIjVgiMCVAiwgPDOrDjwDH++c16t+OYGtqy9whvzNvIFd0zaNckye04NWKFwJgA1qt1CoM7NOaFL9ey51Cp23FMDTw9Ix8U7rmwrdtRaswKgTEB7t4h7TlUWsZzcwrdjmJOY822/Xy4uIgbercio2G823FqzAqBMQGuXZMkruyRwZvzNtrgNQFMVfnDZ6tJiovmrvOrvBN/wHKsEIjI70Vki2dgmiUicnE17YaKSJ6IFIrIA07lMSaY3XPhWYjAE9Ps1hOB6su8EuYW7uTuC9rSIN79Aelrw+kewVOq2s3zmHryTBGJBJ6jcvD6jsA1ItLR4UzGBJ1mDepx64AsPl26lUUbd7sdx5zkeHkFf5iyitapCVx/biu349Sa27uGegGFqrpOVUuBd4DhLmcyJiDdOjCb9PqxPPbZaioq7NYTgeTtBZtYW3KIB4e1JybK7a/V2nM68Z0iskxE/ikiDauY3xzY7PW6yDPtR0RknIjkikhuSUmJE1mNCWgJsVHcN6Q9SzfvZdLSLW7HMR77jhznqRn59M5qxIUd092OUydnVAhEZKaIrKjiMRz4B5ANdAOKgSfPZF2q+pKq5qhqTlpa2pksypigdXn35vwkI5m/fJ7H4dIyt+MY4Lk5hew9cpyHftoh4G8lUZ0zKgSqOlhVO1fxmKSq21W1XFUrgJep3A10si1AC6/XGZ5pxpgqREQIj1zSkW37j/Lif9e5HSfsbdx1iNe/2cBVPTLo3DzZ7Th15uRZQ029Xl4OrKii2UKgrYi0FpEYYCQw2alMxoSCnMwULvlJU178ai1b9x5xO05Y+/Pna4iKFH49pJ3bUc6Ik8cInhCR5SKyDDgPuAdARJqJyFQAVS0D7gSmAauB91R1pYOZjAkJDwxrT4XCE1+scTtK2JpbsJPPV2xj/MBs0uvHuR3njEQ5tWBVvaGa6VuBi71eTwV+dGqpMaZ6GQ3jGdc/i4lzCrmhdyvObhX4d7gMJaVlFfxu8gpapsQzbkCW23HOWPCd52SMAeC2Qdk0TY7jt5+stDEL/Oy1b9aztuQQv7+0I3HRkW7HOWNWCIwJUgmxUTxySUdWF+/nX/M3uh0nbBTvO8IzswoY3KEx57cPztNFT2aFwJggNrRzEwaclcaT0/PZsf+o23HCwuNTVlNWoTxySSe3o/iMFQJjgpiI8OilnSgtq+DxqavdjhPyvl27k8+WFXP7oGxaNgqeu4uejhUCY4Jc69QExg/KZtKSrXy7dqfbcULW8fIKfjdpJS1S6jF+YLbbcXzKCoExIeD2Qdm0SKnHI5NWUlpmB46d8NJX6yjYcZDfXdIpJA4Qe7NCYEwIiIuO5NFLO1G44yAvf21XHPvaupKDPDOrgGGdmzA4SO8ndCpWCIwJEee3T2dY5yY8M6uAtSUH3Y4TMlSV33y8nNioCB69NHQOEHuzQmBMCHl0eCfioiJ44MNldqtqH3kvdzPz1+3mNxd3oHGQX0FcHSsExoSQxklxPHxJRxZu2MN/FmxyO07Q23HgKI9PWU2v1in8LKfF6d8QpKwQGBNirjo7g/5tU/nz1NVssZvSnZFHJ6/iaFkFf7qiCxERwXmL6ZqwQmBMiBER/nh5FyoUHvp4Oaq2i6guPl26lSnLi7n7grZkpyW6HcdRVgiMCUEtUuK5d0g7vswr4f1FRW7HCTo79h/l4Ukr6NqiAbeGwE3lTscKgTEhakyfTM7NSuHRySvZvPuw23GChqry4EfLOVJazpNXdyUqMvS/JkP/X2hMmIqIEP52dVciRPjVe0spt7OIauT93CJmrdnB/UPb06ZxaO8SOsEKgTEhLKNhPI8O78SCDbvtQrMaKNpzmMc+W8W5WSmM6ZPpdhy/cWRgGhF5FzgxdlsDYK+qdqui3QbgAFAOlKlqjhN5jAlnl3dvzszV23lyeh4D2qbRsVl9tyMFpLLyCn7xzhJUlb9e1TWkzxI6mSM9AlX9map283z5fwh8dIrm53naWhEwxgEiwuOXdaFBfAx3v/M9h0vL3I4UkJ6ZVUDuxj388YoutEgJnTuL1oSju4ZERIARwNtOrscYc2oNE2J4akQ3CksO8sgkGxb8ZN8U7mTinEJG5GQwvFtzt+P4ndPHCPoD21W1oJr5CkwXkUUiMu5UCxKRcSKSKyK5JSUlPg9qTKjr1zaVu85vyweLing/d7PbcQJGyYFj/OLdJWSnJfL7EL2X0OnUuRCIyEwRWVHFY7hXs2s4dW+gn6r2AIYBd4jIgOoaqupLqpqjqjlpaWl1jW1MWLv7grb0zmrEw5NWkLftgNtxXFdRofzq/aXsP3Kcidd2Jz7GkcOmAa/OhUBVB6tq5yoekwBEJAq4Anj3FMvY4vm5A/gY6FXXPMaY04uMEJ65phuJsdHc/p9FHDoW3scLnplVwFf5JTzyPx1p3yR8D6I7uWtoMLBGVau8rFFEEkQk6cRz4CJghYN5jDFU3phuwshurN95iPs+WBa2t6CYvnIbz8wq4KqzM7i2V0u347jKyUIwkpN2C4lIMxGZ6nmZDswVkaXAAmCKqn7hYB5jjEefNqncP7Q9U5YXM3F2odtx/K5wxwHueXcJXTOS+cNlnak8ryV8ObZDTFXHVDFtK3Cx5/k6oKtT6zfGnNq4AVnkbTvAkzPyaZuexNDOTdyO5Bf7Dh9n3JuLqBcTyQs3nB1yw07WhV1ZbEyYEhH+eEUXurZowC/fW8KqrfvdjuS4Y2Xl3PrvXDbvOczz151N0+R6bkcKCFYIjAljcdGRvHzD2dSPi+bG1xdQtCd0b06nqtz3wTLmr9vN367uSq/WKW5HChhWCIwJc43rx/H6TT05XFrO6H8uYM+hUrcjOeJv0/OYtGQr9w5pF5YXjZ2KFQJjDO2b1OeVUTls3nOEm95YyJHScrcj+dRr36znuTlruaZXC24flO12nIBjhcAYA8A5WY2YMLIbSzbvZdy/cjl6PDSKwVvfbeLRT1cxpFM6/2+4nSFUFSsExpj/NbRzU/5y5U+YW7iTcf9aFPTF4INFRTz0yXLOb9+YZ6/pERaDzNSFbRVjzA+MyGnBX674CV8XlAR1MXg/dzP3fbCUvtmpPH9dD2Ki7OuuOrZljDE/MqLn/xWDm15fyP6jx92OVCuvfL2Oez9YRp/sVF4aZdcKnI4VAmNMlUb0bMHfR3RlwfrdjHhhHtv2HXU70mmpKk98sYY/TFnNT7s05dUxOWF7I7nasEJgjKnW5d0zeO3GnmzefZgrnv8moO9YevR4Ofe8u4Tnv1zLtee0ZMI13YmNsp5ATVghMMacUv+2abw3vjdlFcoVz3/DlGXFbkf6keJ9Rxjx4jw+WbKVX190Fo9f1pnIMBpq8kxZITDGnFanZslMvrMf7Zokccdbi/nDZ6soLatwOxYAc9bs4JIJc1m74yAvj8rhzvPb2imitWSFwBhTI02S43hnXG9G9W7FK3PXc9lz37Bmm3v3Jzp6vJzHPl3Fja8vJC0plkl39uXCjumu5QlmVgiMMTUWExXBY8M78/KoHHYcOMqlz37DMzML/H6K6Vf5JQx5+iv++c16xvTJ5JM7+tKmcZJfM4QSO5xujKm1Czum06PlAB6ZvJKnZubz/qLNPHRxB4Z2buLobpm1JQd5cnoeU5dvIys1gbduOYc+2amOrS9cSDCOTpSTk6O5ubluxzDGAN8W7uT3n64kf/tBOjStz13nt2FIpyY+PVhbsP0AL3+9jg8WFREXHcmtA7K5dWCWXR9QSyKySFVzfjT9TAqBiFwN/B7oAPRS1VyveQ8CY4Fy4OeqOq2K97cG3gEaAYuAG1T1tLc+tEJgTGApK69g0pKtPDenkHU7D9EsOY6rc1pwZY8MWjaKr9MyDxw9zuw1O3gvdzPfFO4iJiqC685pyR3ntSE1MdbH/4Lw4FQh6ABUAC8Cvz5RCESkI5XDVPYCmgEzgbNUtfyk978HfKSq74jIC8BSVf3H6dZrhcCYwFReoUxbuY23F2xibuFOVCE7LYFB7RrTtUUDOjatT6tG8USfdM+figql5OAx1u44yOJNe8jduIdvC3dRWl5Bs+Q4ru/dipE9W5KSEOPSvyw0OFIIvBb+JT8sBA8CqOqfPK+nAb9X1Xle7xGgBGiiqmUi0tvTZsjp1meFwJjAV7TnMNNWbufLvB18t243peX/d7ppcr1okutFU6FKWbmy+1DpD+afKB4Xd2lC9xYNibBrAnyiukLg1MHi5sB8r9dFnmneGgF7VbXsFG3+l4iMA8YBtGzZ0ndJjTGOyGgYz9h+rRnbrzXHysop3HGQ1cUHKNpzmN2HStl7+DiREUJUhJCSGENGg3q0bJRA14xkGsTbX/7+dNpCICIzgapGtX5IVSf5PlLVVPUl4CWo7BH4a73GmDMXGxVJp2bJdGqW7HYUU4XTFgJVHVyH5W4BWni9zvBM87YLaCAiUZ5eQVVtjDHGOMypC8omAyNFJNZzZlBbYIF3A608ODEHuMozaTTgtx6GMcaYSmdUCETkchEpAnoDUzwHhVHVlcB7wCrgC+COE2cMichUEWnmWcT9wC9FpJDKYwavnkkeY4wxtWcXlBljTJio7qwhu9eQMcaEOSsExhgT5qwQGGNMmLNCYIwxYS4oDxaLSAmwsY5vTwV2+jCOr1iu2rFctWO5aidUc7VS1bSTJwZlITgTIpJb1VFzt1mu2rFctWO5aifcctmuIWOMCXNWCIwxJsyFYyF4ye0A1bBctWO5asdy1U5Y5Qq7YwTGGGN+KBx7BMYYY7xYITDGmDAXkoVARK4WkZUiUiEi1Z5qJSJDRSRPRApF5AGv6a1F5DvP9HdFxCfDJYlIiojMEJECz8+GVbQ5T0SWeD2Oishlnnmvi8h6r3nd/JXL067ca92Tvaa7ub26icg8z+97mYj8zGueT7dXdZ8Xr/mxnn9/oWd7ZHrNe9AzPU9ETjscq49z/VJEVnm2zywRaeU1r8rfqZ9yjRGREq/13+w1b7Tn914gIqP9nOspr0z5IrLXa54j20tE/ikiO0RkRTXzRUQmeDIvE5EeXvPOfFupasg9gA5AO+BLIKeaNpHAWiALiAGWAh09894DRnqevwDc5qNcTwAPeJ4/APzlNO1TgN1AvOf168BVDmyvGuUCDlYz3bXtBZwFtPU8bwYUAw18vb1O9XnxanM78ILn+UjgXc/zjp72sUBrz3Ii/ZjrPK/P0G0ncp3qd+qnXGOAiVW8NwVY5/nZ0PO8ob9yndT+LuCfftheA4AewIpq5l8MfA4IcC7wnS+3VUj2CFR1tarmnaZZL6BQVdepainwDjBcRAQ4H/jA0+4N4DIfRRvuWV5Nl3sV8LmqHvbR+qtT21z/y+3tpar5qlrgeb4V2AH86MpJH6jy83KKvB8AF3i2z3DgHVU9pqrrgULP8vySS1XneH2G5lM5GqDTarK9qjMEmKGqu1V1DzADGOpSrmuAt3207mqp6ldU/tFXneHAm1ppPpWjOzbFR9sqJAtBDTUHNnu9LvJMawTs1crhM72n+0K6qhZ7nm8D0k/TfiQ//hA+7ukaPiUisX7OFSciuSIy/8TuKgJoe4lILyr/ylvrNdlX26u6z0uVbTzbYx+V26cm73Uyl7exVP5leUJVv1N/5rrS8/v5QERODG8bENvLswutNTDba7JT2+t0qsvtk2112jGLA5WIzASaVDHrIVV1bcjLU+XyfqGqKiLVnrvrqfZdgGlekx+k8gsxhsrzie8HHvNjrlaqukVEsoDZIrKcyi+7OvPx9voXMFpVKzyT67y9QpGIXA/kAAO9Jv/od6qqa6tegs99CrytqsdE5FYqe1Pn+2ndNTES+EA9oyt6uLm9HBO0hUBVB5/hIrYALbxeZ3im7aKy2xXl+avuxPQzziUi20WkqaoWe764dpxiUSOAj1X1uNeyT/x1fExEXgN+7c9cqrrF83OdiHwJdAc+xOXtJSL1gSlU/hEw32vZdd5eVaju81JVmyIRiQKSqfw81eS9TuZCRAZTWVwHquqxE9Or+Z364ovttLlUdZfXy1eoPCZ04r2DTnrvlz7IVKNcXkYCd3hPcHB7nU51uX2yrcJ519BCoK1UnvESQ+UvfbJWHoGZQ+X+eYDRgK96GJM9y6vJcn+0b9LzZXhiv/xlQJVnGDiRS0Qanti1IiKpQF9gldvby/O7+5jK/acfnDTPl9urys/LKfJeBcz2bJ/JwEipPKuoNdAWWHAGWWqVS0S6Ay8Cl6rqDq/pVf5O/ZirqdfLS4HVnufTgIs8+RoCF/HDnrGjuTzZ2lN58HWe1zQnt9fpTAZGec4eOhfY5/lDxzfbyokj4G4/gMup3Fd2DNgOTPNMbwZM9Wp3MZBPZUV/yGt6FpX/UQuB94FYH+VqBMwCCoCZQIpneg7wile7TCorfcRJ758NLKfyC+3fQKK/cgF9POte6vk5NhC2F3A9cBxY4vXo5sT2qurzQuWupks9z+M8//5Cz/bI8nrvQ5735QHDfPx5P12umZ7/Bye2z+TT/U79lOtPwErP+ucA7b3ee5NnOxYCN/ozl+f174E/n/Q+x7YXlX/0FXs+y0VUHssZD4z3zBfgOU/m5XidDemLbWW3mDDGmDAXzruGjDHGYIXAGGPCnhUCY4wJc1YIjDEmzFkhMMaYMGeFwBhjwpwVAmOMCXP/HzUv+jOOzBvMAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(xt_bound_3[:,0],u_bound_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrainingdata(N_u1,N_u2,seed):\n",
    "\n",
    "    '''Boundary Conditions'''\n",
    "    #Initial Condition -1 =< x =<1 and t = 0  \n",
    "#     leftedge_x = np.hstack((X[0,:][:,None], T[0,:][:,None])) #L1\n",
    "#     leftedge_u = usol[:,0][:,None]\n",
    "\n",
    "#     #Boundary Condition x = -1 and 0 =< t =<1\n",
    "#     bottomedge_x = np.hstack((X[:,0][:,None], T[:,0][:,None])) #L2\n",
    "#     bottomedge_u = usol[-1,:][:,None]\n",
    "\n",
    "#     #Boundary Condition x = 1 and 0 =< t =<1\n",
    "#     topedge_x = np.hstack((X[:,-1][:,None], T[:,0][:,None])) #L3\n",
    "#     topedge_u = usol[0,:][:,None]\n",
    "\n",
    "#     all_X_u_train = np.vstack([leftedge_x, bottomedge_x, topedge_x]) # X_u_train [456,2] (456 = 256(L1)+100(L2)+100(L3))\n",
    "#     all_u_train = np.vstack([leftedge_u, bottomedge_u, topedge_u])   #corresponding u [456x1]\n",
    "\n",
    "    #choose random N_u points for training\n",
    "    np.random.seed(seed)\n",
    "    idx = np.random.choice(xt_bound.shape[0], N_u1, replace=True) \n",
    "\n",
    "    X_u_train = xt_bound[idx, :] #choose indices from  set 'idx' (x,t)\n",
    "    u_train = u_bound[idx,:]      #choose corresponding u\n",
    "\n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    samples = sampling(N_u2)\n",
    "    \n",
    "    X_u2_train = lb_xt + (ub_xt-lb_xt)*samples \n",
    "    \n",
    "    u2_train = (-10/3)*X_u2_train[:,1].reshape(-1,1)\n",
    "\n",
    "    X_u_train = np.vstack((X_u_train, X_u2_train))\n",
    "    u_train = np.vstack((u_train, u2_train))\n",
    "    \n",
    "    return X_u_train, u_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_u,N_f,seed):\n",
    "\n",
    "    '''Boundary Conditions'''\n",
    "    #Initial Condition -1 =< x =<1 and t = 0  \n",
    "#     leftedge_x = np.hstack((X[0,:][:,None], T[0,:][:,None])) #L1\n",
    "#     leftedge_u = usol[:,0][:,None]\n",
    "\n",
    "#     #Boundary Condition x = -1 and 0 =< t =<1\n",
    "#     bottomedge_x = np.hstack((X[:,0][:,None], T[:,0][:,None])) #L2\n",
    "#     bottomedge_u = usol[-1,:][:,None]\n",
    "\n",
    "#     #Boundary Condition x = 1 and 0 =< t =<1\n",
    "#     topedge_x = np.hstack((X[:,-1][:,None], T[:,0][:,None])) #L3\n",
    "#     topedge_u = usol[0,:][:,None]\n",
    "\n",
    "#     all_X_u_train = np.vstack([leftedge_x, bottomedge_x, topedge_x]) # X_u_train [456,2] (456 = 256(L1)+100(L2)+100(L3))\n",
    "#     all_u_train = np.vstack([leftedge_u, bottomedge_u, topedge_u])   #corresponding u [456x1]\n",
    "\n",
    "    #choose random N_u points for training\n",
    "    np.random.seed(seed)\n",
    "    idx = np.random.choice(xt_bound.shape[0], N_u, replace=True) \n",
    "\n",
    "    X_u_train = xt_bound[idx, :] #choose indices from  set 'idx' (x,t)\n",
    "    u_train = u_bound[idx,:]      #choose corresponding u\n",
    "\n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    samples = sampling(N_f)\n",
    "    X_f_train = lb_xt + (ub_xt-lb_xt)*samples \n",
    "    \n",
    "    X_f_train = np.vstack((X_f_train, X_u_train)) # append training points to collocation points \n",
    "\n",
    "    return X_f_train, X_u_train, u_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n   Fortran Style ('F') flatten,stacked column wise!\\n   u = [c1 \\n        c2\\n        .\\n        .\\n        cn]\\n\\n   u =  [25600x1] \\n\""
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X_u_test = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "X_u_test = xt_test_tensor\n",
    "# Domain bounds\n",
    "lb = X_u_test[0]  # [-1. 0.]\n",
    "ub = X_u_test[-1] # [1.  0.99]\n",
    "\n",
    "'''\n",
    "   Fortran Style ('F') flatten,stacked column wise!\n",
    "   u = [c1 \n",
    "        c2\n",
    "        .\n",
    "        .\n",
    "        cn]\n",
    "\n",
    "   u =  [25600x1] \n",
    "'''\n",
    "#u_true = usol.flatten('F')[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid syntax (<ipython-input-16-1e7871266bed>, line 169)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-16-1e7871266bed>\"\u001b[0;36m, line \u001b[0;32m169\u001b[0m\n\u001b[0;31m    else\u001b[0m\n\u001b[0m             ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
     ]
    }
   ],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        self.iter = 0\n",
    "        \n",
    "        '''\n",
    "        Alternatively:\n",
    "        \n",
    "        *all layers are callable \n",
    "    \n",
    "        Simple linear Layers\n",
    "        self.fc1 = nn.Linear(2,50)\n",
    "        self.fc2 = nn.Linear(50,50)\n",
    "        self.fc3 = nn.Linear(50,50)\n",
    "        self.fc4 = nn.Linear(50,1)\n",
    "        \n",
    "        '''\n",
    "    \n",
    "        'Xavier Normal Initialization'\n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            # weights from a normal distribution with \n",
    "            # Recommended gain value for tanh = 5/3?\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "            \n",
    "        self.beta = Parameter(1*torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "        \n",
    "        self.train_loss = []\n",
    "        self.test_loss = []\n",
    "        self.beta_val = []\n",
    "        \n",
    "        self.W1 = Parameter(torch.tensor(0.0))\n",
    "        self.W1.requiresGrad = True\n",
    "        \n",
    "        self.W2 = Parameter(torch.tensor(0.0))\n",
    "        self.W2.requiresGrad = True\n",
    "        \n",
    "    'foward pass'\n",
    "    def forward(self,x):\n",
    "         if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "         u_b = torch.from_numpy(ub_xt).float().to(device)\n",
    "         l_b = torch.from_numpy(lb_xt).float().to(device)\n",
    "        \n",
    "            \n",
    "         #preprocessing input \n",
    "         x = (x - l_b)/(u_b - l_b) #feature scaling\n",
    "        \n",
    "         #convert to float\n",
    "         a = x.float()\n",
    "                        \n",
    "         '''     \n",
    "         Alternatively:\n",
    "        \n",
    "         a = self.activation(self.fc1(a))\n",
    "         a = self.activation(self.fc2(a))\n",
    "         a = self.activation(self.fc3(a))\n",
    "         a = self.fc4(a)\n",
    "         \n",
    "         '''\n",
    "        \n",
    "         for i in range(len(layers)-2):\n",
    "                z = self.linears[i](a)\n",
    "                a = self.activation(z) + self.beta[:,i]*z*self.activation(z)\n",
    "         \n",
    "         a = self.linears[-1](a)\n",
    "        \n",
    "         return a\n",
    "                        \n",
    "    def loss_BC(self,x,y):\n",
    "                \n",
    "        loss_u = self.loss_function(self.forward(x), y)\n",
    "                \n",
    "        return loss_u\n",
    "    \n",
    "    def loss_PDE(self, x_to_train_f,f_hat):\n",
    "        \n",
    "        nu = 0.01/pi\n",
    "                \n",
    "        x_1_f = x_to_train_f[:,[0]]\n",
    "        x_2_f = x_to_train_f[:,[1]]\n",
    "                        \n",
    "        g = x_to_train_f.clone()\n",
    "                        \n",
    "        g.requires_grad = True\n",
    "        \n",
    "        u = self.forward(g)\n",
    "                \n",
    "        u_x_t = autograd.grad(u,g,torch.ones([x_to_train_f.shape[0], 1]).to(device), retain_graph=True, create_graph=True)[0]\n",
    "                                \n",
    "        u_xx_tt = autograd.grad(u_x_t,g,torch.ones(x_to_train_f.shape).to(device), create_graph=True)[0]\n",
    "                                                            \n",
    "        u_x = u_x_t[:,[0]]\n",
    "        \n",
    "        u_t = u_x_t[:,[1]]\n",
    "        \n",
    "        u_xx = u_xx_tt[:,[0]]\n",
    "                                        \n",
    "        f = u_t + (self.forward(g))*(u_x) - (nu)*u_xx \n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,x,y,x_to_train_f,f_hat):\n",
    "\n",
    "        loss_u = self.loss_BC(x,y)\n",
    "        loss_f = self.loss_PDE(x_to_train_f,f_hat)\n",
    "        \n",
    "        p = torch.exp(self.W1)\n",
    "        q = torch.exp(self.W2)\n",
    "        \n",
    "        \n",
    "        loss_val = (1+p)*loss_u + (1+q)*loss_f\n",
    "        \n",
    "        return loss_val\n",
    "    \n",
    "#     def closure_pretrain(self):\n",
    "#         optimizer.zero_grad()        \n",
    "\n",
    "#         loss = self.loss_BC(X_u_train, u_train)\n",
    "        \n",
    "#         self.train_loss.append(loss.cpu().detach().numpy())\n",
    "        \n",
    "#         u_pred = self.test(xt_test_tensor)\n",
    "#         self.test_loss.append(np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1))))\n",
    "#         self.beta_val.append(self.beta.cpu().detach().numpy())\n",
    "        \n",
    "#         print(self.iter,\"Train Loss\",self.train_loss[-1],\"Test Loss\",self.test_loss[-1])\n",
    "# #         print(self.iter,\"Train Loss\",self.train_loss[-1])\n",
    "#         loss.backward()\n",
    "#         self.iter += 1\n",
    "     \n",
    "#         return loss  \n",
    "        \n",
    "     \n",
    "    'callable for optimizer'                                       \n",
    "    def closure(self):\n",
    "        if (flag == 1):\n",
    "            optimizer.zero_grad()\n",
    "        \n",
    "\n",
    "            loss = self.loss(X_u_train, u_train, X_f_train,f_hat)\n",
    "        \n",
    "            self.train_loss.append(loss.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "            u_pred = self.test(xt_test_tensor)\n",
    "            self.test_loss.append(np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1))))\n",
    "            self.beta_val.append(self.beta.cpu().detach().numpy())\n",
    "\n",
    "            print(self.iter,\"Train Loss\",self.train_loss[-1],\"Test Loss\",self.test_loss[-1])\n",
    "    #         print(self.iter,\"Train Loss\",self.train_loss[-1])\n",
    "            loss.backward()\n",
    "            self.iter += 1\n",
    "            \n",
    "        else:\n",
    "            \n",
    "            optimizer.zero_grad()        \n",
    "\n",
    "            loss = self.loss_BC(X_u_train, u_train)\n",
    "\n",
    "            self.train_loss.append(loss.cpu().detach().numpy())\n",
    "\n",
    "            u_pred = self.test(xt_test_tensor)\n",
    "            self.test_loss.append(np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1))))\n",
    "            self.beta_val.append(self.beta.cpu().detach().numpy())\n",
    "\n",
    "            print(self.iter,\"Train Loss\",self.train_loss[-1],\"Test Loss\",self.test_loss[-1])\n",
    "    #         print(self.iter,\"Train Loss\",self.train_loss[-1])\n",
    "            loss.backward()\n",
    "            self.iter += 1\n",
    "     \n",
    "        return loss        \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self,xt_test_tensor):\n",
    "        u_pred = self.forward(xt_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'X_f_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-15-59fd6f9b10a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPINN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    309\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    310\u001b[0m         \u001b[0;31m# evaluate initial f(x) and df/dx\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 311\u001b[0;31m         \u001b[0morig_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    312\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0morig_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    313\u001b[0m         \u001b[0mcurrent_evals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-8-00acbc0ad60d>\u001b[0m in \u001b[0;36mclosure\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    161\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    162\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 163\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_u_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_f_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    164\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    165\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'X_f_train' is not defined"
     ]
    }
   ],
   "source": [
    "##PRE-TRAIN\n",
    "max_reps = 1\n",
    "flag = 0\n",
    "\n",
    "train_loss_full = []\n",
    "test_loss_full = []\n",
    "beta_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(reps)\n",
    "    N_u1 = 1000 #Total number of data points for 'u'\n",
    "    N_u2 = 10000\n",
    "    \n",
    "    X_u_train, u_train = pretrainingdata(N_u1,N_u2,reps*23)\n",
    "        \n",
    "    X_u_train = torch.from_numpy(X_u_train).float().to(device)\n",
    "    u_train = torch.from_numpy(u_train).float().to(device)\n",
    "\n",
    "\n",
    "    layers = np.array([2,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "\n",
    "    PINN = Sequentialmodel(layers)\n",
    "       \n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "\n",
    "    'L-BFGS Optimizer'\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=1, \n",
    "                                  max_iter = 10000, \n",
    "                                  max_eval = None, \n",
    "                                  tolerance_grad = -1, \n",
    "                                  tolerance_change = -1, \n",
    "                                  history_size = 100, \n",
    "                                  line_search_fn = 'strong_wolfe')\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    optimizer.step(PINN.closure)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt_test_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "flag = 1\n",
    "for reps in range(max_reps):\n",
    "    print(reps)\n",
    "    N_u = 1000 #Total number of data points for 'u'\n",
    "    N_f = 10000 #Total number of collocation points \n",
    "    \n",
    "    X_f_train_np_array, X_u_train_np_array, u_train_np_array = trainingdata(N_u,N_f,reps*32)\n",
    "        \n",
    "    X_f_train = torch.from_numpy(X_f_train_np_array).float().to(device)\n",
    "    X_u_train = torch.from_numpy(X_u_train_np_array).float().to(device)\n",
    "    u_train = torch.from_numpy(u_train_np_array).float().to(device)\n",
    "        \n",
    "    #u = torch.from_numpy(u_true).float().to(device)\n",
    "    f_hat = torch.zeros(X_f_train.shape[0],1).to(device)\n",
    "        \n",
    "\n",
    "    #X_u_test_tensor = torch.from_numpy(X_u_test).float().to(device)\n",
    "    'Convert to tensor and send to GPU'\n",
    "\n",
    "\n",
    "#     layers = np.array([2,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "\n",
    "#     #layers = np.array([2,512,512,1])\n",
    "#     PINN = Sequentialmodel(layers)\n",
    "       \n",
    "#     PINN.to(device)\n",
    "\n",
    "#     'Neural Network Summary'\n",
    "#     print(PINN)\n",
    "\n",
    "#     params = list(PINN.parameters())\n",
    "\n",
    "\n",
    "    'L-BFGS Optimizer'\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=1, \n",
    "                                  max_iter = 10000, \n",
    "                                  max_eval = None, \n",
    "                                  tolerance_grad = -1, \n",
    "                                  tolerance_change = -1, \n",
    "                                  history_size = 100, \n",
    "                                  line_search_fn = 'strong_wolfe')\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    optimizer.step(PINN.closure)\n",
    "    \n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(PINN.train_loss)\n",
    "    test_loss_full.append(PINN.test_loss)\n",
    "    elapsed_time[reps] = time.time() - start_time\n",
    "    beta_full.append(PINN.beta_val)\n",
    "    \n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_loss\": test_loss_full,\"Time\": elapsed_time, \"beta\": beta_full, \"label\": label}\n",
    "savemat(label+'.mat', mdic) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINN.W2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred = PINN.test(xt_test_tensor)\n",
    "\n",
    "cmap = plt.cm.rainbow\n",
    "plt.imshow(u_pred.reshape(100,256),cmap = cmap,aspect =1,vmin=-10,vmax=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_u_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(u_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(u_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.cm.rainbow\n",
    "plt.imshow(u_true.reshape(100,256),cmap = cmap,aspect = 1,vmin=-10,vmax=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(u_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.cm.rainbow\n",
    "plt.imshow(np.transpose(np.abs(u_pred - u_true).reshape(100,256)),cmap = cmap,aspect = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0 \n",
    "for i in range(10):\n",
    "    print(test_loss_full[i][-1])\n",
    "    a = a + test_loss_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
