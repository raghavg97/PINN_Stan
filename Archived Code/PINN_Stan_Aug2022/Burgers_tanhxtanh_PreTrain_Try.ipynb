{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "\n",
    "from torch.nn.parameter import Parameter\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat('burgers_shock_10sin.mat')\n",
    "label = \"QCRE_2D_5_tanhxtanh_NW\"\n",
    "                     \n",
    "x_test = data['x']                                   # 256 points between -1 and 1 [256x1]\n",
    "t_test = data['t']   \n",
    "usol = data['usol']\n",
    "X_test, T_test = np.meshgrid(x_test,t_test)  \n",
    "\n",
    "xt_test_tensor = torch.from_numpy(np.hstack((X_test.flatten()[:,None], T_test.flatten()[:,None]))).float().to(device)\n",
    "\n",
    "u_true = usol.flatten('F')[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# data = scipy.io.loadmat('burgers_shock_10sin.mat')  \t# Load data from file\n",
    "\n",
    "pi = torch.from_numpy(np.array(np.pi)).double().to(device)\n",
    "\n",
    "label = \"QCRE_2D_5_tanhxtanh_NW\"\n",
    "# x = data['x']                                   # 256 points between -1 and 1 [256x1]\n",
    "# t = data['t']                                   # 100 time points between 0 and 0.2 [100x1] \n",
    "# usol = data['usol']   \n",
    "\n",
    "#usol = usol/1000# solution of 256x100 grid points\n",
    "\n",
    "x = np.linspace(-1,1,500).reshape(-1,1)\n",
    "t = np.linspace(0,0.2,500).reshape(-1,1)\n",
    "\n",
    "X,T = np.meshgrid(x,t)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "T = T.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xt = np.hstack((X,T))\n",
    "\n",
    "#y_true = true_2D_1(xt)\n",
    "\n",
    "bound_pts_1 = (X==-1).reshape(-1,)\n",
    "xt_bound_1 = xt[bound_pts_1,:]\n",
    "u_bound_1 = np.zeros((np.shape(xt_bound_1)[0],1))\n",
    "\n",
    "bound_pts_2 = (X==1).reshape(-1,)\n",
    "xt_bound_2 = xt[bound_pts_2,:]\n",
    "u_bound_2 = np.zeros((np.shape(xt_bound_2)[0],1))\n",
    "\n",
    "bound_pts_3 = (T==0).reshape(-1,)\n",
    "xt_bound_3 = xt[bound_pts_3,:]\n",
    "u_bound_3 = -10*np.sin(np.pi*xt_bound_3[:,0].reshape(-1,1))\n",
    "#u_bound_3 = -10*np.ones((np.shape(bound_pts_3)[0],1))\n",
    "\n",
    "\n",
    "xt_bound = np.vstack((xt_bound_1,xt_bound_2,xt_bound_3))\n",
    "u_bound = np.vstack((u_bound_1,u_bound_2,u_bound_3))\n",
    "\n",
    "\n",
    "lb_xt = xt[0]\n",
    "ub_xt = xt[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(xt_bound_3[:,0],u_bound_3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pretrainingdata(N_u1,N_u2,seed):\n",
    "\n",
    "    '''Boundary Conditions'''\n",
    "    #Initial Condition -1 =< x =<1 and t = 0  \n",
    "#     leftedge_x = np.hstack((X[0,:][:,None], T[0,:][:,None])) #L1\n",
    "#     leftedge_u = usol[:,0][:,None]\n",
    "\n",
    "#     #Boundary Condition x = -1 and 0 =< t =<1\n",
    "#     bottomedge_x = np.hstack((X[:,0][:,None], T[:,0][:,None])) #L2\n",
    "#     bottomedge_u = usol[-1,:][:,None]\n",
    "\n",
    "#     #Boundary Condition x = 1 and 0 =< t =<1\n",
    "#     topedge_x = np.hstack((X[:,-1][:,None], T[:,0][:,None])) #L3\n",
    "#     topedge_u = usol[0,:][:,None]\n",
    "\n",
    "#     all_X_u_train = np.vstack([leftedge_x, bottomedge_x, topedge_x]) # X_u_train [456,2] (456 = 256(L1)+100(L2)+100(L3))\n",
    "#     all_u_train = np.vstack([leftedge_u, bottomedge_u, topedge_u])   #corresponding u [456x1]\n",
    "\n",
    "    #choose random N_u points for training\n",
    "    np.random.seed(seed)\n",
    "    idx = np.random.choice(xt_bound.shape[0], N_u1, replace=True) \n",
    "\n",
    "    X_u_train = xt_bound[idx, :] #choose indices from  set 'idx' (x,t)\n",
    "    u_train = u_bound[idx,:]      #choose corresponding u\n",
    "\n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    samples = sampling(N_u2)\n",
    "    \n",
    "    X_u2_train = lb_xt + (ub_xt-lb_xt)*samples \n",
    "    \n",
    "    abc = (X_u2_train[:,1]*10 + (X_u2_train[:,0] - 1) + (X_u2_train[:,1] + 1))\n",
    "    a1 = X_u2_train[:,1]*10/abc\n",
    "#     a2 = (X_u2_train[:,0] - 1)/abc\n",
    "#     a3 = 1-a2-a3\n",
    "    \n",
    "    \n",
    "    \n",
    "    u2_train = -10*a1.reshape(-1,1)\n",
    "\n",
    "    X_u_train = np.vstack((X_u_train, X_u2_train))\n",
    "    u_train = np.vstack((u_train, u2_train))\n",
    "    \n",
    "    return X_u_train, u_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_u,N_f,seed):\n",
    "\n",
    "    '''Boundary Conditions'''\n",
    "    #Initial Condition -1 =< x =<1 and t = 0  \n",
    "#     leftedge_x = np.hstack((X[0,:][:,None], T[0,:][:,None])) #L1\n",
    "#     leftedge_u = usol[:,0][:,None]\n",
    "\n",
    "#     #Boundary Condition x = -1 and 0 =< t =<1\n",
    "#     bottomedge_x = np.hstack((X[:,0][:,None], T[:,0][:,None])) #L2\n",
    "#     bottomedge_u = usol[-1,:][:,None]\n",
    "\n",
    "#     #Boundary Condition x = 1 and 0 =< t =<1\n",
    "#     topedge_x = np.hstack((X[:,-1][:,None], T[:,0][:,None])) #L3\n",
    "#     topedge_u = usol[0,:][:,None]\n",
    "\n",
    "#     all_X_u_train = np.vstack([leftedge_x, bottomedge_x, topedge_x]) # X_u_train [456,2] (456 = 256(L1)+100(L2)+100(L3))\n",
    "#     all_u_train = np.vstack([leftedge_u, bottomedge_u, topedge_u])   #corresponding u [456x1]\n",
    "\n",
    "    #choose random N_u points for training\n",
    "    np.random.seed(seed)\n",
    "    idx = np.random.choice(xt_bound.shape[0], N_u, replace=True) \n",
    "\n",
    "    X_u_train = xt_bound[idx, :] #choose indices from  set 'idx' (x,t)\n",
    "    u_train = u_bound[idx,:]      #choose corresponding u\n",
    "\n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    samples = sampling(N_f)\n",
    "    X_f_train = lb_xt + (ub_xt-lb_xt)*samples \n",
    "    \n",
    "    X_f_train = np.vstack((X_f_train, X_u_train)) # append training points to collocation points \n",
    "\n",
    "    return X_f_train, X_u_train, u_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n   Fortran Style ('F') flatten,stacked column wise!\\n   u = [c1 \\n        c2\\n        .\\n        .\\n        cn]\\n\\n   u =  [25600x1] \\n\""
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#X_u_test = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "X_u_test = xt_test_tensor\n",
    "# Domain bounds\n",
    "lb = X_u_test[0]  # [-1. 0.]\n",
    "ub = X_u_test[-1] # [1.  0.99]\n",
    "\n",
    "'''\n",
    "   Fortran Style ('F') flatten,stacked column wise!\n",
    "   u = [c1 \n",
    "        c2\n",
    "        .\n",
    "        .\n",
    "        cn]\n",
    "\n",
    "   u =  [25600x1] \n",
    "'''\n",
    "#u_true = usol.flatten('F')[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        self.iter = 0\n",
    "        \n",
    "        '''\n",
    "        Alternatively:\n",
    "        \n",
    "        *all layers are callable \n",
    "    \n",
    "        Simple linear Layers\n",
    "        self.fc1 = nn.Linear(2,50)\n",
    "        self.fc2 = nn.Linear(50,50)\n",
    "        self.fc3 = nn.Linear(50,50)\n",
    "        self.fc4 = nn.Linear(50,1)\n",
    "        \n",
    "        '''\n",
    "    \n",
    "        'Xavier Normal Initialization'\n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            # weights from a normal distribution with \n",
    "            # Recommended gain value for tanh = 5/3?\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "            \n",
    "        self.beta = Parameter(1*torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "        \n",
    "        self.train_loss = []\n",
    "        self.test_loss = []\n",
    "        self.beta_val = []\n",
    "        \n",
    "        self.W1 = Parameter(torch.tensor(0.0))\n",
    "        self.W1.requiresGrad = True\n",
    "        \n",
    "        self.W2 = Parameter(torch.tensor(0.0))\n",
    "        self.W2.requiresGrad = True\n",
    "        \n",
    "    'foward pass'\n",
    "    def forward(self,x):\n",
    "         if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "         u_b = torch.from_numpy(ub_xt).float().to(device)\n",
    "         l_b = torch.from_numpy(lb_xt).float().to(device)\n",
    "        \n",
    "            \n",
    "         #preprocessing input \n",
    "         x = (x - l_b)/(u_b - l_b) #feature scaling\n",
    "        \n",
    "         #convert to float\n",
    "         a = x.float()\n",
    "                        \n",
    "         '''     \n",
    "         Alternatively:\n",
    "        \n",
    "         a = self.activation(self.fc1(a))\n",
    "         a = self.activation(self.fc2(a))\n",
    "         a = self.activation(self.fc3(a))\n",
    "         a = self.fc4(a)\n",
    "         \n",
    "         '''\n",
    "        \n",
    "         for i in range(len(layers)-2):\n",
    "                z = self.linears[i](a)\n",
    "                a = self.activation(z) + self.beta[:,i]*z*self.activation(z)\n",
    "         \n",
    "         a = self.linears[-1](a)\n",
    "        \n",
    "         return a\n",
    "                        \n",
    "    def loss_BC(self,x,y):\n",
    "                \n",
    "        loss_u = self.loss_function(self.forward(x), y)\n",
    "                \n",
    "        return loss_u\n",
    "    \n",
    "    def loss_PDE(self, x_to_train_f,f_hat):\n",
    "        \n",
    "        nu = 0.01/pi\n",
    "                \n",
    "        x_1_f = x_to_train_f[:,[0]]\n",
    "        x_2_f = x_to_train_f[:,[1]]\n",
    "                        \n",
    "        g = x_to_train_f.clone()\n",
    "                        \n",
    "        g.requires_grad = True\n",
    "        \n",
    "        u = self.forward(g)\n",
    "                \n",
    "        u_x_t = autograd.grad(u,g,torch.ones([x_to_train_f.shape[0], 1]).to(device), retain_graph=True, create_graph=True)[0]\n",
    "                                \n",
    "        u_xx_tt = autograd.grad(u_x_t,g,torch.ones(x_to_train_f.shape).to(device), create_graph=True)[0]\n",
    "                                                            \n",
    "        u_x = u_x_t[:,[0]]\n",
    "        \n",
    "        u_t = u_x_t[:,[1]]\n",
    "        \n",
    "        u_xx = u_xx_tt[:,[0]]\n",
    "                                        \n",
    "        f = u_t + (self.forward(g))*(u_x) - (nu)*u_xx \n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,x,y,x_to_train_f,f_hat):\n",
    "\n",
    "        loss_u = self.loss_BC(x,y)\n",
    "        loss_f = self.loss_PDE(x_to_train_f,f_hat)\n",
    "        \n",
    "        p = torch.exp(self.W1)\n",
    "        q = torch.exp(self.W2)\n",
    "        \n",
    "        \n",
    "        loss_val = (1+p)*loss_u + (1+q)*loss_f\n",
    "        \n",
    "        return loss_val\n",
    "    \n",
    "#     def closure_pretrain(self):\n",
    "#         optimizer.zero_grad()        \n",
    "\n",
    "#         loss = self.loss_BC(X_u_train, u_train)\n",
    "        \n",
    "#         self.train_loss.append(loss.cpu().detach().numpy())\n",
    "        \n",
    "#         u_pred = self.test(xt_test_tensor)\n",
    "#         self.test_loss.append(np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1))))\n",
    "#         self.beta_val.append(self.beta.cpu().detach().numpy())\n",
    "        \n",
    "#         print(self.iter,\"Train Loss\",self.train_loss[-1],\"Test Loss\",self.test_loss[-1])\n",
    "# #         print(self.iter,\"Train Loss\",self.train_loss[-1])\n",
    "#         loss.backward()\n",
    "#         self.iter += 1\n",
    "     \n",
    "#         return loss  \n",
    "        \n",
    "     \n",
    "    'callable for optimizer'                                       \n",
    "    def closure(self):\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        if (flag==1):\n",
    "            loss = self.loss(X_u_train, u_train, X_f_train,f_hat)\n",
    "        else:\n",
    "            loss = self.loss_BC(X_u_train, u_train)\n",
    "            \n",
    "        self.train_loss.append(loss.cpu().detach().numpy())\n",
    "\n",
    "\n",
    "        u_pred = self.test(xt_test_tensor)\n",
    "        self.test_loss.append(np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1))))\n",
    "        self.beta_val.append(self.beta.cpu().detach().numpy())\n",
    "\n",
    "        print(self.iter,\"Train Loss\",self.train_loss[-1],\"Test Loss\",self.test_loss[-1])\n",
    "    #   print(self.iter,\"Train Loss\",self.train_loss[-1])\n",
    "        loss.backward()\n",
    "        self.iter += 1\n",
    "            \n",
    "            \n",
    "\n",
    "            \n",
    "\n",
    "        \n",
    "     \n",
    "        return loss        \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self,xt_test_tensor):\n",
    "        u_pred = self.forward(xt_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0 Train Loss 27717.355 Test Loss 24.60512888128238\n",
      "1 Train Loss 27716.02 Test Loss 24.552168426891523\n",
      "2 Train Loss 27706.166 Test Loss 24.599322477254148\n",
      "3 Train Loss 27682.96 Test Loss 29.428634868979106\n",
      "4 Train Loss 27649.652 Test Loss 58.596709229092895\n",
      "5 Train Loss 27693.908 Test Loss 274.4541740869089\n",
      "6 Train Loss 27642.271 Test Loss 95.54610455324553\n",
      "7 Train Loss 27639.533 Test Loss 100.30935742867119\n",
      "8 Train Loss 27655.744 Test Loss 65.36906922806537\n",
      "9 Train Loss 27643.602 Test Loss 84.24507171158066\n",
      "10 Train Loss 27638.236 Test Loss 99.89783973172824\n",
      "11 Train Loss 27636.936 Test Loss 107.40799848147086\n",
      "12 Train Loss 27633.209 Test Loss 122.80598079790695\n",
      "13 Train Loss 27628.863 Test Loss 151.08026215378447\n",
      "14 Train Loss 27625.922 Test Loss 123.6309195468861\n",
      "15 Train Loss 27623.354 Test Loss 130.92662513344436\n",
      "16 Train Loss 27620.47 Test Loss 157.619154841817\n",
      "17 Train Loss 27615.271 Test Loss 166.60057851962193\n",
      "18 Train Loss 27630.0 Test Loss 68.76490146943547\n",
      "19 Train Loss 27610.945 Test Loss 147.43525438737078\n",
      "20 Train Loss 27613.945 Test Loss 118.97616912809897\n",
      "21 Train Loss 27609.625 Test Loss 137.2140492172949\n",
      "22 Train Loss 27606.787 Test Loss 140.02581323790588\n",
      "23 Train Loss 27595.67 Test Loss 117.05282751119923\n",
      "24 Train Loss 27585.902 Test Loss 174.97160967309912\n",
      "25 Train Loss 27576.357 Test Loss 172.2822398914489\n",
      "26 Train Loss 27572.197 Test Loss 184.35404182924592\n",
      "27 Train Loss 27567.418 Test Loss 97.18648475707894\n",
      "28 Train Loss 27570.035 Test Loss 98.09777920236412\n",
      "29 Train Loss 27564.506 Test Loss 96.65787548431413\n",
      "30 Train Loss 27542.967 Test Loss 134.3947433544977\n",
      "31 Train Loss 28589.498 Test Loss 1511.4264146188036\n",
      "32 Train Loss 27545.238 Test Loss 204.42118268105128\n",
      "33 Train Loss 27531.17 Test Loss 164.68062895749233\n",
      "34 Train Loss 27522.574 Test Loss 244.62788566511819\n",
      "35 Train Loss 27502.438 Test Loss 204.6232571977345\n",
      "36 Train Loss 27480.328 Test Loss 174.17015769930552\n",
      "37 Train Loss 27458.664 Test Loss 201.62830322446268\n",
      "38 Train Loss 27420.395 Test Loss 219.96669670990096\n",
      "39 Train Loss 27514.604 Test Loss 328.47517298857025\n",
      "40 Train Loss 27405.701 Test Loss 240.4604796524661\n",
      "41 Train Loss 27540.746 Test Loss 455.7435778314294\n",
      "42 Train Loss 27377.713 Test Loss 251.3380189734871\n",
      "43 Train Loss 27366.46 Test Loss 280.5218876614849\n",
      "44 Train Loss 27344.512 Test Loss 298.9397047149074\n",
      "45 Train Loss 28456.098 Test Loss 1801.4935761034026\n",
      "46 Train Loss 27347.83 Test Loss 474.4842840112053\n",
      "47 Train Loss 27315.203 Test Loss 368.1307972216613\n",
      "48 Train Loss 27322.736 Test Loss 460.62425772228335\n",
      "49 Train Loss 27305.129 Test Loss 378.19778078720867\n",
      "50 Train Loss 27358.045 Test Loss 566.1144063499324\n",
      "51 Train Loss 27301.06 Test Loss 402.3659653328014\n",
      "52 Train Loss 27281.35 Test Loss 301.4568285691067\n",
      "53 Train Loss 27224.354 Test Loss 336.5129330943497\n",
      "54 Train Loss 27862.068 Test Loss 1448.432367608577\n",
      "55 Train Loss 27178.99 Test Loss 476.3749824147362\n",
      "56 Train Loss 45917.055 Test Loss 21199.58387534324\n",
      "57 Train Loss 27471.168 Test Loss 1314.656811503812\n",
      "58 Train Loss 27119.033 Test Loss 609.233089152908\n",
      "59 Train Loss 1.177034e+16 Test Loss 1.1896214384101996e+16\n",
      "60 Train Loss 5347490000000.0 Test Loss 5405592275788.397\n",
      "61 Train Loss 6295294000.0 Test Loss 6370078953.3459425\n",
      "62 Train Loss 16251617.0 Test Loss 16451172.68145276\n",
      "63 Train Loss 470125.0 Test Loss 454028.88538437866\n",
      "64 Train Loss 42247.56 Test Loss 17075.95600805959\n",
      "65 Train Loss 27491.455 Test Loss 1342.6918616769408\n",
      "66 Train Loss 27108.672 Test Loss 636.7325605313199\n",
      "67 Train Loss 27139.795 Test Loss 609.066475197167\n",
      "68 Train Loss 27084.775 Test Loss 614.4689057873986\n",
      "69 Train Loss 27116.809 Test Loss 360.82472910182935\n",
      "70 Train Loss 27059.955 Test Loss 478.3815901679873\n",
      "71 Train Loss 26994.36 Test Loss 421.69570636536577\n",
      "72 Train Loss 27055.424 Test Loss 356.8293074463274\n",
      "73 Train Loss 26902.783 Test Loss 417.1825269108049\n",
      "74 Train Loss 26838.771 Test Loss 664.3119694390535\n",
      "75 Train Loss 37708.94 Test Loss 9585.23922381472\n",
      "76 Train Loss 29915.816 Test Loss 2561.1832928063723\n",
      "77 Train Loss 26821.14 Test Loss 639.1465192265907\n",
      "78 Train Loss 29629.893 Test Loss 3100.438865694353\n",
      "79 Train Loss 26783.031 Test Loss 711.959077700287\n",
      "80 Train Loss 33449.188 Test Loss 7260.279702784659\n",
      "81 Train Loss 26744.068 Test Loss 820.1565461814488\n",
      "82 Train Loss 29149.69 Test Loss 2495.2690116898248\n",
      "83 Train Loss 26730.361 Test Loss 822.1108871512856\n",
      "84 Train Loss 26874.717 Test Loss 924.9225778127834\n",
      "85 Train Loss 26697.51 Test Loss 817.9908128439029\n",
      "86 Train Loss 27710.85 Test Loss 1532.0499621476868\n",
      "87 Train Loss 26681.512 Test Loss 828.7885415350479\n",
      "88 Train Loss 26830.016 Test Loss 853.0889471618615\n",
      "89 Train Loss 26667.379 Test Loss 798.4115365086452\n",
      "90 Train Loss 26664.314 Test Loss 818.0680540958572\n",
      "91 Train Loss 26663.758 Test Loss 853.5579361072364\n",
      "92 Train Loss 26546.256 Test Loss 884.0518159211362\n",
      "93 Train Loss 54232.598 Test Loss 29715.946652258586\n",
      "94 Train Loss 35377.71 Test Loss 7561.17220367766\n",
      "95 Train Loss 26496.746 Test Loss 863.8849056299499\n",
      "96 Train Loss 26671.209 Test Loss 1042.2791476033076\n",
      "97 Train Loss 26469.664 Test Loss 872.3252113210744\n",
      "98 Train Loss 26434.36 Test Loss 1169.1881080018868\n",
      "99 Train Loss 28123.51 Test Loss 3055.8995394786534\n",
      "100 Train Loss 26321.457 Test Loss 1137.4198496446356\n",
      "101 Train Loss 26219.979 Test Loss 1147.0888591824425\n",
      "102 Train Loss 2569607.8 Test Loss 2603447.7935631773\n",
      "103 Train Loss 111542.305 Test Loss 93714.97942558765\n",
      "104 Train Loss 26367.277 Test Loss 1481.514881727473\n",
      "105 Train Loss 26216.883 Test Loss 1167.5652166706768\n",
      "106 Train Loss 26155.625 Test Loss 1194.1239745737732\n",
      "107 Train Loss 25935.152 Test Loss 1189.9565308120093\n",
      "108 Train Loss 31568.871 Test Loss 5218.387125164543\n",
      "109 Train Loss 27087.668 Test Loss 1179.8474120416413\n",
      "110 Train Loss 25878.135 Test Loss 1194.9428589436702\n",
      "111 Train Loss 36789.08 Test Loss 10785.537927813022\n",
      "112 Train Loss 25672.666 Test Loss 1118.67682432563\n",
      "113 Train Loss 26305.78 Test Loss 2476.605620208958\n",
      "114 Train Loss 25485.35 Test Loss 1231.2319405381575\n",
      "115 Train Loss 28118.984 Test Loss 4559.450685293249\n",
      "116 Train Loss 25549.244 Test Loss 1847.5389614811345\n",
      "117 Train Loss 25366.818 Test Loss 1468.3500207477905\n",
      "118 Train Loss 25315.578 Test Loss 1842.2624867536995\n",
      "119 Train Loss 25235.715 Test Loss 1661.5213848763672\n",
      "120 Train Loss 26585.932 Test Loss 2086.07261079665\n",
      "121 Train Loss 25218.902 Test Loss 1683.847988281796\n",
      "122 Train Loss 25417.826 Test Loss 1748.7559693536011\n",
      "123 Train Loss 25175.537 Test Loss 1666.931861406745\n",
      "124 Train Loss 25235.467 Test Loss 1518.2198089422316\n",
      "125 Train Loss 25150.004 Test Loss 1590.3982870547816\n",
      "126 Train Loss 25153.893 Test Loss 1616.6083989690756\n",
      "127 Train Loss 25123.086 Test Loss 1590.7593865053186\n",
      "128 Train Loss 25011.55 Test Loss 1707.0463329808629\n",
      "129 Train Loss 27361.32 Test Loss 4438.154021222071\n",
      "130 Train Loss 25170.074 Test Loss 2280.5525430979765\n",
      "131 Train Loss 24848.615 Test Loss 1872.4224950636358\n",
      "132 Train Loss 64044.215 Test Loss 43954.8163174783\n",
      "133 Train Loss 25344.889 Test Loss 3259.7092899002805\n",
      "134 Train Loss 24798.04 Test Loss 2182.093129165278\n",
      "135 Train Loss 24750.832 Test Loss 2009.5260562674353\n",
      "136 Train Loss 24644.115 Test Loss 2245.3905272003085\n",
      "137 Train Loss 26282.484 Test Loss 5178.541637422674\n",
      "138 Train Loss 24581.75 Test Loss 2440.518865266354\n",
      "139 Train Loss 395387.34 Test Loss 383574.386536442\n",
      "140 Train Loss 30595.201 Test Loss 10011.773810474384\n",
      "141 Train Loss 24556.648 Test Loss 2496.6686045505303\n",
      "142 Train Loss 25007.756 Test Loss 2730.151651970716\n",
      "143 Train Loss 24470.51 Test Loss 2550.3214178765347\n",
      "144 Train Loss 24404.998 Test Loss 2498.066446616997\n",
      "145 Train Loss 24879.65 Test Loss 1950.2598911508862\n",
      "146 Train Loss 24190.883 Test Loss 2231.4135662202953\n",
      "147 Train Loss 26360.348 Test Loss 1678.6431391956833\n",
      "148 Train Loss 24133.125 Test Loss 2105.676930284912\n",
      "149 Train Loss 24076.113 Test Loss 2394.584851597833\n",
      "150 Train Loss 24006.541 Test Loss 2453.851902788191\n",
      "151 Train Loss 23947.504 Test Loss 2365.5723441298837\n",
      "152 Train Loss 23913.578 Test Loss 2407.709293906047\n",
      "153 Train Loss 24185.41 Test Loss 2665.4095221925754\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "154 Train Loss 23892.707 Test Loss 2450.0361523174665\n",
      "155 Train Loss 23896.395 Test Loss 2586.1574923157527\n",
      "156 Train Loss 23848.988 Test Loss 2513.136106766921\n",
      "157 Train Loss 23805.473 Test Loss 2679.669038407352\n",
      "158 Train Loss 23756.162 Test Loss 3002.7247375719244\n",
      "159 Train Loss 23748.725 Test Loss 3007.2870677722512\n",
      "160 Train Loss 23720.734 Test Loss 2985.865613894776\n",
      "161 Train Loss 24489.992 Test Loss 2578.454741601559\n",
      "162 Train Loss 23676.09 Test Loss 2903.43290719367\n",
      "163 Train Loss 23832.33 Test Loss 2608.782523197115\n",
      "164 Train Loss 23630.705 Test Loss 2818.1199104429134\n",
      "165 Train Loss 23747.352 Test Loss 3006.051919615291\n",
      "166 Train Loss 23597.67 Test Loss 2858.7080623164657\n",
      "167 Train Loss 23502.818 Test Loss 3174.216715881537\n",
      "168 Train Loss 23409.58 Test Loss 3595.032833307661\n",
      "169 Train Loss 23391.1 Test Loss 3460.3725064062432\n",
      "170 Train Loss 23368.23 Test Loss 3654.63974591603\n",
      "171 Train Loss 23226.236 Test Loss 3167.8281159730436\n",
      "172 Train Loss 23413.771 Test Loss 3005.2047374390477\n",
      "173 Train Loss 23186.678 Test Loss 3108.326400526652\n",
      "174 Train Loss 23180.762 Test Loss 2976.7724313434514\n",
      "175 Train Loss 23113.09 Test Loss 3173.0402222024636\n",
      "176 Train Loss 23078.047 Test Loss 3231.822550467723\n",
      "177 Train Loss 23046.463 Test Loss 3285.4004786035894\n",
      "178 Train Loss 23098.225 Test Loss 3624.288467636519\n",
      "179 Train Loss 23015.016 Test Loss 3451.492116559725\n",
      "180 Train Loss 22955.01 Test Loss 3246.6584614076105\n",
      "181 Train Loss 23067.283 Test Loss 2943.5789878822384\n",
      "182 Train Loss 22904.0 Test Loss 3156.800929262562\n",
      "183 Train Loss 181108.58 Test Loss 162199.72624044353\n",
      "184 Train Loss 45186.133 Test Loss 19695.86553492241\n",
      "185 Train Loss 22953.865 Test Loss 3037.051391118075\n",
      "186 Train Loss 22885.885 Test Loss 3114.366111028709\n",
      "187 Train Loss 22879.75 Test Loss 3095.851527310426\n",
      "188 Train Loss 28342.29 Test Loss 3463.4923712367595\n",
      "189 Train Loss 26788.863 Test Loss 4603.723730304012\n",
      "190 Train Loss 22927.896 Test Loss 3041.6203020486682\n",
      "191 Train Loss 22877.004 Test Loss 3083.867522814257\n",
      "192 Train Loss 22833.73 Test Loss 3005.974046867166\n",
      "193 Train Loss 22778.318 Test Loss 3287.8924564051545\n",
      "194 Train Loss 22718.932 Test Loss 3282.4467265776016\n",
      "195 Train Loss 22683.62 Test Loss 3265.8517390074758\n",
      "196 Train Loss 22644.781 Test Loss 3294.0145013573224\n",
      "197 Train Loss 22629.398 Test Loss 3465.561736321451\n",
      "198 Train Loss 22608.854 Test Loss 3369.1227474411753\n",
      "199 Train Loss 22569.14 Test Loss 3428.5353472611114\n",
      "200 Train Loss 22561.55 Test Loss 3803.404303465529\n",
      "201 Train Loss 22487.424 Test Loss 3905.089934259908\n",
      "202 Train Loss 22419.059 Test Loss 3803.3055577934592\n",
      "203 Train Loss 22378.342 Test Loss 3813.656807390874\n",
      "204 Train Loss 22326.516 Test Loss 3891.324106509633\n",
      "205 Train Loss 22321.836 Test Loss 4288.054974756067\n",
      "206 Train Loss 22278.72 Test Loss 4070.191671469854\n",
      "207 Train Loss 22354.957 Test Loss 4532.617316992862\n",
      "208 Train Loss 22261.637 Test Loss 4174.642578489675\n",
      "209 Train Loss 21883.316 Test Loss 4156.286961501855\n",
      "210 Train Loss 6337502000.0 Test Loss 6465365408.605133\n",
      "211 Train Loss 177158240.0 Test Loss 181225596.75711662\n",
      "212 Train Loss 1959290.2 Test Loss 2004376.0128413704\n",
      "213 Train Loss 25460.012 Test Loss 8600.519521883292\n",
      "214 Train Loss 22067.494 Test Loss 4716.001086439643\n",
      "215 Train Loss 21862.873 Test Loss 4227.220661193676\n",
      "216 Train Loss 21840.354 Test Loss 4132.708472054994\n",
      "217 Train Loss 21981.762 Test Loss 4473.838164864254\n",
      "218 Train Loss 21805.09 Test Loss 4162.441015043232\n",
      "219 Train Loss 21796.543 Test Loss 4267.898192762012\n",
      "220 Train Loss 21742.57 Test Loss 4212.874519563974\n",
      "221 Train Loss 21732.633 Test Loss 4426.03807673739\n",
      "222 Train Loss 21669.18 Test Loss 4743.655410576174\n",
      "223 Train Loss 21654.36 Test Loss 4685.626772589342\n",
      "224 Train Loss 21532.096 Test Loss 4223.047839386782\n",
      "225 Train Loss 24387.889 Test Loss 4805.833232788154\n",
      "226 Train Loss 21522.812 Test Loss 4151.164669324256\n",
      "227 Train Loss 21465.238 Test Loss 4277.05381420518\n",
      "228 Train Loss 22192.043 Test Loss 5929.926097488193\n",
      "229 Train Loss 21443.54 Test Loss 4582.851875538732\n",
      "230 Train Loss 21431.385 Test Loss 4457.849633367276\n",
      "231 Train Loss 21445.693 Test Loss 4933.1234614733285\n",
      "232 Train Loss 21403.793 Test Loss 4652.6235423929875\n",
      "233 Train Loss 21396.947 Test Loss 4932.374704945794\n",
      "234 Train Loss 21345.402 Test Loss 4832.773224528783\n",
      "235 Train Loss 21979.775 Test Loss 5637.552771339414\n",
      "236 Train Loss 21315.287 Test Loss 4854.672669531302\n",
      "237 Train Loss 22560.93 Test Loss 6403.700106097062\n",
      "238 Train Loss 21315.363 Test Loss 4956.498041739429\n",
      "239 Train Loss 21299.496 Test Loss 4888.217116727155\n",
      "240 Train Loss 21340.338 Test Loss 5097.36945604827\n",
      "241 Train Loss 21285.172 Test Loss 4933.808485648446\n",
      "242 Train Loss 21439.574 Test Loss 4683.190665547319\n",
      "243 Train Loss 21236.287 Test Loss 4853.6434667105095\n",
      "244 Train Loss 21328.965 Test Loss 5153.912220550652\n",
      "245 Train Loss 21221.361 Test Loss 4929.092497002609\n",
      "246 Train Loss 21199.002 Test Loss 5121.077922548553\n",
      "247 Train Loss 21184.566 Test Loss 5372.258975142042\n",
      "248 Train Loss 21141.213 Test Loss 5348.568674170546\n",
      "249 Train Loss 21150.895 Test Loss 5353.387661985833\n",
      "250 Train Loss 21131.418 Test Loss 5348.619434564619\n",
      "251 Train Loss 21127.49 Test Loss 5387.143033911549\n",
      "252 Train Loss 21117.14 Test Loss 5403.312387280092\n",
      "253 Train Loss 21104.326 Test Loss 5471.607209647566\n",
      "254 Train Loss 21087.324 Test Loss 5515.61594477202\n",
      "255 Train Loss 21077.682 Test Loss 5793.262943963333\n",
      "256 Train Loss 21068.393 Test Loss 5652.2016463686705\n",
      "257 Train Loss 21111.26 Test Loss 5794.96013639915\n",
      "258 Train Loss 21049.363 Test Loss 5683.893017534587\n",
      "259 Train Loss 21052.514 Test Loss 5486.232764377246\n",
      "260 Train Loss 21031.01 Test Loss 5582.0121173660145\n",
      "261 Train Loss 21077.01 Test Loss 5587.981065538645\n",
      "262 Train Loss 21021.254 Test Loss 5659.701537873954\n",
      "263 Train Loss 21491.686 Test Loss 5948.07824582592\n",
      "264 Train Loss 21014.656 Test Loss 5647.660726149247\n",
      "265 Train Loss 23278.004 Test Loss 8490.359652470515\n",
      "266 Train Loss 21398.47 Test Loss 5860.806976375385\n",
      "267 Train Loss 21013.443 Test Loss 5636.984701360821\n",
      "268 Train Loss 21001.191 Test Loss 5646.697386891785\n",
      "269 Train Loss 20991.424 Test Loss 5539.276245150491\n",
      "270 Train Loss 20986.42 Test Loss 5519.818866431939\n",
      "271 Train Loss 20983.283 Test Loss 5534.2823295241105\n",
      "272 Train Loss 20979.537 Test Loss 5572.7558915101345\n",
      "273 Train Loss 20973.002 Test Loss 5666.851771322261\n",
      "274 Train Loss 20966.781 Test Loss 5627.539362004878\n",
      "275 Train Loss 20924.197 Test Loss 5342.5822201044275\n",
      "276 Train Loss 20983.316 Test Loss 5087.932939843147\n",
      "277 Train Loss 20917.818 Test Loss 5300.792812272482\n",
      "278 Train Loss 21279.955 Test Loss 5592.369412349999\n",
      "279 Train Loss 20908.607 Test Loss 5217.400378392598\n",
      "280 Train Loss 20897.004 Test Loss 5265.739488442249\n",
      "281 Train Loss 20884.246 Test Loss 5473.378957975552\n",
      "282 Train Loss 20870.19 Test Loss 5571.106368805054\n",
      "283 Train Loss 20860.393 Test Loss 5657.421997578955\n",
      "284 Train Loss 20846.537 Test Loss 5622.004840207998\n",
      "285 Train Loss 20828.822 Test Loss 5598.415788030307\n",
      "286 Train Loss 20801.695 Test Loss 5631.682984832479\n",
      "287 Train Loss 20812.895 Test Loss 5674.133444163908\n",
      "288 Train Loss 20774.543 Test Loss 5639.700305824244\n",
      "289 Train Loss 21142.375 Test Loss 6472.569656120428\n",
      "290 Train Loss 20762.395 Test Loss 5706.00800738001\n",
      "291 Train Loss 20764.852 Test Loss 6011.033896234259\n",
      "292 Train Loss 20740.365 Test Loss 5843.0724898231865\n",
      "293 Train Loss 20740.238 Test Loss 6077.69884022484\n",
      "294 Train Loss 20733.14 Test Loss 5961.965192157782\n",
      "295 Train Loss 20720.035 Test Loss 6208.910308337292\n",
      "296 Train Loss 20698.783 Test Loss 6005.883254128119\n",
      "297 Train Loss 20683.164 Test Loss 5885.995533430982\n",
      "298 Train Loss 20693.154 Test Loss 5853.466583965926\n",
      "299 Train Loss 20678.932 Test Loss 5863.517937042078\n",
      "300 Train Loss 20689.293 Test Loss 5652.205341682849\n",
      "301 Train Loss 20675.127 Test Loss 5783.87242399539\n",
      "302 Train Loss 20669.596 Test Loss 5751.9702218387765\n",
      "303 Train Loss 20661.613 Test Loss 5743.565467695607\n",
      "304 Train Loss 20650.268 Test Loss 5744.267149559841\n",
      "305 Train Loss 20634.045 Test Loss 5773.450649467621\n",
      "306 Train Loss 20620.979 Test Loss 5812.208882051392\n",
      "307 Train Loss 20607.465 Test Loss 5808.178180619008\n",
      "308 Train Loss 20602.46 Test Loss 5850.8313186656205\n",
      "309 Train Loss 20599.59 Test Loss 5929.212763148468\n",
      "310 Train Loss 20595.89 Test Loss 5988.313212224281\n",
      "311 Train Loss 20593.854 Test Loss 6022.119947105358\n",
      "312 Train Loss 20588.857 Test Loss 6003.375387561948\n",
      "313 Train Loss 20583.857 Test Loss 5935.046351846425\n",
      "314 Train Loss 20578.479 Test Loss 5958.386823369587\n",
      "315 Train Loss 20574.006 Test Loss 5909.95869940966\n",
      "316 Train Loss 20571.6 Test Loss 5911.4352900844315\n",
      "317 Train Loss 20567.328 Test Loss 5968.163317520239\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "318 Train Loss 20559.053 Test Loss 6080.41879836186\n",
      "319 Train Loss 20556.113 Test Loss 6147.096743351958\n",
      "320 Train Loss 20552.918 Test Loss 6079.998976496735\n",
      "321 Train Loss 20551.19 Test Loss 6080.783719036629\n",
      "322 Train Loss 20548.408 Test Loss 6085.878500892943\n",
      "323 Train Loss 20544.467 Test Loss 6102.7028855150575\n",
      "324 Train Loss 20535.744 Test Loss 6141.102253156405\n",
      "325 Train Loss 20531.521 Test Loss 6154.335398107003\n",
      "326 Train Loss 20525.951 Test Loss 6146.256875354361\n",
      "327 Train Loss 20523.459 Test Loss 6164.2643958024555\n",
      "328 Train Loss 20519.818 Test Loss 6198.189175652533\n",
      "329 Train Loss 20514.5 Test Loss 6190.073559949782\n",
      "330 Train Loss 20503.75 Test Loss 6185.795815425405\n",
      "331 Train Loss 17904060.0 Test Loss 18360726.88019323\n",
      "332 Train Loss 20511.318 Test Loss 6191.9441709432285\n",
      "333 Train Loss 20502.887 Test Loss 6184.69141924617\n",
      "334 Train Loss 20499.557 Test Loss 6211.661475877222\n",
      "335 Train Loss 20485.268 Test Loss 6120.667897616861\n",
      "336 Train Loss 20485.768 Test Loss 5994.201327420604\n",
      "337 Train Loss 20478.436 Test Loss 6059.449170119031\n",
      "338 Train Loss 20469.309 Test Loss 6044.662597134241\n",
      "339 Train Loss 20456.68 Test Loss 5971.056416147976\n",
      "340 Train Loss 20444.975 Test Loss 5970.734855626258\n",
      "341 Train Loss 20438.77 Test Loss 5861.476001150568\n",
      "342 Train Loss 20431.033 Test Loss 5897.4212264214075\n",
      "343 Train Loss 20424.059 Test Loss 5896.713966912016\n",
      "344 Train Loss 20412.668 Test Loss 5899.741444075614\n",
      "345 Train Loss 56877.58 Test Loss 46096.83747844951\n",
      "346 Train Loss 20412.018 Test Loss 5900.948905025957\n",
      "347 Train Loss 20430.05 Test Loss 5997.864943358841\n",
      "348 Train Loss 20409.5 Test Loss 5914.886276161576\n",
      "349 Train Loss 20405.48 Test Loss 5862.1536485113265\n",
      "350 Train Loss 35483.688 Test Loss 23052.9252116772\n",
      "351 Train Loss 20406.047 Test Loss 5889.670493230447\n",
      "352 Train Loss 20403.217 Test Loss 5868.340956595952\n",
      "353 Train Loss 20393.375 Test Loss 5866.023437234416\n",
      "354 Train Loss 20384.086 Test Loss 5919.106926990606\n",
      "355 Train Loss 20381.656 Test Loss 5926.966025402579\n",
      "356 Train Loss 20374.492 Test Loss 5940.040299036682\n",
      "357 Train Loss 20363.807 Test Loss 5978.507028503793\n",
      "358 Train Loss 20358.707 Test Loss 5958.3552694503815\n",
      "359 Train Loss 20350.035 Test Loss 5941.394770278808\n",
      "360 Train Loss 20346.918 Test Loss 5993.862631871861\n",
      "361 Train Loss 20341.396 Test Loss 6048.024173804765\n",
      "362 Train Loss 20338.383 Test Loss 6055.748593850339\n",
      "363 Train Loss 20330.998 Test Loss 6066.072504120426\n",
      "364 Train Loss 20335.615 Test Loss 6080.1321858396705\n",
      "365 Train Loss 20322.145 Test Loss 6070.603798787163\n",
      "366 Train Loss 20307.81 Test Loss 5971.234535535425\n",
      "367 Train Loss 20301.168 Test Loss 6001.3883473152055\n",
      "368 Train Loss 20288.342 Test Loss 6123.000768801352\n",
      "369 Train Loss 20287.844 Test Loss 6234.485364833843\n",
      "370 Train Loss 20280.256 Test Loss 6183.797392916149\n",
      "371 Train Loss 20269.451 Test Loss 6268.515219459047\n",
      "372 Train Loss 20260.744 Test Loss 6380.925113501135\n",
      "373 Train Loss 20257.64 Test Loss 6450.556293213435\n",
      "374 Train Loss 20247.338 Test Loss 6388.306805698329\n",
      "375 Train Loss 20484.953 Test Loss 6673.3017370503185\n",
      "376 Train Loss 20244.809 Test Loss 6394.018489048921\n",
      "377 Train Loss 20666.838 Test Loss 6999.994527415061\n",
      "378 Train Loss 20244.902 Test Loss 6429.294924927463\n",
      "379 Train Loss 20240.648 Test Loss 6407.696476919614\n",
      "380 Train Loss 20239.754 Test Loss 6404.985455526934\n",
      "381 Train Loss 20238.61 Test Loss 6379.549888499028\n",
      "382 Train Loss 20235.83 Test Loss 6361.742702631266\n",
      "383 Train Loss 20232.816 Test Loss 6381.621059797982\n",
      "384 Train Loss 20226.838 Test Loss 6427.034643118942\n",
      "385 Train Loss 20220.549 Test Loss 6413.997917816821\n",
      "386 Train Loss 20213.42 Test Loss 6334.824829318124\n",
      "387 Train Loss 20208.576 Test Loss 6326.9747267797675\n",
      "388 Train Loss 20206.762 Test Loss 6277.563920318163\n",
      "389 Train Loss 20209.05 Test Loss 6228.199557567987\n",
      "390 Train Loss 20205.432 Test Loss 6249.275367044259\n",
      "391 Train Loss 20206.934 Test Loss 6238.30861682499\n",
      "392 Train Loss 20203.54 Test Loss 6242.755498117621\n",
      "393 Train Loss 20201.43 Test Loss 6280.252496274441\n",
      "394 Train Loss 20196.232 Test Loss 6307.6323205431645\n",
      "395 Train Loss 20206.52 Test Loss 6343.4119643615695\n",
      "396 Train Loss 20188.168 Test Loss 6320.5963680883115\n",
      "397 Train Loss 20184.148 Test Loss 6287.972757391565\n",
      "398 Train Loss 20178.188 Test Loss 6273.97270028445\n",
      "399 Train Loss 20185.215 Test Loss 6290.592407346608\n",
      "400 Train Loss 20175.928 Test Loss 6277.586104634934\n",
      "401 Train Loss 20173.531 Test Loss 6291.42895717526\n",
      "402 Train Loss 20170.695 Test Loss 6348.361667474561\n",
      "403 Train Loss 20167.58 Test Loss 6340.995797548861\n",
      "404 Train Loss 20163.514 Test Loss 6335.345003162158\n",
      "405 Train Loss 20157.385 Test Loss 6354.878909257465\n",
      "406 Train Loss 20155.197 Test Loss 6331.644867126351\n",
      "407 Train Loss 20152.377 Test Loss 6299.851753737885\n",
      "408 Train Loss 87344.59 Test Loss 77464.30892271755\n",
      "409 Train Loss 20140.742 Test Loss 6221.93797247151\n",
      "410 Train Loss 20137.104 Test Loss 6218.740956490485\n",
      "411 Train Loss 20208.322 Test Loss 6185.020525087177\n",
      "412 Train Loss 20136.988 Test Loss 6219.225207569715\n",
      "413 Train Loss 20136.621 Test Loss 6221.570368398315\n",
      "414 Train Loss 20134.844 Test Loss 6291.664470462155\n",
      "415 Train Loss 20133.533 Test Loss 6323.706335059624\n",
      "416 Train Loss 20131.416 Test Loss 6313.582311411492\n",
      "417 Train Loss 20125.963 Test Loss 6257.626449227006\n",
      "418 Train Loss 20123.148 Test Loss 6231.552099367404\n",
      "419 Train Loss 20116.494 Test Loss 6179.340737008024\n",
      "420 Train Loss 20145.814 Test Loss 6172.713319570657\n",
      "421 Train Loss 20114.166 Test Loss 6173.73236132395\n",
      "422 Train Loss 20400.504 Test Loss 6126.856783584091\n",
      "423 Train Loss 20111.234 Test Loss 6160.46354397902\n",
      "424 Train Loss 20111.615 Test Loss 6170.202413894576\n",
      "425 Train Loss 20109.346 Test Loss 6166.972097463619\n",
      "426 Train Loss 20109.387 Test Loss 6259.517512651276\n",
      "427 Train Loss 20107.17 Test Loss 6202.884863346849\n",
      "428 Train Loss 20105.297 Test Loss 6238.023287220447\n",
      "429 Train Loss 20106.68 Test Loss 6323.356552718033\n",
      "430 Train Loss 20104.303 Test Loss 6270.492163214105\n",
      "431 Train Loss 20101.402 Test Loss 6266.980916912918\n",
      "432 Train Loss 79327.36 Test Loss 69935.96181664086\n",
      "433 Train Loss 20096.014 Test Loss 6257.111169857148\n",
      "434 Train Loss 20091.973 Test Loss 6216.904041273208\n",
      "435 Train Loss 20089.717 Test Loss 6171.058639341621\n",
      "436 Train Loss 20085.48 Test Loss 6175.142805328126\n",
      "437 Train Loss 20093.596 Test Loss 6251.128131994387\n",
      "438 Train Loss 20082.959 Test Loss 6196.978681453296\n",
      "439 Train Loss 20076.637 Test Loss 6242.5656750087055\n",
      "440 Train Loss 20071.143 Test Loss 6259.4511629154595\n",
      "441 Train Loss 20067.246 Test Loss 6242.589553288048\n",
      "442 Train Loss 20067.615 Test Loss 6225.251256169531\n",
      "443 Train Loss 20065.695 Test Loss 6232.114867803675\n",
      "444 Train Loss 20062.426 Test Loss 6200.84806035309\n",
      "445 Train Loss 20058.564 Test Loss 6215.995630257612\n",
      "446 Train Loss 20051.365 Test Loss 6221.788964826327\n",
      "447 Train Loss 20037.979 Test Loss 6210.8682682747785\n",
      "448 Train Loss 20205.215 Test Loss 6550.988929431137\n",
      "449 Train Loss 20035.996 Test Loss 6217.132188529291\n",
      "450 Train Loss 20033.775 Test Loss 6224.929905720942\n",
      "451 Train Loss 20269.996 Test Loss 6971.958547837657\n",
      "452 Train Loss 20030.133 Test Loss 6250.1985819921465\n",
      "453 Train Loss 20025.408 Test Loss 6287.382014338178\n",
      "454 Train Loss 21021.596 Test Loss 7907.065728297984\n",
      "455 Train Loss 20023.916 Test Loss 6298.774962787555\n",
      "456 Train Loss 20026.34 Test Loss 6373.70553157301\n",
      "457 Train Loss 20021.533 Test Loss 6332.429607606514\n",
      "458 Train Loss 20009.506 Test Loss 6453.047052169402\n",
      "459 Train Loss 20000.936 Test Loss 6615.881207738508\n",
      "460 Train Loss 19997.117 Test Loss 6640.667433113564\n",
      "461 Train Loss 19994.506 Test Loss 6668.312285828008\n",
      "462 Train Loss 19991.781 Test Loss 6741.7606479908445\n",
      "463 Train Loss 19990.504 Test Loss 6720.839901782984\n",
      "464 Train Loss 19988.4 Test Loss 6667.758553054399\n",
      "465 Train Loss 19986.686 Test Loss 6606.284258784157\n",
      "466 Train Loss 19985.375 Test Loss 6517.201189511607\n",
      "467 Train Loss 19983.39 Test Loss 6529.458882353234\n",
      "468 Train Loss 19980.576 Test Loss 6508.164476796569\n",
      "469 Train Loss 19977.8 Test Loss 6485.63294734338\n",
      "470 Train Loss 19975.723 Test Loss 6544.802457083313\n",
      "471 Train Loss 19972.422 Test Loss 6649.742899609229\n",
      "472 Train Loss 19970.795 Test Loss 6643.162283184993\n",
      "473 Train Loss 19966.982 Test Loss 6582.319121790689\n",
      "474 Train Loss 19964.469 Test Loss 6466.938755760698\n",
      "475 Train Loss 19960.9 Test Loss 6510.055533157703\n",
      "476 Train Loss 19960.041 Test Loss 6638.104783465554\n",
      "477 Train Loss 19958.74 Test Loss 6586.424280007404\n",
      "478 Train Loss 19957.027 Test Loss 6674.527816058165\n",
      "479 Train Loss 19954.588 Test Loss 6676.717999906072\n",
      "480 Train Loss 19950.188 Test Loss 6691.4998912981255\n",
      "481 Train Loss 19946.127 Test Loss 6710.397070304964\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "482 Train Loss 20038.941 Test Loss 6953.11545949383\n",
      "483 Train Loss 19943.264 Test Loss 6735.4247528272845\n",
      "484 Train Loss 19941.81 Test Loss 6750.616516020612\n",
      "485 Train Loss 20034.855 Test Loss 7005.516467992015\n",
      "486 Train Loss 19940.02 Test Loss 6769.606280315913\n",
      "487 Train Loss 19934.838 Test Loss 6754.14222304089\n",
      "488 Train Loss 19930.71 Test Loss 6658.063084576749\n",
      "489 Train Loss 19925.098 Test Loss 6693.031318841346\n",
      "490 Train Loss 19924.197 Test Loss 6805.196724627703\n",
      "491 Train Loss 19922.021 Test Loss 6754.362875997323\n",
      "492 Train Loss 19920.738 Test Loss 6816.38171608892\n",
      "493 Train Loss 19918.754 Test Loss 6818.536005204823\n",
      "494 Train Loss 19915.168 Test Loss 6804.7668999753705\n",
      "495 Train Loss 32150.355 Test Loss 20305.79984964514\n",
      "496 Train Loss 19915.074 Test Loss 6804.5993903614035\n",
      "497 Train Loss 19920.406 Test Loss 6796.322016394876\n",
      "498 Train Loss 19911.328 Test Loss 6795.773606748164\n",
      "499 Train Loss 19914.107 Test Loss 6792.8367953355455\n",
      "500 Train Loss 19910.062 Test Loss 6793.368540661305\n",
      "501 Train Loss 19907.994 Test Loss 6775.343694662251\n",
      "502 Train Loss 19904.262 Test Loss 6746.919411976482\n",
      "503 Train Loss 19894.521 Test Loss 6782.440831122196\n",
      "504 Train Loss 19884.992 Test Loss 6806.300192275248\n",
      "505 Train Loss 19890.98 Test Loss 6845.443746126023\n",
      "506 Train Loss 19879.016 Test Loss 6823.513185947409\n",
      "507 Train Loss 19873.05 Test Loss 6882.016789645492\n",
      "508 Train Loss 19868.549 Test Loss 6980.356376379917\n",
      "509 Train Loss 19865.361 Test Loss 6926.123631908707\n",
      "510 Train Loss 19863.078 Test Loss 6903.929274401281\n",
      "511 Train Loss 19860.025 Test Loss 6908.80894061639\n",
      "512 Train Loss 19856.62 Test Loss 6955.162910148904\n",
      "513 Train Loss 19841.346 Test Loss 7235.919175149454\n",
      "514 Train Loss 19863.717 Test Loss 7448.708636881051\n",
      "515 Train Loss 19835.393 Test Loss 7248.349652834404\n",
      "516 Train Loss 19868.918 Test Loss 7159.509881676434\n",
      "517 Train Loss 19828.475 Test Loss 7204.746977269707\n",
      "518 Train Loss 19840.57 Test Loss 7110.258977385209\n",
      "519 Train Loss 19825.785 Test Loss 7174.805764127975\n",
      "520 Train Loss 19821.66 Test Loss 7169.798209004849\n",
      "521 Train Loss 19823.732 Test Loss 7085.131877200919\n",
      "522 Train Loss 19819.402 Test Loss 7134.782374727007\n",
      "523 Train Loss 19817.39 Test Loss 7164.083747065561\n",
      "524 Train Loss 19815.477 Test Loss 7190.579632457187\n",
      "525 Train Loss 19813.215 Test Loss 7210.540500649464\n",
      "526 Train Loss 19811.318 Test Loss 7200.980420387966\n",
      "527 Train Loss 19807.787 Test Loss 7152.02816327797\n",
      "528 Train Loss 19805.459 Test Loss 7107.881870355194\n",
      "529 Train Loss 19851.545 Test Loss 6575.234905311477\n",
      "530 Train Loss 19801.682 Test Loss 7001.529999799706\n",
      "531 Train Loss 19800.459 Test Loss 6887.121975764781\n",
      "532 Train Loss 19796.955 Test Loss 6997.725667038199\n",
      "533 Train Loss 19792.904 Test Loss 7131.581732214099\n",
      "534 Train Loss 19790.223 Test Loss 7087.516664371686\n",
      "535 Train Loss 19787.75 Test Loss 7031.595786971336\n",
      "536 Train Loss 19784.531 Test Loss 6961.190081087082\n",
      "537 Train Loss 19782.205 Test Loss 6940.969939719962\n",
      "538 Train Loss 19779.71 Test Loss 6930.7056461569155\n",
      "539 Train Loss 19774.664 Test Loss 6960.35363762477\n",
      "540 Train Loss 19772.531 Test Loss 6949.33230367929\n",
      "541 Train Loss 19769.352 Test Loss 6908.522132981353\n",
      "542 Train Loss 19766.293 Test Loss 6876.2787750838\n",
      "543 Train Loss 19762.744 Test Loss 6912.434269879769\n",
      "544 Train Loss 19756.148 Test Loss 7058.224464650991\n",
      "545 Train Loss 19754.125 Test Loss 7047.007406081018\n",
      "546 Train Loss 19750.053 Test Loss 7095.69549276151\n",
      "547 Train Loss 19746.434 Test Loss 7232.076525871027\n",
      "548 Train Loss 19743.432 Test Loss 7213.303345985316\n",
      "549 Train Loss 19741.975 Test Loss 7196.089543309828\n",
      "550 Train Loss 19740.014 Test Loss 7188.8698899486035\n",
      "551 Train Loss 19738.564 Test Loss 7211.372089628177\n",
      "552 Train Loss 19736.887 Test Loss 7186.053954054527\n",
      "553 Train Loss 19737.916 Test Loss 7123.943691037871\n",
      "554 Train Loss 19734.834 Test Loss 7154.343866442269\n",
      "555 Train Loss 19733.344 Test Loss 7078.164134745999\n",
      "556 Train Loss 19732.395 Test Loss 7059.499088061218\n",
      "557 Train Loss 19731.453 Test Loss 7054.828977560641\n",
      "558 Train Loss 19729.174 Test Loss 7025.598851182064\n",
      "559 Train Loss 20913.525 Test Loss 6627.615871197483\n",
      "560 Train Loss 19728.344 Test Loss 7012.20848601291\n",
      "561 Train Loss 19726.752 Test Loss 6939.171625663061\n",
      "562 Train Loss 19724.78 Test Loss 6964.87651356959\n",
      "563 Train Loss 19722.52 Test Loss 6969.032802757125\n",
      "564 Train Loss 19721.494 Test Loss 6953.005808615995\n",
      "565 Train Loss 19719.855 Test Loss 6957.600315013935\n",
      "566 Train Loss 19716.648 Test Loss 6969.842554044325\n",
      "567 Train Loss 19714.29 Test Loss 6972.302806222166\n",
      "568 Train Loss 19712.092 Test Loss 6968.408775274969\n",
      "569 Train Loss 19710.186 Test Loss 6944.303724598921\n",
      "570 Train Loss 19709.576 Test Loss 6925.41247042474\n",
      "571 Train Loss 19708.828 Test Loss 6965.050710717537\n",
      "572 Train Loss 19708.416 Test Loss 6965.615455889123\n",
      "573 Train Loss 19707.943 Test Loss 6978.516104725351\n",
      "574 Train Loss 19707.504 Test Loss 6969.315100499026\n",
      "575 Train Loss 19707.139 Test Loss 6976.3380892690075\n",
      "576 Train Loss 19706.658 Test Loss 6999.222738019803\n",
      "577 Train Loss 19706.182 Test Loss 7010.8140637483275\n",
      "578 Train Loss 19705.672 Test Loss 7024.109544958948\n",
      "579 Train Loss 19704.982 Test Loss 7029.690847763739\n",
      "580 Train Loss 19703.86 Test Loss 7045.267955366777\n",
      "581 Train Loss 20189.23 Test Loss 7847.020158036755\n",
      "582 Train Loss 19738.615 Test Loss 7157.634010672446\n",
      "583 Train Loss 19703.316 Test Loss 7053.2320124935395\n",
      "584 Train Loss 19755.316 Test Loss 7175.857127506211\n",
      "585 Train Loss 19702.805 Test Loss 7059.658512466991\n",
      "586 Train Loss 19701.945 Test Loss 7074.048176697473\n",
      "587 Train Loss 19699.98 Test Loss 7068.331169437522\n",
      "588 Train Loss 19696.127 Test Loss 7073.735821773451\n",
      "589 Train Loss 19689.316 Test Loss 7106.611150344415\n",
      "590 Train Loss 19921.416 Test Loss 7531.096503391072\n",
      "591 Train Loss 19686.03 Test Loss 7148.6662435881235\n",
      "592 Train Loss 19695.95 Test Loss 7253.259263514911\n",
      "593 Train Loss 19682.477 Test Loss 7211.909658174433\n",
      "594 Train Loss 19679.566 Test Loss 7192.305109877006\n",
      "595 Train Loss 19677.477 Test Loss 7233.527960241031\n",
      "596 Train Loss 19677.256 Test Loss 7300.571314207653\n",
      "597 Train Loss 19676.746 Test Loss 7270.618203787669\n",
      "598 Train Loss 19675.51 Test Loss 7307.224111134382\n",
      "599 Train Loss 19674.236 Test Loss 7365.969148721097\n",
      "600 Train Loss 19673.31 Test Loss 7332.248571654037\n",
      "601 Train Loss 19672.703 Test Loss 7317.488512607186\n",
      "602 Train Loss 19671.83 Test Loss 7322.741680331691\n",
      "603 Train Loss 19670.941 Test Loss 7323.608828957067\n",
      "604 Train Loss 20106.521 Test Loss 7479.930780549987\n",
      "605 Train Loss 19670.61 Test Loss 7314.445246861159\n",
      "606 Train Loss 19670.365 Test Loss 7318.611364389059\n",
      "607 Train Loss 19669.73 Test Loss 7297.646014510679\n",
      "608 Train Loss 19666.416 Test Loss 7303.869085711653\n",
      "609 Train Loss 19662.143 Test Loss 7417.825887919129\n",
      "610 Train Loss 19657.227 Test Loss 7428.7124625036195\n",
      "611 Train Loss 19656.324 Test Loss 7434.380094914145\n",
      "612 Train Loss 19658.037 Test Loss 7409.087013770053\n",
      "613 Train Loss 19654.336 Test Loss 7423.547917629875\n",
      "614 Train Loss 19653.03 Test Loss 7404.111401276231\n",
      "615 Train Loss 19650.355 Test Loss 7382.012782361249\n",
      "616 Train Loss 19648.465 Test Loss 7415.813613174733\n",
      "617 Train Loss 19682.92 Test Loss 7479.830860151578\n",
      "618 Train Loss 19647.725 Test Loss 7423.181347841519\n",
      "619 Train Loss 19646.996 Test Loss 7430.065471910623\n",
      "620 Train Loss 21709.223 Test Loss 9532.444991920323\n",
      "621 Train Loss 19754.258 Test Loss 7601.962445231073\n",
      "622 Train Loss 19646.37 Test Loss 7438.846871514765\n",
      "623 Train Loss 19643.508 Test Loss 7465.141321726842\n",
      "624 Train Loss 19640.66 Test Loss 7472.074947617906\n",
      "625 Train Loss 19639.703 Test Loss 7430.374339393855\n",
      "626 Train Loss 19633.928 Test Loss 7115.811825093738\n",
      "627 Train Loss 19634.148 Test Loss 7106.57111622531\n",
      "628 Train Loss 19631.818 Test Loss 7113.333926057384\n",
      "629 Train Loss 19627.39 Test Loss 7215.190125280367\n",
      "630 Train Loss 19621.592 Test Loss 7204.449548879646\n",
      "631 Train Loss 19622.781 Test Loss 7182.984788717255\n",
      "632 Train Loss 19618.748 Test Loss 7193.606225738636\n",
      "633 Train Loss 19614.156 Test Loss 7203.625112737619\n",
      "634 Train Loss 19614.246 Test Loss 7243.661062712524\n",
      "635 Train Loss 19611.287 Test Loss 7225.4259545382065\n",
      "636 Train Loss 19645.35 Test Loss 7292.03288480496\n",
      "637 Train Loss 19610.773 Test Loss 7229.0664851550355\n",
      "638 Train Loss 19614.244 Test Loss 7350.870963434219\n",
      "639 Train Loss 19609.457 Test Loss 7267.88351018717\n",
      "640 Train Loss 19607.574 Test Loss 7230.337488440611\n",
      "641 Train Loss 19604.773 Test Loss 7180.27934867472\n",
      "642 Train Loss 19603.13 Test Loss 7193.632347517049\n",
      "643 Train Loss 19602.016 Test Loss 7206.865163077593\n",
      "644 Train Loss 19599.828 Test Loss 7218.009410881125\n",
      "645 Train Loss 19598.365 Test Loss 7203.78960207309\n",
      "646 Train Loss 19595.52 Test Loss 7203.470046557196\n",
      "647 Train Loss 19592.266 Test Loss 7135.606949503054\n",
      "648 Train Loss 19589.574 Test Loss 7066.859593251302\n",
      "649 Train Loss 19586.285 Test Loss 7045.269554976976\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "650 Train Loss 19867.262 Test Loss 7288.4152957757715\n",
      "651 Train Loss 19586.592 Test Loss 7041.438571338865\n",
      "652 Train Loss 19585.26 Test Loss 7042.107834932547\n",
      "653 Train Loss 19987.59 Test Loss 7392.849276558104\n",
      "654 Train Loss 19583.793 Test Loss 7042.633859113733\n",
      "655 Train Loss 26144.477 Test Loss 13366.46519381341\n",
      "656 Train Loss 19583.342 Test Loss 7042.948914320597\n",
      "657 Train Loss 19603.46 Test Loss 7050.284214261935\n",
      "658 Train Loss 19582.545 Test Loss 7042.5408684377\n",
      "659 Train Loss 19580.783 Test Loss 7223.625214761427\n",
      "660 Train Loss 19575.668 Test Loss 7125.931875757391\n",
      "661 Train Loss 19579.135 Test Loss 6959.663925179015\n",
      "662 Train Loss 19574.023 Test Loss 7058.477448996023\n",
      "663 Train Loss 19572.941 Test Loss 7023.43702981903\n",
      "664 Train Loss 19571.28 Test Loss 7076.494159663885\n",
      "665 Train Loss 19570.45 Test Loss 7087.426462658098\n",
      "666 Train Loss 19569.19 Test Loss 7125.003332695733\n",
      "667 Train Loss 19568.244 Test Loss 7139.985055521406\n",
      "668 Train Loss 19567.656 Test Loss 7159.781297134713\n",
      "669 Train Loss 19567.158 Test Loss 7155.653784287743\n",
      "670 Train Loss 19566.748 Test Loss 7163.857145122627\n",
      "671 Train Loss 19566.51 Test Loss 7163.935411105823\n",
      "672 Train Loss 19566.25 Test Loss 7161.667936319765\n",
      "673 Train Loss 19565.643 Test Loss 7160.370383147099\n",
      "674 Train Loss 19565.248 Test Loss 7168.9280987452485\n",
      "675 Train Loss 19564.686 Test Loss 7159.529158696715\n",
      "676 Train Loss 19564.297 Test Loss 7162.187000924073\n",
      "677 Train Loss 19563.63 Test Loss 7158.668636369358\n",
      "678 Train Loss 19563.188 Test Loss 7142.676400819535\n",
      "679 Train Loss 19562.537 Test Loss 7145.470885103967\n",
      "680 Train Loss 19561.016 Test Loss 7139.030418938029\n",
      "681 Train Loss 19561.215 Test Loss 7134.297040473601\n",
      "682 Train Loss 19560.43 Test Loss 7136.441763542013\n",
      "683 Train Loss 19805.34 Test Loss 7394.124809105216\n",
      "684 Train Loss 19558.873 Test Loss 7102.500043237873\n",
      "685 Train Loss 19571.496 Test Loss 7038.112277396608\n",
      "686 Train Loss 19556.36 Test Loss 7074.9856760279545\n",
      "687 Train Loss 19549.645 Test Loss 7036.4845398307925\n",
      "688 Train Loss 19542.877 Test Loss 6946.021290198076\n",
      "689 Train Loss 19535.203 Test Loss 6866.872650035366\n",
      "690 Train Loss 19533.541 Test Loss 6898.375355287726\n",
      "691 Train Loss 19531.686 Test Loss 6911.989737446295\n",
      "692 Train Loss 19529.445 Test Loss 6862.894759030889\n",
      "693 Train Loss 19526.793 Test Loss 6862.71410168652\n",
      "694 Train Loss 19522.967 Test Loss 6868.694824866773\n",
      "695 Train Loss 19521.572 Test Loss 6856.342368740145\n",
      "696 Train Loss 19519.02 Test Loss 6869.62276100039\n",
      "697 Train Loss 19517.67 Test Loss 6878.992008036382\n",
      "698 Train Loss 19514.178 Test Loss 6850.990920985042\n",
      "699 Train Loss 19513.389 Test Loss 6762.7841342963175\n",
      "700 Train Loss 19511.342 Test Loss 6785.125030054995\n",
      "701 Train Loss 19510.975 Test Loss 6791.80765312666\n",
      "702 Train Loss 19509.959 Test Loss 6806.736377308914\n",
      "703 Train Loss 19508.686 Test Loss 6820.7740877694105\n",
      "704 Train Loss 19507.01 Test Loss 6929.738699990299\n",
      "705 Train Loss 19505.768 Test Loss 6885.12097409328\n",
      "706 Train Loss 19506.518 Test Loss 7014.188522793441\n",
      "707 Train Loss 19503.973 Test Loss 6944.724687849555\n",
      "708 Train Loss 19508.098 Test Loss 6888.536536919663\n",
      "709 Train Loss 19503.014 Test Loss 6926.547982959115\n",
      "710 Train Loss 19502.543 Test Loss 6937.090486479091\n",
      "711 Train Loss 19501.246 Test Loss 6970.244548350647\n",
      "712 Train Loss 19500.002 Test Loss 6991.296812570318\n",
      "713 Train Loss 19498.812 Test Loss 6996.60191544801\n",
      "714 Train Loss 19497.914 Test Loss 7001.272680481945\n",
      "715 Train Loss 19497.303 Test Loss 6998.733896254259\n",
      "716 Train Loss 19496.098 Test Loss 6993.261656142848\n",
      "717 Train Loss 19495.215 Test Loss 6994.098405233854\n",
      "718 Train Loss 19493.877 Test Loss 7020.347330777839\n",
      "719 Train Loss 19492.285 Test Loss 7043.68372345756\n",
      "720 Train Loss 19490.182 Test Loss 7072.211837414233\n",
      "721 Train Loss 19488.904 Test Loss 7077.419216083536\n",
      "722 Train Loss 19487.654 Test Loss 7102.396380707334\n",
      "723 Train Loss 19486.812 Test Loss 7119.847349156803\n",
      "724 Train Loss 19486.465 Test Loss 7084.6271240881415\n",
      "725 Train Loss 19486.0 Test Loss 7083.871834589235\n",
      "726 Train Loss 19485.643 Test Loss 7091.262754561882\n",
      "727 Train Loss 19484.338 Test Loss 7091.87608853755\n",
      "728 Train Loss 19483.387 Test Loss 7080.20958017578\n",
      "729 Train Loss 19482.734 Test Loss 7068.124836324248\n",
      "730 Train Loss 19482.043 Test Loss 7061.505168571687\n",
      "731 Train Loss 19481.422 Test Loss 7056.387809767534\n",
      "732 Train Loss 19479.943 Test Loss 7024.935941675268\n",
      "733 Train Loss 19478.496 Test Loss 7011.690388974027\n",
      "734 Train Loss 19476.855 Test Loss 6958.999426280028\n",
      "735 Train Loss 19475.467 Test Loss 6969.984071425648\n",
      "736 Train Loss 19474.582 Test Loss 6915.377844180438\n",
      "737 Train Loss 19473.584 Test Loss 6908.937182555711\n",
      "738 Train Loss 19472.436 Test Loss 6893.153147187092\n",
      "739 Train Loss 19472.885 Test Loss 6794.818614214567\n",
      "740 Train Loss 19470.432 Test Loss 6845.1660661515225\n",
      "741 Train Loss 19467.545 Test Loss 6751.057904669803\n",
      "742 Train Loss 19462.191 Test Loss 6721.029177382884\n",
      "743 Train Loss 19733.623 Test Loss 6763.979522054497\n",
      "744 Train Loss 19462.008 Test Loss 6718.136643315525\n",
      "745 Train Loss 19475.049 Test Loss 6594.6811058677185\n",
      "746 Train Loss 19457.646 Test Loss 6673.247567421777\n",
      "747 Train Loss 19457.785 Test Loss 6587.367606475151\n",
      "748 Train Loss 19455.832 Test Loss 6628.273860891964\n",
      "749 Train Loss 19455.566 Test Loss 6577.826784682369\n",
      "750 Train Loss 19454.576 Test Loss 6601.055320737463\n",
      "751 Train Loss 19453.482 Test Loss 6606.15837776939\n",
      "752 Train Loss 19452.348 Test Loss 6629.491059293854\n",
      "753 Train Loss 19451.664 Test Loss 6644.870826315129\n",
      "754 Train Loss 19450.447 Test Loss 6698.808592624996\n",
      "755 Train Loss 19449.338 Test Loss 6716.179861330162\n",
      "756 Train Loss 19448.123 Test Loss 6708.27923725749\n",
      "757 Train Loss 19447.35 Test Loss 6719.6657135058185\n",
      "758 Train Loss 19445.771 Test Loss 6704.836388126635\n",
      "759 Train Loss 19444.63 Test Loss 6677.030783858612\n",
      "760 Train Loss 19443.75 Test Loss 6691.46865266776\n",
      "761 Train Loss 19443.398 Test Loss 6699.659028116518\n",
      "762 Train Loss 19442.941 Test Loss 6699.8817718065375\n",
      "763 Train Loss 19442.498 Test Loss 6712.265668195214\n",
      "764 Train Loss 19441.967 Test Loss 6714.462965506822\n",
      "765 Train Loss 19441.29 Test Loss 6724.975295278552\n",
      "766 Train Loss 19440.21 Test Loss 6747.281484581945\n",
      "767 Train Loss 19439.084 Test Loss 6783.675868580043\n",
      "768 Train Loss 19438.365 Test Loss 6850.730041090431\n",
      "769 Train Loss 19437.463 Test Loss 6823.648685629395\n",
      "770 Train Loss 19437.129 Test Loss 6827.222107940968\n",
      "771 Train Loss 19437.03 Test Loss 6842.605569234567\n",
      "772 Train Loss 19436.846 Test Loss 6836.569125555139\n",
      "773 Train Loss 19436.287 Test Loss 6832.125181853065\n",
      "774 Train Loss 19434.357 Test Loss 6795.975156135119\n",
      "775 Train Loss 19433.344 Test Loss 6785.62155339351\n",
      "776 Train Loss 19430.818 Test Loss 6726.797057928277\n",
      "777 Train Loss 19427.174 Test Loss 6642.782654799296\n",
      "778 Train Loss 19421.586 Test Loss 6509.52162823718\n",
      "779 Train Loss 19418.012 Test Loss 6473.880722173204\n",
      "780 Train Loss 19413.871 Test Loss 6488.173059673244\n",
      "781 Train Loss 19434.27 Test Loss 6476.347582590212\n",
      "782 Train Loss 19412.66 Test Loss 6484.769134137179\n",
      "783 Train Loss 19412.52 Test Loss 6527.788619510135\n",
      "784 Train Loss 19411.568 Test Loss 6507.18145877209\n",
      "785 Train Loss 19410.729 Test Loss 6594.9284967673\n",
      "786 Train Loss 19409.432 Test Loss 6595.868151710818\n",
      "787 Train Loss 19409.021 Test Loss 6580.0514360016605\n",
      "788 Train Loss 19407.598 Test Loss 6598.293463240606\n",
      "789 Train Loss 19405.629 Test Loss 6641.475442629464\n",
      "790 Train Loss 19407.352 Test Loss 6678.954981654199\n",
      "791 Train Loss 19403.367 Test Loss 6667.388638716121\n",
      "792 Train Loss 19400.28 Test Loss 6671.451511201249\n",
      "793 Train Loss 19401.275 Test Loss 6691.157409247515\n",
      "794 Train Loss 19398.658 Test Loss 6682.805550153395\n",
      "795 Train Loss 19397.658 Test Loss 6680.367245731052\n",
      "796 Train Loss 19458.64 Test Loss 6573.480200880557\n",
      "797 Train Loss 19392.982 Test Loss 6626.74972831278\n",
      "798 Train Loss 19391.213 Test Loss 6657.167845007435\n",
      "799 Train Loss 19385.963 Test Loss 6612.758295244027\n",
      "800 Train Loss 19605.535 Test Loss 6629.8909429915075\n",
      "801 Train Loss 19385.58 Test Loss 6601.543986867319\n",
      "802 Train Loss 19382.787 Test Loss 6579.438685297453\n",
      "803 Train Loss 19378.59 Test Loss 6539.205105770084\n",
      "804 Train Loss 19374.52 Test Loss 6488.972797789314\n",
      "805 Train Loss 19433.63 Test Loss 6273.507384694898\n",
      "806 Train Loss 19368.19 Test Loss 6433.116150115708\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "807 Train Loss 19375.947 Test Loss 6305.979584759362\n",
      "808 Train Loss 19363.246 Test Loss 6389.108630671753\n",
      "809 Train Loss 19358.068 Test Loss 6798.754735524592\n",
      "810 Train Loss 19342.924 Test Loss 6785.6006833688325\n",
      "811 Train Loss 19346.533 Test Loss 6428.582279242015\n",
      "812 Train Loss 19332.775 Test Loss 6628.830837016526\n",
      "813 Train Loss 19330.266 Test Loss 6604.562749584181\n",
      "814 Train Loss 19328.87 Test Loss 6591.608857177719\n",
      "815 Train Loss 19327.357 Test Loss 6602.712818836486\n",
      "816 Train Loss 19325.064 Test Loss 6662.236387010823\n",
      "817 Train Loss 19322.014 Test Loss 6719.214934183121\n",
      "818 Train Loss 19313.582 Test Loss 6879.630164742493\n",
      "819 Train Loss 19321.307 Test Loss 7364.207876487798\n",
      "820 Train Loss 19307.543 Test Loss 7100.632353121116\n",
      "821 Train Loss 19306.006 Test Loss 7380.4411364877315\n",
      "822 Train Loss 19299.955 Test Loss 7269.211479274086\n",
      "823 Train Loss 19299.18 Test Loss 6972.933828480516\n",
      "824 Train Loss 19294.002 Test Loss 7068.805800604024\n",
      "825 Train Loss 19291.258 Test Loss 7162.083344761727\n",
      "826 Train Loss 19289.205 Test Loss 7173.330708216627\n",
      "827 Train Loss 19287.916 Test Loss 7136.3963359756335\n",
      "828 Train Loss 19285.648 Test Loss 7030.477045970369\n",
      "829 Train Loss 19284.055 Test Loss 6977.725185293713\n",
      "830 Train Loss 19282.254 Test Loss 6890.519122331452\n",
      "831 Train Loss 19283.42 Test Loss 6844.752841220696\n",
      "832 Train Loss 19281.982 Test Loss 6874.42023018402\n",
      "833 Train Loss 19281.375 Test Loss 6823.022551382347\n",
      "834 Train Loss 19280.756 Test Loss 6869.819533284762\n",
      "835 Train Loss 19280.31 Test Loss 6863.967170717956\n",
      "836 Train Loss 19279.574 Test Loss 6871.63286342496\n",
      "837 Train Loss 19278.803 Test Loss 6841.717971267215\n",
      "838 Train Loss 19277.744 Test Loss 6861.5765550108745\n",
      "839 Train Loss 19276.633 Test Loss 6875.733728634375\n",
      "840 Train Loss 19275.385 Test Loss 6879.203277586759\n",
      "841 Train Loss 19274.143 Test Loss 6891.457580317378\n",
      "842 Train Loss 19272.322 Test Loss 6892.574778374635\n",
      "843 Train Loss 19270.271 Test Loss 6900.585770951331\n",
      "844 Train Loss 19270.312 Test Loss 6860.564117432435\n",
      "845 Train Loss 19269.453 Test Loss 6884.708975180238\n",
      "846 Train Loss 19268.354 Test Loss 6893.889961773396\n",
      "847 Train Loss 19266.293 Test Loss 6895.29814099115\n",
      "848 Train Loss 19265.574 Test Loss 6901.536349337993\n",
      "849 Train Loss 19264.885 Test Loss 6869.85121903265\n",
      "850 Train Loss 19264.602 Test Loss 6874.947683223378\n",
      "851 Train Loss 19264.322 Test Loss 6857.749824876585\n",
      "852 Train Loss 19263.95 Test Loss 6844.906896003621\n",
      "853 Train Loss 19263.352 Test Loss 6841.85169811596\n",
      "854 Train Loss 19262.451 Test Loss 6854.090577643325\n",
      "855 Train Loss 19261.021 Test Loss 6867.051916010109\n",
      "856 Train Loss 19259.18 Test Loss 6909.545175198827\n",
      "857 Train Loss 19258.855 Test Loss 6993.348936345306\n",
      "858 Train Loss 19257.154 Test Loss 7057.2653041698995\n",
      "859 Train Loss 19255.188 Test Loss 6980.84710178161\n",
      "860 Train Loss 19254.47 Test Loss 6939.109034811189\n",
      "861 Train Loss 19253.71 Test Loss 6908.175796066231\n",
      "862 Train Loss 19252.256 Test Loss 6880.369128684892\n",
      "863 Train Loss 19249.096 Test Loss 6681.2691883002235\n",
      "864 Train Loss 19245.197 Test Loss 6425.233073467035\n",
      "865 Train Loss 19250.023 Test Loss 6360.570205225984\n",
      "866 Train Loss 19240.076 Test Loss 6396.748494279837\n",
      "867 Train Loss 19241.219 Test Loss 6552.027703250959\n",
      "868 Train Loss 19236.428 Test Loss 6466.608130430781\n",
      "869 Train Loss 19235.182 Test Loss 6730.902891165061\n",
      "870 Train Loss 19229.355 Test Loss 6613.67638676426\n",
      "871 Train Loss 19226.848 Test Loss 6542.1749271156095\n",
      "872 Train Loss 19221.18 Test Loss 6589.511084338751\n",
      "873 Train Loss 19213.355 Test Loss 6672.5904840463645\n",
      "874 Train Loss 19205.01 Test Loss 6711.803271379369\n",
      "875 Train Loss 19195.527 Test Loss 6755.898234867167\n",
      "876 Train Loss 19186.172 Test Loss 6910.412916228766\n",
      "877 Train Loss 19181.527 Test Loss 7247.43844476637\n",
      "878 Train Loss 19174.072 Test Loss 7151.7531962788125\n",
      "879 Train Loss 19176.625 Test Loss 6893.146576559848\n",
      "880 Train Loss 19171.295 Test Loss 7039.718362231872\n",
      "881 Train Loss 19169.393 Test Loss 7009.043969061667\n",
      "882 Train Loss 19167.238 Test Loss 7016.408119466743\n",
      "883 Train Loss 19166.021 Test Loss 7013.862904742668\n",
      "884 Train Loss 19162.4 Test Loss 6878.209301501441\n",
      "885 Train Loss 19159.922 Test Loss 6922.638380573202\n",
      "886 Train Loss 19158.537 Test Loss 6950.406042597344\n",
      "887 Train Loss 19157.525 Test Loss 6929.672593838767\n",
      "888 Train Loss 19156.281 Test Loss 6932.077225111899\n",
      "889 Train Loss 19154.994 Test Loss 6935.290083782356\n",
      "890 Train Loss 19153.646 Test Loss 6913.486155314029\n",
      "891 Train Loss 19152.84 Test Loss 6814.937732227245\n",
      "892 Train Loss 19151.453 Test Loss 6871.812662457532\n",
      "893 Train Loss 19150.695 Test Loss 6877.9063860126635\n",
      "894 Train Loss 19216.395 Test Loss 6918.028147152401\n",
      "895 Train Loss 19149.74 Test Loss 6882.029302400607\n",
      "896 Train Loss 19148.115 Test Loss 6877.671497010716\n",
      "897 Train Loss 19147.31 Test Loss 6850.852879265611\n",
      "898 Train Loss 19146.098 Test Loss 6822.685408461854\n",
      "899 Train Loss 19144.703 Test Loss 6828.656229342969\n",
      "900 Train Loss 19148.41 Test Loss 6863.198483655425\n",
      "901 Train Loss 19143.93 Test Loss 6839.203732085488\n",
      "902 Train Loss 19142.672 Test Loss 6873.140473614556\n",
      "903 Train Loss 19141.926 Test Loss 6907.88862850683\n",
      "904 Train Loss 19141.36 Test Loss 6928.406692953323\n",
      "905 Train Loss 19141.062 Test Loss 6921.34037352621\n",
      "906 Train Loss 19140.805 Test Loss 6925.876106302729\n",
      "907 Train Loss 19140.578 Test Loss 6928.754384009628\n",
      "908 Train Loss 19140.21 Test Loss 6928.329905107339\n",
      "909 Train Loss 19139.71 Test Loss 6923.038143966722\n",
      "910 Train Loss 19138.998 Test Loss 6921.880150918588\n",
      "911 Train Loss 19137.887 Test Loss 6918.360386478813\n",
      "912 Train Loss 19135.924 Test Loss 6898.6296308172095\n",
      "913 Train Loss 19134.627 Test Loss 6898.81513055893\n",
      "914 Train Loss 19346.695 Test Loss 6987.752416868205\n",
      "915 Train Loss 19142.754 Test Loss 6896.697196517447\n",
      "916 Train Loss 19134.338 Test Loss 6898.289451685007\n",
      "917 Train Loss 19133.977 Test Loss 6940.511055833993\n",
      "918 Train Loss 19133.45 Test Loss 6952.521798396728\n",
      "919 Train Loss 19132.246 Test Loss 6929.868118276746\n",
      "920 Train Loss 19130.71 Test Loss 6931.45255040623\n",
      "921 Train Loss 19130.564 Test Loss 6947.372280099501\n",
      "922 Train Loss 19129.979 Test Loss 6940.710771923241\n",
      "923 Train Loss 19128.523 Test Loss 6967.51141950772\n",
      "924 Train Loss 19125.887 Test Loss 6990.80791337767\n",
      "925 Train Loss 19127.193 Test Loss 7006.070004844502\n",
      "926 Train Loss 19123.348 Test Loss 7002.854340405939\n",
      "927 Train Loss 19121.18 Test Loss 6974.228684170728\n",
      "928 Train Loss 19119.033 Test Loss 6987.777371782805\n",
      "929 Train Loss 19126.486 Test Loss 6952.493741190912\n",
      "930 Train Loss 19116.125 Test Loss 6976.696671707162\n",
      "931 Train Loss 19118.117 Test Loss 6968.628071389571\n",
      "932 Train Loss 19114.47 Test Loss 6975.0182343697825\n",
      "933 Train Loss 19110.674 Test Loss 6965.49514642248\n",
      "934 Train Loss 19108.945 Test Loss 6925.633882359684\n",
      "935 Train Loss 19110.385 Test Loss 6915.839809831796\n",
      "936 Train Loss 19107.727 Test Loss 6921.664414326206\n",
      "937 Train Loss 19105.99 Test Loss 6934.459945318629\n",
      "938 Train Loss 19105.049 Test Loss 6921.523387583063\n",
      "939 Train Loss 19113.852 Test Loss 6933.236686462547\n",
      "940 Train Loss 19104.91 Test Loss 6922.949616839394\n",
      "941 Train Loss 19104.174 Test Loss 6920.4569772142695\n",
      "942 Train Loss 19103.77 Test Loss 6887.704462045288\n",
      "943 Train Loss 19103.24 Test Loss 6884.166400134197\n",
      "944 Train Loss 19102.557 Test Loss 6872.03005727533\n",
      "945 Train Loss 19101.463 Test Loss 6818.182776266955\n",
      "946 Train Loss 19100.541 Test Loss 6757.55122721244\n",
      "947 Train Loss 19099.938 Test Loss 6729.125486943131\n",
      "948 Train Loss 19098.469 Test Loss 6675.526683821637\n",
      "949 Train Loss 19137.574 Test Loss 6182.244168992018\n",
      "950 Train Loss 19098.598 Test Loss 6515.412534676405\n",
      "951 Train Loss 19097.973 Test Loss 6601.539488990693\n",
      "952 Train Loss 19096.764 Test Loss 6592.452572720137\n",
      "953 Train Loss 19091.904 Test Loss 6476.815869292888\n",
      "954 Train Loss 19327.346 Test Loss 5882.507819926204\n",
      "955 Train Loss 19098.502 Test Loss 6197.286272309273\n",
      "956 Train Loss 19085.35 Test Loss 6369.936687572082\n",
      "957 Train Loss 19083.865 Test Loss 6343.236128112871\n",
      "958 Train Loss 20682.424 Test Loss 6911.585242039994\n",
      "959 Train Loss 19088.932 Test Loss 6229.168801871052\n",
      "960 Train Loss 19083.86 Test Loss 6327.125258553968\n",
      "961 Train Loss 19079.498 Test Loss 6243.415746743075\n",
      "962 Train Loss 19092.666 Test Loss 6277.944406832534\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "963 Train Loss 19077.314 Test Loss 6244.049064972843\n",
      "964 Train Loss 19072.223 Test Loss 6371.543131424543\n",
      "965 Train Loss 19069.123 Test Loss 6380.139852312854\n",
      "966 Train Loss 19065.475 Test Loss 6325.052088406572\n",
      "967 Train Loss 19062.363 Test Loss 6327.2085278157065\n",
      "968 Train Loss 19174.18 Test Loss 6367.81382435566\n",
      "969 Train Loss 19052.15 Test Loss 6323.035642376444\n",
      "970 Train Loss 19046.854 Test Loss 6410.894205245365\n",
      "971 Train Loss 19041.768 Test Loss 6479.480578839629\n",
      "972 Train Loss 19052.871 Test Loss 6549.238325056973\n",
      "973 Train Loss 19039.838 Test Loss 6506.3742098707135\n",
      "974 Train Loss 19037.53 Test Loss 6482.1956729310095\n",
      "975 Train Loss 19038.172 Test Loss 6542.83437736787\n",
      "976 Train Loss 19035.518 Test Loss 6509.026274164195\n",
      "977 Train Loss 19032.764 Test Loss 6559.8110163011925\n",
      "978 Train Loss 19030.793 Test Loss 6614.776784249855\n",
      "979 Train Loss 19027.832 Test Loss 6599.797916053978\n",
      "980 Train Loss 19043.322 Test Loss 6604.708612539902\n",
      "981 Train Loss 19026.365 Test Loss 6600.656840038296\n",
      "982 Train Loss 19024.861 Test Loss 6608.985814334616\n",
      "983 Train Loss 19022.736 Test Loss 6575.243875787332\n",
      "984 Train Loss 19021.188 Test Loss 6582.062228395517\n",
      "985 Train Loss 19020.209 Test Loss 6596.282580488352\n",
      "986 Train Loss 19018.059 Test Loss 6594.547119155465\n",
      "987 Train Loss 19016.46 Test Loss 6586.010767235239\n",
      "988 Train Loss 19018.668 Test Loss 6518.400748088752\n",
      "989 Train Loss 19015.21 Test Loss 6561.15300093439\n",
      "990 Train Loss 19031.297 Test Loss 6347.7752696281195\n",
      "991 Train Loss 19014.793 Test Loss 6527.541953629526\n",
      "992 Train Loss 19013.584 Test Loss 6564.644485086046\n",
      "993 Train Loss 19011.607 Test Loss 6578.711915732769\n",
      "994 Train Loss 19010.098 Test Loss 6561.878425585368\n",
      "995 Train Loss 19007.889 Test Loss 6516.97907342077\n",
      "996 Train Loss 19004.586 Test Loss 6447.589177966393\n",
      "997 Train Loss 19003.686 Test Loss 6421.8782699746025\n",
      "998 Train Loss 19002.307 Test Loss 6428.654511533352\n",
      "999 Train Loss 19000.793 Test Loss 6385.97341544603\n",
      "1000 Train Loss 18999.223 Test Loss 6339.160753012947\n",
      "1001 Train Loss 18996.713 Test Loss 6241.710947230799\n",
      "1002 Train Loss 18995.203 Test Loss 6212.563718821661\n",
      "1003 Train Loss 18991.426 Test Loss 6309.09509770582\n",
      "1004 Train Loss 18987.068 Test Loss 6331.366250761623\n",
      "1005 Train Loss 18987.453 Test Loss 6361.66201858617\n",
      "1006 Train Loss 18980.992 Test Loss 6350.186155185273\n",
      "1007 Train Loss 18968.957 Test Loss 6430.491920784154\n",
      "1008 Train Loss 19103.371 Test Loss 7263.951520272939\n",
      "1009 Train Loss 18948.246 Test Loss 6646.142083656937\n",
      "1010 Train Loss 18926.383 Test Loss 6765.998613244122\n",
      "1011 Train Loss 19029.766 Test Loss 7352.076018830322\n",
      "1012 Train Loss 18919.38 Test Loss 6875.438556275026\n",
      "1013 Train Loss 18925.5 Test Loss 7029.431338105782\n",
      "1014 Train Loss 18911.545 Test Loss 6954.821900349543\n",
      "1015 Train Loss 18911.764 Test Loss 6983.326927316841\n",
      "1016 Train Loss 18903.557 Test Loss 6972.378881154474\n",
      "1017 Train Loss 18897.432 Test Loss 7057.214307773525\n",
      "1018 Train Loss 18896.125 Test Loss 7297.211725642598\n",
      "1019 Train Loss 18892.418 Test Loss 7196.342211742682\n",
      "1020 Train Loss 18883.512 Test Loss 7333.441618390657\n",
      "1021 Train Loss 18916.85 Test Loss 7483.407960208275\n",
      "1022 Train Loss 18876.725 Test Loss 7392.508516674459\n",
      "1023 Train Loss 18871.854 Test Loss 7414.681872257065\n",
      "1024 Train Loss 18866.76 Test Loss 7546.650153306122\n",
      "1025 Train Loss 18863.201 Test Loss 7609.3608295799795\n",
      "1026 Train Loss 18860.412 Test Loss 7658.912335089881\n",
      "1027 Train Loss 18859.033 Test Loss 7643.554247451056\n",
      "1028 Train Loss 18865.477 Test Loss 7748.599060883104\n",
      "1029 Train Loss 18856.055 Test Loss 7687.291500094656\n",
      "1030 Train Loss 18848.227 Test Loss 7723.406602641579\n",
      "1031 Train Loss 18850.527 Test Loss 7704.52146958981\n",
      "1032 Train Loss 18843.395 Test Loss 7715.163885947393\n",
      "1033 Train Loss 18835.916 Test Loss 7817.838684830196\n",
      "1034 Train Loss 18827.861 Test Loss 8051.579238785477\n",
      "1035 Train Loss 18854.387 Test Loss 8320.927585136422\n",
      "1036 Train Loss 18824.912 Test Loss 8107.255980923523\n",
      "1037 Train Loss 18808.107 Test Loss 8344.142886536689\n",
      "1038 Train Loss 20218.463 Test Loss 9926.665438807606\n",
      "1039 Train Loss 18807.693 Test Loss 8347.399750447514\n",
      "1040 Train Loss 20834240.0 Test Loss 21420041.052813068\n",
      "1041 Train Loss 18807.68 Test Loss 8347.406675026323\n",
      "1042 Train Loss 18796.49 Test Loss 8196.998368978073\n",
      "1043 Train Loss 77791.336 Test Loss 69966.01874349242\n",
      "1044 Train Loss 19981.072 Test Loss 9021.66443549892\n",
      "1045 Train Loss 18797.232 Test Loss 8111.312324659788\n",
      "1046 Train Loss 18789.906 Test Loss 8139.7512684680405\n",
      "1047 Train Loss 19113.715 Test Loss 8368.987450710103\n",
      "1048 Train Loss 18782.918 Test Loss 8132.166058225486\n",
      "1049 Train Loss 19746.486 Test Loss 8234.931619791414\n",
      "1050 Train Loss 18787.316 Test Loss 8123.605371953447\n",
      "1051 Train Loss 18777.783 Test Loss 8126.480914641038\n",
      "1052 Train Loss 18769.895 Test Loss 7859.35534681119\n",
      "1053 Train Loss 18756.479 Test Loss 7809.266776633191\n",
      "1054 Train Loss 18748.086 Test Loss 8246.128443385585\n",
      "1055 Train Loss 18820.36 Test Loss 8544.374465187153\n",
      "1056 Train Loss 18734.383 Test Loss 8331.98669129854\n",
      "1057 Train Loss 18738.127 Test Loss 8234.721790255078\n",
      "1058 Train Loss 18723.068 Test Loss 8289.633299955216\n",
      "1059 Train Loss 18714.03 Test Loss 8306.726215630706\n",
      "1060 Train Loss 18702.451 Test Loss 8517.534867035327\n",
      "1061 Train Loss 18691.656 Test Loss 8916.390827600659\n",
      "1062 Train Loss 18673.254 Test Loss 8963.074220885574\n",
      "1063 Train Loss 18663.076 Test Loss 8609.598432565934\n",
      "1064 Train Loss 18645.16 Test Loss 8361.808124450716\n",
      "1065 Train Loss 18863.957 Test Loss 8319.292265722872\n",
      "1066 Train Loss 18642.97 Test Loss 8335.650985464523\n",
      "1067 Train Loss 18647.045 Test Loss 8698.750961302227\n",
      "1068 Train Loss 18634.605 Test Loss 8494.407906286653\n",
      "1069 Train Loss 18636.0 Test Loss 8897.472222859265\n",
      "1070 Train Loss 18624.197 Test Loss 8675.823926788013\n",
      "1071 Train Loss 18616.758 Test Loss 8774.038102532066\n",
      "1072 Train Loss 18606.91 Test Loss 8886.43781617702\n",
      "1073 Train Loss 18607.727 Test Loss 9136.827134871517\n",
      "1074 Train Loss 18600.332 Test Loss 9003.829132104496\n",
      "1075 Train Loss 18594.068 Test Loss 9179.0030240587\n",
      "1076 Train Loss 18587.607 Test Loss 9310.636453647941\n",
      "1077 Train Loss 18579.918 Test Loss 9335.597372606615\n",
      "1078 Train Loss 18567.945 Test Loss 9473.828371141948\n",
      "1079 Train Loss 18633.732 Test Loss 10081.565793360045\n",
      "1080 Train Loss 18561.992 Test Loss 9647.402394268465\n",
      "1081 Train Loss 18554.07 Test Loss 9600.8164730228\n",
      "1082 Train Loss 18544.936 Test Loss 9627.828390748087\n",
      "1083 Train Loss 18537.402 Test Loss 9814.439596448803\n",
      "1084 Train Loss 18533.533 Test Loss 9848.267396252442\n",
      "1085 Train Loss 18532.29 Test Loss 10001.154388691497\n",
      "1086 Train Loss 18529.582 Test Loss 9931.765403892585\n",
      "1087 Train Loss 18525.129 Test Loss 9890.122186253473\n",
      "1088 Train Loss 18523.453 Test Loss 9821.643871269875\n",
      "1089 Train Loss 18519.871 Test Loss 9790.75163126391\n",
      "1090 Train Loss 18514.867 Test Loss 9639.543780073993\n",
      "1091 Train Loss 18512.0 Test Loss 9541.164206247655\n",
      "1092 Train Loss 18510.855 Test Loss 9483.07718090782\n",
      "1093 Train Loss 18510.014 Test Loss 9499.87191133684\n",
      "1094 Train Loss 18509.09 Test Loss 9564.801731679612\n",
      "1095 Train Loss 18508.592 Test Loss 9568.871615926773\n",
      "1096 Train Loss 18507.857 Test Loss 9634.040014932856\n",
      "1097 Train Loss 18506.76 Test Loss 9630.992228577781\n",
      "1098 Train Loss 18505.89 Test Loss 9588.138139149674\n",
      "1099 Train Loss 18505.09 Test Loss 9520.216134586004\n",
      "1100 Train Loss 18503.584 Test Loss 9459.805227969597\n",
      "1101 Train Loss 18502.254 Test Loss 9415.155384362164\n",
      "1102 Train Loss 18502.838 Test Loss 9170.49322784562\n",
      "1103 Train Loss 18500.02 Test Loss 9286.445587602275\n",
      "1104 Train Loss 18498.562 Test Loss 9163.181828313014\n",
      "1105 Train Loss 18495.049 Test Loss 9171.503558281398\n",
      "1106 Train Loss 19869.443 Test Loss 10381.088065640271\n",
      "1107 Train Loss 18488.451 Test Loss 9181.90387214268\n",
      "1108 Train Loss 18487.719 Test Loss 9328.802574784624\n",
      "1109 Train Loss 18486.064 Test Loss 9268.389285246942\n",
      "1110 Train Loss 18481.69 Test Loss 9375.540195249518\n",
      "1111 Train Loss 18479.863 Test Loss 9364.38750247446\n",
      "1112 Train Loss 18478.318 Test Loss 9328.433703902867\n",
      "1113 Train Loss 18475.713 Test Loss 9268.052259019203\n",
      "1114 Train Loss 18473.01 Test Loss 9255.243021810718\n",
      "1115 Train Loss 18467.934 Test Loss 9212.597482246523\n",
      "1116 Train Loss 18463.74 Test Loss 9288.384813128978\n",
      "1117 Train Loss 18462.812 Test Loss 9279.751445304064\n",
      "1118 Train Loss 18460.645 Test Loss 9352.764217983206\n",
      "1119 Train Loss 18459.805 Test Loss 9378.580823412767\n",
      "1120 Train Loss 18459.676 Test Loss 9435.826629116862\n",
      "1121 Train Loss 18459.52 Test Loss 9413.063849464257\n",
      "1122 Train Loss 18459.234 Test Loss 9445.68148752914\n",
      "1123 Train Loss 18458.94 Test Loss 9472.012627707925\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1124 Train Loss 18458.49 Test Loss 9508.027096722128\n",
      "1125 Train Loss 18458.006 Test Loss 9539.965243802395\n",
      "1126 Train Loss 18457.078 Test Loss 9577.835404209154\n",
      "1127 Train Loss 18456.008 Test Loss 9608.209788356751\n",
      "1128 Train Loss 18660.127 Test Loss 10320.749490454215\n",
      "1129 Train Loss 18456.701 Test Loss 9704.437000670172\n",
      "1130 Train Loss 18455.623 Test Loss 9644.073359642372\n",
      "1131 Train Loss 18454.19 Test Loss 9655.353695045464\n",
      "1132 Train Loss 18452.32 Test Loss 9659.113872835522\n",
      "1133 Train Loss 18449.068 Test Loss 9602.938314285442\n",
      "1134 Train Loss 18442.748 Test Loss 9098.29677294885\n",
      "1135 Train Loss 18438.932 Test Loss 9268.125969035213\n",
      "1136 Train Loss 37603.887 Test Loss 26499.590942986313\n",
      "1137 Train Loss 18907.453 Test Loss 9205.489140417827\n",
      "1138 Train Loss 18438.08 Test Loss 9104.305661536993\n",
      "1139 Train Loss 18435.328 Test Loss 9163.281935353469\n",
      "1140 Train Loss 20393.268 Test Loss 10661.127453534184\n",
      "1141 Train Loss 18455.67 Test Loss 9117.29752088545\n",
      "1142 Train Loss 18434.248 Test Loss 9152.003114352037\n",
      "1143 Train Loss 18435.045 Test Loss 8912.02108012765\n",
      "1144 Train Loss 18428.002 Test Loss 9046.576471909919\n",
      "1145 Train Loss 18425.883 Test Loss 9001.70197451366\n",
      "1146 Train Loss 18423.707 Test Loss 9021.042153173154\n",
      "1147 Train Loss 18424.152 Test Loss 9213.70336918022\n",
      "1148 Train Loss 18421.115 Test Loss 9116.408201097558\n",
      "1149 Train Loss 18420.832 Test Loss 9691.541149180259\n",
      "1150 Train Loss 18413.012 Test Loss 9605.348979869328\n",
      "1151 Train Loss 18451.637 Test Loss 9163.14324455339\n",
      "1152 Train Loss 18410.645 Test Loss 9501.154873870948\n",
      "1153 Train Loss 18408.451 Test Loss 9558.319203359262\n",
      "1154 Train Loss 18405.053 Test Loss 9655.18931669259\n",
      "1155 Train Loss 18403.324 Test Loss 9742.468772024238\n",
      "1156 Train Loss 18401.969 Test Loss 9722.819598927068\n",
      "1157 Train Loss 18399.613 Test Loss 9661.404309017602\n",
      "1158 Train Loss 18397.111 Test Loss 9626.007189705004\n",
      "1159 Train Loss 18394.328 Test Loss 9565.188853014855\n",
      "1160 Train Loss 18391.707 Test Loss 9457.837950611498\n",
      "1161 Train Loss 18388.861 Test Loss 9422.883307663136\n",
      "1162 Train Loss 18386.832 Test Loss 9435.383582965871\n",
      "1163 Train Loss 18384.904 Test Loss 9479.525271131675\n",
      "1164 Train Loss 18382.832 Test Loss 9409.852748135429\n",
      "1165 Train Loss 18381.42 Test Loss 9424.480325925799\n",
      "1166 Train Loss 18378.104 Test Loss 9439.391773381642\n",
      "1167 Train Loss 18376.389 Test Loss 9451.433759054045\n",
      "1168 Train Loss 23882.457 Test Loss 16135.406002948781\n",
      "1169 Train Loss 18439.533 Test Loss 9767.770242645369\n",
      "1170 Train Loss 18374.65 Test Loss 9485.24657163231\n",
      "1171 Train Loss 18372.482 Test Loss 9659.815387127665\n",
      "1172 Train Loss 18368.117 Test Loss 9763.40444653506\n",
      "1173 Train Loss 18366.047 Test Loss 9796.71745020146\n",
      "1174 Train Loss 18364.81 Test Loss 9818.713626988856\n",
      "1175 Train Loss 18363.135 Test Loss 9807.560171824116\n",
      "1176 Train Loss 18361.088 Test Loss 9761.387357532245\n",
      "1177 Train Loss 18360.148 Test Loss 9779.610595163338\n",
      "1178 Train Loss 18359.207 Test Loss 9790.834519014874\n",
      "1179 Train Loss 18358.383 Test Loss 9795.510394036934\n",
      "1180 Train Loss 18357.047 Test Loss 9811.478039253905\n",
      "1181 Train Loss 18355.328 Test Loss 9824.575064358856\n",
      "1182 Train Loss 18354.742 Test Loss 9802.349947322284\n",
      "1183 Train Loss 18354.11 Test Loss 9808.912585192795\n",
      "1184 Train Loss 18353.107 Test Loss 9789.738578337521\n",
      "1185 Train Loss 18352.918 Test Loss 9749.061851505965\n",
      "1186 Train Loss 18352.324 Test Loss 9772.043668222712\n",
      "1187 Train Loss 18351.95 Test Loss 9790.23792121559\n",
      "1188 Train Loss 18351.473 Test Loss 9763.576071849084\n",
      "1189 Train Loss 18350.812 Test Loss 9700.67276122139\n",
      "1190 Train Loss 18350.238 Test Loss 9680.137702951435\n",
      "1191 Train Loss 18349.34 Test Loss 9619.565310831911\n",
      "1192 Train Loss 18348.357 Test Loss 9563.18813388867\n",
      "1193 Train Loss 18348.32 Test Loss 9551.62347913806\n",
      "1194 Train Loss 18347.527 Test Loss 9557.47594980437\n",
      "1195 Train Loss 18345.75 Test Loss 9460.175215884912\n",
      "1196 Train Loss 18343.523 Test Loss 9389.29519673786\n",
      "1197 Train Loss 18341.066 Test Loss 9364.989904549091\n",
      "1198 Train Loss 18344.633 Test Loss 9534.61552583948\n",
      "1199 Train Loss 18339.71 Test Loss 9422.886491054802\n",
      "1200 Train Loss 18337.803 Test Loss 9487.34284064625\n",
      "1201 Train Loss 18336.682 Test Loss 9575.384823419585\n",
      "1202 Train Loss 18336.434 Test Loss 9525.365802670805\n",
      "1203 Train Loss 18337.176 Test Loss 9499.000503165982\n",
      "1204 Train Loss 18334.742 Test Loss 9513.666476506336\n",
      "1205 Train Loss 18333.64 Test Loss 9499.226246342923\n",
      "1206 Train Loss 18332.955 Test Loss 9506.563712852989\n",
      "1207 Train Loss 18330.4 Test Loss 9571.682700695814\n",
      "1208 Train Loss 18329.541 Test Loss 9616.897517473535\n",
      "1209 Train Loss 18328.588 Test Loss 9603.036650992848\n",
      "1210 Train Loss 18327.766 Test Loss 9580.80786099907\n",
      "1211 Train Loss 18327.414 Test Loss 9554.726696777172\n",
      "1212 Train Loss 18326.309 Test Loss 9610.748170840328\n",
      "1213 Train Loss 18325.654 Test Loss 9593.902101330565\n",
      "1214 Train Loss 18325.29 Test Loss 9600.02793594692\n",
      "1215 Train Loss 18324.926 Test Loss 9596.46476156029\n",
      "1216 Train Loss 18324.145 Test Loss 9571.51300786922\n",
      "1217 Train Loss 18322.318 Test Loss 9567.975432858186\n",
      "1218 Train Loss 18320.266 Test Loss 9557.589433972413\n",
      "1219 Train Loss 18317.63 Test Loss 9569.281580777524\n",
      "1220 Train Loss 18313.773 Test Loss 9531.360930519746\n",
      "1221 Train Loss 18315.725 Test Loss 9561.174878476271\n",
      "1222 Train Loss 18312.014 Test Loss 9542.58972280973\n",
      "1223 Train Loss 18309.908 Test Loss 9503.5014010859\n",
      "1224 Train Loss 18311.139 Test Loss 9531.032698585866\n",
      "1225 Train Loss 18308.719 Test Loss 9514.66488883759\n",
      "1226 Train Loss 18307.928 Test Loss 9509.926034059592\n",
      "1227 Train Loss 18307.158 Test Loss 9532.358502357676\n",
      "1228 Train Loss 18306.223 Test Loss 9540.972097579732\n",
      "1229 Train Loss 18304.84 Test Loss 9569.736095233578\n",
      "1230 Train Loss 18304.035 Test Loss 9539.34534760902\n",
      "1231 Train Loss 18303.383 Test Loss 9535.559191268334\n",
      "1232 Train Loss 18302.676 Test Loss 9530.31138618546\n",
      "1233 Train Loss 18348.525 Test Loss 9590.814649506761\n",
      "1234 Train Loss 18301.43 Test Loss 9536.38801007752\n",
      "1235 Train Loss 18461.482 Test Loss 9730.423708438057\n",
      "1236 Train Loss 18301.201 Test Loss 9540.242742301827\n",
      "1237 Train Loss 18300.486 Test Loss 9556.915876548972\n",
      "1238 Train Loss 18299.348 Test Loss 9670.372303813525\n",
      "1239 Train Loss 18297.266 Test Loss 9695.472825717388\n",
      "1240 Train Loss 18300.59 Test Loss 9780.729010197323\n",
      "1241 Train Loss 18296.201 Test Loss 9723.13869144096\n",
      "1242 Train Loss 18297.168 Test Loss 9691.682841249627\n",
      "1243 Train Loss 18295.094 Test Loss 9709.511131500607\n",
      "1244 Train Loss 18294.19 Test Loss 9717.981843130461\n",
      "1245 Train Loss 18293.766 Test Loss 9721.93763393553\n",
      "1246 Train Loss 18293.562 Test Loss 9752.669359154515\n",
      "1247 Train Loss 18292.791 Test Loss 9748.047952822706\n",
      "1248 Train Loss 18292.242 Test Loss 9764.206562564628\n",
      "1249 Train Loss 18290.543 Test Loss 9844.98731091882\n",
      "1250 Train Loss 18334.246 Test Loss 9384.444413965595\n",
      "1251 Train Loss 18289.95 Test Loss 9789.733284324697\n",
      "1252 Train Loss 18290.928 Test Loss 10149.856374719431\n",
      "1253 Train Loss 18288.588 Test Loss 9946.752630035515\n",
      "1254 Train Loss 18284.047 Test Loss 10129.943774680783\n",
      "1255 Train Loss 18292.934 Test Loss 10600.22606116534\n",
      "1256 Train Loss 18281.93 Test Loss 10294.168722419392\n",
      "1257 Train Loss 18278.531 Test Loss 10172.998948073022\n",
      "1258 Train Loss 18276.543 Test Loss 10101.07407649628\n",
      "1259 Train Loss 18273.473 Test Loss 10025.881786993104\n",
      "1260 Train Loss 18270.338 Test Loss 10078.340771711337\n",
      "1261 Train Loss 18269.436 Test Loss 10220.163171376655\n",
      "1262 Train Loss 18265.586 Test Loss 10243.767717102037\n",
      "1263 Train Loss 18263.15 Test Loss 10193.691653604372\n",
      "1264 Train Loss 18261.408 Test Loss 10236.777023506595\n",
      "1265 Train Loss 18260.662 Test Loss 10312.915579338243\n",
      "1266 Train Loss 18259.744 Test Loss 10291.732699107773\n",
      "1267 Train Loss 18258.998 Test Loss 10277.439107700207\n",
      "1268 Train Loss 18258.441 Test Loss 10265.800527119345\n",
      "1269 Train Loss 18256.977 Test Loss 10281.331302316328\n",
      "1270 Train Loss 18254.266 Test Loss 10338.132474833761\n",
      "1271 Train Loss 18252.816 Test Loss 10479.676279725547\n",
      "1272 Train Loss 18249.273 Test Loss 10695.089425780272\n",
      "1273 Train Loss 18282.266 Test Loss 11418.902485181645\n",
      "1274 Train Loss 18244.473 Test Loss 10897.903459435205\n",
      "1275 Train Loss 18266.76 Test Loss 11002.469425275067\n",
      "1276 Train Loss 18236.805 Test Loss 10927.492445030368\n",
      "1277 Train Loss 18286.094 Test Loss 10950.252012673309\n",
      "1278 Train Loss 18227.438 Test Loss 10933.049731119512\n",
      "1279 Train Loss 18218.516 Test Loss 10616.056076654073\n",
      "1280 Train Loss 18229.662 Test Loss 10053.512878612943\n",
      "1281 Train Loss 18200.303 Test Loss 10332.169656534565\n",
      "1282 Train Loss 18254.936 Test Loss 10058.59119102155\n",
      "1283 Train Loss 18192.043 Test Loss 10227.60960054485\n",
      "1284 Train Loss 18181.986 Test Loss 10253.600726164348\n",
      "1285 Train Loss 18179.824 Test Loss 10351.11811915968\n",
      "1286 Train Loss 18167.525 Test Loss 10293.30178655394\n",
      "1287 Train Loss 18231.096 Test Loss 10681.446612511963\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1288 Train Loss 18162.822 Test Loss 10363.486944002774\n",
      "1289 Train Loss 18157.814 Test Loss 10309.517486821042\n",
      "1290 Train Loss 18154.943 Test Loss 10365.050482013368\n",
      "1291 Train Loss 18150.6 Test Loss 10465.958885117003\n",
      "1292 Train Loss 18153.531 Test Loss 10595.992732566556\n",
      "1293 Train Loss 18148.92 Test Loss 10519.771262553264\n",
      "1294 Train Loss 18147.438 Test Loss 10645.563196505695\n",
      "1295 Train Loss 18145.807 Test Loss 10709.595624627747\n",
      "1296 Train Loss 18147.312 Test Loss 10832.749578145907\n",
      "1297 Train Loss 18145.031 Test Loss 10748.631595689481\n",
      "1298 Train Loss 18140.855 Test Loss 10704.577774994552\n",
      "1299 Train Loss 18422.898 Test Loss 10081.341000552738\n",
      "1300 Train Loss 18138.373 Test Loss 10666.008181962474\n",
      "1301 Train Loss 18159.508 Test Loss 10480.796137200716\n",
      "1302 Train Loss 18137.102 Test Loss 10627.362036726498\n",
      "1303 Train Loss 18133.35 Test Loss 10623.613806389538\n",
      "1304 Train Loss 18130.855 Test Loss 10634.970526860625\n",
      "1305 Train Loss 18129.732 Test Loss 10593.038723216667\n",
      "1306 Train Loss 18129.043 Test Loss 10567.10969481718\n",
      "1307 Train Loss 18128.084 Test Loss 10552.920617373793\n",
      "1308 Train Loss 18127.355 Test Loss 10561.740638370735\n",
      "1309 Train Loss 18126.35 Test Loss 10557.09153371211\n",
      "1310 Train Loss 18125.299 Test Loss 10516.993159096974\n",
      "1311 Train Loss 18123.648 Test Loss 10557.186107560843\n",
      "1312 Train Loss 18122.531 Test Loss 10573.493767459098\n",
      "1313 Train Loss 18120.223 Test Loss 10614.82883225508\n",
      "1314 Train Loss 18118.172 Test Loss 10676.28706528273\n",
      "1315 Train Loss 18119.402 Test Loss 10776.015103241338\n",
      "1316 Train Loss 18116.857 Test Loss 10722.500032582966\n",
      "1317 Train Loss 18114.998 Test Loss 10741.442693417926\n",
      "1318 Train Loss 18114.055 Test Loss 10727.860654355602\n",
      "1319 Train Loss 18112.594 Test Loss 10633.1188022303\n",
      "1320 Train Loss 18111.791 Test Loss 10645.084733812071\n",
      "1321 Train Loss 18111.092 Test Loss 10658.726692330902\n",
      "1322 Train Loss 18108.809 Test Loss 10751.0689914152\n",
      "1323 Train Loss 18108.135 Test Loss 10727.828866472042\n",
      "1324 Train Loss 20555.37 Test Loss 13082.497252338619\n",
      "1325 Train Loss 18121.291 Test Loss 10864.35129786512\n",
      "1326 Train Loss 18107.654 Test Loss 10749.22774656788\n",
      "1327 Train Loss 18104.764 Test Loss 10772.033651988413\n",
      "1328 Train Loss 18100.936 Test Loss 10972.194226282381\n",
      "1329 Train Loss 18097.521 Test Loss 11021.31988144695\n",
      "1330 Train Loss 18089.445 Test Loss 11167.040175385762\n",
      "1331 Train Loss 25514.402 Test Loss 18313.21257400781\n",
      "1332 Train Loss 18088.992 Test Loss 11175.929052161433\n",
      "1333 Train Loss 18086.992 Test Loss 11220.928106532518\n",
      "1334 Train Loss 18084.908 Test Loss 11203.665274566196\n",
      "1335 Train Loss 18084.055 Test Loss 11208.434568122198\n",
      "1336 Train Loss 18375.504 Test Loss 11344.432371254594\n",
      "1337 Train Loss 18083.96 Test Loss 11207.483204965853\n",
      "1338 Train Loss 18083.488 Test Loss 11199.465019339157\n",
      "1339 Train Loss 18104.459 Test Loss 10751.202451325064\n",
      "1340 Train Loss 18080.975 Test Loss 11090.2749534325\n",
      "1341 Train Loss 18079.385 Test Loss 11067.76067084895\n",
      "1342 Train Loss 18077.107 Test Loss 11052.828473623964\n",
      "1343 Train Loss 18157.283 Test Loss 10827.066809643118\n",
      "1344 Train Loss 18075.818 Test Loss 11031.639598031312\n",
      "1345 Train Loss 18073.791 Test Loss 11058.995151409033\n",
      "1346 Train Loss 18071.24 Test Loss 11055.614766913863\n",
      "1347 Train Loss 18108.826 Test Loss 11027.314519081965\n",
      "1348 Train Loss 18070.354 Test Loss 11057.143006619075\n",
      "1349 Train Loss 18068.49 Test Loss 11091.058082496042\n",
      "1350 Train Loss 18067.117 Test Loss 11157.546311189322\n",
      "1351 Train Loss 18066.246 Test Loss 11112.461573037064\n",
      "1352 Train Loss 18065.13 Test Loss 11023.928887479662\n",
      "1353 Train Loss 18064.312 Test Loss 11059.57576766509\n",
      "1354 Train Loss 18063.54 Test Loss 11152.171524891444\n",
      "1355 Train Loss 18062.99 Test Loss 11161.993932306497\n",
      "1356 Train Loss 18062.312 Test Loss 11174.86897990405\n",
      "1357 Train Loss 18061.29 Test Loss 11180.776952762553\n",
      "1358 Train Loss 18063.574 Test Loss 11249.617115192372\n",
      "1359 Train Loss 18060.717 Test Loss 11202.042640427499\n",
      "1360 Train Loss 18059.805 Test Loss 11193.17570293048\n",
      "1361 Train Loss 18060.023 Test Loss 11196.808792059677\n",
      "1362 Train Loss 18057.426 Test Loss 11198.330920546838\n",
      "1363 Train Loss 18113.838 Test Loss 11316.339534588024\n",
      "1364 Train Loss 18057.326 Test Loss 11214.884888407425\n",
      "1365 Train Loss 18056.668 Test Loss 11207.021788546475\n",
      "1366 Train Loss 18054.979 Test Loss 11233.610156398548\n",
      "1367 Train Loss 19953.557 Test Loss 14204.097887263217\n",
      "1368 Train Loss 18050.506 Test Loss 11317.121776285008\n",
      "1369 Train Loss 18048.75 Test Loss 11360.721980986387\n",
      "1370 Train Loss 18046.27 Test Loss 11372.092354898401\n",
      "1371 Train Loss 18045.248 Test Loss 11365.560214137839\n",
      "1372 Train Loss 18043.975 Test Loss 11385.290244072972\n",
      "1373 Train Loss 18043.455 Test Loss 11398.762796994035\n",
      "1374 Train Loss 18043.021 Test Loss 11426.002090579364\n",
      "1375 Train Loss 18042.441 Test Loss 11437.190834674966\n",
      "1376 Train Loss 18041.986 Test Loss 11459.960036846724\n",
      "1377 Train Loss 18041.078 Test Loss 11485.099982238668\n",
      "1378 Train Loss 18039.861 Test Loss 11494.57993423335\n",
      "1379 Train Loss 18038.533 Test Loss 11420.163587326111\n",
      "1380 Train Loss 18037.723 Test Loss 11426.99437259417\n",
      "1381 Train Loss 18039.566 Test Loss 11481.34764785289\n",
      "1382 Train Loss 18036.969 Test Loss 11446.941494594139\n",
      "1383 Train Loss 18036.535 Test Loss 11458.984407762591\n",
      "1384 Train Loss 18035.95 Test Loss 11469.363932670329\n",
      "1385 Train Loss 18035.18 Test Loss 11479.132985637641\n",
      "1386 Train Loss 18034.312 Test Loss 11490.062539716873\n",
      "1387 Train Loss 18032.854 Test Loss 11476.23600127475\n",
      "1388 Train Loss 18114.496 Test Loss 11441.147922606946\n",
      "1389 Train Loss 18031.404 Test Loss 11462.75318256107\n",
      "1390 Train Loss 18030.412 Test Loss 11449.28437501375\n",
      "1391 Train Loss 18041.24 Test Loss 11310.97238140793\n",
      "1392 Train Loss 18028.965 Test Loss 11416.955089465348\n",
      "1393 Train Loss 18027.312 Test Loss 11259.30943638547\n",
      "1394 Train Loss 18022.543 Test Loss 11209.388986122418\n",
      "1395 Train Loss 18019.91 Test Loss 11180.878800083869\n",
      "1396 Train Loss 18017.914 Test Loss 11076.144890147587\n",
      "1397 Train Loss 18012.176 Test Loss 10994.34161773231\n",
      "1398 Train Loss 18007.072 Test Loss 10865.092639230648\n",
      "1399 Train Loss 18005.447 Test Loss 10892.357549996055\n",
      "1400 Train Loss 18031.63 Test Loss 10998.62578012441\n",
      "1401 Train Loss 18002.766 Test Loss 10913.596621108358\n",
      "1402 Train Loss 18001.38 Test Loss 10965.882255434772\n",
      "1403 Train Loss 17998.053 Test Loss 10945.956445999445\n",
      "1404 Train Loss 18008.008 Test Loss 10953.019491789977\n",
      "1405 Train Loss 17996.275 Test Loss 10950.423816136872\n",
      "1406 Train Loss 17992.941 Test Loss 11013.836427900456\n",
      "1407 Train Loss 17988.234 Test Loss 11140.85367127866\n",
      "1408 Train Loss 18006.777 Test Loss 11313.680188901966\n",
      "1409 Train Loss 17985.928 Test Loss 11189.639160723484\n",
      "1410 Train Loss 18300.588 Test Loss 12092.473275119548\n",
      "1411 Train Loss 17986.54 Test Loss 11271.095215072666\n",
      "1412 Train Loss 17984.47 Test Loss 11223.005638315919\n",
      "1413 Train Loss 17982.68 Test Loss 11159.954963460788\n",
      "1414 Train Loss 17981.465 Test Loss 11213.1244844293\n",
      "1415 Train Loss 17979.902 Test Loss 11288.388142075919\n",
      "1416 Train Loss 17978.496 Test Loss 11418.302262079093\n",
      "1417 Train Loss 17977.111 Test Loss 11368.795635447494\n",
      "1418 Train Loss 17976.445 Test Loss 11331.689664300799\n",
      "1419 Train Loss 17974.92 Test Loss 11219.234422231864\n",
      "1420 Train Loss 17973.312 Test Loss 11084.375433740253\n",
      "1421 Train Loss 17968.697 Test Loss 10900.693283210583\n",
      "1422 Train Loss 17986.453 Test Loss 10584.24578365549\n",
      "1423 Train Loss 17967.59 Test Loss 10805.85715417276\n",
      "1424 Train Loss 17980.371 Test Loss 10671.844635605947\n",
      "1425 Train Loss 17964.127 Test Loss 10750.182045188361\n",
      "1426 Train Loss 17958.8 Test Loss 10655.679559296765\n",
      "1427 Train Loss 17959.959 Test Loss 10511.595249574813\n",
      "1428 Train Loss 17947.832 Test Loss 10579.364266400724\n",
      "1429 Train Loss 17934.559 Test Loss 10751.331307779274\n",
      "1430 Train Loss 17934.879 Test Loss 10730.807326334454\n",
      "1431 Train Loss 17929.129 Test Loss 10746.061782273075\n",
      "1432 Train Loss 17923.625 Test Loss 11005.369162815125\n",
      "1433 Train Loss 17916.473 Test Loss 11079.656592660773\n",
      "1434 Train Loss 17909.654 Test Loss 11019.833946427429\n",
      "1435 Train Loss 21912.854 Test Loss 14230.608962165441\n",
      "1436 Train Loss 18075.982 Test Loss 10953.611725223358\n",
      "1437 Train Loss 17906.254 Test Loss 11002.174392886665\n",
      "1438 Train Loss 17909.979 Test Loss 10977.957605454474\n",
      "1439 Train Loss 17901.143 Test Loss 10987.80303625586\n",
      "1440 Train Loss 17901.271 Test Loss 10972.378532787812\n",
      "1441 Train Loss 17899.482 Test Loss 10980.000538077793\n",
      "1442 Train Loss 17897.35 Test Loss 10921.789221246834\n",
      "1443 Train Loss 17895.436 Test Loss 10885.544958669254\n",
      "1444 Train Loss 17893.283 Test Loss 10857.411951459315\n",
      "1445 Train Loss 17892.047 Test Loss 10887.71722575277\n",
      "1446 Train Loss 17890.764 Test Loss 10870.394585981243\n",
      "1447 Train Loss 17888.924 Test Loss 10858.322488462436\n",
      "1448 Train Loss 17886.295 Test Loss 10783.693369988137\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1449 Train Loss 17884.248 Test Loss 10749.901723361289\n",
      "1450 Train Loss 17881.893 Test Loss 10724.961659420525\n",
      "1451 Train Loss 17880.762 Test Loss 10674.667030331562\n",
      "1452 Train Loss 17879.887 Test Loss 10713.667376603471\n",
      "1453 Train Loss 17878.76 Test Loss 10768.476807060333\n",
      "1454 Train Loss 17877.762 Test Loss 10778.607382969662\n",
      "1455 Train Loss 17876.45 Test Loss 10776.869311192679\n",
      "1456 Train Loss 17881.102 Test Loss 10949.094461405875\n",
      "1457 Train Loss 17875.953 Test Loss 10820.440924546368\n",
      "1458 Train Loss 17874.154 Test Loss 10805.963959946439\n",
      "1459 Train Loss 17873.32 Test Loss 10796.52740837026\n",
      "1460 Train Loss 17872.137 Test Loss 10782.610699173465\n",
      "1461 Train Loss 17871.309 Test Loss 10786.33619694011\n",
      "1462 Train Loss 17870.07 Test Loss 10789.67012046151\n",
      "1463 Train Loss 17868.781 Test Loss 10747.995701595146\n",
      "1464 Train Loss 17867.004 Test Loss 10709.557272115673\n",
      "1465 Train Loss 18225.969 Test Loss 10914.386018227697\n",
      "1466 Train Loss 17865.732 Test Loss 10671.636592540348\n",
      "1467 Train Loss 28064.535 Test Loss 19984.08305685007\n",
      "1468 Train Loss 17865.707 Test Loss 10671.612984453\n",
      "1469 Train Loss 17873.658 Test Loss 10666.433693309471\n",
      "1470 Train Loss 17865.715 Test Loss 10667.13833871163\n",
      "1471 Train Loss 17865.646 Test Loss 10668.750639553991\n",
      "1472 Train Loss 17864.81 Test Loss 10605.55990421605\n",
      "1473 Train Loss 17863.947 Test Loss 10614.540763535144\n",
      "1474 Train Loss 17863.234 Test Loss 10606.601725176526\n",
      "1475 Train Loss 17862.154 Test Loss 10616.615242212822\n",
      "1476 Train Loss 17862.318 Test Loss 10653.945544346843\n",
      "1477 Train Loss 17861.209 Test Loss 10638.599198330543\n",
      "1478 Train Loss 17865.758 Test Loss 10712.448575846607\n",
      "1479 Train Loss 17861.049 Test Loss 10662.565268192966\n",
      "1480 Train Loss 17860.441 Test Loss 10733.26487920292\n",
      "1481 Train Loss 17860.047 Test Loss 10754.96483986226\n",
      "1482 Train Loss 17859.848 Test Loss 10747.993127356716\n",
      "1483 Train Loss 17859.594 Test Loss 10741.203365740585\n",
      "1484 Train Loss 17859.39 Test Loss 10739.302925351361\n",
      "1485 Train Loss 17858.883 Test Loss 10740.242557014755\n",
      "1486 Train Loss 17858.674 Test Loss 10749.801397638035\n",
      "1487 Train Loss 17858.371 Test Loss 10764.270478501934\n",
      "1488 Train Loss 17858.207 Test Loss 10779.636127657275\n",
      "1489 Train Loss 17858.004 Test Loss 10787.719869624932\n",
      "1490 Train Loss 17857.63 Test Loss 10826.918334998465\n",
      "1491 Train Loss 17886.383 Test Loss 11173.71588443363\n",
      "1492 Train Loss 17857.518 Test Loss 10847.126903637178\n",
      "1493 Train Loss 17857.03 Test Loss 10854.561079745079\n",
      "1494 Train Loss 17855.756 Test Loss 10916.610706819887\n",
      "1495 Train Loss 17854.936 Test Loss 10919.145776646186\n",
      "1496 Train Loss 17853.572 Test Loss 10917.46107456869\n",
      "1497 Train Loss 17852.291 Test Loss 10911.450589554845\n",
      "1498 Train Loss 17850.656 Test Loss 10907.133548902151\n",
      "1499 Train Loss 17847.328 Test Loss 10932.869460881306\n",
      "1500 Train Loss 17841.273 Test Loss 10891.76206098278\n",
      "1501 Train Loss 17837.748 Test Loss 10836.203491499786\n",
      "1502 Train Loss 18219.02 Test Loss 10747.171915890516\n",
      "1503 Train Loss 17826.797 Test Loss 10742.685794128305\n",
      "1504 Train Loss 17916.049 Test Loss 9889.938167563942\n",
      "1505 Train Loss 17816.13 Test Loss 10487.802935253803\n",
      "1506 Train Loss 17800.408 Test Loss 10639.086057752482\n",
      "1507 Train Loss 17828.316 Test Loss 10873.857725460384\n",
      "1508 Train Loss 17790.03 Test Loss 10724.831806852198\n",
      "1509 Train Loss 17778.408 Test Loss 10873.728617737663\n",
      "1510 Train Loss 17811.637 Test Loss 11193.424273822546\n",
      "1511 Train Loss 17769.064 Test Loss 10997.484620068948\n",
      "1512 Train Loss 17757.824 Test Loss 11309.358794983453\n",
      "1513 Train Loss 17878.275 Test Loss 11548.298019384343\n",
      "1514 Train Loss 17754.154 Test Loss 11343.484731245804\n",
      "1515 Train Loss 17742.463 Test Loss 11474.059158474458\n",
      "1516 Train Loss 17735.982 Test Loss 11489.393544196575\n",
      "1517 Train Loss 17729.8 Test Loss 11436.216270985946\n",
      "1518 Train Loss 17724.943 Test Loss 11523.878996004618\n",
      "1519 Train Loss 17717.523 Test Loss 11484.255993503162\n",
      "1520 Train Loss 17721.041 Test Loss 11467.197333503364\n",
      "1521 Train Loss 17707.852 Test Loss 11464.864713214021\n",
      "1522 Train Loss 17744.0 Test Loss 11495.260319525965\n",
      "1523 Train Loss 17703.254 Test Loss 11464.95078012177\n",
      "1524 Train Loss 17690.11 Test Loss 11632.523454776257\n",
      "1525 Train Loss 17729.596 Test Loss 11838.962059328347\n",
      "1526 Train Loss 17679.643 Test Loss 11706.4473941815\n",
      "1527 Train Loss 17669.361 Test Loss 12023.676694463446\n",
      "1528 Train Loss 17682.062 Test Loss 12424.215621439309\n",
      "1529 Train Loss 17664.42 Test Loss 12167.049786949789\n",
      "1530 Train Loss 17658.613 Test Loss 12453.27406408801\n",
      "1531 Train Loss 17661.625 Test Loss 12509.47476127288\n",
      "1532 Train Loss 17649.727 Test Loss 12468.45206037455\n",
      "1533 Train Loss 17640.668 Test Loss 12433.174608231246\n",
      "1534 Train Loss 17656.742 Test Loss 12453.086243449794\n",
      "1535 Train Loss 17634.895 Test Loss 12436.987866760155\n",
      "1536 Train Loss 17775.557 Test Loss 12543.273579534369\n",
      "1537 Train Loss 17627.959 Test Loss 12433.578087764852\n",
      "1538 Train Loss 17634.645 Test Loss 12638.911294055852\n",
      "1539 Train Loss 17623.71 Test Loss 12509.265490023454\n",
      "1540 Train Loss 17630.236 Test Loss 12537.696404219801\n",
      "1541 Train Loss 17616.19 Test Loss 12515.192644953733\n",
      "1542 Train Loss 17862.055 Test Loss 12810.372187314355\n",
      "1543 Train Loss 17614.805 Test Loss 12527.106324269205\n",
      "1544 Train Loss 38139.16 Test Loss 35277.448256277174\n",
      "1545 Train Loss 18176.133 Test Loss 14372.950754364438\n",
      "1546 Train Loss 17615.684 Test Loss 13023.045959622921\n",
      "1547 Train Loss 17609.031 Test Loss 12794.56420901079\n",
      "1548 Train Loss 17625.408 Test Loss 13045.350715346442\n",
      "1549 Train Loss 17601.465 Test Loss 12875.885572325804\n",
      "1550 Train Loss 17590.74 Test Loss 12996.749858532365\n",
      "1551 Train Loss 17582.78 Test Loss 12899.258227206292\n",
      "1552 Train Loss 17574.668 Test Loss 12811.893878824943\n",
      "1553 Train Loss 17567.666 Test Loss 12893.821814343355\n",
      "1554 Train Loss 17562.463 Test Loss 12894.896074254317\n",
      "1555 Train Loss 17553.367 Test Loss 12892.312585118818\n",
      "1556 Train Loss 17542.35 Test Loss 12580.546753433275\n",
      "1557 Train Loss 17543.936 Test Loss 12598.306218132282\n",
      "1558 Train Loss 17537.19 Test Loss 12586.582778831273\n",
      "1559 Train Loss 17527.625 Test Loss 12554.017087732824\n",
      "1560 Train Loss 17525.027 Test Loss 12442.48050715945\n",
      "1561 Train Loss 17520.799 Test Loss 12476.635417873737\n",
      "1562 Train Loss 17517.42 Test Loss 12478.380761991679\n",
      "1563 Train Loss 17512.475 Test Loss 12460.821550152228\n",
      "1564 Train Loss 17501.477 Test Loss 12655.442930470817\n",
      "1565 Train Loss 17495.307 Test Loss 12570.383475774244\n",
      "1566 Train Loss 17489.941 Test Loss 12598.636190944706\n",
      "1567 Train Loss 17485.09 Test Loss 12659.801224647688\n",
      "1568 Train Loss 17475.63 Test Loss 13059.602932766174\n",
      "1569 Train Loss 17468.502 Test Loss 12998.861531024193\n",
      "1570 Train Loss 17461.332 Test Loss 13034.849807319155\n",
      "1571 Train Loss 17493.451 Test Loss 13274.18088404135\n",
      "1572 Train Loss 17458.252 Test Loss 13088.426818347478\n",
      "1573 Train Loss 17452.11 Test Loss 13155.096180391329\n",
      "1574 Train Loss 17441.502 Test Loss 13400.95321607471\n",
      "1575 Train Loss 17503.15 Test Loss 14836.463534922703\n",
      "1576 Train Loss 17438.018 Test Loss 14164.27785584202\n",
      "1577 Train Loss 17616.11 Test Loss 14895.476560740957\n",
      "1578 Train Loss 17429.006 Test Loss 14367.799070297497\n",
      "1579 Train Loss 17415.717 Test Loss 14193.18940763192\n",
      "1580 Train Loss 17609.42 Test Loss 14488.182320555024\n",
      "1581 Train Loss 17412.533 Test Loss 14215.372117429168\n",
      "1582 Train Loss 17427.227 Test Loss 14241.533480238086\n",
      "1583 Train Loss 17410.383 Test Loss 14229.218634085777\n",
      "1584 Train Loss 17410.715 Test Loss 14280.991800096532\n",
      "1585 Train Loss 17408.377 Test Loss 14251.791950217008\n",
      "1586 Train Loss 17413.45 Test Loss 14266.218959217349\n",
      "1587 Train Loss 17407.572 Test Loss 14255.120370542683\n",
      "1588 Train Loss 17406.086 Test Loss 14300.078006466125\n",
      "1589 Train Loss 17404.217 Test Loss 14397.160049512118\n",
      "1590 Train Loss 17403.043 Test Loss 14470.946102095802\n",
      "1591 Train Loss 17401.717 Test Loss 14475.664917908436\n",
      "1592 Train Loss 17399.51 Test Loss 14473.353269963733\n",
      "1593 Train Loss 17406.469 Test Loss 14929.080373045741\n",
      "1594 Train Loss 17398.645 Test Loss 14570.67129512107\n",
      "1595 Train Loss 17407.549 Test Loss 14444.729674686061\n",
      "1596 Train Loss 17397.963 Test Loss 14546.895986315223\n",
      "1597 Train Loss 17395.244 Test Loss 14551.6277034803\n",
      "1598 Train Loss 17392.229 Test Loss 14684.623665602938\n",
      "1599 Train Loss 17389.562 Test Loss 14908.286122354793\n",
      "1600 Train Loss 17383.426 Test Loss 15684.684782506776\n",
      "1601 Train Loss 17376.395 Test Loss 16192.748278830644\n",
      "1602 Train Loss 17387.936 Test Loss 16811.45974992966\n",
      "1603 Train Loss 17371.094 Test Loss 16429.997097962358\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1604 Train Loss 17363.33 Test Loss 17033.64190163908\n",
      "1605 Train Loss 17359.438 Test Loss 17472.32816361751\n",
      "1606 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1607 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1608 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1609 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1610 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1611 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1612 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1613 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1614 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1615 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1616 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1617 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1618 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1619 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1620 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1621 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1622 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1623 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1624 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1625 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1626 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1627 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1628 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1629 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1630 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1631 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1632 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1633 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1634 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1635 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1636 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1637 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1638 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1639 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1640 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1641 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1642 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1643 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1644 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1645 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1646 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1647 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1648 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1649 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1650 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1651 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1652 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1653 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1654 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1655 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1656 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1657 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1658 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1659 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1660 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1661 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1662 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1663 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1664 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1665 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1666 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1667 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1668 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1669 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1670 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1671 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1672 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1673 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1674 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1675 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1676 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1677 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1678 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1679 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1680 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1681 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1682 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1683 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1684 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1685 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1686 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1687 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1688 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1689 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1690 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1691 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1692 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1693 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1694 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1695 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1696 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1697 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1698 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1699 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1700 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1701 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1702 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1703 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1704 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1705 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1706 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1707 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1708 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1709 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1710 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1711 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1712 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1713 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1714 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1715 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1716 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1717 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1718 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1719 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1720 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1721 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1722 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1723 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1724 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1725 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1726 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1727 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1728 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1729 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1730 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1731 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1732 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1733 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1734 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1735 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1736 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1737 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1738 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1739 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1740 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1741 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1742 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1743 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1744 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1745 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1746 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1747 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1748 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1749 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1750 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1751 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1752 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1753 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1754 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1755 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1756 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1757 Train Loss 1935229.2 Test Loss 1993941.024958485\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1758 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1759 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1760 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1761 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1762 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1763 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1764 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1765 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1766 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1767 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1768 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1769 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1770 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1771 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1772 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1773 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1774 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1775 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1776 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1777 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1778 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1779 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1780 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1781 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1782 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1783 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1784 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1785 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1786 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1787 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1788 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1789 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1790 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1791 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1792 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1793 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1794 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1795 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1796 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1797 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1798 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1799 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1800 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1801 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1802 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1803 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1804 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1805 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1806 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1807 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1808 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1809 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1810 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1811 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1812 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1813 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1814 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1815 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1816 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1817 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1818 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1819 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1820 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1821 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1822 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1823 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1824 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1825 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1826 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1827 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1828 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1829 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1830 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1831 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1832 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1833 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1834 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1835 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1836 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1837 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1838 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1839 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1840 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1841 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1842 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1843 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1844 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1845 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1846 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1847 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1848 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1849 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1850 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1851 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1852 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1853 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1854 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1855 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1856 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1857 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1858 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1859 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1860 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1861 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1862 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1863 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1864 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1865 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1866 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1867 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1868 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1869 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1870 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1871 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1872 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1873 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1874 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1875 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1876 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1877 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1878 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1879 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1880 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1881 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1882 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1883 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1884 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1885 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1886 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1887 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1888 Train Loss 17359.477 Test Loss 17472.46190101535\n",
      "1889 Train Loss 17359.479 Test Loss 17471.682318532872\n",
      "1890 Train Loss 17359.438 Test Loss 17472.32814802578\n",
      "1891 Train Loss 12392247.0 Test Loss 12697943.075406494\n",
      "1892 Train Loss 1935229.2 Test Loss 1993941.024958485\n",
      "1893 Train Loss 17359.477 Test Loss 17472.46190101535\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-13-59fd6f9b10a3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPINN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                     loss, flat_grad, t, ls_func_evals = _strong_wolfe(\n\u001b[0;32m--> 426\u001b[0;31m                         obj_func, x_init, t, d, loss, flat_grad, gtd)\n\u001b[0m\u001b[1;32m    427\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0mopt_cond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflat_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mtolerance_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36m_strong_wolfe\u001b[0;34m(obj_func, x, t, d, f, g, gtd, c1, c2, tolerance_change, max_ls)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;31m# Evaluate new point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0mf_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mls_func_evals\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mgtd_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mobj_func\u001b[0;34m(x, t, d)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m                     \u001b[0;32mdef\u001b[0m \u001b[0mobj_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m                         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_directional_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                     loss, flat_grad, t, ls_func_evals = _strong_wolfe(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36m_directional_evaluate\u001b[0;34m(self, closure, x, t, d)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_directional_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0mflat_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gather_flat_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-d7b033eddfaf>\u001b[0m in \u001b[0;36mclosure\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    154\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_u_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_f_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    155\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 156\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_BC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_u_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    157\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    158\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-d7b033eddfaf>\u001b[0m in \u001b[0;36mloss_BC\u001b[0;34m(self, x, y)\u001b[0m\n\u001b[1;32m     82\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mloss_BC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     83\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 84\u001b[0;31m         \u001b[0mloss_u\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     85\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     86\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss_u\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-d7b033eddfaf>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     74\u001b[0m          \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m                 \u001b[0mz\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinears\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m                 \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbeta\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mactivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mz\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m          \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlinears\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##PRE-TRAIN\n",
    "max_reps = 1\n",
    "flag = 0\n",
    "\n",
    "train_loss_full = []\n",
    "test_loss_full = []\n",
    "beta_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(reps)\n",
    "    N_u1 = 1000 #Total number of data points for 'u'\n",
    "    N_u2 = 10000\n",
    "    \n",
    "    X_u_train, u_train = pretrainingdata(N_u1,N_u2,reps*23)\n",
    "        \n",
    "    X_u_train = torch.from_numpy(X_u_train).float().to(device)\n",
    "    u_train = torch.from_numpy(u_train).float().to(device)\n",
    "\n",
    "\n",
    "    layers = np.array([2,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "\n",
    "    PINN = Sequentialmodel(layers)\n",
    "       \n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "\n",
    "    'L-BFGS Optimizer'\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=1, \n",
    "                                  max_iter = 10000, \n",
    "                                  max_eval = None, \n",
    "                                  tolerance_grad = -1, \n",
    "                                  tolerance_change = -1, \n",
    "                                  history_size = 100, \n",
    "                                  line_search_fn = 'strong_wolfe')\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    optimizer.step(PINN.closure)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xt_test_tensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1894 Train Loss 5.2800687e+16 Test Loss 17471.682318532872\n",
      "1895 Train Loss 4.2560614e+17 Test Loss 24085.92891815221\n",
      "1896 Train Loss 2.380986e+18 Test Loss 21581.492381383745\n",
      "1897 Train Loss 2.4356868e+17 Test Loss 18308.40552833872\n",
      "1898 Train Loss 3.930569e+17 Test Loss 21136.233481193765\n",
      "1899 Train Loss 7.225028e+17 Test Loss 19554.393852808756\n",
      "1900 Train Loss 9.637539e+17 Test Loss 18846.313377289713\n",
      "1901 Train Loss 2.5603874e+16 Test Loss 19525.330767421183\n",
      "1902 Train Loss 3.8842486e+16 Test Loss 19978.867235658734\n",
      "1903 Train Loss 3.3151521e+16 Test Loss 20250.490489461088\n",
      "1904 Train Loss 3.0141492e+16 Test Loss 19982.166467520972\n",
      "1905 Train Loss 3.0620173e+16 Test Loss 19783.414762038152\n",
      "1906 Train Loss 2.6086946e+16 Test Loss 19662.94316035007\n",
      "1907 Train Loss 2.5634512e+16 Test Loss 19596.112033297442\n",
      "1908 Train Loss 2.5681549e+16 Test Loss 19560.786687886743\n",
      "1909 Train Loss 2.564066e+16 Test Loss 19543.35821714864\n",
      "1910 Train Loss 2.5620043e+16 Test Loss 19533.985557348107\n",
      "1911 Train Loss 2.5611979e+16 Test Loss 19529.7422110109\n",
      "1912 Train Loss 2.5607795e+16 Test Loss 19527.47878635421\n",
      "1913 Train Loss 2.5605796e+16 Test Loss 19526.301495961852\n",
      "1914 Train Loss 2.5605223e+16 Test Loss 19525.33546518564\n",
      "1915 Train Loss 2.5604078e+16 Test Loss 19525.330621405166\n",
      "1916 Train Loss 2.5603982e+16 Test Loss 19525.56558616325\n",
      "1917 Train Loss 2.5603795e+16 Test Loss 19525.366123732645\n",
      "1918 Train Loss 2.5603872e+16 Test Loss 19525.238717350414\n",
      "1919 Train Loss 2.5603795e+16 Test Loss 19525.375974771177\n",
      "1920 Train Loss 2.5603795e+16 Test Loss 19525.372743570733\n",
      "1921 Train Loss 2.5661972e+16 Test Loss 19286.72376008206\n",
      "1922 Train Loss 3.1308525e+16 Test Loss 19372.643705305745\n",
      "1923 Train Loss 2.559205e+16 Test Loss 19447.53279987107\n",
      "1924 Train Loss 2.5564919e+16 Test Loss 19478.243681424567\n",
      "1925 Train Loss 2.556001e+16 Test Loss 19475.010043162467\n",
      "1926 Train Loss 2.550466e+16 Test Loss 19436.71978790564\n",
      "1927 Train Loss 2.5355754e+16 Test Loss 19392.80624360421\n",
      "1928 Train Loss 2.4841365e+16 Test Loss 19608.11966324449\n",
      "1929 Train Loss 2.825512e+17 Test Loss 33037.00507744539\n",
      "1930 Train Loss 2.3606936e+16 Test Loss 25077.644062407737\n",
      "1931 Train Loss 6.006531e+16 Test Loss 29760.680657128774\n",
      "1932 Train Loss 2.3363379e+16 Test Loss 27609.575601362074\n",
      "1933 Train Loss 2.3385586e+16 Test Loss 26957.330243724537\n",
      "1934 Train Loss 2.3337955e+16 Test Loss 27367.093188409905\n",
      "1935 Train Loss 1.6913784e+16 Test Loss 84798.22533977384\n",
      "1936 Train Loss 5523113000000000.0 Test Loss 1781769.3552778175\n",
      "1937 Train Loss 5350977000000000.0 Test Loss 1838173.992914048\n",
      "1938 Train Loss 1455767400000000.0 Test Loss 3567147.955982857\n",
      "1939 Train Loss 3990478800000000.0 Test Loss 78680461.32104228\n",
      "1940 Train Loss 2725542000000000.0 Test Loss 23063978.966778774\n",
      "1941 Train Loss 1005928200000000.0 Test Loss 4403237.838656296\n",
      "1942 Train Loss 1103418200000000.0 Test Loss 5681089.636429889\n",
      "1943 Train Loss 1004857400000000.0 Test Loss 4417082.244724584\n",
      "1944 Train Loss 996174450000000.0 Test Loss 4536221.075435439\n",
      "1945 Train Loss 1094497700000000.0 Test Loss 5386818.58218575\n",
      "1946 Train Loss 974838860000000.0 Test Loss 5037554.130437412\n",
      "1947 Train Loss 970835500000000.0 Test Loss 4914602.642121359\n",
      "1948 Train Loss 966567000000000.0 Test Loss 5114489.4373477185\n",
      "1949 Train Loss 925721800000000.0 Test Loss 5338163.923883846\n",
      "1950 Train Loss 810771240000000.0 Test Loss 5718396.919923086\n",
      "1951 Train Loss 794677670000000.0 Test Loss 5888261.645013009\n",
      "1952 Train Loss 500425700000000.0 Test Loss 6685215.378072173\n",
      "1953 Train Loss 465322430000000.0 Test Loss 6254766.986856236\n",
      "1954 Train Loss 44083630000000.0 Test Loss 531361.6532752571\n",
      "1955 Train Loss 39267342000000.0 Test Loss 514938.06898668763\n",
      "1956 Train Loss 36756603000000.0 Test Loss 337719.561493133\n",
      "1957 Train Loss 35898285000000.0 Test Loss 317254.276343751\n",
      "1958 Train Loss 44362038000000.0 Test Loss 218008.97844448817\n",
      "1959 Train Loss 35026090000000.0 Test Loss 296251.5646230701\n",
      "1960 Train Loss 34923920000000.0 Test Loss 276440.32481625373\n",
      "1961 Train Loss 34859053000000.0 Test Loss 260382.6378996612\n",
      "1962 Train Loss 34623801000000.0 Test Loss 234965.8257801471\n",
      "1963 Train Loss 33883886000000.0 Test Loss 139703.64054445547\n",
      "1964 Train Loss 33483454000000.0 Test Loss 153221.91931959373\n",
      "1965 Train Loss 33111270000000.0 Test Loss 136045.59150423532\n",
      "1966 Train Loss 32961290000000.0 Test Loss 133154.13434462273\n",
      "1967 Train Loss 32804207000000.0 Test Loss 132555.95234385782\n",
      "1968 Train Loss 32333795000000.0 Test Loss 126436.69656119987\n",
      "1969 Train Loss 30848610000000.0 Test Loss 92695.51033069305\n",
      "1970 Train Loss 29051083000000.0 Test Loss 26126.185788118833\n",
      "1971 Train Loss 27496953000000.0 Test Loss 23063.20490283441\n",
      "1972 Train Loss 25830793000000.0 Test Loss 62201.360075880104\n",
      "1973 Train Loss 24652175000000.0 Test Loss 263846.4285212134\n",
      "1974 Train Loss 24112210000000.0 Test Loss 655531.5928915996\n",
      "1975 Train Loss 24007756000000.0 Test Loss 571972.5193627103\n",
      "1976 Train Loss 23948327000000.0 Test Loss 615025.674097772\n",
      "1977 Train Loss 23924723000000.0 Test Loss 639081.5675958223\n",
      "1978 Train Loss 23914472000000.0 Test Loss 636792.66225106\n",
      "1979 Train Loss 23904920000000.0 Test Loss 624212.1364809064\n",
      "1980 Train Loss 23888410000000.0 Test Loss 611809.9017128288\n",
      "1981 Train Loss 23808447000000.0 Test Loss 551599.7157112659\n",
      "1982 Train Loss 23687678000000.0 Test Loss 466512.34992196655\n",
      "1983 Train Loss 23490230000000.0 Test Loss 327360.8252751627\n",
      "1984 Train Loss 23382127000000.0 Test Loss 257838.05610946805\n",
      "1985 Train Loss 23330202000000.0 Test Loss 239587.4158690512\n",
      "1986 Train Loss 23311392000000.0 Test Loss 241283.02022571303\n",
      "1987 Train Loss 23308544000000.0 Test Loss 240617.42306752666\n",
      "1988 Train Loss 23248125000000.0 Test Loss 231213.02717773817\n",
      "1989 Train Loss 23156551000000.0 Test Loss 185794.11229530844\n",
      "1990 Train Loss 22892905000000.0 Test Loss 65867.42218286013\n",
      "1991 Train Loss 22567742000000.0 Test Loss 77536.84984330642\n",
      "1992 Train Loss 22384241000000.0 Test Loss 98179.41879682643\n",
      "1993 Train Loss 22058237000000.0 Test Loss 179739.0602869514\n",
      "1994 Train Loss 21927440000000.0 Test Loss 238839.30607141816\n",
      "1995 Train Loss 21844946000000.0 Test Loss 191826.7598566678\n",
      "1996 Train Loss 21778712000000.0 Test Loss 231182.09623275616\n",
      "1997 Train Loss 21753332000000.0 Test Loss 199897.77471614027\n",
      "1998 Train Loss 21745280000000.0 Test Loss 215655.53469405416\n",
      "1999 Train Loss 21716902000000.0 Test Loss 252355.17478880435\n",
      "2000 Train Loss 21640417000000.0 Test Loss 323615.2561564192\n",
      "2001 Train Loss 21486980000000.0 Test Loss 424741.9479235773\n",
      "2002 Train Loss 21250217000000.0 Test Loss 547328.229775488\n",
      "2003 Train Loss 21062731000000.0 Test Loss 519300.14222788863\n",
      "2004 Train Loss 20987320000000.0 Test Loss 425721.4122997339\n",
      "2005 Train Loss 20885250000000.0 Test Loss 380221.41755736293\n",
      "2006 Train Loss 20833762000000.0 Test Loss 282806.78818430594\n",
      "2007 Train Loss 20535776000000.0 Test Loss 248580.7099938485\n",
      "2008 Train Loss 28545881000000.0 Test Loss 65845.20474233014\n",
      "2009 Train Loss 20080715000000.0 Test Loss 190751.22718890742\n",
      "2010 Train Loss 20852813000000.0 Test Loss 81204.44234542434\n",
      "2011 Train Loss 19803811000000.0 Test Loss 146946.9073622314\n",
      "2012 Train Loss 19592322000000.0 Test Loss 125226.14976065392\n",
      "2013 Train Loss 5.288092e+16 Test Loss 306996794.6491797\n",
      "2014 Train Loss 716759200000000.0 Test Loss 36429119.77123934\n",
      "2015 Train Loss 107923670000000.0 Test Loss 3334066.0367225604\n",
      "2016 Train Loss 19635754000000.0 Test Loss 98114.88851627427\n",
      "2017 Train Loss 19516728000000.0 Test Loss 113475.95167936012\n",
      "2018 Train Loss 19481909000000.0 Test Loss 83266.89696113387\n",
      "2019 Train Loss 19453480000000.0 Test Loss 43946.81981088374\n",
      "2020 Train Loss 19426412000000.0 Test Loss 34514.020789015536\n",
      "2021 Train Loss 19334294000000.0 Test Loss 26512.768589714437\n",
      "2022 Train Loss 19284902000000.0 Test Loss 37472.16512062313\n",
      "2023 Train Loss 19205615000000.0 Test Loss 44055.77259982511\n",
      "2024 Train Loss 19129360000000.0 Test Loss 37104.227509277865\n",
      "2025 Train Loss 18986192000000.0 Test Loss 24075.12037500932\n",
      "2026 Train Loss 18834080000000.0 Test Loss 409831.7003727318\n",
      "2027 Train Loss 18595558000000.0 Test Loss 137004.7281549248\n",
      "2028 Train Loss 18442506000000.0 Test Loss 138272.3236288122\n",
      "2029 Train Loss 17939026000000.0 Test Loss 180946.37176307343\n",
      "2030 Train Loss 17395369000000.0 Test Loss 295191.24156505964\n",
      "2031 Train Loss 17007006000000.0 Test Loss 124232.29254814055\n",
      "2032 Train Loss 16743021000000.0 Test Loss 24335.446217890494\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2033 Train Loss 17588175000000.0 Test Loss 267907.3505355942\n",
      "2034 Train Loss 16674166000000.0 Test Loss 25214.76743052457\n",
      "2035 Train Loss 16493186000000.0 Test Loss 28104.278706364385\n",
      "2036 Train Loss 16308021000000.0 Test Loss 50089.03067686489\n",
      "2037 Train Loss 16102822000000.0 Test Loss 110262.41174265438\n",
      "2038 Train Loss 15936685000000.0 Test Loss 211588.83072117262\n",
      "2039 Train Loss 15908504000000.0 Test Loss 263098.6826723895\n",
      "2040 Train Loss 15706920000000.0 Test Loss 165269.532523306\n",
      "2041 Train Loss 15919086000000.0 Test Loss 120991.00476972047\n",
      "2042 Train Loss 15489145000000.0 Test Loss 41178.922335582974\n",
      "2043 Train Loss 15109935000000.0 Test Loss 24910.303299770276\n",
      "2044 Train Loss 14186638000000.0 Test Loss 75762.00011601584\n",
      "2045 Train Loss 13278542000000.0 Test Loss 1615503.3483147519\n",
      "2046 Train Loss 17553701000000.0 Test Loss 689139.575699256\n",
      "2047 Train Loss 12956870000000.0 Test Loss 1500990.409872352\n",
      "2048 Train Loss 12635795000000.0 Test Loss 1389588.695815312\n",
      "2049 Train Loss 12457532000000.0 Test Loss 1327792.8593690898\n",
      "2050 Train Loss 12224179000000.0 Test Loss 1242749.2597833849\n",
      "2051 Train Loss 12138813000000.0 Test Loss 1208395.4587438435\n",
      "2052 Train Loss 12009713000000.0 Test Loss 1139231.1817487641\n",
      "2053 Train Loss 12693522000000.0 Test Loss 22084.88252447163\n",
      "2054 Train Loss 11297776000000.0 Test Loss 486471.06173213705\n",
      "2055 Train Loss 118967320000000.0 Test Loss 13618008.41203815\n",
      "2056 Train Loss 12572770000000.0 Test Loss 1435928.740541484\n",
      "2057 Train Loss 11056238000000.0 Test Loss 17875.030996216105\n",
      "2058 Train Loss 10570099000000.0 Test Loss 27910.53312196088\n",
      "2059 Train Loss 10172815000000.0 Test Loss 27783.110658671474\n",
      "2060 Train Loss 9794164000000.0 Test Loss 5486.099382065354\n",
      "2061 Train Loss 9584004000000.0 Test Loss 17320.92399880341\n",
      "2062 Train Loss 11535596000000.0 Test Loss 383330.0107698731\n",
      "2063 Train Loss 9400033000000.0 Test Loss 122510.96394510423\n",
      "2064 Train Loss 9234589000000.0 Test Loss 117477.65880672337\n",
      "2065 Train Loss 9623569000000.0 Test Loss 178971.34110454912\n",
      "2066 Train Loss 9180056000000.0 Test Loss 125886.07127248036\n",
      "2067 Train Loss 9248532000000.0 Test Loss 347896.9526378907\n",
      "2068 Train Loss 9114640000000.0 Test Loss 195721.55737245496\n",
      "2069 Train Loss 8986847000000.0 Test Loss 189192.17010688558\n",
      "2070 Train Loss 8546121000000.0 Test Loss 224795.52914992452\n",
      "2071 Train Loss 175994490000000.0 Test Loss 16033114.59360662\n",
      "2072 Train Loss 27936948000000.0 Test Loss 5792577.126228904\n",
      "2073 Train Loss 8507119600000.0 Test Loss 278859.92365083017\n",
      "2074 Train Loss 8347629600000.0 Test Loss 394194.123531731\n",
      "2075 Train Loss 8183689500000.0 Test Loss 412517.8642093765\n",
      "2076 Train Loss 7981864000000.0 Test Loss 337733.7710665366\n",
      "2077 Train Loss 7855811000000.0 Test Loss 317650.8661919041\n",
      "2078 Train Loss 7638960600000.0 Test Loss 303429.9672866922\n",
      "2079 Train Loss 7508439600000.0 Test Loss 257391.51360098895\n",
      "2080 Train Loss 7354540000000.0 Test Loss 155042.90942708496\n",
      "2081 Train Loss 7274544000000.0 Test Loss 139016.52617213715\n",
      "2082 Train Loss 7152906000000.0 Test Loss 117661.25913223963\n",
      "2083 Train Loss 7073781000000.0 Test Loss 107522.35822603674\n",
      "2084 Train Loss 6970186700000.0 Test Loss 104964.26547738198\n",
      "2085 Train Loss 6819182300000.0 Test Loss 101418.18876224216\n",
      "2086 Train Loss 7952122000000.0 Test Loss 147751.5674773706\n",
      "2087 Train Loss 6626262600000.0 Test Loss 101577.2860059162\n",
      "2088 Train Loss 6479766000000.0 Test Loss 95104.72690855496\n",
      "2089 Train Loss 6384990600000.0 Test Loss 101198.61903352164\n",
      "2090 Train Loss 6384054700000.0 Test Loss 99599.5026272534\n",
      "2091 Train Loss 6328787400000.0 Test Loss 94314.18260446443\n",
      "2092 Train Loss 18777203000000.0 Test Loss 162325.387910831\n",
      "2093 Train Loss 6327739400000.0 Test Loss 94184.20773528643\n",
      "2094 Train Loss 6258695000000.0 Test Loss 79658.27096265742\n",
      "2095 Train Loss 6010888000000.0 Test Loss 21016.725524373647\n",
      "2096 Train Loss 5913198000000.0 Test Loss 21350.384057171144\n",
      "2097 Train Loss 5713302000000.0 Test Loss 82899.75122815666\n",
      "2098 Train Loss 7627475500000.0 Test Loss 593486.5197159988\n",
      "2099 Train Loss 5454111400000.0 Test Loss 323327.84181145823\n",
      "2100 Train Loss 5560664000000.0 Test Loss 384045.2909209882\n",
      "2101 Train Loss 5407311400000.0 Test Loss 352728.91192771017\n",
      "2102 Train Loss 5166777500000.0 Test Loss 180877.57556828475\n",
      "2103 Train Loss 12947769000000.0 Test Loss 525872.679444899\n",
      "2104 Train Loss 5123603000000.0 Test Loss 149107.70236302362\n",
      "2105 Train Loss 5032481700000.0 Test Loss 54769.77037704848\n",
      "2106 Train Loss 5009794000000.0 Test Loss 55331.54656189939\n",
      "2107 Train Loss 4950149000000.0 Test Loss 65796.18029790846\n",
      "2108 Train Loss 4924466300000.0 Test Loss 71169.57029719012\n",
      "2109 Train Loss 4884231000000.0 Test Loss 106707.22781769405\n",
      "2110 Train Loss 4820931000000.0 Test Loss 132314.53218668656\n",
      "2111 Train Loss 5083791600000.0 Test Loss 293539.67128902115\n",
      "2112 Train Loss 4705424000000.0 Test Loss 185056.63103826213\n",
      "2113 Train Loss 4635321000000.0 Test Loss 429744.0766595588\n",
      "2114 Train Loss 4607970400000.0 Test Loss 336884.37589953945\n",
      "2115 Train Loss 4593388000000.0 Test Loss 340187.98858902894\n",
      "2116 Train Loss 4565755000000.0 Test Loss 320399.20891745266\n",
      "2117 Train Loss 4522961000000.0 Test Loss 266473.39298316126\n",
      "2118 Train Loss 4434764500000.0 Test Loss 189773.8149439203\n",
      "2119 Train Loss 4444103000000.0 Test Loss 198787.95714284276\n",
      "2120 Train Loss 4421916000000.0 Test Loss 193934.3348798588\n",
      "2121 Train Loss 4382785800000.0 Test Loss 224643.00940367262\n",
      "2122 Train Loss 4362837200000.0 Test Loss 241572.07161594077\n",
      "2123 Train Loss 4301173000000.0 Test Loss 231110.8091063855\n",
      "2124 Train Loss 4190760600000.0 Test Loss 149474.1347099739\n",
      "2125 Train Loss 4129057700000.0 Test Loss 111928.44872560255\n",
      "2126 Train Loss 4083699200000.0 Test Loss 73948.4961016926\n",
      "2127 Train Loss 4038176800000.0 Test Loss 73426.1246239108\n",
      "2128 Train Loss 4020361200000.0 Test Loss 68085.66334821221\n",
      "2129 Train Loss 3992915800000.0 Test Loss 62704.883123481246\n",
      "2130 Train Loss 3914071800000.0 Test Loss 37244.219090473714\n",
      "2131 Train Loss 3845769400000.0 Test Loss 46491.88430331621\n",
      "2132 Train Loss 3762834400000.0 Test Loss 65302.344112186765\n",
      "2133 Train Loss 3703729600000.0 Test Loss 119251.7165782503\n",
      "2134 Train Loss 3674801200000.0 Test Loss 206871.0074603402\n",
      "2135 Train Loss 3632832200000.0 Test Loss 365289.37827646127\n",
      "2136 Train Loss 3579872300000.0 Test Loss 723521.0083532492\n",
      "2137 Train Loss 3518762000000.0 Test Loss 613588.7705463996\n",
      "2138 Train Loss 3432001800000.0 Test Loss 574878.0385166666\n",
      "2139 Train Loss 3293867700000.0 Test Loss 471163.9699323949\n",
      "2140 Train Loss 107741060000000.0 Test Loss 4348929.575458312\n",
      "2141 Train Loss 9970616000000.0 Test Loss 780963.4323372752\n",
      "2142 Train Loss 3212410000000.0 Test Loss 358794.13968890166\n",
      "2143 Train Loss 3106645000000.0 Test Loss 261896.65124605387\n",
      "2144 Train Loss 3013405800000.0 Test Loss 127784.32366080256\n",
      "2145 Train Loss 2984857800000.0 Test Loss 80459.20879014279\n",
      "2146 Train Loss 2964899800000.0 Test Loss 65407.19579354587\n",
      "2147 Train Loss 2927396400000.0 Test Loss 49100.26740256698\n",
      "2148 Train Loss 3100904000000.0 Test Loss 36157.80955892354\n",
      "2149 Train Loss 2918808300000.0 Test Loss 39878.098140804046\n",
      "2150 Train Loss 2903066800000.0 Test Loss 43075.44860881975\n",
      "2151 Train Loss 2897114300000.0 Test Loss 49348.66155427591\n",
      "2152 Train Loss 2887314800000.0 Test Loss 50990.928773402564\n",
      "2153 Train Loss 2879008800000.0 Test Loss 57453.96372055995\n",
      "2154 Train Loss 2870967500000.0 Test Loss 53646.41495562714\n",
      "2155 Train Loss 2853685200000.0 Test Loss 37586.828966003006\n",
      "2156 Train Loss 2846309000000.0 Test Loss 36358.898059943844\n",
      "2157 Train Loss 2832662800000.0 Test Loss 36303.05352936475\n",
      "2158 Train Loss 2814140000000.0 Test Loss 36347.87797008484\n",
      "2159 Train Loss 486449040000000.0 Test Loss 85177.48161064326\n",
      "2160 Train Loss 2814098500000.0 Test Loss 36347.58448652856\n",
      "2161 Train Loss 2844040400000.0 Test Loss 37242.287760919295\n",
      "2162 Train Loss 2808764000000.0 Test Loss 36610.915003372604\n",
      "2163 Train Loss 2794196600000.0 Test Loss 36239.503076509835\n",
      "2164 Train Loss 2780357700000.0 Test Loss 36563.838506190135\n",
      "2165 Train Loss 2756025800000.0 Test Loss 49922.23700212797\n",
      "2166 Train Loss 2747097400000.0 Test Loss 100831.25312890545\n",
      "2167 Train Loss 2740932000000.0 Test Loss 154339.36599717787\n",
      "2168 Train Loss 2731818400000.0 Test Loss 149369.22190281042\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2169 Train Loss 2719508600000.0 Test Loss 258941.90679227264\n",
      "2170 Train Loss 2710950400000.0 Test Loss 219718.96250994282\n",
      "2171 Train Loss 2704188600000.0 Test Loss 269994.9854066796\n",
      "2172 Train Loss 2698031000000.0 Test Loss 306298.30665541167\n",
      "2173 Train Loss 2694738000000.0 Test Loss 347399.9585373647\n",
      "2174 Train Loss 2692304000000.0 Test Loss 325229.5773105295\n",
      "2175 Train Loss 2687244700000.0 Test Loss 303166.8782849654\n",
      "2176 Train Loss 2679073100000.0 Test Loss 337275.84706452757\n",
      "2177 Train Loss 2660469300000.0 Test Loss 392603.04761367274\n",
      "2178 Train Loss 2644038600000.0 Test Loss 400872.28904201376\n",
      "2179 Train Loss 2632029300000.0 Test Loss 348110.99760186224\n",
      "2180 Train Loss 2616206600000.0 Test Loss 296098.5431591096\n",
      "2181 Train Loss 2599668400000.0 Test Loss 258287.368912799\n",
      "2182 Train Loss 2582604400000.0 Test Loss 232563.97147845716\n",
      "2183 Train Loss 2642463400000.0 Test Loss 209145.42661023684\n",
      "2184 Train Loss 2568137700000.0 Test Loss 222895.34560512574\n",
      "2185 Train Loss 2557130300000.0 Test Loss 211731.19441421636\n",
      "2186 Train Loss 7141675000000.0 Test Loss 47414.60267500034\n",
      "2187 Train Loss 2549763000000.0 Test Loss 204885.69628144093\n",
      "2188 Train Loss 2540737000000.0 Test Loss 180968.5015329864\n",
      "2189 Train Loss 2507046400000.0 Test Loss 175065.22443226792\n",
      "2190 Train Loss 2480593400000.0 Test Loss 183953.13769249097\n",
      "2191 Train Loss 2490357000000.0 Test Loss 164460.1186504729\n",
      "2192 Train Loss 2471089000000.0 Test Loss 176976.00142045808\n",
      "2193 Train Loss 2486698500000.0 Test Loss 227380.7595958255\n",
      "2194 Train Loss 2452304200000.0 Test Loss 197115.7922190681\n",
      "2195 Train Loss 2432010800000.0 Test Loss 175290.42671000533\n",
      "2196 Train Loss 2440511400000.0 Test Loss 147305.30955683373\n",
      "2197 Train Loss 2422180400000.0 Test Loss 163018.36553061937\n",
      "2198 Train Loss 2414842500000.0 Test Loss 169317.67550603874\n",
      "2199 Train Loss 2393437400000.0 Test Loss 205942.81924678414\n",
      "2200 Train Loss 2374756500000.0 Test Loss 254660.67831353535\n",
      "2201 Train Loss 2347795000000.0 Test Loss 326750.0416431914\n",
      "2202 Train Loss 20349415000000.0 Test Loss 5692116.003174015\n",
      "2203 Train Loss 2385717000000.0 Test Loss 418843.91937482945\n",
      "2204 Train Loss 2330859700000.0 Test Loss 373629.6677569029\n",
      "2205 Train Loss 2307932000000.0 Test Loss 342060.44315084966\n",
      "2206 Train Loss 2292848000000.0 Test Loss 244479.68535217614\n",
      "2207 Train Loss 2273097500000.0 Test Loss 269508.1987914414\n",
      "2208 Train Loss 2237372600000.0 Test Loss 355427.48331665306\n",
      "2209 Train Loss 2203630400000.0 Test Loss 389680.3731156103\n",
      "2210 Train Loss 1890457100000.0 Test Loss 378398.95393438335\n",
      "2211 Train Loss 4766504600000.0 Test Loss 383547.00763048494\n",
      "2212 Train Loss 1975607500000.0 Test Loss 374930.07678960176\n",
      "2213 Train Loss 1835061900000.0 Test Loss 376822.0650077235\n",
      "2214 Train Loss 4097230000000.0 Test Loss 400978.75610773684\n",
      "2215 Train Loss 2113373700000.0 Test Loss 375355.9148112669\n",
      "2216 Train Loss 1832251600000.0 Test Loss 376641.2117787536\n",
      "2217 Train Loss 1730393100000.0 Test Loss 346739.5527105516\n",
      "2218 Train Loss 1687636100000.0 Test Loss 309463.4529212255\n",
      "2219 Train Loss 1666590600000.0 Test Loss 368869.5904126853\n",
      "2220 Train Loss 1646930400000.0 Test Loss 356734.54569623875\n",
      "2221 Train Loss 1627689800000.0 Test Loss 290364.19380980806\n",
      "2222 Train Loss 1616117500000.0 Test Loss 256049.4384611813\n",
      "2223 Train Loss 1588423000000.0 Test Loss 156317.1737420831\n",
      "2224 Train Loss 1579144300000.0 Test Loss 126507.11214299386\n",
      "2225 Train Loss 1560164600000.0 Test Loss 83361.38648378146\n",
      "2226 Train Loss 1551797000000.0 Test Loss 61368.08548387859\n",
      "2227 Train Loss 1544802300000.0 Test Loss 63103.837339958445\n",
      "2228 Train Loss 1536203700000.0 Test Loss 52647.414308631254\n",
      "2229 Train Loss 1529957000000.0 Test Loss 37108.09988433639\n",
      "2230 Train Loss 1523997300000.0 Test Loss 30088.705397347887\n",
      "2231 Train Loss 1516783500000.0 Test Loss 43617.82866466693\n",
      "2232 Train Loss 1498353800000.0 Test Loss 69330.41511376457\n",
      "2233 Train Loss 1525902300000.0 Test Loss 160614.47672486675\n",
      "2234 Train Loss 1490124400000.0 Test Loss 94911.10524712663\n",
      "2235 Train Loss 1545734600000.0 Test Loss 131436.958694976\n",
      "2236 Train Loss 1483879600000.0 Test Loss 103782.14969483517\n",
      "2237 Train Loss 1455573400000.0 Test Loss 41366.363191581884\n",
      "2238 Train Loss 1437901700000.0 Test Loss 39469.05846077987\n",
      "2239 Train Loss 1425919400000.0 Test Loss 32994.134146401404\n",
      "2240 Train Loss 1419387100000.0 Test Loss 28582.229090881006\n",
      "2241 Train Loss 1413072000000.0 Test Loss 29761.69739282585\n",
      "2242 Train Loss 1415011200000.0 Test Loss 34182.94068327619\n",
      "2243 Train Loss 1405200100000.0 Test Loss 31804.409731305186\n",
      "2244 Train Loss 1398942000000.0 Test Loss 43358.57597987559\n",
      "2245 Train Loss 1394792100000.0 Test Loss 39400.97050456699\n",
      "2246 Train Loss 1392372500000.0 Test Loss 41705.16342030595\n",
      "2247 Train Loss 1388575300000.0 Test Loss 40267.6964158535\n",
      "2248 Train Loss 1376715300000.0 Test Loss 35344.24861833141\n",
      "2249 Train Loss 1362537600000.0 Test Loss 31501.53571997405\n",
      "2250 Train Loss 1353516400000.0 Test Loss 30466.50647360421\n",
      "2251 Train Loss 1344867800000.0 Test Loss 32583.378612492685\n",
      "2252 Train Loss 1338702700000.0 Test Loss 38773.15751242688\n",
      "2253 Train Loss 1333255500000.0 Test Loss 47898.60915593087\n",
      "2254 Train Loss 1731697600000.0 Test Loss 96507.62112454604\n",
      "2255 Train Loss 1331645800000.0 Test Loss 49728.77803686263\n",
      "2256 Train Loss 1327992400000.0 Test Loss 53227.32670113976\n",
      "2257 Train Loss 1327101900000.0 Test Loss 54514.4705829907\n",
      "2258 Train Loss 1324979000000.0 Test Loss 48726.93929341283\n",
      "2259 Train Loss 1942980700000.0 Test Loss 46702.37187666299\n",
      "2260 Train Loss 1320820900000.0 Test Loss 41541.903305936445\n",
      "2261 Train Loss 1413786500000.0 Test Loss 27882.08154344723\n",
      "2262 Train Loss 1331729700000.0 Test Loss 38333.33259010336\n",
      "2263 Train Loss 1319662900000.0 Test Loss 40174.41413231676\n",
      "2264 Train Loss 1310248400000.0 Test Loss 45175.117224772606\n",
      "2265 Train Loss 1307544800000.0 Test Loss 45988.30884554815\n",
      "2266 Train Loss 1303103800000.0 Test Loss 48444.06781692589\n",
      "2267 Train Loss 1299529300000.0 Test Loss 54374.745456193574\n",
      "2268 Train Loss 1293752200000.0 Test Loss 54456.322346526504\n",
      "2269 Train Loss 1282349000000.0 Test Loss 48839.56048310752\n",
      "2270 Train Loss 1273239400000.0 Test Loss 43636.17815491999\n",
      "2271 Train Loss 1266887700000.0 Test Loss 39854.207974236975\n",
      "2272 Train Loss 1258469700000.0 Test Loss 36467.828115212185\n",
      "2273 Train Loss 1237122200000.0 Test Loss 31447.47344661539\n",
      "2274 Train Loss 1218507700000.0 Test Loss 25902.834203318023\n",
      "2275 Train Loss 1215905800000.0 Test Loss 25934.72939952411\n",
      "2276 Train Loss 1195516900000.0 Test Loss 28017.85024328927\n",
      "2277 Train Loss 1195520300000.0 Test Loss 47418.51895733119\n",
      "2278 Train Loss 1190487000000.0 Test Loss 35230.12247874243\n",
      "2279 Train Loss 1179575000000.0 Test Loss 38250.92930609916\n",
      "2280 Train Loss 1157114500000.0 Test Loss 53553.26842463229\n",
      "2281 Train Loss 1146195700000.0 Test Loss 56482.70932578815\n",
      "2282 Train Loss 1130335600000.0 Test Loss 56306.64947846705\n",
      "2283 Train Loss 1109451500000.0 Test Loss 42625.23685097875\n",
      "2284 Train Loss 1090201400000.0 Test Loss 39060.49020204469\n",
      "2285 Train Loss 1078587560000.0 Test Loss 38156.8085764741\n",
      "2286 Train Loss 1107013000000.0 Test Loss 40879.77107551697\n",
      "2287 Train Loss 1066127070000.0 Test Loss 39272.53916193223\n",
      "2288 Train Loss 1064841000000.0 Test Loss 38890.34653292246\n",
      "2289 Train Loss 1059955740000.0 Test Loss 39044.787703480746\n",
      "2290 Train Loss 1055222800000.0 Test Loss 40412.57561029886\n",
      "2291 Train Loss 1041515940000.0 Test Loss 57454.79105582121\n",
      "2292 Train Loss 1442948000000.0 Test Loss 289763.79988091445\n",
      "2293 Train Loss 1037887700000.0 Test Loss 68013.15843846512\n",
      "2294 Train Loss 1027441300000.0 Test Loss 80228.05303204615\n",
      "2295 Train Loss 1013540800000.0 Test Loss 52165.256716327815\n",
      "2296 Train Loss 1006130800000.0 Test Loss 49198.56890694382\n",
      "2297 Train Loss 999038900000.0 Test Loss 48298.83987104234\n",
      "2298 Train Loss 994695500000.0 Test Loss 50099.7155242653\n",
      "2299 Train Loss 988160460000.0 Test Loss 52930.09967876401\n",
      "2300 Train Loss 985256100000.0 Test Loss 53203.855958729655\n",
      "2301 Train Loss 979129700000.0 Test Loss 50712.90128599858\n",
      "2302 Train Loss 978871100000.0 Test Loss 61572.20385712093\n",
      "2303 Train Loss 972448300000.0 Test Loss 56689.05861969265\n",
      "2304 Train Loss 970680960000.0 Test Loss 55184.76248924043\n",
      "2305 Train Loss 967202300000.0 Test Loss 54315.79096355744\n",
      "2306 Train Loss 964299800000.0 Test Loss 55338.51267493286\n",
      "2307 Train Loss 961369900000.0 Test Loss 56292.145880820804\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2308 Train Loss 956479100000.0 Test Loss 57383.35152875947\n",
      "2309 Train Loss 951462200000.0 Test Loss 55958.18280928134\n",
      "2310 Train Loss 1008178400000.0 Test Loss 48987.728857686576\n",
      "2311 Train Loss 945305300000.0 Test Loss 52667.116988465066\n",
      "2312 Train Loss 940803700000.0 Test Loss 53102.60007185975\n",
      "2313 Train Loss 938620700000.0 Test Loss 49332.985244071664\n",
      "2314 Train Loss 930675750000.0 Test Loss 49617.707680993226\n",
      "2315 Train Loss 1349196400000.0 Test Loss 162642.169069399\n",
      "2316 Train Loss 930052600000.0 Test Loss 48506.99415966997\n",
      "2317 Train Loss 948751760000.0 Test Loss 46421.86883387837\n",
      "2318 Train Loss 925451750000.0 Test Loss 47578.45664762904\n",
      "2319 Train Loss 917691960000.0 Test Loss 51816.95501984857\n",
      "2320 Train Loss 904341550000.0 Test Loss 61216.324123564475\n",
      "2321 Train Loss 894555000000.0 Test Loss 63674.37714298044\n",
      "2322 Train Loss 881980100000.0 Test Loss 61724.47927366048\n",
      "2323 Train Loss 873474200000.0 Test Loss 57889.30674863803\n",
      "2324 Train Loss 860841400000.0 Test Loss 49489.80305918094\n",
      "2325 Train Loss 24014440000000.0 Test Loss 878265.0310508317\n",
      "2326 Train Loss 860447050000.0 Test Loss 49256.271506039084\n",
      "2327 Train Loss 855144300000.0 Test Loss 40976.707000854054\n",
      "2328 Train Loss 843728900000.0 Test Loss 41363.540897933475\n",
      "2329 Train Loss 830429000000.0 Test Loss 41963.03593976944\n",
      "2330 Train Loss 814883340000.0 Test Loss 44222.44624023694\n",
      "2331 Train Loss 807229700000.0 Test Loss 42396.51147279007\n",
      "2332 Train Loss 801492700000.0 Test Loss 42097.04057879759\n",
      "2333 Train Loss 796373200000.0 Test Loss 43152.77571377088\n",
      "2334 Train Loss 792846000000.0 Test Loss 43859.878884366044\n",
      "2335 Train Loss 787434960000.0 Test Loss 47562.64242693243\n",
      "2336 Train Loss 781554500000.0 Test Loss 48590.116126656954\n",
      "2337 Train Loss 778795100000.0 Test Loss 49791.230675017294\n",
      "2338 Train Loss 776682100000.0 Test Loss 47105.83924346678\n",
      "2339 Train Loss 775183300000.0 Test Loss 45423.74288046586\n",
      "2340 Train Loss 773015860000.0 Test Loss 43922.157695599766\n",
      "2341 Train Loss 766191100000.0 Test Loss 42076.12386729948\n",
      "2342 Train Loss 7506727000000.0 Test Loss 99827.65413028529\n",
      "2343 Train Loss 883636500000.0 Test Loss 61091.42415055191\n",
      "2344 Train Loss 854184900000.0 Test Loss 41574.28363469639\n",
      "2345 Train Loss 764446300000.0 Test Loss 41713.56753430228\n",
      "2346 Train Loss 775352200000.0 Test Loss 41337.014749376154\n",
      "2347 Train Loss 762871400000.0 Test Loss 41518.653839493425\n",
      "2348 Train Loss 757887200000.0 Test Loss 42890.916143086506\n",
      "2349 Train Loss 750832650000.0 Test Loss 48029.68094497569\n",
      "2350 Train Loss 746808100000.0 Test Loss 48509.53707306023\n",
      "2351 Train Loss 743013600000.0 Test Loss 49549.04206735791\n",
      "2352 Train Loss 736965100000.0 Test Loss 47043.56584910153\n",
      "2353 Train Loss 732298700000.0 Test Loss 44440.40873755621\n",
      "2354 Train Loss 728743000000.0 Test Loss 46799.53734619422\n",
      "2355 Train Loss 722482200000.0 Test Loss 43813.220869746285\n",
      "2356 Train Loss 719466140000.0 Test Loss 43460.368863157484\n",
      "2357 Train Loss 714712800000.0 Test Loss 45006.19954087777\n",
      "2358 Train Loss 709987900000.0 Test Loss 46250.93933241645\n",
      "2359 Train Loss 700119650000.0 Test Loss 46555.25078472329\n",
      "2360 Train Loss 692125100000.0 Test Loss 44900.52162017843\n",
      "2361 Train Loss 686341200000.0 Test Loss 45367.39035611885\n",
      "2362 Train Loss 957439600000.0 Test Loss 43409.77732578592\n",
      "2363 Train Loss 916671600000.0 Test Loss 44121.336384856346\n",
      "2364 Train Loss 685585500000.0 Test Loss 45331.52399623516\n",
      "2365 Train Loss 673558360000.0 Test Loss 44421.65119000067\n",
      "2366 Train Loss 659997070000.0 Test Loss 44228.98938645333\n",
      "2367 Train Loss 645837950000.0 Test Loss 42765.18190005174\n",
      "2368 Train Loss 631330200000.0 Test Loss 41198.741955844744\n",
      "2369 Train Loss 622445900000.0 Test Loss 41678.21434342052\n",
      "2370 Train Loss 614743540000.0 Test Loss 41915.826477629984\n",
      "2371 Train Loss 634426600000.0 Test Loss 41455.85490527668\n",
      "2372 Train Loss 612841200000.0 Test Loss 41851.80390228181\n",
      "2373 Train Loss 615393850000.0 Test Loss 41446.06922162179\n",
      "2374 Train Loss 608350440000.0 Test Loss 41631.18492347492\n",
      "2375 Train Loss 607913050000.0 Test Loss 41941.188341372974\n",
      "2376 Train Loss 604112300000.0 Test Loss 41810.761841181724\n",
      "2377 Train Loss 601017160000.0 Test Loss 42253.176026360095\n",
      "2378 Train Loss 601205700000.0 Test Loss 44683.279635086685\n",
      "2379 Train Loss 597009240000.0 Test Loss 43296.696500165526\n",
      "2380 Train Loss 591460900000.0 Test Loss 44657.170862132916\n",
      "2381 Train Loss 584255540000.0 Test Loss 42854.912238676\n",
      "2382 Train Loss 578042660000.0 Test Loss 41743.98811349241\n",
      "2383 Train Loss 575779500000.0 Test Loss 41805.96350105578\n",
      "2384 Train Loss 574949950000.0 Test Loss 41784.218341015505\n",
      "2385 Train Loss 572957900000.0 Test Loss 42232.15314520923\n",
      "2386 Train Loss 571069240000.0 Test Loss 43170.43629559073\n",
      "2387 Train Loss 569768150000.0 Test Loss 43150.549971333334\n",
      "2388 Train Loss 568211500000.0 Test Loss 42257.927181475156\n",
      "2389 Train Loss 566553300000.0 Test Loss 41332.23146862262\n",
      "2390 Train Loss 564743800000.0 Test Loss 40818.18457256224\n",
      "2391 Train Loss 561535840000.0 Test Loss 40427.719504806206\n",
      "2392 Train Loss 555302200000.0 Test Loss 41560.25220886873\n",
      "2393 Train Loss 1135741200000.0 Test Loss 185493.0946401416\n",
      "2394 Train Loss 553374900000.0 Test Loss 40979.05045703928\n",
      "2395 Train Loss 546418300000.0 Test Loss 40021.042746768835\n",
      "2396 Train Loss 564164030000.0 Test Loss 58016.380484877256\n",
      "2397 Train Loss 533079850000.0 Test Loss 43249.024057574774\n",
      "2398 Train Loss 528126600000.0 Test Loss 52428.3717190936\n",
      "2399 Train Loss 524374440000.0 Test Loss 48599.091595999635\n",
      "2400 Train Loss 516003630000.0 Test Loss 43593.711439216975\n",
      "2401 Train Loss 518828100000.0 Test Loss 42195.03157699178\n",
      "2402 Train Loss 513501000000.0 Test Loss 43128.940042406495\n",
      "2403 Train Loss 508452270000.0 Test Loss 40367.06590228934\n",
      "2404 Train Loss 504252500000.0 Test Loss 40261.361122192815\n",
      "2405 Train Loss 501461880000.0 Test Loss 40250.68326596454\n",
      "2406 Train Loss 499388550000.0 Test Loss 40185.388639605495\n",
      "2407 Train Loss 496895330000.0 Test Loss 40411.40654091165\n",
      "2408 Train Loss 495233530000.0 Test Loss 40505.634180012516\n",
      "2409 Train Loss 494069400000.0 Test Loss 40941.89383326166\n",
      "2410 Train Loss 491935660000.0 Test Loss 41211.671285011194\n",
      "2411 Train Loss 489016200000.0 Test Loss 42332.425793860115\n",
      "2412 Train Loss 485790320000.0 Test Loss 41892.223223381676\n",
      "2413 Train Loss 483085320000.0 Test Loss 41084.85723615751\n",
      "2414 Train Loss 479152800000.0 Test Loss 39953.49801301491\n",
      "2415 Train Loss 475818360000.0 Test Loss 39800.0755555161\n",
      "2416 Train Loss 469278750000.0 Test Loss 39841.6846460299\n",
      "2417 Train Loss 454792480000.0 Test Loss 39835.957185298466\n",
      "2418 Train Loss 450962230000.0 Test Loss 39240.45163891909\n",
      "2419 Train Loss 445325200000.0 Test Loss 42054.453518591334\n",
      "2420 Train Loss 450799700000.0 Test Loss 60610.87234479118\n",
      "2421 Train Loss 444138900000.0 Test Loss 47725.780026354914\n",
      "2422 Train Loss 435307250000.0 Test Loss 40675.125761092444\n",
      "2423 Train Loss 426636080000.0 Test Loss 38593.55995264191\n",
      "2424 Train Loss 422849150000.0 Test Loss 38655.99806459165\n",
      "2425 Train Loss 422026900000.0 Test Loss 38733.830305077274\n",
      "2426 Train Loss 413324080000.0 Test Loss 38762.373389704844\n",
      "2427 Train Loss 523572640000.0 Test Loss 38885.62247065737\n",
      "2428 Train Loss 410091850000.0 Test Loss 38761.37242168703\n",
      "2429 Train Loss 410815330000.0 Test Loss 38344.217611745065\n",
      "2430 Train Loss 408569220000.0 Test Loss 38564.35046338204\n",
      "2431 Train Loss 405290700000.0 Test Loss 38252.96397307095\n",
      "2432 Train Loss 402897080000.0 Test Loss 38094.15867853703\n",
      "2433 Train Loss 399651080000.0 Test Loss 37927.490200514534\n",
      "2434 Train Loss 552775650000.0 Test Loss 35536.43138549807\n",
      "2435 Train Loss 520716580000.0 Test Loss 36358.442535662645\n",
      "2436 Train Loss 398866380000.0 Test Loss 37886.8457935451\n",
      "2437 Train Loss 396197460000.0 Test Loss 37726.00743245371\n",
      "2438 Train Loss 395874900000.0 Test Loss 37374.20047677614\n",
      "2439 Train Loss 390688280000.0 Test Loss 37358.33362903029\n",
      "2440 Train Loss 388847300000.0 Test Loss 37230.25659437813\n",
      "2441 Train Loss 387603070000.0 Test Loss 37238.42050853519\n",
      "2442 Train Loss 386337800000.0 Test Loss 36882.14050702404\n",
      "2443 Train Loss 384054000000.0 Test Loss 36791.93576419603\n",
      "2444 Train Loss 381132600000.0 Test Loss 36629.1653467033\n",
      "2445 Train Loss 379607580000.0 Test Loss 36511.530266591886\n",
      "2446 Train Loss 376438260000.0 Test Loss 36290.012879534195\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2447 Train Loss 372304840000.0 Test Loss 36039.381343531895\n",
      "2448 Train Loss 370366500000.0 Test Loss 41440.06856552775\n",
      "2449 Train Loss 359415150000.0 Test Loss 36123.76340449524\n",
      "2450 Train Loss 360340780000.0 Test Loss 34451.10999541301\n",
      "2451 Train Loss 354423370000.0 Test Loss 34252.47915483915\n",
      "2452 Train Loss 352763940000.0 Test Loss 33900.229470490856\n",
      "2453 Train Loss 349964800000.0 Test Loss 34094.55101690613\n",
      "2454 Train Loss 347221600000.0 Test Loss 39391.1920295978\n",
      "2455 Train Loss 344824000000.0 Test Loss 39715.55057788314\n",
      "2456 Train Loss 342182850000.0 Test Loss 35092.971003309576\n",
      "2457 Train Loss 341375160000.0 Test Loss 34127.952895450086\n",
      "2458 Train Loss 339887720000.0 Test Loss 33592.450863366954\n",
      "2459 Train Loss 338604820000.0 Test Loss 34545.59235280192\n",
      "2460 Train Loss 337437620000.0 Test Loss 35873.53477900018\n",
      "2461 Train Loss 335671260000.0 Test Loss 35662.05060827316\n",
      "2462 Train Loss 334410680000.0 Test Loss 35529.98438667566\n",
      "2463 Train Loss 333000740000.0 Test Loss 34709.796489039276\n",
      "2464 Train Loss 332500270000.0 Test Loss 34309.11081473143\n",
      "2465 Train Loss 331948800000.0 Test Loss 33965.76323943709\n",
      "2466 Train Loss 331504450000.0 Test Loss 34059.61226512277\n",
      "2467 Train Loss 330708650000.0 Test Loss 35715.02786436672\n",
      "2468 Train Loss 329866100000.0 Test Loss 35821.376743287925\n",
      "2469 Train Loss 327785580000.0 Test Loss 35806.514774741336\n",
      "2470 Train Loss 325092180000.0 Test Loss 34989.44620204877\n",
      "2471 Train Loss 323039000000.0 Test Loss 33894.562691375606\n",
      "2472 Train Loss 320797280000.0 Test Loss 32922.01305789183\n",
      "2473 Train Loss 321323660000.0 Test Loss 35118.33203695688\n",
      "2474 Train Loss 320153100000.0 Test Loss 33681.46407743055\n",
      "2475 Train Loss 318990450000.0 Test Loss 33021.99660567859\n",
      "2476 Train Loss 318239670000.0 Test Loss 32932.08498324928\n",
      "2477 Train Loss 317589880000.0 Test Loss 32917.38276992308\n",
      "2478 Train Loss 316669460000.0 Test Loss 32636.51859427525\n",
      "2479 Train Loss 316004240000.0 Test Loss 32435.710367994845\n",
      "2480 Train Loss 315368240000.0 Test Loss 32015.807824424817\n",
      "2481 Train Loss 314501700000.0 Test Loss 31786.313850643513\n",
      "2482 Train Loss 314041960000.0 Test Loss 31939.172714904813\n",
      "2483 Train Loss 311722400000.0 Test Loss 31979.218763718345\n",
      "2484 Train Loss 310906160000.0 Test Loss 32675.294559969927\n",
      "2485 Train Loss 309980730000.0 Test Loss 34386.48512473059\n",
      "2486 Train Loss 308871920000.0 Test Loss 33619.106480423965\n",
      "2487 Train Loss 306782140000.0 Test Loss 32783.915065003705\n",
      "2488 Train Loss 305302470000.0 Test Loss 32444.312321737325\n",
      "2489 Train Loss 303415030000.0 Test Loss 32133.043859396494\n",
      "2490 Train Loss 302183120000.0 Test Loss 32100.610418750537\n",
      "2491 Train Loss 300131220000.0 Test Loss 32061.672635007057\n",
      "2492 Train Loss 297945460000.0 Test Loss 32324.06322825803\n",
      "2493 Train Loss 291693000000.0 Test Loss 32618.156467535566\n",
      "2494 Train Loss 288575160000.0 Test Loss 32204.758757436237\n",
      "2495 Train Loss 285965600000.0 Test Loss 30673.292186770686\n",
      "2496 Train Loss 284555150000.0 Test Loss 28936.06969889774\n",
      "2497 Train Loss 281040550000.0 Test Loss 28913.19060019914\n",
      "2498 Train Loss 279785640000.0 Test Loss 28719.57659221855\n",
      "2499 Train Loss 279281600000.0 Test Loss 28731.046346215742\n",
      "2500 Train Loss 278640160000.0 Test Loss 28393.158671641373\n",
      "2501 Train Loss 277081750000.0 Test Loss 26770.76653281575\n",
      "2502 Train Loss 276359500000.0 Test Loss 25373.771529099344\n",
      "2503 Train Loss 274926140000.0 Test Loss 25778.032319422862\n",
      "2504 Train Loss 273883550000.0 Test Loss 26176.23675553185\n",
      "2505 Train Loss 271432220000.0 Test Loss 25869.19742520202\n",
      "2506 Train Loss 290990720000.0 Test Loss 24669.998677453237\n",
      "2507 Train Loss 271306500000.0 Test Loss 25674.667367226626\n",
      "2508 Train Loss 268287430000.0 Test Loss 25741.439999651546\n",
      "2509 Train Loss 268165610000.0 Test Loss 25726.9672887215\n",
      "2510 Train Loss 269387100000.0 Test Loss 26197.940466537522\n",
      "2511 Train Loss 267562320000.0 Test Loss 25907.286530151367\n",
      "2512 Train Loss 3025861600000.0 Test Loss 177239.69423272007\n",
      "2513 Train Loss 266419500000.0 Test Loss 26162.840678664095\n",
      "2514 Train Loss 264277230000.0 Test Loss 26578.09455191362\n",
      "2515 Train Loss 263380190000.0 Test Loss 25294.064109958057\n",
      "2516 Train Loss 295796200000.0 Test Loss 26539.912649548143\n",
      "2517 Train Loss 261931940000.0 Test Loss 24855.933456001614\n",
      "2518 Train Loss 260030990000.0 Test Loss 25272.013666476054\n",
      "2519 Train Loss 258764520000.0 Test Loss 25239.97868956466\n",
      "2520 Train Loss 256693010000.0 Test Loss 25071.3355041207\n",
      "2521 Train Loss 254429530000.0 Test Loss 24283.01557122301\n",
      "2522 Train Loss 251039860000.0 Test Loss 23964.757041795896\n",
      "2523 Train Loss 244488950000.0 Test Loss 22918.101987248432\n",
      "2524 Train Loss 238421050000.0 Test Loss 21470.891104945567\n",
      "2525 Train Loss 232099660000.0 Test Loss 19167.570773367395\n",
      "2526 Train Loss 232298920000.0 Test Loss 18314.24469473251\n",
      "2527 Train Loss 230607240000.0 Test Loss 18466.55783384569\n",
      "2528 Train Loss 228989650000.0 Test Loss 16970.81442128998\n",
      "2529 Train Loss 227116060000.0 Test Loss 15767.596073847055\n",
      "2530 Train Loss 225493730000.0 Test Loss 13794.250161515709\n",
      "2531 Train Loss 224334400000.0 Test Loss 13336.209644853838\n",
      "2532 Train Loss 222230600000.0 Test Loss 12867.00329560489\n",
      "2533 Train Loss 220058140000.0 Test Loss 12785.575086486144\n",
      "2534 Train Loss 220058100000.0 Test Loss 10559.160932262135\n",
      "2535 Train Loss 218318880000.0 Test Loss 11557.626209116657\n",
      "2536 Train Loss 225473200000.0 Test Loss 3456.4755201319367\n",
      "2537 Train Loss 216915720000.0 Test Loss 9039.171007372537\n",
      "2538 Train Loss 214221210000.0 Test Loss 6420.099248151123\n",
      "2539 Train Loss 213463650000.0 Test Loss 4098.29473055012\n",
      "2540 Train Loss 211098980000.0 Test Loss 3433.032682175351\n",
      "2541 Train Loss 211189990000.0 Test Loss 3419.8790847507817\n",
      "2542 Train Loss 210001150000.0 Test Loss 3416.996771417357\n",
      "2543 Train Loss 209903670000.0 Test Loss 8970.864953941324\n",
      "2544 Train Loss 208764730000.0 Test Loss 5034.224209513901\n",
      "2545 Train Loss 208853250000.0 Test Loss 18375.224123535132\n",
      "2546 Train Loss 207060250000.0 Test Loss 11191.08383642315\n",
      "2547 Train Loss 204978950000.0 Test Loss 12982.260553094793\n",
      "2548 Train Loss 204736760000.0 Test Loss 8901.302252448639\n",
      "2549 Train Loss 202852450000.0 Test Loss 13824.14926473996\n",
      "2550 Train Loss 201923900000.0 Test Loss 18039.840765822286\n",
      "2551 Train Loss 201097950000.0 Test Loss 20280.70122848962\n",
      "2552 Train Loss 200360900000.0 Test Loss 15616.188642552886\n",
      "2553 Train Loss 199953530000.0 Test Loss 16589.12547112031\n",
      "2554 Train Loss 199670700000.0 Test Loss 18182.995477801553\n",
      "2555 Train Loss 199341720000.0 Test Loss 17992.951674672473\n",
      "2556 Train Loss 198895800000.0 Test Loss 18850.871280102176\n",
      "2557 Train Loss 198519800000.0 Test Loss 19410.16790092498\n",
      "2558 Train Loss 197793430000.0 Test Loss 22404.477420134648\n",
      "2559 Train Loss 197318430000.0 Test Loss 19386.30708330495\n",
      "2560 Train Loss 196854400000.0 Test Loss 18737.540227399884\n",
      "2561 Train Loss 196464640000.0 Test Loss 15854.12584245186\n",
      "2562 Train Loss 196175580000.0 Test Loss 13983.555957498387\n",
      "2563 Train Loss 195809600000.0 Test Loss 9538.043367162\n",
      "2564 Train Loss 195705340000.0 Test Loss 9044.259740705904\n",
      "2565 Train Loss 195653910000.0 Test Loss 11537.397843604176\n",
      "2566 Train Loss 195483710000.0 Test Loss 9645.7379300371\n",
      "2567 Train Loss 194818520000.0 Test Loss 7246.835597280658\n",
      "2568 Train Loss 635057900000.0 Test Loss 3001.84698018662\n",
      "2569 Train Loss 200432750000.0 Test Loss 3613.1647688128264\n",
      "2570 Train Loss 193604110000.0 Test Loss 4851.2858866486295\n",
      "2571 Train Loss 39753250000000.0 Test Loss 3367.2440701355727\n",
      "2572 Train Loss 13364742000000.0 Test Loss 2874.5098198181076\n",
      "2573 Train Loss 447904900000.0 Test Loss 3384.4363087213824\n",
      "2574 Train Loss 202543070000.0 Test Loss 4101.070083881632\n",
      "2575 Train Loss 193623690000.0 Test Loss 4686.10484179696\n",
      "2576 Train Loss 193584380000.0 Test Loss 4769.636290062465\n",
      "2577 Train Loss 192870220000.0 Test Loss 6342.5389144226165\n",
      "2578 Train Loss 192248430000.0 Test Loss 10169.429765003195\n",
      "2579 Train Loss 191916700000.0 Test Loss 9385.33178780305\n",
      "2580 Train Loss 191897300000.0 Test Loss 9350.668793678464\n",
      "2581 Train Loss 191585160000.0 Test Loss 9367.10243966263\n",
      "2582 Train Loss 191239080000.0 Test Loss 8533.246037139406\n",
      "2583 Train Loss 190220040000.0 Test Loss 5954.769726209248\n",
      "2584 Train Loss 189293840000.0 Test Loss 4353.667729267974\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2585 Train Loss 188375330000.0 Test Loss 3103.2527154553222\n",
      "2586 Train Loss 187322070000.0 Test Loss 3645.6440261404246\n",
      "2587 Train Loss 204122910000.0 Test Loss 45759.88568091841\n",
      "2588 Train Loss 186604930000.0 Test Loss 9080.438123277163\n",
      "2589 Train Loss 185313130000.0 Test Loss 10472.467149859172\n",
      "2590 Train Loss 183548640000.0 Test Loss 19469.628107784352\n",
      "2591 Train Loss 183708000000.0 Test Loss 7737.4878519628455\n",
      "2592 Train Loss 182841200000.0 Test Loss 13108.046576214932\n",
      "2593 Train Loss 184824700000.0 Test Loss 4677.818037268993\n",
      "2594 Train Loss 182248390000.0 Test Loss 9784.843095746339\n",
      "2595 Train Loss 180549500000.0 Test Loss 9329.357439934456\n",
      "2596 Train Loss 185401840000.0 Test Loss 4950.113441730663\n",
      "2597 Train Loss 177962800000.0 Test Loss 7324.0612833175555\n",
      "2598 Train Loss 196174480000.0 Test Loss 3303.4882143783902\n",
      "2599 Train Loss 177591880000.0 Test Loss 5974.941775546758\n",
      "2600 Train Loss 175257570000.0 Test Loss 6280.6041704474555\n",
      "2601 Train Loss 173316670000.0 Test Loss 11229.10264539464\n",
      "2602 Train Loss 171168510000.0 Test Loss 6171.496599747439\n",
      "2603 Train Loss 169753000000.0 Test Loss 5122.6756817388095\n",
      "2604 Train Loss 169555500000.0 Test Loss 4658.0176306345775\n",
      "2605 Train Loss 168854000000.0 Test Loss 3175.589817159239\n",
      "2606 Train Loss 167126320000.0 Test Loss 3380.135917017802\n",
      "2607 Train Loss 166000510000.0 Test Loss 3817.647365050343\n",
      "2608 Train Loss 164470930000.0 Test Loss 5761.770964092742\n",
      "2609 Train Loss 164060860000.0 Test Loss 6602.8940571449275\n",
      "2610 Train Loss 163876550000.0 Test Loss 7383.365095405342\n",
      "2611 Train Loss 163458420000.0 Test Loss 5321.439923893283\n",
      "2612 Train Loss 163051200000.0 Test Loss 4193.901617283427\n",
      "2613 Train Loss 162381120000.0 Test Loss 3617.4406926140737\n",
      "2614 Train Loss 161673380000.0 Test Loss 3546.389682986709\n",
      "2615 Train Loss 161175240000.0 Test Loss 5215.925204974124\n",
      "2616 Train Loss 160559730000.0 Test Loss 7230.361199042003\n",
      "2617 Train Loss 172614140000.0 Test Loss 14837.853218487035\n",
      "2618 Train Loss 159462950000.0 Test Loss 10503.097099082584\n",
      "2619 Train Loss 159801700000.0 Test Loss 14475.755765413167\n",
      "2620 Train Loss 158938900000.0 Test Loss 12529.072903946217\n",
      "2621 Train Loss 264749250000.0 Test Loss 51498.37224083657\n",
      "2622 Train Loss 157984980000.0 Test Loss 14442.716681165472\n",
      "2623 Train Loss 156809900000.0 Test Loss 17995.271307055922\n",
      "2624 Train Loss 154130220000.0 Test Loss 14213.921675488335\n",
      "2625 Train Loss 153654670000.0 Test Loss 6078.30033437179\n",
      "2626 Train Loss 150724740000.0 Test Loss 11566.705491677327\n",
      "2627 Train Loss 150503280000.0 Test Loss 13652.00325259496\n",
      "2628 Train Loss 150354510000.0 Test Loss 12837.67857693918\n",
      "2629 Train Loss 150285320000.0 Test Loss 12288.530191666414\n",
      "2630 Train Loss 150261650000.0 Test Loss 12600.307359243896\n",
      "2631 Train Loss 150215590000.0 Test Loss 13090.391486712597\n",
      "2632 Train Loss 150141830000.0 Test Loss 12209.073556173991\n",
      "2633 Train Loss 150003370000.0 Test Loss 10092.034731341731\n",
      "2634 Train Loss 149756360000.0 Test Loss 7140.041655974686\n",
      "2635 Train Loss 149991470000.0 Test Loss 3774.7294328657545\n",
      "2636 Train Loss 149506020000.0 Test Loss 3494.037105821063\n",
      "2637 Train Loss 148989050000.0 Test Loss 3191.0955624335857\n",
      "2638 Train Loss 148487910000.0 Test Loss 8793.75651009115\n",
      "2639 Train Loss 148278820000.0 Test Loss 8890.399781045977\n",
      "2640 Train Loss 148043860000.0 Test Loss 5712.321203364687\n",
      "2641 Train Loss 147560630000.0 Test Loss 5274.779863493803\n",
      "2642 Train Loss 147166790000.0 Test Loss 4349.242852226211\n",
      "2643 Train Loss 146313460000.0 Test Loss 3176.629370146207\n",
      "2644 Train Loss 145723200000.0 Test Loss 7346.220934395411\n",
      "2645 Train Loss 145613060000.0 Test Loss 8746.885474264756\n",
      "2646 Train Loss 144446390000.0 Test Loss 7915.48184763316\n",
      "2647 Train Loss 144117020000.0 Test Loss 14213.158855887461\n",
      "2648 Train Loss 143733750000.0 Test Loss 12070.498248257149\n",
      "2649 Train Loss 143501120000.0 Test Loss 16012.069287615283\n",
      "2650 Train Loss 143333240000.0 Test Loss 9125.606558348845\n",
      "2651 Train Loss 142982860000.0 Test Loss 12316.603486807748\n",
      "2652 Train Loss 142241420000.0 Test Loss 19763.660779979044\n",
      "2653 Train Loss 141185520000.0 Test Loss 21051.30060115844\n",
      "2654 Train Loss 139377070000.0 Test Loss 19273.489513776145\n",
      "2655 Train Loss 137119780000.0 Test Loss 13089.889417295113\n",
      "2656 Train Loss 139216780000.0 Test Loss 16998.10122894146\n",
      "2657 Train Loss 135626370000.0 Test Loss 14591.34763222457\n",
      "2658 Train Loss 132728110000.0 Test Loss 11543.771128324794\n",
      "2659 Train Loss 129616036000.0 Test Loss 16952.762218173604\n",
      "2660 Train Loss 125869860000.0 Test Loss 23516.175845547692\n",
      "2661 Train Loss 128104210000.0 Test Loss 25706.939462891285\n",
      "2662 Train Loss 122995155000.0 Test Loss 24573.338999265303\n",
      "2663 Train Loss 126434165000.0 Test Loss 7943.461837100893\n",
      "2664 Train Loss 121483860000.0 Test Loss 17939.48945016065\n",
      "2665 Train Loss 119544270000.0 Test Loss 10607.536618148559\n",
      "2666 Train Loss 118003830000.0 Test Loss 6382.067999931206\n",
      "2667 Train Loss 116014920000.0 Test Loss 4747.866362667231\n",
      "2668 Train Loss 113057170000.0 Test Loss 4609.3892425200165\n",
      "2669 Train Loss 429870740000.0 Test Loss 4344.092886917535\n",
      "2670 Train Loss 113003880000.0 Test Loss 4605.673445621027\n",
      "2671 Train Loss 111340995000.0 Test Loss 4552.984455956211\n",
      "2672 Train Loss 111139900000.0 Test Loss 4549.678988436248\n",
      "2673 Train Loss 110583170000.0 Test Loss 4509.220655337326\n",
      "2674 Train Loss 203790500000.0 Test Loss 6014.288605202408\n",
      "2675 Train Loss 110529240000.0 Test Loss 4749.363193403286\n",
      "2676 Train Loss 110236360000.0 Test Loss 4634.709524705545\n",
      "2677 Train Loss 130564290000.0 Test Loss 31817.57514839809\n",
      "2678 Train Loss 108099710000.0 Test Loss 7980.202822776381\n",
      "2679 Train Loss 108784470000.0 Test Loss 9107.28213976338\n",
      "2680 Train Loss 107311840000.0 Test Loss 8459.595958293912\n",
      "2681 Train Loss 106080470000.0 Test Loss 8990.781396564262\n",
      "2682 Train Loss 104298120000.0 Test Loss 7795.5933509660335\n",
      "2683 Train Loss 103049445000.0 Test Loss 9618.98927800362\n",
      "2684 Train Loss 101793915000.0 Test Loss 14242.811564703583\n",
      "2685 Train Loss 101046580000.0 Test Loss 16166.079598406528\n",
      "2686 Train Loss 100594540000.0 Test Loss 14375.565118266899\n",
      "2687 Train Loss 100182820000.0 Test Loss 11248.458614094243\n",
      "2688 Train Loss 99791810000.0 Test Loss 10562.149732798025\n",
      "2689 Train Loss 99601370000.0 Test Loss 11987.484450249252\n",
      "2690 Train Loss 99355680000.0 Test Loss 12363.574093671583\n",
      "2691 Train Loss 99212760000.0 Test Loss 13140.216521086768\n",
      "2692 Train Loss 99128630000.0 Test Loss 14443.475854143473\n",
      "2693 Train Loss 98954540000.0 Test Loss 13783.649405034423\n",
      "2694 Train Loss 98785124000.0 Test Loss 12406.977500709374\n",
      "2695 Train Loss 98680690000.0 Test Loss 11959.856155338935\n",
      "2696 Train Loss 98462190000.0 Test Loss 11200.088115932513\n",
      "2697 Train Loss 98322100000.0 Test Loss 12096.86558965616\n",
      "2698 Train Loss 98082510000.0 Test Loss 13357.287363466743\n",
      "2699 Train Loss 97626330000.0 Test Loss 14592.58099713912\n",
      "2700 Train Loss 97064880000.0 Test Loss 12953.874354026828\n",
      "2701 Train Loss 96543120000.0 Test Loss 9121.192098167796\n",
      "2702 Train Loss 95822030000.0 Test Loss 3819.2113587654953\n",
      "2703 Train Loss 5470524300000.0 Test Loss 12249.151817528105\n",
      "2704 Train Loss 149902900000.0 Test Loss 3765.452052297717\n",
      "2705 Train Loss 97239020000.0 Test Loss 2762.612429103465\n",
      "2706 Train Loss 95570760000.0 Test Loss 3123.2230911961187\n",
      "2707 Train Loss 94443850000.0 Test Loss 2739.4957431665507\n",
      "2708 Train Loss 93403650000.0 Test Loss 2834.5605226197263\n",
      "2709 Train Loss 91800970000.0 Test Loss 2890.612071484129\n",
      "2710 Train Loss 90710510000.0 Test Loss 2799.58421681241\n",
      "2711 Train Loss 89465010000.0 Test Loss 4113.811179101793\n",
      "2712 Train Loss 97858250000.0 Test Loss 17337.581909846715\n",
      "2713 Train Loss 89043440000.0 Test Loss 6609.072338958296\n",
      "2714 Train Loss 88714200000.0 Test Loss 19538.3584024803\n",
      "2715 Train Loss 88371810000.0 Test Loss 13458.633376088674\n",
      "2716 Train Loss 89256030000.0 Test Loss 2945.3330842239297\n",
      "2717 Train Loss 87913180000.0 Test Loss 8126.559902970413\n",
      "2718 Train Loss 87613640000.0 Test Loss 9042.176320739738\n",
      "2719 Train Loss 87313220000.0 Test Loss 9775.420963732695\n",
      "2720 Train Loss 87167300000.0 Test Loss 7928.081726069105\n",
      "2721 Train Loss 87053320000.0 Test Loss 5961.123032372389\n",
      "2722 Train Loss 86915120000.0 Test Loss 4418.771537398962\n",
      "2723 Train Loss 86779500000.0 Test Loss 3671.3107523623958\n",
      "2724 Train Loss 86636675000.0 Test Loss 3763.5124310923725\n",
      "2725 Train Loss 87171470000.0 Test Loss 4846.393038052845\n",
      "2726 Train Loss 86497650000.0 Test Loss 4098.352678942635\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2727 Train Loss 86328090000.0 Test Loss 5497.60393671112\n",
      "2728 Train Loss 86162220000.0 Test Loss 8188.376201468327\n",
      "2729 Train Loss 86121130000.0 Test Loss 8583.959416820822\n",
      "2730 Train Loss 85984560000.0 Test Loss 8202.635388037346\n",
      "2731 Train Loss 85641280000.0 Test Loss 8416.92119176071\n",
      "2732 Train Loss 85377750000.0 Test Loss 9535.255653483642\n",
      "2733 Train Loss 85855390000.0 Test Loss 10426.838946783777\n",
      "2734 Train Loss 85185225000.0 Test Loss 9822.426486227485\n",
      "2735 Train Loss 84960264000.0 Test Loss 9991.099374468617\n",
      "2736 Train Loss 84771360000.0 Test Loss 9015.157439883147\n",
      "2737 Train Loss 84677665000.0 Test Loss 8507.779776359004\n",
      "2738 Train Loss 84637565000.0 Test Loss 7574.03431940529\n",
      "2739 Train Loss 84569450000.0 Test Loss 6777.440364662725\n",
      "2740 Train Loss 84489080000.0 Test Loss 5468.801852420853\n",
      "2741 Train Loss 84459070000.0 Test Loss 5499.4379947846555\n",
      "2742 Train Loss 86100710000.0 Test Loss 5371.069868690538\n",
      "2743 Train Loss 84363354000.0 Test Loss 5472.217503180575\n",
      "2744 Train Loss 84324295000.0 Test Loss 5241.932952075412\n",
      "2745 Train Loss 84202906000.0 Test Loss 3949.59378910721\n",
      "2746 Train Loss 84094210000.0 Test Loss 4091.925716846117\n",
      "2747 Train Loss 84048150000.0 Test Loss 4010.4685283119165\n",
      "2748 Train Loss 84012030000.0 Test Loss 4095.4499593692385\n",
      "2749 Train Loss 83972880000.0 Test Loss 4411.373914070254\n",
      "2750 Train Loss 83935330000.0 Test Loss 4276.323587948827\n",
      "2751 Train Loss 83874400000.0 Test Loss 4358.076912666155\n",
      "2752 Train Loss 83820460000.0 Test Loss 4833.7036646759425\n",
      "2753 Train Loss 83771154000.0 Test Loss 5458.238040217617\n",
      "2754 Train Loss 83921790000.0 Test Loss 6967.221543159033\n",
      "2755 Train Loss 83774020000.0 Test Loss 5755.546188977571\n",
      "2756 Train Loss 83770670000.0 Test Loss 5540.042317570041\n",
      "2757 Train Loss 83778390000.0 Test Loss 7161.570208259826\n",
      "2758 Train Loss 83768490000.0 Test Loss 6178.070713530626\n",
      "2759 Train Loss 83706260000.0 Test Loss 7815.379558090569\n",
      "2760 Train Loss 83694460000.0 Test Loss 7384.902385592473\n",
      "2761 Train Loss 98355670000.0 Test Loss 3225.797484385945\n",
      "2762 Train Loss 83632850000.0 Test Loss 6624.055194011377\n",
      "2763 Train Loss 83653090000.0 Test Loss 6013.340110020088\n",
      "2764 Train Loss 83666890000.0 Test Loss 6456.638862528729\n",
      "2765 Train Loss 83654570000.0 Test Loss 6610.732380779041\n",
      "2766 Train Loss 83637960000.0 Test Loss 6623.351591104441\n",
      "2767 Train Loss 83632870000.0 Test Loss 6624.055917899242\n",
      "2768 Train Loss 83653090000.0 Test Loss 6013.340110020088\n",
      "2769 Train Loss 83666890000.0 Test Loss 6456.638862528729\n",
      "2770 Train Loss 83654570000.0 Test Loss 6610.732380779041\n",
      "2771 Train Loss 83637960000.0 Test Loss 6623.351591104441\n",
      "2772 Train Loss 83632870000.0 Test Loss 6624.055917899242\n",
      "2773 Train Loss 83653090000.0 Test Loss 6013.340110020088\n",
      "2774 Train Loss 83666890000.0 Test Loss 6456.638862528729\n",
      "2775 Train Loss 83654570000.0 Test Loss 6610.732380779041\n",
      "2776 Train Loss 83637960000.0 Test Loss 6623.351591104441\n",
      "2777 Train Loss 83632870000.0 Test Loss 6624.055917899242\n",
      "2778 Train Loss 83653090000.0 Test Loss 6013.340110020088\n",
      "2779 Train Loss 83666890000.0 Test Loss 6456.638862528729\n",
      "2780 Train Loss 83654570000.0 Test Loss 6610.732380779041\n",
      "2781 Train Loss 83637960000.0 Test Loss 6623.351591104441\n",
      "2782 Train Loss 83632870000.0 Test Loss 6624.055917899242\n",
      "2783 Train Loss 83653090000.0 Test Loss 6013.340110020088\n",
      "2784 Train Loss 83666890000.0 Test Loss 6456.638862528729\n",
      "2785 Train Loss 83654570000.0 Test Loss 6610.732380779041\n",
      "2786 Train Loss 83637960000.0 Test Loss 6623.351591104441\n",
      "2787 Train Loss 83632870000.0 Test Loss 6624.055917899242\n",
      "2788 Train Loss 83653090000.0 Test Loss 6013.340110020088\n",
      "2789 Train Loss 83666890000.0 Test Loss 6456.638862528729\n",
      "2790 Train Loss 83654570000.0 Test Loss 6610.732380779041\n",
      "2791 Train Loss 83637960000.0 Test Loss 6623.351591104441\n",
      "2792 Train Loss 83632870000.0 Test Loss 6624.055917899242\n",
      "2793 Train Loss 83653090000.0 Test Loss 6013.340110020088\n",
      "2794 Train Loss 83666890000.0 Test Loss 6456.638862528729\n",
      "2795 Train Loss 83654570000.0 Test Loss 6610.732380779041\n",
      "2796 Train Loss 83637960000.0 Test Loss 6623.351591104441\n",
      "2797 Train Loss 83632870000.0 Test Loss 6624.055917899242\n",
      "2798 Train Loss 83653090000.0 Test Loss 6013.340110020088\n",
      "2799 Train Loss 83666890000.0 Test Loss 6456.638862528729\n",
      "2800 Train Loss 83654570000.0 Test Loss 6610.732380779041\n",
      "2801 Train Loss 83637960000.0 Test Loss 6623.351591104441\n",
      "2802 Train Loss 83632870000.0 Test Loss 6624.055917899242\n",
      "2803 Train Loss 83653090000.0 Test Loss 6013.340110020088\n",
      "2804 Train Loss 83666890000.0 Test Loss 6456.638862528729\n",
      "2805 Train Loss 83654570000.0 Test Loss 6610.732380779041\n",
      "2806 Train Loss 83637960000.0 Test Loss 6623.351591104441\n",
      "2807 Train Loss 83632870000.0 Test Loss 6624.055917899242\n",
      "2808 Train Loss 83653090000.0 Test Loss 6013.340110020088\n",
      "2809 Train Loss 83666890000.0 Test Loss 6456.638862528729\n",
      "2810 Train Loss 83654570000.0 Test Loss 6610.732380779041\n",
      "2811 Train Loss 83637960000.0 Test Loss 6623.351591104441\n",
      "2812 Train Loss 83632870000.0 Test Loss 6624.055917899242\n",
      "2813 Train Loss 83653090000.0 Test Loss 6013.340110020088\n",
      "2814 Train Loss 83666890000.0 Test Loss 6456.638862528729\n",
      "2815 Train Loss 83654570000.0 Test Loss 6610.732380779041\n",
      "2816 Train Loss 83637960000.0 Test Loss 6623.351591104441\n",
      "2817 Train Loss 83632870000.0 Test Loss 6624.055917899242\n",
      "2818 Train Loss 83653090000.0 Test Loss 6013.340110020088\n",
      "2819 Train Loss 83666890000.0 Test Loss 6456.638862528729\n",
      "2820 Train Loss 83654570000.0 Test Loss 6610.732380779041\n",
      "2821 Train Loss 83637960000.0 Test Loss 6623.351591104441\n",
      "2822 Train Loss 83632870000.0 Test Loss 6624.055917899242\n",
      "2823 Train Loss 83653090000.0 Test Loss 6013.340110020088\n",
      "2824 Train Loss 83666890000.0 Test Loss 6456.638862528729\n",
      "2825 Train Loss 83654570000.0 Test Loss 6610.732380779041\n",
      "2826 Train Loss 83637960000.0 Test Loss 6623.351591104441\n",
      "2827 Train Loss 83632870000.0 Test Loss 6624.055917899242\n",
      "2828 Train Loss 83653090000.0 Test Loss 6013.340110020088\n",
      "2829 Train Loss 83666890000.0 Test Loss 6456.638862528729\n",
      "2830 Train Loss 83654570000.0 Test Loss 6610.732380779041\n",
      "2831 Train Loss 83637960000.0 Test Loss 6623.351591104441\n",
      "2832 Train Loss 83632870000.0 Test Loss 6624.055917899242\n",
      "2833 Train Loss 83653090000.0 Test Loss 6013.340110020088\n",
      "2834 Train Loss 83666890000.0 Test Loss 6456.638862528729\n",
      "2835 Train Loss 83654570000.0 Test Loss 6610.732380779041\n",
      "2836 Train Loss 83637960000.0 Test Loss 6623.351591104441\n",
      "2837 Train Loss 83632870000.0 Test Loss 6624.055917899242\n",
      "2838 Train Loss 83653090000.0 Test Loss 6013.340110020088\n",
      "2839 Train Loss 83666890000.0 Test Loss 6456.638862528729\n",
      "2840 Train Loss 83654570000.0 Test Loss 6610.732380779041\n",
      "2841 Train Loss 83637960000.0 Test Loss 6623.351591104441\n",
      "2842 Train Loss 83632870000.0 Test Loss 6624.055917899242\n",
      "2843 Train Loss 83653090000.0 Test Loss 6013.340110020088\n",
      "2844 Train Loss 83666890000.0 Test Loss 6456.638862528729\n",
      "2845 Train Loss 83654570000.0 Test Loss 6610.732380779041\n",
      "2846 Train Loss 83637960000.0 Test Loss 6623.351591104441\n",
      "2847 Train Loss 83632870000.0 Test Loss 6624.055917899242\n",
      "2848 Train Loss 83653090000.0 Test Loss 6013.340110020088\n",
      "2849 Train Loss 83666890000.0 Test Loss 6456.638862528729\n",
      "2850 Train Loss 83654570000.0 Test Loss 6610.732380779041\n",
      "2851 Train Loss 83637960000.0 Test Loss 6623.351591104441\n",
      "2852 Train Loss 83632870000.0 Test Loss 6624.055917899242\n",
      "2853 Train Loss 83653090000.0 Test Loss 6013.340110020088\n",
      "2854 Train Loss 83666890000.0 Test Loss 6456.638862528729\n",
      "2855 Train Loss 83654570000.0 Test Loss 6610.732380779041\n",
      "2856 Train Loss 83637960000.0 Test Loss 6623.351591104441\n",
      "2857 Train Loss 83632870000.0 Test Loss 6624.055917899242\n",
      "2858 Train Loss 83653090000.0 Test Loss 6013.340110020088\n",
      "2859 Train Loss 83666890000.0 Test Loss 6456.638862528729\n",
      "2860 Train Loss 83654570000.0 Test Loss 6610.732380779041\n",
      "2861 Train Loss 83637960000.0 Test Loss 6623.351591104441\n",
      "2862 Train Loss 83632870000.0 Test Loss 6624.055917899242\n",
      "2863 Train Loss 83653090000.0 Test Loss 6013.340110020088\n",
      "2864 Train Loss 83666890000.0 Test Loss 6456.638862528729\n",
      "2865 Train Loss 83654570000.0 Test Loss 6610.732380779041\n",
      "2866 Train Loss 83637960000.0 Test Loss 6623.351591104441\n",
      "2867 Train Loss 83632870000.0 Test Loss 6624.055917899242\n",
      "2868 Train Loss 83653090000.0 Test Loss 6013.340110020088\n",
      "2869 Train Loss 83666890000.0 Test Loss 6456.638862528729\n",
      "2870 Train Loss 83654570000.0 Test Loss 6610.732380779041\n",
      "2871 Train Loss 83637960000.0 Test Loss 6623.351591104441\n",
      "2872 Train Loss 83632870000.0 Test Loss 6624.055917899242\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2873 Train Loss 83653090000.0 Test Loss 6013.340110020088\n",
      "2874 Train Loss 83666890000.0 Test Loss 6456.638862528729\n",
      "2875 Train Loss 83654570000.0 Test Loss 6610.732380779041\n",
      "2876 Train Loss 83637960000.0 Test Loss 6623.351591104441\n",
      "2877 Train Loss 83632870000.0 Test Loss 6624.055917899242\n",
      "2878 Train Loss 83653090000.0 Test Loss 6013.340110020088\n",
      "2879 Train Loss 83666890000.0 Test Loss 6456.638862528729\n",
      "2880 Train Loss 83654570000.0 Test Loss 6610.732380779041\n",
      "2881 Train Loss 83637960000.0 Test Loss 6623.351591104441\n",
      "2882 Train Loss 83632870000.0 Test Loss 6624.055917899242\n",
      "2883 Train Loss 83653090000.0 Test Loss 6013.340110020088\n",
      "2884 Train Loss 83666890000.0 Test Loss 6456.638862528729\n",
      "2885 Train Loss 83654570000.0 Test Loss 6610.732380779041\n",
      "2886 Train Loss 83637960000.0 Test Loss 6623.351591104441\n",
      "2887 Train Loss 83632870000.0 Test Loss 6624.055917899242\n",
      "2888 Train Loss 83653090000.0 Test Loss 6013.340110020088\n",
      "2889 Train Loss 83666890000.0 Test Loss 6456.638862528729\n",
      "2890 Train Loss 83654570000.0 Test Loss 6610.732380779041\n",
      "2891 Train Loss 83637960000.0 Test Loss 6623.351591104441\n",
      "2892 Train Loss 83632870000.0 Test Loss 6624.055917899242\n",
      "2893 Train Loss 83653090000.0 Test Loss 6013.340110020088\n",
      "2894 Train Loss 83666890000.0 Test Loss 6456.638862528729\n",
      "2895 Train Loss 83654570000.0 Test Loss 6610.732380779041\n",
      "2896 Train Loss 83637960000.0 Test Loss 6623.351591104441\n",
      "2897 Train Loss 83632870000.0 Test Loss 6624.055917899242\n",
      "2898 Train Loss 83653090000.0 Test Loss 6013.340110020088\n",
      "2899 Train Loss 83666890000.0 Test Loss 6456.638862528729\n",
      "2900 Train Loss 83654570000.0 Test Loss 6610.732380779041\n",
      "2901 Train Loss 83637960000.0 Test Loss 6623.351591104441\n",
      "2902 Train Loss 83632870000.0 Test Loss 6624.055917899242\n",
      "2903 Train Loss 83653090000.0 Test Loss 6013.340110020088\n",
      "2904 Train Loss 83666890000.0 Test Loss 6456.638862528729\n",
      "2905 Train Loss 83654570000.0 Test Loss 6610.732380779041\n",
      "2906 Train Loss 83637960000.0 Test Loss 6623.351591104441\n",
      "2907 Train Loss 83632870000.0 Test Loss 6624.055917899242\n",
      "2908 Train Loss 83653090000.0 Test Loss 6013.340110020088\n",
      "2909 Train Loss 83666890000.0 Test Loss 6456.638862528729\n",
      "2910 Train Loss 83654570000.0 Test Loss 6610.732380779041\n",
      "2911 Train Loss 83637960000.0 Test Loss 6623.351591104441\n",
      "2912 Train Loss 83632870000.0 Test Loss 6624.055917899242\n",
      "2913 Train Loss 83653090000.0 Test Loss 6013.340110020088\n",
      "2914 Train Loss 83666890000.0 Test Loss 6456.638862528729\n",
      "2915 Train Loss 83654570000.0 Test Loss 6610.732380779041\n",
      "2916 Train Loss 83637960000.0 Test Loss 6623.351591104441\n",
      "2917 Train Loss 83632870000.0 Test Loss 6624.055917899242\n",
      "2918 Train Loss 83653090000.0 Test Loss 6013.340110020088\n",
      "2919 Train Loss 83666890000.0 Test Loss 6456.638862528729\n",
      "2920 Train Loss 83654570000.0 Test Loss 6610.732380779041\n",
      "2921 Train Loss 83637960000.0 Test Loss 6623.351591104441\n",
      "2922 Train Loss 83632870000.0 Test Loss 6624.055917899242\n",
      "2923 Train Loss 83653090000.0 Test Loss 6013.340110020088\n",
      "2924 Train Loss 83666890000.0 Test Loss 6456.638862528729\n",
      "2925 Train Loss 83654570000.0 Test Loss 6610.732380779041\n",
      "2926 Train Loss 83637960000.0 Test Loss 6623.351591104441\n",
      "2927 Train Loss 83632870000.0 Test Loss 6624.055917899242\n",
      "2928 Train Loss 83653090000.0 Test Loss 6013.340110020088\n",
      "2929 Train Loss 83666890000.0 Test Loss 6456.638862528729\n",
      "2930 Train Loss 83654570000.0 Test Loss 6610.732380779041\n",
      "2931 Train Loss 83637960000.0 Test Loss 6623.351591104441\n",
      "2932 Train Loss 83632870000.0 Test Loss 6624.055917899242\n",
      "2933 Train Loss 83653090000.0 Test Loss 6013.340110020088\n",
      "2934 Train Loss 83666890000.0 Test Loss 6456.638862528729\n",
      "2935 Train Loss 83654570000.0 Test Loss 6610.732380779041\n",
      "2936 Train Loss 83637960000.0 Test Loss 6623.351591104441\n",
      "2937 Train Loss 83632870000.0 Test Loss 6624.055917899242\n",
      "2938 Train Loss 83653090000.0 Test Loss 6013.340110020088\n",
      "2939 Train Loss 83666890000.0 Test Loss 6456.638862528729\n",
      "2940 Train Loss 83654570000.0 Test Loss 6610.732380779041\n",
      "2941 Train Loss 83637960000.0 Test Loss 6623.351591104441\n",
      "2942 Train Loss 83632870000.0 Test Loss 6624.055917899242\n",
      "2943 Train Loss 83653090000.0 Test Loss 6013.340110020088\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-14-6e5916b61de4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     43\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPINN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPINN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                     loss, flat_grad, t, ls_func_evals = _strong_wolfe(\n\u001b[0;32m--> 426\u001b[0;31m                         obj_func, x_init, t, d, loss, flat_grad, gtd)\n\u001b[0m\u001b[1;32m    427\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0mopt_cond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflat_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mtolerance_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36m_strong_wolfe\u001b[0;34m(obj_func, x, t, d, f, g, gtd, c1, c2, tolerance_change, max_ls)\u001b[0m\n\u001b[1;32m    145\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    146\u001b[0m         \u001b[0;31m# Evaluate new point\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 147\u001b[0;31m         \u001b[0mf_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    148\u001b[0m         \u001b[0mls_func_evals\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    149\u001b[0m         \u001b[0mgtd_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mobj_func\u001b[0;34m(x, t, d)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m                     \u001b[0;32mdef\u001b[0m \u001b[0mobj_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m                         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_directional_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                     loss, flat_grad, t, ls_func_evals = _strong_wolfe(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36m_directional_evaluate\u001b[0;34m(self, closure, x, t, d)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_directional_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0mflat_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gather_flat_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-d7b033eddfaf>\u001b[0m in \u001b[0;36mclosure\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    152\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mflag\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_u_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_f_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m             \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_BC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_u_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mu_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-d7b033eddfaf>\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, x, y, x_to_train_f, f_hat)\u001b[0m\n\u001b[1;32m    118\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    119\u001b[0m         \u001b[0mloss_u\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_BC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 120\u001b[0;31m         \u001b[0mloss_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_PDE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_to_train_f\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    121\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    122\u001b[0m         \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexp\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-12-d7b033eddfaf>\u001b[0m in \u001b[0;36mloss_PDE\u001b[0;34m(self, x_to_train_f, f_hat)\u001b[0m\n\u001b[1;32m     99\u001b[0m         \u001b[0mu\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 101\u001b[0;31m         \u001b[0mu_x_t\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mx_to_train_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    102\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    103\u001b[0m         \u001b[0mu_xx_tt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_x_t\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_to_train_f\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mgrad\u001b[0;34m(outputs, inputs, grad_outputs, retain_graph, create_graph, only_inputs, allow_unused)\u001b[0m\n\u001b[1;32m    226\u001b[0m     return Variable._execution_engine.run_backward(\n\u001b[1;32m    227\u001b[0m         \u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_outputs_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 228\u001b[0;31m         inputs, allow_unused, accumulate_grad=False)\n\u001b[0m\u001b[1;32m    229\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    230\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "flag = 1\n",
    "for reps in range(max_reps):\n",
    "    print(reps)\n",
    "    N_u = 1000 #Total number of data points for 'u'\n",
    "    N_f = 10000 #Total number of collocation points \n",
    "    \n",
    "    X_f_train_np_array, X_u_train_np_array, u_train_np_array = trainingdata(N_u,N_f,reps*32)\n",
    "        \n",
    "    X_f_train = torch.from_numpy(X_f_train_np_array).float().to(device)\n",
    "    X_u_train = torch.from_numpy(X_u_train_np_array).float().to(device)\n",
    "    u_train = torch.from_numpy(u_train_np_array).float().to(device)\n",
    "        \n",
    "    #u = torch.from_numpy(u_true).float().to(device)\n",
    "    f_hat = torch.zeros(X_f_train.shape[0],1).to(device)\n",
    "        \n",
    "\n",
    "    #X_u_test_tensor = torch.from_numpy(X_u_test).float().to(device)\n",
    "    'Convert to tensor and send to GPU'\n",
    "\n",
    "\n",
    "#     layers = np.array([2,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "\n",
    "#     #layers = np.array([2,512,512,1])\n",
    "#     PINN = Sequentialmodel(layers)\n",
    "       \n",
    "#     PINN.to(device)\n",
    "\n",
    "#     'Neural Network Summary'\n",
    "#     print(PINN)\n",
    "\n",
    "#     params = list(PINN.parameters())\n",
    "\n",
    "\n",
    "    'L-BFGS Optimizer'\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=1, \n",
    "                                  max_iter = 10000, \n",
    "                                  max_eval = None, \n",
    "                                  tolerance_grad = -1, \n",
    "                                  tolerance_change = -1, \n",
    "                                  history_size = 100, \n",
    "                                  line_search_fn = 'strong_wolfe')\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    optimizer.step(PINN.closure)\n",
    "    \n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(PINN.train_loss)\n",
    "    test_loss_full.append(PINN.test_loss)\n",
    "    elapsed_time[reps] = time.time() - start_time\n",
    "    beta_full.append(PINN.beta_val)\n",
    "    \n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_loss\": test_loss_full,\"Time\": elapsed_time, \"beta\": beta_full, \"label\": label}\n",
    "savemat(label+'.mat', mdic) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PINN.W2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred = PINN.test(xt_test_tensor)\n",
    "\n",
    "cmap = plt.cm.rainbow\n",
    "plt.imshow(u_pred.reshape(100,256),cmap = cmap,aspect =1,vmin=-10,vmax=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_u_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(u_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.max(u_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.cm.rainbow\n",
    "plt.imshow(u_true.reshape(100,256),cmap = cmap,aspect = 1,vmin=-10,vmax=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.min(u_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cmap = plt.cm.rainbow\n",
    "plt.imshow(np.transpose(np.abs(u_pred - u_true).reshape(100,256)),cmap = cmap,aspect = 0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0 \n",
    "for i in range(10):\n",
    "    print(test_loss_full[i][-1])\n",
    "    a = a + test_loss_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
