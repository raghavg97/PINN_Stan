{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "\n",
    "from torch.nn.parameter import Parameter\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat('burgers_shock_10sin.mat')  \t# Load data from file\n",
    "\n",
    "pi = torch.from_numpy(np.array(np.pi)).double().to(device)\n",
    "\n",
    "label = \"QCRE_2D_5_tanhxtanh_NW\"\n",
    "x = data['x']                                   # 256 points between -1 and 1 [256x1]\n",
    "t = data['t']                                   # 100 time points between 0 and 1 [100x1] \n",
    "usol = data['usol']   \n",
    "\n",
    "#usol = usol/1000# solution of 256x100 grid points\n",
    "\n",
    "X, T = np.meshgrid(x,t)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-1.        ,  0.        ],\n",
       "       [-1.        ,  0.0020202 ],\n",
       "       [-1.        ,  0.0040404 ],\n",
       "       [-1.        ,  0.00606061],\n",
       "       [-1.        ,  0.00808081],\n",
       "       [-1.        ,  0.01010101],\n",
       "       [-1.        ,  0.01212121],\n",
       "       [-1.        ,  0.01414141],\n",
       "       [-1.        ,  0.01616162],\n",
       "       [-1.        ,  0.01818182],\n",
       "       [-1.        ,  0.02020202],\n",
       "       [-1.        ,  0.02222222],\n",
       "       [-1.        ,  0.02424242],\n",
       "       [-1.        ,  0.02626263],\n",
       "       [-1.        ,  0.02828283],\n",
       "       [-1.        ,  0.03030303],\n",
       "       [-1.        ,  0.03232323],\n",
       "       [-1.        ,  0.03434343],\n",
       "       [-1.        ,  0.03636364],\n",
       "       [-1.        ,  0.03838384],\n",
       "       [-1.        ,  0.04040404],\n",
       "       [-1.        ,  0.04242424],\n",
       "       [-1.        ,  0.04444444],\n",
       "       [-1.        ,  0.04646465],\n",
       "       [-1.        ,  0.04848485],\n",
       "       [-1.        ,  0.05050505],\n",
       "       [-1.        ,  0.05252525],\n",
       "       [-1.        ,  0.05454545],\n",
       "       [-1.        ,  0.05656566],\n",
       "       [-1.        ,  0.05858586],\n",
       "       [-1.        ,  0.06060606],\n",
       "       [-1.        ,  0.06262626],\n",
       "       [-1.        ,  0.06464646],\n",
       "       [-1.        ,  0.06666667],\n",
       "       [-1.        ,  0.06868687],\n",
       "       [-1.        ,  0.07070707],\n",
       "       [-1.        ,  0.07272727],\n",
       "       [-1.        ,  0.07474747],\n",
       "       [-1.        ,  0.07676768],\n",
       "       [-1.        ,  0.07878788],\n",
       "       [-1.        ,  0.08080808],\n",
       "       [-1.        ,  0.08282828],\n",
       "       [-1.        ,  0.08484848],\n",
       "       [-1.        ,  0.08686869],\n",
       "       [-1.        ,  0.08888889],\n",
       "       [-1.        ,  0.09090909],\n",
       "       [-1.        ,  0.09292929],\n",
       "       [-1.        ,  0.09494949],\n",
       "       [-1.        ,  0.0969697 ],\n",
       "       [-1.        ,  0.0989899 ],\n",
       "       [-1.        ,  0.1010101 ],\n",
       "       [-1.        ,  0.1030303 ],\n",
       "       [-1.        ,  0.10505051],\n",
       "       [-1.        ,  0.10707071],\n",
       "       [-1.        ,  0.10909091],\n",
       "       [-1.        ,  0.11111111],\n",
       "       [-1.        ,  0.11313131],\n",
       "       [-1.        ,  0.11515152],\n",
       "       [-1.        ,  0.11717172],\n",
       "       [-1.        ,  0.11919192],\n",
       "       [-1.        ,  0.12121212],\n",
       "       [-1.        ,  0.12323232],\n",
       "       [-1.        ,  0.12525253],\n",
       "       [-1.        ,  0.12727273],\n",
       "       [-1.        ,  0.12929293],\n",
       "       [-1.        ,  0.13131313],\n",
       "       [-1.        ,  0.13333333],\n",
       "       [-1.        ,  0.13535354],\n",
       "       [-1.        ,  0.13737374],\n",
       "       [-1.        ,  0.13939394],\n",
       "       [-1.        ,  0.14141414],\n",
       "       [-1.        ,  0.14343434],\n",
       "       [-1.        ,  0.14545455],\n",
       "       [-1.        ,  0.14747475],\n",
       "       [-1.        ,  0.14949495],\n",
       "       [-1.        ,  0.15151515],\n",
       "       [-1.        ,  0.15353535],\n",
       "       [-1.        ,  0.15555556],\n",
       "       [-1.        ,  0.15757576],\n",
       "       [-1.        ,  0.15959596],\n",
       "       [-1.        ,  0.16161616],\n",
       "       [-1.        ,  0.16363636],\n",
       "       [-1.        ,  0.16565657],\n",
       "       [-1.        ,  0.16767677],\n",
       "       [-1.        ,  0.16969697],\n",
       "       [-1.        ,  0.17171717],\n",
       "       [-1.        ,  0.17373737],\n",
       "       [-1.        ,  0.17575758],\n",
       "       [-1.        ,  0.17777778],\n",
       "       [-1.        ,  0.17979798],\n",
       "       [-1.        ,  0.18181818],\n",
       "       [-1.        ,  0.18383838],\n",
       "       [-1.        ,  0.18585859],\n",
       "       [-1.        ,  0.18787879],\n",
       "       [-1.        ,  0.18989899],\n",
       "       [-1.        ,  0.19191919],\n",
       "       [-1.        ,  0.19393939],\n",
       "       [-1.        ,  0.1959596 ],\n",
       "       [-1.        ,  0.1979798 ],\n",
       "       [-1.        ,  0.2       ]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.hstack((X[:,0][:,None], T[:,0][:,None]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_u,N_f,seed):\n",
    "\n",
    "    '''Boundary Conditions'''\n",
    "    #Initial Condition -1 =< x =<1 and t = 0  \n",
    "    leftedge_x = np.hstack((X[0,:][:,None], T[0,:][:,None])) #L1\n",
    "    leftedge_u = usol[:,0][:,None]\n",
    "\n",
    "    #Boundary Condition x = -1 and 0 =< t =<1\n",
    "    bottomedge_x = np.hstack((X[:,0][:,None], T[:,0][:,None])) #L2\n",
    "    bottomedge_u = usol[-1,:][:,None]\n",
    "\n",
    "    #Boundary Condition x = 1 and 0 =< t =<1\n",
    "    topedge_x = np.hstack((X[:,-1][:,None], T[:,0][:,None])) #L3\n",
    "    topedge_u = usol[0,:][:,None]\n",
    "\n",
    "    all_X_u_train = np.vstack([leftedge_x, bottomedge_x, topedge_x]) # X_u_train [456,2] (456 = 256(L1)+100(L2)+100(L3))\n",
    "    all_u_train = np.vstack([leftedge_u, bottomedge_u, topedge_u])   #corresponding u [456x1]\n",
    "\n",
    "    #choose random N_u points for training\n",
    "    np.random.seed(seed)\n",
    "    idx = np.random.choice(all_X_u_train.shape[0], N_u, replace=False) \n",
    "\n",
    "    X_u_train = all_X_u_train[idx, :] #choose indices from  set 'idx' (x,t)\n",
    "    u_train = all_u_train[idx,:]      #choose corresponding u\n",
    "\n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    X_f_train = lb + (ub-lb)*lhs(2,N_f) \n",
    "    X_f_train = np.vstack((X_f_train, X_u_train)) # append training points to collocation points \n",
    "\n",
    "    return X_f_train, X_u_train, u_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_u_test = np.hstack((X.flatten()[:,None], T.flatten()[:,None]))\n",
    "\n",
    "# Domain bounds\n",
    "lb = X_u_test[0]  # [-1. 0.]\n",
    "ub = X_u_test[-1] # [1.  0.99]\n",
    "\n",
    "'''\n",
    "   Fortran Style ('F') flatten,stacked column wise!\n",
    "   u = [c1 \n",
    "        c2\n",
    "        .\n",
    "        .\n",
    "        cn]\n",
    "\n",
    "   u =  [25600x1] \n",
    "'''\n",
    "u_true = usol.flatten('F')[:,None]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        self.iter = 0\n",
    "        \n",
    "        '''\n",
    "        Alternatively:\n",
    "        \n",
    "        *all layers are callable \n",
    "    \n",
    "        Simple linear Layers\n",
    "        self.fc1 = nn.Linear(2,50)\n",
    "        self.fc2 = nn.Linear(50,50)\n",
    "        self.fc3 = nn.Linear(50,50)\n",
    "        self.fc4 = nn.Linear(50,1)\n",
    "        \n",
    "        '''\n",
    "    \n",
    "        'Xavier Normal Initialization'\n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            # weights from a normal distribution with \n",
    "            # Recommended gain value for tanh = 5/3?\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "            \n",
    "        self.beta = Parameter(0.25*torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "        \n",
    "        self.train_loss = []\n",
    "        self.test_loss = []\n",
    "        self.beta_val = []\n",
    "        \n",
    "    'foward pass'\n",
    "    def forward(self,x):\n",
    "         if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "         u_b = torch.from_numpy(ub).float().to(device)\n",
    "         l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "         #preprocessing input \n",
    "         x = (x - l_b)/(u_b - l_b) #feature scaling\n",
    "        \n",
    "         #convert to float\n",
    "         a = x.float()\n",
    "                        \n",
    "         '''     \n",
    "         Alternatively:\n",
    "        \n",
    "         a = self.activation(self.fc1(a))\n",
    "         a = self.activation(self.fc2(a))\n",
    "         a = self.activation(self.fc3(a))\n",
    "         a = self.fc4(a)\n",
    "         \n",
    "         '''\n",
    "        \n",
    "         for i in range(len(layers)-2):\n",
    "                z = self.linears[i](a)\n",
    "                a = self.activation(z) + self.beta[:,i]*z*self.activation(z)\n",
    "         \n",
    "         a = self.linears[-1](a)\n",
    "        \n",
    "         return a\n",
    "                        \n",
    "    def loss_BC(self,x,y):\n",
    "                \n",
    "        loss_u = self.loss_function(self.forward(x), y)\n",
    "                \n",
    "        return loss_u\n",
    "    \n",
    "    def loss_PDE(self, x_to_train_f,f_hat):\n",
    "        \n",
    "        nu = 0.01/pi\n",
    "                \n",
    "        x_1_f = x_to_train_f[:,[0]]\n",
    "        x_2_f = x_to_train_f[:,[1]]\n",
    "                        \n",
    "        g = x_to_train_f.clone()\n",
    "                        \n",
    "        g.requires_grad = True\n",
    "        \n",
    "        u = self.forward(g)\n",
    "                \n",
    "        u_x_t = autograd.grad(u,g,torch.ones([x_to_train_f.shape[0], 1]).to(device), retain_graph=True, create_graph=True)[0]\n",
    "                                \n",
    "        u_xx_tt = autograd.grad(u_x_t,g,torch.ones(x_to_train_f.shape).to(device), create_graph=True)[0]\n",
    "                                                            \n",
    "        u_x = u_x_t[:,[0]]\n",
    "        \n",
    "        u_t = u_x_t[:,[1]]\n",
    "        \n",
    "        u_xx = u_xx_tt[:,[0]]\n",
    "                                        \n",
    "        f = u_t + (self.forward(g))*(u_x) - (nu)*u_xx \n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,x,y,x_to_train_f,f_hat):\n",
    "\n",
    "        loss_u = self.loss_BC(x,y)\n",
    "        loss_f = self.loss_PDE(x_to_train_f,f_hat)\n",
    "        \n",
    "        loss_val = loss_u + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "     \n",
    "    'callable for optimizer'                                       \n",
    "    def closure(self):\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        loss = self.loss(X_u_train, u_train, X_f_train,f_hat)\n",
    "        \n",
    "        self.train_loss.append(loss.cpu().detach().numpy())\n",
    "        \n",
    "        \n",
    "        u_pred = self.test(X_u_test_tensor)\n",
    "        self.test_loss.append(np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1))))\n",
    "        self.beta_val.append(self.beta.cpu().detach().numpy())\n",
    "        \n",
    "        print(self.iter,\"Train Loss\",self.train_loss[-1],\"Test Loss\",self.test_loss[-1])\n",
    "        \n",
    "        loss.backward()\n",
    "        self.iter += 1\n",
    "     \n",
    "        return loss        \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self,xt_test_tensor):\n",
    "        u_pred = self.forward(X_u_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0 Train Loss 28.390257 Test Loss 24.524365234656123\n",
      "1 Train Loss 27.838833 Test Loss 24.52289657232022\n",
      "2 Train Loss 26.308802 Test Loss 24.471032347608592\n",
      "3 Train Loss 26.21892 Test Loss 24.39795519783399\n",
      "4 Train Loss 25.410006 Test Loss 23.644246198049185\n",
      "5 Train Loss 25.069773 Test Loss 22.71477875394026\n",
      "6 Train Loss 24.785078 Test Loss 22.97560234727506\n",
      "7 Train Loss 24.524035 Test Loss 22.55434664080324\n",
      "8 Train Loss 24.108715 Test Loss 20.72477977846809\n",
      "9 Train Loss 23.739088 Test Loss 20.792111745884082\n",
      "10 Train Loss 23.763292 Test Loss 21.433286960696815\n",
      "11 Train Loss 23.660053 Test Loss 21.07171896230191\n",
      "12 Train Loss 23.615526 Test Loss 21.03180891312813\n",
      "13 Train Loss 23.189093 Test Loss 20.66279911466084\n",
      "14 Train Loss 22.347654 Test Loss 19.564147077716058\n",
      "15 Train Loss 22.056345 Test Loss 19.055933096041613\n",
      "16 Train Loss 21.44427 Test Loss 18.2294163983548\n",
      "17 Train Loss 21.326612 Test Loss 17.82967044694165\n",
      "18 Train Loss 21.050632 Test Loss 17.391985027572325\n",
      "19 Train Loss 20.924002 Test Loss 17.50780511791904\n",
      "20 Train Loss 20.859537 Test Loss 17.48692140865128\n",
      "21 Train Loss 20.828856 Test Loss 17.51504877373427\n",
      "22 Train Loss 20.689318 Test Loss 17.469986603590698\n",
      "23 Train Loss 20.463087 Test Loss 17.221086955904237\n",
      "24 Train Loss 20.283335 Test Loss 16.82967317977912\n",
      "25 Train Loss 20.105381 Test Loss 16.452739706362557\n",
      "26 Train Loss 19.808556 Test Loss 15.833562702395078\n",
      "27 Train Loss 19.637367 Test Loss 16.033755143838608\n",
      "28 Train Loss 19.54919 Test Loss 16.193295991008917\n",
      "29 Train Loss 19.507114 Test Loss 16.156896553952837\n",
      "30 Train Loss 19.418518 Test Loss 16.133163085411216\n",
      "31 Train Loss 19.171885 Test Loss 15.873893131373379\n",
      "32 Train Loss 19.118134 Test Loss 15.586802372000674\n",
      "33 Train Loss 19.057407 Test Loss 15.717163909055047\n",
      "34 Train Loss 18.967682 Test Loss 15.586496271427782\n",
      "35 Train Loss 18.914526 Test Loss 15.372591240728344\n",
      "36 Train Loss 18.84523 Test Loss 15.274183490923997\n",
      "37 Train Loss 18.731316 Test Loss 15.155382112288876\n",
      "38 Train Loss 18.658632 Test Loss 15.250215221296143\n",
      "39 Train Loss 18.553576 Test Loss 15.274879391713181\n",
      "40 Train Loss 18.409222 Test Loss 15.124164730879949\n",
      "41 Train Loss 18.323652 Test Loss 15.10904918526158\n",
      "42 Train Loss 18.258163 Test Loss 15.007518723670493\n",
      "43 Train Loss 18.119474 Test Loss 14.626884167648369\n",
      "44 Train Loss 18.01405 Test Loss 14.54534887613225\n",
      "45 Train Loss 17.907497 Test Loss 14.670617702597182\n",
      "46 Train Loss 17.868952 Test Loss 14.55653764897723\n",
      "47 Train Loss 17.827427 Test Loss 14.417061833506741\n",
      "48 Train Loss 17.782606 Test Loss 14.39158280703088\n",
      "49 Train Loss 17.687363 Test Loss 14.332107385703683\n",
      "50 Train Loss 17.627449 Test Loss 14.307023919589362\n",
      "51 Train Loss 17.52538 Test Loss 14.19326090070261\n",
      "52 Train Loss 17.498627 Test Loss 14.175069130941004\n",
      "53 Train Loss 17.475336 Test Loss 14.19378167962552\n",
      "54 Train Loss 17.45597 Test Loss 14.195407297615347\n",
      "55 Train Loss 17.428347 Test Loss 14.18397903952352\n",
      "56 Train Loss 17.277231 Test Loss 14.033466963676666\n",
      "57 Train Loss 17.215782 Test Loss 13.660791839553855\n",
      "58 Train Loss 17.048103 Test Loss 13.813582683698478\n",
      "59 Train Loss 16.916567 Test Loss 13.587123236635101\n",
      "60 Train Loss 16.832539 Test Loss 13.712766843532838\n",
      "61 Train Loss 16.743874 Test Loss 13.657013700904054\n",
      "62 Train Loss 16.694078 Test Loss 13.514522745870975\n",
      "63 Train Loss 16.63533 Test Loss 13.518253953554959\n",
      "64 Train Loss 16.566656 Test Loss 13.538556988354273\n",
      "65 Train Loss 16.500376 Test Loss 13.54336011561356\n",
      "66 Train Loss 16.439932 Test Loss 13.554389652714722\n",
      "67 Train Loss 16.384624 Test Loss 13.55774455170175\n",
      "68 Train Loss 16.343645 Test Loss 13.461477281090406\n",
      "69 Train Loss 16.320538 Test Loss 13.399091233535916\n",
      "70 Train Loss 16.299892 Test Loss 13.401093548191387\n",
      "71 Train Loss 16.277561 Test Loss 13.359923716164735\n",
      "72 Train Loss 16.236235 Test Loss 13.282287363275755\n",
      "73 Train Loss 16.170725 Test Loss 13.17388628726213\n",
      "74 Train Loss 16.028847 Test Loss 13.050884534266066\n",
      "75 Train Loss 15.942143 Test Loss 12.778473726237294\n",
      "76 Train Loss 15.832956 Test Loss 12.89529861246071\n",
      "77 Train Loss 15.773825 Test Loss 12.883982431984302\n",
      "78 Train Loss 15.735971 Test Loss 12.884415749039386\n",
      "79 Train Loss 15.705112 Test Loss 12.862361283646049\n",
      "80 Train Loss 15.678932 Test Loss 12.844620980594966\n",
      "81 Train Loss 15.637404 Test Loss 12.805772149953238\n",
      "82 Train Loss 15.607123 Test Loss 12.776900472295958\n",
      "83 Train Loss 15.556992 Test Loss 12.74856605968695\n",
      "84 Train Loss 15.52558 Test Loss 12.725857173121234\n",
      "85 Train Loss 15.467229 Test Loss 12.679072833572238\n",
      "86 Train Loss 15.422711 Test Loss 12.571394178938599\n",
      "87 Train Loss 15.355396 Test Loss 12.623989390396416\n",
      "88 Train Loss 15.342274 Test Loss 12.64905054260222\n",
      "89 Train Loss 15.323558 Test Loss 12.64599090893363\n",
      "90 Train Loss 15.310505 Test Loss 12.639867213841583\n",
      "91 Train Loss 15.288244 Test Loss 12.639208234054003\n",
      "92 Train Loss 15.243076 Test Loss 12.62420529992881\n",
      "93 Train Loss 15.214506 Test Loss 12.63787311128822\n",
      "94 Train Loss 15.181407 Test Loss 12.626390674982817\n",
      "95 Train Loss 15.125159 Test Loss 12.567679357279253\n",
      "96 Train Loss 15.078088 Test Loss 12.560333287394593\n",
      "97 Train Loss 15.044015 Test Loss 12.507462809397106\n",
      "98 Train Loss 15.016138 Test Loss 12.481110463278508\n",
      "99 Train Loss 14.995059 Test Loss 12.507757251260966\n",
      "100 Train Loss 14.98613 Test Loss 12.505284129650592\n",
      "101 Train Loss 14.965452 Test Loss 12.48186235283502\n",
      "102 Train Loss 14.929254 Test Loss 12.439119501330245\n",
      "103 Train Loss 14.879005 Test Loss 12.376529778309326\n",
      "104 Train Loss 14.809638 Test Loss 12.291011310239599\n",
      "105 Train Loss 14.7702465 Test Loss 12.281256500424442\n",
      "106 Train Loss 14.871854 Test Loss 12.336337803975514\n",
      "107 Train Loss 14.751405 Test Loss 12.29417922389457\n",
      "108 Train Loss 14.733654 Test Loss 12.25867221034207\n",
      "109 Train Loss 14.704295 Test Loss 12.269983578748128\n",
      "110 Train Loss 14.652992 Test Loss 12.238504699486288\n",
      "111 Train Loss 14.630786 Test Loss 12.219420783723324\n",
      "112 Train Loss 14.614028 Test Loss 12.19836569929184\n",
      "113 Train Loss 14.602557 Test Loss 12.173173513138543\n",
      "114 Train Loss 14.590649 Test Loss 12.165295059649486\n",
      "115 Train Loss 14.579702 Test Loss 12.133805014136975\n",
      "116 Train Loss 14.560134 Test Loss 12.121338011658823\n",
      "117 Train Loss 14.543069 Test Loss 12.11231467320556\n",
      "118 Train Loss 14.517225 Test Loss 12.073925242372622\n",
      "119 Train Loss 14.52029 Test Loss 12.046780754024885\n",
      "120 Train Loss 14.498274 Test Loss 12.060769757720927\n",
      "121 Train Loss 14.478612 Test Loss 12.086409427416477\n",
      "122 Train Loss 14.458325 Test Loss 12.03901847856005\n",
      "123 Train Loss 14.442067 Test Loss 12.018831983110754\n",
      "124 Train Loss 14.425176 Test Loss 12.00014467664513\n",
      "125 Train Loss 14.404591 Test Loss 12.009523449785046\n",
      "126 Train Loss 14.395546 Test Loss 12.02572379941515\n",
      "127 Train Loss 14.392565 Test Loss 12.02947545088433\n",
      "128 Train Loss 14.391892 Test Loss 12.029427367749797\n",
      "129 Train Loss 14.3897085 Test Loss 12.03121423476009\n",
      "130 Train Loss 14.380634 Test Loss 12.029404205748943\n",
      "131 Train Loss 14.365837 Test Loss 12.026422559784757\n",
      "132 Train Loss 14.331425 Test Loss 12.012520145651177\n",
      "133 Train Loss 14.357859 Test Loss 11.993922881107274\n",
      "134 Train Loss 14.313763 Test Loss 12.003561973568349\n",
      "135 Train Loss 14.28741 Test Loss 11.984333274082084\n",
      "136 Train Loss 14.260451 Test Loss 11.981393039960706\n",
      "137 Train Loss 14.255312 Test Loss 11.978086451395557\n",
      "138 Train Loss 14.249682 Test Loss 11.97780326399155\n",
      "139 Train Loss 14.242419 Test Loss 11.973908404837749\n",
      "140 Train Loss 14.227035 Test Loss 11.972731474351866\n",
      "141 Train Loss 14.200871 Test Loss 11.961243209455882\n",
      "142 Train Loss 14.17582 Test Loss 11.951341504706528\n",
      "143 Train Loss 14.188536 Test Loss 11.953888437447715\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "144 Train Loss 14.166407 Test Loss 11.95177192862225\n",
      "145 Train Loss 14.160181 Test Loss 11.942066643339947\n",
      "146 Train Loss 14.157715 Test Loss 11.939937421048239\n",
      "147 Train Loss 14.154195 Test Loss 11.947688109385757\n",
      "148 Train Loss 14.152421 Test Loss 11.940695876323046\n",
      "149 Train Loss 14.147976 Test Loss 11.934624725645365\n",
      "150 Train Loss 14.11914 Test Loss 11.886290064693076\n",
      "151 Train Loss 14.099616 Test Loss 11.865557872714176\n",
      "152 Train Loss 14.085177 Test Loss 11.859625348185059\n",
      "153 Train Loss 14.075358 Test Loss 11.850659159247641\n",
      "154 Train Loss 14.065081 Test Loss 11.85654936834872\n",
      "155 Train Loss 14.049499 Test Loss 11.865385544243297\n",
      "156 Train Loss 14.039034 Test Loss 11.855189706655738\n",
      "157 Train Loss 14.031663 Test Loss 11.841519318244588\n",
      "158 Train Loss 14.027585 Test Loss 11.833768084918466\n",
      "159 Train Loss 14.026576 Test Loss 11.836116594713282\n",
      "160 Train Loss 14.024035 Test Loss 11.835917629328662\n",
      "161 Train Loss 14.007925 Test Loss 11.823134683064058\n",
      "162 Train Loss 13.993872 Test Loss 11.807271984189853\n",
      "163 Train Loss 13.966535 Test Loss 11.777085008858817\n",
      "164 Train Loss 13.952776 Test Loss 11.74547888696893\n",
      "165 Train Loss 13.944317 Test Loss 11.740241086052508\n",
      "166 Train Loss 13.940985 Test Loss 11.758220192305707\n",
      "167 Train Loss 13.937339 Test Loss 11.749880086298472\n",
      "168 Train Loss 13.931841 Test Loss 11.747679475382288\n",
      "169 Train Loss 13.92184 Test Loss 11.74793006499049\n",
      "170 Train Loss 13.912886 Test Loss 11.750895494901163\n",
      "171 Train Loss 13.903505 Test Loss 11.758705219228213\n",
      "172 Train Loss 13.898712 Test Loss 11.727089052391056\n",
      "173 Train Loss 13.885536 Test Loss 11.741444596558667\n",
      "174 Train Loss 13.878238 Test Loss 11.731824146933757\n",
      "175 Train Loss 13.871834 Test Loss 11.721418318878365\n",
      "176 Train Loss 13.870551 Test Loss 11.714141208082529\n",
      "177 Train Loss 13.869301 Test Loss 11.71334626399447\n",
      "178 Train Loss 13.86802 Test Loss 11.71291369377713\n",
      "179 Train Loss 13.866463 Test Loss 11.708863890996165\n",
      "180 Train Loss 13.86501 Test Loss 11.703309628653235\n",
      "181 Train Loss 13.854888 Test Loss 11.69483039321168\n",
      "182 Train Loss 13.811281 Test Loss 11.653730425345739\n",
      "183 Train Loss 13.799542 Test Loss 11.661458847608426\n",
      "184 Train Loss 13.7871685 Test Loss 11.669377682234554\n",
      "185 Train Loss 13.786415 Test Loss 11.66280225136816\n",
      "186 Train Loss 13.783347 Test Loss 11.665768797779101\n",
      "187 Train Loss 13.78091 Test Loss 11.668523722335667\n",
      "188 Train Loss 13.779415 Test Loss 11.670300906055893\n",
      "189 Train Loss 13.778108 Test Loss 11.67467958560467\n",
      "190 Train Loss 13.776823 Test Loss 11.676327924954226\n",
      "191 Train Loss 13.775904 Test Loss 11.67921120906996\n",
      "192 Train Loss 13.774703 Test Loss 11.684141004884514\n",
      "193 Train Loss 13.769542 Test Loss 11.685920687849125\n",
      "194 Train Loss 13.745973 Test Loss 11.684463759296342\n",
      "195 Train Loss 13.750914 Test Loss 11.697513189590236\n",
      "196 Train Loss 13.729261 Test Loss 11.690272657724192\n",
      "197 Train Loss 13.714699 Test Loss 11.67892139676436\n",
      "198 Train Loss 13.702087 Test Loss 11.636779149341548\n",
      "199 Train Loss 13.7001295 Test Loss 11.621412452951894\n",
      "200 Train Loss 13.693246 Test Loss 11.628212340056814\n",
      "201 Train Loss 13.6914015 Test Loss 11.626080019695417\n",
      "202 Train Loss 13.689214 Test Loss 11.619444864932229\n",
      "203 Train Loss 13.687598 Test Loss 11.617543947658069\n",
      "204 Train Loss 13.686011 Test Loss 11.616216777624947\n",
      "205 Train Loss 13.684396 Test Loss 11.6185813386913\n",
      "206 Train Loss 13.680529 Test Loss 11.617414057798396\n",
      "207 Train Loss 13.672363 Test Loss 11.612200097331513\n",
      "208 Train Loss 13.664589 Test Loss 11.604991547958969\n",
      "209 Train Loss 13.64365 Test Loss 11.582522069413884\n",
      "210 Train Loss 13.622708 Test Loss 11.568266973673524\n",
      "211 Train Loss 13.613272 Test Loss 11.55785088858895\n",
      "212 Train Loss 13.603507 Test Loss 11.573854170406408\n",
      "213 Train Loss 13.600832 Test Loss 11.577441317598163\n",
      "214 Train Loss 13.599356 Test Loss 11.576255793961904\n",
      "215 Train Loss 13.5978775 Test Loss 11.572710956306107\n",
      "216 Train Loss 13.596303 Test Loss 11.569953624341105\n",
      "217 Train Loss 13.593043 Test Loss 11.565071984168558\n",
      "218 Train Loss 13.587277 Test Loss 11.556684118924977\n",
      "219 Train Loss 13.581996 Test Loss 11.549898228618387\n",
      "220 Train Loss 13.576403 Test Loss 11.539204310489982\n",
      "221 Train Loss 13.566939 Test Loss 11.538309783395237\n",
      "222 Train Loss 13.549492 Test Loss 11.540090648948102\n",
      "223 Train Loss 13.536649 Test Loss 11.524731943830725\n",
      "224 Train Loss 13.528447 Test Loss 11.51903423745291\n",
      "225 Train Loss 13.522134 Test Loss 11.502871531465633\n",
      "226 Train Loss 13.519549 Test Loss 11.497152358631116\n",
      "227 Train Loss 13.518482 Test Loss 11.49887916305208\n",
      "228 Train Loss 13.517768 Test Loss 11.49807372289084\n",
      "229 Train Loss 13.51718 Test Loss 11.497365092150462\n",
      "230 Train Loss 13.516626 Test Loss 11.497929896385862\n",
      "231 Train Loss 13.516238 Test Loss 11.499155682656232\n",
      "232 Train Loss 13.515113 Test Loss 11.499054276148218\n",
      "233 Train Loss 13.510675 Test Loss 11.49890400081225\n",
      "234 Train Loss 13.505241 Test Loss 11.495762764368417\n",
      "235 Train Loss 13.492386 Test Loss 11.47962775899719\n",
      "236 Train Loss 13.47582 Test Loss 11.462192120040436\n",
      "237 Train Loss 13.485454 Test Loss 11.424575027547956\n",
      "238 Train Loss 13.467058 Test Loss 11.445550890832116\n",
      "239 Train Loss 13.450622 Test Loss 11.439695170910127\n",
      "240 Train Loss 13.438949 Test Loss 11.413432777097261\n",
      "241 Train Loss 13.433334 Test Loss 11.39902287879017\n",
      "242 Train Loss 13.430183 Test Loss 11.382585188156707\n",
      "243 Train Loss 13.428738 Test Loss 11.364574206829683\n",
      "244 Train Loss 13.427002 Test Loss 11.378878633284653\n",
      "245 Train Loss 13.426121 Test Loss 11.380495266100636\n",
      "246 Train Loss 13.424719 Test Loss 11.365694850453162\n",
      "247 Train Loss 13.425108 Test Loss 11.369307525652175\n",
      "248 Train Loss 13.423274 Test Loss 11.367438032313844\n",
      "249 Train Loss 13.420788 Test Loss 11.361367062162826\n",
      "250 Train Loss 13.416505 Test Loss 11.347719248129408\n",
      "251 Train Loss 13.407911 Test Loss 11.322796319536335\n",
      "252 Train Loss 13.400576 Test Loss 11.274200075138609\n",
      "253 Train Loss 13.408847 Test Loss 11.19143709698974\n",
      "254 Train Loss 13.388981 Test Loss 11.232880497692669\n",
      "255 Train Loss 13.525638 Test Loss 11.13771730172007\n",
      "256 Train Loss 13.376124 Test Loss 11.20866837034383\n",
      "257 Train Loss 13.367751 Test Loss 11.186360357926178\n",
      "258 Train Loss 13.357318 Test Loss 11.170508428447157\n",
      "259 Train Loss 13.356752 Test Loss 11.157820393782371\n",
      "260 Train Loss 13.350612 Test Loss 11.163820463113911\n",
      "261 Train Loss 13.341148 Test Loss 11.19719259472379\n",
      "262 Train Loss 13.330576 Test Loss 11.18011167735734\n",
      "263 Train Loss 13.318969 Test Loss 11.184244277192363\n",
      "264 Train Loss 13.376811 Test Loss 11.164338731565369\n",
      "265 Train Loss 13.31357 Test Loss 11.17856640062315\n",
      "266 Train Loss 13.307167 Test Loss 11.172282727644713\n",
      "267 Train Loss 13.305003 Test Loss 11.16963328723954\n",
      "268 Train Loss 13.302582 Test Loss 11.163444332486025\n",
      "269 Train Loss 13.300116 Test Loss 11.163479239197175\n",
      "270 Train Loss 13.297118 Test Loss 11.158412179698448\n",
      "271 Train Loss 13.293469 Test Loss 11.153995936592832\n",
      "272 Train Loss 13.285592 Test Loss 11.142317392428028\n",
      "273 Train Loss 13.260258 Test Loss 11.115515326818906\n",
      "274 Train Loss 13.226472 Test Loss 11.085031533780041\n",
      "275 Train Loss 13.346761 Test Loss 11.108001082253113\n",
      "276 Train Loss 13.213969 Test Loss 11.089327374616044\n",
      "277 Train Loss 13.173357 Test Loss 11.079267055146111\n",
      "278 Train Loss 13.180324 Test Loss 11.036117052368489\n",
      "279 Train Loss 13.150211 Test Loss 11.057604709370672\n",
      "280 Train Loss 13.134123 Test Loss 11.091749547832874\n",
      "281 Train Loss 13.120075 Test Loss 11.070572144024021\n",
      "282 Train Loss 13.117802 Test Loss 11.072503013446788\n",
      "283 Train Loss 13.116077 Test Loss 11.075107220262973\n",
      "284 Train Loss 13.11371 Test Loss 11.069471813212179\n",
      "285 Train Loss 13.110461 Test Loss 11.07427657629807\n",
      "286 Train Loss 13.105501 Test Loss 11.066044028046628\n",
      "287 Train Loss 13.094307 Test Loss 11.048891219373706\n",
      "288 Train Loss 13.083195 Test Loss 11.035538987015782\n",
      "289 Train Loss 13.0605 Test Loss 11.016837788756117\n",
      "290 Train Loss 13.040186 Test Loss 10.99482256114868\n",
      "291 Train Loss 13.036971 Test Loss 10.977459782406951\n",
      "292 Train Loss 13.029798 Test Loss 10.984771975983001\n",
      "293 Train Loss 13.025618 Test Loss 10.98484835606314\n",
      "294 Train Loss 13.015868 Test Loss 10.9972586132773\n",
      "295 Train Loss 13.013325 Test Loss 10.987567818712757\n",
      "296 Train Loss 13.011311 Test Loss 10.991249610398768\n",
      "297 Train Loss 13.010282 Test Loss 10.991759754523411\n",
      "298 Train Loss 13.009647 Test Loss 10.990982943885136\n",
      "299 Train Loss 13.008771 Test Loss 10.992512136009413\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "300 Train Loss 13.007143 Test Loss 10.990715041647078\n",
      "301 Train Loss 13.003444 Test Loss 10.991118842226228\n",
      "302 Train Loss 12.9958 Test Loss 10.983802129013352\n",
      "303 Train Loss 12.986787 Test Loss 10.971216425781154\n",
      "304 Train Loss 12.972189 Test Loss 10.961345839191267\n",
      "305 Train Loss 12.956332 Test Loss 10.949217141034751\n",
      "306 Train Loss 12.9449625 Test Loss 10.946607582802457\n",
      "307 Train Loss 12.941453 Test Loss 10.94317231061645\n",
      "308 Train Loss 12.9384165 Test Loss 10.942321488167854\n",
      "309 Train Loss 12.9367485 Test Loss 10.9398525179609\n",
      "310 Train Loss 12.935683 Test Loss 10.937674896339537\n",
      "311 Train Loss 12.934379 Test Loss 10.937564043119778\n",
      "312 Train Loss 12.932182 Test Loss 10.934815068473204\n",
      "313 Train Loss 12.928512 Test Loss 10.929371061425515\n",
      "314 Train Loss 12.924051 Test Loss 10.909147049064297\n",
      "315 Train Loss 12.910519 Test Loss 10.909054831883786\n",
      "316 Train Loss 12.89359 Test Loss 10.893297686838345\n",
      "317 Train Loss 12.879066 Test Loss 10.8706377793554\n",
      "318 Train Loss 12.940767 Test Loss 10.905845746966712\n",
      "319 Train Loss 12.868102 Test Loss 10.880116189880473\n",
      "320 Train Loss 12.860964 Test Loss 10.893533061917035\n",
      "321 Train Loss 12.855144 Test Loss 10.901150149125726\n",
      "322 Train Loss 12.851077 Test Loss 10.895309001423806\n",
      "323 Train Loss 12.84911 Test Loss 10.884781816222835\n",
      "324 Train Loss 12.848606 Test Loss 10.880665047537569\n",
      "325 Train Loss 12.848392 Test Loss 10.880538886782386\n",
      "326 Train Loss 12.848252 Test Loss 10.882274515282461\n",
      "327 Train Loss 12.848085 Test Loss 10.882602952378257\n",
      "328 Train Loss 12.846992 Test Loss 10.882698380845575\n",
      "329 Train Loss 12.841893 Test Loss 10.884424786766287\n",
      "330 Train Loss 12.835313 Test Loss 10.879962437562597\n",
      "331 Train Loss 12.82019 Test Loss 10.865713264202686\n",
      "332 Train Loss 12.799246 Test Loss 10.84239707449484\n",
      "333 Train Loss 12.780357 Test Loss 10.814052919370386\n",
      "334 Train Loss 12.763865 Test Loss 10.789065778397992\n",
      "335 Train Loss 12.7460165 Test Loss 10.748613318330097\n",
      "336 Train Loss 12.747806 Test Loss 10.707763433435362\n",
      "337 Train Loss 12.738728 Test Loss 10.72767577288588\n",
      "338 Train Loss 12.763232 Test Loss 10.677330109379723\n",
      "339 Train Loss 12.732396 Test Loss 10.70978580641311\n",
      "340 Train Loss 12.72078 Test Loss 10.668986198590405\n",
      "341 Train Loss 12.746776 Test Loss 10.603032971486817\n",
      "342 Train Loss 12.706876 Test Loss 10.639939956236192\n",
      "343 Train Loss 12.747549 Test Loss 10.588893708627761\n",
      "344 Train Loss 12.696175 Test Loss 10.621197114919685\n",
      "345 Train Loss 12.684454 Test Loss 10.558300170167618\n",
      "346 Train Loss 12.664885 Test Loss 10.54815327369609\n",
      "347 Train Loss 12.730163 Test Loss 10.44626463672793\n",
      "348 Train Loss 12.655487 Test Loss 10.515795316223437\n",
      "349 Train Loss 12.650419 Test Loss 10.516738219041342\n",
      "350 Train Loss 12.641802 Test Loss 10.488862727103585\n",
      "351 Train Loss 12.767583 Test Loss 10.467250960889869\n",
      "352 Train Loss 12.635847 Test Loss 10.483938900202768\n",
      "353 Train Loss 12.629601 Test Loss 10.4923576728312\n",
      "354 Train Loss 12.625274 Test Loss 10.488619214031841\n",
      "355 Train Loss 12.620992 Test Loss 10.480092211338087\n",
      "356 Train Loss 12.617396 Test Loss 10.47942004159093\n",
      "357 Train Loss 12.6152 Test Loss 10.461243996718867\n",
      "358 Train Loss 12.62024 Test Loss 10.458578129610018\n",
      "359 Train Loss 12.612642 Test Loss 10.460220855683978\n",
      "360 Train Loss 12.609788 Test Loss 10.454179272815352\n",
      "361 Train Loss 12.606561 Test Loss 10.446210139079808\n",
      "362 Train Loss 12.604344 Test Loss 10.426799392927618\n",
      "363 Train Loss 12.603054 Test Loss 10.402794738440996\n",
      "364 Train Loss 12.600136 Test Loss 10.39812095148753\n",
      "365 Train Loss 12.598091 Test Loss 10.386548795583234\n",
      "366 Train Loss 12.595179 Test Loss 10.352856493483397\n",
      "367 Train Loss 12.592609 Test Loss 10.343511217574829\n",
      "368 Train Loss 12.589777 Test Loss 10.314588301932325\n",
      "369 Train Loss 12.589048 Test Loss 10.325569541226788\n",
      "370 Train Loss 12.584017 Test Loss 10.319052716082513\n",
      "371 Train Loss 12.581814 Test Loss 10.315884589818188\n",
      "372 Train Loss 12.579319 Test Loss 10.308404450751306\n",
      "373 Train Loss 12.578357 Test Loss 10.330090446945123\n",
      "374 Train Loss 12.575163 Test Loss 10.313572002801298\n",
      "375 Train Loss 12.573297 Test Loss 10.314287557815364\n",
      "376 Train Loss 12.57079 Test Loss 10.31629710055498\n",
      "377 Train Loss 12.565182 Test Loss 10.315336429308358\n",
      "378 Train Loss 12.562194 Test Loss 10.309838147567314\n",
      "379 Train Loss 12.555733 Test Loss 10.300508057065386\n",
      "380 Train Loss 12.548217 Test Loss 10.27809964459757\n",
      "381 Train Loss 12.538008 Test Loss 10.26898616276079\n",
      "382 Train Loss 12.526933 Test Loss 10.26802070398083\n",
      "383 Train Loss 12.532043 Test Loss 10.280932924338845\n",
      "384 Train Loss 12.520752 Test Loss 10.273923179932462\n",
      "385 Train Loss 12.507576 Test Loss 10.28079510379145\n",
      "386 Train Loss 12.490401 Test Loss 10.287286808559122\n",
      "387 Train Loss 12.477882 Test Loss 10.291382976172326\n",
      "388 Train Loss 12.462582 Test Loss 10.291380565878434\n",
      "389 Train Loss 12.439384 Test Loss 10.270532055047665\n",
      "390 Train Loss 12.42036 Test Loss 10.256569332970523\n",
      "391 Train Loss 12.423565 Test Loss 10.190696587372933\n",
      "392 Train Loss 12.4096575 Test Loss 10.22444198970271\n",
      "393 Train Loss 12.396901 Test Loss 10.2097919029727\n",
      "394 Train Loss 12.475111 Test Loss 10.182587587409508\n",
      "395 Train Loss 12.391429 Test Loss 10.202433317083877\n",
      "396 Train Loss 12.380034 Test Loss 10.199741253762813\n",
      "397 Train Loss 12.371019 Test Loss 10.197360096417947\n",
      "398 Train Loss 12.364029 Test Loss 10.188650292012007\n",
      "399 Train Loss 12.355831 Test Loss 10.183275877588787\n",
      "400 Train Loss 12.350309 Test Loss 10.1660887470142\n",
      "401 Train Loss 12.362273 Test Loss 10.153871844703495\n",
      "402 Train Loss 12.349041 Test Loss 10.162955824920404\n",
      "403 Train Loss 12.347346 Test Loss 10.1639574191451\n",
      "404 Train Loss 12.345329 Test Loss 10.165363653421467\n",
      "405 Train Loss 12.341696 Test Loss 10.162381238748646\n",
      "406 Train Loss 12.3375025 Test Loss 10.158066215997486\n",
      "407 Train Loss 12.331502 Test Loss 10.153098241346106\n",
      "408 Train Loss 12.327955 Test Loss 10.152646716886398\n",
      "409 Train Loss 12.324174 Test Loss 10.14975048062222\n",
      "410 Train Loss 12.318839 Test Loss 10.148610403657587\n",
      "411 Train Loss 12.31357 Test Loss 10.14384839864319\n",
      "412 Train Loss 12.305553 Test Loss 10.144521964930032\n",
      "413 Train Loss 12.299167 Test Loss 10.126065986374453\n",
      "414 Train Loss 12.292824 Test Loss 10.132930444805638\n",
      "415 Train Loss 12.283505 Test Loss 10.127636899985141\n",
      "416 Train Loss 12.264315 Test Loss 10.129879847323915\n",
      "417 Train Loss 12.257402 Test Loss 10.130514060649292\n",
      "418 Train Loss 12.252159 Test Loss 10.120800958641109\n",
      "419 Train Loss 12.249032 Test Loss 10.122301945612591\n",
      "420 Train Loss 12.245785 Test Loss 10.120080518993865\n",
      "421 Train Loss 12.242107 Test Loss 10.120321403541874\n",
      "422 Train Loss 12.239215 Test Loss 10.121531542587666\n",
      "423 Train Loss 12.236028 Test Loss 10.120329944445986\n",
      "424 Train Loss 12.232138 Test Loss 10.119679700425237\n",
      "425 Train Loss 12.228706 Test Loss 10.11942657816742\n",
      "426 Train Loss 12.22371 Test Loss 10.116079398624532\n",
      "427 Train Loss 12.220643 Test Loss 10.1148959198354\n",
      "428 Train Loss 12.215117 Test Loss 10.105389773632032\n",
      "429 Train Loss 12.210802 Test Loss 10.099151634034985\n",
      "430 Train Loss 12.20489 Test Loss 10.088459283377414\n",
      "431 Train Loss 12.200326 Test Loss 10.080299038558184\n",
      "432 Train Loss 12.197407 Test Loss 10.081598021111322\n",
      "433 Train Loss 12.194728 Test Loss 10.087914674912774\n",
      "434 Train Loss 12.192407 Test Loss 10.076049781404956\n",
      "435 Train Loss 12.190226 Test Loss 10.07414653056264\n",
      "436 Train Loss 12.185582 Test Loss 10.068446149979343\n",
      "437 Train Loss 12.181691 Test Loss 10.065073057281664\n",
      "438 Train Loss 12.177391 Test Loss 10.064287293694282\n",
      "439 Train Loss 12.175768 Test Loss 10.064402163457473\n",
      "440 Train Loss 12.172734 Test Loss 10.065588094494911\n",
      "441 Train Loss 12.170659 Test Loss 10.071240036977434\n",
      "442 Train Loss 12.168513 Test Loss 10.077968148368754\n",
      "443 Train Loss 12.167864 Test Loss 10.073868286325398\n",
      "444 Train Loss 12.165251 Test Loss 10.077467931296333\n",
      "445 Train Loss 12.163663 Test Loss 10.082147427201143\n",
      "446 Train Loss 12.162454 Test Loss 10.089012515550996\n",
      "447 Train Loss 12.161055 Test Loss 10.08764449701355\n",
      "448 Train Loss 12.159549 Test Loss 10.094053789345551\n",
      "449 Train Loss 12.157816 Test Loss 10.09376312584955\n",
      "450 Train Loss 12.156579 Test Loss 10.09566255009149\n",
      "451 Train Loss 12.1558485 Test Loss 10.096278970129433\n",
      "452 Train Loss 12.156565 Test Loss 10.099158220883771\n",
      "453 Train Loss 12.155441 Test Loss 10.09735118345782\n",
      "454 Train Loss 12.15447 Test Loss 10.098051923780138\n",
      "455 Train Loss 12.152813 Test Loss 10.100572306793298\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "456 Train Loss 12.152025 Test Loss 10.101718070280436\n",
      "457 Train Loss 12.150999 Test Loss 10.10218754256564\n",
      "458 Train Loss 12.2372465 Test Loss 10.120305048844498\n",
      "459 Train Loss 12.15073 Test Loss 10.103353786692429\n",
      "460 Train Loss 12.149659 Test Loss 10.101738039019576\n",
      "461 Train Loss 12.148824 Test Loss 10.097884372385975\n",
      "462 Train Loss 12.1495075 Test Loss 10.093003099139604\n",
      "463 Train Loss 12.147931 Test Loss 10.095740192826094\n",
      "464 Train Loss 12.146547 Test Loss 10.097147246541882\n",
      "465 Train Loss 12.145314 Test Loss 10.09943483711777\n",
      "466 Train Loss 12.143637 Test Loss 10.09706440378031\n",
      "467 Train Loss 12.1422825 Test Loss 10.096106020486033\n",
      "468 Train Loss 12.139427 Test Loss 10.090617357922142\n",
      "469 Train Loss 12.140342 Test Loss 10.086710895400083\n",
      "470 Train Loss 12.137196 Test Loss 10.088721693993913\n",
      "471 Train Loss 12.133603 Test Loss 10.084776514953383\n",
      "472 Train Loss 12.129874 Test Loss 10.064990701600134\n",
      "473 Train Loss 12.125317 Test Loss 10.072359965608369\n",
      "474 Train Loss 12.147983 Test Loss 10.052469016982966\n",
      "475 Train Loss 12.118267 Test Loss 10.06403567207725\n",
      "476 Train Loss 12.111182 Test Loss 10.051179653023794\n",
      "477 Train Loss 12.097156 Test Loss 10.036168275954186\n",
      "478 Train Loss 12.100836 Test Loss 10.00926480066637\n",
      "479 Train Loss 12.08307 Test Loss 10.021730297551192\n",
      "480 Train Loss 12.067648 Test Loss 10.007334030115441\n",
      "481 Train Loss 12.056809 Test Loss 10.016547661254311\n",
      "482 Train Loss 12.044903 Test Loss 9.999420613237941\n",
      "483 Train Loss 12.034704 Test Loss 9.988158126078122\n",
      "484 Train Loss 12.027605 Test Loss 9.990127480681519\n",
      "485 Train Loss 12.022879 Test Loss 9.974982980592712\n",
      "486 Train Loss 12.019802 Test Loss 9.97576367257531\n",
      "487 Train Loss 12.018124 Test Loss 9.978167213954787\n",
      "488 Train Loss 12.014444 Test Loss 9.97819836899385\n",
      "489 Train Loss 12.012823 Test Loss 9.973831454918017\n",
      "490 Train Loss 12.010687 Test Loss 9.976760649953611\n",
      "491 Train Loss 12.013874 Test Loss 9.977512935483631\n",
      "492 Train Loss 12.009622 Test Loss 9.976939878860934\n",
      "493 Train Loss 12.025121 Test Loss 9.987238094787848\n",
      "494 Train Loss 12.008296 Test Loss 9.979102193174501\n",
      "495 Train Loss 12.006577 Test Loss 9.976664791967766\n",
      "496 Train Loss 12.004821 Test Loss 9.96525597171468\n",
      "497 Train Loss 12.004592 Test Loss 9.967348671864151\n",
      "498 Train Loss 12.003461 Test Loss 9.967921037676568\n",
      "499 Train Loss 12.002467 Test Loss 9.967337413116637\n",
      "500 Train Loss 12.001015 Test Loss 9.967489039062956\n",
      "501 Train Loss 11.999234 Test Loss 9.967249121841842\n",
      "502 Train Loss 11.998581 Test Loss 9.965925914638762\n",
      "503 Train Loss 11.997887 Test Loss 9.965519257899143\n",
      "504 Train Loss 11.99732 Test Loss 9.963833053317348\n",
      "505 Train Loss 11.996761 Test Loss 9.961838698960129\n",
      "506 Train Loss 12.492696 Test Loss 9.948508834566605\n",
      "507 Train Loss 11.996694 Test Loss 9.9615100888497\n",
      "508 Train Loss 11.996226 Test Loss 9.96014247957385\n",
      "509 Train Loss 11.995655 Test Loss 9.95867584300975\n",
      "510 Train Loss 11.995053 Test Loss 9.95776911208207\n",
      "511 Train Loss 11.994675 Test Loss 9.9572253552484\n",
      "512 Train Loss 11.994165 Test Loss 9.957548779144602\n",
      "513 Train Loss 11.993627 Test Loss 9.95720053660733\n",
      "514 Train Loss 11.99747 Test Loss 9.965463300257586\n",
      "515 Train Loss 11.993448 Test Loss 9.95867095664958\n",
      "516 Train Loss 11.992435 Test Loss 9.958044399312191\n",
      "517 Train Loss 11.991245 Test Loss 9.954789679391332\n",
      "518 Train Loss 11.989878 Test Loss 9.951800868769656\n",
      "519 Train Loss 11.98859 Test Loss 9.947579916587845\n",
      "520 Train Loss 11.987259 Test Loss 9.946781588808776\n",
      "521 Train Loss 11.986098 Test Loss 9.94665409720321\n",
      "522 Train Loss 11.985217 Test Loss 9.947941067127019\n",
      "523 Train Loss 11.984371 Test Loss 9.948534946200486\n",
      "524 Train Loss 11.982995 Test Loss 9.949006537877407\n",
      "525 Train Loss 11.980833 Test Loss 9.948323633298429\n",
      "526 Train Loss 11.976641 Test Loss 9.944215867892108\n",
      "527 Train Loss 11.97242 Test Loss 9.938603738381085\n",
      "528 Train Loss 11.974987 Test Loss 9.941032893907625\n",
      "529 Train Loss 11.970595 Test Loss 9.939575806732954\n",
      "530 Train Loss 11.967934 Test Loss 9.936023428066802\n",
      "531 Train Loss 11.965781 Test Loss 9.934639501817784\n",
      "532 Train Loss 11.963359 Test Loss 9.931255654909537\n",
      "533 Train Loss 11.960333 Test Loss 9.928728402299065\n",
      "534 Train Loss 11.957417 Test Loss 9.923065178790806\n",
      "535 Train Loss 11.958148 Test Loss 9.915148406542391\n",
      "536 Train Loss 11.956351 Test Loss 9.919559803833698\n",
      "537 Train Loss 11.955545 Test Loss 9.922272681570192\n",
      "538 Train Loss 11.954221 Test Loss 9.925870417598544\n",
      "539 Train Loss 11.9535055 Test Loss 9.924608504001151\n",
      "540 Train Loss 11.957045 Test Loss 9.913262448971642\n",
      "541 Train Loss 11.952689 Test Loss 9.921130659857253\n",
      "542 Train Loss 11.951614 Test Loss 9.920205512812574\n",
      "543 Train Loss 14.886206 Test Loss 9.869541323759227\n",
      "544 Train Loss 11.95174 Test Loss 9.918460672483754\n",
      "545 Train Loss 11.951509 Test Loss 9.919493343906096\n",
      "546 Train Loss 11.949507 Test Loss 9.916387781894974\n",
      "547 Train Loss 11.948301 Test Loss 9.914353435058946\n",
      "548 Train Loss 11.946844 Test Loss 9.912214347453936\n",
      "549 Train Loss 11.944846 Test Loss 9.912744704776182\n",
      "550 Train Loss 11.941855 Test Loss 9.911850564238783\n",
      "551 Train Loss 11.938202 Test Loss 9.919241195780664\n",
      "552 Train Loss 11.935818 Test Loss 9.918161967098264\n",
      "553 Train Loss 11.93749 Test Loss 9.91450749873163\n",
      "554 Train Loss 11.93317 Test Loss 9.91638389976405\n",
      "555 Train Loss 11.930969 Test Loss 9.913840735174864\n",
      "556 Train Loss 11.932651 Test Loss 9.898479856856817\n",
      "557 Train Loss 11.929691 Test Loss 9.907681820659285\n",
      "558 Train Loss 11.933283 Test Loss 9.901578250670413\n",
      "559 Train Loss 11.928242 Test Loss 9.905491296014679\n",
      "560 Train Loss 11.92357 Test Loss 9.904504495529526\n",
      "561 Train Loss 11.913673 Test Loss 9.893642948940077\n",
      "562 Train Loss 11.917309 Test Loss 9.895959259792592\n",
      "563 Train Loss 11.905802 Test Loss 9.894623947157\n",
      "564 Train Loss 11.890667 Test Loss 9.906402750088096\n",
      "565 Train Loss 11.875379 Test Loss 9.910441714872025\n",
      "566 Train Loss 11.872144 Test Loss 9.925463003624564\n",
      "567 Train Loss 11.867517 Test Loss 9.919062827110936\n",
      "568 Train Loss 11.907353 Test Loss 9.930154828987675\n",
      "569 Train Loss 11.86269 Test Loss 9.922462483084308\n",
      "570 Train Loss 11.860514 Test Loss 9.926783602598835\n",
      "571 Train Loss 11.85905 Test Loss 9.927477967745524\n",
      "572 Train Loss 11.858363 Test Loss 9.926874096902246\n",
      "573 Train Loss 11.858017 Test Loss 9.925758279631168\n",
      "574 Train Loss 11.857832 Test Loss 9.926406355442044\n",
      "575 Train Loss 11.857743 Test Loss 9.927197084447586\n",
      "576 Train Loss 11.857654 Test Loss 9.92772659034915\n",
      "577 Train Loss 11.8575325 Test Loss 9.927847258821123\n",
      "578 Train Loss 11.857418 Test Loss 9.92771870023801\n",
      "579 Train Loss 11.857045 Test Loss 9.927147461039318\n",
      "580 Train Loss 11.85665 Test Loss 9.926581683825583\n",
      "581 Train Loss 11.85581 Test Loss 9.926802769150878\n",
      "582 Train Loss 11.854812 Test Loss 9.925928490586726\n",
      "583 Train Loss 11.853262 Test Loss 9.927665892872549\n",
      "584 Train Loss 11.852699 Test Loss 9.927611787860755\n",
      "585 Train Loss 11.850527 Test Loss 9.928891478671316\n",
      "586 Train Loss 11.847633 Test Loss 9.931391606414264\n",
      "587 Train Loss 11.84024 Test Loss 9.934995217650805\n",
      "588 Train Loss 11.847046 Test Loss 9.941812233767894\n",
      "589 Train Loss 11.836723 Test Loss 9.937622525797286\n",
      "590 Train Loss 11.830876 Test Loss 9.940840699308046\n",
      "591 Train Loss 11.957166 Test Loss 9.942524409555487\n",
      "592 Train Loss 11.825791 Test Loss 9.941395998501287\n",
      "593 Train Loss 11.818483 Test Loss 9.935692275484328\n",
      "594 Train Loss 11.807211 Test Loss 9.921901509519095\n",
      "595 Train Loss 11.805062 Test Loss 9.915358262678028\n",
      "596 Train Loss 11.800731 Test Loss 9.913501163975184\n",
      "597 Train Loss 11.798252 Test Loss 9.914341270167984\n",
      "598 Train Loss 11.792471 Test Loss 9.915921905092178\n",
      "599 Train Loss 11.789835 Test Loss 9.914682821711112\n",
      "600 Train Loss 11.786847 Test Loss 9.91041267208381\n",
      "601 Train Loss 11.782655 Test Loss 9.905665974413068\n",
      "602 Train Loss 11.811503 Test Loss 9.90358771599705\n",
      "603 Train Loss 11.782225 Test Loss 9.905306702178247\n",
      "604 Train Loss 11.780434 Test Loss 9.902606130972488\n",
      "605 Train Loss 11.778968 Test Loss 9.897904544722358\n",
      "606 Train Loss 11.776996 Test Loss 9.894116380786672\n",
      "607 Train Loss 11.775287 Test Loss 9.893524801848637\n",
      "608 Train Loss 11.771799 Test Loss 9.896933834988404\n",
      "609 Train Loss 11.770038 Test Loss 9.891061834633001\n",
      "610 Train Loss 11.7675085 Test Loss 9.891791304607922\n",
      "611 Train Loss 11.765174 Test Loss 9.89390880088017\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "612 Train Loss 11.766082 Test Loss 9.89416352186999\n",
      "613 Train Loss 11.76352 Test Loss 9.893987273914512\n",
      "614 Train Loss 11.766336 Test Loss 9.89368618050123\n",
      "615 Train Loss 11.762441 Test Loss 9.893795873711673\n",
      "616 Train Loss 11.7606325 Test Loss 9.889288666756709\n",
      "617 Train Loss 11.839729 Test Loss 9.897954825958097\n",
      "618 Train Loss 11.7604685 Test Loss 9.88969418221244\n",
      "619 Train Loss 11.758952 Test Loss 9.887653588206744\n",
      "620 Train Loss 11.758087 Test Loss 9.886110782682117\n",
      "621 Train Loss 11.757768 Test Loss 9.887651606316586\n",
      "622 Train Loss 11.757512 Test Loss 9.88813316805407\n",
      "623 Train Loss 11.757344 Test Loss 9.888689061878827\n",
      "624 Train Loss 11.757375 Test Loss 9.888603715325553\n",
      "625 Train Loss 11.757245 Test Loss 9.888642889253543\n",
      "626 Train Loss 11.757005 Test Loss 9.88876167413983\n",
      "627 Train Loss 11.75676 Test Loss 9.88807358588088\n",
      "628 Train Loss 11.756199 Test Loss 9.886356114177273\n",
      "629 Train Loss 11.755666 Test Loss 9.884824414863761\n",
      "630 Train Loss 11.754956 Test Loss 9.883777925343344\n",
      "631 Train Loss 11.754337 Test Loss 9.88206756834359\n",
      "632 Train Loss 11.754108 Test Loss 9.882057626056703\n",
      "633 Train Loss 11.752975 Test Loss 9.881644039536543\n",
      "634 Train Loss 11.752281 Test Loss 9.88047269525633\n",
      "635 Train Loss 11.751086 Test Loss 9.876461009439822\n",
      "636 Train Loss 11.749696 Test Loss 9.87258450724057\n",
      "637 Train Loss 11.749278 Test Loss 9.865707376377484\n",
      "638 Train Loss 11.74856 Test Loss 9.86835546500233\n",
      "639 Train Loss 11.7472725 Test Loss 9.861906228648367\n",
      "640 Train Loss 11.745893 Test Loss 9.861196265391222\n",
      "641 Train Loss 11.744491 Test Loss 9.865085244255276\n",
      "642 Train Loss 11.743013 Test Loss 9.864459881885773\n",
      "643 Train Loss 11.7390995 Test Loss 9.859632631181409\n",
      "644 Train Loss 11.739252 Test Loss 9.857748083188289\n",
      "645 Train Loss 11.738317 Test Loss 9.858570297369804\n",
      "646 Train Loss 11.736738 Test Loss 9.854902644894633\n",
      "647 Train Loss 11.736733 Test Loss 9.851615285126881\n",
      "648 Train Loss 11.736095 Test Loss 9.85317530771756\n",
      "649 Train Loss 11.734836 Test Loss 9.85187461671421\n",
      "650 Train Loss 11.74036 Test Loss 9.857014797986325\n",
      "651 Train Loss 11.734591 Test Loss 9.852746258449468\n",
      "652 Train Loss 11.73393 Test Loss 9.851994649044784\n",
      "653 Train Loss 11.733457 Test Loss 9.85175923948081\n",
      "654 Train Loss 11.7331705 Test Loss 9.851105468422512\n",
      "655 Train Loss 11.73286 Test Loss 9.85054900707272\n",
      "656 Train Loss 11.73297 Test Loss 9.852677814948544\n",
      "657 Train Loss 11.732692 Test Loss 9.851473112768637\n",
      "658 Train Loss 11.7323675 Test Loss 9.85044437517459\n",
      "659 Train Loss 11.73198 Test Loss 9.850031267304674\n",
      "660 Train Loss 11.731232 Test Loss 9.847078580624276\n",
      "661 Train Loss 11.73036 Test Loss 9.844902689757367\n",
      "662 Train Loss 11.745522 Test Loss 9.845505846733412\n",
      "663 Train Loss 11.730265 Test Loss 9.844942413755353\n",
      "664 Train Loss 11.727915 Test Loss 9.841027981569727\n",
      "665 Train Loss 11.723531 Test Loss 9.834356649151681\n",
      "666 Train Loss 11.719372 Test Loss 9.8322118207737\n",
      "667 Train Loss 11.712601 Test Loss 9.827833675049884\n",
      "668 Train Loss 11.714106 Test Loss 9.829888186371967\n",
      "669 Train Loss 11.706299 Test Loss 9.828777457151451\n",
      "670 Train Loss 11.696886 Test Loss 9.8295204210233\n",
      "671 Train Loss 11.690466 Test Loss 9.831051251098692\n",
      "672 Train Loss 11.6862545 Test Loss 9.827596603481949\n",
      "673 Train Loss 11.683526 Test Loss 9.81918738196467\n",
      "674 Train Loss 11.677346 Test Loss 9.817394288673503\n",
      "675 Train Loss 11.673699 Test Loss 9.819728446066302\n",
      "676 Train Loss 11.667292 Test Loss 9.815923645564572\n",
      "677 Train Loss 11.662881 Test Loss 9.813899172836216\n",
      "678 Train Loss 11.660038 Test Loss 9.812669144991343\n",
      "679 Train Loss 11.657434 Test Loss 9.811871910927355\n",
      "680 Train Loss 11.655991 Test Loss 9.811705000890806\n",
      "681 Train Loss 11.654151 Test Loss 9.81171530791596\n",
      "682 Train Loss 11.653127 Test Loss 9.8116964483778\n",
      "683 Train Loss 11.651223 Test Loss 9.811710750515068\n",
      "684 Train Loss 11.649512 Test Loss 9.809120534741183\n",
      "685 Train Loss 11.7311125 Test Loss 9.778043532003894\n",
      "686 Train Loss 11.649177 Test Loss 9.80715071363887\n",
      "687 Train Loss 11.64708 Test Loss 9.806321869066045\n",
      "688 Train Loss 11.645727 Test Loss 9.8061590060838\n",
      "689 Train Loss 11.644724 Test Loss 9.80639010090251\n",
      "690 Train Loss 12.240268 Test Loss 9.835244748277379\n",
      "691 Train Loss 11.644476 Test Loss 9.807152604735505\n",
      "692 Train Loss 11.643612 Test Loss 9.809369495017014\n",
      "693 Train Loss 11.64338 Test Loss 9.810924587656077\n",
      "694 Train Loss 11.642609 Test Loss 9.80984672299095\n",
      "695 Train Loss 11.642075 Test Loss 9.80921035961445\n",
      "696 Train Loss 11.640986 Test Loss 9.80781468507289\n",
      "697 Train Loss 11.64032 Test Loss 9.80682624095392\n",
      "698 Train Loss 11.639767 Test Loss 9.805492572334805\n",
      "699 Train Loss 11.672238 Test Loss 9.80970626223807\n",
      "700 Train Loss 11.639748 Test Loss 9.805612732811133\n",
      "701 Train Loss 11.639361 Test Loss 9.804424608185043\n",
      "702 Train Loss 11.639024 Test Loss 9.803690357934032\n",
      "703 Train Loss 11.638743 Test Loss 9.804023800293164\n",
      "704 Train Loss 11.638487 Test Loss 9.804887511252824\n",
      "705 Train Loss 11.638185 Test Loss 9.805632321062483\n",
      "706 Train Loss 11.638707 Test Loss 9.807411874828741\n",
      "707 Train Loss 11.638072 Test Loss 9.80613615177028\n",
      "708 Train Loss 11.637808 Test Loss 9.80562099761161\n",
      "709 Train Loss 11.637528 Test Loss 9.804938116244683\n",
      "710 Train Loss 11.637455 Test Loss 9.804274017895203\n",
      "711 Train Loss 11.637125 Test Loss 9.803851881036978\n",
      "712 Train Loss 11.636929 Test Loss 9.803615230784464\n",
      "713 Train Loss 11.636642 Test Loss 9.803428705620679\n",
      "714 Train Loss 11.635735 Test Loss 9.802744078956529\n",
      "715 Train Loss 11.635096 Test Loss 9.803101012334738\n",
      "716 Train Loss 11.635555 Test Loss 9.80717514045992\n",
      "717 Train Loss 11.634798 Test Loss 9.804646820573618\n",
      "718 Train Loss 11.636163 Test Loss 9.806951189133203\n",
      "719 Train Loss 11.63441 Test Loss 9.805363261250317\n",
      "720 Train Loss 11.633816 Test Loss 9.806059456928736\n",
      "721 Train Loss 11.632821 Test Loss 9.808585680326606\n",
      "722 Train Loss 11.637968 Test Loss 9.811077863917207\n",
      "723 Train Loss 11.632415 Test Loss 9.809268909233184\n",
      "724 Train Loss 11.631736 Test Loss 9.809828550781026\n",
      "725 Train Loss 11.630881 Test Loss 9.811211230513686\n",
      "726 Train Loss 11.630785 Test Loss 9.810047963277846\n",
      "727 Train Loss 11.630186 Test Loss 9.811243187784852\n",
      "728 Train Loss 11.629832 Test Loss 9.812025531281984\n",
      "729 Train Loss 11.629217 Test Loss 9.813530728257359\n",
      "730 Train Loss 11.628486 Test Loss 9.812460235418442\n",
      "731 Train Loss 11.627643 Test Loss 9.811748352844296\n",
      "732 Train Loss 11.62681 Test Loss 9.811253290327254\n",
      "733 Train Loss 11.637427 Test Loss 9.81040055433477\n",
      "734 Train Loss 11.626737 Test Loss 9.8111404216768\n",
      "735 Train Loss 11.626068 Test Loss 9.81100077319653\n",
      "736 Train Loss 11.62609 Test Loss 9.810012819725273\n",
      "737 Train Loss 11.625617 Test Loss 9.810514766310739\n",
      "738 Train Loss 11.624575 Test Loss 9.809485755492162\n",
      "739 Train Loss 11.623307 Test Loss 9.809205745571957\n",
      "740 Train Loss 11.622856 Test Loss 9.806794084292848\n",
      "741 Train Loss 11.622309 Test Loss 9.807690088667261\n",
      "742 Train Loss 11.621113 Test Loss 9.808190732508951\n",
      "743 Train Loss 11.622695 Test Loss 9.80727603100029\n",
      "744 Train Loss 11.620775 Test Loss 9.807902763646062\n",
      "745 Train Loss 11.620083 Test Loss 9.808517064648624\n",
      "746 Train Loss 11.619375 Test Loss 9.809814895727548\n",
      "747 Train Loss 11.618792 Test Loss 9.812188820166446\n",
      "748 Train Loss 11.618172 Test Loss 9.813299945324912\n",
      "749 Train Loss 11.617646 Test Loss 9.81438681315172\n",
      "750 Train Loss 11.617257 Test Loss 9.816168236920964\n",
      "751 Train Loss 11.61703 Test Loss 9.81622283162144\n",
      "752 Train Loss 11.618322 Test Loss 9.811583224298161\n",
      "753 Train Loss 11.61696 Test Loss 9.815233372410919\n",
      "754 Train Loss 11.616626 Test Loss 9.8148239222624\n",
      "755 Train Loss 11.616237 Test Loss 9.81375886063981\n",
      "756 Train Loss 11.615704 Test Loss 9.812211138955615\n",
      "757 Train Loss 11.6152115 Test Loss 9.812592144255744\n",
      "758 Train Loss 11.613934 Test Loss 9.812723426903098\n",
      "759 Train Loss 11.612871 Test Loss 9.81296480276881\n",
      "760 Train Loss 11.611662 Test Loss 9.81290636934035\n",
      "761 Train Loss 11.6108055 Test Loss 9.814241977537916\n",
      "762 Train Loss 11.609513 Test Loss 9.81664622772801\n",
      "763 Train Loss 11.60813 Test Loss 9.816209086400681\n",
      "764 Train Loss 11.60684 Test Loss 9.816234086169404\n",
      "765 Train Loss 11.605236 Test Loss 9.817521895419928\n",
      "766 Train Loss 11.604037 Test Loss 9.81745018455218\n",
      "767 Train Loss 11.602549 Test Loss 9.816732887583752\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "768 Train Loss 11.600964 Test Loss 9.813255510872049\n",
      "769 Train Loss 11.600067 Test Loss 9.809460736687447\n",
      "770 Train Loss 11.598964 Test Loss 9.804712272086007\n",
      "771 Train Loss 11.598324 Test Loss 9.803516635742817\n",
      "772 Train Loss 11.597854 Test Loss 9.804639726688436\n",
      "773 Train Loss 11.597139 Test Loss 9.80559517330072\n",
      "774 Train Loss 11.598902 Test Loss 9.813634840154657\n",
      "775 Train Loss 11.596699 Test Loss 9.80803652642067\n",
      "776 Train Loss 11.602973 Test Loss 9.830198203663882\n",
      "777 Train Loss 11.595713 Test Loss 9.81400712327154\n",
      "778 Train Loss 11.593597 Test Loss 9.812074142510905\n",
      "779 Train Loss 11.589872 Test Loss 9.812838477959845\n",
      "780 Train Loss 11.586966 Test Loss 9.809382278766556\n",
      "781 Train Loss 11.588918 Test Loss 9.802382820437952\n",
      "782 Train Loss 11.582345 Test Loss 9.805800710272768\n",
      "783 Train Loss 11.578335 Test Loss 9.810658131806102\n",
      "784 Train Loss 11.574189 Test Loss 9.816263411549956\n",
      "785 Train Loss 11.574616 Test Loss 9.814767069177094\n",
      "786 Train Loss 11.572982 Test Loss 9.815492480520595\n",
      "787 Train Loss 11.571171 Test Loss 9.814331076232136\n",
      "788 Train Loss 11.577306 Test Loss 9.815854576430718\n",
      "789 Train Loss 11.570468 Test Loss 9.814674910014707\n",
      "790 Train Loss 11.568564 Test Loss 9.810501962783022\n",
      "791 Train Loss 11.567251 Test Loss 9.806343843218047\n",
      "792 Train Loss 11.566666 Test Loss 9.8032434664205\n",
      "793 Train Loss 11.565721 Test Loss 9.804152189351889\n",
      "794 Train Loss 11.564867 Test Loss 9.805161895029899\n",
      "795 Train Loss 11.563488 Test Loss 9.806497348130796\n",
      "796 Train Loss 11.562501 Test Loss 9.805402656051154\n",
      "797 Train Loss 11.5622425 Test Loss 9.806470984942568\n",
      "798 Train Loss 11.561157 Test Loss 9.805965203385144\n",
      "799 Train Loss 11.559019 Test Loss 9.802827206595797\n",
      "800 Train Loss 11.611406 Test Loss 9.807621617286626\n",
      "801 Train Loss 11.558439 Test Loss 9.8034786954815\n",
      "802 Train Loss 11.561005 Test Loss 9.784903667708225\n",
      "803 Train Loss 11.557457 Test Loss 9.796778883196502\n",
      "804 Train Loss 11.554939 Test Loss 9.79284939272705\n",
      "805 Train Loss 11.5553665 Test Loss 9.787647077630426\n",
      "806 Train Loss 11.553993 Test Loss 9.790396470166865\n",
      "807 Train Loss 11.5531025 Test Loss 9.790115225461953\n",
      "808 Train Loss 11.552115 Test Loss 9.790190901875297\n",
      "809 Train Loss 11.55022 Test Loss 9.78954256831664\n",
      "810 Train Loss 11.547011 Test Loss 9.791602072765949\n",
      "811 Train Loss 11.548573 Test Loss 9.787872209317772\n",
      "812 Train Loss 11.546134 Test Loss 9.790029621105242\n",
      "813 Train Loss 11.544581 Test Loss 9.788614919317334\n",
      "814 Train Loss 11.543135 Test Loss 9.786764561435314\n",
      "815 Train Loss 11.542067 Test Loss 9.785923185384203\n",
      "816 Train Loss 11.541262 Test Loss 9.784982424123292\n",
      "817 Train Loss 11.541505 Test Loss 9.785891165069373\n",
      "818 Train Loss 11.540975 Test Loss 9.785355523728889\n",
      "819 Train Loss 11.540274 Test Loss 9.78408357997961\n",
      "820 Train Loss 11.539696 Test Loss 9.783573836408385\n",
      "821 Train Loss 11.539467 Test Loss 9.785501632856123\n",
      "822 Train Loss 11.539026 Test Loss 9.78629723268397\n",
      "823 Train Loss 11.538805 Test Loss 9.78639925653688\n",
      "824 Train Loss 11.540164 Test Loss 9.782324412118237\n",
      "825 Train Loss 11.53875 Test Loss 9.785662365946216\n",
      "826 Train Loss 11.538909 Test Loss 9.788824542185782\n",
      "827 Train Loss 11.538509 Test Loss 9.787032407326663\n",
      "828 Train Loss 11.538206 Test Loss 9.78755783692918\n",
      "829 Train Loss 11.537575 Test Loss 9.787414919281115\n",
      "830 Train Loss 11.537182 Test Loss 9.7873972408767\n",
      "831 Train Loss 11.545279 Test Loss 9.787286057636898\n",
      "832 Train Loss 11.537037 Test Loss 9.787359150466049\n",
      "833 Train Loss 11.53614 Test Loss 9.785754095420742\n",
      "834 Train Loss 11.535136 Test Loss 9.784211689579742\n",
      "835 Train Loss 11.559372 Test Loss 9.785486992061873\n",
      "836 Train Loss 11.535071 Test Loss 9.784234815906238\n",
      "837 Train Loss 11.5361395 Test Loss 9.787363103746747\n",
      "838 Train Loss 11.534774 Test Loss 9.78521204023087\n",
      "839 Train Loss 11.534496 Test Loss 9.784350385143735\n",
      "840 Train Loss 11.53427 Test Loss 9.781998153720803\n",
      "841 Train Loss 11.533622 Test Loss 9.784115181486541\n",
      "842 Train Loss 11.533269 Test Loss 9.784183080515307\n",
      "843 Train Loss 11.532879 Test Loss 9.783178116695252\n",
      "844 Train Loss 11.561234 Test Loss 9.762977411385084\n",
      "845 Train Loss 11.532778 Test Loss 9.781900652534816\n",
      "846 Train Loss 11.570243 Test Loss 9.772234827499448\n",
      "847 Train Loss 11.532618 Test Loss 9.781108760751\n",
      "848 Train Loss 11.531379 Test Loss 9.776292985779229\n",
      "849 Train Loss 11.533569 Test Loss 9.77478547787072\n",
      "850 Train Loss 11.531 Test Loss 9.775814092586876\n",
      "851 Train Loss 11.529947 Test Loss 9.774777463997207\n",
      "852 Train Loss 11.528009 Test Loss 9.772799101818203\n",
      "853 Train Loss 11.526842 Test Loss 9.771871485245128\n",
      "854 Train Loss 11.5244875 Test Loss 9.773855017257427\n",
      "855 Train Loss 11.520148 Test Loss 9.775198325596053\n",
      "856 Train Loss 11.5161495 Test Loss 9.77698558688634\n",
      "857 Train Loss 11.513622 Test Loss 9.781180372794413\n",
      "858 Train Loss 11.511505 Test Loss 9.77281216466777\n",
      "859 Train Loss 11.508884 Test Loss 9.772705349110481\n",
      "860 Train Loss 11.507275 Test Loss 9.770461321867005\n",
      "861 Train Loss 11.50404 Test Loss 9.764423931772201\n",
      "862 Train Loss 11.501197 Test Loss 9.75873551277199\n",
      "863 Train Loss 11.498351 Test Loss 9.750639127777422\n",
      "864 Train Loss 11.494327 Test Loss 9.752396113579243\n",
      "865 Train Loss 11.4906845 Test Loss 9.747077931442043\n",
      "866 Train Loss 11.579333 Test Loss 9.743152133872362\n",
      "867 Train Loss 11.490319 Test Loss 9.74669214444733\n",
      "868 Train Loss 11.48779 Test Loss 9.737759996009217\n",
      "869 Train Loss 11.485964 Test Loss 9.727341400027106\n",
      "870 Train Loss 11.483588 Test Loss 9.72240287162475\n",
      "871 Train Loss 11.48207 Test Loss 9.721786123078822\n",
      "872 Train Loss 11.484519 Test Loss 9.718538410134364\n",
      "873 Train Loss 11.480528 Test Loss 9.720366086290777\n",
      "874 Train Loss 11.479695 Test Loss 9.717668026917687\n",
      "875 Train Loss 11.478305 Test Loss 9.714999581463116\n",
      "876 Train Loss 11.477802 Test Loss 9.715019171023094\n",
      "877 Train Loss 11.47823 Test Loss 9.712713849751411\n",
      "878 Train Loss 11.477567 Test Loss 9.714135164563258\n",
      "879 Train Loss 11.476913 Test Loss 9.715863178834065\n",
      "880 Train Loss 11.476472 Test Loss 9.717827371347356\n",
      "881 Train Loss 11.478322 Test Loss 9.722554095721653\n",
      "882 Train Loss 11.475938 Test Loss 9.719284564789964\n",
      "883 Train Loss 11.475215 Test Loss 9.72245732687235\n",
      "884 Train Loss 11.474682 Test Loss 9.724320597884132\n",
      "885 Train Loss 11.474231 Test Loss 9.727710332149593\n",
      "886 Train Loss 11.473577 Test Loss 9.731422923118007\n",
      "887 Train Loss 11.473023 Test Loss 9.734157912304244\n",
      "888 Train Loss 11.4784 Test Loss 9.73723480781277\n",
      "889 Train Loss 11.472853 Test Loss 9.73456985287557\n",
      "890 Train Loss 11.472255 Test Loss 9.733262343861714\n",
      "891 Train Loss 11.471511 Test Loss 9.732104241541863\n",
      "892 Train Loss 11.471174 Test Loss 9.73005934482806\n",
      "893 Train Loss 11.470716 Test Loss 9.731437790006673\n",
      "894 Train Loss 11.469995 Test Loss 9.733316972414364\n",
      "895 Train Loss 11.469578 Test Loss 9.733735888405453\n",
      "896 Train Loss 11.468947 Test Loss 9.733976825294416\n",
      "897 Train Loss 11.47075 Test Loss 9.743408037992324\n",
      "898 Train Loss 11.468758 Test Loss 9.736139513754928\n",
      "899 Train Loss 11.468085 Test Loss 9.731461832434526\n",
      "900 Train Loss 11.467005 Test Loss 9.73486924575084\n",
      "901 Train Loss 11.46504 Test Loss 9.741084469705106\n",
      "902 Train Loss 11.46262 Test Loss 9.747968425646217\n",
      "903 Train Loss 11.459345 Test Loss 9.757659762710293\n",
      "904 Train Loss 11.458268 Test Loss 9.754157764281175\n",
      "905 Train Loss 11.457782 Test Loss 9.749179401827474\n",
      "906 Train Loss 11.456542 Test Loss 9.751305642600586\n",
      "907 Train Loss 11.455375 Test Loss 9.751171727277852\n",
      "908 Train Loss 11.454093 Test Loss 9.746216480780205\n",
      "909 Train Loss 11.452466 Test Loss 9.751546444608103\n",
      "910 Train Loss 11.448923 Test Loss 9.747475999960479\n",
      "911 Train Loss 11.440797 Test Loss 9.7511373708181\n",
      "912 Train Loss 11.610102 Test Loss 9.749889179613573\n",
      "913 Train Loss 11.440223 Test Loss 9.750745589958019\n",
      "914 Train Loss 11.433055 Test Loss 9.753063992079628\n",
      "915 Train Loss 11.433088 Test Loss 9.750604618271051\n",
      "916 Train Loss 11.430072 Test Loss 9.751790617719776\n",
      "917 Train Loss 11.437468 Test Loss 9.745796629631165\n",
      "918 Train Loss 11.428409 Test Loss 9.74992934421691\n",
      "919 Train Loss 11.424943 Test Loss 9.746847317383395\n",
      "920 Train Loss 11.422376 Test Loss 9.745719785145054\n",
      "921 Train Loss 11.419672 Test Loss 9.74416444892902\n",
      "922 Train Loss 11.550351 Test Loss 9.703936781500209\n",
      "923 Train Loss 11.419297 Test Loss 9.741727926498191\n",
      "924 Train Loss 11.415719 Test Loss 9.740603215924903\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "925 Train Loss 11.412497 Test Loss 9.742093281587765\n",
      "926 Train Loss 11.410153 Test Loss 9.743435423625375\n",
      "927 Train Loss 11.410089 Test Loss 9.738310192288226\n",
      "928 Train Loss 11.4084635 Test Loss 9.740755945645459\n",
      "929 Train Loss 11.412455 Test Loss 9.743926796334565\n",
      "930 Train Loss 11.407117 Test Loss 9.74174690384572\n",
      "931 Train Loss 11.404041 Test Loss 9.735566941366915\n",
      "932 Train Loss 11.401448 Test Loss 9.733478525489478\n",
      "933 Train Loss 11.399484 Test Loss 9.732380122021015\n",
      "934 Train Loss 11.39714 Test Loss 9.728124007737646\n",
      "935 Train Loss 11.394794 Test Loss 9.726613167520876\n",
      "936 Train Loss 11.392118 Test Loss 9.724961911757243\n",
      "937 Train Loss 11.390857 Test Loss 9.723894677780674\n",
      "938 Train Loss 11.389114 Test Loss 9.724987744855106\n",
      "939 Train Loss 11.386501 Test Loss 9.723565433076962\n",
      "940 Train Loss 11.385202 Test Loss 9.723873757250262\n",
      "941 Train Loss 11.383327 Test Loss 9.722695857535177\n",
      "942 Train Loss 11.381906 Test Loss 9.717750728343034\n",
      "943 Train Loss 11.380524 Test Loss 9.707300077008997\n",
      "944 Train Loss 11.378847 Test Loss 9.697269210831303\n",
      "945 Train Loss 11.385498 Test Loss 9.694688463570223\n",
      "946 Train Loss 11.376606 Test Loss 9.696146946738203\n",
      "947 Train Loss 11.367271 Test Loss 9.67937028707474\n",
      "948 Train Loss 11.395597 Test Loss 9.599652576949856\n",
      "949 Train Loss 11.354248 Test Loss 9.644419206396854\n",
      "950 Train Loss 11.354708 Test Loss 9.625429555872827\n",
      "951 Train Loss 11.347047 Test Loss 9.63497971007694\n",
      "952 Train Loss 11.406392 Test Loss 9.582571272214679\n",
      "953 Train Loss 11.3382 Test Loss 9.615316649962127\n",
      "954 Train Loss 11.495579 Test Loss 9.58930899182981\n",
      "955 Train Loss 11.330396 Test Loss 9.607965083291385\n",
      "956 Train Loss 11.343828 Test Loss 9.599062344147953\n",
      "957 Train Loss 11.324711 Test Loss 9.60445411822569\n",
      "958 Train Loss 11.315108 Test Loss 9.609595281837748\n",
      "959 Train Loss 11.306083 Test Loss 9.59265500035159\n",
      "960 Train Loss 11.300346 Test Loss 9.581846717052208\n",
      "961 Train Loss 11.305043 Test Loss 9.579874837599698\n",
      "962 Train Loss 11.297509 Test Loss 9.580828924719931\n",
      "963 Train Loss 11.294647 Test Loss 9.570665412078336\n",
      "964 Train Loss 11.293092 Test Loss 9.57081171641917\n",
      "965 Train Loss 11.290769 Test Loss 9.570802216239155\n",
      "966 Train Loss 11.288697 Test Loss 9.563362787523566\n",
      "967 Train Loss 11.286286 Test Loss 9.555531941035355\n",
      "968 Train Loss 11.309455 Test Loss 9.548196811957634\n",
      "969 Train Loss 11.285686 Test Loss 9.55436816812079\n",
      "970 Train Loss 11.810412 Test Loss 9.468562527342357\n",
      "971 Train Loss 11.283831 Test Loss 9.543548526468927\n",
      "972 Train Loss 11.280668 Test Loss 9.536371340946076\n",
      "973 Train Loss 11.276136 Test Loss 9.534255041468052\n",
      "974 Train Loss 11.271119 Test Loss 9.543349177713136\n",
      "975 Train Loss 11.270708 Test Loss 9.542935853235942\n",
      "976 Train Loss 11.267151 Test Loss 9.542980790811017\n",
      "977 Train Loss 11.261827 Test Loss 9.532330090195114\n",
      "978 Train Loss 11.51264 Test Loss 9.48214881139992\n",
      "979 Train Loss 11.260922 Test Loss 9.528940039937243\n",
      "980 Train Loss 11.261507 Test Loss 9.509799597504637\n",
      "981 Train Loss 11.258269 Test Loss 9.519272680204647\n",
      "982 Train Loss 11.254007 Test Loss 9.513648218801691\n",
      "983 Train Loss 11.257129 Test Loss 9.507029762962103\n",
      "984 Train Loss 11.252335 Test Loss 9.51089280368673\n",
      "985 Train Loss 50.585777 Test Loss 8.806355146054585\n",
      "986 Train Loss 11.8846855 Test Loss 9.339715034891963\n",
      "987 Train Loss 11.251261 Test Loss 9.502405675037252\n",
      "988 Train Loss 11.247551 Test Loss 9.496159436386622\n",
      "989 Train Loss 11.243104 Test Loss 9.49034970991088\n",
      "990 Train Loss 11.235731 Test Loss 9.496872329725717\n",
      "991 Train Loss 11.2284155 Test Loss 9.496329177478708\n",
      "992 Train Loss 11.220075 Test Loss 9.485484767454095\n",
      "993 Train Loss 11.271838 Test Loss 9.469144205917713\n",
      "994 Train Loss 11.217326 Test Loss 9.48195597153691\n",
      "995 Train Loss 11.20967 Test Loss 9.479374613256743\n",
      "996 Train Loss 11.208892 Test Loss 9.485944219240718\n",
      "997 Train Loss 11.207132 Test Loss 9.483020134593367\n",
      "998 Train Loss 11.204508 Test Loss 9.481408611422971\n",
      "999 Train Loss 11.200257 Test Loss 9.489013324107546\n",
      "1000 Train Loss 11.19586 Test Loss 9.490394069961404\n",
      "1001 Train Loss 11.253555 Test Loss 9.495903133758233\n",
      "1002 Train Loss 11.194979 Test Loss 9.490596218180148\n",
      "1003 Train Loss 11.225165 Test Loss 9.504570051854234\n",
      "1004 Train Loss 11.1933 Test Loss 9.493510074075429\n",
      "1005 Train Loss 11.190358 Test Loss 9.498103265118408\n",
      "1006 Train Loss 11.188174 Test Loss 9.502754789393254\n",
      "1007 Train Loss 11.18602 Test Loss 9.500650034051509\n",
      "1008 Train Loss 11.184567 Test Loss 9.499996656019734\n",
      "1009 Train Loss 11.180925 Test Loss 9.499318466120036\n",
      "1010 Train Loss 11.179687 Test Loss 9.502053591176095\n",
      "1011 Train Loss 11.178482 Test Loss 9.503904100387022\n",
      "1012 Train Loss 11.177878 Test Loss 9.502642850952817\n",
      "1013 Train Loss 11.177373 Test Loss 9.503194167516687\n",
      "1014 Train Loss 11.177955 Test Loss 9.505010021518789\n",
      "1015 Train Loss 11.177028 Test Loss 9.503860558990754\n",
      "1016 Train Loss 11.176447 Test Loss 9.504030414076091\n",
      "1017 Train Loss 11.175472 Test Loss 9.50295648510801\n",
      "1018 Train Loss 11.172981 Test Loss 9.501899031478773\n",
      "1019 Train Loss 11.170326 Test Loss 9.503403585676086\n",
      "1020 Train Loss 11.16647 Test Loss 9.505119462861524\n",
      "1021 Train Loss 11.158177 Test Loss 9.509449550307801\n",
      "1022 Train Loss 11.147654 Test Loss 9.515702310724864\n",
      "1023 Train Loss 11.192651 Test Loss 9.520620870092904\n",
      "1024 Train Loss 11.140792 Test Loss 9.516347873011064\n",
      "1025 Train Loss 11.130697 Test Loss 9.51401095926091\n",
      "1026 Train Loss 11.127969 Test Loss 9.507494386266933\n",
      "1027 Train Loss 11.120308 Test Loss 9.504271576035556\n",
      "1028 Train Loss 11.120189 Test Loss 9.502038303006213\n",
      "1029 Train Loss 11.118369 Test Loss 9.503130159178442\n",
      "1030 Train Loss 11.116764 Test Loss 9.497871374406381\n",
      "1031 Train Loss 11.114941 Test Loss 9.495758009520063\n",
      "1032 Train Loss 11.113176 Test Loss 9.496501930832402\n",
      "1033 Train Loss 11.114001 Test Loss 9.489112398340009\n",
      "1034 Train Loss 11.112116 Test Loss 9.49319360173258\n",
      "1035 Train Loss 11.11048 Test Loss 9.49177809234066\n",
      "1036 Train Loss 11.109518 Test Loss 9.496604591538464\n",
      "1037 Train Loss 11.106267 Test Loss 9.498291004345257\n",
      "1038 Train Loss 11.104595 Test Loss 9.494870463674745\n",
      "1039 Train Loss 11.102055 Test Loss 9.494161022731697\n",
      "1040 Train Loss 11.099368 Test Loss 9.491782398619232\n",
      "1041 Train Loss 11.0962715 Test Loss 9.488147482662598\n",
      "1042 Train Loss 11.104733 Test Loss 9.485579861283517\n",
      "1043 Train Loss 11.093706 Test Loss 9.487096546630973\n",
      "1044 Train Loss 11.089283 Test Loss 9.48005226138602\n",
      "1045 Train Loss 11.081049 Test Loss 9.476475793464754\n",
      "1046 Train Loss 11.072553 Test Loss 9.479678195080922\n",
      "1047 Train Loss 11.065336 Test Loss 9.479315861684157\n",
      "1048 Train Loss 11.14004 Test Loss 9.478569274674749\n",
      "1049 Train Loss 11.063006 Test Loss 9.478666339431618\n",
      "1050 Train Loss 11.07046 Test Loss 9.4779652041892\n",
      "1051 Train Loss 11.057995 Test Loss 9.478377220558349\n",
      "1052 Train Loss 11.050897 Test Loss 9.48580602546013\n",
      "1053 Train Loss 11.039402 Test Loss 9.487260028603531\n",
      "1054 Train Loss 11.031947 Test Loss 9.49514826153726\n",
      "1055 Train Loss 11.033099 Test Loss 9.498246884203157\n",
      "1056 Train Loss 11.028834 Test Loss 9.496490211658736\n",
      "1057 Train Loss 11.023317 Test Loss 9.49828357939888\n",
      "1058 Train Loss 11.017344 Test Loss 9.491936358587918\n",
      "1059 Train Loss 11.015143 Test Loss 9.493779683328652\n",
      "1060 Train Loss 11.012164 Test Loss 9.49504294373687\n",
      "1061 Train Loss 11.009496 Test Loss 9.49044735608097\n",
      "1062 Train Loss 11.007293 Test Loss 9.490998630525011\n",
      "1063 Train Loss 11.004513 Test Loss 9.492931662715751\n",
      "1064 Train Loss 11.019111 Test Loss 9.492548700521894\n",
      "1065 Train Loss 11.002758 Test Loss 9.492602169006503\n",
      "1066 Train Loss 11.058496 Test Loss 9.50700681098358\n",
      "1067 Train Loss 11.001374 Test Loss 9.494580278371496\n",
      "1068 Train Loss 11.342429 Test Loss 9.48037692592555\n",
      "1069 Train Loss 11.000601 Test Loss 9.493732508164834\n",
      "1070 Train Loss 10.995256 Test Loss 9.496118639351344\n",
      "1071 Train Loss 10.989896 Test Loss 9.493739421150957\n",
      "1072 Train Loss 10.984194 Test Loss 9.49079774467345\n",
      "1073 Train Loss 11.0959635 Test Loss 9.481000292184298\n",
      "1074 Train Loss 10.982161 Test Loss 9.489404288584076\n",
      "1075 Train Loss 10.980244 Test Loss 9.48282432377276\n",
      "1076 Train Loss 10.977108 Test Loss 9.478709266126472\n",
      "1077 Train Loss 10.969364 Test Loss 9.480410797810405\n",
      "1078 Train Loss 10.965284 Test Loss 9.482984018597772\n",
      "1079 Train Loss 10.957262 Test Loss 9.484100524617885\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1080 Train Loss 10.951582 Test Loss 9.482075079113931\n",
      "1081 Train Loss 10.946526 Test Loss 9.483202828473337\n",
      "1082 Train Loss 10.943583 Test Loss 9.48911846355934\n",
      "1083 Train Loss 10.939517 Test Loss 9.49186018433144\n",
      "1084 Train Loss 10.942879 Test Loss 9.490423971993728\n",
      "1085 Train Loss 10.937102 Test Loss 9.491234140186439\n",
      "1086 Train Loss 10.932574 Test Loss 9.487501639528391\n",
      "1087 Train Loss 10.927037 Test Loss 9.49350318751917\n",
      "1088 Train Loss 10.919942 Test Loss 9.491745698795729\n",
      "1089 Train Loss 10.913895 Test Loss 9.487441017054625\n",
      "1090 Train Loss 10.912032 Test Loss 9.486116365560562\n",
      "1091 Train Loss 10.909717 Test Loss 9.486304075134326\n",
      "1092 Train Loss 10.908158 Test Loss 9.488297530520654\n",
      "1093 Train Loss 10.9068985 Test Loss 9.489466552152429\n",
      "1094 Train Loss 10.905229 Test Loss 9.491131626378614\n",
      "1095 Train Loss 10.902983 Test Loss 9.492571445926496\n",
      "1096 Train Loss 10.9009905 Test Loss 9.493143671636235\n",
      "1097 Train Loss 10.898201 Test Loss 9.491283093104693\n",
      "1098 Train Loss 10.894264 Test Loss 9.482617797310672\n",
      "1099 Train Loss 10.896179 Test Loss 9.490457042327053\n",
      "1100 Train Loss 10.889969 Test Loss 9.486016028860544\n",
      "1101 Train Loss 10.902967 Test Loss 9.497876379197267\n",
      "1102 Train Loss 10.887994 Test Loss 9.488943974893877\n",
      "1103 Train Loss 10.878517 Test Loss 9.476091778223429\n",
      "1104 Train Loss 10.868121 Test Loss 9.466140742917634\n",
      "1105 Train Loss 10.860507 Test Loss 9.453456103897787\n",
      "1106 Train Loss 10.853224 Test Loss 9.457105495268234\n",
      "1107 Train Loss 10.87845 Test Loss 9.456060996473902\n",
      "1108 Train Loss 10.839421 Test Loss 9.455728171441859\n",
      "1109 Train Loss 10.834127 Test Loss 9.447632593684602\n",
      "1110 Train Loss 10.846117 Test Loss 9.43690097971992\n",
      "1111 Train Loss 10.812856 Test Loss 9.441709755295644\n",
      "1112 Train Loss 10.819819 Test Loss 9.455298371406391\n",
      "1113 Train Loss 10.803282 Test Loss 9.447457751744084\n",
      "1114 Train Loss 10.8098955 Test Loss 9.456067072142854\n",
      "1115 Train Loss 10.796446 Test Loss 9.450880131425068\n",
      "1116 Train Loss 10.821056 Test Loss 9.437209137133545\n",
      "1117 Train Loss 10.794027 Test Loss 9.44755606606638\n",
      "1118 Train Loss 10.792845 Test Loss 9.440971292023981\n",
      "1119 Train Loss 10.788935 Test Loss 9.443994346635352\n",
      "1120 Train Loss 10.785926 Test Loss 9.440289623912266\n",
      "1121 Train Loss 10.782842 Test Loss 9.441138241143083\n",
      "1122 Train Loss 10.77953 Test Loss 9.437010191040565\n",
      "1123 Train Loss 10.777477 Test Loss 9.440223685600543\n",
      "1124 Train Loss 10.774685 Test Loss 9.43928216433433\n",
      "1125 Train Loss 10.77157 Test Loss 9.437167277938434\n",
      "1126 Train Loss 10.77363 Test Loss 9.436675307123878\n",
      "1127 Train Loss 10.770354 Test Loss 9.436793580593543\n",
      "1128 Train Loss 10.769301 Test Loss 9.441028342444362\n",
      "1129 Train Loss 10.767351 Test Loss 9.439161813160709\n",
      "1130 Train Loss 10.783287 Test Loss 9.44201451958863\n",
      "1131 Train Loss 10.765234 Test Loss 9.439633002277382\n",
      "1132 Train Loss 10.761244 Test Loss 9.442184729182735\n",
      "1133 Train Loss 10.755502 Test Loss 9.445924289015592\n",
      "1134 Train Loss 10.748959 Test Loss 9.449133028147624\n",
      "1135 Train Loss 10.760558 Test Loss 9.443694366704198\n",
      "1136 Train Loss 10.746128 Test Loss 9.447407753900197\n",
      "1137 Train Loss 10.739122 Test Loss 9.441256814198633\n",
      "1138 Train Loss 10.730565 Test Loss 9.430929916120293\n",
      "1139 Train Loss 10.730683 Test Loss 9.411929633158746\n",
      "1140 Train Loss 10.725149 Test Loss 9.42124822447338\n",
      "1141 Train Loss 10.718754 Test Loss 9.414519970741328\n",
      "1142 Train Loss 10.711992 Test Loss 9.401677992198902\n",
      "1143 Train Loss 10.706815 Test Loss 9.394869039253688\n",
      "1144 Train Loss 10.70179 Test Loss 9.387640112393612\n",
      "1145 Train Loss 10.7027445 Test Loss 9.380321753237133\n",
      "1146 Train Loss 10.699089 Test Loss 9.384285329528304\n",
      "1147 Train Loss 10.695589 Test Loss 9.364116249458629\n",
      "1148 Train Loss 10.687849 Test Loss 9.35663028505865\n",
      "1149 Train Loss 10.682697 Test Loss 9.350084911135276\n",
      "1150 Train Loss 10.677493 Test Loss 9.335329308842033\n",
      "1151 Train Loss 10.675113 Test Loss 9.322961333139917\n",
      "1152 Train Loss 10.675319 Test Loss 9.312710998541025\n",
      "1153 Train Loss 10.671401 Test Loss 9.317790252399055\n",
      "1154 Train Loss 10.672358 Test Loss 9.293771312749946\n",
      "1155 Train Loss 10.668138 Test Loss 9.30615595798772\n",
      "1156 Train Loss 10.659711 Test Loss 9.295920522521248\n",
      "1157 Train Loss 10.717595 Test Loss 9.244828420701028\n",
      "1158 Train Loss 10.654152 Test Loss 9.284731073805762\n",
      "1159 Train Loss 10.662771 Test Loss 9.282393430599233\n",
      "1160 Train Loss 10.647084 Test Loss 9.28283513078912\n",
      "1161 Train Loss 10.648299 Test Loss 9.286477819648352\n",
      "1162 Train Loss 10.642225 Test Loss 9.284522128546442\n",
      "1163 Train Loss 10.636499 Test Loss 9.288303935006589\n",
      "1164 Train Loss 10.631886 Test Loss 9.287732545686016\n",
      "1165 Train Loss 10.628551 Test Loss 9.275674189774948\n",
      "1166 Train Loss 10.623943 Test Loss 9.2764827394239\n",
      "1167 Train Loss 10.620493 Test Loss 9.276898626521119\n",
      "1168 Train Loss 10.620171 Test Loss 9.289103573576595\n",
      "1169 Train Loss 10.618535 Test Loss 9.28323806525286\n",
      "1170 Train Loss 10.614628 Test Loss 9.290716095188422\n",
      "1171 Train Loss 10.611519 Test Loss 9.293693180542014\n",
      "1172 Train Loss 10.612807 Test Loss 9.296402767730354\n",
      "1173 Train Loss 10.609932 Test Loss 9.294760808878596\n",
      "1174 Train Loss 10.60894 Test Loss 9.296243727505644\n",
      "1175 Train Loss 10.6067505 Test Loss 9.29541672038817\n",
      "1176 Train Loss 10.60567 Test Loss 9.296728679783573\n",
      "1177 Train Loss 10.6040745 Test Loss 9.297433430037552\n",
      "1178 Train Loss 10.610701 Test Loss 9.300809737357703\n",
      "1179 Train Loss 10.602659 Test Loss 9.298365997105188\n",
      "1180 Train Loss 10.6536 Test Loss 9.333777442272039\n",
      "1181 Train Loss 10.602198 Test Loss 9.301413532467805\n",
      "1182 Train Loss 10.608251 Test Loss 9.29908348671151\n",
      "1183 Train Loss 10.601327 Test Loss 9.300664111421122\n",
      "1184 Train Loss 10.602904 Test Loss 9.313241275666602\n",
      "1185 Train Loss 10.598073 Test Loss 9.306239697394377\n",
      "1186 Train Loss 10.593605 Test Loss 9.308991345847085\n",
      "1187 Train Loss 10.586629 Test Loss 9.303934787397836\n",
      "1188 Train Loss 10.584384 Test Loss 9.310961272473468\n",
      "1189 Train Loss 10.577419 Test Loss 9.308000345289011\n",
      "1190 Train Loss 10.572585 Test Loss 9.308496609837356\n",
      "1191 Train Loss 10.564579 Test Loss 9.303899124008671\n",
      "1192 Train Loss 10.578893 Test Loss 9.306117497132412\n",
      "1193 Train Loss 10.560515 Test Loss 9.304171708087997\n",
      "1194 Train Loss 10.613073 Test Loss 9.308238127799967\n",
      "1195 Train Loss 10.554931 Test Loss 9.30491511666229\n",
      "1196 Train Loss 10.545262 Test Loss 9.301274098270277\n",
      "1197 Train Loss 10.529877 Test Loss 9.292209106026897\n",
      "1198 Train Loss 10.515371 Test Loss 9.27559385603716\n",
      "1199 Train Loss 10.50599 Test Loss 9.260820995234445\n",
      "1200 Train Loss 10.501702 Test Loss 9.241689365092219\n",
      "1201 Train Loss 10.492426 Test Loss 9.248958139366469\n",
      "1202 Train Loss 10.489138 Test Loss 9.26356504406114\n",
      "1203 Train Loss 10.482846 Test Loss 9.261245873097108\n",
      "1204 Train Loss 10.478964 Test Loss 9.25792430173923\n",
      "1205 Train Loss 10.475548 Test Loss 9.253252305355726\n",
      "1206 Train Loss 10.473037 Test Loss 9.257603588572271\n",
      "1207 Train Loss 10.471172 Test Loss 9.258431470962273\n",
      "1208 Train Loss 10.469652 Test Loss 9.25836656628805\n",
      "1209 Train Loss 10.468025 Test Loss 9.258276515714384\n",
      "1210 Train Loss 10.465996 Test Loss 9.258554101402042\n",
      "1211 Train Loss 10.497992 Test Loss 9.24931413216569\n",
      "1212 Train Loss 10.465103 Test Loss 9.257177554868136\n",
      "1213 Train Loss 10.462246 Test Loss 9.257292443234801\n",
      "1214 Train Loss 10.458733 Test Loss 9.250459583938724\n",
      "1215 Train Loss 10.45609 Test Loss 9.249979127316369\n",
      "1216 Train Loss 10.456124 Test Loss 9.235362822626918\n",
      "1217 Train Loss 10.453117 Test Loss 9.242444337549943\n",
      "1218 Train Loss 10.449985 Test Loss 9.239141472691806\n",
      "1219 Train Loss 10.446646 Test Loss 9.237907253485272\n",
      "1220 Train Loss 10.442874 Test Loss 9.23327508665038\n",
      "1221 Train Loss 10.436669 Test Loss 9.229223990921398\n",
      "1222 Train Loss 10.430618 Test Loss 9.224890552701208\n",
      "1223 Train Loss 10.424817 Test Loss 9.229291532242764\n",
      "1224 Train Loss 10.420232 Test Loss 9.224557396248159\n",
      "1225 Train Loss 10.412552 Test Loss 9.236428123403641\n",
      "1226 Train Loss 10.408097 Test Loss 9.233227760663334\n",
      "1227 Train Loss 10.404073 Test Loss 9.230849670969326\n",
      "1228 Train Loss 10.399045 Test Loss 9.227114330301795\n",
      "1229 Train Loss 10.393899 Test Loss 9.228600897348139\n",
      "1230 Train Loss 10.386595 Test Loss 9.225827634332548\n",
      "1231 Train Loss 10.379901 Test Loss 9.232396173620025\n",
      "1232 Train Loss 10.374842 Test Loss 9.232635383915133\n",
      "1233 Train Loss 10.369308 Test Loss 9.232397406633137\n",
      "1234 Train Loss 10.364934 Test Loss 9.23050520062451\n",
      "1235 Train Loss 10.361141 Test Loss 9.228293544854207\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1236 Train Loss 10.358498 Test Loss 9.225472587174716\n",
      "1237 Train Loss 10.356133 Test Loss 9.223217061061307\n",
      "1238 Train Loss 10.353873 Test Loss 9.224401789188658\n",
      "1239 Train Loss 10.352098 Test Loss 9.22428624874558\n",
      "1240 Train Loss 10.350659 Test Loss 9.2264874995518\n",
      "1241 Train Loss 10.349638 Test Loss 9.226106229021307\n",
      "1242 Train Loss 10.34832 Test Loss 9.22407257990168\n",
      "1243 Train Loss 10.346712 Test Loss 9.220373863180304\n",
      "1244 Train Loss 10.34523 Test Loss 9.218294977789215\n",
      "1245 Train Loss 10.343661 Test Loss 9.21404730703172\n",
      "1246 Train Loss 10.34249 Test Loss 9.21499180946852\n",
      "1247 Train Loss 10.341228 Test Loss 9.217269809436038\n",
      "1248 Train Loss 10.339427 Test Loss 9.222038314897658\n",
      "1249 Train Loss 10.338452 Test Loss 9.22433815742978\n",
      "1250 Train Loss 10.337552 Test Loss 9.225238808930747\n",
      "1251 Train Loss 10.336578 Test Loss 9.224885589145293\n",
      "1252 Train Loss 10.335423 Test Loss 9.223165757517524\n",
      "1253 Train Loss 10.333715 Test Loss 9.22132340470475\n",
      "1254 Train Loss 10.331425 Test Loss 9.218846507059999\n",
      "1255 Train Loss 10.3288555 Test Loss 9.216639770562333\n",
      "1256 Train Loss 10.325224 Test Loss 9.213033291240547\n",
      "1257 Train Loss 10.320712 Test Loss 9.211456286848081\n",
      "1258 Train Loss 10.31506 Test Loss 9.209387550713025\n",
      "1259 Train Loss 10.313165 Test Loss 9.205105719880386\n",
      "1260 Train Loss 10.310001 Test Loss 9.204364789831628\n",
      "1261 Train Loss 10.307029 Test Loss 9.20527080895224\n",
      "1262 Train Loss 10.302973 Test Loss 9.201827523922704\n",
      "1263 Train Loss 10.29789 Test Loss 9.198743819560816\n",
      "1264 Train Loss 10.293779 Test Loss 9.196497261836939\n",
      "1265 Train Loss 10.291378 Test Loss 9.198860769256761\n",
      "1266 Train Loss 10.2902975 Test Loss 9.196699181462177\n",
      "1267 Train Loss 10.289056 Test Loss 9.195565347316267\n",
      "1268 Train Loss 10.2872 Test Loss 9.19479718188536\n",
      "1269 Train Loss 10.285336 Test Loss 9.191103835422206\n",
      "1270 Train Loss 10.289882 Test Loss 9.19160361750105\n",
      "1271 Train Loss 10.284207 Test Loss 9.191198547929075\n",
      "1272 Train Loss 10.282913 Test Loss 9.190805704176018\n",
      "1273 Train Loss 10.281037 Test Loss 9.189079765087081\n",
      "1274 Train Loss 10.279349 Test Loss 9.186531712810341\n",
      "1275 Train Loss 10.277666 Test Loss 9.186416975962027\n",
      "1276 Train Loss 10.275372 Test Loss 9.184062646267488\n",
      "1277 Train Loss 10.2730255 Test Loss 9.181252775849046\n",
      "1278 Train Loss 10.269955 Test Loss 9.180163146205963\n",
      "1279 Train Loss 10.270794 Test Loss 9.176727004217234\n",
      "1280 Train Loss 10.267029 Test Loss 9.178431435337455\n",
      "1281 Train Loss 10.263694 Test Loss 9.175862600470312\n",
      "1282 Train Loss 10.259568 Test Loss 9.17895036304199\n",
      "1283 Train Loss 10.256287 Test Loss 9.178230663740004\n",
      "1284 Train Loss 10.252034 Test Loss 9.174668325922745\n",
      "1285 Train Loss 10.248975 Test Loss 9.173396779875223\n",
      "1286 Train Loss 10.24534 Test Loss 9.168305932250611\n",
      "1287 Train Loss 10.24049 Test Loss 9.16432317881537\n",
      "1288 Train Loss 10.236694 Test Loss 9.1585380918359\n",
      "1289 Train Loss 10.233084 Test Loss 9.159574682284154\n",
      "1290 Train Loss 10.2295265 Test Loss 9.15385865773961\n",
      "1291 Train Loss 10.225398 Test Loss 9.146442031064637\n",
      "1292 Train Loss 10.223126 Test Loss 9.144208370011974\n",
      "1293 Train Loss 10.236816 Test Loss 9.139386420829432\n",
      "1294 Train Loss 10.219 Test Loss 9.142148010922588\n",
      "1295 Train Loss 10.216087 Test Loss 9.134691769266603\n",
      "1296 Train Loss 10.213263 Test Loss 9.124157921381732\n",
      "1297 Train Loss 10.209704 Test Loss 9.090521128970881\n",
      "1298 Train Loss 10.3409815 Test Loss 8.94568756735184\n",
      "1299 Train Loss 10.206502 Test Loss 9.077794984709854\n",
      "1300 Train Loss 10.204134 Test Loss 9.065082725333674\n",
      "1301 Train Loss 10.278776 Test Loss 8.938857145251353\n",
      "1302 Train Loss 10.200054 Test Loss 9.051619011760828\n",
      "1303 Train Loss 10.183301 Test Loss 9.039566232238219\n",
      "1304 Train Loss 10.1712675 Test Loss 9.04221025874943\n",
      "1305 Train Loss 10.163325 Test Loss 9.033815935822332\n",
      "1306 Train Loss 10.160447 Test Loss 9.043366308245597\n",
      "1307 Train Loss 10.161858 Test Loss 9.027557709043341\n",
      "1308 Train Loss 10.156357 Test Loss 9.03595653225231\n",
      "1309 Train Loss 10.153029 Test Loss 9.03118029233237\n",
      "1310 Train Loss 10.15076 Test Loss 9.031380893443803\n",
      "1311 Train Loss 10.148243 Test Loss 9.026929279027398\n",
      "1312 Train Loss 10.146853 Test Loss 9.02934968745785\n",
      "1313 Train Loss 10.145682 Test Loss 9.028969639609635\n",
      "1314 Train Loss 10.145372 Test Loss 9.029058832582267\n",
      "1315 Train Loss 10.144247 Test Loss 9.030693324452397\n",
      "1316 Train Loss 10.14378 Test Loss 9.033035924480139\n",
      "1317 Train Loss 10.142423 Test Loss 9.034303519388303\n",
      "1318 Train Loss 10.140671 Test Loss 9.035229363964595\n",
      "1319 Train Loss 10.138626 Test Loss 9.034819347019823\n",
      "1320 Train Loss 10.134781 Test Loss 9.03360295521975\n",
      "1321 Train Loss 10.121768 Test Loss 9.0193349473573\n",
      "1322 Train Loss 34.892822 Test Loss 8.803626287624855\n",
      "1323 Train Loss 10.137699 Test Loss 9.001291322072031\n",
      "1324 Train Loss 10.115543 Test Loss 9.010830734143143\n",
      "1325 Train Loss 10.173485 Test Loss 8.98897695828777\n",
      "1326 Train Loss 10.110598 Test Loss 9.003125029941563\n",
      "1327 Train Loss 10.095185 Test Loss 8.99674756444163\n",
      "1328 Train Loss 10.085939 Test Loss 8.99985125701267\n",
      "1329 Train Loss 10.076399 Test Loss 9.000311894657756\n",
      "1330 Train Loss 10.068293 Test Loss 9.010091530214947\n",
      "1331 Train Loss 10.056672 Test Loss 8.998959557993741\n",
      "1332 Train Loss 10.033916 Test Loss 8.98774542762354\n",
      "1333 Train Loss 10.138065 Test Loss 8.981062844146459\n",
      "1334 Train Loss 10.023561 Test Loss 8.984878509061867\n",
      "1335 Train Loss 10.06393 Test Loss 8.98222655138786\n",
      "1336 Train Loss 10.008685 Test Loss 8.98367734379121\n",
      "1337 Train Loss 10.19525 Test Loss 8.994758093320518\n",
      "1338 Train Loss 10.002363 Test Loss 8.984547934530564\n",
      "1339 Train Loss 9.994638 Test Loss 8.98736156231867\n",
      "1340 Train Loss 9.991398 Test Loss 8.983066705854938\n",
      "1341 Train Loss 9.987678 Test Loss 8.98650055460591\n",
      "1342 Train Loss 9.986005 Test Loss 8.988942166193768\n",
      "1343 Train Loss 9.983145 Test Loss 8.991607689975673\n",
      "1344 Train Loss 9.980913 Test Loss 8.98912671954198\n",
      "1345 Train Loss 9.9779825 Test Loss 8.984147787074699\n",
      "1346 Train Loss 9.976379 Test Loss 8.98401026820385\n",
      "1347 Train Loss 9.974599 Test Loss 8.983298667088796\n",
      "1348 Train Loss 9.973556 Test Loss 8.986115756740046\n",
      "1349 Train Loss 9.972493 Test Loss 8.985405627317235\n",
      "1350 Train Loss 9.971221 Test Loss 8.985178192028966\n",
      "1351 Train Loss 9.9702425 Test Loss 8.978809528339509\n",
      "1352 Train Loss 9.968881 Test Loss 8.978922251617979\n",
      "1353 Train Loss 9.967253 Test Loss 8.977279197295418\n",
      "1354 Train Loss 9.965706 Test Loss 8.97536901799586\n",
      "1355 Train Loss 9.963572 Test Loss 8.971285859452575\n",
      "1356 Train Loss 9.9613695 Test Loss 8.968911678127618\n",
      "1357 Train Loss 9.959249 Test Loss 8.967823000799344\n",
      "1358 Train Loss 9.954744 Test Loss 8.964530123431224\n",
      "1359 Train Loss 10.020947 Test Loss 8.948330637126936\n",
      "1360 Train Loss 9.953331 Test Loss 8.962235274954093\n",
      "1361 Train Loss 9.946222 Test Loss 8.957316948238955\n",
      "1362 Train Loss 9.934922 Test Loss 8.94697151752039\n",
      "1363 Train Loss 9.925217 Test Loss 8.937800881105549\n",
      "1364 Train Loss 9.921401 Test Loss 8.921646054361007\n",
      "1365 Train Loss 9.916992 Test Loss 8.927708867788073\n",
      "1366 Train Loss 9.918727 Test Loss 8.913442935471423\n",
      "1367 Train Loss 9.906684 Test Loss 8.920604776634251\n",
      "1368 Train Loss 9.905716 Test Loss 8.918414127750674\n",
      "1369 Train Loss 9.895566 Test Loss 8.919070908707832\n",
      "1370 Train Loss 9.887683 Test Loss 8.916449980440238\n",
      "1371 Train Loss 9.896733 Test Loss 8.906511122376957\n",
      "1372 Train Loss 9.884289 Test Loss 8.912648535397526\n",
      "1373 Train Loss 9.876734 Test Loss 8.907917841248732\n",
      "1374 Train Loss 9.860048 Test Loss 8.885715088227023\n",
      "1375 Train Loss 13.322778 Test Loss 8.640630239029768\n",
      "1376 Train Loss 10.010705 Test Loss 8.78988562944982\n",
      "1377 Train Loss 9.85283 Test Loss 8.871788893931443\n",
      "1378 Train Loss 9.852995 Test Loss 8.849678989043081\n",
      "1379 Train Loss 9.843833 Test Loss 8.859515770960133\n",
      "1380 Train Loss 9.839007 Test Loss 8.844114051263109\n",
      "1381 Train Loss 9.824232 Test Loss 8.84130927893223\n",
      "1382 Train Loss 9.822114 Test Loss 8.836644324133417\n",
      "1383 Train Loss 9.811135 Test Loss 8.840668549885475\n",
      "1384 Train Loss 9.806779 Test Loss 8.828182187283856\n",
      "1385 Train Loss 9.802298 Test Loss 8.824579678664412\n",
      "1386 Train Loss 9.796823 Test Loss 8.822665938805683\n",
      "1387 Train Loss 9.787832 Test Loss 8.813470749601297\n",
      "1388 Train Loss 9.785849 Test Loss 8.805625848991179\n",
      "1389 Train Loss 9.779749 Test Loss 8.809065003293396\n",
      "1390 Train Loss 9.773218 Test Loss 8.802807807160898\n",
      "1391 Train Loss 9.759868 Test Loss 8.80061932706128\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1392 Train Loss 9.747797 Test Loss 8.804528053741059\n",
      "1393 Train Loss 9.731474 Test Loss 8.775200687672509\n",
      "1394 Train Loss 9.720197 Test Loss 8.76220336292743\n",
      "1395 Train Loss 9.7095585 Test Loss 8.76443016378958\n",
      "1396 Train Loss 9.70207 Test Loss 8.75861598271038\n",
      "1397 Train Loss 9.693399 Test Loss 8.750371634784033\n",
      "1398 Train Loss 9.690643 Test Loss 8.736422759151226\n",
      "1399 Train Loss 9.685627 Test Loss 8.743824984943629\n",
      "1400 Train Loss 9.686213 Test Loss 8.746737503963468\n",
      "1401 Train Loss 9.682552 Test Loss 8.745177078845915\n",
      "1402 Train Loss 9.67892 Test Loss 8.740071541576743\n",
      "1403 Train Loss 9.673414 Test Loss 8.721769662856905\n",
      "1404 Train Loss 9.665843 Test Loss 8.719007278780104\n",
      "1405 Train Loss 9.659861 Test Loss 8.719985191323962\n",
      "1406 Train Loss 9.872245 Test Loss 8.703436869238283\n",
      "1407 Train Loss 9.655711 Test Loss 8.71399156407335\n",
      "1408 Train Loss 9.648774 Test Loss 8.712585646001303\n",
      "1409 Train Loss 9.645083 Test Loss 8.705207233330377\n",
      "1410 Train Loss 9.653674 Test Loss 8.70645901077649\n",
      "1411 Train Loss 9.642474 Test Loss 8.705596186524453\n",
      "1412 Train Loss 9.639105 Test Loss 8.70816156291027\n",
      "1413 Train Loss 9.635012 Test Loss 8.70601941770013\n",
      "1414 Train Loss 9.63126 Test Loss 8.693566177235734\n",
      "1415 Train Loss 9.625937 Test Loss 8.684399238782081\n",
      "1416 Train Loss 9.620581 Test Loss 8.687907167916993\n",
      "1417 Train Loss 9.612072 Test Loss 8.683535543713242\n",
      "1418 Train Loss 9.621735 Test Loss 8.676829833868524\n",
      "1419 Train Loss 9.607531 Test Loss 8.68114640921884\n",
      "1420 Train Loss 9.594117 Test Loss 8.659779906304005\n",
      "1421 Train Loss 9.584075 Test Loss 8.652554214144166\n",
      "1422 Train Loss 9.570792 Test Loss 8.650428387013744\n",
      "1423 Train Loss 9.554421 Test Loss 8.631437730960286\n",
      "1424 Train Loss 9.536337 Test Loss 8.628969514172844\n",
      "1425 Train Loss 9.517683 Test Loss 8.612930340876142\n",
      "1426 Train Loss 9.6050825 Test Loss 8.57217145897989\n",
      "1427 Train Loss 9.5096245 Test Loss 8.6010372896143\n",
      "1428 Train Loss 9.496913 Test Loss 8.597085712804072\n",
      "1429 Train Loss 9.493174 Test Loss 8.580615458565662\n",
      "1430 Train Loss 9.486124 Test Loss 8.587447958496382\n",
      "1431 Train Loss 9.478981 Test Loss 8.590370578550912\n",
      "1432 Train Loss 9.472559 Test Loss 8.585930627317417\n",
      "1433 Train Loss 9.462589 Test Loss 8.578687746089424\n",
      "1434 Train Loss 9.46052 Test Loss 8.576585065500396\n",
      "1435 Train Loss 9.450787 Test Loss 8.57539024172661\n",
      "1436 Train Loss 9.445409 Test Loss 8.577093969744714\n",
      "1437 Train Loss 9.440762 Test Loss 8.574861036359213\n",
      "1438 Train Loss 9.437166 Test Loss 8.576382117839307\n",
      "1439 Train Loss 9.435003 Test Loss 8.578914824044439\n",
      "1440 Train Loss 9.432529 Test Loss 8.577209156289802\n",
      "1441 Train Loss 9.430186 Test Loss 8.576297754105255\n",
      "1442 Train Loss 9.42635 Test Loss 8.573671048176596\n",
      "1443 Train Loss 9.424001 Test Loss 8.571323297829172\n",
      "1444 Train Loss 9.420843 Test Loss 8.568334484979738\n",
      "1445 Train Loss 9.418192 Test Loss 8.566806750732669\n",
      "1446 Train Loss 9.416961 Test Loss 8.563511622046718\n",
      "1447 Train Loss 9.415245 Test Loss 8.559046889541737\n",
      "1448 Train Loss 9.412645 Test Loss 8.55126111158463\n",
      "1449 Train Loss 9.409703 Test Loss 8.542322987439265\n",
      "1450 Train Loss 9.405963 Test Loss 8.536041114323275\n",
      "1451 Train Loss 9.402792 Test Loss 8.534992798238036\n",
      "1452 Train Loss 9.4006815 Test Loss 8.534167835953472\n",
      "1453 Train Loss 9.398657 Test Loss 8.536586771599467\n",
      "1454 Train Loss 9.3963175 Test Loss 8.539715034234776\n",
      "1455 Train Loss 9.393411 Test Loss 8.541911851859785\n",
      "1456 Train Loss 9.391173 Test Loss 8.54414932751775\n",
      "1457 Train Loss 9.388587 Test Loss 8.5424230557797\n",
      "1458 Train Loss 9.384616 Test Loss 8.541828751858006\n",
      "1459 Train Loss 9.382303 Test Loss 8.538663915675675\n",
      "1460 Train Loss 9.380637 Test Loss 8.540244508300464\n",
      "1461 Train Loss 9.379168 Test Loss 8.53922714267175\n",
      "1462 Train Loss 9.376888 Test Loss 8.537096375270401\n",
      "1463 Train Loss 9.3747225 Test Loss 8.537620361524894\n",
      "1464 Train Loss 9.372361 Test Loss 8.536512514685937\n",
      "1465 Train Loss 9.3704815 Test Loss 8.537682370329046\n",
      "1466 Train Loss 9.368511 Test Loss 8.537564347084107\n",
      "1467 Train Loss 9.366219 Test Loss 8.537362348170548\n",
      "1468 Train Loss 9.363221 Test Loss 8.536131739978655\n",
      "1469 Train Loss 9.362349 Test Loss 8.52563336112382\n",
      "1470 Train Loss 9.356657 Test Loss 8.530605563592264\n",
      "1471 Train Loss 9.353786 Test Loss 8.53001016192113\n",
      "1472 Train Loss 9.351015 Test Loss 8.529313261084859\n",
      "1473 Train Loss 9.347858 Test Loss 8.526411832937061\n",
      "1474 Train Loss 9.34589 Test Loss 8.526984135455404\n",
      "1475 Train Loss 9.343115 Test Loss 8.52556074679195\n",
      "1476 Train Loss 9.33998 Test Loss 8.523589238525535\n",
      "1477 Train Loss 9.336048 Test Loss 8.520681290006145\n",
      "1478 Train Loss 9.332123 Test Loss 8.518674471085255\n",
      "1479 Train Loss 9.328212 Test Loss 8.516199069322203\n",
      "1480 Train Loss 9.324293 Test Loss 8.516618876611679\n",
      "1481 Train Loss 9.318779 Test Loss 8.51734027630528\n",
      "1482 Train Loss 9.31413 Test Loss 8.521724494584555\n",
      "1483 Train Loss 9.310914 Test Loss 8.521380530524192\n",
      "1484 Train Loss 9.308444 Test Loss 8.524381883059547\n",
      "1485 Train Loss 9.306287 Test Loss 8.522478838926517\n",
      "1486 Train Loss 9.303801 Test Loss 8.51948530270681\n",
      "1487 Train Loss 9.302179 Test Loss 8.519870737524874\n",
      "1488 Train Loss 9.300612 Test Loss 8.52072342377845\n",
      "1489 Train Loss 9.29922 Test Loss 8.520999036965193\n",
      "1490 Train Loss 9.296538 Test Loss 8.521103694457434\n",
      "1491 Train Loss 9.294382 Test Loss 8.520204306307834\n",
      "1492 Train Loss 9.339392 Test Loss 8.517549873604732\n",
      "1493 Train Loss 9.293755 Test Loss 8.519860671204151\n",
      "1494 Train Loss 9.291775 Test Loss 8.518554790131523\n",
      "1495 Train Loss 9.290126 Test Loss 8.517113804139342\n",
      "1496 Train Loss 9.287853 Test Loss 8.515407079144277\n",
      "1497 Train Loss 9.28514 Test Loss 8.513612316114049\n",
      "1498 Train Loss 9.281202 Test Loss 8.513490297335132\n",
      "1499 Train Loss 9.277564 Test Loss 8.512930162711811\n",
      "1500 Train Loss 9.274188 Test Loss 8.512933820149259\n",
      "1501 Train Loss 9.274441 Test Loss 8.513965715573457\n",
      "1502 Train Loss 9.272363 Test Loss 8.513403168252466\n",
      "1503 Train Loss 9.2698345 Test Loss 8.515079970352902\n",
      "1504 Train Loss 9.267154 Test Loss 8.51616865335424\n",
      "1505 Train Loss 9.264202 Test Loss 8.515867733441011\n",
      "1506 Train Loss 9.2616825 Test Loss 8.51607476615133\n",
      "1507 Train Loss 9.25871 Test Loss 8.515648261309241\n",
      "1508 Train Loss 9.256251 Test Loss 8.514671605266404\n",
      "1509 Train Loss 9.254047 Test Loss 8.51294371266364\n",
      "1510 Train Loss 9.251186 Test Loss 8.509606772227984\n",
      "1511 Train Loss 9.248404 Test Loss 8.505278109911677\n",
      "1512 Train Loss 9.24596 Test Loss 8.499617260587153\n",
      "1513 Train Loss 9.244168 Test Loss 8.49820433638067\n",
      "1514 Train Loss 9.242497 Test Loss 8.497404518929875\n",
      "1515 Train Loss 9.240214 Test Loss 8.497996329392283\n",
      "1516 Train Loss 9.237082 Test Loss 8.49688554284022\n",
      "1517 Train Loss 9.234449 Test Loss 8.494927863915937\n",
      "1518 Train Loss 9.244867 Test Loss 8.498501381526266\n",
      "1519 Train Loss 9.233092 Test Loss 8.49566486333819\n",
      "1520 Train Loss 9.230668 Test Loss 8.493839396801604\n",
      "1521 Train Loss 9.227423 Test Loss 8.492511882584292\n",
      "1522 Train Loss 9.225777 Test Loss 8.490764272888141\n",
      "1523 Train Loss 9.222883 Test Loss 8.490540586562684\n",
      "1524 Train Loss 9.219792 Test Loss 8.487798569921079\n",
      "1525 Train Loss 9.216387 Test Loss 8.488329177463221\n",
      "1526 Train Loss 9.212023 Test Loss 8.487218853943364\n",
      "1527 Train Loss 9.208635 Test Loss 8.491196676742394\n",
      "1528 Train Loss 9.204452 Test Loss 8.486649736063788\n",
      "1529 Train Loss 9.1998825 Test Loss 8.477615372823312\n",
      "1530 Train Loss 9.197815 Test Loss 8.474916624837261\n",
      "1531 Train Loss 9.196247 Test Loss 8.470958432785148\n",
      "1532 Train Loss 9.195175 Test Loss 8.469718994424221\n",
      "1533 Train Loss 9.194277 Test Loss 8.469867618393769\n",
      "1534 Train Loss 9.193029 Test Loss 8.467353114618708\n",
      "1535 Train Loss 9.192131 Test Loss 8.465639667798103\n",
      "1536 Train Loss 9.191089 Test Loss 8.462677633567445\n",
      "1537 Train Loss 9.189816 Test Loss 8.461752987905967\n",
      "1538 Train Loss 9.187636 Test Loss 8.459700256998724\n",
      "1539 Train Loss 9.185145 Test Loss 8.461371229732457\n",
      "1540 Train Loss 9.183476 Test Loss 8.461730382679338\n",
      "1541 Train Loss 9.182488 Test Loss 8.460478121374063\n",
      "1542 Train Loss 9.181632 Test Loss 8.461307096140741\n",
      "1543 Train Loss 9.180404 Test Loss 8.45920954537284\n",
      "1544 Train Loss 9.187236 Test Loss 8.447673526665591\n",
      "1545 Train Loss 9.179786 Test Loss 8.456531895258326\n",
      "1546 Train Loss 9.178187 Test Loss 8.452742706997634\n",
      "1547 Train Loss 9.176955 Test Loss 8.449702728043176\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1548 Train Loss 9.174635 Test Loss 8.445455605899944\n",
      "1549 Train Loss 9.172651 Test Loss 8.43882206870393\n",
      "1550 Train Loss 9.169485 Test Loss 8.424114216972102\n",
      "1551 Train Loss 9.167734 Test Loss 8.411665256820118\n",
      "1552 Train Loss 9.164643 Test Loss 8.409360990004451\n",
      "1553 Train Loss 9.163194 Test Loss 8.39298568509144\n",
      "1554 Train Loss 9.161966 Test Loss 8.39909877220038\n",
      "1555 Train Loss 9.15796 Test Loss 8.392628572934479\n",
      "1556 Train Loss 9.153213 Test Loss 8.38692985800137\n",
      "1557 Train Loss 9.150781 Test Loss 8.370587038060085\n",
      "1558 Train Loss 9.153283 Test Loss 8.38365023476829\n",
      "1559 Train Loss 9.146297 Test Loss 8.37630562998458\n",
      "1560 Train Loss 9.137266 Test Loss 8.368638439853738\n",
      "1561 Train Loss 9.125798 Test Loss 8.362334838621207\n",
      "1562 Train Loss 9.170238 Test Loss 8.328581254978447\n",
      "1563 Train Loss 9.11892 Test Loss 8.35170826736213\n",
      "1564 Train Loss 9.109496 Test Loss 8.335530153404559\n",
      "1565 Train Loss 9.094287 Test Loss 8.334239967658398\n",
      "1566 Train Loss 9.086509 Test Loss 8.32801454776793\n",
      "1567 Train Loss 9.079109 Test Loss 8.317866248761636\n",
      "1568 Train Loss 9.072689 Test Loss 8.307135768661214\n",
      "1569 Train Loss 9.069071 Test Loss 8.310074082435731\n",
      "1570 Train Loss 9.065028 Test Loss 8.303729228088777\n",
      "1571 Train Loss 9.060734 Test Loss 8.297572227756426\n",
      "1572 Train Loss 9.056747 Test Loss 8.293533015189864\n",
      "1573 Train Loss 9.055195 Test Loss 8.293594198724033\n",
      "1574 Train Loss 9.048563 Test Loss 8.294767748432006\n",
      "1575 Train Loss 9.043717 Test Loss 8.295158488719288\n",
      "1576 Train Loss 9.039382 Test Loss 8.293092478204025\n",
      "1577 Train Loss 9.036371 Test Loss 8.290770716085827\n",
      "1578 Train Loss 9.033852 Test Loss 8.289790185862607\n",
      "1579 Train Loss 9.03084 Test Loss 8.291105957601877\n",
      "1580 Train Loss 9.028494 Test Loss 8.293077090946605\n",
      "1581 Train Loss 9.026916 Test Loss 8.296460984753786\n",
      "1582 Train Loss 9.025709 Test Loss 8.296568879781598\n",
      "1583 Train Loss 9.022571 Test Loss 8.295417148151401\n",
      "1584 Train Loss 9.01908 Test Loss 8.293801809746135\n",
      "1585 Train Loss 9.019086 Test Loss 8.284128432242149\n",
      "1586 Train Loss 9.016745 Test Loss 8.288676179785888\n",
      "1587 Train Loss 9.013281 Test Loss 8.28598464999706\n",
      "1588 Train Loss 9.010724 Test Loss 8.286568864802181\n",
      "1589 Train Loss 9.008222 Test Loss 8.286509978634886\n",
      "1590 Train Loss 9.006937 Test Loss 8.285593132349035\n",
      "1591 Train Loss 9.004761 Test Loss 8.282695893059\n",
      "1592 Train Loss 9.001625 Test Loss 8.274488932505914\n",
      "1593 Train Loss 8.998451 Test Loss 8.269732054426958\n",
      "1594 Train Loss 8.992824 Test Loss 8.257391974078754\n",
      "1595 Train Loss 8.986059 Test Loss 8.225326880857653\n",
      "1596 Train Loss 8.978348 Test Loss 8.230793279346015\n",
      "1597 Train Loss 8.980807 Test Loss 8.218851151864218\n",
      "1598 Train Loss 8.974511 Test Loss 8.224971077120463\n",
      "1599 Train Loss 8.968622 Test Loss 8.207731938166958\n",
      "1600 Train Loss 8.963893 Test Loss 8.212421725142777\n",
      "1601 Train Loss 8.959804 Test Loss 8.207979438257277\n",
      "1602 Train Loss 8.953414 Test Loss 8.19976963100581\n",
      "1603 Train Loss 8.949041 Test Loss 8.172754234267993\n",
      "1604 Train Loss 8.941371 Test Loss 8.174609654091672\n",
      "1605 Train Loss 8.935966 Test Loss 8.177283809600736\n",
      "1606 Train Loss 8.929136 Test Loss 8.164477968937407\n",
      "1607 Train Loss 8.92302 Test Loss 8.132373405581054\n",
      "1608 Train Loss 8.92027 Test Loss 8.141892498076366\n",
      "1609 Train Loss 8.951211 Test Loss 8.092541200788249\n",
      "1610 Train Loss 8.910486 Test Loss 8.119170597204754\n",
      "1611 Train Loss 8.931763 Test Loss 8.088696521538283\n",
      "1612 Train Loss 8.89824 Test Loss 8.105379457579408\n",
      "1613 Train Loss 8.895048 Test Loss 8.11171268285678\n",
      "1614 Train Loss 8.919716 Test Loss 8.086038295537218\n",
      "1615 Train Loss 8.878103 Test Loss 8.100577879675276\n",
      "1616 Train Loss 8.88686 Test Loss 8.105593230272506\n",
      "1617 Train Loss 8.866218 Test Loss 8.102731867393366\n",
      "1618 Train Loss 8.862535 Test Loss 8.09819597013906\n",
      "1619 Train Loss 8.85422 Test Loss 8.096662884455435\n",
      "1620 Train Loss 8.853995 Test Loss 8.100879203678941\n",
      "1621 Train Loss 8.846379 Test Loss 8.098531761460775\n",
      "1622 Train Loss 8.84116 Test Loss 8.090727600347105\n",
      "1623 Train Loss 8.830737 Test Loss 8.095862637729196\n",
      "1624 Train Loss 8.828021 Test Loss 8.094186454224243\n",
      "1625 Train Loss 8.822734 Test Loss 8.095264780863298\n",
      "1626 Train Loss 8.819869 Test Loss 8.096504008537128\n",
      "1627 Train Loss 8.816794 Test Loss 8.09715270032847\n",
      "1628 Train Loss 8.813653 Test Loss 8.09841277191196\n",
      "1629 Train Loss 8.809315 Test Loss 8.095517276017175\n",
      "1630 Train Loss 8.804026 Test Loss 8.09543546325884\n",
      "1631 Train Loss 8.796661 Test Loss 8.09245151538119\n",
      "1632 Train Loss 8.792695 Test Loss 8.091980631345944\n",
      "1633 Train Loss 8.790419 Test Loss 8.09270351443011\n",
      "1634 Train Loss 8.786693 Test Loss 8.08868189474794\n",
      "1635 Train Loss 8.784256 Test Loss 8.08969491109397\n",
      "1636 Train Loss 8.78184 Test Loss 8.091730447287212\n",
      "1637 Train Loss 8.780095 Test Loss 8.091765311067302\n",
      "1638 Train Loss 8.783574 Test Loss 8.081477807249446\n",
      "1639 Train Loss 8.779082 Test Loss 8.08840059013392\n",
      "1640 Train Loss 8.77728 Test Loss 8.088198464157818\n",
      "1641 Train Loss 8.77375 Test Loss 8.085210806760708\n",
      "1642 Train Loss 8.771693 Test Loss 8.08344491838592\n",
      "1643 Train Loss 8.768558 Test Loss 8.081478511853307\n",
      "1644 Train Loss 8.768006 Test Loss 8.074186423780189\n",
      "1645 Train Loss 8.765231 Test Loss 8.077470323842947\n",
      "1646 Train Loss 8.761759 Test Loss 8.070981009583795\n",
      "1647 Train Loss 8.756823 Test Loss 8.067197310971602\n",
      "1648 Train Loss 8.752483 Test Loss 8.061876797936291\n",
      "1649 Train Loss 8.747991 Test Loss 8.059300594688702\n",
      "1650 Train Loss 8.743149 Test Loss 8.057296481647104\n",
      "1651 Train Loss 8.738316 Test Loss 8.05752034180406\n",
      "1652 Train Loss 8.73196 Test Loss 8.05559821167123\n",
      "1653 Train Loss 8.728397 Test Loss 8.05668830184281\n",
      "1654 Train Loss 8.724168 Test Loss 8.05829418945987\n",
      "1655 Train Loss 8.720718 Test Loss 8.057395697717913\n",
      "1656 Train Loss 8.716541 Test Loss 8.053485331957832\n",
      "1657 Train Loss 8.721409 Test Loss 8.05299511723926\n",
      "1658 Train Loss 8.714514 Test Loss 8.053205925061096\n",
      "1659 Train Loss 8.7113085 Test Loss 8.050786199824799\n",
      "1660 Train Loss 8.708633 Test Loss 8.049268518695763\n",
      "1661 Train Loss 8.705374 Test Loss 8.050722235236497\n",
      "1662 Train Loss 8.700821 Test Loss 8.053474624650736\n",
      "1663 Train Loss 8.696286 Test Loss 8.055748080829778\n",
      "1664 Train Loss 8.690536 Test Loss 8.056641651714504\n",
      "1665 Train Loss 8.682661 Test Loss 8.059599849208315\n",
      "1666 Train Loss 8.676026 Test Loss 8.057162009467092\n",
      "1667 Train Loss 8.669928 Test Loss 8.054517010480634\n",
      "1668 Train Loss 8.664529 Test Loss 8.054343697207088\n",
      "1669 Train Loss 8.659831 Test Loss 8.056745101855116\n",
      "1670 Train Loss 8.655634 Test Loss 8.058539701948009\n",
      "1671 Train Loss 8.651327 Test Loss 8.059485301074071\n",
      "1672 Train Loss 8.648892 Test Loss 8.057906871153612\n",
      "1673 Train Loss 8.64638 Test Loss 8.056267580600725\n",
      "1674 Train Loss 8.642994 Test Loss 8.053846616580017\n",
      "1675 Train Loss 8.642042 Test Loss 8.051327813618316\n",
      "1676 Train Loss 8.638825 Test Loss 8.049621082202147\n",
      "1677 Train Loss 8.636875 Test Loss 8.05255057614352\n",
      "1678 Train Loss 8.633699 Test Loss 8.057235051488822\n",
      "1679 Train Loss 8.62898 Test Loss 8.063154744834495\n",
      "1680 Train Loss 8.625784 Test Loss 8.065444206712597\n",
      "1681 Train Loss 8.623471 Test Loss 8.06632148021234\n",
      "1682 Train Loss 8.622877 Test Loss 8.066407112493323\n",
      "1683 Train Loss 8.619868 Test Loss 8.063840896594959\n",
      "1684 Train Loss 8.617564 Test Loss 8.06245640280471\n",
      "1685 Train Loss 8.614589 Test Loss 8.060691119406014\n",
      "1686 Train Loss 8.613197 Test Loss 8.058746834443692\n",
      "1687 Train Loss 8.6118145 Test Loss 8.05902400668426\n",
      "1688 Train Loss 8.610638 Test Loss 8.059089399018742\n",
      "1689 Train Loss 8.609181 Test Loss 8.058840766349677\n",
      "1690 Train Loss 8.607419 Test Loss 8.057535701355695\n",
      "1691 Train Loss 8.605928 Test Loss 8.05485587264132\n",
      "1692 Train Loss 8.60435 Test Loss 8.053748755050572\n",
      "1693 Train Loss 8.602559 Test Loss 8.051657343147319\n",
      "1694 Train Loss 8.600898 Test Loss 8.045910211606307\n",
      "1695 Train Loss 8.599672 Test Loss 8.04565522537933\n",
      "1696 Train Loss 8.598537 Test Loss 8.044012165788207\n",
      "1697 Train Loss 8.596567 Test Loss 8.042367470866367\n",
      "1698 Train Loss 8.595212 Test Loss 8.043020802741985\n",
      "1699 Train Loss 8.593573 Test Loss 8.042709460122591\n",
      "1700 Train Loss 8.592018 Test Loss 8.042968819045079\n",
      "1701 Train Loss 8.59062 Test Loss 8.044931016405133\n",
      "1702 Train Loss 8.58871 Test Loss 8.046618217866127\n",
      "1703 Train Loss 8.586818 Test Loss 8.047391249426582\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1704 Train Loss 8.584406 Test Loss 8.046400870201746\n",
      "1705 Train Loss 8.5812645 Test Loss 8.043489785667765\n",
      "1706 Train Loss 8.579025 Test Loss 8.04047970140183\n",
      "1707 Train Loss 8.575571 Test Loss 8.036957394168478\n",
      "1708 Train Loss 8.583448 Test Loss 8.037997132040843\n",
      "1709 Train Loss 8.574152 Test Loss 8.037036204737403\n",
      "1710 Train Loss 8.571787 Test Loss 8.038035971360623\n",
      "1711 Train Loss 8.567379 Test Loss 8.034190718520826\n",
      "1712 Train Loss 8.564042 Test Loss 8.034921439109814\n",
      "1713 Train Loss 8.561444 Test Loss 8.03442659933135\n",
      "1714 Train Loss 8.559813 Test Loss 8.035999108980832\n",
      "1715 Train Loss 8.558739 Test Loss 8.03413254999695\n",
      "1716 Train Loss 8.557717 Test Loss 8.03154227131315\n",
      "1717 Train Loss 8.556305 Test Loss 8.028788156674423\n",
      "1718 Train Loss 8.554029 Test Loss 8.02400197960461\n",
      "1719 Train Loss 8.551967 Test Loss 8.021464509123625\n",
      "1720 Train Loss 8.548716 Test Loss 8.018848876764721\n",
      "1721 Train Loss 8.54522 Test Loss 8.016973492147525\n",
      "1722 Train Loss 8.541243 Test Loss 8.019615242284857\n",
      "1723 Train Loss 8.538183 Test Loss 8.015705727165658\n",
      "1724 Train Loss 8.53576 Test Loss 8.016079955650524\n",
      "1725 Train Loss 8.533012 Test Loss 8.01120256338607\n",
      "1726 Train Loss 8.53069 Test Loss 7.99496365032914\n",
      "1727 Train Loss 8.529209 Test Loss 8.000380452245818\n",
      "1728 Train Loss 8.52671 Test Loss 7.999730106015494\n",
      "1729 Train Loss 8.523016 Test Loss 7.99414258357524\n",
      "1730 Train Loss 8.518612 Test Loss 7.993571233713655\n",
      "1731 Train Loss 8.515825 Test Loss 7.987296814614957\n",
      "1732 Train Loss 8.514198 Test Loss 7.989786083042073\n",
      "1733 Train Loss 8.51214 Test Loss 7.9866900233746465\n",
      "1734 Train Loss 8.511074 Test Loss 7.985411446857698\n",
      "1735 Train Loss 8.510065 Test Loss 7.982961090202651\n",
      "1736 Train Loss 8.509224 Test Loss 7.983761044630099\n",
      "1737 Train Loss 8.508588 Test Loss 7.983832488551039\n",
      "1738 Train Loss 8.507734 Test Loss 7.983567243607147\n",
      "1739 Train Loss 8.5065 Test Loss 7.98255939986013\n",
      "1740 Train Loss 8.504943 Test Loss 7.9795647815412405\n",
      "1741 Train Loss 8.502834 Test Loss 7.979009676731046\n",
      "1742 Train Loss 8.502813 Test Loss 7.9800675868556175\n",
      "1743 Train Loss 8.501985 Test Loss 7.97952180827983\n",
      "1744 Train Loss 8.500302 Test Loss 7.980536099909139\n",
      "1745 Train Loss 8.499046 Test Loss 7.980243130211831\n",
      "1746 Train Loss 8.4975815 Test Loss 7.9810626377620215\n",
      "1747 Train Loss 8.496216 Test Loss 7.982824344050155\n",
      "1748 Train Loss 8.492478 Test Loss 7.985721514254061\n",
      "1749 Train Loss 8.48944 Test Loss 7.98799348176141\n",
      "1750 Train Loss 8.486076 Test Loss 7.991252207353763\n",
      "1751 Train Loss 8.48489 Test Loss 7.991377915354994\n",
      "1752 Train Loss 8.484717 Test Loss 7.99193550863271\n",
      "1753 Train Loss 8.484269 Test Loss 7.991644863105673\n",
      "1754 Train Loss 8.483685 Test Loss 7.990130007792159\n",
      "1755 Train Loss 8.482842 Test Loss 7.989964742498597\n",
      "1756 Train Loss 8.481449 Test Loss 7.9887969159848895\n",
      "1757 Train Loss 8.479988 Test Loss 7.987254315450002\n",
      "1758 Train Loss 8.478338 Test Loss 7.98586802863325\n",
      "1759 Train Loss 8.476926 Test Loss 7.984162374727913\n",
      "1760 Train Loss 8.475567 Test Loss 7.985224240391623\n",
      "1761 Train Loss 8.474286 Test Loss 7.984644644790552\n",
      "1762 Train Loss 8.473048 Test Loss 7.984078566979845\n",
      "1763 Train Loss 8.471376 Test Loss 7.9857957723759565\n",
      "1764 Train Loss 8.470336 Test Loss 7.984058311131355\n",
      "1765 Train Loss 8.469336 Test Loss 7.983926789185257\n",
      "1766 Train Loss 8.46828 Test Loss 7.98423773713988\n",
      "1767 Train Loss 8.466915 Test Loss 7.983368168377291\n",
      "1768 Train Loss 8.465366 Test Loss 7.9815620440134065\n",
      "1769 Train Loss 8.464024 Test Loss 7.979313947320772\n",
      "1770 Train Loss 8.462864 Test Loss 7.976894191650408\n",
      "1771 Train Loss 8.461929 Test Loss 7.975928264456088\n",
      "1772 Train Loss 8.461067 Test Loss 7.975764133820826\n",
      "1773 Train Loss 8.460076 Test Loss 7.976090385525106\n",
      "1774 Train Loss 8.459306 Test Loss 7.976759989920011\n",
      "1775 Train Loss 8.457342 Test Loss 7.980214444190065\n",
      "1776 Train Loss 8.455461 Test Loss 7.9789998296971945\n",
      "1777 Train Loss 8.453375 Test Loss 7.979024723155178\n",
      "1778 Train Loss 8.450692 Test Loss 7.9775320089044985\n",
      "1779 Train Loss 8.448317 Test Loss 7.975885548225201\n",
      "1780 Train Loss 8.446055 Test Loss 7.9730762711098535\n",
      "1781 Train Loss 8.444443 Test Loss 7.971003668522666\n",
      "1782 Train Loss 8.443119 Test Loss 7.969870583970311\n",
      "1783 Train Loss 8.441978 Test Loss 7.9703095697839395\n",
      "1784 Train Loss 8.44093 Test Loss 7.971125097517327\n",
      "1785 Train Loss 8.440083 Test Loss 7.971030182856287\n",
      "1786 Train Loss 8.439179 Test Loss 7.9712834944761894\n",
      "1787 Train Loss 8.442939 Test Loss 7.972020237011972\n",
      "1788 Train Loss 8.438135 Test Loss 7.971501541275183\n",
      "1789 Train Loss 8.436287 Test Loss 7.968850164793968\n",
      "1790 Train Loss 8.437578 Test Loss 7.962717480739627\n",
      "1791 Train Loss 8.434205 Test Loss 7.96608094557331\n",
      "1792 Train Loss 8.431452 Test Loss 7.965905833715945\n",
      "1793 Train Loss 8.430482 Test Loss 7.9645534908523565\n",
      "1794 Train Loss 8.429345 Test Loss 7.964186366208526\n",
      "1795 Train Loss 8.4285755 Test Loss 7.965442083353593\n",
      "1796 Train Loss 8.427464 Test Loss 7.965665876851571\n",
      "1797 Train Loss 8.426334 Test Loss 7.965249842945417\n",
      "1798 Train Loss 8.424189 Test Loss 7.9633968097611305\n",
      "1799 Train Loss 8.421546 Test Loss 7.962027325260347\n",
      "1800 Train Loss 8.419461 Test Loss 7.958065326899236\n",
      "1801 Train Loss 8.41655 Test Loss 7.958487551898296\n",
      "1802 Train Loss 8.414742 Test Loss 7.957866456189132\n",
      "1803 Train Loss 8.413314 Test Loss 7.957181826549889\n",
      "1804 Train Loss 8.412894 Test Loss 7.955963443178076\n",
      "1805 Train Loss 8.412213 Test Loss 7.956255091734588\n",
      "1806 Train Loss 8.411623 Test Loss 7.956737317308709\n",
      "1807 Train Loss 8.410918 Test Loss 7.956821154697172\n",
      "1808 Train Loss 8.409835 Test Loss 7.958293994860898\n",
      "1809 Train Loss 8.409077 Test Loss 7.9592462917349245\n",
      "1810 Train Loss 8.408375 Test Loss 7.961528564348352\n",
      "1811 Train Loss 8.407942 Test Loss 7.962562347883703\n",
      "1812 Train Loss 8.407506 Test Loss 7.96392338279637\n",
      "1813 Train Loss 8.407144 Test Loss 7.9641175559485715\n",
      "1814 Train Loss 8.406882 Test Loss 7.963578578614444\n",
      "1815 Train Loss 8.406614 Test Loss 7.963241242945465\n",
      "1816 Train Loss 8.406154 Test Loss 7.962687131552713\n",
      "1817 Train Loss 8.40569 Test Loss 7.962633176555284\n",
      "1818 Train Loss 8.404702 Test Loss 7.962845604586422\n",
      "1819 Train Loss 8.403965 Test Loss 7.963664135505639\n",
      "1820 Train Loss 8.403197 Test Loss 7.964632907996859\n",
      "1821 Train Loss 8.401012 Test Loss 7.966449842536109\n",
      "1822 Train Loss 8.399317 Test Loss 7.964786295562366\n",
      "1823 Train Loss 8.3957405 Test Loss 7.967247034928424\n",
      "1824 Train Loss 8.41289 Test Loss 7.971554774199308\n",
      "1825 Train Loss 8.392945 Test Loss 7.968754006775815\n",
      "1826 Train Loss 8.390432 Test Loss 7.975905713739523\n",
      "1827 Train Loss 8.388699 Test Loss 7.9725172611837145\n",
      "1828 Train Loss 8.386078 Test Loss 7.972194003193047\n",
      "1829 Train Loss 8.383122 Test Loss 7.9713642318277484\n",
      "1830 Train Loss 8.3821945 Test Loss 7.973661869749889\n",
      "1831 Train Loss 8.38028 Test Loss 7.972119691349287\n",
      "1832 Train Loss 8.379079 Test Loss 7.970814703517684\n",
      "1833 Train Loss 8.377949 Test Loss 7.969534101523561\n",
      "1834 Train Loss 8.378293 Test Loss 7.968115629689014\n",
      "1835 Train Loss 8.377485 Test Loss 7.968906394902112\n",
      "1836 Train Loss 8.376999 Test Loss 7.967766545246211\n",
      "1837 Train Loss 8.375706 Test Loss 7.967544712665239\n",
      "1838 Train Loss 8.374832 Test Loss 7.967624108371795\n",
      "1839 Train Loss 8.373733 Test Loss 7.968148392139193\n",
      "1840 Train Loss 8.372152 Test Loss 7.966990896790409\n",
      "1841 Train Loss 8.370426 Test Loss 7.964844251485972\n",
      "1842 Train Loss 8.368713 Test Loss 7.963884401126519\n",
      "1843 Train Loss 8.367413 Test Loss 7.962613711631342\n",
      "1844 Train Loss 8.365747 Test Loss 7.9638546776870625\n",
      "1845 Train Loss 8.363989 Test Loss 7.965189010396664\n",
      "1846 Train Loss 8.362878 Test Loss 7.966837833182361\n",
      "1847 Train Loss 8.361915 Test Loss 7.966751010845312\n",
      "1848 Train Loss 8.360704 Test Loss 7.9663598579827966\n",
      "1849 Train Loss 8.359077 Test Loss 7.967080610071223\n",
      "1850 Train Loss 8.35806 Test Loss 7.967949522403018\n",
      "1851 Train Loss 8.357004 Test Loss 7.969761885183419\n",
      "1852 Train Loss 8.356179 Test Loss 7.970697370307664\n",
      "1853 Train Loss 8.355239 Test Loss 7.971701641139066\n",
      "1854 Train Loss 8.354283 Test Loss 7.972347405155898\n",
      "1855 Train Loss 8.353397 Test Loss 7.972248480897101\n",
      "1856 Train Loss 8.351804 Test Loss 7.9723275970659175\n",
      "1857 Train Loss 8.350791 Test Loss 7.972813323825181\n",
      "1858 Train Loss 8.350072 Test Loss 7.973576148622442\n",
      "1859 Train Loss 8.352338 Test Loss 7.97569909323602\n",
      "1860 Train Loss 8.349816 Test Loss 7.974039572089209\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1861 Train Loss 8.349397 Test Loss 7.975193652672649\n",
      "1862 Train Loss 8.3489275 Test Loss 7.975700835798246\n",
      "1863 Train Loss 8.348514 Test Loss 7.975827180948137\n",
      "1864 Train Loss 8.3479595 Test Loss 7.9765122157290484\n",
      "1865 Train Loss 8.347224 Test Loss 7.9766889634095595\n",
      "1866 Train Loss 8.346278 Test Loss 7.9771987386968375\n",
      "1867 Train Loss 8.344778 Test Loss 7.976943717255557\n",
      "1868 Train Loss 8.342304 Test Loss 7.97724972171639\n",
      "1869 Train Loss 8.340214 Test Loss 7.97423904745609\n",
      "1870 Train Loss 8.338004 Test Loss 7.9757429385825285\n",
      "1871 Train Loss 8.336031 Test Loss 7.973783108041702\n",
      "1872 Train Loss 8.3347645 Test Loss 7.974171116937434\n",
      "1873 Train Loss 8.333462 Test Loss 7.972903834021347\n",
      "1874 Train Loss 8.332048 Test Loss 7.973331626497655\n",
      "1875 Train Loss 8.330329 Test Loss 7.970344554385666\n",
      "1876 Train Loss 8.32805 Test Loss 7.9673862418041095\n",
      "1877 Train Loss 8.327049 Test Loss 7.961845073030934\n",
      "1878 Train Loss 8.32398 Test Loss 7.9612327668406495\n",
      "1879 Train Loss 8.3225565 Test Loss 7.961512455424075\n",
      "1880 Train Loss 8.321243 Test Loss 7.961987947596209\n",
      "1881 Train Loss 8.3200865 Test Loss 7.962486688363763\n",
      "1882 Train Loss 8.318821 Test Loss 7.961637863285436\n",
      "1883 Train Loss 8.317677 Test Loss 7.961617594002195\n",
      "1884 Train Loss 8.316587 Test Loss 7.961525352353094\n",
      "1885 Train Loss 8.315283 Test Loss 7.961135615820328\n",
      "1886 Train Loss 8.313737 Test Loss 7.959754080973853\n",
      "1887 Train Loss 8.312449 Test Loss 7.958467281886708\n",
      "1888 Train Loss 8.311261 Test Loss 7.956022196652672\n",
      "1889 Train Loss 8.322624 Test Loss 7.960530732570804\n",
      "1890 Train Loss 8.310353 Test Loss 7.956981690707887\n",
      "1891 Train Loss 8.309317 Test Loss 7.957440450009643\n",
      "1892 Train Loss 8.307819 Test Loss 7.957263932177168\n",
      "1893 Train Loss 8.306369 Test Loss 7.956936444726339\n",
      "1894 Train Loss 8.305366 Test Loss 7.957690471640892\n",
      "1895 Train Loss 8.304328 Test Loss 7.957717998300962\n",
      "1896 Train Loss 8.30356 Test Loss 7.958945000554453\n",
      "1897 Train Loss 8.302777 Test Loss 7.959756432871262\n",
      "1898 Train Loss 8.301765 Test Loss 7.960444514351789\n",
      "1899 Train Loss 8.300862 Test Loss 7.961949788511056\n",
      "1900 Train Loss 8.299946 Test Loss 7.96267693579333\n",
      "1901 Train Loss 8.299273 Test Loss 7.96442031069061\n",
      "1902 Train Loss 8.299151 Test Loss 7.96411131107244\n",
      "1903 Train Loss 8.298121 Test Loss 7.964989787439514\n",
      "1904 Train Loss 8.297334 Test Loss 7.966179851941298\n",
      "1905 Train Loss 8.296016 Test Loss 7.968160276719236\n",
      "1906 Train Loss 8.294793 Test Loss 7.970169896702082\n",
      "1907 Train Loss 8.293995 Test Loss 7.9722629205642015\n",
      "1908 Train Loss 8.293357 Test Loss 7.972385488832753\n",
      "1909 Train Loss 8.292715 Test Loss 7.972984246586812\n",
      "1910 Train Loss 8.291846 Test Loss 7.973716981243684\n",
      "1911 Train Loss 8.290878 Test Loss 7.974228517148547\n",
      "1912 Train Loss 8.289864 Test Loss 7.975194802247546\n",
      "1913 Train Loss 8.289049 Test Loss 7.974689825730267\n",
      "1914 Train Loss 8.290693 Test Loss 7.97919992741181\n",
      "1915 Train Loss 8.28842 Test Loss 7.976227141068486\n",
      "1916 Train Loss 8.28738 Test Loss 7.977131211370253\n",
      "1917 Train Loss 8.285996 Test Loss 7.980840899631552\n",
      "1918 Train Loss 8.285632 Test Loss 7.977440084481905\n",
      "1919 Train Loss 8.28399 Test Loss 7.977536287128768\n",
      "1920 Train Loss 8.283381 Test Loss 7.9782623017843886\n",
      "1921 Train Loss 8.282386 Test Loss 7.978931025476793\n",
      "1922 Train Loss 8.280716 Test Loss 7.978846679633657\n",
      "1923 Train Loss 8.27839 Test Loss 7.978196532835109\n",
      "1924 Train Loss 8.275041 Test Loss 7.97602798762338\n",
      "1925 Train Loss 8.270873 Test Loss 7.972438458316702\n",
      "1926 Train Loss 8.268037 Test Loss 7.9695037990525295\n",
      "1927 Train Loss 8.266416 Test Loss 7.969084537423337\n",
      "1928 Train Loss 8.264283 Test Loss 7.970666206003531\n",
      "1929 Train Loss 8.2616825 Test Loss 7.97199876520727\n",
      "1930 Train Loss 8.259553 Test Loss 7.9731539303602155\n",
      "1931 Train Loss 8.257627 Test Loss 7.975036691080587\n",
      "1932 Train Loss 8.256175 Test Loss 7.975311000743217\n",
      "1933 Train Loss 8.254763 Test Loss 7.975102645909536\n",
      "1934 Train Loss 8.253306 Test Loss 7.974992018650117\n",
      "1935 Train Loss 8.251602 Test Loss 7.975308924772914\n",
      "1936 Train Loss 8.250418 Test Loss 7.975472775766531\n",
      "1937 Train Loss 8.248496 Test Loss 7.977544423982335\n",
      "1938 Train Loss 8.245387 Test Loss 7.976704970072198\n",
      "1939 Train Loss 8.242886 Test Loss 7.979027564468378\n",
      "1940 Train Loss 8.240209 Test Loss 7.982231692842842\n",
      "1941 Train Loss 8.238138 Test Loss 7.986717216095038\n",
      "1942 Train Loss 8.236704 Test Loss 7.986984980516339\n",
      "1943 Train Loss 8.235625 Test Loss 7.98791943150042\n",
      "1944 Train Loss 8.234833 Test Loss 7.987044779594494\n",
      "1945 Train Loss 8.234341 Test Loss 7.98859090652385\n",
      "1946 Train Loss 8.233746 Test Loss 7.988278885197433\n",
      "1947 Train Loss 8.232562 Test Loss 7.989274356582293\n",
      "1948 Train Loss 8.231908 Test Loss 7.988101920563452\n",
      "1949 Train Loss 8.231399 Test Loss 7.988500103938807\n",
      "1950 Train Loss 8.230942 Test Loss 7.987934842458631\n",
      "1951 Train Loss 8.230814 Test Loss 7.98575395515018\n",
      "1952 Train Loss 8.229877 Test Loss 7.9873068903675115\n",
      "1953 Train Loss 8.229186 Test Loss 7.9884115121545936\n",
      "1954 Train Loss 8.228439 Test Loss 7.9891973266741605\n",
      "1955 Train Loss 8.227695 Test Loss 7.989317431076454\n",
      "1956 Train Loss 8.227587 Test Loss 7.988542180603559\n",
      "1957 Train Loss 8.22615 Test Loss 7.98813842735147\n",
      "1958 Train Loss 8.225594 Test Loss 7.987243934557962\n",
      "1959 Train Loss 8.225012 Test Loss 7.9856032751351\n",
      "1960 Train Loss 8.224567 Test Loss 7.984316291596583\n",
      "1961 Train Loss 8.223882 Test Loss 7.9827655275317815\n",
      "1962 Train Loss 8.223402 Test Loss 7.981784918755966\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-6-1e79bffbb986>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0mstart_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPINN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mPINN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstate_dict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'_'\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m'.pt'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                     loss, flat_grad, t, ls_func_evals = _strong_wolfe(\n\u001b[0;32m--> 426\u001b[0;31m                         obj_func, x_init, t, d, loss, flat_grad, gtd)\n\u001b[0m\u001b[1;32m    427\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0mopt_cond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflat_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mtolerance_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36m_strong_wolfe\u001b[0;34m(obj_func, x, t, d, f, g, gtd, c1, c2, tolerance_change, max_ls)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# evaluate objective and gradient using initial step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mf_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mls_func_evals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mgtd_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mobj_func\u001b[0;34m(x, t, d)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m                     \u001b[0;32mdef\u001b[0m \u001b[0mobj_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m                         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_directional_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                     loss, flat_grad, t, ls_func_evals = _strong_wolfe(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36m_directional_evaluate\u001b[0;34m(self, closure, x, t, d)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_directional_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0mflat_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gather_flat_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-5-219cce0bb599>\u001b[0m in \u001b[0;36mclosure\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    132\u001b[0m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Train Loss\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\"Test Loss\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtest_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0miter\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_reps = 1\n",
    "\n",
    "train_loss_full = []\n",
    "test_loss_full = []\n",
    "beta_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(reps)\n",
    "    N_u = 200 #Total number of data points for 'u'\n",
    "    N_f = 10000 #Total number of collocation points \n",
    "    \n",
    "    X_f_train_np_array, X_u_train_np_array, u_train_np_array = trainingdata(N_u,N_f,reps*43)\n",
    "        \n",
    "    X_f_train = torch.from_numpy(X_f_train_np_array).float().to(device)\n",
    "    X_u_train = torch.from_numpy(X_u_train_np_array).float().to(device)\n",
    "    u_train = torch.from_numpy(u_train_np_array).float().to(device)\n",
    "        \n",
    "    u = torch.from_numpy(u_true).float().to(device)\n",
    "    f_hat = torch.zeros(X_f_train.shape[0],1).to(device)\n",
    "        \n",
    "\n",
    "    X_u_test_tensor = torch.from_numpy(X_u_test).float().to(device)\n",
    "    'Convert to tensor and send to GPU'\n",
    "\n",
    "\n",
    "    layers = np.array([2,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "\n",
    "    PINN = Sequentialmodel(layers)\n",
    "       \n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "\n",
    "    'L-BFGS Optimizer'\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=1, \n",
    "                                  max_iter = 3000, \n",
    "                                  max_eval = None, \n",
    "                                  tolerance_grad = -1, \n",
    "                                  tolerance_change = -1, \n",
    "                                  history_size = 100, \n",
    "                                  line_search_fn = 'strong_wolfe')\n",
    "\n",
    "    start_time = time.time()\n",
    "\n",
    "    optimizer.step(PINN.closure)\n",
    "    \n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(PINN.train_loss)\n",
    "    test_loss_full.append(PINN.test_loss)\n",
    "    elapsed_time[reps] = time.time() - start_time\n",
    "    beta_full.append(PINN.beta_val)\n",
    "    \n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_loss\": test_loss_full,\"Time\": elapsed_time, \"beta\": beta_full, \"label\": label}\n",
    "savemat(label+'.mat', mdic) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f423865ef50>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "u_pred = PINN.test(X_u_test_tensor)\n",
    "\n",
    "cmap = plt.cm.rainbow\n",
    "plt.imshow(np.transpose(u_pred.reshape(100,256)),cmap = cmap,aspect = 0.2,vmin=-10,vmax=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-7.1225796"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.min(u_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f42386ea710>"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cmap = plt.cm.rainbow\n",
    "plt.imshow(np.transpose(u_true.reshape(100,256)),cmap = cmap,aspect = 0.2,vmin=-10,vmax=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x7f42385dd550>"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "cmap = plt.cm.rainbow\n",
    "plt.imshow(np.transpose(np.abs(u_pred - u_true).reshape(100,256)),cmap = cmap,aspect = 0.2,vmin=0,vmax=20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.027523394560718\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-2c9903751109>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_loss_full\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtest_loss_full\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "a = 0 \n",
    "for i in range(10):\n",
    "    print(test_loss_full[i][-1])\n",
    "    a = a + test_loss_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
