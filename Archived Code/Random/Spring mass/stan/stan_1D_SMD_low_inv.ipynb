{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8byrnUmNKGR",
    "outputId": "510f67c6-b225-41ba-e78a-d0a6e54ded85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uoNYpDzlNKGV"
   },
   "outputs": [],
   "source": [
    "tstart= 0 \n",
    "tstop= 60 \n",
    "\n",
    "increment = 0.1 \n",
    "# Initial condition\n",
    "x_init= [0,0] \n",
    "\n",
    "t = np.arange(tstart,tstop+1,increment)\n",
    "\n",
    "c = 2 # Damping constant \n",
    "k = 0.1 # Stiffness of the spring\n",
    "m = 5 # Mass \n",
    "F0 = 1\n",
    "\n",
    "def mydiff(x, t):    \n",
    "    F =F0\n",
    "    \n",
    "    dx1dt = x[1] \n",
    "    dx2dt = (F -c*x[1] -k*x[0])/m\n",
    "    dxdt= [dx1dt, dx2dt] \n",
    "    \n",
    "    return dxdt \n",
    "\n",
    "x_full_sol = odeint(mydiff, x_init, t) \n",
    "\n",
    "x_sol = x_full_sol[:,0]\n",
    "v_sol = x_full_sol[:,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BR02v-fkNKGV"
   },
   "outputs": [],
   "source": [
    "level = \"low\"\n",
    "label = \"1D_SMD_stan_\" + level\n",
    "\n",
    "loss_thresh = 0.005\n",
    "\n",
    "t_test = t.reshape(-1,1)\n",
    "t_test_tensor = torch.from_numpy(t_test).float().to(device)\n",
    "\n",
    "# bc1_t = t[0].reshape(-1,1)\n",
    "# bc1_x = x_sol[0].reshape(-1,1)\n",
    "# t_bc1_train = torch.from_numpy(bc1_t).float().to(device)\n",
    "# x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "\n",
    "bc1_t = t.reshape(-1,1)\n",
    "bc1_x = x_sol.reshape(-1,1)\n",
    "t_bc1_train = torch.from_numpy(bc1_t).float().to(device)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "\n",
    "\n",
    "bc2_t = t[0].reshape(-1,1)\n",
    "t_bc2_train = torch.from_numpy(bc2_t).float().to(device)\n",
    "bc2_val = v_sol[0].reshape(-1,1)\n",
    "bc2_val =torch.from_numpy(bc2_val).float().to(device)\n",
    "\n",
    "t_test = t.reshape(-1,1)\n",
    "t_test_tensor = torch.from_numpy(t_test).float().to(device)\n",
    "x_true = x_sol\n",
    "x_true_norm = np.linalg.norm(x_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(t[0]) \n",
    "ub = np.array(t[-1]) \n",
    "\n",
    "#torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SyyktBKBXRo1"
   },
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    \n",
    "    t01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=t01,random_state =seed)\n",
    "\n",
    "    t_coll = lb + (ub-lb)*sampling(N_f)\n",
    "    # t_coll = np.vstack((t_coll,)) # append training points to collocation points \n",
    "\n",
    "    return t_coll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "o1b21zLnNKGW"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "        \n",
    "        self.beta = Parameter(0.0*torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "        \n",
    "        self.k = Parameter(torch.tensor(0.0))\n",
    "        self.k.requiresGrad = True\n",
    "        self.c = Parameter(torch.tensor(0.0))\n",
    "        self.c.requiresGrad = True\n",
    "        self.m = Parameter(torch.tensor(0.0))\n",
    "        self.m.requiresGrad = True\n",
    "        \n",
    "    'forward pass'\n",
    "    def forward(self,t):\n",
    "        if torch.is_tensor(t) != True:         \n",
    "            t = torch.from_numpy(t)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        t = 2.0*(t - l_b)/(u_b - l_b) - 1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = t.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z) + self.beta[:,i]*z*self.activation(z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,t,x):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(t), x)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,t_bc2,bc2_val):\n",
    "        g = t_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        x = self.forward(g)    \n",
    "            \n",
    "        x_t = autograd.grad(x,g,torch.ones([t_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dx_dt = x_t[:,[0]]\n",
    "        \n",
    "        bc2 = dx_dt\n",
    "        \n",
    "        loss_bc2= self.loss_function(bc2,bc2_val)\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self,t_coll,f_hat):\n",
    "             \n",
    "        g = t_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        x = self.forward(g) \n",
    "\n",
    "        x_t = autograd.grad(x,g,torch.ones([t_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        x_tt = autograd.grad(x_t,g,torch.ones(t_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dx_dt = x_t[:,[0]]\n",
    "        \n",
    "        dx2_d2t = x_tt[:,[0]]\n",
    "        \n",
    "        f = self.m*dx2_d2t + self.c*dx_dt + self.k*x - F0\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,t_bc1,x_bc1,t_bc2,bc2_val,t_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(t_bc1,x_bc1)\n",
    "        loss_bc2 = self.loss_BC2(t_bc2,bc2_val)\n",
    "        loss_f = self.loss_PDE(t_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self):\n",
    "        x_pred = self.forward(t_test_tensor)\n",
    "        x_pred = x_pred.cpu().detach().numpy()\n",
    "\n",
    "        return x_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        x_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(x_pred.reshape(-1,1) - x_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(x_pred.reshape(-1,1) - x_true.reshape(-1,1),2)/x_true_norm\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fLY2mT5BOgjD"
   },
   "outputs": [],
   "source": [
    "def train_step(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8srA5uGuObil"
   },
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0SezTZ_racQB"
   },
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "\n",
    "    t_coll = colloc_pts(N_f,rep*11)\n",
    "    t_coll =  torch.from_numpy(t_coll).float().to(device)\n",
    "    \n",
    "    f_hat = torch.zeros(t_coll.shape[0],1).to(device)\n",
    "    for i in range(max_iter):      \n",
    "        train_step(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat)\n",
    "         \n",
    "        loss_np = PINN.loss(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test RE\",test_re_loss[-1],\"c\",PINN.c.cpu().detach().numpy(),\"k\",PINN.k.cpu().detach().numpy(),\"m\",PINN.m.cpu().detach().numpy())\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I9BYbcJ0NKGX",
    "outputId": "42720d97-b37a-4c42-b7e5-b77a0ef391f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 2.3471487 Test RE 0.20322058836021092 c 0.0012606364 k 0.13622308 m -2.5162148e-05\n",
      "1 Train Loss 0.5654264 Test RE 0.0861690312356455 c 0.013445648 k 0.12718058 m 5.2905766e-06\n",
      "2 Train Loss 0.22626296 Test RE 0.013997409663672423 c 0.11681664 k 0.12274464 m 0.00041282424\n",
      "3 Train Loss 0.17027372 Test RE 0.02882743587832267 c 0.70609045 k 0.1091515 m 0.0023886913\n",
      "4 Train Loss 0.06245187 Test RE 0.01138952607661723 c 2.3506377 k 0.09433521 m 0.023048699\n",
      "5 Train Loss 0.05665744 Test RE 0.011621073194125692 c 2.375178 k 0.091177166 m 0.026478171\n",
      "6 Train Loss 0.034127552 Test RE 0.008758729801366142 c 1.9169699 k 0.09706358 m 0.0783596\n",
      "7 Train Loss 0.018417796 Test RE 0.011411112193292109 c 2.0965033 k 0.09553048 m 0.18467242\n",
      "8 Train Loss 0.014167665 Test RE 0.008923959397685123 c 2.0736623 k 0.09592519 m 0.20597927\n",
      "9 Train Loss 0.0102191325 Test RE 0.007608239795399837 c 2.0130482 k 0.09660091 m 0.41174242\n",
      "10 Train Loss 0.007899121 Test RE 0.007750423142258908 c 2.0420842 k 0.096218295 m 0.57506\n",
      "11 Train Loss 0.0076363124 Test RE 0.007556211844630496 c 2.028422 k 0.09647066 m 0.6109086\n",
      "12 Train Loss 0.0056723077 Test RE 0.007351907223149895 c 2.0795572 k 0.09583409 m 0.7755024\n",
      "13 Train Loss 0.0050696405 Test RE 0.006307571894126158 c 2.0882394 k 0.095812835 m 1.0610529\n",
      "14 Train Loss 0.004893815 Test RE 0.006193034035525171 c 2.0851235 k 0.0959102 m 1.2250793\n",
      "15 Train Loss 0.004687296 Test RE 0.006013377536302428 c 2.0814948 k 0.096105464 m 1.4396099\n",
      "16 Train Loss 0.0041905944 Test RE 0.00546724710717712 c 2.0704672 k 0.09666611 m 1.5296488\n",
      "17 Train Loss 0.0038049463 Test RE 0.005614582932876684 c 2.0657008 k 0.09693967 m 1.5861729\n",
      "18 Train Loss 0.0035027396 Test RE 0.005093274837777858 c 2.0684645 k 0.09677278 m 1.6483729\n",
      "19 Train Loss 0.0034772689 Test RE 0.004974372761485412 c 2.06805 k 0.09681253 m 1.6728946\n",
      "20 Train Loss 0.0034108004 Test RE 0.00486829979299254 c 2.062247 k 0.097016096 m 1.7925462\n",
      "21 Train Loss 0.0031814124 Test RE 0.004677130142922376 c 2.0520148 k 0.09735486 m 2.043926\n",
      "22 Train Loss 0.0027232063 Test RE 0.005268737653395691 c 2.066944 k 0.09736762 m 2.2941065\n",
      "23 Train Loss 0.0025880921 Test RE 0.004960357102446818 c 2.0403183 k 0.09795183 m 2.3021014\n",
      "24 Train Loss 0.002038953 Test RE 0.003755622286178063 c 2.062443 k 0.097710274 m 2.6880174\n",
      "25 Train Loss 0.0017370596 Test RE 0.0033522345119989345 c 2.0218213 k 0.09862006 m 3.061199\n",
      "26 Train Loss 0.0012641194 Test RE 0.002329452639836342 c 2.051481 k 0.098696806 m 3.7072306\n",
      "27 Train Loss 0.00074464455 Test RE 0.0024307302196368966 c 2.0077 k 0.09966017 m 4.0451274\n",
      "28 Train Loss 0.00063043064 Test RE 0.002686748636649658 c 2.0065262 k 0.09966884 m 4.197898\n",
      "29 Train Loss 0.00023374491 Test RE 0.0013477658145058975 c 2.014014 k 0.099301495 m 4.428573\n",
      "30 Train Loss 0.00010457249 Test RE 0.0006448752213245222 c 1.9926896 k 0.100093015 m 4.646757\n",
      "31 Train Loss 5.0269417e-05 Test RE 0.0007235310250743184 c 2.0011804 k 0.09984321 m 4.853734\n",
      "32 Train Loss 3.1316973e-05 Test RE 0.0003886344742849355 c 2.000047 k 0.09999061 m 4.8983326\n",
      "33 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "34 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "35 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "36 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "37 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "38 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "39 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "40 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "41 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "42 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "43 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "44 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "45 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "46 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "47 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "48 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "49 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "50 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "51 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "52 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "53 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "54 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "55 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "56 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "57 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "58 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "59 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "60 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "61 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "62 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "63 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "64 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "65 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "66 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "67 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "68 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "69 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "70 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "71 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "72 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "73 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "74 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "75 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "76 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "77 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "78 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "79 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "80 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "81 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "82 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "83 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "84 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "85 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "86 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "87 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "88 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "89 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "90 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "91 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "92 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "93 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "94 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "95 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "96 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "97 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "98 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "99 Train Loss 3.1034368e-05 Test RE 0.0003799987100662925 c 2.0002532 k 0.099995784 m 4.9020157\n",
      "Training time: 16.91\n",
      "Training time: 16.91\n"
     ]
    }
   ],
   "source": [
    "max_reps = 1\n",
    "max_iter = 100\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "\n",
    "for reps in range(max_reps):   \n",
    "  train_loss = []\n",
    "  test_mse_loss = []\n",
    "  test_re_loss = []   \n",
    "  beta_val = []\n",
    "\n",
    "  torch.manual_seed(reps*36)\n",
    "  N_f = 10000 #Total number of collocation points\n",
    "\n",
    "  layers = np.array([1,50,50,50,1]) #9 hidden layers\n",
    "  PINN = Sequentialmodel(layers)\n",
    "\n",
    "  PINN.to(device)\n",
    "\n",
    "  'Neural Network Summary'\n",
    "  print(PINN)\n",
    "\n",
    "  params = list(PINN.parameters())\n",
    "\n",
    "  optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.1, \n",
    "                            max_iter = 10, \n",
    "                            max_eval = 15, \n",
    "                            tolerance_grad = 1e-6, \n",
    "                            tolerance_change = 1e-6, \n",
    "                            history_size = 100, \n",
    "                            line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "  train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "  torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "  train_loss_full.append(train_loss)\n",
    "  test_mse_full.append(test_mse_loss)\n",
    "  test_re_full.append(test_re_loss)\n",
    "  beta_full.append(beta_val)\n",
    "\n",
    "\n",
    "  print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"beta\": beta_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "pmHEeBpzfFQh",
    "outputId": "990b2054-f35e-4c8e-c378-69ff2eb19f4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1bc00cb850>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGdCAYAAACyzRGfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA7zElEQVR4nO3dd3QUZcPG4d+mk5CE0FIgQOi9BaQ3QYogIoooqIivBaWKBQEVGwQsqChFEEFEBRUVFGlSlSIQWmihd0KoSQikz/cHr/nMSzHA7s6W+zpnzyEzk8yd58Tk9pmZZy2GYRiIiIiI2ImH2QFERETEvah8iIiIiF2pfIiIiIhdqXyIiIiIXal8iIiIiF2pfIiIiIhdqXyIiIiIXal8iIiIiF15mR3gf+Xk5HDixAkCAwOxWCxmxxEREZF8MAyDlJQUIiIi8PC48dyGw5WPEydOEBkZaXYMERERuQVHjx6lZMmSNzzmpsvHqlWreO+994iNjeXkyZP89NNPdOnSJXe/YRi8+eabTJ48mfPnz9OgQQPGjx9PtWrV8vX1AwMDc8MHBQXdbDwRERExQXJyMpGRkbl/x2/kpstHamoqtWrVonfv3tx///1X7X/33XcZO3Ys06dPp2LFirzzzjvcddddxMfH5yvQ35dagoKCVD5EREScTH5umbjp8tGhQwc6dOhwzX2GYfDRRx8xfPhwunbtCsCXX35JaGgo33zzDc8888zNnk5ERERcjFWfdjl48CAJCQm0bds2d5uvry8tWrRgzZo11/yc9PR0kpOT87xERETEdVm1fCQkJAAQGhqaZ3toaGjuvv8VExNDcHBw7ks3m4qIiLg2m6zz8b/XewzDuO41oKFDh5KUlJT7Onr0qC0iiYiIiIOw6qO2YWFhwJUZkPDw8NztiYmJV82G/M3X1xdfX19rxhAREREHZtWZj6ioKMLCwliyZEnutoyMDFauXEnjxo2teSoRERFxUjc983Hx4kX27duX+/HBgwfZsmULhQsXplSpUgwaNIhRo0ZRoUIFKlSowKhRo/D396dHjx5WDS4iIiLO6abLx8aNG2nVqlXux4MHDwagV69eTJ8+nZdffpnLly/z3HPP5S4ytnjx4nyt8SEiIiKuz2IYhmF2iH9KTk4mODiYpKQkLTImIiLiJG7m77fe1VZERETsSuVDRERE7ErlQ0REROzKqut8iIiIiONISYGTJyEhAc7tTiRr206SE9OIK9GeDz80L5fKh4iIiJPJzLxSKBLjTnHprzgu7z9B1rGTcCoR73OnKHAxkaC0RJ7OmcRfNASgD3OYyHOcojjP+pxi7FjIxxvQ2oTKh4iIiANJTUwl4a/DXNh5gtT9J8k6fALLyRN4nz1JwaQTvOL3IYvP1ccw4FnmMIG+1/1akRxlR8GGhIdDkHckx49W4lJwOG8+Z5CVZcHb247f2D+ofIiIiNiJcTmNc9uOcWbTEVJ2HiVj3xEsR48wO2wgf5yvzuHD8MDZr5jEs9f9GsGpBzCoj5cXpIZEceBiNVICI0gvEg7FQ/EqURz/0sUJLB/K9FZ1CIj6+zM7/fcFr9j8O70xlQ8RERErMZKSObP+AGc2HGRHwQbsTo7gyBEo/dd3PLOjP0WzEykCFPmfzxu34042UR2Ak4RznhDO+oaTHBDB5ZAIskMj8CgRjl/ZCF5t2YhxdaBYMfDw6AB0sPe3edtUPkRERG5CWhocPAiJy7YT8PPXeBw5SMHEAxRLOUBI9lmKAcWAN5jFd3QHoBMFGE4iAKn4c8KzFOcKRpJauBRZ4ZHc26IaPRpB6dJQKrIzhULOEWLet2hzKh8iIiL/lJlJ6vaDJKzaQ3LsHnJ27cHv+D5Czh/gTf93mXzuAQA6cZBfGH3VpydSjOM+ZalY0Zfe9a8UinJFm7PeezPFoksRUS2ECn43utPTpLtA7UjlQ0RE3I9hkHX4OAmr9rAnswxbk8sSHw9BaxcxaltHAsim3DU+LSTtyhurFiwI2ZE1+C29H1mlyuJTuSxBtaIIbRhFqWqBFPeGOnk+MxiobfNvy1mofIiIiEtLOnyBhJm/k7p+Bx7xOwk6GU9Yyl78jUuUBD5hNO8yBICqlORdsknFnwOeFUksVJHUEhWhfHkKVC9HtyZVeLEuFCkCFksZ4BMzvzWnpfIhIiLOLzOTlE17OfH7TlLX72Cjxx3MudSBnTuhyLFDbKHbVZ+ShScHLWUpHO7HA42hUiWoVK4Sm4oeo0zjCGoUcf3LH2ZR+RAREady8SLsWn0Oj4nj8dwZR8iJHUSk7iGQLCr995iNPMXi/z4FcprKbPJuwOliVUiLqopXjSqENKhIyWZRlIvyZkieNxrxAkrY+TtyPyofIiLikIzLaSQs28GpRVvJ2LCVremVeDflOfbvh0DDkyRez3P8RQLY512VU8Wq4VezFZO7QrVqULWqH4UKrTPpu5BrUfkQERHTpafDji2ZZH/wIZZtWylydCuRl3YTTjbh/z0mlZbs4zkAAsKD+dF7EF6lIvCpW51iLapSrmUktQvr/VKdgcqHiIjYVfrpZA7O2cS5xRs5dgxGpr/Izp2QleXFKd6nOKdzjz1LYfYH1OJcZC0y6zViSS+oVevKAltg4jujyW1R+RAREZvJzIQDX6/l3G/r8Ni8kdCjGymTvofK/91/jBJs40UAQkIszA15nogwA987ahHWrhYVWpbgjhuuiSHOSOVDRESswsjO4dDiPRxasIsfsu9j40bYuhWWpw+mEXnvuTjsUYYjxaK5XLUeP/XNpm59TyIjwWIZalJ6sSeVDxERuSUXTySz9+v1JC9aS8C2tZQ7s44o4zyl8KATyVwiAIAVvu3JDgojtXI9/JvXo3TXaErVKUppTWi4LZUPERH5V0aOwb59sHadhbVrocGcl3ns9PvUwchz3GX8iA+qz7BuiZRrE0W9elCu3AgsKhryDyofIiJylZxsgz2/xHNi1iq81q6i7LFVdMpZwp7/rqThRSSPY3DEM4ojJRqRGd2QYp0bUbFbLWoHeGshcbkhlQ8RESEzE3YsPMrZz3/Cb/0qKp5aRWXjdO6NoQCtvVZR9I5KNGoEzWs8wsma3ShVJ4xSpqUWZ6XyISLihjIzDOJm7+TPrYH8uq0Ua9ZAy9St/MrA3GMu40d84Uak1GpGyL3N+fDRhvgW/nuvK7/hu9iayoeIiBswDNi9+AhHpi/FZ9VSqpxcSl0jgd8YzhLeAWBnoSbE+nTgUr3mFO3anAoPRVM7wNfk5OKKVD5ERFzU4cOw8pdkIj4eQtTBpVTJ3kuVf+y/RAHqVEjl04HQvDlUqxaCh8dvpuUV96HyISLiIi4mZbPpsw3sWJ7I2H2d2bcPLBTkNN9RhHNk4cne4PpcqN+GYt1bU7ZnIzoW0MyG2J/Kh4iIkzIM2LvmNPsnLMJn6QJqnVpEc84SRUme4x48PS00aODBqiLvUaFRUSo+2YIqxYLNji2i8iEi4kxSU2HZMrj4/iSqrJtGzYwNVPzHWhvJHsGcjmrIbyNTaNIhiKAggCdMyytyLSofIiIO7mj8Jba+v4QJh+5m6SpvMjJgLPE8zHoA9gfW4lyDuwnr3YGSDzSkro+3yYlFbkzlQ0TEwRgG7FyRyP6PfiF4xVzuSF5CJ9L4kN/JoDVlykBa/cfYGl6DCv3bU658BOXMDi1yE1Q+REQcQFYWrJ+XwOkPvyJiw1yi09dQ7R+XU076lmFA9yTGD4VKlcBiqQPUMS+wyG1Q+RARMUl6Oiydn8bsuX78+iuUOneSzbycu39fcDTJre6lzMB7CW9Rg3v1BiniIlQ+RETsKCMD1n6+g/OffUfl7d+TmHMHM5gOgKVwbdYW6oF/68ZUeKEz5StFmhtWxEZUPkREbCwj3eCvL3Zw7rPvqRT3PS1yduXuK+xxjoHPZXPfA540aWLBy+trE5OK2IfKh4iIDWRlXXkkdvZs6PZVZ9pn/pq7Lx0f9pZpi3fPB6nwQmc+CvE0MamI/al8iIhYiWFA3PIzxI/8gRd39ObIqSurh5akHq1YzJ4y7fDu0Y0KL3SmemEt9iXuS+VDROQ2Hd1zmdg35hH8y9c0vbiAmmTxDaGkFrmPbt3grrsH4NVkEDVUOEQAlQ8RkVuSdD6H1SOXY5k5kyan5tCFlNx9BwrV5ZV+3sx+DXx8QG8/L5KXyoeISD4ZBqxaBVOnQtx38WxOb5O7L8G3NKfa9KTsaz0p26AqZU3MKeLoVD5ERP7FiQNprB8+lz1LDjPk7N/rcFThz4B2FKxWmvCXHyXsvsaEeXiYmlPEWah8iIhcQ2Ym/PnpFi6O+4Imh2bShfOk4csnAU/RoUcI//kP3FF/ARYPLfwlcrNUPkRE/uHI7ktseOFbyi+eQKusTbnbT/mU5GT73uz+JIeAUn9vVfEQuRUqHyLi9nJyYPFimDABqvzyKWMYAlxZj2N3pS4UfvEJInu3IdRT63GIWIPKh4i4rbMJmax+eS6zlxblmxMtAVjH4zwXMJ3krr2pPKY3tcKLmhtSxAWpfIiI29ky/ziHh0/mjq1T6MxJAmnJ/OCW9O4NffoUp3TFHaA3cROxGZUPEXELWVmw4v2NZL//IXee/Y7aZAFwxiuUQu2acvzbHAIC/35aRcVDxJZUPkTEpSUlXVmXo9iI53j04sTc7buLN8Ozf1/Kv3QfRX19TEwo4n5UPkTEJR3amsTEyZ5M/KogKSnwEM3ozufsrPkQke8PovJddc2OKOK2tCKOiLiUzXOP8GuFQRSpXZLsCZNISYEqVaDNxAfI3neI2ltnUETFQ8RUmvkQEadnGLB26k6SX3uX1glfU+e/93N0L7KUNjNfpF07sFi8gQhzg4oIoPIhIk4sJwdWvrsOy+jRtEyam7t9Z/id+I94mfpPt9W9oyIOSJddRMTpZGbC9OlQrRqcHjqWlklzycHC1nJdSZi3nqonllLmmXZ6XFbEQWnmQ0ScRtqlHJYNmss78+uy9kRpAMYXfIVy5QKJmvAStRpXNjmhiOSHyoeIOLz0yzksGziXUtPf5O7MrRzlaQ6EfsbgwdCnT12CgqaaHVFEboLKh4g4rIy0HJYPmkvJL96kQ+ZWAFIsgdTpEMGhOeDnZ3JAEbklKh8i4nAyM2HZ4F+J/OxV2v2jdMS3HUCNL57njogiJicUkduhG05FxGFkZcG0aVCxImz8dC1VM7eSYglkfdvheB89SL2F7+Cr4iHi9DTzISKmMwz4I+ZPPp7iz4+HriwANrPYYFrX8aTW1IHcUVKFQ8SVWH3mIysri1dffZWoqCgKFChA2bJleeutt8jJybH2qUTEBcRO38aawp1oPrwZzx8aQOEQg/feg9hDRWi46C0KqHiIuByrz3yMGTOGSZMm8eWXX1KtWjU2btxI7969CQ4OZuDAgdY+nYg4qfgFBzjx1Ou0OP4NHhhk4YlPneocWJBOcKjuJBVxZVYvH2vXruXee++lY8eOAJQpU4Zvv/2WjRs3XvP49PR00tPTcz9OTk62diQRcSAn4s6yvftbtNw1kUpkArChXHdKTX+bO5pWMDmdiNiD1S+7NG3alKVLl7Jnzx4Atm7dyp9//sndd999zeNjYmIIDg7OfUVGRlo7kog4gNRUeOMNeCt6Lm13jcOHTLaEtuPQnFjq75tFqIqHiNuwGIZhWPMLGobBsGHDGDNmDJ6enmRnZzNy5EiGDh16zeOvNfMRGRlJUlISQUFB1owmIibIyTb4acJJBo6J4Phx8CCbBcUeo9Trvancr43Z8UTESpKTkwkODs7X32+rX3aZPXs2M2fO5JtvvqFatWps2bKFQYMGERERQa9eva463tfXF19fX2vHEBEHsG36JjIHDKZ+ygHOEk+ZMgV47z1P7rr/a73tiogbs3r5eOmll3jllVd46KGHAKhRowaHDx8mJibmmuVDRFzPsY0J7H/wFZodnIEHBpfx4/On1nP/uBZalVRErH/Px6VLl/DwyPtlPT099aitiBvIuJTF750+IrB+JVoc/BIPDNaV60nKhnh6TlbxEJErrD7zcc899zBy5EhKlSpFtWrV2Lx5M2PHjuWJJ56w9qlExIEsn5tEiQeb0iZjOwC7CtbDY/ynNHysgcnJRMTRWL18fPLJJ7z22ms899xzJCYmEhERwTPPPMPrr79u7VOJiAM4dgxeeAG++y6YHylPEctJ9vaOocFnT2Dx8jQ7nog4IKs/7XK7buZuWRExT+alTP7s/gn/WdqDg5fD8PCAYb1P8uIwH4LLalVSEXdj6tMuIuL6tk35C7/+T9EqPY432MKkRjOYMAFq1w43O5qIOAGVDxHJt+RjyWy+ezjN4sbjgcFZSxGiHm/Fn5+Dh94jW0TySb8uRCRf1g+fS2rpqrSI+xQPDFaXfwyP+N00+6K3ioeI3BTNfIjIDSUkwM+dPqdP7FMAHPEqy5mRn9HkZa1OKiK3Rv+/IiLXZBjwxRdQpQoMie3GIUqzvMErFD0ZR10VDxG5DZr5EJGrnFh7mGU9p/Kfg28CFurWDebCp7to1aiA2dFExAWofIhILiPHYM1/PqfG9Bd4hBRWe5Wi7Kgnef558PJS8RAR61D5EBEAEmOPcqzDUzQ5vQiAbQUb89L3zSnb3uRgIuJydM+HiLszDNY/Ow2/+tWpe3oRafiy9O4PqHpmFWXbVzQ7nYi4IM18iLixs2dhQ8MBtN/3KQDb/Bvg9+10WneubHIyEXFlmvkQcVNLlkD16vDmvh5cogCLW4+h8pnVVFTxEBEb08yHiJtJP5fK1D4b6Pt9SwBCqjRiz7gjtG1T1NxgIuI2NPMh4kYOztnEiYhoen/fgSrspG9fiI2F2ioeImJHmvkQcQNGVjZ/PfgBdX96FR8yOWmJYHLMeZoOMTuZiLgjlQ8RF3dux0mOt+xJwzPLAfijWFfKL5tM0+p623sRMYcuu4i4sB0fLSG7Zm1qnFnORQJY9OBUmpz8gXAVDxExkWY+RFyQYcCHH0Lqi3/ympHIbp+aZH/7He26VjI7moiIyoeIqzl/Hnr3hrlzwYPXKV07mC4LnyUoVMuji4hj0GUXERey59PFbItoz8K5afj4wCfjPXl002AVDxFxKJr5EHEBRnYOG+95k+gFb1MRg3dCxtJqyTCio81OJiJyNZUPESd36fh54u94hPonfgNgUZlneHLtYAqFmRxMROQ6VD5EnNix37Zh3HcfdTIOcBk/VvaYTLuZj2KxmJ1MROT6dM+HiJPa/M58CndsSGTGAQ57lGHnlDW0/1rFQ0Qcn8qHiJMxDHjvPXjg9aqk4ce6oLZ4bd5I9JN1zI4mIpIvuuwi4kRSz2fwRB8fvvsOIIr371vD619VwC/A0+xoIiL5ppkPESdxfGEc58KqkPzdAry8YMIEGDmnsoqHiDgdzXyIOIFd7/5C5Cs9KGhcZLTXa6QsbUfT5vp/BxFxTiofIo7MMNjc4z1qzXoFDwz+KngnJdd8T60aKh4i4rxUPkQcVM7ldLY0fIa6274EYGGZPjTdNI6CId4mJxMRuT0qHyIO6NKZSxyp3Ja6Z1eTjQe/tvmYexb2xcNTz9GKiPPT3K2IgzlxApq3K8DKs9W4QDBLX1jAvUv6qXiIiMvQzIeIA9mx3aB9BwvHjlk4VvhT6k4YQtvuZc2OJSJiVZr5EHEQO16fzYE693PyWBaVKsGaDd7UV/EQERekmQ8RsxkGmx/9gDpfv0Q1IKbcVP6z5hkKFzY7mIiIbah8iJjIyMpmS8tB1Fn9KQC/lh9I/y1P4hdgcjARERtS+RAxSXbKJXbU7kGdA3PJwcLcZh/QefnzeGrBUhFxcSofIiZIO3meQzU6UfPsGtLwZcljM7nvywfMjiUiYhe64VTEzpKTYcA9Byl5divnKcTqN37nHhUPEXEjmvkQsaMzZ6BDB9gYW5cTBeYx4tNitH6ihtmxRETsSuVDxE4SlsTR7+kMNh6KpmhReHPhnURHm51KRMT+VD5E7ODo7DUE9ejIpBxPToeu5rMVlahc2exUIiLm0D0fIjZ2YMJCijzUhuCcCxz2q8zMxcVVPETEral8iNjQ7pFzKNm3M/5c5s/ADpTcuZjImiFmxxIRMZXKh4iN7HhtFuVf7Y4PmfxetDvV9/1MaJS/2bFEREynez5EbGBzzEJqvtMTT3JYEv4YTeK/wD9Qq4eJiIDKh4jVLVwIPd5oxs804XJkJVrs/gw/f00yioj8TeVDxIrmz4euXSEjI4BPOy7kq+/98C2g4iEi8k/6rShiJXFPjWNj57fIyLhSQGb+6K/iISJyDZr5ELGCbb0+oOaMF6kBWFo2Y+isVnh7m51KRMQx6X/LRG7Tlsc/ouaMFwH4qdqrDFvcUsVDROQGVD5EbsPWZyZQ+8vnAfi55mt03vo2Xt4Wk1OJiDg2lQ+RWxQ38HNqTe4LwC/VXuGe2Dfx1NO0IiL/SuVD5BasnbabquOeAeDXCs/TYfMoPL004yEikh+64VTkJv3xB7TvV5nH+JROUTtpt/0DXWoREbkJKh8iN+GvdQZ3323h0iU41P5Z2vwM3j5mpxIRcS667CKST/s+XUhO02b4XDxLq1bw44/g62t2KhER56PyIZIPB75aTUT/rjTKXs24Eu/yyy9QoIDZqUREnJPKh8i/OP7bVor06og/l1kT3J57trxNQIDZqUREnJdNysfx48d55JFHKFKkCP7+/tSuXZvY2FhbnErEps7+tQ+fzu0INpLYXKAxleN+IKiobvIQEbkdVr/h9Pz58zRp0oRWrVqxYMECihcvzv79+ylUqJC1TyViUym7j5PW/C5KZJ9il3dNQjf8SuFITXmIiNwuq5ePMWPGEBkZybRp03K3lSlTxtqnEbGptDTY06gX0RmHOOBZHt8Vi4ioFmJ2LBERl2D1yy7z5s2jXr16dOvWjeLFi1OnTh2mTJly3ePT09NJTk7O8xIxU1YWPPwwdL8wiTWeTbk8dwllG4eZHUtExGVYvXwcOHCAiRMnUqFCBRYtWkSfPn0YMGAAM2bMuObxMTExBAcH574iIyOtHUkk3wwDnnkGfv4ZjvmWJ2PJKqp1LGN2LBERl2IxDMOw5hf08fGhXr16rFmzJnfbgAED2LBhA2vXrr3q+PT0dNLT03M/Tk5OJjIykqSkJIKCgqwZTeTGDIPVDV/gnfV3sdijAz/+CPfea3YoERHnkJycTHBwcL7+flt95iM8PJyqVavm2ValShWOHDlyzeN9fX0JCgrK8xIxw4YuI2my/kN+pgtfjz6q4iEiYiNWLx9NmjQhPj4+z7Y9e/ZQunRpa59KxGq2Pj+d+vNeA2Bphw946CVd/hMRsRWrl4/nn3+edevWMWrUKPbt28c333zD5MmT6du3r7VPJWIVe8YtpNpHTwLwW40hdJjfz+REIiKuzer3fAD8+uuvDB06lL179xIVFcXgwYN56qmn8vW5N3PNSOR2HZ8XS6EuLQgwUvk9/BFaHPwSb18t/CsicrNu5u+3TcrH7VD5EHs5t/0EWbWjKZ6dwLqCbah6cL5WLxURuUWm3nAq4gzS0uC+p4vxc/Y97PauTumNc1Q8RETsxOornIo4OsOA//wHVq31ZmvQZ6xdlEx4Jc2yiYjYi2Y+xO380ONHZn+ThZcXzPnRQpWGwWZHEhFxK5r5ELcS+9Qkus16lgA6cPyTX2jd2tPsSCIibkczH+I24j9ZTK3PrzxGazRuwlN9VDxERMyg8iFuIWHZTsIHdsOLbJaWeIz2K4eZHUlExG2pfIjLSz16jswOnQkykon1b0r9zZPx9LKYHUtExG2pfIhLy8nIYm+9h4jM2M8Rj9IUW/UjQcV8zY4lIuLWVD7EpU0esJ0KiatJxZ9zX8ylVHQxsyOJiLg9lQ9xWT//DM9+VpvGrGHdwFnU7lXL7EgiIoIetRUXtXOHwaOPXrmvo9XAWrT+SMVDRMRRaOZDXE5SfALZ0XdQ/eJaWrWC994zO5GIiPyTZj7EpWRfzuBYoweokb6Rqd7PUuzbTXh7q2OLiDgS/VYWl7KxyUCqnV/NBYLhu+8oFqofcRERR6PfzOIy1vebQYPNk8jBwuaXvqVql4pmRxIRkWtQ+RCXsP+nbVQf3weA3xuPoNW7HUxOJCIi16PyIU4v5egFvLrfjz+XWV+kPXcuf83sSCIicgO64VScmmHAgEEedMqsiYdnJmVXz8TLR51aRMSR6be0OLVPP4XpPwbxkOcPJPy4lqKVipgdSURE/oXKhzitTXOP8sJgA4D33rdQv3O4yYlERCQ/VD7EKZ2LO05k13p8lfUQj9x3kYEDzU4kIiL5pXs+xOlkp2dxvPnD1MhJpJbPbjpM8MBiMTuViIjkl2Y+xOn82fYtalz4g2QCscz5gaAwf7MjiYjITVD5EKey+f2lNFv1DgBbn/2MSp0qmJxIRERulsqHOI3T209RYkhPPDBYVfFJmk142OxIIiJyC1Q+xCnk5MChVr0pnnOKvb7VqLf6Y7MjiYjILVL5EKcwdiy8cOYV9loqwOzv8C+q+zxERJyVnnYRh7d+PQwdClk0Z8WEXTx1r6fZkURE5DZo5kMcWvLBswztGk9WFnTrBk8+o+IhIuLsVD7EYRk5BruaPcXc49E8XewnpkxB63mIiLgAlQ9xWH/0/oIGx3/Chwz6v1+a4GCzE4mIiDWofIhD2r9wL9EzBgCw9u53qP5YXZMTiYiItah8iMPJSM3k8v09CeASW0Ja0Wzui2ZHEhERK1L5EIezuu2bVL+0gfOWEMIWfYmHl35MRURciX6ri0PZ/Nl6WqwZBcCewZ8RVj/S5EQiImJtWudDHEZSEjwwsg5PMJTmFRJo9n43syOJiIgNqHyIw+jXDw4c9WZauZEM2GiYHUdERGxEl13EISwYs41ZMzPx9ISZMyEwSAt6iIi4Ks18iOmObzhBo1dasI6yLB/4Kw0bhpsdSUREbEgzH2KqnGyDY+2fpBAXKBDgwaB3ipodSUREbEzlQ0y1/JGpNDi3gDR8Cfj+S7wKeJsdSUREbEzlQ0xzcPkh7pj1PACx942kdIeqJicSERF7UPkQU2Rn5nC+S28CuUhccFMazR5kdiQREbETlQ8xxYpun1I3eQWp+FN43nQ8vD3NjiQiInai8iF2t2sXDPytPWtpyJae71OieTmzI4mIiB3pUVuxq6ws6NULdmRWZGS7P/hlhmY8RETcjWY+xK4+HXGaDRugUCH4bKoXFg8tJiYi4m408yF2s3veHp4cVZdsniF07GhKlNBjtSIi7kjlQ+wiKyOHSz2epCCp3FlsO7V76UdPRMRd6bKL2MWyBydRN/UPLhJAyfmf6XKLiIgbU/kQmzuw/DCN5g4BYEfPGIrVL2NuIBERMZXKh9hUTrbB6fv7EMhFthdqwh1f9jU7koiImEzlQ2xqxX++osH5haThS6EfpmLx1I+ciIi7018CsZljx+Db2R4kEURspzco2bqS2ZFERMQB6JEDsQnDgD59YH7aI5yq24qffgg1O5KIiDgIlQ+xiVnfGsyfb8HHB0Z/VQJPX7MTiYiIo9BlF7G6s/svULFXQ9qxkOHDoWpVsxOJiIgj0cyHWN3mDkNpk7WeCb7PU/LFNujHTERE/kkzH2JVa8eu5c69nwGQ/tEkfPxVPEREJC+bl4+YmBgsFguDBg2y9anEZJeSMgl55Rk8MFhf9XGq9GlhdiQREXFANi0fGzZsYPLkydSsWdOWpxEHsbLLh1TOjOOcRxGqzn/P7DgiIuKgbFY+Ll68SM+ePZkyZQohISHXPS49PZ3k5OQ8L3E+exYdpMWKNwA41O8DCpYpam4gERFxWDYrH3379qVjx460adPmhsfFxMQQHByc+4qMjLRVJLGRnBxY9sxs/LlMXJGW1P3oMbMjiYiIA7NJ+Zg1axabNm0iJibmX48dOnQoSUlJua+jR4/aIpLY0PTp8OzhV+jmM5ci308Ci96xVkRErs/qjyIcPXqUgQMHsnjxYvz8/P71eF9fX3x9tQKVszp9Gl566cq/G4zsTEQrc/OIiIjjs/rMR2xsLImJiURHR+Pl5YWXlxcrV65k3LhxeHl5kZ2dbe1Tiol+uv8rPM8lUrMmDBxodhoREXEGVp/5aN26NXFxcXm29e7dm8qVKzNkyBA8PT2tfUoxyaaJf/HkH714gELsH7Ubb+/iZkcSEREnYPXyERgYSPXq1fNsCwgIoEiRIldtF+eVcTkbv+efxQOD+Ir30KijioeIiOSPVjiVW7Ks+2dUTd9MkiWYKr9oTQ8REck/u6x9vWLFCnucRuzk8MbTNPhlOAB7Hn2H+hU16yEiIvmnmQ+5aXseGEoIF9hbsDb1Pu9jdhwREXEyKh9yU9Z+9Bd3HZ4KgOfE8Vi89cZxIiJyc/SXQ/ItPR36fVKJXvTnjuqXaPhIY7MjiYiIE1L5kHz76CPYdKAQx0PHEf+HYXYcERFxUrrsIvlybF8ab791pXC8+y4EF9IS6iIicmtUPiRf4tsNYO6lNjxUezePPGJ2GhERcWa67CL/KnbSBlod+BwPDEr1P42HR2WzI4mIiBPTzIfcUGZaNr6Dn8MDg78qPkqFJ5qZHUlERJycyofc0Mre06l+eSPJliAq/vyu2XFERMQFqHzIdSXuS6bm7GEA7HxgBCFVwkxOJCIirkDlQ64r9r53KG4kcti3IvW/7Gd2HBERcREqH3JNG9dlEb59MQCXR47Fs4CPyYlERMRVqHzIVQwDBr3oRX028HHzH6g8+G6zI4mIiAtR+ZCrfP89rF4NPv7e3P/1/WDRgmIiImI9Kh+Sx+WULGKfnYI3GQwZAiVLmp1IRERcjcqH5LGy52TGnHuatT4tePEFvX+LiIhYn8qH5Dq54xz1f3kNgJyej+IfoMstIiJifSofkiuu25sU4Rz7C1Sn3mdPmx1HRERclMqHALD9u53cuWs8AJnvfojFW2/7IyIitqHyIRg5BqnPDMaLbGJL3kvlfm3MjiQiIi5M5UP4Y+hvNLiwiAy8ifjmfbPjiIiIi9Pcupu7fBne+iqKwXSgUJPqNG5W3uxIIiLi4lQ+3NyHH8LSk1XZE/kbuxdkmx1HRETcgC67uLHERBg9+sq/Y2LAP9DT3EAiIuIWVD7c2MaOIxid8hytayTy8MNmpxEREXehyy5uav+Ko7Ta+C4FSKPZA3fh4XGf2ZFERMRNaObDTR157FUKkMb2ws2o8VoXs+OIiIgbUflwQ5umbqbF0a8A8B//vt61VkRE7Erlw80YOQbZg1/EA4P15R+m7EN3mB1JRETcjMqHm/lj2ALqJy8jHR+ivhlldhwREXFDKh9uJD0d/D8aCUBsk4EUq1/G3EAiIuKWVD7cyPjx0D59LhMCXqLW7GFmxxERETelR23dxPnz8M47cJ6i+H78LgElzE4kIiLuSjMfbmLSi/s4f96genV4/HGz04iIiDvTzIcbOLL2OAO/qEl9GmO8/gOenoXMjiQiIm5M5cMN7H/kdUpxmfDgy1S9P9jsOCIi4uZ02cXF7Zqzk+YHpgPg9dH7WDy0oJiIiJhL5cPFnev7Kp7ksL7kfVR6vJHZcURERFQ+XNnmSX/R5NRPZONB6JSRZscREREBVD5clpFjYAx5BYB1lR+ndPsqJicSERG5QuXDRS3+9iwFk4+Thi/lZ4wwO46IiEgulQ8XlJMDL40pSjV2MPWRFYTWL2V2JBERkVwqHy7o228hLg4Cgr3pMa6h2XFERETyUPlwMRmpmcQNmoo3GQwZAiEhZicSERHJS+XDxax58gtGn3mStd7NGdDfMDuOiIjIVVQ+XEjq6UtU/u5NAC516UlAQS0oJiIijkflw4Wsf3QcYTknOepVhgZTnzY7joiIyDWpfLiICwfPU2fxGACOPPk2PoG+JicSERG5NpUPF7Hl4TEUMi6wx68GDT9+2Ow4IiIi16Xy4QISYo/T4K+PATj/0ig8fTxNTiQiInJ9XmYHkNs3/oM0GtOSiMAU7nijo9lxREREbkjlw8nt2wejvy9HFgv447uLWDz0hIuIiDg2XXZxcq+/DllZ0KEDNG1f0Ow4IiIi/0rlw4nFf7uJxt/2I5QERo40O42IiEj+6LKLE7s4YCj9WEz1MqnUqTPN7DgiIiL5opkPJ7Xto2VEn1lMBt6U/vw1s+OIiIjkm8qHEzJyDLxeGwrA6mrPENW6rMmJRERE8k/lwwnFvvYzVS+u5yIBVJ75qtlxREREborVy0dMTAz169cnMDCQ4sWL06VLF+Lj4619GreVk5FF4Q+GAbCu0fOE1w41OZGIiMjNsXr5WLlyJX379mXdunUsWbKErKws2rZtS2pqqrVP5ZbW95tB2fTdnLMUJvqbF82OIyIictMshmEYtjzB6dOnKV68OCtXrqR58+b/enxycjLBwcEkJSURFBRky2hOJyMDmlQ8TY/Do6jasSztfu1vdiQRERHg5v5+2/xR26SkJAAKFy58zf3p6emkp6fnfpycnGzrSE5r6lTYeLgYR0M/ZN8ss9OIiIjcGpvecGoYBoMHD6Zp06ZUr179msfExMQQHByc+4qMjLRlJKd1KSWbt9668u9XX4WCWsxURESclE0vu/Tt25f58+fz559/UrJkyWsec62Zj8jISF12+R+rW7/OpWVr+TD8PX4+VBsfH7MTiYiI/D+HuOzSv39/5s2bx6pVq65bPAB8fX3x9fW1VQyXcCH+FLWWjaUgqfg8sB8fn9pmRxIREbllVr/sYhgG/fr148cff2TZsmVERUVZ+xRuZ2fPkRQklbgC9Wk6tqvZcURERG6L1Wc++vbtyzfffMPcuXMJDAwkISEBgODgYAoUKGDt07m8xL8OUi92EgApw0bj6WUxOZGIiMjtsfo9HxbLtf84Tps2jccff/xfP1+P2ub1V6XHaLDnK9YH30X984u5zvCKiIiYytR7Pmy8bIhbOTp/G/X3zATAc8woFQ8REXEJem8XB7Z30Hg8MFgV2o3oZ+qZHUdERMQqbL7ImNyabdug/b5PeYI69J94p9lxRERErEblw0ENHw6ZeHP+wT5Uu8/sNCIiItajyy4OaOMPh1j4ayaenvD222anERERsS6VDwdjZOcQ0qszu6jCG503UbGi2YlERESsS+XDwWx95VvKXYqjKGfo/WYZs+OIiIhYncqHA8lJy6DouNcAWNNsCCVqXPudgEVERJyZyocD2dTnM0pmHCTBEkaDmQPMjiMiImITKh8OIuNsClFfXbm7dFOn1ylSKsDkRCIiIrah8uEgtvT6kCI5pzngWZ4WM540O46IiIjNqHw4gIsX4dTvcQDs7TWSgELeJicSERGxHZUPB/DRR9A5/Xu6Raym1fgHzI4jIiJiU1rh1GRnzsC77175d9f3G+PjZ24eERERW9PMh8kWP/oVfimJ1KkD3bubnUZERMT2NPNhopMLt/LQwl50oiAbh+zHw6OY2ZFERERsTjMfJjr95FA8MNhY7G5aPajiISIi7kHlwyQHp62g5vEFZOJF4QnvYLGYnUhERMQ+VD7MYBhkDh4CwO9ln6b2A+VNDiQiImI/Kh8m2B3zExUvrCcVf8pPf83sOCIiInal8mFnRmYWBd4ZBsCy2oOp0CzM5EQiIiL2pfJhZ4vnpTH3cltOEk7db14yO46IiIjdqXzYUXY2vPhGQQYyjk8G7adElSCzI4mIiNidyocdTZsG27dDSAi8+FoBs+OIiIiYQuXDTlL3naRUv3uoxRZefx0KFzY7kYiIiDm0wqmd7H7wNdqm/0oRv3PUeG612XFERERMo5kPOzi1ZBt1Nn8BQPKr7+HjY3IgERERE6l82EFi75fxwGBpkW60HNbY7DgiIiKmUvmwsQMTF1Hj+CIy8KbwpBgtoy4iIm5P5cOGjKxsLC+9CMDiiv2p80A5kxOJiIiYT+XDhrYOm01U6nbOEUKNWcPNjiMiIuIQ9LSLjWRmQq9fu9GU87Rs60u3Onq2VkREBFQ+bGb8eNi2y5sTRfvyziyz04iIiDgOXXaxgcTtiYx8PR2AUaOurGgqIiIiV6h82EBCh96sTalGr4preeIJs9OIiIg4Fl12sbL4sfOpeew3MvBm4NtF8fQ0O5GIiIhj0cyHFeVcTqfAsEEALKn2PHUerGBuIBEREQek8mFFsY9+RKn0fSRYwoj+6VWz44iIiDgklQ8rSdp1gqpz3gZg04NjCKsQaHIiERERx6TyYSX7Ow0kgFS2+DWkzfRHzI4jIiLisFQ+rGDNsjSOH0gjC08yPpmEj5+GVURE5Hr0V/I2ZWTAU/396Mw83rpvC3c8WcvsSCIiIg5N5eM2jRkDO3dCsWIWBn1e3ew4IiIiDk/l4zYc+nkLpd94nKKc5uOPobDevkVERORfaZGxW5STkcXlx57msZwNhJXw4K6HvjA7koiIiFPQzMctWnv/+1RJ2UASQVT+4R0sFrMTiYiIOAeVj1twYN526v06AoDYx8ZRqmGEyYlERESch8rHTcq8lEnGw4/hSwbrit1Dq2mPmR1JRETEqah83KTVd4+k8qXNnLMUpvSiyVg8dL1FRETkZqh83IRVi9MouXImALv6TyC8TpjJiURERJyPnnbJp4QE6N7LjzQ2MK7+TB79uLvZkURERJySZj7yITsbevS4UkBKVAuh6/L+ZkcSERFxWiof+bCww8eUXz6ZAH+D77+HgACzE4mIiDgvXXb5F8tfWUT7JYPpSA4PD4yiSpW7zI4kIiLi1DTzcQObZu6k7pgH8SSHDdUep9XINmZHEhERcXoqH9cRvzKBkF73EEwyO4o0o+76SWgZUxERkdun8nEN+9adwWjThqicAxz3iaLMxjl4+vuaHUtERMQlqHz8j/id2VxsfjeVs3aQ6BWB/+olBJQpZnYsERERl6Hy8Q8rV0Kjpp58kfkIZz2L4bn8d0LqlTM7loiIiEtR+QAMA6Z8ms5dd8H587Ch4QCMLdso0rSK2dFERERcjs3Kx4QJE4iKisLPz4/o6Gj++OMPW53qthzYlc7Xld+mZf/qFMhMols3WLYMilbX0ukiIiK2YJPyMXv2bAYNGsTw4cPZvHkzzZo1o0OHDhw5csQWp7tp2dmwYeUlvmz2OVStyiN7XqcC+5hz39fMmgUFCpidUERExHVZDMMwrP1FGzRoQN26dZk4cWLutipVqtClSxdiYmLyHJuenk56enrux8nJyURGRpKUlERQUJDVMiUlQc/70/jPpr4UST5I/ey1FCANgLM+4Vx++31KvvSwHqcVERG5BcnJyQQHB+fr77fVZz4yMjKIjY2lbdu2eba3bduWNWvWXHV8TEwMwcHBua/IyEhrRwLA3x9+W+pDp/MzaJ69nAKkcSqgLAefe4/CZ/ZQ8uUeKh4iIiJ2YPXl1c+cOUN2djahoaF5toeGhpKQkHDV8UOHDmXw4MG5H/8982Ft3t4w82sP9q96j4CwQMK7NiK0RhUVDhERETuz2Xu7WP7nj7phGFdtA/D19cXX1z4LePXoAfQYZJdziYiIyLVZ/bJL0aJF8fT0vGqWIzEx8arZEBEREXE/Vi8fPj4+REdHs2TJkjzblyxZQuPGja19OhEREXEyNrnsMnjwYB599FHq1atHo0aNmDx5MkeOHKFPnz62OJ2IiIg4EZuUj+7du3P27FneeustTp48SfXq1fntt98oXbq0LU4nIiIiTsQm63zcjpt5TlhEREQcg6nrfIiIiIjciMqHiIiI2JXKh4iIiNiVyoeIiIjYlcqHiIiI2JXKh4iIiNiVyoeIiIjYlcqHiIiI2JXN3tX2Vv295llycrLJSURERCS//v67nZ+1Sx2ufKSkpAAQGRlpchIRERG5WSkpKQQHB9/wGIdbXj0nJ4cTJ04QGBiIxWKx6tdOTk4mMjKSo0ePaun2G9A45Y/GKX80TvmjccofjVP+mDFOhmGQkpJCREQEHh43vqvD4WY+PDw8KFmypE3PERQUpB/afNA45Y/GKX80TvmjccofjVP+2Huc/m3G42+64VRERETsSuVDRERE7Mqtyoevry8jRozA19fX7CgOTeOUPxqn/NE45Y/GKX80Tvnj6OPkcDecioiIiGtzq5kPERERMZ/Kh4iIiNiVyoeIiIjYlcqHiIiI2JXKh4iIiNiV25SPCRMmEBUVhZ+fH9HR0fzxxx9mR7KrVatWcc899xAREYHFYuHnn3/Os98wDN544w0iIiIoUKAALVu2ZMeOHXmOSU9Pp3///hQtWpSAgAA6d+7MsWPH7Phd2F5MTAz169cnMDCQ4sWL06VLF+Lj4/Mco7GCiRMnUrNmzdzVExs1asSCBQty92uMri0mJgaLxcKgQYNyt2ms4I033sBiseR5hYWF5e7XGP2/48eP88gjj1CkSBH8/f2pXbs2sbGxufudZqwMNzBr1izD29vbmDJlirFz505j4MCBRkBAgHH48GGzo9nNb7/9ZgwfPtyYM2eOARg//fRTnv2jR482AgMDjTlz5hhxcXFG9+7djfDwcCM5OTn3mD59+hglSpQwlixZYmzatMlo1aqVUatWLSMrK8vO343ttGvXzpg2bZqxfft2Y8uWLUbHjh2NUqVKGRcvXsw9RmNlGPPmzTPmz59vxMfHG/Hx8cawYcMMb29vY/v27YZhaIyuZf369UaZMmWMmjVrGgMHDszdrrEyjBEjRhjVqlUzTp48mftKTEzM3a8xuuLcuXNG6dKljccff9z466+/jIMHDxq///67sW/fvtxjnGWs3KJ83HHHHUafPn3ybKtcubLxyiuvmJTIXP9bPnJycoywsDBj9OjRudvS0tKM4OBgY9KkSYZhGMaFCxcMb29vY9asWbnHHD9+3PDw8DAWLlxot+z2lpiYaADGypUrDcPQWN1ISEiI8fnnn2uMriElJcWoUKGCsWTJEqNFixa55UNjdcWIESOMWrVqXXOfxuj/DRkyxGjatOl19zvTWLn8ZZeMjAxiY2Np27Ztnu1t27ZlzZo1JqVyLAcPHiQhISHPGPn6+tKiRYvcMYqNjSUzMzPPMREREVSvXt2lxzEpKQmAwoULAxqra8nOzmbWrFmkpqbSqFEjjdE19O3bl44dO9KmTZs82zVW/2/v3r1EREQQFRXFQw89xIEDBwCN0T/NmzePevXq0a1bN4oXL06dOnWYMmVK7n5nGiuXLx9nzpwhOzub0NDQPNtDQ0NJSEgwKZVj+XscbjRGCQkJ+Pj4EBISct1jXI1hGAwePJimTZtSvXp1QGP1T3FxcRQsWBBfX1/69OnDTz/9RNWqVTVG/2PWrFls2rSJmJiYq/ZprK5o0KABM2bMYNGiRUyZMoWEhAQaN27M2bNnNUb/cODAASZOnEiFChVYtGgRffr0YcCAAcyYMQNwrp8nL7udyWQWiyXPx4ZhXLXN3d3KGLnyOPbr149t27bx559/XrVPYwWVKlViy5YtXLhwgTlz5tCrVy9WrlyZu19jBEePHmXgwIEsXrwYPz+/6x7n7mPVoUOH3H/XqFGDRo0aUa5cOb788ksaNmwIaIwAcnJyqFevHqNGjQKgTp067Nixg4kTJ/LYY4/lHucMY+XyMx9FixbF09PzqkaXmJh4VTt0V3/fVX6jMQoLCyMjI4Pz589f9xhX0r9/f+bNm8fy5cspWbJk7naN1f/z8fGhfPny1KtXj5iYGGrVqsXHH3+sMfqH2NhYEhMTiY6OxsvLCy8vL1auXMm4cePw8vLK/V41VnkFBARQo0YN9u7dq5+nfwgPD6dq1ap5tlWpUoUjR44AzvX7yeXLh4+PD9HR0SxZsiTP9iVLltC4cWOTUjmWqKgowsLC8oxRRkYGK1euzB2j6OhovL298xxz8uRJtm/f7lLjaBgG/fr148cff2TZsmVERUXl2a+xuj7DMEhPT9cY/UPr1q2Ji4tjy5Ytua969erRs2dPtmzZQtmyZTVW15Cens6uXbsIDw/Xz9M/NGnS5KpH//fs2UPp0qUBJ/v9ZLdbW03096O2U6dONXbu3GkMGjTICAgIMA4dOmR2NLtJSUkxNm/ebGzevNkAjLFjxxqbN2/Ofdx49OjRRnBwsPHjjz8acXFxxsMPP3zNx7NKlixp/P7778amTZuMO++80+UeZXv22WeN4OBgY8WKFXke+7t06VLuMRorwxg6dKixatUq4+DBg8a2bduMYcOGGR4eHsbixYsNw9AY3cg/n3YxDI2VYRjGCy+8YKxYscI4cOCAsW7dOqNTp05GYGBg7u9ojdEV69evN7y8vIyRI0cae/fuNb7++mvD39/fmDlzZu4xzjJWblE+DMMwxo8fb5QuXdrw8fEx6tatm/vopLtYvny5AVz16tWrl2EYVx7RGjFihBEWFmb4+voazZs3N+Li4vJ8jcuXLxv9+vUzChcubBQoUMDo1KmTceTIERO+G9u51hgBxrRp03KP0VgZxhNPPJH731OxYsWM1q1b5xYPw9AY3cj/lg+NlZG7FoW3t7cRERFhdO3a1dixY0fufo3R//vll1+M6tWrG76+vkblypWNyZMn59nvLGNlMQzDsN88i4iIiLg7l7/nQ0RERByLyoeIiIjYlcqHiIiI2JXKh4iIiNiVyoeIiIjYlcqHiIiI2JXKh4iIiNiVyoeIiIjYlcqHiIiI2JXKh4iIiNiVyoeIiIjY1f8BRcNFRpmozhUAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_pred = PINN.test()\n",
    "plt.plot(x_true,'b')\n",
    "plt.plot(x_pred,'r--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Ky6HsA0AWWTD"
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SF7H51LTWXDq",
    "outputId": "aefc6a9f-3ad4-417a-b9f7-438009e620af"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '1D_FODE_stan_tune0.mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '1D_FODE_stan_tune0.mat'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_25047/2016390461.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtune_reps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"1D_FODE_stan_tune\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtune_reps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".mat\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test_re_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \"\"\"\n\u001b[1;32m    223\u001b[0m     \u001b[0mvariable_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'variable_names'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0mMR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mmatfile_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file_context\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mappendmat\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.mat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mfile_like\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'.mat'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             raise IOError(\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '1D_FODE_stan_tune0.mat'"
     ]
    }
   ],
   "source": [
    "for tune_reps in range(5):\n",
    "    label = \"1D_FODE_stan_tune\"+str(tune_reps)+\".mat\"\n",
    "    data = sio.loadmat(label)\n",
    "    re = np.array(data[\"test_re_loss\"])\n",
    "    print(np.mean(re[:,-1]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "stan_1D_FODE_tune.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
