{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8byrnUmNKGR",
    "outputId": "510f67c6-b225-41ba-e78a-d0a6e54ded85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uoNYpDzlNKGV"
   },
   "outputs": [],
   "source": [
    "tstart= 0 \n",
    "tstop= 60 \n",
    "\n",
    "increment = 0.1 \n",
    "# Initial condition\n",
    "x_init= [0,0] \n",
    "\n",
    "t = np.arange(tstart,tstop+1,increment)\n",
    "\n",
    "c = 2 # Damping constant \n",
    "k = 0.1 # Stiffness of the spring\n",
    "m = 5 # Mass \n",
    "F0 = 1\n",
    "\n",
    "def mydiff(x, t):    \n",
    "    F =F0\n",
    "    \n",
    "    dx1dt = x[1] \n",
    "    dx2dt = (F -c*x[1] -k*x[0])/m\n",
    "    dxdt= [dx1dt, dx2dt] \n",
    "    \n",
    "    return dxdt \n",
    "\n",
    "x_full_sol = odeint(mydiff, x_init, t) \n",
    "\n",
    "x_sol = x_full_sol[:,0]\n",
    "v_sol = x_full_sol[:,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BR02v-fkNKGV"
   },
   "outputs": [],
   "source": [
    "level = \"low\"\n",
    "label = \"1D_SMD_stan_\" + level\n",
    "\n",
    "loss_thresh = 0.005\n",
    "\n",
    "t_test = t.reshape(-1,1)\n",
    "t_test_tensor = torch.from_numpy(t_test).float().to(device)\n",
    "\n",
    "# bc1_t = t[0].reshape(-1,1)\n",
    "# bc1_x = x_sol[0].reshape(-1,1)\n",
    "# t_bc1_train = torch.from_numpy(bc1_t).float().to(device)\n",
    "# x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "\n",
    "bc1_t = t.reshape(-1,1)\n",
    "bc1_x = x_sol.reshape(-1,1)\n",
    "t_bc1_train = torch.from_numpy(bc1_t).float().to(device)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "\n",
    "\n",
    "bc2_t = t[0].reshape(-1,1)\n",
    "t_bc2_train = torch.from_numpy(bc2_t).float().to(device)\n",
    "bc2_val = v_sol[0].reshape(-1,1)\n",
    "bc2_val =torch.from_numpy(bc2_val).float().to(device)\n",
    "\n",
    "t_test = t.reshape(-1,1)\n",
    "t_test_tensor = torch.from_numpy(t_test).float().to(device)\n",
    "x_true = x_sol\n",
    "x_true_norm = np.linalg.norm(x_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(t[0]) \n",
    "ub = np.array(t[-1]) \n",
    "\n",
    "#torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SyyktBKBXRo1"
   },
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    \n",
    "    t01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=t01,random_state =seed)\n",
    "\n",
    "    t_coll = lb + (ub-lb)*sampling(N_f)\n",
    "    # t_coll = np.vstack((t_coll,)) # append training points to collocation points \n",
    "\n",
    "    return t_coll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "o1b21zLnNKGW"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "        \n",
    "        self.beta = Parameter(0.0*torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "        \n",
    "        self.k = Parameter(torch.tensor(0.0))\n",
    "        self.k.requiresGrad = True\n",
    "        self.c = Parameter(torch.tensor(0.0))\n",
    "        self.c.requiresGrad = True\n",
    "        self.m = Parameter(torch.tensor(0.0))\n",
    "        self.m.requiresGrad = True\n",
    "        \n",
    "    'forward pass'\n",
    "    def forward(self,t):\n",
    "        if torch.is_tensor(t) != True:         \n",
    "            t = torch.from_numpy(t)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        t = 2.0*(t - l_b)/(u_b - l_b) - 1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = t.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z) + self.beta[:,i]*z*self.activation(z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,t,x):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(t), x)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,t_bc2,bc2_val):\n",
    "        g = t_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        x = self.forward(g)    \n",
    "            \n",
    "        x_t = autograd.grad(x,g,torch.ones([t_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dx_dt = x_t[:,[0]]\n",
    "        \n",
    "        bc2 = dx_dt\n",
    "        \n",
    "        loss_bc2= self.loss_function(bc2,bc2_val)\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self,t_coll,f_hat):\n",
    "             \n",
    "        g = t_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        x = self.forward(g) \n",
    "\n",
    "        x_t = autograd.grad(x,g,torch.ones([t_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        x_tt = autograd.grad(x_t,g,torch.ones(t_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dx_dt = x_t[:,[0]]\n",
    "        \n",
    "        dx2_d2t = x_tt[:,[0]]\n",
    "        \n",
    "        f = self.m*dx2_d2t + self.c*dx_dt + self.k*x - F0\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,t_bc1,x_bc1,t_bc2,bc2_val,t_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(t_bc1,x_bc1)\n",
    "        loss_bc2 = self.loss_BC2(t_bc2,bc2_val)\n",
    "        loss_f = self.loss_PDE(t_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self):\n",
    "        x_pred = self.forward(t_test_tensor)\n",
    "        x_pred = x_pred.cpu().detach().numpy()\n",
    "\n",
    "        return x_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        x_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(x_pred.reshape(-1,1) - x_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(x_pred.reshape(-1,1) - x_true.reshape(-1,1),2)/x_true_norm\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fLY2mT5BOgjD"
   },
   "outputs": [],
   "source": [
    "def train_step(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8srA5uGuObil"
   },
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0SezTZ_racQB"
   },
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "\n",
    "    t_coll = colloc_pts(N_f,rep*11)\n",
    "    t_coll =  torch.from_numpy(t_coll).float().to(device)\n",
    "    \n",
    "    f_hat = torch.zeros(t_coll.shape[0],1).to(device)\n",
    "    for i in range(max_iter):      \n",
    "        train_step(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat)\n",
    "         \n",
    "        loss_np = PINN.loss(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test RE\",test_re_loss[-1],\"c\",PINN.c.cpu().detach().numpy(),\"k\",PINN.k.cpu().detach().numpy(),\"m\",PINN.m.cpu().detach().numpy())\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I9BYbcJ0NKGX",
    "outputId": "42720d97-b37a-4c42-b7e5-b77a0ef391f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 756.2536 Test RE 0.37184690552596916 c -0.008580634 k 0.13766417 m -3.9626255e-05\n",
      "1 Train Loss 198.3879 Test RE 0.18331061978579796 c 0.0010812031 k 0.10291811 m 4.3947002e-05\n",
      "2 Train Loss 18.6748 Test RE 0.0300298992042438 c 0.060079012 k 0.123548895 m 0.0029435214\n",
      "3 Train Loss 9.488646 Test RE 0.01013288750147231 c 0.6740592 k 0.11669718 m 0.050513756\n",
      "4 Train Loss 4.9248123 Test RE 0.015706754142740316 c 1.556143 k 0.10568728 m 0.121905595\n",
      "5 Train Loss 2.5178342 Test RE 0.006371832807583472 c 2.193433 k 0.093604185 m 0.18873002\n",
      "6 Train Loss 1.8242979 Test RE 0.006205724186105709 c 2.272769 k 0.092669666 m 0.27320305\n",
      "7 Train Loss 1.1989921 Test RE 0.005642526977778063 c 2.1115048 k 0.09533122 m 0.35628253\n",
      "8 Train Loss 1.038624 Test RE 0.00538919168001912 c 2.131533 k 0.095005035 m 0.43122298\n",
      "9 Train Loss 0.7677573 Test RE 0.007741906642747897 c 2.0007994 k 0.09732248 m 0.6764939\n",
      "10 Train Loss 0.5614297 Test RE 0.006585005641378428 c 2.0776503 k 0.096648894 m 0.9991516\n",
      "11 Train Loss 0.5315831 Test RE 0.006200855639330906 c 2.0868354 k 0.0965135 m 1.085276\n",
      "12 Train Loss 0.49829856 Test RE 0.006004396168596417 c 2.0769007 k 0.096595556 m 1.1755698\n",
      "13 Train Loss 0.48535237 Test RE 0.006140000992389144 c 2.0731525 k 0.09658586 m 1.162769\n",
      "14 Train Loss 0.4713404 Test RE 0.005819804765965578 c 2.0670764 k 0.096670985 m 1.2119066\n",
      "15 Train Loss 0.447638 Test RE 0.006108961781288266 c 2.0540636 k 0.09658879 m 1.3612362\n",
      "16 Train Loss 0.4253023 Test RE 0.005312021116959976 c 2.0757532 k 0.096824884 m 1.4720582\n",
      "17 Train Loss 0.40091586 Test RE 0.005147793671787128 c 2.0598946 k 0.09703295 m 1.5886523\n",
      "18 Train Loss 0.39239544 Test RE 0.005098949204751645 c 2.0562005 k 0.0971468 m 1.6992172\n",
      "19 Train Loss 0.37706253 Test RE 0.005225516738455369 c 2.0605004 k 0.09718087 m 1.850166\n",
      "20 Train Loss 0.3508459 Test RE 0.0050710163806475615 c 2.0644937 k 0.09710833 m 2.0055768\n",
      "21 Train Loss 0.33008868 Test RE 0.004830647043217785 c 2.0477612 k 0.097539194 m 2.120675\n",
      "22 Train Loss 0.314915 Test RE 0.0044720548694176295 c 2.0518026 k 0.09738716 m 2.1870158\n",
      "23 Train Loss 0.28797132 Test RE 0.0038248981139873185 c 2.038403 k 0.09801471 m 2.428114\n",
      "24 Train Loss 0.23305087 Test RE 0.0035750741347157174 c 2.0337656 k 0.098309994 m 2.8976724\n",
      "25 Train Loss 0.18530057 Test RE 0.0032722085483427887 c 2.0372648 k 0.098282665 m 3.2164128\n",
      "26 Train Loss 0.12845725 Test RE 0.002099228070639303 c 2.0107484 k 0.099455535 m 3.8805823\n",
      "27 Train Loss 0.09804812 Test RE 0.0011937022595249658 c 2.0096602 k 0.09960755 m 4.319719\n",
      "28 Train Loss 0.094283044 Test RE 0.0011293965462121922 c 2.0160866 k 0.09956909 m 4.4745026\n",
      "29 Train Loss 0.09128563 Test RE 0.0010253968765590368 c 2.002801 k 0.09989693 m 4.613701\n",
      "30 Train Loss 0.08443699 Test RE 0.0009144897162619204 c 2.0176747 k 0.09965108 m 4.4906435\n",
      "31 Train Loss 0.07076202 Test RE 0.0009317780468924133 c 2.002079 k 0.09962821 m 4.5480294\n",
      "32 Train Loss 0.060248915 Test RE 0.0008517468853458405 c 2.0205927 k 0.099387236 m 4.4642544\n",
      "33 Train Loss 0.049770497 Test RE 0.000895357464855388 c 1.9943337 k 0.0996446 m 4.582982\n",
      "34 Train Loss 0.039667122 Test RE 0.0011855572768091002 c 2.011863 k 0.09945134 m 4.6908827\n",
      "35 Train Loss 0.025826797 Test RE 0.0011875717701723574 c 1.9996735 k 0.09981184 m 4.948957\n",
      "36 Train Loss 0.018659757 Test RE 0.0009833341946404368 c 2.0049255 k 0.0998175 m 5.04689\n",
      "37 Train Loss 0.010508884 Test RE 0.0009787264325629784 c 2.002602 k 0.099964194 m 5.1864095\n",
      "38 Train Loss 0.0058488576 Test RE 0.0006139186287148745 c 1.9945402 k 0.10013988 m 5.1680164\n",
      "39 Train Loss 0.0042138393 Test RE 0.0003817503799614934 c 1.9969393 k 0.10008852 m 5.1057234\n",
      "40 Train Loss 0.0037964897 Test RE 0.00027753979639149077 c 1.9981177 k 0.100076646 m 5.0558233\n",
      "41 Train Loss 0.003174461 Test RE 0.0002668274987449965 c 2.0005324 k 0.099960625 m 4.9963818\n",
      "42 Train Loss 0.0031491965 Test RE 0.00025784041218336026 c 2.0006206 k 0.09996371 m 4.9845724\n",
      "43 Train Loss 0.0030801 Test RE 0.00025073633828128416 c 2.0007403 k 0.09996757 m 4.978589\n",
      "44 Train Loss 0.0030010957 Test RE 0.0002528847938677488 c 2.0008287 k 0.099950075 m 4.9887853\n",
      "45 Train Loss 0.0023424514 Test RE 0.0001690665825114379 c 1.9973595 k 0.10010373 m 5.04109\n",
      "46 Train Loss 0.0022192665 Test RE 0.00017581781452648504 c 1.9979268 k 0.100100085 m 5.072178\n",
      "47 Train Loss 0.0020021645 Test RE 0.0001804669754811543 c 1.9984072 k 0.10010198 m 5.071886\n",
      "48 Train Loss 0.0013693821 Test RE 0.00012245970677103184 c 1.9985825 k 0.10007059 m 5.056427\n",
      "49 Train Loss 0.0011339855 Test RE 7.118803596386387e-05 c 1.9992965 k 0.10003845 m 5.012486\n",
      "50 Train Loss 0.0011158399 Test RE 6.215768188784977e-05 c 1.9996035 k 0.1000311 m 5.004751\n",
      "51 Train Loss 0.0011150094 Test RE 6.185941522780107e-05 c 1.9995668 k 0.10003124 m 5.004925\n",
      "52 Train Loss 0.0009862139 Test RE 6.805137405118077e-05 c 1.9984975 k 0.10004984 m 5.0107584\n",
      "53 Train Loss 0.00076360063 Test RE 4.748280715275631e-05 c 1.9997568 k 0.10002362 m 5.007014\n",
      "54 Train Loss 0.0007252586 Test RE 5.7364271886708726e-05 c 1.9993908 k 0.1000399 m 5.016664\n",
      "55 Train Loss 0.0006511612 Test RE 6.843837221203868e-05 c 1.9989046 k 0.10005349 m 5.0210824\n",
      "56 Train Loss 0.00062644307 Test RE 7.153498961139066e-05 c 1.9991288 k 0.1000418 m 5.0159855\n",
      "57 Train Loss 0.0006120097 Test RE 7.147542086845374e-05 c 1.9994075 k 0.100035086 m 5.0048933\n",
      "58 Train Loss 0.0006114399 Test RE 7.139490902673881e-05 c 1.9994131 k 0.100035 m 5.004489\n",
      "59 Train Loss 0.00061087456 Test RE 7.158751692713641e-05 c 1.9994212 k 0.10003733 m 5.0044813\n",
      "60 Train Loss 0.0006104183 Test RE 7.154702044847917e-05 c 1.9994272 k 0.10003701 m 5.0041895\n",
      "61 Train Loss 0.000608745 Test RE 7.16735264163774e-05 c 1.9994307 k 0.10003729 m 5.0030055\n",
      "62 Train Loss 0.00060850754 Test RE 7.168913606967348e-05 c 1.9994133 k 0.10003774 m 5.0029216\n",
      "63 Train Loss 0.0006080131 Test RE 7.202923765180565e-05 c 1.9993739 k 0.10003886 m 5.002692\n",
      "64 Train Loss 0.0006080131 Test RE 7.202923765180565e-05 c 1.9993739 k 0.10003886 m 5.002692\n",
      "65 Train Loss 0.0006080131 Test RE 7.202923765180565e-05 c 1.9993739 k 0.10003886 m 5.002692\n",
      "66 Train Loss 0.0006080131 Test RE 7.202923765180565e-05 c 1.9993739 k 0.10003886 m 5.002692\n",
      "67 Train Loss 0.0006080131 Test RE 7.202923765180565e-05 c 1.9993739 k 0.10003886 m 5.002692\n",
      "68 Train Loss 0.0006080131 Test RE 7.202923765180565e-05 c 1.9993739 k 0.10003886 m 5.002692\n",
      "69 Train Loss 0.0006080131 Test RE 7.202923765180565e-05 c 1.9993739 k 0.10003886 m 5.002692\n",
      "70 Train Loss 0.0006080131 Test RE 7.202923765180565e-05 c 1.9993739 k 0.10003886 m 5.002692\n",
      "71 Train Loss 0.0006080131 Test RE 7.202923765180565e-05 c 1.9993739 k 0.10003886 m 5.002692\n",
      "72 Train Loss 0.0006080131 Test RE 7.202923765180565e-05 c 1.9993739 k 0.10003886 m 5.002692\n",
      "73 Train Loss 0.0006080131 Test RE 7.202923765180565e-05 c 1.9993739 k 0.10003886 m 5.002692\n",
      "74 Train Loss 0.0006080131 Test RE 7.202923765180565e-05 c 1.9993739 k 0.10003886 m 5.002692\n",
      "75 Train Loss 0.0006080131 Test RE 7.202923765180565e-05 c 1.9993739 k 0.10003886 m 5.002692\n",
      "76 Train Loss 0.0006080131 Test RE 7.202923765180565e-05 c 1.9993739 k 0.10003886 m 5.002692\n",
      "77 Train Loss 0.0006080131 Test RE 7.202923765180565e-05 c 1.9993739 k 0.10003886 m 5.002692\n",
      "78 Train Loss 0.0006080131 Test RE 7.202923765180565e-05 c 1.9993739 k 0.10003886 m 5.002692\n",
      "79 Train Loss 0.0006080131 Test RE 7.202923765180565e-05 c 1.9993739 k 0.10003886 m 5.002692\n",
      "80 Train Loss 0.0006080131 Test RE 7.202923765180565e-05 c 1.9993739 k 0.10003886 m 5.002692\n",
      "81 Train Loss 0.0006080131 Test RE 7.202923765180565e-05 c 1.9993739 k 0.10003886 m 5.002692\n",
      "82 Train Loss 0.0006080131 Test RE 7.202923765180565e-05 c 1.9993739 k 0.10003886 m 5.002692\n",
      "83 Train Loss 0.0006080131 Test RE 7.202923765180565e-05 c 1.9993739 k 0.10003886 m 5.002692\n",
      "84 Train Loss 0.0006080131 Test RE 7.202923765180565e-05 c 1.9993739 k 0.10003886 m 5.002692\n",
      "85 Train Loss 0.0006080131 Test RE 7.202923765180565e-05 c 1.9993739 k 0.10003886 m 5.002692\n",
      "86 Train Loss 0.0006080131 Test RE 7.202923765180565e-05 c 1.9993739 k 0.10003886 m 5.002692\n",
      "87 Train Loss 0.0006080131 Test RE 7.202923765180565e-05 c 1.9993739 k 0.10003886 m 5.002692\n",
      "88 Train Loss 0.0006080131 Test RE 7.202923765180565e-05 c 1.9993739 k 0.10003886 m 5.002692\n",
      "89 Train Loss 0.0006080131 Test RE 7.202923765180565e-05 c 1.9993739 k 0.10003886 m 5.002692\n",
      "90 Train Loss 0.0006080131 Test RE 7.202923765180565e-05 c 1.9993739 k 0.10003886 m 5.002692\n",
      "91 Train Loss 0.0006080131 Test RE 7.202923765180565e-05 c 1.9993739 k 0.10003886 m 5.002692\n",
      "92 Train Loss 0.0006080131 Test RE 7.202923765180565e-05 c 1.9993739 k 0.10003886 m 5.002692\n",
      "93 Train Loss 0.0006080131 Test RE 7.202923765180565e-05 c 1.9993739 k 0.10003886 m 5.002692\n",
      "94 Train Loss 0.0006080131 Test RE 7.202923765180565e-05 c 1.9993739 k 0.10003886 m 5.002692\n",
      "95 Train Loss 0.0006080131 Test RE 7.202923765180565e-05 c 1.9993739 k 0.10003886 m 5.002692\n",
      "96 Train Loss 0.0006080131 Test RE 7.202923765180565e-05 c 1.9993739 k 0.10003886 m 5.002692\n",
      "97 Train Loss 0.0006080131 Test RE 7.202923765180565e-05 c 1.9993739 k 0.10003886 m 5.002692\n",
      "98 Train Loss 0.0006080131 Test RE 7.202923765180565e-05 c 1.9993739 k 0.10003886 m 5.002692\n",
      "99 Train Loss 0.0006080131 Test RE 7.202923765180565e-05 c 1.9993739 k 0.10003886 m 5.002692\n",
      "Training time: 27.70\n",
      "Training time: 27.70\n"
     ]
    }
   ],
   "source": [
    "max_reps = 1\n",
    "max_iter = 100\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "\n",
    "for reps in range(max_reps):   \n",
    "  train_loss = []\n",
    "  test_mse_loss = []\n",
    "  test_re_loss = []   \n",
    "  beta_val = []\n",
    "\n",
    "  torch.manual_seed(reps*36)\n",
    "  N_f = 10000 #Total number of collocation points\n",
    "\n",
    "  layers = np.array([1,50,50,50,1]) #9 hidden layers\n",
    "  PINN = Sequentialmodel(layers)\n",
    "\n",
    "  PINN.to(device)\n",
    "\n",
    "  'Neural Network Summary'\n",
    "  print(PINN)\n",
    "\n",
    "  params = list(PINN.parameters())\n",
    "\n",
    "  optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.1, \n",
    "                            max_iter = 10, \n",
    "                            max_eval = 15, \n",
    "                            tolerance_grad = 1e-6, \n",
    "                            tolerance_change = 1e-6, \n",
    "                            history_size = 100, \n",
    "                            line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "  train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "  torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "  train_loss_full.append(train_loss)\n",
    "  test_mse_full.append(test_mse_loss)\n",
    "  test_re_full.append(test_re_loss)\n",
    "  beta_full.append(beta_val)\n",
    "\n",
    "\n",
    "  print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"beta\": beta_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "pmHEeBpzfFQh",
    "outputId": "990b2054-f35e-4c8e-c378-69ff2eb19f4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f90000e63d0>]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGdCAYAAAA44ojeAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA/7ElEQVR4nO3deVxU9eL/8dewDYuAisqiqKi4gfsKmmgp3VvZ9ZutWtn601ySzDSz0sxE7WZpuaQtWlbWvWbZqliG+4ZiiruiuCGugIogzPn9Yc2N1NIaODPwfj4e83jIOYfhPR+Refs553ywGIZhICIiIuJE3MwOICIiIvJ7KigiIiLidFRQRERExOmooIiIiIjTUUERERERp6OCIiIiIk5HBUVEREScjgqKiIiIOB0PswP8FTabjSNHjuDv74/FYjE7joiIiFwDwzDIzc0lLCwMN7c/niNxyYJy5MgRwsPDzY4hIiIif8HBgwepUaPGHx5z3QVl2bJlvPrqq6SkpHD06FEWLFhAjx497PsNw+Cll15i5syZnD59mnbt2jF16lSioqLsx+Tn5zN06FA++eQT8vLyuOmmm5g2bdqfhv2Vv7+//QUGBARc70sQERERE+Tk5BAeHm5/H/8j111Qzp07R7NmzXj44Yfp2bPnZfsnTpzIpEmTmD17NvXr12fs2LF069aNnTt32gMlJCTw1VdfMW/ePIKCgnj66ae57bbbSElJwd3d/U8z/HpaJyAgQAVFRETExVzL5RmWv/PLAi0WS7EZFMMwCAsLIyEhgeHDhwOXZkuCg4OZMGECffv2JTs7m6pVq/Lhhx9yzz33AP87ZfPtt99y8803/+nXzcnJITAwkOzsbBUUERERF3E9798OvYsnPT2dzMxM4uPj7dusVitxcXGsWrUKgJSUFC5evFjsmLCwMKKjo+3H/F5+fj45OTnFHiIiIlJ2ObSgZGZmAhAcHFxse3BwsH1fZmYmXl5eVKpU6arH/F5iYiKBgYH2hy6QFRERKdtKZB2U359bMgzjT883/dExI0aMIDs72/44ePCgw7KKiIiI83FoQQkJCQG4bCYkKyvLPqsSEhJCQUEBp0+fvuoxv2e1Wu0XxOrCWBERkbLPoQUlIiKCkJAQkpKS7NsKCgpITk4mNjYWgFatWuHp6VnsmKNHj7J161b7MSIiIlK+XfdtxmfPnmXPnj32j9PT00lNTaVy5crUrFmThIQExo0bR2RkJJGRkYwbNw5fX1969eoFQGBgII8++ihPP/00QUFBVK5cmaFDh9KkSRO6du3quFcmIiIiLuu6C8qGDRvo0qWL/eMhQ4YA0KdPH2bPns2wYcPIy8ujf//+9oXaFi9eXGxRltdffx0PDw/uvvtu+0Jts2fPvqY1UERERKTs+1vroJhF66CIiIi4HtPWQRERERFxBBUUERERcToqKCIiIuJ0rvsiWRERESkbDAOys+HoUcjMhLy1P+O2KYXj5/040O5unn/evGwqKCIiImVQQQEcOQIn1+wmf91mCtIPYxw6jMfxo3hnH8P/fCaVCzJpyzoOUBuAcXzCCMazgVaM3aWCIiIiItch93AOx9bu58y2I+TtOUxRxmHcjh7G+9QR/HMPc5/Pl2w+den31o3jPUYw/qrPFUImZwJrExoKhZampJ75B2fDGjPooVJ6MVehgiIiIuJEbAWFnNh8mBMpB8hJy+DingO4ZRzg7Rov8/OxYA4cgGfOjOc5Eq/6HH55GUA4Xl5wPKAxW/JjOVuxOherhEFYGNZaIfjVDaFiwxB+bBeJb9Cvn3nfLw/oXMKv88+ooIiIiJQiW14+x9bu5/jaffzs34E9WQFkZEDTldO5e18iwYWHqYaNar/7vOHbHmQzl35n3WGqc9ISxAlrdXL8q5NXuTq2kOq41ayOT50w3r6pMSGNICgILJYHgAdK/XX+XSooIiIiDnbuHKSnw4mkTXgs+Q639L34HdtH1Zy9hBQeIhSDUKA/y1lJRwCewCCMgwAU4MkR95qcrFCTs0G1KAytSf+bQ3muFdSqBTXD++MfMICgP8jg6lRQRERErpdhkLP7GEeW7uTM+l0U7tiL18G9BJ7cy2Dvt1l0sg0AT7CaaYy87NPP4schrzrENC2kcQuoWRMaBvRgk2dLqrSqRUizYGpb3X65dPVKLCX1ypyGCoqIiMhVFJw+x+Glu9h+riZpmUHs2gXhyz8mYdcTBBg5XGmx9qBzu4A2VKoE54JbkZz3IAXhdfGoXwf/5nWp1r4OYc2r0dDTwqvFPjPsl4eACoqIiAin089waMF6ctZux9ixkwqHdxJ8ZiehRYeIAEbyEZ/QC4B/UIkXyaEINw661+ZYQH3OhdbDFlEX78Z1GNa1HW+1gUqVANr98pDrpYIiIiLlg2FwescxDi1KI3tVGqvcOrLoeEvS0qDtsWUs5F9X/LSTBNG45jnujYUGDaBxeEe2+qdRs0tdale1/sFpGPk7VFBERKTMyc6G7clZXPz4MyxpaQQcTKNGThqVjVNU+uWY73mZH2kJwFai2ePRgGNBjblQswFujRoQ2LYBoXH1CYkK4vlil3z4A41L+RWVPyooIiLismyFNg4l7+Xo95u5sCaVlQVtmJX1L/bvh0acYBuDih+PhQMedcmsHEWDtvV57w6IioJGjerg77+Deua8DLkCFRQREXEJ58/DtvXnyHvnI4zUVCodSKV27hZqcpaavxyznwfZ/8upmgs1Illd8C/yajfCvVkUQTdEUevmhkRU8yECiDHtlci1UEERERGnc/5kHnvnp3Jq8Qa2H6vMm6d6s2MHWG0WcnkCd2z2Y/PwZp9vNCeqN6dKTDeWPgxNm0Llyp7AF6a9Bvl7VFBERMRUF/IM9ny2kZPfr8eycQPVMjZQ78JWmlAEgAexbKM3AP7VfEnyeAhr9Sp4tWlGtfjm1I6vT5SP3s7KGv2NiohIqbEVGez9bhe7lx7iy7M3sWEDbNliIf3i7URzpNixWW7B7K/ahrwmHfnmKWjZEkJCAN41JbuULhUUEREpMWcO5rL7o3XkLl6N35bVRJ5cQ6RxigCqcSuZ/Loi6jJrPPUrHOFsw9Z439CGmne0JqRVdaq5lf0VU+XKVFBERMQhbEUGO3ZaWL0aVq+GW//7MLdnf0Cb31wvApeuGTkWUJ8XH8mhacdAWreGmjXfx6IuIr+hgiIiIn9JYYGNnfO3kvnZMrzXJlMncyXtjR3k/rIAfEOCcMfGYY+aZFSPobBNLFVvj6Fez2Y09fWiqcn5xbmpoIiIyDXJz4ct83dxZu5XVNiQTP3jK4jiNFG/OeYGr3Wcbd+VmBhoUv8pjrd8iurNq1PdtNTiqlRQRETkii7kXmTr7PX8eKAe36VUY80aeORCElMZaj/mLH7sqtqBs63iCOrRiS96t8Gzwq97VUvkr1NBERERAIoKDXZ8vo3MuUvwXf0DUSd+ojW5zORtfuL/AbC58o1ssHbnQttOVLsrjrp3tqClVW8l4nj6rhIRKacMA/buhTX/OUjNGc9R/+ASoozMYqdsTlkq07FFHq37QqdO0KBBIyyWhaZllvJDBUVEpBzJPnKOrW8uZcNGN17feQsHDkBFKnCCj3HHRh7e7KjaiXMxNxH2QFci/q85D7q7mR1byiEVFBGRMsywGez8ejeHZn1HwMpvaXo6mQ7kY9CBA9yCpyc0i63E4gqTqfmPKBo8FEOLCt5mxxZRQRERKWtycmDJErCOGUnU1k9pWLSXhr/Zf9ijFkZ0S74bZ3BDJwt+fgADTUorcmUqKCIiZcD+5QfZNv0nXj36ACtWQGEhzGc7tdlLAZ5sC+rEubhbqNXvn9To2pDqWhVNnJwKioiICzJsBts+28rR6V8Quu5Loi6kUBsYTHsKiSQyEg42G8LGqD40HnAjzav6mx1Z5LqooIiIuIiCAlg/ZxsXpr5Dva1fEFWUbr/jxoaFbYExjH30NK2egHr1ADqamFbk71FBERFxYnnZBSR9dYF53wbwzTfQOWc3X/I6ABewsjW0G4W39qDh0NuIbhBMtMl5RRxFBUVExMlcyClg06tLKPjoM5qlf0EqT/IJYwD4uVo3VlZ8GO+7uhP1VDytg/xMTitSMlRQREScwIWcAjb9+wcKPvqMpvu+IIYz9n3x1mXkDoKePaFtW1/c3N4zL6hIKVFBERExycWLsGgRfDrPYMzHjYgx9tn3ZbmFsKvpnVTudzftHutAe3cTg4qYQAVFRKQUGTaDrR9s5MD0b3loz/OcPGUBLNxIJ3zdzrO7SU8q9b2bRo91oJqnWomUXyooIiKlYP9P+9n38kfUWj6XJhd30ASI4B94BLfhnnug8S2vU/XGdwhWKREBVFBERErMqX1n+Pn5z6j49Vya5y6n9i/b8/Amtda/eGO4lXaPg4cHQEXTcoo4IxUUEREHstkuLTP/7rtw/vM1fFXY99J2LGyufCPne95Ps9F3EBMWYHJSEeemgiIi4gCHVmWwe+RsVqRW4MUzQwBwoxsr/f9BYdxNNHzpPlq0rG5yShHXoYIiIvIX5efkk/Lil3jNfZeWJ5OogUEjgpkSOIh77vfk0UfdadHiO7NjirgkFRQRkeu0P2k3+0fMoMnGOcQaJ+3bN1XsQt59j5AxHnx0Bkfkb1FBERG5BkVF8PXXMG0a3L74DQYwDYCjbtXZGfMQdV5+mBZd6pqcUqTsUEEREfkDx7ceI+2pd3htSzxfH2sDwBGeoG3V/dC/Py2f+wehXro1WMTRVFBERH7HsBlsmb6CsxOn0TpjPp25yCG2szpoLo89Bn37RhMR8Y3ZMUXKNBUUEZFfFOTms/7peVSZ+zpN8zbbt2+p0J5aD97GodfA29vEgCLliAqKiJR7p07B229D/Kg4OlxcC8B5fFjf4H6qvvAETXq3MDmhSPmjgiIi5Vb6dzuY9GVd3vvQk/Pn4RQ9CXU7xI5ug2j+1uPE1atsdkSRcksFRUTKFcNmsGXKUvITX6NN1rcc5xPOcy/Nm0OLgQOpcm8CN/p5mh1TpNxTQRGRcsEosrHhxYX4Tkmk6dl1wKXl53vW+5l+s+4lLg4sFh+TU4rIr1RQRKRMK7xosP7JD6n6/gTa5G8DLv2yvrVRj1Lr9QTu6lbP5IQiciVuZgcQESkJFy7AjBnQoAHYZrxNvfxtZBPAj+2fI3fLATpvfYsIlRMRp6UZFBEpU85l5pLy6DT6bnicHVmVAQuTAl4iv90GWs56ghtrBZodUUSugQqKiJQJ57POkvLIVBp/+yqdjJPcRT6zw19k6FB47LGu+Pp2NTuiiFwHFRQRcWl5J8+z4ZFpNP5qAjcYJwDY71GPzo835vk3wMvL3Hwi8teooIiIS7pwAdbe/yaNF4zlBlsWAPs96nLggReIndqb2j768SbiynSRrIi4lIsXYfp0qFcP0uenUNWWRYZ7BMl93iMsewdx7/XBU+VExOXpX7GIuATbxSLWJXzMyK/a8+PBSADeCXmB2l1vIGb6g9SsoMXVRMoSh8+gFBYW8vzzzxMREYGPjw916tRhzJgx2Gw2+zGGYTB69GjCwsLw8fGhc+fOpKWlOTqKiJQFhsGmV75lT2BL2k97kL4HR1KtGkyZAkvS69L5w0exqpyIlDkOn0GZMGECM2bMYM6cOURFRbFhwwYefvhhAgMDGTx4MAATJ05k0qRJzJ49m/r16zN27Fi6devGzp078ff3d3QkEXFRO2avIf+pZ2lxJhmAMwQS1K0Ve+cbVPC3mJxOREqSxTAMw5FPeNtttxEcHMy7775r39azZ098fX358MMPMQyDsLAwEhISGD58OAD5+fkEBwczYcIE+vbte9lz5ufnk5+fb/84JyeH8PBwsrOzCQgIcGR8EXECGT/s5uhDz9Lu0OcAXMDKqpaDaPrJCKrU1y/wE3FVOTk5BAYGXtP7t8NP8XTs2JEffviBXbt2AbB582ZWrFjBLbfcAkB6ejqZmZnEx8fbP8dqtRIXF8eqVauu+JyJiYkEBgbaH+Hh4Y6OLSJOICcHhg+H92/+hHaHPqcIN5LrPkzWit3cmPKqyolIOeLwUzzDhw8nOzubhg0b4u7uTlFREa+88gr33XcfAJmZmQAEBwcX+7zg4GAOHDhwxeccMWIEQ4YMsX/86wyKiJQNRfmFfDo5k6deq0FWFvgwlPah6dR6cyhxPaPMjiciJnB4Qfn000+ZO3cuH3/8MVFRUaSmppKQkEBYWBh9+vSxH2exFD9/bBjGZdt+ZbVasVqtjo4qIk5g08Qk/F98iob5XpxgPfXruzNpki/xt7zPVX4kiEg54PCC8swzz/Dss89y7733AtCkSRMOHDhAYmIiffr0ISQkBLg0kxIaGmr/vKysrMtmVUSk7DqQtIusB56mzbGvAThlqcz7w3dx70uNtPqriDj+GpTz58/j5lb8ad3d3e23GUdERBASEkJSUpJ9f0FBAcnJycTGxjo6jog4mfPHz5Hc4TlC46Npc+xrLuLB0mYJsHsPDyaqnIjIJQ6fQenevTuvvPIKNWvWJCoqik2bNjFp0iQeeeQR4NKpnYSEBMaNG0dkZCSRkZGMGzcOX19fevXq5eg4IuJEkt7NoHHfG4grygBgbZVbCJo9iS63NjA5mYg4G4cXlDfffJMXXniB/v37k5WVRVhYGH379uXFF1+0HzNs2DDy8vLo378/p0+fpl27dixevFhroIiUUfv2weDB8M3XNVhBdWzuFg4NnUz7cbdjcdOFJiJyOYevg1Iaruc+ahExz4XTeay4ewr3Lh/AyfwKeHrCmMcyGPRSFfyq+podT0RK2fW8f+t38YhIidiYuIiqLz5B18J0nuEUi2+cwNSp0LBhTbOjiYgLUEEREYc6tfsk2//5FB32fgjAUbfqdBzclmGvoduGReSaOfwuHhEpnwybweonP8HWoBEd9n6IDQtLmw3GL2M7HSb1VDkRkeuiGRQR+dsOHoQf48fTZ8dzAOyyRnPhzXfo8ng7k5OJiKvSDIqI/GU2G0ydCo0bw3M7HiSLqiztMoZax1NoqnIiIn+DZlBE5C858MMevn7sCwbuHwpAk5jqnJySTpfWfiYnE5GyQAVFRK6LrdDG8t4zaP3ZMwzgPD95N6Tzv2/jiSfAzU3lREQcQwVFRK7Z4dUZZN72KHGnlgCwsWIXXv8qmhodTQ4mImWOrkERkT9l2AyWPzqbCrFNaHVqCefx4aeeU2h+fAk1OtY2O56IlEGaQRGRP3T0KGxp9zjxB98FYEuF9lT47xw631zf5GQiUpZpBkVEruqLLyA6GqYdvI18vPjpH+NpfHIFESonIlLCNIMiIpc5f+I8b/TdzsjPWwFwoHkP9k3cS+duNUxOJiLlhWZQRKSYnZ+mcrR6K/p+Hk8Yhxk2DNauhUYqJyJSijSDIiLApduHl905hZgvh2OlgEy3UBa8cYi2g6qbHU1EyiEVFBEha8sxDtz4EJ1PfA/A2pDbqffTu7RtUMXkZCJSXukUj0g5t/G1pdC8GW1OfE8e3iy7dxptD39BkMqJiJhIMygi5VRREbzyCoSO+oSWHGO3NRq3z+bR6fYos6OJiKigiJRHWVlw//2QlATeTCa4RShdFw/Ht4qv2dFERACd4hEpdzZPW8nS2g+zJMmGjw/MmO3D7RtfUjkREaeiGRSRcsJWaCP59te44bsRNKOIPcEt6fHDIKJ0RkdEnJAKikg5kHM4l7Q2fehydAEAq2r3YvDqh6kQYnIwEZGr0CkekTJu33c7yarTjpijC8jHi+W9ZxCzdy4VQiqYHU1E5KpUUETKsFWjFxF0S1vqFWznqHt19r63jBvm9sXiZjE7mojIH1JBESmDbDYYNQoGvlQVLwr4OaAjnqkbaPxwO7OjiYhcE12DIlLGnDll44E+bnz9NUBLpt+1lEHvt8TTz8vsaCIi10wzKCJlyL7vd3E0rCVHv96AtzfMmQNDPmuvciIiLkczKCJlxMZ//0jEsDupZJxmhtdgLMtX0Kq1rjUREdekgiJSBiy7fyYxHw3Ak0K2VmhP7dXzqRKtciIirksFRcSFFRUUsaLd08SlTgZgZe1etNr0Lt4VvU1OJiLy96igiLionKPn2NnsLuKOfwdActeX6bRopG4hFpEyQRfJirig9HTo2NWbzOPunMeHNUM+Iy7peZUTESkzNIMi4mI2bIBbb4WsLHeeDvmYL/69h/a9W5gdS0TEoTSDIuJC1o/5jnUxg8nKMmjeHJZu8KexyomIlEGaQRFxEcsfepeYOX1pQxFno1vzxLIH8Pc3O5WISMlQQRFxcobNIPnGl+ic/BIAK+r24am19+Lpa3IwEZESpIIi4sQunr/Imub96Lz7PQB+6vg8ccljdDGsiJR5KigiTups5lm2N7mLG058TxFurOo9jc5z+5odS0SkVOgiWREndOoUDOuyjhYnkjiHLykvfMkNKiciUo5oBkXEyRw5AvHxkLbjRi76fcjgyXVo+2g7s2OJiJQqFRQRJ5KxdC/3P+hG2qEIwsLgqaT7aNzY7FQiIqVPp3hEnMTuz7dg7dqR9w91pX2to6xcicqJiJRbKigiTiDt3TVUuTOOYFsmhd4V+HKhhdq1zU4lImIeFRQRk22cuIRaj3WlknGan/1jCd72E9WahpgdS0TEVCooIiZa9+LXRA2/lQqcY0NQPHX3LKZiRCWzY4mImE4FRcQkq0YvovnLd2ClgNVhPWmybyF+1fzMjiUi4hR0F4+ICebPhyfHNmcJdTlVszltd3yIp4/+OYqI/EozKCKl7LPP4J574EhRMJN7LqfdTpUTEZHfU0ERKUWr+s/l+3tnU1QEffrA1E+r4OGtciIi8nv6yShSSlY+PpuYdx6hPRB5W32GvxeLm/6LICJyRfrxKFIKVjz8LjHvPIIbBssb92X4gvYqJyIif0A/IkVK2Mp+HxI7+3HcMPipySA6bZmGm4f+6YmI/BH9lBQpQauf+oz2bz/0SzkZSFzqZCxuFrNjiYg4PRUUkRLy05SfafNGL9yxsaz+Y3TaqHIiInKtdJGsSAlYtAhuH9qEMTxNh9pH6LBlhk7riIhcB/3EFHGwpUuhRw8ouGhhQ8/xtN85B3cvd7NjiYi4FBUUEQf6edoKTne7G+PCBbp3h48+tuDhpX9mIiLXS6d4RBxk25z11B5wC03J5Z2Ihtz52Ri8vMxOJSLimvRfOxEH2PfNdoIf/icB5LKxYhfuWD8Cb2+zU4mIuK4SKSiHDx/m/vvvJygoCF9fX5o3b05KSop9v2EYjB49mrCwMHx8fOjcuTNpaWklEUWkxB1Zk4H3v+IJMk6S5tuGyLQv8Q3yMTuWiIhLc3hBOX36NB06dMDT05PvvvuObdu28dprr1GxYkX7MRMnTmTSpEm89dZbrF+/npCQELp160Zubq6j44iUqJM7jnMhLp6wokPs9WpIcMq3+If5mx1LRMTlWQzDMBz5hM8++ywrV65k+fLlV9xvGAZhYWEkJCQwfPhwAPLz8wkODmbChAn07dv3T79GTk4OgYGBZGdnExAQ4Mj4ItfsbK7Bjuo30Tp3KYfdw2HFSqq3Dzc7loiI07qe92+Hz6AsXLiQ1q1bc9ddd1GtWjVatGjBrFmz7PvT09PJzMwkPj7evs1qtRIXF8eqVauu+Jz5+fnk5OQUe4iYKT8f7uhpYUBuIrvdGnDhy8UqJyIiDuTwgrJv3z6mT59OZGQkixYtol+/fjz55JN88MEHAGRmZgIQHBxc7POCg4Pt+34vMTGRwMBA+yM8XG8EYp6iInjgAUhKgjS/dpxekUbdWxuaHUtEpExxeEGx2Wy0bNmScePG0aJFC/r27cvjjz/O9OnTix1nsRRf8tswjMu2/WrEiBFkZ2fbHwcPHnR0bJFrYtgMFse8yJ7/bMTTE774AtrGaBE2ERFHc3hBCQ0NpXHjxsW2NWrUiIyMDABCQkIALpstycrKumxW5VdWq5WAgIBiDxEzLOv+Kv9c/zJL6cJ/ph2na1ezE4mIlE0OLygdOnRg586dxbbt2rWLWrVqARAREUFISAhJSUn2/QUFBSQnJxMbG+voOCIOs2rwPOK+vXRh96bbR/Ovx6qanEhEpOxy+EqyTz31FLGxsYwbN467776bdevWMXPmTGbOnAlcOrWTkJDAuHHjiIyMJDIyknHjxuHr60uvXr0cHUfEITZPSabVlD4A/NQ8gbgvnjI5kYhI2ebwgtKmTRsWLFjAiBEjGDNmDBEREbzxxhv07t3bfsywYcPIy8ujf//+nD59mnbt2rF48WL8/bV+hDifPQu3USuhB1YKWB3WkxvWvcZVLpcSEREHcfg6KKVB66BIaTmWepSLrdtToyiDn/1jidy/BJ/KWiVWROSvMHUdFJGy4uxZuKOPPz8XRbHPsz7VNyxUORERKSUqKCJXUFQEvXrBqp8r8GiVhbgl/0RQ/SCzY4mIlBsqKCJX8HbvZXz1lYHVCgu+8qB2TKjZkUREyhWHXyQr4uqWP/Qu/T99DE8eo8J7M2nfXlfEioiUNhUUkd9InZxM+zn9AKjXqTpdeqmciIiYQad4RH5x4Ic91HzqDjwpZFX4PcT9OMrsSCIi5ZYKigiQfeAMhbd0p7Jxiq1+bWmR+j5u7po9ERExiwqKlHuFFwrZ0/Ju6hbs4Ih7Daqu+EK3E4uImEwFRcq9GfevoPmpHziLH7kffUVwc92xIyJiNhUUKdc+/BAGze/MzSxiy7Mf0+Ce5mZHEhERdBePlGMbNsDjj1/6c+wLXYkZY24eERH5H82gSLl0fOsxznb8B+H5u7ntNhg92uxEIiLyW5pBkXLn4vmLHO54N53zl/Ef7weI+HA1bm66Y0dExJloBkXKnVXth9A8exk5+BMwfzaBFVVOREScjQqKlCvLH3mfuC1vAbBj5Fzq3NLQ5EQiInIlKihSbmz7YANt3n8CgJ+6vETbsbebnEhERK5GBUXKhdN7TxHwyJ14k8/akNvptPh5syOJiMgfUEGRMs9mg0H/L5+MojAOeNSl4Zo5uHnoW19ExJnpp7SUeePHw0c/hnKzNZmzX/5AYK2KZkcSEZE/oYIiZVryglO88MKlP0+e5knULbXMDSQiItdEBUXKrKMbDtOoZ2Mm2Iby+EMXeeQRsxOJiMi1UkGRMuni+Yscv/EeqhnH6O6dxOR/XzQ7koiIXAcVFCmTVnZ6lqa5K8kmAO+v/otPkK/ZkURE5DqooEiZs3bkQjqnTAJgx7D3qdU10uREIiJyvVRQpEw5svYgkYkPA5DcIoF2E+4wOZGIiPwVKihSZhQW2MiK701l4xTbfFsRkzze7EgiIvIXqaBImTFmrBvjc/pzyFID34Wf4uVvNTuSiIj8RSooUiYsXQpjx8Kn3MvK2XuofVNdsyOJiMjfoIIiLu/EjhMMvvcYhgGPPgr3PKiZExERV6eCIi7NVmgjvdODLM5qxv21ljN5stmJRETEETzMDiDydyzr+Qadj3/HBay88FpF/PzMTiQiIo6gGRRxWds/3kTswmcBWHffG9Tv2cTkRCIi4igqKOKSzp84j+fDvfHiImtC/48b5vY1O5KIiDiQCoq4pPU3DqdewXYy3UKpnzwLi5vF7EgiIuJAKijictaNXUzclrcAODx2NpUjg0xOJCIijqaLZMWlZGVBrynteYEHqd28EnEj4s2OJCIiJUAFRVyGYcDjj8Pe4wG8GjWH9cuLzI4kIiIlRKd4xGX89+XtLFxo4OUFH30EPhXczY4kIiIlRDMo4hLSF+3illGt+You7HvxE5o18zc7koiIlCAVFHF6hRcKOd/zfvw4T1ilPG4ZrtXYRETKOp3iEae3ovsEos6t54ylIqGL5uDmoW9bEZGyTj/pxant/GwzsUteAiCt75uEtqlhciIRESkNKijitArOFkCfPpdWiw3pQezU3mZHEhGRUqKCIk5r1S1jaXBhMyctQdRNmqHVYkVEyhEVFHFKKSnw4sqb2U09dg6eTtXoYLMjiYhIKdJdPOJ08vOhTx9Is3VgdM+tfPS61exIIiJSyjSDIk5n/PDTpKVBtWoweYbKiYhIeaSCIk5l6ztrGDK5JoN5g7dnGFSpYnYiERExg07xiNO4cOYCvgMewp+z9KyziRv+TxfFioiUV5pBEaex5tYx1CnYyTG3EJosecPsOCIiYiIVFHEKO+dtouOqiQCkPzOdihGVTE4kIiJmUkER0xXmXcR45BE8KGJV9btoP76H2ZFERMRkKihiupU9X6NhXiqnLJWp992bZscREREnoIIiptq9G5YkGRTgydZH36BaEy3IJiIiuotHTGSzweOPQ3LhCA7E3s2ct+uYHUlERJyECoqY5p13IDkZfH3hpbl1sWg+T0REfqG3BDFF5roMovp3ogUbeeUViIgwO5GIiDgTFRQpdYbN4GD3J+hQtJx3/IcwaJDZiURExNmooEipW/30f2mT9S35eFFh7gzc3c1OJCIizqbEC0piYiIWi4WEhAT7NsMwGD16NGFhYfj4+NC5c2fS0tJKOoo4gTMHsqkzZTAAq+NGUP/2hiYnEhERZ1SiBWX9+vXMnDmTpk2bFts+ceJEJk2axFtvvcX69esJCQmhW7du5ObmlmQccQKbu48kxHaUdM9IYr581uw4IiLipEqsoJw9e5bevXsza9YsKlX637LlhmHwxhtvMHLkSO644w6io6OZM2cO58+f5+OPP77ic+Xn55OTk1PsIa5n2+x13LBlGgBnEmdgDfQ2OZGIiDirEisoAwYM4NZbb6Vr167Ftqenp5OZmUl8fLx9m9VqJS4ujlWrVl3xuRITEwkMDLQ/wsPDSyq2lJDCQtgxdBZuGKyIuJ8WT99odiQREXFiJVJQ5s2bx8aNG0lMTLxsX2ZmJgDBwcVXDA0ODrbv+70RI0aQnZ1tfxw8eNDxoaVETZ0Kd558mwSft6m/8DWz44iIiJNz+EJtBw8eZPDgwSxevBhv76tP4VsslmIfG4Zx2bZfWa1WrFarQ3NK6Tl0CJ5/HgzcaPT6/6NatNmJRETE2Tl8BiUlJYWsrCxatWqFh4cHHh4eJCcnM2XKFDw8POwzJ7+fLcnKyrpsVkXKhi96zMZ29hzt219a2l5EROTPOLyg3HTTTWzZsoXU1FT7o3Xr1vTu3ZvU1FTq1KlDSEgISUlJ9s8pKCggOTmZ2NhYR8cRk2144UsGpjxMKs15e0o+blp5R0REroHDT/H4+/sTHV18Dt/Pz4+goCD79oSEBMaNG0dkZCSRkZGMGzcOX19fevXq5eg4YqLzWWcJTby0TGxG2zu5qY1O04mIyLUx5ZcFDhs2jLy8PPr378/p06dp164dixcvxt/f34w4UkI2dH+JTkUHyXCPoN1XL5gdR0REXIjFMAzD7BDXKycnh8DAQLKzswkICDA7jlzBnq+2U+v2pnhSyJrnv6b9y7eaHUlEREx2Pe/fuiJAHM6wGWT3GYQnhawN7q5yIiIi100FRRxu9TPzaXX6By5gJXTeG2bHERERF2TKNShSdp07B0M+actQehLUKZouneuYHUlERFyQCoo4VGIirD1ak6G1/su2b2xmxxERERelUzziMHu2FfDqq5f+/Prr4FtB314iIvLX6B1EHMMwONO5B+8V9ObeuKP06GF2IBERcWU6xSMOsf6FhbQ5/h1N8STm2VFYLKFmRxIRERemGRT52y6cziNkQgIAK9oPpc4/6psbSEREXJ4Kivxt63pOILxwP4fdw2mzYKTZcUREpAxQQZG/5dCK/bRdOh6A9IGT8A/xMzmRiIiUBSoo8rccvPcZvMlnQ8Wb6DCpp9lxRESkjFBBkb9s5cKT1D68giLcCHjndSxuFrMjiYhIGaGCIn9JUREMHBVEfXYx6x+fU79nE7MjiYhIGaKCIn/J7NmQmgrugf7c+eG/zI4jIiJljAqKXLecQzmsH/IJFmyMGgVVqpidSEREyhoVFLluG+8cx4ycXizwf5ABA8xOIyIiZZEKilyXjKV7iVn7OgChg+/By8vkQCIiUiapoMh1OXr/M1gpIKVyN9q8dJvZcUREpIxSQZFrtvmNpbQ7soBC3Al4V7cVi4hIyVFBkWtSVFCE94inAFgR3Y/IHlEmJxIRkbJMBUWuyerH3qXBhc2coSJN/jva7DgiIlLGeZgdQJxfTg68+nUjfGhJbo8H6dxA9xWLiEjJUkGRPzV+PCw8fQM76q1ny0c2s+OIiEg5oIIif+jgQXj90l3FvPqaG16+OisoIiIlTwVF/tCOboMYdqEKazsOpXt3P7PjiIhIOaGCIle149PN3LRzKt0w2P7YP7FY2podSUREygnN18sVGQac7f8MbhisrnkPjfqonIiISOlRQZEr2vDKIlqfSiIfL8LnJpodR0REyhkVFLlMUUERFccOBWBNq4HUuCHC5EQiIlLeqKDIZVb2nUNk/lZOWyrR7LORZscREZFySAVFijmXU0TtD18GYHP356lYp7LJiUREpDxSQZFiJk12p3PRD8wJGEjMhwPMjiMiIuWUbjMWu8xMmDABzlEH75lvYg0wO5GIiJRXmkERu8lDD3LuHLRtC3ffbXYaEREpzzSDIgDs/Xo7Yz5qSjR3U2v8bCwWT7MjiYhIOaYZFAHg2OPP40kh9ULO0bGLyomIiJhLBUXY/M56YjM/pwg3qs58xew4IiIiKijlnWHAxaEjAFgb+QB1ukeZnEhEREQFpdxbN24JrbN/IB8vIuaMNjuOiIgIoIJSrtmKDPxeeQ6AdS37ERpT29xAIiIiv1BBKce+mbqfann7OYsfUR9pSXsREXEeKijl1MWL8NSUCOqwj88f+YbKDauZHUlERMROBaWceu892LsX/KpV4I7JcWbHERERKUYFpRw6f+oC60YswIKNkSOhQgWzE4mIiBSnglIOre4zg3dP38H3PnfQt6/ZaURERC6nglLOnDmYS7NvLi3GVuHe27BaTQ4kIiJyBSoo5UxK70lUMU6Q7lWfdtMfMjuOiIjIFamglCPHth6nzfLXADj+5FjcrfpdkSIi4pxUUMqRtPsTCSCXHb4taTO+p9lxRERErkoFpZzIWJFB7OZpAFwYlYjFXX/1IiLivDTHX068Pf40dxCFe6UAmj/Tzew4IiIif0j/jS4Htm6FxG+b0Yb1GJ/9FywWsyOJiIj8IRWUcmDkSDAM6HmnGy26BpkdR0RE5E+poJRxP7+3gSYLx+JvOcvLL5udRkRE5NroGpQyzDDg4tPPMpYfuLHBURo2nGp2JBERkWuiGZQybMP4JbQ68wMFeNLgnWfMjiMiInLNVFDKKFuRgc/Y5wBY27wf1TvUNjeQiIjIdVBBKaNWD1tA9Pn1nMWPRnNHmh1HRETkuji8oCQmJtKmTRv8/f2pVq0aPXr0YOfOncWOMQyD0aNHExYWho+PD507dyYtLc3RUcqtwguFhLx1qZRs7PQUVaKCTU4kIiJyfRxeUJKTkxkwYABr1qwhKSmJwsJC4uPjOXfunP2YiRMnMmnSJN566y3Wr19PSEgI3bp1Izc319FxyqWVT3xI3YIdnLJUpsVHQ82OIyIict0shmEYJfkFjh8/TrVq1UhOTqZTp04YhkFYWBgJCQkMHz4cgPz8fIKDg5kwYQJ9+/b90+fMyckhMDCQ7OxsAgICSjK+y8nLg24Re+h/7EVq3N6KTl8+bXYkERER4Prev0v8NuPs7GwAKleuDEB6ejqZmZnEx8fbj7FarcTFxbFq1aorFpT8/Hzy8/PtH+fk5JRwatc1bRqsPFaPjPCP2TWvRLuniIhIiSnRi2QNw2DIkCF07NiR6OhoADIzMwEIDi5+XURwcLB93+8lJiYSGBhof4SHh5dkbJeVfcZg3LhLfx49Grx9tKS9iIi4phItKAMHDuTnn3/mk08+uWyf5Xe/D8YwjMu2/WrEiBFkZ2fbHwcPHiyRvK5uQ4+xTD51P13r7OPBB81OIyIi8teV2CmeQYMGsXDhQpYtW0aNGjXs20NCQoBLMymhoaH27VlZWZfNqvzKarVitVpLKmqZcHzbcdomT8Sfs9Tr8S88POqYHUlEROQvc/gMimEYDBw4kM8//5wff/yRiIiIYvsjIiIICQkhKSnJvq2goIDk5GRiY2MdHafc2No7EX/Ost23Je0m9jQ7joiIyN/i8BmUAQMG8PHHH/Pll1/i7+9vv64kMDAQHx8fLBYLCQkJjBs3jsjISCIjIxk3bhy+vr706tXL0XHKhUOrMohJnQZA3guJWNy1/p6IiLg2hxeU6dOnA9C5c+di299//30eeughAIYNG0ZeXh79+/fn9OnTtGvXjsWLF+Pv7+/oOOXCvodeogb5pFbsTMvh3cyOIyIi8reV+DooJUHroPzPnq93ENE9CndspL2zmqhH25sdSURE5Iqu5/1b5wJcXMaACbhjY23ov1RORESkzCjxhdqk5KxdCz0yJjPEUpsHpuvCWBERKTtUUFzYc89BLgEc6DOKuv8yO42IiIjj6BSPi0qef4IffzTw8rq0aqyIiEhZohkUF2TYDIL63MpaDJbcNZtatRqbHUlERMShVFBc0JpnvyDm3DrO4kfEs0FmxxEREXE4neJxMYX5RVSdPBKADR2fomr0lX89gIiIiCtTQXExq574kHoF2zllqUyLj4aaHUdERKREqKC4kLwz+dT5YBQAW24dQWDNQJMTiYiIlAwVFBeyps80ahRlcNS9Ou0+GGB2HBERkRKjguIisrOBb78DYO/9o/Gu5GNuIBERkRKkguIiJk6Emwq/Z1D1z2k/4yGz44iIiJQo3WbsAo4ehTfeAAM3bnrr//DwNjuRiIhIydIMiguY//j3cP4c7dvDv7SkvYiIlAOaQXFy+3/YS99vutOTKqQPS8Vi0bonIiJS9mkGxckdfuQFPCnkcJVmxP6fyomIiJQPKihObPsnqXTI+ASAgLcSTU4jIiJSelRQnNjZJ0cAsKrWfdS/p4XJaUREREqPCoqT2vT6T7Q58T0X8aDG+y+bHUdERKRUqaA4IcNm4PnCcABWN/l/1OxS1+REIiIipUsFxQkt/OQcaedqk0sFGn30gtlxRERESp0KipO5eBGGv1yBe/mUaUP2UrVJiNmRRERESp0KipOZORN27oSqVeGJUdXMjiMiImIKFRQnkp2RTYWhfanJAV56CQICzE4kIiJiDq0k60Q23TOePhdm0sa6jshHNwIWsyOJiIiYQjMoTuLQygO0X/M6AGefGYOnl8qJiIiUXyooTuLA/SPxJp9NFbvQ5qXbzI4jIiJiKhUUJ7Btzno67P8IGxa83/o3FjfNnoiISPmmgmIyw2Zw8cmnAVhV5wEa9W5pciIRERHzqaCYbN3zC2mWs5w8vKnz8Viz44iIiDgF3cVjooICeOI/N9KDF4iJs9KtXbjZkURERJyCCoqJpkyBTXv8OVxtDAkLzU4jIiLiPHSKxySZu3MZM9oGwPjxWpRNRETkt1RQTLL35v78cK4dvaNS6dPH7DQiIiLORad4TLBlxko6pM/FhoURzxTippooIiJSjN4aS1lRQRGeQwYCsKL+o0T1aW1yIhEREeejglLKVj08i4Z5qZyhIo0WjDM7joiIiFNSQSlFp/ecJOqTkQCk3jGGqo2rmpxIRETEOamglKKt3UdQ2TjFLms0HeY+YXYcERERp6WCUkpW/piP2440AM5PnIqnj65PFhERuRoVlFKQnw+PD7RyA8v59y0/0vzJTmZHEhERcWoqKKVgwgTYvh2qBbvx6NwuZscRERFxeiooJWzfN9vxe2kYvpxj8mSoVMnsRCIiIs5PF0KUIFuhjdxefXnatpyo8NPcfPcssyOJiIi4BM2glKDl906lWc5yzuFL9CfPY7GYnUhERMQ1qKCUkH3f7qDt/GEAbLhrIjU61DI5kYiIiOtQQSkBhRcKybv7QXy4QErlbnSa19/sSCIiIi5FBaUErLg1kahz6zljqUjY9+9hcdO5HRERkeuhguJga344R/0fpwOQ9sRUQtvUMDmRiIiI69FdPA50/Djc2ccPC2v5d9MPuPvN+8yOJCIi4pI0g+IgNhs88AAcPgx+DcK5ZcVIndoRERH5i1RQHOSHW17De9EX+PjAf/8L/v5mJxIREXFdOsXjAGtHfMFNi56hGwZfDd9AdHQrsyOJiIi4NM2g/E3bP9xAk/G9cMMgOao/3UepnIiIiPxdKih/w6Hl6VR+qDu+5LE+6B/Erp9sdiQREZEyQQXlLzqyJgNu7EKwLZNd1ibU3/Qpnj46YyYiIuIIKih/wZHd5yi8oQs1Cg+Q7hlJwOpFBIYHmB1LRESkzFBBuU6bN0O7G/2YV9iTAx51sK74kZAWoWbHEhERKVNUUK7D1//Jo2NHOHQI5jQcj9vqVYS11UqxIiIijmZqQZk2bRoRERF4e3vTqlUrli9fbmacq8o+cIZlUf2odnccF85epEsXWLHKjfDWwWZHExERKZNMKyiffvopCQkJjBw5kk2bNnHDDTfwz3/+k4yMDLMiXebUrhP8dMsECiPq0Wnb27RlPdN6JPH991CpktnpREREyi6LYRiGGV+4Xbt2tGzZkunTp9u3NWrUiB49epCYmFjs2Pz8fPLz8+0f5+TkEB4eTnZ2NgEBjrs49fBhmP3KYTovepbArN00PLsBD4oA2OPViJxX3qLl0Bsd9vVERETKk5ycHAIDA6/p/duUGZSCggJSUlKIj48vtj0+Pp5Vq1ZddnxiYiKBgYH2R3h4eInkys2F6dMNOuybS/TZtXhQxHaflix/+D1qnd6sciIiIlJKTFm448SJExQVFREcXPwajuDgYDIzMy87fsSIEQwZMsT+8a8zKI5WvTrcO6gaS/dMxLN2dWrf35FGsTUd/nVERETkj5m6spjFUvy3/RqGcdk2AKvVitVqLfE8/v7w7ylewDMl/rVERETk6kw5xVOlShXc3d0vmy3Jysq6bFZFREREyh9TCoqXlxetWrUiKSmp2PakpCRiY2PNiCQiIiJOxLRTPEOGDOGBBx6gdevWxMTEMHPmTDIyMujXr59ZkURERMRJmFZQ7rnnHk6ePMmYMWM4evQo0dHRfPvtt9SqVcusSCIiIuIkTFsH5e+4nvuoRURExDk4/TooIiIiIn9EBUVEREScjgqKiIiIOB0VFBEREXE6KigiIiLidFRQRERExOmooIiIiIjTUUERERERp2PqbzP+q35dWy4nJ8fkJCIiInKtfn3fvpY1Yl2yoOTm5gIQHh5uchIRERG5Xrm5uQQGBv7hMS651L3NZuPIkSP4+/tjsVgc+tw5OTmEh4dz8OBBLaP/BzRO10bjdG00TtdG43RtNE7XxoxxMgyD3NxcwsLCcHP746tMXHIGxc3NjRo1apTo1wgICNA39jXQOF0bjdO10ThdG43TtdE4XZvSHqc/mzn5lS6SFREREaejgiIiIiJORwXld6xWK6NGjcJqtZodxalpnK6NxunaaJyujcbp2micro2zj5NLXiQrIiIiZZtmUERERMTpqKCIiIiI01FBEREREaejgiIiIiJORwVFREREnI4Kym9MmzaNiIgIvL29adWqFcuXLzc7UqlatmwZ3bt3JywsDIvFwhdffFFsv2EYjB49mrCwMHx8fOjcuTNpaWnFjsnPz2fQoEFUqVIFPz8/br/9dg4dOlSKr6LkJSYm0qZNG/z9/alWrRo9evRg586dxY7RWMH06dNp2rSpfZXKmJgYvvvuO/t+jdHlEhMTsVgsJCQk2LdpnC4ZPXo0Foul2CMkJMS+X+P0P4cPH+b+++8nKCgIX19fmjdvTkpKin2/y4yVIYZhGMa8efMMT09PY9asWca2bduMwYMHG35+fsaBAwfMjlZqvv32W2PkyJHG/PnzDcBYsGBBsf3jx483/P39jfnz5xtbtmwx7rnnHiM0NNTIycmxH9OvXz+jevXqRlJSkrFx40ajS5cuRrNmzYzCwsJSfjUl5+abbzbef/99Y+vWrUZqaqpx6623GjVr1jTOnj1rP0ZjZRgLFy40vvnmG2Pnzp3Gzp07jeeee87w9PQ0tm7dahiGxuj31q1bZ9SuXdto2rSpMXjwYPt2jdMlo0aNMqKiooyjR4/aH1lZWfb9GqdLTp06ZdSqVct46KGHjLVr1xrp6enGkiVLjD179tiPcZWxUkH5Rdu2bY1+/foV29awYUPj2WefNSmRuX5fUGw2mxESEmKMHz/evu3ChQtGYGCgMWPGDMMwDOPMmTOGp6enMW/ePPsxhw8fNtzc3Izvv/++1LKXtqysLAMwkpOTDcPQWP2RSpUqGe+8847G6Hdyc3ONyMhIIykpyYiLi7MXFI3T/4waNcpo1qzZFfdpnP5n+PDhRseOHa+635XGSqd4gIKCAlJSUoiPjy+2PT4+nlWrVpmUyrmkp6eTmZlZbIysVitxcXH2MUpJSeHixYvFjgkLCyM6OrpMj2N2djYAlStXBjRWV1JUVMS8efM4d+4cMTExGqPfGTBgALfeeitdu3Yttl3jVNzu3bsJCwsjIiKCe++9l3379gEap99auHAhrVu35q677qJatWq0aNGCWbNm2fe70lipoAAnTpygqKiI4ODgYtuDg4PJzMw0KZVz+XUc/miMMjMz8fLyolKlSlc9pqwxDIMhQ4bQsWNHoqOjAY3Vb23ZsoUKFSpgtVrp168fCxYsoHHjxhqj35g3bx4bN24kMTHxsn0ap/9p164dH3zwAYsWLWLWrFlkZmYSGxvLyZMnNU6/sW/fPqZPn05kZCSLFi2iX79+PPnkk3zwwQeAa31PeZTaV3IBFoul2MeGYVy2rbz7K2NUlsdx4MCB/Pzzz6xYseKyfRoraNCgAampqZw5c4b58+fTp08fkpOT7fvL+xgdPHiQwYMHs3jxYry9va96XHkfJ4B//vOf9j83adKEmJgY6taty5w5c2jfvj2gcQKw2Wy0bt2acePGAdCiRQvS0tKYPn06Dz74oP04VxgrzaAAVapUwd3d/bJmmJWVdVnLLK9+vVr+j8YoJCSEgoICTp8+fdVjypJBgwaxcOFCli5dSo0aNezbNVb/4+XlRb169WjdujWJiYk0a9aMyZMna4x+kZKSQlZWFq1atcLDwwMPDw+Sk5OZMmUKHh4e9tdZ3sfpSvz8/GjSpAm7d+/W99NvhIaG0rhx42LbGjVqREZGBuBaP59UULj0Q7RVq1YkJSUV256UlERsbKxJqZxLREQEISEhxcaooKCA5ORk+xi1atUKT0/PYsccPXqUrVu3lqlxNAyDgQMH8vnnn/Pjjz8SERFRbL/G6uoMwyA/P19j9IubbrqJLVu2kJqaan+0bt2a3r17k5qaSp06dTROV5Gfn8/27dsJDQ3V99NvdOjQ4bJlD3bt2kWtWrUAF/v5VGqX4zq5X28zfvfdd41t27YZCQkJhp+fn7F//36zo5Wa3NxcY9OmTcamTZsMwJg0aZKxadMm+63W48ePNwIDA43PP//c2LJli3Hfffdd8da0GjVqGEuWLDE2btxo3HjjjWXuNr4nnnjCCAwMNH766aditzyeP3/efozGyjBGjBhhLFu2zEhPTzd+/vln47nnnjPc3NyMxYsXG4ahMbqa397FYxgap189/fTTxk8//WTs27fPWLNmjXHbbbcZ/v7+9p/RGqdL1q1bZ3h4eBivvPKKsXv3buOjjz4yfH19jblz59qPcZWxUkH5jalTpxq1atUyvLy8jJYtW9pvGy0vli5dagCXPfr06WMYxqXb00aNGmWEhIQYVqvV6NSpk7Fly5Ziz5GXl2cMHDjQqFy5suHj42PcdtttRkZGhgmvpuRcaYwA4/3337cfo7EyjEceecT+76lq1arGTTfdZC8nhqExuprfFxSN0yW/rtXh6elphIWFGXfccYeRlpZm369x+p+vvvrKiI6ONqxWq9GwYUNj5syZxfa7ylhZDMMwSm++RkREROTP6RoUERERcToqKCIiIuJ0VFBERETE6aigiIiIiNNRQRERERGno4IiIiIiTkcFRURERJyOCoqIiIg4HRUUERERcToqKCIiIuJ0VFBERETE6fx/IRp90gNjygIAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_pred = PINN.test()\n",
    "plt.plot(x_true,'b')\n",
    "plt.plot(x_pred,'r--')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "Ky6HsA0AWWTD"
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SF7H51LTWXDq",
    "outputId": "aefc6a9f-3ad4-417a-b9f7-438009e620af"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '1D_FODE_stan_tune0.mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '1D_FODE_stan_tune0.mat'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_17104/2016390461.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtune_reps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"1D_FODE_stan_tune\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtune_reps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".mat\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test_re_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \"\"\"\n\u001b[1;32m    223\u001b[0m     \u001b[0mvariable_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'variable_names'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0mMR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mmatfile_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file_context\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mappendmat\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.mat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mfile_like\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'.mat'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             raise IOError(\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '1D_FODE_stan_tune0.mat'"
     ]
    }
   ],
   "source": [
    "for tune_reps in range(5):\n",
    "    label = \"1D_FODE_stan_tune\"+str(tune_reps)+\".mat\"\n",
    "    data = sio.loadmat(label)\n",
    "    re = np.array(data[\"test_re_loss\"])\n",
    "    print(np.mean(re[:,-1]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "stan_1D_FODE_tune.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
