{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "wLsZ-c_nCQr2",
    "outputId": "0238c820-5951-4e75-a35b-19e4de8c9b38"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SV23gJi7JexL",
    "outputId": "6f051579-557f-463f-d7b4-955ed617736e"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BOyXTKXGJf97",
    "outputId": "11b7b7db-47b0-4cf8-c699-473f1c6b8c5f"
   },
   "outputs": [],
   "source": [
    "# %cd '/content/gdrive/MyDrive/Virginia Tech /Fall 2022/Codes from GPU/MURI Aug17 Thin Plate'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "APjvgycyCTj0",
    "outputId": "19bce659-211e-4bec-d94d-7c94148b0d09"
   },
   "outputs": [],
   "source": [
    "# !pip install smt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "lxFUD2gACQr7"
   },
   "outputs": [],
   "source": [
    "#Material Properties This link - https://www.mathworks.com/help/pde/ug/nonlinear-heat-transfer-in-a-thin-plate.html#heatTransferThinPlateExample-1\n",
    "data = scipy.io.loadmat('cylinder_nektar_wake.mat')\n",
    "           \n",
    "U_star = data['U_star'] # N x 2 x T\n",
    "P_star = data['p_star'] # N x T\n",
    "t_star = data['t'] # T x 1\n",
    "X_star = data['X_star'] # N x 2\n",
    "\n",
    "N = X_star.shape[0]\n",
    "T = t_star.shape[0]\n",
    "\n",
    "# # Rearrange Data \n",
    "XX = np.tile(X_star[:,0:1], (1,T)) # N x T\n",
    "YY = np.tile(X_star[:,1:2], (1,T)) # N x T\n",
    "TT = np.tile(t_star, (1,N)).T # N x T\n",
    "\n",
    "UU = U_star[:,0,:] # N x T\n",
    "VV = U_star[:,1,:] # N x T\n",
    "PP = P_star # N x T\n",
    "\n",
    "x = XX.flatten()[:,None].reshape(-1,1) # NT x 1\n",
    "y = YY.flatten()[:,None].reshape(-1,1) # NT x 1\n",
    "t = TT.flatten()[:,None].reshape(-1,1) # NT x 1\n",
    "\n",
    "u_true = UU.flatten()[:,None].reshape(-1,1) # NT x 1\n",
    "v_true = VV.flatten()[:,None].reshape(-1,1) # NT x 1\n",
    "p_true = PP.flatten()[:,None].reshape(-1,1) # NT x 1\n",
    "\n",
    "label = \"3D_NS_stan\"\n",
    "\n",
    "loss_thresh = 10000\n",
    "\n",
    "N_train = x.shape[0]\n",
    "xyt = np.hstack((x,y,t))\n",
    "\n",
    "lb_xyt = xyt[0]\n",
    "ub_xyt = xyt[-1]\n",
    "\n",
    "# xyt_test_tensor = torch.from_numpy(xyt).float().to(device)\n",
    "np.random.seed(1234)\n",
    "idx = np.random.choice(N_train, 5000, replace=False)\n",
    "u_true_test = u_true[idx,:]\n",
    "v_true_test = v_true[idx,:]\n",
    "p_true_test = p_true[idx,:]\n",
    "\n",
    "u_true_norm = np.linalg.norm(u_true,2)\n",
    "v_true_norm = np.linalg.norm(v_true,2)\n",
    "p_true_norm = np.linalg.norm(p_true,2)\n",
    "\n",
    "# u_true_test = torch.from_numpy(u_true_test).float().to(device)\n",
    "# v_true_test = torch.from_numpy(v_true_test).float().to(device)\n",
    "# p_true_test = torch.from_numpy(p_true_test).float().to(device)\n",
    "\n",
    "x_tensor = torch.from_numpy(x[idx,:]).float().to(device)\n",
    "y_tensor = torch.from_numpy(y[idx,:]).float().to(device)\n",
    "t_tensor = torch.from_numpy(t[idx,:]).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "CUcT7YuXCQr7"
   },
   "outputs": [],
   "source": [
    "\n",
    "# x = np.linspace(1,8,100).reshape(-1,1)\n",
    "# y = np.linspace(-2,2,100).reshape(-1,1)\n",
    "# t = np.linspace(0,20,100).reshape(-1,1)\n",
    "\n",
    "# X,Y,T = np.meshgrid(x,y,t)\n",
    "\n",
    "# X = X.flatten('F').reshape(-1,1)\n",
    "# Y = Y.flatten('F').reshape(-1,1)\n",
    "# T = T.flatten('F').reshape(-1,1)\n",
    "  \n",
    "# xyt = np.hstack((X,Y,T))\n",
    "\n",
    "# initial_pts = np.logical_and(T==0,Y!=0).reshape(-1,)\n",
    "\n",
    "# DBC_pts = (Y == 0).reshape(-1,)\n",
    "\n",
    "\n",
    "# NBC_pts_x0 = (X == 0).reshape(-1,)\n",
    "# NBC_pts_x1 = (X == 1).reshape(-1,)\n",
    "\n",
    "# NBC_pts_y0 = (Y == 0).reshape(-1,)\n",
    "# NBC_pts_y1 = (Y == 1).reshape(-1,)\n",
    "\n",
    "# xyt_initial = xyt[initial_pts,:]\n",
    "# xyt_DBC = xyt[DBC_pts,:]\n",
    "\n",
    "# xyt_NBC_x0 = xyt[NBC_pts_x0,:]\n",
    "# xyt_NBC_x1 = xyt[NBC_pts_x1,:]\n",
    "\n",
    "# #xyt_NBC_y0 = xyt[NBC_pts_y0,:]\n",
    "# xyt_NBC_y1 = xyt[NBC_pts_y1,:]\n",
    "\n",
    "# u_initial = 300*np.ones((np.shape(xyt_initial)[0],1))\n",
    "# u_DBC = 1000*np.ones((np.shape(xyt_DBC)[0],1))\n",
    "\n",
    "# xyt_I_DBC = np.vstack((xyt_initial,xyt_DBC))\n",
    "# #xyt_NBC = np.vstack((xyt_NBC_1,xyt_NBC_2,xyt_NBC_3,xyt_NBC_4))\n",
    "# xyt_NBC_x = np.vstack((xyt_NBC_x0,xyt_NBC_x1))\n",
    "# #xyt_NBC_y = np.vstack((xyt_NBC_y0,xyt_NBC_y1))\n",
    "# xyt_NBC_y = np.vstack((xyt_NBC_y1))\n",
    "\n",
    "# u_I_DBC = np.vstack((u_initial,u_DBC))\n",
    "\n",
    "\n",
    "# lb_xyt = xyt[0]\n",
    "# ub_xyt = xyt[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fea_data = scipy.io.loadmat('./../3D_HTTP_FEA.mat')\n",
    "# xy = fea_data['xy']\n",
    "# t = fea_data['t']/3000\n",
    "# xyt = np.zeros((497*101,3))\n",
    "# u_true = np.ones((497*101,1))\n",
    "\n",
    "\n",
    "# for i in range(101):\n",
    "#     t_temp = t[0,i]*np.ones((497,1))\n",
    "#     xyt[497*i:497*(i+1)] = np.hstack((xy,t_temp))\n",
    "#     u_true[497*i:497*(i+1)] = fea_data['u'][:,i].reshape(-1,1)\n",
    "#     #print(i)\n",
    "# #print(xyt)\n",
    "\n",
    "# xyt_test_tensor = torch.from_numpy(xyt).float().to(device)\n",
    "# u_true_norm = np.linalg.norm(u_true,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "gp2G6x6BCQr8"
   },
   "outputs": [],
   "source": [
    "def trainingdata(N_T,seed):\n",
    "    '''Boundary Conditions''' \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    idx = np.random.choice(N_train, N_T, replace=False)\n",
    "    x_train = x[idx,:]\n",
    "    y_train = y[idx,:]\n",
    "    t_train = t[idx,:]\n",
    "    u_train = u_true[idx,:]\n",
    "    v_train = v_true[idx,:]\n",
    "    \n",
    "    return x_train,y_train,t_train,u_train,v_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "VRolFlBzCQr9"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,beta_init):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)   \n",
    "        \n",
    "        self.beta = Parameter(beta_init*torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "        \n",
    "        self.lambda1 = Parameter(torch.tensor(0.0))\n",
    "        self.lambda1.requires_grad = True\n",
    "        \n",
    "        self.lambda2 = Parameter(torch.tensor(0.0))\n",
    "        self.lambda2.requires_grad = True\n",
    "    \n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xyt):\n",
    "        if torch.is_tensor(xyt) != True:         \n",
    "            xyt = torch.from_numpy(xyt)                \n",
    "        \n",
    "        ubxyt = torch.from_numpy(ub_xyt).float().to(device)\n",
    "        lbxyt = torch.from_numpy(lb_xyt).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xyt = 2.0*(xyt- lbxyt)/(ubxyt - lbxyt)-1.0\n",
    "        \n",
    "        #convert to float\n",
    "        a = xyt.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            z1 = self.activation(z) \n",
    "            a = z1 + self.beta[:,i]*z*z1\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_uv(self,x_train,y_train,t_train,u_train,v_train):\n",
    "        x1 = x_train.clone()\n",
    "        x1.requires_grad = True\n",
    "        \n",
    "        y1 = y_train.clone()\n",
    "        y1.requires_grad = True\n",
    "        \n",
    "        t1 = t_train.clone()\n",
    "        t1.requires_grad = True\n",
    "        \n",
    "#         print(torch.cat((x1,y1,t1),dim=1).shape)\n",
    "        psi_p = self.forward(torch.cat((x1,y1,t1),dim =1))\n",
    "        \n",
    "        psi = psi_p[:,0:1]\n",
    "        \n",
    "#         print(psi.shape)\n",
    "        psi_x = autograd.grad(psi,x1,torch.ones([x1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        psi_y = autograd.grad(psi,y1,torch.ones([y1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "                \n",
    "        loss_u = self.loss_function(psi_y,u_train)\n",
    "        loss_v = self.loss_function(-1*psi_x,v_train)\n",
    "                \n",
    "        return loss_u + loss_v\n",
    "    \n",
    "    def loss_PDE(self, x_train,y_train,t_train,fg_hat):\n",
    "        \n",
    "        x1 = x_train.clone()\n",
    "        x1.requires_grad = True\n",
    "        \n",
    "        y1 = y_train.clone()\n",
    "        y1.requires_grad = True\n",
    "        \n",
    "        t1 = t_train.clone()\n",
    "        t1.requires_grad = True\n",
    "        \n",
    "        psi_p = self.forward(torch.cat((x1,y1,t1),dim =1))\n",
    "        psi = psi_p[:,0:1]\n",
    "        p = psi_p[:,1:2]\n",
    "        \n",
    "        \n",
    "        u = autograd.grad(psi,y1,torch.ones([y1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        v = -1*autograd.grad(psi,x1,torch.ones([x1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        #---------------------------------------------------------------------------------------------------------------\n",
    "        u_t = autograd.grad(u,t1,torch.ones([t1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_x = autograd.grad(u,x1,torch.ones([x1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        u_xx = autograd.grad(u_x,x1,torch.ones([x1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "                \n",
    "        u_y = autograd.grad(u,y1,torch.ones([y1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        u_yy = autograd.grad(u_y,y1,torch.ones([y1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        #---------------------------------------------------------------------------------\n",
    "        \n",
    "        v_t = autograd.grad(v,t1,torch.ones([t1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        v_x = autograd.grad(v,x1,torch.ones([x1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        v_xx = autograd.grad(v_x,x1,torch.ones([x1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "                \n",
    "        v_y = autograd.grad(v,y1,torch.ones([y1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        v_yy = autograd.grad(v_y,y1,torch.ones([y1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        #------------------------------------------------------------------------------------\n",
    "        p_x = autograd.grad(p,x1,torch.ones([x1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        p_y = autograd.grad(p,y1,torch.ones([y1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "            \n",
    "            \n",
    "\n",
    "        f = u_t + self.lambda1*(u*u_x + v*u_y) + p_x - self.lambda2*(u_xx + u_yy)\n",
    "        g = v_t + self.lambda1*(u*v_x + v*v_y) + p_y - self.lambda2*(v_xx + v_yy)\n",
    "        \n",
    "        loss_f = self.loss_function(f,fg_hat)\n",
    "        loss_g = self.loss_function(g,fg_hat)\n",
    "                \n",
    "        return loss_f + loss_g\n",
    "    \n",
    "    def loss(self,x_train,y_train,t_train,u_train,v_train,fg_hat):\n",
    "\n",
    "        loss_uv = self.loss_uv(x_train,y_train,t_train,u_train,v_train)\n",
    "        loss_fg = self.loss_PDE(x_train,y_train,t_train,fg_hat)\n",
    "        loss_val = loss_uv + loss_fg\n",
    "        \n",
    "        #print(self.iter,\"loss_D:\",loss_D.cpu().detach().numpy(),\"loss_N:\",loss_N.cpu().detach().numpy(),\"loss_f:\",loss_f.cpu().detach().numpy())\n",
    "        \n",
    "        return loss_val\n",
    "       \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        x1 = x_tensor.clone()\n",
    "        x1.requires_grad = True\n",
    "        \n",
    "        y1 = y_tensor.clone()\n",
    "        y1.requires_grad = True\n",
    "        \n",
    "        t1 = t_tensor.clone()\n",
    "        t1.requires_grad = True\n",
    "        \n",
    "        psi_p_pred = self.forward(torch.cat((x1,y1,t1),dim =1))\n",
    "        psi = psi_p_pred[:,0:1]\n",
    "        p_pred = psi_p_pred[:,1:2]\n",
    "        \n",
    "        u_pred = autograd.grad(psi,y1,torch.ones([y1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        v_pred = -1*autograd.grad(psi,x1,torch.ones([x1.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "   \n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "        v_pred = v_pred.cpu().detach().numpy()\n",
    "        p_pred = p_pred.cpu().detach().numpy()\n",
    "    \n",
    "        return u_pred,v_pred,p_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        u_pred,v_pred,p_pred = self.test()\n",
    "        \n",
    "        test_mse_u = np.mean(np.square(u_pred.reshape(-1,1) - u_true_test.reshape(-1,1)))\n",
    "        test_re_u = np.linalg.norm(u_pred.reshape(-1,1) - u_true_test.reshape(-1,1),2)/u_true_norm\n",
    "        \n",
    "        test_mse_v = np.mean(np.square(v_pred.reshape(-1,1) - v_true_test.reshape(-1,1)))\n",
    "        test_re_v = np.linalg.norm(v_pred.reshape(-1,1) - v_true_test.reshape(-1,1),2)/v_true_norm\n",
    "        \n",
    "        test_mse_p = np.mean(np.square(p_pred.reshape(-1,1) - p_true_test.reshape(-1,1)))\n",
    "        test_re_p = np.linalg.norm(p_pred.reshape(-1,1) - p_true_test.reshape(-1,1),2)/p_true_norm\n",
    "        \n",
    "        return test_mse_u, test_re_u, test_mse_v, test_re_v, test_mse_p, test_re_p "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_train,y_train,t_train,u_train,v_train,fg_hat,seed):    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(x_train,y_train,t_train,u_train,v_train,fg_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np,x_train,y_train,t_train):\n",
    "    train_loss.append(loss_np)\n",
    "    beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    \n",
    "    lambda1_val.append(PINN.lambda1.cpu().detach().numpy())\n",
    "    lambda2_val.append(PINN.lambda2.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse_u, test_re_u, test_mse_v, test_re_v, test_mse_p, test_re_p = PINN.test_loss()\n",
    "    \n",
    "    test_mse_u_loss.append(test_mse_u)\n",
    "    test_re_u_loss.append(test_re_u)\n",
    "    \n",
    "    test_mse_v_loss.append(test_mse_v)\n",
    "    test_re_v_loss.append(test_re_v)\n",
    "    \n",
    "    test_mse_p_loss.append(test_mse_p)\n",
    "    test_re_p_loss.append(test_re_p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*11)\n",
    "    start_time = time.time() \n",
    "    thresh_flag = 0\n",
    "    \n",
    "    x_train,y_train,t_train,u_train,v_train = trainingdata(N_T,(reps)*22)\n",
    "\n",
    "    x_train = torch.from_numpy(x_train).float().to(device)\n",
    "    y_train = torch.from_numpy(y_train).float().to(device)\n",
    "    t_train = torch.from_numpy(t_train).float().to(device)\n",
    "    u_train = torch.from_numpy(u_train).float().to(device)\n",
    "    v_train = torch.from_numpy(v_train).float().to(device)\n",
    "        \n",
    "    fg_hat = torch.zeros(x_train.shape[0],1).to(device)\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        train_step(x_train,y_train,t_train,u_train,v_train,fg_hat,i)\n",
    "\n",
    "        loss_np = PINN.loss(x_train,y_train,t_train,u_train,v_train,fg_hat).cpu().detach().numpy()\n",
    "        \n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np,x_train,y_train,t_train)\n",
    "        print(i,\"Loss\",train_loss[-1], \"L1\",lambda1_val[-1],\"L2\",lambda2_val[-1])\n",
    "        # print(i,\"Loss\",train_loss[-1],\"RE\",test_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time  \n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fVnXJfj0CQr-",
    "outputId": "1f2921b0-e258-465d-aa27-cdeb80b78a0b",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3D_NS_stan\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=3, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=2, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Loss 0.10068626 L1 0.00802775 L2 -0.0067287567\n",
      "1 Loss 0.09513332 L1 -0.005040844 L2 -0.0016071522\n",
      "2 Loss 0.09371072 L1 -0.026520284 L2 -0.00045677263\n",
      "3 Loss 0.09257895 L1 0.0021382172 L2 0.00085439417\n",
      "4 Loss 0.09143096 L1 0.07746466 L2 0.0028363497\n",
      "5 Loss 0.08983109 L1 0.1050835 L2 0.005258222\n",
      "6 Loss 0.08204637 L1 0.40558124 L2 0.01583911\n",
      "7 Loss 0.056964204 L1 0.86050296 L2 0.024711141\n",
      "8 Loss 0.02665618 L1 0.83406 L2 0.013724968\n",
      "9 Loss 0.019367829 L1 0.86586016 L2 0.015405356\n",
      "10 Loss 0.014933664 L1 0.8815587 L2 0.015499877\n",
      "11 Loss 0.012233402 L1 0.9093702 L2 0.016982947\n",
      "12 Loss 0.010393122 L1 0.9316768 L2 0.01716269\n",
      "13 Loss 0.0088426825 L1 0.9395032 L2 0.016273284\n",
      "14 Loss 0.007618496 L1 0.93951726 L2 0.016495094\n",
      "15 Loss 0.006563664 L1 0.95184654 L2 0.015154654\n",
      "16 Loss 0.005702785 L1 0.9589874 L2 0.015104154\n",
      "17 Loss 0.004985895 L1 0.9645907 L2 0.013703351\n",
      "18 Loss 0.004298128 L1 0.97125137 L2 0.01375439\n",
      "19 Loss 0.0037596228 L1 0.9763527 L2 0.013579377\n",
      "20 Loss 0.0033653458 L1 0.9763076 L2 0.013806518\n",
      "21 Loss 0.0029932812 L1 0.9774334 L2 0.012939824\n",
      "22 Loss 0.0026948152 L1 0.98042816 L2 0.013220652\n",
      "23 Loss 0.0024115015 L1 0.98067087 L2 0.012633342\n",
      "24 Loss 0.002203406 L1 0.9827834 L2 0.012977184\n",
      "25 Loss 0.0020117678 L1 0.9827406 L2 0.012654515\n",
      "26 Loss 0.0018505987 L1 0.98576015 L2 0.012701921\n",
      "27 Loss 0.0017045309 L1 0.985462 L2 0.012535146\n",
      "28 Loss 0.001569304 L1 0.98590463 L2 0.012370067\n",
      "29 Loss 0.0014727081 L1 0.9888077 L2 0.012455392\n",
      "30 Loss 0.0013820992 L1 0.986552 L2 0.012298132\n",
      "31 Loss 0.0012994041 L1 0.987524 L2 0.012199431\n",
      "32 Loss 0.0012316015 L1 0.98869854 L2 0.012054604\n",
      "33 Loss 0.0011697987 L1 0.9895594 L2 0.011881231\n",
      "34 Loss 0.0011130874 L1 0.9890603 L2 0.011951046\n",
      "35 Loss 0.0010646766 L1 0.9897771 L2 0.011960815\n",
      "36 Loss 0.0010237794 L1 0.99021214 L2 0.011936945\n",
      "37 Loss 0.0009850063 L1 0.9908807 L2 0.012078524\n",
      "38 Loss 0.00094369915 L1 0.99030113 L2 0.01172872\n",
      "39 Loss 0.000909372 L1 0.99178416 L2 0.011904592\n",
      "40 Loss 0.0008779195 L1 0.9918898 L2 0.011782996\n",
      "41 Loss 0.0008408873 L1 0.9912613 L2 0.011819939\n",
      "42 Loss 0.00081376056 L1 0.9927826 L2 0.011959429\n",
      "43 Loss 0.0007842764 L1 0.9928612 L2 0.011942213\n",
      "44 Loss 0.00075382955 L1 0.9919025 L2 0.012003733\n",
      "45 Loss 0.0007282117 L1 0.99368054 L2 0.011921755\n",
      "46 Loss 0.00070313684 L1 0.9926226 L2 0.011912248\n",
      "47 Loss 0.0006816195 L1 0.99461895 L2 0.011922328\n",
      "48 Loss 0.0006570162 L1 0.9928658 L2 0.011872671\n",
      "49 Loss 0.0006347969 L1 0.9951312 L2 0.01181245\n",
      "50 Loss 0.0006136324 L1 0.99407053 L2 0.011726132\n",
      "51 Loss 0.0005924428 L1 0.9949101 L2 0.011701147\n",
      "52 Loss 0.00057274255 L1 0.99490285 L2 0.011774219\n",
      "53 Loss 0.000553872 L1 0.99532723 L2 0.011790823\n",
      "54 Loss 0.00053647894 L1 0.9956429 L2 0.0118475\n",
      "55 Loss 0.0005199671 L1 0.99561924 L2 0.011803304\n",
      "56 Loss 0.0005037559 L1 0.9965614 L2 0.011803355\n",
      "57 Loss 0.0004901375 L1 0.99588144 L2 0.011796318\n",
      "58 Loss 0.0004773331 L1 0.9953257 L2 0.011778787\n",
      "59 Loss 0.0004665122 L1 0.9958123 L2 0.011758658\n",
      "60 Loss 0.00045438128 L1 0.9958575 L2 0.011681993\n",
      "61 Loss 0.00044272013 L1 0.9960835 L2 0.011629707\n",
      "62 Loss 0.00043243088 L1 0.9965337 L2 0.011618598\n",
      "63 Loss 0.00042298948 L1 0.99726415 L2 0.011597281\n",
      "64 Loss 0.000413773 L1 0.99616086 L2 0.011528955\n",
      "65 Loss 0.00040504144 L1 0.99706143 L2 0.011592743\n",
      "66 Loss 0.00039612467 L1 0.99689245 L2 0.011669616\n",
      "67 Loss 0.00038861885 L1 0.99695146 L2 0.011634199\n",
      "68 Loss 0.0003807851 L1 0.9967936 L2 0.011625215\n",
      "69 Loss 0.00037271623 L1 0.9968263 L2 0.0116090095\n",
      "70 Loss 0.00036537403 L1 0.9971371 L2 0.011604909\n",
      "71 Loss 0.00035847945 L1 0.99659663 L2 0.01151061\n",
      "72 Loss 0.00035194244 L1 0.99664235 L2 0.0115343975\n",
      "73 Loss 0.00034543144 L1 0.9968274 L2 0.011556196\n",
      "74 Loss 0.00033788633 L1 0.99699396 L2 0.01154846\n",
      "75 Loss 0.0003309943 L1 0.9967456 L2 0.01151413\n",
      "76 Loss 0.00032491225 L1 0.99699515 L2 0.011546411\n",
      "77 Loss 0.00031871223 L1 0.99745893 L2 0.011522921\n",
      "78 Loss 0.00031320727 L1 0.9972237 L2 0.011536341\n",
      "79 Loss 0.0003075126 L1 0.99720466 L2 0.011603214\n",
      "80 Loss 0.0003021138 L1 0.9971078 L2 0.011567494\n",
      "81 Loss 0.0002966125 L1 0.9971009 L2 0.0116284015\n",
      "82 Loss 0.00029157673 L1 0.9974081 L2 0.011558342\n",
      "83 Loss 0.00028695245 L1 0.99712396 L2 0.011541191\n",
      "84 Loss 0.0002824354 L1 0.9971433 L2 0.011509729\n",
      "85 Loss 0.0002784396 L1 0.996755 L2 0.011520431\n",
      "86 Loss 0.00027420124 L1 0.9974033 L2 0.011497384\n",
      "87 Loss 0.00026982644 L1 0.99686617 L2 0.011471441\n",
      "88 Loss 0.00026544297 L1 0.9972693 L2 0.011437028\n",
      "89 Loss 0.00026182865 L1 0.9973639 L2 0.0113998605\n",
      "90 Loss 0.0002582373 L1 0.9974224 L2 0.011346982\n",
      "91 Loss 0.00025478133 L1 0.9974791 L2 0.011341179\n",
      "92 Loss 0.00025136693 L1 0.99748486 L2 0.011280025\n",
      "93 Loss 0.0002475087 L1 0.99738926 L2 0.011298979\n",
      "94 Loss 0.00024376332 L1 0.9975693 L2 0.011315746\n",
      "95 Loss 0.00024003483 L1 0.99759537 L2 0.011313231\n",
      "96 Loss 0.00023643476 L1 0.99784464 L2 0.011292983\n",
      "97 Loss 0.00023311151 L1 0.9977519 L2 0.011276438\n",
      "98 Loss 0.00022995983 L1 0.9982776 L2 0.011278167\n",
      "99 Loss 0.00022715297 L1 0.9979162 L2 0.011272709\n",
      "100 Loss 0.00022342938 L1 0.9982535 L2 0.011374868\n",
      "101 Loss 0.00021963709 L1 0.9981172 L2 0.011347824\n",
      "102 Loss 0.00021622745 L1 0.9985154 L2 0.011353115\n",
      "103 Loss 0.00021286089 L1 0.9984369 L2 0.011361101\n",
      "104 Loss 0.00020995276 L1 0.99832785 L2 0.011390993\n",
      "105 Loss 0.0002070398 L1 0.99854255 L2 0.011342194\n",
      "106 Loss 0.00020436855 L1 0.9981764 L2 0.011363458\n",
      "107 Loss 0.00020178952 L1 0.9984796 L2 0.0113606155\n",
      "108 Loss 0.00019913798 L1 0.99824804 L2 0.011376883\n",
      "109 Loss 0.0001964122 L1 0.99834013 L2 0.011381318\n",
      "110 Loss 0.00019419345 L1 0.9983124 L2 0.011378142\n",
      "111 Loss 0.00019205123 L1 0.9979031 L2 0.011364844\n",
      "112 Loss 0.00018985501 L1 0.9985748 L2 0.011356987\n",
      "113 Loss 0.00018764453 L1 0.9983463 L2 0.011350222\n",
      "114 Loss 0.00018569367 L1 0.9984916 L2 0.011320127\n",
      "115 Loss 0.00018367097 L1 0.9983966 L2 0.011314716\n",
      "116 Loss 0.00018131283 L1 0.99847287 L2 0.011300725\n",
      "117 Loss 0.00017924892 L1 0.99836975 L2 0.011302153\n",
      "118 Loss 0.00017745644 L1 0.9985043 L2 0.011298963\n",
      "119 Loss 0.00017555768 L1 0.9985865 L2 0.011295749\n",
      "120 Loss 0.00017361726 L1 0.99858314 L2 0.011268708\n",
      "121 Loss 0.00017172779 L1 0.9986009 L2 0.01128031\n",
      "122 Loss 0.00017002574 L1 0.9985773 L2 0.011273807\n",
      "123 Loss 0.00016825146 L1 0.99861914 L2 0.011283757\n",
      "124 Loss 0.00016641509 L1 0.9987231 L2 0.011287682\n",
      "125 Loss 0.00016477199 L1 0.998545 L2 0.011257992\n",
      "126 Loss 0.00016290424 L1 0.9988342 L2 0.011259818\n",
      "127 Loss 0.00016116508 L1 0.9986784 L2 0.011264425\n",
      "128 Loss 0.00015957274 L1 0.9987797 L2 0.011272865\n",
      "129 Loss 0.00015797622 L1 0.9985543 L2 0.011248091\n",
      "130 Loss 0.00015641496 L1 0.9987765 L2 0.011238116\n",
      "131 Loss 0.0001550395 L1 0.9987938 L2 0.011231032\n",
      "132 Loss 0.0001535313 L1 0.99878836 L2 0.011235949\n",
      "133 Loss 0.00015200357 L1 0.9988044 L2 0.01119286\n",
      "134 Loss 0.00015057129 L1 0.9989454 L2 0.011241103\n",
      "135 Loss 0.00014911903 L1 0.9988128 L2 0.011233197\n",
      "136 Loss 0.00014767105 L1 0.99902296 L2 0.01124291\n",
      "137 Loss 0.00014590955 L1 0.99889654 L2 0.0112264585\n",
      "138 Loss 0.00014434659 L1 0.9992492 L2 0.011218326\n",
      "139 Loss 0.00014295553 L1 0.9989751 L2 0.0112076495\n",
      "140 Loss 0.00014143421 L1 0.99905306 L2 0.011180658\n",
      "141 Loss 0.00013991189 L1 0.9990208 L2 0.011189128\n",
      "142 Loss 0.00013859275 L1 0.99893814 L2 0.011169171\n",
      "143 Loss 0.000137313 L1 0.9990721 L2 0.011155228\n",
      "144 Loss 0.00013598191 L1 0.99892735 L2 0.011149961\n",
      "145 Loss 0.00013466137 L1 0.99923056 L2 0.0111472765\n",
      "146 Loss 0.0001334011 L1 0.99901915 L2 0.011144992\n",
      "147 Loss 0.00013193462 L1 0.9992153 L2 0.011132361\n",
      "148 Loss 0.00013064235 L1 0.99911076 L2 0.011150057\n",
      "149 Loss 0.00012943502 L1 0.9992364 L2 0.011140323\n",
      "150 Loss 0.00012826144 L1 0.9990795 L2 0.011127895\n",
      "151 Loss 0.00012713554 L1 0.99910975 L2 0.011117501\n",
      "152 Loss 0.00012597501 L1 0.9989895 L2 0.011113203\n",
      "153 Loss 0.00012475217 L1 0.99925786 L2 0.01110214\n",
      "154 Loss 0.00012357275 L1 0.9989746 L2 0.0110956635\n",
      "155 Loss 0.00012237864 L1 0.9991824 L2 0.011101067\n",
      "156 Loss 0.00012119144 L1 0.9990544 L2 0.011066453\n",
      "157 Loss 0.00012002826 L1 0.99921274 L2 0.011064934\n",
      "158 Loss 0.00011887552 L1 0.99908763 L2 0.011057433\n",
      "159 Loss 0.00011791396 L1 0.9990598 L2 0.0110460725\n",
      "160 Loss 0.00011681438 L1 0.999154 L2 0.011036386\n",
      "161 Loss 0.00011575171 L1 0.9990059 L2 0.011041465\n",
      "162 Loss 0.000114626775 L1 0.9990111 L2 0.011046918\n",
      "163 Loss 0.00011356718 L1 0.99900603 L2 0.011037644\n",
      "164 Loss 0.00011264794 L1 0.9989027 L2 0.011023477\n",
      "165 Loss 0.00011168205 L1 0.99895054 L2 0.011016234\n",
      "166 Loss 0.00011068357 L1 0.9988898 L2 0.011007527\n",
      "167 Loss 0.00010987218 L1 0.9990786 L2 0.011026675\n",
      "168 Loss 0.00010901857 L1 0.99885195 L2 0.011015688\n",
      "169 Loss 0.00010825231 L1 0.99896854 L2 0.011025314\n",
      "170 Loss 0.000107470914 L1 0.9988645 L2 0.011018116\n",
      "171 Loss 0.00010659045 L1 0.9988195 L2 0.0110174045\n",
      "172 Loss 0.00010576732 L1 0.998862 L2 0.011022517\n",
      "173 Loss 0.000104936626 L1 0.9988912 L2 0.01102103\n",
      "174 Loss 0.000104047685 L1 0.9988444 L2 0.011029313\n",
      "175 Loss 0.000103172126 L1 0.99888504 L2 0.011034531\n",
      "176 Loss 0.00010223464 L1 0.9988092 L2 0.011001434\n",
      "177 Loss 0.00010143035 L1 0.99890876 L2 0.011008158\n",
      "178 Loss 0.000100662364 L1 0.99877495 L2 0.01098684\n",
      "179 Loss 9.981688e-05 L1 0.9987223 L2 0.010979621\n",
      "180 Loss 9.891704e-05 L1 0.9989509 L2 0.010970358\n",
      "181 Loss 9.806063e-05 L1 0.998849 L2 0.010963934\n",
      "182 Loss 9.7160766e-05 L1 0.9989469 L2 0.010968221\n",
      "183 Loss 9.679361e-05 L1 0.9989872 L2 0.01095593\n",
      "184 Loss 9.5971394e-05 L1 0.99890757 L2 0.010964118\n",
      "185 Loss 9.517794e-05 L1 0.99899566 L2 0.010945915\n",
      "186 Loss 9.478905e-05 L1 0.9988711 L2 0.010927355\n",
      "187 Loss 9.4038114e-05 L1 0.9990896 L2 0.010955445\n",
      "188 Loss 9.33672e-05 L1 0.99889624 L2 0.010956092\n",
      "189 Loss 9.2606286e-05 L1 0.99900836 L2 0.010962514\n",
      "190 Loss 9.173545e-05 L1 0.99895424 L2 0.010975807\n",
      "191 Loss 9.118337e-05 L1 0.99906343 L2 0.010955324\n",
      "192 Loss 9.0338915e-05 L1 0.9991156 L2 0.010936522\n",
      "193 Loss 8.9632274e-05 L1 0.9990137 L2 0.010932206\n",
      "194 Loss 8.894953e-05 L1 0.9993052 L2 0.010914799\n",
      "195 Loss 8.8281595e-05 L1 0.99896485 L2 0.010897353\n",
      "196 Loss 8.758455e-05 L1 0.99930197 L2 0.010902777\n",
      "197 Loss 8.700667e-05 L1 0.9991194 L2 0.010900883\n",
      "198 Loss 8.639417e-05 L1 0.9993116 L2 0.010872961\n",
      "199 Loss 8.608196e-05 L1 0.99916667 L2 0.010873472\n",
      "200 Loss 8.545868e-05 L1 0.9991649 L2 0.010866512\n",
      "201 Loss 8.485201e-05 L1 0.9992671 L2 0.010864806\n",
      "202 Loss 8.421058e-05 L1 0.99921393 L2 0.01086018\n",
      "203 Loss 8.353776e-05 L1 0.9991103 L2 0.0108535215\n",
      "204 Loss 8.288717e-05 L1 0.99938583 L2 0.010860329\n",
      "205 Loss 8.233933e-05 L1 0.99933827 L2 0.010853535\n",
      "206 Loss 8.223485e-05 L1 0.99936664 L2 0.0108571155\n",
      "207 Loss 8.161795e-05 L1 0.9991749 L2 0.010862757\n",
      "208 Loss 8.109223e-05 L1 0.9994488 L2 0.010866821\n",
      "209 Loss 8.0731945e-05 L1 0.9993197 L2 0.010871667\n",
      "210 Loss 8.048638e-05 L1 0.99935025 L2 0.01085907\n",
      "211 Loss 7.9897916e-05 L1 0.9995048 L2 0.010866251\n",
      "212 Loss 7.945887e-05 L1 0.9993103 L2 0.01086415\n",
      "213 Loss 7.896258e-05 L1 0.99958444 L2 0.010870899\n",
      "214 Loss 7.841173e-05 L1 0.99948716 L2 0.010859143\n",
      "215 Loss 7.7996716e-05 L1 0.9995188 L2 0.01085776\n",
      "216 Loss 7.737693e-05 L1 0.999411 L2 0.010844106\n",
      "217 Loss 7.6846976e-05 L1 0.99952275 L2 0.010849871\n",
      "218 Loss 7.6837474e-05 L1 0.99951154 L2 0.010856226\n",
      "219 Loss 7.662721e-05 L1 0.99943185 L2 0.010848096\n",
      "220 Loss 7.644131e-05 L1 0.9994373 L2 0.010853287\n",
      "221 Loss 7.592804e-05 L1 0.9995311 L2 0.01085854\n",
      "222 Loss 7.544707e-05 L1 0.9994319 L2 0.01083928\n",
      "223 Loss 7.505016e-05 L1 0.99946016 L2 0.010839623\n",
      "224 Loss 7.495319e-05 L1 0.9995069 L2 0.010852312\n",
      "225 Loss 7.4617354e-05 L1 0.99959755 L2 0.010829093\n",
      "226 Loss 7.411984e-05 L1 0.9994262 L2 0.010808542\n",
      "227 Loss 7.3600924e-05 L1 0.99942243 L2 0.0108172735\n",
      "228 Loss 7.311038e-05 L1 0.9995095 L2 0.010813229\n",
      "229 Loss 7.3006915e-05 L1 0.99940467 L2 0.01081203\n",
      "230 Loss 7.292296e-05 L1 0.99939555 L2 0.010811022\n",
      "231 Loss 7.241692e-05 L1 0.9995525 L2 0.010817282\n",
      "232 Loss 7.194752e-05 L1 0.99946135 L2 0.010822259\n",
      "233 Loss 7.152738e-05 L1 0.99944377 L2 0.010803127\n",
      "234 Loss 7.1022485e-05 L1 0.9994953 L2 0.010802987\n",
      "235 Loss 7.090313e-05 L1 0.9994824 L2 0.010802915\n",
      "236 Loss 7.041007e-05 L1 0.99946314 L2 0.01080455\n",
      "237 Loss 6.9994436e-05 L1 0.99949104 L2 0.010788951\n",
      "238 Loss 6.954146e-05 L1 0.99949276 L2 0.010806042\n",
      "239 Loss 6.914053e-05 L1 0.9994968 L2 0.010789898\n",
      "240 Loss 6.8831374e-05 L1 0.99951696 L2 0.010784822\n",
      "241 Loss 6.836873e-05 L1 0.99954146 L2 0.010779252\n",
      "242 Loss 6.7885e-05 L1 0.9995688 L2 0.01078542\n",
      "243 Loss 6.743545e-05 L1 0.9994897 L2 0.010775031\n",
      "244 Loss 6.7007735e-05 L1 0.99968135 L2 0.010773036\n",
      "245 Loss 6.672589e-05 L1 0.9995378 L2 0.010756196\n",
      "246 Loss 6.646266e-05 L1 0.9995918 L2 0.01075162\n",
      "247 Loss 6.609909e-05 L1 0.99953556 L2 0.010752961\n",
      "248 Loss 6.57176e-05 L1 0.99969494 L2 0.010762316\n",
      "249 Loss 6.571199e-05 L1 0.999687 L2 0.010753742\n",
      "250 Loss 6.53299e-05 L1 0.9996707 L2 0.010751615\n",
      "251 Loss 6.517506e-05 L1 0.9997221 L2 0.010753165\n",
      "252 Loss 6.483443e-05 L1 0.9996991 L2 0.0107472725\n",
      "253 Loss 6.445596e-05 L1 0.9997436 L2 0.010744376\n",
      "254 Loss 6.4080115e-05 L1 0.9996715 L2 0.010737309\n",
      "255 Loss 6.368403e-05 L1 0.9997515 L2 0.010728338\n",
      "256 Loss 6.359539e-05 L1 0.99978906 L2 0.010723947\n",
      "257 Loss 6.3559404e-05 L1 0.9997857 L2 0.010733069\n",
      "258 Loss 6.341422e-05 L1 0.9997081 L2 0.010732715\n",
      "259 Loss 6.334713e-05 L1 0.9996288 L2 0.0107254535\n",
      "260 Loss 6.331067e-05 L1 0.9996244 L2 0.010730936\n",
      "261 Loss 6.314676e-05 L1 0.9996706 L2 0.010735103\n",
      "262 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "263 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "264 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "265 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "266 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "267 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "268 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "269 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "270 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "271 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "272 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "273 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "274 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "275 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "276 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "277 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "278 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "279 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "280 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "281 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "282 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "283 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "284 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "285 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "286 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "287 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "288 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "289 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "290 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "291 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "292 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "293 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "294 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "295 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "296 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "297 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "298 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "299 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "300 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "301 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "302 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "303 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "304 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "305 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "306 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "307 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "308 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "309 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "310 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "311 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "312 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "313 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "314 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "315 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "316 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "317 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "318 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "319 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "320 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "321 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "322 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "323 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "324 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "325 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "326 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "327 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "328 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "329 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "330 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "331 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "332 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "333 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "334 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "335 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "336 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "337 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "338 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "339 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "340 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "341 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "342 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "343 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "344 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "345 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "346 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "347 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "348 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "349 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "350 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "351 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "352 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "353 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "354 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "355 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "356 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "357 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "358 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "359 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "360 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "361 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "362 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "363 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "364 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "365 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "366 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "367 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "368 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "369 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "370 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "371 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "372 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "373 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "374 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "375 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "376 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "377 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "378 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "379 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "380 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "381 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "382 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "383 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "384 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "385 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "386 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "387 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "388 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "389 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "390 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "391 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "392 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "393 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "394 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "395 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "396 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "397 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "398 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "399 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "400 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "401 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "402 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "403 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "404 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "405 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "406 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "407 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "408 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "409 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "410 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "411 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "412 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "413 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "414 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "415 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "416 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "417 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "418 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "419 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "420 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "421 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "422 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "423 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "424 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "425 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "426 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "427 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "428 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "429 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "430 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "431 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "432 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "433 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "434 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "435 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "436 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "437 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "438 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "439 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "440 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "441 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "442 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "443 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "444 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "445 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "446 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "447 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "448 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "449 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "450 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "451 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "452 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "453 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "454 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "455 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "456 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "457 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "458 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "459 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "460 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "461 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "462 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "463 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "464 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "465 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "466 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "467 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "468 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "469 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "470 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "471 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "472 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "473 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "474 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "475 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "476 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "477 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "478 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "479 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "480 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "481 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "482 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "483 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "484 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "485 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "486 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "487 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "488 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "489 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "490 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "491 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "492 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "493 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "494 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "495 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "496 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "497 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "498 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "499 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "500 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "501 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "502 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "503 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "504 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "505 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "506 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "507 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "508 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "509 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "510 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "511 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "512 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "513 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "514 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "515 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "516 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "517 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "518 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "519 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "520 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "521 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "522 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "523 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "524 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "525 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "526 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "527 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "528 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "529 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "530 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "531 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "532 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "533 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "534 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "535 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "536 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "537 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "538 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "539 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "540 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "541 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "542 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "543 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "544 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "545 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "546 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "547 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "548 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "549 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "550 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "551 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "552 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "553 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "554 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "555 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "556 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "557 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "558 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "559 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "560 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "561 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "562 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "563 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "564 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "565 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "566 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "567 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "568 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "569 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "570 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "571 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "572 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "573 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "574 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "575 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "576 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "577 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "578 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "579 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "580 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "581 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "582 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "583 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "584 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "585 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "586 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "587 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "588 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "589 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "590 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "591 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "592 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "593 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "594 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "595 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "596 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "597 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "598 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "599 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "600 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "601 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "602 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "603 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "604 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "605 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "606 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "607 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "608 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "609 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "610 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "611 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "612 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "613 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "614 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "615 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "616 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "617 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "618 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "619 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "620 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "621 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "622 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "623 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "624 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "625 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "626 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "627 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "628 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "629 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "630 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "631 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "632 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "633 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "634 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "635 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "636 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "637 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "638 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "639 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "640 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "641 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "642 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "643 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "644 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "645 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "646 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "647 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "648 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "649 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "650 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "651 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "652 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "653 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "654 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "655 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "656 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "657 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "658 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "659 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "660 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "661 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "662 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "663 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "664 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "665 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "666 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "667 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "668 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "669 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "670 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "671 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "672 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "673 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "674 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "675 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "676 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "677 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "678 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "679 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "680 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "681 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "682 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "683 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "684 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "685 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "686 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "687 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "688 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "689 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "690 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "691 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "692 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "693 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "694 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "695 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "696 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "697 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "698 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "699 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "700 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "701 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "702 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "703 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "704 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "705 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "706 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "707 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "708 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "709 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "710 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "711 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "712 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "713 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "714 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "715 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "716 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "717 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "718 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "719 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "720 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "721 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "722 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "723 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "724 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "725 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "726 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "727 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "728 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "729 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "730 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "731 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "732 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "733 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "734 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "735 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "736 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "737 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "738 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "739 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "740 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "741 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "742 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "743 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "744 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "745 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "746 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "747 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "748 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "749 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "750 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "751 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "752 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "753 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "754 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "755 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "756 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "757 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "758 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "759 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "760 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "761 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "762 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "763 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "764 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "765 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "766 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "767 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "768 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "769 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "770 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "771 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "772 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "773 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "774 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "775 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "776 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "777 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "778 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "779 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "780 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "781 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "782 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "783 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "784 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "785 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "786 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "787 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "788 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "789 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "790 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "791 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "792 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "793 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "794 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "795 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "796 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "797 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "798 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "799 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "800 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "801 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "802 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "803 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "804 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "805 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "806 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "807 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "808 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "809 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "810 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "811 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "812 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "813 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "814 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "815 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "816 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "817 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "818 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "819 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "820 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "821 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "822 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "823 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "824 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "825 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "826 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "827 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "828 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "829 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "830 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "831 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "832 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "833 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "834 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "835 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "836 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "837 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "838 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "839 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "840 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "841 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "842 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "843 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "844 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "845 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "846 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "847 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "848 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "849 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "850 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "851 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "852 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "853 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "854 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "855 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "856 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "857 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "858 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "859 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "860 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "861 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "862 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "863 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "864 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "865 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "866 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "867 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "868 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "869 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "870 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "871 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "872 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "873 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "874 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "875 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "876 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "877 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "878 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "879 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "880 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "881 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "882 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "883 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "884 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "885 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "886 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "887 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "888 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "889 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "890 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "891 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "892 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "893 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "894 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "895 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "896 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "897 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "898 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "899 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "900 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "901 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "902 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "903 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "904 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "905 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "906 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "907 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "908 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "909 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "910 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "911 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "912 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "913 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "914 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "915 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "916 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "917 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "918 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "919 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "920 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "921 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "922 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "923 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "924 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "925 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "926 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "927 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "928 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "929 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "930 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "931 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "932 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "933 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "934 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "935 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "936 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "937 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "938 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "939 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "940 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "941 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "942 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "943 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "944 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "945 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "946 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "947 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "948 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "949 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "950 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "951 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "952 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "953 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "954 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "955 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "956 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "957 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "958 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "959 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "960 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "961 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "962 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "963 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "964 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "965 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "966 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "967 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "968 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "969 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "970 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "971 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "972 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "973 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "974 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "975 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "976 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "977 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "978 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "979 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "980 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "981 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "982 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "983 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "984 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "985 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "986 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "987 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "988 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "989 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "990 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "991 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "992 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "993 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "994 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "995 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "996 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "997 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "998 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "999 Loss 6.3120504e-05 L1 0.99966794 L2 0.010735364\n",
      "Training time: 680.10\n"
     ]
    }
   ],
   "source": [
    "max_reps = 1\n",
    "max_iter = 1000\n",
    "\n",
    "train_loss_full = []\n",
    "\n",
    "test_mse_u_full = []\n",
    "test_re_u_full = []\n",
    "test_mse_v_full = []\n",
    "test_re_v_full = []\n",
    "test_mse_p_full = []\n",
    "test_re_p_full = []\n",
    "\n",
    "\n",
    "beta_full = []\n",
    "lambda1_full = []\n",
    "lambda2_full = []\n",
    "\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "beta_init = 1.0\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(label)\n",
    "    train_loss = []\n",
    "    \n",
    "    test_mse_u_loss = []\n",
    "    test_re_u_loss = []\n",
    "    \n",
    "    test_mse_v_loss = []\n",
    "    test_re_v_loss = []\n",
    "    \n",
    "    test_mse_p_loss = []\n",
    "    test_re_p_loss = []\n",
    "    \n",
    "    lambda1_val = []\n",
    "    lambda2_val = []\n",
    "    beta_val = []\n",
    "\n",
    "    print(reps)\n",
    "\n",
    "    torch.manual_seed(reps*36)\n",
    "    N_T = 5000\n",
    "\n",
    "    layers = np.array([3,50,50,50,50,2]) #9 hidden layers\n",
    "\n",
    "    PINN = Sequentialmodel(layers,beta_init)\n",
    "\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=1, \n",
    "                              max_iter = 20, \n",
    "                              max_eval = 30, \n",
    "                              tolerance_grad = 1e-8, \n",
    "                              tolerance_change = 1e-8, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "    nan_flag = train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    \n",
    "    test_mse_u_full.append(test_mse_u_loss)\n",
    "    test_re_u_full.append(test_re_u_loss)\n",
    "    test_mse_v_full.append(test_mse_v_loss)\n",
    "    test_re_v_full.append(test_re_v_loss)\n",
    "    test_mse_p_full.append(test_mse_p_loss)\n",
    "    test_re_p_full.append(test_re_p_loss)\n",
    "\n",
    "    #elapsed_time[reps] = time.time() - start_time\n",
    "    beta_full.append(beta_val)\n",
    "    lambda1_full.append(lambda1_val)\n",
    "    lambda2_full.append(lambda2_val)\n",
    "\n",
    "    #print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_u\": test_mse_u_full,\"test_re_u_loss\": test_re_u_full,\"test_mse_v\": test_mse_v_full,\"test_re_v_loss\": test_re_v_full,\"test_mse_p\": test_mse_p_full,\"test_re_p_loss\": test_re_p_full,\"Time\": elapsed_time, \"beta\": beta_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold, \"lambda1\":lambda1_full,\"lambda2\":lambda2_full}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat('cylinder_nektar_wake.mat')\n",
    "           \n",
    "U_star = data['U_star'] # N x 2 x T\n",
    "P_star = data['p_star'] # N x T\n",
    "t_star = data['t'] # T x 1\n",
    "X_star = data['X_star'] # N x 2\n",
    "\n",
    "N = X_star.shape[0]\n",
    "T = t_star.shape[0]\n",
    "\n",
    "# # Rearrange Data \n",
    "XX = np.tile(X_star[:,0:1], (1,T)) # N x T\n",
    "YY = np.tile(X_star[:,1:2], (1,T)) # N x T\n",
    "TT = np.tile(t_star, (1,N)).T # N x T\n",
    "\n",
    "UU = U_star[:,0,:] # N x T\n",
    "VV = U_star[:,1,:] # N x T\n",
    "PP = P_star # N x T\n",
    "\n",
    "x = XX.flatten()[:,None].reshape(-1,1) # NT x 1\n",
    "y = YY.flatten()[:,None].reshape(-1,1) # NT x 1\n",
    "t = TT.flatten()[:,None].reshape(-1,1) # NT x 1\n",
    "\n",
    "u_true = UU.flatten()[:,None].reshape(-1,1) # NT x 1\n",
    "v_true = VV.flatten()[:,None].reshape(-1,1) # NT x 1\n",
    "p_true = PP.flatten()[:,None].reshape(-1,1) # NT x 1\n",
    "\n",
    "label = \"3D_NS_stan\"\n",
    "\n",
    "loss_thresh = 10000\n",
    "\n",
    "N_train = x.shape[0]\n",
    "xyt = np.hstack((x,y,t))\n",
    "\n",
    "lb_xyt = xyt[0]\n",
    "ub_xyt = xyt[-1]\n",
    "\n",
    "# xyt_test_tensor = torch.from_numpy(xyt).float().to(device)\n",
    "np.random.seed(1234)\n",
    "idx = np.where(t==5)\n",
    "idx = idx[0]\n",
    "u_true_test = u_true[idx,:]\n",
    "v_true_test = v_true[idx,:]\n",
    "p_true_test = p_true[idx,:]\n",
    "\n",
    "u_true_norm = np.linalg.norm(u_true,2)\n",
    "v_true_norm = np.linalg.norm(v_true,2)\n",
    "p_true_norm = np.linalg.norm(p_true,2)\n",
    "\n",
    "# u_true_test = torch.from_numpy(u_true_test).float().to(device)\n",
    "# v_true_test = torch.from_numpy(v_true_test).float().to(device)\n",
    "# p_true_test = torch.from_numpy(p_true_test).float().to(device)\n",
    "\n",
    "x_tensor = torch.from_numpy(x[idx,:]).float().to(device)\n",
    "y_tensor = torch.from_numpy(y[idx,:]).float().to(device)\n",
    "t_tensor = torch.from_numpy(t[idx,:]).float().to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "u,v,p = PINN.test()\n",
    "fig,ax = plt.subplots(1,1)\n",
    "img = ax.imshow(p.reshape(50,100),cmap = 'jet')\n",
    "cbar = fig.colorbar(img, ax=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig,ax = plt.subplots(1,1)\n",
    "img = ax.imshow(p_true_test.reshape(50,100),cmap = 'jet')\n",
    "cbar = fig.colorbar(img, ax=ax)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "HT_stan_v3_15Aug2022_MP4Video.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
