{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8byrnUmNKGR",
    "outputId": "510f67c6-b225-41ba-e78a-d0a6e54ded85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YvP0Nx4vNOlZ",
    "outputId": "4c900037-3d26-473d-c9aa-392edcfda7eb"
   },
   "outputs": [],
   "source": [
    "# from google.colab import drive\n",
    "# drive.mount('/content/gdrive')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "hDzAYhTsNbP6",
    "outputId": "a1b70a95-2d46-4fe5-8a51-d369c604c433"
   },
   "outputs": [],
   "source": [
    "# %cd '/content/gdrive/MyDrive/Virginia Tech /Fall 2022/Codes from GPU/PINN_Stan/1D FODE/stan'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "1wXUvTNETmrW",
    "outputId": "2d3abfaa-8287-4fe9-d053-d79e6506dc03"
   },
   "outputs": [],
   "source": [
    "# !pip install smt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "dmSz5jcVVt4p"
   },
   "outputs": [],
   "source": [
    "# lr_tune = np.array([0.05,0.1,0.25,0.5,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "uoNYpDzlNKGV"
   },
   "outputs": [],
   "source": [
    "def true_1D_2(x): #True function for 1D_1 dy/dx = cos(0.01*x) BC1: y(0)=0; x \\in [-100,100]\n",
    "    y = extent*np.sin(x)/2 + np.square(x)/2\n",
    "    return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "BR02v-fkNKGV"
   },
   "outputs": [],
   "source": [
    "level = \"high\"\n",
    "label = \"1D_FODE_stan_\" + level\n",
    "extent = 100.0 #The domain/support is between -extent to +extent\n",
    "\n",
    "loss_thresh = 0.005\n",
    "\n",
    "x = np.linspace(extent,-1.0*extent,5000).reshape(-1,1)\n",
    "ysol = true_1D_2(x)\n",
    "\n",
    "bc1_x = np.array(0).reshape(-1,1) \n",
    "bc1_y = np.array(0).reshape(-1,1)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "y_bc1_train = torch.from_numpy(bc1_y).float().to(device)\n",
    "\n",
    " \n",
    "x_test = x.reshape(-1,1)\n",
    "x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "\n",
    "y_true = true_1D_2(x_test)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(x[0]) \n",
    "ub = np.array(x[-1]) \n",
    "\n",
    "#torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff478099810>]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "SyyktBKBXRo1"
   },
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "\n",
    "  #Collocation Points\n",
    "  # Latin Hypercube sampling for collocation points \n",
    "  # N_f sets of tuples(x,y)\n",
    "  x01 = np.array([[0.0, 1.0]])\n",
    "  sampling = LHS(xlimits=x01,random_state =seed)\n",
    "\n",
    "  x_coll_train = lb + (ub-lb)*sampling(N_f)\n",
    "  x_coll_train = np.vstack((x_coll_train, bc1_x)) # append training points to collocation points \n",
    "\n",
    "  return x_coll_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "o1b21zLnNKGW"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "        \n",
    "        self.beta = Parameter(torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "        \n",
    "        self.iter = 0\n",
    "        \n",
    "              \n",
    "    'forward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = 2.0*(x - l_b)/(u_b - l_b) - 1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z) + self.beta[:,i]*z*self.activation(z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,x,y):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(x), y)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_PDE(self, x_coll,f_hat):\n",
    "             \n",
    "        g = x_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        y = self.forward(g) \n",
    "\n",
    "        y_x = autograd.grad(y,g,torch.ones([x_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        f = dy_dx - extent*torch.cos(g)/2.0 -g \n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,x_bc1,y_bc1,x_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(x_bc1,y_bc1)\n",
    "        loss_f = self.loss_PDE(x_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "     \n",
    "    \n",
    "    def test(self):\n",
    "        y_pred = self.forward(x_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "\n",
    "        return y_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "       \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "      \n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "fLY2mT5BOgjD"
   },
   "outputs": [],
   "source": [
    "def train_step(seed):\n",
    "    x_coll_np_array = colloc_pts(N_f,seed*123)\n",
    "    x_coll_train = torch.from_numpy(x_coll_np_array).float().to(device)        \n",
    "    \n",
    "    f_hat = torch.zeros(x_coll_train.shape[0],1).to(device)\n",
    "    \n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(x_bc1_train,y_bc1_train,x_coll_train,f_hat)\n",
    "        loss.backward()\n",
    "        #print(loss.cpu().detach().numpy())\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "id": "8srA5uGuObil"
   },
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "id": "0SezTZ_racQB"
   },
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*11)\n",
    "    start_time = time.time() \n",
    "    thresh_flag = 0\n",
    "\n",
    "    x_coll = torch.from_numpy(colloc_pts(N_f,123)).float().to(device)\n",
    "    f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "    \n",
    "    loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_coll,f_hat).cpu().detach().numpy()\n",
    "    data_update(loss_np)\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        train_step(i)\n",
    "\n",
    "        loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "\n",
    "    elapsed_time[rep] = time.time() - start_time  \n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I9BYbcJ0NKGX",
    "outputId": "42720d97-b37a-4c42-b7e5-b77a0ef391f3",
    "scrolled": true,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 4575.6055 Test MSE 5002477.787752137 Test RE 0.9997223654898356\n",
      "1 Train Loss 4567.502 Test MSE 4948827.535363594 Test RE 0.9943470352917565\n",
      "2 Train Loss 4427.264 Test MSE 4606974.914367602 Test RE 0.9593890280300079\n",
      "3 Train Loss 3086.7095 Test MSE 1576836.9947972102 Test RE 0.561280848558456\n",
      "4 Train Loss 1917.0099 Test MSE 260234.9816933635 Test RE 0.2280182779332087\n",
      "5 Train Loss 1692.2147 Test MSE 232397.0194574576 Test RE 0.21547758554364943\n",
      "6 Train Loss 1555.5781 Test MSE 168569.85912997302 Test RE 0.18351720473486605\n",
      "7 Train Loss 1507.9075 Test MSE 248629.94584196588 Test RE 0.22287612140038876\n",
      "8 Train Loss 1473.5692 Test MSE 201898.9945772143 Test RE 0.20084170602231996\n",
      "9 Train Loss 1402.7524 Test MSE 86243.28184286792 Test RE 0.1312651565676845\n",
      "10 Train Loss 1374.4008 Test MSE 85993.02438137213 Test RE 0.1310745680975922\n",
      "11 Train Loss 1344.0465 Test MSE 76203.48767343821 Test RE 0.1233883759173978\n",
      "12 Train Loss 1327.998 Test MSE 79403.76404547643 Test RE 0.12595266722584228\n",
      "13 Train Loss 1303.7274 Test MSE 60504.39266987529 Test RE 0.10994621339508799\n",
      "14 Train Loss 1279.507 Test MSE 45688.674728995815 Test RE 0.09554129071344496\n",
      "15 Train Loss 1252.7797 Test MSE 41638.322235961765 Test RE 0.09120810540816537\n",
      "16 Train Loss 1245.1082 Test MSE 39788.36650342645 Test RE 0.08915893629405235\n",
      "17 Train Loss 1216.8048 Test MSE 31736.648315886265 Test RE 0.07962828337489634\n",
      "18 Train Loss 1201.9758 Test MSE 27637.168519959767 Test RE 0.0743076620046301\n",
      "19 Train Loss 1193.291 Test MSE 24211.186618325908 Test RE 0.06954963598326679\n",
      "20 Train Loss 1182.19 Test MSE 23226.99098972363 Test RE 0.06812135836762712\n",
      "21 Train Loss 1157.6863 Test MSE 27256.303534236264 Test RE 0.07379387260839063\n",
      "22 Train Loss 1147.4243 Test MSE 30023.436955726207 Test RE 0.07744921622628283\n",
      "23 Train Loss 1138.5791 Test MSE 31786.7624622064 Test RE 0.07969112758789684\n",
      "24 Train Loss 1134.717 Test MSE 32579.233165770565 Test RE 0.08067839550830135\n",
      "25 Train Loss 1125.7673 Test MSE 38632.050804095525 Test RE 0.087853831222584\n",
      "26 Train Loss 1118.5344 Test MSE 42993.068727261016 Test RE 0.09268000448632496\n",
      "27 Train Loss 1114.3191 Test MSE 46104.11529742611 Test RE 0.09597467938642522\n",
      "28 Train Loss 1109.4386 Test MSE 54252.26794792566 Test RE 0.10411079736568331\n",
      "29 Train Loss 1104.5767 Test MSE 65104.77951021141 Test RE 0.11404946696944107\n",
      "30 Train Loss 1102.8862 Test MSE 70661.50032627789 Test RE 0.11881690920619117\n",
      "31 Train Loss 1100.8335 Test MSE 70546.13899045945 Test RE 0.11871987987406997\n",
      "32 Train Loss 1098.9265 Test MSE 64340.61151358459 Test RE 0.11337816291937657\n",
      "33 Train Loss 1095.6196 Test MSE 57516.242412113235 Test RE 0.10719686299888477\n",
      "34 Train Loss 1088.6567 Test MSE 52538.40273932231 Test RE 0.10245313600178647\n",
      "35 Train Loss 1086.9519 Test MSE 49845.50266666853 Test RE 0.09979293910113031\n",
      "36 Train Loss 1085.2693 Test MSE 47725.43282783551 Test RE 0.09764764225754763\n",
      "37 Train Loss 1078.8523 Test MSE 50892.24910445907 Test RE 0.10083531185700041\n",
      "38 Train Loss 1076.0482 Test MSE 52805.45583583644 Test RE 0.1027131909859678\n",
      "39 Train Loss 1071.5801 Test MSE 54448.76898240164 Test RE 0.10429917094970109\n",
      "40 Train Loss 1070.0419 Test MSE 56916.07284277414 Test RE 0.10663610826561558\n",
      "41 Train Loss 1067.6285 Test MSE 56681.105228392444 Test RE 0.10641576676941351\n",
      "42 Train Loss 1062.4725 Test MSE 52753.405802883644 Test RE 0.1026625566057098\n",
      "43 Train Loss 1058.7733 Test MSE 53911.07789104644 Test RE 0.10378290698512818\n",
      "44 Train Loss 1055.322 Test MSE 50530.151271030096 Test RE 0.10047595039483227\n",
      "45 Train Loss 1051.921 Test MSE 44664.27674215133 Test RE 0.09446414030903554\n",
      "46 Train Loss 1049.9906 Test MSE 43340.27482392268 Test RE 0.09305348742380569\n",
      "47 Train Loss 1049.1854 Test MSE 43501.370501015284 Test RE 0.0932262667904234\n",
      "48 Train Loss 1042.1005 Test MSE 42577.053236246786 Test RE 0.09223051293195975\n",
      "49 Train Loss 1036.7913 Test MSE 38606.62893430956 Test RE 0.08782492030333001\n",
      "50 Train Loss 1031.779 Test MSE 31259.47978782384 Test RE 0.07902740053215354\n",
      "51 Train Loss 1025.6838 Test MSE 24342.396165674803 Test RE 0.06973783918977762\n",
      "52 Train Loss 1021.72943 Test MSE 19344.785886798167 Test RE 0.06216827073494933\n",
      "53 Train Loss 1019.28595 Test MSE 20152.29019591312 Test RE 0.06345254238485429\n",
      "54 Train Loss 1017.6251 Test MSE 20551.446363113715 Test RE 0.06407786300636217\n",
      "55 Train Loss 1009.1561 Test MSE 18471.756340148215 Test RE 0.06074924981463276\n",
      "56 Train Loss 1004.6824 Test MSE 18047.145207160906 Test RE 0.06004696756899374\n",
      "57 Train Loss 1001.64014 Test MSE 15150.873205568594 Test RE 0.05501810850716891\n",
      "58 Train Loss 1000.811 Test MSE 14387.974079715115 Test RE 0.05361504156763627\n",
      "59 Train Loss 999.74023 Test MSE 15009.532042028743 Test RE 0.05476087763282802\n",
      "60 Train Loss 995.01215 Test MSE 17645.709129301416 Test RE 0.059375377208277964\n",
      "61 Train Loss 991.5493 Test MSE 16203.404245291013 Test RE 0.05689707718256026\n",
      "62 Train Loss 988.9493 Test MSE 16448.57523616511 Test RE 0.0573259111829382\n",
      "63 Train Loss 986.1896 Test MSE 15978.99252860884 Test RE 0.05650170072946639\n",
      "64 Train Loss 981.0219 Test MSE 11977.543954277706 Test RE 0.04891822725687883\n",
      "65 Train Loss 972.06226 Test MSE 4635.539066692123 Test RE 0.030432452050217836\n",
      "66 Train Loss 967.76587 Test MSE 5725.683500978692 Test RE 0.03382209402709105\n",
      "67 Train Loss 964.59314 Test MSE 7725.292655169196 Test RE 0.03928658633292857\n",
      "68 Train Loss 961.87866 Test MSE 8760.103864977566 Test RE 0.041835161645432266\n",
      "69 Train Loss 960.2846 Test MSE 8497.17622992896 Test RE 0.04120255392250973\n",
      "70 Train Loss 958.77673 Test MSE 7337.481621597838 Test RE 0.03828779331670014\n",
      "71 Train Loss 956.7959 Test MSE 6722.014530021004 Test RE 0.036646841269468766\n",
      "72 Train Loss 952.6341 Test MSE 8604.06174157614 Test RE 0.04146088640168082\n",
      "73 Train Loss 948.4447 Test MSE 8612.56669611162 Test RE 0.04148137299668526\n",
      "74 Train Loss 942.3632 Test MSE 10263.959750867112 Test RE 0.04528394912606912\n",
      "75 Train Loss 936.8499 Test MSE 8372.54762921262 Test RE 0.04089927758908724\n",
      "76 Train Loss 932.8087 Test MSE 3162.094665791402 Test RE 0.02513473191479591\n",
      "77 Train Loss 928.7966 Test MSE 2113.285934448859 Test RE 0.020547829569999676\n",
      "78 Train Loss 925.2342 Test MSE 2236.1964817970984 Test RE 0.021136924862953682\n",
      "79 Train Loss 922.7771 Test MSE 2499.401990847669 Test RE 0.02234626163944201\n",
      "80 Train Loss 920.5352 Test MSE 4134.329244431222 Test RE 0.028740171369684146\n",
      "81 Train Loss 918.3696 Test MSE 3845.785640696256 Test RE 0.027719115010877617\n",
      "82 Train Loss 916.69244 Test MSE 3088.0416281918406 Test RE 0.024838673383232014\n",
      "83 Train Loss 914.53064 Test MSE 2397.3027158108994 Test RE 0.02188508624099488\n",
      "84 Train Loss 912.5186 Test MSE 3182.527347861675 Test RE 0.025215808395597707\n",
      "85 Train Loss 912.00397 Test MSE 3019.1013411548433 Test RE 0.02455984769190937\n",
      "86 Train Loss 911.0951 Test MSE 2194.1417063074446 Test RE 0.02093722691746747\n",
      "87 Train Loss 909.5701 Test MSE 1853.0577666375618 Test RE 0.0192411622910705\n",
      "88 Train Loss 908.35504 Test MSE 1865.1003957528487 Test RE 0.019303583155222903\n",
      "89 Train Loss 907.4528 Test MSE 1934.1013668899052 Test RE 0.019657416500986825\n",
      "90 Train Loss 906.403 Test MSE 1786.2746374957733 Test RE 0.018891260624936835\n",
      "91 Train Loss 905.17285 Test MSE 1808.3100694071316 Test RE 0.019007424494956075\n",
      "92 Train Loss 904.243 Test MSE 2136.1954314086643 Test RE 0.020658905764485945\n",
      "93 Train Loss 902.9546 Test MSE 2708.7800015683206 Test RE 0.02326342692788512\n",
      "94 Train Loss 901.30115 Test MSE 3101.770586321038 Test RE 0.024893826611322983\n",
      "95 Train Loss 899.661 Test MSE 3466.2545320600384 Test RE 0.026315828586184798\n",
      "96 Train Loss 897.76855 Test MSE 3961.9252868421554 Test RE 0.02813454942894242\n",
      "97 Train Loss 896.9894 Test MSE 4825.246515546048 Test RE 0.031048925590190395\n",
      "98 Train Loss 894.17523 Test MSE 4315.109182835086 Test RE 0.02936180283925564\n",
      "99 Train Loss 891.77893 Test MSE 3643.4143980526233 Test RE 0.02697994802976366\n",
      "100 Train Loss 889.4338 Test MSE 3219.6885276947814 Test RE 0.025362598909813364\n",
      "101 Train Loss 886.54474 Test MSE 3797.643303305757 Test RE 0.027545071820642007\n",
      "102 Train Loss 882.5551 Test MSE 3469.444883205791 Test RE 0.026327936384888322\n",
      "103 Train Loss 880.3527 Test MSE 2483.0165294042245 Test RE 0.02227289291195016\n",
      "104 Train Loss 878.84814 Test MSE 2059.794324806837 Test RE 0.02028610888755473\n",
      "105 Train Loss 877.3946 Test MSE 2030.0478762181087 Test RE 0.020139095615166427\n",
      "106 Train Loss 876.26105 Test MSE 2241.9845476653163 Test RE 0.021164262096204062\n",
      "107 Train Loss 873.68225 Test MSE 1838.7955339122266 Test RE 0.019166973561009718\n",
      "108 Train Loss 872.43567 Test MSE 1895.3492366218043 Test RE 0.01945948964290096\n",
      "109 Train Loss 871.6686 Test MSE 1919.9197438411463 Test RE 0.019585215793398652\n",
      "110 Train Loss 869.8745 Test MSE 1644.5484484703886 Test RE 0.018126341583069822\n",
      "111 Train Loss 868.49054 Test MSE 1356.229064261302 Test RE 0.0164608913070459\n",
      "112 Train Loss 866.6332 Test MSE 1404.3139691780357 Test RE 0.01675015891187339\n",
      "113 Train Loss 864.6674 Test MSE 1333.1353499143881 Test RE 0.016320142477652896\n",
      "114 Train Loss 863.376 Test MSE 1376.537111012527 Test RE 0.016583675296885543\n",
      "115 Train Loss 862.07886 Test MSE 1373.9817409362922 Test RE 0.0165682753804389\n",
      "116 Train Loss 860.24005 Test MSE 1428.2566471425398 Test RE 0.016892345311020886\n",
      "117 Train Loss 857.5359 Test MSE 1644.6657526156675 Test RE 0.018126988039272773\n",
      "118 Train Loss 856.5688 Test MSE 1659.726196945359 Test RE 0.018209794640760658\n",
      "119 Train Loss 853.5072 Test MSE 1736.8501649010677 Test RE 0.018628075984482073\n",
      "120 Train Loss 851.78314 Test MSE 1934.9247400653094 Test RE 0.019661600270127196\n",
      "121 Train Loss 850.78015 Test MSE 2238.071848614658 Test RE 0.021145786154993684\n",
      "122 Train Loss 849.34576 Test MSE 2949.7625748444734 Test RE 0.024276180294785257\n",
      "123 Train Loss 848.5809 Test MSE 3213.904233801874 Test RE 0.0253398062275182\n",
      "124 Train Loss 847.1678 Test MSE 4193.227318709693 Test RE 0.028944165110102837\n",
      "125 Train Loss 844.9184 Test MSE 5708.469293973476 Test RE 0.03377121287055541\n",
      "126 Train Loss 841.4976 Test MSE 5193.12398993784 Test RE 0.032210774657901634\n",
      "127 Train Loss 837.6047 Test MSE 4103.5923600381375 Test RE 0.02863313691571216\n",
      "128 Train Loss 831.7873 Test MSE 5376.094650067468 Test RE 0.03277330779564162\n",
      "129 Train Loss 829.9847 Test MSE 5623.856933083088 Test RE 0.033519995809099014\n",
      "130 Train Loss 827.9151 Test MSE 3675.5200887118517 Test RE 0.027098560665741123\n",
      "131 Train Loss 826.15045 Test MSE 2127.114149379279 Test RE 0.02061494696731613\n",
      "132 Train Loss 825.12164 Test MSE 1727.041157152173 Test RE 0.018575399690849696\n",
      "133 Train Loss 824.4075 Test MSE 2079.3023570492714 Test RE 0.020381945998010485\n",
      "134 Train Loss 822.35986 Test MSE 2651.43502743031 Test RE 0.023015865890139493\n",
      "135 Train Loss 819.25323 Test MSE 2154.807842623664 Test RE 0.020748709843504836\n",
      "136 Train Loss 818.09515 Test MSE 1895.0231102955572 Test RE 0.019457815406627436\n",
      "137 Train Loss 817.03723 Test MSE 1952.449402173482 Test RE 0.019750437377994562\n",
      "138 Train Loss 814.80457 Test MSE 2018.594982975439 Test RE 0.020082206033713006\n",
      "139 Train Loss 812.7864 Test MSE 1945.8533404331208 Test RE 0.019717047186359617\n",
      "140 Train Loss 810.9119 Test MSE 2791.427965061454 Test RE 0.023615657205959588\n",
      "141 Train Loss 810.09766 Test MSE 3438.984304338716 Test RE 0.026212106299002703\n",
      "142 Train Loss 809.5409 Test MSE 3643.4430690399886 Test RE 0.02698005418571283\n",
      "143 Train Loss 808.5064 Test MSE 3923.5252488175915 Test RE 0.027997873664471602\n",
      "144 Train Loss 805.7667 Test MSE 4841.4050699735335 Test RE 0.03110086971700396\n",
      "145 Train Loss 804.8448 Test MSE 4941.190929550955 Test RE 0.031419743943387216\n",
      "146 Train Loss 803.96594 Test MSE 4440.210104674902 Test RE 0.0297843813670026\n",
      "147 Train Loss 803.3968 Test MSE 4396.450918127416 Test RE 0.02963725236150034\n",
      "148 Train Loss 802.67926 Test MSE 4499.255705864482 Test RE 0.02998176264618119\n",
      "149 Train Loss 801.85956 Test MSE 4546.619000027745 Test RE 0.030139157283228708\n",
      "150 Train Loss 800.952 Test MSE 4676.685729781292 Test RE 0.03056721818750504\n",
      "151 Train Loss 799.60614 Test MSE 4431.407007018082 Test RE 0.02975484167219596\n",
      "152 Train Loss 798.72375 Test MSE 4950.52523477228 Test RE 0.03144940714672692\n",
      "153 Train Loss 797.7166 Test MSE 6036.637502007666 Test RE 0.034728367799231934\n",
      "154 Train Loss 796.6859 Test MSE 6455.419376714678 Test RE 0.03591278242579942\n",
      "155 Train Loss 796.25323 Test MSE 6053.608656351762 Test RE 0.0347771504891893\n",
      "156 Train Loss 795.6846 Test MSE 5233.937725213678 Test RE 0.032337102194897965\n",
      "157 Train Loss 795.2589 Test MSE 4518.061885232941 Test RE 0.030044356825974717\n",
      "158 Train Loss 794.8133 Test MSE 4669.629145124192 Test RE 0.03054414826111781\n",
      "159 Train Loss 793.70294 Test MSE 5512.577671264742 Test RE 0.033186708791635676\n",
      "160 Train Loss 792.5694 Test MSE 5280.966255309973 Test RE 0.03248205668114201\n",
      "161 Train Loss 792.0533 Test MSE 4736.93862067176 Test RE 0.030763497038429653\n",
      "162 Train Loss 790.78064 Test MSE 3577.9402625059874 Test RE 0.02673642685826093\n",
      "163 Train Loss 789.7863 Test MSE 3371.784230083926 Test RE 0.025954741703071636\n",
      "164 Train Loss 789.2452 Test MSE 3779.9475309638465 Test RE 0.027480821386173068\n",
      "165 Train Loss 787.6768 Test MSE 4674.585522693204 Test RE 0.030560353850121187\n",
      "166 Train Loss 786.4574 Test MSE 5511.047231355956 Test RE 0.033182101710132215\n",
      "167 Train Loss 785.5284 Test MSE 5656.156838789731 Test RE 0.03361611690429972\n",
      "168 Train Loss 784.71185 Test MSE 4807.738656915122 Test RE 0.030992545652645427\n",
      "169 Train Loss 783.4068 Test MSE 4504.946459804349 Test RE 0.030000717438769836\n",
      "170 Train Loss 782.1266 Test MSE 4731.619012886638 Test RE 0.030746218398979765\n",
      "171 Train Loss 781.0157 Test MSE 4863.687525661638 Test RE 0.031172358074987864\n",
      "172 Train Loss 778.7122 Test MSE 4292.0773597540065 Test RE 0.029283338933478835\n",
      "173 Train Loss 778.0398 Test MSE 3905.658928612464 Test RE 0.02793405481551781\n",
      "174 Train Loss 775.4139 Test MSE 3145.4814270223455 Test RE 0.025068617636718873\n",
      "175 Train Loss 772.77594 Test MSE 3172.930440390783 Test RE 0.025177760570382412\n",
      "176 Train Loss 769.9061 Test MSE 3415.1115033322267 Test RE 0.02612096802974002\n",
      "177 Train Loss 767.81256 Test MSE 3192.892730460882 Test RE 0.02525683852658864\n",
      "178 Train Loss 765.971 Test MSE 2762.7693423678843 Test RE 0.023494117569175528\n",
      "179 Train Loss 762.5825 Test MSE 2705.452947279957 Test RE 0.023249135906635\n",
      "180 Train Loss 756.26074 Test MSE 2727.9227713572523 Test RE 0.02334548276494376\n",
      "181 Train Loss 752.38605 Test MSE 3261.4667560381836 Test RE 0.025526619317626256\n",
      "182 Train Loss 750.0572 Test MSE 3871.9423257836975 Test RE 0.02781321951701548\n",
      "183 Train Loss 747.9708 Test MSE 3874.3875232904074 Test RE 0.027822000391768142\n",
      "184 Train Loss 743.94214 Test MSE 3115.143894451124 Test RE 0.024947433855475307\n",
      "185 Train Loss 740.40485 Test MSE 2406.5453512443432 Test RE 0.02192723387674886\n",
      "186 Train Loss 739.81464 Test MSE 2327.941615340908 Test RE 0.021566162122412733\n",
      "187 Train Loss 738.84564 Test MSE 2574.502336808495 Test RE 0.022679499640373475\n",
      "188 Train Loss 737.7521 Test MSE 3188.376894558801 Test RE 0.02523897132828827\n",
      "189 Train Loss 734.5 Test MSE 3277.6285594044666 Test RE 0.02558978819258215\n",
      "190 Train Loss 731.559 Test MSE 2501.8878081541443 Test RE 0.022357371280725007\n",
      "191 Train Loss 729.56256 Test MSE 1990.6426091793835 Test RE 0.019942677746487118\n",
      "192 Train Loss 727.1313 Test MSE 1732.2645425818976 Test RE 0.018603468859611907\n",
      "193 Train Loss 724.45276 Test MSE 1649.989528780196 Test RE 0.018156302827722848\n",
      "194 Train Loss 722.5862 Test MSE 1581.736106390064 Test RE 0.017776810294291193\n",
      "195 Train Loss 719.8555 Test MSE 1685.4492239060505 Test RE 0.018350363027385238\n",
      "196 Train Loss 717.523 Test MSE 2269.202646650176 Test RE 0.021292343525929538\n",
      "197 Train Loss 715.10596 Test MSE 2960.2085321554737 Test RE 0.024319126773746928\n",
      "198 Train Loss 714.22797 Test MSE 2880.259280458338 Test RE 0.02398847368432042\n",
      "199 Train Loss 713.07196 Test MSE 3114.189748680757 Test RE 0.02494361295469366\n",
      "Training time: 36.54\n",
      "Training time: 36.54\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 4572.621 Test MSE 4990181.008028632 Test RE 0.9984928817988272\n",
      "1 Train Loss 4513.056 Test MSE 4697832.180832419 Test RE 0.9688032185172402\n",
      "2 Train Loss 4102.0684 Test MSE 3740515.705785242 Test RE 0.8644752521111368\n",
      "3 Train Loss 2120.2798 Test MSE 832630.4334896202 Test RE 0.40786173657945135\n",
      "4 Train Loss 1555.7091 Test MSE 86855.67146755644 Test RE 0.13173037092410694\n",
      "5 Train Loss 1430.1832 Test MSE 62784.022026290106 Test RE 0.1119982896503811\n",
      "6 Train Loss 1332.234 Test MSE 19973.79693641845 Test RE 0.06317091082898632\n",
      "7 Train Loss 1282.9316 Test MSE 24081.0858950144 Test RE 0.06936251904201712\n",
      "8 Train Loss 1258.8229 Test MSE 20858.392081662932 Test RE 0.06455460633069063\n",
      "9 Train Loss 1253.9983 Test MSE 19183.716571660847 Test RE 0.06190891579636137\n",
      "10 Train Loss 1246.4608 Test MSE 10318.924595049852 Test RE 0.04540503796005293\n",
      "11 Train Loss 1242.2457 Test MSE 11375.177190913739 Test RE 0.047672278421494414\n",
      "12 Train Loss 1240.7582 Test MSE 11370.59321854963 Test RE 0.04766267195896429\n",
      "13 Train Loss 1239.7394 Test MSE 7849.880854810805 Test RE 0.039602112776894724\n",
      "14 Train Loss 1235.7164 Test MSE 2236.6643576948736 Test RE 0.021139135970193613\n",
      "15 Train Loss 1234.7831 Test MSE 2288.5975579067267 Test RE 0.021383142922818767\n",
      "16 Train Loss 1232.9802 Test MSE 4039.529759388306 Test RE 0.028408756846871705\n",
      "17 Train Loss 1231.3429 Test MSE 5752.242390252797 Test RE 0.03390044606858215\n",
      "18 Train Loss 1230.5939 Test MSE 3487.823733964923 Test RE 0.02639757837450231\n",
      "19 Train Loss 1229.665 Test MSE 4274.020547337221 Test RE 0.02922167636134856\n",
      "20 Train Loss 1229.0558 Test MSE 4903.049445702582 Test RE 0.03129824314809289\n",
      "21 Train Loss 1227.808 Test MSE 6631.596682942024 Test RE 0.0363995384243208\n",
      "22 Train Loss 1227.3079 Test MSE 6074.593425163153 Test RE 0.03483737564954158\n",
      "23 Train Loss 1226.3636 Test MSE 5566.50557835968 Test RE 0.033348641569170195\n",
      "24 Train Loss 1225.7069 Test MSE 6330.125721777992 Test RE 0.035562557950497335\n",
      "25 Train Loss 1225.5295 Test MSE 7398.437608051788 Test RE 0.03844650193195343\n",
      "26 Train Loss 1224.7095 Test MSE 8313.03741986061 Test RE 0.04075366689402107\n",
      "27 Train Loss 1223.9036 Test MSE 7182.090735694799 Test RE 0.03788020035699327\n",
      "28 Train Loss 1223.3281 Test MSE 6874.897469577421 Test RE 0.037061239192994165\n",
      "29 Train Loss 1222.5548 Test MSE 7219.538716385631 Test RE 0.03797882712028836\n",
      "30 Train Loss 1221.666 Test MSE 7537.303394437471 Test RE 0.03880563747581665\n",
      "31 Train Loss 1220.9224 Test MSE 5850.360108847788 Test RE 0.03418834857304965\n",
      "32 Train Loss 1220.0946 Test MSE 4606.855729063242 Test RE 0.030338152466454917\n",
      "33 Train Loss 1219.369 Test MSE 4399.29565926592 Test RE 0.029646839262238513\n",
      "34 Train Loss 1217.5541 Test MSE 4506.121069297951 Test RE 0.030004628343271122\n",
      "35 Train Loss 1216.6562 Test MSE 3874.246765800497 Test RE 0.027821494996981826\n",
      "36 Train Loss 1215.6051 Test MSE 4394.360560453485 Test RE 0.029630205788609337\n",
      "37 Train Loss 1213.5083 Test MSE 6822.423155241935 Test RE 0.03691952883858342\n",
      "38 Train Loss 1211.8245 Test MSE 5217.4274104174365 Test RE 0.03228605865271327\n",
      "39 Train Loss 1210.4435 Test MSE 4664.20135976954 Test RE 0.030526391467216973\n",
      "40 Train Loss 1209.4055 Test MSE 4356.519321427023 Test RE 0.02950235237508494\n",
      "41 Train Loss 1206.5339 Test MSE 4660.618333492187 Test RE 0.030514664070803013\n",
      "42 Train Loss 1203.5111 Test MSE 4300.379444741755 Test RE 0.02931164635895425\n",
      "43 Train Loss 1202.5349 Test MSE 4646.312005545064 Test RE 0.030467793860874597\n",
      "44 Train Loss 1201.2281 Test MSE 4989.536860733171 Test RE 0.0315730793690491\n",
      "45 Train Loss 1196.5865 Test MSE 4734.458635147848 Test RE 0.0307554429953342\n",
      "46 Train Loss 1191.9894 Test MSE 6768.49705143257 Test RE 0.03677332886554026\n",
      "47 Train Loss 1186.6976 Test MSE 6846.912698993306 Test RE 0.03698573203935888\n",
      "48 Train Loss 1182.6873 Test MSE 6544.317252757248 Test RE 0.036159215203666994\n",
      "49 Train Loss 1174.9604 Test MSE 3677.507632061813 Test RE 0.027105886470464222\n",
      "50 Train Loss 1169.9027 Test MSE 2473.9033895101174 Test RE 0.02223198247717042\n",
      "51 Train Loss 1163.7137 Test MSE 1647.6513116188848 Test RE 0.018143433524898972\n",
      "52 Train Loss 1159.5654 Test MSE 2352.3174984067323 Test RE 0.021678777761933343\n",
      "53 Train Loss 1155.3969 Test MSE 1662.9766998134983 Test RE 0.018227617471238567\n",
      "54 Train Loss 1147.7587 Test MSE 5223.466981043133 Test RE 0.032304740037107364\n",
      "55 Train Loss 1146.259 Test MSE 4428.829343553247 Test RE 0.029746186506372337\n",
      "56 Train Loss 1144.1328 Test MSE 4067.3465534968213 Test RE 0.02850640246690879\n",
      "57 Train Loss 1141.5458 Test MSE 3467.559548840809 Test RE 0.026320781968252965\n",
      "58 Train Loss 1138.688 Test MSE 4363.803343632972 Test RE 0.029527005776518195\n",
      "59 Train Loss 1137.168 Test MSE 4595.528515290001 Test RE 0.03030083219248257\n",
      "60 Train Loss 1134.6445 Test MSE 5717.069244148098 Test RE 0.03379664188051539\n",
      "61 Train Loss 1133.4688 Test MSE 2759.6699526021166 Test RE 0.023480935531083866\n",
      "62 Train Loss 1132.9335 Test MSE 2232.4845048140346 Test RE 0.021119374445062105\n",
      "63 Train Loss 1132.313 Test MSE 2419.9022957403363 Test RE 0.021988000564276865\n",
      "64 Train Loss 1131.2638 Test MSE 2791.128737718183 Test RE 0.023614391430908946\n",
      "65 Train Loss 1128.853 Test MSE 3592.5626817476214 Test RE 0.026791004714597708\n",
      "66 Train Loss 1126.3004 Test MSE 2949.2170153827133 Test RE 0.024273935247607587\n",
      "67 Train Loss 1124.9213 Test MSE 4683.018288085836 Test RE 0.030587906258129544\n",
      "68 Train Loss 1123.7726 Test MSE 4452.773437737114 Test RE 0.02982648824706066\n",
      "69 Train Loss 1120.397 Test MSE 3808.094013047828 Test RE 0.02758294633328656\n",
      "70 Train Loss 1116.1642 Test MSE 2386.05351691273 Test RE 0.021833678637445026\n",
      "71 Train Loss 1113.8391 Test MSE 2326.4246554428355 Test RE 0.02155913438293947\n",
      "72 Train Loss 1108.6494 Test MSE 5427.927559407281 Test RE 0.03293091857890535\n",
      "73 Train Loss 1105.8131 Test MSE 6776.563896591602 Test RE 0.03679523597529946\n",
      "74 Train Loss 1100.472 Test MSE 5193.85334426336 Test RE 0.03221303651830115\n",
      "75 Train Loss 1097.2474 Test MSE 3053.077396242959 Test RE 0.024697655620310587\n",
      "76 Train Loss 1093.2158 Test MSE 2922.4069734094487 Test RE 0.02416335149244257\n",
      "77 Train Loss 1089.4473 Test MSE 4772.25314844202 Test RE 0.03087795715261934\n",
      "78 Train Loss 1086.4686 Test MSE 3023.232928275139 Test RE 0.0245766468064968\n",
      "79 Train Loss 1082.6252 Test MSE 5284.381582990672 Test RE 0.03249255844647816\n",
      "80 Train Loss 1079.1035 Test MSE 8887.866612439311 Test RE 0.04213913220941135\n",
      "81 Train Loss 1075.2712 Test MSE 4415.381662353054 Test RE 0.029700991566964815\n",
      "82 Train Loss 1074.4401 Test MSE 3243.0015145840525 Test RE 0.025454255519318337\n",
      "83 Train Loss 1070.7273 Test MSE 1809.2004058980897 Test RE 0.019012103150840523\n",
      "84 Train Loss 1066.7792 Test MSE 1860.3472560079053 Test RE 0.019278970228170798\n",
      "85 Train Loss 1065.7195 Test MSE 2358.7387134557644 Test RE 0.021708346309928763\n",
      "86 Train Loss 1062.6119 Test MSE 2193.6770373695576 Test RE 0.02093500978761915\n",
      "87 Train Loss 1060.8625 Test MSE 1866.445216890582 Test RE 0.01931054127631096\n",
      "88 Train Loss 1059.1344 Test MSE 1812.3034948692143 Test RE 0.019028400674297202\n",
      "89 Train Loss 1050.8217 Test MSE 8822.165185493413 Test RE 0.04198309157604274\n",
      "90 Train Loss 1046.811 Test MSE 9923.967851531308 Test RE 0.0445276215675004\n",
      "91 Train Loss 1041.1985 Test MSE 9226.298908352092 Test RE 0.042933924072459245\n",
      "92 Train Loss 1038.7281 Test MSE 9428.476520146549 Test RE 0.04340178443403822\n",
      "93 Train Loss 1037.1271 Test MSE 10419.033428406032 Test RE 0.045624754370673315\n",
      "94 Train Loss 1034.3862 Test MSE 12844.65308294443 Test RE 0.05065799691116178\n",
      "95 Train Loss 1025.9325 Test MSE 8771.154935634318 Test RE 0.041861541333301736\n",
      "96 Train Loss 1023.8597 Test MSE 9004.440416398495 Test RE 0.04241458167087377\n",
      "97 Train Loss 1017.38934 Test MSE 7804.023835586559 Test RE 0.039486270583884014\n",
      "98 Train Loss 1012.69855 Test MSE 6810.822194569947 Test RE 0.03688812619613194\n",
      "99 Train Loss 1010.5053 Test MSE 5791.152115021223 Test RE 0.03401490872805551\n",
      "100 Train Loss 1001.54517 Test MSE 4310.120659441283 Test RE 0.029344825934242442\n",
      "101 Train Loss 995.1846 Test MSE 2404.7516434679774 Test RE 0.021919060670821847\n",
      "102 Train Loss 990.2584 Test MSE 1699.5543345726817 Test RE 0.018426987888713227\n",
      "103 Train Loss 980.2967 Test MSE 5641.625820372824 Test RE 0.03357290818829478\n",
      "104 Train Loss 977.1319 Test MSE 6954.705393411924 Test RE 0.03727573301717038\n",
      "105 Train Loss 973.5249 Test MSE 5895.2854060304135 Test RE 0.03431936481373915\n",
      "106 Train Loss 970.89374 Test MSE 7032.99829367424 Test RE 0.03748496241060299\n",
      "107 Train Loss 966.9949 Test MSE 8753.556939214282 Test RE 0.04181952582109347\n",
      "108 Train Loss 965.1617 Test MSE 6279.7717556647 Test RE 0.035420831602311804\n",
      "109 Train Loss 963.41315 Test MSE 3630.46216780233 Test RE 0.026931948874902457\n",
      "110 Train Loss 961.5868 Test MSE 2577.398541493291 Test RE 0.02269225278677739\n",
      "111 Train Loss 960.16 Test MSE 2459.6821549061137 Test RE 0.022167990099581147\n",
      "112 Train Loss 956.136 Test MSE 2101.9426590962807 Test RE 0.020492609096322424\n",
      "113 Train Loss 952.21576 Test MSE 1372.4491334627398 Test RE 0.016559032263544958\n",
      "114 Train Loss 948.6442 Test MSE 2192.70273581014 Test RE 0.020930360224893037\n",
      "115 Train Loss 944.8162 Test MSE 2155.110389703491 Test RE 0.020750166409867033\n",
      "116 Train Loss 941.59674 Test MSE 1992.8796751119912 Test RE 0.01995388029934108\n",
      "117 Train Loss 939.8717 Test MSE 2206.846375848467 Test RE 0.020997755507176176\n",
      "118 Train Loss 937.32556 Test MSE 2935.9326339665404 Test RE 0.024219204083752263\n",
      "119 Train Loss 934.651 Test MSE 3621.5365347769452 Test RE 0.02689882187730692\n",
      "120 Train Loss 932.3167 Test MSE 3597.7012253090957 Test RE 0.026810157832440222\n",
      "121 Train Loss 929.4534 Test MSE 2362.661945776568 Test RE 0.021726392289504834\n",
      "122 Train Loss 925.704 Test MSE 4512.76056613075 Test RE 0.03002672520915177\n",
      "123 Train Loss 923.37164 Test MSE 3866.3574680897646 Test RE 0.027793153499357574\n",
      "124 Train Loss 921.79114 Test MSE 3296.120332057078 Test RE 0.02566187308347556\n",
      "125 Train Loss 920.79834 Test MSE 3344.0626921094527 Test RE 0.025847826447713534\n",
      "126 Train Loss 918.93 Test MSE 2900.0132654482136 Test RE 0.024070594455882607\n",
      "127 Train Loss 916.46277 Test MSE 2509.6096839863285 Test RE 0.02239184681536953\n",
      "128 Train Loss 914.9788 Test MSE 1991.5445614908508 Test RE 0.01994719520913213\n",
      "129 Train Loss 912.5718 Test MSE 2036.3663689626 Test RE 0.020170412578150186\n",
      "130 Train Loss 910.0258 Test MSE 3737.1206702288837 Test RE 0.027324698854961416\n",
      "131 Train Loss 907.4924 Test MSE 4139.671030787866 Test RE 0.02875873233663662\n",
      "132 Train Loss 905.8144 Test MSE 3205.4164441204875 Test RE 0.025306323410099228\n",
      "133 Train Loss 901.92377 Test MSE 1727.5720940824858 Test RE 0.018578254749566894\n",
      "134 Train Loss 898.49963 Test MSE 1376.7011579656712 Test RE 0.01658466343601177\n",
      "135 Train Loss 896.1696 Test MSE 2059.5442589924423 Test RE 0.02028487744996563\n",
      "136 Train Loss 894.1561 Test MSE 2136.1406599598868 Test RE 0.020658640918527725\n",
      "137 Train Loss 891.61896 Test MSE 1621.6508101307782 Test RE 0.01799970947185411\n",
      "138 Train Loss 890.8192 Test MSE 1616.985842810594 Test RE 0.017973801141215197\n",
      "139 Train Loss 889.30035 Test MSE 1746.6918803484186 Test RE 0.01868077864442575\n",
      "140 Train Loss 887.39435 Test MSE 1922.5530358045532 Test RE 0.019598642375295655\n",
      "141 Train Loss 885.6214 Test MSE 1690.9388313921388 Test RE 0.018380222840460984\n",
      "142 Train Loss 882.2287 Test MSE 1988.279139497591 Test RE 0.019930835361338304\n",
      "143 Train Loss 877.4921 Test MSE 2208.4637839677407 Test RE 0.021005448774858552\n",
      "144 Train Loss 873.13605 Test MSE 2220.9241953528935 Test RE 0.0210646230266069\n",
      "145 Train Loss 870.47205 Test MSE 2396.379505214594 Test RE 0.02188087181930917\n",
      "146 Train Loss 869.1794 Test MSE 2608.570692683227 Test RE 0.02282906522828528\n",
      "147 Train Loss 866.9385 Test MSE 1902.473864421864 Test RE 0.01949602949895125\n",
      "148 Train Loss 862.6054 Test MSE 1367.6848282896785 Test RE 0.016530265853664936\n",
      "149 Train Loss 860.03613 Test MSE 1382.3363016510211 Test RE 0.016618571129448714\n",
      "150 Train Loss 857.44305 Test MSE 1409.2166090562237 Test RE 0.016779371912252667\n",
      "151 Train Loss 849.85785 Test MSE 2292.655479525062 Test RE 0.02140209179570856\n",
      "152 Train Loss 846.94196 Test MSE 2411.807535768901 Test RE 0.021951193978705416\n",
      "153 Train Loss 844.56177 Test MSE 2130.8151165378604 Test RE 0.020632873152431654\n",
      "154 Train Loss 843.7923 Test MSE 2187.6008535789488 Test RE 0.020905996133323158\n",
      "155 Train Loss 842.427 Test MSE 2310.7423671447186 Test RE 0.021486347099517387\n",
      "156 Train Loss 838.6587 Test MSE 2803.7147388293756 Test RE 0.023667573581540823\n",
      "157 Train Loss 836.8759 Test MSE 2237.079037043494 Test RE 0.02114109548567431\n",
      "158 Train Loss 834.72095 Test MSE 1645.5839875374052 Test RE 0.018132047581254838\n",
      "159 Train Loss 831.94995 Test MSE 1859.5604753544862 Test RE 0.019274893052569454\n",
      "160 Train Loss 830.3079 Test MSE 1917.0999856742571 Test RE 0.019570828248160504\n",
      "161 Train Loss 829.2167 Test MSE 1880.3428176463253 Test RE 0.019382301344331523\n",
      "162 Train Loss 828.04834 Test MSE 1828.5976113380157 Test RE 0.019113749841527493\n",
      "163 Train Loss 826.6906 Test MSE 2135.330286669177 Test RE 0.020654721982104697\n",
      "164 Train Loss 824.5148 Test MSE 3214.080424377912 Test RE 0.02534050079922032\n",
      "165 Train Loss 822.4281 Test MSE 2010.0143859510622 Test RE 0.020039478089363016\n",
      "166 Train Loss 819.4112 Test MSE 1371.794936975819 Test RE 0.016555085249522834\n",
      "167 Train Loss 818.10583 Test MSE 1350.2637830475583 Test RE 0.016424650357372728\n",
      "168 Train Loss 817.2238 Test MSE 1193.500132110747 Test RE 0.015441804840184202\n",
      "169 Train Loss 815.80164 Test MSE 1337.6377966520215 Test RE 0.016347678554411122\n",
      "170 Train Loss 814.05975 Test MSE 1636.2116726849438 Test RE 0.018080338910923234\n",
      "171 Train Loss 811.5424 Test MSE 1400.6146739481446 Test RE 0.016728082422934477\n",
      "172 Train Loss 810.92865 Test MSE 1097.3925901495177 Test RE 0.014807025876806388\n",
      "173 Train Loss 809.6143 Test MSE 1035.7044477168977 Test RE 0.014384830348864822\n",
      "174 Train Loss 807.1618 Test MSE 1121.6034585204175 Test RE 0.014969472388804498\n",
      "175 Train Loss 806.1513 Test MSE 1137.2065542465202 Test RE 0.01507323606195925\n",
      "176 Train Loss 805.4398 Test MSE 1140.455495068847 Test RE 0.015094752436446692\n",
      "177 Train Loss 804.7571 Test MSE 1068.0290725169264 Test RE 0.014607582944268635\n",
      "178 Train Loss 804.356 Test MSE 1187.0832678677448 Test RE 0.015400237391325233\n",
      "179 Train Loss 803.84735 Test MSE 1469.3564708425008 Test RE 0.017133670418245945\n",
      "180 Train Loss 803.1738 Test MSE 1633.4954778870247 Test RE 0.018065325535389403\n",
      "181 Train Loss 802.52 Test MSE 1401.2143614410647 Test RE 0.016731663189450994\n",
      "182 Train Loss 801.27826 Test MSE 1318.3502243240596 Test RE 0.01622939095981357\n",
      "183 Train Loss 799.77124 Test MSE 1255.593027600329 Test RE 0.01583839870276782\n",
      "184 Train Loss 797.46576 Test MSE 1600.0844556725008 Test RE 0.017879619685604357\n",
      "185 Train Loss 796.1188 Test MSE 1524.7842818316344 Test RE 0.01745384146311488\n",
      "186 Train Loss 795.5599 Test MSE 1455.4663227389292 Test RE 0.01705249395096641\n",
      "187 Train Loss 794.72986 Test MSE 1565.0125200858054 Test RE 0.01768258407862286\n",
      "188 Train Loss 792.94006 Test MSE 1834.6722217250312 Test RE 0.019145471504652144\n",
      "189 Train Loss 791.54895 Test MSE 1404.5655636231415 Test RE 0.016751659309324377\n",
      "190 Train Loss 790.4597 Test MSE 1259.0114119556963 Test RE 0.015859944272336085\n",
      "191 Train Loss 789.419 Test MSE 1292.7449175062557 Test RE 0.016071012657896885\n",
      "192 Train Loss 788.0017 Test MSE 1739.8563976172081 Test RE 0.01864419024737427\n",
      "193 Train Loss 787.1883 Test MSE 1916.668926595178 Test RE 0.01956862787845213\n",
      "194 Train Loss 786.07153 Test MSE 1579.3363359878792 Test RE 0.0177633199098731\n",
      "195 Train Loss 785.4533 Test MSE 1228.560305943981 Test RE 0.015666971859502245\n",
      "196 Train Loss 784.6945 Test MSE 1175.3527311979562 Test RE 0.015323957332401746\n",
      "197 Train Loss 783.95966 Test MSE 1255.179464638014 Test RE 0.015835790088989592\n",
      "198 Train Loss 783.1348 Test MSE 1251.5273707336537 Test RE 0.01581273524943375\n",
      "199 Train Loss 782.38293 Test MSE 1213.4146042743073 Test RE 0.015570101098054831\n",
      "Training time: 36.98\n",
      "Training time: 36.98\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 4566.1206 Test MSE 4982162.311023857 Test RE 0.997690322643086\n",
      "1 Train Loss 3583.9915 Test MSE 2272054.7354867063 Test RE 0.6737460289487389\n",
      "2 Train Loss 2807.164 Test MSE 1428126.9896986934 Test RE 0.5341586148813876\n",
      "3 Train Loss 1971.0822 Test MSE 1352983.7308809732 Test RE 0.5199159139851235\n",
      "4 Train Loss 1700.5248 Test MSE 482922.85489439213 Test RE 0.31061734506870853\n",
      "5 Train Loss 1641.5815 Test MSE 437429.0627899372 Test RE 0.2956246478509056\n",
      "6 Train Loss 1567.3561 Test MSE 474450.69835836685 Test RE 0.3078806315624527\n",
      "7 Train Loss 1440.6128 Test MSE 247461.96519845075 Test RE 0.22235200626757573\n",
      "8 Train Loss 1371.0576 Test MSE 202610.9206162506 Test RE 0.20119549336076845\n",
      "9 Train Loss 1332.43 Test MSE 76255.05475633299 Test RE 0.12343011746191619\n",
      "10 Train Loss 1317.0967 Test MSE 47653.47120474577 Test RE 0.09757399668496383\n",
      "11 Train Loss 1305.7976 Test MSE 39398.967785278364 Test RE 0.08872157555536152\n",
      "12 Train Loss 1277.2468 Test MSE 15818.26680997026 Test RE 0.05621681955925586\n",
      "13 Train Loss 1269.885 Test MSE 4717.41120727285 Test RE 0.03070002229649055\n",
      "14 Train Loss 1259.76 Test MSE 3662.4949635391235 Test RE 0.027050502786330054\n",
      "15 Train Loss 1254.9965 Test MSE 3234.6921555204044 Test RE 0.0254216246053642\n",
      "16 Train Loss 1253.2755 Test MSE 4701.420647213903 Test RE 0.0306479463544189\n",
      "17 Train Loss 1248.9492 Test MSE 5551.199941030743 Test RE 0.03330276237683837\n",
      "18 Train Loss 1245.345 Test MSE 2984.0543792980156 Test RE 0.02441688120431944\n",
      "19 Train Loss 1243.6656 Test MSE 2981.163758685786 Test RE 0.024405052157005214\n",
      "20 Train Loss 1240.947 Test MSE 2437.2055663591786 Test RE 0.022066472043780518\n",
      "21 Train Loss 1238.8813 Test MSE 3629.9185259554447 Test RE 0.02692993234341063\n",
      "22 Train Loss 1236.3346 Test MSE 5927.968260130789 Test RE 0.03441436483711545\n",
      "23 Train Loss 1234.3945 Test MSE 5891.945283801468 Test RE 0.0343096411865732\n",
      "24 Train Loss 1231.4697 Test MSE 5515.55612438846 Test RE 0.033195672992635324\n",
      "25 Train Loss 1226.2045 Test MSE 13099.124496220822 Test RE 0.05115734046109995\n",
      "26 Train Loss 1221.6133 Test MSE 11078.582287593477 Test RE 0.047046673078429246\n",
      "27 Train Loss 1217.108 Test MSE 15350.166384141898 Test RE 0.055378777873157446\n",
      "28 Train Loss 1214.5798 Test MSE 19821.623286091755 Test RE 0.0629298117614837\n",
      "29 Train Loss 1209.5731 Test MSE 29100.489193907655 Test RE 0.07624949444429345\n",
      "30 Train Loss 1206.6307 Test MSE 27389.975799710985 Test RE 0.07397460382009013\n",
      "31 Train Loss 1204.2548 Test MSE 29988.732469492203 Test RE 0.07740444099888952\n",
      "32 Train Loss 1200.2562 Test MSE 32899.6188197767 Test RE 0.08107412248634886\n",
      "33 Train Loss 1198.0094 Test MSE 30125.632356314443 Test RE 0.07758091716696607\n",
      "34 Train Loss 1190.0859 Test MSE 17850.82275559029 Test RE 0.05971946969477934\n",
      "35 Train Loss 1186.961 Test MSE 26546.235787060676 Test RE 0.07282630865190193\n",
      "36 Train Loss 1183.4946 Test MSE 19861.595895037808 Test RE 0.06299323244639879\n",
      "37 Train Loss 1180.5366 Test MSE 21315.76009599617 Test RE 0.06525852230278342\n",
      "38 Train Loss 1175.5186 Test MSE 21059.53847509731 Test RE 0.06486512335022995\n",
      "39 Train Loss 1168.9926 Test MSE 19671.095936624057 Test RE 0.06269040880454521\n",
      "40 Train Loss 1167.0977 Test MSE 22838.742396359776 Test RE 0.06754962099943625\n",
      "41 Train Loss 1161.6152 Test MSE 24468.41515704773 Test RE 0.06991812027917356\n",
      "42 Train Loss 1157.3878 Test MSE 18717.77394010763 Test RE 0.06115245864286185\n",
      "43 Train Loss 1153.5397 Test MSE 19035.979019585102 Test RE 0.0616700687231605\n",
      "44 Train Loss 1146.669 Test MSE 17594.230890939187 Test RE 0.05928870536401542\n",
      "45 Train Loss 1144.0588 Test MSE 15358.176019594877 Test RE 0.055393224165278136\n",
      "46 Train Loss 1140.5665 Test MSE 17415.448471682754 Test RE 0.058986707421142655\n",
      "47 Train Loss 1137.4447 Test MSE 20212.653093185687 Test RE 0.063547502197826\n",
      "48 Train Loss 1134.392 Test MSE 17003.93506806035 Test RE 0.05828563638091676\n",
      "49 Train Loss 1131.9291 Test MSE 15143.6131862782 Test RE 0.0550049250957144\n",
      "50 Train Loss 1127.0099 Test MSE 18724.410314094515 Test RE 0.06116329846339038\n",
      "51 Train Loss 1125.1025 Test MSE 19588.083330736994 Test RE 0.06255799126693154\n",
      "52 Train Loss 1120.8691 Test MSE 17610.158451753712 Test RE 0.05931553548933494\n",
      "53 Train Loss 1117.0718 Test MSE 16591.12598653671 Test RE 0.05757378136652891\n",
      "54 Train Loss 1110.8597 Test MSE 19551.531092852823 Test RE 0.062499596010062275\n",
      "55 Train Loss 1107.832 Test MSE 18021.784887370803 Test RE 0.06000476295347422\n",
      "56 Train Loss 1103.6337 Test MSE 13698.407577302643 Test RE 0.052314474310368106\n",
      "57 Train Loss 1095.9595 Test MSE 16869.981511741556 Test RE 0.058055601223720714\n",
      "58 Train Loss 1092.3168 Test MSE 17396.13220501359 Test RE 0.0589539859242316\n",
      "59 Train Loss 1088.222 Test MSE 12810.456647323861 Test RE 0.05059051834825495\n",
      "60 Train Loss 1084.1157 Test MSE 8999.17405548304 Test RE 0.042402176504386706\n",
      "61 Train Loss 1081.3868 Test MSE 9311.77711645571 Test RE 0.043132348922138665\n",
      "62 Train Loss 1077.9207 Test MSE 8810.483248537555 Test RE 0.04195528625373814\n",
      "63 Train Loss 1071.2267 Test MSE 4258.182195553187 Test RE 0.02916748233547063\n",
      "64 Train Loss 1067.0192 Test MSE 3711.488107949899 Test RE 0.02723082883554579\n",
      "65 Train Loss 1057.3258 Test MSE 3475.3023157554694 Test RE 0.026350151621338136\n",
      "66 Train Loss 1049.4459 Test MSE 2469.351937507234 Test RE 0.02221152202074523\n",
      "67 Train Loss 1044.7655 Test MSE 3769.060242117113 Test RE 0.02744121668437435\n",
      "68 Train Loss 1037.5137 Test MSE 2870.910789020377 Test RE 0.02394951220854363\n",
      "69 Train Loss 1032.0878 Test MSE 2350.711924604136 Test RE 0.021671378076505047\n",
      "70 Train Loss 1027.8572 Test MSE 3460.2176227705045 Test RE 0.026292902470952974\n",
      "71 Train Loss 1017.903 Test MSE 8283.556564691691 Test RE 0.040681339537988366\n",
      "72 Train Loss 1013.42676 Test MSE 6197.673992415197 Test RE 0.03518853510790782\n",
      "73 Train Loss 1011.5949 Test MSE 6806.22546396912 Test RE 0.03687567590791501\n",
      "74 Train Loss 1006.5136 Test MSE 9223.685177348658 Test RE 0.0429278422362719\n",
      "75 Train Loss 994.86884 Test MSE 8435.16571174011 Test RE 0.04105193504755848\n",
      "76 Train Loss 988.2901 Test MSE 10882.106896127498 Test RE 0.04662762741431085\n",
      "77 Train Loss 973.8082 Test MSE 11906.259450227903 Test RE 0.048772441297898414\n",
      "78 Train Loss 966.4347 Test MSE 13412.80908264602 Test RE 0.0517662487747129\n",
      "79 Train Loss 958.6394 Test MSE 16460.162314557747 Test RE 0.05734609903747072\n",
      "80 Train Loss 949.64435 Test MSE 17411.95187150953 Test RE 0.05898078557292238\n",
      "81 Train Loss 941.6748 Test MSE 22300.519783209664 Test RE 0.06674893127190289\n",
      "82 Train Loss 935.8009 Test MSE 26937.807293694073 Test RE 0.07336145640072309\n",
      "83 Train Loss 929.37756 Test MSE 22006.18650547878 Test RE 0.06630697545406693\n",
      "84 Train Loss 922.7633 Test MSE 15220.684483042494 Test RE 0.05514471738648402\n",
      "85 Train Loss 914.3707 Test MSE 14425.70874255704 Test RE 0.05368530235418254\n",
      "86 Train Loss 907.2853 Test MSE 12797.881536044568 Test RE 0.050565681700523654\n",
      "87 Train Loss 905.0755 Test MSE 9975.249035120252 Test RE 0.04464251950527282\n",
      "88 Train Loss 899.77783 Test MSE 10812.786988989305 Test RE 0.04647887925946657\n",
      "89 Train Loss 891.93274 Test MSE 11302.275259020787 Test RE 0.04751927036753715\n",
      "90 Train Loss 887.516 Test MSE 9292.239071857452 Test RE 0.04308707484276269\n",
      "91 Train Loss 879.6463 Test MSE 6413.980594210047 Test RE 0.03579733075390457\n",
      "92 Train Loss 873.3289 Test MSE 5238.0950116684535 Test RE 0.03234994223230758\n",
      "93 Train Loss 868.3815 Test MSE 4350.341918540185 Test RE 0.029481428266349214\n",
      "94 Train Loss 864.69006 Test MSE 5106.519013433443 Test RE 0.03194105821311376\n",
      "95 Train Loss 859.18494 Test MSE 4114.268888800635 Test RE 0.028670360877155097\n",
      "96 Train Loss 855.118 Test MSE 3650.211733631473 Test RE 0.027005103870429252\n",
      "97 Train Loss 849.90063 Test MSE 4168.690480420373 Test RE 0.028859356890323443\n",
      "98 Train Loss 846.0005 Test MSE 2713.5516216473798 Test RE 0.023283907622778162\n",
      "99 Train Loss 841.9634 Test MSE 3253.1022956063525 Test RE 0.02549386511732764\n",
      "100 Train Loss 836.64355 Test MSE 4788.021592325901 Test RE 0.03092892844248298\n",
      "101 Train Loss 832.09515 Test MSE 4610.03338912065 Test RE 0.030348613800883422\n",
      "102 Train Loss 826.9062 Test MSE 3552.944910117203 Test RE 0.026642873359275142\n",
      "103 Train Loss 821.50885 Test MSE 3079.835754206816 Test RE 0.024805649442240108\n",
      "104 Train Loss 815.4457 Test MSE 2685.9573039282836 Test RE 0.023165217173879767\n",
      "105 Train Loss 807.046 Test MSE 3150.3100021657588 Test RE 0.025087851462863988\n",
      "106 Train Loss 803.478 Test MSE 4142.77495483396 Test RE 0.028769511960638\n",
      "107 Train Loss 801.2352 Test MSE 4490.9570024923405 Test RE 0.02995409978292411\n",
      "108 Train Loss 796.2836 Test MSE 5193.270031599114 Test RE 0.03221122757236884\n",
      "109 Train Loss 791.23956 Test MSE 5618.584879776931 Test RE 0.03350428055866118\n",
      "110 Train Loss 789.9878 Test MSE 5911.281565235346 Test RE 0.03436589403948797\n",
      "111 Train Loss 786.0379 Test MSE 5671.139319457497 Test RE 0.0336606099892714\n",
      "112 Train Loss 782.8026 Test MSE 5546.791411984217 Test RE 0.03328953592515076\n",
      "113 Train Loss 779.04376 Test MSE 5718.51326771935 Test RE 0.03380090980732559\n",
      "114 Train Loss 774.6106 Test MSE 5717.588448427812 Test RE 0.03379817649221725\n",
      "115 Train Loss 771.3959 Test MSE 5000.297979282526 Test RE 0.03160710844456199\n",
      "116 Train Loss 767.4256 Test MSE 4762.907317807271 Test RE 0.030847707124591856\n",
      "117 Train Loss 763.22327 Test MSE 4217.198632359584 Test RE 0.02902677938692\n",
      "118 Train Loss 758.9808 Test MSE 2889.451275266918 Test RE 0.02402672133115692\n",
      "119 Train Loss 755.97894 Test MSE 2401.1492962795733 Test RE 0.02190263700816445\n",
      "120 Train Loss 753.95056 Test MSE 2533.5106119142324 Test RE 0.022498221465236588\n",
      "121 Train Loss 750.96075 Test MSE 2515.353879637222 Test RE 0.022417458294573346\n",
      "122 Train Loss 749.38495 Test MSE 2292.722495453506 Test RE 0.021402404592503373\n",
      "123 Train Loss 747.0716 Test MSE 1728.9444208072505 Test RE 0.018585632263479278\n",
      "124 Train Loss 745.7421 Test MSE 1289.6317876178778 Test RE 0.016051650250752714\n",
      "125 Train Loss 743.0305 Test MSE 1433.1635990454588 Test RE 0.0169213382982161\n",
      "126 Train Loss 742.01196 Test MSE 1405.5638339007473 Test RE 0.016757611225610532\n",
      "127 Train Loss 740.60754 Test MSE 1301.3670709480089 Test RE 0.016124517589417815\n",
      "128 Train Loss 739.73096 Test MSE 1279.5269384584938 Test RE 0.01598864060581202\n",
      "129 Train Loss 738.2842 Test MSE 1486.4348512829624 Test RE 0.017232955371814393\n",
      "130 Train Loss 735.86304 Test MSE 1987.7900495320157 Test RE 0.01992838385165629\n",
      "131 Train Loss 734.9144 Test MSE 2244.7800518381277 Test RE 0.02117745272046308\n",
      "132 Train Loss 734.18774 Test MSE 2308.2923479165925 Test RE 0.021474953374840722\n",
      "133 Train Loss 731.83997 Test MSE 2564.759097134742 Test RE 0.022636543519928165\n",
      "134 Train Loss 729.2001 Test MSE 2947.060133085235 Test RE 0.024265057366296\n",
      "135 Train Loss 726.3977 Test MSE 3142.5332523276466 Test RE 0.02505686681388575\n",
      "136 Train Loss 722.8132 Test MSE 2605.1571806557567 Test RE 0.022814123559446734\n",
      "137 Train Loss 716.9169 Test MSE 1168.0422340363118 Test RE 0.015276226773396137\n",
      "138 Train Loss 714.89056 Test MSE 1075.024375813493 Test RE 0.014655342739044582\n",
      "139 Train Loss 712.9015 Test MSE 998.1560307482633 Test RE 0.014121669435993878\n",
      "140 Train Loss 711.6271 Test MSE 1112.927506385833 Test RE 0.014911463225305593\n",
      "141 Train Loss 709.5716 Test MSE 1261.2260764379387 Test RE 0.015873887363751314\n",
      "142 Train Loss 704.2289 Test MSE 1108.6304471188073 Test RE 0.014882648495665092\n",
      "143 Train Loss 703.01965 Test MSE 1167.8860202492676 Test RE 0.015275205219140829\n",
      "144 Train Loss 701.39886 Test MSE 1273.3998751200336 Test RE 0.015950313561748555\n",
      "145 Train Loss 695.9378 Test MSE 1562.747464714052 Test RE 0.017669783371298077\n",
      "146 Train Loss 694.05505 Test MSE 1472.2444403222435 Test RE 0.017150499972045756\n",
      "147 Train Loss 691.31915 Test MSE 1437.09629872778 Test RE 0.016944539052215497\n",
      "148 Train Loss 689.22723 Test MSE 1628.8207166293823 Test RE 0.018039457208077807\n",
      "149 Train Loss 687.6304 Test MSE 1846.4131250664364 Test RE 0.019206634105855117\n",
      "150 Train Loss 683.88446 Test MSE 2116.59731895762 Test RE 0.020563921838916577\n",
      "151 Train Loss 680.9388 Test MSE 1885.6148511843946 Test RE 0.01940945400421001\n",
      "152 Train Loss 676.63873 Test MSE 1751.708830100067 Test RE 0.018707587415888172\n",
      "153 Train Loss 673.08325 Test MSE 1758.5206701675359 Test RE 0.01874392605987708\n",
      "154 Train Loss 670.5666 Test MSE 1608.7003957774602 Test RE 0.01792769305796873\n",
      "155 Train Loss 665.31085 Test MSE 1796.9121089147397 Test RE 0.018947426947393174\n",
      "156 Train Loss 661.5356 Test MSE 2724.003693061752 Test RE 0.023328707059225413\n",
      "157 Train Loss 657.32275 Test MSE 3401.6496273209273 Test RE 0.02606943465476508\n",
      "158 Train Loss 655.6213 Test MSE 2673.6011347433237 Test RE 0.023111872452001036\n",
      "159 Train Loss 654.2524 Test MSE 2470.3362595277677 Test RE 0.02221594850843516\n",
      "160 Train Loss 649.87177 Test MSE 2792.4396594078744 Test RE 0.023619936317367973\n",
      "161 Train Loss 644.74945 Test MSE 3073.598036702085 Test RE 0.024780516761294048\n",
      "162 Train Loss 640.9601 Test MSE 1981.81137483664 Test RE 0.01989839199019455\n",
      "163 Train Loss 638.57263 Test MSE 1601.5979200666786 Test RE 0.017888073543113427\n",
      "164 Train Loss 635.70056 Test MSE 1588.5651320663708 Test RE 0.017815143978263587\n",
      "165 Train Loss 633.0255 Test MSE 1652.439027329354 Test RE 0.018169774835558802\n",
      "166 Train Loss 630.8921 Test MSE 1541.8818769036106 Test RE 0.017551424715839124\n",
      "167 Train Loss 626.46484 Test MSE 1720.8660026652783 Test RE 0.0185421611427243\n",
      "168 Train Loss 622.97125 Test MSE 1887.7049941412124 Test RE 0.019420208400198518\n",
      "169 Train Loss 620.34296 Test MSE 1935.9582691011808 Test RE 0.01966685063504064\n",
      "170 Train Loss 618.4471 Test MSE 2083.4966591795082 Test RE 0.02040249254616412\n",
      "171 Train Loss 616.93835 Test MSE 1816.3772531641246 Test RE 0.019049775017168123\n",
      "172 Train Loss 615.06256 Test MSE 1449.1057135520837 Test RE 0.01701519215670732\n",
      "173 Train Loss 612.417 Test MSE 1450.26725300295 Test RE 0.01702201010584105\n",
      "174 Train Loss 610.8433 Test MSE 1677.3881196978034 Test RE 0.018306427712591167\n",
      "175 Train Loss 608.92035 Test MSE 1484.4858496415522 Test RE 0.017221653808455493\n",
      "176 Train Loss 604.27374 Test MSE 1768.7306171453706 Test RE 0.018798260796604075\n",
      "177 Train Loss 602.798 Test MSE 1883.3165133124735 Test RE 0.019397621501704675\n",
      "178 Train Loss 600.1189 Test MSE 1872.2537545601942 Test RE 0.019340565965323835\n",
      "179 Train Loss 596.9819 Test MSE 1586.119827857969 Test RE 0.017801427126791825\n",
      "180 Train Loss 595.8483 Test MSE 2075.2116860552064 Test RE 0.020361887134921892\n",
      "181 Train Loss 594.1153 Test MSE 3402.7354845920877 Test RE 0.0260735951988507\n",
      "182 Train Loss 591.4444 Test MSE 4418.525194956978 Test RE 0.029711562501757702\n",
      "183 Train Loss 589.3514 Test MSE 3044.4280166623594 Test RE 0.0246626465325452\n",
      "184 Train Loss 587.509 Test MSE 2397.8262120964864 Test RE 0.0218874756213388\n",
      "185 Train Loss 586.1351 Test MSE 3569.6450225347703 Test RE 0.026705415472021345\n",
      "186 Train Loss 584.0439 Test MSE 4285.4913510963115 Test RE 0.029260863294253113\n",
      "187 Train Loss 581.67474 Test MSE 3632.8706563454575 Test RE 0.026940880869341724\n",
      "188 Train Loss 578.9917 Test MSE 2917.2352497084707 Test RE 0.02414196133096927\n",
      "189 Train Loss 576.6263 Test MSE 2453.6011693966984 Test RE 0.022140570571662334\n",
      "190 Train Loss 575.1825 Test MSE 1628.288903850262 Test RE 0.01803651201067688\n",
      "191 Train Loss 574.4032 Test MSE 1571.2366675936983 Test RE 0.01771771152951339\n",
      "192 Train Loss 572.60095 Test MSE 1290.4778554601075 Test RE 0.016056914760561317\n",
      "193 Train Loss 571.0558 Test MSE 1316.7183476546247 Test RE 0.01621934333940327\n",
      "194 Train Loss 570.5034 Test MSE 1413.6933662476852 Test RE 0.016806002882419832\n",
      "195 Train Loss 569.59644 Test MSE 1452.3123232124376 Test RE 0.01703400752984533\n",
      "196 Train Loss 568.23895 Test MSE 1290.4892408623493 Test RE 0.016056985592474208\n",
      "197 Train Loss 567.41547 Test MSE 1162.9673022108332 Test RE 0.015243004429418956\n",
      "198 Train Loss 565.8184 Test MSE 1138.5192136861556 Test RE 0.015081932951344895\n",
      "199 Train Loss 564.0411 Test MSE 1615.0171162992497 Test RE 0.017962856000214193\n",
      "Training time: 44.77\n",
      "Training time: 44.77\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 4564.068 Test MSE 4956849.460571886 Test RE 0.9951526147598673\n",
      "1 Train Loss 4159.827 Test MSE 3811544.6035199235 Test RE 0.8726444417615464\n",
      "2 Train Loss 2607.849 Test MSE 1165627.6012234113 Test RE 0.4825771286055167\n",
      "3 Train Loss 1907.9126 Test MSE 348727.00322986825 Test RE 0.2639548294703959\n",
      "4 Train Loss 1772.7122 Test MSE 425928.15533753345 Test RE 0.29171247255455474\n",
      "5 Train Loss 1664.2343 Test MSE 373745.95853762777 Test RE 0.2732593784952499\n",
      "6 Train Loss 1561.5342 Test MSE 284442.85322771757 Test RE 0.23838797078828466\n",
      "7 Train Loss 1479.9144 Test MSE 207563.3434281442 Test RE 0.2036395611746641\n",
      "8 Train Loss 1418.427 Test MSE 180911.38757059985 Test RE 0.19011648491256244\n",
      "9 Train Loss 1342.9686 Test MSE 62647.68109216521 Test RE 0.11187661657212994\n",
      "10 Train Loss 1317.2937 Test MSE 26891.19893439759 Test RE 0.0732979631590564\n",
      "11 Train Loss 1308.2411 Test MSE 35161.953449696055 Test RE 0.08381530331037199\n",
      "12 Train Loss 1287.1526 Test MSE 9111.214595470252 Test RE 0.04266531549937064\n",
      "13 Train Loss 1274.6432 Test MSE 10616.014407657125 Test RE 0.046054022975257104\n",
      "14 Train Loss 1263.6788 Test MSE 10323.249925838876 Test RE 0.045414553061700946\n",
      "15 Train Loss 1260.764 Test MSE 9850.20984822752 Test RE 0.044361841422073355\n",
      "16 Train Loss 1256.7998 Test MSE 6049.071170684557 Test RE 0.03476411442988476\n",
      "17 Train Loss 1254.7943 Test MSE 4358.530804275634 Test RE 0.029509162470830342\n",
      "18 Train Loss 1250.9113 Test MSE 2355.29718205625 Test RE 0.02169250368470959\n",
      "19 Train Loss 1248.2219 Test MSE 1836.7873604547187 Test RE 0.019156504446410903\n",
      "20 Train Loss 1245.075 Test MSE 2365.3266535443627 Test RE 0.021738640798364493\n",
      "21 Train Loss 1243.4762 Test MSE 1456.1612894370207 Test RE 0.01705656463982483\n",
      "22 Train Loss 1242.7245 Test MSE 1429.9119372867744 Test RE 0.01690213123984251\n",
      "23 Train Loss 1241.7642 Test MSE 1524.56688905191 Test RE 0.017452597197152215\n",
      "24 Train Loss 1240.7701 Test MSE 1411.9407578187095 Test RE 0.016795582137162215\n",
      "25 Train Loss 1240.3525 Test MSE 1534.0788941027208 Test RE 0.017506957245033164\n",
      "26 Train Loss 1240.1836 Test MSE 1751.1582541255423 Test RE 0.018704647213309535\n",
      "27 Train Loss 1239.5597 Test MSE 2501.622365979042 Test RE 0.022356185227005596\n",
      "28 Train Loss 1238.8368 Test MSE 2512.5812304321266 Test RE 0.022405099618864884\n",
      "29 Train Loss 1237.6777 Test MSE 3038.1090323198914 Test RE 0.02463703846644201\n",
      "30 Train Loss 1235.301 Test MSE 2050.139711811905 Test RE 0.02023851079278636\n",
      "31 Train Loss 1233.1171 Test MSE 2975.7145903773567 Test RE 0.02438273737092331\n",
      "32 Train Loss 1229.8785 Test MSE 4341.578969029065 Test RE 0.029451720889321267\n",
      "33 Train Loss 1227.5391 Test MSE 4789.555921714628 Test RE 0.030933883658514683\n",
      "34 Train Loss 1224.4633 Test MSE 4707.753475273347 Test RE 0.030668580847036914\n",
      "35 Train Loss 1218.8582 Test MSE 6868.647006455694 Test RE 0.037044387844578544\n",
      "36 Train Loss 1213.174 Test MSE 6508.676160195772 Test RE 0.036060617186827224\n",
      "37 Train Loss 1209.9369 Test MSE 6295.071073022905 Test RE 0.03546395298487596\n",
      "38 Train Loss 1207.0269 Test MSE 7595.297052333245 Test RE 0.038954640932229355\n",
      "39 Train Loss 1203.2943 Test MSE 6974.045663016071 Test RE 0.0373275268876152\n",
      "40 Train Loss 1198.4384 Test MSE 7746.148038100451 Test RE 0.03933958009047807\n",
      "41 Train Loss 1193.4977 Test MSE 6833.520714755263 Test RE 0.03694954384792594\n",
      "42 Train Loss 1191.2441 Test MSE 5540.197237630156 Test RE 0.0332697422941593\n",
      "43 Train Loss 1188.976 Test MSE 7723.760885998164 Test RE 0.03928269127239296\n",
      "44 Train Loss 1185.1422 Test MSE 12369.357779012744 Test RE 0.049711904248893726\n",
      "45 Train Loss 1179.3713 Test MSE 5425.0756856319695 Test RE 0.032922266366694194\n",
      "46 Train Loss 1175.6726 Test MSE 9676.250787251978 Test RE 0.043968371604297854\n",
      "47 Train Loss 1173.401 Test MSE 7251.722259710879 Test RE 0.0380633847428133\n",
      "48 Train Loss 1169.4523 Test MSE 4104.799271785669 Test RE 0.02863734726675415\n",
      "49 Train Loss 1168.6444 Test MSE 4746.585647691134 Test RE 0.030794806853100376\n",
      "50 Train Loss 1166.6486 Test MSE 3711.110546876927 Test RE 0.027229443735804045\n",
      "51 Train Loss 1161.8992 Test MSE 3321.244784384079 Test RE 0.02575949035466179\n",
      "52 Train Loss 1158.8262 Test MSE 3542.9318336292713 Test RE 0.026605303770968862\n",
      "53 Train Loss 1155.4147 Test MSE 4639.869924576268 Test RE 0.030446664836843666\n",
      "54 Train Loss 1151.7175 Test MSE 4170.858764096558 Test RE 0.028866861301835352\n",
      "55 Train Loss 1148.3185 Test MSE 4708.07585742998 Test RE 0.030669630905629523\n",
      "56 Train Loss 1144.8904 Test MSE 4376.427845784885 Test RE 0.029569685808834095\n",
      "57 Train Loss 1140.1389 Test MSE 3536.062337548395 Test RE 0.026579498348129355\n",
      "58 Train Loss 1136.4961 Test MSE 3104.2877329404505 Test RE 0.024903925472953028\n",
      "59 Train Loss 1130.1876 Test MSE 2000.1958216579721 Test RE 0.01999047352074769\n",
      "60 Train Loss 1126.3983 Test MSE 1991.0184897014406 Test RE 0.019944560482811246\n",
      "61 Train Loss 1122.2133 Test MSE 2512.778475241537 Test RE 0.02240597903379131\n",
      "62 Train Loss 1117.6818 Test MSE 3803.0673121019236 Test RE 0.027564735516112764\n",
      "63 Train Loss 1114.9252 Test MSE 3786.5789204937564 Test RE 0.027504916452684516\n",
      "64 Train Loss 1111.8589 Test MSE 5125.180148273462 Test RE 0.031999367291375876\n",
      "65 Train Loss 1109.7522 Test MSE 6077.367305939317 Test RE 0.034845328749496335\n",
      "66 Train Loss 1104.183 Test MSE 8372.055300077775 Test RE 0.04089807507565503\n",
      "67 Train Loss 1100.04 Test MSE 9911.403681880649 Test RE 0.044499425699435304\n",
      "68 Train Loss 1097.6846 Test MSE 8929.974366826278 Test RE 0.04223883485232922\n",
      "69 Train Loss 1094.9645 Test MSE 8876.739544825674 Test RE 0.04211274613502937\n",
      "70 Train Loss 1092.9564 Test MSE 7597.71038623648 Test RE 0.038960829175325506\n",
      "71 Train Loss 1090.3391 Test MSE 7026.347848174974 Test RE 0.03746723521588784\n",
      "72 Train Loss 1083.3513 Test MSE 7676.513704343489 Test RE 0.03916235847524582\n",
      "73 Train Loss 1076.8208 Test MSE 8956.743981083571 Test RE 0.042302097698442086\n",
      "74 Train Loss 1068.8938 Test MSE 8677.387223249676 Test RE 0.04163718045014251\n",
      "75 Train Loss 1059.4935 Test MSE 9458.192929317753 Test RE 0.04347012689291097\n",
      "76 Train Loss 1053.7648 Test MSE 7394.655534804237 Test RE 0.038436673770798915\n",
      "77 Train Loss 1049.5818 Test MSE 7791.803679918766 Test RE 0.039455343114496146\n",
      "78 Train Loss 1043.2936 Test MSE 6818.600750187232 Test RE 0.03690918492099508\n",
      "79 Train Loss 1040.2568 Test MSE 5273.759777901247 Test RE 0.032459886389905136\n",
      "80 Train Loss 1034.2393 Test MSE 3763.8476222946883 Test RE 0.027422234483047574\n",
      "81 Train Loss 1026.7234 Test MSE 2661.0758073041484 Test RE 0.023057671470516142\n",
      "82 Train Loss 1022.6257 Test MSE 1982.6141366370052 Test RE 0.019902421650109827\n",
      "83 Train Loss 1011.27704 Test MSE 3286.4096181213818 Test RE 0.025624043924798924\n",
      "84 Train Loss 997.4649 Test MSE 3530.5148265732623 Test RE 0.026558640693277402\n",
      "85 Train Loss 987.1539 Test MSE 2227.075966826147 Test RE 0.02109377646117208\n",
      "86 Train Loss 980.0874 Test MSE 2734.503954187766 Test RE 0.023373626585203297\n",
      "87 Train Loss 974.8684 Test MSE 2702.1017939889184 Test RE 0.0232347324848529\n",
      "88 Train Loss 966.2908 Test MSE 2420.354643751839 Test RE 0.02199005555707254\n",
      "89 Train Loss 957.19073 Test MSE 2804.7886208864707 Test RE 0.023672105738287073\n",
      "90 Train Loss 945.0435 Test MSE 2177.1393663595563 Test RE 0.02085594817490162\n",
      "91 Train Loss 929.9745 Test MSE 1633.319444130147 Test RE 0.018064352103614723\n",
      "92 Train Loss 918.4268 Test MSE 2148.009915894316 Test RE 0.020715955268828388\n",
      "93 Train Loss 908.24945 Test MSE 2872.0800556486197 Test RE 0.023954388799409494\n",
      "94 Train Loss 905.6823 Test MSE 3731.942186275515 Test RE 0.027305760535874156\n",
      "95 Train Loss 896.75525 Test MSE 3897.3875357580973 Test RE 0.027904459809085663\n",
      "96 Train Loss 889.8269 Test MSE 3882.5065812165076 Test RE 0.0278511366376793\n",
      "97 Train Loss 879.0102 Test MSE 6494.960938866034 Test RE 0.03602260329923006\n",
      "98 Train Loss 872.59 Test MSE 9541.927112352547 Test RE 0.043662125242349276\n",
      "99 Train Loss 865.221 Test MSE 10458.877273303031 Test RE 0.045711908856785576\n",
      "100 Train Loss 854.3345 Test MSE 8089.0646832764805 Test RE 0.04020091867764307\n",
      "101 Train Loss 850.3932 Test MSE 4536.584818558634 Test RE 0.030105881038214637\n",
      "102 Train Loss 846.5292 Test MSE 2546.1140712358138 Test RE 0.022554113009811524\n",
      "103 Train Loss 837.7237 Test MSE 2651.2310264113244 Test RE 0.02301498045444854\n",
      "104 Train Loss 830.55695 Test MSE 2295.588395194076 Test RE 0.02141577690206676\n",
      "105 Train Loss 818.96063 Test MSE 2277.588314895531 Test RE 0.021331649376004676\n",
      "106 Train Loss 813.9433 Test MSE 2077.185235547874 Test RE 0.020371567025097327\n",
      "107 Train Loss 809.9968 Test MSE 2367.4641907149617 Test RE 0.02174846114587684\n",
      "108 Train Loss 804.1862 Test MSE 2953.6143647940885 Test RE 0.024292025000862114\n",
      "109 Train Loss 799.6163 Test MSE 2122.693018833768 Test RE 0.020593512109305028\n",
      "110 Train Loss 794.2938 Test MSE 1487.5724721733347 Test RE 0.01723954860408247\n",
      "111 Train Loss 789.7933 Test MSE 1594.0285891719961 Test RE 0.017845752962851503\n",
      "112 Train Loss 786.1266 Test MSE 1684.3469922452193 Test RE 0.018344361761146143\n",
      "113 Train Loss 779.6681 Test MSE 1465.852186299064 Test RE 0.01711322708178291\n",
      "114 Train Loss 773.29846 Test MSE 1877.0377380738896 Test RE 0.01936525971181637\n",
      "115 Train Loss 765.7939 Test MSE 4782.788939783837 Test RE 0.030912023278791573\n",
      "116 Train Loss 759.393 Test MSE 2783.592893252874 Test RE 0.023582491315203853\n",
      "117 Train Loss 756.1946 Test MSE 2141.109881166607 Test RE 0.020682655658254525\n",
      "118 Train Loss 754.3291 Test MSE 1986.2797680750236 Test RE 0.01992081182773047\n",
      "119 Train Loss 746.36755 Test MSE 1524.5795881345243 Test RE 0.01745266988386531\n",
      "120 Train Loss 738.34973 Test MSE 1797.396591027514 Test RE 0.01894998107085912\n",
      "121 Train Loss 735.22894 Test MSE 1754.2743504754112 Test RE 0.018721281796964803\n",
      "122 Train Loss 731.6124 Test MSE 1561.8889542906672 Test RE 0.017664929171396095\n",
      "123 Train Loss 726.86865 Test MSE 1563.741463373622 Test RE 0.01767540198511095\n",
      "124 Train Loss 722.7385 Test MSE 1421.1481647052215 Test RE 0.016850255983997517\n",
      "125 Train Loss 719.4737 Test MSE 1593.4200559947062 Test RE 0.017842346258144088\n",
      "126 Train Loss 715.26953 Test MSE 1486.7327661536747 Test RE 0.017234682220607356\n",
      "127 Train Loss 711.6177 Test MSE 1499.0915714094547 Test RE 0.017306167582143577\n",
      "128 Train Loss 703.8945 Test MSE 1580.207861967533 Test RE 0.017768220404870208\n",
      "129 Train Loss 699.06665 Test MSE 1223.3012734482854 Test RE 0.015633403512512144\n",
      "130 Train Loss 696.44604 Test MSE 1361.314014790609 Test RE 0.01649172109440665\n",
      "131 Train Loss 691.2483 Test MSE 2033.2562300625395 Test RE 0.020155003573912502\n",
      "132 Train Loss 686.7829 Test MSE 2642.572110855741 Test RE 0.022977366275890882\n",
      "133 Train Loss 683.54065 Test MSE 1669.8916385031905 Test RE 0.01826547491750089\n",
      "134 Train Loss 678.1343 Test MSE 2513.792895828154 Test RE 0.022410501277419642\n",
      "135 Train Loss 674.8506 Test MSE 1646.8672325285854 Test RE 0.018139115991329183\n",
      "136 Train Loss 673.0894 Test MSE 1211.9302862318907 Test RE 0.015560575065528517\n",
      "137 Train Loss 668.41095 Test MSE 1608.0995764023296 Test RE 0.017924344917048023\n",
      "138 Train Loss 666.46515 Test MSE 1600.6997975353313 Test RE 0.017883057323188286\n",
      "139 Train Loss 664.4166 Test MSE 1235.7572444436973 Test RE 0.01571279361729209\n",
      "140 Train Loss 661.2813 Test MSE 974.4296002616951 Test RE 0.013952822123120353\n",
      "141 Train Loss 659.9339 Test MSE 1046.1123342276594 Test RE 0.014456926898025082\n",
      "142 Train Loss 658.2533 Test MSE 1101.3087160697776 Test RE 0.014833422327117641\n",
      "143 Train Loss 656.58636 Test MSE 1052.7489260138948 Test RE 0.014502712149932607\n",
      "144 Train Loss 654.7278 Test MSE 1092.0593862386422 Test RE 0.01477100181948958\n",
      "145 Train Loss 652.4509 Test MSE 1107.8155144317332 Test RE 0.014877177517301547\n",
      "146 Train Loss 651.4756 Test MSE 1063.9938358227346 Test RE 0.014579961581547536\n",
      "147 Train Loss 650.0894 Test MSE 994.1197430733599 Test RE 0.01409308830348443\n",
      "148 Train Loss 648.154 Test MSE 1223.6175017964583 Test RE 0.015635424031097547\n",
      "149 Train Loss 645.39655 Test MSE 1500.6050160093116 Test RE 0.017314901310966645\n",
      "150 Train Loss 643.93256 Test MSE 1379.6239446564016 Test RE 0.016602259024146058\n",
      "151 Train Loss 642.23047 Test MSE 2071.7162763985143 Test RE 0.02034473150376051\n",
      "152 Train Loss 640.88763 Test MSE 2302.91283090148 Test RE 0.02144991489478593\n",
      "153 Train Loss 639.5903 Test MSE 1963.2428241318732 Test RE 0.01980495377173915\n",
      "154 Train Loss 637.17847 Test MSE 1450.2390139255085 Test RE 0.017021844381833785\n",
      "155 Train Loss 636.2301 Test MSE 1435.5305992707365 Test RE 0.016935306099071703\n",
      "156 Train Loss 634.7521 Test MSE 1488.8323314992401 Test RE 0.01724684734406647\n",
      "157 Train Loss 632.7282 Test MSE 1594.9597625208478 Test RE 0.017850964620765873\n",
      "158 Train Loss 630.4653 Test MSE 1223.66137106134 Test RE 0.015635704310028503\n",
      "159 Train Loss 629.1953 Test MSE 1457.3894304373403 Test RE 0.017063755962469625\n",
      "160 Train Loss 627.63574 Test MSE 1800.141602395448 Test RE 0.018964445899666425\n",
      "161 Train Loss 625.1572 Test MSE 1646.153724006386 Test RE 0.01813518616149083\n",
      "162 Train Loss 624.22723 Test MSE 1504.843422386389 Test RE 0.01733933673542437\n",
      "163 Train Loss 622.5802 Test MSE 1619.2513048138733 Test RE 0.017986387742229045\n",
      "164 Train Loss 620.29486 Test MSE 1366.3098683763271 Test RE 0.016521954666889053\n",
      "165 Train Loss 618.4426 Test MSE 1229.5805355635348 Test RE 0.01567347564758344\n",
      "166 Train Loss 615.921 Test MSE 1210.382137278001 Test RE 0.015550633162716173\n",
      "167 Train Loss 611.9766 Test MSE 1241.9084756110187 Test RE 0.01575185187375622\n",
      "168 Train Loss 607.97687 Test MSE 2111.6335978194816 Test RE 0.020539795027659374\n",
      "169 Train Loss 605.98267 Test MSE 2465.995581941948 Test RE 0.022196421881767573\n",
      "170 Train Loss 604.7467 Test MSE 2151.951914373057 Test RE 0.02073495537481551\n",
      "171 Train Loss 601.5385 Test MSE 1728.9809127281762 Test RE 0.018585828401028886\n",
      "172 Train Loss 599.0645 Test MSE 1267.363935695106 Test RE 0.015912466265695237\n",
      "173 Train Loss 597.3396 Test MSE 1232.8821567657014 Test RE 0.01569450444022145\n",
      "174 Train Loss 594.4206 Test MSE 1833.892328673701 Test RE 0.019141401838382876\n",
      "175 Train Loss 592.0429 Test MSE 2169.9455845034113 Test RE 0.020821463179293547\n",
      "176 Train Loss 590.9537 Test MSE 1983.683457365165 Test RE 0.019907788101093896\n",
      "177 Train Loss 588.48926 Test MSE 1787.6222801688325 Test RE 0.018898385472500195\n",
      "178 Train Loss 585.74426 Test MSE 1900.4370640302743 Test RE 0.019485590418743894\n",
      "179 Train Loss 583.40106 Test MSE 1618.0245103533775 Test RE 0.017979572931656137\n",
      "180 Train Loss 580.22064 Test MSE 1656.8163964976955 Test RE 0.018193825105252873\n",
      "181 Train Loss 576.4661 Test MSE 2003.2576585748286 Test RE 0.02000576806426276\n",
      "182 Train Loss 573.57074 Test MSE 2024.0038155310679 Test RE 0.020109093206532695\n",
      "183 Train Loss 571.13464 Test MSE 1975.6194401049777 Test RE 0.019867282588191097\n",
      "184 Train Loss 568.5792 Test MSE 1734.7470349323262 Test RE 0.01861679431705479\n",
      "185 Train Loss 561.2447 Test MSE 2231.2340317646153 Test RE 0.02111345885916092\n",
      "186 Train Loss 555.2739 Test MSE 3179.822099845872 Test RE 0.025205089003340994\n",
      "187 Train Loss 552.48505 Test MSE 2447.800249550226 Test RE 0.02211438219187602\n",
      "188 Train Loss 549.49414 Test MSE 1472.1662884135396 Test RE 0.01715004476159994\n",
      "189 Train Loss 544.67487 Test MSE 1179.291546012144 Test RE 0.015349612501837494\n",
      "190 Train Loss 540.65045 Test MSE 1792.5385534544603 Test RE 0.01892435455918474\n",
      "191 Train Loss 536.5802 Test MSE 3168.3075244966535 Test RE 0.025159412059600723\n",
      "192 Train Loss 533.656 Test MSE 3003.687546371989 Test RE 0.02449707323939326\n",
      "193 Train Loss 531.7387 Test MSE 2528.98419259227 Test RE 0.02247811460042511\n",
      "194 Train Loss 525.9171 Test MSE 1917.3272680142202 Test RE 0.01957198832636293\n",
      "195 Train Loss 522.592 Test MSE 1630.0677586095894 Test RE 0.018046361484304007\n",
      "196 Train Loss 518.56647 Test MSE 1509.7283904652004 Test RE 0.017367457095761922\n",
      "197 Train Loss 511.09985 Test MSE 1521.6992893044376 Test RE 0.01743617593693805\n",
      "198 Train Loss 505.11557 Test MSE 1357.2927205474818 Test RE 0.016467344973099672\n",
      "199 Train Loss 502.70142 Test MSE 1609.399851087787 Test RE 0.017931590072415488\n",
      "Training time: 44.56\n",
      "Training time: 44.56\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 4567.718 Test MSE 4969709.174318287 Test RE 0.9964426567965232\n",
      "1 Train Loss 4269.614 Test MSE 3987060.5805837996 Test RE 0.8925103084865998\n",
      "2 Train Loss 2870.924 Test MSE 662119.0520025515 Test RE 0.36370968514616225\n",
      "3 Train Loss 1922.0248 Test MSE 545704.9789867941 Test RE 0.3301914183530808\n",
      "4 Train Loss 1797.5375 Test MSE 385936.72088863235 Test RE 0.2776801753587763\n",
      "5 Train Loss 1655.6614 Test MSE 324775.4919118522 Test RE 0.2547290337823596\n",
      "6 Train Loss 1556.7853 Test MSE 418406.9332615497 Test RE 0.28912540882783816\n",
      "7 Train Loss 1490.5647 Test MSE 151225.87513540633 Test RE 0.1738200525426241\n",
      "8 Train Loss 1468.9165 Test MSE 173704.50722157312 Test RE 0.18629121165720786\n",
      "9 Train Loss 1408.5388 Test MSE 142702.81678550175 Test RE 0.1688507898371191\n",
      "10 Train Loss 1365.1444 Test MSE 117174.87131218195 Test RE 0.15300445147737743\n",
      "11 Train Loss 1359.6039 Test MSE 117008.10985707027 Test RE 0.15289553593357597\n",
      "12 Train Loss 1339.2646 Test MSE 105139.36694938494 Test RE 0.14493374109079804\n",
      "13 Train Loss 1334.5841 Test MSE 88022.24881710675 Test RE 0.1326120698090135\n",
      "14 Train Loss 1313.9313 Test MSE 71576.15348933869 Test RE 0.11958342873016141\n",
      "15 Train Loss 1298.8197 Test MSE 52064.51631149523 Test RE 0.10199003543921756\n",
      "16 Train Loss 1284.144 Test MSE 8072.840773653699 Test RE 0.04016058376525574\n",
      "17 Train Loss 1277.816 Test MSE 16747.339274600923 Test RE 0.05784418852838522\n",
      "18 Train Loss 1263.084 Test MSE 10378.197049667686 Test RE 0.045535255716277465\n",
      "19 Train Loss 1251.5082 Test MSE 11597.89160057541 Test RE 0.04813670358053868\n",
      "20 Train Loss 1245.0316 Test MSE 9772.123127774 Test RE 0.04418565414311283\n",
      "21 Train Loss 1238.8424 Test MSE 7618.268580378628 Test RE 0.03901350446569523\n",
      "22 Train Loss 1234.6611 Test MSE 6247.247316293369 Test RE 0.035328986047539174\n",
      "23 Train Loss 1232.8883 Test MSE 3343.9528862331063 Test RE 0.02584740207371166\n",
      "24 Train Loss 1231.9103 Test MSE 2885.1023452583777 Test RE 0.024008633143056717\n",
      "25 Train Loss 1231.1167 Test MSE 3700.975090372928 Test RE 0.02719223499035472\n",
      "26 Train Loss 1229.8787 Test MSE 3576.536876885266 Test RE 0.026731182891863572\n",
      "27 Train Loss 1226.0004 Test MSE 1875.7109260734528 Test RE 0.019358414191180787\n",
      "28 Train Loss 1224.5165 Test MSE 2172.411480490566 Test RE 0.020833290430774306\n",
      "29 Train Loss 1221.2816 Test MSE 3803.9268849469327 Test RE 0.027567850443815475\n",
      "30 Train Loss 1217.881 Test MSE 9392.704209805128 Test RE 0.0433193714614406\n",
      "31 Train Loss 1215.1832 Test MSE 7657.685227131392 Test RE 0.039114301486079896\n",
      "32 Train Loss 1212.1671 Test MSE 8292.421396892494 Test RE 0.040703101737145454\n",
      "33 Train Loss 1209.7853 Test MSE 8194.830105575365 Test RE 0.0404628809083417\n",
      "34 Train Loss 1206.6571 Test MSE 6778.783994958199 Test RE 0.036801262802096016\n",
      "35 Train Loss 1203.368 Test MSE 10916.907284952315 Test RE 0.0467021242169664\n",
      "36 Train Loss 1200.114 Test MSE 9764.303942398996 Test RE 0.04416797298231938\n",
      "37 Train Loss 1198.8234 Test MSE 8129.795034724736 Test RE 0.04030200215344809\n",
      "38 Train Loss 1197.0187 Test MSE 10250.360913107792 Test RE 0.04525394057183771\n",
      "39 Train Loss 1193.037 Test MSE 17344.112104036707 Test RE 0.05886577412013938\n",
      "40 Train Loss 1190.559 Test MSE 19049.171031663416 Test RE 0.06169143382843491\n",
      "41 Train Loss 1186.1826 Test MSE 22983.149641809454 Test RE 0.06776283947799736\n",
      "42 Train Loss 1182.9199 Test MSE 31156.869326414133 Test RE 0.07889758865443718\n",
      "43 Train Loss 1179.1604 Test MSE 24694.220654465447 Test RE 0.07023999724771013\n",
      "44 Train Loss 1176.0966 Test MSE 19244.012081101642 Test RE 0.06200613108516054\n",
      "45 Train Loss 1172.9749 Test MSE 21908.432795077588 Test RE 0.06615954038944875\n",
      "46 Train Loss 1169.2305 Test MSE 22339.581987126923 Test RE 0.06680736532653027\n",
      "47 Train Loss 1168.8431 Test MSE 21457.9308246701 Test RE 0.06547578954477802\n",
      "48 Train Loss 1167.6755 Test MSE 17643.372080723417 Test RE 0.05937144515503184\n",
      "49 Train Loss 1165.1033 Test MSE 17460.363289909805 Test RE 0.05906272243863807\n",
      "50 Train Loss 1163.0117 Test MSE 19821.36165276164 Test RE 0.06292939644255878\n",
      "51 Train Loss 1161.5724 Test MSE 19201.118778909837 Test RE 0.06193698928160842\n",
      "52 Train Loss 1153.7794 Test MSE 22607.92094205237 Test RE 0.06720740650690898\n",
      "53 Train Loss 1149.1411 Test MSE 20998.05027073715 Test RE 0.06477035974663173\n",
      "54 Train Loss 1145.1616 Test MSE 17126.97951313053 Test RE 0.058496140491254414\n",
      "55 Train Loss 1141.9246 Test MSE 20556.574571297275 Test RE 0.0640858571912727\n",
      "56 Train Loss 1140.483 Test MSE 23294.220055542082 Test RE 0.06821987362807783\n",
      "57 Train Loss 1135.0743 Test MSE 32004.987185196518 Test RE 0.07996421026709494\n",
      "58 Train Loss 1130.1493 Test MSE 32182.622437821257 Test RE 0.08018581335014893\n",
      "59 Train Loss 1128.8353 Test MSE 28382.228753249525 Test RE 0.07530261736801354\n",
      "60 Train Loss 1128.2985 Test MSE 27025.629816948913 Test RE 0.07348094544256549\n",
      "61 Train Loss 1124.9229 Test MSE 27761.298360768968 Test RE 0.07447434811935891\n",
      "62 Train Loss 1117.5144 Test MSE 29477.073041144835 Test RE 0.07674127359250194\n",
      "63 Train Loss 1115.9261 Test MSE 28476.267757554902 Test RE 0.07542726450585271\n",
      "64 Train Loss 1109.9758 Test MSE 28020.805036486847 Test RE 0.07482162357672817\n",
      "65 Train Loss 1108.5056 Test MSE 30193.940106219805 Test RE 0.07766882199887021\n",
      "66 Train Loss 1106.1636 Test MSE 29263.750622171807 Test RE 0.07646308518680453\n",
      "67 Train Loss 1104.6423 Test MSE 26774.016662928105 Test RE 0.07313808558211932\n",
      "68 Train Loss 1102.8044 Test MSE 25321.066921507765 Test RE 0.07112590808092381\n",
      "69 Train Loss 1101.0554 Test MSE 27900.024081457937 Test RE 0.07466019376605672\n",
      "70 Train Loss 1099.2015 Test MSE 28762.41591204583 Test RE 0.07580528845346933\n",
      "71 Train Loss 1096.2194 Test MSE 27564.68349621665 Test RE 0.0742101533014258\n",
      "72 Train Loss 1092.8208 Test MSE 25270.78585257003 Test RE 0.07105525418909032\n",
      "73 Train Loss 1089.671 Test MSE 24101.917216953592 Test RE 0.06939251354944695\n",
      "74 Train Loss 1085.7654 Test MSE 21271.32170450997 Test RE 0.06519046240780546\n",
      "75 Train Loss 1078.4496 Test MSE 26548.678117635176 Test RE 0.07282965869050662\n",
      "76 Train Loss 1074.4729 Test MSE 23669.632372054733 Test RE 0.06876739610709434\n",
      "77 Train Loss 1071.2678 Test MSE 21220.042781011787 Test RE 0.06511183744271688\n",
      "78 Train Loss 1066.0603 Test MSE 21431.625127353193 Test RE 0.06543564321109814\n",
      "79 Train Loss 1064.2808 Test MSE 23282.1570274669 Test RE 0.06820220733967747\n",
      "80 Train Loss 1060.7537 Test MSE 27587.36928085036 Test RE 0.07424068456644167\n",
      "81 Train Loss 1058.1407 Test MSE 25632.182133877577 Test RE 0.07156152943002103\n",
      "82 Train Loss 1055.6445 Test MSE 26637.835427763825 Test RE 0.07295184658308625\n",
      "83 Train Loss 1052.1404 Test MSE 26594.806923235315 Test RE 0.07289290265131071\n",
      "84 Train Loss 1046.6008 Test MSE 23393.66628677906 Test RE 0.06836533855554502\n",
      "85 Train Loss 1045.8694 Test MSE 22510.091850744146 Test RE 0.06706183877373044\n",
      "86 Train Loss 1043.7273 Test MSE 21618.78919230532 Test RE 0.06572074940544483\n",
      "87 Train Loss 1036.7369 Test MSE 32075.73343429608 Test RE 0.08005254095596849\n",
      "88 Train Loss 1033.0225 Test MSE 30141.691406154365 Test RE 0.07760159241471484\n",
      "89 Train Loss 1029.3342 Test MSE 29196.73861806311 Test RE 0.07637548737031633\n",
      "90 Train Loss 1026.8281 Test MSE 25926.37697079294 Test RE 0.07197103352984836\n",
      "91 Train Loss 1021.79175 Test MSE 19680.60691145642 Test RE 0.06270556237932227\n",
      "92 Train Loss 1020.8927 Test MSE 17777.25176783539 Test RE 0.05959627770551001\n",
      "93 Train Loss 1017.7143 Test MSE 16206.434353518756 Test RE 0.05690239693635268\n",
      "94 Train Loss 1014.28644 Test MSE 15623.249159177994 Test RE 0.05586920649015474\n",
      "95 Train Loss 1008.8865 Test MSE 13745.774861603468 Test RE 0.052404844527303165\n",
      "96 Train Loss 1003.33386 Test MSE 11557.40052324796 Test RE 0.048052601606117205\n",
      "97 Train Loss 1000.75946 Test MSE 10943.926525307594 Test RE 0.046759882155217626\n",
      "98 Train Loss 999.2626 Test MSE 9143.58517819269 Test RE 0.042741039574396344\n",
      "99 Train Loss 996.48846 Test MSE 6876.30965499071 Test RE 0.03706504540641165\n",
      "100 Train Loss 993.50635 Test MSE 7682.567119667481 Test RE 0.0391777964274912\n",
      "101 Train Loss 991.2808 Test MSE 7560.84012226751 Test RE 0.038866179406773366\n",
      "102 Train Loss 988.6596 Test MSE 7250.013787750411 Test RE 0.03805890070068646\n",
      "103 Train Loss 985.31506 Test MSE 6780.662504472987 Test RE 0.03680636155828017\n",
      "104 Train Loss 981.40436 Test MSE 5935.287993191131 Test RE 0.03443560535659146\n",
      "105 Train Loss 976.95685 Test MSE 5674.049942751646 Test RE 0.0336692467712666\n",
      "106 Train Loss 973.7107 Test MSE 6139.3810334885375 Test RE 0.035022659171553464\n",
      "107 Train Loss 967.8074 Test MSE 4993.672127668254 Test RE 0.031586160349681695\n",
      "108 Train Loss 963.9042 Test MSE 3814.7468257016726 Test RE 0.027607029785189228\n",
      "109 Train Loss 958.6432 Test MSE 3566.549285844223 Test RE 0.026693832966467785\n",
      "110 Train Loss 955.6643 Test MSE 3459.7612793232092 Test RE 0.02629116862200335\n",
      "111 Train Loss 953.9072 Test MSE 3602.07199632324 Test RE 0.026826438435637607\n",
      "112 Train Loss 947.5975 Test MSE 5874.19049946793 Test RE 0.0342579078579343\n",
      "113 Train Loss 945.50336 Test MSE 5701.113107667925 Test RE 0.033749446315804274\n",
      "114 Train Loss 943.7586 Test MSE 5428.090626950525 Test RE 0.032931413235807054\n",
      "115 Train Loss 940.66284 Test MSE 4073.1504865799407 Test RE 0.028526733937312077\n",
      "116 Train Loss 935.55 Test MSE 3726.5056552338474 Test RE 0.027285864365174974\n",
      "117 Train Loss 932.8856 Test MSE 3583.46485781857 Test RE 0.026757060372760794\n",
      "118 Train Loss 930.7392 Test MSE 3985.777403620277 Test RE 0.02821911204995797\n",
      "119 Train Loss 926.89014 Test MSE 4568.428455177968 Test RE 0.03021135733111986\n",
      "120 Train Loss 922.6481 Test MSE 7632.392384673199 Test RE 0.03904965204592543\n",
      "121 Train Loss 920.89136 Test MSE 9175.859296477654 Test RE 0.04281640466589702\n",
      "122 Train Loss 919.36597 Test MSE 10182.601234067552 Test RE 0.045104117706789446\n",
      "123 Train Loss 917.02344 Test MSE 9553.775606059737 Test RE 0.04368922510978644\n",
      "124 Train Loss 914.22577 Test MSE 7615.625904768992 Test RE 0.03900673724699143\n",
      "125 Train Loss 913.27124 Test MSE 7082.450134644163 Test RE 0.03761651748855017\n",
      "126 Train Loss 909.6953 Test MSE 6667.53105659273 Test RE 0.036498023567985066\n",
      "127 Train Loss 906.1152 Test MSE 7619.635473123902 Test RE 0.03901700426908484\n",
      "128 Train Loss 904.2121 Test MSE 6623.784818212762 Test RE 0.03637809320741255\n",
      "129 Train Loss 902.41846 Test MSE 5711.94042516169 Test RE 0.03378147888843018\n",
      "130 Train Loss 901.31726 Test MSE 4670.37549045098 Test RE 0.030546589094296545\n",
      "131 Train Loss 898.85864 Test MSE 3796.6918713298282 Test RE 0.027541621140619865\n",
      "132 Train Loss 896.7881 Test MSE 4048.005349950543 Test RE 0.028438544326931683\n",
      "133 Train Loss 895.6968 Test MSE 4479.073600582396 Test RE 0.029914443158483548\n",
      "134 Train Loss 894.1099 Test MSE 4675.166163170927 Test RE 0.030562251775336557\n",
      "135 Train Loss 891.97296 Test MSE 4806.907957719782 Test RE 0.03098986803258649\n",
      "136 Train Loss 890.634 Test MSE 5091.302271573364 Test RE 0.031893432672171665\n",
      "137 Train Loss 888.94525 Test MSE 5351.358205530913 Test RE 0.032697822721811876\n",
      "138 Train Loss 887.8733 Test MSE 5443.224552319491 Test RE 0.03297728891166914\n",
      "139 Train Loss 886.1813 Test MSE 5612.874726538726 Test RE 0.03348725107409671\n",
      "140 Train Loss 884.5832 Test MSE 5907.865066209698 Test RE 0.03435596150523929\n",
      "141 Train Loss 881.17694 Test MSE 6184.937784235618 Test RE 0.03515236032746146\n",
      "142 Train Loss 879.2679 Test MSE 6272.394132614661 Test RE 0.03540001887609834\n",
      "143 Train Loss 877.7312 Test MSE 7277.714158073761 Test RE 0.038131537842876345\n",
      "144 Train Loss 876.4784 Test MSE 7364.150254819857 Test RE 0.03835731015167947\n",
      "145 Train Loss 873.7484 Test MSE 6267.686923511023 Test RE 0.03538673315434659\n",
      "146 Train Loss 871.7654 Test MSE 5553.97774291175 Test RE 0.033311093630181546\n",
      "147 Train Loss 870.47943 Test MSE 5246.2263313078565 Test RE 0.03237504159706845\n",
      "148 Train Loss 869.8562 Test MSE 5222.115811452936 Test RE 0.0323005615857458\n",
      "149 Train Loss 869.57086 Test MSE 5229.225500402883 Test RE 0.03232254202850055\n",
      "150 Train Loss 868.6629 Test MSE 5511.040576256676 Test RE 0.03318208167489765\n",
      "151 Train Loss 867.1719 Test MSE 5903.796410959832 Test RE 0.034344129258193336\n",
      "152 Train Loss 865.66986 Test MSE 6020.2766917933395 Test RE 0.03468127455090814\n",
      "153 Train Loss 864.13184 Test MSE 5865.454900265534 Test RE 0.03423242564934938\n",
      "154 Train Loss 862.93414 Test MSE 5789.409605169209 Test RE 0.034009790940469395\n",
      "155 Train Loss 862.3231 Test MSE 5836.843645244365 Test RE 0.03414883196403916\n",
      "156 Train Loss 861.27966 Test MSE 6186.907594389002 Test RE 0.035157957632262954\n",
      "157 Train Loss 859.2747 Test MSE 7069.532523098701 Test RE 0.037582197633287295\n",
      "158 Train Loss 856.73254 Test MSE 6291.85518786828 Test RE 0.03545489331200651\n",
      "159 Train Loss 855.49603 Test MSE 5879.592783002123 Test RE 0.03427365712588233\n",
      "160 Train Loss 854.70135 Test MSE 6112.7912979535495 Test RE 0.03494673508930827\n",
      "161 Train Loss 853.3906 Test MSE 6661.810993501902 Test RE 0.036482364413868554\n",
      "162 Train Loss 852.2368 Test MSE 6684.655523411503 Test RE 0.036544863124801485\n",
      "163 Train Loss 850.9015 Test MSE 6501.409601270192 Test RE 0.03604048177286237\n",
      "164 Train Loss 849.45636 Test MSE 6535.832155144063 Test RE 0.036135766310978534\n",
      "165 Train Loss 847.773 Test MSE 6277.413723643871 Test RE 0.03541418077959599\n",
      "166 Train Loss 846.95233 Test MSE 5859.973464084154 Test RE 0.034216426317700825\n",
      "167 Train Loss 846.13293 Test MSE 5870.213975097961 Test RE 0.0342463104754512\n",
      "168 Train Loss 845.03735 Test MSE 5724.820661562382 Test RE 0.033819545498571205\n",
      "169 Train Loss 842.802 Test MSE 5381.905564918097 Test RE 0.03279101502362626\n",
      "170 Train Loss 839.93567 Test MSE 5454.4562599692745 Test RE 0.03301129452657897\n",
      "171 Train Loss 839.2049 Test MSE 5722.551852568237 Test RE 0.033812843306721754\n",
      "172 Train Loss 837.5685 Test MSE 5888.550688321232 Test RE 0.03429975615475866\n",
      "173 Train Loss 835.9267 Test MSE 5830.694504963762 Test RE 0.034130839252107505\n",
      "174 Train Loss 833.36896 Test MSE 5799.830019807834 Test RE 0.03404038445696148\n",
      "175 Train Loss 830.7354 Test MSE 5057.517347667693 Test RE 0.03178743712875983\n",
      "176 Train Loss 827.3542 Test MSE 3778.5227301125965 Test RE 0.027475641633307554\n",
      "177 Train Loss 825.877 Test MSE 3374.1322852786243 Test RE 0.02596377735833048\n",
      "178 Train Loss 824.7616 Test MSE 3210.543048305423 Test RE 0.025326552243679037\n",
      "179 Train Loss 823.46814 Test MSE 3125.773239681522 Test RE 0.024989959831299962\n",
      "180 Train Loss 821.07825 Test MSE 3136.609196893503 Test RE 0.02503323805992728\n",
      "181 Train Loss 817.1128 Test MSE 3244.113879661045 Test RE 0.025458620610915368\n",
      "182 Train Loss 815.86066 Test MSE 3719.5131415166798 Test RE 0.027260252386203013\n",
      "183 Train Loss 812.95526 Test MSE 5269.660196074208 Test RE 0.03244726751484814\n",
      "184 Train Loss 809.0088 Test MSE 6855.797109366411 Test RE 0.037009720215488276\n",
      "185 Train Loss 806.57495 Test MSE 4843.059179242866 Test RE 0.031106182208191377\n",
      "186 Train Loss 804.14996 Test MSE 4214.429226267542 Test RE 0.029017246975904798\n",
      "187 Train Loss 802.6693 Test MSE 3977.5904484468283 Test RE 0.02819011552812456\n",
      "188 Train Loss 800.5455 Test MSE 3784.181058323151 Test RE 0.02749620628915801\n",
      "189 Train Loss 799.2077 Test MSE 3613.4928839061918 Test RE 0.026868933319878607\n",
      "190 Train Loss 797.6747 Test MSE 3180.765395461758 Test RE 0.02520882727645672\n",
      "191 Train Loss 795.8174 Test MSE 3042.209071226664 Test RE 0.024653657152282942\n",
      "192 Train Loss 793.77985 Test MSE 2644.4704372526767 Test RE 0.022985617840735374\n",
      "193 Train Loss 791.5341 Test MSE 1840.502099021932 Test RE 0.019175865822787214\n",
      "194 Train Loss 789.926 Test MSE 1462.4412409248037 Test RE 0.017093304786746002\n",
      "195 Train Loss 788.5704 Test MSE 1366.3062326691183 Test RE 0.01652193268467585\n",
      "196 Train Loss 787.4174 Test MSE 1450.0308026384887 Test RE 0.0170206224221052\n",
      "197 Train Loss 786.2342 Test MSE 1433.2744619212976 Test RE 0.016921992763606782\n",
      "198 Train Loss 783.87146 Test MSE 1537.8958769360634 Test RE 0.01752872347904558\n",
      "199 Train Loss 781.8059 Test MSE 1281.3349568192282 Test RE 0.015999932885087057\n",
      "Training time: 44.24\n",
      "Training time: 44.24\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 4460.3066 Test MSE 4563830.199157692 Test RE 0.954886080340978\n",
      "1 Train Loss 3338.6277 Test MSE 2056610.1971740918 Test RE 0.6410070647690554\n",
      "2 Train Loss 2178.9639 Test MSE 1408686.8515350125 Test RE 0.5305105856990961\n",
      "3 Train Loss 2000.8911 Test MSE 1577155.1349997562 Test RE 0.5613374672823771\n",
      "4 Train Loss 1849.6162 Test MSE 972652.6270050575 Test RE 0.4408244818449777\n",
      "5 Train Loss 1632.3942 Test MSE 308470.3955329368 Test RE 0.24825246462635123\n",
      "6 Train Loss 1510.8419 Test MSE 123649.12581607337 Test RE 0.15717459458402233\n",
      "7 Train Loss 1377.0308 Test MSE 49428.12815685746 Test RE 0.09937425954564853\n",
      "8 Train Loss 1362.0686 Test MSE 56612.46528757701 Test RE 0.10635131332753235\n",
      "9 Train Loss 1339.9286 Test MSE 36862.56280282178 Test RE 0.08581823660370111\n",
      "10 Train Loss 1318.923 Test MSE 13259.364308699105 Test RE 0.051469289754645306\n",
      "11 Train Loss 1300.7561 Test MSE 21179.681560658082 Test RE 0.06504988554026207\n",
      "12 Train Loss 1275.4397 Test MSE 46964.51739951027 Test RE 0.09686608684100464\n",
      "13 Train Loss 1259.27 Test MSE 29182.49612520368 Test RE 0.07635685669314436\n",
      "14 Train Loss 1255.9681 Test MSE 23147.81475713222 Test RE 0.06800515311333519\n",
      "15 Train Loss 1252.2997 Test MSE 12173.14587090077 Test RE 0.04931604460941284\n",
      "16 Train Loss 1241.8204 Test MSE 5776.583731228213 Test RE 0.03397209736014679\n",
      "17 Train Loss 1240.4355 Test MSE 7477.027519703087 Test RE 0.038650161521686446\n",
      "18 Train Loss 1237.3236 Test MSE 7595.941535506813 Test RE 0.038956293604790526\n",
      "19 Train Loss 1233.3702 Test MSE 2610.786533950329 Test RE 0.022838759204968966\n",
      "20 Train Loss 1232.4008 Test MSE 1904.5705811186433 Test RE 0.019506769828876853\n",
      "21 Train Loss 1231.9624 Test MSE 1925.9632841069981 Test RE 0.01961601683081616\n",
      "22 Train Loss 1230.9728 Test MSE 1821.6327736504018 Test RE 0.01907731449659334\n",
      "23 Train Loss 1229.9949 Test MSE 2388.1687926798477 Test RE 0.021843354451453623\n",
      "24 Train Loss 1229.5065 Test MSE 2837.2444993041195 Test RE 0.02380867380907557\n",
      "25 Train Loss 1228.9912 Test MSE 3028.724185873671 Test RE 0.024598956611111698\n",
      "26 Train Loss 1228.4855 Test MSE 3185.2047645155417 Test RE 0.025226413022745114\n",
      "27 Train Loss 1227.6748 Test MSE 2330.9841560952977 Test RE 0.0215802506419211\n",
      "28 Train Loss 1226.5245 Test MSE 1903.619426511556 Test RE 0.019501898318404823\n",
      "29 Train Loss 1225.4667 Test MSE 2395.5455345176624 Test RE 0.02187706407646375\n",
      "30 Train Loss 1225.1547 Test MSE 3037.2421173924618 Test RE 0.024633523164649104\n",
      "31 Train Loss 1223.4878 Test MSE 3216.2285051472104 Test RE 0.025348967350047696\n",
      "32 Train Loss 1222.5154 Test MSE 2341.5624535846355 Test RE 0.021629162150646475\n",
      "33 Train Loss 1221.5566 Test MSE 1965.6270896999963 Test RE 0.019816976212564848\n",
      "34 Train Loss 1220.5442 Test MSE 1697.3581372145475 Test RE 0.018415078182869766\n",
      "35 Train Loss 1219.1356 Test MSE 2092.8113476161293 Test RE 0.020448048397642887\n",
      "36 Train Loss 1218.4169 Test MSE 2705.0224361748774 Test RE 0.023247286048294364\n",
      "37 Train Loss 1217.9163 Test MSE 3280.414992097389 Test RE 0.025600663292213627\n",
      "38 Train Loss 1216.8727 Test MSE 4711.875317795702 Test RE 0.030682003746495552\n",
      "39 Train Loss 1216.2023 Test MSE 3965.859688030726 Test RE 0.028148515509753638\n",
      "40 Train Loss 1214.5758 Test MSE 3707.463815752138 Test RE 0.027216061910514296\n",
      "41 Train Loss 1214.0028 Test MSE 5118.731271683237 Test RE 0.03197922898210377\n",
      "42 Train Loss 1213.4739 Test MSE 5906.446296279319 Test RE 0.03435183597669979\n",
      "43 Train Loss 1213.2727 Test MSE 5297.5788328354665 Test RE 0.03253310671208252\n",
      "44 Train Loss 1213.1727 Test MSE 4895.693761031619 Test RE 0.03127475710942744\n",
      "45 Train Loss 1212.8499 Test MSE 4630.028531765209 Test RE 0.030414358258365196\n",
      "46 Train Loss 1212.2255 Test MSE 6234.060937957898 Test RE 0.0352916810200877\n",
      "47 Train Loss 1211.2058 Test MSE 5827.288370347899 Test RE 0.03412086863745306\n",
      "48 Train Loss 1210.2273 Test MSE 5715.564633456211 Test RE 0.033792194310204214\n",
      "49 Train Loss 1209.8694 Test MSE 6100.075304943461 Test RE 0.0349103675967656\n",
      "50 Train Loss 1209.1127 Test MSE 7288.767761694144 Test RE 0.038160484499330995\n",
      "51 Train Loss 1208.546 Test MSE 6889.009554939788 Test RE 0.03709925745090359\n",
      "52 Train Loss 1208.1488 Test MSE 7011.15367224323 Test RE 0.03742670264676384\n",
      "53 Train Loss 1207.948 Test MSE 7534.020950969178 Test RE 0.038797186762371545\n",
      "54 Train Loss 1207.6388 Test MSE 9012.5379930464 Test RE 0.04243364882711326\n",
      "55 Train Loss 1207.1483 Test MSE 11993.195073185201 Test RE 0.04895017767368391\n",
      "56 Train Loss 1205.8862 Test MSE 14900.294211403254 Test RE 0.05456124171919922\n",
      "57 Train Loss 1205.2035 Test MSE 11691.940470346972 Test RE 0.0483314830207402\n",
      "58 Train Loss 1204.9541 Test MSE 9828.018763146421 Test RE 0.04431184286943348\n",
      "59 Train Loss 1203.81 Test MSE 9017.41013966726 Test RE 0.04244511701872158\n",
      "60 Train Loss 1203.1846 Test MSE 9815.029277976904 Test RE 0.04428255017315929\n",
      "61 Train Loss 1202.6683 Test MSE 11283.427433458104 Test RE 0.047479631949471884\n",
      "62 Train Loss 1201.6823 Test MSE 10199.093882702606 Test RE 0.0451406302528356\n",
      "63 Train Loss 1200.9341 Test MSE 8399.912845893132 Test RE 0.04096606159610981\n",
      "64 Train Loss 1198.5758 Test MSE 7050.213822755932 Test RE 0.03753081263030336\n",
      "65 Train Loss 1197.0457 Test MSE 11590.692427087371 Test RE 0.04812176128384383\n",
      "66 Train Loss 1194.4924 Test MSE 11132.924250061393 Test RE 0.047161917114773425\n",
      "67 Train Loss 1193.1233 Test MSE 12860.471746014344 Test RE 0.05068918090698833\n",
      "68 Train Loss 1192.2122 Test MSE 13151.173309623353 Test RE 0.05125887545682529\n",
      "69 Train Loss 1190.9156 Test MSE 13939.875225617787 Test RE 0.05277354481902701\n",
      "70 Train Loss 1188.4421 Test MSE 14456.950180376027 Test RE 0.05374340344908142\n",
      "71 Train Loss 1186.507 Test MSE 12855.282389507569 Test RE 0.050678953025122944\n",
      "72 Train Loss 1184.3999 Test MSE 10387.506758724434 Test RE 0.045555674723274237\n",
      "73 Train Loss 1182.7164 Test MSE 8586.171352906582 Test RE 0.041417759249871504\n",
      "74 Train Loss 1181.3029 Test MSE 8772.246972554487 Test RE 0.04186414720027133\n",
      "75 Train Loss 1179.6407 Test MSE 10413.698508552972 Test RE 0.045613072117444725\n",
      "76 Train Loss 1177.3579 Test MSE 10236.559739856333 Test RE 0.04522346516475399\n",
      "77 Train Loss 1175.6442 Test MSE 9254.031571450338 Test RE 0.04299840164180189\n",
      "78 Train Loss 1174.3759 Test MSE 8400.584487732809 Test RE 0.04096769934947098\n",
      "79 Train Loss 1171.627 Test MSE 7839.318192253917 Test RE 0.0395754598523568\n",
      "80 Train Loss 1170.6392 Test MSE 7537.486482526538 Test RE 0.03880610878541461\n",
      "81 Train Loss 1168.382 Test MSE 7172.022389307733 Test RE 0.037853639516201\n",
      "82 Train Loss 1165.8868 Test MSE 6210.655548766752 Test RE 0.03522536852311523\n",
      "83 Train Loss 1163.4844 Test MSE 6467.132212803867 Test RE 0.03594534807939023\n",
      "84 Train Loss 1159.7875 Test MSE 7214.207250986802 Test RE 0.03796480127967145\n",
      "85 Train Loss 1154.9082 Test MSE 8901.162316838103 Test RE 0.04217063921183827\n",
      "86 Train Loss 1151.3722 Test MSE 8158.57136129037 Test RE 0.04037326588898761\n",
      "87 Train Loss 1146.4434 Test MSE 8273.486509660375 Test RE 0.040656604516015354\n",
      "88 Train Loss 1143.976 Test MSE 10378.996972088456 Test RE 0.04553701054758187\n",
      "89 Train Loss 1138.0084 Test MSE 6676.498799269098 Test RE 0.0365225600043002\n",
      "90 Train Loss 1129.977 Test MSE 2661.0479959385493 Test RE 0.02305755098033603\n",
      "91 Train Loss 1124.2181 Test MSE 2852.879451386848 Test RE 0.02387418385845599\n",
      "92 Train Loss 1113.4424 Test MSE 2626.0203967174007 Test RE 0.022905294033627958\n",
      "93 Train Loss 1106.0743 Test MSE 3797.1623793652607 Test RE 0.02754332764631545\n",
      "94 Train Loss 1099.894 Test MSE 2530.666565265738 Test RE 0.022485589988906454\n",
      "95 Train Loss 1096.742 Test MSE 2542.39746268255 Test RE 0.02253764567577421\n",
      "96 Train Loss 1090.6512 Test MSE 4216.059980693613 Test RE 0.029022860479503722\n",
      "97 Train Loss 1086.5988 Test MSE 6040.517250226258 Test RE 0.03473952597140033\n",
      "98 Train Loss 1080.8856 Test MSE 7489.6560065604535 Test RE 0.038682787263370724\n",
      "99 Train Loss 1073.3438 Test MSE 9590.985946537969 Test RE 0.043774223498689306\n",
      "100 Train Loss 1069.6101 Test MSE 9334.825590724324 Test RE 0.04318569644175528\n",
      "101 Train Loss 1063.4973 Test MSE 9700.56143701348 Test RE 0.04402357010892973\n",
      "102 Train Loss 1060.3875 Test MSE 10127.243439088823 Test RE 0.044981346164659826\n",
      "103 Train Loss 1057.4105 Test MSE 4702.340287843245 Test RE 0.030650943716344973\n",
      "104 Train Loss 1054.3445 Test MSE 3296.5941685592256 Test RE 0.025663717539075783\n",
      "105 Train Loss 1049.802 Test MSE 1999.8578494090598 Test RE 0.019988784558436414\n",
      "106 Train Loss 1048.3212 Test MSE 1932.3749360950223 Test RE 0.019648641173471617\n",
      "107 Train Loss 1044.646 Test MSE 2699.8505406307672 Test RE 0.023225051471145963\n",
      "108 Train Loss 1040.317 Test MSE 3068.5248955690304 Test RE 0.024760057518555832\n",
      "109 Train Loss 1034.8193 Test MSE 3726.1879793303847 Test RE 0.0272847013122835\n",
      "110 Train Loss 1029.3866 Test MSE 3797.722313732406 Test RE 0.02754535835842124\n",
      "111 Train Loss 1021.40436 Test MSE 2520.186075758683 Test RE 0.02243898082869826\n",
      "112 Train Loss 1012.5623 Test MSE 3571.9152279121736 Test RE 0.026713906112016537\n",
      "113 Train Loss 1001.84326 Test MSE 5891.544307289687 Test RE 0.03430847369488302\n",
      "114 Train Loss 997.6328 Test MSE 7722.868873587906 Test RE 0.03928042283992822\n",
      "115 Train Loss 986.95416 Test MSE 5988.5651622419255 Test RE 0.03458981294310563\n",
      "116 Train Loss 983.4943 Test MSE 7639.527430579537 Test RE 0.039067900319861466\n",
      "117 Train Loss 976.39276 Test MSE 8780.788848913237 Test RE 0.04188452461103519\n",
      "118 Train Loss 968.4037 Test MSE 7507.538345628857 Test RE 0.03872893933696758\n",
      "119 Train Loss 963.66516 Test MSE 9264.838340056505 Test RE 0.043023500875155433\n",
      "120 Train Loss 950.9925 Test MSE 14092.04630514668 Test RE 0.05306080743613172\n",
      "121 Train Loss 946.8655 Test MSE 12652.840112516138 Test RE 0.050278328795575644\n",
      "122 Train Loss 938.0333 Test MSE 13173.131037955547 Test RE 0.051301649550828\n",
      "123 Train Loss 925.62054 Test MSE 10234.447128308579 Test RE 0.04521879833590634\n",
      "124 Train Loss 908.0005 Test MSE 9364.837922103541 Test RE 0.04325506374369223\n",
      "125 Train Loss 902.4669 Test MSE 10157.867608541463 Test RE 0.04504930525765146\n",
      "126 Train Loss 893.8149 Test MSE 12971.339072556057 Test RE 0.050907202232223774\n",
      "127 Train Loss 885.2547 Test MSE 14836.131737437963 Test RE 0.054443641320236165\n",
      "128 Train Loss 880.36993 Test MSE 11390.0813173372 Test RE 0.04770349908163861\n",
      "129 Train Loss 877.2119 Test MSE 9317.651258820506 Test RE 0.04314595135322775\n",
      "130 Train Loss 866.9636 Test MSE 5354.627160764469 Test RE 0.032707808168133255\n",
      "131 Train Loss 859.24713 Test MSE 5558.842717294215 Test RE 0.03332567976572629\n",
      "132 Train Loss 849.32385 Test MSE 4505.8309085009505 Test RE 0.03000366228988535\n",
      "133 Train Loss 842.07715 Test MSE 4633.7814081800525 Test RE 0.030426681962961326\n",
      "134 Train Loss 833.83215 Test MSE 5388.464049730769 Test RE 0.03281098879430449\n",
      "135 Train Loss 824.86676 Test MSE 4789.573911350568 Test RE 0.030933941752499996\n",
      "136 Train Loss 817.88654 Test MSE 3154.7494191963374 Test RE 0.02510552214051531\n",
      "137 Train Loss 813.8295 Test MSE 2113.607531671034 Test RE 0.020549392982081978\n",
      "138 Train Loss 805.49554 Test MSE 1834.1286612608067 Test RE 0.019142635169046067\n",
      "139 Train Loss 799.4828 Test MSE 3051.105978602547 Test RE 0.024689680510273686\n",
      "140 Train Loss 797.0163 Test MSE 2842.765635858442 Test RE 0.02383182779742745\n",
      "141 Train Loss 789.18506 Test MSE 2792.9540985476256 Test RE 0.02362211191677585\n",
      "142 Train Loss 782.0061 Test MSE 4157.060058211464 Test RE 0.0288190707449548\n",
      "143 Train Loss 780.15094 Test MSE 3565.8091304982886 Test RE 0.026691062976940873\n",
      "144 Train Loss 778.0909 Test MSE 2578.5671035455725 Test RE 0.022697396403506327\n",
      "145 Train Loss 775.27527 Test MSE 2179.5187757406898 Test RE 0.02086734186151754\n",
      "146 Train Loss 771.70776 Test MSE 2838.865062577746 Test RE 0.02381547229867936\n",
      "147 Train Loss 767.8529 Test MSE 2742.426527242817 Test RE 0.023407461849851898\n",
      "148 Train Loss 763.3395 Test MSE 2329.3666217862165 Test RE 0.021572761776430503\n",
      "149 Train Loss 759.3321 Test MSE 2288.584233694775 Test RE 0.02138308067641327\n",
      "150 Train Loss 755.53204 Test MSE 2640.1197256517594 Test RE 0.022966701962817457\n",
      "151 Train Loss 752.5309 Test MSE 2275.6532172367183 Test RE 0.021322585491168517\n",
      "152 Train Loss 746.56177 Test MSE 2078.398988843496 Test RE 0.020377517973820203\n",
      "153 Train Loss 743.52057 Test MSE 2048.4797600347724 Test RE 0.020230315801297218\n",
      "154 Train Loss 740.2569 Test MSE 2106.8094011764733 Test RE 0.020516319205743063\n",
      "155 Train Loss 737.39014 Test MSE 2310.798731080742 Test RE 0.02148660914684156\n",
      "156 Train Loss 732.237 Test MSE 3073.974420026366 Test RE 0.024782033987626833\n",
      "157 Train Loss 727.685 Test MSE 3793.1505281385944 Test RE 0.027528773497188082\n",
      "158 Train Loss 724.278 Test MSE 4372.687424987285 Test RE 0.029557046881130627\n",
      "159 Train Loss 718.1764 Test MSE 2433.926763526893 Test RE 0.022051623899188934\n",
      "160 Train Loss 713.7173 Test MSE 1714.241584828661 Test RE 0.018506438006991775\n",
      "161 Train Loss 708.59607 Test MSE 1795.3373102621551 Test RE 0.018939122444721138\n",
      "162 Train Loss 704.34796 Test MSE 1684.3368252935886 Test RE 0.018344306396508976\n",
      "163 Train Loss 701.1777 Test MSE 1504.1272113418286 Test RE 0.01733521002620541\n",
      "164 Train Loss 694.9302 Test MSE 1630.696497407125 Test RE 0.018049841509490586\n",
      "165 Train Loss 691.52 Test MSE 1399.3511899075677 Test RE 0.016720535581431013\n",
      "166 Train Loss 685.9983 Test MSE 2287.1019145435093 Test RE 0.021376154629368437\n",
      "167 Train Loss 682.08417 Test MSE 1492.1580705501935 Test RE 0.01726609951790434\n",
      "168 Train Loss 677.4194 Test MSE 2316.346744504107 Test RE 0.021512387361165775\n",
      "169 Train Loss 672.31726 Test MSE 2586.0016989070123 Test RE 0.022730093729780316\n",
      "170 Train Loss 668.45825 Test MSE 1817.8465054735414 Test RE 0.019057478061283908\n",
      "171 Train Loss 665.136 Test MSE 1369.9236457368006 Test RE 0.016543789844773945\n",
      "172 Train Loss 660.6243 Test MSE 1522.16455071967 Test RE 0.017438841299257643\n",
      "173 Train Loss 656.49457 Test MSE 1599.153141527814 Test RE 0.017874415595804685\n",
      "174 Train Loss 653.92566 Test MSE 1658.0839248587165 Test RE 0.018200783263192678\n",
      "175 Train Loss 649.50104 Test MSE 2440.6383922400964 Test RE 0.02208200698723837\n",
      "176 Train Loss 645.18756 Test MSE 2111.665024360136 Test RE 0.02053994786958765\n",
      "177 Train Loss 638.3655 Test MSE 1464.412167570727 Test RE 0.017104819200356905\n",
      "178 Train Loss 632.6909 Test MSE 1320.3318113090936 Test RE 0.016241583423405658\n",
      "179 Train Loss 630.7444 Test MSE 1278.3491710432675 Test RE 0.015981280371856456\n",
      "180 Train Loss 628.30615 Test MSE 1424.161801429033 Test RE 0.01686811255206833\n",
      "181 Train Loss 626.35016 Test MSE 1315.5444336887474 Test RE 0.016212111588068272\n",
      "182 Train Loss 624.8304 Test MSE 1702.2984787536222 Test RE 0.018441858233133386\n",
      "183 Train Loss 622.56085 Test MSE 1621.762578295739 Test RE 0.01800032975330486\n",
      "184 Train Loss 621.09106 Test MSE 1775.522076122608 Test RE 0.01883431639999444\n",
      "185 Train Loss 617.7634 Test MSE 1851.9591300743152 Test RE 0.01923545761770873\n",
      "186 Train Loss 614.2063 Test MSE 1596.405656552102 Test RE 0.017859054090127658\n",
      "187 Train Loss 612.3162 Test MSE 1501.5815225305655 Test RE 0.01732053416040233\n",
      "188 Train Loss 607.0599 Test MSE 2459.340896369356 Test RE 0.022166452242695516\n",
      "189 Train Loss 603.3783 Test MSE 4158.235753714081 Test RE 0.02882314574700896\n",
      "190 Train Loss 598.59 Test MSE 4127.683228896562 Test RE 0.028717061883479375\n",
      "191 Train Loss 594.6125 Test MSE 2720.773229715555 Test RE 0.023314869910235375\n",
      "192 Train Loss 591.2997 Test MSE 3033.2428278469615 Test RE 0.024617299722516455\n",
      "193 Train Loss 588.70306 Test MSE 3997.267860626328 Test RE 0.028259758717423002\n",
      "194 Train Loss 585.26294 Test MSE 2047.0623682808607 Test RE 0.020223315672426967\n",
      "195 Train Loss 583.21533 Test MSE 1788.4367186733944 Test RE 0.01890269002281751\n",
      "196 Train Loss 581.163 Test MSE 1682.447559090612 Test RE 0.01833401540200214\n",
      "197 Train Loss 577.5757 Test MSE 1321.2549886345707 Test RE 0.016247260496927273\n",
      "198 Train Loss 575.4439 Test MSE 1815.9896650764033 Test RE 0.019047742438368695\n",
      "199 Train Loss 573.3919 Test MSE 1869.2246732402741 Test RE 0.01932491427771137\n",
      "Training time: 45.63\n",
      "Training time: 45.63\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 4567.556 Test MSE 4961771.429161817 Test RE 0.9956465670821731\n",
      "1 Train Loss 4220.522 Test MSE 2601139.5924995625 Test RE 0.7208894228881536\n",
      "2 Train Loss 2305.99 Test MSE 1367781.5542050297 Test RE 0.5227513883855671\n",
      "3 Train Loss 1930.1841 Test MSE 1218680.4792343052 Test RE 0.4934370447389272\n",
      "4 Train Loss 1699.2549 Test MSE 731443.0205167914 Test RE 0.38227603582094394\n",
      "5 Train Loss 1491.8184 Test MSE 185932.34202800805 Test RE 0.19273664440514\n",
      "6 Train Loss 1376.9775 Test MSE 64209.52464971378 Test RE 0.1132626063113718\n",
      "7 Train Loss 1338.7157 Test MSE 161746.60649348403 Test RE 0.17976469926549923\n",
      "8 Train Loss 1307.4231 Test MSE 180828.31892298372 Test RE 0.19007283223597893\n",
      "9 Train Loss 1292.0688 Test MSE 101376.78563691437 Test RE 0.142316771177936\n",
      "10 Train Loss 1288.0762 Test MSE 58178.16085066602 Test RE 0.10781192926257675\n",
      "11 Train Loss 1276.4269 Test MSE 53366.53638570378 Test RE 0.10325743488909556\n",
      "12 Train Loss 1261.9241 Test MSE 29664.251365146232 Test RE 0.07698454014526018\n",
      "13 Train Loss 1259.4005 Test MSE 13571.444480605083 Test RE 0.05207147271872485\n",
      "14 Train Loss 1254.5874 Test MSE 4276.450218131945 Test RE 0.0292299810671197\n",
      "15 Train Loss 1253.3912 Test MSE 3166.8477883802498 Test RE 0.025153615536796178\n",
      "16 Train Loss 1252.4971 Test MSE 4141.858761080422 Test RE 0.028766330529722457\n",
      "17 Train Loss 1250.3113 Test MSE 6194.7393574146945 Test RE 0.03518020313255938\n",
      "18 Train Loss 1247.0391 Test MSE 2362.564589660379 Test RE 0.02172594465480066\n",
      "19 Train Loss 1246.2257 Test MSE 1838.1391667971507 Test RE 0.019163552382911456\n",
      "20 Train Loss 1246.0981 Test MSE 1656.5098087601245 Test RE 0.01819214167739699\n",
      "21 Train Loss 1245.9628 Test MSE 1448.5776468906113 Test RE 0.017012091632587937\n",
      "22 Train Loss 1245.8242 Test MSE 1445.7158849315708 Test RE 0.01699527906287107\n",
      "23 Train Loss 1245.7783 Test MSE 1516.8910554720942 Test RE 0.017408606908272412\n",
      "24 Train Loss 1245.7277 Test MSE 1600.7174794495786 Test RE 0.01788315609430493\n",
      "25 Train Loss 1245.6277 Test MSE 1635.901109812891 Test RE 0.01807862295065262\n",
      "26 Train Loss 1245.3551 Test MSE 1444.4847725842449 Test RE 0.0169880412804239\n",
      "27 Train Loss 1245.17 Test MSE 1368.3080814084274 Test RE 0.016534031840567072\n",
      "28 Train Loss 1245.0778 Test MSE 1354.5173945298425 Test RE 0.016450500545732324\n",
      "29 Train Loss 1245.0197 Test MSE 1378.006511433669 Test RE 0.01659252415382787\n",
      "30 Train Loss 1244.9773 Test MSE 1381.8773467291962 Test RE 0.016615812101782413\n",
      "31 Train Loss 1244.9285 Test MSE 1371.9937987921555 Test RE 0.016556285157369836\n",
      "32 Train Loss 1244.8324 Test MSE 1349.273479750362 Test RE 0.016418626212675685\n",
      "33 Train Loss 1244.7498 Test MSE 1349.3778556621653 Test RE 0.016419261249221877\n",
      "34 Train Loss 1244.7051 Test MSE 1352.1730056210051 Test RE 0.01643625817673947\n",
      "35 Train Loss 1244.6764 Test MSE 1361.21465008699 Test RE 0.016491119203562655\n",
      "36 Train Loss 1244.6764 Test MSE 1361.296350044741 Test RE 0.0164916140937426\n",
      "37 Train Loss 1244.6506 Test MSE 1375.0462023993214 Test RE 0.016574692087305734\n",
      "38 Train Loss 1244.6213 Test MSE 1388.713996576774 Test RE 0.016656863623191843\n",
      "39 Train Loss 1244.5311 Test MSE 1416.2115720363383 Test RE 0.016820964452226403\n",
      "40 Train Loss 1244.4127 Test MSE 1400.1205260312906 Test RE 0.016725131262827726\n",
      "41 Train Loss 1244.3428 Test MSE 1371.8247073341497 Test RE 0.016555264885754487\n",
      "42 Train Loss 1244.3115 Test MSE 1356.0491464215163 Test RE 0.01645979941692517\n",
      "43 Train Loss 1244.2592 Test MSE 1350.5825990467913 Test RE 0.016426589286757814\n",
      "44 Train Loss 1244.1433 Test MSE 1345.1578423061437 Test RE 0.016393566533707996\n",
      "45 Train Loss 1244.0382 Test MSE 1343.6254848783183 Test RE 0.016384226380113286\n",
      "46 Train Loss 1243.9927 Test MSE 1357.3572648857337 Test RE 0.01646773651032387\n",
      "47 Train Loss 1243.9532 Test MSE 1370.8282769982907 Test RE 0.016549251302289746\n",
      "48 Train Loss 1243.8842 Test MSE 1363.808408844556 Test RE 0.016506823423344412\n",
      "49 Train Loss 1243.751 Test MSE 1340.0287109155036 Test RE 0.01636228207706771\n",
      "50 Train Loss 1243.6604 Test MSE 1351.9445781174693 Test RE 0.016434869799665814\n",
      "51 Train Loss 1243.5519 Test MSE 1403.3738374840398 Test RE 0.016744551194491118\n",
      "52 Train Loss 1243.4489 Test MSE 1446.829172299828 Test RE 0.01700182149233341\n",
      "53 Train Loss 1243.3568 Test MSE 1443.145881873902 Test RE 0.016980166360542507\n",
      "54 Train Loss 1243.2822 Test MSE 1384.5273081964604 Test RE 0.01663173615356778\n",
      "55 Train Loss 1243.2466 Test MSE 1343.2744225662652 Test RE 0.01638208580589042\n",
      "56 Train Loss 1243.2153 Test MSE 1314.9966233143393 Test RE 0.01620873576607284\n",
      "57 Train Loss 1243.2054 Test MSE 1311.6025851013608 Test RE 0.01618780468115651\n",
      "58 Train Loss 1243.1984 Test MSE 1314.1204374513084 Test RE 0.016203334903683572\n",
      "59 Train Loss 1243.1572 Test MSE 1357.9895918340362 Test RE 0.016471571823766223\n",
      "60 Train Loss 1243.053 Test MSE 1500.0418538034658 Test RE 0.017311651950556475\n",
      "61 Train Loss 1242.8821 Test MSE 1578.0119705029133 Test RE 0.01775587055880165\n",
      "62 Train Loss 1242.8042 Test MSE 1558.5495852680597 Test RE 0.01764603497237792\n",
      "63 Train Loss 1242.7499 Test MSE 1564.2728936975802 Test RE 0.017678405181897602\n",
      "64 Train Loss 1242.6906 Test MSE 1632.5199548859187 Test RE 0.018059930426330233\n",
      "65 Train Loss 1242.647 Test MSE 1698.5563644309536 Test RE 0.01842157697489233\n",
      "66 Train Loss 1242.6146 Test MSE 1742.6968762324066 Test RE 0.01865940323463478\n",
      "67 Train Loss 1242.5039 Test MSE 1753.1823315917236 Test RE 0.01871545398109854\n",
      "68 Train Loss 1242.4174 Test MSE 1689.5269114683435 Test RE 0.01837254757098498\n",
      "69 Train Loss 1242.3934 Test MSE 1620.211449129888 Test RE 0.01799171951767594\n",
      "70 Train Loss 1242.3302 Test MSE 1505.3454848434365 Test RE 0.017342228964574654\n",
      "71 Train Loss 1242.1166 Test MSE 1573.7729008429856 Test RE 0.017732005407391078\n",
      "72 Train Loss 1242.0332 Test MSE 1834.6417284299732 Test RE 0.01914531239968218\n",
      "73 Train Loss 1241.9286 Test MSE 2016.5247239115868 Test RE 0.02007190529613387\n",
      "74 Train Loss 1241.4342 Test MSE 2392.7601660618575 Test RE 0.02186434183709879\n",
      "75 Train Loss 1241.1335 Test MSE 2283.3707570058295 Test RE 0.021358711078741016\n",
      "76 Train Loss 1241.0175 Test MSE 2085.0897792715896 Test RE 0.02041029131312195\n",
      "77 Train Loss 1240.8639 Test MSE 1860.8029826005093 Test RE 0.019281331454382405\n",
      "78 Train Loss 1240.5582 Test MSE 1637.4826706185074 Test RE 0.018087359888960285\n",
      "79 Train Loss 1239.9673 Test MSE 1602.0560500293243 Test RE 0.01789063176212862\n",
      "80 Train Loss 1239.4979 Test MSE 1661.7851557763342 Test RE 0.018221086140765563\n",
      "81 Train Loss 1239.1737 Test MSE 1787.8597590099123 Test RE 0.018899640720235592\n",
      "82 Train Loss 1238.8718 Test MSE 1835.9201373591907 Test RE 0.01915198162443853\n",
      "83 Train Loss 1238.3641 Test MSE 2056.944319746584 Test RE 0.020272069736889043\n",
      "84 Train Loss 1237.6313 Test MSE 3174.235466357817 Test RE 0.02518293784326356\n",
      "85 Train Loss 1236.5846 Test MSE 6327.034316617754 Test RE 0.035553873154512\n",
      "86 Train Loss 1235.8722 Test MSE 5753.4137383452 Test RE 0.033903897522712195\n",
      "87 Train Loss 1234.8314 Test MSE 3108.601994006481 Test RE 0.024921224889301742\n",
      "88 Train Loss 1234.0111 Test MSE 2881.3251145908184 Test RE 0.023992911716918346\n",
      "89 Train Loss 1233.3978 Test MSE 2156.8433899308143 Test RE 0.020758507703426216\n",
      "90 Train Loss 1232.0471 Test MSE 1724.8460949196615 Test RE 0.018563591309117354\n",
      "91 Train Loss 1230.3535 Test MSE 2002.674714957881 Test RE 0.02000285703499751\n",
      "92 Train Loss 1228.2756 Test MSE 1815.805902736447 Test RE 0.01904677868115629\n",
      "93 Train Loss 1225.0879 Test MSE 2572.8876688596442 Test RE 0.022672386497723716\n",
      "94 Train Loss 1224.4952 Test MSE 2733.834060745906 Test RE 0.023370763397126328\n",
      "95 Train Loss 1223.4993 Test MSE 2706.5540369207174 Test RE 0.02325386649686614\n",
      "96 Train Loss 1222.4972 Test MSE 2231.824613638962 Test RE 0.02111625291861941\n",
      "97 Train Loss 1222.3 Test MSE 2079.006115512743 Test RE 0.020380494022049684\n",
      "98 Train Loss 1221.0515 Test MSE 2504.431343384739 Test RE 0.022368733164204745\n",
      "99 Train Loss 1219.7057 Test MSE 3157.2865457392827 Test RE 0.025115615348696885\n",
      "100 Train Loss 1218.7799 Test MSE 3848.9140369181628 Test RE 0.027730386927054033\n",
      "101 Train Loss 1217.5891 Test MSE 3722.9824365233585 Test RE 0.027272962626930523\n",
      "102 Train Loss 1216.4376 Test MSE 2979.349839686346 Test RE 0.024397626277823405\n",
      "103 Train Loss 1213.4762 Test MSE 4898.874503331065 Test RE 0.031284915096768035\n",
      "104 Train Loss 1211.3209 Test MSE 3821.1551234410003 Test RE 0.027630208234324116\n",
      "105 Train Loss 1210.2549 Test MSE 5148.354391939314 Test RE 0.03207163058107284\n",
      "106 Train Loss 1208.1572 Test MSE 9290.616096014606 Test RE 0.04308331189958052\n",
      "107 Train Loss 1202.727 Test MSE 4821.298661630376 Test RE 0.031036221399394163\n",
      "108 Train Loss 1198.9294 Test MSE 6639.802102302081 Test RE 0.03642205043461553\n",
      "109 Train Loss 1194.6559 Test MSE 6539.125576611536 Test RE 0.03614486961389917\n",
      "110 Train Loss 1192.6677 Test MSE 3393.3570838529304 Test RE 0.026037639223621293\n",
      "111 Train Loss 1191.7235 Test MSE 2368.0478811563885 Test RE 0.021751141986118095\n",
      "112 Train Loss 1189.896 Test MSE 1682.0032413586912 Test RE 0.018331594326232086\n",
      "113 Train Loss 1188.0312 Test MSE 2790.7902752434275 Test RE 0.02361295960378239\n",
      "114 Train Loss 1186.7843 Test MSE 2865.50281887344 Test RE 0.023926944579683664\n",
      "115 Train Loss 1184.2797 Test MSE 2446.834645489371 Test RE 0.02211001993977566\n",
      "116 Train Loss 1180.7448 Test MSE 6016.3352071793215 Test RE 0.034669919749618486\n",
      "117 Train Loss 1177.6483 Test MSE 3191.6449658196925 Test RE 0.025251902928644144\n",
      "118 Train Loss 1175.8462 Test MSE 3515.008995098512 Test RE 0.026500254453770144\n",
      "119 Train Loss 1174.4835 Test MSE 3455.9017118772863 Test RE 0.026276499856476176\n",
      "120 Train Loss 1169.2781 Test MSE 2547.7170780088877 Test RE 0.022561211809411105\n",
      "121 Train Loss 1168.4418 Test MSE 2402.734001275222 Test RE 0.02190986344206424\n",
      "122 Train Loss 1164.3875 Test MSE 2260.037617454843 Test RE 0.021249301448826294\n",
      "123 Train Loss 1160.0677 Test MSE 2537.732972767393 Test RE 0.022516961481964363\n",
      "124 Train Loss 1155.4114 Test MSE 2293.2760740062754 Test RE 0.02140498824514764\n",
      "125 Train Loss 1153.0154 Test MSE 2854.179305404062 Test RE 0.02387962212184095\n",
      "126 Train Loss 1151.2668 Test MSE 3420.369976145237 Test RE 0.026141070384609937\n",
      "127 Train Loss 1146.1273 Test MSE 3369.093573888645 Test RE 0.025944383800644885\n",
      "128 Train Loss 1141.2883 Test MSE 4222.85174110201 Test RE 0.029046227911961566\n",
      "129 Train Loss 1136.2017 Test MSE 4487.016730304797 Test RE 0.029940956347543773\n",
      "130 Train Loss 1133.6813 Test MSE 5864.38986710766 Test RE 0.034229317593437475\n",
      "131 Train Loss 1129.3385 Test MSE 3731.223353353099 Test RE 0.02730313064184878\n",
      "132 Train Loss 1127.4116 Test MSE 4158.62312384652 Test RE 0.028824488259366142\n",
      "133 Train Loss 1121.2894 Test MSE 4069.2368607543044 Test RE 0.02851302590055577\n",
      "134 Train Loss 1118.7596 Test MSE 2689.2889839187046 Test RE 0.0231795798690503\n",
      "135 Train Loss 1115.471 Test MSE 1938.7146151345055 Test RE 0.01968084612380101\n",
      "136 Train Loss 1105.6997 Test MSE 1852.0635810343995 Test RE 0.019236000052380933\n",
      "137 Train Loss 1096.5115 Test MSE 2802.32784193922 Test RE 0.02366171910854104\n",
      "138 Train Loss 1086.2932 Test MSE 3415.381352582076 Test RE 0.02612199999961654\n",
      "139 Train Loss 1080.5403 Test MSE 8804.861172643059 Test RE 0.04194189802986608\n",
      "140 Train Loss 1076.0898 Test MSE 14736.75983668797 Test RE 0.05426100416236075\n",
      "141 Train Loss 1066.9779 Test MSE 3891.161213769854 Test RE 0.027882161333943657\n",
      "142 Train Loss 1059.7236 Test MSE 11402.613779262987 Test RE 0.04772973585330176\n",
      "143 Train Loss 1048.9008 Test MSE 6080.7427056702245 Test RE 0.03485500403985191\n",
      "144 Train Loss 1040.2487 Test MSE 3121.2022885678866 Test RE 0.024971681206045766\n",
      "145 Train Loss 1033.5901 Test MSE 3437.5220280008257 Test RE 0.026206532935656605\n",
      "146 Train Loss 1025.7388 Test MSE 3587.591047864804 Test RE 0.02677246068466419\n",
      "147 Train Loss 1023.41046 Test MSE 2911.9538781419865 Test RE 0.02412009809106666\n",
      "148 Train Loss 1017.312 Test MSE 2235.983096458971 Test RE 0.021135916360906143\n",
      "149 Train Loss 1008.30066 Test MSE 1512.9398466076914 Test RE 0.017385919091285583\n",
      "150 Train Loss 1003.4773 Test MSE 1866.0569154764398 Test RE 0.019308532457500888\n",
      "151 Train Loss 1001.7794 Test MSE 1565.2632874858543 Test RE 0.0176840006928561\n",
      "152 Train Loss 996.63995 Test MSE 1709.7073780482337 Test RE 0.018481946833103696\n",
      "153 Train Loss 984.92224 Test MSE 2497.6708871458823 Test RE 0.02233852170867885\n",
      "154 Train Loss 980.2447 Test MSE 4337.482091464161 Test RE 0.029437821732619097\n",
      "155 Train Loss 976.76324 Test MSE 3718.9844984573997 Test RE 0.027258315109098173\n",
      "156 Train Loss 973.8867 Test MSE 2639.714484007892 Test RE 0.02296493927358438\n",
      "157 Train Loss 972.7999 Test MSE 2485.255603760384 Test RE 0.022282933003315544\n",
      "158 Train Loss 970.714 Test MSE 2667.7828311520807 Test RE 0.023086710671500952\n",
      "159 Train Loss 968.4132 Test MSE 3432.9220775242047 Test RE 0.026188992814017223\n",
      "160 Train Loss 967.6545 Test MSE 3074.995019557556 Test RE 0.02478614762518525\n",
      "161 Train Loss 966.2226 Test MSE 2554.2715539465557 Test RE 0.02259021462395719\n",
      "162 Train Loss 963.8923 Test MSE 2394.1155164095367 Test RE 0.021870533357771248\n",
      "163 Train Loss 961.7644 Test MSE 2179.003751871455 Test RE 0.020864876222259264\n",
      "164 Train Loss 960.173 Test MSE 1767.8926191446953 Test RE 0.018793807102207204\n",
      "165 Train Loss 958.2561 Test MSE 1383.3871855527964 Test RE 0.01662488683964925\n",
      "166 Train Loss 957.2662 Test MSE 1459.4572812757353 Test RE 0.01707585732462637\n",
      "167 Train Loss 955.6731 Test MSE 1948.2930778907769 Test RE 0.019729404065608188\n",
      "168 Train Loss 949.80554 Test MSE 3274.6688937283852 Test RE 0.025578231922307876\n",
      "169 Train Loss 945.2093 Test MSE 2163.0201308041756 Test RE 0.02078821042921562\n",
      "170 Train Loss 943.48096 Test MSE 1602.3337427985116 Test RE 0.017892182233414707\n",
      "171 Train Loss 942.94086 Test MSE 1791.7475409344413 Test RE 0.018920178623009792\n",
      "172 Train Loss 940.7504 Test MSE 1925.727183287266 Test RE 0.019614814445589428\n",
      "173 Train Loss 938.5134 Test MSE 1827.8021829069166 Test RE 0.019109592209010633\n",
      "174 Train Loss 936.48987 Test MSE 2591.527632468323 Test RE 0.022754366327755206\n",
      "175 Train Loss 935.3013 Test MSE 2170.9095187240996 Test RE 0.020826087325766983\n",
      "176 Train Loss 934.07465 Test MSE 1562.186626993946 Test RE 0.017666612427010252\n",
      "177 Train Loss 933.05066 Test MSE 1302.48031733042 Test RE 0.016131412924328438\n",
      "178 Train Loss 931.3876 Test MSE 1189.3275916733776 Test RE 0.015414788518156542\n",
      "179 Train Loss 929.9636 Test MSE 1320.832524085905 Test RE 0.01624466279989178\n",
      "180 Train Loss 926.972 Test MSE 1640.8175088479272 Test RE 0.018105768553968157\n",
      "181 Train Loss 925.5491 Test MSE 1790.1643604097462 Test RE 0.018911817880946577\n",
      "182 Train Loss 924.4228 Test MSE 1602.3131540537124 Test RE 0.017892067282718912\n",
      "183 Train Loss 922.1547 Test MSE 2518.4823982208045 Test RE 0.022431395029588273\n",
      "184 Train Loss 920.8082 Test MSE 2490.696995966766 Test RE 0.022307313570439222\n",
      "185 Train Loss 918.8526 Test MSE 1613.671668869273 Test RE 0.017955372143289836\n",
      "186 Train Loss 917.85675 Test MSE 1343.4924556696294 Test RE 0.0163834152781186\n",
      "187 Train Loss 915.71136 Test MSE 1246.3467387769015 Test RE 0.015779973315800007\n",
      "188 Train Loss 914.20953 Test MSE 1323.0407606564102 Test RE 0.016258236465418192\n",
      "189 Train Loss 912.9776 Test MSE 1127.5560774044243 Test RE 0.01500914311522604\n",
      "190 Train Loss 912.0696 Test MSE 1211.6155040822664 Test RE 0.015558554112085543\n",
      "191 Train Loss 911.5888 Test MSE 1278.8348589042268 Test RE 0.015984315996606706\n",
      "192 Train Loss 910.1283 Test MSE 1044.138002587351 Test RE 0.01444327814960525\n",
      "193 Train Loss 908.73175 Test MSE 1081.8855421692658 Test RE 0.014702036012430015\n",
      "194 Train Loss 907.7957 Test MSE 1114.8876004622487 Test RE 0.01492458852449138\n",
      "195 Train Loss 906.14435 Test MSE 1083.8233256313424 Test RE 0.01471519665447632\n",
      "196 Train Loss 904.6796 Test MSE 1197.330331112239 Test RE 0.015466563031436233\n",
      "197 Train Loss 903.78156 Test MSE 1112.2301285915983 Test RE 0.014906790615088032\n",
      "198 Train Loss 903.2225 Test MSE 1027.2287319714378 Test RE 0.014325850107588524\n",
      "199 Train Loss 900.9439 Test MSE 1257.324232550814 Test RE 0.015849313891320067\n",
      "Training time: 45.28\n",
      "Training time: 45.28\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 4535.022 Test MSE 4795001.327599706 Test RE 0.978771217185537\n",
      "1 Train Loss 4289.0146 Test MSE 3457687.1565633314 Test RE 0.831150502989338\n",
      "2 Train Loss 3589.0972 Test MSE 2276114.6579516036 Test RE 0.6743477168697757\n",
      "3 Train Loss 2905.453 Test MSE 1271622.4002096413 Test RE 0.5040410509940391\n",
      "4 Train Loss 1889.2886 Test MSE 380482.3345115696 Test RE 0.27571098669233857\n",
      "5 Train Loss 1716.2322 Test MSE 222637.51319495073 Test RE 0.21090457217092226\n",
      "6 Train Loss 1645.7155 Test MSE 164884.04580164203 Test RE 0.18149979602652971\n",
      "7 Train Loss 1602.3488 Test MSE 191134.3028045661 Test RE 0.19541421033745102\n",
      "8 Train Loss 1511.8533 Test MSE 215132.8631195372 Test RE 0.20731952360500747\n",
      "9 Train Loss 1450.0239 Test MSE 340280.6428912131 Test RE 0.26073867004262763\n",
      "10 Train Loss 1388.4642 Test MSE 275039.8744957455 Test RE 0.23441459836458284\n",
      "11 Train Loss 1294.86 Test MSE 42116.35566500863 Test RE 0.09173017371093571\n",
      "12 Train Loss 1283.2651 Test MSE 36771.64317143314 Test RE 0.08571233811289099\n",
      "13 Train Loss 1269.6401 Test MSE 17293.954218385894 Test RE 0.0587805947663575\n",
      "14 Train Loss 1262.2975 Test MSE 7054.442628622022 Test RE 0.03754206666688996\n",
      "15 Train Loss 1257.2329 Test MSE 2505.717841338379 Test RE 0.022374477708733645\n",
      "16 Train Loss 1256.3887 Test MSE 2414.977570086973 Test RE 0.02196561535905487\n",
      "17 Train Loss 1255.7605 Test MSE 2943.048737963854 Test RE 0.02424853753408831\n",
      "18 Train Loss 1253.725 Test MSE 3207.4225568204797 Test RE 0.025314241163838413\n",
      "19 Train Loss 1250.9185 Test MSE 1865.4619101056096 Test RE 0.01930545388139277\n",
      "20 Train Loss 1249.3981 Test MSE 1858.8023147317037 Test RE 0.019270963372698693\n",
      "21 Train Loss 1248.9333 Test MSE 1666.788820957748 Test RE 0.018248497532306184\n",
      "22 Train Loss 1248.1504 Test MSE 1906.7328485486448 Test RE 0.019517839748998112\n",
      "23 Train Loss 1246.4868 Test MSE 1953.818044249004 Test RE 0.019757358567370747\n",
      "24 Train Loss 1245.2367 Test MSE 1625.1337144191928 Test RE 0.01801902856314378\n",
      "25 Train Loss 1245.0627 Test MSE 1702.380131061148 Test RE 0.018442300518150385\n",
      "26 Train Loss 1244.4674 Test MSE 2383.3730926687504 Test RE 0.02182141152644127\n",
      "27 Train Loss 1243.8071 Test MSE 1806.3920089096605 Test RE 0.018997341306417833\n",
      "28 Train Loss 1243.0958 Test MSE 1409.4513789556001 Test RE 0.01678076954245948\n",
      "29 Train Loss 1242.5216 Test MSE 1499.7482232448704 Test RE 0.017309957504897434\n",
      "30 Train Loss 1241.8011 Test MSE 1868.8844239566583 Test RE 0.01932315536997271\n",
      "31 Train Loss 1241.4908 Test MSE 1818.7050649054252 Test RE 0.019061977903824263\n",
      "32 Train Loss 1241.172 Test MSE 1605.1747302935023 Test RE 0.017908036906238625\n",
      "33 Train Loss 1240.7556 Test MSE 1670.0099739789769 Test RE 0.01826612208984548\n",
      "34 Train Loss 1240.5017 Test MSE 1922.3348261825774 Test RE 0.019597530121532047\n",
      "35 Train Loss 1240.4526 Test MSE 1874.1546900832045 Test RE 0.019350381900110045\n",
      "36 Train Loss 1240.3838 Test MSE 1735.7869067556858 Test RE 0.018622373280209964\n",
      "37 Train Loss 1240.2136 Test MSE 1573.8520351558166 Test RE 0.01773245121263504\n",
      "38 Train Loss 1240.0286 Test MSE 1724.6776278334983 Test RE 0.018562684726636667\n",
      "39 Train Loss 1239.8204 Test MSE 2057.933158813358 Test RE 0.020276941868268665\n",
      "40 Train Loss 1239.4498 Test MSE 2375.3630368067143 Test RE 0.0217847118941829\n",
      "41 Train Loss 1239.2335 Test MSE 2722.431687430731 Test RE 0.02332197466172499\n",
      "42 Train Loss 1238.9006 Test MSE 3242.689669145353 Test RE 0.025453031655518668\n",
      "43 Train Loss 1238.5609 Test MSE 3240.9524312081535 Test RE 0.02544621264095979\n",
      "44 Train Loss 1237.7582 Test MSE 3886.2965112003744 Test RE 0.027864726841448664\n",
      "45 Train Loss 1236.8179 Test MSE 2953.6812453803254 Test RE 0.024292300029263813\n",
      "46 Train Loss 1236.2151 Test MSE 1987.1090931511008 Test RE 0.019924970130336603\n",
      "47 Train Loss 1235.5527 Test MSE 1980.325078409989 Test RE 0.01989092900546484\n",
      "48 Train Loss 1235.0228 Test MSE 1518.5237100976826 Test RE 0.017417972972754533\n",
      "49 Train Loss 1234.7085 Test MSE 1435.7934328737279 Test RE 0.016936856384310427\n",
      "50 Train Loss 1234.5546 Test MSE 1442.722545453794 Test RE 0.016977675673298696\n",
      "51 Train Loss 1232.9143 Test MSE 1829.6028863446481 Test RE 0.019119003029761186\n",
      "52 Train Loss 1232.1451 Test MSE 1954.0260131892153 Test RE 0.019758410049006893\n",
      "53 Train Loss 1231.5884 Test MSE 1940.7417640552253 Test RE 0.019691132728686375\n",
      "54 Train Loss 1231.0773 Test MSE 1819.8189902634501 Test RE 0.01906781457593233\n",
      "55 Train Loss 1230.6798 Test MSE 2105.529183743375 Test RE 0.02051008481624655\n",
      "56 Train Loss 1230.2675 Test MSE 2266.471525819782 Test RE 0.02127952636504386\n",
      "57 Train Loss 1229.3784 Test MSE 1807.4262779334101 Test RE 0.019002779093422264\n",
      "58 Train Loss 1228.4915 Test MSE 1675.2466128985063 Test RE 0.018294738152020457\n",
      "59 Train Loss 1227.9631 Test MSE 2099.614281289456 Test RE 0.02048125584722064\n",
      "60 Train Loss 1226.4246 Test MSE 2745.3971007440127 Test RE 0.02342013580225357\n",
      "61 Train Loss 1226.1978 Test MSE 2635.7779981199246 Test RE 0.022947809601445292\n",
      "62 Train Loss 1225.5563 Test MSE 2456.087997389869 Test RE 0.022151787929862436\n",
      "63 Train Loss 1224.5938 Test MSE 2246.539944383199 Test RE 0.021185752584659608\n",
      "64 Train Loss 1223.0251 Test MSE 1930.2887026625026 Test RE 0.019638031761913605\n",
      "65 Train Loss 1221.499 Test MSE 2438.14984850126 Test RE 0.02207074639754754\n",
      "66 Train Loss 1220.91 Test MSE 3145.390695358637 Test RE 0.025068256080969745\n",
      "67 Train Loss 1220.3849 Test MSE 3418.095979679735 Test RE 0.02613237913400645\n",
      "68 Train Loss 1219.2366 Test MSE 2454.734529126678 Test RE 0.022145683532627657\n",
      "69 Train Loss 1218.1166 Test MSE 1820.1069280508493 Test RE 0.019069323002652506\n",
      "70 Train Loss 1217.5413 Test MSE 1898.3377814413027 Test RE 0.01947482524683489\n",
      "71 Train Loss 1217.2571 Test MSE 1950.0840057519886 Test RE 0.019738469904716248\n",
      "72 Train Loss 1216.8069 Test MSE 2089.6435252575557 Test RE 0.020432566754777504\n",
      "73 Train Loss 1215.6986 Test MSE 2131.241322756482 Test RE 0.020634936545703975\n",
      "74 Train Loss 1214.8636 Test MSE 2250.560759189165 Test RE 0.0212047030392861\n",
      "75 Train Loss 1214.5741 Test MSE 2361.2996062524894 Test RE 0.021720127535683243\n",
      "76 Train Loss 1214.218 Test MSE 2385.1699895843067 Test RE 0.021829635886994322\n",
      "77 Train Loss 1213.2095 Test MSE 3978.9280141725376 Test RE 0.028194854950577015\n",
      "78 Train Loss 1211.5292 Test MSE 3929.1944449674197 Test RE 0.02801809376404453\n",
      "79 Train Loss 1210.8263 Test MSE 4685.0671665598165 Test RE 0.030594596820232526\n",
      "80 Train Loss 1209.9281 Test MSE 5898.584233327624 Test RE 0.03432896552145306\n",
      "81 Train Loss 1209.021 Test MSE 4270.803737937718 Test RE 0.029210677554018273\n",
      "82 Train Loss 1207.425 Test MSE 3027.9173833937293 Test RE 0.02459568001346672\n",
      "83 Train Loss 1206.8201 Test MSE 3191.3205488691965 Test RE 0.02525061952249861\n",
      "84 Train Loss 1206.3124 Test MSE 3618.2773413887157 Test RE 0.026886715386773247\n",
      "85 Train Loss 1205.807 Test MSE 3580.806291376942 Test RE 0.02674713302251247\n",
      "86 Train Loss 1205.2648 Test MSE 3752.008171526233 Test RE 0.027379071217200877\n",
      "87 Train Loss 1204.4448 Test MSE 4187.858095595312 Test RE 0.02892562837832268\n",
      "88 Train Loss 1203.7717 Test MSE 5198.797149852687 Test RE 0.03222836397370534\n",
      "89 Train Loss 1202.4763 Test MSE 5449.02282077293 Test RE 0.03299484838201869\n",
      "90 Train Loss 1201.5845 Test MSE 5549.2265928452225 Test RE 0.03329684259482529\n",
      "91 Train Loss 1201.1298 Test MSE 6939.656457284692 Test RE 0.03723538163773632\n",
      "92 Train Loss 1200.0996 Test MSE 9248.733635365943 Test RE 0.04298609158020883\n",
      "93 Train Loss 1198.905 Test MSE 7979.3065535360365 Test RE 0.03992724997809701\n",
      "94 Train Loss 1196.6987 Test MSE 9388.2998006023 Test RE 0.04330921365138599\n",
      "95 Train Loss 1196.1924 Test MSE 11183.240688837566 Test RE 0.04726837362274004\n",
      "96 Train Loss 1194.9835 Test MSE 12513.947206826371 Test RE 0.05000160936620625\n",
      "97 Train Loss 1193.804 Test MSE 13060.916636148893 Test RE 0.05108267745957877\n",
      "98 Train Loss 1193.196 Test MSE 13317.34599924325 Test RE 0.051581701669733776\n",
      "99 Train Loss 1191.792 Test MSE 15944.658081767866 Test RE 0.056440964801822986\n",
      "100 Train Loss 1189.7976 Test MSE 17931.144015806265 Test RE 0.05985367525494859\n",
      "101 Train Loss 1189.0969 Test MSE 17408.818161104173 Test RE 0.05897547781019423\n",
      "102 Train Loss 1188.241 Test MSE 18398.411286631257 Test RE 0.06062852256288542\n",
      "103 Train Loss 1186.9867 Test MSE 21102.833873158474 Test RE 0.06493176582256101\n",
      "104 Train Loss 1184.9438 Test MSE 17003.199539486144 Test RE 0.058284375754641994\n",
      "105 Train Loss 1182.4006 Test MSE 10673.861448593403 Test RE 0.04617932751637145\n",
      "106 Train Loss 1176.6417 Test MSE 9910.603821824623 Test RE 0.044497630089428475\n",
      "107 Train Loss 1173.6007 Test MSE 12242.42177544445 Test RE 0.049456171361246484\n",
      "108 Train Loss 1171.5868 Test MSE 10064.067943265825 Test RE 0.04484082596661702\n",
      "109 Train Loss 1169.6353 Test MSE 8343.444833448837 Test RE 0.04082813320587549\n",
      "110 Train Loss 1161.0978 Test MSE 9242.332001704504 Test RE 0.042971212308313196\n",
      "111 Train Loss 1155.8298 Test MSE 9930.170767562277 Test RE 0.04454153525383032\n",
      "112 Train Loss 1151.9917 Test MSE 11030.77369957984 Test RE 0.046945050560622266\n",
      "113 Train Loss 1148.7474 Test MSE 12818.169416473886 Test RE 0.05060574552894459\n",
      "114 Train Loss 1145.3477 Test MSE 18482.995948089272 Test RE 0.06076772921375567\n",
      "115 Train Loss 1136.3665 Test MSE 30429.298656772327 Test RE 0.07797094449040443\n",
      "116 Train Loss 1128.2369 Test MSE 25825.53092757766 Test RE 0.07183092398821102\n",
      "117 Train Loss 1122.8394 Test MSE 31602.923736762776 Test RE 0.07946034662456104\n",
      "118 Train Loss 1120.9382 Test MSE 34514.884602236205 Test RE 0.08304051549464694\n",
      "119 Train Loss 1119.2678 Test MSE 39048.1908831318 Test RE 0.08832573955430156\n",
      "120 Train Loss 1116.2719 Test MSE 45729.61101236372 Test RE 0.09558408282433774\n",
      "121 Train Loss 1109.395 Test MSE 37671.79918860542 Test RE 0.08675509803011514\n",
      "122 Train Loss 1106.4703 Test MSE 35128.269210959836 Test RE 0.08377514724842658\n",
      "123 Train Loss 1104.6403 Test MSE 36701.99460122685 Test RE 0.0856311264677016\n",
      "124 Train Loss 1102.2186 Test MSE 32361.696453214383 Test RE 0.08040859314748182\n",
      "125 Train Loss 1098.6992 Test MSE 28461.60240843891 Test RE 0.07540783938960045\n",
      "126 Train Loss 1090.0349 Test MSE 27824.48087309838 Test RE 0.07455904880728748\n",
      "127 Train Loss 1075.9705 Test MSE 12994.952643041479 Test RE 0.05095351796763513\n",
      "128 Train Loss 1070.5681 Test MSE 17448.606098405307 Test RE 0.05904283371512154\n",
      "129 Train Loss 1068.3324 Test MSE 12854.774756060102 Test RE 0.05067795240200023\n",
      "130 Train Loss 1067.1073 Test MSE 11612.775906150573 Test RE 0.04816758211006325\n",
      "131 Train Loss 1064.7325 Test MSE 10766.089570686034 Test RE 0.04637840598977486\n",
      "132 Train Loss 1062.6116 Test MSE 9640.72294939122 Test RE 0.04388757906782843\n",
      "133 Train Loss 1060.2303 Test MSE 8077.650839460019 Test RE 0.040172546486291184\n",
      "134 Train Loss 1059.6946 Test MSE 8177.950465738326 Test RE 0.0404211868817929\n",
      "135 Train Loss 1059.2352 Test MSE 8182.745356151008 Test RE 0.040433035006894186\n",
      "136 Train Loss 1057.688 Test MSE 7271.943043455593 Test RE 0.03811641598494684\n",
      "137 Train Loss 1055.9246 Test MSE 6051.066710308103 Test RE 0.03476984815704691\n",
      "138 Train Loss 1055.5117 Test MSE 6394.322038070048 Test RE 0.03574243006671139\n",
      "139 Train Loss 1055.0333 Test MSE 7324.09225295074 Test RE 0.038252843759826356\n",
      "140 Train Loss 1053.784 Test MSE 8537.437317649377 Test RE 0.04130005097586076\n",
      "141 Train Loss 1052.3784 Test MSE 6482.253244228908 Test RE 0.03598734609170653\n",
      "142 Train Loss 1049.9485 Test MSE 5910.328472098953 Test RE 0.03436312347123372\n",
      "143 Train Loss 1048.7375 Test MSE 6399.5782292104695 Test RE 0.0357571173511082\n",
      "144 Train Loss 1047.3556 Test MSE 7168.28389032179 Test RE 0.0378437724081697\n",
      "145 Train Loss 1046.4269 Test MSE 7050.197487408483 Test RE 0.03753076915082636\n",
      "146 Train Loss 1045.512 Test MSE 7645.644732450539 Test RE 0.03908353887333154\n",
      "147 Train Loss 1044.521 Test MSE 7448.6959790604815 Test RE 0.03857686649334392\n",
      "148 Train Loss 1043.4304 Test MSE 7513.864473357153 Test RE 0.03874525311487284\n",
      "149 Train Loss 1042.0859 Test MSE 8014.725077121425 Test RE 0.04001576634300678\n",
      "150 Train Loss 1041.4023 Test MSE 8142.946580890402 Test RE 0.04033458719737348\n",
      "151 Train Loss 1041.2301 Test MSE 8787.240962280235 Test RE 0.04189991013506408\n",
      "152 Train Loss 1040.7924 Test MSE 10097.817326122331 Test RE 0.04491594884919319\n",
      "153 Train Loss 1038.7461 Test MSE 9060.972079274343 Test RE 0.04254751692697911\n",
      "154 Train Loss 1036.9489 Test MSE 6328.697028715547 Test RE 0.03555854453518846\n",
      "155 Train Loss 1036.1467 Test MSE 5407.351314909862 Test RE 0.03286844187826546\n",
      "156 Train Loss 1034.5682 Test MSE 5888.879224800479 Test RE 0.034300712974573044\n",
      "157 Train Loss 1033.4056 Test MSE 6550.999659263099 Test RE 0.036177671592223806\n",
      "158 Train Loss 1031.3173 Test MSE 6237.206947962579 Test RE 0.03530058484527331\n",
      "159 Train Loss 1030.5383 Test MSE 7059.226046730647 Test RE 0.037554792617001787\n",
      "160 Train Loss 1029.4152 Test MSE 8638.62801314159 Test RE 0.041544086161880184\n",
      "161 Train Loss 1028.4629 Test MSE 8903.317194961684 Test RE 0.04217574343786522\n",
      "162 Train Loss 1027.4895 Test MSE 7639.000045878829 Test RE 0.03906655179610092\n",
      "163 Train Loss 1026.8059 Test MSE 7758.015421088218 Test RE 0.03936970339739229\n",
      "164 Train Loss 1026.399 Test MSE 8956.14860663895 Test RE 0.04230069171840481\n",
      "165 Train Loss 1026.0833 Test MSE 9966.210077158697 Test RE 0.04462228876673162\n",
      "166 Train Loss 1025.6326 Test MSE 10273.531850041038 Test RE 0.04530505995700584\n",
      "167 Train Loss 1024.8868 Test MSE 9575.02643643839 Test RE 0.043737787931278416\n",
      "168 Train Loss 1023.49097 Test MSE 7471.920628692908 Test RE 0.038636960027044885\n",
      "169 Train Loss 1021.26447 Test MSE 7235.131208378082 Test RE 0.03801981763098895\n",
      "170 Train Loss 1019.79865 Test MSE 9154.068858371187 Test RE 0.04276553516422703\n",
      "171 Train Loss 1017.25995 Test MSE 10864.141733779568 Test RE 0.04658912297226951\n",
      "172 Train Loss 1013.5837 Test MSE 12453.502254173938 Test RE 0.04988070413385449\n",
      "173 Train Loss 1011.3572 Test MSE 11491.663615549496 Test RE 0.047915748427196966\n",
      "174 Train Loss 1009.814 Test MSE 9497.059633492125 Test RE 0.04355935156923673\n",
      "175 Train Loss 1008.9328 Test MSE 9650.617117115407 Test RE 0.04391009396199551\n",
      "176 Train Loss 1007.0745 Test MSE 9061.417157505684 Test RE 0.04254856188887029\n",
      "177 Train Loss 1005.2376 Test MSE 9418.594610855624 Test RE 0.043379033943302156\n",
      "178 Train Loss 1001.5325 Test MSE 11589.763149728995 Test RE 0.048119832177371424\n",
      "179 Train Loss 998.26587 Test MSE 13883.79315429109 Test RE 0.052667280149106904\n",
      "180 Train Loss 996.086 Test MSE 9399.396521957611 Test RE 0.04333480126356479\n",
      "181 Train Loss 994.2165 Test MSE 6661.797505804375 Test RE 0.03648232748221845\n",
      "182 Train Loss 992.9032 Test MSE 7928.90243214511 Test RE 0.03980094287463145\n",
      "183 Train Loss 992.1224 Test MSE 8833.782843857942 Test RE 0.042010725650896065\n",
      "184 Train Loss 990.87115 Test MSE 7246.1337856462505 Test RE 0.038048715314739044\n",
      "185 Train Loss 989.6472 Test MSE 5777.33704915136 Test RE 0.033974312419836394\n",
      "186 Train Loss 988.6359 Test MSE 7181.473346944387 Test RE 0.03787857218818827\n",
      "187 Train Loss 987.6352 Test MSE 8160.25324299227 Test RE 0.04037742712966083\n",
      "188 Train Loss 985.8659 Test MSE 7921.749933393681 Test RE 0.039782987020983866\n",
      "189 Train Loss 983.203 Test MSE 7806.567682509656 Test RE 0.03949270565160028\n",
      "190 Train Loss 982.1205 Test MSE 7455.1961293137165 Test RE 0.038593694992402776\n",
      "191 Train Loss 981.03406 Test MSE 7046.670717672141 Test RE 0.037521380836645\n",
      "192 Train Loss 979.4464 Test MSE 7369.761953571172 Test RE 0.03837192206690455\n",
      "193 Train Loss 977.13446 Test MSE 6488.505344794278 Test RE 0.036004696715981686\n",
      "194 Train Loss 974.00226 Test MSE 6813.687817919789 Test RE 0.03689588563809725\n",
      "195 Train Loss 972.68243 Test MSE 7356.905092223715 Test RE 0.03833843673611298\n",
      "196 Train Loss 971.8325 Test MSE 6732.551940727073 Test RE 0.03667555376312546\n",
      "197 Train Loss 970.2498 Test MSE 7421.655249245608 Test RE 0.0385067807311161\n",
      "198 Train Loss 966.86743 Test MSE 7202.299222373163 Test RE 0.037933455309445634\n",
      "199 Train Loss 963.35364 Test MSE 6888.389428510615 Test RE 0.037097587635553174\n",
      "Training time: 44.36\n",
      "Training time: 44.36\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 4555.15 Test MSE 4931008.910468897 Test RE 0.9925553104699169\n",
      "1 Train Loss 4280.1865 Test MSE 3551022.181570952 Test RE 0.8422936299119176\n",
      "2 Train Loss 2769.56 Test MSE 1358045.4086979397 Test RE 0.5208875406401766\n",
      "3 Train Loss 2226.5264 Test MSE 1345990.570207161 Test RE 0.518570529787713\n",
      "4 Train Loss 1855.8359 Test MSE 1034029.7786718302 Test RE 0.45452036490549236\n",
      "5 Train Loss 1521.4661 Test MSE 548441.9641103394 Test RE 0.3310184207660898\n",
      "6 Train Loss 1430.5514 Test MSE 209366.8735758295 Test RE 0.20452236572316335\n",
      "7 Train Loss 1405.8717 Test MSE 133804.50044012515 Test RE 0.16350166669973693\n",
      "8 Train Loss 1400.6603 Test MSE 122965.16160885642 Test RE 0.1567392867312566\n",
      "9 Train Loss 1389.4348 Test MSE 132894.21427716519 Test RE 0.16294455800299884\n",
      "10 Train Loss 1354.744 Test MSE 80866.07717647789 Test RE 0.12710715894378505\n",
      "11 Train Loss 1347.019 Test MSE 73452.37573944296 Test RE 0.12114060749502457\n",
      "12 Train Loss 1338.2827 Test MSE 68007.60169446722 Test RE 0.11656429825308041\n",
      "13 Train Loss 1332.1888 Test MSE 60948.94582652493 Test RE 0.11034938647188339\n",
      "14 Train Loss 1331.3257 Test MSE 58641.127361004874 Test RE 0.10824004873064222\n",
      "15 Train Loss 1316.591 Test MSE 45075.86410476745 Test RE 0.09489839210781263\n",
      "16 Train Loss 1300.1912 Test MSE 20747.617346081886 Test RE 0.06438295985090381\n",
      "17 Train Loss 1283.5386 Test MSE 29446.724693808716 Test RE 0.07670175863482633\n",
      "18 Train Loss 1269.492 Test MSE 20725.640868828676 Test RE 0.06434885266871024\n",
      "19 Train Loss 1266.7988 Test MSE 16715.149823376665 Test RE 0.05778857168611988\n",
      "20 Train Loss 1262.4966 Test MSE 13692.000876345017 Test RE 0.05230223922433265\n",
      "21 Train Loss 1254.836 Test MSE 13555.87264803129 Test RE 0.052041590822920716\n",
      "22 Train Loss 1254.2739 Test MSE 14809.66731939534 Test RE 0.05439506186537595\n",
      "23 Train Loss 1248.2913 Test MSE 15033.371832191791 Test RE 0.05480434900354335\n",
      "24 Train Loss 1243.7777 Test MSE 14306.561534352362 Test RE 0.053463139391460184\n",
      "25 Train Loss 1243.2659 Test MSE 13110.652191896002 Test RE 0.05117984565083971\n",
      "26 Train Loss 1239.6394 Test MSE 4471.6444949158795 Test RE 0.029889624433929306\n",
      "27 Train Loss 1235.944 Test MSE 5481.956560893948 Test RE 0.033094408140689646\n",
      "28 Train Loss 1233.5591 Test MSE 3168.969630288082 Test RE 0.025162040800986753\n",
      "29 Train Loss 1231.6079 Test MSE 2436.6036139541025 Test RE 0.022063746835374796\n",
      "30 Train Loss 1230.2917 Test MSE 2752.9208911312357 Test RE 0.023452205412984335\n",
      "31 Train Loss 1228.3373 Test MSE 3529.795061113853 Test RE 0.0265559333024928\n",
      "32 Train Loss 1226.6669 Test MSE 2375.0474325376854 Test RE 0.021783264625627213\n",
      "33 Train Loss 1226.0841 Test MSE 2068.187801235684 Test RE 0.02032739890071911\n",
      "34 Train Loss 1224.9235 Test MSE 2300.3167561360688 Test RE 0.02143782123572961\n",
      "35 Train Loss 1223.786 Test MSE 2609.890968191345 Test RE 0.02283484173320655\n",
      "36 Train Loss 1223.4438 Test MSE 3090.1320617030924 Test RE 0.024847079165453884\n",
      "37 Train Loss 1222.6237 Test MSE 2528.49633819937 Test RE 0.02247594642240977\n",
      "38 Train Loss 1221.4755 Test MSE 1565.5796795582012 Test RE 0.01768578786660566\n",
      "39 Train Loss 1220.4176 Test MSE 1647.105470912556 Test RE 0.018140427960296704\n",
      "40 Train Loss 1218.7179 Test MSE 1752.1818527050127 Test RE 0.018710113096602896\n",
      "41 Train Loss 1217.3242 Test MSE 1652.4285241589996 Test RE 0.018169717090449754\n",
      "42 Train Loss 1213.2295 Test MSE 2145.1282407785034 Test RE 0.020702054800447105\n",
      "43 Train Loss 1212.7334 Test MSE 2650.0242957569685 Test RE 0.023009742124945526\n",
      "44 Train Loss 1211.859 Test MSE 3408.863487189953 Test RE 0.026097062669040848\n",
      "45 Train Loss 1211.2035 Test MSE 3648.2413499779827 Test RE 0.026997814211391292\n",
      "46 Train Loss 1210.0381 Test MSE 4038.1212419784633 Test RE 0.02840380358254664\n",
      "47 Train Loss 1209.4727 Test MSE 4745.242180074854 Test RE 0.03079044848303194\n",
      "48 Train Loss 1208.9612 Test MSE 5505.472453584449 Test RE 0.033165314551590795\n",
      "49 Train Loss 1206.9707 Test MSE 5221.328137940349 Test RE 0.0322981254796553\n",
      "50 Train Loss 1205.4933 Test MSE 4638.290846295023 Test RE 0.03044148346762386\n",
      "51 Train Loss 1204.845 Test MSE 5423.522053668952 Test RE 0.032917551893493546\n",
      "52 Train Loss 1204.4238 Test MSE 5921.754568308119 Test RE 0.03439632355263387\n",
      "53 Train Loss 1203.5405 Test MSE 4680.7981537317155 Test RE 0.030580654811746773\n",
      "54 Train Loss 1200.0698 Test MSE 1969.2139145863027 Test RE 0.019835048721621285\n",
      "55 Train Loss 1195.8762 Test MSE 4186.602739389258 Test RE 0.028921292666855763\n",
      "56 Train Loss 1193.319 Test MSE 4395.627111615561 Test RE 0.029634475518793845\n",
      "57 Train Loss 1191.9022 Test MSE 4845.460299096588 Test RE 0.031113892254020323\n",
      "58 Train Loss 1187.2297 Test MSE 6174.291050449535 Test RE 0.03512209171072064\n",
      "59 Train Loss 1183.3171 Test MSE 2916.5687393327376 Test RE 0.024139203276590953\n",
      "60 Train Loss 1178.7849 Test MSE 1617.349875015233 Test RE 0.0179758242492921\n",
      "61 Train Loss 1172.7476 Test MSE 3343.039202075197 Test RE 0.025843870628140427\n",
      "62 Train Loss 1166.6898 Test MSE 8674.277819870464 Test RE 0.04162971977180526\n",
      "63 Train Loss 1164.7703 Test MSE 6541.603958021418 Test RE 0.03615171856377124\n",
      "64 Train Loss 1161.8718 Test MSE 5367.900923650515 Test RE 0.03274832331039804\n",
      "65 Train Loss 1155.948 Test MSE 4233.855619403835 Test RE 0.0290840475218885\n",
      "66 Train Loss 1148.483 Test MSE 3641.3073225281046 Test RE 0.026972145320056127\n",
      "67 Train Loss 1128.1755 Test MSE 4180.188869551527 Test RE 0.02889913048451202\n",
      "68 Train Loss 1118.975 Test MSE 3561.391538563131 Test RE 0.02667452440716911\n",
      "69 Train Loss 1116.1359 Test MSE 4237.599856494041 Test RE 0.029096905010880518\n",
      "70 Train Loss 1112.9352 Test MSE 4990.146497092797 Test RE 0.03157500815620855\n",
      "71 Train Loss 1106.7938 Test MSE 5838.282330969145 Test RE 0.034153040266764426\n",
      "72 Train Loss 1101.6844 Test MSE 7997.594836145581 Test RE 0.03997297969826937\n",
      "73 Train Loss 1090.4225 Test MSE 10800.301293248867 Test RE 0.046452036560693834\n",
      "74 Train Loss 1086.6156 Test MSE 9512.748671800598 Test RE 0.043595316507364985\n",
      "75 Train Loss 1076.8793 Test MSE 5037.296168238075 Test RE 0.031723826545719716\n",
      "76 Train Loss 1067.274 Test MSE 2480.652852456664 Test RE 0.022262289185034912\n",
      "77 Train Loss 1063.5358 Test MSE 2165.3822519532323 Test RE 0.02079955819063267\n",
      "78 Train Loss 1061.0092 Test MSE 2063.631567586194 Test RE 0.020304995847249947\n",
      "79 Train Loss 1053.4353 Test MSE 3397.4006645966397 Test RE 0.02605314804684875\n",
      "80 Train Loss 1050.4266 Test MSE 2933.320450208779 Test RE 0.02420842742470354\n",
      "81 Train Loss 1042.8438 Test MSE 4401.435133696627 Test RE 0.029654047341627965\n",
      "82 Train Loss 1039.15 Test MSE 7031.928059317622 Test RE 0.037482110197415985\n",
      "83 Train Loss 1033.878 Test MSE 9202.711219741577 Test RE 0.04287900713674129\n",
      "84 Train Loss 1028.0624 Test MSE 8694.570803677634 Test RE 0.04167838651608628\n",
      "85 Train Loss 1025.1027 Test MSE 9546.407991091051 Test RE 0.043672375882661776\n",
      "86 Train Loss 1024.4156 Test MSE 9731.223922044801 Test RE 0.044093092221068664\n",
      "87 Train Loss 1020.2035 Test MSE 5969.068278803414 Test RE 0.03453346026667434\n",
      "88 Train Loss 1015.7585 Test MSE 3514.7513082302094 Test RE 0.026499283063263233\n",
      "89 Train Loss 1012.61365 Test MSE 4687.074734978037 Test RE 0.0306011510665023\n",
      "90 Train Loss 1010.8051 Test MSE 4075.9767493678555 Test RE 0.02853662923407605\n",
      "91 Train Loss 1008.12445 Test MSE 3289.429564206526 Test RE 0.02563581443898433\n",
      "92 Train Loss 1005.7064 Test MSE 2943.7528097958193 Test RE 0.024251437875333206\n",
      "93 Train Loss 1003.92505 Test MSE 3155.7847916283376 Test RE 0.025109641553767055\n",
      "94 Train Loss 1002.5571 Test MSE 2981.9587527618773 Test RE 0.024408306016931815\n",
      "95 Train Loss 1000.4918 Test MSE 2195.6307177083672 Test RE 0.020944330032567425\n",
      "96 Train Loss 997.9613 Test MSE 1973.1911283585403 Test RE 0.01985506900382008\n",
      "97 Train Loss 995.57776 Test MSE 2293.8531450585647 Test RE 0.02140768120965022\n",
      "98 Train Loss 990.686 Test MSE 3300.134648061661 Test RE 0.02567749501339791\n",
      "99 Train Loss 989.6261 Test MSE 3147.4290899558127 Test RE 0.025076377603319634\n",
      "100 Train Loss 987.1301 Test MSE 3732.324060774149 Test RE 0.027307157543968064\n",
      "101 Train Loss 985.8754 Test MSE 4114.046406249015 Test RE 0.02866958567973226\n",
      "102 Train Loss 985.0531 Test MSE 4665.075652076856 Test RE 0.03052925237914951\n",
      "103 Train Loss 983.8123 Test MSE 5818.052918110494 Test RE 0.0340938194691483\n",
      "104 Train Loss 982.4189 Test MSE 6084.069441902357 Test RE 0.03486453721329911\n",
      "105 Train Loss 979.68005 Test MSE 5602.44035473174 Test RE 0.03345611008222943\n",
      "106 Train Loss 978.38153 Test MSE 5659.684727866291 Test RE 0.03362659888422475\n",
      "107 Train Loss 977.2861 Test MSE 6994.523678736757 Test RE 0.03738228946129987\n",
      "108 Train Loss 973.5074 Test MSE 10626.500977613818 Test RE 0.0460767635965981\n",
      "109 Train Loss 969.0262 Test MSE 7858.652660538337 Test RE 0.0396242331794367\n",
      "110 Train Loss 966.0757 Test MSE 8931.321506355622 Test RE 0.042242020721400146\n",
      "111 Train Loss 962.6236 Test MSE 7687.018274918366 Test RE 0.03918914427513845\n",
      "112 Train Loss 953.47754 Test MSE 4754.878628202892 Test RE 0.030821696632669596\n",
      "113 Train Loss 950.77747 Test MSE 4121.683834496863 Test RE 0.028696184842018455\n",
      "114 Train Loss 947.49207 Test MSE 5179.882956139127 Test RE 0.032169684153345694\n",
      "115 Train Loss 943.88904 Test MSE 5224.626119806553 Test RE 0.03230832420825217\n",
      "116 Train Loss 942.32947 Test MSE 5716.01725591662 Test RE 0.03379353230603794\n",
      "117 Train Loss 940.76624 Test MSE 7033.098640490193 Test RE 0.0374852298273611\n",
      "118 Train Loss 937.8647 Test MSE 9437.254521655761 Test RE 0.04342198347125731\n",
      "119 Train Loss 934.8684 Test MSE 8996.432911383894 Test RE 0.042395718171239445\n",
      "120 Train Loss 932.82 Test MSE 7377.830977780293 Test RE 0.03839292269527952\n",
      "121 Train Loss 927.67725 Test MSE 7334.09332144681 Test RE 0.03827895203281282\n",
      "122 Train Loss 924.9608 Test MSE 7173.043361294868 Test RE 0.037856333744421614\n",
      "123 Train Loss 921.8974 Test MSE 8170.6652825620085 Test RE 0.040403178619011534\n",
      "124 Train Loss 918.0011 Test MSE 5981.272741220322 Test RE 0.0345687461007365\n",
      "125 Train Loss 914.09344 Test MSE 4321.67745407462 Test RE 0.029384140970622\n",
      "126 Train Loss 906.7039 Test MSE 2041.0955609777004 Test RE 0.020193820555527875\n",
      "127 Train Loss 898.9689 Test MSE 3837.0170616769033 Test RE 0.027687496509898785\n",
      "128 Train Loss 894.19867 Test MSE 3069.571023029158 Test RE 0.024764277782312393\n",
      "129 Train Loss 890.4002 Test MSE 4647.300819301207 Test RE 0.030471035719249716\n",
      "130 Train Loss 888.46643 Test MSE 5490.168422232008 Test RE 0.03311918624578667\n",
      "131 Train Loss 886.3574 Test MSE 5110.523258147682 Test RE 0.03195357894886819\n",
      "132 Train Loss 884.56555 Test MSE 3490.303455402946 Test RE 0.02640696058751372\n",
      "133 Train Loss 882.4157 Test MSE 2861.292803614795 Test RE 0.023909361308148358\n",
      "134 Train Loss 879.37274 Test MSE 3382.5672366124504 Test RE 0.025996210363311605\n",
      "135 Train Loss 878.51575 Test MSE 3603.56366014207 Test RE 0.02683199244534569\n",
      "136 Train Loss 877.54236 Test MSE 3771.639058846201 Test RE 0.027450602812961293\n",
      "137 Train Loss 875.9649 Test MSE 3933.869896759103 Test RE 0.02803475854131934\n",
      "138 Train Loss 875.0539 Test MSE 4094.65312656962 Test RE 0.02860193281211487\n",
      "139 Train Loss 872.90265 Test MSE 4186.082334948935 Test RE 0.02891949511908241\n",
      "140 Train Loss 871.68176 Test MSE 3727.8847283081905 Test RE 0.027290912756833104\n",
      "141 Train Loss 871.21515 Test MSE 3385.7322563908956 Test RE 0.026008369660747706\n",
      "142 Train Loss 870.48395 Test MSE 3034.435698393033 Test RE 0.024622139817245068\n",
      "143 Train Loss 868.37103 Test MSE 3247.1413357920906 Test RE 0.025470497025622887\n",
      "144 Train Loss 867.0889 Test MSE 3433.431649351852 Test RE 0.026190936446502798\n",
      "145 Train Loss 864.8594 Test MSE 3031.6391415618746 Test RE 0.024610791235122307\n",
      "146 Train Loss 861.7905 Test MSE 3185.1754261048854 Test RE 0.025226296844253683\n",
      "147 Train Loss 861.2582 Test MSE 3303.6727627072396 Test RE 0.025691255904023852\n",
      "148 Train Loss 859.8445 Test MSE 2337.4183143155583 Test RE 0.02161001383489202\n",
      "149 Train Loss 858.31757 Test MSE 2012.2297917631383 Test RE 0.02005051864465783\n",
      "150 Train Loss 856.88055 Test MSE 2418.0834956888707 Test RE 0.021979735913692715\n",
      "151 Train Loss 854.62946 Test MSE 2909.8936468271686 Test RE 0.024111563997440435\n",
      "152 Train Loss 853.4302 Test MSE 2359.0070162435172 Test RE 0.02170958091980295\n",
      "153 Train Loss 852.5843 Test MSE 2106.0156217331246 Test RE 0.020512453890096045\n",
      "154 Train Loss 852.11536 Test MSE 2004.350487489666 Test RE 0.02001122415245781\n",
      "155 Train Loss 851.9322 Test MSE 1940.6945161416882 Test RE 0.019690893034097327\n",
      "156 Train Loss 851.1748 Test MSE 1786.7300590226405 Test RE 0.018893668692058718\n",
      "157 Train Loss 850.08203 Test MSE 1994.4674503474496 Test RE 0.019961827585218528\n",
      "158 Train Loss 849.5655 Test MSE 2206.3124633056073 Test RE 0.020995215311510072\n",
      "159 Train Loss 848.6319 Test MSE 2137.774743954162 Test RE 0.02066654103054089\n",
      "160 Train Loss 847.35156 Test MSE 1984.1239290047743 Test RE 0.019909998214132422\n",
      "161 Train Loss 846.7685 Test MSE 1850.80115408246 Test RE 0.019229442992252814\n",
      "162 Train Loss 846.33435 Test MSE 1725.716292593706 Test RE 0.01856827345353823\n",
      "163 Train Loss 845.07263 Test MSE 1851.1287069866232 Test RE 0.019231144520595204\n",
      "164 Train Loss 844.13873 Test MSE 2111.715500192112 Test RE 0.020540193354726156\n",
      "165 Train Loss 842.02606 Test MSE 2007.9908146173186 Test RE 0.02002938823004806\n",
      "166 Train Loss 840.23236 Test MSE 1385.6238997771986 Test RE 0.016638321293435118\n",
      "167 Train Loss 839.2749 Test MSE 1618.6547400666655 Test RE 0.0179830741635097\n",
      "168 Train Loss 836.93335 Test MSE 2401.0788045946197 Test RE 0.021902315502724982\n",
      "169 Train Loss 834.3059 Test MSE 1988.0927230051077 Test RE 0.019929901004728537\n",
      "170 Train Loss 832.2043 Test MSE 2199.565108549025 Test RE 0.02096308689766213\n",
      "171 Train Loss 831.2633 Test MSE 2698.792277519925 Test RE 0.023220499251715956\n",
      "172 Train Loss 828.95825 Test MSE 3009.7269504828737 Test RE 0.02452168855456037\n",
      "173 Train Loss 828.27637 Test MSE 3241.3474988990915 Test RE 0.025447763523044953\n",
      "174 Train Loss 827.5893 Test MSE 3655.8875679507187 Test RE 0.02702609127734374\n",
      "175 Train Loss 823.2245 Test MSE 3133.807078338903 Test RE 0.025022053725210647\n",
      "176 Train Loss 821.3997 Test MSE 2635.816733082489 Test RE 0.022947978219455\n",
      "177 Train Loss 820.23676 Test MSE 2753.3459448427366 Test RE 0.023454015864972064\n",
      "178 Train Loss 818.282 Test MSE 4274.751039242261 Test RE 0.029224173458143224\n",
      "179 Train Loss 817.0313 Test MSE 5104.212752948548 Test RE 0.03193384461816162\n",
      "180 Train Loss 816.3198 Test MSE 4834.283785245694 Test RE 0.031077987965833498\n",
      "181 Train Loss 815.7383 Test MSE 4285.4867002798255 Test RE 0.029260847416617258\n",
      "182 Train Loss 814.3871 Test MSE 2949.045339085647 Test RE 0.02427322873466815\n",
      "183 Train Loss 813.41974 Test MSE 2899.692015175405 Test RE 0.02406926120349933\n",
      "184 Train Loss 812.2485 Test MSE 2852.613307462116 Test RE 0.02387307022619483\n",
      "185 Train Loss 811.0991 Test MSE 2521.9221707556358 Test RE 0.022446708332639814\n",
      "186 Train Loss 809.04803 Test MSE 1858.3140880875708 Test RE 0.01926843238390264\n",
      "187 Train Loss 807.0508 Test MSE 2259.814949824643 Test RE 0.02124825464143946\n",
      "188 Train Loss 806.0842 Test MSE 2664.3947754414976 Test RE 0.023072046076028436\n",
      "189 Train Loss 805.66205 Test MSE 2547.354566342191 Test RE 0.022559606648162724\n",
      "190 Train Loss 804.16376 Test MSE 2262.9374982183167 Test RE 0.021262929692495044\n",
      "191 Train Loss 802.7881 Test MSE 2118.41631378045 Test RE 0.020572756215184163\n",
      "192 Train Loss 801.7043 Test MSE 2223.086690106044 Test RE 0.021074875752438722\n",
      "193 Train Loss 801.1689 Test MSE 2386.8654979469397 Test RE 0.021837393353989227\n",
      "194 Train Loss 798.79663 Test MSE 2119.5835708357895 Test RE 0.020578423275922162\n",
      "195 Train Loss 797.25116 Test MSE 1542.0810882552623 Test RE 0.01755255850246879\n",
      "196 Train Loss 795.81665 Test MSE 1261.878115501076 Test RE 0.015877990140178926\n",
      "197 Train Loss 795.06116 Test MSE 1358.986470314705 Test RE 0.01647761647361283\n",
      "198 Train Loss 794.1774 Test MSE 1785.4329947194867 Test RE 0.018886809583081223\n",
      "199 Train Loss 793.5258 Test MSE 1739.4623211573319 Test RE 0.018642078678688674\n",
      "Training time: 45.91\n",
      "Training time: 45.91\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 4567.672 Test MSE 4973501.292832172 Test RE 0.9968227502760423\n",
      "1 Train Loss 4419.859 Test MSE 4194838.081583939 Test RE 0.9154706484436045\n",
      "2 Train Loss 3737.1282 Test MSE 2575025.095706011 Test RE 0.717261560320842\n",
      "3 Train Loss 2575.9473 Test MSE 968582.9823237682 Test RE 0.4399012953714065\n",
      "4 Train Loss 2093.0317 Test MSE 842828.6701503216 Test RE 0.4103519238420659\n",
      "5 Train Loss 1724.9905 Test MSE 101068.9703655476 Test RE 0.14210054524718435\n",
      "6 Train Loss 1592.9353 Test MSE 125003.39009650025 Test RE 0.15803297624610466\n",
      "7 Train Loss 1520.7903 Test MSE 213890.32607126387 Test RE 0.2067199517411202\n",
      "8 Train Loss 1438.7687 Test MSE 235533.47932775374 Test RE 0.21692676885421888\n",
      "9 Train Loss 1393.84 Test MSE 372247.03520704596 Test RE 0.2727108690757599\n",
      "10 Train Loss 1299.0065 Test MSE 100900.67402560318 Test RE 0.1419821856484078\n",
      "11 Train Loss 1270.8724 Test MSE 21988.106720479398 Test RE 0.06627973170170057\n",
      "12 Train Loss 1259.9382 Test MSE 23926.83790980959 Test RE 0.06914001627101297\n",
      "13 Train Loss 1257.9457 Test MSE 18451.89075138027 Test RE 0.06071657440905832\n",
      "14 Train Loss 1253.6726 Test MSE 13051.2377392697 Test RE 0.051063746338035516\n",
      "15 Train Loss 1248.8479 Test MSE 9672.115752430636 Test RE 0.04395897591068393\n",
      "16 Train Loss 1246.5199 Test MSE 6174.037798025547 Test RE 0.03512137139757051\n",
      "17 Train Loss 1246.0728 Test MSE 5133.484318394323 Test RE 0.03202528058919116\n",
      "18 Train Loss 1245.2926 Test MSE 3763.1643760504917 Test RE 0.027419745409237736\n",
      "19 Train Loss 1244.2625 Test MSE 3375.027084129363 Test RE 0.025967219846074297\n",
      "20 Train Loss 1242.8103 Test MSE 2376.082552038334 Test RE 0.021788011020460758\n",
      "21 Train Loss 1241.8853 Test MSE 2531.8427298137067 Test RE 0.022490814636432606\n",
      "22 Train Loss 1241.5936 Test MSE 2848.048056625931 Test RE 0.023853959645505286\n",
      "23 Train Loss 1240.7158 Test MSE 2249.8502010380143 Test RE 0.021201355348275264\n",
      "24 Train Loss 1240.0328 Test MSE 1514.1112694130752 Test RE 0.01739264848024462\n",
      "25 Train Loss 1238.7776 Test MSE 1542.2938111416247 Test RE 0.017553769107468385\n",
      "26 Train Loss 1238.4984 Test MSE 1491.875630461092 Test RE 0.01726446535140063\n",
      "27 Train Loss 1237.9493 Test MSE 1509.362615007131 Test RE 0.017365353083399114\n",
      "28 Train Loss 1236.3203 Test MSE 2353.493934747504 Test RE 0.021684198057892946\n",
      "29 Train Loss 1234.9143 Test MSE 3503.775222383493 Test RE 0.026457873897545687\n",
      "30 Train Loss 1233.2429 Test MSE 2577.9646978932306 Test RE 0.022694744962220122\n",
      "31 Train Loss 1232.4957 Test MSE 2426.640664721896 Test RE 0.022018592762590286\n",
      "32 Train Loss 1231.5132 Test MSE 2283.1878950048613 Test RE 0.021357855813595547\n",
      "33 Train Loss 1230.627 Test MSE 1617.5850570414343 Test RE 0.017977131151762773\n",
      "34 Train Loss 1229.4146 Test MSE 1688.4472296709191 Test RE 0.018366676200969434\n",
      "35 Train Loss 1228.3517 Test MSE 1484.698489755192 Test RE 0.01722288719282725\n",
      "36 Train Loss 1227.914 Test MSE 1422.438991172901 Test RE 0.01685790677655715\n",
      "37 Train Loss 1227.6438 Test MSE 1482.8710011791172 Test RE 0.017212284258573962\n",
      "38 Train Loss 1226.9967 Test MSE 1628.3489584686079 Test RE 0.018036844619299122\n",
      "39 Train Loss 1225.6158 Test MSE 1540.8788753365002 Test RE 0.01754571514412055\n",
      "40 Train Loss 1225.0273 Test MSE 1681.936805025313 Test RE 0.01833123228889815\n",
      "41 Train Loss 1224.283 Test MSE 2071.2407152908413 Test RE 0.020342396309879232\n",
      "42 Train Loss 1222.906 Test MSE 2606.488081904525 Test RE 0.022819950361147326\n",
      "43 Train Loss 1222.1559 Test MSE 2284.1523604792123 Test RE 0.02136236633585515\n",
      "44 Train Loss 1221.0404 Test MSE 2136.8959691069654 Test RE 0.02066229289746333\n",
      "45 Train Loss 1219.51 Test MSE 2186.0339498465555 Test RE 0.020898507666855186\n",
      "46 Train Loss 1217.3508 Test MSE 2460.280614438397 Test RE 0.022170686756563675\n",
      "47 Train Loss 1214.1633 Test MSE 1981.1481448382128 Test RE 0.01989506212868998\n",
      "48 Train Loss 1211.5513 Test MSE 2298.7722024608875 Test RE 0.021430622786729463\n",
      "49 Train Loss 1210.6409 Test MSE 2414.4075858960728 Test RE 0.021963023038428866\n",
      "50 Train Loss 1210.052 Test MSE 2093.6210432717853 Test RE 0.020452003626302697\n",
      "51 Train Loss 1207.4182 Test MSE 2221.261426162668 Test RE 0.021066222219019107\n",
      "52 Train Loss 1205.1061 Test MSE 1989.0186684863254 Test RE 0.019934541596524264\n",
      "53 Train Loss 1204.3646 Test MSE 2160.256638570079 Test RE 0.020774926591025283\n",
      "54 Train Loss 1203.3495 Test MSE 2988.032752721036 Test RE 0.024433152207396232\n",
      "55 Train Loss 1199.1992 Test MSE 4319.415661775716 Test RE 0.029376450725055205\n",
      "56 Train Loss 1196.5028 Test MSE 3667.881645386603 Test RE 0.027070387989698342\n",
      "57 Train Loss 1193.4495 Test MSE 2726.60257705699 Test RE 0.02333983298932347\n",
      "58 Train Loss 1191.7471 Test MSE 2911.650420978889 Test RE 0.024118841270466606\n",
      "59 Train Loss 1190.2529 Test MSE 3528.6370501011565 Test RE 0.026551576875945917\n",
      "60 Train Loss 1187.7725 Test MSE 5328.544789750594 Test RE 0.032628051113318554\n",
      "61 Train Loss 1183.5671 Test MSE 8669.479257745119 Test RE 0.041618203514469765\n",
      "62 Train Loss 1180.5994 Test MSE 9130.610595960852 Test RE 0.04271070442770007\n",
      "63 Train Loss 1178.2866 Test MSE 7815.487751552788 Test RE 0.03951526211313111\n",
      "64 Train Loss 1173.3285 Test MSE 4647.36916345917 Test RE 0.03047125977507298\n",
      "65 Train Loss 1165.8179 Test MSE 3674.144590274158 Test RE 0.02709348961254335\n",
      "66 Train Loss 1161.8599 Test MSE 3361.3641015356616 Test RE 0.025914605531781397\n",
      "67 Train Loss 1156.0894 Test MSE 2070.8282415996855 Test RE 0.020340370683159307\n",
      "68 Train Loss 1153.1267 Test MSE 2606.863715551125 Test RE 0.022821594648803026\n",
      "69 Train Loss 1148.4049 Test MSE 3341.5455770527383 Test RE 0.02583809663596068\n",
      "70 Train Loss 1143.2527 Test MSE 2013.9734421275355 Test RE 0.020059203916063088\n",
      "71 Train Loss 1137.8889 Test MSE 2095.226207293633 Test RE 0.020459842325313605\n",
      "72 Train Loss 1133.5574 Test MSE 2257.7107719526825 Test RE 0.02123835991094424\n",
      "73 Train Loss 1126.9723 Test MSE 4562.414638516903 Test RE 0.03019146587420652\n",
      "74 Train Loss 1122.1691 Test MSE 5627.874934673782 Test RE 0.03353196796186831\n",
      "75 Train Loss 1114.7476 Test MSE 10060.748559612548 Test RE 0.04483343053861039\n",
      "76 Train Loss 1111.0406 Test MSE 10160.092854831559 Test RE 0.045054239379346155\n",
      "77 Train Loss 1108.8938 Test MSE 14522.2257190748 Test RE 0.05386459701118364\n",
      "78 Train Loss 1105.8177 Test MSE 11079.614103687823 Test RE 0.04704886389955473\n",
      "79 Train Loss 1101.5947 Test MSE 9960.264426373116 Test RE 0.04460897637789384\n",
      "80 Train Loss 1090.3469 Test MSE 10870.7302616903 Test RE 0.04660324775179008\n",
      "81 Train Loss 1081.3883 Test MSE 7589.595456089398 Test RE 0.03894001705837504\n",
      "82 Train Loss 1075.536 Test MSE 9878.26218668264 Test RE 0.04442496538853017\n",
      "83 Train Loss 1067.7704 Test MSE 6597.882423609203 Test RE 0.036306895036286035\n",
      "84 Train Loss 1065.7064 Test MSE 5580.975774290638 Test RE 0.03339195852733377\n",
      "85 Train Loss 1061.8994 Test MSE 7053.067362486201 Test RE 0.03753840706879999\n",
      "86 Train Loss 1061.0741 Test MSE 7279.355663251027 Test RE 0.038135837928474615\n",
      "87 Train Loss 1059.3064 Test MSE 8021.898744408065 Test RE 0.04003367061208227\n",
      "88 Train Loss 1054.2125 Test MSE 12355.283299184439 Test RE 0.04968361384169844\n",
      "89 Train Loss 1051.4128 Test MSE 14258.931478985083 Test RE 0.05337406923788666\n",
      "90 Train Loss 1045.1299 Test MSE 14876.676476050956 Test RE 0.05451798337866814\n",
      "91 Train Loss 1040.6896 Test MSE 12777.885026491336 Test RE 0.05052616217476879\n",
      "92 Train Loss 1036.5934 Test MSE 16002.848675105763 Test RE 0.05654386265314737\n",
      "93 Train Loss 1034.2904 Test MSE 17001.297177731372 Test RE 0.058281115160405565\n",
      "94 Train Loss 1032.056 Test MSE 17867.589326764704 Test RE 0.05974750917914954\n",
      "95 Train Loss 1028.0403 Test MSE 17953.068169616945 Test RE 0.05989025519406621\n",
      "96 Train Loss 1019.71747 Test MSE 15298.148394422838 Test RE 0.055284865622893496\n",
      "97 Train Loss 1011.7436 Test MSE 15063.68209794216 Test RE 0.054859569414168675\n",
      "98 Train Loss 1006.3011 Test MSE 11150.465555605953 Test RE 0.04719905722121969\n",
      "99 Train Loss 999.4049 Test MSE 10687.923168567166 Test RE 0.046209735773467875\n",
      "100 Train Loss 996.7537 Test MSE 8225.001257478758 Test RE 0.04053729917568924\n",
      "101 Train Loss 992.9842 Test MSE 5190.354394162429 Test RE 0.032202184190657304\n",
      "102 Train Loss 987.9824 Test MSE 4353.194166833883 Test RE 0.02949109125108523\n",
      "103 Train Loss 982.6752 Test MSE 5263.145174163987 Test RE 0.03242720359900183\n",
      "104 Train Loss 978.8351 Test MSE 2353.3820240399245 Test RE 0.021683682500414615\n",
      "105 Train Loss 976.51514 Test MSE 1851.202834411572 Test RE 0.019231529567011924\n",
      "106 Train Loss 973.7033 Test MSE 2020.8762931482986 Test RE 0.0200935507572811\n",
      "107 Train Loss 972.128 Test MSE 2175.3087361764956 Test RE 0.020847178052527063\n",
      "108 Train Loss 970.3123 Test MSE 2021.6551280451126 Test RE 0.02009742235770024\n",
      "109 Train Loss 966.5176 Test MSE 1802.377771829538 Test RE 0.018976221237911844\n",
      "110 Train Loss 963.48016 Test MSE 2094.4863161889334 Test RE 0.020456229495358597\n",
      "111 Train Loss 961.4745 Test MSE 2015.6474413408562 Test RE 0.020067538712366974\n",
      "112 Train Loss 960.16614 Test MSE 1897.6202360571706 Test RE 0.019471144291883083\n",
      "113 Train Loss 958.8127 Test MSE 1927.6376357721615 Test RE 0.019624541649407936\n",
      "114 Train Loss 957.5185 Test MSE 2223.3817942115816 Test RE 0.021076274500405823\n",
      "115 Train Loss 956.3122 Test MSE 2470.844216351831 Test RE 0.02221823244094934\n",
      "116 Train Loss 955.02765 Test MSE 2469.917219535052 Test RE 0.022214064197001268\n",
      "117 Train Loss 953.43036 Test MSE 3094.821652959709 Test RE 0.02486592600883125\n",
      "118 Train Loss 952.5373 Test MSE 3473.0717186127813 Test RE 0.02634169393882159\n",
      "119 Train Loss 950.9366 Test MSE 3727.6104091201796 Test RE 0.02728990862723928\n",
      "120 Train Loss 949.02997 Test MSE 3923.1063557204834 Test RE 0.027996379035493686\n",
      "121 Train Loss 945.4286 Test MSE 3966.9298229182596 Test RE 0.028152313006270024\n",
      "122 Train Loss 941.9644 Test MSE 3893.0052323194773 Test RE 0.027888767219682205\n",
      "123 Train Loss 938.4251 Test MSE 2057.917605113151 Test RE 0.02027686524234194\n",
      "124 Train Loss 936.8012 Test MSE 2075.1458740317553 Test RE 0.020361564260000926\n",
      "125 Train Loss 934.67206 Test MSE 2949.165813673804 Test RE 0.02427372453534827\n",
      "126 Train Loss 933.3096 Test MSE 4346.424067972803 Test RE 0.02946815001723907\n",
      "127 Train Loss 927.6436 Test MSE 8511.53707756682 Test RE 0.04123735688461428\n",
      "128 Train Loss 925.1632 Test MSE 8999.86192254977 Test RE 0.04240379701440563\n",
      "129 Train Loss 921.5376 Test MSE 11164.575075300092 Test RE 0.04722891003257784\n",
      "130 Train Loss 917.56824 Test MSE 14013.625520016783 Test RE 0.05291296250169198\n",
      "131 Train Loss 916.4238 Test MSE 14172.439206307849 Test RE 0.0532119439582373\n",
      "132 Train Loss 912.4356 Test MSE 11374.530594261096 Test RE 0.04767092348991017\n",
      "133 Train Loss 910.0422 Test MSE 9510.315022664206 Test RE 0.043589739649691095\n",
      "134 Train Loss 905.8654 Test MSE 6347.333719971786 Test RE 0.03561086229162884\n",
      "135 Train Loss 904.09143 Test MSE 7738.817079028747 Test RE 0.03932096018306295\n",
      "136 Train Loss 903.0762 Test MSE 8345.071741944113 Test RE 0.040832113600087615\n",
      "137 Train Loss 901.4636 Test MSE 8753.540591328558 Test RE 0.04181948677061643\n",
      "138 Train Loss 897.3921 Test MSE 8750.19391964435 Test RE 0.04181149175017892\n",
      "139 Train Loss 892.5002 Test MSE 7496.797882670224 Test RE 0.0387012261506533\n",
      "140 Train Loss 888.3272 Test MSE 7355.555604038024 Test RE 0.0383349203362435\n",
      "141 Train Loss 882.3513 Test MSE 8151.562289077495 Test RE 0.04035591971850665\n",
      "142 Train Loss 879.21 Test MSE 10825.208817067272 Test RE 0.04650556927593455\n",
      "143 Train Loss 876.5622 Test MSE 11889.301434389194 Test RE 0.04873769576979761\n",
      "144 Train Loss 872.07776 Test MSE 7002.231670977286 Test RE 0.03740288150376997\n",
      "145 Train Loss 869.1166 Test MSE 4478.631073780365 Test RE 0.02991296536742282\n",
      "146 Train Loss 864.8126 Test MSE 4351.130619728823 Test RE 0.029484100583165897\n",
      "147 Train Loss 862.1575 Test MSE 6317.450851776126 Test RE 0.035526936488887155\n",
      "148 Train Loss 859.5969 Test MSE 5059.33702794512 Test RE 0.03179315512896023\n",
      "149 Train Loss 856.8754 Test MSE 3997.8453958251207 Test RE 0.028261800163787803\n",
      "150 Train Loss 851.6993 Test MSE 3704.8893225753136 Test RE 0.027206610741565773\n",
      "151 Train Loss 846.6296 Test MSE 4387.083706604029 Test RE 0.0296056625120617\n",
      "152 Train Loss 841.83496 Test MSE 3399.507408598467 Test RE 0.02606122463428451\n",
      "153 Train Loss 839.4585 Test MSE 3177.0399194866286 Test RE 0.025194060013817035\n",
      "154 Train Loss 835.2354 Test MSE 2940.0625727846764 Test RE 0.024236232519468295\n",
      "155 Train Loss 832.4887 Test MSE 2825.591232857935 Test RE 0.023759729438221574\n",
      "156 Train Loss 830.45294 Test MSE 3243.731724114968 Test RE 0.025457121058192735\n",
      "157 Train Loss 828.40326 Test MSE 3028.33624674626 Test RE 0.024597381161718435\n",
      "158 Train Loss 827.43195 Test MSE 3012.680530217481 Test RE 0.024533717719227476\n",
      "159 Train Loss 824.8997 Test MSE 3141.2560869271215 Test RE 0.025051774582091842\n",
      "160 Train Loss 822.7657 Test MSE 4206.362924678926 Test RE 0.028989464569663396\n",
      "161 Train Loss 819.5095 Test MSE 6079.026951182518 Test RE 0.03485008631420573\n",
      "162 Train Loss 818.22833 Test MSE 6329.306788153479 Test RE 0.03556025749725603\n",
      "163 Train Loss 817.04407 Test MSE 6263.362406340068 Test RE 0.03537452315225528\n",
      "164 Train Loss 815.0726 Test MSE 6508.727663019968 Test RE 0.036060759859462\n",
      "165 Train Loss 813.1747 Test MSE 5888.129392334402 Test RE 0.034298529145868555\n",
      "166 Train Loss 811.26416 Test MSE 4530.209219761044 Test RE 0.0300847185876055\n",
      "167 Train Loss 809.74396 Test MSE 3659.045644493301 Test RE 0.027037761773103415\n",
      "168 Train Loss 808.21783 Test MSE 3715.5255079724193 Test RE 0.027245635816334792\n",
      "169 Train Loss 807.16266 Test MSE 3223.456210774881 Test RE 0.025377434241665348\n",
      "170 Train Loss 805.65295 Test MSE 2475.7569265861066 Test RE 0.02224030941674862\n",
      "171 Train Loss 804.5015 Test MSE 2189.877638899127 Test RE 0.02091687245171576\n",
      "172 Train Loss 802.128 Test MSE 2659.018316055028 Test RE 0.02304875587878535\n",
      "173 Train Loss 798.95044 Test MSE 4103.94174533331 Test RE 0.028634355821391136\n",
      "174 Train Loss 795.6118 Test MSE 5277.751565577876 Test RE 0.03247216875298917\n",
      "175 Train Loss 793.7401 Test MSE 4809.119814026452 Test RE 0.03099699706983428\n",
      "176 Train Loss 789.2134 Test MSE 6924.945266305533 Test RE 0.037195893557435036\n",
      "177 Train Loss 786.2585 Test MSE 7297.451734323996 Test RE 0.0381832102856426\n",
      "178 Train Loss 784.1103 Test MSE 6768.8051681826455 Test RE 0.036774165857087225\n",
      "179 Train Loss 782.5186 Test MSE 7179.327435285544 Test RE 0.03787291247611899\n",
      "180 Train Loss 781.40466 Test MSE 8674.273683829575 Test RE 0.0416297098469304\n",
      "181 Train Loss 778.07916 Test MSE 8047.691072247967 Test RE 0.040097977887598446\n",
      "182 Train Loss 776.19775 Test MSE 6323.308950114743 Test RE 0.035543404528606215\n",
      "183 Train Loss 771.39185 Test MSE 4382.884497373439 Test RE 0.02959149021185439\n",
      "184 Train Loss 765.75726 Test MSE 3483.3253202787832 Test RE 0.026380549770118555\n",
      "185 Train Loss 760.7378 Test MSE 2390.613211141706 Test RE 0.02185453051301627\n",
      "186 Train Loss 759.3661 Test MSE 2460.5050394238838 Test RE 0.022171697930362437\n",
      "187 Train Loss 758.3467 Test MSE 2592.9938223779754 Test RE 0.022760802204504257\n",
      "188 Train Loss 754.8174 Test MSE 2843.9700371487925 Test RE 0.023836875706452463\n",
      "189 Train Loss 752.2341 Test MSE 3320.601985714962 Test RE 0.025756997468945338\n",
      "190 Train Loss 748.4672 Test MSE 3481.286829043354 Test RE 0.026372829505063644\n",
      "191 Train Loss 746.04877 Test MSE 2929.8945298145063 Test RE 0.024194286389528044\n",
      "192 Train Loss 743.8072 Test MSE 2531.2931091830746 Test RE 0.022488373314443044\n",
      "193 Train Loss 740.9559 Test MSE 2226.8896921475707 Test RE 0.02109289429131046\n",
      "194 Train Loss 738.59296 Test MSE 1971.6479214962326 Test RE 0.019847303290828396\n",
      "195 Train Loss 736.63745 Test MSE 2005.8200017137542 Test RE 0.020018558545994908\n",
      "196 Train Loss 734.83295 Test MSE 2765.40070814327 Test RE 0.023505303248457957\n",
      "197 Train Loss 733.5924 Test MSE 3342.5643075745566 Test RE 0.025842034941032228\n",
      "198 Train Loss 731.161 Test MSE 2629.3500675946116 Test RE 0.022919810853473117\n",
      "199 Train Loss 728.8144 Test MSE 2122.9371745654544 Test RE 0.020594696425500963\n",
      "Training time: 45.22\n",
      "Training time: 45.22\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 200\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "\n",
    "for reps in range(max_reps):   \n",
    "  train_loss = []\n",
    "  test_mse_loss = []\n",
    "  test_re_loss = []   \n",
    "  beta_val = []\n",
    "\n",
    "  torch.manual_seed(reps*36)\n",
    "  N_f = 10000 #Total number of collocation points\n",
    "\n",
    "  layers = np.array([1,50,50,50,1]) #9 hidden layers\n",
    "  PINN = Sequentialmodel(layers)\n",
    "\n",
    "  PINN.to(device)\n",
    "\n",
    "  'Neural Network Summary'\n",
    "  print(PINN)\n",
    "\n",
    "  params = list(PINN.parameters())\n",
    "\n",
    "  optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.01, \n",
    "                            max_iter = 10, \n",
    "                            max_eval = 15, \n",
    "                            tolerance_grad = 1e-6, \n",
    "                            tolerance_change = 1e-6, \n",
    "                            history_size = 100, \n",
    "                            line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "  train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "  torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "  train_loss_full.append(train_loss)\n",
    "  test_mse_full.append(test_mse_loss)\n",
    "  test_re_full.append(test_re_loss)\n",
    "  beta_full.append(beta_val)\n",
    "\n",
    "\n",
    "  print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"beta\": beta_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "pmHEeBpzfFQh",
    "outputId": "990b2054-f35e-4c8e-c378-69ff2eb19f4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff47065a0d0>]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "u_pred = PINN.test()\n",
    "plt.plot(y_true)\n",
    "plt.plot(u_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "id": "Ky6HsA0AWWTD"
   },
   "outputs": [],
   "source": [
    "import scipy.io as sio"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "SF7H51LTWXDq",
    "outputId": "aefc6a9f-3ad4-417a-b9f7-438009e620af"
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '1D_FODE_stan_tune0.mat'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     38\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0mIOError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '1D_FODE_stan_tune0.mat'",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_32468/2016390461.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtune_reps\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0mlabel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"1D_FODE_stan_tune\"\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtune_reps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;34m\".mat\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msio\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloadmat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlabel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mre\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"test_re_loss\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mre\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36mloadmat\u001b[0;34m(file_name, mdict, appendmat, **kwargs)\u001b[0m\n\u001b[1;32m    222\u001b[0m     \"\"\"\n\u001b[1;32m    223\u001b[0m     \u001b[0mvariable_names\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpop\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'variable_names'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 224\u001b[0;31m     \u001b[0;32mwith\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    225\u001b[0m         \u001b[0mMR\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmat_reader_factory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    226\u001b[0m         \u001b[0mmatfile_dict\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMR\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_variables\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvariable_names\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/contextlib.py\u001b[0m in \u001b[0;36m__enter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    110\u001b[0m         \u001b[0;32mdel\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    111\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 112\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgen\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    113\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mStopIteration\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mRuntimeError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"generator didn't yield\"\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file_context\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m@\u001b[0m\u001b[0mcontextmanager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0m_open_file_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'rb'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m     \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mopened\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_open_file\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mappendmat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m         \u001b[0;32myield\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/scipy/io/matlab/mio.py\u001b[0m in \u001b[0;36m_open_file\u001b[0;34m(file_like, appendmat, mode)\u001b[0m\n\u001b[1;32m     43\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mappendmat\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mfile_like\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mendswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'.mat'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m                 \u001b[0mfile_like\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m'.mat'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile_like\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m             raise IOError(\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '1D_FODE_stan_tune0.mat'"
     ]
    }
   ],
   "source": [
    "for tune_reps in range(5):\n",
    "    label = \"1D_FODE_stan_tune\"+str(tune_reps)+\".mat\"\n",
    "    data = sio.loadmat(label)\n",
    "    re = np.array(data[\"test_re_loss\"])\n",
    "    print(np.mean(re[:,-1]))"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "stan_1D_FODE_tune.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
