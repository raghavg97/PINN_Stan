{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "#Trying to replicate the results of \"Adaptive activation functions accelerate convergence in deep and physics-informed neural networks\"\n",
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_disc(x): #[-4,3.75]\n",
    "    y  = np.zeros((np.shape(x)[0],))\n",
    "    \n",
    "    y = 0.2*np.sin(6*x)*np.exp(-1*x)\n",
    "    y[x>0] = 1 + 0.1*x[x>0]*np.exp(x[x>0]) + 10\n",
    "    \n",
    "    return scale*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain bounds\n",
    "level = \"medium\"\n",
    "label = \"Regr_disc_swish_\" + level\n",
    "loss_thresh = 0.1\n",
    "scale = 10.0\n",
    "\n",
    "lb = np.array(-4.0)  # [-1. 0.]\n",
    "ub = np.array(3.75) # [1.  0.99]\n",
    "\n",
    "x_test = np.linspace(lb,ub,1000).reshape(-1,1)\n",
    "u_true = true_disc(x_test)\n",
    "\n",
    "u_true_norm = np.linalg.norm(u_true,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_train,seed):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    x_train = np.random.uniform(low=lb, high=ub, size=(N_train,)).reshape(-1,1)\n",
    "\n",
    "    u_train = true_disc(x_train)\n",
    "\n",
    "    return x_train, u_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Sigmoid()\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "     \n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "\n",
    "        \n",
    "        self.beta = Parameter(torch.ones((layers[1],len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "            \n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = 2.0*(x - l_b)/(u_b - l_b)-1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = z*self.activation(self.beta[:,i]*z)\n",
    "    \n",
    "        a = self.linears[-1](a) \n",
    "        \n",
    "        return a\n",
    "    \n",
    "    \n",
    "    def loss(self,x,u):\n",
    "\n",
    "        loss_val = self.loss_function(self.forward(x),u)\n",
    "        \n",
    "        return loss_val\n",
    "        \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        u_pred = self.forward(x_test_tensor)       \n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "        \n",
    "        return u_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "        \n",
    "        test_mse_loss = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re_loss = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "        \n",
    "        return test_mse_loss, test_re_loss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    beta_val.append(model_NN.beta.cpu().detach().numpy())\n",
    "    test_mse, test_re = model_NN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step():\n",
    "    optimizer.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "    loss = model_NN.loss(x_train,u_train)\n",
    "    loss.backward() #backprop\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "   \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "    \n",
    "    loss_np = model_NN.loss(x_train,u_train).cpu().detach().numpy()\n",
    "    data_update(loss_np)\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        loss_np = train_step().cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1\n",
    "        data_update(loss_np)\n",
    "        if(i%100==0):        \n",
    "            print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 11620.754 Test MSE 10564.600947807769 Test RE 0.9829344460787268\n",
      "100 Train Loss 506.1575 Test MSE 551.2817236023128 Test RE 0.22453539091101254\n",
      "200 Train Loss 514.642 Test MSE 504.65370980888656 Test RE 0.2148299074054046\n",
      "300 Train Loss 491.19272 Test MSE 463.2532886121026 Test RE 0.20582933062518238\n",
      "400 Train Loss 493.86362 Test MSE 472.2321322862998 Test RE 0.20781446499262482\n",
      "500 Train Loss 482.33365 Test MSE 448.6027826652246 Test RE 0.20254847968629425\n",
      "600 Train Loss 495.0422 Test MSE 478.05116139191375 Test RE 0.20909093033087722\n",
      "700 Train Loss 479.85892 Test MSE 454.4921758448417 Test RE 0.20387370327372412\n",
      "800 Train Loss 479.1936 Test MSE 455.0899247798118 Test RE 0.2040077267535998\n",
      "900 Train Loss 479.08237 Test MSE 455.55996609883766 Test RE 0.20411305463268\n",
      "1000 Train Loss 479.0341 Test MSE 455.8452874537359 Test RE 0.20417696355990106\n",
      "1100 Train Loss 479.00693 Test MSE 455.8291965429744 Test RE 0.20417335990051932\n",
      "1200 Train Loss 571.69904 Test MSE 572.8213486903823 Test RE 0.2288798730476927\n",
      "1300 Train Loss 479.4245 Test MSE 454.79470965772293 Test RE 0.2039415464967188\n",
      "1400 Train Loss 479.09106 Test MSE 452.0896862243215 Test RE 0.2033341412229298\n",
      "1500 Train Loss 479.0439 Test MSE 451.80589857465907 Test RE 0.20327031232111992\n",
      "1600 Train Loss 479.02292 Test MSE 451.5358838093912 Test RE 0.20320956257357264\n",
      "1700 Train Loss 479.0403 Test MSE 452.28710798737677 Test RE 0.20337853308188095\n",
      "1800 Train Loss 479.01877 Test MSE 451.73198074819044 Test RE 0.20325368359369006\n",
      "1900 Train Loss 479.06 Test MSE 453.0041045734241 Test RE 0.20353967405343892\n",
      "2000 Train Loss 288805.97 Test MSE 553904.3619546132 Test RE 7.117302076143394\n",
      "2100 Train Loss 573.17566 Test MSE 554.0047745471895 Test RE 0.22508925304153157\n",
      "2200 Train Loss 544.28784 Test MSE 532.6260234591182 Test RE 0.22070348811037682\n",
      "2300 Train Loss 538.7371 Test MSE 526.2177557752428 Test RE 0.21937177800064744\n",
      "2400 Train Loss 534.70087 Test MSE 522.4608217879714 Test RE 0.218587272358964\n",
      "2500 Train Loss 531.5834 Test MSE 519.0710770599733 Test RE 0.21787701737562393\n",
      "2600 Train Loss 528.93634 Test MSE 515.9647735062065 Test RE 0.21722411282665877\n",
      "2700 Train Loss 526.5697 Test MSE 514.6513859992273 Test RE 0.2169474648465833\n",
      "2800 Train Loss 524.3257 Test MSE 512.4591411333594 Test RE 0.21648490947674742\n",
      "2900 Train Loss 521.9889 Test MSE 509.8484513528097 Test RE 0.21593277122169816\n",
      "Training time: 14.78\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 11710.872 Test MSE 10775.506586878588 Test RE 0.992697331694\n",
      "100 Train Loss 583.84424 Test MSE 553.2310095570771 Test RE 0.22493200977614652\n",
      "200 Train Loss 448.97537 Test MSE 518.7297783579068 Test RE 0.21780537654067902\n",
      "300 Train Loss 369.29044 Test MSE 445.24522899546866 Test RE 0.20178907215720449\n",
      "400 Train Loss 232.67876 Test MSE 262.34576547544964 Test RE 0.15489415703819734\n",
      "500 Train Loss 149.48279 Test MSE 215.8073108002203 Test RE 0.14048536451245658\n",
      "600 Train Loss 39.86754 Test MSE 44.18721358678372 Test RE 0.06356913680177088\n",
      "700 Train Loss 56.61581 Test MSE 114.17782101155285 Test RE 0.10218540756924419\n",
      "800 Train Loss 33.728237 Test MSE 37.315998366382075 Test RE 0.05841784591437014\n",
      "900 Train Loss 17.33459 Test MSE 21.617994364342454 Test RE 0.044463710912439496\n",
      "1000 Train Loss 45.253014 Test MSE 54.544784778790216 Test RE 0.07062762824498575\n",
      "1100 Train Loss 42.24539 Test MSE 32.9411896750839 Test RE 0.05488676646601397\n",
      "1200 Train Loss 29.603458 Test MSE 53.573691546974615 Test RE 0.06999609188913551\n",
      "1300 Train Loss 22.434908 Test MSE 23.46292614859916 Test RE 0.0463221910022382\n",
      "1400 Train Loss 23.444838 Test MSE 33.36981851808922 Test RE 0.05524270405394438\n",
      "1500 Train Loss 50.098812 Test MSE 43.46011174711869 Test RE 0.06304395138994627\n",
      "1600 Train Loss 21.808514 Test MSE 32.64800481301575 Test RE 0.05464196755182912\n",
      "1700 Train Loss 36.845253 Test MSE 40.74411705717419 Test RE 0.061042239936112984\n",
      "1800 Train Loss 3.641915 Test MSE 8.404407350237626 Test RE 0.027723723137446674\n",
      "1900 Train Loss 35.96688 Test MSE 45.39237097642062 Test RE 0.06443019415677911\n",
      "2000 Train Loss 65.12361 Test MSE 139.98881109305466 Test RE 0.11314742343018222\n",
      "2100 Train Loss 28.467363 Test MSE 41.450713441934276 Test RE 0.061569270958083774\n",
      "2200 Train Loss 45.821377 Test MSE 81.15634236889969 Test RE 0.08615078498143428\n",
      "2300 Train Loss 31.891043 Test MSE 46.36784641777189 Test RE 0.06511881199203828\n",
      "2400 Train Loss 40.098427 Test MSE 46.38215048263288 Test RE 0.06512885550231953\n",
      "2500 Train Loss 8.649773 Test MSE 14.338216189283484 Test RE 0.03621142105790554\n",
      "2600 Train Loss 4933.6367 Test MSE 3934.932554911566 Test RE 0.5998831352576083\n",
      "2700 Train Loss 399.19724 Test MSE 455.7655959843014 Test RE 0.2041591155378887\n",
      "2800 Train Loss 284.58884 Test MSE 349.3306960977222 Test RE 0.17873778876549412\n",
      "2900 Train Loss 248.38654 Test MSE 310.2135884357791 Test RE 0.1684334773745351\n",
      "Training time: 14.81\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 11182.914 Test MSE 10646.248789234573 Test RE 0.9867254085626516\n",
      "100 Train Loss 405.64297 Test MSE 463.88670049905716 Test RE 0.205969999037692\n",
      "200 Train Loss 380.69876 Test MSE 450.09283963255575 Test RE 0.20288458835758555\n",
      "300 Train Loss 414.43466 Test MSE 463.3927639127938 Test RE 0.20586031361868412\n",
      "400 Train Loss 522.5709 Test MSE 456.50934462360755 Test RE 0.2043256278517935\n",
      "500 Train Loss 428.12244 Test MSE 488.5139391697915 Test RE 0.21136666075624858\n",
      "600 Train Loss 516.89307 Test MSE 568.8708344692245 Test RE 0.22808926219052358\n",
      "700 Train Loss 339.24094 Test MSE 326.9731123098827 Test RE 0.1729235038273205\n",
      "800 Train Loss 76.87718 Test MSE 57.883594822130206 Test RE 0.07275716253045261\n",
      "900 Train Loss 52.38247 Test MSE 33.43006578646998 Test RE 0.055292550311605494\n",
      "1000 Train Loss 40.12333 Test MSE 24.614067593692745 Test RE 0.04744491802652697\n",
      "1100 Train Loss 72.22753 Test MSE 63.63737221755509 Test RE 0.07628763037351458\n",
      "1200 Train Loss 39.205647 Test MSE 18.154593944121633 Test RE 0.04074659062308329\n",
      "1300 Train Loss 45.92478 Test MSE 36.34797827328548 Test RE 0.05765515394145262\n",
      "1400 Train Loss 28.773643 Test MSE 12.838498779899178 Test RE 0.034265346529446856\n",
      "1500 Train Loss 26.583084 Test MSE 11.611403106111194 Test RE 0.032586698122182296\n",
      "1600 Train Loss 26.26818 Test MSE 11.181471664282869 Test RE 0.03197771959306107\n",
      "1700 Train Loss 25.91103 Test MSE 10.864216895358132 Test RE 0.03152079916301927\n",
      "1800 Train Loss 25.436495 Test MSE 10.467517668806769 Test RE 0.030939967807350973\n",
      "1900 Train Loss 23.770218 Test MSE 9.91088866780154 Test RE 0.030106086490562387\n",
      "2000 Train Loss 24.206896 Test MSE 18.959617558551656 Test RE 0.04164019863588042\n",
      "2100 Train Loss 42.922222 Test MSE 35.513111238517034 Test RE 0.05698917461132395\n",
      "2200 Train Loss 28.47267 Test MSE 22.8009061333714 Test RE 0.04566401136375312\n",
      "2300 Train Loss 144.86362 Test MSE 96.42727998983722 Test RE 0.0939069995217569\n",
      "2400 Train Loss 18.978712 Test MSE 12.817936675540674 Test RE 0.03423789589141524\n",
      "2500 Train Loss 17.458319 Test MSE 6.580823177454453 Test RE 0.024532291532411658\n",
      "2600 Train Loss 14.787344 Test MSE 5.395085341783502 Test RE 0.02221248592552498\n",
      "2700 Train Loss 4.9468627 Test MSE 5.580864621493614 Test RE 0.022591691556856933\n",
      "2800 Train Loss 497.76376 Test MSE 518.714315082151 Test RE 0.21780213013972108\n",
      "2900 Train Loss 351.53223 Test MSE 435.59265019320264 Test RE 0.19958977004011988\n",
      "Training time: 14.80\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 12224.658 Test MSE 10769.834982872198 Test RE 0.9924360480520492\n",
      "100 Train Loss 363.2753 Test MSE 519.5409684860704 Test RE 0.2179756121420772\n",
      "200 Train Loss 336.22955 Test MSE 465.16546737710877 Test RE 0.20625369583925707\n",
      "300 Train Loss 6767.1733 Test MSE 2369.671911967997 Test RE 0.46552404364503797\n",
      "400 Train Loss 442.01096 Test MSE 600.7860118835767 Test RE 0.23440016463408297\n",
      "500 Train Loss 377.06265 Test MSE 538.2861307410543 Test RE 0.22187307438729598\n",
      "600 Train Loss 360.7171 Test MSE 522.3050473840002 Test RE 0.21855468346512325\n",
      "700 Train Loss 358.08487 Test MSE 516.4750704404626 Test RE 0.217331505240128\n",
      "800 Train Loss 350.4845 Test MSE 507.82828159547813 Test RE 0.21550455199306326\n",
      "900 Train Loss 349.97894 Test MSE 502.4288201442769 Test RE 0.21435581912370907\n",
      "1000 Train Loss 1798.805 Test MSE 1141.8769231978886 Test RE 0.32315259981629896\n",
      "1100 Train Loss 346.9012 Test MSE 492.62729345349567 Test RE 0.21225466351656502\n",
      "1200 Train Loss 341.2834 Test MSE 481.4723469699827 Test RE 0.20983777884512414\n",
      "1300 Train Loss 337.7869 Test MSE 475.04953775448365 Test RE 0.2084334687352181\n",
      "1400 Train Loss 341.3828 Test MSE 486.057498157717 Test RE 0.21083457350502796\n",
      "1500 Train Loss 336.21954 Test MSE 475.55964864045 Test RE 0.20854534722934454\n",
      "1600 Train Loss 332.82498 Test MSE 470.1742002229939 Test RE 0.20736115509350192\n",
      "1700 Train Loss 354.7694 Test MSE 503.45168114896285 Test RE 0.21457390447324726\n",
      "1800 Train Loss 340.27448 Test MSE 464.4556800189621 Test RE 0.20609627642118647\n",
      "1900 Train Loss 384.76807 Test MSE 561.539716147357 Test RE 0.22661478717003036\n",
      "2000 Train Loss 332.11414 Test MSE 473.5157233038578 Test RE 0.2080967072877226\n",
      "2100 Train Loss 329.7438 Test MSE 466.8480980512441 Test RE 0.2066263971072391\n",
      "2200 Train Loss 729.42114 Test MSE 570.0655692179832 Test RE 0.2283286515408574\n",
      "2300 Train Loss 438.19897 Test MSE 647.7184659037022 Test RE 0.24338350741914488\n",
      "2400 Train Loss 334.90598 Test MSE 453.62601302195606 Test RE 0.2036793412511731\n",
      "2500 Train Loss 333.5847 Test MSE 451.9193341171455 Test RE 0.2032958284012529\n",
      "2600 Train Loss 332.94293 Test MSE 449.39794682316324 Test RE 0.20272791234909315\n",
      "2700 Train Loss 332.47263 Test MSE 447.67397476780496 Test RE 0.20233868818108977\n",
      "2800 Train Loss 332.05063 Test MSE 446.10754905075066 Test RE 0.20198438317118414\n",
      "2900 Train Loss 331.68835 Test MSE 444.4837975908338 Test RE 0.20161645460515018\n",
      "Training time: 14.85\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 10331.903 Test MSE 10197.777113803317 Test RE 0.9657189732919691\n",
      "100 Train Loss 516.3508 Test MSE 608.7901391112289 Test RE 0.2359564268056977\n",
      "200 Train Loss 421.8618 Test MSE 490.87383411221253 Test RE 0.21187657677352034\n",
      "300 Train Loss 474.70917 Test MSE 603.5385406318303 Test RE 0.23493650858306903\n",
      "400 Train Loss 414.3828 Test MSE 465.6407454100371 Test RE 0.20635903773452074\n",
      "500 Train Loss 415.46234 Test MSE 481.95763970509813 Test RE 0.20994350361104022\n",
      "600 Train Loss 421.16983 Test MSE 461.4705604857773 Test RE 0.205432904472799\n",
      "700 Train Loss 408.97986 Test MSE 454.5752797716494 Test RE 0.20389234158235994\n",
      "800 Train Loss 409.1079 Test MSE 454.73665478471116 Test RE 0.20392852943847672\n",
      "900 Train Loss 441.97604 Test MSE 466.1433668719271 Test RE 0.20647038162771975\n",
      "1000 Train Loss 410.66107 Test MSE 449.1524057484217 Test RE 0.20267252176365397\n",
      "1100 Train Loss 411.31833 Test MSE 466.9174022414519 Test RE 0.2066417335129864\n",
      "1200 Train Loss 411.20407 Test MSE 482.3589834909462 Test RE 0.21003089924440693\n",
      "1300 Train Loss 446.81912 Test MSE 481.7881076620447 Test RE 0.20990657579969385\n",
      "1400 Train Loss 428.62692 Test MSE 456.90664116398625 Test RE 0.20441452001969437\n",
      "1500 Train Loss 427.04404 Test MSE 458.5817836202049 Test RE 0.20478889645710846\n",
      "1600 Train Loss 5849.551 Test MSE 6059.019350682265 Test RE 0.7443876786259339\n",
      "1700 Train Loss 5835.9243 Test MSE 6064.115213589405 Test RE 0.7447006418382395\n",
      "1800 Train Loss 5835.922 Test MSE 6064.3856570064 Test RE 0.7447172474875141\n",
      "1900 Train Loss 5835.922 Test MSE 6064.386671116156 Test RE 0.7447173097547444\n",
      "2000 Train Loss 5835.922 Test MSE 6064.386628860245 Test RE 0.7447173071601944\n",
      "2100 Train Loss 5835.922 Test MSE 6064.386628860245 Test RE 0.7447173071601944\n",
      "2200 Train Loss 5835.922 Test MSE 6064.386628860245 Test RE 0.7447173071601944\n",
      "2300 Train Loss 5835.922 Test MSE 6064.386671116156 Test RE 0.7447173097547444\n",
      "2400 Train Loss 5835.922 Test MSE 6064.386671116156 Test RE 0.7447173097547444\n",
      "2500 Train Loss 5835.922 Test MSE 6064.386671116156 Test RE 0.7447173097547444\n",
      "2600 Train Loss 5835.922 Test MSE 6064.386671116156 Test RE 0.7447173097547444\n",
      "2700 Train Loss 5835.922 Test MSE 6064.386671116156 Test RE 0.7447173097547444\n",
      "2800 Train Loss 5835.922 Test MSE 6064.386628860245 Test RE 0.7447173071601944\n",
      "2900 Train Loss 5835.922 Test MSE 6064.386671116156 Test RE 0.7447173097547444\n",
      "Training time: 14.65\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 10554.739 Test MSE 10801.915410600961 Test RE 0.9939130483580477\n",
      "100 Train Loss 386.99002 Test MSE 511.2751917627969 Test RE 0.2162346891552863\n",
      "200 Train Loss 330.26776 Test MSE 433.87427336133464 Test RE 0.19919569847708501\n",
      "300 Train Loss 337.47482 Test MSE 452.14345168717983 Test RE 0.20334623177656708\n",
      "400 Train Loss 341.74072 Test MSE 469.43752265449064 Test RE 0.20719864277943018\n",
      "500 Train Loss 330.67163 Test MSE 446.22164471096386 Test RE 0.20201021110019654\n",
      "600 Train Loss 330.22635 Test MSE 445.2506307763956 Test RE 0.20179029622091607\n",
      "700 Train Loss 337.04187 Test MSE 446.8773859362358 Test RE 0.20215858780313434\n",
      "800 Train Loss 330.2062 Test MSE 443.54820498355593 Test RE 0.20140415189333294\n",
      "900 Train Loss 330.5125 Test MSE 444.8250275232256 Test RE 0.2016938301705252\n",
      "1000 Train Loss 332.11642 Test MSE 443.8859825527443 Test RE 0.20148082547384052\n",
      "1100 Train Loss 1654.7043 Test MSE 10445.658055628208 Test RE 0.9773855378842646\n",
      "1200 Train Loss 546.1526 Test MSE 637.7949064673822 Test RE 0.24151189715055837\n",
      "1300 Train Loss 256.1867 Test MSE 318.49170916121625 Test RE 0.1706660244046372\n",
      "1400 Train Loss 150.34755 Test MSE 173.68213504215512 Test RE 0.12603047722602034\n",
      "1500 Train Loss 91.94521 Test MSE 99.1741458785053 Test RE 0.09523514351707167\n",
      "1600 Train Loss 64.1556 Test MSE 62.83375055904627 Test RE 0.07580441455137321\n",
      "1700 Train Loss 95.01292 Test MSE 92.08223954634437 Test RE 0.09176687503875355\n",
      "1800 Train Loss 72.62122 Test MSE 74.35555830375053 Test RE 0.08246216363931186\n",
      "1900 Train Loss 64.11728 Test MSE 62.59092157667203 Test RE 0.07565779487909073\n",
      "2000 Train Loss 56.56047 Test MSE 50.970065323521965 Test RE 0.06827403984439726\n",
      "2100 Train Loss 52.396465 Test MSE 49.259989993410244 Test RE 0.06711895185133346\n",
      "2200 Train Loss 45.936275 Test MSE 42.42231232122568 Test RE 0.06228667883938857\n",
      "2300 Train Loss 40.217266 Test MSE 38.37735946595365 Test RE 0.059242796552733235\n",
      "2400 Train Loss 514.04175 Test MSE 11060.28493241666 Test RE 1.0057294416479194\n",
      "2500 Train Loss 1110.2942 Test MSE 1233.405340817794 Test RE 0.33585430423400797\n",
      "2600 Train Loss 886.4922 Test MSE 1004.9206994932541 Test RE 0.3031544182624151\n",
      "2700 Train Loss 750.1998 Test MSE 863.3423465387667 Test RE 0.2809891353096163\n",
      "2800 Train Loss 655.7672 Test MSE 767.2930246548248 Test RE 0.2648979706430145\n",
      "2900 Train Loss 585.0423 Test MSE 696.2155672756354 Test RE 0.2523305713553085\n",
      "Training time: 14.96\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 12428.54 Test MSE 10795.347695563629 Test RE 0.9936108459260734\n",
      "100 Train Loss 371.81384 Test MSE 373.4126568937458 Test RE 0.18479598165058075\n",
      "200 Train Loss 610.09717 Test MSE 616.4063845577574 Test RE 0.23742780108405837\n",
      "300 Train Loss 449.48206 Test MSE 465.68071171851716 Test RE 0.206367893523107\n",
      "400 Train Loss 466.78027 Test MSE 456.5746212180119 Test RE 0.20434023566271325\n",
      "500 Train Loss 448.87543 Test MSE 464.8130634231528 Test RE 0.20617555333459708\n",
      "600 Train Loss 1194.4069 Test MSE 837.0667832443672 Test RE 0.2766801869115469\n",
      "700 Train Loss 586.4761 Test MSE 560.3617976619145 Test RE 0.22637698219367114\n",
      "800 Train Loss 489.69122 Test MSE 497.62752211856434 Test RE 0.2133291495542094\n",
      "900 Train Loss 465.74777 Test MSE 476.15543378002593 Test RE 0.20867594002534312\n",
      "1000 Train Loss 455.36752 Test MSE 466.16141498354506 Test RE 0.20647437864311177\n",
      "1100 Train Loss 460.4371 Test MSE 474.1412794525183 Test RE 0.20823411899674418\n",
      "1200 Train Loss 466.48544 Test MSE 474.98961123436465 Test RE 0.20842032159433221\n",
      "1300 Train Loss 454.37793 Test MSE 465.95963920507614 Test RE 0.20642968807416875\n",
      "1400 Train Loss 452.9428 Test MSE 464.09881797600946 Test RE 0.2060170847196046\n",
      "1500 Train Loss 464.03647 Test MSE 507.0857845226877 Test RE 0.2153469494766861\n",
      "1600 Train Loss 460.71533 Test MSE 474.09449338565037 Test RE 0.20822384495376936\n",
      "1700 Train Loss 442.94794 Test MSE 452.1688481354853 Test RE 0.2033519425742444\n",
      "1800 Train Loss 472.16138 Test MSE 455.2566218063472 Test RE 0.20404508680704053\n",
      "1900 Train Loss 441.99808 Test MSE 473.90678821995914 Test RE 0.20818262051300226\n",
      "2000 Train Loss 461.272 Test MSE 457.4055112561788 Test RE 0.20452608380640305\n",
      "2100 Train Loss 437.3446 Test MSE 451.4512952719318 Test RE 0.2031905275362446\n",
      "2200 Train Loss 472.74432 Test MSE 481.11910278605535 Test RE 0.2097607883636076\n",
      "2300 Train Loss 475.44336 Test MSE 463.5956318225501 Test RE 0.20590537030169695\n",
      "2400 Train Loss 441.81958 Test MSE 453.42427610452125 Test RE 0.20363404599622725\n",
      "2500 Train Loss 439.5227 Test MSE 451.24030099237444 Test RE 0.20314303952385612\n",
      "2600 Train Loss 443.26135 Test MSE 456.3090302583679 Test RE 0.20428079432338941\n",
      "2700 Train Loss 442.9824 Test MSE 457.514392258782 Test RE 0.20455042509596158\n",
      "2800 Train Loss 445.02948 Test MSE 460.2423169271039 Test RE 0.20515933365266822\n",
      "2900 Train Loss 446.0974 Test MSE 469.21303853270075 Test RE 0.20714909585888272\n",
      "Training time: 14.37\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 12242.463 Test MSE 10793.472468876373 Test RE 0.9935245436341388\n",
      "100 Train Loss 422.6258 Test MSE 504.9475337017 Test RE 0.21489243837788113\n",
      "200 Train Loss 395.64923 Test MSE 481.4549498130592 Test RE 0.20983398775123532\n",
      "300 Train Loss 415.5737 Test MSE 477.86072971389757 Test RE 0.20904928049707722\n",
      "400 Train Loss 412.2362 Test MSE 466.80939713921606 Test RE 0.20661783244157572\n",
      "500 Train Loss 399.04752 Test MSE 495.3361466058507 Test RE 0.2128374352008048\n",
      "600 Train Loss 395.63632 Test MSE 490.8451244419831 Test RE 0.21187038068486078\n",
      "700 Train Loss 407.82584 Test MSE 512.876123097792 Test RE 0.21657296717742636\n",
      "800 Train Loss 395.5244 Test MSE 492.39087026436925 Test RE 0.21220372445225294\n",
      "900 Train Loss 395.17813 Test MSE 493.8252010021405 Test RE 0.21251257358835993\n",
      "1000 Train Loss 400.66605 Test MSE 489.7291399595838 Test RE 0.21162938959314623\n",
      "1100 Train Loss 400.4162 Test MSE 502.5088708958622 Test RE 0.214372894837029\n",
      "1200 Train Loss 402.43112 Test MSE 503.69207323753693 Test RE 0.2146251265815657\n",
      "1300 Train Loss 13518.959 Test MSE 11952.029418550926 Test RE 1.0454874715671894\n",
      "1400 Train Loss 12838.163 Test MSE 11489.742065865898 Test RE 1.0250691085001167\n",
      "1500 Train Loss 12770.198 Test MSE 11426.166056935253 Test RE 1.0222291751289307\n",
      "1600 Train Loss 12698.165 Test MSE 11358.847700336322 Test RE 1.0192134459371835\n",
      "1700 Train Loss 12622.411 Test MSE 11288.0868919659 Test RE 1.016033852359762\n",
      "1800 Train Loss 12542.931 Test MSE 11213.877781784846 Test RE 1.012688586767497\n",
      "1900 Train Loss 12459.708 Test MSE 11136.217790968078 Test RE 1.0091758847808985\n",
      "2000 Train Loss 12372.744 Test MSE 11055.108936922748 Test RE 1.0054940833245143\n",
      "2100 Train Loss 12282.038 Test MSE 10970.558411936658 Test RE 1.001641646026357\n",
      "2200 Train Loss 12187.594 Test MSE 10882.580517001863 Test RE 0.997617251578699\n",
      "2300 Train Loss 12089.432 Test MSE 10791.197359730686 Test RE 0.9934198277556986\n",
      "2400 Train Loss 11987.573 Test MSE 10696.44009964444 Test RE 0.9890486121852216\n",
      "2500 Train Loss 11882.054 Test MSE 10598.35036040058 Test RE 0.9845032232048778\n",
      "2600 Train Loss 11772.918 Test MSE 10496.98106212469 Test RE 0.9797837065223031\n",
      "2700 Train Loss 11660.228 Test MSE 10392.397813344058 Test RE 0.9748906105469816\n",
      "2800 Train Loss 11544.055 Test MSE 10284.680642468062 Test RE 0.9698250812411504\n",
      "2900 Train Loss 11424.488 Test MSE 10173.923703835906 Test RE 0.9645888653929356\n",
      "Training time: 15.22\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 10968.037 Test MSE 10860.350422355863 Test RE 0.9965978029791505\n",
      "100 Train Loss 254.09668 Test MSE 449.6129804240744 Test RE 0.20277640845347383\n",
      "200 Train Loss 265.1063 Test MSE 478.75304960171826 Test RE 0.20924437063330292\n",
      "300 Train Loss 256.2457 Test MSE 460.7232314961658 Test RE 0.2052664928038739\n",
      "400 Train Loss 370.97797 Test MSE 725.4435765595653 Test RE 0.2575726973253775\n",
      "500 Train Loss 258.29068 Test MSE 472.41250503417797 Test RE 0.20785414938181349\n",
      "600 Train Loss 256.86694 Test MSE 474.4031673749722 Test RE 0.20829161923443876\n",
      "700 Train Loss 256.63995 Test MSE 474.65281762437957 Test RE 0.20834641778863297\n",
      "800 Train Loss 276.437 Test MSE 471.5435030801369 Test RE 0.20766288771782826\n",
      "900 Train Loss 292.75198 Test MSE 601.9239549083283 Test RE 0.23462204716958968\n",
      "1000 Train Loss 258.5112 Test MSE 470.1187959254744 Test RE 0.2073489372414844\n",
      "1100 Train Loss 257.51193 Test MSE 467.3864086907721 Test RE 0.20674549060148462\n",
      "1200 Train Loss 257.4764 Test MSE 467.70934317242785 Test RE 0.20681690230488592\n",
      "1300 Train Loss 257.4439 Test MSE 468.0312384487742 Test RE 0.20688805967457652\n",
      "1400 Train Loss 257.20065 Test MSE 468.0743738636673 Test RE 0.206897593222709\n",
      "1500 Train Loss 258.6338 Test MSE 470.1700653121729 Test RE 0.20736024328063685\n",
      "1600 Train Loss 260.26096 Test MSE 467.27893821368355 Test RE 0.2067217197841249\n",
      "1700 Train Loss 261.01913 Test MSE 475.0017611743263 Test RE 0.20842298720863284\n",
      "1800 Train Loss 261.69647 Test MSE 480.24137922335353 Test RE 0.2095693637946303\n",
      "1900 Train Loss 258.38266 Test MSE 470.2722500035624 Test RE 0.20738277543747866\n",
      "2000 Train Loss 259.97922 Test MSE 469.8729350243131 Test RE 0.207294710880031\n",
      "2100 Train Loss 259.45767 Test MSE 470.38810199912757 Test RE 0.2074083183319122\n",
      "2200 Train Loss 259.14188 Test MSE 470.5376430657275 Test RE 0.20744128429802214\n",
      "2300 Train Loss 258.92773 Test MSE 471.1530932423456 Test RE 0.20757690368691606\n",
      "2400 Train Loss 257.9077 Test MSE 474.3856143775638 Test RE 0.20828776578622824\n",
      "2500 Train Loss 260.53116 Test MSE 483.1067672098704 Test RE 0.21019363785736997\n",
      "2600 Train Loss 265.47742 Test MSE 474.7310700621312 Test RE 0.20836359133377505\n",
      "2700 Train Loss 257.54053 Test MSE 474.0492829215876 Test RE 0.2082139164255617\n",
      "2800 Train Loss 257.4896 Test MSE 474.34968097073 Test RE 0.2082798770238613\n",
      "2900 Train Loss 257.54593 Test MSE 472.8500421127306 Test RE 0.20795038185719353\n",
      "Training time: 15.78\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 11134.771 Test MSE 10924.450146618075 Test RE 0.9995345250380934\n",
      "100 Train Loss 487.7919 Test MSE 551.8032919509143 Test RE 0.22464158241121263\n",
      "200 Train Loss 454.16544 Test MSE 492.28438503205626 Test RE 0.2121807774541617\n",
      "300 Train Loss 449.01782 Test MSE 488.31980895483036 Test RE 0.211324659157837\n",
      "400 Train Loss 447.8873 Test MSE 488.67978080022857 Test RE 0.21140253528749173\n",
      "500 Train Loss 447.52887 Test MSE 486.8898383149471 Test RE 0.2110150161687141\n",
      "600 Train Loss 448.53833 Test MSE 485.03754222521684 Test RE 0.21061324690704125\n",
      "700 Train Loss 447.45428 Test MSE 484.1180604030873 Test RE 0.21041352327758506\n",
      "800 Train Loss 474.11554 Test MSE 520.8483341705712 Test RE 0.21824969522869206\n",
      "900 Train Loss 447.9086 Test MSE 508.46697642318577 Test RE 0.21564002927639597\n",
      "1000 Train Loss 447.52365 Test MSE 498.1592101225732 Test RE 0.21344308443910195\n",
      "1100 Train Loss 447.46954 Test MSE 492.0138444650089 Test RE 0.2121224662437592\n",
      "1200 Train Loss 447.4425 Test MSE 478.2042512206044 Test RE 0.20912440701185356\n",
      "1300 Train Loss 448.2724 Test MSE 488.4228162625319 Test RE 0.21134694663826006\n",
      "1400 Train Loss 447.45554 Test MSE 484.6846512977961 Test RE 0.21053661672786506\n",
      "1500 Train Loss 447.48553 Test MSE 483.7936792299593 Test RE 0.21034301813814435\n",
      "1600 Train Loss 447.49115 Test MSE 484.49564991900843 Test RE 0.21049556365277544\n",
      "1700 Train Loss 462.81897 Test MSE 489.3148321977541 Test RE 0.2115398520880744\n",
      "1800 Train Loss 449.4946 Test MSE 483.5589893502842 Test RE 0.21029199290981299\n",
      "1900 Train Loss 450.42642 Test MSE 496.07844569508575 Test RE 0.21299685208369684\n",
      "2000 Train Loss 109538.234 Test MSE 66106.29639182887 Test RE 2.458778570698868\n",
      "2100 Train Loss 292.61627 Test MSE 342.6365965976096 Test RE 0.17701696074610165\n",
      "2200 Train Loss 219.4968 Test MSE 277.46027760802855 Test RE 0.15929363193389137\n",
      "2300 Train Loss 162.04805 Test MSE 226.30484926157564 Test RE 0.1438616165951433\n",
      "2400 Train Loss 90.51411 Test MSE 152.74815039962425 Test RE 0.11819142973484091\n",
      "2500 Train Loss 65.75267 Test MSE 121.21030205650405 Test RE 0.10528530823419453\n",
      "2600 Train Loss 59.291218 Test MSE 115.53176119882107 Test RE 0.10278948807018166\n",
      "2700 Train Loss 52.354847 Test MSE 109.66291719645369 Test RE 0.10014468468126889\n",
      "2800 Train Loss 44.67833 Test MSE 104.06472837240896 Test RE 0.09755505624675387\n",
      "2900 Train Loss 33.164562 Test MSE 86.64814237414905 Test RE 0.08901795937018281\n",
      "Training time: 16.23\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 3000\n",
    "layers = np.array([1,50,50,50,50,1])\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []\n",
    "    beta_val = []\n",
    "    'Generate Training data'\n",
    "    N_train = 300\n",
    "    \n",
    "    x_train_np_array, u_train_np_array = trainingdata(N_train,reps*1234)\n",
    "\n",
    "    'Convert to tensor and send to GPU'\n",
    "    x_train = torch.from_numpy(x_train_np_array).float().to(device)\n",
    "    u_train = torch.from_numpy(u_train_np_array).float().to(device)\n",
    "    \n",
    "    x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "       \n",
    "     \n",
    "    model_NN = Sequentialmodel(layers)\n",
    "    model_NN.to(device)\n",
    "    print(model_NN)\n",
    "    params = list(model_NN.parameters())\n",
    "    #optimizer = optim.Adam(PINN.parameters(), lr=0.0008,betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "    optimizer = optim.Adam(model_NN.parameters(), lr=0.08)\n",
    "\n",
    "    train_model(max_iter,reps)\n",
    " \n",
    "    torch.save(model_NN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    beta_full.append(beta_val)\n",
    "                 \n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse\": test_mse_full, \"test_re\": test_re_full,\"Time\": elapsed_time, \"Thresh_time\": time_threshold, \"epoch_thresh\": epoch_threshold,\"beta\": beta_full, \"label\": label}\n",
    "savemat(label+'.mat', mdic) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred = model_NN.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1bbc7bb710>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_test,u_pred,'rx')\n",
    "plt.plot(x_test,u_true,'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.32513266568307514\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + test_re_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
