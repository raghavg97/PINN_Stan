{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#Trying to replicate the results of \"Adaptive activation functions accelerate convergence in deep and physics-informed neural networks\"\n",
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_disc(x): #[-4,3.75]\n",
    "    y  = np.zeros((np.shape(x)[0],))\n",
    "    \n",
    "    y = 0.2*np.sin(6*x)*np.exp(-1*x)\n",
    "    y[x>0] = 1 + 0.1*x[x>0]*np.exp(x[x>0]) + 10\n",
    "    \n",
    "    return scale*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain bounds\n",
    "level = \"low\"\n",
    "label = \"Regr_disc_swish_\" + level\n",
    "loss_thresh = 0.1\n",
    "scale = 1.0\n",
    "\n",
    "lb = np.array(-4.0)  # [-1. 0.]\n",
    "ub = np.array(3.75) # [1.  0.99]\n",
    "\n",
    "x_test = np.linspace(lb,ub,1000).reshape(-1,1)\n",
    "u_true = true_disc(x_test)\n",
    "\n",
    "u_true_norm = np.linalg.norm(u_true,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_train,seed):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    x_train = np.random.uniform(low=lb, high=ub, size=(N_train,)).reshape(-1,1)\n",
    "\n",
    "    u_train = true_disc(x_train)\n",
    "\n",
    "    return x_train, u_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Sigmoid()\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "     \n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "\n",
    "        \n",
    "        self.beta = Parameter(torch.ones((layers[1],len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "            \n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = 2.0*(x - l_b)/(u_b - l_b)-1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = z*self.activation(self.beta[:,i]*z)\n",
    "    \n",
    "        a = self.linears[-1](a) \n",
    "        \n",
    "        return a\n",
    "    \n",
    "    \n",
    "    def loss(self,x,u):\n",
    "\n",
    "        loss_val = self.loss_function(self.forward(x),u)\n",
    "        \n",
    "        return loss_val\n",
    "        \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        u_pred = self.forward(x_test_tensor)       \n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "        \n",
    "        return u_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "        \n",
    "        test_mse_loss = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re_loss = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "        \n",
    "        return test_mse_loss, test_re_loss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    beta_val.append(model_NN.beta.cpu().detach().numpy())\n",
    "    test_mse, test_re = model_NN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step():\n",
    "    optimizer.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "    loss = model_NN.loss(x_train,u_train)\n",
    "    loss.backward() #backprop\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "   \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "    \n",
    "    loss_np = model_NN.loss(x_train,u_train).cpu().detach().numpy()\n",
    "    data_update(loss_np)\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        loss_np = train_step().cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1\n",
    "        data_update(loss_np)\n",
    "        if(i%100==0):        \n",
    "            print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 116.12308 Test MSE 75.63190874665975 Test RE 0.831669043946498\n",
      "100 Train Loss 4.720688 Test MSE 4.964822893628993 Test RE 0.21308353185354678\n",
      "200 Train Loss 4.1734004 Test MSE 4.389811051815341 Test RE 0.20036456595782642\n",
      "300 Train Loss 5.80998 Test MSE 7.244239928570426 Test RE 0.25739162930551096\n",
      "400 Train Loss 4.9052753 Test MSE 4.737379391241922 Test RE 0.20814553033018274\n",
      "500 Train Loss 4.931789 Test MSE 4.75468956719188 Test RE 0.20852546089114\n",
      "600 Train Loss 4.871687 Test MSE 4.775370407756632 Test RE 0.20897846655671706\n",
      "700 Train Loss 4.7298384 Test MSE 4.429204748292735 Test RE 0.20126158314757564\n",
      "800 Train Loss 4.260977 Test MSE 4.421563094026992 Test RE 0.2010878910903501\n",
      "900 Train Loss 119.714836 Test MSE 94.70074136411652 Test RE 0.930624958245444\n",
      "1000 Train Loss 9.96845 Test MSE 11.17095443046818 Test RE 0.31962677017017627\n",
      "1100 Train Loss 6.465484 Test MSE 7.1590426058161425 Test RE 0.25587360014033256\n",
      "1200 Train Loss 4.7469687 Test MSE 5.219528271031139 Test RE 0.21848097905170777\n",
      "1300 Train Loss 3.6433065 Test MSE 4.058001871954517 Test RE 0.19264339693665453\n",
      "1400 Train Loss 3.0170581 Test MSE 3.4138954650688356 Test RE 0.17669453424027368\n",
      "1500 Train Loss 2.6808436 Test MSE 3.070391622588104 Test RE 0.16756946858822178\n",
      "1600 Train Loss 2.4740472 Test MSE 2.867416350970653 Test RE 0.1619359915562578\n",
      "1700 Train Loss 2.3349879 Test MSE 2.736813598102937 Test RE 0.15820514957623397\n",
      "1800 Train Loss 2.2369077 Test MSE 2.6475326270935313 Test RE 0.15560325106972053\n",
      "1900 Train Loss 2.161145 Test MSE 2.580373376701921 Test RE 0.15361700101352024\n",
      "2000 Train Loss 2.098548 Test MSE 2.52509782513186 Test RE 0.151962738069628\n",
      "2100 Train Loss 2.04392 Test MSE 2.476463194029578 Test RE 0.15049218405259723\n",
      "2200 Train Loss 1.9940894 Test MSE 2.431958609509974 Test RE 0.14913380403567109\n",
      "2300 Train Loss 1.9477594 Test MSE 2.390708623573734 Test RE 0.14786361857657515\n",
      "2400 Train Loss 1.9052434 Test MSE 2.3523693958833336 Test RE 0.14667319976459875\n",
      "2500 Train Loss 1.8669841 Test MSE 2.316559843061644 Test RE 0.1455525330999902\n",
      "2600 Train Loss 1.8328149 Test MSE 2.283346175533488 Test RE 0.14450533657259831\n",
      "2700 Train Loss 1.8022681 Test MSE 2.252978408842089 Test RE 0.1435411829563423\n",
      "2800 Train Loss 1.7747042 Test MSE 2.2253647327342883 Test RE 0.1426588131974779\n",
      "2900 Train Loss 1.7493885 Test MSE 2.200139500770981 Test RE 0.14184796685807805\n",
      "Training time: 75.08\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 117.14744 Test MSE 94.03708520865102 Test RE 0.9273583479114834\n",
      "100 Train Loss 4.228319 Test MSE 4.6081341991377895 Test RE 0.20528658268974934\n",
      "200 Train Loss 4.188154 Test MSE 4.703747839513052 Test RE 0.20740538214640622\n",
      "300 Train Loss 4.030581 Test MSE 4.514346595075896 Test RE 0.2031867837648693\n",
      "400 Train Loss 4.103756 Test MSE 4.792362310111228 Test RE 0.20934993392247545\n",
      "500 Train Loss 4.0983987 Test MSE 4.532167805968051 Test RE 0.2035874472728512\n",
      "600 Train Loss 4.0023847 Test MSE 4.4963731165237215 Test RE 0.20278189509704822\n",
      "700 Train Loss 60353.574 Test MSE 137719.99142332407 Test RE 35.489223705932666\n",
      "800 Train Loss 10.992787 Test MSE 11.82701727249021 Test RE 0.3288786050491516\n",
      "900 Train Loss 8.243887 Test MSE 8.908361477630756 Test RE 0.28542822829800035\n",
      "1000 Train Loss 7.1506424 Test MSE 7.795361642839447 Test RE 0.26700299939681726\n",
      "1100 Train Loss 6.6377287 Test MSE 7.286841130408198 Test RE 0.2581473413839499\n",
      "1200 Train Loss 6.426446 Test MSE 7.0854020895509295 Test RE 0.2545541939356884\n",
      "1300 Train Loss 6.284166 Test MSE 6.9502341092535 Test RE 0.2521144411846881\n",
      "1400 Train Loss 6.1469555 Test MSE 6.820437549778161 Test RE 0.249749210794395\n",
      "1500 Train Loss 6.0092587 Test MSE 6.690032189061723 Test RE 0.24735011114999134\n",
      "1600 Train Loss 5.8676987 Test MSE 6.555626429786847 Test RE 0.24485281701104544\n",
      "1700 Train Loss 5.7131877 Test MSE 6.409211394244616 Test RE 0.2421030742638202\n",
      "1800 Train Loss 5.533197 Test MSE 6.2401128774986105 Test RE 0.23888794198485513\n",
      "1900 Train Loss 5.3235927 Test MSE 6.046270024899393 Test RE 0.23514826327636634\n",
      "2000 Train Loss 5.1072373 Test MSE 5.850024059670862 Test RE 0.23130063884580734\n",
      "2100 Train Loss 4.922834 Test MSE 5.68396810978708 Test RE 0.22799421231778322\n",
      "2200 Train Loss 4.7832665 Test MSE 5.555416330665736 Test RE 0.2254012456216059\n",
      "2300 Train Loss 4.6794424 Test MSE 5.4588358495613365 Test RE 0.22343336368584876\n",
      "2400 Train Loss 4.605212 Test MSE 5.390992634475264 Test RE 0.22204059140295862\n",
      "2500 Train Loss 4.554745 Test MSE 5.345916948674418 Test RE 0.22111036928791591\n",
      "2600 Train Loss 4.5210924 Test MSE 5.31612593507962 Test RE 0.22049342137503025\n",
      "2700 Train Loss 4.4977655 Test MSE 5.295194982984206 Test RE 0.22005892367816024\n",
      "2800 Train Loss 4.4808726 Test MSE 5.2801925200851505 Test RE 0.21974696466765298\n",
      "2900 Train Loss 4.4672217 Test MSE 5.267444740343363 Test RE 0.21948154075564896\n",
      "Training time: 78.84\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 111.76875 Test MSE 82.48601003249502 Test RE 0.8685366604458679\n",
      "100 Train Loss 4.589711 Test MSE 5.0181159805152955 Test RE 0.21422411309850617\n",
      "200 Train Loss 4.442127 Test MSE 4.472784069004453 Test RE 0.2022492744823026\n",
      "300 Train Loss 3.7964365 Test MSE 4.407157899198698 Test RE 0.20076005754296847\n",
      "400 Train Loss 4.0043626 Test MSE 5.018326416474714 Test RE 0.21422860482252581\n",
      "500 Train Loss 3.7112777 Test MSE 4.298564703563627 Test RE 0.19827124803779642\n",
      "600 Train Loss 3.8138053 Test MSE 4.6915193124893 Test RE 0.20713560653771593\n",
      "700 Train Loss 3.7898433 Test MSE 4.662958151928279 Test RE 0.2065041410804506\n",
      "800 Train Loss 3.9180837 Test MSE 4.498033311590843 Test RE 0.20281932820401644\n",
      "900 Train Loss 3.8891525 Test MSE 4.293885841525002 Test RE 0.19816331242946597\n",
      "1000 Train Loss 3.7522726 Test MSE 4.668841798405177 Test RE 0.2066343818308581\n",
      "1100 Train Loss 3.5259025 Test MSE 4.442944736459733 Test RE 0.20157351165847973\n",
      "1200 Train Loss 3.5162582 Test MSE 4.445425730958844 Test RE 0.20162978435675386\n",
      "1300 Train Loss 3.5136597 Test MSE 4.446093158408724 Test RE 0.2016449199377055\n",
      "1400 Train Loss 3.5160124 Test MSE 4.449374461830264 Test RE 0.201719315154813\n",
      "1500 Train Loss 4.1718397 Test MSE 5.149563381184546 Test RE 0.21701173047711161\n",
      "1600 Train Loss 3.6853235 Test MSE 4.475643144432546 Test RE 0.20231390464839175\n",
      "1700 Train Loss 3.5510974 Test MSE 4.458901075321043 Test RE 0.20193515160778827\n",
      "1800 Train Loss 3.6173854 Test MSE 4.467590709253289 Test RE 0.20213182433328938\n",
      "1900 Train Loss 4.5649905 Test MSE 4.783869039696685 Test RE 0.2091643413087918\n",
      "2000 Train Loss 3.5472527 Test MSE 4.46092050046231 Test RE 0.20198087439391077\n",
      "2100 Train Loss 3.5348914 Test MSE 4.459450945912438 Test RE 0.20194760252072316\n",
      "2200 Train Loss 3.531233 Test MSE 4.464825749882549 Test RE 0.20206926569513023\n",
      "2300 Train Loss 3.5306034 Test MSE 4.468229216018647 Test RE 0.20214626812801728\n",
      "2400 Train Loss 3.530404 Test MSE 4.469055070055241 Test RE 0.20216494841389862\n",
      "2500 Train Loss 3.5308745 Test MSE 4.473335816845827 Test RE 0.20226174849823356\n",
      "2600 Train Loss 3.5302796 Test MSE 4.469165342559322 Test RE 0.20216744257622507\n",
      "2700 Train Loss 3.5305889 Test MSE 4.469227849398537 Test RE 0.2021688563529232\n",
      "2800 Train Loss 3.533339 Test MSE 4.4754652318392925 Test RE 0.2023098834889766\n",
      "2900 Train Loss 3.5304744 Test MSE 4.4716728642218255 Test RE 0.2022241498264052\n",
      "Training time: 75.22\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 122.351814 Test MSE 93.54557923750542 Test RE 0.9249316493696482\n",
      "100 Train Loss 3.414166 Test MSE 4.6497119277438355 Test RE 0.2062106208022433\n",
      "200 Train Loss 3.3315392 Test MSE 4.515324229812882 Test RE 0.20320878381518634\n",
      "300 Train Loss 3.361647 Test MSE 4.400036611781144 Test RE 0.20059779332381683\n",
      "400 Train Loss 3.3253152 Test MSE 4.395394995927481 Test RE 0.20049195970648778\n",
      "500 Train Loss 3.3254862 Test MSE 4.339713725143039 Test RE 0.19921798679049402\n",
      "600 Train Loss 3.6617723 Test MSE 4.7846947241609525 Test RE 0.20918239116490653\n",
      "700 Train Loss 521.0753 Test MSE 1219.8210284331278 Test RE 3.339996902894806\n",
      "800 Train Loss 51.895794 Test MSE 49.00890847736725 Test RE 0.6694767839536157\n",
      "900 Train Loss 35.884384 Test MSE 33.78645491667671 Test RE 0.5558649862312809\n",
      "1000 Train Loss 27.937061 Test MSE 26.707926426526495 Test RE 0.4942174684436452\n",
      "1100 Train Loss 20.735714 Test MSE 20.645375601729764 Test RE 0.4345196293151735\n",
      "1200 Train Loss 14.813132 Test MSE 15.585353471450059 Test RE 0.37753423027795646\n",
      "1300 Train Loss 10.7551155 Test MSE 12.07267414559502 Test RE 0.3322765905136657\n",
      "1400 Train Loss 8.383478 Test MSE 10.04582640660986 Test RE 0.30310342287087194\n",
      "1500 Train Loss 6.982073 Test MSE 8.648029493896082 Test RE 0.2812267236174168\n",
      "1600 Train Loss 6.124878 Test MSE 7.583799245053258 Test RE 0.26335491024827257\n",
      "1700 Train Loss 5.5050354 Test MSE 6.645775791147134 Test RE 0.24653060917853623\n",
      "1800 Train Loss 4.745007 Test MSE 5.417046053522571 Test RE 0.2225764801077399\n",
      "1900 Train Loss 4.0908165 Test MSE 4.519734968416871 Test RE 0.20330801058203468\n",
      "2000 Train Loss 3.607518 Test MSE 3.9543171399447647 Test RE 0.19016638674734535\n",
      "2100 Train Loss 3.2257378 Test MSE 3.5544825379786147 Test RE 0.18029604441744013\n",
      "2200 Train Loss 2.9078395 Test MSE 3.2544777466839907 Test RE 0.1725196861145716\n",
      "2300 Train Loss 2.6452556 Test MSE 3.025081036901099 Test RE 0.16632843941638484\n",
      "2400 Train Loss 2.4312744 Test MSE 2.84416553440582 Test RE 0.1612781157518781\n",
      "2500 Train Loss 2.253638 Test MSE 2.696957610349485 Test RE 0.15704896058031614\n",
      "2600 Train Loss 2.1106234 Test MSE 2.578375340406753 Test RE 0.15355751509078974\n",
      "2700 Train Loss 1.9947759 Test MSE 2.481005291427788 Test RE 0.1506301301808548\n",
      "2800 Train Loss 1.8960674 Test MSE 2.3966153315269225 Test RE 0.14804616872104495\n",
      "2900 Train Loss 1.8039545 Test MSE 2.3181631904252353 Test RE 0.1456028946100533\n",
      "Training time: 71.86\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 103.07 Test MSE 48.78265538402947 Test RE 0.6679296528087896\n",
      "100 Train Loss 3.9925911 Test MSE 5.299168105723335 Test RE 0.22014146616917762\n",
      "200 Train Loss 0.7650958 Test MSE 0.7072738784536025 Test RE 0.0804251381381072\n",
      "300 Train Loss 0.45439962 Test MSE 0.3094183020071608 Test RE 0.0531950235419773\n",
      "400 Train Loss 0.28770474 Test MSE 0.18533474906656414 Test RE 0.04116957970342159\n",
      "500 Train Loss 0.28306594 Test MSE 0.27145890385597593 Test RE 0.04982531484891098\n",
      "600 Train Loss 1.5641952 Test MSE 3.5988206680777703 Test RE 0.18141705354941082\n",
      "700 Train Loss 0.5669463 Test MSE 0.5541392420475101 Test RE 0.07118810942569684\n",
      "800 Train Loss 0.23097286 Test MSE 0.17817669240580272 Test RE 0.04036671909420621\n",
      "900 Train Loss 0.14241806 Test MSE 0.07527732394970045 Test RE 0.026237961598266354\n",
      "1000 Train Loss 0.16473708 Test MSE 0.09601634759735793 Test RE 0.02963265720000027\n",
      "1100 Train Loss 0.21940713 Test MSE 0.19538052901196298 Test RE 0.042270622813162\n",
      "1200 Train Loss 0.27293307 Test MSE 0.08314816245974858 Test RE 0.02757555957768809\n",
      "1300 Train Loss 0.38510722 Test MSE 0.8751962289175501 Test RE 0.08946449710036028\n",
      "1400 Train Loss 0.24707332 Test MSE 0.2794038008937132 Test RE 0.05054918536836101\n",
      "1500 Train Loss 0.14115623 Test MSE 0.07647525809364285 Test RE 0.026445907942719078\n",
      "1600 Train Loss 0.2337545 Test MSE 0.29873459176142897 Test RE 0.05226858742798383\n",
      "1700 Train Loss 0.2725347 Test MSE 0.22836916752153402 Test RE 0.04570005687311064\n",
      "1800 Train Loss 0.33513325 Test MSE 0.21792086926817966 Test RE 0.044642387973167844\n",
      "1900 Train Loss 0.22706895 Test MSE 0.11696139398409426 Test RE 0.03270538561085005\n",
      "2000 Train Loss 0.63531196 Test MSE 0.5848249311894294 Test RE 0.07313258849671361\n",
      "2100 Train Loss 0.3978535 Test MSE 0.40450804392585893 Test RE 0.06082212470299622\n",
      "2200 Train Loss 0.5502845 Test MSE 0.28986782355263835 Test RE 0.05148705021096209\n",
      "2300 Train Loss 0.13469289 Test MSE 0.06486175220278105 Test RE 0.02435523613009854\n",
      "2400 Train Loss 0.18405345 Test MSE 0.1435059079254124 Test RE 0.03622704382858809\n",
      "2500 Train Loss 0.50076365 Test MSE 0.26754386597779106 Test RE 0.049464714334250745\n",
      "2600 Train Loss 0.14641808 Test MSE 0.09500381505965518 Test RE 0.029475998717077935\n",
      "2700 Train Loss 1.6114273 Test MSE 1.3417829566918744 Test RE 0.11077433249936247\n",
      "2800 Train Loss 621.4062 Test MSE 484.6306225668585 Test RE 2.105248819398911\n",
      "2900 Train Loss 81.139595 Test MSE 88.20951669353728 Test RE 0.8981641730123376\n",
      "Training time: 29.89\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 105.634254 Test MSE 96.4967984137983 Test RE 0.9394084414749866\n",
      "100 Train Loss 4.1008096 Test MSE 4.740281565331123 Test RE 0.20820927676334405\n",
      "200 Train Loss 3.3799398 Test MSE 4.417664451705924 Test RE 0.20099921852872388\n",
      "300 Train Loss 3.4034493 Test MSE 4.444651210021814 Test RE 0.2016122187396757\n",
      "400 Train Loss 52728.316 Test MSE 30382.394115496874 Test RE 16.66897912400588\n",
      "500 Train Loss 8.3566265 Test MSE 11.025077771112315 Test RE 0.3175329783957296\n",
      "600 Train Loss 3.4403005 Test MSE 4.590729331561714 Test RE 0.20489853344900827\n",
      "700 Train Loss 2.0661712 Test MSE 2.7308360792705817 Test RE 0.15803228592342494\n",
      "800 Train Loss 1.4797697 Test MSE 1.89750274736344 Test RE 0.1317313713240227\n",
      "900 Train Loss 1.1448265 Test MSE 1.4219392018496506 Test RE 0.11403509270347684\n",
      "1000 Train Loss 0.91969264 Test MSE 1.1118554728019243 Test RE 0.10083752444198447\n",
      "1100 Train Loss 0.7673561 Test MSE 0.9075899038750993 Test RE 0.09110513043072306\n",
      "1200 Train Loss 0.6484326 Test MSE 0.7650729951158292 Test RE 0.08364682145679354\n",
      "1300 Train Loss 0.5612412 Test MSE 0.6605286811807318 Test RE 0.07772197826665216\n",
      "1400 Train Loss 0.5070102 Test MSE 0.5965842688183396 Test RE 0.07386418407601293\n",
      "1500 Train Loss 0.4695919 Test MSE 0.5543094674660686 Test RE 0.0711990426842681\n",
      "1600 Train Loss 0.44156906 Test MSE 0.5222197508811268 Test RE 0.06910741572247053\n",
      "1700 Train Loss 0.41413194 Test MSE 0.49167924698018545 Test RE 0.06705620096496728\n",
      "1800 Train Loss 0.3837396 Test MSE 0.4661593431809173 Test RE 0.06529278640427405\n",
      "1900 Train Loss 0.3533139 Test MSE 0.4298456834288949 Test RE 0.06269808713669334\n",
      "2000 Train Loss 0.3335321 Test MSE 0.41029800241713565 Test RE 0.06125586930028645\n",
      "2100 Train Loss 0.31332517 Test MSE 0.3928533108255085 Test RE 0.059939513677655334\n",
      "2200 Train Loss 0.2931953 Test MSE 0.37627102428807035 Test RE 0.05866085571936995\n",
      "2300 Train Loss 0.27513587 Test MSE 0.36088321228177017 Test RE 0.057448850872502\n",
      "2400 Train Loss 0.26070088 Test MSE 0.3473787656652138 Test RE 0.05636371886068437\n",
      "2500 Train Loss 0.2423301 Test MSE 0.3043509181880296 Test RE 0.05275763444138894\n",
      "2600 Train Loss 0.22858836 Test MSE 0.2937442199112688 Test RE 0.051830174496555845\n",
      "2700 Train Loss 0.22079392 Test MSE 0.2851031085559458 Test RE 0.0510621365243984\n",
      "2800 Train Loss 0.21404801 Test MSE 0.27841462085103913 Test RE 0.05045962577802276\n",
      "2900 Train Loss 0.20823583 Test MSE 0.2730082072927625 Test RE 0.04996729711605805\n",
      "Training time: 10.14\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 124.36448 Test MSE 95.90636843195662 Test RE 0.9365300768122593\n",
      "100 Train Loss 5.855965 Test MSE 5.973146249946682 Test RE 0.23372199266764382\n",
      "200 Train Loss 3.8799167 Test MSE 4.057978584257874 Test RE 0.1926428441735502\n",
      "300 Train Loss 5.0973024 Test MSE 4.826703768792473 Test RE 0.21009868245533758\n",
      "400 Train Loss 3.9384384 Test MSE 4.308919334236737 Test RE 0.19850990801075213\n",
      "500 Train Loss 6.6378584 Test MSE 5.6558727638741475 Test RE 0.22743003678860033\n",
      "600 Train Loss 3.9168994 Test MSE 3.7599566860149722 Test RE 0.18543402585937394\n",
      "700 Train Loss 17783.633 Test MSE 773285.5299998581 Test RE 84.09456878345354\n",
      "800 Train Loss 41.384876 Test MSE 38.062072225656884 Test RE 0.5899894158780813\n",
      "900 Train Loss 24.375027 Test MSE 22.765663102778017 Test RE 0.4562870661525221\n",
      "1000 Train Loss 18.21098 Test MSE 17.165141839121258 Test RE 0.396206598775483\n",
      "1100 Train Loss 14.825352 Test MSE 13.913838214850738 Test RE 0.3567150893751542\n",
      "1200 Train Loss 12.509626 Test MSE 11.684681530856611 Test RE 0.3268936212721179\n",
      "1300 Train Loss 10.9177885 Test MSE 10.14880024276632 Test RE 0.30465292935412314\n",
      "1400 Train Loss 9.866717 Test MSE 9.126506117441851 Test RE 0.28890182199644776\n",
      "1500 Train Loss 9.167046 Test MSE 8.440998617359295 Test RE 0.2778400962376123\n",
      "1600 Train Loss 8.68127 Test MSE 7.966567790568277 Test RE 0.2699191105877277\n",
      "1700 Train Loss 8.308715 Test MSE 7.610326577688973 Test RE 0.2638151020714869\n",
      "1800 Train Loss 7.9951077 Test MSE 7.314854814392358 Test RE 0.2586430788960918\n",
      "1900 Train Loss 7.6921844 Test MSE 7.039108163958794 Test RE 0.253721240159004\n",
      "2000 Train Loss 7.5243206 Test MSE 6.892361304376224 Test RE 0.2510626010794125\n",
      "2100 Train Loss 7.3857603 Test MSE 6.766788779962919 Test RE 0.2487650224493686\n",
      "2200 Train Loss 7.2620096 Test MSE 6.6538581520807805 Test RE 0.24668047459515596\n",
      "2300 Train Loss 7.1463265 Test MSE 6.5471006263909715 Test RE 0.24469354572096533\n",
      "2400 Train Loss 7.031147 Test MSE 6.44145471034452 Test RE 0.24271129361170324\n",
      "2500 Train Loss 6.894035 Test MSE 6.317353642247215 Test RE 0.24036188472664657\n",
      "2600 Train Loss 6.775677 Test MSE 6.219431996400126 Test RE 0.23849175417723248\n",
      "2700 Train Loss 6.668672 Test MSE 6.121042112272113 Test RE 0.23659779337800363\n",
      "2800 Train Loss 6.5624933 Test MSE 6.024258934039479 Test RE 0.23471985129893325\n",
      "2900 Train Loss 6.4567895 Test MSE 5.927898180256231 Test RE 0.2328350586515462\n",
      "Training time: 8.58\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 122.418495 Test MSE 95.75511807988056 Test RE 0.9357913021189007\n",
      "100 Train Loss 3.668239 Test MSE 4.30947292864988 Test RE 0.19852265951966144\n",
      "200 Train Loss 4.0151377 Test MSE 4.5724093949576075 Test RE 0.20448928686695225\n",
      "300 Train Loss 4.0839314 Test MSE 4.70803150082275 Test RE 0.2074998017726703\n",
      "400 Train Loss 3490.758 Test MSE 2510.011587385905 Test RE 4.791106992042358\n",
      "500 Train Loss 86.86425 Test MSE 76.79130153757671 Test RE 0.8380192993503369\n",
      "600 Train Loss 82.738785 Test MSE 73.16385111386963 Test RE 0.8179867799867168\n",
      "700 Train Loss 79.09113 Test MSE 70.01227619418661 Test RE 0.8001752327752053\n",
      "800 Train Loss 75.98575 Test MSE 67.38848822355085 Test RE 0.785038331827163\n",
      "900 Train Loss 73.43245 Test MSE 65.29239915761761 Test RE 0.7727327512911936\n",
      "1000 Train Loss 71.401436 Test MSE 63.68672413687233 Test RE 0.76317205850833\n",
      "1100 Train Loss 69.837036 Test MSE 62.51053303294411 Test RE 0.756091937887743\n",
      "1200 Train Loss 68.669754 Test MSE 61.691203740817755 Test RE 0.7511205228904633\n",
      "1300 Train Loss 67.82608 Test MSE 61.15389126094529 Test RE 0.7478423484150002\n",
      "1400 Train Loss 67.2356 Test MSE 60.82845238545847 Test RE 0.7458498208476532\n",
      "1500 Train Loss 66.83564 Test MSE 60.653785808166084 Test RE 0.7447782113891942\n",
      "1600 Train Loss 66.57363 Test MSE 60.5799443443222 Test RE 0.7443247173404199\n",
      "1700 Train Loss 66.40782 Test MSE 60.568463681959706 Test RE 0.7442541845447801\n",
      "1800 Train Loss 66.30655 Test MSE 60.59146609159862 Test RE 0.744395495827022\n",
      "1900 Train Loss 66.24693 Test MSE 60.630041301278304 Test RE 0.7446324156880069\n",
      "2000 Train Loss 66.213165 Test MSE 60.67234198328733 Test RE 0.7448921299011866\n",
      "2100 Train Loss 66.19477 Test MSE 60.71171247266674 Test RE 0.7451337722271104\n",
      "2200 Train Loss 66.185165 Test MSE 60.745044266240484 Test RE 0.7453382899063723\n",
      "2300 Train Loss 66.18036 Test MSE 60.77146449459723 Test RE 0.7455003596466546\n",
      "2400 Train Loss 66.17806 Test MSE 60.791361603567985 Test RE 0.7456223913277857\n",
      "2500 Train Loss 66.17702 Test MSE 60.805714980217154 Test RE 0.7457104101440466\n",
      "2600 Train Loss 66.17657 Test MSE 60.81568024812089 Test RE 0.7457715137719998\n",
      "2700 Train Loss 66.17637 Test MSE 60.82236076486664 Test RE 0.7458124736219364\n",
      "2800 Train Loss 66.1763 Test MSE 60.82668922995763 Test RE 0.745839011278345\n",
      "2900 Train Loss 66.17628 Test MSE 60.82940134639534 Test RE 0.7458556386808923\n",
      "Training time: 8.95\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 109.8178 Test MSE 102.0686248029121 Test RE 0.9661490649366303\n",
      "100 Train Loss 2.2760055 Test MSE 3.8880408544015994 Test RE 0.188566011910264\n",
      "200 Train Loss 2.7172432 Test MSE 4.919627222105428 Test RE 0.21211144576077626\n",
      "300 Train Loss 2.5451152 Test MSE 4.6830376299852166 Test RE 0.20694828414844735\n",
      "400 Train Loss 2.5261219 Test MSE 4.6281840813537425 Test RE 0.20573269647631193\n",
      "500 Train Loss 2.4578674 Test MSE 4.580335937140448 Test RE 0.20466645725439106\n",
      "600 Train Loss 2.5732064 Test MSE 4.727002431800499 Test RE 0.20791743989884648\n",
      "700 Train Loss 2.3773372 Test MSE 4.571016919877178 Test RE 0.20445814705706922\n",
      "800 Train Loss 2.1558611 Test MSE 4.331006335782238 Test RE 0.1990180266443728\n",
      "900 Train Loss 2.18943 Test MSE 4.377796094101388 Test RE 0.20009017814621521\n",
      "1000 Train Loss 2.65644 Test MSE 5.030320030879225 Test RE 0.21448445126627683\n",
      "1100 Train Loss 2.5879636 Test MSE 4.7948224598451725 Test RE 0.20940366171580457\n",
      "1200 Train Loss 2.6202352 Test MSE 4.905742227920234 Test RE 0.21181190607202022\n",
      "1300 Train Loss 2.5705452 Test MSE 4.78440725920643 Test RE 0.20917610722065816\n",
      "1400 Train Loss 2.5718327 Test MSE 4.7809212442786 Test RE 0.20909988838444585\n",
      "1500 Train Loss 2.5690935 Test MSE 4.79304906005633 Test RE 0.20936493340457463\n",
      "1600 Train Loss 2.5705898 Test MSE 4.7920205954006745 Test RE 0.20934247004321022\n",
      "1700 Train Loss 2.5687892 Test MSE 4.793076816252106 Test RE 0.20936553961218837\n",
      "1800 Train Loss 2.5695007 Test MSE 4.797164043797292 Test RE 0.20945478732136677\n",
      "1900 Train Loss 2.568724 Test MSE 4.791618854809942 Test RE 0.20933369471256974\n",
      "2000 Train Loss 2.5690453 Test MSE 4.794925695533272 Test RE 0.20940591600310385\n",
      "2100 Train Loss 2.572337 Test MSE 4.808216559445457 Test RE 0.2096959371344652\n",
      "2200 Train Loss 2.5688164 Test MSE 4.785972490761289 Test RE 0.20921032068243126\n",
      "2300 Train Loss 2.5707395 Test MSE 4.80377036118718 Test RE 0.2095989609129763\n",
      "2400 Train Loss 2.573758 Test MSE 4.798929936877251 Test RE 0.20949333517209265\n",
      "2500 Train Loss 2.5738294 Test MSE 4.78348675874439 Test RE 0.20915598393734394\n",
      "2600 Train Loss 2.5790741 Test MSE 4.819060130202926 Test RE 0.20993225887035444\n",
      "2700 Train Loss 2.5849426 Test MSE 4.825784579895882 Test RE 0.21007867609275516\n",
      "2800 Train Loss 2.5753047 Test MSE 4.8106401871378015 Test RE 0.20974878010089124\n",
      "2900 Train Loss 2.5966752 Test MSE 4.7667632872009005 Test RE 0.2087900503787423\n",
      "Training time: 8.41\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 111.55627 Test MSE 108.3984194329259 Test RE 0.9956563801650414\n",
      "100 Train Loss 4.2636647 Test MSE 4.482362887011392 Test RE 0.20246572500247004\n",
      "200 Train Loss 4.5303254 Test MSE 4.634981504711854 Test RE 0.20588372105866423\n",
      "300 Train Loss 4.4357357 Test MSE 4.670823395769489 Test RE 0.20667822811285486\n",
      "400 Train Loss 4.354918 Test MSE 4.564656020070103 Test RE 0.20431583840080136\n",
      "500 Train Loss 4.987128 Test MSE 5.19266120177976 Test RE 0.21791794762918723\n",
      "600 Train Loss 4.36805 Test MSE 4.560652506433443 Test RE 0.20422621930610807\n",
      "700 Train Loss 4.435245 Test MSE 4.643822901543141 Test RE 0.20607999287032344\n",
      "800 Train Loss 4.5092335 Test MSE 4.624581171942204 Test RE 0.2056526023649532\n",
      "900 Train Loss 4.497955 Test MSE 4.66584351820634 Test RE 0.20656802198179003\n",
      "1000 Train Loss 4.4387655 Test MSE 4.613628028931785 Test RE 0.2054089178422656\n",
      "1100 Train Loss 4.4284735 Test MSE 4.813527163598355 Test RE 0.20981170820867628\n",
      "1200 Train Loss 4.43279 Test MSE 4.6296992348208885 Test RE 0.20576636962921413\n",
      "1300 Train Loss 4.497714 Test MSE 4.636877328837209 Test RE 0.2059258225665729\n",
      "1400 Train Loss 4.4427958 Test MSE 4.688350644049628 Test RE 0.20706564466852717\n",
      "1500 Train Loss 4.4254217 Test MSE 4.665963055415856 Test RE 0.20657066806354077\n",
      "1600 Train Loss 4.418719 Test MSE 4.614882886479319 Test RE 0.20543685045893229\n",
      "1700 Train Loss 4.46627 Test MSE 4.61776892429554 Test RE 0.20550107807496\n",
      "1800 Train Loss 4.4285245 Test MSE 4.545370468068448 Test RE 0.20388376701508193\n",
      "1900 Train Loss 56958.695 Test MSE 4113.381953842357 Test RE 6.133346849714161\n",
      "2000 Train Loss 65.96832 Test MSE 63.87676686683412 Test RE 0.7643098722937789\n",
      "2100 Train Loss 32.733047 Test MSE 37.159708024840086 Test RE 0.5829538201826943\n",
      "2200 Train Loss 28.087475 Test MSE 32.59619568514566 Test RE 0.5459859466117158\n",
      "2300 Train Loss 24.906729 Test MSE 29.490996000061028 Test RE 0.5193291811035267\n",
      "2400 Train Loss 22.24969 Test MSE 26.815053537311275 Test RE 0.49520764462523875\n",
      "2500 Train Loss 19.98515 Test MSE 24.502244335168818 Test RE 0.47337022718618305\n",
      "2600 Train Loss 18.048302 Test MSE 22.511535167679476 Test RE 0.45373320457877364\n",
      "2700 Train Loss 16.41427 Test MSE 20.79019144818286 Test RE 0.4360409231849024\n",
      "2800 Train Loss 15.049348 Test MSE 19.331374162867686 Test RE 0.4204645344106187\n",
      "2900 Train Loss 13.906869 Test MSE 18.09865667006483 Test RE 0.4068376873867238\n",
      "Training time: 8.19\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 3000\n",
    "layers = np.array([1,50,50,50,50,1])\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []\n",
    "    beta_val = []\n",
    "    'Generate Training data'\n",
    "    N_train = 300\n",
    "    \n",
    "    x_train_np_array, u_train_np_array = trainingdata(N_train,reps*1234)\n",
    "\n",
    "    'Convert to tensor and send to GPU'\n",
    "    x_train = torch.from_numpy(x_train_np_array).float().to(device)\n",
    "    u_train = torch.from_numpy(u_train_np_array).float().to(device)\n",
    "    \n",
    "    x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "       \n",
    "     \n",
    "    model_NN = Sequentialmodel(layers)\n",
    "    model_NN.to(device)\n",
    "    print(model_NN)\n",
    "    params = list(model_NN.parameters())\n",
    "    #optimizer = optim.Adam(PINN.parameters(), lr=0.0008,betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "    optimizer = optim.Adam(model_NN.parameters(), lr=0.08)\n",
    "\n",
    "    train_model(max_iter,reps)\n",
    " \n",
    "    torch.save(model_NN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    beta_full.append(beta_val)\n",
    "                 \n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse\": test_mse_full, \"test_re\": test_re_full,\"Time\": elapsed_time, \"Thresh_time\": time_threshold, \"epoch_thresh\": epoch_threshold,\"beta\": beta_full, \"label\": label}\n",
    "savemat(label+'.mat', mdic) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred = model_NN.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f514df853d0>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_test,u_pred,'rx')\n",
    "plt.plot(x_test,u_true,'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3088965140896882\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + test_re_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
