{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:2\n"
     ]
    }
   ],
   "source": [
    "#Trying to replicate the results of \"Adaptive activation functions accelerate convergence in deep and physics-informed neural networks\"\n",
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_disc(x): #[-4,3.75]\n",
    "    y  = np.zeros((np.shape(x)[0],))\n",
    "    \n",
    "    y = 0.2*np.sin(6*x)*np.exp(-1*x)\n",
    "    y[x>0] = 1 + 0.1*x[x>0]*np.exp(x[x>0]) + 10\n",
    "    \n",
    "    return scale*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain bounds\n",
    "level = \"high\"\n",
    "label = \"Regr_disc_swish_\" + level\n",
    "loss_thresh = 0.1\n",
    "scale = 50.0\n",
    "\n",
    "lb = np.array(-4.0)  # [-1. 0.]\n",
    "ub = np.array(3.75) # [1.  0.99]\n",
    "\n",
    "x_test = np.linspace(lb,ub,1000).reshape(-1,1)\n",
    "u_true = true_disc(x_test)\n",
    "\n",
    "u_true_norm = np.linalg.norm(u_true,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_train,seed):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    x_train = np.random.uniform(low=lb, high=ub, size=(N_train,)).reshape(-1,1)\n",
    "\n",
    "    u_train = true_disc(x_train)\n",
    "\n",
    "    return x_train, u_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Sigmoid()\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "     \n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "\n",
    "        \n",
    "        self.beta = Parameter(torch.ones((layers[1],len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "            \n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = 2.0*(x - l_b)/(u_b - l_b)-1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = z*self.activation(self.beta[:,i]*z)\n",
    "    \n",
    "        a = self.linears[-1](a) \n",
    "        \n",
    "        return a\n",
    "    \n",
    "    \n",
    "    def loss(self,x,u):\n",
    "\n",
    "        loss_val = self.loss_function(self.forward(x),u)\n",
    "        \n",
    "        return loss_val\n",
    "        \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        u_pred = self.forward(x_test_tensor)       \n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "        \n",
    "        return u_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "        \n",
    "        test_mse_loss = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re_loss = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "        \n",
    "        return test_mse_loss, test_re_loss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    beta_val.append(model_NN.beta.cpu().detach().numpy())\n",
    "    test_mse, test_re = model_NN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step():\n",
    "    optimizer.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "    loss = model_NN.loss(x_train,u_train)\n",
    "    loss.backward() #backprop\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "   \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "    \n",
    "    loss_np = model_NN.loss(x_train,u_train).cpu().detach().numpy()\n",
    "    data_update(loss_np)\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        loss_np = train_step().cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1\n",
    "        data_update(loss_np)\n",
    "        if(i%100==0):        \n",
    "            print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 290537.62 Test MSE 271500.94917109644 Test RE 0.9965834666684225\n",
      "100 Train Loss 9440.17 Test MSE 9544.052213844046 Test RE 0.18685056058121155\n",
      "200 Train Loss 11459.162 Test MSE 11329.59032174481 Test RE 0.20357999757470396\n",
      "300 Train Loss 12660.562 Test MSE 11912.393147122953 Test RE 0.20875049350842811\n",
      "400 Train Loss 11827.69 Test MSE 11496.911919781878 Test RE 0.20507777831018498\n",
      "500 Train Loss 12391.171 Test MSE 11466.3831738884 Test RE 0.2048053169247746\n",
      "600 Train Loss 12181.671 Test MSE 11365.08048338245 Test RE 0.20389860740430163\n",
      "700 Train Loss 12121.574 Test MSE 11368.375088373436 Test RE 0.203928159183253\n",
      "800 Train Loss 13022.73 Test MSE 13310.22268240237 Test RE 0.22065850046739308\n",
      "900 Train Loss 12048.487 Test MSE 11376.57867549177 Test RE 0.20400172469769992\n",
      "1000 Train Loss 12015.124 Test MSE 11344.530674984935 Test RE 0.2037141840246906\n",
      "1100 Train Loss 12017.875 Test MSE 11346.762044392915 Test RE 0.2037342174338001\n",
      "1200 Train Loss 19985.121 Test MSE 17631.104607129746 Test RE 0.25396142946423517\n",
      "1300 Train Loss 4428.8896 Test MSE 5943.649136473336 Test RE 0.14745332971799655\n",
      "1400 Train Loss 2992.328 Test MSE 3323.7624109435064 Test RE 0.11026634717914577\n",
      "1500 Train Loss 1958.0 Test MSE 4999.622211896417 Test RE 0.13523733260876444\n",
      "1600 Train Loss 1006.9665 Test MSE 1316.3561156915332 Test RE 0.06939284600782528\n",
      "1700 Train Loss 302.45746 Test MSE 1310.5791972512882 Test RE 0.06924041096632014\n",
      "1800 Train Loss 224.40138 Test MSE 775.8274955885092 Test RE 0.0532734208877326\n",
      "1900 Train Loss 146.20195 Test MSE 915.8255702014986 Test RE 0.05788078124172914\n",
      "2000 Train Loss 134.73802 Test MSE 951.9496286859496 Test RE 0.059011273384010486\n",
      "2100 Train Loss 149.23894 Test MSE 761.607277937772 Test RE 0.05278293610023649\n",
      "2200 Train Loss 1085.3894 Test MSE 1653.2910545397208 Test RE 0.07776830972743468\n",
      "2300 Train Loss 273.7287 Test MSE 596.0140331445133 Test RE 0.04669348021406401\n",
      "2400 Train Loss 112.47438 Test MSE 720.5866291210647 Test RE 0.05134180128582632\n",
      "2500 Train Loss 101.0067 Test MSE 729.4516540887405 Test RE 0.051656652419924586\n",
      "2600 Train Loss 94.838776 Test MSE 740.4652772091857 Test RE 0.05204516033101764\n",
      "2700 Train Loss 88.921776 Test MSE 746.1670952338346 Test RE 0.05224515819728881\n",
      "2800 Train Loss 83.449234 Test MSE 777.4364630370566 Test RE 0.05332863342272764\n",
      "2900 Train Loss 12715.938 Test MSE 13523.905484002216 Test RE 0.22242267793415688\n",
      "Training time: 8.57\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 292763.22 Test MSE 272567.40015802626 Test RE 0.9985388296189482\n",
      "100 Train Loss 11091.817 Test MSE 12504.6768310656 Test RE 0.21387707598192932\n",
      "200 Train Loss 10368.162 Test MSE 11419.03531477005 Test RE 0.20438203068184793\n",
      "300 Train Loss 10029.715 Test MSE 11415.533989533185 Test RE 0.20435069428582478\n",
      "400 Train Loss 10415.815 Test MSE 11891.322700432253 Test RE 0.20856579456520968\n",
      "500 Train Loss 10977.465 Test MSE 12396.780184005785 Test RE 0.21295235738298612\n",
      "600 Train Loss 9796.778 Test MSE 11169.026968683247 Test RE 0.20213227854260982\n",
      "700 Train Loss 10089.578 Test MSE 12367.283736844724 Test RE 0.21269886096244375\n",
      "800 Train Loss 9604.715 Test MSE 11035.883915905391 Test RE 0.20092388346080803\n",
      "900 Train Loss 9312.908 Test MSE 10696.891041750161 Test RE 0.19781389203888491\n",
      "1000 Train Loss 9937.967 Test MSE 10386.456235753083 Test RE 0.19492237735773132\n",
      "1100 Train Loss 8526.958 Test MSE 9784.528255413235 Test RE 0.18918990001747515\n",
      "1200 Train Loss 9685.8955 Test MSE 13456.81695451013 Test RE 0.22187030190304666\n",
      "1300 Train Loss 6943.2866 Test MSE 7767.097779069371 Test RE 0.16856111215899675\n",
      "1400 Train Loss 10229.926 Test MSE 11429.695806702342 Test RE 0.2044774110991362\n",
      "1500 Train Loss 8936.17 Test MSE 10253.525941610678 Test RE 0.19367101074778936\n",
      "1600 Train Loss 8819.86 Test MSE 10127.0305145379 Test RE 0.19247266565384583\n",
      "1700 Train Loss 8717.05 Test MSE 10024.835639990586 Test RE 0.1914990537391202\n",
      "1800 Train Loss 8468.269 Test MSE 9708.937927155192 Test RE 0.18845769025140574\n",
      "1900 Train Loss 9642.483 Test MSE 10894.808316197923 Test RE 0.19963551232796256\n",
      "2000 Train Loss 9029.01 Test MSE 10329.154919023666 Test RE 0.19438394748644128\n",
      "2100 Train Loss 6497.346 Test MSE 8686.44384681867 Test RE 0.17825799200672066\n",
      "2200 Train Loss 4531.046 Test MSE 5438.889619268975 Test RE 0.14105325882756062\n",
      "2300 Train Loss 5186.4854 Test MSE 6758.6794823738655 Test RE 0.1572385127975862\n",
      "2400 Train Loss 2719.803 Test MSE 3328.169825292577 Test RE 0.11033943129558887\n",
      "2500 Train Loss 1384.1057 Test MSE 1708.3747671256078 Test RE 0.07905322211324985\n",
      "2600 Train Loss 640.9042 Test MSE 869.7472239579212 Test RE 0.05640589926398698\n",
      "2700 Train Loss 938.3913 Test MSE 1175.3832002434563 Test RE 0.06557189599365616\n",
      "2800 Train Loss 434.2554 Test MSE 726.3933262292654 Test RE 0.05154824981517108\n",
      "2900 Train Loss 271.87964 Test MSE 401.2866012369605 Test RE 0.03831380913170212\n",
      "Training time: 8.33\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 279586.28 Test MSE 271914.9995705633 Test RE 0.9973430930745901\n",
      "100 Train Loss 11002.364 Test MSE 12446.195682904363 Test RE 0.21337636591376613\n",
      "200 Train Loss 17529.6 Test MSE 13579.676427011467 Test RE 0.22288082813047966\n",
      "300 Train Loss 9554.75 Test MSE 11476.638672524952 Test RE 0.20489688508462975\n",
      "400 Train Loss 10497.684 Test MSE 11911.284545193335 Test RE 0.20874077981859737\n",
      "500 Train Loss 12291.826 Test MSE 12107.358321866408 Test RE 0.21045182667836962\n",
      "600 Train Loss 9728.806 Test MSE 11458.85595588 Test RE 0.20473808267438542\n",
      "700 Train Loss 9475.351 Test MSE 11452.314384673069 Test RE 0.2046796344335357\n",
      "800 Train Loss 9369.777 Test MSE 11479.347534537585 Test RE 0.2049210648404132\n",
      "900 Train Loss 8869.493 Test MSE 11153.67666743316 Test RE 0.20199332918737029\n",
      "1000 Train Loss 15184.113 Test MSE 15882.764575757918 Test RE 0.24104107253964902\n",
      "1100 Train Loss 9435.004 Test MSE 11310.690394173575 Test RE 0.2034101214762381\n",
      "1200 Train Loss 11144.092 Test MSE 12012.204379273875 Test RE 0.2096232057050049\n",
      "1300 Train Loss 9410.88 Test MSE 11295.302306662517 Test RE 0.20327170560678115\n",
      "1400 Train Loss 31625.977 Test MSE 27553.46829614783 Test RE 0.3174798296322183\n",
      "1500 Train Loss 17814.691 Test MSE 19448.704089579733 Test RE 0.26673090355064727\n",
      "1600 Train Loss 15651.662 Test MSE 17174.479148284936 Test RE 0.25065120069487623\n",
      "1700 Train Loss 15095.448 Test MSE 16605.194797990942 Test RE 0.24646201180908453\n",
      "1800 Train Loss 14546.317 Test MSE 16235.089505893033 Test RE 0.24369989691166935\n",
      "1900 Train Loss 15757.514 Test MSE 16724.204540343144 Test RE 0.2473436339078719\n",
      "2000 Train Loss 16009.025 Test MSE 18021.462304553985 Test RE 0.25675742747398045\n",
      "2100 Train Loss 15042.247 Test MSE 16408.280427332287 Test RE 0.24499630628254404\n",
      "2200 Train Loss 14861.959 Test MSE 15651.43475519561 Test RE 0.23927927239141455\n",
      "2300 Train Loss 13681.735 Test MSE 15381.033194290758 Test RE 0.23720331630550326\n",
      "2400 Train Loss 13946.455 Test MSE 16887.198139467346 Test RE 0.24854601326589235\n",
      "2500 Train Loss 15274.813 Test MSE 16534.779985451525 Test RE 0.2459388919712769\n",
      "2600 Train Loss 12638.954 Test MSE 14262.101466119693 Test RE 0.22841244505649097\n",
      "2700 Train Loss 11916.322 Test MSE 13629.669745954148 Test RE 0.2232907169797259\n",
      "2800 Train Loss 14168.407 Test MSE 14489.27389900751 Test RE 0.23022438045668914\n",
      "2900 Train Loss 11389.928 Test MSE 13080.292751974586 Test RE 0.21874429436397427\n",
      "Training time: 8.94\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 305593.1 Test MSE 272538.7022583773 Test RE 0.998486261480886\n",
      "100 Train Loss 8799.3 Test MSE 12248.209762608258 Test RE 0.21167243684065806\n",
      "200 Train Loss 8569.477 Test MSE 11960.50018742862 Test RE 0.20917157808480177\n",
      "300 Train Loss 8417.703 Test MSE 12237.617696708568 Test RE 0.2115808914893092\n",
      "400 Train Loss 8415.604 Test MSE 12239.751855011229 Test RE 0.21159933982796741\n",
      "500 Train Loss 8479.289 Test MSE 12275.204929826627 Test RE 0.2119055724540765\n",
      "600 Train Loss 8465.436 Test MSE 12147.581680607609 Test RE 0.21080112088562636\n",
      "700 Train Loss 2211.3406 Test MSE 3673.220338692347 Test RE 0.11591816351248437\n",
      "800 Train Loss 636.42554 Test MSE 1477.1985299635476 Test RE 0.07351017137498889\n",
      "900 Train Loss 243.98987 Test MSE 1033.7505844438756 Test RE 0.06149444494499878\n",
      "1000 Train Loss 998.1665 Test MSE 1519.3350082621746 Test RE 0.07455122332691798\n",
      "1100 Train Loss 60.484196 Test MSE 749.6949919364263 Test RE 0.05236852076173235\n",
      "1200 Train Loss 43.867622 Test MSE 682.1730806346011 Test RE 0.04995457755151838\n",
      "1300 Train Loss 98.568 Test MSE 725.7208137136978 Test RE 0.05152438198112632\n",
      "1400 Train Loss 30.089396 Test MSE 654.9869313790407 Test RE 0.04894905583605101\n",
      "1500 Train Loss 12.426598 Test MSE 616.7963242732078 Test RE 0.04750057756481088\n",
      "1600 Train Loss 16.424513 Test MSE 618.1235875801744 Test RE 0.047551657549486204\n",
      "1700 Train Loss 319.16214 Test MSE 1114.0883966746665 Test RE 0.06383925752345297\n",
      "1800 Train Loss 7.4721956 Test MSE 658.6142309789727 Test RE 0.04908440796273372\n",
      "1900 Train Loss 6.975397 Test MSE 600.034257970422 Test RE 0.046850693628861746\n",
      "2000 Train Loss 11.997969 Test MSE 617.9534937443858 Test RE 0.04754511452112109\n",
      "2100 Train Loss 39.27747 Test MSE 682.037722510524 Test RE 0.04994962126289311\n",
      "2200 Train Loss 6.1348495 Test MSE 645.9483012944391 Test RE 0.04861014114126672\n",
      "2300 Train Loss 5.4800725 Test MSE 636.3395326371138 Test RE 0.048247237748164806\n",
      "2400 Train Loss 5.0862613 Test MSE 584.0846561841408 Test RE 0.04622382716102573\n",
      "2500 Train Loss 8934.915 Test MSE 12271.217768711349 Test RE 0.21187115468738368\n",
      "2600 Train Loss 8398.612 Test MSE 11576.62075931919 Test RE 0.2057874591694166\n",
      "2700 Train Loss 10631.227 Test MSE 10285.355215730013 Test RE 0.19397137723661767\n",
      "2800 Train Loss 811.1277 Test MSE 1785.4595107735609 Test RE 0.08081705207793478\n",
      "2900 Train Loss 340.95987 Test MSE 1045.85740830707 Test RE 0.061853494447286834\n",
      "Training time: 9.44\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 258353.02 Test MSE 269623.1087142095 Test RE 0.9931310442517307\n",
      "100 Train Loss 10679.773 Test MSE 11690.399206894352 Test RE 0.20679625650091715\n",
      "200 Train Loss 10418.947 Test MSE 11585.685207676976 Test RE 0.20586800878910247\n",
      "300 Train Loss 10518.096 Test MSE 11723.160751554926 Test RE 0.20708581995887188\n",
      "400 Train Loss 10380.322 Test MSE 11456.886165542444 Test RE 0.20472048456373745\n",
      "500 Train Loss 11564.656 Test MSE 12438.393706570241 Test RE 0.21330947726874333\n",
      "600 Train Loss 10565.155 Test MSE 11485.199309322734 Test RE 0.20497328902536607\n",
      "700 Train Loss 10228.641 Test MSE 11239.635091733684 Test RE 0.20277018977066144\n",
      "800 Train Loss 10091.255 Test MSE 11353.913825894506 Test RE 0.2037984134167653\n",
      "900 Train Loss 10216.08 Test MSE 11236.461866514863 Test RE 0.2027415642461058\n",
      "1000 Train Loss 10259.838 Test MSE 11248.27303673465 Test RE 0.20284809182622487\n",
      "1100 Train Loss 10423.862 Test MSE 11454.778260168532 Test RE 0.20470165085735012\n",
      "1200 Train Loss 19429.074 Test MSE 89944.67051651869 Test RE 0.5736086864413145\n",
      "1300 Train Loss 145915.75 Test MSE 151521.6496643688 Test RE 0.7445011052552989\n",
      "1400 Train Loss 145898.03 Test MSE 151609.45112185425 Test RE 0.74471678009574\n",
      "1500 Train Loss 145898.03 Test MSE 151609.667411744 Test RE 0.7447173113114778\n",
      "1600 Train Loss 145898.03 Test MSE 151609.667411744 Test RE 0.7447173113114778\n",
      "1700 Train Loss 145898.03 Test MSE 151609.667411744 Test RE 0.7447173113114778\n",
      "1800 Train Loss 145898.03 Test MSE 151609.667411744 Test RE 0.7447173113114778\n",
      "1900 Train Loss 145898.03 Test MSE 151609.667411744 Test RE 0.7447173113114778\n",
      "2000 Train Loss 145898.03 Test MSE 151609.667411744 Test RE 0.7447173113114778\n",
      "2100 Train Loss 145898.03 Test MSE 151609.667411744 Test RE 0.7447173113114778\n",
      "2200 Train Loss 145898.03 Test MSE 151609.667411744 Test RE 0.7447173113114778\n",
      "2300 Train Loss 145898.03 Test MSE 151609.667411744 Test RE 0.7447173113114778\n",
      "2400 Train Loss 145898.03 Test MSE 151609.667411744 Test RE 0.7447173113114778\n",
      "2500 Train Loss 145898.03 Test MSE 151609.667411744 Test RE 0.7447173113114778\n",
      "2600 Train Loss 145898.03 Test MSE 151609.667411744 Test RE 0.7447173113114778\n",
      "2700 Train Loss 145898.03 Test MSE 151609.667411744 Test RE 0.7447173113114778\n",
      "2800 Train Loss 145898.03 Test MSE 151609.667411744 Test RE 0.7447173113114778\n",
      "2900 Train Loss 145898.03 Test MSE 151609.667411744 Test RE 0.7447173113114778\n",
      "Training time: 8.82\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 263849.2 Test MSE 272700.2490754857 Test RE 0.9987821430938577\n",
      "100 Train Loss 8064.24 Test MSE 9929.221289108775 Test RE 0.19058363093279357\n",
      "200 Train Loss 8345.622 Test MSE 11010.462534102691 Test RE 0.20069233392453495\n",
      "300 Train Loss 8360.575 Test MSE 11300.737288604065 Test RE 0.20332060404057856\n",
      "400 Train Loss 8275.72 Test MSE 11182.37842497898 Test RE 0.2022530569348362\n",
      "500 Train Loss 8942.028 Test MSE 11534.338259351962 Test RE 0.20541130588419293\n",
      "600 Train Loss 8268.846 Test MSE 11156.890500770534 Test RE 0.20202242838748172\n",
      "700 Train Loss 8262.416 Test MSE 11140.646030355605 Test RE 0.20187530214338523\n",
      "800 Train Loss 8259.242 Test MSE 11129.085442641213 Test RE 0.20177053250579338\n",
      "900 Train Loss 8336.813 Test MSE 11287.11816547647 Test RE 0.20319805083651582\n",
      "1000 Train Loss 8827.67 Test MSE 11468.594870368179 Test RE 0.20482506793741062\n",
      "1100 Train Loss 8253.841 Test MSE 11135.547360134802 Test RE 0.20182910134931387\n",
      "1200 Train Loss 8239.969 Test MSE 11106.29931777054 Test RE 0.20156387022903913\n",
      "1300 Train Loss 8236.595 Test MSE 11092.615384151304 Test RE 0.20143965978562442\n",
      "1400 Train Loss 8234.986 Test MSE 11082.91854417199 Test RE 0.2013515942054595\n",
      "1500 Train Loss 8240.529 Test MSE 11126.217960020753 Test RE 0.20174453707376133\n",
      "1600 Train Loss 8244.373 Test MSE 11127.828891937006 Test RE 0.20175914154007207\n",
      "1700 Train Loss 8234.431 Test MSE 11083.032128140287 Test RE 0.20135262598487774\n",
      "1800 Train Loss 8233.44 Test MSE 11074.833715340328 Test RE 0.2012781392686073\n",
      "1900 Train Loss 8233.082 Test MSE 11070.410115833605 Test RE 0.20123793718393884\n",
      "2000 Train Loss 8232.86 Test MSE 11067.390974124153 Test RE 0.20121049433073498\n",
      "2100 Train Loss 8232.885 Test MSE 11067.39769562491 Test RE 0.20121055543078498\n",
      "2200 Train Loss 24695.908 Test MSE 20854.52394170325 Test RE 0.2762028414127087\n",
      "2300 Train Loss 8362.459 Test MSE 11074.897887169127 Test RE 0.20127872240908212\n",
      "2400 Train Loss 8042.8154 Test MSE 10444.543300537594 Test RE 0.19546667669190024\n",
      "2500 Train Loss 7739.5986 Test MSE 9892.675746641575 Test RE 0.19023257606990726\n",
      "2600 Train Loss 6862.3784 Test MSE 8511.001952975843 Test RE 0.1764486528331388\n",
      "2700 Train Loss 4693.993 Test MSE 5382.635194711111 Test RE 0.14032190602875605\n",
      "2800 Train Loss 7837.7427 Test MSE 9847.09212838299 Test RE 0.18979379178217054\n",
      "2900 Train Loss 6400.4785 Test MSE 7611.496603788689 Test RE 0.16686414626608964\n",
      "Training time: 11.19\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 310695.94 Test MSE 272667.1176435951 Test RE 0.9987214682543367\n",
      "100 Train Loss 11700.275 Test MSE 12062.930864898528 Test RE 0.21006534961989956\n",
      "200 Train Loss 12150.052 Test MSE 12033.079119651351 Test RE 0.2098052676494664\n",
      "300 Train Loss 11258.747 Test MSE 11475.477459797965 Test RE 0.20488651903163208\n",
      "400 Train Loss 11180.464 Test MSE 11398.25776888399 Test RE 0.20419600400041046\n",
      "500 Train Loss 10957.664 Test MSE 11611.169321590634 Test RE 0.2060943001934981\n",
      "600 Train Loss 10943.589 Test MSE 11980.94497240116 Test RE 0.20935027638023995\n",
      "700 Train Loss 15128.247 Test MSE 14084.994647944603 Test RE 0.2269898014522078\n",
      "800 Train Loss 11104.223 Test MSE 11343.803576579443 Test RE 0.2037076556527356\n",
      "900 Train Loss 10872.38 Test MSE 11437.344939462671 Test RE 0.2045458211992484\n",
      "1000 Train Loss 10778.674 Test MSE 11320.832394381034 Test RE 0.2035012973095989\n",
      "1100 Train Loss 10743.297 Test MSE 11394.250611210648 Test RE 0.20416010739546922\n",
      "1200 Train Loss 26346.344 Test MSE 24034.653157359455 Test RE 0.29651517973369035\n",
      "1300 Train Loss 20735.197 Test MSE 17875.507640266856 Test RE 0.25571558278038575\n",
      "1400 Train Loss 12384.849 Test MSE 12349.600975286046 Test RE 0.21254674798872789\n",
      "1500 Train Loss 11509.275 Test MSE 11749.276846050596 Test RE 0.20731635779154833\n",
      "1600 Train Loss 11289.29 Test MSE 11581.741472244868 Test RE 0.2058329673543924\n",
      "1700 Train Loss 11221.94 Test MSE 11505.016576703589 Test RE 0.205150049552752\n",
      "1800 Train Loss 11169.869 Test MSE 11447.09296204838 Test RE 0.20463296959879382\n",
      "1900 Train Loss 11143.634 Test MSE 11410.766899416954 Test RE 0.20430802173548962\n",
      "2000 Train Loss 11139.027 Test MSE 11419.804308004754 Test RE 0.204388912425734\n",
      "2100 Train Loss 11083.658 Test MSE 11341.354198514875 Test RE 0.20368566197147842\n",
      "2200 Train Loss 11669.892 Test MSE 12453.826108094145 Test RE 0.21344176352486305\n",
      "2300 Train Loss 11033.578 Test MSE 11293.877955417141 Test RE 0.20325888879908197\n",
      "2400 Train Loss 10993.565 Test MSE 11263.714905686758 Test RE 0.202987281163902\n",
      "2500 Train Loss 12436.298 Test MSE 12149.268367931914 Test RE 0.21081575520736057\n",
      "2600 Train Loss 10962.684 Test MSE 11247.684370203286 Test RE 0.20284278383607568\n",
      "2700 Train Loss 10902.517 Test MSE 11225.839101729109 Test RE 0.2026457073283319\n",
      "2800 Train Loss 10851.685 Test MSE 11218.140922220562 Test RE 0.202576212720652\n",
      "2900 Train Loss 11478.446 Test MSE 11917.990594322615 Test RE 0.2087995320442495\n",
      "Training time: 82.94\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 306062.97 Test MSE 272657.58173734683 Test RE 0.9987040041067696\n",
      "100 Train Loss 10990.731 Test MSE 12714.640414228405 Test RE 0.21566518535688775\n",
      "200 Train Loss 9907.582 Test MSE 11871.834732075904 Test RE 0.20839482156462463\n",
      "300 Train Loss 9933.233 Test MSE 12457.238728014516 Test RE 0.2134710053705211\n",
      "400 Train Loss 11692.825 Test MSE 26224.61134106436 Test RE 0.30972947192728695\n",
      "500 Train Loss 9886.8955 Test MSE 12121.9336123153 Test RE 0.21057846346450534\n",
      "600 Train Loss 9882.222 Test MSE 12106.115341452345 Test RE 0.21044102356980085\n",
      "700 Train Loss 9879.741 Test MSE 12125.160440387903 Test RE 0.2106064893267487\n",
      "800 Train Loss 10056.056 Test MSE 11943.297924609476 Test RE 0.20902110280670105\n",
      "900 Train Loss 9941.755 Test MSE 11799.511213137863 Test RE 0.2077590785699536\n",
      "1000 Train Loss 9882.635 Test MSE 12168.327772539566 Test RE 0.21098105109396223\n",
      "1100 Train Loss 9882.476 Test MSE 12034.61305704167 Test RE 0.20981863986641056\n",
      "1200 Train Loss 10900.386 Test MSE 12600.301718266765 Test RE 0.21469329141121096\n",
      "1300 Train Loss 9878.009 Test MSE 12157.317077878059 Test RE 0.21088557479986847\n",
      "1400 Train Loss 10163.0 Test MSE 11659.182748486259 Test RE 0.2065199715842689\n",
      "1500 Train Loss 10262.083 Test MSE 12130.99757108321 Test RE 0.21065717689052804\n",
      "1600 Train Loss 10135.87 Test MSE 12092.644850546512 Test RE 0.21032391197747363\n",
      "1700 Train Loss 10002.285 Test MSE 11973.384311564696 Test RE 0.20928420996262098\n",
      "1800 Train Loss 9937.187 Test MSE 12344.139666238803 Test RE 0.21249974599082982\n",
      "1900 Train Loss 9958.367 Test MSE 11729.776943979705 Test RE 0.20714424815940066\n",
      "2000 Train Loss 9881.059 Test MSE 12182.087654959929 Test RE 0.21110030553378775\n",
      "2100 Train Loss 10246.582 Test MSE 12190.093863908909 Test RE 0.21116966292161377\n",
      "2200 Train Loss 9876.323 Test MSE 12016.98587439743 Test RE 0.20966492213692214\n",
      "2300 Train Loss 9876.547 Test MSE 12031.251874356134 Test RE 0.2097893373859314\n",
      "2400 Train Loss 9959.743 Test MSE 12111.921570845068 Test RE 0.21049148246398902\n",
      "2500 Train Loss 9906.146 Test MSE 12176.82414446356 Test RE 0.2110546955877497\n",
      "2600 Train Loss 9945.115 Test MSE 12300.082766885469 Test RE 0.21212019550949862\n",
      "2700 Train Loss 9883.156 Test MSE 12150.403324273982 Test RE 0.21082560193571742\n",
      "2800 Train Loss 10367.425 Test MSE 12284.799713652741 Test RE 0.21198837314950564\n",
      "2900 Train Loss 40082.95 Test MSE 21473.48593396683 Test RE 0.28027171942842927\n",
      "Training time: 80.16\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 274170.4 Test MSE 272993.6296049959 Test RE 0.9993192611679828\n",
      "100 Train Loss 6531.7925 Test MSE 11219.611003690527 Test RE 0.20258948558746928\n",
      "200 Train Loss 4982.4 Test MSE 9998.974642268171 Test RE 0.19125188985600677\n",
      "300 Train Loss 7992.074 Test MSE 14511.539631246325 Test RE 0.2304012059700566\n",
      "400 Train Loss 6498.747 Test MSE 11736.36574430621 Test RE 0.20720241808404274\n",
      "500 Train Loss 8085.2188 Test MSE 14017.675588309336 Test RE 0.22644670425357905\n",
      "600 Train Loss 6458.726 Test MSE 11920.38460395261 Test RE 0.20882050214765926\n",
      "700 Train Loss 6431.5024 Test MSE 11852.442124527604 Test RE 0.20822454583214667\n",
      "800 Train Loss 6421.2876 Test MSE 11821.360120433454 Test RE 0.20795134117189756\n",
      "900 Train Loss 6710.5254 Test MSE 18074.692510535857 Test RE 0.25713634160199134\n",
      "1000 Train Loss 6436.572 Test MSE 11935.780462293558 Test RE 0.2089553104431414\n",
      "1100 Train Loss 6425.4043 Test MSE 11861.424079424572 Test RE 0.20830342870140536\n",
      "1200 Train Loss 6423.2007 Test MSE 11841.097590140245 Test RE 0.2081248711823041\n",
      "1300 Train Loss 6421.9834 Test MSE 11823.538923060043 Test RE 0.20797050411290272\n",
      "1400 Train Loss 6421.1934 Test MSE 11811.24206657005 Test RE 0.20786232800988036\n",
      "1500 Train Loss 6420.6533 Test MSE 11801.94760802092 Test RE 0.2077805267901373\n",
      "1600 Train Loss 6492.133 Test MSE 11680.543240257435 Test RE 0.20670906500567943\n",
      "1700 Train Loss 6420.3096 Test MSE 11803.856515427484 Test RE 0.20779732985360633\n",
      "1800 Train Loss 6419.929 Test MSE 11749.468522340574 Test RE 0.2073180488517202\n",
      "1900 Train Loss 6719.095 Test MSE 12297.376939057169 Test RE 0.21209686264630076\n",
      "2000 Train Loss 6537.26 Test MSE 12809.527983203228 Test RE 0.21646842895661306\n",
      "2100 Train Loss 6440.309 Test MSE 12330.331126679 Test RE 0.21238085831252226\n",
      "2200 Train Loss 6483.6606 Test MSE 12475.941857346144 Test RE 0.2136311965024914\n",
      "2300 Train Loss 6516.7275 Test MSE 12770.874103465934 Test RE 0.21614157591125788\n",
      "2400 Train Loss 6438.1646 Test MSE 12591.998542419076 Test RE 0.21462254191794813\n",
      "2500 Train Loss 6468.9897 Test MSE 12556.95749207836 Test RE 0.21432370775555162\n",
      "2600 Train Loss 6598.421 Test MSE 12619.939920942055 Test RE 0.21486053140898442\n",
      "2700 Train Loss 6733.146 Test MSE 12847.936870830083 Test RE 0.21679272226243998\n",
      "2800 Train Loss 6492.4536 Test MSE 12502.585040745464 Test RE 0.2138591864868787\n",
      "2900 Train Loss 6448.6074 Test MSE 12534.400000351403 Test RE 0.21413111418907846\n",
      "Training time: 80.57\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 278322.97 Test MSE 273314.48798886786 Test RE 0.9999063550943669\n",
      "100 Train Loss 12367.729 Test MSE 14894.286244669469 Test RE 0.23341988451292967\n",
      "200 Train Loss 11348.485 Test MSE 12131.429995034574 Test RE 0.21066093142087525\n",
      "300 Train Loss 11215.286 Test MSE 11869.667413408408 Test RE 0.20837579844761286\n",
      "400 Train Loss 11943.645 Test MSE 13041.198829426336 Test RE 0.21841716209366288\n",
      "500 Train Loss 11207.856 Test MSE 11913.138572009588 Test RE 0.20875702474765104\n",
      "600 Train Loss 11187.241 Test MSE 11764.019936057428 Test RE 0.2074463881559131\n",
      "700 Train Loss 11205.017 Test MSE 11756.403047842414 Test RE 0.2073792192881246\n",
      "800 Train Loss 11407.151 Test MSE 12423.772209757772 Test RE 0.2131840663421846\n",
      "900 Train Loss 11197.174 Test MSE 11839.415482057948 Test RE 0.20811008788374274\n",
      "1000 Train Loss 166537.4 Test MSE 59801.89451530496 Test RE 0.4677194961069165\n",
      "1100 Train Loss 10760.452 Test MSE 10837.206132489568 Test RE 0.19910706422797725\n",
      "1200 Train Loss 8164.035 Test MSE 9049.927074750956 Test RE 0.1819493639108685\n",
      "1300 Train Loss 6007.681 Test MSE 7224.3825893767125 Test RE 0.16256549489407035\n",
      "1400 Train Loss 4108.4663 Test MSE 4265.020786072763 Test RE 0.12490751776116311\n",
      "1500 Train Loss 2479.362 Test MSE 4075.1333532361296 Test RE 0.12209529101105936\n",
      "1600 Train Loss 690.61816 Test MSE 2252.2603530350557 Test RE 0.09076894710886885\n",
      "1700 Train Loss 424.02554 Test MSE 1925.1953311461923 Test RE 0.0839199858046461\n",
      "1800 Train Loss 216.63545 Test MSE 1595.9942150440725 Test RE 0.07640884910577449\n",
      "1900 Train Loss 188.42386 Test MSE 1512.5964625336314 Test RE 0.07438571503313691\n",
      "2000 Train Loss 143.51439 Test MSE 1409.6480748311008 Test RE 0.07180973980504786\n",
      "2100 Train Loss 214.9985 Test MSE 1549.6972864946113 Test RE 0.07529245154442707\n",
      "2200 Train Loss 94.5759 Test MSE 1321.8533937357815 Test RE 0.06953759191833318\n",
      "2300 Train Loss 67.903885 Test MSE 1263.8976638501395 Test RE 0.06799609271336175\n",
      "2400 Train Loss 430.97275 Test MSE 1625.5571555289748 Test RE 0.07711327072163383\n",
      "2500 Train Loss 45.6318 Test MSE 1216.962358295331 Test RE 0.06672161878243318\n",
      "2600 Train Loss 30.40879 Test MSE 1190.2659704349992 Test RE 0.06598572769753482\n",
      "2700 Train Loss 20.846325 Test MSE 1168.2509704044696 Test RE 0.06537264801415692\n",
      "2800 Train Loss 14.93009 Test MSE 1153.2538380241647 Test RE 0.06495169007866816\n",
      "2900 Train Loss 20.9903 Test MSE 1204.491465759332 Test RE 0.06637887160663379\n",
      "Training time: 79.26\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 3000\n",
    "layers = np.array([1,50,50,50,50,1])\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []\n",
    "    beta_val = []\n",
    "    'Generate Training data'\n",
    "    N_train = 300\n",
    "    \n",
    "    x_train_np_array, u_train_np_array = trainingdata(N_train,reps*1234)\n",
    "\n",
    "    'Convert to tensor and send to GPU'\n",
    "    x_train = torch.from_numpy(x_train_np_array).float().to(device)\n",
    "    u_train = torch.from_numpy(u_train_np_array).float().to(device)\n",
    "    \n",
    "    x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "       \n",
    "     \n",
    "    model_NN = Sequentialmodel(layers)\n",
    "    model_NN.to(device)\n",
    "    print(model_NN)\n",
    "    params = list(model_NN.parameters())\n",
    "    #optimizer = optim.Adam(PINN.parameters(), lr=0.0008,betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "    optimizer = optim.Adam(model_NN.parameters(), lr=0.08)\n",
    "\n",
    "    train_model(max_iter,reps)\n",
    " \n",
    "    torch.save(model_NN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    beta_full.append(beta_val)\n",
    "                 \n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse\": test_mse_full, \"test_re\": test_re_full,\"Time\": elapsed_time, \"Thresh_time\": time_threshold, \"epoch_thresh\": epoch_threshold,\"beta\": beta_full, \"label\": label}\n",
    "savemat(label+'.mat', mdic) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred = model_NN.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd5d463a110>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_test,u_pred,'rx')\n",
    "plt.plot(x_test,u_true,'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.2161621929183899\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + test_re_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
