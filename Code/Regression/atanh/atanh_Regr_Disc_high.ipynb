{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:2\n"
     ]
    }
   ],
   "source": [
    "#Trying to replicate the results of \"Adaptive activation functions accelerate convergence in deep and physics-informed neural networks\"\n",
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_disc(x): #[-4,3.75]\n",
    "    y  = np.zeros((np.shape(x)[0],))\n",
    "    \n",
    "    y = 0.2*np.sin(6*x)*np.exp(-1*x)\n",
    "    y[x>0] = 1 + 0.1*x[x>0]*np.exp(x[x>0]) + 10\n",
    "    \n",
    "    return scale*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain bounds\n",
    "level = \"high\"\n",
    "label = \"Regr_disc_atanh_\" + level\n",
    "scale = 50.0\n",
    "loss_thresh = 0.1\n",
    "\n",
    "lb = np.array(-4.0)  # [-1. 0.]\n",
    "ub = np.array(3.75) # [1.  0.99]\n",
    "\n",
    "x_test = np.linspace(lb,ub,1000).reshape(-1,1)\n",
    "u_true = true_disc(x_test)\n",
    "\n",
    "u_true_norm = np.linalg.norm(u_true,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_train,seed):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    x_train = np.random.uniform(low=lb, high=ub, size=(N_train,)).reshape(-1,1)\n",
    "\n",
    "    u_train = true_disc(x_train)\n",
    "\n",
    "    return x_train, u_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,n_val):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "     \n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "\n",
    "        \n",
    "        self.alpha = Parameter(torch.ones((layers[1],len(layers)-2)))\n",
    "        self.alpha.requiresGrad = True\n",
    "        \n",
    "        self.n = torch.tensor(n_val)\n",
    "            \n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = 2.0*(x - l_b)/(u_b - l_b)-1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(self.n*self.alpha[:,i]*z)\n",
    "    \n",
    "        a = self.linears[-1](a) \n",
    "        \n",
    "        return a\n",
    "    \n",
    "    \n",
    "    def loss(self,x,u):\n",
    "\n",
    "        loss_val = self.loss_function(self.forward(x),u)\n",
    "        \n",
    "        return loss_val\n",
    "        \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        u_pred = self.forward(x_test_tensor)       \n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "        \n",
    "        return u_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "        \n",
    "        test_mse_loss = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re_loss = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "        \n",
    "        return test_mse_loss, test_re_loss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    alpha_val.append(model_NN.alpha.cpu().detach().numpy())\n",
    "    test_mse, test_re = model_NN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step():\n",
    "    optimizer.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "    loss = model_NN.loss(x_train,u_train)\n",
    "    loss.backward() #backprop\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "   \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "    \n",
    "    loss_np = model_NN.loss(x_train,u_train).cpu().detach().numpy()\n",
    "    data_update(loss_np)\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        loss_np = train_step().cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1\n",
    "        data_update(loss_np)\n",
    "        if(i%100==0):        \n",
    "            print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 290473.06 Test MSE 269128.79046123114 Test RE 0.9922202394952372\n",
      "100 Train Loss 92640.31 Test MSE 85689.88390268966 Test RE 0.5598771962048881\n",
      "200 Train Loss 41205.746 Test MSE 39278.20491526068 Test RE 0.37905638596146224\n",
      "300 Train Loss 31647.467 Test MSE 29109.009632560268 Test RE 0.32631851559948993\n",
      "400 Train Loss 24281.732 Test MSE 23030.973887545228 Test RE 0.2902579698103556\n",
      "500 Train Loss 19577.143 Test MSE 19101.158663068272 Test RE 0.2643369397658538\n",
      "600 Train Loss 27696.9 Test MSE 26076.755340099557 Test RE 0.30885510066602634\n",
      "700 Train Loss 34043.875 Test MSE 33335.60486390943 Test RE 0.3492063816073922\n",
      "800 Train Loss 42649.742 Test MSE 43111.12483448696 Test RE 0.39712084053162944\n",
      "900 Train Loss 44077.375 Test MSE 40751.244355811476 Test RE 0.3860987887234207\n",
      "1000 Train Loss 37567.98 Test MSE 35152.6396905537 Test RE 0.35859726524236846\n",
      "1100 Train Loss 57609.656 Test MSE 55745.90705966689 Test RE 0.4515797876663853\n",
      "1200 Train Loss 28013.88 Test MSE 27021.876677740507 Test RE 0.3144023298221681\n",
      "1300 Train Loss 28391.314 Test MSE 27232.424930486024 Test RE 0.31562482843396117\n",
      "1400 Train Loss 24261.51 Test MSE 24501.245406335987 Test RE 0.29937951601493396\n",
      "1500 Train Loss 24860.293 Test MSE 25556.970546245673 Test RE 0.3057614209958659\n",
      "1600 Train Loss 42244.875 Test MSE 45690.29609001927 Test RE 0.40882739353044356\n",
      "1700 Train Loss 27559.268 Test MSE 26250.41590107324 Test RE 0.3098818186580748\n",
      "1800 Train Loss 33416.926 Test MSE 33126.54129997669 Test RE 0.34810963901405245\n",
      "1900 Train Loss 28669.354 Test MSE 28399.163344936238 Test RE 0.3223151909946733\n",
      "2000 Train Loss 27229.006 Test MSE 27086.3116757823 Test RE 0.3147769605569088\n",
      "2100 Train Loss 26557.373 Test MSE 26538.057436408268 Test RE 0.31157497344557344\n",
      "2200 Train Loss 26083.41 Test MSE 26179.48943962568 Test RE 0.3094628979202337\n",
      "2300 Train Loss 25681.756 Test MSE 25885.446390660578 Test RE 0.30772007611822605\n",
      "2400 Train Loss 25325.541 Test MSE 25630.116216786453 Test RE 0.3061986626552834\n",
      "2500 Train Loss 25007.74 Test MSE 25407.449752956625 Test RE 0.3048656819217967\n",
      "2600 Train Loss 24724.965 Test MSE 25214.69526515898 Test RE 0.3037070432560291\n",
      "2700 Train Loss 24474.477 Test MSE 25049.485444225116 Test RE 0.3027104449447831\n",
      "2800 Train Loss 24253.734 Test MSE 24909.510135539247 Test RE 0.3018634944717855\n",
      "2900 Train Loss 24060.297 Test MSE 24792.492961358486 Test RE 0.3011536291426626\n",
      "Training time: 8.20\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 292808.28 Test MSE 270496.58405551227 Test RE 0.9947384250987196\n",
      "100 Train Loss 88919.96 Test MSE 86240.44842266972 Test RE 0.561672944778311\n",
      "200 Train Loss 46086.26 Test MSE 49423.862183940466 Test RE 0.4252030213426155\n",
      "300 Train Loss 43118.105 Test MSE 45599.27812855713 Test RE 0.4084199855127533\n",
      "400 Train Loss 41567.54 Test MSE 44065.07813325253 Test RE 0.4014905005149307\n",
      "500 Train Loss 41511.92 Test MSE 43968.376967724675 Test RE 0.40104972150555496\n",
      "600 Train Loss 41510.227 Test MSE 43956.92322152451 Test RE 0.4009974814499486\n",
      "700 Train Loss 41510.188 Test MSE 43955.31047125203 Test RE 0.40099012521705046\n",
      "800 Train Loss 41510.188 Test MSE 43955.12605638435 Test RE 0.40098928403747913\n",
      "900 Train Loss 41510.188 Test MSE 43955.11640360817 Test RE 0.40098924000780184\n",
      "1000 Train Loss 41510.188 Test MSE 43955.120413170946 Test RE 0.40098925829681603\n",
      "1100 Train Loss 41510.188 Test MSE 43955.12527591251 Test RE 0.4009892804774752\n",
      "1200 Train Loss 41510.188 Test MSE 43955.130415721454 Test RE 0.4009893039219337\n",
      "1300 Train Loss 41510.188 Test MSE 43955.136716479596 Test RE 0.400989332661884\n",
      "1400 Train Loss 41510.188 Test MSE 43955.14396555779 Test RE 0.40098936572744964\n",
      "1500 Train Loss 41510.188 Test MSE 43955.15222986521 Test RE 0.4009894034238253\n",
      "1600 Train Loss 41510.188 Test MSE 43955.1622814928 Test RE 0.4009894492727839\n",
      "1700 Train Loss 41510.188 Test MSE 43955.17455269553 Test RE 0.4009895052459874\n",
      "1800 Train Loss 41510.188 Test MSE 43955.1897602213 Test RE 0.40098957461276574\n",
      "1900 Train Loss 41510.188 Test MSE 43955.2098791143 Test RE 0.40098966638196826\n",
      "2000 Train Loss 41510.188 Test MSE 43955.243327934266 Test RE 0.4009898189535139\n",
      "2100 Train Loss 41510.188 Test MSE 43955.31333050507 Test RE 0.4009901382590721\n",
      "2200 Train Loss 41510.176 Test MSE 43955.63125387104 Test RE 0.40099158841258375\n",
      "2300 Train Loss 41510.188 Test MSE 43009.63797220535 Test RE 0.39665313880752795\n",
      "2400 Train Loss 41510.188 Test MSE 43009.38052725984 Test RE 0.3966519516724656\n",
      "2500 Train Loss 41510.188 Test MSE 43009.3717401267 Test RE 0.39665191115300624\n",
      "2600 Train Loss 41510.188 Test MSE 43009.36420271389 Test RE 0.396651876396287\n",
      "2700 Train Loss 41510.188 Test MSE 43009.358280261724 Test RE 0.3966518490865151\n",
      "2800 Train Loss 41510.188 Test MSE 43009.35319254414 Test RE 0.39665182562589246\n",
      "2900 Train Loss 41510.188 Test MSE 43009.348101877986 Test RE 0.39665180215167195\n",
      "Training time: 7.96\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 279550.8 Test MSE 269729.2475277535 Test RE 0.9933265011238691\n",
      "100 Train Loss 95183.47 Test MSE 90175.32494224227 Test RE 0.5743436975814419\n",
      "200 Train Loss 43155.27 Test MSE 40601.091996526426 Test RE 0.3853868209527474\n",
      "300 Train Loss 29942.068 Test MSE 29913.595901756293 Test RE 0.3307975718926707\n",
      "400 Train Loss 32049.064 Test MSE 31848.65194244654 Test RE 0.34132926784097406\n",
      "500 Train Loss 25023.18 Test MSE 24707.52091913145 Test RE 0.30063710983543124\n",
      "600 Train Loss 19109.273 Test MSE 20850.197178481376 Test RE 0.27617418752706147\n",
      "700 Train Loss 42163.777 Test MSE 40802.525402055784 Test RE 0.3863416441891721\n",
      "800 Train Loss 37895.348 Test MSE 43630.749677115295 Test RE 0.3995069505082485\n",
      "900 Train Loss 45437.83 Test MSE 44326.53036524055 Test RE 0.4026798248745821\n",
      "1000 Train Loss 39823.934 Test MSE 39397.53461964079 Test RE 0.37963174809568917\n",
      "1100 Train Loss 38307.914 Test MSE 38229.518808454384 Test RE 0.37396195166165463\n",
      "1200 Train Loss 37927.98 Test MSE 38028.618262647076 Test RE 0.3729780506624297\n",
      "1300 Train Loss 37839.746 Test MSE 38026.53070650406 Test RE 0.3729678133287836\n",
      "1400 Train Loss 37818.383 Test MSE 38044.01713875702 Test RE 0.37305355776409205\n",
      "1500 Train Loss 37810.395 Test MSE 38052.179851764544 Test RE 0.3730935767478179\n",
      "1600 Train Loss 37804.63 Test MSE 38052.36516060346 Test RE 0.37309448520370153\n",
      "1700 Train Loss 37799.062 Test MSE 38048.47669735322 Test RE 0.3730754219831536\n",
      "1800 Train Loss 37793.395 Test MSE 38042.824047178095 Test RE 0.373047708085465\n",
      "1900 Train Loss 37787.594 Test MSE 38036.41568390339 Test RE 0.37301628657593533\n",
      "2000 Train Loss 37781.676 Test MSE 38029.66605558144 Test RE 0.3729831889121495\n",
      "2100 Train Loss 37775.656 Test MSE 38022.735531442115 Test RE 0.3729492011462246\n",
      "2200 Train Loss 37769.555 Test MSE 38015.6665796134 Test RE 0.37291453133031144\n",
      "2300 Train Loss 37763.387 Test MSE 38008.49579452775 Test RE 0.3728793587776055\n",
      "2400 Train Loss 37757.176 Test MSE 38001.253624631136 Test RE 0.37284383271613203\n",
      "2500 Train Loss 37750.938 Test MSE 37993.9613142849 Test RE 0.3728080572728429\n",
      "2600 Train Loss 37744.7 Test MSE 37986.63667231641 Test RE 0.37277211975712876\n",
      "2700 Train Loss 37738.473 Test MSE 37979.31052835265 Test RE 0.37273617140605053\n",
      "2800 Train Loss 37732.293 Test MSE 37972.00320070156 Test RE 0.37270031192994674\n",
      "2900 Train Loss 37726.176 Test MSE 37964.736072363936 Test RE 0.372664646303044\n",
      "Training time: 8.62\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 305647.25 Test MSE 269077.20314490574 Test RE 0.9921251392416139\n",
      "100 Train Loss 95568.14 Test MSE 85268.39763286263 Test RE 0.558498553605286\n",
      "200 Train Loss 38466.664 Test MSE 36812.13841703399 Test RE 0.36696405402217414\n",
      "300 Train Loss 30477.678 Test MSE 31113.00878828361 Test RE 0.33736421001161954\n",
      "400 Train Loss 29914.361 Test MSE 31050.938297163888 Test RE 0.33702752099534244\n",
      "500 Train Loss 29897.018 Test MSE 31102.50965555821 Test RE 0.33730728317262026\n",
      "600 Train Loss 44946.438 Test MSE 44164.48908004557 Test RE 0.40194312725613535\n",
      "700 Train Loss 35822.1 Test MSE 37101.33675620164 Test RE 0.36840267973632435\n",
      "800 Train Loss 44470.305 Test MSE 44192.4706924968 Test RE 0.4020704380905792\n",
      "900 Train Loss 34214.363 Test MSE 34881.966204681376 Test RE 0.3572140072558193\n",
      "1000 Train Loss 20250.143 Test MSE 23348.966904964236 Test RE 0.29225492373577444\n",
      "1100 Train Loss 30011.334 Test MSE 33403.57985317105 Test RE 0.3495622355316369\n",
      "1200 Train Loss 30254.307 Test MSE 35616.486295623756 Test RE 0.3609553955632398\n",
      "1300 Train Loss 30182.725 Test MSE 35747.22522828193 Test RE 0.3616172757896777\n",
      "1400 Train Loss 28002.248 Test MSE 32218.317652009835 Test RE 0.34330444878651895\n",
      "1500 Train Loss 27900.602 Test MSE 32460.977039177582 Test RE 0.34459486029273706\n",
      "1600 Train Loss 27889.334 Test MSE 32491.22788481806 Test RE 0.34475538932442007\n",
      "1700 Train Loss 27882.482 Test MSE 32490.1984785857 Test RE 0.34474992790943915\n",
      "1800 Train Loss 27843.97 Test MSE 32448.167141387552 Test RE 0.3445268607912079\n",
      "1900 Train Loss 27842.062 Test MSE 32437.437721401355 Test RE 0.3444698948769127\n",
      "2000 Train Loss 27840.17 Test MSE 32429.53569813503 Test RE 0.34442793448929937\n",
      "2100 Train Loss 27838.275 Test MSE 32421.589394876642 Test RE 0.34438573381576\n",
      "2200 Train Loss 27836.383 Test MSE 32413.50503949282 Test RE 0.3443427946771844\n",
      "2300 Train Loss 27834.488 Test MSE 32405.29396080586 Test RE 0.344299176980418\n",
      "2400 Train Loss 27832.598 Test MSE 32396.959901527494 Test RE 0.344254900352843\n",
      "2500 Train Loss 27830.686 Test MSE 32389.864027417756 Test RE 0.34421719738291795\n",
      "2600 Train Loss 27828.76 Test MSE 32381.522225378674 Test RE 0.34417286906129346\n",
      "2700 Train Loss 27826.848 Test MSE 32372.832110071904 Test RE 0.3441266837308421\n",
      "2800 Train Loss 27824.932 Test MSE 32364.022972381557 Test RE 0.3440798595051433\n",
      "2900 Train Loss 27822.746 Test MSE 32354.857498104226 Test RE 0.34403113443758404\n",
      "Training time: 11.05\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 258160.55 Test MSE 268532.1578210303 Test RE 0.9911198008519307\n",
      "100 Train Loss 80540.23 Test MSE 87153.97721957976 Test RE 0.5646399559882312\n",
      "200 Train Loss 42931.555 Test MSE 44424.37899915596 Test RE 0.4031240278169203\n",
      "300 Train Loss 29748.375 Test MSE 32994.370428563234 Test RE 0.34741448736460484\n",
      "400 Train Loss 32927.273 Test MSE 36782.09985163775 Test RE 0.3668143028283636\n",
      "500 Train Loss 25013.283 Test MSE 28306.578345214188 Test RE 0.3217893671688625\n",
      "600 Train Loss 22635.822 Test MSE 25119.379867144096 Test RE 0.3031324702509353\n",
      "700 Train Loss 29431.55 Test MSE 33338.79950012882 Test RE 0.34922311387658544\n",
      "800 Train Loss 30410.65 Test MSE 33617.305982689446 Test RE 0.350678754473057\n",
      "900 Train Loss 24868.947 Test MSE 27461.770601362972 Test RE 0.316951104357932\n",
      "1000 Train Loss 34653.812 Test MSE 39928.70124892835 Test RE 0.3821823213253653\n",
      "1100 Train Loss 31198.928 Test MSE 36258.80817454341 Test RE 0.3641956583749429\n",
      "1200 Train Loss 32435.021 Test MSE 38035.00304832316 Test RE 0.37300935978010674\n",
      "1300 Train Loss 31808.055 Test MSE 37310.2491322258 Test RE 0.36943843540798826\n",
      "1400 Train Loss 31240.453 Test MSE 36647.89411082184 Test RE 0.36614449914294844\n",
      "1500 Train Loss 30727.395 Test MSE 36043.23038219817 Test RE 0.36311137640641616\n",
      "1600 Train Loss 30264.943 Test MSE 35492.58463880128 Test RE 0.3603270077115976\n",
      "1700 Train Loss 29849.46 Test MSE 34992.310080643314 Test RE 0.35777855764744304\n",
      "1800 Train Loss 29477.557 Test MSE 34538.98264006303 Test RE 0.3554534815684256\n",
      "1900 Train Loss 29145.988 Test MSE 34129.32970663457 Test RE 0.3533392487098293\n",
      "2000 Train Loss 28851.684 Test MSE 33760.26583378554 Test RE 0.35142360590622224\n",
      "2100 Train Loss 28591.73 Test MSE 33428.86031194958 Test RE 0.3496944881575137\n",
      "2200 Train Loss 28363.314 Test MSE 33132.28968028594 Test RE 0.34813984107730955\n",
      "2300 Train Loss 28163.758 Test MSE 32867.86726243472 Test RE 0.34674783970541206\n",
      "2400 Train Loss 27990.477 Test MSE 32633.01467414933 Test RE 0.34550680056418115\n",
      "2500 Train Loss 27841.0 Test MSE 32425.253993053862 Test RE 0.34440519615082965\n",
      "2600 Train Loss 27712.973 Test MSE 32242.220813352313 Test RE 0.3434317760454667\n",
      "2700 Train Loss 27604.137 Test MSE 32081.641346368833 Test RE 0.3425754930926294\n",
      "2800 Train Loss 27512.352 Test MSE 31941.373950186684 Test RE 0.3418257681880912\n",
      "2900 Train Loss 27435.604 Test MSE 31819.370285477373 Test RE 0.34117232266024256\n",
      "Training time: 11.98\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 263929.72 Test MSE 270781.6828479147 Test RE 0.9952625057207364\n",
      "100 Train Loss 156134.25 Test MSE 154590.26356708776 Test RE 0.7520021298932881\n",
      "200 Train Loss 154626.77 Test MSE 151775.02099498344 Test RE 0.7451233148338995\n",
      "300 Train Loss 154626.47 Test MSE 151768.3202236014 Test RE 0.7451068662903215\n",
      "400 Train Loss 154626.75 Test MSE 151764.08120883405 Test RE 0.7450964604919964\n",
      "500 Train Loss 154626.72 Test MSE 151767.07869924972 Test RE 0.7451038186509672\n",
      "600 Train Loss 154626.73 Test MSE 151768.70687833993 Test RE 0.7451078154308164\n",
      "700 Train Loss 154626.72 Test MSE 151768.72288204337 Test RE 0.7451078547158718\n",
      "800 Train Loss 154626.72 Test MSE 151768.72288204337 Test RE 0.7451078547158718\n",
      "900 Train Loss 154626.72 Test MSE 151768.72288204337 Test RE 0.7451078547158718\n",
      "1000 Train Loss 154626.72 Test MSE 151768.724025179 Test RE 0.7451078575219817\n",
      "1100 Train Loss 154626.72 Test MSE 151768.724025179 Test RE 0.7451078575219817\n",
      "1200 Train Loss 154626.72 Test MSE 151768.724025179 Test RE 0.7451078575219817\n",
      "1300 Train Loss 154626.72 Test MSE 151768.724025179 Test RE 0.7451078575219817\n",
      "1400 Train Loss 154626.72 Test MSE 151768.724025179 Test RE 0.7451078575219817\n",
      "1500 Train Loss 154626.73 Test MSE 151768.72516831648 Test RE 0.7451078603280957\n",
      "1600 Train Loss 154626.73 Test MSE 151768.72516831648 Test RE 0.7451078603280957\n",
      "1700 Train Loss 154626.73 Test MSE 151768.72516831648 Test RE 0.7451078603280957\n",
      "1800 Train Loss 154626.73 Test MSE 151768.72516831648 Test RE 0.7451078603280957\n",
      "1900 Train Loss 154626.73 Test MSE 151768.72516831648 Test RE 0.7451078603280957\n",
      "2000 Train Loss 154626.72 Test MSE 151768.72631145586 Test RE 0.7451078631342145\n",
      "2100 Train Loss 154626.72 Test MSE 151768.72631145586 Test RE 0.7451078631342145\n",
      "2200 Train Loss 154626.72 Test MSE 151768.72631145586 Test RE 0.7451078631342145\n",
      "2300 Train Loss 154626.72 Test MSE 151768.72631145586 Test RE 0.7451078631342145\n",
      "2400 Train Loss 154626.72 Test MSE 151768.72631145586 Test RE 0.7451078631342145\n",
      "2500 Train Loss 154626.72 Test MSE 151768.72631145586 Test RE 0.7451078631342145\n",
      "2600 Train Loss 154626.72 Test MSE 151768.72631145586 Test RE 0.7451078631342145\n",
      "2700 Train Loss 154626.72 Test MSE 151768.72631145586 Test RE 0.7451078631342145\n",
      "2800 Train Loss 154626.72 Test MSE 151768.72631145586 Test RE 0.7451078631342145\n",
      "2900 Train Loss 154626.72 Test MSE 151768.72631145586 Test RE 0.7451078631342145\n",
      "Training time: 73.25\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 310754.5 Test MSE 270388.53620084625 Test RE 0.994539734811626\n",
      "100 Train Loss 165888.94 Test MSE 152515.82253004893 Test RE 0.746939544467303\n",
      "200 Train Loss 160829.34 Test MSE 151893.82227621324 Test RE 0.7454148789140453\n",
      "300 Train Loss 160546.67 Test MSE 152877.54118489765 Test RE 0.7478247705252431\n",
      "400 Train Loss 160546.67 Test MSE 152882.9420243322 Test RE 0.7478379799404679\n",
      "500 Train Loss 160546.66 Test MSE 152882.91866303253 Test RE 0.7478379228037215\n",
      "600 Train Loss 160546.66 Test MSE 152882.91866303253 Test RE 0.7478379228037215\n",
      "700 Train Loss 160546.66 Test MSE 152882.91866303253 Test RE 0.7478379228037215\n",
      "800 Train Loss 160546.66 Test MSE 152882.91866303253 Test RE 0.7478379228037215\n",
      "900 Train Loss 160546.66 Test MSE 152882.91866303253 Test RE 0.7478379228037215\n",
      "1000 Train Loss 160546.66 Test MSE 152882.91866303253 Test RE 0.7478379228037215\n",
      "1100 Train Loss 160546.66 Test MSE 152882.91866303253 Test RE 0.7478379228037215\n",
      "1200 Train Loss 160546.66 Test MSE 152882.91866303253 Test RE 0.7478379228037215\n",
      "1300 Train Loss 160546.67 Test MSE 152882.9233352776 Test RE 0.7478379342310344\n",
      "1400 Train Loss 160546.67 Test MSE 152882.9233352776 Test RE 0.7478379342310344\n",
      "1500 Train Loss 160546.67 Test MSE 152882.9233352776 Test RE 0.7478379342310344\n",
      "1600 Train Loss 160546.67 Test MSE 152882.9233352776 Test RE 0.7478379342310344\n",
      "1700 Train Loss 160546.67 Test MSE 152882.9233352776 Test RE 0.7478379342310344\n",
      "1800 Train Loss 160546.67 Test MSE 152882.9233352776 Test RE 0.7478379342310344\n",
      "1900 Train Loss 160546.67 Test MSE 152882.9233352776 Test RE 0.7478379342310344\n",
      "2000 Train Loss 160546.67 Test MSE 152882.92567140286 Test RE 0.7478379399446982\n",
      "2100 Train Loss 160546.67 Test MSE 152882.92567140286 Test RE 0.7478379399446982\n",
      "2200 Train Loss 160546.67 Test MSE 152882.92567140286 Test RE 0.7478379399446982\n",
      "2300 Train Loss 160546.67 Test MSE 152882.92567140286 Test RE 0.7478379399446982\n",
      "2400 Train Loss 160546.67 Test MSE 152882.92567140286 Test RE 0.7478379399446982\n",
      "2500 Train Loss 160546.67 Test MSE 152882.92567140286 Test RE 0.7478379399446982\n",
      "2600 Train Loss 160546.67 Test MSE 152882.92567140286 Test RE 0.7478379399446982\n",
      "2700 Train Loss 160546.67 Test MSE 152882.92567140286 Test RE 0.7478379399446982\n",
      "2800 Train Loss 160546.64 Test MSE 152882.9233352776 Test RE 0.7478379342310344\n",
      "2900 Train Loss 160546.67 Test MSE 152882.92176364284 Test RE 0.7478379303871516\n",
      "Training time: 77.28\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 306037.84 Test MSE 269987.801895966 Test RE 0.9938024735863276\n",
      "100 Train Loss 169499.27 Test MSE 152738.76545126707 Test RE 0.7474852716923848\n",
      "200 Train Loss 54137.176 Test MSE 47430.85805265952 Test RE 0.416541707064174\n",
      "300 Train Loss 35145.723 Test MSE 33534.36099568973 Test RE 0.350245867186541\n",
      "400 Train Loss 34079.133 Test MSE 33602.17938512328 Test RE 0.35059984908354075\n",
      "500 Train Loss 34034.008 Test MSE 33774.28832090783 Test RE 0.35149658106315773\n",
      "600 Train Loss 34032.805 Test MSE 33807.6889929243 Test RE 0.3516703422502826\n",
      "700 Train Loss 34032.79 Test MSE 33811.84101353789 Test RE 0.35169193641606805\n",
      "800 Train Loss 34032.79 Test MSE 33812.20079656416 Test RE 0.35169380754240737\n",
      "900 Train Loss 34032.79 Test MSE 33812.198196941696 Test RE 0.3516937940225666\n",
      "1000 Train Loss 34032.79 Test MSE 33812.19618728849 Test RE 0.35169378357097514\n",
      "1100 Train Loss 34032.79 Test MSE 33812.19254792447 Test RE 0.3516937646437553\n",
      "1200 Train Loss 34032.79 Test MSE 33812.18652890641 Test RE 0.35169373334068005\n",
      "1300 Train Loss 34032.79 Test MSE 33812.18445040794 Test RE 0.35169372253104353\n",
      "1400 Train Loss 34032.79 Test MSE 33812.18341260675 Test RE 0.3516937171337559\n",
      "1500 Train Loss 34032.79 Test MSE 33812.18205672945 Test RE 0.35169371008225137\n",
      "1600 Train Loss 34032.79 Test MSE 33812.18384402141 Test RE 0.3516937193774121\n",
      "1700 Train Loss 34032.79 Test MSE 33812.18555144678 Test RE 0.3516937282572108\n",
      "1800 Train Loss 34032.79 Test MSE 33812.18857153897 Test RE 0.3516937439637887\n",
      "1900 Train Loss 34032.79 Test MSE 33812.195172216176 Test RE 0.3516937782918944\n",
      "2000 Train Loss 34032.79 Test MSE 33812.211241907535 Test RE 0.35169386186543594\n",
      "2100 Train Loss 34032.79 Test MSE 33812.242119230024 Test RE 0.3516940224488748\n",
      "2200 Train Loss 34032.79 Test MSE 33812.35974370751 Test RE 0.3516946341768623\n",
      "2300 Train Loss 34035.527 Test MSE 33908.82523946277 Test RE 0.3521959631915081\n",
      "2400 Train Loss 34032.79 Test MSE 33840.16230372036 Test RE 0.35183919672818126\n",
      "2500 Train Loss 34032.79 Test MSE 33839.97493942473 Test RE 0.35183822270515613\n",
      "2600 Train Loss 34032.79 Test MSE 33839.96620826587 Test RE 0.35183817731570743\n",
      "2700 Train Loss 34032.79 Test MSE 33839.95793252866 Test RE 0.3518381342937898\n",
      "2800 Train Loss 34032.79 Test MSE 33839.94892255449 Test RE 0.35183808745489115\n",
      "2900 Train Loss 34032.79 Test MSE 33839.93999288243 Test RE 0.3518380410334419\n",
      "Training time: 74.80\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 274257.62 Test MSE 271425.54637734126 Test RE 0.9964450686333731\n",
      "100 Train Loss 84721.805 Test MSE 89578.80976721212 Test RE 0.5724408865796309\n",
      "200 Train Loss 30807.46 Test MSE 39148.212472188105 Test RE 0.3784286166651386\n",
      "300 Train Loss 26329.18 Test MSE 35344.9310084056 Test RE 0.35957672367100074\n",
      "400 Train Loss 25847.727 Test MSE 35954.22043053651 Test RE 0.3626627414679255\n",
      "500 Train Loss 21430.389 Test MSE 31765.464341150022 Test RE 0.3408832061265186\n",
      "600 Train Loss 16928.057 Test MSE 26875.078056349965 Test RE 0.3135471582214149\n",
      "700 Train Loss 15495.197 Test MSE 24323.584514363072 Test RE 0.2982921263737891\n",
      "800 Train Loss 21727.377 Test MSE 30641.30662112599 Test RE 0.33479706508527673\n",
      "900 Train Loss 69703.53 Test MSE 82064.92272906429 Test RE 0.5479069183946799\n",
      "1000 Train Loss 69903.234 Test MSE 76120.22875031944 Test RE 0.5276890037119689\n",
      "1100 Train Loss 55396.95 Test MSE 62827.437741993854 Test RE 0.4794051289672666\n",
      "1200 Train Loss 46357.668 Test MSE 54209.458126201345 Test RE 0.44531316533140275\n",
      "1300 Train Loss 40372.76 Test MSE 48575.576113579205 Test RE 0.4215382438294933\n",
      "1400 Train Loss 36282.902 Test MSE 44787.581862635685 Test RE 0.4047685950199432\n",
      "1500 Train Loss 33461.78 Test MSE 42228.56682391301 Test RE 0.3930349517890547\n",
      "1600 Train Loss 31520.701 Test MSE 40515.122125759204 Test RE 0.3849785904024676\n",
      "1700 Train Loss 30198.158 Test MSE 39389.29318959256 Test RE 0.3795920391118126\n",
      "1800 Train Loss 29310.064 Test MSE 38669.86837263163 Test RE 0.3761095393991805\n",
      "1900 Train Loss 28724.01 Test MSE 38227.01707444605 Test RE 0.3739497154543\n",
      "2000 Train Loss 28344.371 Test MSE 37967.63352746673 Test RE 0.37267886684810064\n",
      "2100 Train Loss 28102.598 Test MSE 37825.72712315328 Test RE 0.3719817595841497\n",
      "2200 Train Loss 27950.414 Test MSE 37755.599284811564 Test RE 0.37163677773297793\n",
      "2300 Train Loss 27854.643 Test MSE 37726.614752793896 Test RE 0.3714940997406848\n",
      "2400 Train Loss 27793.123 Test MSE 37719.07188466438 Test RE 0.37145696056057853\n",
      "2500 Train Loss 27751.566 Test MSE 37720.972794611305 Test RE 0.3714663205116564\n",
      "2600 Train Loss 27721.068 Test MSE 37725.51007348091 Test RE 0.3714886608104531\n",
      "2700 Train Loss 27696.303 Test MSE 37729.207645637216 Test RE 0.37150686563352797\n",
      "2800 Train Loss 27674.215 Test MSE 37730.598217838946 Test RE 0.37151371181971\n",
      "2900 Train Loss 27653.125 Test MSE 37729.32656411085 Test RE 0.37150745110824834\n",
      "Training time: 76.44\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 278456.53 Test MSE 272831.2772148416 Test RE 0.9990220637576335\n",
      "100 Train Loss 153958.67 Test MSE 153504.17473982205 Test RE 0.7493558419664296\n",
      "200 Train Loss 150956.16 Test MSE 151461.7956940304 Test RE 0.744354044592065\n",
      "300 Train Loss 150954.97 Test MSE 151476.6954407644 Test RE 0.7443906558508695\n",
      "400 Train Loss 150954.97 Test MSE 151476.62937861128 Test RE 0.7443904935286895\n",
      "500 Train Loss 150954.97 Test MSE 151476.61967586903 Test RE 0.7443904696879519\n",
      "600 Train Loss 150954.97 Test MSE 151476.61967586903 Test RE 0.7443904696879519\n",
      "700 Train Loss 150954.97 Test MSE 151476.61967586903 Test RE 0.7443904696879519\n",
      "800 Train Loss 150954.97 Test MSE 151476.61967586903 Test RE 0.7443904696879519\n",
      "900 Train Loss 150954.97 Test MSE 151476.62014638307 Test RE 0.7443904708440583\n",
      "1000 Train Loss 150954.97 Test MSE 151476.61967586903 Test RE 0.7443904696879519\n",
      "1100 Train Loss 150954.97 Test MSE 151476.61967586903 Test RE 0.7443904696879519\n",
      "1200 Train Loss 150954.97 Test MSE 151476.61967586903 Test RE 0.7443904696879519\n",
      "1300 Train Loss 150954.97 Test MSE 151476.61967586903 Test RE 0.7443904696879519\n",
      "1400 Train Loss 150954.97 Test MSE 151476.61967586903 Test RE 0.7443904696879519\n",
      "1500 Train Loss 150954.97 Test MSE 151476.61967586903 Test RE 0.7443904696879519\n",
      "1600 Train Loss 150954.97 Test MSE 151476.62059092513 Test RE 0.7443904719363481\n",
      "1700 Train Loss 150954.97 Test MSE 151476.62014340365 Test RE 0.7443904708367374\n",
      "1800 Train Loss 150954.97 Test MSE 151476.62014340365 Test RE 0.7443904708367374\n",
      "1900 Train Loss 150954.97 Test MSE 151476.62014340365 Test RE 0.7443904708367374\n",
      "2000 Train Loss 150954.97 Test MSE 151476.62014340365 Test RE 0.7443904708367374\n",
      "2100 Train Loss 150954.97 Test MSE 151476.62014340365 Test RE 0.7443904708367374\n",
      "2200 Train Loss 150954.97 Test MSE 151476.62014340365 Test RE 0.7443904708367374\n",
      "2300 Train Loss 150954.95 Test MSE 151476.62061094012 Test RE 0.7443904719855275\n",
      "2400 Train Loss 150954.95 Test MSE 151476.62061094012 Test RE 0.7443904719855275\n",
      "2500 Train Loss 150954.95 Test MSE 151476.62061094012 Test RE 0.7443904719855275\n",
      "2600 Train Loss 150954.95 Test MSE 151476.62061094012 Test RE 0.7443904719855275\n",
      "2700 Train Loss 150954.95 Test MSE 151476.62061094012 Test RE 0.7443904719855275\n",
      "2800 Train Loss 150954.95 Test MSE 151476.62061094012 Test RE 0.7443904719855275\n",
      "2900 Train Loss 150954.95 Test MSE 151476.62061094012 Test RE 0.7443904719855275\n",
      "Training time: 78.31\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 3000\n",
    "layers = np.array([1,50,50,50,50,1])\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "alpha_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "n_val = 1\n",
    "\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []\n",
    "    alpha_val = []\n",
    "    'Generate Training data'\n",
    "    N_train = 300\n",
    "    \n",
    "    x_train_np_array, u_train_np_array = trainingdata(N_train,reps*1234)\n",
    "\n",
    "    'Convert to tensor and send to GPU'\n",
    "    x_train = torch.from_numpy(x_train_np_array).float().to(device)\n",
    "    u_train = torch.from_numpy(u_train_np_array).float().to(device)\n",
    "    \n",
    "    x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "       \n",
    "     \n",
    "    model_NN = Sequentialmodel(layers,n_val)\n",
    "    model_NN.to(device)\n",
    "    print(model_NN)\n",
    "    params = list(model_NN.parameters())\n",
    "    #optimizer = optim.Adam(PINN.parameters(), lr=0.0008,betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "    optimizer = optim.Adam(model_NN.parameters(), lr=0.08)\n",
    "\n",
    "    train_model(max_iter,reps)\n",
    " \n",
    "    torch.save(model_NN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    alpha_full.append(alpha_val)\n",
    "                 \n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse\": test_mse_full, \"test_re\": test_re_full,\"Time\": elapsed_time, \"Thresh_time\": time_threshold, \"epoch_thresh\": epoch_threshold,\"alpha\": alpha_full, \"label\": label}\n",
    "savemat(label+'.mat', mdic) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred = model_NN.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f95cc5b33d0>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_test,u_pred,'rx')\n",
    "plt.plot(x_test,u_true,'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.47149381752407277\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + test_re_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
