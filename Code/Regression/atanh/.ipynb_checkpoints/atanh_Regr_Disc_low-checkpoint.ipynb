{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:2\n"
     ]
    }
   ],
   "source": [
    "#Trying to replicate the results of \"Adaptive activation functions accelerate convergence in deep and physics-informed neural networks\"\n",
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_disc(x): #[-4,3.75]\n",
    "    y  = np.zeros((np.shape(x)[0],))\n",
    "    \n",
    "    y = 0.2*np.sin(6*x)*np.exp(-1*x)\n",
    "    y[x>0] = 1 + 0.1*x[x>0]*np.exp(x[x>0]) + 10\n",
    "    \n",
    "    return scale*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain bounds\n",
    "level = \"low\"\n",
    "label = \"Regr_disc_atanh_\" + level\n",
    "scale = 1.0\n",
    "loss_thresh = 0.1\n",
    "\n",
    "lb = np.array(-4.0)  # [-1. 0.]\n",
    "ub = np.array(3.75) # [1.  0.99]\n",
    "\n",
    "x_test = np.linspace(lb,ub,1000).reshape(-1,1)\n",
    "u_true = true_disc(x_test)\n",
    "\n",
    "u_true_norm = np.linalg.norm(u_true,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_train,seed):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    x_train = np.random.uniform(low=lb, high=ub, size=(N_train,)).reshape(-1,1)\n",
    "\n",
    "    u_train = true_disc(x_train)\n",
    "\n",
    "    return x_train, u_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,n_val):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "     \n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "\n",
    "        \n",
    "        self.alpha = Parameter(torch.ones((layers[1],len(layers)-2)))\n",
    "        self.alpha.requiresGrad = True\n",
    "        \n",
    "        self.n = torch.tensor(n_val)\n",
    "            \n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = 2.0*(x - l_b)/(u_b - l_b)-1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(self.n*self.alpha[:,i]*z)\n",
    "    \n",
    "        a = self.linears[-1](a) \n",
    "        \n",
    "        return a\n",
    "    \n",
    "    \n",
    "    def loss(self,x,u):\n",
    "\n",
    "        loss_val = self.loss_function(self.forward(x),u)\n",
    "        \n",
    "        return loss_val\n",
    "        \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        u_pred = self.forward(x_test_tensor)       \n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "        \n",
    "        return u_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "        \n",
    "        test_mse_loss = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re_loss = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "        \n",
    "        return test_mse_loss, test_re_loss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    alpha_val.append(model_NN.alpha.cpu().detach().numpy())\n",
    "    test_mse, test_re = model_NN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step():\n",
    "    optimizer.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "    loss = model_NN.loss(x_train,u_train)\n",
    "    loss.backward() #backprop\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "   \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "    \n",
    "    loss_np = model_NN.loss(x_train,u_train).cpu().detach().numpy()\n",
    "    data_update(loss_np)\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        loss_np = train_step().cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1\n",
    "        data_update(loss_np)\n",
    "        if(i%100==0):        \n",
    "            print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 114.84176 Test MSE 57.297199396536406 Test RE 0.7238768761812056\n",
      "100 Train Loss 3.9712632 Test MSE 4.547356401559462 Test RE 0.20392830193112152\n",
      "200 Train Loss 4.70408 Test MSE 4.441681259562263 Test RE 0.20154484805816333\n",
      "300 Train Loss 2.7406356 Test MSE 4.458139687672421 Test RE 0.20191790997226824\n",
      "400 Train Loss 5.319574 Test MSE 5.678530176072918 Test RE 0.22788512355535973\n",
      "500 Train Loss 5.0623193 Test MSE 4.7309367810941545 Test RE 0.20800394817794168\n",
      "600 Train Loss 5.4867105 Test MSE 6.008828273772626 Test RE 0.23441905043786984\n",
      "700 Train Loss 5.816504 Test MSE 6.923974588202645 Test RE 0.2516377184189147\n",
      "800 Train Loss 8.518267 Test MSE 9.92230446093554 Test RE 0.301234202512436\n",
      "900 Train Loss 4.8858056 Test MSE 4.729156881359388 Test RE 0.2079648162873646\n",
      "1000 Train Loss 4.79822 Test MSE 4.561450350194559 Test RE 0.2042440822642141\n",
      "1100 Train Loss 4.7933035 Test MSE 4.556195284644698 Test RE 0.2041263976170708\n",
      "1200 Train Loss 4.791834 Test MSE 4.553327393391226 Test RE 0.20406214395535194\n",
      "1300 Train Loss 4.8222637 Test MSE 4.550456344957895 Test RE 0.2039977992870495\n",
      "1400 Train Loss 4.7906156 Test MSE 4.551614853680658 Test RE 0.20402376571456526\n",
      "1500 Train Loss 4.790239 Test MSE 4.55049201183241 Test RE 0.20399859876177243\n",
      "1600 Train Loss 4.790852 Test MSE 4.5284194017245865 Test RE 0.20350323967742973\n",
      "1700 Train Loss 4.790246 Test MSE 4.517483961340576 Test RE 0.20325737655486717\n",
      "1800 Train Loss 4.791659 Test MSE 4.516763033202915 Test RE 0.20324115737070775\n",
      "1900 Train Loss 4.790347 Test MSE 4.440762974391017 Test RE 0.2015240130183162\n",
      "2000 Train Loss 4.7902217 Test MSE 4.44022488390639 Test RE 0.2015118032427061\n",
      "2100 Train Loss 4.790268 Test MSE 4.4343502056146935 Test RE 0.20137845314545907\n",
      "2200 Train Loss 4.7905707 Test MSE 4.430016165568385 Test RE 0.20128001756821196\n",
      "2300 Train Loss 4.79112 Test MSE 4.4345159326873524 Test RE 0.20138221621650326\n",
      "2400 Train Loss 4.828661 Test MSE 4.431241094941515 Test RE 0.2013078432866109\n",
      "2500 Train Loss 4.798561 Test MSE 4.451605468345937 Test RE 0.20176988189290349\n",
      "2600 Train Loss 4.822523 Test MSE 4.4464428036213555 Test RE 0.20165284856091695\n",
      "2700 Train Loss 4.8855333 Test MSE 4.488011693508597 Test RE 0.20259326148608744\n",
      "2800 Train Loss 4.839345 Test MSE 4.493235499044055 Test RE 0.20271113105714952\n",
      "2900 Train Loss 4.8126726 Test MSE 4.471052696847788 Test RE 0.20221012630854196\n",
      "Training time: 14.10\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 118.05292 Test MSE 65.65022367716192 Test RE 0.7748472771115356\n",
      "100 Train Loss 4.6548204 Test MSE 4.982083622117965 Test RE 0.21345361411971994\n",
      "200 Train Loss 4.239847 Test MSE 5.057568113098871 Test RE 0.21506457310717486\n",
      "300 Train Loss 4.175927 Test MSE 4.596542048093136 Test RE 0.2050282122247427\n",
      "400 Train Loss 4.493415 Test MSE 5.10489225028395 Test RE 0.21606841995095002\n",
      "500 Train Loss 3.86251 Test MSE 4.51604687392721 Test RE 0.20322504419308304\n",
      "600 Train Loss 3.85987 Test MSE 4.418300224244003 Test RE 0.20101368150880036\n",
      "700 Train Loss 3.8450425 Test MSE 4.457049107349706 Test RE 0.20189321119837714\n",
      "800 Train Loss 3.8441675 Test MSE 4.47283208818034 Test RE 0.2022503601391629\n",
      "900 Train Loss 3.8527296 Test MSE 4.44374018846406 Test RE 0.2015915554208071\n",
      "1000 Train Loss 3.8476353 Test MSE 4.3760819841768495 Test RE 0.20005100202515266\n",
      "1100 Train Loss 3.842969 Test MSE 4.405247747723885 Test RE 0.20071654609068923\n",
      "1200 Train Loss 3.9220166 Test MSE 4.520863622057277 Test RE 0.2033333937078579\n",
      "1300 Train Loss 3.8380685 Test MSE 4.4317794299397955 Test RE 0.2013200709857982\n",
      "1400 Train Loss 3.8447933 Test MSE 4.47191235865379 Test RE 0.20222956512744633\n",
      "1500 Train Loss 3.9739356 Test MSE 4.430583644602358 Test RE 0.20129290900336325\n",
      "1600 Train Loss 3.970748 Test MSE 4.475087198290786 Test RE 0.20230133895401928\n",
      "1700 Train Loss 3.9469118 Test MSE 4.447211588747956 Test RE 0.20167028058659525\n",
      "1800 Train Loss 3.957627 Test MSE 4.456233373984908 Test RE 0.20187473500923264\n",
      "1900 Train Loss 3.9458082 Test MSE 4.413698254890777 Test RE 0.20090896931645902\n",
      "2000 Train Loss 3.9467604 Test MSE 4.412439998847682 Test RE 0.2008803297347067\n",
      "2100 Train Loss 3.9603167 Test MSE 4.513555248793447 Test RE 0.2031689740830349\n",
      "2200 Train Loss 3.9514313 Test MSE 4.428404070811357 Test RE 0.2012433910668325\n",
      "2300 Train Loss 3.9512787 Test MSE 4.42606301278268 Test RE 0.2011901907796013\n",
      "2400 Train Loss 3.9821575 Test MSE 4.450888182572428 Test RE 0.20175362567958402\n",
      "2500 Train Loss 3.9512842 Test MSE 4.431447226083091 Test RE 0.20131252542103947\n",
      "2600 Train Loss 3.9521387 Test MSE 4.431766166473779 Test RE 0.20131976972938864\n",
      "2700 Train Loss 4.1815996 Test MSE 4.716874375589876 Test RE 0.2076945789284697\n",
      "2800 Train Loss 29.149168 Test MSE 27.903385759346353 Test RE 0.505157095771878\n",
      "2900 Train Loss 14.763289 Test MSE 16.13374210904219 Test RE 0.3841187982742844\n",
      "Training time: 13.92\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 111.0624 Test MSE 60.08289079782018 Test RE 0.7412648658844226\n",
      "100 Train Loss 7.07953 Test MSE 5.759219832462431 Test RE 0.22949849095136196\n",
      "200 Train Loss 4.8184443 Test MSE 5.079900527601487 Test RE 0.21553887427056798\n",
      "300 Train Loss 4.0213 Test MSE 4.72215963617847 Test RE 0.20781090730169674\n",
      "400 Train Loss 4.391118 Test MSE 4.864083375463799 Test RE 0.21091065064624714\n",
      "500 Train Loss 5.149808 Test MSE 5.902955623735601 Test RE 0.23234469737069424\n",
      "600 Train Loss 6.730678 Test MSE 10.505466546901173 Test RE 0.30996001857204963\n",
      "700 Train Loss 4.4813113 Test MSE 4.81300410852606 Test RE 0.20980030845378697\n",
      "800 Train Loss 6.302987 Test MSE 6.490710791572601 Test RE 0.24363750054077693\n",
      "900 Train Loss 5.3016634 Test MSE 6.027574695385042 Test RE 0.23478443749592579\n",
      "1000 Train Loss 5.24649 Test MSE 6.252166123032226 Test RE 0.23911854566025026\n",
      "1100 Train Loss 7.107119 Test MSE 7.242861460567985 Test RE 0.2573671392962938\n",
      "1200 Train Loss 6.4986835 Test MSE 6.820316175210588 Test RE 0.24974698855153274\n",
      "1300 Train Loss 7.6994624 Test MSE 13.516507958234483 Test RE 0.3515849354835306\n",
      "1400 Train Loss 9.249891 Test MSE 10.226115360665128 Test RE 0.3058111740052269\n",
      "1500 Train Loss 7.033623 Test MSE 7.1296927503179575 Test RE 0.25534856024047325\n",
      "1600 Train Loss 6.9264994 Test MSE 7.087203382069525 Test RE 0.25458654901029776\n",
      "1700 Train Loss 6.920675 Test MSE 7.09113639178945 Test RE 0.2546571800082324\n",
      "1800 Train Loss 6.919816 Test MSE 7.064164683302823 Test RE 0.25417241405855895\n",
      "1900 Train Loss 6.9192953 Test MSE 7.061071053458026 Test RE 0.2541167527377729\n",
      "2000 Train Loss 6.91918 Test MSE 7.05652535413069 Test RE 0.2540349433129918\n",
      "2100 Train Loss 6.9185777 Test MSE 7.053542469033542 Test RE 0.2539812456995133\n",
      "2200 Train Loss 6.918353 Test MSE 7.051207865174731 Test RE 0.2539392104618471\n",
      "2300 Train Loss 6.918306 Test MSE 7.04953426485238 Test RE 0.2539090725072189\n",
      "2400 Train Loss 6.918271 Test MSE 7.047695804250774 Test RE 0.2538759616486043\n",
      "2500 Train Loss 6.9614615 Test MSE 7.115567870145903 Test RE 0.255095494920086\n",
      "2600 Train Loss 6.8858285 Test MSE 6.975476995568365 Test RE 0.25257185945514316\n",
      "2700 Train Loss 6.878753 Test MSE 6.960694218657898 Test RE 0.2523040861027866\n",
      "2800 Train Loss 6.877554 Test MSE 6.953153661227177 Test RE 0.252167387885517\n",
      "2900 Train Loss 6.87653 Test MSE 6.951223042551598 Test RE 0.2521323769472523\n",
      "Training time: 14.11\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 123.44076 Test MSE 54.29737105312385 Test RE 0.7046726364313257\n",
      "100 Train Loss 3.369587 Test MSE 4.894207989279908 Test RE 0.21156275653678153\n",
      "200 Train Loss 3.3596091 Test MSE 5.057982541654349 Test RE 0.2150733843651345\n",
      "300 Train Loss 3.372504 Test MSE 4.785171097392382 Test RE 0.2091928042024353\n",
      "400 Train Loss 3.3777826 Test MSE 4.559389255821918 Test RE 0.20419793313833262\n",
      "500 Train Loss 3.382603 Test MSE 5.015523803883645 Test RE 0.21416877574944707\n",
      "600 Train Loss 3.361246 Test MSE 5.129207340609244 Test RE 0.21658238594165977\n",
      "700 Train Loss 3.3606544 Test MSE 4.970220017507882 Test RE 0.21319931904964098\n",
      "800 Train Loss 3.3620486 Test MSE 4.965505555663956 Test RE 0.21309818081893936\n",
      "900 Train Loss 3.3699691 Test MSE 4.819320803226919 Test RE 0.20993793663067137\n",
      "1000 Train Loss 3.3886414 Test MSE 4.542521658536058 Test RE 0.20381986495696416\n",
      "1100 Train Loss 3.3689053 Test MSE 4.501513393745664 Test RE 0.2028977726491422\n",
      "1200 Train Loss 3.3680887 Test MSE 4.497426121246992 Test RE 0.20280563843279992\n",
      "1300 Train Loss 3.3662205 Test MSE 4.561133267308643 Test RE 0.20423698326898582\n",
      "1400 Train Loss 3.3562186 Test MSE 4.538073076950139 Test RE 0.20372003809155395\n",
      "1500 Train Loss 3.3560762 Test MSE 4.547373285622298 Test RE 0.20392868051757518\n",
      "1600 Train Loss 3.360648 Test MSE 4.7640220481147235 Test RE 0.2087300069361623\n",
      "1700 Train Loss 3.3752098 Test MSE 4.698036312428207 Test RE 0.20727942287762205\n",
      "1800 Train Loss 3.364596 Test MSE 4.562795433939723 Test RE 0.20427419386515128\n",
      "1900 Train Loss 3.4379895 Test MSE 4.817814882142274 Test RE 0.20990513380644465\n",
      "2000 Train Loss 3.3571448 Test MSE 4.659796615252601 Test RE 0.20643412317601265\n",
      "2100 Train Loss 29.671566 Test MSE 30.191749937759244 Test RE 0.5254630097760755\n",
      "2200 Train Loss 29.489935 Test MSE 30.04424402404225 Test RE 0.5241778275062479\n",
      "2300 Train Loss 29.48993 Test MSE 30.044252601378044 Test RE 0.5241779023300465\n",
      "2400 Train Loss 29.489925 Test MSE 30.0442524824563 Test RE 0.5241779012926409\n",
      "2500 Train Loss 29.489925 Test MSE 30.044253032940958 Test RE 0.5241779060947556\n",
      "2600 Train Loss 29.489925 Test MSE 30.044253032940958 Test RE 0.5241779060947556\n",
      "2700 Train Loss 29.489925 Test MSE 30.044253032940958 Test RE 0.5241779060947556\n",
      "2800 Train Loss 29.489925 Test MSE 30.044253032940958 Test RE 0.5241779060947556\n",
      "2900 Train Loss 29.489925 Test MSE 30.044252485089352 Test RE 0.52417790131561\n",
      "Training time: 13.87\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 99.31753 Test MSE 59.0928263553662 Test RE 0.7351321007701463\n",
      "100 Train Loss 4.7624245 Test MSE 5.055265300452637 Test RE 0.21501560591630514\n",
      "200 Train Loss 4.2206106 Test MSE 4.614113186225267 Test RE 0.2054197176959022\n",
      "300 Train Loss 4.224122 Test MSE 5.165341632922413 Test RE 0.2173439379516269\n",
      "400 Train Loss 4.9884925 Test MSE 6.926692367619677 Test RE 0.251687099645398\n",
      "500 Train Loss 4.503952 Test MSE 4.851071510441778 Test RE 0.2106283591531507\n",
      "600 Train Loss 4.598025 Test MSE 5.2956138421269605 Test RE 0.22006762702805774\n",
      "700 Train Loss 5.076233 Test MSE 5.742643006281833 Test RE 0.22916796892049526\n",
      "800 Train Loss 4.5202785 Test MSE 4.914379560757488 Test RE 0.211998288201209\n",
      "900 Train Loss 8.660751 Test MSE 8.937369289212839 Test RE 0.28589256279414677\n",
      "1000 Train Loss 7.160871 Test MSE 7.45424119766665 Test RE 0.2610957043792018\n",
      "1100 Train Loss 16.18861 Test MSE 12.4936323861823 Test RE 0.3380199768753658\n",
      "1200 Train Loss 4.7987204 Test MSE 5.2829098363842055 Test RE 0.2198035009779153\n",
      "1300 Train Loss 12.356401 Test MSE 11.022240824925893 Test RE 0.3174921223606186\n",
      "1400 Train Loss 58.40375 Test MSE 60.56913255408271 Test RE 0.7442582940224839\n",
      "1500 Train Loss 58.359234 Test MSE 60.644991486501894 Test RE 0.7447242159418579\n",
      "1600 Train Loss 58.35922 Test MSE 60.64385442444972 Test RE 0.7447172343132535\n",
      "1700 Train Loss 58.35922 Test MSE 60.64386708090176 Test RE 0.7447173120249815\n",
      "1800 Train Loss 58.36047 Test MSE 60.599047948293084 Test RE 0.7444420677602669\n",
      "1900 Train Loss 58.37107 Test MSE 60.64074913402857 Test RE 0.7446981673119817\n",
      "2000 Train Loss 58.366238 Test MSE 60.73388408437396 Test RE 0.7452698193582202\n",
      "2100 Train Loss 58.35922 Test MSE 60.64416157097317 Test RE 0.744719120217621\n",
      "2200 Train Loss 58.35922 Test MSE 60.643866024503595 Test RE 0.7447173055386043\n",
      "2300 Train Loss 58.35922 Test MSE 60.64386708090176 Test RE 0.7447173120249815\n",
      "2400 Train Loss 58.35922 Test MSE 60.64386708090176 Test RE 0.7447173120249815\n",
      "2500 Train Loss 58.35922 Test MSE 60.64386655270177 Test RE 0.7447173087817872\n",
      "2600 Train Loss 58.35922 Test MSE 60.64386655270177 Test RE 0.7447173087817872\n",
      "2700 Train Loss 58.35922 Test MSE 60.64386655270177 Test RE 0.7447173087817872\n",
      "2800 Train Loss 58.35922 Test MSE 60.64386655270177 Test RE 0.7447173087817872\n",
      "2900 Train Loss 58.35922 Test MSE 60.64386655270177 Test RE 0.7447173087817872\n",
      "Training time: 14.18\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 107.2605 Test MSE 69.69179605875789 Test RE 0.7983417371557797\n",
      "100 Train Loss 11.618226 Test MSE 12.067284657680263 Test RE 0.3322024147115842\n",
      "200 Train Loss 3.5425246 Test MSE 4.757288590264566 Test RE 0.20858244552014418\n",
      "300 Train Loss 3.3051283 Test MSE 4.514385080568725 Test RE 0.20318764986217475\n",
      "400 Train Loss 3.3082862 Test MSE 4.522189018706335 Test RE 0.20336319748781267\n",
      "500 Train Loss 3.3005364 Test MSE 4.52012865424778 Test RE 0.20331686483353137\n",
      "600 Train Loss 3.4280136 Test MSE 4.706511023451725 Test RE 0.20746629262664631\n",
      "700 Train Loss 16.218307 Test MSE 19.948835587492614 Test RE 0.42712676032371466\n",
      "800 Train Loss 62.33657 Test MSE 60.591602873822914 Test RE 0.7443963360444715\n",
      "900 Train Loss 61.850697 Test MSE 60.709683136470815 Test RE 0.7451213187851843\n",
      "1000 Train Loss 61.850693 Test MSE 60.70748159394086 Test RE 0.7451078083280334\n",
      "1100 Train Loss 61.850693 Test MSE 60.707491239045936 Test RE 0.7451078675187842\n",
      "1200 Train Loss 61.850693 Test MSE 60.707490524582354 Test RE 0.7451078631342145\n",
      "1300 Train Loss 61.850693 Test MSE 60.707491239045936 Test RE 0.7451078675187842\n",
      "1400 Train Loss 61.850693 Test MSE 60.707491239045936 Test RE 0.7451078675187842\n",
      "1500 Train Loss 61.850693 Test MSE 60.707491239045936 Test RE 0.7451078675187842\n",
      "1600 Train Loss 61.850693 Test MSE 60.707491239045936 Test RE 0.7451078675187842\n",
      "1700 Train Loss 61.850693 Test MSE 60.707491239045936 Test RE 0.7451078675187842\n",
      "1800 Train Loss 61.850693 Test MSE 60.707491239045936 Test RE 0.7451078675187842\n",
      "1900 Train Loss 61.850693 Test MSE 60.707490524582354 Test RE 0.7451078631342145\n",
      "2000 Train Loss 61.850693 Test MSE 60.707491239045936 Test RE 0.7451078675187842\n",
      "2100 Train Loss 61.850693 Test MSE 60.707491239045936 Test RE 0.7451078675187842\n",
      "2200 Train Loss 61.850693 Test MSE 60.707491239045936 Test RE 0.7451078675187842\n",
      "2300 Train Loss 61.850693 Test MSE 60.707491239045936 Test RE 0.7451078675187842\n",
      "2400 Train Loss 61.850693 Test MSE 60.707490524582354 Test RE 0.7451078631342145\n",
      "2500 Train Loss 61.850693 Test MSE 60.707491239045936 Test RE 0.7451078675187842\n",
      "2600 Train Loss 61.850693 Test MSE 60.707490524582354 Test RE 0.7451078631342145\n",
      "2700 Train Loss 61.850693 Test MSE 60.707491239045936 Test RE 0.7451078675187842\n",
      "2800 Train Loss 61.850693 Test MSE 60.707491239045936 Test RE 0.7451078675187842\n",
      "2900 Train Loss 61.850693 Test MSE 60.70749195351136 Test RE 0.745107871903365\n",
      "Training time: 14.27\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 125.543 Test MSE 63.804444871113766 Test RE 0.7638770696251879\n",
      "100 Train Loss 27.812744 Test MSE 26.022725666319836 Test RE 0.4878366198379668\n",
      "200 Train Loss 14.687243 Test MSE 13.813093552508983 Test RE 0.35542132596671633\n",
      "300 Train Loss 5.7222795 Test MSE 7.036276232131435 Test RE 0.2536701972199783\n",
      "400 Train Loss 5.4188647 Test MSE 5.188463765297326 Test RE 0.21782985391102655\n",
      "500 Train Loss 4.394081 Test MSE 4.690270771465972 Test RE 0.20710804249145576\n",
      "600 Train Loss 4.3199534 Test MSE 4.649154674820122 Test RE 0.2061982635942832\n",
      "700 Train Loss 4.303505 Test MSE 4.5408376631410965 Test RE 0.2037820815894593\n",
      "800 Train Loss 4.294686 Test MSE 4.533152337121581 Test RE 0.203609558910006\n",
      "900 Train Loss 4.292621 Test MSE 4.531457820530253 Test RE 0.20357150018192327\n",
      "1000 Train Loss 4.290839 Test MSE 4.523816151967171 Test RE 0.20339978035303266\n",
      "1100 Train Loss 4.288471 Test MSE 4.48979930972555 Test RE 0.20263360484621828\n",
      "1200 Train Loss 4.2883735 Test MSE 4.414537320345926 Test RE 0.20092806529391852\n",
      "1300 Train Loss 4.2901726 Test MSE 4.409635166302593 Test RE 0.200816473313523\n",
      "1400 Train Loss 4.2892313 Test MSE 4.411840394323756 Test RE 0.20086668050132128\n",
      "1500 Train Loss 4.2885303 Test MSE 4.408741861057908 Test RE 0.20079613155187767\n",
      "1600 Train Loss 4.332379 Test MSE 4.436857097230315 Test RE 0.2014353682065714\n",
      "1700 Train Loss 4.287805 Test MSE 4.407685858133177 Test RE 0.20077208228706658\n",
      "1800 Train Loss 4.290054 Test MSE 4.41884123764365 Test RE 0.20102598802497118\n",
      "1900 Train Loss 4.2885427 Test MSE 4.408553202241025 Test RE 0.20079183527336142\n",
      "2000 Train Loss 4.288842 Test MSE 4.407204684051694 Test RE 0.20076112313955785\n",
      "2100 Train Loss 4.3005548 Test MSE 4.466401661301035 Test RE 0.2021049238805085\n",
      "2200 Train Loss 4.310147 Test MSE 4.419934106443082 Test RE 0.20105084538420828\n",
      "2300 Train Loss 4.289859 Test MSE 4.408031114657159 Test RE 0.2007799454284179\n",
      "2400 Train Loss 4.295166 Test MSE 4.413515920507477 Test RE 0.20090481939622834\n",
      "2500 Train Loss 4.3264594 Test MSE 4.481573597610116 Test RE 0.20244789834603902\n",
      "2600 Train Loss 64.24185 Test MSE 65.64382760354296 Test RE 0.7748095308506157\n",
      "2700 Train Loss 64.218735 Test MSE 61.15367354896596 Test RE 0.7478410172292046\n",
      "2800 Train Loss 64.218666 Test MSE 61.15325367123533 Test RE 0.7478384499073255\n",
      "2900 Train Loss 64.218666 Test MSE 61.15317336393251 Test RE 0.7478379588712402\n",
      "Training time: 14.96\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 121.91756 Test MSE 59.19284379663374 Test RE 0.7357539609092137\n",
      "100 Train Loss 4.2819467 Test MSE 4.866498530240446 Test RE 0.21096300569560708\n",
      "200 Train Loss 4.0246677 Test MSE 5.155538663993877 Test RE 0.21713759848058595\n",
      "300 Train Loss 4.20431 Test MSE 5.003279357484725 Test RE 0.21390718985498747\n",
      "400 Train Loss 3.9503443 Test MSE 4.990592171088328 Test RE 0.21363580754481531\n",
      "500 Train Loss 4.0238047 Test MSE 4.699765491726827 Test RE 0.20731756544279217\n",
      "600 Train Loss 3.9504697 Test MSE 4.7644815663062 Test RE 0.20874007331720953\n",
      "700 Train Loss 3.9503207 Test MSE 4.7728412702923215 Test RE 0.20892311951310824\n",
      "800 Train Loss 3.9502418 Test MSE 4.77801017190356 Test RE 0.2090362188984885\n",
      "900 Train Loss 4.088776 Test MSE 4.785958166276925 Test RE 0.20921000759745054\n",
      "1000 Train Loss 3.9508533 Test MSE 4.736086347994687 Test RE 0.2081171222676352\n",
      "1100 Train Loss 3.9559176 Test MSE 4.724216762616602 Test RE 0.20785616697069495\n",
      "1200 Train Loss 3.9934673 Test MSE 4.731094437655553 Test RE 0.20800741397293576\n",
      "1300 Train Loss 3.950296 Test MSE 4.678267112075734 Test RE 0.2068428502265812\n",
      "1400 Train Loss 3.9531682 Test MSE 4.653729768168753 Test RE 0.20629969540256837\n",
      "1500 Train Loss 3.9515939 Test MSE 4.607952769642276 Test RE 0.20528254142205513\n",
      "1600 Train Loss 4.034993 Test MSE 4.652754324695667 Test RE 0.2062780735801388\n",
      "1700 Train Loss 3.950175 Test MSE 4.598242406519523 Test RE 0.20506613086118725\n",
      "1800 Train Loss 3.9505644 Test MSE 4.614712860401946 Test RE 0.20543306597030875\n",
      "1900 Train Loss 4.145677 Test MSE 4.640254306430868 Test RE 0.20600079547092065\n",
      "2000 Train Loss 3.9508948 Test MSE 4.638512200967458 Test RE 0.20596212207316195\n",
      "2100 Train Loss 3.990556 Test MSE 4.708261347590853 Test RE 0.20750486679578337\n",
      "2200 Train Loss 3.950509 Test MSE 4.659025891064327 Test RE 0.20641705050475823\n",
      "2300 Train Loss 3.9523304 Test MSE 4.681439444987585 Test RE 0.20691296840980752\n",
      "2400 Train Loss 3.950194 Test MSE 4.665517368095131 Test RE 0.20656080213392364\n",
      "2500 Train Loss 4.1174736 Test MSE 5.266163673968319 Test RE 0.21945484968120732\n",
      "2600 Train Loss 22.42814 Test MSE 29.232258776668637 Test RE 0.5170460128551223\n",
      "2700 Train Loss 66.57857 Test MSE 60.91845803394602 Test RE 0.7464014203032582\n",
      "2800 Train Loss 66.17626 Test MSE 60.835239548507396 Test RE 0.7458914301846071\n",
      "2900 Train Loss 66.17626 Test MSE 60.83316582255557 Test RE 0.7458787172602465\n",
      "Training time: 15.37\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 111.57866 Test MSE 76.15927908870945 Test RE 0.8345635608252178\n",
      "100 Train Loss 5.0326653 Test MSE 7.238089190887635 Test RE 0.2572823366401762\n",
      "200 Train Loss 2.5853622 Test MSE 4.625836759067459 Test RE 0.20568051810759788\n",
      "300 Train Loss 2.578674 Test MSE 4.689106391417457 Test RE 0.20708233316112493\n",
      "400 Train Loss 2.575671 Test MSE 4.6718423891460565 Test RE 0.2067007714893149\n",
      "500 Train Loss 2.5752714 Test MSE 4.6698613346279565 Test RE 0.20665694200171164\n",
      "600 Train Loss 2.5768127 Test MSE 4.687274075444676 Test RE 0.20704186944481798\n",
      "700 Train Loss 2.61772 Test MSE 4.757418737152754 Test RE 0.20858529863384953\n",
      "800 Train Loss 2.5876274 Test MSE 4.69694319058903 Test RE 0.20725530696722125\n",
      "900 Train Loss 2.5759802 Test MSE 4.714034233406361 Test RE 0.20763204059161963\n",
      "1000 Train Loss 2.576083 Test MSE 4.760738579155656 Test RE 0.20865806387802416\n",
      "1100 Train Loss 2.5924828 Test MSE 4.758933076916821 Test RE 0.2086184935129645\n",
      "1200 Train Loss 2.5759914 Test MSE 4.751981622649266 Test RE 0.20846607154523047\n",
      "1300 Train Loss 2.6320055 Test MSE 4.800112142231467 Test RE 0.20951913768377506\n",
      "1400 Train Loss 2.59267 Test MSE 4.798685310077016 Test RE 0.20948799561327067\n",
      "1500 Train Loss 2.5788643 Test MSE 4.79665384602297 Test RE 0.20944364884383873\n",
      "1600 Train Loss 2.5937016 Test MSE 4.837766267577424 Test RE 0.21033931108260157\n",
      "1700 Train Loss 2.6012907 Test MSE 4.733061903106351 Test RE 0.20805066029442587\n",
      "1800 Train Loss 2.5803595 Test MSE 4.697256096344696 Test RE 0.20726221042504078\n",
      "1900 Train Loss 2.623189 Test MSE 4.720640628907124 Test RE 0.2077774806819655\n",
      "2000 Train Loss 2.5836058 Test MSE 4.714077390584576 Test RE 0.20763299102940033\n",
      "2100 Train Loss 2.7523458 Test MSE 4.786456450424786 Test RE 0.20922089813530292\n",
      "2200 Train Loss 2.5762405 Test MSE 4.73054639225187 Test RE 0.20799536593519372\n",
      "2300 Train Loss 12.275884 Test MSE 16.761234024431143 Test RE 0.39151733973312824\n",
      "2400 Train Loss 16.166195 Test MSE 20.08841964168104 Test RE 0.428618480377097\n",
      "2500 Train Loss 15.667634 Test MSE 20.108171379701695 Test RE 0.42882914602653555\n",
      "2600 Train Loss 15.66763 Test MSE 20.10788968099843 Test RE 0.42882614224674986\n",
      "2700 Train Loss 15.66763 Test MSE 20.107894506325582 Test RE 0.4288261936998445\n",
      "2800 Train Loss 15.66763 Test MSE 20.107894506325582 Test RE 0.4288261936998445\n",
      "2900 Train Loss 15.66763 Test MSE 20.107894247246172 Test RE 0.428826190937247\n",
      "Training time: 15.39\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 114.27008 Test MSE 99.10104363456212 Test RE 0.9520003766349185\n",
      "100 Train Loss 4.525506 Test MSE 5.970656554420521 Test RE 0.233673278202639\n",
      "200 Train Loss 4.464416 Test MSE 5.678376874301991 Test RE 0.22788204745752166\n",
      "300 Train Loss 4.4709125 Test MSE 5.977243644623895 Test RE 0.23380214197548876\n",
      "400 Train Loss 4.487471 Test MSE 5.77114925095951 Test RE 0.22973605500995764\n",
      "500 Train Loss 4.477586 Test MSE 4.46443204704919 Test RE 0.2020603563892867\n",
      "600 Train Loss 4.4749975 Test MSE 4.9341816949714445 Test RE 0.2124249746323678\n",
      "700 Train Loss 4.474248 Test MSE 5.015446943174332 Test RE 0.2141671347217372\n",
      "800 Train Loss 4.4753165 Test MSE 5.123538215981836 Test RE 0.21646266256983754\n",
      "900 Train Loss 8.150363 Test MSE 6.973474813320914 Test RE 0.2525356088025828\n",
      "1000 Train Loss 11.859708 Test MSE 11.2991767035835 Test RE 0.32145590384105066\n",
      "1100 Train Loss 12.359139 Test MSE 13.28599097337625 Test RE 0.3485739940311039\n",
      "1200 Train Loss 12.359006 Test MSE 13.28532775772348 Test RE 0.3485652937913341\n",
      "1300 Train Loss 12.359008 Test MSE 13.285351665866731 Test RE 0.3485656074285051\n",
      "1400 Train Loss 12.359008 Test MSE 13.285344982550948 Test RE 0.34856551975396033\n",
      "1500 Train Loss 12.359007 Test MSE 13.285335561567106 Test RE 0.3485653961654982\n",
      "1600 Train Loss 12.359006 Test MSE 13.285324139356618 Test RE 0.34856524632403735\n",
      "1700 Train Loss 12.359006 Test MSE 13.285310988478946 Test RE 0.3485650738050982\n",
      "1800 Train Loss 12.359007 Test MSE 13.285296770452849 Test RE 0.34856488728674556\n",
      "1900 Train Loss 12.359006 Test MSE 13.28527847883253 Test RE 0.34856464732903597\n",
      "2000 Train Loss 12.359005 Test MSE 13.285259012106279 Test RE 0.3485643919555762\n",
      "2100 Train Loss 12.359008 Test MSE 13.28523602859359 Test RE 0.3485640904470553\n",
      "2200 Train Loss 12.359005 Test MSE 13.285211438364206 Test RE 0.3485637678605808\n",
      "2300 Train Loss 12.359006 Test MSE 13.285180670324714 Test RE 0.348563364230197\n",
      "2400 Train Loss 12.359005 Test MSE 13.28514566597866 Test RE 0.3485629050252767\n",
      "2500 Train Loss 12.359005 Test MSE 13.285106898312483 Test RE 0.34856239645049175\n",
      "2600 Train Loss 12.359006 Test MSE 13.285060917862612 Test RE 0.3485617932536268\n",
      "2700 Train Loss 12.359006 Test MSE 13.28501195605591 Test RE 0.3485611509445333\n",
      "2800 Train Loss 12.359006 Test MSE 13.284955150494145 Test RE 0.34856040573507396\n",
      "2900 Train Loss 12.359005 Test MSE 13.284887400501656 Test RE 0.34855951694791926\n",
      "Training time: 15.32\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 3000\n",
    "layers = np.array([1,50,50,50,50,1])\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "alpha_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "n_val = 1\n",
    "\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []\n",
    "    alpha_val = []\n",
    "    'Generate Training data'\n",
    "    N_train = 300\n",
    "    \n",
    "    x_train_np_array, u_train_np_array = trainingdata(N_train,reps*1234)\n",
    "\n",
    "    'Convert to tensor and send to GPU'\n",
    "    x_train = torch.from_numpy(x_train_np_array).float().to(device)\n",
    "    u_train = torch.from_numpy(u_train_np_array).float().to(device)\n",
    "    \n",
    "    x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "       \n",
    "     \n",
    "    model_NN = Sequentialmodel(layers,n_val)\n",
    "    model_NN.to(device)\n",
    "    print(model_NN)\n",
    "    params = list(model_NN.parameters())\n",
    "    #optimizer = optim.Adam(PINN.parameters(), lr=0.0008,betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "    optimizer = optim.Adam(model_NN.parameters(), lr=0.08)\n",
    "\n",
    "    train_model(max_iter,reps)\n",
    " \n",
    "    torch.save(model_NN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    alpha_full.append(alpha_val)\n",
    "                 \n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse\": test_mse_full, \"test_re\": test_re_full,\"Time\": elapsed_time, \"Thresh_time\": time_threshold, \"epoch_thresh\": epoch_threshold,\"alpha\": alpha_full, \"label\": label}\n",
    "savemat(label+'.mat', mdic) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred = model_NN.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7eff400345d0>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_test,u_pred,'rx')\n",
    "plt.plot(x_test,u_true,'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5123566767547494\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + test_re_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
