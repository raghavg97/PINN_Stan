{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "#Trying to replicate the results of \"Adaptive activation functions accelerate convergence in deep and physics-informed neural networks\"\n",
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_disc(x): #[-4,3.75]\n",
    "    y  = np.zeros((np.shape(x)[0],))\n",
    "    \n",
    "    y = 0.2*np.sin(6*x)*np.exp(-1*x)\n",
    "    y[x>0] = 1 + 0.1*x[x>0]*np.exp(x[x>0]) + 10\n",
    "    \n",
    "    return scale*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain bounds\n",
    "level = \"medium\"\n",
    "label = \"Regr_disc_atanh_\" + level\n",
    "scale = 10.0\n",
    "loss_thresh = 0.1\n",
    "\n",
    "lb = np.array(-4.0)  # [-1. 0.]\n",
    "ub = np.array(3.75) # [1.  0.99]\n",
    "\n",
    "x_test = np.linspace(lb,ub,1000).reshape(-1,1)\n",
    "u_true = true_disc(x_test)\n",
    "\n",
    "u_true_norm = np.linalg.norm(u_true,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_train,seed):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    x_train = np.random.uniform(low=lb, high=ub, size=(N_train,)).reshape(-1,1)\n",
    "\n",
    "    u_train = true_disc(x_train)\n",
    "\n",
    "    return x_train, u_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,n_val):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "     \n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "\n",
    "        \n",
    "        self.alpha = Parameter(torch.ones((layers[1],len(layers)-2)))\n",
    "        self.alpha.requiresGrad = True\n",
    "        \n",
    "        self.n = torch.tensor(n_val)\n",
    "            \n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = 2.0*(x - l_b)/(u_b - l_b)-1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(self.n*self.alpha[:,i]*z)\n",
    "    \n",
    "        a = self.linears[-1](a) \n",
    "        \n",
    "        return a\n",
    "    \n",
    "    \n",
    "    def loss(self,x,u):\n",
    "\n",
    "        loss_val = self.loss_function(self.forward(x),u)\n",
    "        \n",
    "        return loss_val\n",
    "        \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        u_pred = self.forward(x_test_tensor)       \n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "        \n",
    "        return u_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "        \n",
    "        test_mse_loss = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re_loss = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "        \n",
    "        return test_mse_loss, test_re_loss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    alpha_val.append(model_NN.alpha.cpu().detach().numpy())\n",
    "    test_mse, test_re = model_NN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step():\n",
    "    optimizer.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "    loss = model_NN.loss(x_train,u_train)\n",
    "    loss.backward() #backprop\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "   \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "    \n",
    "    loss_np = model_NN.loss(x_train,u_train).cpu().detach().numpy()\n",
    "    data_update(loss_np)\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        loss_np = train_step().cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1\n",
    "        data_update(loss_np)\n",
    "        if(i%100==0):        \n",
    "            print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 11607.848 Test MSE 10113.933325858354 Test RE 0.9617408194327417\n",
      "100 Train Loss 6008.6104 Test MSE 6079.06799256097 Test RE 0.7456182108343667\n",
      "200 Train Loss 6008.4883 Test MSE 6082.522275917709 Test RE 0.7458300205069971\n",
      "300 Train Loss 6008.4883 Test MSE 6082.516850371319 Test RE 0.7458296878706258\n",
      "400 Train Loss 6008.488 Test MSE 6082.516002155542 Test RE 0.7458296358671094\n",
      "500 Train Loss 6008.4883 Test MSE 6082.516390347203 Test RE 0.7458296596668692\n",
      "600 Train Loss 6008.4883 Test MSE 6082.516850371319 Test RE 0.7458296878706258\n",
      "700 Train Loss 6008.4883 Test MSE 6082.516850371319 Test RE 0.7458296878706258\n",
      "800 Train Loss 6008.4883 Test MSE 6082.516850371319 Test RE 0.7458296878706258\n",
      "900 Train Loss 6008.4883 Test MSE 6082.517005379014 Test RE 0.745829697374039\n",
      "1000 Train Loss 6008.4883 Test MSE 6082.516823233243 Test RE 0.7458296862068092\n",
      "1100 Train Loss 6008.4883 Test MSE 6082.516927875109 Test RE 0.7458296926223289\n",
      "1200 Train Loss 6008.4883 Test MSE 6082.516927875109 Test RE 0.7458296926223289\n",
      "1300 Train Loss 6008.4883 Test MSE 6082.516927875109 Test RE 0.7458296926223289\n",
      "1400 Train Loss 6008.4883 Test MSE 6082.516850371319 Test RE 0.7458296878706258\n",
      "1500 Train Loss 6008.4883 Test MSE 6082.516927875109 Test RE 0.7458296926223289\n",
      "1600 Train Loss 6008.4883 Test MSE 6082.516927875109 Test RE 0.7458296926223289\n",
      "1700 Train Loss 6008.4883 Test MSE 6082.516927875109 Test RE 0.7458296926223289\n",
      "1800 Train Loss 6008.4883 Test MSE 6082.516927875109 Test RE 0.7458296926223289\n",
      "1900 Train Loss 6008.6484 Test MSE 6081.015853541512 Test RE 0.7457376571292569\n",
      "2000 Train Loss 6008.4893 Test MSE 6082.034455367285 Test RE 0.7458001119846964\n",
      "2100 Train Loss 6008.4883 Test MSE 6082.51545532302 Test RE 0.7458296023411877\n",
      "2200 Train Loss 6008.4883 Test MSE 6082.516927875109 Test RE 0.7458296926223289\n",
      "2300 Train Loss 6008.4883 Test MSE 6082.516850371319 Test RE 0.7458296878706258\n",
      "2400 Train Loss 6008.4883 Test MSE 6082.516850371319 Test RE 0.7458296878706258\n",
      "2500 Train Loss 6008.4883 Test MSE 6082.516927875109 Test RE 0.7458296926223289\n",
      "2600 Train Loss 6008.4883 Test MSE 6082.516907985434 Test RE 0.7458296914029067\n",
      "2700 Train Loss 6008.4883 Test MSE 6082.516927875109 Test RE 0.7458296926223289\n",
      "2800 Train Loss 6008.4883 Test MSE 6082.516927875109 Test RE 0.7458296926223289\n",
      "2900 Train Loss 6008.489 Test MSE 6082.517138328007 Test RE 0.7458297055250481\n",
      "Training time: 14.55\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 11719.888 Test MSE 10371.97957320019 Test RE 0.973932441997956\n",
      "100 Train Loss 568.22217 Test MSE 651.4932881646233 Test RE 0.24409168146097274\n",
      "200 Train Loss 426.00522 Test MSE 474.55533350599273 Test RE 0.20832502161331265\n",
      "300 Train Loss 405.40393 Test MSE 453.9558491830892 Test RE 0.20375337647243943\n",
      "400 Train Loss 398.96976 Test MSE 453.0243111208898 Test RE 0.2035442135136562\n",
      "500 Train Loss 431.27643 Test MSE 488.0411376405351 Test RE 0.21126435183096773\n",
      "600 Train Loss 538.07794 Test MSE 528.9542965940245 Test RE 0.2199414484136952\n",
      "700 Train Loss 557.5578 Test MSE 579.8039125178241 Test RE 0.23027064475818051\n",
      "800 Train Loss 1071.2898 Test MSE 1347.5333899615496 Test RE 0.35104902709785896\n",
      "900 Train Loss 1259.9883 Test MSE 1721.9265254632298 Test RE 0.39683074675392077\n",
      "1000 Train Loss 1312.531 Test MSE 1395.5614498377513 Test RE 0.3572502063306936\n",
      "1100 Train Loss 1257.0374 Test MSE 1341.8783790229727 Test RE 0.35031165271251363\n",
      "1200 Train Loss 1256.0979 Test MSE 1339.0971234402634 Test RE 0.3499484261973826\n",
      "1300 Train Loss 1256.0212 Test MSE 1338.7991245479243 Test RE 0.3499094857635698\n",
      "1400 Train Loss 1255.9521 Test MSE 1338.700171002687 Test RE 0.3498965542403428\n",
      "1500 Train Loss 1255.88 Test MSE 1338.6146063086235 Test RE 0.34988537202393816\n",
      "1600 Train Loss 1255.8044 Test MSE 1338.5264126408126 Test RE 0.3498738458600959\n",
      "1700 Train Loss 1255.7255 Test MSE 1338.4342292148938 Test RE 0.3498617978628474\n",
      "1800 Train Loss 1255.643 Test MSE 1338.3380047473174 Test RE 0.3498492212753735\n",
      "1900 Train Loss 1255.557 Test MSE 1338.237795844321 Test RE 0.34983612343954057\n",
      "2000 Train Loss 1255.4677 Test MSE 1338.1335493064375 Test RE 0.34982249734294585\n",
      "2100 Train Loss 1255.3749 Test MSE 1338.025496992977 Test RE 0.34980837323207337\n",
      "2200 Train Loss 1255.2788 Test MSE 1337.9134159812088 Test RE 0.3497937219053348\n",
      "2300 Train Loss 1255.1793 Test MSE 1337.7977898759436 Test RE 0.34977860651777964\n",
      "2400 Train Loss 1255.0768 Test MSE 1337.6783803054127 Test RE 0.3497629958456909\n",
      "2500 Train Loss 1254.9712 Test MSE 1337.5554599576028 Test RE 0.34974692547427566\n",
      "2600 Train Loss 1254.8625 Test MSE 1337.4291973809488 Test RE 0.3497304173774113\n",
      "2700 Train Loss 1254.7511 Test MSE 1337.2996476576004 Test RE 0.34971347869529856\n",
      "2800 Train Loss 1254.6371 Test MSE 1337.1671459885222 Test RE 0.34969615319627356\n",
      "2900 Train Loss 1254.5205 Test MSE 1337.0317366482177 Test RE 0.34967844661247\n",
      "Training time: 13.95\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 11175.824 Test MSE 10226.495436654 Test RE 0.9670778150688585\n",
      "100 Train Loss 541.33716 Test MSE 600.0376534377199 Test RE 0.2342541309388531\n",
      "200 Train Loss 415.17844 Test MSE 489.2747404472523 Test RE 0.21153118570792226\n",
      "300 Train Loss 809.82855 Test MSE 836.0015273745112 Test RE 0.2765040784879362\n",
      "400 Train Loss 531.2928 Test MSE 563.5207759783892 Test RE 0.22701417316500613\n",
      "500 Train Loss 828.9189 Test MSE 971.2420695854046 Test RE 0.2980312120554365\n",
      "600 Train Loss 428.43787 Test MSE 506.6242419528272 Test RE 0.2152489242371037\n",
      "700 Train Loss 776.8899 Test MSE 5642.3303057572575 Test RE 0.7183353836884384\n",
      "800 Train Loss 542.1604 Test MSE 555.8401675746392 Test RE 0.2254618000502888\n",
      "900 Train Loss 1005.5725 Test MSE 1085.3091394780813 Test RE 0.3150465559287905\n",
      "1000 Train Loss 1391.9281 Test MSE 1430.2888504875657 Test RE 0.36166783196381697\n",
      "1100 Train Loss 1722.0419 Test MSE 1610.180031129239 Test RE 0.38373836812655276\n",
      "1200 Train Loss 1092.1975 Test MSE 1632.6913152367745 Test RE 0.38641150399761964\n",
      "1300 Train Loss 1651.71 Test MSE 1678.5862799198 Test RE 0.3918048798027774\n",
      "1400 Train Loss 1557.3745 Test MSE 1578.361518423472 Test RE 0.37992795337613855\n",
      "1500 Train Loss 1532.8455 Test MSE 1548.7140584452272 Test RE 0.37634281278773596\n",
      "1600 Train Loss 1508.3436 Test MSE 1498.9718369666275 Test RE 0.37024972345806784\n",
      "1700 Train Loss 1505.809 Test MSE 1493.6736523327445 Test RE 0.36959481193673727\n",
      "1800 Train Loss 1504.0795 Test MSE 1489.6852603051905 Test RE 0.36910103796565047\n",
      "1900 Train Loss 1502.5568 Test MSE 1486.1740768768047 Test RE 0.36866579636133545\n",
      "2000 Train Loss 1501.1118 Test MSE 1482.8924088026174 Test RE 0.36825854010447273\n",
      "2100 Train Loss 1499.7219 Test MSE 1479.7609692547744 Test RE 0.36786950689195913\n",
      "2200 Train Loss 1498.3849 Test MSE 1476.7542195214867 Test RE 0.36749557691018636\n",
      "2300 Train Loss 1497.1064 Test MSE 1473.8672662450663 Test RE 0.3671361868469151\n",
      "2400 Train Loss 1495.8705 Test MSE 1470.916242334638 Test RE 0.36676845681025255\n",
      "2500 Train Loss 1482.6565 Test MSE 1463.8774747043703 Test RE 0.3658898568921856\n",
      "2600 Train Loss 1481.6344 Test MSE 1461.6228254061875 Test RE 0.365607978383104\n",
      "2700 Train Loss 1480.6498 Test MSE 1459.4415298397812 Test RE 0.3653350636429211\n",
      "2800 Train Loss 1479.7023 Test MSE 1457.338595378594 Test RE 0.3650717599632275\n",
      "2900 Train Loss 1478.7943 Test MSE 1455.3225851503607 Test RE 0.3648191614548348\n",
      "Training time: 14.04\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 12235.496 Test MSE 10102.00925185853 Test RE 0.9611737180683942\n",
      "100 Train Loss 477.41458 Test MSE 590.3490380258389 Test RE 0.23235522133839434\n",
      "200 Train Loss 357.7922 Test MSE 476.85919358444687 Test RE 0.20883009505991249\n",
      "300 Train Loss 343.03067 Test MSE 464.97851609517255 Test RE 0.20621224470992142\n",
      "400 Train Loss 337.6148 Test MSE 475.8624432918448 Test RE 0.20861172835268998\n",
      "500 Train Loss 346.4689 Test MSE 496.4959023750847 Test RE 0.21308645309454013\n",
      "600 Train Loss 390.72318 Test MSE 562.076057606514 Test RE 0.2267229842340692\n",
      "700 Train Loss 370.6099 Test MSE 484.73972399846383 Test RE 0.21054857758807852\n",
      "800 Train Loss 370.2892 Test MSE 485.23199327731436 Test RE 0.210655459993765\n",
      "900 Train Loss 361.14496 Test MSE 525.1722105421915 Test RE 0.21915373408413028\n",
      "1000 Train Loss 473.77954 Test MSE 706.9089862582765 Test RE 0.2542610039089469\n",
      "1100 Train Loss 471.80646 Test MSE 670.0291899427586 Test RE 0.2475397042801117\n",
      "1200 Train Loss 559.8872 Test MSE 687.0862926138874 Test RE 0.250670743052353\n",
      "1300 Train Loss 863.50665 Test MSE 1018.320268303789 Test RE 0.3051688493156104\n",
      "1400 Train Loss 860.84454 Test MSE 942.7022259722855 Test RE 0.2936197553394751\n",
      "1500 Train Loss 1203.6765 Test MSE 1197.4164623783302 Test RE 0.33091817343050606\n",
      "1600 Train Loss 1197.9242 Test MSE 1292.3817114064227 Test RE 0.3437901329979948\n",
      "1700 Train Loss 1197.921 Test MSE 1292.8076087749603 Test RE 0.3438467754166959\n",
      "1800 Train Loss 1197.9211 Test MSE 1292.8589141870957 Test RE 0.34385359817415173\n",
      "1900 Train Loss 1197.9211 Test MSE 1292.9067712196843 Test RE 0.3438599622330572\n",
      "2000 Train Loss 1197.9211 Test MSE 1292.95446575744 Test RE 0.3438663045661126\n",
      "2100 Train Loss 1197.9211 Test MSE 1293.0033955930057 Test RE 0.34387281104526773\n",
      "2200 Train Loss 1197.9211 Test MSE 1293.0517061569292 Test RE 0.34387923505556184\n",
      "2300 Train Loss 1197.9211 Test MSE 1293.098180542209 Test RE 0.3438854147900556\n",
      "2400 Train Loss 1197.9211 Test MSE 1293.1451517221924 Test RE 0.3438916604708835\n",
      "2500 Train Loss 1197.9211 Test MSE 1293.1896314906287 Test RE 0.34389757476823535\n",
      "2600 Train Loss 1197.9211 Test MSE 1293.2320734953898 Test RE 0.34390321801745255\n",
      "2700 Train Loss 1197.9211 Test MSE 1293.2721926023137 Test RE 0.3439085523202644\n",
      "2800 Train Loss 1197.9211 Test MSE 1293.3098960000887 Test RE 0.34391356535097717\n",
      "2900 Train Loss 1197.9211 Test MSE 1293.3451268657022 Test RE 0.34391824956862943\n",
      "Training time: 14.37\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 10293.483 Test MSE 10005.813697292755 Test RE 0.9565864224609185\n",
      "100 Train Loss 5835.9404 Test MSE 6064.9148605274795 Test RE 0.7447497403412332\n",
      "200 Train Loss 5836.8804 Test MSE 6069.947590800144 Test RE 0.7450586768567558\n",
      "300 Train Loss 5840.8276 Test MSE 6081.414283715727 Test RE 0.7457620872183277\n",
      "400 Train Loss 5835.922 Test MSE 6064.248153401448 Test RE 0.7447088045971326\n",
      "500 Train Loss 5835.922 Test MSE 6064.384727464696 Test RE 0.7447171904128322\n",
      "600 Train Loss 5835.922 Test MSE 6064.386671116156 Test RE 0.7447173097547444\n",
      "700 Train Loss 5835.922 Test MSE 6064.386671116156 Test RE 0.7447173097547444\n",
      "800 Train Loss 5835.922 Test MSE 6064.386671116156 Test RE 0.7447173097547444\n",
      "900 Train Loss 5835.922 Test MSE 6064.386671116156 Test RE 0.7447173097547444\n",
      "1000 Train Loss 5835.922 Test MSE 6064.386671116156 Test RE 0.7447173097547444\n",
      "1100 Train Loss 5835.922 Test MSE 6064.386671116156 Test RE 0.7447173097547444\n",
      "1200 Train Loss 5835.922 Test MSE 6064.386671116156 Test RE 0.7447173097547444\n",
      "1300 Train Loss 5835.922 Test MSE 6064.386671116156 Test RE 0.7447173097547444\n",
      "1400 Train Loss 5835.922 Test MSE 6064.386671116156 Test RE 0.7447173097547444\n",
      "1500 Train Loss 5835.922 Test MSE 6064.386671116156 Test RE 0.7447173097547444\n",
      "1600 Train Loss 5835.922 Test MSE 6064.386713372186 Test RE 0.7447173123493015\n",
      "1700 Train Loss 5835.922 Test MSE 6064.386671116156 Test RE 0.7447173097547444\n",
      "1800 Train Loss 5835.922 Test MSE 6064.386628860245 Test RE 0.7447173071601944\n",
      "1900 Train Loss 5835.922 Test MSE 6064.386713372186 Test RE 0.7447173123493015\n",
      "2000 Train Loss 5835.922 Test MSE 6064.386671116156 Test RE 0.7447173097547444\n",
      "2100 Train Loss 5835.922 Test MSE 6064.386628860245 Test RE 0.7447173071601944\n",
      "2200 Train Loss 5835.922 Test MSE 6064.386671116156 Test RE 0.7447173097547444\n",
      "2300 Train Loss 5835.922 Test MSE 6064.386671116156 Test RE 0.7447173097547444\n",
      "2400 Train Loss 5835.922 Test MSE 6064.386671116156 Test RE 0.7447173097547444\n",
      "2500 Train Loss 5835.922 Test MSE 6064.386671116156 Test RE 0.7447173097547444\n",
      "2600 Train Loss 5835.922 Test MSE 6064.386671116156 Test RE 0.7447173097547444\n",
      "2700 Train Loss 5835.922 Test MSE 6064.386671116156 Test RE 0.7447173097547444\n",
      "2800 Train Loss 5835.922 Test MSE 6064.386628860245 Test RE 0.7447173071601944\n",
      "2900 Train Loss 5835.922 Test MSE 6064.386671116156 Test RE 0.7447173097547444\n",
      "Training time: 13.98\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 10570.856 Test MSE 10427.643799666084 Test RE 0.9765423899093785\n",
      "100 Train Loss 2538.903 Test MSE 2455.6523008224754 Test RE 0.47389425456825857\n",
      "200 Train Loss 1095.9702 Test MSE 1241.463159609822 Test RE 0.33694958387805185\n",
      "300 Train Loss 1121.2739 Test MSE 1264.3262107019984 Test RE 0.34003809687577136\n",
      "400 Train Loss 703.3299 Test MSE 780.4049385243202 Test RE 0.2671517421650513\n",
      "500 Train Loss 1308.9303 Test MSE 1414.6252139534658 Test RE 0.35968199918568766\n",
      "600 Train Loss 777.3816 Test MSE 948.4900993476525 Test RE 0.2945197391344612\n",
      "700 Train Loss 587.70026 Test MSE 832.0286458861432 Test RE 0.27584628892958\n",
      "800 Train Loss 552.6139 Test MSE 755.2131568492066 Test RE 0.2628044893945089\n",
      "900 Train Loss 473.38626 Test MSE 677.671341049069 Test RE 0.24894738341963338\n",
      "1000 Train Loss 443.65115 Test MSE 645.850511073368 Test RE 0.24303230726653394\n",
      "1100 Train Loss 424.23157 Test MSE 624.1419524113007 Test RE 0.23891295161114126\n",
      "1200 Train Loss 412.02286 Test MSE 609.936077120252 Test RE 0.23617839518148123\n",
      "1300 Train Loss 404.4096 Test MSE 600.9692297417512 Test RE 0.2344359036672043\n",
      "1400 Train Loss 399.93652 Test MSE 595.5410931228029 Test RE 0.23337475393098503\n",
      "1500 Train Loss 397.45718 Test MSE 592.1350890144496 Test RE 0.23270644140861016\n",
      "1600 Train Loss 396.10382 Test MSE 590.2466610324398 Test RE 0.23233507320667376\n",
      "1700 Train Loss 395.41733 Test MSE 588.7981254846359 Test RE 0.23204980912195045\n",
      "1800 Train Loss 384.2003 Test MSE 577.5619156055099 Test RE 0.2298250060624588\n",
      "1900 Train Loss 379.5394 Test MSE 572.412389613864 Test RE 0.22879815541068654\n",
      "2000 Train Loss 376.64655 Test MSE 568.9302840292087 Test RE 0.22810118005588606\n",
      "2100 Train Loss 377.26364 Test MSE 569.0798460322528 Test RE 0.22813116002756142\n",
      "2200 Train Loss 373.2134 Test MSE 564.5173432211932 Test RE 0.22721481790114115\n",
      "2300 Train Loss 371.42023 Test MSE 563.4702284608186 Test RE 0.22700399141063876\n",
      "2400 Train Loss 369.54428 Test MSE 568.5078324894962 Test RE 0.22801647759576466\n",
      "2500 Train Loss 367.4102 Test MSE 563.2468377012033 Test RE 0.22695898848179458\n",
      "2600 Train Loss 364.97928 Test MSE 553.0686021835869 Test RE 0.22489899165380653\n",
      "2700 Train Loss 1397.849 Test MSE 1495.9876306604676 Test RE 0.3698809867014051\n",
      "2800 Train Loss 1184.841 Test MSE 1349.3364615678481 Test RE 0.3512838097547041\n",
      "2900 Train Loss 1184.8367 Test MSE 1353.514370822837 Test RE 0.35182722410356776\n",
      "Training time: 14.19\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 12440.262 Test MSE 10350.627206200317 Test RE 0.9729294282210869\n",
      "100 Train Loss 2333.358 Test MSE 2045.1168986044177 Test RE 0.4324710839451192\n",
      "200 Train Loss 660.0525 Test MSE 631.7650680733352 Test RE 0.24036753550188958\n",
      "300 Train Loss 473.77844 Test MSE 497.50025148441716 Test RE 0.2133018678313032\n",
      "400 Train Loss 521.5208 Test MSE 608.1666658838888 Test RE 0.2358355721877112\n",
      "500 Train Loss 685.73895 Test MSE 705.0658019434358 Test RE 0.25392930931313884\n",
      "600 Train Loss 545.2782 Test MSE 610.8220607774637 Test RE 0.23634986746596132\n",
      "700 Train Loss 491.23547 Test MSE 531.5503448896249 Test RE 0.22048051177430322\n",
      "800 Train Loss 477.96432 Test MSE 510.27374571807724 Test RE 0.21602281351503533\n",
      "900 Train Loss 473.48407 Test MSE 508.7969971584317 Test RE 0.21570999855712183\n",
      "1000 Train Loss 467.08585 Test MSE 501.09350540586627 Test RE 0.21407078081818603\n",
      "1100 Train Loss 466.70996 Test MSE 500.705563979519 Test RE 0.2139878990776557\n",
      "1200 Train Loss 466.55075 Test MSE 500.59439235782156 Test RE 0.21396414189967433\n",
      "1300 Train Loss 466.53345 Test MSE 500.5351849581639 Test RE 0.21395148830700583\n",
      "1400 Train Loss 466.48917 Test MSE 500.5031404925829 Test RE 0.21394463956685436\n",
      "1500 Train Loss 466.5462 Test MSE 500.58464268333114 Test RE 0.21396205828574924\n",
      "1600 Train Loss 466.5021 Test MSE 500.51119756418774 Test RE 0.21394636159435398\n",
      "1700 Train Loss 466.44565 Test MSE 500.42383307732945 Test RE 0.213927688555766\n",
      "1800 Train Loss 466.45172 Test MSE 500.4574964405518 Test RE 0.2139348838609255\n",
      "1900 Train Loss 466.46616 Test MSE 500.4136794817088 Test RE 0.2139255182492015\n",
      "2000 Train Loss 466.442 Test MSE 500.3632034670158 Test RE 0.2139147287960388\n",
      "2100 Train Loss 466.4688 Test MSE 500.5398249769954 Test RE 0.21395247998218064\n",
      "2200 Train Loss 466.4375 Test MSE 500.36903910746116 Test RE 0.2139159762157079\n",
      "2300 Train Loss 466.42578 Test MSE 500.343331675608 Test RE 0.21391048097061258\n",
      "2400 Train Loss 466.50027 Test MSE 500.412714893419 Test RE 0.21392531206963675\n",
      "2500 Train Loss 466.46063 Test MSE 500.40807927640543 Test RE 0.21392432120940946\n",
      "2600 Train Loss 481.9251 Test MSE 511.22318980819415 Test RE 0.21622369222779178\n",
      "2700 Train Loss 1744.5978 Test MSE 1367.7104552467251 Test RE 0.35366744876692235\n",
      "2800 Train Loss 1367.7096 Test MSE 1189.584143033927 Test RE 0.32983412742189533\n",
      "2900 Train Loss 1452.5496 Test MSE 1284.5222237089258 Test RE 0.34274317649821545\n",
      "Training time: 14.06\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 12237.443 Test MSE 10273.258445950003 Test RE 0.9692863863392145\n",
      "100 Train Loss 4540.062 Test MSE 4060.6588958248867 Test RE 0.6093913156167837\n",
      "200 Train Loss 1009.5369 Test MSE 869.4437287420162 Test RE 0.2819802854292269\n",
      "300 Train Loss 482.3736 Test MSE 581.0313097440539 Test RE 0.23051424791838146\n",
      "400 Train Loss 521.29846 Test MSE 550.5642419946637 Test RE 0.22438922928660568\n",
      "500 Train Loss 478.23834 Test MSE 520.2827945626211 Test RE 0.21813117476689428\n",
      "600 Train Loss 588.0339 Test MSE 727.8613273412609 Test RE 0.2580015580958217\n",
      "700 Train Loss 424.57294 Test MSE 528.8583186312015 Test RE 0.21992149348568435\n",
      "800 Train Loss 597.7631 Test MSE 607.925839565685 Test RE 0.23578887361031642\n",
      "900 Train Loss 452.84375 Test MSE 546.5077083961488 Test RE 0.22356105589617728\n",
      "1000 Train Loss 811.4179 Test MSE 826.5071182161221 Test RE 0.2749294766058223\n",
      "1100 Train Loss 539.9247 Test MSE 608.7791101571152 Test RE 0.23595428948100786\n",
      "1200 Train Loss 915.91534 Test MSE 914.619984749334 Test RE 0.2892133589861255\n",
      "1300 Train Loss 915.8492 Test MSE 914.1583849344177 Test RE 0.28914036819016614\n",
      "1400 Train Loss 915.8397 Test MSE 914.0585789699608 Test RE 0.2891245838788849\n",
      "1500 Train Loss 915.8378 Test MSE 914.0159653674875 Test RE 0.28911784427599335\n",
      "1600 Train Loss 915.8375 Test MSE 913.998277203347 Test RE 0.2891150467380832\n",
      "1700 Train Loss 915.8375 Test MSE 913.9912210866671 Test RE 0.2891139307439441\n",
      "1800 Train Loss 915.8375 Test MSE 913.988830883431 Test RE 0.2891135527088554\n",
      "1900 Train Loss 915.8374 Test MSE 913.987869336925 Test RE 0.2891134006303014\n",
      "2000 Train Loss 915.8374 Test MSE 913.987503422842 Test RE 0.28911334275717254\n",
      "2100 Train Loss 915.8375 Test MSE 913.9875055894273 Test RE 0.28911334309984055\n",
      "2200 Train Loss 915.8374 Test MSE 913.9875190465106 Test RE 0.2891133452282189\n",
      "2300 Train Loss 915.8374 Test MSE 913.9875190535787 Test RE 0.2891133452293368\n",
      "2400 Train Loss 915.8374 Test MSE 913.9875108494064 Test RE 0.2891133439317613\n",
      "2500 Train Loss 915.8375 Test MSE 913.9874819552826 Test RE 0.28911333936185374\n",
      "2600 Train Loss 915.8374 Test MSE 913.9875031623728 Test RE 0.28911334271597655\n",
      "2700 Train Loss 915.8375 Test MSE 913.9874956190182 Test RE 0.2891133415229163\n",
      "2800 Train Loss 915.8375 Test MSE 913.9875008995182 Test RE 0.28911334235808256\n",
      "2900 Train Loss 915.8375 Test MSE 913.987495652209 Test RE 0.28911334152816576\n",
      "Training time: 14.14\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 10985.497 Test MSE 10551.184093643604 Test RE 0.9823100932074017\n",
      "100 Train Loss 437.61188 Test MSE 738.39169017033 Test RE 0.25986118013374754\n",
      "200 Train Loss 299.21146 Test MSE 596.7856708599563 Test RE 0.23361848306717595\n",
      "300 Train Loss 269.57413 Test MSE 553.706695229087 Test RE 0.2250286908573018\n",
      "400 Train Loss 260.8547 Test MSE 495.58636723311395 Test RE 0.2128911861666436\n",
      "500 Train Loss 258.78375 Test MSE 501.2283326674315 Test RE 0.2140995784733378\n",
      "600 Train Loss 257.9235 Test MSE 561.5178530257217 Test RE 0.22661037558992334\n",
      "700 Train Loss 549.6717 Test MSE 873.6862230058587 Test RE 0.28266741669630857\n",
      "800 Train Loss 498.11313 Test MSE 834.9317480220238 Test RE 0.2763271092877279\n",
      "900 Train Loss 533.1215 Test MSE 927.1191689782615 Test RE 0.2911828459818802\n",
      "1000 Train Loss 339.98788 Test MSE 650.585935456918 Test RE 0.24392164562158314\n",
      "1100 Train Loss 624.9761 Test MSE 914.8741302149708 Test RE 0.28925353805486664\n",
      "1200 Train Loss 611.17535 Test MSE 911.6893856748899 Test RE 0.2887496426641631\n",
      "1300 Train Loss 554.65295 Test MSE 818.2644701775561 Test RE 0.27355512349479943\n",
      "1400 Train Loss 538.5433 Test MSE 794.6320103593753 Test RE 0.2695758815579218\n",
      "1500 Train Loss 533.1278 Test MSE 785.8322118249247 Test RE 0.26807907703704487\n",
      "1600 Train Loss 530.4941 Test MSE 782.0751153749644 Test RE 0.2674374606175556\n",
      "1700 Train Loss 528.59106 Test MSE 780.4484815018659 Test RE 0.26715919497543195\n",
      "1800 Train Loss 526.94 Test MSE 779.8412814346713 Test RE 0.26705524791841473\n",
      "1900 Train Loss 525.442 Test MSE 779.8104678695257 Test RE 0.26704997184124146\n",
      "2000 Train Loss 524.0778 Test MSE 780.161378397675 Test RE 0.2671100506107445\n",
      "2100 Train Loss 522.8414 Test MSE 780.7983597320058 Test RE 0.26721907254247446\n",
      "2200 Train Loss 521.7273 Test MSE 781.6663109632856 Test RE 0.2673675543499465\n",
      "2300 Train Loss 520.7286 Test MSE 782.7248039962076 Test RE 0.267548520924822\n",
      "2400 Train Loss 519.83704 Test MSE 783.9388486875205 Test RE 0.26775593099160216\n",
      "2500 Train Loss 519.0436 Test MSE 785.2752147240765 Test RE 0.2679840531024317\n",
      "2600 Train Loss 518.33826 Test MSE 786.701628578418 Test RE 0.2682273326106947\n",
      "2700 Train Loss 517.71124 Test MSE 788.1879669858184 Test RE 0.26848059791595197\n",
      "2800 Train Loss 517.1526 Test MSE 789.7053647495271 Test RE 0.2687389093781593\n",
      "2900 Train Loss 516.65295 Test MSE 791.2285785052284 Test RE 0.2689979614379124\n",
      "Training time: 14.10\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 11161.517 Test MSE 10828.10753947994 Test RE 0.9951173225984911\n",
      "100 Train Loss 719.6306 Test MSE 724.669905006947 Test RE 0.25743531253445656\n",
      "200 Train Loss 487.97302 Test MSE 566.0342962066517 Test RE 0.22751989529309516\n",
      "300 Train Loss 468.17 Test MSE 603.7969025942672 Test RE 0.23498678885413693\n",
      "400 Train Loss 450.74408 Test MSE 490.13758528847745 Test RE 0.21171762308086634\n",
      "500 Train Loss 448.6812 Test MSE 461.37076016156897 Test RE 0.20541068921051553\n",
      "600 Train Loss 447.97742 Test MSE 463.07750352351326 Test RE 0.20579027514406756\n",
      "700 Train Loss 879.4994 Test MSE 1022.337892437756 Test RE 0.30577025481185266\n",
      "800 Train Loss 1994.5745 Test MSE 1907.4123559933957 Test RE 0.41765751674078083\n",
      "900 Train Loss 3274.516 Test MSE 3017.0172713715706 Test RE 0.5252752091420909\n",
      "1000 Train Loss 1494.1775 Test MSE 1506.5205063403923 Test RE 0.3711808226330636\n",
      "1100 Train Loss 1493.9728 Test MSE 1506.5357118938346 Test RE 0.3711826958221673\n",
      "1200 Train Loss 1493.9723 Test MSE 1506.5364539754737 Test RE 0.37118278723979103\n",
      "1300 Train Loss 1303.0931 Test MSE 1332.5146056386259 Test RE 0.34908725655976336\n",
      "1400 Train Loss 1235.9008 Test MSE 1296.3056060091699 Test RE 0.3443116406301494\n",
      "1500 Train Loss 1235.9005 Test MSE 1296.3067132238416 Test RE 0.34431178767372345\n",
      "1600 Train Loss 1235.9006 Test MSE 1296.3083051276124 Test RE 0.3443119990862924\n",
      "1700 Train Loss 1235.9006 Test MSE 1296.3098533998002 Test RE 0.3443122047042503\n",
      "1800 Train Loss 1235.9006 Test MSE 1296.3114159511485 Test RE 0.3443124122184245\n",
      "1900 Train Loss 1235.9006 Test MSE 1296.3128830307805 Test RE 0.3443126070533917\n",
      "2000 Train Loss 1235.9005 Test MSE 1296.3143168994561 Test RE 0.3443127974776834\n",
      "2100 Train Loss 1235.9005 Test MSE 1296.315771621136 Test RE 0.34431299067124144\n",
      "2200 Train Loss 1235.9006 Test MSE 1296.3172025446786 Test RE 0.34431318070419464\n",
      "2300 Train Loss 1235.9005 Test MSE 1296.3198339152927 Test RE 0.34431353016153576\n",
      "2400 Train Loss 1235.9005 Test MSE 1296.3228291454652 Test RE 0.3443139279405606\n",
      "2500 Train Loss 1235.9006 Test MSE 1296.3257778120337 Test RE 0.3443143195352961\n",
      "2600 Train Loss 1235.9005 Test MSE 1296.328737230831 Test RE 0.3443147125575229\n",
      "2700 Train Loss 1235.9006 Test MSE 1296.3316488104329 Test RE 0.34431509922608244\n",
      "2800 Train Loss 1235.9005 Test MSE 1296.3346674497266 Test RE 0.3443155001121072\n",
      "2900 Train Loss 1235.9005 Test MSE 1296.3376423776774 Test RE 0.3443158951926536\n",
      "Training time: 14.93\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 3000\n",
    "layers = np.array([1,50,50,50,50,1])\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "alpha_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "n_val = 1\n",
    "\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []\n",
    "    alpha_val = []\n",
    "    'Generate Training data'\n",
    "    N_train = 300\n",
    "    \n",
    "    x_train_np_array, u_train_np_array = trainingdata(N_train,reps*1234)\n",
    "\n",
    "    'Convert to tensor and send to GPU'\n",
    "    x_train = torch.from_numpy(x_train_np_array).float().to(device)\n",
    "    u_train = torch.from_numpy(u_train_np_array).float().to(device)\n",
    "    \n",
    "    x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "       \n",
    "     \n",
    "    model_NN = Sequentialmodel(layers,n_val)\n",
    "    model_NN.to(device)\n",
    "    print(model_NN)\n",
    "    params = list(model_NN.parameters())\n",
    "    #optimizer = optim.Adam(PINN.parameters(), lr=0.0008,betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "    optimizer = optim.Adam(model_NN.parameters(), lr=0.08)\n",
    "\n",
    "    train_model(max_iter,reps)\n",
    " \n",
    "    torch.save(model_NN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    alpha_full.append(alpha_val)\n",
    "                 \n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse\": test_mse_full, \"test_re\": test_re_full,\"Time\": elapsed_time, \"Thresh_time\": time_threshold, \"epoch_thresh\": epoch_threshold,\"alpha\": alpha_full, \"label\": label}\n",
    "savemat(label+'.mat', mdic) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred = model_NN.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7ff9002c4610>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_test,u_pred,'rx')\n",
    "plt.plot(x_test,u_true,'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41459604716762416\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + test_re_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
