{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "#Trying to replicate the results of \"Adaptive activation functions accelerate convergence in deep and physics-informed neural networks\"\n",
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_disc(x): #[-4,3.75]\n",
    "    y  = np.zeros((np.shape(x)[0],))\n",
    "    \n",
    "    y = 0.2*np.sin(6*x)*np.exp(-1*x)\n",
    "    y[x>0] = 1 + 0.1*x[x>0]*np.exp(x[x>0]) + 10\n",
    "    \n",
    "    return scale*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain bounds\n",
    "level = \"high\"\n",
    "label = \"Regr_disc_rowdy_\" + level\n",
    "loss_thresh = 0.1\n",
    "scale = 50.0\n",
    "\n",
    "lb = np.array(-4.0)  # [-1. 0.]\n",
    "ub = np.array(3.75) # [1.  0.99]\n",
    "\n",
    "x_test = np.linspace(lb,ub,1000).reshape(-1,1)\n",
    "u_true = true_disc(x_test)\n",
    "\n",
    "u_true_norm = np.linalg.norm(u_true,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_train,seed):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    x_train = np.random.uniform(low=lb, high=ub, size=(N_train,)).reshape(-1,1)\n",
    "\n",
    "    u_train = true_disc(x_train)\n",
    "\n",
    "    return x_train, u_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,n_val,rowdy_terms):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "     \n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "\n",
    "        self.omega1 = Parameter(torch.ones((len(layers)-2,1))) \n",
    "        #self.alpha = Parameter(torch.ones((layers[1],len(layers)-2)))\n",
    "        \n",
    "        self.alpha = Parameter(torch.zeros(rowdy_terms,len(layers)-2))\n",
    "        self.omega = Parameter((1/n_val)*torch.ones(rowdy_terms,len(layers)-2))\n",
    "        \n",
    "        self.n = torch.tensor(n_val)\n",
    "            \n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = 2.0*(x - l_b)/(u_b - l_b)-1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(self.omega1[i,0]*z)\n",
    "            for j in range(rowdy_terms):\n",
    "                a = a + self.alpha[j,i]*self.n*torch.sin((j+1)*self.n*self.omega[j,i]*z)\n",
    "    \n",
    "        a = self.linears[-1](a) \n",
    "        \n",
    "        return a\n",
    "    \n",
    "    \n",
    "    def loss(self,x,u):\n",
    "\n",
    "        loss_val = self.loss_function(self.forward(x),u)\n",
    "        \n",
    "        return loss_val\n",
    "        \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        u_pred = self.forward(x_test_tensor)       \n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "        \n",
    "        return u_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "        \n",
    "        test_mse_loss = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re_loss = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "        \n",
    "        return test_mse_loss, test_re_loss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    alpha_val.append(model_NN.alpha.cpu().detach().numpy())\n",
    "    omega_val.append(model_NN.omega.cpu().detach().numpy())    \n",
    "    test_mse, test_re = model_NN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step():\n",
    "    optimizer.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "    loss = model_NN.loss(x_train,u_train)\n",
    "    loss.backward() #backprop\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "   \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "    \n",
    "    loss_np = model_NN.loss(x_train,u_train).cpu().detach().numpy()\n",
    "    data_update(loss_np)\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        loss_np = train_step().cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1\n",
    "        data_update(loss_np)\n",
    "        if(i%100==0):        \n",
    "            print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 290473.06 Test MSE 272486.1058524544 Test RE 0.9983899094356719\n",
      "100 Train Loss 190906.17 Test MSE 177905.14236965243 Test RE 0.8067189386650893\n",
      "200 Train Loss 57124.95 Test MSE 52423.48575953296 Test RE 0.43791613721225014\n",
      "300 Train Loss 23674.09 Test MSE 21922.265994378064 Test RE 0.28318531154821996\n",
      "400 Train Loss 10433.592 Test MSE 10256.341532668017 Test RE 0.1936975996958156\n",
      "500 Train Loss 4740.2344 Test MSE 5572.991773245933 Test RE 0.14278158635237842\n",
      "600 Train Loss 2528.7966 Test MSE 4138.147762883655 Test RE 0.1230356587634681\n",
      "700 Train Loss 1413.9517 Test MSE 2742.3996697210946 Test RE 0.10015978319567587\n",
      "800 Train Loss 893.384 Test MSE 2058.067251977093 Test RE 0.08676763954694833\n",
      "900 Train Loss 838.5646 Test MSE 1834.990840427168 Test RE 0.08193037680290956\n",
      "1000 Train Loss 362.9988 Test MSE 1248.2683272424274 Test RE 0.06757436563817167\n",
      "1100 Train Loss 421.8458 Test MSE 1065.1353230397367 Test RE 0.06242095309336482\n",
      "1200 Train Loss 145.16846 Test MSE 939.099151659943 Test RE 0.05861162009214221\n",
      "1300 Train Loss 106.57928 Test MSE 938.6158760131735 Test RE 0.05859653690646617\n",
      "1400 Train Loss 87.975586 Test MSE 997.9923818126307 Test RE 0.06042151560881587\n",
      "1500 Train Loss 69.78659 Test MSE 1006.7626395716856 Test RE 0.06068642401852926\n",
      "1600 Train Loss 4985.4976 Test MSE 3335.9105416676402 Test RE 0.11046767147567046\n",
      "1700 Train Loss 87.130936 Test MSE 359.9815741049108 Test RE 0.03628842653996222\n",
      "1800 Train Loss 42.965755 Test MSE 281.01522063941707 Test RE 0.032062170808809\n",
      "1900 Train Loss 38.10273 Test MSE 295.83334598911523 Test RE 0.032896641698103246\n",
      "2000 Train Loss 49.61785 Test MSE 294.8823986294517 Test RE 0.0328437265095014\n",
      "2100 Train Loss 49.958965 Test MSE 378.9459828046344 Test RE 0.037232024829903364\n",
      "2200 Train Loss 42.61105 Test MSE 289.4273928492823 Test RE 0.03253852170302852\n",
      "2300 Train Loss 124.232895 Test MSE 305.51726987059175 Test RE 0.033430731850456634\n",
      "2400 Train Loss 59.20996 Test MSE 299.79892719150496 Test RE 0.03311639385531659\n",
      "2500 Train Loss 27.422592 Test MSE 275.1176001759883 Test RE 0.03172394506022201\n",
      "2600 Train Loss 23.534534 Test MSE 264.73626148654375 Test RE 0.031119650986215948\n",
      "2700 Train Loss 31.354887 Test MSE 279.4534752767102 Test RE 0.03197295373596912\n",
      "2800 Train Loss 56.615086 Test MSE 263.5949683199594 Test RE 0.031052499236540576\n",
      "2900 Train Loss 61.0909 Test MSE 328.12306477403797 Test RE 0.034645463950033276\n",
      "Training time: 28.95\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 292808.28 Test MSE 272792.0562813317 Test RE 0.9989502538342733\n",
      "100 Train Loss 193851.23 Test MSE 181973.48412195686 Test RE 0.815890839333316\n",
      "200 Train Loss 56429.84 Test MSE 56839.14313466531 Test RE 0.45598626676301796\n",
      "300 Train Loss 21725.78 Test MSE 22261.035094724404 Test RE 0.28536498234766294\n",
      "400 Train Loss 10617.17 Test MSE 11632.65835658062 Test RE 0.2062849235620898\n",
      "500 Train Loss 6090.59 Test MSE 6956.574686424885 Test RE 0.15952388871223916\n",
      "600 Train Loss 3661.5671 Test MSE 4793.9128323417535 Test RE 0.13242594120612566\n",
      "700 Train Loss 2158.9475 Test MSE 3024.7548882394785 Test RE 0.10518967068051413\n",
      "800 Train Loss 1343.7919 Test MSE 2199.9057238131754 Test RE 0.08970776499485227\n",
      "900 Train Loss 831.36255 Test MSE 1695.582918345224 Test RE 0.07875670141713512\n",
      "1000 Train Loss 630.0027 Test MSE 1518.7430960232882 Test RE 0.07453669984131264\n",
      "1100 Train Loss 315.16074 Test MSE 1141.3656865219411 Test RE 0.06461605028152372\n",
      "1200 Train Loss 426.96298 Test MSE 1552.4279575631776 Test RE 0.0753587575355467\n",
      "1300 Train Loss 99.30856 Test MSE 337.4966387409462 Test RE 0.03513684216331438\n",
      "1400 Train Loss 27.770378 Test MSE 270.968656779905 Test RE 0.03148382799647614\n",
      "1500 Train Loss 11.704545 Test MSE 271.9632642473221 Test RE 0.03154155675442391\n",
      "1600 Train Loss 6.2593403 Test MSE 277.0914057061745 Test RE 0.031837541917470645\n",
      "1700 Train Loss 4.5782394 Test MSE 213.39383499722103 Test RE 0.027939519700720106\n",
      "1800 Train Loss 2.9190335 Test MSE 229.7934613790121 Test RE 0.028993245658319493\n",
      "1900 Train Loss 6.2264743 Test MSE 189.76551310987242 Test RE 0.026347332145508162\n",
      "2000 Train Loss 1.8770736 Test MSE 203.25605367815567 Test RE 0.027267777806891946\n",
      "2100 Train Loss 253.78725 Test MSE 938.2591545390076 Test RE 0.05858540102545605\n",
      "2200 Train Loss 3.886766 Test MSE 151.9010962337283 Test RE 0.023572652599925754\n",
      "2300 Train Loss 2.2312956 Test MSE 151.41888555217494 Test RE 0.02353520711519542\n",
      "2400 Train Loss 1.5574303 Test MSE 151.9482697371468 Test RE 0.02357631260737333\n",
      "2500 Train Loss 1.1452713 Test MSE 152.63569185426877 Test RE 0.023629582677193804\n",
      "2600 Train Loss 0.8754616 Test MSE 152.9616591770837 Test RE 0.02365480077580823\n",
      "2700 Train Loss 0.694928 Test MSE 152.840576765762 Test RE 0.023645436509727186\n",
      "2800 Train Loss 1.4729599 Test MSE 127.24334001997008 Test RE 0.021574738080348738\n",
      "2900 Train Loss 0.5826437 Test MSE 130.54746121501242 Test RE 0.02185305793139298\n",
      "Training time: 28.79\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 279550.8 Test MSE 272684.9213664642 Test RE 0.9987540733410015\n",
      "100 Train Loss 189236.52 Test MSE 182859.48973562813 Test RE 0.8178746615797131\n",
      "200 Train Loss 60420.668 Test MSE 58654.16196566397 Test RE 0.46320945849526357\n",
      "300 Train Loss 24320.912 Test MSE 26638.886283793167 Test RE 0.3121663121632689\n",
      "400 Train Loss 11088.971 Test MSE 13189.05391674355 Test RE 0.21965182887416776\n",
      "500 Train Loss 7149.9043 Test MSE 8824.938528836032 Test RE 0.17967342489204513\n",
      "600 Train Loss 3014.9292 Test MSE 5279.618770653235 Test RE 0.13897263240430693\n",
      "700 Train Loss 28747.146 Test MSE 28292.674757433437 Test RE 0.32171032942398964\n",
      "800 Train Loss 1442.5518 Test MSE 1786.634117887902 Test RE 0.08084363141960642\n",
      "900 Train Loss 906.6469 Test MSE 1392.2081025029922 Test RE 0.0713641471951825\n",
      "1000 Train Loss 872.636 Test MSE 1150.9142291737535 Test RE 0.06488577280468591\n",
      "1100 Train Loss 596.17554 Test MSE 884.4952332022842 Test RE 0.056882116854422474\n",
      "1200 Train Loss 423.15765 Test MSE 562.5978625321386 Test RE 0.045365639857599734\n",
      "1300 Train Loss 682.38544 Test MSE 701.9061277121233 Test RE 0.05067193848052602\n",
      "1400 Train Loss 242.09566 Test MSE 415.5919406289429 Test RE 0.038990747415899446\n",
      "1500 Train Loss 406.62256 Test MSE 720.9888122476509 Test RE 0.05135612706217329\n",
      "1600 Train Loss 113.03045 Test MSE 358.2616683814677 Test RE 0.03620163404304639\n",
      "1700 Train Loss 123.67985 Test MSE 236.83784560661954 Test RE 0.029434289209599476\n",
      "1800 Train Loss 60.895443 Test MSE 215.76505944231027 Test RE 0.02809432230616053\n",
      "1900 Train Loss 88.00195 Test MSE 255.96887448274734 Test RE 0.030600010904144437\n",
      "2000 Train Loss 27.473988 Test MSE 276.4623616034245 Test RE 0.031801383096256665\n",
      "2100 Train Loss 554.8343 Test MSE 518.908029761759 Test RE 0.043568559123609595\n",
      "2200 Train Loss 200.53696 Test MSE 279.6081215609321 Test RE 0.03198179924168036\n",
      "2300 Train Loss 77.91506 Test MSE 190.35104058145959 Test RE 0.026387948600666067\n",
      "2400 Train Loss 36.952625 Test MSE 218.61662497662587 Test RE 0.028279361167768732\n",
      "2500 Train Loss 3756.8367 Test MSE 2915.998644008598 Test RE 0.10328129165894269\n",
      "2600 Train Loss 38.07726 Test MSE 205.12660152930667 Test RE 0.027392961950086323\n",
      "2700 Train Loss 22.146217 Test MSE 227.7121362850935 Test RE 0.028861645672678193\n",
      "2800 Train Loss 7.2100067 Test MSE 206.93851290829164 Test RE 0.02751367885217108\n",
      "2900 Train Loss 12.410059 Test MSE 216.28293518440148 Test RE 0.028128017864244684\n",
      "Training time: 28.86\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 305647.25 Test MSE 272728.33190057724 Test RE 0.9988335693434592\n",
      "100 Train Loss 203088.56 Test MSE 180656.6557670221 Test RE 0.8129334333946849\n",
      "200 Train Loss 62457.035 Test MSE 59468.5669579481 Test RE 0.46641417248301686\n",
      "300 Train Loss 21422.63 Test MSE 23968.500972523147 Test RE 0.2961068392749963\n",
      "400 Train Loss 8415.408 Test MSE 12100.065488426046 Test RE 0.21038843459616866\n",
      "500 Train Loss 4409.565 Test MSE 7607.749753306531 Test RE 0.16682307077195355\n",
      "600 Train Loss 3129.8147 Test MSE 5659.0707055246985 Test RE 0.14388004397902554\n",
      "700 Train Loss 1615.6444 Test MSE 3964.3164699098315 Test RE 0.12042375365598547\n",
      "800 Train Loss 1062.3142 Test MSE 3320.991437372092 Test RE 0.11022037385061245\n",
      "900 Train Loss 715.562 Test MSE 2723.031921300995 Test RE 0.09980547549670682\n",
      "1000 Train Loss 489.53146 Test MSE 2305.627971584512 Test RE 0.0918380424337525\n",
      "1100 Train Loss 367.7234 Test MSE 1902.872989198919 Test RE 0.08343204766444513\n",
      "1200 Train Loss 536.9052 Test MSE 1995.419669971451 Test RE 0.08543683003125728\n",
      "1300 Train Loss 188.69904 Test MSE 1604.0355121088908 Test RE 0.0766010973785764\n",
      "1400 Train Loss 4012.3093 Test MSE 8246.225643975507 Test RE 0.17368231893652938\n",
      "1500 Train Loss 253.8038 Test MSE 826.9431219139449 Test RE 0.055000396643815806\n",
      "1600 Train Loss 70.16617 Test MSE 684.3996475007971 Test RE 0.05003603532128223\n",
      "1700 Train Loss 135.82274 Test MSE 737.6215476548509 Test RE 0.05194512543534203\n",
      "1800 Train Loss 27.087831 Test MSE 753.4899620444256 Test RE 0.05250089866541716\n",
      "1900 Train Loss 17.49155 Test MSE 786.2504064437614 Test RE 0.053630079815827125\n",
      "2000 Train Loss 53.148003 Test MSE 827.9252901660028 Test RE 0.055033049199722996\n",
      "2100 Train Loss 55.280678 Test MSE 844.8480247302807 Test RE 0.05559263998816643\n",
      "2200 Train Loss 11.281762 Test MSE 844.8246229141105 Test RE 0.055591870040369584\n",
      "2300 Train Loss 9.475362 Test MSE 833.084612366356 Test RE 0.055204255628137654\n",
      "2400 Train Loss 5.4809732 Test MSE 891.4577597816015 Test RE 0.05710555893738372\n",
      "2500 Train Loss 5.556625 Test MSE 888.4827522084494 Test RE 0.05701019185695421\n",
      "2600 Train Loss 5.410046 Test MSE 917.0857194923963 Test RE 0.057920588686929855\n",
      "2700 Train Loss 11.307718 Test MSE 869.5857659000117 Test RE 0.056400663484327124\n",
      "2800 Train Loss 5.926526 Test MSE 914.2878736651585 Test RE 0.05783216910925064\n",
      "2900 Train Loss 26.439259 Test MSE 1013.916255790946 Test RE 0.0609016480037438\n",
      "Training time: 29.17\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 258160.55 Test MSE 272092.12923247786 Test RE 0.9976678825950689\n",
      "100 Train Loss 164712.52 Test MSE 175832.59507575407 Test RE 0.8020061428907843\n",
      "200 Train Loss 46636.45 Test MSE 54178.60581749337 Test RE 0.4451864264273844\n",
      "300 Train Loss 18822.33 Test MSE 23349.86358027764 Test RE 0.2922605354457162\n",
      "400 Train Loss 9633.858 Test MSE 13504.729242296022 Test RE 0.22226492971254994\n",
      "500 Train Loss 5710.335 Test MSE 8657.250294573229 Test RE 0.17795819361582224\n",
      "600 Train Loss 3673.7522 Test MSE 6444.962653932084 Test RE 0.153545892943499\n",
      "700 Train Loss 2560.0256 Test MSE 5159.652043527022 Test RE 0.13738464920850096\n",
      "800 Train Loss 2039.8672 Test MSE 4410.955563119817 Test RE 0.12702650325823486\n",
      "900 Train Loss 4839.9106 Test MSE 5543.605992470234 Test RE 0.14240465290908497\n",
      "1000 Train Loss 753.7099 Test MSE 657.5749161376996 Test RE 0.04904566426978956\n",
      "1100 Train Loss 615.74036 Test MSE 457.29922362269014 Test RE 0.04090046389847087\n",
      "1200 Train Loss 588.2549 Test MSE 339.91426915244733 Test RE 0.03526246757927\n",
      "1300 Train Loss 539.7743 Test MSE 341.94738340220295 Test RE 0.03536776728016295\n",
      "1400 Train Loss 522.0661 Test MSE 301.23829845743916 Test RE 0.03319579658969498\n",
      "1500 Train Loss 552.75256 Test MSE 264.73935874923797 Test RE 0.03111983302673356\n",
      "1600 Train Loss 300.8897 Test MSE 145.0026004601093 Test RE 0.023031164499614756\n",
      "1700 Train Loss 435.96 Test MSE 232.77711708113972 Test RE 0.029180863858698305\n",
      "1800 Train Loss 466.1 Test MSE 296.87299275442314 Test RE 0.03295439531554272\n",
      "1900 Train Loss 358.126 Test MSE 218.76039968287836 Test RE 0.028288658696217182\n",
      "2000 Train Loss 351.76096 Test MSE 225.02490420169542 Test RE 0.02869084204956114\n",
      "2100 Train Loss 383.36078 Test MSE 194.70183718964947 Test RE 0.026687815500530832\n",
      "2200 Train Loss 313.283 Test MSE 271.49521985704416 Test RE 0.03151440381178667\n",
      "2300 Train Loss 375.28107 Test MSE 178.7590323061587 Test RE 0.025571841229602845\n",
      "2400 Train Loss 364.12436 Test MSE 185.0923440732641 Test RE 0.02602089496151366\n",
      "2500 Train Loss 352.71213 Test MSE 283.5452237432894 Test RE 0.032206176589216984\n",
      "2600 Train Loss 513.91705 Test MSE 424.6189526526413 Test RE 0.0394119287709539\n",
      "2700 Train Loss 403.06213 Test MSE 105.76201513703359 Test RE 0.019669479170447014\n",
      "2800 Train Loss 1444.0326 Test MSE 1466.0071623176007 Test RE 0.07323118265492477\n",
      "2900 Train Loss 431.1218 Test MSE 335.1267641361157 Test RE 0.03501326077830665\n",
      "Training time: 28.81\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 263929.72 Test MSE 272841.899888008 Test RE 0.9990415120072673\n",
      "100 Train Loss 175811.8 Test MSE 179838.52940551474 Test RE 0.8110906090709514\n",
      "200 Train Loss 54811.84 Test MSE 55633.34636200181 Test RE 0.4511236481639905\n",
      "300 Train Loss 19805.963 Test MSE 22519.12624579481 Test RE 0.2870144549065287\n",
      "400 Train Loss 9107.18 Test MSE 11174.376868746245 Test RE 0.20218068284985785\n",
      "500 Train Loss 6189.6035 Test MSE 7411.454588838901 Test RE 0.16465682156930006\n",
      "600 Train Loss 3276.497 Test MSE 3392.8704183284144 Test RE 0.11140678423005713\n",
      "700 Train Loss 1599.3739 Test MSE 1673.6856265433253 Test RE 0.0782465044306461\n",
      "800 Train Loss 852.9351 Test MSE 936.2162283165549 Test RE 0.05852158556888158\n",
      "900 Train Loss 499.8624 Test MSE 697.0370820723532 Test RE 0.05049587979107565\n",
      "1000 Train Loss 382.3123 Test MSE 571.501808222468 Test RE 0.045723219874431925\n",
      "1100 Train Loss 193.85042 Test MSE 436.4142699343736 Test RE 0.03995558311849931\n",
      "1200 Train Loss 120.195755 Test MSE 441.7013263494632 Test RE 0.040196880756729815\n",
      "1300 Train Loss 194.46848 Test MSE 437.4550418972897 Test RE 0.0400031982974916\n",
      "1400 Train Loss 168.62578 Test MSE 539.7095888190116 Test RE 0.0444332488422372\n",
      "1500 Train Loss 48.180218 Test MSE 279.93400705802935 Test RE 0.0320004313354216\n",
      "1600 Train Loss 28.864704 Test MSE 285.4710858190475 Test RE 0.03231536497603208\n",
      "1700 Train Loss 66.64558 Test MSE 283.37199148105964 Test RE 0.03219633688681991\n",
      "1800 Train Loss 290.17627 Test MSE 196.81593427065536 Test RE 0.026832314145141034\n",
      "1900 Train Loss 119.31467 Test MSE 304.11237934900447 Test RE 0.033353779350307936\n",
      "2000 Train Loss 13.711885 Test MSE 263.69061547524086 Test RE 0.03105813252661367\n",
      "2100 Train Loss 24.048264 Test MSE 233.5970438821061 Test RE 0.02923221156090961\n",
      "2200 Train Loss 25.5756 Test MSE 232.79165294940034 Test RE 0.029181774950339197\n",
      "2300 Train Loss 51.718197 Test MSE 330.1690462103969 Test RE 0.03475331040950912\n",
      "2400 Train Loss 57.542828 Test MSE 265.2501982029279 Test RE 0.031149842877108008\n",
      "2500 Train Loss 42.038982 Test MSE 216.68059370008618 Test RE 0.028153864122506204\n",
      "2600 Train Loss 11552.417 Test MSE 12357.56386076302 Test RE 0.21261526083704174\n",
      "2700 Train Loss 414.98788 Test MSE 849.4661187030229 Test RE 0.05574437269492561\n",
      "2800 Train Loss 25.407957 Test MSE 482.4493459530318 Test RE 0.04201011426724024\n",
      "2900 Train Loss 19.722233 Test MSE 383.1249533758185 Test RE 0.03743675707543162\n",
      "Training time: 28.97\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 310754.5 Test MSE 272840.27725640056 Test RE 0.9990385412781647\n",
      "100 Train Loss 208649.42 Test MSE 180181.10950607294 Test RE 0.811862777586454\n",
      "200 Train Loss 67188.37 Test MSE 58190.06361259711 Test RE 0.4613732573224272\n",
      "300 Train Loss 25232.572 Test MSE 23378.23597550159 Test RE 0.29243804427742004\n",
      "400 Train Loss 15494.417 Test MSE 15499.988553465897 Test RE 0.23811880290477178\n",
      "500 Train Loss 5610.0044 Test MSE 7641.176285021683 Test RE 0.16718915831651163\n",
      "600 Train Loss 3031.458 Test MSE 6017.592834709408 Test RE 0.14836771264463566\n",
      "700 Train Loss 2368.6357 Test MSE 3300.1748194224137 Test RE 0.10987438949186981\n",
      "800 Train Loss 1254.6372 Test MSE 2573.6924935827915 Test RE 0.09703006666702163\n",
      "900 Train Loss 766.0389 Test MSE 1830.1001679522606 Test RE 0.08182112231180468\n",
      "1000 Train Loss 464.34174 Test MSE 1626.5464197766457 Test RE 0.07713673153898422\n",
      "1100 Train Loss 1296.7108 Test MSE 3043.6043229175684 Test RE 0.10551691808984065\n",
      "1200 Train Loss 287.5905 Test MSE 1829.8366761314157 Test RE 0.08181523193133795\n",
      "1300 Train Loss 407.8075 Test MSE 1680.408048119452 Test RE 0.0784034869782185\n",
      "1400 Train Loss 143.2545 Test MSE 1897.9555106356243 Test RE 0.08332417375125742\n",
      "1500 Train Loss 185.57668 Test MSE 2127.0016149018966 Test RE 0.08820879953480258\n",
      "1600 Train Loss 61.762184 Test MSE 1609.5500092680593 Test RE 0.07673265746501753\n",
      "1700 Train Loss 44.37658 Test MSE 1484.530420400884 Test RE 0.07369237484678183\n",
      "1800 Train Loss 38.11492 Test MSE 1596.8062377780777 Test RE 0.07642828458741509\n",
      "1900 Train Loss 69.095695 Test MSE 1545.9875213544854 Test RE 0.07520227758696697\n",
      "2000 Train Loss 24.711527 Test MSE 1328.1166330422457 Test RE 0.0697021396242938\n",
      "2100 Train Loss 11.612644 Test MSE 1453.518957540249 Test RE 0.07291860507365562\n",
      "2200 Train Loss 39.684536 Test MSE 1171.86124155277 Test RE 0.06547358134662767\n",
      "2300 Train Loss 48.612663 Test MSE 1309.687641793659 Test RE 0.06921685566778156\n",
      "2400 Train Loss 12605.591 Test MSE 9630.337626846178 Test RE 0.18769329495615986\n",
      "2500 Train Loss 22.79055 Test MSE 523.0823761347368 Test RE 0.04374345134647686\n",
      "2600 Train Loss 6.526003 Test MSE 483.09217021200794 Test RE 0.04203809246978068\n",
      "2700 Train Loss 4.227972 Test MSE 478.6598510954346 Test RE 0.0418448005835314\n",
      "2800 Train Loss 2.9474125 Test MSE 477.7126994933599 Test RE 0.04180337973379235\n",
      "2900 Train Loss 2.0667098 Test MSE 475.3977443989073 Test RE 0.04170196891691823\n",
      "Training time: 28.90\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 306037.84 Test MSE 272613.99580696766 Test RE 0.9986241765394157\n",
      "100 Train Loss 204482.83 Test MSE 179912.78181609424 Test RE 0.811258034866408\n",
      "200 Train Loss 62187.414 Test MSE 54716.252598379346 Test RE 0.44738989920726413\n",
      "300 Train Loss 23556.635 Test MSE 23115.815910322508 Test RE 0.2907921077839015\n",
      "400 Train Loss 16375.919 Test MSE 18648.003858732824 Test RE 0.2611825616171183\n",
      "500 Train Loss 6127.4414 Test MSE 9569.291841251747 Test RE 0.18709746433876998\n",
      "600 Train Loss 3794.665 Test MSE 6624.883860009391 Test RE 0.1556743770046061\n",
      "700 Train Loss 2252.2793 Test MSE 5164.19416422114 Test RE 0.13744510681109715\n",
      "800 Train Loss 1737.9683 Test MSE 4440.56564839699 Test RE 0.1274521450987341\n",
      "900 Train Loss 899.77374 Test MSE 3454.990983886677 Test RE 0.1124220398252306\n",
      "1000 Train Loss 582.51526 Test MSE 3003.4294418477566 Test RE 0.10481820512272473\n",
      "1100 Train Loss 501.7805 Test MSE 2276.2839053027656 Test RE 0.09125175287555017\n",
      "1200 Train Loss 255.26332 Test MSE 2148.037919431536 Test RE 0.08864392419789928\n",
      "1300 Train Loss 337.3907 Test MSE 2288.6243123975855 Test RE 0.09149876988666902\n",
      "1400 Train Loss 137.27504 Test MSE 1929.189255330347 Test RE 0.08400698903057632\n",
      "1500 Train Loss 326.35052 Test MSE 2043.1031290035041 Test RE 0.08645162207889442\n",
      "1600 Train Loss 94.85742 Test MSE 1784.5593042452047 Test RE 0.08079667603190148\n",
      "1700 Train Loss 36.614758 Test MSE 1785.376416060637 Test RE 0.0808151714562133\n",
      "1800 Train Loss 57.842983 Test MSE 1787.5119704429708 Test RE 0.08086349001077098\n",
      "1900 Train Loss 135.50505 Test MSE 1901.0314714390495 Test RE 0.08339166693795919\n",
      "2000 Train Loss 24.296307 Test MSE 1877.9942239212166 Test RE 0.08288484467056324\n",
      "2100 Train Loss 190.94562 Test MSE 2177.01010169138 Test RE 0.08923972513470667\n",
      "2200 Train Loss 147.04463 Test MSE 2494.696752999235 Test RE 0.09552936319535785\n",
      "2300 Train Loss 1187.4559 Test MSE 1860.7292102084155 Test RE 0.08250297126064655\n",
      "2400 Train Loss 237.6834 Test MSE 1037.6026858008154 Test RE 0.061608912870027815\n",
      "2500 Train Loss 115.17587 Test MSE 961.2979834260285 Test RE 0.059300317341090036\n",
      "2600 Train Loss 108.68448 Test MSE 1148.4856438567822 Test RE 0.06481727775610852\n",
      "2700 Train Loss 166.18857 Test MSE 1012.0155449310904 Test RE 0.060844537407129785\n",
      "2800 Train Loss 274.1332 Test MSE 997.7148985264171 Test RE 0.060413115180824165\n",
      "2900 Train Loss 48.19638 Test MSE 1154.9023856841773 Test RE 0.06499809691248283\n",
      "Training time: 28.98\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 274257.62 Test MSE 272865.1661397709 Test RE 0.9990841071069309\n",
      "100 Train Loss 180296.56 Test MSE 182486.70958832436 Test RE 0.8170405703796999\n",
      "200 Train Loss 49467.223 Test MSE 57427.34418440683 Test RE 0.45833958575283024\n",
      "300 Train Loss 17839.402 Test MSE 26076.74847232539 Test RE 0.3088550599948003\n",
      "400 Train Loss 7226.1133 Test MSE 13798.54338996396 Test RE 0.22466976115721982\n",
      "500 Train Loss 3645.3801 Test MSE 9326.133729850317 Test RE 0.1847050719214699\n",
      "600 Train Loss 4007.8293 Test MSE 8158.879572819745 Test RE 0.17276002700783494\n",
      "700 Train Loss 1393.8773 Test MSE 5029.855670599278 Test RE 0.1356456164274228\n",
      "800 Train Loss 1257.3832 Test MSE 4612.1683445249055 Test RE 0.12989145366356394\n",
      "900 Train Loss 628.68054 Test MSE 3732.3698513816385 Test RE 0.11684774571562649\n",
      "1000 Train Loss 431.79953 Test MSE 3294.2905715887405 Test RE 0.1097763921701165\n",
      "1100 Train Loss 1248.8416 Test MSE 3333.955100912041 Test RE 0.11043528981418217\n",
      "1200 Train Loss 112.1787 Test MSE 1171.370795773106 Test RE 0.0654598789555067\n",
      "1300 Train Loss 65.608635 Test MSE 1078.99131917352 Test RE 0.0628256480647264\n",
      "1400 Train Loss 45.08751 Test MSE 1087.814734971218 Test RE 0.06308200237830916\n",
      "1500 Train Loss 127.30286 Test MSE 1096.3374553227106 Test RE 0.06332863505760811\n",
      "1600 Train Loss 32.336452 Test MSE 1121.2307242848733 Test RE 0.06404356467014656\n",
      "1700 Train Loss 16.065367 Test MSE 1125.679686991868 Test RE 0.0641704990027349\n",
      "1800 Train Loss 17.343685 Test MSE 1101.8490837324855 Test RE 0.06348762183385617\n",
      "1900 Train Loss 10.375734 Test MSE 1073.4785036087303 Test RE 0.06266494717270399\n",
      "2000 Train Loss 47.152096 Test MSE 1106.7574782154168 Test RE 0.06362887349123778\n",
      "2100 Train Loss 24.56932 Test MSE 1149.7128853564834 Test RE 0.06485189952722968\n",
      "2200 Train Loss 1661.7968 Test MSE 2218.8130533908247 Test RE 0.09009244180960269\n",
      "2300 Train Loss 5.5149965 Test MSE 1147.9826125980799 Test RE 0.06480308137150699\n",
      "2400 Train Loss 4.887491 Test MSE 1157.2272683826652 Test RE 0.06506348641187959\n",
      "2500 Train Loss 8.237671 Test MSE 1031.8147678396365 Test RE 0.06143684026183674\n",
      "2600 Train Loss 3.2143648 Test MSE 1184.6805278720703 Test RE 0.06583072331436937\n",
      "2700 Train Loss 2.8586729 Test MSE 1154.7903388230668 Test RE 0.0649949438280844\n",
      "2800 Train Loss 3.4654205 Test MSE 1102.226483057854 Test RE 0.06349849361960085\n",
      "2900 Train Loss 499.79562 Test MSE 2071.273400078618 Test RE 0.08704557846769825\n",
      "Training time: 28.91\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 278456.53 Test MSE 272994.3588002718 Test RE 0.9993205958115766\n",
      "100 Train Loss 188168.38 Test MSE 184659.38630195416 Test RE 0.8218899988477363\n",
      "200 Train Loss 60224.48 Test MSE 63231.06084662099 Test RE 0.4809425878645193\n",
      "300 Train Loss 25391.87 Test MSE 29587.219155573508 Test RE 0.32898801438979614\n",
      "400 Train Loss 11940.064 Test MSE 16129.80375437925 Test RE 0.24290840571212788\n",
      "500 Train Loss 5710.671 Test MSE 9071.741719644517 Test RE 0.182168524336965\n",
      "600 Train Loss 2819.783 Test MSE 5859.384558990086 Test RE 0.1464043574784302\n",
      "700 Train Loss 1386.0044 Test MSE 4149.811892263467 Test RE 0.12320893604488968\n",
      "800 Train Loss 819.0493 Test MSE 3658.5814071655095 Test RE 0.11568694785341906\n",
      "900 Train Loss 456.99557 Test MSE 3022.9839573113404 Test RE 0.10515887299238823\n",
      "1000 Train Loss 279.8046 Test MSE 2663.651181817295 Test RE 0.09871125589735218\n",
      "1100 Train Loss 1431.6453 Test MSE 3137.826759784624 Test RE 0.10713774050691885\n",
      "1200 Train Loss 228.79259 Test MSE 1596.2708874725047 Test RE 0.07641547171929305\n",
      "1300 Train Loss 169.60956 Test MSE 1498.4423109466234 Test RE 0.07403686411361803\n",
      "1400 Train Loss 70.832985 Test MSE 1581.2513677849133 Test RE 0.07605512052902598\n",
      "1500 Train Loss 39.737072 Test MSE 1535.0914619780447 Test RE 0.07493679763625331\n",
      "1600 Train Loss 30.389193 Test MSE 1453.5916927404403 Test RE 0.0729204295022126\n",
      "1700 Train Loss 13.700625 Test MSE 1451.9330791778605 Test RE 0.07287881488130378\n",
      "1800 Train Loss 8.69666 Test MSE 1450.9512860031664 Test RE 0.0728541704868921\n",
      "1900 Train Loss 6.4945397 Test MSE 1404.4452332479082 Test RE 0.07167709674100207\n",
      "2000 Train Loss 4.411014 Test MSE 1438.2168530386896 Test RE 0.07253375960170776\n",
      "2100 Train Loss 117.74091 Test MSE 1576.0132185459395 Test RE 0.07592904362608159\n",
      "2200 Train Loss 2.8972807 Test MSE 1412.643170521658 Test RE 0.07188598682127155\n",
      "2300 Train Loss 26.156015 Test MSE 1444.3086804768723 Test RE 0.07268721220397516\n",
      "2400 Train Loss 2.0592196 Test MSE 1391.5169653090943 Test RE 0.07134643125918853\n",
      "2500 Train Loss 10.26707 Test MSE 1326.2206018653603 Test RE 0.06965236815561607\n",
      "2600 Train Loss 122.86569 Test MSE 1432.8969751789332 Test RE 0.07239948630674951\n",
      "2700 Train Loss 92.389595 Test MSE 1361.153004061674 Test RE 0.07056372110900193\n",
      "2800 Train Loss 101.21681 Test MSE 1582.8443648155778 Test RE 0.07609342091728627\n",
      "2900 Train Loss 8.3319025 Test MSE 1349.31006551723 Test RE 0.07025607475758051\n",
      "Training time: 28.71\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 3000\n",
    "layers = np.array([1,50,50,50,50,1])\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "alpha_full = []\n",
    "omega_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "n_val = 1\n",
    "rowdy_terms = 2\n",
    "\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(reps)\n",
    "    \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []\n",
    "    alpha_val = []\n",
    "    omega_val = []\n",
    "    'Generate Training data'\n",
    "    N_train = 300\n",
    "    \n",
    "    x_train_np_array, u_train_np_array = trainingdata(N_train,reps*1234)\n",
    "\n",
    "    'Convert to tensor and send to GPU'\n",
    "    x_train = torch.from_numpy(x_train_np_array).float().to(device)\n",
    "    u_train = torch.from_numpy(u_train_np_array).float().to(device)\n",
    "    \n",
    "    x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "       \n",
    "     \n",
    "    model_NN = Sequentialmodel(layers,n_val,rowdy_terms)\n",
    "    model_NN.to(device)\n",
    "    print(model_NN)\n",
    "    params = list(model_NN.parameters())\n",
    "    #optimizer = optim.Adam(PINN.parameters(), lr=0.0008,betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "    optimizer = optim.Adam(model_NN.parameters(), lr=8.0e-3)\n",
    "\n",
    "    train_model(max_iter,reps)\n",
    " \n",
    "    torch.save(model_NN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    alpha_full.append(alpha_val)\n",
    "    omega_full.append(omega_val)\n",
    "                 \n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse\": test_mse_full, \"test_re\": test_re_full,\"Time\": elapsed_time, \"Thresh_time\": time_threshold, \"epoch_thresh\": epoch_threshold,\"alpha\": alpha_full,\"omega\": omega_full, \"label\": label}\n",
    "savemat(label+'.mat', mdic) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred = model_NN.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f978044d410>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_test,u_pred,'rx')\n",
    "plt.plot(x_test,u_true,'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   0.034645463950033276\n",
      "1   0.02185305793139298\n",
      "2   0.028128017864244684\n",
      "3   0.0609016480037438\n",
      "4   0.03501326077830665\n",
      "5   0.03743675707543162\n",
      "6   0.04170196891691823\n",
      "7   0.06499809691248283\n",
      "8   0.08704557846769825\n",
      "9   0.07025607475758051\n",
      "0.04819799246578328\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    print(i,\" \",test_re_full[i][-1])\n",
    "    a = a + test_re_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
