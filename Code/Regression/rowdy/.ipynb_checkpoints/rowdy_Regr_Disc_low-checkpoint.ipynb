{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:2\n"
     ]
    }
   ],
   "source": [
    "#Trying to replicate the results of \"Adaptive activation functions accelerate convergence in deep and physics-informed neural networks\"\n",
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_disc(x): #[-4,3.75]\n",
    "    y  = np.zeros((np.shape(x)[0],))\n",
    "    \n",
    "    y = 0.2*np.sin(6*x)*np.exp(-1*x)\n",
    "    y[x>0] = 1 + 0.1*x[x>0]*np.exp(x[x>0]) + 10\n",
    "    \n",
    "    return scale*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain bounds\n",
    "level = \"low\"\n",
    "label = \"Regr_disc_rowdy_\" + level\n",
    "loss_thresh = 0.1\n",
    "scale = 1.0\n",
    "\n",
    "lb = np.array(-4.0)  # [-1. 0.]\n",
    "ub = np.array(3.75) # [1.  0.99]\n",
    "\n",
    "x_test = np.linspace(lb,ub,1000).reshape(-1,1)\n",
    "u_true = true_disc(x_test)\n",
    "\n",
    "u_true_norm = np.linalg.norm(u_true,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_train,seed):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    x_train = np.random.uniform(low=lb, high=ub, size=(N_train,)).reshape(-1,1)\n",
    "\n",
    "    u_train = true_disc(x_train)\n",
    "\n",
    "    return x_train, u_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,n_val,rowdy_terms):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "     \n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "\n",
    "        self.omega1 = Parameter(torch.ones((len(layers)-2,1))) \n",
    "        #self.alpha = Parameter(torch.ones((layers[1],len(layers)-2)))\n",
    "        \n",
    "        self.alpha = Parameter(torch.zeros(rowdy_terms,len(layers)-2))\n",
    "        self.omega = Parameter((1/n_val)*torch.ones(rowdy_terms,len(layers)-2))\n",
    "        \n",
    "        self.n = torch.tensor(n_val)\n",
    "            \n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = 2.0*(x - l_b)/(u_b - l_b)-1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(self.omega1[i,0]*z)\n",
    "            for j in range(rowdy_terms):\n",
    "                a = a + self.alpha[j,i]*self.n*torch.sin((j+1)*self.n*self.omega[j,i]*z)\n",
    "    \n",
    "        a = self.linears[-1](a) \n",
    "        \n",
    "        return a\n",
    "    \n",
    "    \n",
    "    def loss(self,x,u):\n",
    "\n",
    "        loss_val = self.loss_function(self.forward(x),u)\n",
    "        \n",
    "        return loss_val\n",
    "        \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        u_pred = self.forward(x_test_tensor)       \n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "        \n",
    "        return u_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "        \n",
    "        test_mse_loss = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re_loss = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "        \n",
    "        return test_mse_loss, test_re_loss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    alpha_val.append(model_NN.alpha.cpu().detach().numpy())\n",
    "    omega_val.append(model_NN.omega.cpu().detach().numpy())    \n",
    "    test_mse, test_re = model_NN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step():\n",
    "    optimizer.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "    loss = model_NN.loss(x_train,u_train)\n",
    "    loss.backward() #backprop\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "   \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "    \n",
    "    loss_np = model_NN.loss(x_train,u_train).cpu().detach().numpy()\n",
    "    data_update(loss_np)\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        loss_np = train_step().cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1\n",
    "        data_update(loss_np)\n",
    "        if(i%100==0):        \n",
    "            print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 114.84176 Test MSE 92.83916122621426 Test RE 0.9214326781095001\n",
      "100 Train Loss 6.7679563 Test MSE 7.09749969573007 Test RE 0.25477141400235054\n",
      "200 Train Loss 1.3835348 Test MSE 2.005520621105228 Test RE 0.13542896894439327\n",
      "300 Train Loss 3.1581783 Test MSE 2.0482491198336463 Test RE 0.13686405232726326\n",
      "400 Train Loss 0.263694 Test MSE 0.5994081845075141 Test RE 0.07403879476163183\n",
      "500 Train Loss 0.5569159 Test MSE 0.6254962790304076 Test RE 0.07563283356147495\n",
      "600 Train Loss 0.14407212 Test MSE 0.3180357893503666 Test RE 0.053930693337169384\n",
      "700 Train Loss 0.2135731 Test MSE 0.4010047264910156 Test RE 0.060558171272564285\n",
      "800 Train Loss 0.16940638 Test MSE 0.38107818238045527 Test RE 0.05903438574105741\n",
      "900 Train Loss 0.2022028 Test MSE 0.25667549136908596 Test RE 0.048449601265808544\n",
      "1000 Train Loss 0.15287359 Test MSE 0.23637360211334074 Test RE 0.04649406226791868\n",
      "1100 Train Loss 0.20696044 Test MSE 0.3360101857238737 Test RE 0.05543374599558597\n",
      "1200 Train Loss 0.09818271 Test MSE 0.2997625337363531 Test RE 0.05235843797685038\n",
      "1300 Train Loss 0.17124332 Test MSE 0.32553957222547397 Test RE 0.05456320850305195\n",
      "1400 Train Loss 0.19573963 Test MSE 0.4949752477262072 Test RE 0.06728058314652915\n",
      "1500 Train Loss 1.8863465 Test MSE 2.5184396805467 Test RE 0.15176225915731134\n",
      "1600 Train Loss 0.22944319 Test MSE 0.466884754163889 Test RE 0.06534356913880737\n",
      "1700 Train Loss 0.06783151 Test MSE 0.15060512513697819 Test RE 0.037112300994123985\n",
      "1800 Train Loss 0.089362144 Test MSE 0.2461246899254129 Test RE 0.04744337730734643\n",
      "1900 Train Loss 0.17546369 Test MSE 0.29494198124040766 Test RE 0.05193573746458166\n",
      "2000 Train Loss 0.0727476 Test MSE 0.16445962788933413 Test RE 0.03878177280489438\n",
      "2100 Train Loss 0.09811984 Test MSE 0.21142722275391435 Test RE 0.043972226738391726\n",
      "2200 Train Loss 0.15232244 Test MSE 0.14084301218881182 Test RE 0.03588935543024663\n",
      "2300 Train Loss 0.086439945 Test MSE 0.25115683116589815 Test RE 0.04792592498424917\n",
      "2400 Train Loss 0.10676895 Test MSE 0.19133676015296147 Test RE 0.04183090053082734\n",
      "2500 Train Loss 0.31164533 Test MSE 0.24894816647919674 Test RE 0.04771473016648259\n",
      "2600 Train Loss 0.035514984 Test MSE 0.14876212650635123 Test RE 0.03688452500920835\n",
      "2700 Train Loss 0.037497833 Test MSE 0.14231687623641892 Test RE 0.03607665036741277\n",
      "2800 Train Loss 0.023545008 Test MSE 0.20208762776788908 Test RE 0.04299004197106296\n",
      "2900 Train Loss 0.082592525 Test MSE 0.1735112086841453 Test RE 0.039834720447842135\n",
      "Training time: 29.06\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 118.05292 Test MSE 98.32769417362223 Test RE 0.9482785645310893\n",
      "100 Train Loss 5.3075027 Test MSE 6.073247028466103 Test RE 0.23567226694747115\n",
      "200 Train Loss 3.609544 Test MSE 4.024202508293401 Test RE 0.19183944966185532\n",
      "300 Train Loss 0.6441168 Test MSE 0.5935717153652665 Test RE 0.07367745318247593\n",
      "400 Train Loss 0.100642934 Test MSE 0.14758751640445877 Test RE 0.03673861825761971\n",
      "500 Train Loss 0.14602275 Test MSE 0.15167821684386612 Test RE 0.03724428260461779\n",
      "600 Train Loss 0.19401002 Test MSE 0.21638804453691948 Test RE 0.04448510677391404\n",
      "700 Train Loss 0.14266996 Test MSE 0.18187868394636428 Test RE 0.04078391450198682\n",
      "800 Train Loss 0.30772293 Test MSE 0.44567347574929544 Test RE 0.06384198768609667\n",
      "900 Train Loss 0.06463681 Test MSE 0.12785131216419804 Test RE 0.03419405461259457\n",
      "1000 Train Loss 0.013522207 Test MSE 0.08030166340041454 Test RE 0.027099437534977712\n",
      "1100 Train Loss 0.07552507 Test MSE 0.16204397411002266 Test RE 0.03849589747337219\n",
      "1200 Train Loss 0.047315795 Test MSE 0.11071706155314563 Test RE 0.03182037413700885\n",
      "1300 Train Loss 0.004776929 Test MSE 0.06816361690672629 Test RE 0.02496745752074342\n",
      "1400 Train Loss 0.087221295 Test MSE 0.16115377705459968 Test RE 0.0383900122374878\n",
      "1500 Train Loss 0.0008143005 Test MSE 0.051480142546803984 Test RE 0.021697908655322992\n",
      "1600 Train Loss 0.09919144 Test MSE 0.12038855155825232 Test RE 0.0331810863566593\n",
      "1700 Train Loss 0.0004461227 Test MSE 0.048443963695190856 Test RE 0.021048339551268974\n",
      "1800 Train Loss 0.03988076 Test MSE 0.07697574574396436 Test RE 0.026532303634118987\n",
      "1900 Train Loss 0.0004004296 Test MSE 0.04503319236160376 Test RE 0.02029384661484518\n",
      "2000 Train Loss 0.0014481132 Test MSE 0.04586653211799219 Test RE 0.020480754829441332\n",
      "2100 Train Loss 0.6443339 Test MSE 0.4913404529149543 Test RE 0.06703309427707849\n",
      "2200 Train Loss 0.030324936 Test MSE 0.08817145203538768 Test RE 0.028396316143527205\n",
      "2300 Train Loss 0.027493 Test MSE 0.10146408843748979 Test RE 0.030461703399599964\n",
      "2400 Train Loss 0.002641175 Test MSE 0.06721586732834117 Test RE 0.024793275693868792\n",
      "2500 Train Loss 0.00057753775 Test MSE 0.058363903966554845 Test RE 0.023103095496117384\n",
      "2600 Train Loss 0.00031844273 Test MSE 0.06234135296009404 Test RE 0.02387734957956828\n",
      "2700 Train Loss 0.0012756449 Test MSE 0.05401444611373686 Test RE 0.022225573168536395\n",
      "2800 Train Loss 0.07905406 Test MSE 0.16913357501864545 Test RE 0.03932900150294675\n",
      "2900 Train Loss 0.0018521957 Test MSE 0.048737483800566694 Test RE 0.021112008792826223\n",
      "Training time: 29.26\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 111.0624 Test MSE 96.39300989699439 Test RE 0.9389031084413776\n",
      "100 Train Loss 8.889349 Test MSE 7.48188616499508 Test RE 0.2615794090389833\n",
      "200 Train Loss 3.6634693 Test MSE 4.134453433127541 Test RE 0.19444960199427064\n",
      "300 Train Loss 1.0499054 Test MSE 0.8648655036191875 Test RE 0.08893491486639804\n",
      "400 Train Loss 0.48860973 Test MSE 0.27169839721935957 Test RE 0.04984728908019275\n",
      "500 Train Loss 0.464026 Test MSE 0.23005588112273756 Test RE 0.04586851459386791\n",
      "600 Train Loss 0.4212756 Test MSE 0.40786679438537393 Test RE 0.061074114782414805\n",
      "700 Train Loss 0.5014306 Test MSE 0.2247448651926407 Test RE 0.04533596822058144\n",
      "800 Train Loss 3.4339845 Test MSE 3.5217677955484676 Test RE 0.1794644222066991\n",
      "900 Train Loss 0.41690615 Test MSE 0.16494307175923525 Test RE 0.03883873223113053\n",
      "1000 Train Loss 0.33574036 Test MSE 0.18054754016925995 Test RE 0.040634394635803296\n",
      "1100 Train Loss 0.22595663 Test MSE 0.1613975730984332 Test RE 0.03841903977942475\n",
      "1200 Train Loss 0.28579813 Test MSE 0.18299166378139056 Test RE 0.04090850976294268\n",
      "1300 Train Loss 0.19339575 Test MSE 0.11239695119549281 Test RE 0.03206086767681117\n",
      "1400 Train Loss 0.4404089 Test MSE 0.7716700561411078 Test RE 0.0840066817100446\n",
      "1500 Train Loss 0.3143298 Test MSE 0.12697149099845528 Test RE 0.03407619665341599\n",
      "1600 Train Loss 0.692722 Test MSE 0.35884266298861245 Test RE 0.057286203528085174\n",
      "1700 Train Loss 0.20034824 Test MSE 0.1563438837113548 Test RE 0.03781276661289846\n",
      "1800 Train Loss 0.23678483 Test MSE 0.14188271951573384 Test RE 0.036021580004900126\n",
      "1900 Train Loss 0.19801448 Test MSE 0.10024252456947934 Test RE 0.030277778253604063\n",
      "2000 Train Loss 0.30972326 Test MSE 0.2154976881564659 Test RE 0.04439349259668506\n",
      "2100 Train Loss 2.0651603 Test MSE 1.2605383841300064 Test RE 0.10736829176910663\n",
      "2200 Train Loss 0.2740036 Test MSE 0.2272971459045853 Test RE 0.045592666984955794\n",
      "2300 Train Loss 0.19482332 Test MSE 0.1671068732628682 Test RE 0.03909265463774102\n",
      "2400 Train Loss 0.19956096 Test MSE 0.08453538354599796 Test RE 0.02780463958099963\n",
      "2500 Train Loss 0.04630592 Test MSE 0.0592307109286325 Test RE 0.023274024055276062\n",
      "2600 Train Loss 0.08048 Test MSE 0.082629104686624 Test RE 0.027489353725042562\n",
      "2700 Train Loss 0.028133526 Test MSE 0.05284136421406009 Test RE 0.021982901646491398\n",
      "2800 Train Loss 0.027994322 Test MSE 0.06411049391858759 Test RE 0.024213778625469643\n",
      "2900 Train Loss 0.1909816 Test MSE 0.1131795828286008 Test RE 0.03217229561630615\n",
      "Training time: 29.85\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 123.44076 Test MSE 97.1249582377106 Test RE 0.942461088976602\n",
      "100 Train Loss 8.371506 Test MSE 10.19112475760337 Test RE 0.30528753007092235\n",
      "200 Train Loss 3.7621229 Test MSE 5.109770866135649 Test RE 0.21617164084568546\n",
      "300 Train Loss 2.4033365 Test MSE 3.5210177694975324 Test RE 0.17944531104335687\n",
      "400 Train Loss 0.6984263 Test MSE 1.172617442929369 Test RE 0.10355621803336526\n",
      "500 Train Loss 0.36857447 Test MSE 0.5633485256853094 Test RE 0.07177721231857526\n",
      "600 Train Loss 0.058494925 Test MSE 0.37048002872043256 Test RE 0.05820769570297211\n",
      "700 Train Loss 0.13377856 Test MSE 0.3556125414587927 Test RE 0.05702778986140936\n",
      "800 Train Loss 0.12093961 Test MSE 0.40112479912904886 Test RE 0.060567237044940485\n",
      "900 Train Loss 0.13308164 Test MSE 0.42388275157138966 Test RE 0.062261686244502484\n",
      "1000 Train Loss 0.21645711 Test MSE 0.517196072403815 Test RE 0.06877421076778202\n",
      "1100 Train Loss 0.02901491 Test MSE 0.3250610874050139 Test RE 0.054523094689391344\n",
      "1200 Train Loss 0.04817087 Test MSE 0.3266997001326664 Test RE 0.05466034570327054\n",
      "1300 Train Loss 0.09037291 Test MSE 0.37681752330862245 Test RE 0.05870344000863573\n",
      "1400 Train Loss 0.12221919 Test MSE 0.40257835545101434 Test RE 0.06067687658721465\n",
      "1500 Train Loss 0.02143357 Test MSE 0.35141021614936235 Test RE 0.05668983562217907\n",
      "1600 Train Loss 0.045453373 Test MSE 0.41306559768720624 Test RE 0.06146211759653786\n",
      "1700 Train Loss 0.042156152 Test MSE 0.3230415504824493 Test RE 0.05435346048018553\n",
      "1800 Train Loss 0.2823334 Test MSE 0.6160409325976858 Test RE 0.0750590029237828\n",
      "1900 Train Loss 0.119624615 Test MSE 0.5014088997279723 Test RE 0.06771642550218053\n",
      "2000 Train Loss 0.09706748 Test MSE 0.4864889773357279 Test RE 0.06670133227579814\n",
      "2100 Train Loss 0.11416733 Test MSE 0.5179973456345772 Test RE 0.0688274648523693\n",
      "2200 Train Loss 0.07374368 Test MSE 0.3773830418890976 Test RE 0.0587474738415955\n",
      "2300 Train Loss 0.19686913 Test MSE 0.5843354202336855 Test RE 0.07310197532028027\n",
      "2400 Train Loss 0.055509657 Test MSE 0.3345908010727453 Test RE 0.05531653961306128\n",
      "2500 Train Loss 0.07390086 Test MSE 0.412213148709963 Test RE 0.06139866474703404\n",
      "2600 Train Loss 0.058056757 Test MSE 0.3216410662388874 Test RE 0.05423551304223879\n",
      "2700 Train Loss 0.032170273 Test MSE 0.3357637141733282 Test RE 0.05541341127310779\n",
      "2800 Train Loss 2.3679717 Test MSE 7.753913658387074 Test RE 0.26629222506784217\n",
      "2900 Train Loss 0.18367928 Test MSE 0.6243244274742259 Test RE 0.07556195224103937\n",
      "Training time: 29.41\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 99.31753 Test MSE 86.37407211605743 Test RE 0.8887706484422049\n",
      "100 Train Loss 9.1039915 Test MSE 9.125450008547789 Test RE 0.2888851058179287\n",
      "200 Train Loss 5.538383 Test MSE 6.039267220096415 Test RE 0.23501204917583338\n",
      "300 Train Loss 4.923746 Test MSE 5.140389014612606 Test RE 0.21681833226381614\n",
      "400 Train Loss 4.520373 Test MSE 4.682071602151266 Test RE 0.20692693816220806\n",
      "500 Train Loss 2.8132641 Test MSE 2.7341947272276976 Test RE 0.15812943780283575\n",
      "600 Train Loss 1.2963072 Test MSE 1.350780686800784 Test RE 0.11114512725924378\n",
      "700 Train Loss 0.96378535 Test MSE 0.9879327145421741 Test RE 0.09505209417958356\n",
      "800 Train Loss 0.69931203 Test MSE 0.7310439123771021 Test RE 0.08176543256547404\n",
      "900 Train Loss 0.6332294 Test MSE 0.6380421263680922 Test RE 0.07638756794507118\n",
      "1000 Train Loss 0.5261223 Test MSE 0.5264062669046904 Test RE 0.06938387191149648\n",
      "1100 Train Loss 0.7480667 Test MSE 1.0976361122710072 Test RE 0.10019065125984049\n",
      "1200 Train Loss 0.41835237 Test MSE 0.3948591232623377 Test RE 0.06009233706782892\n",
      "1300 Train Loss 0.45382085 Test MSE 0.4199664155716065 Test RE 0.061973394829769365\n",
      "1400 Train Loss 0.39363205 Test MSE 0.3320323141919606 Test RE 0.05510464163242566\n",
      "1500 Train Loss 10.337107 Test MSE 9.196253845725945 Test RE 0.2900036616955768\n",
      "1600 Train Loss 0.2871221 Test MSE 0.24116192956278784 Test RE 0.0469626268554092\n",
      "1700 Train Loss 0.2631978 Test MSE 0.21658249177486127 Test RE 0.04450508954098317\n",
      "1800 Train Loss 0.3875378 Test MSE 0.23960123018798024 Test RE 0.04681041893706633\n",
      "1900 Train Loss 0.2811567 Test MSE 0.22896741867323855 Test RE 0.04575987718482839\n",
      "2000 Train Loss 0.27142337 Test MSE 0.22445273092034435 Test RE 0.045306493694582436\n",
      "2100 Train Loss 0.25505114 Test MSE 0.18536284363558017 Test RE 0.0411726999973065\n",
      "2200 Train Loss 0.22233078 Test MSE 0.20798402064128255 Test RE 0.043612701693635236\n",
      "2300 Train Loss 0.22981495 Test MSE 0.18566778513461718 Test RE 0.04120655280504442\n",
      "2400 Train Loss 0.32538077 Test MSE 0.3581088491958199 Test RE 0.05722759996212157\n",
      "2500 Train Loss 0.32136744 Test MSE 0.5509010816415908 Test RE 0.07097980774710773\n",
      "2600 Train Loss 0.3029632 Test MSE 0.2642879267723996 Test RE 0.04916280663930281\n",
      "2700 Train Loss 0.25339693 Test MSE 0.20859359038063133 Test RE 0.04367656605319654\n",
      "2800 Train Loss 0.24300304 Test MSE 0.17828754164638957 Test RE 0.04037927383512829\n",
      "2900 Train Loss 0.2880603 Test MSE 0.2269301913782356 Test RE 0.04555584911506451\n",
      "Training time: 29.39\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 107.2605 Test MSE 99.22445313387665 Test RE 0.9525929502965429\n",
      "100 Train Loss 5.9826612 Test MSE 7.145782051389817 Test RE 0.25563651547318583\n",
      "200 Train Loss 4.316776 Test MSE 5.344470009394893 Test RE 0.22108044411951913\n",
      "300 Train Loss 4.0392375 Test MSE 5.327636707294845 Test RE 0.22073200461997053\n",
      "400 Train Loss 4.0818954 Test MSE 5.247582359987839 Test RE 0.21906734151530285\n",
      "500 Train Loss 0.69882995 Test MSE 1.0551247225283968 Test RE 0.098231303196547\n",
      "600 Train Loss 1.6782153 Test MSE 1.3080956021108963 Test RE 0.10937492014170463\n",
      "700 Train Loss 0.15952802 Test MSE 0.22340628433745924 Test RE 0.04520075603134021\n",
      "800 Train Loss 0.21585241 Test MSE 0.32223456362895603 Test RE 0.054285528111442514\n",
      "900 Train Loss 0.1675075 Test MSE 0.21747918611148095 Test RE 0.04459712431375567\n",
      "1000 Train Loss 0.1700563 Test MSE 0.19369808746390563 Test RE 0.0420882310058646\n",
      "1100 Train Loss 0.14462583 Test MSE 0.16922736626940113 Test RE 0.03933990473430853\n",
      "1200 Train Loss 0.15332335 Test MSE 0.21133341607560907 Test RE 0.04396247078995801\n",
      "1300 Train Loss 0.26402253 Test MSE 0.30032975177584603 Test RE 0.05240795152732188\n",
      "1400 Train Loss 0.15628868 Test MSE 0.19767932441879602 Test RE 0.04251856809128309\n",
      "1500 Train Loss 1.0113301 Test MSE 1.3097110331545714 Test RE 0.10944243551471051\n",
      "1600 Train Loss 0.1540628 Test MSE 0.19031497398111816 Test RE 0.04171905727572191\n",
      "1700 Train Loss 0.32268906 Test MSE 0.4585842049104131 Test RE 0.06476010619616246\n",
      "1800 Train Loss 0.1511218 Test MSE 0.2027869805766755 Test RE 0.04306436428515061\n",
      "1900 Train Loss 0.07285482 Test MSE 0.16266211525673718 Test RE 0.03856925178432264\n",
      "2000 Train Loss 0.16217354 Test MSE 0.23822537288596157 Test RE 0.046675826180236815\n",
      "2100 Train Loss 0.88351136 Test MSE 1.5063623904073276 Test RE 0.11737152251669254\n",
      "2200 Train Loss 0.15671256 Test MSE 0.1897393644147744 Test RE 0.04165591964383426\n",
      "2300 Train Loss 0.16651139 Test MSE 0.29272362180724804 Test RE 0.051740055607686\n",
      "2400 Train Loss 0.2636053 Test MSE 0.4042468732064242 Test RE 0.060802486622660544\n",
      "2500 Train Loss 0.16669531 Test MSE 0.3896246579401466 Test RE 0.05969269999642946\n",
      "2600 Train Loss 2.1413016 Test MSE 2.074470353192853 Test RE 0.13773731807818662\n",
      "2700 Train Loss 0.062600665 Test MSE 0.22698802214852037 Test RE 0.04556165345957491\n",
      "2800 Train Loss 0.052521605 Test MSE 0.21542040070886828 Test RE 0.04438553110194354\n",
      "2900 Train Loss 0.0640313 Test MSE 0.19127427677152087 Test RE 0.041824069774590064\n",
      "Training time: 29.20\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 125.543 Test MSE 99.19315121091579 Test RE 0.9524426831889317\n",
      "100 Train Loss 6.9240007 Test MSE 6.759756890501178 Test RE 0.24863573344036635\n",
      "200 Train Loss 5.900455 Test MSE 5.8569777279379736 Test RE 0.23143806650634574\n",
      "300 Train Loss 4.9831142 Test MSE 4.885570783719606 Test RE 0.2113759931284574\n",
      "400 Train Loss 4.1202607 Test MSE 4.42660824232092 Test RE 0.2012025823176892\n",
      "500 Train Loss 1.050634 Test MSE 0.8907415199025263 Test RE 0.09025553697493893\n",
      "600 Train Loss 0.2162016 Test MSE 0.1945808639105728 Test RE 0.04218403025434496\n",
      "700 Train Loss 0.11547146 Test MSE 0.12360555677727608 Test RE 0.03362149368978247\n",
      "800 Train Loss 0.27781257 Test MSE 0.32864535945080164 Test RE 0.05482286886790113\n",
      "900 Train Loss 0.31335074 Test MSE 0.16617328804009762 Test RE 0.038983301141761455\n",
      "1000 Train Loss 0.1328031 Test MSE 0.43395446649310954 Test RE 0.0629970318042228\n",
      "1100 Train Loss 0.06265674 Test MSE 0.09273107010705943 Test RE 0.02912129219319319\n",
      "1200 Train Loss 0.039622065 Test MSE 0.16194145172717897 Test RE 0.03848371770671164\n",
      "1300 Train Loss 0.056895453 Test MSE 0.09162011863114895 Test RE 0.02894632480905837\n",
      "1400 Train Loss 4.707431 Test MSE 5.915993581570091 Test RE 0.23260114767189743\n",
      "1500 Train Loss 0.098742574 Test MSE 0.20663982560152536 Test RE 0.04347153939249022\n",
      "1600 Train Loss 0.041504536 Test MSE 0.21707318005898985 Test RE 0.04455547627987819\n",
      "1700 Train Loss 0.87459475 Test MSE 1.3056717145473897 Test RE 0.10927353785937705\n",
      "1800 Train Loss 0.00274306 Test MSE 0.11273066219760079 Test RE 0.03210842739461567\n",
      "1900 Train Loss 0.02289472 Test MSE 0.09815208181715951 Test RE 0.029960410802511172\n",
      "2000 Train Loss 0.0019219232 Test MSE 0.11181418986697493 Test RE 0.03197764426706247\n",
      "2100 Train Loss 0.04032081 Test MSE 0.06273081369818714 Test RE 0.02395181709783847\n",
      "2200 Train Loss 0.0014368593 Test MSE 0.10539915782457786 Test RE 0.031046780879304566\n",
      "2300 Train Loss 0.0012988731 Test MSE 0.10045389204741269 Test RE 0.030309682715543063\n",
      "2400 Train Loss 0.041764762 Test MSE 0.2560258812735314 Test RE 0.048388252812064526\n",
      "2500 Train Loss 0.001011517 Test MSE 0.10289121258235293 Test RE 0.030675182053578064\n",
      "2600 Train Loss 0.7383225 Test MSE 1.5019679627459017 Test RE 0.11720019675113232\n",
      "2700 Train Loss 0.14559433 Test MSE 0.13990068902473607 Test RE 0.035769093415199314\n",
      "2800 Train Loss 0.10272721 Test MSE 0.1176558904429987 Test RE 0.03280234134916349\n",
      "2900 Train Loss 0.15593047 Test MSE 0.37963927906774764 Test RE 0.058922827123853806\n",
      "Training time: 29.20\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 121.91756 Test MSE 95.06604682479608 Test RE 0.9324181604747566\n",
      "100 Train Loss 6.850289 Test MSE 7.306727201885144 Test RE 0.2584993484303889\n",
      "200 Train Loss 5.138269 Test MSE 5.708478996747961 Test RE 0.228485271432195\n",
      "300 Train Loss 3.6367917 Test MSE 4.570902789439342 Test RE 0.20445559455683662\n",
      "400 Train Loss 3.203068 Test MSE 5.879538712866947 Test RE 0.23188338595080618\n",
      "500 Train Loss 0.5242943 Test MSE 0.9278288030788262 Test RE 0.09211533404975478\n",
      "600 Train Loss 0.21519786 Test MSE 0.3562328342810957 Test RE 0.057077504807321634\n",
      "700 Train Loss 0.32351732 Test MSE 0.5035605829102849 Test RE 0.06786156484276473\n",
      "800 Train Loss 0.0865422 Test MSE 0.30908074738454394 Test RE 0.053165999521883604\n",
      "900 Train Loss 0.5943284 Test MSE 0.6752318540870651 Test RE 0.07858225137337436\n",
      "1000 Train Loss 0.110877074 Test MSE 0.44002147081702564 Test RE 0.06343587583063762\n",
      "1100 Train Loss 0.1022386 Test MSE 0.3422941871995547 Test RE 0.055949700772470995\n",
      "1200 Train Loss 0.0575392 Test MSE 0.36293853260731535 Test RE 0.05761221140868062\n",
      "1300 Train Loss 1.2890768 Test MSE 1.9496561535976267 Test RE 0.1335294372105223\n",
      "1400 Train Loss 0.11048696 Test MSE 0.3007695523918856 Test RE 0.05244631039318556\n",
      "1500 Train Loss 0.08369267 Test MSE 0.3400749430264923 Test RE 0.05576803254239762\n",
      "1600 Train Loss 0.07835631 Test MSE 0.2848685672048201 Test RE 0.051041128955187434\n",
      "1700 Train Loss 0.04650179 Test MSE 0.30083977198322726 Test RE 0.05245243226211958\n",
      "1800 Train Loss 0.18422563 Test MSE 0.41463038067664626 Test RE 0.06157842353666667\n",
      "1900 Train Loss 0.041999176 Test MSE 0.31348956274683565 Test RE 0.053543844312814816\n",
      "2000 Train Loss 0.14635473 Test MSE 0.3579807567089816 Test RE 0.0572173641340537\n",
      "2100 Train Loss 0.09120522 Test MSE 0.4470730986245516 Test RE 0.06394215593402879\n",
      "2200 Train Loss 0.037099395 Test MSE 0.3647141560972771 Test RE 0.05775296910295166\n",
      "2300 Train Loss 0.03583296 Test MSE 0.3789621888466072 Test RE 0.05887025896654883\n",
      "2400 Train Loss 0.036730368 Test MSE 0.32877498024790075 Test RE 0.0548336791276681\n",
      "2500 Train Loss 0.06718649 Test MSE 0.3571163098335628 Test RE 0.05714823853654584\n",
      "2600 Train Loss 0.013025909 Test MSE 0.3286885997675117 Test RE 0.054826475308971064\n",
      "2700 Train Loss 0.06893207 Test MSE 0.353108757916011 Test RE 0.056826675666883826\n",
      "2800 Train Loss 0.040394213 Test MSE 0.3840962741818113 Test RE 0.059267697198498996\n",
      "2900 Train Loss 0.24942261 Test MSE 0.5727537285388682 Test RE 0.07237389878146368\n",
      "Training time: 29.27\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 111.57866 Test MSE 99.67036121003737 Test RE 0.9547309955311608\n",
      "100 Train Loss 4.2452946 Test MSE 6.753933533609248 Test RE 0.24852861371692353\n",
      "200 Train Loss 2.0980456 Test MSE 3.5836243686148115 Test RE 0.18103362456520064\n",
      "300 Train Loss 0.629923 Test MSE 0.9779826679697626 Test RE 0.09457222031000864\n",
      "400 Train Loss 0.10487146 Test MSE 0.3909838489581247 Test RE 0.05979672722511322\n",
      "500 Train Loss 0.12760934 Test MSE 0.3829950163581932 Test RE 0.059182671814683965\n",
      "600 Train Loss 0.46613353 Test MSE 0.5942116816057369 Test RE 0.0737171605836417\n",
      "700 Train Loss 0.13432193 Test MSE 0.5240422847495452 Test RE 0.06922790226642661\n",
      "800 Train Loss 0.04850394 Test MSE 0.40467916920395713 Test RE 0.060834988603489314\n",
      "900 Train Loss 0.0098700365 Test MSE 0.3702736813846103 Test RE 0.058191483390677104\n",
      "1000 Train Loss 0.08754335 Test MSE 0.4949267265659335 Test RE 0.06727728539378242\n",
      "1100 Train Loss 0.016795797 Test MSE 0.384464390364195 Test RE 0.05929609134727927\n",
      "1200 Train Loss 0.056518402 Test MSE 0.4629342318276031 Test RE 0.06506653116320325\n",
      "1300 Train Loss 0.25074494 Test MSE 0.5606694495883593 Test RE 0.0716063360554001\n",
      "1400 Train Loss 0.0028397548 Test MSE 0.424458975774212 Test RE 0.06230399099271315\n",
      "1500 Train Loss 0.013216216 Test MSE 0.40626533587912445 Test RE 0.06095409537957853\n",
      "1600 Train Loss 0.02035155 Test MSE 0.5075828748777148 Test RE 0.0681320547537221\n",
      "1700 Train Loss 0.051649973 Test MSE 0.4855467256934526 Test RE 0.06663670604102895\n",
      "1800 Train Loss 0.0043416554 Test MSE 0.41210346602092846 Test RE 0.0613904956496778\n",
      "1900 Train Loss 0.029333157 Test MSE 0.4456780095848954 Test RE 0.06384231241749325\n",
      "2000 Train Loss 0.02856322 Test MSE 0.45280958417145273 Test RE 0.06435107576342149\n",
      "2100 Train Loss 0.008179053 Test MSE 0.43023589210087176 Test RE 0.06272653896073745\n",
      "2200 Train Loss 0.012565661 Test MSE 0.3932372556365234 Test RE 0.05996879667523058\n",
      "2300 Train Loss 0.020505203 Test MSE 0.40620082975935873 Test RE 0.06094925609362733\n",
      "2400 Train Loss 0.23532654 Test MSE 0.3414852664447001 Test RE 0.05588355059109276\n",
      "2500 Train Loss 0.02352749 Test MSE 0.4332436890791517 Test RE 0.06294541899580869\n",
      "2600 Train Loss 0.022584217 Test MSE 0.43565094927947984 Test RE 0.06312005062350423\n",
      "2700 Train Loss 0.028584534 Test MSE 0.37985930671243145 Test RE 0.05893989961035281\n",
      "2800 Train Loss 0.010354907 Test MSE 0.43005386490930286 Test RE 0.06271326816687302\n",
      "2900 Train Loss 1.146298 Test MSE 2.3693296256928016 Test RE 0.14720099600377548\n",
      "Training time: 29.47\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 114.27008 Test MSE 102.08013554953799 Test RE 0.9662035419298504\n",
      "100 Train Loss 7.5636687 Test MSE 7.554266837421179 Test RE 0.26284163907841385\n",
      "200 Train Loss 4.800748 Test MSE 5.013104947752672 Test RE 0.2141171255179541\n",
      "300 Train Loss 3.4533553 Test MSE 3.6082914920244358 Test RE 0.1816556094841272\n",
      "400 Train Loss 1.5935177 Test MSE 2.264514285622759 Test RE 0.14390819916232978\n",
      "500 Train Loss 0.05918377 Test MSE 0.5547061739852024 Test RE 0.07122451588494184\n",
      "600 Train Loss 0.135605 Test MSE 0.5227775786037548 Test RE 0.06914431565119737\n",
      "700 Train Loss 0.028685138 Test MSE 0.46394642674788056 Test RE 0.06513762555065104\n",
      "800 Train Loss 0.12146683 Test MSE 0.5606167861499685 Test RE 0.07160297300037086\n",
      "900 Train Loss 0.034555595 Test MSE 0.5556046246696288 Test RE 0.071282173278497\n",
      "1000 Train Loss 0.044038694 Test MSE 0.4750853478285787 Test RE 0.06591493443298002\n",
      "1100 Train Loss 0.078864634 Test MSE 0.6009770070199301 Test RE 0.07413562178905357\n",
      "1200 Train Loss 0.011464053 Test MSE 0.4831591207587245 Test RE 0.06647266600895799\n",
      "1300 Train Loss 0.024020221 Test MSE 0.46984608898707914 Test RE 0.06555047064604749\n",
      "1400 Train Loss 0.103163876 Test MSE 0.6151321532187396 Test RE 0.07500361922643445\n",
      "1500 Train Loss 0.20490146 Test MSE 0.5921825000265524 Test RE 0.07359118407318804\n",
      "1600 Train Loss 0.07695472 Test MSE 0.5216768181245741 Test RE 0.06907148215528432\n",
      "1700 Train Loss 0.016096475 Test MSE 0.4956103022658044 Test RE 0.0673237298938391\n",
      "1800 Train Loss 0.18079016 Test MSE 0.599296313429178 Test RE 0.07403188529115264\n",
      "1900 Train Loss 0.015760813 Test MSE 0.5510588720729969 Test RE 0.07098997212415782\n",
      "2000 Train Loss 0.0069176606 Test MSE 0.47690368366490415 Test RE 0.06604095497177924\n",
      "2100 Train Loss 0.07406165 Test MSE 0.4375299901491602 Test RE 0.06325602822064078\n",
      "2200 Train Loss 0.064990774 Test MSE 0.5399659055652795 Test RE 0.07027181575664856\n",
      "2300 Train Loss 1.7578176 Test MSE 1.5606591359339599 Test RE 0.11946812169704406\n",
      "2400 Train Loss 0.07516218 Test MSE 0.5144163201177869 Test RE 0.06858914281396226\n",
      "2500 Train Loss 0.036125835 Test MSE 0.501281436823353 Test RE 0.067707817875849\n",
      "2600 Train Loss 0.02483883 Test MSE 0.49210544370531184 Test RE 0.06708525745136339\n",
      "2700 Train Loss 0.01989748 Test MSE 0.4696031715690131 Test RE 0.0655335231709698\n",
      "2800 Train Loss 0.016231993 Test MSE 0.4697969085032516 Test RE 0.06554703985423793\n",
      "2900 Train Loss 0.06461717 Test MSE 0.4414152291204624 Test RE 0.06353626226928562\n",
      "Training time: 30.21\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 3000\n",
    "layers = np.array([1,50,50,50,50,1])\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "alpha_full = []\n",
    "omega_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "n_val = 1\n",
    "rowdy_terms = 2\n",
    "\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(reps)\n",
    "    \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []\n",
    "    alpha_val = []\n",
    "    omega_val = []\n",
    "    'Generate Training data'\n",
    "    N_train = 300\n",
    "    \n",
    "    x_train_np_array, u_train_np_array = trainingdata(N_train,reps*1234)\n",
    "\n",
    "    'Convert to tensor and send to GPU'\n",
    "    x_train = torch.from_numpy(x_train_np_array).float().to(device)\n",
    "    u_train = torch.from_numpy(u_train_np_array).float().to(device)\n",
    "    \n",
    "    x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "       \n",
    "     \n",
    "    model_NN = Sequentialmodel(layers,n_val,rowdy_terms)\n",
    "    model_NN.to(device)\n",
    "    print(model_NN)\n",
    "    params = list(model_NN.parameters())\n",
    "    #optimizer = optim.Adam(PINN.parameters(), lr=0.0008,betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "    optimizer = optim.Adam(model_NN.parameters(), lr=8.0e-3)\n",
    "\n",
    "    train_model(max_iter,reps)\n",
    " \n",
    "    torch.save(model_NN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    alpha_full.append(alpha_val)\n",
    "    omega_full.append(omega_val)\n",
    "                 \n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse\": test_mse_full, \"test_re\": test_re_full,\"Time\": elapsed_time, \"Thresh_time\": time_threshold, \"epoch_thresh\": epoch_threshold,\"alpha\": alpha_full,\"omega\": omega_full, \"label\": label}\n",
    "savemat(label+'.mat', mdic) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred = model_NN.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f02e075e5d0>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_test,u_pred,'rx')\n",
    "plt.plot(x_test,u_true,'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   0.039834720447842135\n",
      "1   0.021112008792826223\n",
      "2   0.03217229561630615\n",
      "3   0.07556195224103937\n",
      "4   0.04555584911506451\n",
      "5   0.041824069774590064\n",
      "6   0.058922827123853806\n",
      "7   0.07237389878146368\n",
      "8   0.14720099600377548\n",
      "9   0.06353626226928562\n",
      "0.059809488016604705\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    print(i,\" \",test_re_full[i][-1])\n",
    "    a = a + test_re_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
