{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "#Trying to replicate the results of \"Adaptive activation functions accelerate convergence in deep and physics-informed neural networks\"\n",
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_disc(x): #[-4,3.75]\n",
    "    y  = np.zeros((np.shape(x)[0],))\n",
    "    \n",
    "    y = 0.2*np.sin(6*x)*np.exp(-1*x)\n",
    "    y[x>0] = 1 + 0.1*x[x>0]*np.exp(x[x>0]) + 10\n",
    "    \n",
    "    return scale*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain bounds\n",
    "level = \"medium\"\n",
    "label = \"Regr_disc_rowdy_\" + level\n",
    "loss_thresh = 0.1\n",
    "scale = 10.0\n",
    "\n",
    "lb = np.array(-4.0)  # [-1. 0.]\n",
    "ub = np.array(3.75) # [1.  0.99]\n",
    "\n",
    "x_test = np.linspace(lb,ub,1000).reshape(-1,1)\n",
    "u_true = true_disc(x_test)\n",
    "\n",
    "u_true_norm = np.linalg.norm(u_true,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_train,seed):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    x_train = np.random.uniform(low=lb, high=ub, size=(N_train,)).reshape(-1,1)\n",
    "\n",
    "    u_train = true_disc(x_train)\n",
    "\n",
    "    return x_train, u_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,n_val,rowdy_terms):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "     \n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "\n",
    "        self.omega1 = Parameter(torch.ones((len(layers)-2,1))) \n",
    "        #self.alpha = Parameter(torch.ones((layers[1],len(layers)-2)))\n",
    "        \n",
    "        self.alpha = Parameter(torch.zeros(rowdy_terms,len(layers)-2))\n",
    "        self.omega = Parameter((1/n_val)*torch.ones(rowdy_terms,len(layers)-2))\n",
    "        \n",
    "        self.n = torch.tensor(n_val)\n",
    "            \n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = 2.0*(x - l_b)/(u_b - l_b)-1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(self.omega1[i,0]*z)\n",
    "            for j in range(rowdy_terms):\n",
    "                a = a + self.alpha[j,i]*self.n*torch.sin((j+1)*self.n*self.omega[j,i]*z)\n",
    "    \n",
    "        a = self.linears[-1](a) \n",
    "        \n",
    "        return a\n",
    "    \n",
    "    \n",
    "    def loss(self,x,u):\n",
    "\n",
    "        loss_val = self.loss_function(self.forward(x),u)\n",
    "        \n",
    "        return loss_val\n",
    "        \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        u_pred = self.forward(x_test_tensor)       \n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "        \n",
    "        return u_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "        \n",
    "        test_mse_loss = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re_loss = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "        \n",
    "        return test_mse_loss, test_re_loss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    alpha_val.append(model_NN.alpha.cpu().detach().numpy())\n",
    "    omega_val.append(model_NN.omega.cpu().detach().numpy())    \n",
    "    test_mse, test_re = model_NN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step():\n",
    "    optimizer.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "    loss = model_NN.loss(x_train,u_train)\n",
    "    loss.backward() #backprop\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "   \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "    \n",
    "    loss_np = model_NN.loss(x_train,u_train).cpu().detach().numpy()\n",
    "    data_update(loss_np)\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        loss_np = train_step().cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1\n",
    "        data_update(loss_np)\n",
    "        if(i%100==0):        \n",
    "            print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 11607.848 Test MSE 10759.596966503392 Test RE 0.9919642213072402\n",
      "100 Train Loss 1761.2155 Test MSE 1590.5725929616588 Test RE 0.3813947877948217\n",
      "200 Train Loss 305.66312 Test MSE 295.20720383444245 Test RE 0.1643090489084778\n",
      "300 Train Loss 476.0513 Test MSE 452.60271112053294 Test RE 0.203449478845892\n",
      "400 Train Loss 23.488157 Test MSE 49.3663423595704 Test RE 0.0671913677269853\n",
      "500 Train Loss 9.310617 Test MSE 31.791151733094342 Test RE 0.053920155637908945\n",
      "600 Train Loss 4.3618298 Test MSE 23.07362024332104 Test RE 0.04593628571919919\n",
      "700 Train Loss 2.475684 Test MSE 21.141559353783947 Test RE 0.04397101741077715\n",
      "800 Train Loss 4.050837 Test MSE 27.756609708102218 Test RE 0.050382674243861655\n",
      "900 Train Loss 4.8996134 Test MSE 21.29663440861883 Test RE 0.044131988258408514\n",
      "1000 Train Loss 9.871962 Test MSE 29.46423786726076 Test RE 0.05190935255705376\n",
      "1100 Train Loss 1.2683227 Test MSE 15.995363122138329 Test RE 0.0382467958793991\n",
      "1200 Train Loss 3.316771 Test MSE 18.14311751519401 Test RE 0.040733709605791105\n",
      "1300 Train Loss 7.8832326 Test MSE 27.827693861653643 Test RE 0.050447147530417125\n",
      "1400 Train Loss 11.283096 Test MSE 32.669508489526066 Test RE 0.054659959612675786\n",
      "1500 Train Loss 7.0044394 Test MSE 19.32755210830422 Test RE 0.04204229668060673\n",
      "1600 Train Loss 2.0682545 Test MSE 13.029729982725193 Test RE 0.03451959677754463\n",
      "1700 Train Loss 541.66785 Test MSE 529.6916487041957 Test RE 0.22009469210085603\n",
      "1800 Train Loss 0.5397747 Test MSE 17.25249262709933 Test RE 0.03972134369559453\n",
      "1900 Train Loss 0.35141236 Test MSE 17.028726802528208 Test RE 0.03946290892267964\n",
      "2000 Train Loss 2.2980978 Test MSE 15.86511344684473 Test RE 0.03809075642213681\n",
      "2100 Train Loss 0.65257674 Test MSE 17.02790244801956 Test RE 0.03946195371854164\n",
      "2200 Train Loss 1.8059111 Test MSE 17.574498165520545 Test RE 0.04009031535992664\n",
      "2300 Train Loss 1.1131756 Test MSE 14.16956907397348 Test RE 0.03599783048242369\n",
      "2400 Train Loss 2.6780682 Test MSE 14.945475520684496 Test RE 0.036970291506195005\n",
      "2500 Train Loss 2.0408309 Test MSE 19.656052840981744 Test RE 0.042398077234659375\n",
      "2600 Train Loss 4.1779294 Test MSE 18.1545821370413 Test RE 0.04074657737303902\n",
      "2700 Train Loss 4.3933277 Test MSE 26.899969710372677 Test RE 0.049599112058208425\n",
      "2800 Train Loss 2.90285 Test MSE 20.5804626105401 Test RE 0.04338359852979395\n",
      "2900 Train Loss 0.41454524 Test MSE 16.089525504327376 Test RE 0.03835920735401453\n",
      "Training time: 29.60\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 11719.888 Test MSE 10820.2723794508 Test RE 0.9947572266906086\n",
      "100 Train Loss 1752.2075 Test MSE 1712.0976588960611 Test RE 0.39569655831089334\n",
      "200 Train Loss 846.1845 Test MSE 812.5146061541687 Test RE 0.2725923066224403\n",
      "300 Train Loss 130.13487 Test MSE 136.69698482375782 Test RE 0.11180918293579635\n",
      "400 Train Loss 49.80241 Test MSE 54.022194087676866 Test RE 0.07028847415464594\n",
      "500 Train Loss 21.698006 Test MSE 27.045050729752635 Test RE 0.04973268494693702\n",
      "600 Train Loss 5.551555 Test MSE 10.280201334139594 Test RE 0.030661882588218357\n",
      "700 Train Loss 2.970333 Test MSE 7.800388916999221 Test RE 0.0267089081415642\n",
      "800 Train Loss 3.3090804 Test MSE 6.55638356177084 Test RE 0.024486695605242532\n",
      "900 Train Loss 0.95177716 Test MSE 6.432858134930338 Test RE 0.024254928187376377\n",
      "1000 Train Loss 0.44384792 Test MSE 5.090124631524725 Test RE 0.021575566829368314\n",
      "1100 Train Loss 0.45815384 Test MSE 4.226123428127951 Test RE 0.01965934728519734\n",
      "1200 Train Loss 1.6129708 Test MSE 7.845659191071865 Test RE 0.026786299818292204\n",
      "1300 Train Loss 0.20916982 Test MSE 5.431091845102209 Test RE 0.022286485119875632\n",
      "1400 Train Loss 0.7042269 Test MSE 4.04027898075202 Test RE 0.019222226185320424\n",
      "1500 Train Loss 2.2187376 Test MSE 4.64744786076816 Test RE 0.020616041000305025\n",
      "1600 Train Loss 47.98219 Test MSE 30.814114717772565 Test RE 0.05308512614525031\n",
      "1700 Train Loss 0.0844577 Test MSE 5.398689793784061 Test RE 0.02221990475822915\n",
      "1800 Train Loss 0.040018063 Test MSE 4.949414834981829 Test RE 0.021275262832512305\n",
      "1900 Train Loss 0.024807477 Test MSE 4.697004261047586 Test RE 0.020725665434731283\n",
      "2000 Train Loss 0.01700847 Test MSE 4.541081201569252 Test RE 0.020378754623123926\n",
      "2100 Train Loss 0.012414856 Test MSE 4.440218620521482 Test RE 0.020151166111629745\n",
      "2200 Train Loss 0.009513198 Test MSE 4.373895860036814 Test RE 0.020000102684654048\n",
      "2300 Train Loss 0.42728043 Test MSE 4.7140323110720725 Test RE 0.020763199825651855\n",
      "2400 Train Loss 0.013827162 Test MSE 4.493935004167582 Test RE 0.020272690943739315\n",
      "2500 Train Loss 0.009431293 Test MSE 4.453279107063678 Test RE 0.020180780732846314\n",
      "2600 Train Loss 22.01078 Test MSE 11.561418787651816 Test RE 0.03251648349828073\n",
      "2700 Train Loss 0.06469055 Test MSE 4.540461749638791 Test RE 0.02037736463593132\n",
      "2800 Train Loss 0.03002604 Test MSE 4.655071940954687 Test RE 0.02063294424967571\n",
      "2900 Train Loss 0.017482005 Test MSE 4.748377550199165 Test RE 0.020838700249601827\n",
      "Training time: 28.98\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 11175.824 Test MSE 10799.015203228835 Test RE 0.9937796114928805\n",
      "100 Train Loss 1939.0609 Test MSE 1832.727068415106 Test RE 0.40939911854038863\n",
      "200 Train Loss 461.8896 Test MSE 362.1207794633843 Test RE 0.1819804478247724\n",
      "300 Train Loss 68.37203 Test MSE 62.979766987284194 Test RE 0.07589244261524561\n",
      "400 Train Loss 50.768253 Test MSE 34.454852568564874 Test RE 0.056133639878350655\n",
      "500 Train Loss 28.125528 Test MSE 23.13024418283475 Test RE 0.045992616272986384\n",
      "600 Train Loss 46.785862 Test MSE 22.72303625360248 Test RE 0.045585968582390786\n",
      "700 Train Loss 27.301306 Test MSE 16.818935437262134 Test RE 0.03921906700574979\n",
      "800 Train Loss 14.229941 Test MSE 8.121091685344288 Test RE 0.027252428755817765\n",
      "900 Train Loss 44.346992 Test MSE 13.788925922878715 Test RE 0.03551102641689113\n",
      "1000 Train Loss 14.464263 Test MSE 16.034670581032337 Test RE 0.03829376142384052\n",
      "1100 Train Loss 10.140606 Test MSE 12.285778827998588 Test RE 0.03351964053070631\n",
      "1200 Train Loss 20.733025 Test MSE 40.60779323070372 Test RE 0.06094003519302665\n",
      "1300 Train Loss 11.223741 Test MSE 6.518651846856298 Test RE 0.024416133981186416\n",
      "1400 Train Loss 4.799474 Test MSE 4.420759228072829 Test RE 0.020106961078383907\n",
      "1500 Train Loss 3.3051002 Test MSE 7.1561743715147 Test RE 0.025582233777183043\n",
      "1600 Train Loss 3.1570215 Test MSE 8.592034184314926 Test RE 0.02803147845982187\n",
      "1700 Train Loss 637.8713 Test MSE 633.5646260599104 Test RE 0.24070963080660449\n",
      "1800 Train Loss 10.513716 Test MSE 18.735900232931936 Test RE 0.04139379921493658\n",
      "1900 Train Loss 2.2230947 Test MSE 6.503810669818135 Test RE 0.024388323732877058\n",
      "2000 Train Loss 1.0721135 Test MSE 7.763876132433576 Test RE 0.026646324049972053\n",
      "2100 Train Loss 132.02197 Test MSE 85.00077603945954 Test RE 0.08816768779696912\n",
      "2200 Train Loss 9.521004 Test MSE 11.98478613125953 Test RE 0.03310649089590378\n",
      "2300 Train Loss 5.1076713 Test MSE 8.860481330869211 Test RE 0.02846601432645433\n",
      "2400 Train Loss 9.540243 Test MSE 11.405388784888885 Test RE 0.032296321015413174\n",
      "2500 Train Loss 9.386618 Test MSE 18.309951748143853 Test RE 0.04092056354758875\n",
      "2600 Train Loss 1.8872833 Test MSE 8.733460662827468 Test RE 0.02826123851642517\n",
      "2700 Train Loss 344.4915 Test MSE 144.35641962979471 Test RE 0.11489894927405671\n",
      "2800 Train Loss 3.1816819 Test MSE 11.684294651001691 Test RE 0.032688820950273\n",
      "2900 Train Loss 1.401749 Test MSE 8.927913543026687 Test RE 0.028574128545334698\n",
      "Training time: 29.40\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 12235.496 Test MSE 10807.586075964 Test RE 0.994173900650303\n",
      "100 Train Loss 1976.9755 Test MSE 1933.0687202867568 Test RE 0.42045706352726875\n",
      "200 Train Loss 240.59366 Test MSE 346.1956268686825 Test RE 0.17793393979251249\n",
      "300 Train Loss 145.25586 Test MSE 217.7789131931333 Test RE 0.14112563831088454\n",
      "400 Train Loss 17.886305 Test MSE 57.56183870624159 Test RE 0.07255466400683441\n",
      "500 Train Loss 6.8971486 Test MSE 38.307129785715745 Test RE 0.05918856525710071\n",
      "600 Train Loss 7.3856034 Test MSE 52.62416883565151 Test RE 0.06937302478609561\n",
      "700 Train Loss 4.7333937 Test MSE 34.568605453192106 Test RE 0.056226226282574454\n",
      "800 Train Loss 3.9259422 Test MSE 26.15132683832025 Test RE 0.04890405491247557\n",
      "900 Train Loss 0.9589134 Test MSE 23.522861820559697 Test RE 0.046381317916998975\n",
      "1000 Train Loss 9.118198 Test MSE 28.695597157427528 Test RE 0.05122779224933322\n",
      "1100 Train Loss 1.0311179 Test MSE 23.594545139656898 Test RE 0.04645193512601682\n",
      "1200 Train Loss 110.90602 Test MSE 157.51380490520364 Test RE 0.12002102113040748\n",
      "1300 Train Loss 4.0646687 Test MSE 34.150392603074444 Test RE 0.05588507737684135\n",
      "1400 Train Loss 2.3283768 Test MSE 30.13694377547239 Test RE 0.0524985864658047\n",
      "1500 Train Loss 18.124622 Test MSE 81.3506182482271 Test RE 0.08625383925004203\n",
      "1600 Train Loss 1.0473208 Test MSE 27.218104602113637 Test RE 0.049891544142984266\n",
      "1700 Train Loss 1.0905606 Test MSE 26.669206403471247 Test RE 0.04938590907062583\n",
      "1800 Train Loss 0.65589195 Test MSE 25.57350294653119 Test RE 0.04836075986861824\n",
      "1900 Train Loss 2.459533 Test MSE 30.101952439170482 Test RE 0.05246810014241371\n",
      "2000 Train Loss 0.46617845 Test MSE 24.299404472841097 Test RE 0.047140677646561414\n",
      "2100 Train Loss 0.97875875 Test MSE 25.087119420672632 Test RE 0.047898664507554506\n",
      "2200 Train Loss 3.5850892 Test MSE 25.916488034363244 Test RE 0.04868398062903374\n",
      "2300 Train Loss 1.30498 Test MSE 25.022177094921577 Test RE 0.04783662736521054\n",
      "2400 Train Loss 0.7677044 Test MSE 24.266067875262546 Test RE 0.04710833016321854\n",
      "2500 Train Loss 0.8976873 Test MSE 23.80181021351669 Test RE 0.046655516339401806\n",
      "2600 Train Loss 0.35996482 Test MSE 22.83879314516608 Test RE 0.04570193430487962\n",
      "2700 Train Loss 0.52629024 Test MSE 23.119504937673764 Test RE 0.04598193797398231\n",
      "2800 Train Loss 0.5652299 Test MSE 22.51925598570053 Test RE 0.04538110067178601\n",
      "2900 Train Loss 1.5844283 Test MSE 25.247323677424053 Test RE 0.04805135956361336\n",
      "Training time: 29.72\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 10293.483 Test MSE 10681.95629146405 Test RE 0.9883787611466179\n",
      "100 Train Loss 1522.775 Test MSE 1723.3669222362232 Test RE 0.3969966871217575\n",
      "200 Train Loss 325.64966 Test MSE 380.81952557409767 Test RE 0.1866197528752597\n",
      "300 Train Loss 147.984 Test MSE 176.69602034131375 Test RE 0.12711927001121878\n",
      "400 Train Loss 75.51757 Test MSE 96.16838671704704 Test RE 0.09378085145004154\n",
      "500 Train Loss 29.993929 Test MSE 29.929076551969324 Test RE 0.05231722072910847\n",
      "600 Train Loss 21.130823 Test MSE 19.901193763167466 Test RE 0.042661642321775106\n",
      "700 Train Loss 12.299507 Test MSE 7.938257921732665 Test RE 0.026943909362975818\n",
      "800 Train Loss 10.746531 Test MSE 6.299628519438076 Test RE 0.02400244462328645\n",
      "900 Train Loss 9.74162 Test MSE 5.426588272946122 Test RE 0.022277242999478832\n",
      "1000 Train Loss 8.971294 Test MSE 5.006588057543937 Test RE 0.021397790724943917\n",
      "1100 Train Loss 1.9081959 Test MSE 2.1380412075083672 Test RE 0.013983183014866786\n",
      "1200 Train Loss 13.645957 Test MSE 45.6770289209637 Test RE 0.0646319009897001\n",
      "1300 Train Loss 21.234655 Test MSE 51.92629223886985 Test RE 0.06891149356442913\n",
      "1400 Train Loss 3.539725 Test MSE 28.88572032649732 Test RE 0.051397217386591705\n",
      "1500 Train Loss 27.79037 Test MSE 44.87158105631132 Test RE 0.06405952178817811\n",
      "1600 Train Loss 4.2780733 Test MSE 29.78565729471948 Test RE 0.05219171890445292\n",
      "1700 Train Loss 0.36778718 Test MSE 25.313660021492908 Test RE 0.04811444467545488\n",
      "1800 Train Loss 0.065550864 Test MSE 24.446049865749007 Test RE 0.04728270920809619\n",
      "1900 Train Loss 23.987429 Test MSE 32.15404046696929 Test RE 0.05422702553918319\n",
      "2000 Train Loss 0.1355253 Test MSE 21.549218687063764 Test RE 0.04439292594826199\n",
      "2100 Train Loss 14.205782 Test MSE 22.096946467826875 Test RE 0.044953564916922435\n",
      "2200 Train Loss 0.77029586 Test MSE 13.51949453829983 Test RE 0.03516237760836974\n",
      "2300 Train Loss 0.6111398 Test MSE 13.169436087726703 Test RE 0.03470416469112475\n",
      "2400 Train Loss 0.1584374 Test MSE 13.92323689887191 Test RE 0.03568355481004058\n",
      "2500 Train Loss 0.04840989 Test MSE 13.154601874065282 Test RE 0.03468461358642937\n",
      "2600 Train Loss 36.443653 Test MSE 14.211700414535823 Test RE 0.03605130815492018\n",
      "2700 Train Loss 0.030879334 Test MSE 13.129990556780339 Test RE 0.03465215218303529\n",
      "2800 Train Loss 0.027591102 Test MSE 12.953336349009634 Test RE 0.03441825338357474\n",
      "2900 Train Loss 0.08788495 Test MSE 12.954474221424196 Test RE 0.03441976506828645\n",
      "Training time: 29.13\n",
      "5\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 10570.856 Test MSE 10830.159375645355 Test RE 0.9952116013533978\n",
      "100 Train Loss 1706.8943 Test MSE 1841.2482157057068 Test RE 0.41034975235306426\n",
      "200 Train Loss 377.63403 Test MSE 540.3895138031745 Test RE 0.22230614244351599\n",
      "300 Train Loss 81.51647 Test MSE 176.85086708657636 Test RE 0.1271749580074031\n",
      "400 Train Loss 38.249287 Test MSE 88.24912838304054 Test RE 0.08983658168281067\n",
      "500 Train Loss 15.4211235 Test MSE 64.00963688623577 Test RE 0.07651043795681818\n",
      "600 Train Loss 6.9994917 Test MSE 42.289717890990346 Test RE 0.06218926158668823\n",
      "700 Train Loss 11.172001 Test MSE 35.95238443873711 Test RE 0.057340550180648035\n",
      "800 Train Loss 2.8964136 Test MSE 27.303858456453863 Test RE 0.04997007693714648\n",
      "900 Train Loss 3.7817163 Test MSE 25.993295264550554 Test RE 0.048756068238376776\n",
      "1000 Train Loss 2.88298 Test MSE 23.01072628829974 Test RE 0.0458736365525602\n",
      "1100 Train Loss 6.5549607 Test MSE 23.820498660789074 Test RE 0.046673828981705176\n",
      "1200 Train Loss 8.424962 Test MSE 24.500538685491847 Test RE 0.047335375078067954\n",
      "1300 Train Loss 807.0375 Test MSE 757.9265061311352 Test RE 0.26327617147575705\n",
      "1400 Train Loss 0.7389542 Test MSE 9.434859059175015 Test RE 0.02937417748411029\n",
      "1500 Train Loss 0.14760605 Test MSE 7.975601161752027 Test RE 0.027007209922560824\n",
      "1600 Train Loss 0.07681749 Test MSE 7.355798945081367 Test RE 0.02593659325026522\n",
      "1700 Train Loss 3.3690042 Test MSE 9.816995110628643 Test RE 0.02996313793202415\n",
      "1800 Train Loss 1.2461947 Test MSE 10.045167933802995 Test RE 0.030309348896278874\n",
      "1900 Train Loss 2.8537068 Test MSE 10.345818752359039 Test RE 0.03075958268146799\n",
      "2000 Train Loss 1.9440129 Test MSE 8.751393606630156 Test RE 0.02829023889347771\n",
      "2100 Train Loss 0.9966864 Test MSE 9.090298489899258 Test RE 0.028832817174021915\n",
      "2200 Train Loss 0.4059778 Test MSE 10.726645125369444 Test RE 0.0313205920301178\n",
      "2300 Train Loss 0.8036509 Test MSE 13.15143470480459 Test RE 0.03468043791254913\n",
      "2400 Train Loss 1.3887652 Test MSE 10.311368279393664 Test RE 0.030708326912062975\n",
      "2500 Train Loss 0.3873744 Test MSE 9.459761008056953 Test RE 0.029412916390663298\n",
      "2600 Train Loss 0.14396866 Test MSE 9.765119585887668 Test RE 0.029883866610721992\n",
      "2700 Train Loss 0.16544983 Test MSE 10.953106795599938 Test RE 0.03164948643391096\n",
      "2800 Train Loss 0.2612802 Test MSE 9.665206161921292 Test RE 0.029730592697596883\n",
      "2900 Train Loss 2.6939054 Test MSE 15.149505245155432 Test RE 0.03722178798653355\n",
      "Training time: 29.38\n",
      "6\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 12440.262 Test MSE 10829.835789603367 Test RE 0.9951967336595182\n",
      "100 Train Loss 1966.7281 Test MSE 1772.0192121321202 Test RE 0.40256148292534055\n",
      "200 Train Loss 506.49335 Test MSE 617.7419722967637 Test RE 0.2376848831674226\n",
      "300 Train Loss 128.96216 Test MSE 290.1236535867267 Test RE 0.1628881816266328\n",
      "400 Train Loss 90.09736 Test MSE 266.760568507352 Test RE 0.15619201342838088\n",
      "500 Train Loss 24.74133 Test MSE 198.15424803595857 Test RE 0.13461693548902168\n",
      "600 Train Loss 1451.0162 Test MSE 1366.542546220141 Test RE 0.3535164154763429\n",
      "700 Train Loss 10.63922 Test MSE 162.86589129786222 Test RE 0.12204305665988127\n",
      "800 Train Loss 10.036367 Test MSE 139.40768271232062 Test RE 0.11291232763964293\n",
      "900 Train Loss 0.8353965 Test MSE 138.37867689764852 Test RE 0.11249483759494094\n",
      "1000 Train Loss 4.3574605 Test MSE 120.35548537901984 Test RE 0.10491339728713824\n",
      "1100 Train Loss 0.41000047 Test MSE 108.40426744025393 Test RE 0.09956832372322785\n",
      "1200 Train Loss 1.340848 Test MSE 115.62984683531137 Test RE 0.10283311258192711\n",
      "1300 Train Loss 0.18789837 Test MSE 117.41765931290603 Test RE 0.10362504101742424\n",
      "1400 Train Loss 0.16003712 Test MSE 115.79089060657516 Test RE 0.1029046982084331\n",
      "1500 Train Loss 0.21688345 Test MSE 99.06833878471417 Test RE 0.09518432763793958\n",
      "1600 Train Loss 0.10858246 Test MSE 99.04282655263823 Test RE 0.0951720708410372\n",
      "1700 Train Loss 0.08387451 Test MSE 98.96425790045319 Test RE 0.09513431432132917\n",
      "1800 Train Loss 3.9396183 Test MSE 107.85743250465897 Test RE 0.09931687478754579\n",
      "1900 Train Loss 0.09588055 Test MSE 115.3438407364485 Test RE 0.10270585691488136\n",
      "2000 Train Loss 0.07504448 Test MSE 116.68287232267188 Test RE 0.10330029502411812\n",
      "2100 Train Loss 1.1787478 Test MSE 90.88448327193295 Test RE 0.09116809442003758\n",
      "2200 Train Loss 0.13177843 Test MSE 92.92666336917594 Test RE 0.09218668071113628\n",
      "2300 Train Loss 4.1750107 Test MSE 75.13778940089169 Test RE 0.08289478567651175\n",
      "2400 Train Loss 0.067997806 Test MSE 86.84770719186416 Test RE 0.08912041185948005\n",
      "2500 Train Loss 0.046445455 Test MSE 88.51407649472122 Test RE 0.08997133765616128\n",
      "2600 Train Loss 0.036254387 Test MSE 89.81265166562501 Test RE 0.09062891184992537\n",
      "2700 Train Loss 0.030206563 Test MSE 90.67533372561425 Test RE 0.09106313289332903\n",
      "2800 Train Loss 0.02615792 Test MSE 91.19918225622554 Test RE 0.09132579853224564\n",
      "2900 Train Loss 0.18725932 Test MSE 89.6974706492707 Test RE 0.09057077928762944\n",
      "Training time: 29.70\n",
      "7\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 12237.443 Test MSE 10784.904824346595 Test RE 0.9931301453061785\n",
      "100 Train Loss 2396.421 Test MSE 2381.5727434253054 Test RE 0.46669154382594324\n",
      "200 Train Loss 457.59802 Test MSE 749.6707024018381 Test RE 0.26183836202595945\n",
      "300 Train Loss 89.31641 Test MSE 352.8130111837705 Test RE 0.17962645648308162\n",
      "400 Train Loss 32.31001 Test MSE 265.442025636112 Test RE 0.15580552267947675\n",
      "500 Train Loss 34.624935 Test MSE 65.44977412765309 Test RE 0.07736634538276484\n",
      "600 Train Loss 5.534268 Test MSE 31.7500027572674 Test RE 0.053885248478378445\n",
      "700 Train Loss 2.4920666 Test MSE 33.70760585607027 Test RE 0.055521598304808496\n",
      "800 Train Loss 1.3005927 Test MSE 35.72311976016473 Test RE 0.057157430367642065\n",
      "900 Train Loss 0.99632734 Test MSE 36.22482558941053 Test RE 0.057557398690641026\n",
      "1000 Train Loss 1.167815 Test MSE 36.479194601003854 Test RE 0.05775912784236717\n",
      "1100 Train Loss 0.7693037 Test MSE 37.34221493689771 Test RE 0.058438363209156394\n",
      "1200 Train Loss 0.91374594 Test MSE 36.96118032381497 Test RE 0.05813945039379191\n",
      "1300 Train Loss 3.5280323 Test MSE 41.339340675890455 Test RE 0.06148650094397476\n",
      "1400 Train Loss 1.2304575 Test MSE 37.32659235171562 Test RE 0.058426137718307014\n",
      "1500 Train Loss 2.7249124 Test MSE 37.96146122610999 Test RE 0.058920912857241055\n",
      "1600 Train Loss 1.9249728 Test MSE 33.998592488110326 Test RE 0.05576073313407322\n",
      "1700 Train Loss 0.6226793 Test MSE 33.840302735043544 Test RE 0.055630777018361006\n",
      "1800 Train Loss 5.627383 Test MSE 41.17984078455212 Test RE 0.06136776939135693\n",
      "1900 Train Loss 0.7669365 Test MSE 34.2060833724012 Test RE 0.05593062612328554\n",
      "2000 Train Loss 0.2909698 Test MSE 33.23561620501228 Test RE 0.055131508250698576\n",
      "2100 Train Loss 1.9669391 Test MSE 30.54399629245197 Test RE 0.05285194024909496\n",
      "2200 Train Loss 7.1665335 Test MSE 46.95150569519761 Test RE 0.06552737464754936\n",
      "2300 Train Loss 0.24833156 Test MSE 31.76983665663228 Test RE 0.05390207663026649\n",
      "2400 Train Loss 0.53422123 Test MSE 30.879262832858803 Test RE 0.05314121358939979\n",
      "2500 Train Loss 1.8069763 Test MSE 30.941863380245078 Test RE 0.05319505206300113\n",
      "2600 Train Loss 15.068313 Test MSE 53.351487104727255 Test RE 0.06985078172695622\n",
      "2700 Train Loss 0.061955873 Test MSE 31.12844614646307 Test RE 0.05335519694759424\n",
      "2800 Train Loss 0.06279735 Test MSE 30.532303175926682 Test RE 0.05284182266250407\n",
      "2900 Train Loss 51.49856 Test MSE 208.83744010235608 Test RE 0.13819813642289175\n",
      "Training time: 29.16\n",
      "8\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 10985.497 Test MSE 10834.796774946231 Test RE 0.9954246499098197\n",
      "100 Train Loss 2107.8625 Test MSE 2579.8243564967474 Test RE 0.48572792866359477\n",
      "200 Train Loss 293.2887 Test MSE 729.5088647624959 Test RE 0.2582933904403037\n",
      "300 Train Loss 79.10871 Test MSE 462.7761241267887 Test RE 0.2057232981878532\n",
      "400 Train Loss 650.8151 Test MSE 696.7630922236174 Test RE 0.25242977204641537\n",
      "500 Train Loss 21.202608 Test MSE 259.2633774968587 Test RE 0.15398151680870723\n",
      "600 Train Loss 6.9333353 Test MSE 248.17097845685382 Test RE 0.15065151473045046\n",
      "700 Train Loss 22.877481 Test MSE 253.63890506416863 Test RE 0.15230211733264215\n",
      "800 Train Loss 1.8208381 Test MSE 232.5386942626376 Test RE 0.14582957863234722\n",
      "900 Train Loss 1.0949656 Test MSE 226.9621781030711 Test RE 0.14407039652474107\n",
      "1000 Train Loss 0.6831476 Test MSE 221.8917695416619 Test RE 0.14245201703196775\n",
      "1100 Train Loss 0.46115217 Test MSE 216.66924136998335 Test RE 0.14076563297096478\n",
      "1200 Train Loss 2.34016 Test MSE 217.5561992503854 Test RE 0.14105345802789981\n",
      "1300 Train Loss 0.632922 Test MSE 210.30918697443133 Test RE 0.1386842455901635\n",
      "1400 Train Loss 0.3148223 Test MSE 207.53325275140156 Test RE 0.1377659377309686\n",
      "1500 Train Loss 0.20865577 Test MSE 207.8444152163339 Test RE 0.1378691778912435\n",
      "1600 Train Loss 0.17973334 Test MSE 205.74537591195642 Test RE 0.13717123470414483\n",
      "1700 Train Loss 0.48486727 Test MSE 204.1376280756944 Test RE 0.1366342377503113\n",
      "1800 Train Loss 2.5879774 Test MSE 202.7837028118089 Test RE 0.1361803765333065\n",
      "1900 Train Loss 1.3825865 Test MSE 203.99921750222904 Test RE 0.1365879091285145\n",
      "2000 Train Loss 3.4116538 Test MSE 198.76943062954123 Test RE 0.13482573701755912\n",
      "2100 Train Loss 0.08114586 Test MSE 200.63815794992504 Test RE 0.13545803526145764\n",
      "2200 Train Loss 709.4575 Test MSE 1060.0958943191413 Test RE 0.3113655662006958\n",
      "2300 Train Loss 0.64788383 Test MSE 41.535502527079984 Test RE 0.06163220998634368\n",
      "2400 Train Loss 0.18374182 Test MSE 39.89297573108909 Test RE 0.06040129118805607\n",
      "2500 Train Loss 0.10899522 Test MSE 38.47696968107686 Test RE 0.059319630440616916\n",
      "2600 Train Loss 0.06694158 Test MSE 37.73492568952191 Test RE 0.058744844116293306\n",
      "2700 Train Loss 0.050868012 Test MSE 37.63799921763444 Test RE 0.05866934918077905\n",
      "2800 Train Loss 0.041423645 Test MSE 37.587568521684695 Test RE 0.058630030833234495\n",
      "2900 Train Loss 0.03597685 Test MSE 37.72390876089938 Test RE 0.058736268042706465\n",
      "Training time: 29.27\n",
      "9\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 11161.517 Test MSE 10860.493202137732 Test RE 0.996604354036147\n",
      "100 Train Loss 1892.0038 Test MSE 1944.3040071641226 Test RE 0.42167717315619063\n",
      "200 Train Loss 335.02783 Test MSE 488.0735960474727 Test RE 0.21127137704847757\n",
      "300 Train Loss 103.82404 Test MSE 157.6628804191419 Test RE 0.12007780334070811\n",
      "400 Train Loss 34.110977 Test MSE 87.90978306252049 Test RE 0.08966369053180993\n",
      "500 Train Loss 19.79917 Test MSE 75.21444377385184 Test RE 0.08293705886600104\n",
      "600 Train Loss 8.495558 Test MSE 67.77072798289751 Test RE 0.07872616236424028\n",
      "700 Train Loss 4.3521914 Test MSE 53.10485642573852 Test RE 0.06968914329616166\n",
      "800 Train Loss 112.53248 Test MSE 117.04391923226238 Test RE 0.10345999046625952\n",
      "900 Train Loss 2.2284591 Test MSE 47.291195999602344 Test RE 0.06576399004423615\n",
      "1000 Train Loss 1.2987835 Test MSE 46.5291991169274 Test RE 0.06523201512748039\n",
      "1100 Train Loss 5.9095144 Test MSE 52.105170335588745 Test RE 0.06903008627132194\n",
      "1200 Train Loss 4.1961865 Test MSE 51.733241480202636 Test RE 0.06878327524394104\n",
      "1300 Train Loss 2.712469 Test MSE 49.18344726117236 Test RE 0.06706678512100593\n",
      "1400 Train Loss 0.809038 Test MSE 48.29702515623014 Test RE 0.06645967249850705\n",
      "1500 Train Loss 11.720951 Test MSE 63.87539049522976 Test RE 0.07643016378438872\n",
      "1600 Train Loss 1.3448446 Test MSE 49.62959362204993 Test RE 0.06737028207566251\n",
      "1700 Train Loss 0.8653704 Test MSE 50.97428214248956 Test RE 0.06827686398548927\n",
      "1800 Train Loss 3.7742724 Test MSE 49.90814541365821 Test RE 0.06755907925595211\n",
      "1900 Train Loss 9.669377 Test MSE 68.61138135563851 Test RE 0.07921293183143141\n",
      "2000 Train Loss 2.6375186 Test MSE 55.479783379616784 Test RE 0.0712304002218222\n",
      "2100 Train Loss 3.1696124 Test MSE 48.439218761128004 Test RE 0.06655743416135333\n",
      "2200 Train Loss 1.9622382 Test MSE 51.32684351204951 Test RE 0.06851257407427684\n",
      "2300 Train Loss 0.71264255 Test MSE 51.34365903909676 Test RE 0.06852379608423613\n",
      "2400 Train Loss 1.3874803 Test MSE 53.88193553686381 Test RE 0.07019716939566197\n",
      "2500 Train Loss 0.4509137 Test MSE 51.683372824445556 Test RE 0.06875011516735981\n",
      "2600 Train Loss 1.565584 Test MSE 49.797539512566445 Test RE 0.06748417587643782\n",
      "2700 Train Loss 44.25418 Test MSE 87.70435950353486 Test RE 0.08955886825516238\n",
      "2800 Train Loss 15.061073 Test MSE 64.25965386489008 Test RE 0.07665971443160353\n",
      "2900 Train Loss 5.8075824 Test MSE 53.61413270022769 Test RE 0.07002250586764215\n",
      "Training time: 29.49\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 3000\n",
    "layers = np.array([1,50,50,50,50,1])\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "alpha_full = []\n",
    "omega_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "n_val = 1\n",
    "rowdy_terms = 2\n",
    "\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(reps)\n",
    "    \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []\n",
    "    alpha_val = []\n",
    "    omega_val = []\n",
    "    'Generate Training data'\n",
    "    N_train = 300\n",
    "    \n",
    "    x_train_np_array, u_train_np_array = trainingdata(N_train,reps*1234)\n",
    "\n",
    "    'Convert to tensor and send to GPU'\n",
    "    x_train = torch.from_numpy(x_train_np_array).float().to(device)\n",
    "    u_train = torch.from_numpy(u_train_np_array).float().to(device)\n",
    "    \n",
    "    x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "       \n",
    "     \n",
    "    model_NN = Sequentialmodel(layers,n_val,rowdy_terms)\n",
    "    model_NN.to(device)\n",
    "    print(model_NN)\n",
    "    params = list(model_NN.parameters())\n",
    "    #optimizer = optim.Adam(PINN.parameters(), lr=0.0008,betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "    optimizer = optim.Adam(model_NN.parameters(), lr=8.0e-3)\n",
    "\n",
    "    train_model(max_iter,reps)\n",
    " \n",
    "    torch.save(model_NN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    alpha_full.append(alpha_val)\n",
    "    omega_full.append(omega_val)\n",
    "                 \n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse\": test_mse_full, \"test_re\": test_re_full,\"Time\": elapsed_time, \"Thresh_time\": time_threshold, \"epoch_thresh\": epoch_threshold,\"alpha\": alpha_full,\"omega\": omega_full, \"label\": label}\n",
    "savemat(label+'.mat', mdic) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred = model_NN.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f542c58e3d0>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_test,u_pred,'rx')\n",
    "plt.plot(x_test,u_true,'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0   0.03835920735401453\n",
      "1   0.020838700249601827\n",
      "2   0.028574128545334698\n",
      "3   0.04805135956361336\n",
      "4   0.03441976506828645\n",
      "5   0.03722178798653355\n",
      "6   0.09057077928762944\n",
      "7   0.13819813642289175\n",
      "8   0.058736268042706465\n",
      "9   0.07002250586764215\n",
      "0.056499263838825434\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    print(i,\" \",test_re_full[i][-1])\n",
    "    a = a + test_re_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
