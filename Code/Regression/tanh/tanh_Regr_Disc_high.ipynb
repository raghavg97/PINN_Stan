{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:2\n"
     ]
    }
   ],
   "source": [
    "#Trying to replicate the results of \"Adaptive activation functions accelerate convergence in deep and physics-informed neural networks\"\n",
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_disc(x): #[-4,3.75]\n",
    "    y  = np.zeros((np.shape(x)[0],))\n",
    "    \n",
    "    y = 0.2*np.sin(6*x)*np.exp(-1*x)\n",
    "    y[x>0] = 1 + 0.1*x[x>0]*np.exp(x[x>0]) + 10\n",
    "    \n",
    "    return scale*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain bounds\n",
    "level= \"high\"\n",
    "label = \"Regr_disc_tanh_\" + level\n",
    "\n",
    "scale = 50.0\n",
    "loss_thresh = 0.1\n",
    "\n",
    "lb = np.array(-4.0)  # [-1. 0.]\n",
    "ub = np.array(3.75) # [1.  0.99]\n",
    "\n",
    "x_test = np.linspace(lb,ub,1000).reshape(-1,1)\n",
    "u_true = true_disc(x_test)\n",
    "\n",
    "u_true_norm = np.linalg.norm(u_true,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_train,seed):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    x_train = np.random.uniform(low=lb, high=ub, size=(N_train,)).reshape(-1,1)\n",
    "\n",
    "    u_train = true_disc(x_train)\n",
    "\n",
    "    return x_train, u_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "     \n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "\n",
    "            \n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = 2.0*(x - l_b)/(u_b - l_b)-1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z) \n",
    "    \n",
    "        a = self.linears[-1](a) \n",
    "        \n",
    "        return a\n",
    "    \n",
    "    \n",
    "    def loss(self,x,u):\n",
    "\n",
    "        loss_val = self.loss_function(self.forward(x),u)\n",
    "        \n",
    "        return loss_val\n",
    "        \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        u_pred = self.forward(x_test_tensor)       \n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "        \n",
    "        return u_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "        \n",
    "        test_mse_loss = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re_loss = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "        \n",
    "        return test_mse_loss, test_re_loss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    test_mse, test_re = model_NN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step():\n",
    "    optimizer.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "    loss = model_NN.loss(x_train,u_train)\n",
    "    loss.backward() #backprop\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "   \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "    \n",
    "    loss_np = model_NN.loss(x_train,u_train).cpu().detach().numpy()\n",
    "    data_update(loss_np)\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        loss_np = train_step().cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1\n",
    "        data_update(loss_np)\n",
    "        if(i%100==0):        \n",
    "            print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 290473.06 Test MSE 272569.77568804275 Test RE 0.9985431809349766\n",
      "100 Train Loss 258424.44 Test MSE 242625.80491972435 Test RE 0.9420988993795767\n",
      "200 Train Loss 236092.53 Test MSE 221617.59285753864 Test RE 0.9003888712868896\n",
      "300 Train Loss 205897.0 Test MSE 193945.18419485076 Test RE 0.8423013600417114\n",
      "400 Train Loss 182987.17 Test MSE 172425.39017752322 Test RE 0.7941976735354956\n",
      "500 Train Loss 162996.72 Test MSE 153707.63044907842 Test RE 0.7498522787566474\n",
      "600 Train Loss 145385.34 Test MSE 137305.72052110033 Test RE 0.708716130627221\n",
      "700 Train Loss 129446.55 Test MSE 121999.06987025314 Test RE 0.6680458364488265\n",
      "800 Train Loss 115262.414 Test MSE 108648.17684837944 Test RE 0.6304334139716035\n",
      "900 Train Loss 102874.086 Test MSE 97080.01388577532 Test RE 0.5959267996971604\n",
      "1000 Train Loss 91944.4 Test MSE 86747.58212839744 Test RE 0.5633219723591739\n",
      "1100 Train Loss 82416.48 Test MSE 77875.35013329305 Test RE 0.5337378575010888\n",
      "1200 Train Loss 74077.055 Test MSE 69926.5951460671 Test RE 0.5057654902285463\n",
      "1300 Train Loss 76850.49 Test MSE 75927.82449428563 Test RE 0.5270216788067567\n",
      "1400 Train Loss 60737.48 Test MSE 57463.19771306488 Test RE 0.45848264066503885\n",
      "1500 Train Loss 55403.395 Test MSE 52270.010168439476 Test RE 0.4372746432334595\n",
      "1600 Train Loss 50912.72 Test MSE 48082.48775427344 Test RE 0.4193932792699558\n",
      "1700 Train Loss 46975.516 Test MSE 44436.33045671376 Test RE 0.4031782502486207\n",
      "1800 Train Loss 43903.85 Test MSE 41149.19517976344 Test RE 0.38797940667630626\n",
      "1900 Train Loss 41328.3 Test MSE 38737.76512562484 Test RE 0.37643958211808\n",
      "2000 Train Loss 38619.625 Test MSE 36354.228969220756 Test RE 0.3646745627733806\n",
      "2100 Train Loss 36916.727 Test MSE 34695.82747117165 Test RE 0.3562596415595976\n",
      "2200 Train Loss 34383.523 Test MSE 32547.328800702482 Test RE 0.345052896570844\n",
      "2300 Train Loss 32313.512 Test MSE 31017.538887547165 Test RE 0.33684621328866565\n",
      "2400 Train Loss 31442.004 Test MSE 30114.522097856952 Test RE 0.3319066772696287\n",
      "2500 Train Loss 30700.371 Test MSE 29326.837923112857 Test RE 0.32753719190665703\n",
      "2600 Train Loss 30159.59 Test MSE 28805.966452040808 Test RE 0.3246154806324222\n",
      "2700 Train Loss 29034.602 Test MSE 27744.202278377805 Test RE 0.31857678335824463\n",
      "2800 Train Loss 28216.154 Test MSE 26823.9807634584 Test RE 0.31324894450968266\n",
      "2900 Train Loss 27955.324 Test MSE 26044.33356435757 Test RE 0.3086630379428162\n",
      "Training time: 6.15\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 292808.28 Test MSE 272729.19670090696 Test RE 0.9988351529539871\n",
      "100 Train Loss 259977.55 Test MSE 243150.18448006638 Test RE 0.9431164143318475\n",
      "200 Train Loss 236611.98 Test MSE 222059.2810323723 Test RE 0.9012856709447022\n",
      "300 Train Loss 207240.36 Test MSE 195254.25323766636 Test RE 0.845139214175681\n",
      "400 Train Loss 183419.12 Test MSE 173541.0275646192 Test RE 0.7967628642311418\n",
      "500 Train Loss 162420.28 Test MSE 154310.46930534198 Test RE 0.7513212940954236\n",
      "600 Train Loss 144253.5 Test MSE 138630.93472180923 Test RE 0.7121280254850162\n",
      "700 Train Loss 127823.766 Test MSE 122729.82297789608 Test RE 0.6700435882745301\n",
      "800 Train Loss 113679.9 Test MSE 110035.73343100275 Test RE 0.6344463060141958\n",
      "900 Train Loss 100753.36 Test MSE 97841.47382223149 Test RE 0.5982593500734055\n",
      "1000 Train Loss 89609.94 Test MSE 87576.71670662463 Test RE 0.5660076896543791\n",
      "1100 Train Loss 79860.29 Test MSE 78432.23254671897 Test RE 0.5356428232548764\n",
      "1200 Train Loss 71391.336 Test MSE 70593.91435458287 Test RE 0.5081730550915177\n",
      "1300 Train Loss 64097.902 Test MSE 63808.49827360491 Test RE 0.4831336241310549\n",
      "1400 Train Loss 57857.27 Test MSE 57991.80149660095 Test RE 0.4605866033736893\n",
      "1500 Train Loss 52583.176 Test MSE 52881.67143788581 Test RE 0.43982568583618337\n",
      "1600 Train Loss 47659.47 Test MSE 48463.63224408617 Test RE 0.4210522399135231\n",
      "1700 Train Loss 43800.547 Test MSE 44496.03477874913 Test RE 0.40344901297479396\n",
      "1800 Train Loss 40269.906 Test MSE 41044.64924665667 Test RE 0.3874862322046342\n",
      "1900 Train Loss 37674.33 Test MSE 38638.411641318125 Test RE 0.37595653158876213\n",
      "2000 Train Loss 35579.797 Test MSE 36610.886299267615 Test RE 0.36595958225340114\n",
      "2100 Train Loss 33905.11 Test MSE 34979.31699546612 Test RE 0.35771212763940957\n",
      "2200 Train Loss 34609.848 Test MSE 35989.34987258304 Test RE 0.36283986989894723\n",
      "2300 Train Loss 33517.258 Test MSE 34715.34482960239 Test RE 0.35635983040148633\n",
      "2400 Train Loss 32675.635 Test MSE 33761.595085855784 Test RE 0.3514305241864271\n",
      "2500 Train Loss 31418.895 Test MSE 33775.76695813499 Test RE 0.3515042752333007\n",
      "2600 Train Loss 29398.184 Test MSE 31078.83744432653 Test RE 0.33717889595821904\n",
      "2700 Train Loss 27703.1 Test MSE 29559.783586087287 Test RE 0.3288354473836134\n",
      "2800 Train Loss 26252.162 Test MSE 28098.83904073903 Test RE 0.3206064018706175\n",
      "2900 Train Loss 24904.484 Test MSE 26765.05986063238 Test RE 0.3129047178747596\n",
      "Training time: 6.15\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 279550.8 Test MSE 272751.164921508 Test RE 0.9988753800169492\n",
      "100 Train Loss 249302.03 Test MSE 243356.41714952208 Test RE 0.9435162910398791\n",
      "200 Train Loss 227841.36 Test MSE 222346.0427440531 Test RE 0.9018674318916934\n",
      "300 Train Loss 201284.94 Test MSE 195737.94175719563 Test RE 0.8461853662546901\n",
      "400 Train Loss 179338.4 Test MSE 173999.26597695143 Test RE 0.7978141045472591\n",
      "500 Train Loss 160256.67 Test MSE 155257.6489202101 Test RE 0.7536236249898165\n",
      "600 Train Loss 144291.8 Test MSE 139355.2578715646 Test RE 0.7139859759267922\n",
      "700 Train Loss 127646.22 Test MSE 123393.75816606093 Test RE 0.671853521167839\n",
      "800 Train Loss 113675.51 Test MSE 110021.25267820983 Test RE 0.6344045579287643\n",
      "900 Train Loss 101244.695 Test MSE 98135.21511272131 Test RE 0.5991567291030484\n",
      "1000 Train Loss 90244.266 Test MSE 87478.29715018648 Test RE 0.5656895578069526\n",
      "1100 Train Loss 80596.03 Test MSE 78130.67494876326 Test RE 0.5346121071978998\n",
      "1200 Train Loss 72178.9 Test MSE 70042.13349651599 Test RE 0.5061831510082003\n",
      "1300 Train Loss 64888.023 Test MSE 63066.43974816709 Test RE 0.4803161165159954\n",
      "1400 Train Loss 58583.227 Test MSE 57053.21200178778 Test RE 0.45684413269193314\n",
      "1500 Train Loss 53103.19 Test MSE 51888.112388292444 Test RE 0.4356742957318686\n",
      "1600 Train Loss 48453.207 Test MSE 47469.730600134 Test RE 0.41671236306440457\n",
      "1700 Train Loss 44571.125 Test MSE 43871.558675576955 Test RE 0.4006079227553099\n",
      "1800 Train Loss 41319.65 Test MSE 40936.087680842254 Test RE 0.38697344959419255\n",
      "1900 Train Loss 38406.082 Test MSE 38298.001862732184 Test RE 0.37429675312460103\n",
      "2000 Train Loss 36172.0 Test MSE 36334.07271532328 Test RE 0.3645734535982166\n",
      "2100 Train Loss 34360.047 Test MSE 34685.90805686005 Test RE 0.3562087112336438\n",
      "2200 Train Loss 32060.348 Test MSE 32516.015717351078 Test RE 0.34488687265282864\n",
      "2300 Train Loss 30107.137 Test MSE 30698.651159949502 Test RE 0.3351102013591832\n",
      "2400 Train Loss 28295.607 Test MSE 29065.212902783558 Test RE 0.3260729376184818\n",
      "2500 Train Loss 26684.555 Test MSE 27633.41573218615 Test RE 0.31794008591052225\n",
      "2600 Train Loss 25182.365 Test MSE 26239.596963241016 Test RE 0.3098179541910795\n",
      "2700 Train Loss 23822.34 Test MSE 25022.337421796925 Test RE 0.30254636537139906\n",
      "2800 Train Loss 22533.27 Test MSE 23620.262068642824 Test RE 0.2939478972015012\n",
      "2900 Train Loss 21364.604 Test MSE 22625.088441324304 Test RE 0.28768892578051647\n",
      "Training time: 6.15\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 305647.25 Test MSE 272657.01156425173 Test RE 0.9987029598767739\n",
      "100 Train Loss 270758.97 Test MSE 242393.67878371762 Test RE 0.9416481268249288\n",
      "200 Train Loss 246451.36 Test MSE 221339.0434722409 Test RE 0.8998228475769579\n",
      "300 Train Loss 217903.16 Test MSE 194280.51011871966 Test RE 0.8430292035898793\n",
      "400 Train Loss 194343.27 Test MSE 173212.1912040542 Test RE 0.7960076283149441\n",
      "500 Train Loss 173310.9 Test MSE 154507.03783144354 Test RE 0.7517996774399983\n",
      "600 Train Loss 154486.7 Test MSE 137774.81541600355 Test RE 0.7099257366416484\n",
      "700 Train Loss 137650.88 Test MSE 122847.26514828322 Test RE 0.6703640994391719\n",
      "800 Train Loss 122627.2 Test MSE 109676.20243935761 Test RE 0.6334089623550028\n",
      "900 Train Loss 109270.984 Test MSE 98020.04333728975 Test RE 0.5988050398421796\n",
      "1000 Train Loss 97441.38 Test MSE 87720.14866153942 Test RE 0.5664709998772534\n",
      "1100 Train Loss 87017.73 Test MSE 78291.64515357015 Test RE 0.5351625461970521\n",
      "1200 Train Loss 77867.88 Test MSE 70843.2765706602 Test RE 0.5090697857340644\n",
      "1300 Train Loss 69893.38 Test MSE 64071.75880926598 Test RE 0.48412925245797883\n",
      "1400 Train Loss 62984.496 Test MSE 58258.43258677573 Test RE 0.4616442172978094\n",
      "1500 Train Loss 57040.543 Test MSE 53310.17718018418 Test RE 0.44160406738389996\n",
      "1600 Train Loss 51965.652 Test MSE 49134.915140085286 Test RE 0.423958265788823\n",
      "1700 Train Loss 47668.586 Test MSE 45643.48353075586 Test RE 0.40861790527118824\n",
      "1800 Train Loss 44062.83 Test MSE 42744.47429744746 Test RE 0.3954285223985789\n",
      "1900 Train Loss 41064.934 Test MSE 40191.423547400955 Test RE 0.38343759873772026\n",
      "2000 Train Loss 38671.867 Test MSE 41838.292706795444 Test RE 0.39121453215046675\n",
      "2100 Train Loss 43543.234 Test MSE 41376.57540504994 Test RE 0.3890498688227604\n",
      "2200 Train Loss 40135.613 Test MSE 39157.359030367785 Test RE 0.3784728219683852\n",
      "2300 Train Loss 37407.406 Test MSE 37001.332423348955 Test RE 0.367905841519943\n",
      "2400 Train Loss 35118.902 Test MSE 35642.13656822036 Test RE 0.3610853485553697\n",
      "2500 Train Loss 32694.607 Test MSE 33797.48139770817 Test RE 0.3516172481226998\n",
      "2600 Train Loss 30439.14 Test MSE 31926.155099522828 Test RE 0.3417443249925515\n",
      "2700 Train Loss 28854.807 Test MSE 30717.871055411153 Test RE 0.3352150883003781\n",
      "2800 Train Loss 27260.182 Test MSE 28557.177854924666 Test RE 0.3232106368879892\n",
      "2900 Train Loss 25470.418 Test MSE 26790.831670394913 Test RE 0.31305532804756414\n",
      "Training time: 6.09\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 258160.55 Test MSE 272206.69566592237 Test RE 0.9978778982656551\n",
      "100 Train Loss 227269.45 Test MSE 241844.19298490012 Test RE 0.9405802032580732\n",
      "200 Train Loss 206541.61 Test MSE 221037.94465097052 Test RE 0.8992106017681752\n",
      "300 Train Loss 181305.22 Test MSE 193803.157936342 Test RE 0.841992894481412\n",
      "400 Train Loss 162453.53 Test MSE 174230.93852466877 Test RE 0.798345055474694\n",
      "500 Train Loss 143252.03 Test MSE 154363.07753452152 Test RE 0.7514493551231693\n",
      "600 Train Loss 127670.58 Test MSE 138130.25806192512 Test RE 0.7108409087674241\n",
      "700 Train Loss 113654.73 Test MSE 124104.41613242491 Test RE 0.6737854364615532\n",
      "800 Train Loss 101485.11 Test MSE 111117.43413455742 Test RE 0.6375571256590536\n",
      "900 Train Loss 90693.55 Test MSE 99821.9159257576 Test RE 0.6042838011944004\n",
      "1000 Train Loss 81258.125 Test MSE 89870.60948851268 Test RE 0.5733724812423393\n",
      "1100 Train Loss 73057.82 Test MSE 81215.82611417855 Test RE 0.5450650491646447\n",
      "1200 Train Loss 65976.625 Test MSE 73717.32613841257 Test RE 0.5192933832490034\n",
      "1300 Train Loss 59906.92 Test MSE 67273.7258593375 Test RE 0.4960788852136134\n",
      "1400 Train Loss 54741.62 Test MSE 61719.96749557507 Test RE 0.47516106401767594\n",
      "1500 Train Loss 50354.61 Test MSE 57132.194247410465 Test RE 0.45716024194364885\n",
      "1600 Train Loss 44394.8 Test MSE 50078.834595153494 Test RE 0.4280111754497279\n",
      "1700 Train Loss 41037.14 Test MSE 46485.46962565354 Test RE 0.41236957319033113\n",
      "1800 Train Loss 38985.008 Test MSE 43253.567294161825 Test RE 0.3977763583503577\n",
      "1900 Train Loss 36374.836 Test MSE 40768.044274919615 Test RE 0.3861783661712062\n",
      "2000 Train Loss 34284.28 Test MSE 38571.780866797584 Test RE 0.37563222887089903\n",
      "2100 Train Loss 32231.22 Test MSE 36636.57434105844 Test RE 0.36608794757796675\n",
      "2200 Train Loss 33443.45 Test MSE 37176.78946858163 Test RE 0.3687770983636186\n",
      "2300 Train Loss 29646.797 Test MSE 33544.84268896304 Test RE 0.3503006003440461\n",
      "2400 Train Loss 28254.758 Test MSE 31904.194984225654 Test RE 0.3416267719123697\n",
      "2500 Train Loss 27067.545 Test MSE 30510.0563724418 Test RE 0.334079253739849\n",
      "2600 Train Loss 25805.695 Test MSE 29068.50351347176 Test RE 0.3260913952284382\n",
      "2700 Train Loss 24675.191 Test MSE 27757.8881942624 Test RE 0.3186553489169432\n",
      "2800 Train Loss 23663.871 Test MSE 26582.41946716129 Test RE 0.31183528501929997\n",
      "2900 Train Loss 22794.844 Test MSE 25554.467032287987 Test RE 0.30574644471511064\n",
      "Training time: 6.10\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 263929.72 Test MSE 272786.459742137 Test RE 0.9989400066646896\n",
      "100 Train Loss 235052.78 Test MSE 243088.41567116755 Test RE 0.9429966141394613\n",
      "200 Train Loss 215163.94 Test MSE 222261.6970772516 Test RE 0.9016963566065558\n",
      "300 Train Loss 190195.08 Test MSE 195782.54585130236 Test RE 0.8462817736793542\n",
      "400 Train Loss 169017.1 Test MSE 173661.4496262095 Test RE 0.7970392576264396\n",
      "500 Train Loss 150316.64 Test MSE 154178.55460862158 Test RE 0.7510000861059286\n",
      "600 Train Loss 133634.4 Test MSE 136875.62735469828 Test RE 0.7076052772843195\n",
      "700 Train Loss 119108.87 Test MSE 122309.35828344345 Test RE 0.6688948398405665\n",
      "800 Train Loss 106038.06 Test MSE 109029.01043261231 Test RE 0.6315373450291364\n",
      "900 Train Loss 94437.49 Test MSE 97109.20645310455 Test RE 0.5960163924202384\n",
      "1000 Train Loss 84223.19 Test MSE 86703.28933015958 Test RE 0.5631781395765343\n",
      "1100 Train Loss 75322.234 Test MSE 77680.8943739165 Test RE 0.5330710658174969\n",
      "1200 Train Loss 67564.17 Test MSE 69945.76267152582 Test RE 0.5058348029747086\n",
      "1300 Train Loss 60827.027 Test MSE 63197.004833607134 Test RE 0.4808130534804577\n",
      "1400 Train Loss 55025.78 Test MSE 57425.710756014174 Test RE 0.4583330673405024\n",
      "1500 Train Loss 50016.48 Test MSE 52407.27671480565 Test RE 0.43784843138424323\n",
      "1600 Train Loss 48993.43 Test MSE 50796.8559763614 Test RE 0.43106862902146315\n",
      "1700 Train Loss 42218.31 Test MSE 45420.067422116095 Test RE 0.40761662531302134\n",
      "1800 Train Loss 34826.254 Test MSE 37051.219800774554 Test RE 0.3681537741496635\n",
      "1900 Train Loss 31826.254 Test MSE 33185.10403368557 Test RE 0.34841720583647406\n",
      "2000 Train Loss 29316.508 Test MSE 30363.6062740468 Test RE 0.3332764890353428\n",
      "2100 Train Loss 27062.67 Test MSE 28269.106403156173 Test RE 0.32157630599268733\n",
      "2200 Train Loss 25072.46 Test MSE 26106.333487817505 Test RE 0.30903021395451696\n",
      "2300 Train Loss 22255.129 Test MSE 22715.891153859484 Test RE 0.28826564788147474\n",
      "2400 Train Loss 20332.361 Test MSE 21102.801049037575 Test RE 0.27784210082438615\n",
      "2500 Train Loss 18780.719 Test MSE 19592.652895307434 Test RE 0.26771618284448856\n",
      "2600 Train Loss 17480.055 Test MSE 18343.196210586964 Test RE 0.25903921041152345\n",
      "2700 Train Loss 16377.466 Test MSE 17283.356121564353 Test RE 0.25144444249234965\n",
      "2800 Train Loss 15051.915 Test MSE 15895.085445824034 Test RE 0.2411345468208609\n",
      "2900 Train Loss 13885.838 Test MSE 14897.318843480274 Test RE 0.23344364640471335\n",
      "Training time: 7.10\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 310754.5 Test MSE 272781.69480505015 Test RE 0.998931282061797\n",
      "100 Train Loss 276776.16 Test MSE 242544.5896205761 Test RE 0.9419412095412776\n",
      "200 Train Loss 253062.36 Test MSE 221497.1327122633 Test RE 0.9001441350380058\n",
      "300 Train Loss 223113.66 Test MSE 194228.95514893444 Test RE 0.842917341548736\n",
      "400 Train Loss 199811.1 Test MSE 173060.09168609907 Test RE 0.7956580600595756\n",
      "500 Train Loss 177779.8 Test MSE 153566.92675195713 Test RE 0.7495089934431814\n",
      "600 Train Loss 158423.72 Test MSE 136248.33890193867 Test RE 0.7059819702218961\n",
      "700 Train Loss 144218.22 Test MSE 124499.16352910659 Test RE 0.6748561633933452\n",
      "800 Train Loss 130231.15 Test MSE 112306.19643538856 Test RE 0.6409584262330753\n",
      "900 Train Loss 117983.375 Test MSE 101687.33273992054 Test RE 0.6099039271834557\n",
      "1000 Train Loss 109071.414 Test MSE 93032.03122576134 Test RE 0.5833702174087138\n",
      "1100 Train Loss 98107.69 Test MSE 84271.25829470762 Test RE 0.5552233742853642\n",
      "1200 Train Loss 89775.2 Test MSE 77354.35727238527 Test RE 0.531949485061669\n",
      "1300 Train Loss 82911.164 Test MSE 71678.50909998202 Test RE 0.5120619238818499\n",
      "1400 Train Loss 77050.24 Test MSE 66915.98790814093 Test RE 0.4947581407709495\n",
      "1500 Train Loss 72160.57 Test MSE 62979.01354869288 Test RE 0.47998308062669526\n",
      "1600 Train Loss 68445.59 Test MSE 60958.7779388875 Test RE 0.47222190385735746\n",
      "1700 Train Loss 64589.727 Test MSE 57049.30968411961 Test RE 0.4568285088440097\n",
      "1800 Train Loss 61641.656 Test MSE 54070.92187710019 Test RE 0.4447437860865558\n",
      "1900 Train Loss 59402.14 Test MSE 52137.71487697928 Test RE 0.4367209220198211\n",
      "2000 Train Loss 57483.406 Test MSE 50454.17792231557 Test RE 0.4296121635791179\n",
      "2100 Train Loss 56074.65 Test MSE 49406.568588023016 Test RE 0.4251286247631346\n",
      "2200 Train Loss 54461.28 Test MSE 48018.02975330974 Test RE 0.41911207170624043\n",
      "2300 Train Loss 53407.664 Test MSE 47125.000929769245 Test RE 0.41519650368749034\n",
      "2400 Train Loss 52756.586 Test MSE 46586.70321711791 Test RE 0.4128183472920811\n",
      "2500 Train Loss 52274.273 Test MSE 46198.8255359439 Test RE 0.4110962066352094\n",
      "2600 Train Loss 50913.543 Test MSE 44958.50278084817 Test RE 0.4055402100220405\n",
      "2700 Train Loss 50648.77 Test MSE 44827.511920486366 Test RE 0.40494898913782\n",
      "2800 Train Loss 50461.176 Test MSE 44729.4629539531 Test RE 0.4045058844120241\n",
      "2900 Train Loss 50327.016 Test MSE 44646.45480997058 Test RE 0.40413037269134294\n",
      "Training time: 8.18\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 306037.84 Test MSE 272689.36256038205 Test RE 0.998762206615172\n",
      "100 Train Loss 271515.03 Test MSE 242096.2147948158 Test RE 0.9410701571387415\n",
      "200 Train Loss 247698.33 Test MSE 221082.6887915038 Test RE 0.8993016096032043\n",
      "300 Train Loss 219954.48 Test MSE 194418.59756037046 Test RE 0.8433287474503018\n",
      "400 Train Loss 196483.14 Test MSE 173121.39687003978 Test RE 0.7957989754035252\n",
      "500 Train Loss 175523.97 Test MSE 154209.4991632729 Test RE 0.7510754474209765\n",
      "600 Train Loss 156929.53 Test MSE 137487.44363628796 Test RE 0.7091849658568786\n",
      "700 Train Loss 140272.7 Test MSE 122537.45453925886 Test RE 0.6695182645046498\n",
      "800 Train Loss 125428.88 Test MSE 109274.11704779127 Test RE 0.632246821711001\n",
      "900 Train Loss 112240.18 Test MSE 97547.54836437615 Test RE 0.5973600590663359\n",
      "1000 Train Loss 100566.55 Test MSE 87224.75814297744 Test RE 0.5648691918052986\n",
      "1100 Train Loss 90280.375 Test MSE 78184.13975630743 Test RE 0.5347949934028959\n",
      "1200 Train Loss 81262.98 Test MSE 70312.26921406743 Test RE 0.507158325192333\n",
      "1300 Train Loss 73402.95 Test MSE 63502.01103048264 Test RE 0.4819719251899256\n",
      "1400 Train Loss 66594.87 Test MSE 57650.85401405193 Test RE 0.4592306588043343\n",
      "1500 Train Loss 60686.355 Test MSE 52543.10314954211 Test RE 0.43841546053801983\n",
      "1600 Train Loss 55739.613 Test MSE 48398.84532060135 Test RE 0.42077071126281856\n",
      "1700 Train Loss 51491.406 Test MSE 44526.1431944526 Test RE 0.40358548756729123\n",
      "1800 Train Loss 48012.484 Test MSE 41575.92437346564 Test RE 0.38998594797091835\n",
      "1900 Train Loss 45024.016 Test MSE 39407.46614601705 Test RE 0.37967959480964475\n",
      "2000 Train Loss 42602.223 Test MSE 38055.97622601498 Test RE 0.3731121876070023\n",
      "2100 Train Loss 39968.42 Test MSE 35456.05496681184 Test RE 0.36014153212440214\n",
      "2200 Train Loss 37302.305 Test MSE 33591.683638304734 Test RE 0.35054508932222944\n",
      "2300 Train Loss 34975.63 Test MSE 31714.213791277365 Test RE 0.34060810383325\n",
      "2400 Train Loss 33106.168 Test MSE 30314.304833773727 Test RE 0.3330058083073055\n",
      "2500 Train Loss 30792.725 Test MSE 29402.083707124217 Test RE 0.3279571145093034\n",
      "2600 Train Loss 29327.814 Test MSE 28595.668632558027 Test RE 0.3234283831865787\n",
      "2700 Train Loss 28158.447 Test MSE 27660.67766502352 Test RE 0.3180968802174825\n",
      "2800 Train Loss 27194.984 Test MSE 26890.980404717975 Test RE 0.3136399095569909\n",
      "2900 Train Loss 26398.668 Test MSE 26289.895025015787 Test RE 0.3101147533755786\n",
      "Training time: 58.43\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 274257.62 Test MSE 272809.3472585632 Test RE 0.9989819126600987\n",
      "100 Train Loss 242579.55 Test MSE 243299.20490122776 Test RE 0.9434053758193435\n",
      "200 Train Loss 220022.16 Test MSE 222192.82688335556 Test RE 0.9015566455923175\n",
      "300 Train Loss 193288.89 Test MSE 195601.31730885655 Test RE 0.8458899973838269\n",
      "400 Train Loss 171146.89 Test MSE 174438.03335655457 Test RE 0.7988193800577768\n",
      "500 Train Loss 151504.52 Test MSE 155633.29677443753 Test RE 0.7545347751559408\n",
      "600 Train Loss 134026.75 Test MSE 138892.3908578722 Test RE 0.7127992412453826\n",
      "700 Train Loss 118356.54 Test MSE 123821.84508066956 Test RE 0.6730179345301797\n",
      "800 Train Loss 104534.43 Test MSE 110551.12727459634 Test RE 0.6359304044484817\n",
      "900 Train Loss 92064.875 Test MSE 98679.08867045086 Test RE 0.6008147234063935\n",
      "1000 Train Loss 81117.484 Test MSE 88185.59171819064 Test RE 0.5679718590160597\n",
      "1100 Train Loss 71658.22 Test MSE 79075.59261624649 Test RE 0.5378352088755651\n",
      "1200 Train Loss 64043.15 Test MSE 71365.60944839416 Test RE 0.5109430443211961\n",
      "1300 Train Loss 57039.535 Test MSE 64184.86170092308 Test RE 0.48455636945907354\n",
      "1400 Train Loss 54366.875 Test MSE 65222.67949139217 Test RE 0.48845810486013164\n",
      "1500 Train Loss 46300.035 Test MSE 55422.23708867065 Test RE 0.4502669056626196\n",
      "1600 Train Loss 41456.535 Test MSE 50758.391755232464 Test RE 0.43090539195946964\n",
      "1700 Train Loss 37647.914 Test MSE 47018.79650207653 Test RE 0.4147283808379965\n",
      "1800 Train Loss 34487.5 Test MSE 43903.76307543537 Test RE 0.40075493110646776\n",
      "1900 Train Loss 31891.621 Test MSE 41329.298534611014 Test RE 0.3888275411283657\n",
      "2000 Train Loss 29783.203 Test MSE 39222.45718566644 Test RE 0.37878729224528634\n",
      "2100 Train Loss 28091.273 Test MSE 37516.230134311525 Test RE 0.3704568225945809\n",
      "2200 Train Loss 26751.066 Test MSE 36149.516483840154 Test RE 0.3636463628985468\n",
      "2300 Train Loss 25704.145 Test MSE 35067.18993362701 Test RE 0.3581611572656035\n",
      "2400 Train Loss 24898.361 Test MSE 34219.99333916304 Test RE 0.35380825547396283\n",
      "2500 Train Loss 24287.852 Test MSE 33564.57845510225 Test RE 0.3504036330848071\n",
      "2600 Train Loss 23832.914 Test MSE 33063.316251324366 Test RE 0.3477772807342392\n",
      "2700 Train Loss 23499.725 Test MSE 32684.12260842972 Test RE 0.34577725106735185\n",
      "2800 Train Loss 23260.033 Test MSE 32400.038082363324 Test RE 0.34427125456740676\n",
      "2900 Train Loss 23090.703 Test MSE 32188.895102660907 Test RE 0.3431476560057199\n",
      "Training time: 65.61\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 278456.53 Test MSE 272952.998259245 Test RE 0.9992448909392484\n",
      "100 Train Loss 247380.06 Test MSE 243218.7590378008 Test RE 0.9432493964060149\n",
      "200 Train Loss 225440.81 Test MSE 222155.88899274106 Test RE 0.9014817039630689\n",
      "300 Train Loss 197877.55 Test MSE 195506.88569387476 Test RE 0.8456857850531967\n",
      "400 Train Loss 175934.75 Test MSE 174230.53487250704 Test RE 0.798344130685168\n",
      "500 Train Loss 156517.39 Test MSE 155479.00064940905 Test RE 0.7541606564290635\n",
      "600 Train Loss 139273.6 Test MSE 138814.51577502 Test RE 0.7125993847858735\n",
      "700 Train Loss 123964.24 Test MSE 124006.96230102974 Test RE 0.6735208372232463\n",
      "800 Train Loss 110368.46 Test MSE 110762.3261656612 Test RE 0.636537561131127\n",
      "900 Train Loss 98366.09 Test MSE 99151.59826914588 Test RE 0.6022514598879635\n",
      "1000 Train Loss 87814.26 Test MSE 88921.19614535866 Test RE 0.5703358219986622\n",
      "1100 Train Loss 78583.88 Test MSE 79960.13053348007 Test RE 0.5408349501157191\n",
      "1200 Train Loss 70555.29 Test MSE 72154.8194698799 Test RE 0.5137604564189171\n",
      "1300 Train Loss 63616.203 Test MSE 65399.39599127433 Test RE 0.48911937957124674\n",
      "1400 Train Loss 57660.727 Test MSE 59595.23897788648 Test RE 0.4669106549096865\n",
      "1500 Train Loss 52588.67 Test MSE 54651.24073155608 Test RE 0.44712403398505396\n",
      "1600 Train Loss 48305.227 Test MSE 50486.410370810554 Test RE 0.42974936967001515\n",
      "1700 Train Loss 44720.793 Test MSE 47040.329189668206 Test RE 0.4148233342942707\n",
      "1800 Train Loss 41751.04 Test MSE 44358.38681247227 Test RE 0.40282449720205066\n",
      "1900 Train Loss 39314.562 Test MSE 43313.75577686974 Test RE 0.3980530202990287\n",
      "2000 Train Loss 37336.24 Test MSE 41332.67275403191 Test RE 0.38884341319382293\n",
      "2100 Train Loss 35758.543 Test MSE 38110.92329896011 Test RE 0.37338144922792077\n",
      "2200 Train Loss 33582.23 Test MSE 36531.658124101305 Test RE 0.36556338844090824\n",
      "2300 Train Loss 31864.701 Test MSE 34999.38998407292 Test RE 0.35781475002112045\n",
      "2400 Train Loss 30301.191 Test MSE 34279.29059729874 Test RE 0.3541146666778975\n",
      "2500 Train Loss 28846.81 Test MSE 32795.81177661504 Test RE 0.34636754742119835\n",
      "2600 Train Loss 27578.855 Test MSE 31938.038212977193 Test RE 0.34180791875701827\n",
      "2700 Train Loss 26304.078 Test MSE 30344.02788964934 Test RE 0.33316902374742435\n",
      "2800 Train Loss 25196.943 Test MSE 29732.031080376426 Test RE 0.3297921324698884\n",
      "2900 Train Loss 24248.145 Test MSE 28785.783182193907 Test RE 0.3245017377051327\n",
      "Training time: 63.27\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 3000\n",
    "layers = np.array([1,50,50,50,50,1])\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []\n",
    "\n",
    "    'Generate Training data'\n",
    "    N_train = 300\n",
    "    \n",
    "    x_train_np_array, u_train_np_array = trainingdata(N_train,reps*1234)\n",
    "\n",
    "    'Convert to tensor and send to GPU'\n",
    "    x_train = torch.from_numpy(x_train_np_array).float().to(device)\n",
    "    u_train = torch.from_numpy(u_train_np_array).float().to(device)\n",
    "    \n",
    "    x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "       \n",
    "     \n",
    "    model_NN = Sequentialmodel(layers)\n",
    "    model_NN.to(device)\n",
    "    print(model_NN)\n",
    "    params = list(model_NN.parameters())\n",
    "    #optimizer = optim.Adam(PINN.parameters(), lr=0.0008,betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "    optimizer = optim.Adam(model_NN.parameters(), lr=0.008)\n",
    "\n",
    "    train_model(max_iter,reps)\n",
    " \n",
    "    torch.save(model_NN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "\n",
    "                 \n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse\": test_mse_full, \"test_re\": test_re_full,\"Time\": elapsed_time, \"Thresh_time\": time_threshold, \"epoch_thresh\": epoch_threshold, \"label\": label}\n",
    "savemat(label+'.mat', mdic) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred = model_NN.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f0870984f10>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_test,u_pred,'rx')\n",
    "plt.plot(x_test,u_true,'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.3075413007533908\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + test_re_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
