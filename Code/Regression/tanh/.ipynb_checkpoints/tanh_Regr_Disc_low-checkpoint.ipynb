{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:2\n"
     ]
    }
   ],
   "source": [
    "#Trying to replicate the results of \"Adaptive activation functions accelerate convergence in deep and physics-informed neural networks\"\n",
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_disc(x): #[-4,3.75]\n",
    "    y  = np.zeros((np.shape(x)[0],))\n",
    "    \n",
    "    y = 0.2*np.sin(6*x)*np.exp(-1*x)\n",
    "    y[x>0] = 1 + 0.1*x[x>0]*np.exp(x[x>0]) + 10\n",
    "    \n",
    "    return scale*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain bounds\n",
    "level= \"low\"\n",
    "label = \"Regr_disc_tanh_\" + level\n",
    "\n",
    "scale = 1.0\n",
    "loss_thresh = 0.1\n",
    "\n",
    "lb = np.array(-4.0)  # [-1. 0.]\n",
    "ub = np.array(3.75) # [1.  0.99]\n",
    "\n",
    "x_test = np.linspace(lb,ub,1000).reshape(-1,1)\n",
    "u_true = true_disc(x_test)\n",
    "\n",
    "u_true_norm = np.linalg.norm(u_true,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_train,seed):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    x_train = np.random.uniform(low=lb, high=ub, size=(N_train,)).reshape(-1,1)\n",
    "\n",
    "    u_train = true_disc(x_train)\n",
    "\n",
    "    return x_train, u_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "     \n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "\n",
    "            \n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = 2.0*(x - l_b)/(u_b - l_b)-1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z) \n",
    "    \n",
    "        a = self.linears[-1](a) \n",
    "        \n",
    "        return a\n",
    "    \n",
    "    \n",
    "    def loss(self,x,u):\n",
    "\n",
    "        loss_val = self.loss_function(self.forward(x),u)\n",
    "        \n",
    "        return loss_val\n",
    "        \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        u_pred = self.forward(x_test_tensor)       \n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "        \n",
    "        return u_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "        \n",
    "        test_mse_loss = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re_loss = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "        \n",
    "        return test_mse_loss, test_re_loss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    test_mse, test_re = model_NN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step():\n",
    "    optimizer.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "    loss = model_NN.loss(x_train,u_train)\n",
    "    loss.backward() #backprop\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "   \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "    \n",
    "    loss_np = model_NN.loss(x_train,u_train).cpu().detach().numpy()\n",
    "    data_update(loss_np)\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        loss_np = train_step().cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1\n",
    "        data_update(loss_np)\n",
    "        if(i%100==0):        \n",
    "            print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 114.84176 Test MSE 94.3027907052709 Test RE 0.9286675675618589\n",
      "100 Train Loss 6.8874025 Test MSE 7.263783826108371 Test RE 0.2577385978905062\n",
      "200 Train Loss 1.9008521 Test MSE 2.0035465683071423 Test RE 0.13536230053122492\n",
      "300 Train Loss 0.32559496 Test MSE 0.5268959652662547 Test RE 0.0694161371714712\n",
      "400 Train Loss 0.22286782 Test MSE 0.3810227097190182 Test RE 0.05903008883581659\n",
      "500 Train Loss 0.13020979 Test MSE 0.2714571402580117 Test RE 0.04982515299758598\n",
      "600 Train Loss 0.0734801 Test MSE 0.19728093977610994 Test RE 0.042475702487073164\n",
      "700 Train Loss 0.042308446 Test MSE 0.17304895626725783 Test RE 0.03978162308220189\n",
      "800 Train Loss 0.02913801 Test MSE 0.1587066315519626 Test RE 0.03809741802296259\n",
      "900 Train Loss 0.03295482 Test MSE 0.14207257111474944 Test RE 0.036045671974667015\n",
      "1000 Train Loss 0.031265743 Test MSE 0.13867602255642253 Test RE 0.03561219105349942\n",
      "1100 Train Loss 0.03019156 Test MSE 0.13749526514394694 Test RE 0.03546025689708598\n",
      "1200 Train Loss 0.05271431 Test MSE 0.14058512707891463 Test RE 0.035856483473159416\n",
      "1300 Train Loss 0.012083226 Test MSE 0.15134264877109976 Test RE 0.037203060758156214\n",
      "1400 Train Loss 0.015458053 Test MSE 0.12261371031835056 Test RE 0.033486327734134694\n",
      "1500 Train Loss 0.024084633 Test MSE 0.16475313657352708 Test RE 0.0388163640085419\n",
      "1600 Train Loss 0.0059449347 Test MSE 0.14223955619214873 Test RE 0.036066848904460665\n",
      "1700 Train Loss 0.004853267 Test MSE 0.1584218424528936 Test RE 0.03806322096102777\n",
      "1800 Train Loss 0.046652477 Test MSE 0.19091256347349758 Test RE 0.041784504911438876\n",
      "1900 Train Loss 0.04496803 Test MSE 0.18586557762422715 Test RE 0.04122849569936351\n",
      "2000 Train Loss 0.027455578 Test MSE 0.15602502495545978 Test RE 0.03777418791973498\n",
      "2100 Train Loss 0.003988238 Test MSE 0.14231768810053735 Test RE 0.03607675326910986\n",
      "2200 Train Loss 0.005821057 Test MSE 0.17055590960057063 Test RE 0.03949402457891441\n",
      "2300 Train Loss 0.0026462697 Test MSE 0.1530625927536676 Test RE 0.03741386191057553\n",
      "2400 Train Loss 0.14898258 Test MSE 0.48336122246915936 Test RE 0.06648656705558492\n",
      "2500 Train Loss 0.015256898 Test MSE 0.18063110943706817 Test RE 0.040643797682078445\n",
      "2600 Train Loss 0.007186181 Test MSE 0.18974885454265056 Test RE 0.041656961375639415\n",
      "2700 Train Loss 0.0054472494 Test MSE 0.17520526531176941 Test RE 0.04002870894594224\n",
      "2800 Train Loss 0.025148902 Test MSE 0.16763841997598117 Test RE 0.039154779772326426\n",
      "2900 Train Loss 0.10930799 Test MSE 0.2914162934357953 Test RE 0.05162438859350842\n",
      "Training time: 7.13\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 118.05292 Test MSE 97.18453039987902 Test RE 0.9427500766856145\n",
      "100 Train Loss 5.348105 Test MSE 6.098616724787 Test RE 0.23616398932368507\n",
      "200 Train Loss 3.8400295 Test MSE 4.473650862985517 Test RE 0.20226887077384365\n",
      "300 Train Loss 3.0942173 Test MSE 3.236412910832862 Test RE 0.17204021184985485\n",
      "400 Train Loss 1.5477065 Test MSE 1.8346148184409976 Test RE 0.12953002659303064\n",
      "500 Train Loss 0.82330406 Test MSE 0.9569259273773709 Test RE 0.09354857298054829\n",
      "600 Train Loss 0.3809449 Test MSE 0.5111457721694841 Test RE 0.06837075766510195\n",
      "700 Train Loss 0.24297242 Test MSE 0.3466152324451035 Test RE 0.056301741489120566\n",
      "800 Train Loss 0.19290863 Test MSE 0.2897526167347666 Test RE 0.051476817532082\n",
      "900 Train Loss 0.19054906 Test MSE 0.2868601367802158 Test RE 0.051219237252444996\n",
      "1000 Train Loss 1.4009448 Test MSE 0.8725218720184373 Test RE 0.08932770317633945\n",
      "1100 Train Loss 0.13524066 Test MSE 0.212558857130234 Test RE 0.04408974726469527\n",
      "1200 Train Loss 0.12834816 Test MSE 0.16315731339558903 Test RE 0.038627916051323534\n",
      "1300 Train Loss 0.09500541 Test MSE 0.14555202980579998 Test RE 0.036484394187406566\n",
      "1400 Train Loss 0.1492461 Test MSE 0.19740776799721776 Test RE 0.04248935371017604\n",
      "1500 Train Loss 0.09676275 Test MSE 0.13902372647190053 Test RE 0.0356568085225532\n",
      "1600 Train Loss 0.09098924 Test MSE 0.13562862536045497 Test RE 0.0352187296025336\n",
      "1700 Train Loss 0.102503926 Test MSE 0.14959284495976186 Test RE 0.036987367040882124\n",
      "1800 Train Loss 0.08324013 Test MSE 0.11640165814609707 Test RE 0.0326270335567146\n",
      "1900 Train Loss 0.096061 Test MSE 0.12852055924960673 Test RE 0.03428343344561674\n",
      "2000 Train Loss 0.08206741 Test MSE 0.11407495416799476 Test RE 0.03229930352308351\n",
      "2100 Train Loss 0.07859424 Test MSE 0.11349844845130477 Test RE 0.03221758392474232\n",
      "2200 Train Loss 0.08100591 Test MSE 0.11314262454513634 Test RE 0.03216704232823266\n",
      "2300 Train Loss 0.055825654 Test MSE 0.0946536231298659 Test RE 0.029421623076415798\n",
      "2400 Train Loss 0.18443653 Test MSE 0.19652089815949367 Test RE 0.042393802899602366\n",
      "2500 Train Loss 0.027224291 Test MSE 0.07090446402965883 Test RE 0.025464479025341467\n",
      "2600 Train Loss 0.033862438 Test MSE 0.06008494982372141 Test RE 0.02344125490923914\n",
      "2700 Train Loss 0.121210836 Test MSE 0.29217841603824324 Test RE 0.05169184951506487\n",
      "2800 Train Loss 0.023004387 Test MSE 0.05504468689448356 Test RE 0.022436530959656833\n",
      "2900 Train Loss 0.01120516 Test MSE 0.03964242105216188 Test RE 0.019040488708829257\n",
      "Training time: 7.23\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 111.0624 Test MSE 97.58663931998522 Test RE 0.9446984160753484\n",
      "100 Train Loss 7.0206075 Test MSE 7.306671421534204 Test RE 0.2584983617224213\n",
      "200 Train Loss 3.992326 Test MSE 4.514478262993078 Test RE 0.20318974687204133\n",
      "300 Train Loss 3.5971665 Test MSE 4.117752070574174 Test RE 0.19405645938368477\n",
      "400 Train Loss 2.1411252 Test MSE 2.109328699308277 Test RE 0.13888973107103955\n",
      "500 Train Loss 0.69569826 Test MSE 0.5741177079398001 Test RE 0.07246002463291747\n",
      "600 Train Loss 0.4520031 Test MSE 0.3622982651637111 Test RE 0.057561371524307264\n",
      "700 Train Loss 0.25877294 Test MSE 0.23573598252608996 Test RE 0.04643131087818034\n",
      "800 Train Loss 0.13302368 Test MSE 0.14881470826764978 Test RE 0.0368910430725915\n",
      "900 Train Loss 0.08188694 Test MSE 0.1166012117544829 Test RE 0.03265498871774332\n",
      "1000 Train Loss 0.110030025 Test MSE 0.15948666833723843 Test RE 0.03819092691325659\n",
      "1100 Train Loss 0.03425721 Test MSE 0.08300377302506802 Test RE 0.027551606255117055\n",
      "1200 Train Loss 0.23503737 Test MSE 0.1710277083855472 Test RE 0.039548611853673234\n",
      "1300 Train Loss 0.016466288 Test MSE 0.07490163713205304 Test RE 0.02617240675901711\n",
      "1400 Train Loss 0.009190852 Test MSE 0.07145782292339425 Test RE 0.025563651978310724\n",
      "1500 Train Loss 0.0067484826 Test MSE 0.07147171294287327 Test RE 0.02556613639748227\n",
      "1600 Train Loss 0.0055177 Test MSE 0.07133619451894171 Test RE 0.025541886758821627\n",
      "1700 Train Loss 0.048090573 Test MSE 0.11115291982423801 Test RE 0.03188294602168412\n",
      "1800 Train Loss 0.051107254 Test MSE 0.10875326564636933 Test RE 0.03153691151510567\n",
      "1900 Train Loss 0.03404344 Test MSE 0.12163008039842196 Test RE 0.03335174050452803\n",
      "2000 Train Loss 0.013642007 Test MSE 0.08271523368169204 Test RE 0.027503676848613365\n",
      "2100 Train Loss 0.0050304853 Test MSE 0.07790050480834594 Test RE 0.026691202620799045\n",
      "2200 Train Loss 2.6209202 Test MSE 2.486801441884546 Test RE 0.15080597937673643\n",
      "2300 Train Loss 0.08665799 Test MSE 0.1334050434662466 Test RE 0.03492883740340702\n",
      "2400 Train Loss 0.010029519 Test MSE 0.09105271959916642 Test RE 0.02885655400102153\n",
      "2500 Train Loss 0.0037625593 Test MSE 0.08880822856487089 Test RE 0.028498671141797625\n",
      "2600 Train Loss 0.0021960672 Test MSE 0.08879275927704851 Test RE 0.028496188976839098\n",
      "2700 Train Loss 0.0016732184 Test MSE 0.08934965912639885 Test RE 0.02858541200676103\n",
      "2800 Train Loss 0.0013594595 Test MSE 0.0896775169624014 Test RE 0.02863780935129702\n",
      "2900 Train Loss 0.008152938 Test MSE 0.09318990076466536 Test RE 0.029193248949536586\n",
      "Training time: 7.41\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 123.44076 Test MSE 95.8337111113438 Test RE 0.9361752585677519\n",
      "100 Train Loss 8.46765 Test MSE 9.805373582715065 Test RE 0.29945397240629923\n",
      "200 Train Loss 3.4444947 Test MSE 4.665515600587019 Test RE 0.2065607630066523\n",
      "300 Train Loss 2.6228552 Test MSE 3.4979494755465543 Test RE 0.17885651827988494\n",
      "400 Train Loss 0.2338707 Test MSE 0.5892527080117279 Test RE 0.0734089140794739\n",
      "500 Train Loss 0.11875393 Test MSE 0.39384844452217105 Test RE 0.06001538182306874\n",
      "600 Train Loss 0.088019885 Test MSE 0.34589726733399234 Test RE 0.056243400659729915\n",
      "700 Train Loss 0.056830384 Test MSE 0.31454553709564287 Test RE 0.05363394841679163\n",
      "800 Train Loss 0.064425886 Test MSE 0.3265024950600289 Test RE 0.054643845956556816\n",
      "900 Train Loss 0.038599364 Test MSE 0.2791654499874807 Test RE 0.05052761977988217\n",
      "1000 Train Loss 0.019226203 Test MSE 0.22435607106889108 Test RE 0.04529673709567871\n",
      "1100 Train Loss 0.13957714 Test MSE 0.5452437256588375 Test RE 0.0706144115727223\n",
      "1200 Train Loss 0.006390268 Test MSE 0.22034083869853252 Test RE 0.04488957619875341\n",
      "1300 Train Loss 0.016055467 Test MSE 0.2183230539845072 Test RE 0.04468356395104873\n",
      "1400 Train Loss 0.0065733427 Test MSE 0.23045353343011132 Test RE 0.0459081394135555\n",
      "1500 Train Loss 0.033161033 Test MSE 0.2484672812859298 Test RE 0.047668623382576304\n",
      "1600 Train Loss 0.004934614 Test MSE 0.20892730205087126 Test RE 0.043711489357774004\n",
      "1700 Train Loss 0.0096690925 Test MSE 0.2170541735428083 Test RE 0.044553525640654845\n",
      "1800 Train Loss 0.008772363 Test MSE 0.24054629279429887 Test RE 0.046902645590904214\n",
      "1900 Train Loss 0.014802624 Test MSE 0.2313689247596908 Test RE 0.045999225594180904\n",
      "2000 Train Loss 0.01943996 Test MSE 0.22762418465381384 Test RE 0.04562545492250237\n",
      "2100 Train Loss 0.013845281 Test MSE 0.23374524732724244 Test RE 0.04623484444392279\n",
      "2200 Train Loss 0.028247986 Test MSE 0.24104487224252996 Test RE 0.04695122790345129\n",
      "2300 Train Loss 0.028675506 Test MSE 0.3267086518804666 Test RE 0.05466109455960166\n",
      "2400 Train Loss 0.055095267 Test MSE 0.2956309226592777 Test RE 0.05199635923545412\n",
      "2500 Train Loss 0.014488513 Test MSE 0.21619831425625308 Test RE 0.04446560009848502\n",
      "2600 Train Loss 0.0024473015 Test MSE 0.21595275280499684 Test RE 0.044440340559356716\n",
      "2700 Train Loss 0.04757704 Test MSE 0.2624550064225029 Test RE 0.04899203022143592\n",
      "2800 Train Loss 0.014512968 Test MSE 0.2505336849915432 Test RE 0.04786643346228561\n",
      "2900 Train Loss 0.025724335 Test MSE 0.20622617841015678 Test RE 0.04342800739647738\n",
      "Training time: 7.37\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 99.31753 Test MSE 88.20617277986221 Test RE 0.8981471487045454\n",
      "100 Train Loss 8.610227 Test MSE 9.515993041388313 Test RE 0.2950020704548993\n",
      "200 Train Loss 4.7504654 Test MSE 5.239910544970078 Test RE 0.21890714785677454\n",
      "300 Train Loss 4.64544 Test MSE 5.3903666151052185 Test RE 0.22202769899555388\n",
      "400 Train Loss 4.1177964 Test MSE 4.561426032156699 Test RE 0.20424353782970703\n",
      "500 Train Loss 4.093107 Test MSE 4.524894639259097 Test RE 0.20342402437602855\n",
      "600 Train Loss 4.073878 Test MSE 4.4460621404564264 Test RE 0.201644216553499\n",
      "700 Train Loss 4.024612 Test MSE 4.320120113809452 Test RE 0.1987677479102157\n",
      "800 Train Loss 2.9616342 Test MSE 2.9432374904955894 Test RE 0.1640630041309029\n",
      "900 Train Loss 0.57894045 Test MSE 0.7296208103402724 Test RE 0.0816858085950311\n",
      "1000 Train Loss 0.28697902 Test MSE 0.2262334862243992 Test RE 0.04548586417201002\n",
      "1100 Train Loss 0.267243 Test MSE 0.20644693245252074 Test RE 0.043451244853678975\n",
      "1200 Train Loss 0.22945674 Test MSE 0.1689403585985321 Test RE 0.03930653056430046\n",
      "1300 Train Loss 0.20513105 Test MSE 0.14317180357500747 Test RE 0.03618484812350407\n",
      "1400 Train Loss 0.1861593 Test MSE 0.12556489321765132 Test RE 0.033886921920767504\n",
      "1500 Train Loss 0.17005561 Test MSE 0.11174279900318933 Test RE 0.031967434131112034\n",
      "1600 Train Loss 0.16076387 Test MSE 0.10394731539254494 Test RE 0.03083220925935922\n",
      "1700 Train Loss 0.15466495 Test MSE 0.09969947478003095 Test RE 0.03019565407418196\n",
      "1800 Train Loss 0.15372524 Test MSE 0.10116523252268493 Test RE 0.03041680882740366\n",
      "1900 Train Loss 0.14283788 Test MSE 0.08793264113172808 Test RE 0.02835783459545041\n",
      "2000 Train Loss 0.20843574 Test MSE 0.18426572822133455 Test RE 0.04105067381160345\n",
      "2100 Train Loss 0.121263355 Test MSE 0.07065797961838288 Test RE 0.025420179547689832\n",
      "2200 Train Loss 0.12286789 Test MSE 0.08402410664731685 Test RE 0.02772042969615862\n",
      "2300 Train Loss 0.10424303 Test MSE 0.07298799254826717 Test RE 0.02583590718540296\n",
      "2400 Train Loss 0.105235405 Test MSE 0.08479525837348151 Test RE 0.02784734466425287\n",
      "2500 Train Loss 0.023608305 Test MSE 0.026891927945271916 Test RE 0.015682271763718704\n",
      "2600 Train Loss 0.0063951747 Test MSE 0.019636812775100026 Test RE 0.013400885789111196\n",
      "2700 Train Loss 0.09231771 Test MSE 0.11629887403016816 Test RE 0.03261262533657173\n",
      "2800 Train Loss 0.1738077 Test MSE 0.09677412475500859 Test RE 0.029749360348566775\n",
      "2900 Train Loss 0.9093442 Test MSE 1.0848052572419606 Test RE 0.09960333883296485\n",
      "Training time: 6.97\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 107.2605 Test MSE 98.20211696275135 Test RE 0.9476728337478866\n",
      "100 Train Loss 5.605217 Test MSE 6.562677045222519 Test RE 0.2449844519616157\n",
      "200 Train Loss 3.3973887 Test MSE 4.468892966134852 Test RE 0.2021612818638816\n",
      "300 Train Loss 2.9584932 Test MSE 3.7638918119362748 Test RE 0.1855310370056648\n",
      "400 Train Loss 1.1829001 Test MSE 1.2356931035775307 Test RE 0.10630490840199062\n",
      "500 Train Loss 0.51168734 Test MSE 0.47896077585390956 Test RE 0.06618323333414945\n",
      "600 Train Loss 0.758496 Test MSE 0.757930634128056 Test RE 0.08325546227302122\n",
      "700 Train Loss 0.1878138 Test MSE 0.24121834540187312 Test RE 0.04696811959855217\n",
      "800 Train Loss 0.06955241 Test MSE 0.1171025240593058 Test RE 0.03272511144447495\n",
      "900 Train Loss 0.0486632 Test MSE 0.09235618448923846 Test RE 0.02906236798522533\n",
      "1000 Train Loss 0.06266329 Test MSE 0.11307208398196734 Test RE 0.032157013238513346\n",
      "1100 Train Loss 0.04067684 Test MSE 0.09493661660305876 Test RE 0.029465572335782005\n",
      "1200 Train Loss 0.027437426 Test MSE 0.08094650937913218 Test RE 0.02720802819465353\n",
      "1300 Train Loss 0.5374042 Test MSE 1.2725271010608734 Test RE 0.10787766218981937\n",
      "1400 Train Loss 0.0070634717 Test MSE 0.05753858864287576 Test RE 0.022939165177962146\n",
      "1500 Train Loss 0.029468711 Test MSE 0.07993341317005889 Test RE 0.027037229350597408\n",
      "1600 Train Loss 0.01182396 Test MSE 0.054628509922341965 Test RE 0.022351551965775714\n",
      "1700 Train Loss 0.025781598 Test MSE 0.07819891366871977 Test RE 0.026742275962945695\n",
      "1800 Train Loss 0.03141431 Test MSE 0.09417696090211267 Test RE 0.029347448013310476\n",
      "1900 Train Loss 0.004163608 Test MSE 0.048202987917641264 Test RE 0.020995923698488204\n",
      "2000 Train Loss 0.010063639 Test MSE 0.07283869215381321 Test RE 0.025809469372844077\n",
      "2100 Train Loss 0.019977313 Test MSE 0.0666867435133963 Test RE 0.02469549645724742\n",
      "2200 Train Loss 0.015325359 Test MSE 0.05610947356330645 Test RE 0.02265249812787134\n",
      "2300 Train Loss 0.013939531 Test MSE 0.05204804764621844 Test RE 0.02181726103960219\n",
      "2400 Train Loss 0.025468199 Test MSE 0.09872899160029844 Test RE 0.030048331149600326\n",
      "2500 Train Loss 0.1323286 Test MSE 0.2568866523984183 Test RE 0.04846952635556045\n",
      "2600 Train Loss 0.0043920344 Test MSE 0.09018938059682165 Test RE 0.028719422869918316\n",
      "2700 Train Loss 0.0021387099 Test MSE 0.06890061355278815 Test RE 0.0251020708369729\n",
      "2800 Train Loss 0.0016276165 Test MSE 0.0653619962988504 Test RE 0.024448975209698215\n",
      "2900 Train Loss 0.0062989355 Test MSE 0.06735545293233745 Test RE 0.024819006147340377\n",
      "Training time: 7.24\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 125.543 Test MSE 98.11302228380244 Test RE 0.9472428442083537\n",
      "100 Train Loss 7.682146 Test MSE 7.206176900137936 Test RE 0.25671453909145503\n",
      "200 Train Loss 1.9251771 Test MSE 2.0571731689511057 Test RE 0.1371618808579098\n",
      "300 Train Loss 0.6572876 Test MSE 0.7415095245789941 Test RE 0.0823486289954029\n",
      "400 Train Loss 0.45791364 Test MSE 0.4987865856585379 Test RE 0.0675391186001994\n",
      "500 Train Loss 0.31218868 Test MSE 0.3294610956370072 Test RE 0.05489086508385714\n",
      "600 Train Loss 0.21380913 Test MSE 0.19587710451649992 Test RE 0.042324305837712725\n",
      "700 Train Loss 0.20289297 Test MSE 0.20849609707494834 Test RE 0.0436663579963133\n",
      "800 Train Loss 0.13264598 Test MSE 0.13932344665558252 Test RE 0.035695223948578596\n",
      "900 Train Loss 0.13269189 Test MSE 0.1597738753283419 Test RE 0.038225298963266076\n",
      "1000 Train Loss 0.061830178 Test MSE 0.09502735887899072 Test RE 0.02947965085777401\n",
      "1100 Train Loss 0.046769213 Test MSE 0.10511957111368717 Test RE 0.03100557546743552\n",
      "1200 Train Loss 0.28699404 Test MSE 0.2189879652194869 Test RE 0.04475155497703938\n",
      "1300 Train Loss 0.027021939 Test MSE 0.1002925727461304 Test RE 0.030285335717395518\n",
      "1400 Train Loss 0.019957218 Test MSE 0.09995488878809682 Test RE 0.030234307536949973\n",
      "1500 Train Loss 0.014640041 Test MSE 0.10419044078966658 Test RE 0.030868245375362\n",
      "1600 Train Loss 0.022219323 Test MSE 0.11086280636303017 Test RE 0.03184131096959114\n",
      "1700 Train Loss 0.012595712 Test MSE 0.11224640246113718 Test RE 0.032039388711603424\n",
      "1800 Train Loss 0.009044287 Test MSE 0.11090532394352809 Test RE 0.03184741619897045\n",
      "1900 Train Loss 0.015707761 Test MSE 0.12895469261575593 Test RE 0.03434128813716626\n",
      "2000 Train Loss 0.010946445 Test MSE 0.12813153015657594 Test RE 0.03423150649595416\n",
      "2100 Train Loss 0.110729605 Test MSE 0.20785323283781224 Test RE 0.043598986922558695\n",
      "2200 Train Loss 0.010579693 Test MSE 0.11781748010207835 Test RE 0.03282485913573238\n",
      "2300 Train Loss 0.021257129 Test MSE 0.13502949558790792 Test RE 0.03514085539420071\n",
      "2400 Train Loss 0.012835741 Test MSE 0.12430327369025247 Test RE 0.03371625186540614\n",
      "2500 Train Loss 0.010404749 Test MSE 0.12359898366364078 Test RE 0.0336205997136437\n",
      "2600 Train Loss 0.15368253 Test MSE 0.4338775860285664 Test RE 0.0629914512007573\n",
      "2700 Train Loss 0.053570826 Test MSE 0.10865937168537822 Test RE 0.03152329461389167\n",
      "2800 Train Loss 0.02809482 Test MSE 0.09976916618303037 Test RE 0.030206205834270507\n",
      "2900 Train Loss 0.029196028 Test MSE 0.1031693359525657 Test RE 0.03071661283884142\n",
      "Training time: 7.95\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 121.91756 Test MSE 96.41990086252129 Test RE 0.9390340632197467\n",
      "100 Train Loss 5.840098 Test MSE 6.202318372875125 Test RE 0.23816340669789973\n",
      "200 Train Loss 2.371289 Test MSE 3.1703300631645797 Test RE 0.17027474778140642\n",
      "300 Train Loss 1.3187239 Test MSE 1.761669589248811 Test RE 0.12692881773288558\n",
      "400 Train Loss 0.321816 Test MSE 0.5941257736349155 Test RE 0.07371183157313334\n",
      "500 Train Loss 0.16858347 Test MSE 0.5260171155895446 Test RE 0.06935822079494659\n",
      "600 Train Loss 0.05223183 Test MSE 0.36735530016860585 Test RE 0.057961706211536505\n",
      "700 Train Loss 0.15255502 Test MSE 0.49114989615843174 Test RE 0.06702009428116122\n",
      "800 Train Loss 0.10625867 Test MSE 0.4319915976521792 Test RE 0.0628543958056393\n",
      "900 Train Loss 0.024609497 Test MSE 0.3247012751525605 Test RE 0.054492910348474095\n",
      "1000 Train Loss 0.02508866 Test MSE 0.3942919323650197 Test RE 0.06004916208059323\n",
      "1100 Train Loss 0.07266265 Test MSE 0.3347316736229963 Test RE 0.055328183331190335\n",
      "1200 Train Loss 0.024811711 Test MSE 0.39159204907375705 Test RE 0.059843217948670854\n",
      "1300 Train Loss 0.044739235 Test MSE 0.44976581873790233 Test RE 0.06413442857391957\n",
      "1400 Train Loss 0.01584768 Test MSE 0.347844899099745 Test RE 0.05640152227397224\n",
      "1500 Train Loss 0.10903601 Test MSE 0.48474656089270246 Test RE 0.066581775867014\n",
      "1600 Train Loss 0.03336991 Test MSE 0.34542365975280115 Test RE 0.056204882828348054\n",
      "1700 Train Loss 0.010740794 Test MSE 0.3668177922346786 Test RE 0.057919286404876605\n",
      "1800 Train Loss 0.13645756 Test MSE 0.5082155924098867 Test RE 0.06817450587065105\n",
      "1900 Train Loss 0.01917983 Test MSE 0.4017380769254951 Test RE 0.06061351984155023\n",
      "2000 Train Loss 0.011910749 Test MSE 0.37169309279436924 Test RE 0.0583029126567603\n",
      "2100 Train Loss 0.13518357 Test MSE 0.5153683434315123 Test RE 0.06865258197467079\n",
      "2200 Train Loss 0.014693283 Test MSE 0.377011101347467 Test RE 0.058718516586856756\n",
      "2300 Train Loss 0.086701475 Test MSE 0.5574619159795643 Test RE 0.07140121593841485\n",
      "2400 Train Loss 0.03825651 Test MSE 0.3563259162133795 Test RE 0.05708496135948877\n",
      "2500 Train Loss 0.020763287 Test MSE 0.3524656672110685 Test RE 0.05677490498674302\n",
      "2600 Train Loss 0.0165112 Test MSE 0.3660663454364396 Test RE 0.0578599305392837\n",
      "2700 Train Loss 0.018388556 Test MSE 0.3621504495010734 Test RE 0.05754962796779355\n",
      "2800 Train Loss 0.00845798 Test MSE 0.418862527895204 Test RE 0.061891892263471006\n",
      "2900 Train Loss 0.04764449 Test MSE 0.47202643419021695 Test RE 0.06570238978637191\n",
      "Training time: 8.68\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 111.57866 Test MSE 98.63982363486384 Test RE 0.9497824702174316\n",
      "100 Train Loss 4.571416 Test MSE 7.088907700723439 Test RE 0.2546171584410266\n",
      "200 Train Loss 2.1498008 Test MSE 3.7430083301757593 Test RE 0.18501562323661044\n",
      "300 Train Loss 1.2317697 Test MSE 1.9416000871378278 Test RE 0.13325327683394034\n",
      "400 Train Loss 0.35595435 Test MSE 0.829709840183797 Test RE 0.08710861859061561\n",
      "500 Train Loss 0.14848858 Test MSE 0.51515254052371 Test RE 0.06863820684058716\n",
      "600 Train Loss 0.0778072 Test MSE 0.3863891900302144 Test RE 0.05944433733038357\n",
      "700 Train Loss 0.05060788 Test MSE 0.38247862819398476 Test RE 0.05914276066931269\n",
      "800 Train Loss 0.272248 Test MSE 0.544469434996108 Test RE 0.070564254641169\n",
      "900 Train Loss 0.02303178 Test MSE 0.38025855123129987 Test RE 0.05897086535427163\n",
      "1000 Train Loss 0.05019905 Test MSE 0.38671384084499355 Test RE 0.0594693051620203\n",
      "1100 Train Loss 0.045757078 Test MSE 0.4514834582876539 Test RE 0.06425677541829075\n",
      "1200 Train Loss 0.07163671 Test MSE 0.3778538876835678 Test RE 0.05878411085945326\n",
      "1300 Train Loss 0.06263346 Test MSE 0.42740794726823483 Test RE 0.06252004799975959\n",
      "1400 Train Loss 0.020727599 Test MSE 0.377697533806581 Test RE 0.05877194732886156\n",
      "1500 Train Loss 0.021648318 Test MSE 0.44852810940661386 Test RE 0.06404612209984377\n",
      "1600 Train Loss 0.0149038 Test MSE 0.42198472921930236 Test RE 0.062122135088051776\n",
      "1700 Train Loss 0.03045141 Test MSE 0.4064229088979571 Test RE 0.06096591498223233\n",
      "1800 Train Loss 0.021730952 Test MSE 0.4040536589781299 Test RE 0.06078795427818793\n",
      "1900 Train Loss 0.0045192777 Test MSE 0.3795287617221371 Test RE 0.0589142499444012\n",
      "2000 Train Loss 0.034572717 Test MSE 0.41277518673358765 Test RE 0.06144050794098356\n",
      "2100 Train Loss 0.027909799 Test MSE 0.47456378130266613 Test RE 0.06587874255256987\n",
      "2200 Train Loss 0.002355201 Test MSE 0.39710951232177744 Test RE 0.06026333350559851\n",
      "2300 Train Loss 0.0035232545 Test MSE 0.3922880851679434 Test RE 0.059896378562048935\n",
      "2400 Train Loss 0.0016988093 Test MSE 0.39947367339306883 Test RE 0.06044245387691114\n",
      "2500 Train Loss 0.008524358 Test MSE 0.41117709981128614 Test RE 0.0613214570696252\n",
      "2600 Train Loss 0.012712939 Test MSE 0.45740363001376916 Test RE 0.06467669357093939\n",
      "2700 Train Loss 0.012310867 Test MSE 0.39246216033973425 Test RE 0.05990966639358078\n",
      "2800 Train Loss 0.023206268 Test MSE 0.4076783501547906 Test RE 0.06106000430026687\n",
      "2900 Train Loss 0.016093435 Test MSE 0.38852596697191427 Test RE 0.05960847774715288\n",
      "Training time: 9.47\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 114.27008 Test MSE 101.29366647918144 Test RE 0.9624743222793157\n",
      "100 Train Loss 7.206324 Test MSE 7.365631883266819 Test RE 0.2595392296166287\n",
      "200 Train Loss 4.2384167 Test MSE 4.667085429396116 Test RE 0.2065955113380462\n",
      "300 Train Loss 2.1186025 Test MSE 2.532431807500091 Test RE 0.15218326100565152\n",
      "400 Train Loss 0.28387803 Test MSE 0.8551280079539983 Test RE 0.08843283983817185\n",
      "500 Train Loss 0.12830417 Test MSE 0.6449350098259636 Test RE 0.07679907383199755\n",
      "600 Train Loss 0.11288348 Test MSE 0.6383119842424313 Test RE 0.07640372017600298\n",
      "700 Train Loss 0.107679054 Test MSE 0.6477744657272327 Test RE 0.07696794983145891\n",
      "800 Train Loss 0.109189585 Test MSE 0.6852444369070041 Test RE 0.07916273046485992\n",
      "900 Train Loss 0.1104836 Test MSE 0.580556365633459 Test RE 0.07286520680274974\n",
      "1000 Train Loss 0.0819504 Test MSE 0.5301530151399776 Test RE 0.06963035734989217\n",
      "1100 Train Loss 0.045787062 Test MSE 0.5413711467559725 Test RE 0.07036319623543578\n",
      "1200 Train Loss 0.07192082 Test MSE 0.6935834948140912 Test RE 0.07964295787780126\n",
      "1300 Train Loss 0.02251194 Test MSE 0.550235153258543 Test RE 0.07093689463345766\n",
      "1400 Train Loss 0.0038794375 Test MSE 0.5227503759222195 Test RE 0.06914251666885333\n",
      "1500 Train Loss 0.021532726 Test MSE 0.5750031909455561 Test RE 0.07251588199341921\n",
      "1600 Train Loss 0.014487277 Test MSE 0.5476000076460145 Test RE 0.07076682789946133\n",
      "1700 Train Loss 0.029688168 Test MSE 0.5078309597446121 Test RE 0.06814870274143686\n",
      "1800 Train Loss 0.08131095 Test MSE 0.5934565270655137 Test RE 0.07367030392644013\n",
      "1900 Train Loss 0.02253333 Test MSE 0.5587454742970333 Test RE 0.07148336947021647\n",
      "2000 Train Loss 0.010516027 Test MSE 0.560972988183855 Test RE 0.07162571676687977\n",
      "2100 Train Loss 0.017286396 Test MSE 0.57085263661222 Test RE 0.07225368673406413\n",
      "2200 Train Loss 0.0073759193 Test MSE 0.5507212057885623 Test RE 0.0709682189190476\n",
      "2300 Train Loss 0.0056906333 Test MSE 0.5444215415587415 Test RE 0.07056115103349553\n",
      "2400 Train Loss 0.0032708612 Test MSE 0.5767556453681126 Test RE 0.0726263023357784\n",
      "2500 Train Loss 0.030512435 Test MSE 0.5202624366234294 Test RE 0.06897778455336696\n",
      "2600 Train Loss 0.0024412486 Test MSE 0.5361171568555603 Test RE 0.0700209274840797\n",
      "2700 Train Loss 0.034503043 Test MSE 0.6547199356515504 Test RE 0.07737947675583232\n",
      "2800 Train Loss 0.04200548 Test MSE 0.5039490886781794 Test RE 0.06788773798560604\n",
      "2900 Train Loss 0.013569832 Test MSE 0.6462421728248263 Test RE 0.07687686313651403\n",
      "Training time: 10.01\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 3000\n",
    "layers = np.array([1,50,50,50,50,1])\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []\n",
    "\n",
    "    'Generate Training data'\n",
    "    N_train = 300\n",
    "    \n",
    "    x_train_np_array, u_train_np_array = trainingdata(N_train,reps*1234)\n",
    "\n",
    "    'Convert to tensor and send to GPU'\n",
    "    x_train = torch.from_numpy(x_train_np_array).float().to(device)\n",
    "    u_train = torch.from_numpy(u_train_np_array).float().to(device)\n",
    "    \n",
    "    x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "       \n",
    "     \n",
    "    model_NN = Sequentialmodel(layers)\n",
    "    model_NN.to(device)\n",
    "    print(model_NN)\n",
    "    params = list(model_NN.parameters())\n",
    "    #optimizer = optim.Adam(PINN.parameters(), lr=0.0008,betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "    optimizer = optim.Adam(model_NN.parameters(), lr=0.008)\n",
    "\n",
    "    train_model(max_iter,reps)\n",
    " \n",
    "    torch.save(model_NN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "\n",
    "                 \n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse\": test_mse_full, \"test_re\": test_re_full,\"Time\": elapsed_time, \"Thresh_time\": time_threshold, \"epoch_thresh\": epoch_threshold, \"label\": label}\n",
    "savemat(label+'.mat', mdic) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred = model_NN.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f04bc094650>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_test,u_pred,'rx')\n",
    "plt.plot(x_test,u_true,'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.05006128221375371\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + test_re_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
