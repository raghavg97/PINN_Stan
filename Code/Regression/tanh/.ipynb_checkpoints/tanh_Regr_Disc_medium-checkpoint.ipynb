{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "#Trying to replicate the results of \"Adaptive activation functions accelerate convergence in deep and physics-informed neural networks\"\n",
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_disc(x): #[-4,3.75]\n",
    "    y  = np.zeros((np.shape(x)[0],))\n",
    "    \n",
    "    y = 0.2*np.sin(6*x)*np.exp(-1*x)\n",
    "    y[x>0] = 1 + 0.1*x[x>0]*np.exp(x[x>0]) + 10\n",
    "    \n",
    "    return scale*y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Domain bounds\n",
    "level= \"medium\"\n",
    "label = \"Regr_disc_tanh_\" + level\n",
    "\n",
    "scale = 10.0\n",
    "loss_thresh = 0.1\n",
    "\n",
    "lb = np.array(-4.0)  # [-1. 0.]\n",
    "ub = np.array(3.75) # [1.  0.99]\n",
    "\n",
    "x_test = np.linspace(lb,ub,1000).reshape(-1,1)\n",
    "u_true = true_disc(x_test)\n",
    "\n",
    "u_true_norm = np.linalg.norm(u_true,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_train,seed):\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    x_train = np.random.uniform(low=lb, high=ub, size=(N_train,)).reshape(-1,1)\n",
    "\n",
    "    u_train = true_disc(x_train)\n",
    "\n",
    "    return x_train, u_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "     \n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)\n",
    "\n",
    "            \n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = 2.0*(x - l_b)/(u_b - l_b)-1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z) \n",
    "    \n",
    "        a = self.linears[-1](a) \n",
    "        \n",
    "        return a\n",
    "    \n",
    "    \n",
    "    def loss(self,x,u):\n",
    "\n",
    "        loss_val = self.loss_function(self.forward(x),u)\n",
    "        \n",
    "        return loss_val\n",
    "        \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        u_pred = self.forward(x_test_tensor)       \n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "        \n",
    "        return u_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "        \n",
    "        test_mse_loss = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re_loss = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "        \n",
    "        return test_mse_loss, test_re_loss  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    test_mse, test_re = model_NN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step():\n",
    "    optimizer.zero_grad()     # zeroes the gradient buffers of all parameters\n",
    "    loss = model_NN.loss(x_train,u_train)\n",
    "    loss.backward() #backprop\n",
    "    optimizer.step()\n",
    "    \n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "   \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "    \n",
    "    loss_np = model_NN.loss(x_train,u_train).cpu().detach().numpy()\n",
    "    data_update(loss_np)\n",
    "    \n",
    "    for i in range(max_iter):\n",
    "        loss_np = train_step().cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1\n",
    "        data_update(loss_np)\n",
    "        if(i%100==0):        \n",
    "            print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 11607.848 Test MSE 10776.159695096949 Test RE 0.9927274151502495\n",
      "100 Train Loss 6122.031 Test MSE 5730.655999091277 Test RE 0.7239360033389581\n",
      "200 Train Loss 3004.575 Test MSE 2805.258538585388 Test RE 0.5065058347587741\n",
      "300 Train Loss 1649.0049 Test MSE 1489.275841417428 Test RE 0.36905031338476857\n",
      "400 Train Loss 1150.8237 Test MSE 1057.7936743701775 Test RE 0.311027284714743\n",
      "500 Train Loss 923.82043 Test MSE 850.2918821941217 Test RE 0.2788573029153973\n",
      "600 Train Loss 629.2103 Test MSE 587.828060276105 Test RE 0.23185857529357548\n",
      "700 Train Loss 468.30975 Test MSE 439.94774041895585 Test RE 0.2005850457738353\n",
      "800 Train Loss 330.85437 Test MSE 320.88859036771123 Test RE 0.1713070135218305\n",
      "900 Train Loss 242.94545 Test MSE 244.69344311434557 Test RE 0.14959227678800213\n",
      "1000 Train Loss 169.43495 Test MSE 179.91933487024014 Test RE 0.12827349403583194\n",
      "1100 Train Loss 120.75741 Test MSE 138.76730527281813 Test RE 0.11265269440719315\n",
      "1200 Train Loss 94.2986 Test MSE 115.3201861420593 Test RE 0.10269532496995029\n",
      "1300 Train Loss 69.23853 Test MSE 93.70959233955362 Test RE 0.092574213381106\n",
      "1400 Train Loss 53.931538 Test MSE 78.29633681300255 Test RE 0.0846191635252446\n",
      "1500 Train Loss 41.67426 Test MSE 63.44438198727359 Test RE 0.0761718654851452\n",
      "1600 Train Loss 34.419857 Test MSE 60.90598787836023 Test RE 0.07463250213059829\n",
      "1700 Train Loss 25.86303 Test MSE 50.288393768460665 Test RE 0.06781595599140738\n",
      "1800 Train Loss 26.867395 Test MSE 49.456458357046735 Test RE 0.06725266714745445\n",
      "1900 Train Loss 21.981518 Test MSE 42.077850396833924 Test RE 0.0620332848194732\n",
      "2000 Train Loss 14.652263 Test MSE 37.649031681638256 Test RE 0.058677947141238536\n",
      "2100 Train Loss 12.727565 Test MSE 36.06559377943778 Test RE 0.05743075815715944\n",
      "2200 Train Loss 10.075013 Test MSE 32.47831777369869 Test RE 0.054499782555520945\n",
      "2300 Train Loss 8.884655 Test MSE 29.32327402636708 Test RE 0.05178503040901338\n",
      "2400 Train Loss 7.1092415 Test MSE 29.06713392461448 Test RE 0.05155836206282825\n",
      "2500 Train Loss 6.9428973 Test MSE 25.17555453954134 Test RE 0.04798301451896758\n",
      "2600 Train Loss 196.44539 Test MSE 309.2631209313063 Test RE 0.16817524664740893\n",
      "2700 Train Loss 52.671093 Test MSE 75.59568093518908 Test RE 0.08314698346529602\n",
      "2800 Train Loss 23.938719 Test MSE 45.69670633495626 Test RE 0.06464582102524682\n",
      "2900 Train Loss 11.326347 Test MSE 27.13607202819045 Test RE 0.04981630341136152\n",
      "Training time: 9.06\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 11719.888 Test MSE 10807.793546761399 Test RE 0.9941834430708116\n",
      "100 Train Loss 6493.67 Test MSE 6097.943175751035 Test RE 0.7467748661601729\n",
      "200 Train Loss 3115.172 Test MSE 3045.6437338561464 Test RE 0.5277613186783168\n",
      "300 Train Loss 1747.1455 Test MSE 1757.5145705733041 Test RE 0.4009105392588622\n",
      "400 Train Loss 1254.0736 Test MSE 1275.8693324905057 Test RE 0.34158682017609887\n",
      "500 Train Loss 946.79785 Test MSE 951.8775766569127 Test RE 0.2950452004614674\n",
      "600 Train Loss 678.6562 Test MSE 698.7747163699925 Test RE 0.2527939043072367\n",
      "700 Train Loss 522.07306 Test MSE 540.4769078654202 Test RE 0.2223241178583914\n",
      "800 Train Loss 414.48294 Test MSE 433.4491803789546 Test RE 0.19909809250038285\n",
      "900 Train Loss 321.68793 Test MSE 338.44400301677626 Test RE 0.17593061373025523\n",
      "1000 Train Loss 311.28958 Test MSE 315.18042066522014 Test RE 0.16977651773703936\n",
      "1100 Train Loss 209.49951 Test MSE 223.3451977269549 Test RE 0.14291779780442337\n",
      "1200 Train Loss 165.44446 Test MSE 181.0331306671975 Test RE 0.12866992182785805\n",
      "1300 Train Loss 130.67883 Test MSE 147.9391622066996 Test RE 0.11631603343640186\n",
      "1400 Train Loss 106.1772 Test MSE 123.86387238465713 Test RE 0.10643153673045949\n",
      "1500 Train Loss 88.18077 Test MSE 110.00229476065473 Test RE 0.10029952555893301\n",
      "1600 Train Loss 74.8453 Test MSE 92.47374745259228 Test RE 0.09196175165058204\n",
      "1700 Train Loss 54.513985 Test MSE 66.51835547201102 Test RE 0.07799535838810641\n",
      "1800 Train Loss 45.044342 Test MSE 56.90150599186216 Test RE 0.07213730055775212\n",
      "1900 Train Loss 35.86816 Test MSE 43.41649455267827 Test RE 0.06301230753933561\n",
      "2000 Train Loss 26.153412 Test MSE 36.46083494555963 Test RE 0.05774459118443989\n",
      "2100 Train Loss 21.34638 Test MSE 31.24136563598738 Test RE 0.05345188323307539\n",
      "2200 Train Loss 17.096745 Test MSE 27.74462141025503 Test RE 0.05037179273232314\n",
      "2300 Train Loss 13.7108755 Test MSE 23.703880514818522 Test RE 0.046559438143667205\n",
      "2400 Train Loss 20.991869 Test MSE 21.33040002241401 Test RE 0.04416695982852945\n",
      "2500 Train Loss 10.204265 Test MSE 17.477717488301337 Test RE 0.03997977666279609\n",
      "2600 Train Loss 9.639114 Test MSE 16.250224853034883 Test RE 0.03855029391035359\n",
      "2700 Train Loss 8.076077 Test MSE 14.340633745626493 Test RE 0.03621447372009434\n",
      "2800 Train Loss 7.2768297 Test MSE 13.125484063816685 Test RE 0.03464620499362389\n",
      "2900 Train Loss 11.241184 Test MSE 21.018754307406873 Test RE 0.043843124118947606\n",
      "Training time: 7.39\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 11175.824 Test MSE 10812.156778733945 Test RE 0.9943841045307359\n",
      "100 Train Loss 6342.936 Test MSE 6052.388985021174 Test RE 0.7439802766026815\n",
      "200 Train Loss 3183.4067 Test MSE 3048.224367683071 Test RE 0.5279848626068055\n",
      "300 Train Loss 1843.8688 Test MSE 1788.6053725934396 Test RE 0.4044410895016417\n",
      "400 Train Loss 1333.6062 Test MSE 1328.5092143354411 Test RE 0.34856220288693923\n",
      "500 Train Loss 1306.688 Test MSE 1245.4572770118327 Test RE 0.33749117686118846\n",
      "600 Train Loss 899.1776 Test MSE 945.1375219437584 Test RE 0.29399876675093717\n",
      "700 Train Loss 743.0045 Test MSE 800.7088347185229 Test RE 0.2706046881485436\n",
      "800 Train Loss 654.6446 Test MSE 728.503499701133 Test RE 0.25811534690093696\n",
      "900 Train Loss 580.37305 Test MSE 664.4221948864123 Test RE 0.24650178695442412\n",
      "1000 Train Loss 542.0648 Test MSE 630.2350101996691 Test RE 0.2400762886395569\n",
      "1100 Train Loss 487.50888 Test MSE 565.1122270863459 Test RE 0.22733450495782773\n",
      "1200 Train Loss 442.76495 Test MSE 536.3133244907852 Test RE 0.2214661213272582\n",
      "1300 Train Loss 400.57907 Test MSE 498.5558983977223 Test RE 0.21352805076921152\n",
      "1400 Train Loss 381.42072 Test MSE 480.2468645810489 Test RE 0.20957056065072135\n",
      "1500 Train Loss 361.29205 Test MSE 450.6762653311281 Test RE 0.2030160387372918\n",
      "1600 Train Loss 350.20895 Test MSE 439.17148632256993 Test RE 0.20040800917133153\n",
      "1700 Train Loss 342.08313 Test MSE 440.0681701410277 Test RE 0.20061249761208746\n",
      "1800 Train Loss 335.4751 Test MSE 428.7598984747671 Test RE 0.19801818971781088\n",
      "1900 Train Loss 330.9763 Test MSE 421.3514051316552 Test RE 0.19629996828955285\n",
      "2000 Train Loss 402.8529 Test MSE 500.5551423856012 Test RE 0.2139557536202839\n",
      "2100 Train Loss 313.34903 Test MSE 408.67646468741225 Test RE 0.19332491151221481\n",
      "2200 Train Loss 307.3819 Test MSE 408.76722130107595 Test RE 0.19334637658523354\n",
      "2300 Train Loss 304.77676 Test MSE 406.8360497007643 Test RE 0.19288911501923936\n",
      "2400 Train Loss 301.56262 Test MSE 401.2412191403493 Test RE 0.19155821293854966\n",
      "2500 Train Loss 298.52267 Test MSE 399.1236586916776 Test RE 0.1910520676554861\n",
      "2600 Train Loss 296.58026 Test MSE 397.1258578696971 Test RE 0.1905733152746519\n",
      "2700 Train Loss 295.08496 Test MSE 395.71930309988363 Test RE 0.19023552617021228\n",
      "2800 Train Loss 293.88455 Test MSE 394.6388307893257 Test RE 0.18997563903140305\n",
      "2900 Train Loss 292.91824 Test MSE 393.80148584699805 Test RE 0.18977398678741977\n",
      "Training time: 6.85\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 12235.496 Test MSE 10793.432343686538 Test RE 0.993522696897693\n",
      "100 Train Loss 6440.565 Test MSE 5731.267797765333 Test RE 0.7239746456223792\n",
      "200 Train Loss 3329.477 Test MSE 2993.338599597781 Test RE 0.5232098713362412\n",
      "300 Train Loss 1957.2266 Test MSE 1821.8197781668903 Test RE 0.40817905178005803\n",
      "400 Train Loss 1421.2334 Test MSE 1391.5979967284834 Test RE 0.3567425428355519\n",
      "500 Train Loss 1104.894 Test MSE 1123.3601024086934 Test RE 0.3205217489301818\n",
      "600 Train Loss 887.24054 Test MSE 943.550646378224 Test RE 0.29375185270845633\n",
      "700 Train Loss 726.7813 Test MSE 807.086747921664 Test RE 0.27168027888124874\n",
      "800 Train Loss 622.759 Test MSE 725.6882538396832 Test RE 0.25761613066459416\n",
      "900 Train Loss 547.0191 Test MSE 665.4858654673169 Test RE 0.2466990198640615\n",
      "1000 Train Loss 492.91574 Test MSE 627.3547729308186 Test RE 0.23952707405881452\n",
      "1100 Train Loss 453.61075 Test MSE 577.1401259098266 Test RE 0.2297410708847078\n",
      "1200 Train Loss 427.2787 Test MSE 570.1938796363278 Test RE 0.2283543462135641\n",
      "1300 Train Loss 406.06226 Test MSE 543.1650813427705 Test RE 0.22287631953616469\n",
      "1400 Train Loss 390.65002 Test MSE 523.7975316328934 Test RE 0.21886672021009718\n",
      "1500 Train Loss 376.88687 Test MSE 504.364957700078 Test RE 0.21476843806161125\n",
      "1600 Train Loss 373.4607 Test MSE 522.397863684657 Test RE 0.2185741017496922\n",
      "1700 Train Loss 361.1687 Test MSE 494.0177561114228 Test RE 0.21255400160129764\n",
      "1800 Train Loss 355.1424 Test MSE 484.29219621842384 Test RE 0.2104513624320709\n",
      "1900 Train Loss 353.63235 Test MSE 500.6562302001929 Test RE 0.213977356862229\n",
      "2000 Train Loss 309.1 Test MSE 454.8278001930594 Test RE 0.20394896568174609\n",
      "2100 Train Loss 266.13968 Test MSE 474.91596425102654 Test RE 0.2084041632167386\n",
      "2200 Train Loss 259.14584 Test MSE 495.4123826838369 Test RE 0.2128538132373613\n",
      "2300 Train Loss 256.6903 Test MSE 451.279584891018 Test RE 0.2031518819043787\n",
      "2400 Train Loss 251.3398 Test MSE 429.67510311351634 Test RE 0.19822941581475204\n",
      "2500 Train Loss 248.6161 Test MSE 428.74674427371195 Test RE 0.1980151521307277\n",
      "2600 Train Loss 246.413 Test MSE 432.17071903700685 Test RE 0.19880425507416402\n",
      "2700 Train Loss 244.54808 Test MSE 439.8713908628629 Test RE 0.20056764002050792\n",
      "2800 Train Loss 241.07487 Test MSE 458.467608338349 Test RE 0.20476340123863096\n",
      "2900 Train Loss 240.37135 Test MSE 456.93368189554235 Test RE 0.2044205687787666\n",
      "Training time: 6.94\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 10293.483 Test MSE 10704.494698235982 Test RE 0.9894209271957608\n",
      "100 Train Loss 5217.587 Test MSE 5608.935424664646 Test RE 0.7162064472414758\n",
      "200 Train Loss 2658.093 Test MSE 2953.459238604445 Test RE 0.5197129001569419\n",
      "300 Train Loss 1620.4348 Test MSE 1833.9210112517283 Test RE 0.40953244978521486\n",
      "400 Train Loss 1251.6193 Test MSE 1418.2769144574538 Test RE 0.360145939876299\n",
      "500 Train Loss 1169.5254 Test MSE 1312.9852520210477 Test RE 0.3465197003222654\n",
      "600 Train Loss 955.8409 Test MSE 1095.4766041854043 Test RE 0.3165188357584249\n",
      "700 Train Loss 805.441 Test MSE 932.8696442936536 Test RE 0.29208448376943497\n",
      "800 Train Loss 743.21356 Test MSE 857.6918521559826 Test RE 0.2800681020976739\n",
      "900 Train Loss 696.1585 Test MSE 808.9962539319346 Test RE 0.2720014764746519\n",
      "1000 Train Loss 638.5475 Test MSE 751.3925545691128 Test RE 0.2621388862220897\n",
      "1100 Train Loss 1020.2517 Test MSE 1066.2708595179786 Test RE 0.31227108796645353\n",
      "1200 Train Loss 729.778 Test MSE 827.4856373167455 Test RE 0.2750921758492067\n",
      "1300 Train Loss 705.7292 Test MSE 790.4007870868021 Test RE 0.2688572104074289\n",
      "1400 Train Loss 687.90375 Test MSE 759.2833790639004 Test RE 0.26351173031146635\n",
      "1500 Train Loss 668.81335 Test MSE 729.4171226540707 Test RE 0.2582771486052627\n",
      "1600 Train Loss 653.3048 Test MSE 707.2947701089934 Test RE 0.2543303738080523\n",
      "1700 Train Loss 637.9172 Test MSE 684.251017062634 Test RE 0.25015300943263435\n",
      "1800 Train Loss 624.6796 Test MSE 665.0745951819117 Test RE 0.2466227780819388\n",
      "1900 Train Loss 611.48444 Test MSE 648.0087114451412 Test RE 0.24343803193075328\n",
      "2000 Train Loss 600.16504 Test MSE 630.5620324833463 Test RE 0.24013856708569054\n",
      "2100 Train Loss 589.10046 Test MSE 613.5468474446604 Test RE 0.23687644173854353\n",
      "2200 Train Loss 469.6476 Test MSE 517.8956926167384 Test RE 0.21763019723424895\n",
      "2300 Train Loss 431.35413 Test MSE 480.0816177189877 Test RE 0.20953450226116543\n",
      "2400 Train Loss 526.731 Test MSE 596.1935746770258 Test RE 0.2335025629426741\n",
      "2500 Train Loss 517.9884 Test MSE 573.3418638432703 Test RE 0.22898383948546225\n",
      "2600 Train Loss 511.23538 Test MSE 561.7846003063754 Test RE 0.22666419447026975\n",
      "2700 Train Loss 504.7745 Test MSE 549.1061867111365 Test RE 0.22409190812460492\n",
      "2800 Train Loss 498.32407 Test MSE 534.6178240295384 Test RE 0.22111577281189693\n",
      "2900 Train Loss 431.8301 Test MSE 474.78070479531584 Test RE 0.20837448360689864\n",
      "Training time: 7.01\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 10570.856 Test MSE 10819.141931481967 Test RE 0.9947052617021325\n",
      "100 Train Loss 5664.583 Test MSE 5773.150877490153 Test RE 0.7266151688803473\n",
      "200 Train Loss 2927.3699 Test MSE 2939.189861320012 Test RE 0.5184559065828925\n",
      "300 Train Loss 1585.9849 Test MSE 1598.291796928706 Test RE 0.3823191407151637\n",
      "400 Train Loss 1300.3618 Test MSE 1395.2663227829835 Test RE 0.3572124295004442\n",
      "500 Train Loss 828.8815 Test MSE 856.0071326159017 Test RE 0.2797929052821484\n",
      "600 Train Loss 595.87836 Test MSE 627.1601150864151 Test RE 0.23948991052448607\n",
      "700 Train Loss 438.04053 Test MSE 470.7933181835937 Test RE 0.20749763512650599\n",
      "800 Train Loss 331.31104 Test MSE 362.0984462975089 Test RE 0.1819748360752915\n",
      "900 Train Loss 253.08139 Test MSE 282.84530727402307 Test RE 0.160832012012785\n",
      "1000 Train Loss 193.28325 Test MSE 219.13561789671323 Test RE 0.14156454340148358\n",
      "1100 Train Loss 145.65839 Test MSE 168.67559615624293 Test RE 0.12420072559684601\n",
      "1200 Train Loss 117.16296 Test MSE 143.41065165579664 Test RE 0.11452194380535798\n",
      "1300 Train Loss 91.34459 Test MSE 109.92257140321651 Test RE 0.10026317329796784\n",
      "1400 Train Loss 72.34434 Test MSE 92.06160751922978 Test RE 0.0917565937796807\n",
      "1500 Train Loss 58.890614 Test MSE 81.16190312425917 Test RE 0.08615373641578533\n",
      "1600 Train Loss 53.131546 Test MSE 71.18897024291736 Test RE 0.08068714736910493\n",
      "1700 Train Loss 38.62373 Test MSE 56.64600597901817 Test RE 0.07197516233873749\n",
      "1800 Train Loss 30.614391 Test MSE 46.49427523097435 Test RE 0.0652075296093001\n",
      "1900 Train Loss 24.554808 Test MSE 38.93958142709066 Test RE 0.05967516734978112\n",
      "2000 Train Loss 20.418978 Test MSE 33.99001302755243 Test RE 0.055753697148757754\n",
      "2100 Train Loss 16.768396 Test MSE 30.175082053286445 Test RE 0.05253179442218332\n",
      "2200 Train Loss 16.592154 Test MSE 29.150486117389253 Test RE 0.05163223288375957\n",
      "2300 Train Loss 12.955328 Test MSE 23.995526961902307 Test RE 0.046844990163067084\n",
      "2400 Train Loss 9.506198 Test MSE 20.33660517086753 Test RE 0.0431258069698437\n",
      "2500 Train Loss 9.846807 Test MSE 20.613723520108405 Test RE 0.04341864136220679\n",
      "2600 Train Loss 139.22992 Test MSE 147.02233219965387 Test RE 0.11595504799316776\n",
      "2700 Train Loss 10.98789 Test MSE 26.917426206232513 Test RE 0.04961520289923063\n",
      "2800 Train Loss 5.9065175 Test MSE 18.225059102883517 Test RE 0.04082559085574464\n",
      "2900 Train Loss 4.505393 Test MSE 16.283691757085776 Test RE 0.038589970143151464\n",
      "Training time: 7.31\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 12440.262 Test MSE 10818.19400995317 Test RE 0.9946616850861492\n",
      "100 Train Loss 7560.285 Test MSE 6571.756482941481 Test RE 0.7752445776204334\n",
      "200 Train Loss 3570.7812 Test MSE 2989.2863926872105 Test RE 0.5228556059220214\n",
      "300 Train Loss 2069.868 Test MSE 1695.834877518565 Test RE 0.393812763596891\n",
      "400 Train Loss 1545.4486 Test MSE 1303.9572965164298 Test RE 0.3453263279750786\n",
      "500 Train Loss 1103.5996 Test MSE 882.0706696812239 Test RE 0.28402050600430107\n",
      "600 Train Loss 769.5448 Test MSE 605.2959196803474 Test RE 0.23527830314724765\n",
      "700 Train Loss 569.21405 Test MSE 445.7657657376864 Test RE 0.201906993627367\n",
      "800 Train Loss 422.84125 Test MSE 333.98225680376294 Test RE 0.17476710959944125\n",
      "900 Train Loss 379.7424 Test MSE 263.93855981257906 Test RE 0.15536365422128365\n",
      "1000 Train Loss 243.92728 Test MSE 200.85222689829914 Test RE 0.13553027881912305\n",
      "1100 Train Loss 275.6111 Test MSE 191.11848382851363 Test RE 0.13220544790916608\n",
      "1200 Train Loss 150.71748 Test MSE 138.0226053483445 Test RE 0.11235001032439125\n",
      "1300 Train Loss 121.61714 Test MSE 110.01096336727089 Test RE 0.10030347747650109\n",
      "1400 Train Loss 102.361694 Test MSE 95.21355921722736 Test RE 0.09331412888873857\n",
      "1500 Train Loss 72.55106 Test MSE 74.470005127721 Test RE 0.08252560142534078\n",
      "1600 Train Loss 57.09672 Test MSE 60.14925121366602 Test RE 0.07416741094233119\n",
      "1700 Train Loss 45.117207 Test MSE 48.62672129873827 Test RE 0.0666861277549584\n",
      "1800 Train Loss 35.5381 Test MSE 39.6476092112894 Test RE 0.06021525199900086\n",
      "1900 Train Loss 31.009865 Test MSE 35.797618587613876 Test RE 0.057216998846482924\n",
      "2000 Train Loss 26.27213 Test MSE 33.17884913203991 Test RE 0.055084405289636915\n",
      "2100 Train Loss 21.703571 Test MSE 28.87309643029268 Test RE 0.05138598512311734\n",
      "2200 Train Loss 18.689917 Test MSE 26.444456732637963 Test RE 0.049177373609200194\n",
      "2300 Train Loss 21.143053 Test MSE 32.22855915892901 Test RE 0.054289826176317356\n",
      "2400 Train Loss 12.825289 Test MSE 22.12974277042287 Test RE 0.04498691260327083\n",
      "2500 Train Loss 11.853698 Test MSE 20.894004830221864 Test RE 0.04371282271422655\n",
      "2600 Train Loss 9.895549 Test MSE 19.97100739610511 Test RE 0.04273640559502083\n",
      "2700 Train Loss 8.84263 Test MSE 18.798790637973422 Test RE 0.041463213863750945\n",
      "2800 Train Loss 7.5342793 Test MSE 17.971543931134402 Test RE 0.040540649347400176\n",
      "2900 Train Loss 62.114857 Test MSE 32.655755416911056 Test RE 0.054648453141329205\n",
      "Training time: 7.01\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 12237.443 Test MSE 10799.852885292757 Test RE 0.9938181546006203\n",
      "100 Train Loss 6658.5327 Test MSE 5801.226809903671 Test RE 0.7283798599629351\n",
      "200 Train Loss 3401.1143 Test MSE 2932.000001470304 Test RE 0.5178213936884147\n",
      "300 Train Loss 1928.0114 Test MSE 1696.4712670846588 Test RE 0.39388664898199854\n",
      "400 Train Loss 1358.7797 Test MSE 1160.181747476313 Test RE 0.3257324459870343\n",
      "500 Train Loss 1002.61835 Test MSE 851.4439712935209 Test RE 0.27904615554059675\n",
      "600 Train Loss 730.9239 Test MSE 642.7978701451793 Test RE 0.24245727552344812\n",
      "700 Train Loss 536.4678 Test MSE 486.7408169605888 Test RE 0.2109827212351108\n",
      "800 Train Loss 406.9748 Test MSE 383.45739876873733 Test RE 0.1872649793755791\n",
      "900 Train Loss 311.00293 Test MSE 299.01906740091266 Test RE 0.1653664668285565\n",
      "1000 Train Loss 241.7455 Test MSE 249.20435797279956 Test RE 0.15096484400144633\n",
      "1100 Train Loss 187.68233 Test MSE 197.71157200938637 Test RE 0.13446648448974463\n",
      "1200 Train Loss 149.88432 Test MSE 170.99601986937648 Test RE 0.12505210513473605\n",
      "1300 Train Loss 154.71756 Test MSE 171.58871117110473 Test RE 0.12526863994470336\n",
      "1400 Train Loss 85.64808 Test MSE 132.92163252809885 Test RE 0.11025437696944498\n",
      "1500 Train Loss 67.154434 Test MSE 110.81439689727523 Test RE 0.10066908001749796\n",
      "1600 Train Loss 53.74148 Test MSE 100.48667593141735 Test RE 0.09586327156054564\n",
      "1700 Train Loss 42.71123 Test MSE 94.95734307596116 Test RE 0.09318849188817714\n",
      "1800 Train Loss 34.34689 Test MSE 83.80127478426043 Test RE 0.08754338167019207\n",
      "1900 Train Loss 27.886635 Test MSE 68.3119396146325 Test RE 0.07903988769212339\n",
      "2000 Train Loss 22.405437 Test MSE 49.09304916097407 Test RE 0.06700512313263152\n",
      "2100 Train Loss 17.803417 Test MSE 50.10184285858173 Test RE 0.06769005335381313\n",
      "2200 Train Loss 14.614341 Test MSE 49.48887382321365 Test RE 0.06727470339438617\n",
      "2300 Train Loss 12.233514 Test MSE 52.7037515283558 Test RE 0.069425460829487\n",
      "2400 Train Loss 10.427684 Test MSE 51.13797831767529 Test RE 0.06838640650731961\n",
      "2500 Train Loss 7.1463413 Test MSE 50.906633499478495 Test RE 0.06823154338016506\n",
      "2600 Train Loss 4.8472843 Test MSE 47.252623909005926 Test RE 0.06573716505089533\n",
      "2700 Train Loss 6.858193 Test MSE 56.60165825633774 Test RE 0.07194698242151276\n",
      "2800 Train Loss 3.745045 Test MSE 43.90130857745707 Test RE 0.06336314701745026\n",
      "2900 Train Loss 3.0224357 Test MSE 43.61234518751823 Test RE 0.06315427103021425\n",
      "Training time: 7.27\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 10985.497 Test MSE 10823.703072650915 Test RE 0.9949149138664479\n",
      "100 Train Loss 5721.988 Test MSE 5876.7411537768085 Test RE 0.7331051785207682\n",
      "200 Train Loss 2808.2185 Test MSE 3076.159228308063 Test RE 0.5303986524436276\n",
      "300 Train Loss 1545.637 Test MSE 1846.2647345739501 Test RE 0.4109083753131528\n",
      "400 Train Loss 1070.9868 Test MSE 1394.9157144780645 Test RE 0.3571675457706657\n",
      "500 Train Loss 922.18085 Test MSE 1241.766114953294 Test RE 0.3369906944213559\n",
      "600 Train Loss 720.13306 Test MSE 1012.6662349296776 Test RE 0.3043204735032397\n",
      "700 Train Loss 667.4332 Test MSE 970.6076439711 Test RE 0.2979338575812827\n",
      "800 Train Loss 490.7889 Test MSE 823.6166002542475 Test RE 0.2744483043324812\n",
      "900 Train Loss 409.9686 Test MSE 726.0226676625151 Test RE 0.25767548154418246\n",
      "1000 Train Loss 361.3105 Test MSE 664.2828234042969 Test RE 0.24647593206767612\n",
      "1100 Train Loss 313.229 Test MSE 593.5946762636881 Test RE 0.23299307051541768\n",
      "1200 Train Loss 300.11136 Test MSE 813.4993279188823 Test RE 0.27275743983646283\n",
      "1300 Train Loss 189.464 Test MSE 391.9729247164216 Test RE 0.18933287989342404\n",
      "1400 Train Loss 160.15788 Test MSE 338.3945036584499 Test RE 0.17591774783390246\n",
      "1500 Train Loss 144.52086 Test MSE 314.05904130186894 Test RE 0.16947422496384537\n",
      "1600 Train Loss 132.13557 Test MSE 291.88735637656976 Test RE 0.1633825416095708\n",
      "1700 Train Loss 122.28447 Test MSE 276.1146520533053 Test RE 0.15890689170623998\n",
      "1800 Train Loss 113.21318 Test MSE 259.4252679948979 Test RE 0.15402958424974314\n",
      "1900 Train Loss 112.23393 Test MSE 247.56673395647013 Test RE 0.15046800047075046\n",
      "2000 Train Loss 106.01963 Test MSE 242.30499210121675 Test RE 0.14886040186011987\n",
      "2100 Train Loss 96.141975 Test MSE 222.04205218782326 Test RE 0.14250024874991998\n",
      "2200 Train Loss 88.028404 Test MSE 208.17623424283184 Test RE 0.13797918655358704\n",
      "2300 Train Loss 82.895325 Test MSE 198.40805302209398 Test RE 0.13470311965186124\n",
      "2400 Train Loss 78.41386 Test MSE 190.78466545448532 Test RE 0.13208993868485916\n",
      "2500 Train Loss 74.14499 Test MSE 184.5470290687532 Test RE 0.12991267796139025\n",
      "2600 Train Loss 70.440994 Test MSE 176.8689181270135 Test RE 0.1271814481699107\n",
      "2700 Train Loss 67.38732 Test MSE 170.96822081982128 Test RE 0.1250419397783446\n",
      "2800 Train Loss 64.04824 Test MSE 166.63217193852196 Test RE 0.12344611699193384\n",
      "2900 Train Loss 61.192802 Test MSE 162.32423131930764 Test RE 0.12183994201667242\n",
      "Training time: 8.48\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 11161.517 Test MSE 10852.254353189392 Test RE 0.9962262666703549\n",
      "100 Train Loss 5967.193 Test MSE 5865.1623299721705 Test RE 0.7323826113043512\n",
      "200 Train Loss 3056.1501 Test MSE 3082.5392349842377 Test RE 0.5309483954703798\n",
      "300 Train Loss 1836.5276 Test MSE 1894.8705666214205 Test RE 0.41628214249628986\n",
      "400 Train Loss 1379.1816 Test MSE 1493.573333124233 Test RE 0.36958240022902505\n",
      "500 Train Loss 943.7454 Test MSE 1013.8738810634326 Test RE 0.30450187677695995\n",
      "600 Train Loss 826.8911 Test MSE 766.5625926483937 Test RE 0.264771854530547\n",
      "700 Train Loss 423.5391 Test MSE 557.5164890498925 Test RE 0.2258015217475288\n",
      "800 Train Loss 312.30453 Test MSE 453.94089951099687 Test RE 0.20375002144215407\n",
      "900 Train Loss 217.81534 Test MSE 298.28599128859895 Test RE 0.16516363595472547\n",
      "1000 Train Loss 156.07109 Test MSE 220.78432610958725 Test RE 0.1420960892694604\n",
      "1100 Train Loss 116.850105 Test MSE 170.87625730030805 Test RE 0.12500830532981144\n",
      "1200 Train Loss 89.66429 Test MSE 158.88442998639917 Test RE 0.1205420786596807\n",
      "1300 Train Loss 66.584946 Test MSE 162.4827791111826 Test RE 0.12189943017473999\n",
      "1400 Train Loss 51.935204 Test MSE 133.77358795666606 Test RE 0.11060714792424185\n",
      "1500 Train Loss 40.994404 Test MSE 124.77105750156288 Test RE 0.10682058060082879\n",
      "1600 Train Loss 34.158695 Test MSE 115.70454023135308 Test RE 0.1028663207656214\n",
      "1700 Train Loss 31.24542 Test MSE 103.53867837419081 Test RE 0.09730817213465334\n",
      "1800 Train Loss 25.795929 Test MSE 91.13951311674596 Test RE 0.09129591765335562\n",
      "1900 Train Loss 17.685768 Test MSE 87.47287567558605 Test RE 0.08944060088907375\n",
      "2000 Train Loss 14.543712 Test MSE 80.82601923214068 Test RE 0.08597528043761749\n",
      "2100 Train Loss 12.2902355 Test MSE 79.31515471608259 Test RE 0.08516793042157453\n",
      "2200 Train Loss 10.716322 Test MSE 83.70022338623741 Test RE 0.08749058384832273\n",
      "2300 Train Loss 11.188218 Test MSE 76.35864487076678 Test RE 0.08356551853377069\n",
      "2400 Train Loss 10.220897 Test MSE 84.1788258703262 Test RE 0.08774036527822246\n",
      "2500 Train Loss 9.258741 Test MSE 72.77246958780675 Test RE 0.08157959835559732\n",
      "2600 Train Loss 4.738601 Test MSE 75.02822671365678 Test RE 0.08283432682634988\n",
      "2700 Train Loss 4.063761 Test MSE 72.29566087409496 Test RE 0.08131190242377169\n",
      "2800 Train Loss 7.8168964 Test MSE 65.92155322078781 Test RE 0.07764468312860437\n",
      "2900 Train Loss 3.5782146 Test MSE 62.769600193738256 Test RE 0.07576570826159651\n",
      "Training time: 8.89\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 3000\n",
    "layers = np.array([1,50,50,50,50,1])\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []\n",
    "\n",
    "    'Generate Training data'\n",
    "    N_train = 300\n",
    "    \n",
    "    x_train_np_array, u_train_np_array = trainingdata(N_train,reps*1234)\n",
    "\n",
    "    'Convert to tensor and send to GPU'\n",
    "    x_train = torch.from_numpy(x_train_np_array).float().to(device)\n",
    "    u_train = torch.from_numpy(u_train_np_array).float().to(device)\n",
    "    \n",
    "    x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "       \n",
    "     \n",
    "    model_NN = Sequentialmodel(layers)\n",
    "    model_NN.to(device)\n",
    "    print(model_NN)\n",
    "    params = list(model_NN.parameters())\n",
    "    #optimizer = optim.Adam(PINN.parameters(), lr=0.0008,betas=(0.9, 0.999), eps=1e-08, weight_decay=0, amsgrad=False)\n",
    "    optimizer = optim.Adam(model_NN.parameters(), lr=0.008)\n",
    "\n",
    "    train_model(max_iter,reps)\n",
    " \n",
    "    torch.save(model_NN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "\n",
    "                 \n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse\": test_mse_full, \"test_re\": test_re_full,\"Time\": elapsed_time, \"Thresh_time\": time_threshold, \"epoch_thresh\": epoch_threshold, \"label\": label}\n",
    "savemat(label+'.mat', mdic) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_pred = model_NN.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f43e0081450>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(x_test,u_pred,'rx')\n",
    "plt.plot(x_test,u_true,'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.1050226811296358\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + test_re_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
