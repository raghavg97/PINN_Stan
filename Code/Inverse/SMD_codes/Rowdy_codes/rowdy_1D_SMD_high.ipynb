{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8byrnUmNKGR",
    "outputId": "510f67c6-b225-41ba-e78a-d0a6e54ded85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uoNYpDzlNKGV"
   },
   "outputs": [],
   "source": [
    "tstart= 0 \n",
    "tstop= 60 \n",
    "\n",
    "increment = 0.1 \n",
    "# Initial condition\n",
    "x_init= [0,0] \n",
    "\n",
    "t = np.arange(tstart,tstop+1,increment)\n",
    "\n",
    "c = 1 # Damping constant \n",
    "k = 0.01 # Stiffness of the spring\n",
    "m = 5 # Mass \n",
    "F0 = 100\n",
    "\n",
    "def mydiff(x, t):    \n",
    "    F =F0\n",
    "    \n",
    "    dx1dt = x[1] \n",
    "    dx2dt = (F -c*x[1] -k*x[0])/m\n",
    "    dxdt= [dx1dt, dx2dt] \n",
    "    \n",
    "    return dxdt \n",
    "\n",
    "x_full_sol = odeint(mydiff, x_init, t) \n",
    "\n",
    "x_sol = x_full_sol[:,0]\n",
    "v_sol = x_full_sol[:,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BR02v-fkNKGV"
   },
   "outputs": [],
   "source": [
    "level = \"high\"\n",
    "label = \"1D_SMD_rowdy_\" + level\n",
    "\n",
    "loss_thresh = 0.005\n",
    "\n",
    "t_test = t.reshape(-1,1)\n",
    "t_test_tensor = torch.from_numpy(t_test).float().to(device)\n",
    "\n",
    "# bc1_t = t[0].reshape(-1,1)\n",
    "# bc1_x = x_sol[0].reshape(-1,1)\n",
    "# t_bc1_train = torch.from_numpy(bc1_t).float().to(device)\n",
    "# x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "\n",
    "bc1_t = t.reshape(-1,1)\n",
    "bc1_x = x_sol.reshape(-1,1)\n",
    "t_bc1_train = torch.from_numpy(bc1_t).float().to(device)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "\n",
    "\n",
    "bc2_t = t[0].reshape(-1,1)\n",
    "t_bc2_train = torch.from_numpy(bc2_t).float().to(device)\n",
    "bc2_val = v_sol[0].reshape(-1,1)\n",
    "bc2_val =torch.from_numpy(bc2_val).float().to(device)\n",
    "\n",
    "t_test = t.reshape(-1,1)\n",
    "t_test_tensor = torch.from_numpy(t_test).float().to(device)\n",
    "x_true = x_sol\n",
    "x_true_norm = np.linalg.norm(x_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(t[0]) \n",
    "ub = np.array(t[-1]) \n",
    "\n",
    "#torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SyyktBKBXRo1"
   },
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    \n",
    "    t01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=t01,random_state =seed)\n",
    "\n",
    "    t_coll = lb + (ub-lb)*sampling(N_f)\n",
    "    # t_coll = np.vstack((t_coll,)) # append training points to collocation points \n",
    "\n",
    "    return t_coll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "o1b21zLnNKGW"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,n_val,rowdy_terms):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "            \n",
    "        self.k = Parameter(torch.tensor(0.0))\n",
    "        self.k.requiresGrad = True\n",
    "        self.c = Parameter(torch.tensor(0.0))\n",
    "        self.c.requiresGrad = True\n",
    "        self.m = Parameter(torch.tensor(0.0))\n",
    "        self.m.requiresGrad = True\n",
    "        \n",
    "        \n",
    "        self.omega1 = Parameter(torch.ones((len(layers)-2,1))) \n",
    "        \n",
    "        \n",
    "        self.alpha = Parameter(torch.zeros(rowdy_terms,len(layers)-2))\n",
    "        self.omega = Parameter((1/n_val)*torch.ones(rowdy_terms,len(layers)-2))\n",
    "        \n",
    "        self.n = torch.tensor(n_val)\n",
    "    \n",
    "    'forward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = 2.0*(x - l_b)/(u_b - l_b)-1.0  #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(self.omega1[i,0]*z)\n",
    "            for j in range(rowdy_terms):\n",
    "                a = a + self.alpha[j,i]*self.n*torch.sin((j+1)*self.n*self.omega[j,i]*z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,t,x):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(t), x)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,t_bc2,bc2_val):\n",
    "        g = t_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        x = self.forward(g)    \n",
    "            \n",
    "        x_t = autograd.grad(x,g,torch.ones([t_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dx_dt = x_t[:,[0]]\n",
    "        \n",
    "        bc2 = dx_dt\n",
    "        \n",
    "        loss_bc2= self.loss_function(bc2,bc2_val)\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self,t_coll,f_hat):\n",
    "             \n",
    "        g = t_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        x = self.forward(g) \n",
    "\n",
    "        x_t = autograd.grad(x,g,torch.ones([t_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        x_tt = autograd.grad(x_t,g,torch.ones(t_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dx_dt = x_t[:,[0]]\n",
    "        \n",
    "        dx2_d2t = x_tt[:,[0]]\n",
    "        \n",
    "        f = self.m*dx2_d2t + self.c*dx_dt + self.k*x - F0\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,t_bc1,x_bc1,t_bc2,bc2_val,t_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(t_bc1,x_bc1)\n",
    "        loss_bc2 = self.loss_BC2(t_bc2,bc2_val)\n",
    "        loss_f = self.loss_PDE(t_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self):\n",
    "        x_pred = self.forward(t_test_tensor)\n",
    "        x_pred = x_pred.cpu().detach().numpy()\n",
    "\n",
    "        return x_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        x_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(x_pred.reshape(-1,1) - x_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(x_pred.reshape(-1,1) - x_true.reshape(-1,1),2)/x_true_norm\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fLY2mT5BOgjD"
   },
   "outputs": [],
   "source": [
    "def train_step(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8srA5uGuObil"
   },
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    m_val.append(PINN.m.cpu().detach().numpy())\n",
    "    k_val.append(PINN.k.cpu().detach().numpy())\n",
    "    c_val.append(PINN.c.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0SezTZ_racQB"
   },
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "\n",
    "    t_coll = colloc_pts(N_f,rep*11)\n",
    "    t_coll =  torch.from_numpy(t_coll).float().to(device)\n",
    "    \n",
    "    f_hat = torch.zeros(t_coll.shape[0],1).to(device)\n",
    "    for i in range(max_iter):      \n",
    "        train_step(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat)\n",
    "         \n",
    "        loss_np = PINN.loss(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test RE\",test_re_loss[-1],\"c\",PINN.c.cpu().detach().numpy(),\"k\",PINN.k.cpu().detach().numpy(),\"m\",PINN.m.cpu().detach().numpy())\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I9BYbcJ0NKGX",
    "outputId": "42720d97-b37a-4c42-b7e5-b77a0ef391f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 6857598.0 Test RE 0.9966919950424951 c -0.00021331053 k 0.0063250205 m -3.3818856e-07\n",
      "1 Train Loss 6537010.0 Test RE 0.9731287143904045 c -0.00057207403 k 0.045268368 m -3.6453207e-06\n",
      "2 Train Loss 6061260.0 Test RE 0.937120492446901 c -0.00047674065 k 0.061892197 m -7.3280507e-06\n",
      "3 Train Loss 3195196.5 Test RE 0.679354586866909 c -0.0007842227 k 0.19292592 m -1.9805992e-05\n",
      "4 Train Loss 1754237.0 Test RE 0.48918842344422625 c -0.0008290277 k 0.23163204 m -1.7896198e-05\n",
      "5 Train Loss 1138719.4 Test RE 0.38426934129336066 c -0.0008233567 k 0.21557054 m 1.2698727e-05\n",
      "6 Train Loss 641133.4 Test RE 0.2834291376492458 c -0.0010490159 k 0.18184052 m 0.00015386865\n",
      "7 Train Loss 193686.69 Test RE 0.16399279595041313 c -0.0018981618 k 0.071548514 m 0.0007252215\n",
      "8 Train Loss 23530.547 Test RE 0.05306591461629238 c -0.002138453 k 0.018313179 m 0.0011106306\n",
      "9 Train Loss 5038.0283 Test RE 0.018339280818359522 c -0.0010987311 k 0.03200753 m 0.0012682558\n",
      "10 Train Loss 3332.478 Test RE 0.009513863506236848 c 0.00039661294 k 0.032497488 m 0.0015083427\n",
      "11 Train Loss 2952.174 Test RE 0.006160268652344018 c 0.0045371912 k 0.03252935 m 0.0020532191\n",
      "12 Train Loss 2786.4983 Test RE 0.00410323084520452 c 0.010318528 k 0.03232915 m 0.0027411408\n",
      "13 Train Loss 2719.8215 Test RE 0.0032416501155507938 c 0.015791459 k 0.032138973 m 0.0033143943\n",
      "14 Train Loss 2685.0496 Test RE 0.002841739445525586 c 0.020220717 k 0.032069523 m 0.003745444\n",
      "15 Train Loss 2675.3376 Test RE 0.00287216960462837 c 0.023086913 k 0.03202695 m 0.0040327418\n",
      "16 Train Loss 2650.0925 Test RE 0.0029199356398459594 c 0.029451784 k 0.03179516 m 0.004649116\n",
      "17 Train Loss 2621.9402 Test RE 0.002877743635699382 c 0.036366537 k 0.031735323 m 0.005290213\n",
      "18 Train Loss 2595.766 Test RE 0.0030373205373631874 c 0.044956993 k 0.03137067 m 0.006072865\n",
      "19 Train Loss 2540.42 Test RE 0.003605486875629184 c 0.06612998 k 0.031131083 m 0.007972841\n",
      "20 Train Loss 2419.2327 Test RE 0.003966179133381085 c 0.10332902 k 0.029977376 m 0.011292993\n",
      "21 Train Loss 2352.0874 Test RE 0.004350579864218312 c 0.12712812 k 0.029124137 m 0.013394621\n",
      "22 Train Loss 2258.8235 Test RE 0.003983842309537707 c 0.1492016 k 0.029490083 m 0.015325637\n",
      "23 Train Loss 2169.8833 Test RE 0.003982113420886017 c 0.1759324 k 0.02862661 m 0.017682564\n",
      "24 Train Loss 2059.8772 Test RE 0.004140856062407654 c 0.21068437 k 0.02678607 m 0.02070647\n",
      "25 Train Loss 1959.9254 Test RE 0.004781060472485217 c 0.2506258 k 0.026624233 m 0.024172291\n",
      "26 Train Loss 1756.4567 Test RE 0.004048483157297542 c 0.30342117 k 0.026245512 m 0.028699812\n",
      "27 Train Loss 1690.8531 Test RE 0.003980672399315796 c 0.324267 k 0.025158275 m 0.030503167\n",
      "28 Train Loss 1603.6528 Test RE 0.0043580563823294935 c 0.36183423 k 0.023828816 m 0.03379278\n",
      "29 Train Loss 1508.5424 Test RE 0.004698312464388317 c 0.40173182 k 0.022726785 m 0.037269093\n",
      "30 Train Loss 1411.5349 Test RE 0.005214917759126859 c 0.4443586 k 0.022504123 m 0.040946744\n",
      "31 Train Loss 1328.6558 Test RE 0.005195009404739726 c 0.4739439 k 0.022303002 m 0.043497507\n",
      "32 Train Loss 1191.7814 Test RE 0.004759545515524365 c 0.52347577 k 0.020780686 m 0.04772165\n",
      "33 Train Loss 1092.4613 Test RE 0.004539470945166918 c 0.5580051 k 0.019605238 m 0.050646424\n",
      "34 Train Loss 1035.2731 Test RE 0.004099070271983071 c 0.58016056 k 0.019466111 m 0.052517094\n",
      "35 Train Loss 997.3837 Test RE 0.003602554844819966 c 0.59978575 k 0.018687734 m 0.054164663\n",
      "36 Train Loss 966.1153 Test RE 0.0032366113814643236 c 0.61738163 k 0.017876543 m 0.0556409\n",
      "37 Train Loss 897.9805 Test RE 0.003230351463079609 c 0.65832794 k 0.017758202 m 0.059102617\n",
      "38 Train Loss 819.45575 Test RE 0.0038702045835619175 c 0.71439224 k 0.017035754 m 0.063877255\n",
      "39 Train Loss 761.946 Test RE 0.004194227336832215 c 0.7653268 k 0.014410096 m 0.06823769\n",
      "40 Train Loss 702.97974 Test RE 0.0040321043131992425 c 0.8148892 k 0.012949378 m 0.07250522\n",
      "41 Train Loss 640.7849 Test RE 0.004097155407677688 c 0.8735244 k 0.011984316 m 0.07758633\n",
      "42 Train Loss 569.46375 Test RE 0.003650622924077817 c 0.92001915 k 0.010086349 m 0.08164672\n",
      "43 Train Loss 484.34277 Test RE 0.0030048286290729823 c 0.97271127 k 0.009692815 m 0.08617668\n",
      "44 Train Loss 444.0385 Test RE 0.0026997170561771622 c 1.0029473 k 0.009829627 m 0.0887613\n",
      "45 Train Loss 409.54526 Test RE 0.002747557892505108 c 1.0380145 k 0.007997346 m 0.09180339\n",
      "46 Train Loss 382.43616 Test RE 0.0024783021670249047 c 1.0738811 k 0.007195664 m 0.09490723\n",
      "47 Train Loss 359.04074 Test RE 0.0025847670483743392 c 1.0877136 k 0.007335437 m 0.09611263\n",
      "48 Train Loss 337.27356 Test RE 0.002742913927745435 c 1.1139941 k 0.006066055 m 0.09840653\n",
      "49 Train Loss 318.52606 Test RE 0.00288745559033979 c 1.1518948 k 0.005403431 m 0.10169918\n",
      "50 Train Loss 315.22137 Test RE 0.0028250361363458963 c 1.1510226 k 0.0055287536 m 0.10162174\n",
      "51 Train Loss 306.8562 Test RE 0.0026273325131499544 c 1.14533 k 0.005624335 m 0.10120314\n",
      "52 Train Loss 295.49966 Test RE 0.0028789683382965943 c 1.1482768 k 0.005551115 m 0.10161659\n",
      "53 Train Loss 291.32938 Test RE 0.002882724765413894 c 1.1387026 k 0.0058456655 m 0.10086549\n",
      "54 Train Loss 283.35202 Test RE 0.0026024431681273474 c 1.1390005 k 0.0057450864 m 0.1010339\n",
      "55 Train Loss 280.94977 Test RE 0.002674923859505908 c 1.1469587 k 0.0054928907 m 0.10184064\n",
      "56 Train Loss 276.6195 Test RE 0.002620661922203862 c 1.1548051 k 0.005426538 m 0.10281449\n",
      "57 Train Loss 273.83994 Test RE 0.0023611301849869853 c 1.1647589 k 0.0052437135 m 0.10388533\n",
      "58 Train Loss 272.23346 Test RE 0.0023380005506948424 c 1.1748314 k 0.0048989872 m 0.104809985\n",
      "59 Train Loss 271.75778 Test RE 0.0024195592209390634 c 1.1796402 k 0.0048045535 m 0.10526179\n",
      "60 Train Loss 270.94626 Test RE 0.002383245994699447 c 1.1781157 k 0.004856565 m 0.10516902\n",
      "61 Train Loss 270.30017 Test RE 0.0022888469202936535 c 1.1721033 k 0.00500908 m 0.104673505\n",
      "62 Train Loss 269.96216 Test RE 0.0023283029633627884 c 1.170619 k 0.0050557647 m 0.104606114\n",
      "63 Train Loss 269.60303 Test RE 0.0023263084508012752 c 1.1710619 k 0.004999532 m 0.10473716\n",
      "64 Train Loss 268.73398 Test RE 0.002246782007456755 c 1.1691077 k 0.004969055 m 0.10472584\n",
      "65 Train Loss 267.7397 Test RE 0.002275868127026218 c 1.1664457 k 0.0051119053 m 0.104712546\n",
      "66 Train Loss 267.26736 Test RE 0.002343273757234028 c 1.1650695 k 0.0051303473 m 0.104644\n",
      "67 Train Loss 267.13214 Test RE 0.002352401328716918 c 1.1649482 k 0.0051360033 m 0.10460973\n",
      "68 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "69 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "70 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "71 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "72 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "73 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "74 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "75 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "76 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "77 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "78 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "79 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "80 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "81 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "82 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "83 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "84 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "85 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "86 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "87 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "88 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "89 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "90 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "91 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "92 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "93 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "94 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "95 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "96 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "97 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "98 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "99 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "100 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "101 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "102 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "103 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "104 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "105 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "106 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "107 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "108 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "109 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "110 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "111 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "112 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "113 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "114 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "115 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "116 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "117 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "118 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "119 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "120 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "121 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "122 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "123 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "124 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "125 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "126 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "127 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "128 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "129 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "130 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "131 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "132 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "133 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "134 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "135 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "136 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "137 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "138 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "139 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "140 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "141 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "142 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "143 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "144 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "145 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "146 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "147 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "148 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "149 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "150 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "151 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "152 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "153 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "154 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "155 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "156 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "157 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "158 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "159 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "160 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "161 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "162 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "163 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "164 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "165 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "166 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "167 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "168 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "169 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "170 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "171 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "172 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "173 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "174 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "175 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "176 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "177 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "178 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "179 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "180 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "181 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "182 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "183 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "184 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "185 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "186 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "187 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "188 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "189 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "190 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "191 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "192 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "193 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "194 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "195 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "196 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "197 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "198 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "199 Train Loss 267.0819 Test RE 0.0023472792285562333 c 1.1643782 k 0.0051800357 m 0.1045966\n",
      "Training time: 76.86\n",
      "Training time: 76.86\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 6743391.0 Test RE 0.9883528681125764 c -0.0004312485 k 0.015545532 m -5.214897e-06\n",
      "1 Train Loss 6363438.0 Test RE 0.9602375193926658 c -0.0012631381 k 0.101827994 m -8.0237205e-06\n",
      "2 Train Loss 3622960.5 Test RE 0.7247705950698279 c -0.0015821977 k 0.16064243 m -1.1825639e-05\n",
      "3 Train Loss 2973255.2 Test RE 0.6556535180460341 c -0.00171189 k 0.18034315 m -1.3324685e-05\n",
      "4 Train Loss 2002675.6 Test RE 0.5245629937022439 c -0.0021218876 k 0.24784197 m -1.7095168e-05\n",
      "5 Train Loss 1497919.9 Test RE 0.4450710639399189 c -0.0021257945 k 0.2523662 m -1.6331342e-05\n",
      "6 Train Loss 1307149.5 Test RE 0.4032407000580726 c -0.002154808 k 0.269148 m -1.3351842e-05\n",
      "7 Train Loss 1140274.8 Test RE 0.35261344392221844 c -0.002207269 k 0.28999093 m -1.0351896e-05\n",
      "8 Train Loss 1055678.1 Test RE 0.33547948791015897 c -0.0023068744 k 0.2900447 m -1.1564347e-05\n",
      "9 Train Loss 847895.06 Test RE 0.2956789382301728 c -0.0028429218 k 0.2744826 m -1.3313265e-06\n",
      "10 Train Loss 756939.8 Test RE 0.25861913886853544 c -0.003108431 k 0.28585032 m 6.108297e-06\n",
      "11 Train Loss 639090.0 Test RE 0.18566079609102284 c -0.0035115336 k 0.31374502 m 2.1612826e-05\n",
      "12 Train Loss 614895.0 Test RE 0.15552819850513377 c -0.0035922853 k 0.32509112 m 2.761114e-05\n",
      "13 Train Loss 600389.75 Test RE 0.15654730488106094 c -0.003494426 k 0.31967062 m 3.3845226e-05\n",
      "14 Train Loss 523020.8 Test RE 0.17064543258343018 c -0.0034899365 k 0.2797395 m 6.742503e-05\n",
      "15 Train Loss 283932.16 Test RE 0.15912593536400688 c -0.0049547474 k 0.17083529 m 0.00025468686\n",
      "16 Train Loss 252964.03 Test RE 0.15037333436640252 c -0.005022283 k 0.15872502 m 0.00027953926\n",
      "17 Train Loss 238844.47 Test RE 0.15071145641350994 c -0.004920138 k 0.14997068 m 0.0002743535\n",
      "18 Train Loss 199999.81 Test RE 0.15645193618417225 c -0.005105298 k 0.10276104 m 0.00030410907\n",
      "19 Train Loss 164314.36 Test RE 0.14653474143540532 c -0.0054141 k 0.07971648 m 0.00033607296\n",
      "20 Train Loss 108281.164 Test RE 0.12105916939845489 c -0.0062875208 k 0.06078278 m 0.00040953667\n",
      "21 Train Loss 63684.125 Test RE 0.08928216367693581 c -0.006620903 k 0.06483391 m 0.000425715\n",
      "22 Train Loss 58518.758 Test RE 0.08271315552424373 c -0.006695672 k 0.07040839 m 0.00041936673\n",
      "23 Train Loss 48802.605 Test RE 0.07693133764768834 c -0.0070468 k 0.062450763 m 0.00043068695\n",
      "24 Train Loss 18258.094 Test RE 0.046991000843096695 c -0.008080028 k 0.041990492 m 0.00047525205\n",
      "25 Train Loss 13729.167 Test RE 0.03815607451165798 c -0.0081954785 k 0.04593081 m 0.00047332345\n",
      "26 Train Loss 12832.719 Test RE 0.0344482137060828 c -0.008274158 k 0.05016285 m 0.00047258296\n",
      "27 Train Loss 11705.297 Test RE 0.033404768918411064 c -0.008637309 k 0.047206797 m 0.0004817854\n",
      "28 Train Loss 5951.6113 Test RE 0.019721512873612703 c -0.010721282 k 0.024473721 m 0.00056105\n",
      "29 Train Loss 3113.1143 Test RE 0.0048350080169605 c -0.011306628 k 0.03219592 m 0.00058369007\n",
      "30 Train Loss 3008.967 Test RE 0.0049444703178336555 c -0.011351947 k 0.03269029 m 0.0005852615\n",
      "31 Train Loss 3006.8345 Test RE 0.00504215790100938 c -0.011362186 k 0.032666013 m 0.0005858226\n",
      "32 Train Loss 2998.6096 Test RE 0.005083992917578679 c -0.011354902 k 0.03266875 m 0.00058812194\n",
      "33 Train Loss 2996.9565 Test RE 0.005023943072447955 c -0.011350637 k 0.032502387 m 0.0005887953\n",
      "34 Train Loss 2996.5508 Test RE 0.005004079642364182 c -0.011356729 k 0.032542966 m 0.000588362\n",
      "35 Train Loss 2995.4807 Test RE 0.005058031437582156 c -0.011369293 k 0.0326499 m 0.0005883999\n",
      "36 Train Loss 2994.7195 Test RE 0.005129799906926973 c -0.011366899 k 0.03253298 m 0.00058848516\n",
      "37 Train Loss 2993.8489 Test RE 0.005170004787191316 c -0.011369199 k 0.032550644 m 0.00058748823\n",
      "38 Train Loss 2986.6895 Test RE 0.005076825447070893 c -0.011396143 k 0.032815497 m 0.0005856445\n",
      "39 Train Loss 2965.864 Test RE 0.004693731483704832 c -0.011401776 k 0.032465518 m 0.00058361347\n",
      "40 Train Loss 2948.784 Test RE 0.004419792278140628 c -0.0113420235 k 0.032792352 m 0.00057726575\n",
      "41 Train Loss 2947.437 Test RE 0.004422921801957898 c -0.011317746 k 0.032713234 m 0.0005772458\n",
      "42 Train Loss 2947.0369 Test RE 0.004336385110988155 c -0.011316959 k 0.032735124 m 0.0005771619\n",
      "43 Train Loss 2945.4314 Test RE 0.004270649207260884 c -0.0113403015 k 0.03274113 m 0.00057755946\n",
      "44 Train Loss 2941.3386 Test RE 0.004460781971745654 c -0.011341856 k 0.032674007 m 0.00057933596\n",
      "45 Train Loss 2940.8367 Test RE 0.00446215950844383 c -0.0113301445 k 0.03273882 m 0.00057838275\n",
      "46 Train Loss 2940.3657 Test RE 0.004440493214395247 c -0.011343981 k 0.032755062 m 0.0005779006\n",
      "47 Train Loss 2939.616 Test RE 0.004521725790437864 c -0.011366255 k 0.032787826 m 0.0005780394\n",
      "48 Train Loss 2937.1587 Test RE 0.004645451271017662 c -0.011418921 k 0.032803755 m 0.0005775871\n",
      "49 Train Loss 2929.2961 Test RE 0.004630831437142374 c -0.011464457 k 0.032744195 m 0.00057531864\n",
      "50 Train Loss 2919.8096 Test RE 0.004584719271577686 c -0.011507691 k 0.03278129 m 0.0005741448\n",
      "51 Train Loss 2914.9214 Test RE 0.004510654090702807 c -0.011535214 k 0.032797765 m 0.00057561754\n",
      "52 Train Loss 2913.3274 Test RE 0.004454800418788104 c -0.011527555 k 0.03280521 m 0.00057625835\n",
      "53 Train Loss 2912.6406 Test RE 0.004431569070348292 c -0.011495686 k 0.032817308 m 0.00057640555\n",
      "54 Train Loss 2912.323 Test RE 0.00441684495112761 c -0.011466536 k 0.032802876 m 0.0005763991\n",
      "55 Train Loss 2912.1516 Test RE 0.004425602052832687 c -0.011464358 k 0.032800153 m 0.00057647674\n",
      "56 Train Loss 2911.351 Test RE 0.004460111013820972 c -0.011414783 k 0.03272793 m 0.00057969516\n",
      "57 Train Loss 2910.711 Test RE 0.004432305605385071 c -0.011351247 k 0.03276997 m 0.0005825872\n",
      "58 Train Loss 2910.2568 Test RE 0.0044012274176831725 c -0.011306959 k 0.032787804 m 0.00058554014\n",
      "59 Train Loss 2909.3042 Test RE 0.004410593526506619 c -0.011255562 k 0.032777995 m 0.00059227925\n",
      "60 Train Loss 2909.0073 Test RE 0.004438098779421591 c -0.0112090185 k 0.032803126 m 0.0005975133\n",
      "61 Train Loss 2907.2312 Test RE 0.004493822364481968 c -0.010637876 k 0.032719366 m 0.0006389101\n",
      "62 Train Loss 2905.3516 Test RE 0.004495953237449648 c -0.009935167 k 0.032723635 m 0.0006873443\n",
      "63 Train Loss 2904.4395 Test RE 0.004512801312598349 c -0.009313947 k 0.032727942 m 0.00073215563\n",
      "64 Train Loss 2902.8242 Test RE 0.004523337929242449 c -0.009126142 k 0.032743953 m 0.0007497459\n",
      "65 Train Loss 2902.6187 Test RE 0.004501882756973562 c -0.009198843 k 0.032746375 m 0.0007443765\n",
      "66 Train Loss 2902.4207 Test RE 0.004514856550324683 c -0.009074344 k 0.032715455 m 0.0007534109\n",
      "67 Train Loss 2902.2136 Test RE 0.004527196785060216 c -0.008844762 k 0.03273879 m 0.0007704444\n",
      "68 Train Loss 2902.1309 Test RE 0.004527364877678396 c -0.008774848 k 0.032738827 m 0.0007756841\n",
      "69 Train Loss 2902.0642 Test RE 0.004522767005329336 c -0.008825807 k 0.03272456 m 0.00077184686\n",
      "70 Train Loss 2901.942 Test RE 0.004525167157418686 c -0.008725844 k 0.032740425 m 0.00077912107\n",
      "71 Train Loss 2901.5374 Test RE 0.004539330115013914 c -0.008471872 k 0.03273813 m 0.00079742225\n",
      "72 Train Loss 2899.7358 Test RE 0.00452282772396496 c -0.008150136 k 0.032727838 m 0.0008223701\n",
      "73 Train Loss 2896.9417 Test RE 0.004510474179407281 c -0.0076294662 k 0.03269591 m 0.0008612604\n",
      "74 Train Loss 2884.951 Test RE 0.004491749659940881 c -0.0053745294 k 0.032701723 m 0.001025001\n",
      "75 Train Loss 2860.181 Test RE 0.004425153914930529 c -0.00073858525 k 0.032311205 m 0.0013678306\n",
      "76 Train Loss 2805.6711 Test RE 0.00394304490541249 c 0.0037664392 k 0.032472514 m 0.0017073465\n",
      "77 Train Loss 2785.4941 Test RE 0.003680015104947908 c 0.00564689 k 0.032679163 m 0.0018603009\n",
      "78 Train Loss 2773.884 Test RE 0.0033935200127270535 c 0.004936956 k 0.032537114 m 0.0018267259\n",
      "79 Train Loss 2770.9773 Test RE 0.003307081951762273 c 0.0046229996 k 0.032439694 m 0.0018086714\n",
      "80 Train Loss 2769.0137 Test RE 0.00326027774754747 c 0.0046394407 k 0.032583013 m 0.0018144129\n",
      "81 Train Loss 2765.091 Test RE 0.003186734411958647 c 0.0050565246 k 0.032502424 m 0.0018521624\n",
      "82 Train Loss 2744.4238 Test RE 0.002971054648080485 c 0.007440116 k 0.032441795 m 0.0020368814\n",
      "83 Train Loss 2733.2961 Test RE 0.0028494185284069057 c 0.009400542 k 0.032432046 m 0.002180729\n",
      "84 Train Loss 2723.5894 Test RE 0.0029177666196191143 c 0.012141215 k 0.032611687 m 0.0023799501\n",
      "85 Train Loss 2712.0496 Test RE 0.002868429042944938 c 0.014817969 k 0.032154027 m 0.0025801323\n",
      "86 Train Loss 2694.473 Test RE 0.0028174268862259924 c 0.018586414 k 0.03213033 m 0.002851475\n",
      "87 Train Loss 2669.2617 Test RE 0.00283360610868007 c 0.024936063 k 0.032054897 m 0.0033023697\n",
      "88 Train Loss 2642.2612 Test RE 0.002890085485188319 c 0.03276235 k 0.03178561 m 0.003842779\n",
      "89 Train Loss 2615.832 Test RE 0.0028937139616551154 c 0.039666247 k 0.031615518 m 0.0043154834\n",
      "90 Train Loss 2603.8672 Test RE 0.002828434066690976 c 0.04193884 k 0.031563412 m 0.0044808397\n",
      "91 Train Loss 2600.7202 Test RE 0.0027558121304869392 c 0.04190069 k 0.031532485 m 0.004485705\n",
      "92 Train Loss 2599.453 Test RE 0.0027419440023063314 c 0.041876726 k 0.031561155 m 0.004482272\n",
      "93 Train Loss 2598.6978 Test RE 0.0026396713515953233 c 0.041122664 k 0.0315713 m 0.0044293334\n",
      "94 Train Loss 2597.6663 Test RE 0.0026186459117959818 c 0.041210305 k 0.031605706 m 0.004433769\n",
      "95 Train Loss 2595.5369 Test RE 0.0025893856973615383 c 0.041474506 k 0.031491324 m 0.004447561\n",
      "96 Train Loss 2592.1316 Test RE 0.002563687886066389 c 0.041964725 k 0.031520274 m 0.0044719675\n",
      "97 Train Loss 2589.28 Test RE 0.0025257103496103086 c 0.042423677 k 0.03156013 m 0.0045019267\n",
      "98 Train Loss 2585.4736 Test RE 0.0023867680036177507 c 0.04226566 k 0.031425983 m 0.0044829473\n",
      "99 Train Loss 2582.489 Test RE 0.0023374155499767124 c 0.042506557 k 0.03158482 m 0.0044919327\n",
      "100 Train Loss 2578.2375 Test RE 0.002386182928100641 c 0.04388211 k 0.031577367 m 0.0045815124\n",
      "101 Train Loss 2574.0896 Test RE 0.0022032963364778256 c 0.043569826 k 0.03138504 m 0.004555242\n",
      "102 Train Loss 2568.763 Test RE 0.002117304779928865 c 0.044539765 k 0.03157388 m 0.004616202\n",
      "103 Train Loss 2561.7615 Test RE 0.0021442504369808667 c 0.046807513 k 0.031305112 m 0.0047643683\n",
      "104 Train Loss 2549.6816 Test RE 0.0019122665407843578 c 0.048679736 k 0.031399675 m 0.0048818127\n",
      "105 Train Loss 2539.211 Test RE 0.0017156108184139744 c 0.049652148 k 0.031616904 m 0.0049371263\n",
      "106 Train Loss 2508.8416 Test RE 0.001673190004028327 c 0.0568793 k 0.031293403 m 0.0054115956\n",
      "107 Train Loss 2480.139 Test RE 0.0015280226370351644 c 0.06235793 k 0.031093251 m 0.005750366\n",
      "108 Train Loss 2457.5293 Test RE 0.0014078435794191919 c 0.06725009 k 0.030978605 m 0.0060409037\n",
      "109 Train Loss 2438.8545 Test RE 0.0011479347364797832 c 0.071652055 k 0.030875802 m 0.006297777\n",
      "110 Train Loss 2427.6855 Test RE 0.001407174375723963 c 0.07535159 k 0.03090609 m 0.0065101525\n",
      "111 Train Loss 2410.0166 Test RE 0.001262763626530897 c 0.07959186 k 0.030695304 m 0.00674817\n",
      "112 Train Loss 2400.6848 Test RE 0.0010625868087563409 c 0.081699766 k 0.030685516 m 0.006863543\n",
      "113 Train Loss 2395.0908 Test RE 0.0011999593432003195 c 0.083980225 k 0.03061397 m 0.0069849733\n",
      "114 Train Loss 2387.2314 Test RE 0.0011370319728315145 c 0.08631663 k 0.030529438 m 0.0071231434\n",
      "115 Train Loss 2383.991 Test RE 0.001245195200647519 c 0.08815998 k 0.030545942 m 0.007241305\n",
      "116 Train Loss 2380.3093 Test RE 0.0015004800404761884 c 0.09134931 k 0.03055774 m 0.0074383095\n",
      "117 Train Loss 2368.5767 Test RE 0.0014977960824718957 c 0.09509635 k 0.030322282 m 0.00766135\n",
      "118 Train Loss 2350.6892 Test RE 0.001308667042802017 c 0.09863479 k 0.030293824 m 0.007872766\n",
      "119 Train Loss 2329.033 Test RE 0.001354774570569479 c 0.10343914 k 0.030128464 m 0.008169547\n",
      "120 Train Loss 2315.8447 Test RE 0.0014265311632223198 c 0.106768586 k 0.030019669 m 0.008369929\n",
      "121 Train Loss 2302.6262 Test RE 0.001620623004036294 c 0.111894526 k 0.029886624 m 0.008679\n",
      "122 Train Loss 2278.1633 Test RE 0.0013599708913592178 c 0.116555616 k 0.029850626 m 0.008968304\n",
      "123 Train Loss 2265.7998 Test RE 0.001558739134288407 c 0.12167764 k 0.029718189 m 0.009298398\n",
      "124 Train Loss 2253.4966 Test RE 0.002594755643174856 c 0.1334001 k 0.029508227 m 0.010062736\n",
      "125 Train Loss 2207.3745 Test RE 0.0024301365466297053 c 0.14542443 k 0.029430838 m 0.010862496\n",
      "126 Train Loss 2146.3894 Test RE 0.0016964174192830453 c 0.15921497 k 0.02869717 m 0.011775434\n",
      "127 Train Loss 2120.042 Test RE 0.00188183802526253 c 0.17109719 k 0.028542496 m 0.012549451\n",
      "128 Train Loss 2088.825 Test RE 0.0018818500619059992 c 0.18204242 k 0.028075252 m 0.013248568\n",
      "129 Train Loss 2054.9639 Test RE 0.0015601500095494202 c 0.18631603 k 0.028336639 m 0.013496518\n",
      "130 Train Loss 2026.1842 Test RE 0.0013848156558213505 c 0.19102143 k 0.028014544 m 0.013766399\n",
      "131 Train Loss 2008.206 Test RE 0.0011555545184995717 c 0.19635762 k 0.027886033 m 0.014088657\n",
      "132 Train Loss 1992.1134 Test RE 0.0010787024013418796 c 0.19960862 k 0.028195905 m 0.014267737\n",
      "133 Train Loss 1979.6631 Test RE 0.0011676576085546855 c 0.20286134 k 0.027662747 m 0.014449294\n",
      "134 Train Loss 1972.9856 Test RE 0.0012778922659967046 c 0.20581427 k 0.027676636 m 0.014624485\n",
      "135 Train Loss 1971.5297 Test RE 0.0013134589001084181 c 0.20580009 k 0.027898133 m 0.014623887\n",
      "136 Train Loss 1968.19 Test RE 0.0011666250880504802 c 0.20540386 k 0.027749589 m 0.01460256\n",
      "137 Train Loss 1951.1927 Test RE 0.0013808683283800473 c 0.20962225 k 0.02733027 m 0.0148748625\n",
      "138 Train Loss 1932.0184 Test RE 0.001838132483870954 c 0.21729563 k 0.027494142 m 0.015373745\n",
      "139 Train Loss 1912.3234 Test RE 0.002048043502048054 c 0.22615188 k 0.027386146 m 0.015946979\n",
      "140 Train Loss 1889.5039 Test RE 0.0023312642747029457 c 0.23770969 k 0.0274952 m 0.01670484\n",
      "141 Train Loss 1866.6443 Test RE 0.0031992127485753158 c 0.2543036 k 0.026375063 m 0.017796375\n",
      "142 Train Loss 1783.2777 Test RE 0.0024922020619949905 c 0.27524832 k 0.026768964 m 0.019207839\n",
      "143 Train Loss 1741.7699 Test RE 0.0021250069235385157 c 0.28168696 k 0.026393183 m 0.01965448\n",
      "144 Train Loss 1737.0807 Test RE 0.00194341143213123 c 0.28219217 k 0.025930919 m 0.019692656\n",
      "145 Train Loss 1733.9116 Test RE 0.0016409135065315373 c 0.28094625 k 0.026125755 m 0.019615116\n",
      "146 Train Loss 1732.0214 Test RE 0.0016015836286207934 c 0.27966762 k 0.02611717 m 0.01953293\n",
      "147 Train Loss 1728.3508 Test RE 0.0014141884728684105 c 0.27954122 k 0.026059477 m 0.019528579\n",
      "148 Train Loss 1724.6235 Test RE 0.0015916192644951255 c 0.28162703 k 0.026060998 m 0.019662619\n",
      "149 Train Loss 1717.9635 Test RE 0.001605286663514062 c 0.28360015 k 0.02597644 m 0.019791031\n",
      "150 Train Loss 1702.2261 Test RE 0.0016710136871509558 c 0.29411957 k 0.025675103 m 0.020470476\n",
      "151 Train Loss 1661.7114 Test RE 0.0016321245776591557 c 0.3158747 k 0.025256444 m 0.021882692\n",
      "152 Train Loss 1643.2375 Test RE 0.0021955245975932712 c 0.3231153 k 0.025142498 m 0.022357332\n",
      "153 Train Loss 1640.9784 Test RE 0.0024843420621407813 c 0.32880718 k 0.02524925 m 0.022734726\n",
      "154 Train Loss 1606.7346 Test RE 0.004096225457902467 c 0.36838254 k 0.023891127 m 0.025360743\n",
      "155 Train Loss 1537.7456 Test RE 0.004572691853878535 c 0.39246008 k 0.024015436 m 0.026987566\n",
      "156 Train Loss 1449.5216 Test RE 0.004155349103268719 c 0.4069488 k 0.023192853 m 0.02796878\n",
      "157 Train Loss 1422.1793 Test RE 0.00424379365097978 c 0.4177833 k 0.022839356 m 0.028695824\n",
      "158 Train Loss 1390.1085 Test RE 0.004217283837865766 c 0.4319644 k 0.022522558 m 0.029616252\n",
      "159 Train Loss 1314.1783 Test RE 0.003939487827119872 c 0.45539647 k 0.021741409 m 0.031113233\n",
      "160 Train Loss 1212.7888 Test RE 0.0031875888074615793 c 0.4846646 k 0.020771204 m 0.032987077\n",
      "161 Train Loss 1088.5859 Test RE 0.0029142145575588446 c 0.53183913 k 0.020042986 m 0.036056247\n",
      "162 Train Loss 1053.5918 Test RE 0.003166388632034722 c 0.55113375 k 0.020005463 m 0.037323162\n",
      "163 Train Loss 1046.366 Test RE 0.0034355370327068657 c 0.5595049 k 0.019667082 m 0.03787096\n",
      "164 Train Loss 1023.76373 Test RE 0.003841629125227728 c 0.5739022 k 0.019421466 m 0.038822167\n",
      "165 Train Loss 993.921 Test RE 0.0036931861026087724 c 0.5862588 k 0.0189273 m 0.039646845\n",
      "166 Train Loss 929.75275 Test RE 0.0035332330970075975 c 0.6169199 k 0.018301753 m 0.04163885\n",
      "167 Train Loss 889.86707 Test RE 0.003909982748433773 c 0.6444459 k 0.017626602 m 0.04343528\n",
      "168 Train Loss 829.8361 Test RE 0.0039040576308943097 c 0.6745489 k 0.016986027 m 0.045425586\n",
      "169 Train Loss 773.0869 Test RE 0.003785592741620449 c 0.7036715 k 0.016402772 m 0.04737428\n",
      "170 Train Loss 742.004 Test RE 0.003657727663807941 c 0.7180201 k 0.015886344 m 0.048354074\n",
      "171 Train Loss 732.82806 Test RE 0.003847903564131195 c 0.724495 k 0.01584448 m 0.04880321\n",
      "172 Train Loss 720.44855 Test RE 0.0035439255489669442 c 0.7221698 k 0.01583902 m 0.048677117\n",
      "173 Train Loss 714.7827 Test RE 0.0030627471431989165 c 0.716085 k 0.01587553 m 0.04828189\n",
      "174 Train Loss 713.97797 Test RE 0.0029516511781614158 c 0.71409166 k 0.015983578 m 0.04815248\n",
      "175 Train Loss 709.80005 Test RE 0.00303258511612206 c 0.7151107 k 0.015888644 m 0.048215967\n",
      "176 Train Loss 708.30493 Test RE 0.0030157228892413995 c 0.7168874 k 0.015947333 m 0.048329588\n",
      "177 Train Loss 707.71185 Test RE 0.0030471159833437528 c 0.71814567 k 0.015948119 m 0.04841618\n",
      "178 Train Loss 705.2467 Test RE 0.00301494506625162 c 0.7180065 k 0.015879054 m 0.04843058\n",
      "179 Train Loss 704.4633 Test RE 0.0029039900461521237 c 0.7165125 k 0.015904786 m 0.048330124\n",
      "180 Train Loss 701.3087 Test RE 0.002993262903138461 c 0.7190476 k 0.015879463 m 0.048502494\n",
      "181 Train Loss 699.0083 Test RE 0.0030876536645689655 c 0.7214491 k 0.015772324 m 0.048676956\n",
      "182 Train Loss 695.67487 Test RE 0.003026271961317544 c 0.7234695 k 0.015752815 m 0.04880772\n",
      "183 Train Loss 691.0257 Test RE 0.003284558290306328 c 0.7319625 k 0.015555318 m 0.04937334\n",
      "184 Train Loss 688.73584 Test RE 0.003475247377900632 c 0.7388586 k 0.0153876105 m 0.049823623\n",
      "185 Train Loss 687.6753 Test RE 0.0034229368606153066 c 0.7405523 k 0.015370788 m 0.049930215\n",
      "186 Train Loss 687.2568 Test RE 0.003384081913740129 c 0.73965365 k 0.015350849 m 0.049864233\n",
      "187 Train Loss 687.2568 Test RE 0.003384081913740129 c 0.73965365 k 0.015350849 m 0.049864233\n",
      "188 Train Loss 687.2568 Test RE 0.003384081913740129 c 0.73965365 k 0.015350849 m 0.049864233\n",
      "189 Train Loss 687.2568 Test RE 0.003384081913740129 c 0.73965365 k 0.015350849 m 0.049864233\n",
      "190 Train Loss 687.2568 Test RE 0.003384081913740129 c 0.73965365 k 0.015350849 m 0.049864233\n",
      "191 Train Loss 687.2568 Test RE 0.003384081913740129 c 0.73965365 k 0.015350849 m 0.049864233\n",
      "192 Train Loss 687.2568 Test RE 0.003384081913740129 c 0.73965365 k 0.015350849 m 0.049864233\n",
      "193 Train Loss 687.2568 Test RE 0.003384081913740129 c 0.73965365 k 0.015350849 m 0.049864233\n",
      "194 Train Loss 687.2568 Test RE 0.003384081913740129 c 0.73965365 k 0.015350849 m 0.049864233\n",
      "195 Train Loss 687.2568 Test RE 0.003384081913740129 c 0.73965365 k 0.015350849 m 0.049864233\n",
      "196 Train Loss 687.2568 Test RE 0.003384081913740129 c 0.73965365 k 0.015350849 m 0.049864233\n",
      "197 Train Loss 687.2568 Test RE 0.003384081913740129 c 0.73965365 k 0.015350849 m 0.049864233\n",
      "198 Train Loss 687.2568 Test RE 0.003384081913740129 c 0.73965365 k 0.015350849 m 0.049864233\n",
      "199 Train Loss 687.2568 Test RE 0.003384081913740129 c 0.73965365 k 0.015350849 m 0.049864233\n",
      "Training time: 87.27\n",
      "Training time: 87.27\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 6752145.0 Test RE 0.9889909348081326 c 0.00035145486 k 0.0092678685 m -8.688632e-06\n",
      "1 Train Loss 6589715.5 Test RE 0.9770119779590187 c 0.00041740367 k 0.01243465 m -9.152585e-06\n",
      "2 Train Loss 6216004.0 Test RE 0.9489233884672218 c 0.00087426393 k 0.03664974 m -2.2044565e-05\n",
      "3 Train Loss 5820359.0 Test RE 0.9181767835053724 c 0.0010132277 k 0.044280685 m -2.6406788e-05\n",
      "4 Train Loss 5172320.0 Test RE 0.865496864385057 c 0.001193175 k 0.055753376 m -3.340688e-05\n",
      "5 Train Loss 4846379.0 Test RE 0.8375791627945656 c 0.0013379127 k 0.06340571 m -3.7996917e-05\n",
      "6 Train Loss 4513598.5 Test RE 0.8085482919747352 c 0.0014157847 k 0.06656897 m -4.030555e-05\n",
      "7 Train Loss 3812522.5 Test RE 0.7431325629021145 c 0.0017836669 k 0.087653026 m -5.5397628e-05\n",
      "8 Train Loss 3162643.5 Test RE 0.6746869770413327 c 0.0021888847 k 0.11052916 m -6.967454e-05\n",
      "9 Train Loss 2347557.5 Test RE 0.5749590510170498 c 0.0026151224 k 0.13597022 m -8.511817e-05\n",
      "10 Train Loss 1581006.5 Test RE 0.46894785582555054 c 0.0030114248 k 0.15959021 m -9.755023e-05\n",
      "11 Train Loss 1173545.4 Test RE 0.40553805130622617 c 0.0031462759 k 0.16786073 m -0.000101918595\n",
      "12 Train Loss 762382.06 Test RE 0.3174562741635522 c 0.0033060645 k 0.17786449 m -0.00010597095\n",
      "13 Train Loss 569068.56 Test RE 0.26416327148207674 c 0.003448323 k 0.18668617 m -0.00010886039\n",
      "14 Train Loss 485882.75 Test RE 0.23429402513246786 c 0.0035423061 k 0.1921165 m -0.00010812767\n",
      "15 Train Loss 362806.0 Test RE 0.1872790584300467 c 0.0035866133 k 0.19434586 m -9.287245e-05\n",
      "16 Train Loss 311315.03 Test RE 0.16144243480852272 c 0.0035845723 k 0.19385768 m -7.68416e-05\n",
      "17 Train Loss 270995.8 Test RE 0.13840102160451626 c 0.0035942502 k 0.19331698 m -4.1117935e-05\n",
      "18 Train Loss 236544.7 Test RE 0.12063363977105662 c 0.0035508187 k 0.18986738 m -7.976832e-06\n",
      "19 Train Loss 211950.94 Test RE 0.10116071099872531 c 0.0035381808 k 0.18793483 m 3.6448e-05\n",
      "20 Train Loss 199230.38 Test RE 0.09546257326019512 c 0.003510028 k 0.18497433 m 7.811703e-05\n",
      "21 Train Loss 189788.89 Test RE 0.08940220354102336 c 0.003484207 k 0.182445 m 0.000114617076\n",
      "22 Train Loss 164372.94 Test RE 0.07686297143114039 c 0.003353132 k 0.17166638 m 0.00026346213\n",
      "23 Train Loss 148058.1 Test RE 0.07908444069801013 c 0.0032371744 k 0.162465 m 0.00038166865\n",
      "24 Train Loss 131863.6 Test RE 0.08496576840025725 c 0.003058499 k 0.1482877 m 0.00057934725\n",
      "25 Train Loss 124876.95 Test RE 0.08491041211868225 c 0.0029890249 k 0.14285651 m 0.00065506255\n",
      "26 Train Loss 120930.25 Test RE 0.08373646353978575 c 0.0029645506 k 0.14045762 m 0.00069500256\n",
      "27 Train Loss 112354.875 Test RE 0.07782838709781338 c 0.002935205 k 0.1373826 m 0.00075329334\n",
      "28 Train Loss 107314.836 Test RE 0.07646553373668799 c 0.0028924532 k 0.13378061 m 0.0008085924\n",
      "29 Train Loss 94142.05 Test RE 0.07645239585101563 c 0.002733852 k 0.12346343 m 0.0009063405\n",
      "30 Train Loss 83325.305 Test RE 0.06632954915187075 c 0.0026668718 k 0.11964152 m 0.0009474684\n",
      "31 Train Loss 73644.305 Test RE 0.058888943330954194 c 0.0026313108 k 0.11700462 m 0.0010095943\n",
      "32 Train Loss 63887.1 Test RE 0.052859505694046634 c 0.0025510478 k 0.11102924 m 0.0010827614\n",
      "33 Train Loss 52405.945 Test RE 0.050514690198894534 c 0.0024020527 k 0.10092121 m 0.0011765751\n",
      "34 Train Loss 38066.06 Test RE 0.04566523166152832 c 0.0022238593 k 0.08828423 m 0.0013039896\n",
      "35 Train Loss 32545.691 Test RE 0.04412648292039322 c 0.00214519 k 0.082108505 m 0.0013705133\n",
      "36 Train Loss 28159.281 Test RE 0.03994492605075183 c 0.0020854583 k 0.07853067 m 0.0013985282\n",
      "37 Train Loss 19419.02 Test RE 0.04236346549115937 c 0.0017549399 k 0.05785867 m 0.0015574949\n",
      "38 Train Loss 15581.119 Test RE 0.03804874967396658 c 0.001680327 k 0.05293561 m 0.0016105406\n",
      "39 Train Loss 14298.354 Test RE 0.036612506527448005 c 0.0016440373 k 0.051238112 m 0.0016169591\n",
      "40 Train Loss 12930.701 Test RE 0.03514073003964205 c 0.0015921006 k 0.04850602 m 0.0016188228\n",
      "41 Train Loss 12257.271 Test RE 0.034579639071804905 c 0.0015587868 k 0.04631472 m 0.001632938\n",
      "42 Train Loss 10080.3125 Test RE 0.03068729146133315 c 0.0015234572 k 0.0438072 m 0.0016351972\n",
      "43 Train Loss 8716.27 Test RE 0.027356278216401443 c 0.0015315933 k 0.04377428 m 0.0016261357\n",
      "44 Train Loss 7918.26 Test RE 0.02567269566160077 c 0.0015119629 k 0.04251384 m 0.0016298628\n",
      "45 Train Loss 7416.0654 Test RE 0.02492851765669536 c 0.0014870751 k 0.040725887 m 0.0016410057\n",
      "46 Train Loss 7121.8545 Test RE 0.023474140226636436 c 0.0015142233 k 0.04217336 m 0.0016351702\n",
      "47 Train Loss 6766.7036 Test RE 0.02316388413198564 c 0.0014809141 k 0.03981887 m 0.0016613638\n",
      "48 Train Loss 6323.0107 Test RE 0.022159548952758856 c 0.0014565991 k 0.03823756 m 0.0016718202\n",
      "49 Train Loss 5924.6055 Test RE 0.020420327558793167 c 0.001479695 k 0.03949895 m 0.001666578\n",
      "50 Train Loss 5256.262 Test RE 0.018942042704273302 c 0.0014186531 k 0.035246804 m 0.0016949676\n",
      "51 Train Loss 4978.7134 Test RE 0.017895105764707715 c 0.0014295686 k 0.03546672 m 0.0016949871\n",
      "52 Train Loss 4806.4087 Test RE 0.017226000533382857 c 0.0014350216 k 0.035208855 m 0.0016954881\n",
      "53 Train Loss 4745.547 Test RE 0.01702672600598523 c 0.0014307492 k 0.034843165 m 0.0016958324\n",
      "54 Train Loss 4537.8594 Test RE 0.016077784407526618 c 0.0014489968 k 0.03507075 m 0.0016955116\n",
      "55 Train Loss 4390.4146 Test RE 0.015369557181547984 c 0.001462008 k 0.0349483 m 0.0017021423\n",
      "56 Train Loss 4237.8228 Test RE 0.014765468750606164 c 0.0014796563 k 0.034284733 m 0.0017103422\n",
      "57 Train Loss 4083.1514 Test RE 0.013993219341683516 c 0.0014973311 k 0.03436848 m 0.0017090315\n",
      "58 Train Loss 3942.9717 Test RE 0.013299187226306583 c 0.0015134744 k 0.034288414 m 0.0017155514\n",
      "59 Train Loss 3811.1511 Test RE 0.012594887733320058 c 0.0015217572 k 0.033832192 m 0.0017270627\n",
      "60 Train Loss 3757.5168 Test RE 0.012320764064640267 c 0.0015274489 k 0.033175055 m 0.0017370157\n",
      "61 Train Loss 3711.2026 Test RE 0.012064564942780274 c 0.001528076 k 0.032689724 m 0.0017428545\n",
      "62 Train Loss 3659.147 Test RE 0.011766778194675401 c 0.0015288603 k 0.03247324 m 0.001745506\n",
      "63 Train Loss 3635.6577 Test RE 0.011602822028995324 c 0.0015358934 k 0.032555077 m 0.0017454657\n",
      "64 Train Loss 3612.7092 Test RE 0.011442076838114407 c 0.0015472597 k 0.03272639 m 0.0017441219\n",
      "65 Train Loss 3560.2783 Test RE 0.011134116943477441 c 0.0015679082 k 0.032253526 m 0.0017520814\n",
      "66 Train Loss 3477.915 Test RE 0.010576804251560138 c 0.001610032 k 0.031864636 m 0.0017643969\n",
      "67 Train Loss 3424.664 Test RE 0.010204479869820244 c 0.0016446972 k 0.03220621 m 0.0017663597\n",
      "68 Train Loss 3401.5347 Test RE 0.010050918837721416 c 0.001677521 k 0.032380834 m 0.0017686144\n",
      "69 Train Loss 3326.9358 Test RE 0.009513417451269055 c 0.0017680085 k 0.033031914 m 0.0017770013\n",
      "70 Train Loss 3295.5215 Test RE 0.009235904118254644 c 0.0017735566 k 0.03316692 m 0.0017792954\n",
      "71 Train Loss 3268.192 Test RE 0.009037204152448308 c 0.0017604245 k 0.032682624 m 0.001780406\n",
      "72 Train Loss 3261.768 Test RE 0.008988031399398402 c 0.0017533376 k 0.03247467 m 0.0017796702\n",
      "73 Train Loss 3230.8354 Test RE 0.008725373318008036 c 0.0017740749 k 0.03273213 m 0.0017800424\n",
      "74 Train Loss 3183.037 Test RE 0.008333894851805355 c 0.0018064852 k 0.03293918 m 0.0017837898\n",
      "75 Train Loss 3166.3628 Test RE 0.008154353900561306 c 0.0018260351 k 0.033218093 m 0.0017841556\n",
      "76 Train Loss 3146.146 Test RE 0.007987271663234092 c 0.0018631031 k 0.03298326 m 0.0017909679\n",
      "77 Train Loss 3132.4434 Test RE 0.007888855845848756 c 0.001869932 k 0.032451816 m 0.0017970213\n",
      "78 Train Loss 3120.4858 Test RE 0.007771710744801932 c 0.0018872337 k 0.032283727 m 0.0018009706\n",
      "79 Train Loss 3066.4358 Test RE 0.007258240620729064 c 0.001991475 k 0.032739248 m 0.0018036916\n",
      "80 Train Loss 3022.606 Test RE 0.006778131129134052 c 0.0020051424 k 0.032502837 m 0.0018006668\n",
      "81 Train Loss 3015.9111 Test RE 0.0066930300706695015 c 0.0020085364 k 0.03214708 m 0.00180456\n",
      "82 Train Loss 3011.4517 Test RE 0.006632580201517515 c 0.0020165641 k 0.031974822 m 0.0018070642\n",
      "83 Train Loss 3005.4111 Test RE 0.006568298442339716 c 0.0020257283 k 0.03207553 m 0.0018062743\n",
      "84 Train Loss 2995.1443 Test RE 0.006463779828698162 c 0.0020399396 k 0.031894486 m 0.0018080145\n",
      "85 Train Loss 2957.9531 Test RE 0.006065735095013545 c 0.0020978895 k 0.032476574 m 0.0018083741\n",
      "86 Train Loss 2942.9187 Test RE 0.0058987356030654236 c 0.0021320696 k 0.032567784 m 0.0018118236\n",
      "87 Train Loss 2939.7056 Test RE 0.005853422655968807 c 0.0021457595 k 0.032334253 m 0.0018163895\n",
      "88 Train Loss 2937.5803 Test RE 0.005815350415547906 c 0.0021573217 k 0.032369692 m 0.0018164909\n",
      "89 Train Loss 2933.8796 Test RE 0.005765322990013668 c 0.0021740948 k 0.032558303 m 0.0018150182\n",
      "90 Train Loss 2927.7747 Test RE 0.005711376633281056 c 0.0021928116 k 0.03246693 m 0.0018174866\n",
      "91 Train Loss 2910.39 Test RE 0.005522330176789973 c 0.0022816209 k 0.032414958 m 0.0018279024\n",
      "92 Train Loss 2873.3071 Test RE 0.004993267692698113 c 0.002474095 k 0.03218622 m 0.0018509785\n",
      "93 Train Loss 2870.1423 Test RE 0.004943173298032918 c 0.002492055 k 0.032315902 m 0.001852277\n",
      "94 Train Loss 2869.213 Test RE 0.004936934735728476 c 0.0024853013 k 0.032363515 m 0.0018512502\n",
      "95 Train Loss 2865.0774 Test RE 0.004880599127985685 c 0.002495732 k 0.032115817 m 0.0018534501\n",
      "96 Train Loss 2820.018 Test RE 0.004110847173685774 c 0.0027146467 k 0.03205196 m 0.0018744896\n",
      "97 Train Loss 2798.6624 Test RE 0.0037892625717413863 c 0.002813364 k 0.03230017 m 0.0018807397\n",
      "98 Train Loss 2796.8699 Test RE 0.0037486751654426404 c 0.0028232427 k 0.03212748 m 0.0018835164\n",
      "99 Train Loss 2795.871 Test RE 0.003720693397393383 c 0.0028521172 k 0.032141116 m 0.0018866344\n",
      "100 Train Loss 2791.5632 Test RE 0.003657921764217989 c 0.0029204907 k 0.032123633 m 0.0018925256\n",
      "101 Train Loss 2781.4426 Test RE 0.0034455177229314534 c 0.0029947194 k 0.03209502 m 0.0018957511\n",
      "102 Train Loss 2769.0386 Test RE 0.0031559872069770716 c 0.0031840885 k 0.032075983 m 0.001911605\n",
      "103 Train Loss 2764.8875 Test RE 0.0030222349743323595 c 0.0032429544 k 0.031963177 m 0.00192083\n",
      "104 Train Loss 2763.6729 Test RE 0.003018899326123544 c 0.0032358149 k 0.032042317 m 0.0019204123\n",
      "105 Train Loss 2761.1794 Test RE 0.002997409538879178 c 0.0032403776 k 0.03220372 m 0.0019189944\n",
      "106 Train Loss 2760.005 Test RE 0.002940673745607526 c 0.003230277 k 0.03212795 m 0.0019195565\n",
      "107 Train Loss 2759.535 Test RE 0.002932252328042942 c 0.0032330044 k 0.03212967 m 0.0019201628\n",
      "108 Train Loss 2757.8318 Test RE 0.002911563616822295 c 0.0033032992 k 0.032223135 m 0.0019257128\n",
      "109 Train Loss 2753.8938 Test RE 0.0028285803507723944 c 0.0034264708 k 0.032404102 m 0.0019359844\n",
      "110 Train Loss 2747.5876 Test RE 0.0026673392751722018 c 0.0035439215 k 0.03258529 m 0.0019451164\n",
      "111 Train Loss 2743.1064 Test RE 0.0025788755283128514 c 0.0036243128 k 0.032677617 m 0.001953465\n",
      "112 Train Loss 2740.5847 Test RE 0.0025327662465384018 c 0.0037189461 k 0.032506604 m 0.0019655114\n",
      "113 Train Loss 2739.136 Test RE 0.0024843705148966896 c 0.0037425184 k 0.03239003 m 0.0019692685\n",
      "114 Train Loss 2738.6152 Test RE 0.002458282578428282 c 0.0037453738 k 0.032372937 m 0.0019699142\n",
      "115 Train Loss 2738.1118 Test RE 0.0024346583780003295 c 0.003776342 k 0.03239686 m 0.0019725927\n",
      "116 Train Loss 2737.455 Test RE 0.0024175525623658482 c 0.0038119482 k 0.03238013 m 0.001976014\n",
      "117 Train Loss 2735.4539 Test RE 0.0023750141854676087 c 0.003947017 k 0.03254949 m 0.001988561\n",
      "118 Train Loss 2732.1814 Test RE 0.002287397533796367 c 0.0041027023 k 0.03255833 m 0.002001992\n",
      "119 Train Loss 2728.616 Test RE 0.002202082116596533 c 0.004328513 k 0.032398276 m 0.0020241283\n",
      "120 Train Loss 2727.583 Test RE 0.002180400830558659 c 0.004434933 k 0.03246872 m 0.0020348816\n",
      "121 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "122 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "123 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "124 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "125 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "126 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "127 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "128 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "129 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "130 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "131 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "132 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "133 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "134 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "135 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "136 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "137 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "138 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "139 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "140 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "141 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "142 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "143 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "144 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "145 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "146 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "147 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "148 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "149 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "150 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "151 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "152 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "153 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "154 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "155 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "156 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "157 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "158 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "159 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "160 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "161 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "162 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "163 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "164 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "165 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "166 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "167 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "168 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "169 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "170 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "171 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "172 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "173 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "174 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "175 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "176 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "177 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "178 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "179 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "180 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "181 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "182 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "183 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "184 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "185 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "186 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "187 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "188 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "189 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "190 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "191 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "192 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "193 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "194 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "195 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "196 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "197 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "198 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "199 Train Loss 2727.5042 Test RE 0.0021800552194774494 c 0.004453466 k 0.032467313 m 0.0020367259\n",
      "Training time: 70.07\n",
      "Training time: 70.07\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 6369596.5 Test RE 0.9602001584730451 c -0.00048318823 k 0.035576414 m -2.6350642e-06\n",
      "1 Train Loss 5899072.0 Test RE 0.9244652630494908 c -0.0006967515 k 0.05321336 m -1.0484648e-05\n",
      "2 Train Loss 4802307.5 Test RE 0.8343585296039552 c -0.00090248743 k 0.10525562 m -8.466484e-06\n",
      "3 Train Loss 2663909.8 Test RE 0.6187842942663565 c -0.001186168 k 0.19448762 m 6.2331914e-05\n",
      "4 Train Loss 1966951.5 Test RE 0.5218543571253424 c -0.001220215 k 0.23480114 m 8.535957e-05\n",
      "5 Train Loss 1342838.6 Test RE 0.39770506299380515 c -0.0012963689 k 0.26701966 m 0.00011593922\n",
      "6 Train Loss 620446.0 Test RE 0.2390859985498481 c -0.0014347914 k 0.2323873 m 0.00010627262\n",
      "7 Train Loss 313165.9 Test RE 0.12674719721773467 c -0.0014992275 k 0.22030555 m 0.000112409325\n",
      "8 Train Loss 277264.3 Test RE 0.091883909963648 c -0.0015219642 k 0.22135882 m 0.000118386386\n",
      "9 Train Loss 262645.2 Test RE 0.09801308347268775 c -0.0016046178 k 0.21146409 m 0.00012488478\n",
      "10 Train Loss 159758.06 Test RE 0.12014632042450192 c -0.0025159721 k 0.13334177 m 0.0001926602\n",
      "11 Train Loss 56135.51 Test RE 0.07609722460998461 c -0.0036481253 k 0.07699188 m 0.00028975925\n",
      "12 Train Loss 13262.5 Test RE 0.03166968646561036 c -0.004792114 k 0.014984815 m 0.00039017087\n",
      "13 Train Loss 9990.194 Test RE 0.024188935780628106 c -0.004774574 k 0.022410195 m 0.0003950852\n",
      "14 Train Loss 7730.4805 Test RE 0.017339098883299105 c -0.0046033254 k 0.033346012 m 0.00039095973\n",
      "15 Train Loss 6421.825 Test RE 0.017513750841928556 c -0.0045173983 k 0.034379315 m 0.00038829423\n",
      "16 Train Loss 5187.127 Test RE 0.017239724948513998 c -0.0045223045 k 0.03536302 m 0.0003931429\n",
      "17 Train Loss 4673.9834 Test RE 0.01625254462112731 c -0.004650316 k 0.03398526 m 0.00041170974\n",
      "18 Train Loss 4091.6885 Test RE 0.013834757696802922 c -0.004887238 k 0.033184756 m 0.00044496852\n",
      "19 Train Loss 4003.3152 Test RE 0.013489024869518529 c -0.0048800306 k 0.031873554 m 0.00046028715\n",
      "20 Train Loss 3941.6006 Test RE 0.013116909275074123 c -0.004791039 k 0.0326077 m 0.00046757577\n",
      "21 Train Loss 3919.1548 Test RE 0.013023294687993054 c -0.004672274 k 0.032719556 m 0.0004831474\n",
      "22 Train Loss 3717.7234 Test RE 0.011996364123130201 c -0.0031307284 k 0.03167514 m 0.0005972015\n",
      "23 Train Loss 3381.1812 Test RE 0.009844107619791844 c -0.00085724634 k 0.03221344 m 0.0007066906\n",
      "24 Train Loss 3219.6125 Test RE 0.008568416637497628 c 0.00028683717 k 0.03242508 m 0.000767735\n",
      "25 Train Loss 3120.1855 Test RE 0.0076092467456092125 c 0.00083231216 k 0.032286003 m 0.0008314643\n",
      "26 Train Loss 3100.5435 Test RE 0.007370415548422605 c 0.0005737023 k 0.032413065 m 0.00082145823\n",
      "27 Train Loss 3093.0952 Test RE 0.007357265429785247 c 0.0006792824 k 0.032492876 m 0.00083530054\n",
      "28 Train Loss 3078.7456 Test RE 0.007289975318692368 c 0.0014040347 k 0.03235071 m 0.0008876127\n",
      "29 Train Loss 3046.537 Test RE 0.00702814889382469 c 0.0024352053 k 0.03230553 m 0.0009348855\n",
      "30 Train Loss 3043.6165 Test RE 0.006983569527029307 c 0.0023467613 k 0.03242586 m 0.0009259401\n",
      "31 Train Loss 3043.2466 Test RE 0.006974199156164706 c 0.002260721 k 0.03241984 m 0.00092249125\n",
      "32 Train Loss 3040.135 Test RE 0.006952035697930286 c 0.0025959928 k 0.032444615 m 0.0009534769\n",
      "33 Train Loss 2973.0447 Test RE 0.006463461198817595 c 0.009830289 k 0.03263724 m 0.0015304851\n",
      "34 Train Loss 2884.2139 Test RE 0.005871500196575178 c 0.018961174 k 0.03205273 m 0.00225443\n",
      "35 Train Loss 2844.1216 Test RE 0.005746170517728285 c 0.02489158 k 0.031712614 m 0.002728637\n",
      "36 Train Loss 2815.019 Test RE 0.005678381012684967 c 0.032866955 k 0.031755447 m 0.0033587522\n",
      "37 Train Loss 2778.5662 Test RE 0.005659529662075777 c 0.040735662 k 0.03137804 m 0.003996717\n",
      "38 Train Loss 2739.419 Test RE 0.005310237264593869 c 0.046332024 k 0.031518 m 0.0044631455\n",
      "39 Train Loss 2716.6052 Test RE 0.005000938407192399 c 0.048892215 k 0.031277135 m 0.0046963203\n",
      "40 Train Loss 2702.077 Test RE 0.004855281950568348 c 0.051860552 k 0.03137943 m 0.0049585993\n",
      "41 Train Loss 2691.708 Test RE 0.00500341250439585 c 0.05720075 k 0.03127826 m 0.005396017\n",
      "42 Train Loss 2683.2153 Test RE 0.005147748285653364 c 0.062990874 k 0.030918287 m 0.005861344\n",
      "43 Train Loss 2665.3777 Test RE 0.005285586686069589 c 0.07005344 k 0.030988263 m 0.006427028\n",
      "44 Train Loss 2638.293 Test RE 0.005009378592812995 c 0.074728616 k 0.030784525 m 0.0068122153\n",
      "45 Train Loss 2619.7278 Test RE 0.004711465774012985 c 0.08152752 k 0.030853113 m 0.007337939\n",
      "46 Train Loss 2595.4644 Test RE 0.005275757024537973 c 0.094643876 k 0.029913116 m 0.008334909\n",
      "47 Train Loss 2578.4321 Test RE 0.005668631020682786 c 0.10267308 k 0.0300641 m 0.008947032\n",
      "48 Train Loss 2556.1353 Test RE 0.005720882026305175 c 0.11615492 k 0.030283572 m 0.00997529\n",
      "49 Train Loss 2534.6597 Test RE 0.005746730294930708 c 0.13046323 k 0.029277427 m 0.011071266\n",
      "50 Train Loss 2531.1577 Test RE 0.005622832072621115 c 0.13298376 k 0.029422216 m 0.011266577\n",
      "51 Train Loss 2530.1733 Test RE 0.0056442021358699196 c 0.13318343 k 0.029405702 m 0.011283392\n",
      "52 Train Loss 2524.595 Test RE 0.005716279954945561 c 0.13370019 k 0.029683875 m 0.011327855\n",
      "53 Train Loss 2498.8198 Test RE 0.005306510343106335 c 0.13709235 k 0.02940975 m 0.011611587\n",
      "54 Train Loss 2481.984 Test RE 0.005004820771343044 c 0.1354941 k 0.029007196 m 0.01151783\n",
      "55 Train Loss 2478.5552 Test RE 0.004938266050572498 c 0.13524356 k 0.029364884 m 0.011506081\n",
      "56 Train Loss 2469.3447 Test RE 0.004621579992177663 c 0.13846704 k 0.029497337 m 0.011762588\n",
      "57 Train Loss 2336.5576 Test RE 0.004101704180590967 c 0.17484327 k 0.027938519 m 0.014477209\n",
      "58 Train Loss 2028.0737 Test RE 0.004688644948730849 c 0.26268244 k 0.026512707 m 0.020944782\n",
      "59 Train Loss 1824.5095 Test RE 0.004066570752689468 c 0.33277947 k 0.024842013 m 0.02609984\n",
      "60 Train Loss 1781.2244 Test RE 0.004306481037718528 c 0.3606145 k 0.023939857 m 0.028135546\n",
      "61 Train Loss 1739.5077 Test RE 0.004660051292556445 c 0.3874684 k 0.02351232 m 0.030077366\n",
      "62 Train Loss 1702.8635 Test RE 0.005141389157964853 c 0.40323642 k 0.02259938 m 0.031199079\n",
      "63 Train Loss 1686.3186 Test RE 0.005074891581146047 c 0.40528268 k 0.02312497 m 0.031339765\n",
      "64 Train Loss 1681.3984 Test RE 0.004964650621813143 c 0.4067487 k 0.023534082 m 0.031439777\n",
      "65 Train Loss 1666.3708 Test RE 0.005362804334298994 c 0.41282025 k 0.02310769 m 0.03186019\n",
      "66 Train Loss 1646.6418 Test RE 0.006126544401201753 c 0.43307695 k 0.022075672 m 0.03332409\n",
      "67 Train Loss 1583.0173 Test RE 0.005828873062598405 c 0.4784852 k 0.021776643 m 0.036655977\n",
      "68 Train Loss 1512.9485 Test RE 0.005892358474495689 c 0.5343743 k 0.01986021 m 0.04070877\n",
      "69 Train Loss 1443.5991 Test RE 0.0064135714680532445 c 0.5985966 k 0.018924281 m 0.045343503\n",
      "70 Train Loss 1393.8083 Test RE 0.006157688718996253 c 0.6784394 k 0.016892405 m 0.05111435\n",
      "71 Train Loss 1389.8948 Test RE 0.006211216900489462 c 0.69743824 k 0.016537642 m 0.052485\n",
      "72 Train Loss 1381.3931 Test RE 0.0065299016615135656 c 0.69536346 k 0.017106563 m 0.052320004\n",
      "73 Train Loss 1358.3911 Test RE 0.006231628621097635 c 0.6916276 k 0.016440356 m 0.052032903\n",
      "74 Train Loss 1341.6672 Test RE 0.0060402285783173505 c 0.6777349 k 0.0169166 m 0.05099536\n",
      "75 Train Loss 1328.2571 Test RE 0.006128648321627263 c 0.6597804 k 0.017484214 m 0.049658053\n",
      "76 Train Loss 1316.6055 Test RE 0.006020592319345897 c 0.6449662 k 0.017686216 m 0.04857361\n",
      "77 Train Loss 1314.2832 Test RE 0.006086241974973875 c 0.63806415 k 0.01791965 m 0.04806294\n",
      "78 Train Loss 1300.2562 Test RE 0.006243393553056131 c 0.6276058 k 0.018299606 m 0.047248032\n",
      "79 Train Loss 1273.8212 Test RE 0.00595897700346086 c 0.6344383 k 0.017665587 m 0.047689326\n",
      "80 Train Loss 1233.6365 Test RE 0.005849532049696965 c 0.65771925 k 0.017377425 m 0.04928408\n",
      "81 Train Loss 1158.5092 Test RE 0.005656248709729762 c 0.69379884 k 0.016823782 m 0.051756576\n",
      "82 Train Loss 1105.1631 Test RE 0.00573123156728731 c 0.72780013 k 0.015913166 m 0.054098617\n",
      "83 Train Loss 1013.3451 Test RE 0.005838636986218141 c 0.78853816 k 0.014207609 m 0.058422\n",
      "84 Train Loss 951.00323 Test RE 0.006068198169463727 c 0.830575 k 0.013899614 m 0.06142121\n",
      "85 Train Loss 907.9843 Test RE 0.006278454847552462 c 0.8550287 k 0.01237059 m 0.06316824\n",
      "86 Train Loss 850.32214 Test RE 0.005869849253446816 c 0.8765854 k 0.0122306775 m 0.0647162\n",
      "87 Train Loss 766.8821 Test RE 0.00524895870052846 c 0.9055627 k 0.011681265 m 0.06679696\n",
      "88 Train Loss 724.76874 Test RE 0.004642548579467804 c 0.938996 k 0.011158129 m 0.06921469\n",
      "89 Train Loss 677.8861 Test RE 0.00456249868227539 c 0.99113375 k 0.009690252 m 0.07297546\n",
      "90 Train Loss 625.74146 Test RE 0.004424415539131598 c 1.0375049 k 0.008817198 m 0.0762987\n",
      "91 Train Loss 590.95123 Test RE 0.004488067206832823 c 1.0529236 k 0.00830359 m 0.07740161\n",
      "92 Train Loss 572.9467 Test RE 0.004524623223644838 c 1.0585438 k 0.007798184 m 0.07782369\n",
      "93 Train Loss 568.1882 Test RE 0.004449247560921672 c 1.0630808 k 0.007965579 m 0.078155704\n",
      "94 Train Loss 562.91595 Test RE 0.004236582283616689 c 1.0754082 k 0.007639611 m 0.079047546\n",
      "95 Train Loss 558.59485 Test RE 0.00422501324175034 c 1.1032473 k 0.006946761 m 0.08103496\n",
      "96 Train Loss 556.51935 Test RE 0.0042361717793103345 c 1.1273538 k 0.0064127715 m 0.08275618\n",
      "97 Train Loss 550.7121 Test RE 0.004090256536463803 c 1.1528407 k 0.0058217254 m 0.08457861\n",
      "98 Train Loss 531.35913 Test RE 0.0040258251786443 c 1.1293272 k 0.0063085994 m 0.0828618\n",
      "99 Train Loss 506.91486 Test RE 0.003679806022039655 c 1.0588062 k 0.008027954 m 0.07779081\n",
      "100 Train Loss 456.00772 Test RE 0.0031754298374186557 c 1.1173992 k 0.0064999396 m 0.081962034\n",
      "101 Train Loss 407.7974 Test RE 0.0024583649575391597 c 1.162034 k 0.005601448 m 0.08515287\n",
      "102 Train Loss 365.6839 Test RE 0.0019378105059106947 c 1.1826909 k 0.0049455236 m 0.086632065\n",
      "103 Train Loss 361.79376 Test RE 0.002094104100435118 c 1.1887119 k 0.0046294974 m 0.08705496\n",
      "104 Train Loss 358.0528 Test RE 0.002097949119538465 c 1.191128 k 0.004711414 m 0.0872171\n",
      "105 Train Loss 355.3602 Test RE 0.0019486038501724098 c 1.1990768 k 0.004474348 m 0.08777899\n",
      "106 Train Loss 354.92645 Test RE 0.0019355878760928 c 1.2012736 k 0.004479206 m 0.08793334\n",
      "107 Train Loss 354.5971 Test RE 0.0020762011259493965 c 1.1991313 k 0.0045366865 m 0.08776923\n",
      "108 Train Loss 353.50293 Test RE 0.0022953273682952565 c 1.1852405 k 0.0047909743 m 0.08675249\n",
      "109 Train Loss 351.33154 Test RE 0.0022670148272944534 c 1.1731817 k 0.0051620547 m 0.085875146\n",
      "110 Train Loss 347.5772 Test RE 0.0023275549322084896 c 1.137302 k 0.005990811 m 0.08327387\n",
      "111 Train Loss 345.63358 Test RE 0.002258691660853207 c 1.1172694 k 0.0064433846 m 0.08182589\n",
      "112 Train Loss 337.85486 Test RE 0.002285497537031888 c 1.1189923 k 0.0063475063 m 0.081950285\n",
      "113 Train Loss 332.98206 Test RE 0.0024704750168968014 c 1.1270941 k 0.00604373 m 0.08253467\n",
      "114 Train Loss 329.35516 Test RE 0.0023926627244119873 c 1.1380205 k 0.005785102 m 0.08333109\n",
      "115 Train Loss 329.01508 Test RE 0.002413675746887555 c 1.1397343 k 0.005702581 m 0.08345776\n",
      "116 Train Loss 328.919 Test RE 0.0024182104441052617 c 1.1385636 k 0.005732313 m 0.083376095\n",
      "117 Train Loss 328.4583 Test RE 0.0023535711305376805 c 1.1317462 k 0.0059276056 m 0.08289245\n",
      "118 Train Loss 327.10083 Test RE 0.0020916577577510687 c 1.1264901 k 0.0060960283 m 0.08252858\n",
      "119 Train Loss 325.41232 Test RE 0.0019203253527222566 c 1.1402285 k 0.0057888962 m 0.08352813\n",
      "120 Train Loss 324.33755 Test RE 0.001970675583158729 c 1.1456301 k 0.0057040853 m 0.083929166\n",
      "121 Train Loss 324.23694 Test RE 0.00197842714343595 c 1.1409328 k 0.0057963934 m 0.0835943\n",
      "122 Train Loss 323.5071 Test RE 0.0019475736092226697 c 1.1321266 k 0.0059908037 m 0.08297517\n",
      "123 Train Loss 322.01678 Test RE 0.0020241443785558137 c 1.12497 k 0.0060329805 m 0.08247329\n",
      "124 Train Loss 319.6185 Test RE 0.0021599941180774174 c 1.1394212 k 0.005800498 m 0.08352006\n",
      "125 Train Loss 319.09973 Test RE 0.002236776838219651 c 1.1463752 k 0.0055049025 m 0.08402355\n",
      "126 Train Loss 319.02594 Test RE 0.002278868035216555 c 1.1454452 k 0.0055408115 m 0.0839602\n",
      "127 Train Loss 318.98007 Test RE 0.0022528405792394458 c 1.1415521 k 0.005633442 m 0.083682396\n",
      "128 Train Loss 318.88568 Test RE 0.0022315833542355537 c 1.1411119 k 0.0056547723 m 0.08365414\n",
      "129 Train Loss 317.678 Test RE 0.002170908702214505 c 1.1528119 k 0.005434558 m 0.08451021\n",
      "130 Train Loss 314.23975 Test RE 0.0018839461600930788 c 1.1628737 k 0.0051478795 m 0.085270956\n",
      "131 Train Loss 313.91492 Test RE 0.0018284030679760822 c 1.1604822 k 0.0052178726 m 0.085106514\n",
      "132 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "133 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "134 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "135 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "136 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "137 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "138 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "139 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "140 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "141 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "142 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "143 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "144 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "145 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "146 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "147 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "148 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "149 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "150 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "151 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "152 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "153 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "154 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "155 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "156 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "157 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "158 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "159 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "160 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "161 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "162 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "163 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "164 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "165 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "166 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "167 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "168 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "169 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "170 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "171 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "172 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "173 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "174 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "175 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "176 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "177 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "178 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "179 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "180 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "181 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "182 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "183 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "184 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "185 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "186 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "187 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "188 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "189 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "190 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "191 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "192 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "193 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "194 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "195 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "196 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "197 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "198 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "199 Train Loss 313.8971 Test RE 0.0018339525997288547 c 1.1611922 k 0.005226603 m 0.085157245\n",
      "Training time: 77.36\n",
      "Training time: 77.36\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 6776196.5 Test RE 0.9907510749247904 c 0.0003258305 k 0.005214038 m -3.4669265e-06\n",
      "1 Train Loss 6400562.5 Test RE 0.9628825952002065 c 0.0008221073 k 0.01639134 m -1.0602681e-05\n",
      "2 Train Loss 5872287.5 Test RE 0.9223305006311654 c 0.0016303555 k 0.034850694 m -2.3755238e-05\n",
      "3 Train Loss 4789143.5 Test RE 0.8331035513090801 c 0.0026321895 k 0.057718925 m -3.7463684e-05\n",
      "4 Train Loss 3884845.0 Test RE 0.7506457598969459 c 0.0038453317 k 0.08722451 m -5.519509e-05\n",
      "5 Train Loss 2012911.8 Test RE 0.536703102640556 c 0.0060717645 k 0.14531314 m -9.184353e-05\n",
      "6 Train Loss 1556825.2 Test RE 0.46920377414188985 c 0.006770918 k 0.1627337 m -0.00010202141\n",
      "7 Train Loss 517353.38 Test RE 0.24759700322202366 c 0.00778261 k 0.18891548 m -0.00011798465\n",
      "8 Train Loss 256919.5 Test RE 0.10223619803121352 c 0.008722633 k 0.21262617 m -0.00013171851\n",
      "9 Train Loss 248421.16 Test RE 0.07708368279988416 c 0.008910712 k 0.21742374 m -0.00013463439\n",
      "10 Train Loss 245545.52 Test RE 0.0710515706514758 c 0.008952903 k 0.21834958 m -0.0001355317\n",
      "11 Train Loss 244297.06 Test RE 0.07706418278743146 c 0.0088788085 k 0.21630053 m -0.00013437083\n",
      "12 Train Loss 241614.86 Test RE 0.08150905639353577 c 0.008799309 k 0.21362352 m -0.00013228215\n",
      "13 Train Loss 240228.44 Test RE 0.07653661457997672 c 0.008841469 k 0.21428406 m -0.00013250303\n",
      "14 Train Loss 239637.97 Test RE 0.07541326287138395 c 0.008858163 k 0.21417564 m -0.00013251524\n",
      "15 Train Loss 236231.75 Test RE 0.07945799195465139 c 0.008785524 k 0.21087071 m -0.00013052647\n",
      "16 Train Loss 232989.84 Test RE 0.07794242302779925 c 0.008811381 k 0.20990081 m -0.0001292108\n",
      "17 Train Loss 229482.53 Test RE 0.07570194456662818 c 0.008851084 k 0.20901307 m -0.00012876734\n",
      "18 Train Loss 225862.02 Test RE 0.08620896113031477 c 0.0087326905 k 0.20395887 m -0.00012579063\n",
      "19 Train Loss 211099.1 Test RE 0.07537336615085483 c 0.008723988 k 0.20088317 m -0.00012301863\n",
      "20 Train Loss 207436.06 Test RE 0.0632635338487756 c 0.008844753 k 0.20256688 m -0.0001229905\n",
      "21 Train Loss 203564.56 Test RE 0.05937307652869332 c 0.008829596 k 0.2020102 m -0.00012156813\n",
      "22 Train Loss 200722.05 Test RE 0.0706808665028983 c 0.00868473 k 0.19757475 m -0.000118843775\n",
      "23 Train Loss 191549.2 Test RE 0.07112589664083817 c 0.008721481 k 0.19325149 m -0.00011632893\n",
      "24 Train Loss 179117.33 Test RE 0.06511635168924718 c 0.008807188 k 0.18799353 m -0.00011153411\n",
      "25 Train Loss 165615.47 Test RE 0.06699576419244227 c 0.008785145 k 0.18055592 m -0.0001069052\n",
      "26 Train Loss 152178.48 Test RE 0.06941904701818938 c 0.008775073 k 0.1717501 m -0.00010078081\n",
      "27 Train Loss 144084.42 Test RE 0.06426588187892687 c 0.008865581 k 0.16818772 m -9.80023e-05\n",
      "28 Train Loss 128720.07 Test RE 0.06438536027817239 c 0.008966891 k 0.15818839 m -9.1013615e-05\n",
      "29 Train Loss 97320.14 Test RE 0.05444124929054513 c 0.0091561545 k 0.14057373 m -7.890957e-05\n",
      "30 Train Loss 79703.61 Test RE 0.06518751334875661 c 0.00921579 k 0.11995647 m -6.5161534e-05\n",
      "31 Train Loss 65324.375 Test RE 0.04833818990470517 c 0.009495525 k 0.11652674 m -6.4279084e-05\n",
      "32 Train Loss 52094.47 Test RE 0.041205266757762433 c 0.009784474 k 0.107083455 m -5.97133e-05\n",
      "33 Train Loss 43096.234 Test RE 0.0405768685245679 c 0.009834527 k 0.097456045 m -5.264951e-05\n",
      "34 Train Loss 40024.64 Test RE 0.035617112305061004 c 0.0100370385 k 0.096873544 m -5.154714e-05\n",
      "35 Train Loss 38818.465 Test RE 0.03503822915802803 c 0.010228923 k 0.095798045 m -5.019413e-05\n",
      "36 Train Loss 38454.04 Test RE 0.03566376225405463 c 0.010390809 k 0.09492435 m -4.9070673e-05\n",
      "37 Train Loss 37974.62 Test RE 0.03453240985775971 c 0.010484744 k 0.09549915 m -4.9222777e-05\n",
      "38 Train Loss 37485.59 Test RE 0.03404731198887959 c 0.010610314 k 0.09592962 m -4.9247847e-05\n",
      "39 Train Loss 36868.684 Test RE 0.03452780009710018 c 0.01059289 k 0.09430421 m -4.8174625e-05\n",
      "40 Train Loss 34913.65 Test RE 0.032659853938768996 c 0.010680343 k 0.092936195 m -4.689076e-05\n",
      "41 Train Loss 32386.104 Test RE 0.029020794412458747 c 0.010790453 k 0.09200217 m -4.5787474e-05\n",
      "42 Train Loss 24360.637 Test RE 0.02669292200789573 c 0.010904702 k 0.08225236 m -3.8000064e-05\n",
      "43 Train Loss 19503.744 Test RE 0.025217202793922748 c 0.011176016 k 0.07532616 m -3.16543e-05\n",
      "44 Train Loss 9248.216 Test RE 0.023623258961363962 c 0.011470289 k 0.051106133 m -1.1579067e-05\n",
      "45 Train Loss 7086.956 Test RE 0.022162009715440845 c 0.011603744 k 0.045095675 m -6.2479803e-06\n",
      "46 Train Loss 5986.859 Test RE 0.020480919377644416 c 0.011652578 k 0.039895594 m -1.8449723e-06\n",
      "47 Train Loss 5144.0186 Test RE 0.018925004053623202 c 0.01173898 k 0.030100426 m 6.428946e-06\n",
      "48 Train Loss 4909.171 Test RE 0.018062683259788062 c 0.011698495 k 0.029908495 m 6.471837e-06\n",
      "49 Train Loss 4783.014 Test RE 0.017695853136609686 c 0.011651181 k 0.03076736 m 5.6449067e-06\n",
      "50 Train Loss 4751.4316 Test RE 0.01749824114729534 c 0.011640824 k 0.030073073 m 6.2047857e-06\n",
      "51 Train Loss 4734.259 Test RE 0.01727347155968008 c 0.011656005 k 0.029272292 m 6.936695e-06\n",
      "52 Train Loss 4705.1685 Test RE 0.0172042122272363 c 0.011670406 k 0.029489333 m 6.878582e-06\n",
      "53 Train Loss 4678.7153 Test RE 0.017218445849783204 c 0.01167591 k 0.030216949 m 6.433646e-06\n",
      "54 Train Loss 4669.4473 Test RE 0.01725755791203817 c 0.011696678 k 0.030950401 m 5.9717054e-06\n",
      "55 Train Loss 4658.2407 Test RE 0.017209254294731895 c 0.011729886 k 0.0310861 m 5.972435e-06\n",
      "56 Train Loss 4647.898 Test RE 0.01717688817184005 c 0.011717308 k 0.031272147 m 5.871993e-06\n",
      "57 Train Loss 4639.6606 Test RE 0.017177561114786466 c 0.011752495 k 0.03178985 m 5.6163462e-06\n",
      "58 Train Loss 4629.437 Test RE 0.01710579460818728 c 0.011802246 k 0.031122088 m 6.298037e-06\n",
      "59 Train Loss 4620.1772 Test RE 0.01706221640071488 c 0.011843647 k 0.031038376 m 6.559878e-06\n",
      "60 Train Loss 4614.834 Test RE 0.017059509908754638 c 0.011888574 k 0.031229762 m 6.6181497e-06\n",
      "61 Train Loss 4611.967 Test RE 0.0170336744481795 c 0.011924408 k 0.030982522 m 6.967565e-06\n",
      "62 Train Loss 4610.4893 Test RE 0.01703534058049809 c 0.011946531 k 0.03104909 m 7.0301735e-06\n",
      "63 Train Loss 4610.052 Test RE 0.017033889810974586 c 0.0119598545 k 0.031025086 m 7.116027e-06\n",
      "64 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "65 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "66 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "67 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "68 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "69 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "70 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "71 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "72 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "73 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "74 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "75 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "76 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "77 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "78 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "79 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "80 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "81 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "82 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "83 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "84 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "85 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "86 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "87 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "88 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "89 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "90 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "91 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "92 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "93 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "94 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "95 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "96 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "97 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "98 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "99 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "100 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "101 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "102 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "103 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "104 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "105 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "106 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "107 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "108 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "109 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "110 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "111 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "112 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "113 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "114 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "115 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "116 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "117 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "118 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "119 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "120 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "121 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "122 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "123 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "124 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "125 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "126 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "127 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "128 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "129 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "130 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "131 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "132 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "133 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "134 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "135 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "136 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "137 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "138 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "139 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "140 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "141 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "142 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "143 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "144 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "145 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "146 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "147 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "148 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "149 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "150 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "151 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "152 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "153 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "154 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "155 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "156 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "157 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "158 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "159 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "160 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "161 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "162 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "163 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "164 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "165 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "166 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "167 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "168 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "169 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "170 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "171 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "172 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "173 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "174 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "175 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "176 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "177 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "178 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "179 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "180 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "181 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "182 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "183 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "184 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "185 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "186 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "187 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "188 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "189 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "190 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "191 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "192 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "193 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "194 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "195 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "196 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "197 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "198 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "199 Train Loss 4610.0024 Test RE 0.017033694908710203 c 0.011963609 k 0.031017562 m 7.1389127e-06\n",
      "Training time: 58.84\n",
      "Training time: 58.84\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 6785026.0 Test RE 0.9914015336925079 c -0.00030353887 k 0.016051717 m -1.15328355e-07\n",
      "1 Train Loss 6417179.5 Test RE 0.9641685542750387 c -0.00035686797 k 0.040255193 m -4.161731e-06\n",
      "2 Train Loss 4917169.5 Test RE 0.8444159291151674 c -0.00020117256 k 0.10685663 m -1.4421838e-05\n",
      "3 Train Loss nan Test RE nan c nan k nan m nan\n",
      "4 Train Loss nan Test RE nan c nan k nan m nan\n",
      "5 Train Loss nan Test RE nan c nan k nan m nan\n",
      "6 Train Loss nan Test RE nan c nan k nan m nan\n",
      "7 Train Loss nan Test RE nan c nan k nan m nan\n",
      "8 Train Loss nan Test RE nan c nan k nan m nan\n",
      "9 Train Loss nan Test RE nan c nan k nan m nan\n",
      "10 Train Loss nan Test RE nan c nan k nan m nan\n",
      "11 Train Loss nan Test RE nan c nan k nan m nan\n",
      "12 Train Loss nan Test RE nan c nan k nan m nan\n",
      "13 Train Loss nan Test RE nan c nan k nan m nan\n",
      "14 Train Loss nan Test RE nan c nan k nan m nan\n",
      "15 Train Loss nan Test RE nan c nan k nan m nan\n",
      "16 Train Loss nan Test RE nan c nan k nan m nan\n",
      "17 Train Loss nan Test RE nan c nan k nan m nan\n",
      "18 Train Loss nan Test RE nan c nan k nan m nan\n",
      "19 Train Loss nan Test RE nan c nan k nan m nan\n",
      "20 Train Loss nan Test RE nan c nan k nan m nan\n",
      "21 Train Loss nan Test RE nan c nan k nan m nan\n",
      "22 Train Loss nan Test RE nan c nan k nan m nan\n",
      "23 Train Loss nan Test RE nan c nan k nan m nan\n",
      "24 Train Loss nan Test RE nan c nan k nan m nan\n",
      "25 Train Loss nan Test RE nan c nan k nan m nan\n",
      "26 Train Loss nan Test RE nan c nan k nan m nan\n",
      "27 Train Loss nan Test RE nan c nan k nan m nan\n",
      "28 Train Loss nan Test RE nan c nan k nan m nan\n",
      "29 Train Loss nan Test RE nan c nan k nan m nan\n",
      "30 Train Loss nan Test RE nan c nan k nan m nan\n",
      "31 Train Loss nan Test RE nan c nan k nan m nan\n",
      "32 Train Loss nan Test RE nan c nan k nan m nan\n",
      "33 Train Loss nan Test RE nan c nan k nan m nan\n",
      "34 Train Loss nan Test RE nan c nan k nan m nan\n",
      "35 Train Loss nan Test RE nan c nan k nan m nan\n",
      "36 Train Loss nan Test RE nan c nan k nan m nan\n",
      "37 Train Loss nan Test RE nan c nan k nan m nan\n",
      "38 Train Loss nan Test RE nan c nan k nan m nan\n",
      "39 Train Loss nan Test RE nan c nan k nan m nan\n",
      "40 Train Loss nan Test RE nan c nan k nan m nan\n",
      "41 Train Loss nan Test RE nan c nan k nan m nan\n",
      "42 Train Loss nan Test RE nan c nan k nan m nan\n",
      "43 Train Loss nan Test RE nan c nan k nan m nan\n",
      "44 Train Loss nan Test RE nan c nan k nan m nan\n",
      "45 Train Loss nan Test RE nan c nan k nan m nan\n",
      "46 Train Loss nan Test RE nan c nan k nan m nan\n",
      "47 Train Loss nan Test RE nan c nan k nan m nan\n",
      "48 Train Loss nan Test RE nan c nan k nan m nan\n",
      "49 Train Loss nan Test RE nan c nan k nan m nan\n",
      "50 Train Loss nan Test RE nan c nan k nan m nan\n",
      "51 Train Loss nan Test RE nan c nan k nan m nan\n",
      "52 Train Loss nan Test RE nan c nan k nan m nan\n",
      "53 Train Loss nan Test RE nan c nan k nan m nan\n",
      "54 Train Loss nan Test RE nan c nan k nan m nan\n",
      "55 Train Loss nan Test RE nan c nan k nan m nan\n",
      "56 Train Loss nan Test RE nan c nan k nan m nan\n",
      "57 Train Loss nan Test RE nan c nan k nan m nan\n",
      "58 Train Loss nan Test RE nan c nan k nan m nan\n",
      "59 Train Loss nan Test RE nan c nan k nan m nan\n",
      "60 Train Loss nan Test RE nan c nan k nan m nan\n",
      "61 Train Loss nan Test RE nan c nan k nan m nan\n",
      "62 Train Loss nan Test RE nan c nan k nan m nan\n",
      "63 Train Loss nan Test RE nan c nan k nan m nan\n",
      "64 Train Loss nan Test RE nan c nan k nan m nan\n",
      "65 Train Loss nan Test RE nan c nan k nan m nan\n",
      "66 Train Loss nan Test RE nan c nan k nan m nan\n",
      "67 Train Loss nan Test RE nan c nan k nan m nan\n",
      "68 Train Loss nan Test RE nan c nan k nan m nan\n",
      "69 Train Loss nan Test RE nan c nan k nan m nan\n",
      "70 Train Loss nan Test RE nan c nan k nan m nan\n",
      "71 Train Loss nan Test RE nan c nan k nan m nan\n",
      "72 Train Loss nan Test RE nan c nan k nan m nan\n",
      "73 Train Loss nan Test RE nan c nan k nan m nan\n",
      "74 Train Loss nan Test RE nan c nan k nan m nan\n",
      "75 Train Loss nan Test RE nan c nan k nan m nan\n",
      "76 Train Loss nan Test RE nan c nan k nan m nan\n",
      "77 Train Loss nan Test RE nan c nan k nan m nan\n",
      "78 Train Loss nan Test RE nan c nan k nan m nan\n",
      "79 Train Loss nan Test RE nan c nan k nan m nan\n",
      "80 Train Loss nan Test RE nan c nan k nan m nan\n",
      "81 Train Loss nan Test RE nan c nan k nan m nan\n",
      "82 Train Loss nan Test RE nan c nan k nan m nan\n",
      "83 Train Loss nan Test RE nan c nan k nan m nan\n",
      "84 Train Loss nan Test RE nan c nan k nan m nan\n",
      "85 Train Loss nan Test RE nan c nan k nan m nan\n",
      "86 Train Loss nan Test RE nan c nan k nan m nan\n",
      "87 Train Loss nan Test RE nan c nan k nan m nan\n",
      "88 Train Loss nan Test RE nan c nan k nan m nan\n",
      "89 Train Loss nan Test RE nan c nan k nan m nan\n",
      "90 Train Loss nan Test RE nan c nan k nan m nan\n",
      "91 Train Loss nan Test RE nan c nan k nan m nan\n",
      "92 Train Loss nan Test RE nan c nan k nan m nan\n",
      "93 Train Loss nan Test RE nan c nan k nan m nan\n",
      "94 Train Loss nan Test RE nan c nan k nan m nan\n",
      "95 Train Loss nan Test RE nan c nan k nan m nan\n",
      "96 Train Loss nan Test RE nan c nan k nan m nan\n",
      "97 Train Loss nan Test RE nan c nan k nan m nan\n",
      "98 Train Loss nan Test RE nan c nan k nan m nan\n",
      "99 Train Loss nan Test RE nan c nan k nan m nan\n",
      "100 Train Loss nan Test RE nan c nan k nan m nan\n",
      "101 Train Loss nan Test RE nan c nan k nan m nan\n",
      "102 Train Loss nan Test RE nan c nan k nan m nan\n",
      "103 Train Loss nan Test RE nan c nan k nan m nan\n",
      "104 Train Loss nan Test RE nan c nan k nan m nan\n",
      "105 Train Loss nan Test RE nan c nan k nan m nan\n",
      "106 Train Loss nan Test RE nan c nan k nan m nan\n",
      "107 Train Loss nan Test RE nan c nan k nan m nan\n",
      "108 Train Loss nan Test RE nan c nan k nan m nan\n",
      "109 Train Loss nan Test RE nan c nan k nan m nan\n",
      "110 Train Loss nan Test RE nan c nan k nan m nan\n",
      "111 Train Loss nan Test RE nan c nan k nan m nan\n",
      "112 Train Loss nan Test RE nan c nan k nan m nan\n",
      "113 Train Loss nan Test RE nan c nan k nan m nan\n",
      "114 Train Loss nan Test RE nan c nan k nan m nan\n",
      "115 Train Loss nan Test RE nan c nan k nan m nan\n",
      "116 Train Loss nan Test RE nan c nan k nan m nan\n",
      "117 Train Loss nan Test RE nan c nan k nan m nan\n",
      "118 Train Loss nan Test RE nan c nan k nan m nan\n",
      "119 Train Loss nan Test RE nan c nan k nan m nan\n",
      "120 Train Loss nan Test RE nan c nan k nan m nan\n",
      "121 Train Loss nan Test RE nan c nan k nan m nan\n",
      "122 Train Loss nan Test RE nan c nan k nan m nan\n",
      "123 Train Loss nan Test RE nan c nan k nan m nan\n",
      "124 Train Loss nan Test RE nan c nan k nan m nan\n",
      "125 Train Loss nan Test RE nan c nan k nan m nan\n",
      "126 Train Loss nan Test RE nan c nan k nan m nan\n",
      "127 Train Loss nan Test RE nan c nan k nan m nan\n",
      "128 Train Loss nan Test RE nan c nan k nan m nan\n",
      "129 Train Loss nan Test RE nan c nan k nan m nan\n",
      "130 Train Loss nan Test RE nan c nan k nan m nan\n",
      "131 Train Loss nan Test RE nan c nan k nan m nan\n",
      "132 Train Loss nan Test RE nan c nan k nan m nan\n",
      "133 Train Loss nan Test RE nan c nan k nan m nan\n",
      "134 Train Loss nan Test RE nan c nan k nan m nan\n",
      "135 Train Loss nan Test RE nan c nan k nan m nan\n",
      "136 Train Loss nan Test RE nan c nan k nan m nan\n",
      "137 Train Loss nan Test RE nan c nan k nan m nan\n",
      "138 Train Loss nan Test RE nan c nan k nan m nan\n",
      "139 Train Loss nan Test RE nan c nan k nan m nan\n",
      "140 Train Loss nan Test RE nan c nan k nan m nan\n",
      "141 Train Loss nan Test RE nan c nan k nan m nan\n",
      "142 Train Loss nan Test RE nan c nan k nan m nan\n",
      "143 Train Loss nan Test RE nan c nan k nan m nan\n",
      "144 Train Loss nan Test RE nan c nan k nan m nan\n",
      "145 Train Loss nan Test RE nan c nan k nan m nan\n",
      "146 Train Loss nan Test RE nan c nan k nan m nan\n",
      "147 Train Loss nan Test RE nan c nan k nan m nan\n",
      "148 Train Loss nan Test RE nan c nan k nan m nan\n",
      "149 Train Loss nan Test RE nan c nan k nan m nan\n",
      "150 Train Loss nan Test RE nan c nan k nan m nan\n",
      "151 Train Loss nan Test RE nan c nan k nan m nan\n",
      "152 Train Loss nan Test RE nan c nan k nan m nan\n",
      "153 Train Loss nan Test RE nan c nan k nan m nan\n",
      "154 Train Loss nan Test RE nan c nan k nan m nan\n",
      "155 Train Loss nan Test RE nan c nan k nan m nan\n",
      "156 Train Loss nan Test RE nan c nan k nan m nan\n",
      "157 Train Loss nan Test RE nan c nan k nan m nan\n",
      "158 Train Loss nan Test RE nan c nan k nan m nan\n",
      "159 Train Loss nan Test RE nan c nan k nan m nan\n",
      "160 Train Loss nan Test RE nan c nan k nan m nan\n",
      "161 Train Loss nan Test RE nan c nan k nan m nan\n",
      "162 Train Loss nan Test RE nan c nan k nan m nan\n",
      "163 Train Loss nan Test RE nan c nan k nan m nan\n",
      "164 Train Loss nan Test RE nan c nan k nan m nan\n",
      "165 Train Loss nan Test RE nan c nan k nan m nan\n",
      "166 Train Loss nan Test RE nan c nan k nan m nan\n",
      "167 Train Loss nan Test RE nan c nan k nan m nan\n",
      "168 Train Loss nan Test RE nan c nan k nan m nan\n",
      "169 Train Loss nan Test RE nan c nan k nan m nan\n",
      "170 Train Loss nan Test RE nan c nan k nan m nan\n",
      "171 Train Loss nan Test RE nan c nan k nan m nan\n",
      "172 Train Loss nan Test RE nan c nan k nan m nan\n",
      "173 Train Loss nan Test RE nan c nan k nan m nan\n",
      "174 Train Loss nan Test RE nan c nan k nan m nan\n",
      "175 Train Loss nan Test RE nan c nan k nan m nan\n",
      "176 Train Loss nan Test RE nan c nan k nan m nan\n",
      "177 Train Loss nan Test RE nan c nan k nan m nan\n",
      "178 Train Loss nan Test RE nan c nan k nan m nan\n",
      "179 Train Loss nan Test RE nan c nan k nan m nan\n",
      "180 Train Loss nan Test RE nan c nan k nan m nan\n",
      "181 Train Loss nan Test RE nan c nan k nan m nan\n",
      "182 Train Loss nan Test RE nan c nan k nan m nan\n",
      "183 Train Loss nan Test RE nan c nan k nan m nan\n",
      "184 Train Loss nan Test RE nan c nan k nan m nan\n",
      "185 Train Loss nan Test RE nan c nan k nan m nan\n",
      "186 Train Loss nan Test RE nan c nan k nan m nan\n",
      "187 Train Loss nan Test RE nan c nan k nan m nan\n",
      "188 Train Loss nan Test RE nan c nan k nan m nan\n",
      "189 Train Loss nan Test RE nan c nan k nan m nan\n",
      "190 Train Loss nan Test RE nan c nan k nan m nan\n",
      "191 Train Loss nan Test RE nan c nan k nan m nan\n",
      "192 Train Loss nan Test RE nan c nan k nan m nan\n",
      "193 Train Loss nan Test RE nan c nan k nan m nan\n",
      "194 Train Loss nan Test RE nan c nan k nan m nan\n",
      "195 Train Loss nan Test RE nan c nan k nan m nan\n",
      "196 Train Loss nan Test RE nan c nan k nan m nan\n",
      "197 Train Loss nan Test RE nan c nan k nan m nan\n",
      "198 Train Loss nan Test RE nan c nan k nan m nan\n",
      "199 Train Loss nan Test RE nan c nan k nan m nan\n",
      "Training time: 155.32\n",
      "Training time: 155.32\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 6838033.0 Test RE 0.9952666708463681 c 0.0001666472 k 0.0013314742 m -2.5296913e-06\n",
      "1 Train Loss 6450386.5 Test RE 0.9666199958787633 c 0.0008969789 k 0.012521264 m -1.8274039e-05\n",
      "2 Train Loss 6099562.5 Test RE 0.9399544749698978 c 0.0012468668 k 0.016866047 m -2.3912638e-05\n",
      "3 Train Loss 4282566.5 Test RE 0.7877530099208906 c 0.0033068394 k 0.043359183 m -5.5738754e-05\n",
      "4 Train Loss 2774670.2 Test RE 0.6343583527004004 c 0.0047830045 k 0.06260927 m -7.8462435e-05\n",
      "5 Train Loss 2477677.0 Test RE 0.5995157121749001 c 0.0051806984 k 0.067590125 m -8.03214e-05\n",
      "6 Train Loss 904957.0 Test RE 0.35976084594703206 c 0.0077470397 k 0.1015376 m -0.00010979313\n",
      "7 Train Loss 190188.08 Test RE 0.1480357022932118 c 0.008793 k 0.114729196 m -0.000121566445\n",
      "8 Train Loss 104226.78 Test RE 0.08376719537869272 c 0.009482821 k 0.123081006 m -0.00012785594\n",
      "9 Train Loss 86213.56 Test RE 0.05502039976890064 c 0.010005764 k 0.12909783 m -0.00013183156\n",
      "10 Train Loss 83018.31 Test RE 0.04585019060155863 c 0.010258485 k 0.13168454 m -0.00013303453\n",
      "11 Train Loss 81309.555 Test RE 0.04110932193021931 c 0.010429711 k 0.13270319 m -0.00013245098\n",
      "12 Train Loss 80760.08 Test RE 0.04278129911658305 c 0.010432299 k 0.13200387 m -0.00013112098\n",
      "13 Train Loss 79834.12 Test RE 0.04599803709637609 c 0.010204446 k 0.12967856 m -0.00013019494\n",
      "14 Train Loss 79557.73 Test RE 0.047630490334092675 c 0.010121012 k 0.12882973 m -0.00012988462\n",
      "15 Train Loss 79221.54 Test RE 0.05076065945817985 c 0.010002954 k 0.1272617 m -0.00012910349\n",
      "16 Train Loss 77031.61 Test RE 0.05508553663839514 c 0.009851444 k 0.12309726 m -0.00012693738\n",
      "17 Train Loss 62280.492 Test RE 0.0728294800693042 c 0.009121486 k 0.09147103 m -0.00010733786\n",
      "18 Train Loss 44804.68 Test RE 0.06477803786751495 c 0.0089490665 k 0.07752098 m -9.613364e-05\n",
      "19 Train Loss 17942.582 Test RE 0.04606179922526564 c 0.008797393 k 0.03704989 m -6.5050146e-05\n",
      "20 Train Loss 13579.699 Test RE 0.03798864559341678 c 0.00902182 k 0.027399568 m -5.679785e-05\n",
      "21 Train Loss 12002.355 Test RE 0.0356856314897668 c 0.009503989 k 0.031964477 m -5.7682984e-05\n",
      "22 Train Loss 11959.216 Test RE 0.0355649604405488 c 0.00949386 k 0.030698052 m -5.678302e-05\n",
      "23 Train Loss 11945.035 Test RE 0.035512054366213255 c 0.009487086 k 0.031032085 m -5.7129942e-05\n",
      "24 Train Loss 11910.252 Test RE 0.0353764193631711 c 0.009462188 k 0.030514555 m -5.6996232e-05\n",
      "25 Train Loss 11866.587 Test RE 0.035197106641361345 c 0.009458711 k 0.029970186 m -5.6629153e-05\n",
      "26 Train Loss 11134.248 Test RE 0.03366556849108135 c 0.00974972 k 0.029127328 m -5.384742e-05\n",
      "27 Train Loss 10446.5 Test RE 0.03233206358248835 c 0.010001476 k 0.032505557 m -5.5582706e-05\n",
      "28 Train Loss 10325.84 Test RE 0.03185119711966568 c 0.009996949 k 0.030130018 m -5.396263e-05\n",
      "29 Train Loss 10311.555 Test RE 0.03174372760073418 c 0.010020293 k 0.029771928 m -5.353e-05\n",
      "30 Train Loss 10291.43 Test RE 0.03178764364980718 c 0.010021115 k 0.030400293 m -5.3928805e-05\n",
      "31 Train Loss 10256.856 Test RE 0.031693965996563475 c 0.010021507 k 0.030404478 m -5.388835e-05\n",
      "32 Train Loss 10254.348 Test RE 0.03165360029783755 c 0.010027299 k 0.03025953 m -5.3753567e-05\n",
      "33 Train Loss 10220.768 Test RE 0.031390152071084074 c 0.010058607 k 0.029129835 m -5.269083e-05\n",
      "34 Train Loss 7656.8726 Test RE 0.025369727585706856 c 0.011080779 k 0.026927482 m -4.2018906e-05\n",
      "35 Train Loss 5937.2695 Test RE 0.020651065143349856 c 0.011566439 k 0.03487227 m -4.368378e-05\n",
      "36 Train Loss 5179.457 Test RE 0.018042386800652378 c 0.011687259 k 0.03305366 m -4.0778003e-05\n",
      "37 Train Loss 4713.8423 Test RE 0.015582848802300075 c 0.011787916 k 0.03255736 m -3.9850696e-05\n",
      "38 Train Loss 4101.859 Test RE 0.012180688580699733 c 0.011940199 k 0.03207521 m -4.1074854e-05\n",
      "39 Train Loss 3889.7327 Test RE 0.011056350072658844 c 0.012062182 k 0.031757258 m -4.0962997e-05\n",
      "40 Train Loss 3539.6118 Test RE 0.007207125362491458 c 0.012234638 k 0.030956598 m -3.914662e-05\n",
      "41 Train Loss 3395.3394 Test RE 0.0065810989127340555 c 0.012334967 k 0.031701107 m -3.9926625e-05\n",
      "42 Train Loss 3221.6023 Test RE 0.005592875205976502 c 0.012481656 k 0.031898245 m -3.9570725e-05\n",
      "43 Train Loss 3087.106 Test RE 0.004424581992132689 c 0.012566698 k 0.031786192 m -3.797958e-05\n",
      "44 Train Loss 2940.3057 Test RE 0.004406723410986358 c 0.012660618 k 0.03224458 m -3.653599e-05\n",
      "45 Train Loss 2881.5076 Test RE 0.004580194242311293 c 0.012712011 k 0.031968445 m -3.525624e-05\n",
      "46 Train Loss 2862.7202 Test RE 0.004211818567756376 c 0.012802911 k 0.032167148 m -3.5023826e-05\n",
      "47 Train Loss 2822.4802 Test RE 0.00395461589573773 c 0.012906079 k 0.03198581 m -3.4169076e-05\n",
      "48 Train Loss 2776.2563 Test RE 0.003483020192983542 c 0.013003967 k 0.032353316 m -3.3835324e-05\n",
      "49 Train Loss 2748.3623 Test RE 0.0033243652292265567 c 0.013134426 k 0.032137517 m -3.3126402e-05\n",
      "50 Train Loss 2741.4749 Test RE 0.0032718024421359722 c 0.01321795 k 0.03222203 m -3.2817396e-05\n",
      "51 Train Loss 2740.8672 Test RE 0.003237293775327148 c 0.013245833 k 0.032244507 m -3.2692835e-05\n",
      "52 Train Loss 2740.185 Test RE 0.003146072882983545 c 0.013236492 k 0.032246545 m -3.2674267e-05\n",
      "53 Train Loss 2733.0022 Test RE 0.003151820029914341 c 0.013242251 k 0.032293998 m -3.2043517e-05\n",
      "54 Train Loss 2715.9507 Test RE 0.0027719972400049187 c 0.013303799 k 0.03217338 m -3.0478066e-05\n",
      "55 Train Loss 2712.2595 Test RE 0.0027212090208184672 c 0.013361465 k 0.03226296 m -2.942402e-05\n",
      "56 Train Loss 2711.6975 Test RE 0.0027232564873310655 c 0.013385295 k 0.0321928 m -2.8859224e-05\n",
      "57 Train Loss 2711.6067 Test RE 0.002713822720448879 c 0.013379618 k 0.03225065 m -2.9002158e-05\n",
      "58 Train Loss 2711.5803 Test RE 0.002711508062555295 c 0.013377008 k 0.032246675 m -2.9032304e-05\n",
      "59 Train Loss 2711.5693 Test RE 0.002712233255715321 c 0.013375588 k 0.032220356 m -2.9018594e-05\n",
      "60 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "61 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "62 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "63 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "64 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "65 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "66 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "67 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "68 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "69 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "70 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "71 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "72 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "73 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "74 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "75 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "76 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "77 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "78 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "79 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "80 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "81 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "82 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "83 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "84 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "85 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "86 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "87 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "88 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "89 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "90 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "91 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "92 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "93 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "94 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "95 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "96 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "97 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "98 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "99 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "100 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "101 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "102 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "103 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "104 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "105 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "106 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "107 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "108 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "109 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "110 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "111 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "112 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "113 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "114 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "115 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "116 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "117 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "118 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "119 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "120 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "121 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "122 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "123 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "124 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "125 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "126 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "127 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "128 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "129 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "130 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "131 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "132 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "133 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "134 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "135 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "136 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "137 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "138 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "139 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "140 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "141 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "142 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "143 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "144 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "145 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "146 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "147 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "148 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "149 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "150 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "151 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "152 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "153 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "154 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "155 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "156 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "157 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "158 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "159 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "160 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "161 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "162 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "163 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "164 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "165 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "166 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "167 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "168 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "169 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "170 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "171 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "172 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "173 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "174 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "175 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "176 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "177 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "178 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "179 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "180 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "181 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "182 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "183 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "184 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "185 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "186 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "187 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "188 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "189 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "190 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "191 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "192 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "193 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "194 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "195 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "196 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "197 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "198 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "199 Train Loss 2711.5493 Test RE 0.002712432170496882 c 0.013373221 k 0.032182463 m -2.8999144e-05\n",
      "Training time: 62.73\n",
      "Training time: 62.73\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 6807390.0 Test RE 0.9930312022057266 c 0.00029909186 k 0.0025887073 m -4.0110053e-06\n",
      "1 Train Loss 6530018.0 Test RE 0.9725697152897166 c 0.00082628505 k 0.009306246 m -1.4750299e-05\n",
      "2 Train Loss 5913604.0 Test RE 0.9255174884516388 c 0.0016749463 k 0.020711318 m -2.9766612e-05\n",
      "3 Train Loss 4101248.5 Test RE 0.7710355244583851 c 0.0045288336 k 0.05832667 m -7.68243e-05\n",
      "4 Train Loss 2525288.2 Test RE 0.6050889038694499 c 0.005902512 k 0.07533834 m -9.914059e-05\n",
      "5 Train Loss 388252.2 Test RE 0.23057008161465986 c 0.008292599 k 0.10857276 m -0.00014103588\n",
      "6 Train Loss 103605.914 Test RE 0.07636212746810925 c 0.009869829 k 0.13067877 m -0.00016835879\n",
      "7 Train Loss 87959.234 Test RE 0.05098576251970007 c 0.010336734 k 0.13255571 m -0.00017359237\n",
      "8 Train Loss 47038.59 Test RE 0.03485272456781608 c 0.01025193 k 0.10278088 m -0.00015512863\n",
      "9 Train Loss 17839.19 Test RE 0.0326839246565594 c 0.009781091 k 0.063526906 m -0.0001280211\n",
      "10 Train Loss 10353.243 Test RE 0.028320827482008898 c 0.00957717 k 0.044417948 m -0.00011512542\n",
      "11 Train Loss 9524.103 Test RE 0.026914073649274284 c 0.009471138 k 0.031544704 m -0.00010665498\n",
      "12 Train Loss 9194.029 Test RE 0.025464689869263233 c 0.009511124 k 0.0332833 m -0.00010801499\n",
      "13 Train Loss 9047.809 Test RE 0.025059920019578092 c 0.009498104 k 0.03166328 m -0.000106867294\n",
      "14 Train Loss 8961.15 Test RE 0.02477761116439993 c 0.009512719 k 0.032739177 m -0.00010749355\n",
      "15 Train Loss 8939.38 Test RE 0.024707659239338275 c 0.009555434 k 0.032903414 m -0.00010781603\n",
      "16 Train Loss 8870.215 Test RE 0.02443424017199501 c 0.009725022 k 0.032393757 m -0.00010878148\n",
      "17 Train Loss 8864.273 Test RE 0.024381714490734622 c 0.009735738 k 0.0322827 m -0.000108826185\n",
      "18 Train Loss 8864.175 Test RE 0.024380037122583954 c 0.009735121 k 0.032279234 m -0.0001088194\n",
      "19 Train Loss 8860.766 Test RE 0.0243639519758511 c 0.009694966 k 0.032318264 m -0.00010828313\n",
      "20 Train Loss 8850.03 Test RE 0.02441562775332002 c 0.009620929 k 0.032089014 m -0.00010693987\n",
      "21 Train Loss 8846.284 Test RE 0.024460627602543996 c 0.009564895 k 0.032268602 m -0.00010610209\n",
      "22 Train Loss 8837.478 Test RE 0.024560277993109888 c 0.009454314 k 0.03213947 m -0.00010330384\n",
      "23 Train Loss 8792.113 Test RE 0.024563982872250614 c 0.009360643 k 0.03162597 m -9.551048e-05\n",
      "24 Train Loss 8751.048 Test RE 0.024423665539116336 c 0.009405122 k 0.030848837 m -8.589562e-05\n",
      "25 Train Loss 8634.772 Test RE 0.024530472482473843 c 0.009589079 k 0.030333554 m -7.172466e-05\n",
      "26 Train Loss 8613.451 Test RE 0.02439189601952655 c 0.009651106 k 0.03039919 m -6.984794e-05\n",
      "27 Train Loss 8533.717 Test RE 0.024043765076626974 c 0.009893185 k 0.030388849 m -6.885926e-05\n",
      "28 Train Loss 8493.87 Test RE 0.023868762921331634 c 0.010069634 k 0.028929424 m -6.647976e-05\n",
      "29 Train Loss 8430.067 Test RE 0.023788001788413865 c 0.010268735 k 0.029134702 m -6.98981e-05\n",
      "30 Train Loss 8396.414 Test RE 0.023587021033743938 c 0.010488521 k 0.029183442 m -6.9744056e-05\n",
      "31 Train Loss 8063.0366 Test RE 0.023171901692295452 c 0.010769337 k 0.031814344 m -8.134126e-05\n",
      "32 Train Loss 7992.3745 Test RE 0.023388299199587972 c 0.010711095 k 0.031119168 m -8.4322885e-05\n",
      "33 Train Loss 7925.1455 Test RE 0.023328449877385984 c 0.010965505 k 0.030736597 m -8.401697e-05\n",
      "34 Train Loss 7910.567 Test RE 0.023384316736218474 c 0.010963758 k 0.031809863 m -8.767029e-05\n",
      "35 Train Loss 7910.0806 Test RE 0.02339807985928385 c 0.010963102 k 0.031781442 m -8.7901266e-05\n",
      "36 Train Loss 7902.2896 Test RE 0.023362261699228358 c 0.010892506 k 0.031644437 m -8.585841e-05\n",
      "37 Train Loss 7888.793 Test RE 0.023281951543525893 c 0.010848438 k 0.032127876 m -8.397392e-05\n",
      "38 Train Loss 7883.3896 Test RE 0.023247172127570055 c 0.010787696 k 0.0319758 m -8.2107035e-05\n",
      "39 Train Loss 7878.5967 Test RE 0.023197432389396318 c 0.010724585 k 0.03199206 m -7.972249e-05\n",
      "40 Train Loss 7876.3955 Test RE 0.023175994007110148 c 0.010681233 k 0.03211974 m -7.805587e-05\n",
      "41 Train Loss 7874.103 Test RE 0.02313166960317361 c 0.010626782 k 0.032001026 m -7.5365235e-05\n",
      "42 Train Loss 7868.8096 Test RE 0.023098814905732488 c 0.010585384 k 0.032104753 m -6.9824986e-05\n",
      "43 Train Loss 7858.8584 Test RE 0.0230888643141426 c 0.0106382845 k 0.032078166 m -6.0339968e-05\n",
      "44 Train Loss 7842.444 Test RE 0.02307225486180913 c 0.010796124 k 0.032071978 m -4.6933852e-05\n",
      "45 Train Loss 7813.5767 Test RE 0.023141376644847788 c 0.011214081 k 0.03218488 m -2.2922874e-05\n",
      "46 Train Loss 7777.422 Test RE 0.023227647621663544 c 0.011922548 k 0.03210881 m 1.19244105e-05\n",
      "47 Train Loss 7745.634 Test RE 0.02334001019820083 c 0.012787465 k 0.03235373 m 4.8873793e-05\n",
      "48 Train Loss 7721.287 Test RE 0.023229297086056026 c 0.013676349 k 0.032147557 m 8.4476625e-05\n",
      "49 Train Loss 7713.12 Test RE 0.023077334450239233 c 0.01406677 k 0.032090414 m 9.845661e-05\n",
      "50 Train Loss 7672.2812 Test RE 0.0229591548235458 c 0.013950591 k 0.032068018 m 8.717972e-05\n",
      "51 Train Loss 7597.5376 Test RE 0.023109419445473 c 0.013705155 k 0.032269467 m 7.413528e-05\n",
      "52 Train Loss 6923.9395 Test RE 0.02220179895342054 c 0.020247258 k 0.033698846 m 0.00029369778\n",
      "53 Train Loss 6195.6606 Test RE 0.021357399265254894 c 0.028703326 k 0.030949978 m 0.000591182\n",
      "54 Train Loss 5519.6846 Test RE 0.01974420765499726 c 0.03693927 k 0.030846735 m 0.0008852818\n",
      "55 Train Loss 5382.4414 Test RE 0.019535519688309015 c 0.039750744 k 0.032265704 m 0.0009851799\n",
      "56 Train Loss 5307.74 Test RE 0.019649278143077056 c 0.04115398 k 0.03173741 m 0.0010364649\n",
      "57 Train Loss 5273.379 Test RE 0.019709991432155863 c 0.044790197 k 0.03151188 m 0.0011636491\n",
      "58 Train Loss 5248.4263 Test RE 0.01980090067054711 c 0.045932274 k 0.031630848 m 0.0011955578\n",
      "59 Train Loss 5227.436 Test RE 0.019695987287558503 c 0.044126604 k 0.031565104 m 0.0011272753\n",
      "60 Train Loss 5207.6826 Test RE 0.019618653033912935 c 0.043353368 k 0.031660248 m 0.001091858\n",
      "61 Train Loss 5133.6426 Test RE 0.019391244722600443 c 0.046711683 k 0.03156431 m 0.0012000039\n",
      "62 Train Loss 5073.697 Test RE 0.019213646631842957 c 0.045786254 k 0.031294093 m 0.0011531565\n",
      "63 Train Loss 5063.3525 Test RE 0.019207009828378793 c 0.045177422 k 0.031508684 m 0.0011247434\n",
      "64 Train Loss 5052.1953 Test RE 0.019049812284354865 c 0.046122156 k 0.03173063 m 0.0011510234\n",
      "65 Train Loss 4999.999 Test RE 0.018712637923261197 c 0.047259983 k 0.03157394 m 0.001194372\n",
      "66 Train Loss 4980.3047 Test RE 0.018738931996003145 c 0.049494702 k 0.031377606 m 0.0012740911\n",
      "67 Train Loss 4967.871 Test RE 0.018813766230833442 c 0.051779926 k 0.031628307 m 0.0013519545\n",
      "68 Train Loss 4924.6357 Test RE 0.018738964520844375 c 0.05510529 k 0.031470206 m 0.0014727173\n",
      "69 Train Loss 4912.286 Test RE 0.01872286487628161 c 0.05789494 k 0.03158462 m 0.001572462\n",
      "70 Train Loss 4899.486 Test RE 0.018677654353016344 c 0.057842515 k 0.03122292 m 0.0015676705\n",
      "71 Train Loss 4877.4927 Test RE 0.018502875208027878 c 0.057980035 k 0.030620538 m 0.0015638423\n",
      "72 Train Loss 4838.7476 Test RE 0.018380027031774774 c 0.059770186 k 0.03143609 m 0.0016172699\n",
      "73 Train Loss 4768.276 Test RE 0.018084907075788557 c 0.061739497 k 0.031079916 m 0.0016666574\n",
      "74 Train Loss 4166.334 Test RE 0.015323648280641325 c 0.05248511 k 0.029007 m 0.0012823782\n",
      "75 Train Loss 3862.6548 Test RE 0.013886942223554213 c 0.04157564 k 0.031998377 m 0.0008602454\n",
      "76 Train Loss 3746.4966 Test RE 0.013262733099630674 c 0.042121563 k 0.03218297 m 0.00084883824\n",
      "77 Train Loss 3515.3352 Test RE 0.01187189340922193 c 0.040276404 k 0.031588525 m 0.00071933476\n",
      "78 Train Loss 3441.7754 Test RE 0.011237211781548103 c 0.032777287 k 0.032108407 m 0.00042548607\n",
      "79 Train Loss 3412.3022 Test RE 0.010912170805289554 c 0.02856711 k 0.031180518 m 0.00025299034\n",
      "80 Train Loss 3401.4219 Test RE 0.010830626761100786 c 0.02774132 k 0.03212818 m 0.00021192191\n",
      "81 Train Loss 3368.803 Test RE 0.010471900850976115 c 0.02493675 k 0.03316941 m 8.932168e-05\n",
      "82 Train Loss 3350.501 Test RE 0.010356076031234689 c 0.022896314 k 0.032066472 m 5.1960383e-06\n",
      "83 Train Loss 3347.1294 Test RE 0.010318643294024196 c 0.022824982 k 0.03214246 m 2.9529328e-06\n",
      "84 Train Loss 3338.8435 Test RE 0.01026592780797762 c 0.023781154 k 0.032010935 m 4.238101e-05\n",
      "85 Train Loss 3325.9465 Test RE 0.010198208593208049 c 0.024685923 k 0.03208536 m 7.17354e-05\n",
      "86 Train Loss 3319.377 Test RE 0.010132911604665763 c 0.02371264 k 0.032099143 m 3.187415e-05\n",
      "87 Train Loss 3317.5015 Test RE 0.010078404144345542 c 0.022261413 k 0.032093607 m -2.3862958e-05\n",
      "88 Train Loss 3316.5344 Test RE 0.01006345150540379 c 0.021909531 k 0.032043032 m -3.963964e-05\n",
      "89 Train Loss 3314.037 Test RE 0.010061004284500465 c 0.022624094 k 0.032015253 m -1.884401e-05\n",
      "90 Train Loss 3313.8032 Test RE 0.010047980090653176 c 0.022276856 k 0.032049656 m -3.449976e-05\n",
      "91 Train Loss 3313.662 Test RE 0.010030035521840855 c 0.021713527 k 0.03205097 m -5.871761e-05\n",
      "92 Train Loss 3313.3848 Test RE 0.010007383057911302 c 0.021049626 k 0.032022882 m -8.703006e-05\n",
      "93 Train Loss 3305.1433 Test RE 0.009879352581240912 c 0.019041546 k 0.03219612 m -0.00015594352\n",
      "94 Train Loss 3272.937 Test RE 0.009715242413264377 c 0.021788817 k 0.032043014 m 5.328131e-06\n",
      "95 Train Loss 3241.6746 Test RE 0.009591439797107176 c 0.025385397 k 0.031994484 m 0.00022450057\n",
      "96 Train Loss 3205.7349 Test RE 0.009537134313166815 c 0.03380803 k 0.031757444 m 0.00066176\n",
      "97 Train Loss 3174.7563 Test RE 0.009418457446863822 c 0.03790321 k 0.031677883 m 0.0009018373\n",
      "98 Train Loss 3165.8052 Test RE 0.009416406808741389 c 0.03858019 k 0.03159297 m 0.0009630968\n",
      "99 Train Loss 3158.8647 Test RE 0.009426196408422953 c 0.04094499 k 0.031663924 m 0.0011210284\n",
      "100 Train Loss 3118.5342 Test RE 0.009390146678254625 c 0.05276416 k 0.03192758 m 0.0017814675\n",
      "101 Train Loss 3048.2537 Test RE 0.009134384936257588 c 0.066256925 k 0.029671049 m 0.0026337143\n",
      "102 Train Loss 2947.2534 Test RE 0.0089018238922783 c 0.07984858 k 0.0312334 m 0.0034306902\n",
      "103 Train Loss 2866.604 Test RE 0.008794417971908166 c 0.09940314 k 0.029222727 m 0.0044565126\n",
      "104 Train Loss 2778.0088 Test RE 0.008937589530057964 c 0.12588234 k 0.030287197 m 0.0059496793\n",
      "105 Train Loss 2683.0376 Test RE 0.008357288329598131 c 0.13161601 k 0.029688936 m 0.006225436\n",
      "106 Train Loss 2588.076 Test RE 0.007954700574301247 c 0.14236231 k 0.029092664 m 0.0067179874\n",
      "107 Train Loss 2559.229 Test RE 0.007625142261708258 c 0.14098066 k 0.029304504 m 0.0066189957\n",
      "108 Train Loss 2539.2678 Test RE 0.007499696399244448 c 0.14229208 k 0.029488012 m 0.0066688224\n",
      "109 Train Loss 2514.7986 Test RE 0.007454079168820705 c 0.14734872 k 0.029221006 m 0.006935563\n",
      "110 Train Loss 2511.209 Test RE 0.007453233504259797 c 0.14881517 k 0.028937556 m 0.0070281858\n",
      "111 Train Loss 2497.9736 Test RE 0.007506469694102723 c 0.15421228 k 0.029058376 m 0.0074124276\n",
      "112 Train Loss 2469.46 Test RE 0.007556009972085987 c 0.16369005 k 0.029028 m 0.008037113\n",
      "113 Train Loss 2431.4338 Test RE 0.007458210768210288 c 0.17308338 k 0.028569404 m 0.008545808\n",
      "114 Train Loss 2395.0142 Test RE 0.007589012782379922 c 0.18564059 k 0.028516058 m 0.009268708\n",
      "115 Train Loss 2357.8547 Test RE 0.007506281049525735 c 0.19019522 k 0.027969351 m 0.009582341\n",
      "116 Train Loss 2292.1978 Test RE 0.006830084460418808 c 0.18843465 k 0.028723367 m 0.009355073\n",
      "117 Train Loss 2281.1033 Test RE 0.006909654079030605 c 0.19250344 k 0.027655642 m 0.009579393\n",
      "118 Train Loss 2271.6057 Test RE 0.007169702224826077 c 0.20212965 k 0.027803222 m 0.010117107\n",
      "119 Train Loss 2262.417 Test RE 0.007918297145938943 c 0.22947325 k 0.02771784 m 0.011621795\n",
      "120 Train Loss 2155.6047 Test RE 0.007939836835198505 c 0.25872505 k 0.026635168 m 0.013275427\n",
      "121 Train Loss 1967.4875 Test RE 0.006509030113556915 c 0.27483833 k 0.025694296 m 0.014212309\n",
      "122 Train Loss 1937.8229 Test RE 0.006001265027874902 c 0.26918963 k 0.026168404 m 0.013884796\n",
      "123 Train Loss 1924.5673 Test RE 0.00584336635409849 c 0.2685654 k 0.026300753 m 0.013817493\n",
      "124 Train Loss 1919.9296 Test RE 0.0058929943435502025 c 0.2709435 k 0.026139703 m 0.01397132\n",
      "125 Train Loss 1914.3098 Test RE 0.005865914665952091 c 0.27133435 k 0.026134262 m 0.0139921\n",
      "126 Train Loss 1910.3989 Test RE 0.005894623080972562 c 0.27235365 k 0.026278568 m 0.014049548\n",
      "127 Train Loss 1897.2117 Test RE 0.005746421760857496 c 0.2733829 k 0.026169986 m 0.014147684\n",
      "128 Train Loss 1888.7733 Test RE 0.005727172407343202 c 0.27639875 k 0.025935452 m 0.014382042\n",
      "129 Train Loss 1872.7262 Test RE 0.005703385802812454 c 0.28039265 k 0.025940005 m 0.014701194\n",
      "130 Train Loss 1848.8683 Test RE 0.005214785929625724 c 0.27814806 k 0.02599306 m 0.014520694\n",
      "131 Train Loss 1829.0627 Test RE 0.005188927070920099 c 0.28260902 k 0.02610458 m 0.014758543\n",
      "132 Train Loss 1821.0195 Test RE 0.005143871661189027 c 0.28427476 k 0.02581233 m 0.014930612\n",
      "133 Train Loss 1815.1523 Test RE 0.005121763757679276 c 0.28629678 k 0.026056524 m 0.015102349\n",
      "134 Train Loss 1808.1375 Test RE 0.005181291269962328 c 0.29055092 k 0.025737965 m 0.015380727\n",
      "135 Train Loss 1801.1532 Test RE 0.005220529309743359 c 0.29222944 k 0.025712129 m 0.015490816\n",
      "136 Train Loss 1799.6208 Test RE 0.005210274725691257 c 0.29270256 k 0.025743637 m 0.015547677\n",
      "137 Train Loss 1799.6066 Test RE 0.005208992113308936 c 0.29271665 k 0.025741437 m 0.015549908\n",
      "138 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "139 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "140 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "141 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "142 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "143 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "144 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "145 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "146 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "147 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "148 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "149 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "150 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "151 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "152 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "153 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "154 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "155 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "156 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "157 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "158 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "159 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "160 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "161 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "162 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "163 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "164 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "165 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "166 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "167 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "168 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "169 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "170 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "171 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "172 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "173 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "174 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "175 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "176 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "177 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "178 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "179 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "180 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "181 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "182 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "183 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "184 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "185 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "186 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "187 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "188 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "189 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "190 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "191 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "192 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "193 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "194 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "195 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "196 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "197 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "198 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "199 Train Loss 1799.5984 Test RE 0.005207953627922615 c 0.29276392 k 0.025737736 m 0.015554523\n",
      "Training time: 74.72\n",
      "Training time: 74.72\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 6859105.5 Test RE 0.9968004241329477 c 0.0002813808 k 6.418331e-05 m 1.6004387e-07\n",
      "1 Train Loss 6833979.0 Test RE 0.9949701462463822 c 0.00030897916 k 0.0006070322 m 9.553147e-07\n",
      "2 Train Loss 6803120.0 Test RE 0.9927160075179088 c 0.00039982802 k 0.0020765497 m -5.085729e-06\n",
      "3 Train Loss 6760260.5 Test RE 0.9895785417253281 c 0.0005391369 k 0.004321867 m -1.7765627e-05\n",
      "4 Train Loss 6692403.0 Test RE 0.9845878360864501 c 0.0007342097 k 0.0073270514 m -3.9800696e-05\n",
      "5 Train Loss 6618176.0 Test RE 0.9791202293712895 c 0.00091591006 k 0.010595837 m -5.8668003e-05\n",
      "6 Train Loss 6520537.5 Test RE 0.9717313700640136 c 0.0010661703 k 0.013315646 m -7.330937e-05\n",
      "7 Train Loss 6384185.0 Test RE 0.9616336550430664 c 0.0013158979 k 0.018490795 m -9.9998186e-05\n",
      "8 Train Loss 6213785.5 Test RE 0.9486660047931951 c 0.0015915938 k 0.024206849 m -0.00013104682\n",
      "9 Train Loss 6047178.0 Test RE 0.9358559762960306 c 0.0018484886 k 0.029748967 m -0.00016656434\n",
      "10 Train Loss 5779017.0 Test RE 0.9148838331371179 c 0.0021929229 k 0.037519548 m -0.00022042064\n",
      "11 Train Loss 5515505.0 Test RE 0.8937627964697865 c 0.0025084994 k 0.04465401 m -0.00026617458\n",
      "12 Train Loss 5152229.5 Test RE 0.8638405127982456 c 0.0028248746 k 0.051662397 m -0.00031198034\n",
      "13 Train Loss 4688741.0 Test RE 0.8242024146462685 c 0.0032755493 k 0.0615451 m -0.00037487113\n",
      "14 Train Loss 4277847.5 Test RE 0.7870149589212534 c 0.00354236 k 0.06736147 m -0.00040652216\n",
      "15 Train Loss 3776474.5 Test RE 0.7394049629331124 c 0.003885236 k 0.07490568 m -0.00045159017\n",
      "16 Train Loss 2935232.5 Test RE 0.6514681062832605 c 0.004377791 k 0.08584028 m -0.0005198606\n",
      "17 Train Loss 2557040.2 Test RE 0.6068813194486992 c 0.0047019296 k 0.09290632 m -0.00056185725\n",
      "18 Train Loss 2157358.0 Test RE 0.5588696003922249 c 0.0049444535 k 0.09770492 m -0.0005873854\n",
      "19 Train Loss 1875394.4 Test RE 0.5208197135048774 c 0.0051144767 k 0.10111898 m -0.000604609\n",
      "20 Train Loss 1668906.9 Test RE 0.4910353019914775 c 0.0052039376 k 0.10283993 m -0.0006173396\n",
      "21 Train Loss 1363873.4 Test RE 0.4430453988902243 c 0.0053887074 k 0.1065905 m -0.00063734734\n",
      "22 Train Loss 1075692.4 Test RE 0.3920487543270928 c 0.0057060504 k 0.11321413 m -0.00066814903\n",
      "23 Train Loss 954790.7 Test RE 0.36859993744467295 c 0.005878167 k 0.11663195 m -0.00067993975\n",
      "24 Train Loss 665694.2 Test RE 0.30373533160258315 c 0.0063659633 k 0.1256105 m -0.00070677325\n",
      "25 Train Loss 442672.0 Test RE 0.2426258288762574 c 0.0066835643 k 0.13162027 m -0.000725826\n",
      "26 Train Loss 280970.3 Test RE 0.18246991485935576 c 0.006903495 k 0.13584661 m -0.00074143015\n",
      "27 Train Loss 198460.98 Test RE 0.13948999422130065 c 0.007083525 k 0.13914198 m -0.00075565616\n",
      "28 Train Loss 145980.84 Test RE 0.09960168933628896 c 0.0073533687 k 0.1431714 m -0.0007661858\n",
      "29 Train Loss 129044.7 Test RE 0.08325319725809417 c 0.007401752 k 0.14362781 m -0.0007639336\n",
      "30 Train Loss 120215.35 Test RE 0.07728509617186956 c 0.007323263 k 0.14214073 m -0.0007570502\n",
      "31 Train Loss 100949.836 Test RE 0.05530207242701624 c 0.0074169147 k 0.1415254 m -0.0007388295\n",
      "32 Train Loss 94794.66 Test RE 0.051268591973272076 c 0.0073647103 k 0.13912831 m -0.0007250382\n",
      "33 Train Loss 88702.78 Test RE 0.045134935551151584 c 0.007319525 k 0.13742884 m -0.00071608444\n",
      "34 Train Loss 84607.16 Test RE 0.04591036630603101 c 0.007243653 k 0.13419142 m -0.00069909927\n",
      "35 Train Loss 80984.49 Test RE 0.046395630142493156 c 0.007192969 k 0.1313941 m -0.0006834607\n",
      "36 Train Loss 77967.266 Test RE 0.045527194483819124 c 0.007150057 k 0.12900786 m -0.00067028677\n",
      "37 Train Loss 75955.89 Test RE 0.0457808434069207 c 0.00713252 k 0.12751968 m -0.0006628755\n",
      "38 Train Loss 71478.39 Test RE 0.044147891898250094 c 0.0071694334 k 0.124307595 m -0.0006459589\n",
      "39 Train Loss 66898.92 Test RE 0.04278784500320691 c 0.0071810284 k 0.12064705 m -0.00062779564\n",
      "40 Train Loss 63955.56 Test RE 0.04220532472091446 c 0.0071717124 k 0.11800588 m -0.0006118966\n",
      "41 Train Loss 54684.33 Test RE 0.0485856059339556 c 0.006911034 k 0.10590447 m -0.0005583922\n",
      "42 Train Loss 39772.484 Test RE 0.04639635082997391 c 0.0067188437 k 0.09086513 m -0.0004923421\n",
      "43 Train Loss 22428.559 Test RE 0.0375485637068283 c 0.0066668247 k 0.07104622 m -0.00039431095\n",
      "44 Train Loss 18819.684 Test RE 0.035040229891700324 c 0.0066578933 k 0.06624344 m -0.0003682407\n",
      "45 Train Loss 15232.335 Test RE 0.030731184034510365 c 0.006667984 k 0.062441412 m -0.00034566782\n",
      "46 Train Loss 13343.43 Test RE 0.026497660282839026 c 0.0066980855 k 0.06180927 m -0.0003401286\n",
      "47 Train Loss 11019.996 Test RE 0.02617053498294679 c 0.006666502 k 0.0553194 m -0.0003015849\n",
      "48 Train Loss 8694.976 Test RE 0.026841638218705857 c 0.0065297545 k 0.043926332 m -0.00024031158\n",
      "49 Train Loss 7873.4443 Test RE 0.025144034339390862 c 0.0065163253 k 0.042027723 m -0.00023014909\n",
      "50 Train Loss 6830.8633 Test RE 0.024398946797384694 c 0.0064547076 k 0.033294972 m -0.00018563643\n",
      "51 Train Loss 6221.7666 Test RE 0.022203530307227083 c 0.0063960995 k 0.028455934 m -0.0001625194\n",
      "52 Train Loss 5798.158 Test RE 0.020899033290890812 c 0.0064175334 k 0.029217493 m -0.0001661329\n",
      "53 Train Loss 5402.9785 Test RE 0.019823077578249578 c 0.006473307 k 0.03194837 m -0.0001774267\n",
      "54 Train Loss 5191.3125 Test RE 0.018995769035743962 c 0.0064531127 k 0.033346012 m -0.00018275775\n",
      "55 Train Loss 5012.2793 Test RE 0.018320489515306303 c 0.006400363 k 0.032318555 m -0.0001769904\n",
      "56 Train Loss 4871.007 Test RE 0.01773275589119601 c 0.0064052604 k 0.0331621 m -0.00018111485\n",
      "57 Train Loss 4614.4854 Test RE 0.016678791606850398 c 0.0063841734 k 0.032111827 m -0.00017569795\n",
      "58 Train Loss 4461.3228 Test RE 0.015925603816057995 c 0.006347394 k 0.030548776 m -0.00016699183\n",
      "59 Train Loss 4398.375 Test RE 0.015602959147339557 c 0.006334691 k 0.030258719 m -0.00016609677\n",
      "60 Train Loss 4339.7627 Test RE 0.015303582369389715 c 0.006331049 k 0.030391341 m -0.00016769869\n",
      "61 Train Loss 4258.451 Test RE 0.01503624734251416 c 0.0063245674 k 0.030982267 m -0.0001705442\n",
      "62 Train Loss 3999.5122 Test RE 0.013597271053437329 c 0.0063603213 k 0.0346454 m -0.00018427445\n",
      "63 Train Loss 3397.692 Test RE 0.010145345483528813 c 0.006372294 k 0.032848313 m -0.00017130919\n",
      "64 Train Loss 3163.8315 Test RE 0.00824651498908044 c 0.006437055 k 0.032213267 m -0.00016569122\n",
      "65 Train Loss 3044.252 Test RE 0.007126868614258902 c 0.006497416 k 0.03199212 m -0.0001643834\n",
      "66 Train Loss 3008.6206 Test RE 0.006713265871427137 c 0.006519863 k 0.03140651 m -0.00016179708\n",
      "67 Train Loss 2980.7964 Test RE 0.006414298285974242 c 0.0065227267 k 0.031563364 m -0.00016300457\n",
      "68 Train Loss 2964.4746 Test RE 0.006202686836908651 c 0.0065160906 k 0.031415857 m -0.00016305238\n",
      "69 Train Loss 2956.2783 Test RE 0.006151329681902967 c 0.006525366 k 0.03154153 m -0.00016397567\n",
      "70 Train Loss 2945.733 Test RE 0.0060651738132024265 c 0.0065328986 k 0.031805243 m -0.00016515447\n",
      "71 Train Loss 2912.3657 Test RE 0.005495710392499447 c 0.0065285205 k 0.030807544 m -0.00015897384\n",
      "72 Train Loss 2862.5522 Test RE 0.004848365560275438 c 0.0065503577 k 0.031202858 m -0.00015796282\n",
      "73 Train Loss 2840.3591 Test RE 0.00463589411668366 c 0.0065743616 k 0.03175631 m -0.00015832324\n",
      "74 Train Loss 2836.4731 Test RE 0.004592557657278398 c 0.0065796557 k 0.03176361 m -0.00015768816\n",
      "75 Train Loss 2833.96 Test RE 0.004536992457460468 c 0.006578578 k 0.03174935 m -0.00015753022\n",
      "76 Train Loss 2831.8235 Test RE 0.004498013467808641 c 0.0065831016 k 0.031745516 m -0.00015707566\n",
      "77 Train Loss 2830.946 Test RE 0.004521703910736823 c 0.006591397 k 0.031898517 m -0.0001572982\n",
      "78 Train Loss 2830.6367 Test RE 0.004522635260543275 c 0.006594564 k 0.031968445 m -0.00015737273\n",
      "79 Train Loss 2830.0735 Test RE 0.004506312890423583 c 0.006598157 k 0.031921126 m -0.00015670001\n",
      "80 Train Loss 2829.5894 Test RE 0.0045081980625373 c 0.0066020596 k 0.031901944 m -0.00015622089\n",
      "81 Train Loss 2828.1223 Test RE 0.004478726403430711 c 0.0065968498 k 0.031877965 m -0.00015643399\n",
      "82 Train Loss 2826.6804 Test RE 0.0044402646379220366 c 0.006593882 k 0.031951 m -0.00015699572\n",
      "83 Train Loss 2824.5796 Test RE 0.00441978643757638 c 0.0065964307 k 0.032061543 m -0.00015698028\n",
      "84 Train Loss 2821.4163 Test RE 0.004398161165794393 c 0.006595981 k 0.032175947 m -0.00015720756\n",
      "85 Train Loss 2821.068 Test RE 0.004389801211313713 c 0.0065942924 k 0.032109484 m -0.00015685399\n",
      "86 Train Loss 2819.6492 Test RE 0.004357063450175221 c 0.0065963436 k 0.0319524 m -0.00015548887\n",
      "87 Train Loss 2814.845 Test RE 0.004254487003115016 c 0.006619338 k 0.031879663 m -0.0001522713\n",
      "88 Train Loss 2813.7327 Test RE 0.0042511856041937785 c 0.00663068 k 0.03205316 m -0.00015213297\n",
      "89 Train Loss 2813.1426 Test RE 0.004235168023410298 c 0.0066341925 k 0.032012664 m -0.00015135325\n",
      "90 Train Loss 2811.3154 Test RE 0.0042030173864669165 c 0.006634451 k 0.03191686 m -0.00015059473\n",
      "91 Train Loss 2799.1208 Test RE 0.004030016700344231 c 0.006690247 k 0.032084584 m -0.00014663528\n",
      "92 Train Loss 2784.47 Test RE 0.0037766998867423346 c 0.006772319 k 0.03261438 m -0.00014210261\n",
      "93 Train Loss 2774.4294 Test RE 0.00357534053161003 c 0.00681327 k 0.032586895 m -0.00013781463\n",
      "94 Train Loss 2764.4438 Test RE 0.0033605792066114166 c 0.0068337684 k 0.032302126 m -0.00013463096\n",
      "95 Train Loss 2756.596 Test RE 0.0031902945190491845 c 0.006850348 k 0.032498267 m -0.00013529866\n",
      "96 Train Loss 2754.5984 Test RE 0.003143867993985352 c 0.006861771 k 0.032505836 m -0.0001342997\n",
      "97 Train Loss 2752.6362 Test RE 0.003098971811721548 c 0.0068782438 k 0.032376457 m -0.0001322155\n",
      "98 Train Loss 2751.0928 Test RE 0.0030648226667321274 c 0.0068958458 k 0.03236507 m -0.00013086044\n",
      "99 Train Loss 2748.88 Test RE 0.003005185402946485 c 0.006920748 k 0.03245537 m -0.00012965972\n",
      "100 Train Loss 2748.4485 Test RE 0.0030077785686766726 c 0.0069207507 k 0.032439232 m -0.00012978655\n",
      "101 Train Loss 2748.2217 Test RE 0.003005340733211596 c 0.006921279 k 0.032488886 m -0.00013021316\n",
      "102 Train Loss 2747.957 Test RE 0.00299507235397547 c 0.00692584 k 0.032484215 m -0.00013001084\n",
      "103 Train Loss 2747.1016 Test RE 0.002984214283130852 c 0.006940363 k 0.032445263 m -0.00012881368\n",
      "104 Train Loss 2745.9717 Test RE 0.002956323210756463 c 0.0069669713 k 0.03259104 m -0.00012733351\n",
      "105 Train Loss 2745.0852 Test RE 0.002918395090681851 c 0.0069838413 k 0.032611236 m -0.0001260805\n",
      "106 Train Loss 2744.3894 Test RE 0.0029165818373382254 c 0.0069848355 k 0.03250516 m -0.00012548601\n",
      "107 Train Loss 2743.5928 Test RE 0.0029064343013103892 c 0.0070118834 k 0.032545414 m -0.00012330493\n",
      "108 Train Loss 2742.9285 Test RE 0.002873355187278048 c 0.007036315 k 0.03256843 m -0.00012132165\n",
      "109 Train Loss 2742.4739 Test RE 0.0028752114460565475 c 0.007046956 k 0.03249796 m -0.00011998358\n",
      "110 Train Loss 2742.2275 Test RE 0.0028714063467402393 c 0.00706458 k 0.032501556 m -0.000118439144\n",
      "111 Train Loss 2740.4846 Test RE 0.0028393655751173027 c 0.007120975 k 0.032515038 m -0.000113642884\n",
      "112 Train Loss 2739.589 Test RE 0.002808215790297585 c 0.0071317093 k 0.032428358 m -0.00011203714\n",
      "113 Train Loss 2739.3445 Test RE 0.0028009653242770704 c 0.0071407766 k 0.032439988 m -0.0001110978\n",
      "114 Train Loss 2739.1833 Test RE 0.0027980590701948624 c 0.0071522966 k 0.032449346 m -0.000110022\n",
      "115 Train Loss 2739.1191 Test RE 0.0027966196745483557 c 0.0071602645 k 0.03241829 m -0.0001091021\n",
      "116 Train Loss 2739.0405 Test RE 0.002797823464288497 c 0.0071715103 k 0.032417018 m -0.000108165135\n",
      "117 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "118 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "119 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "120 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "121 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "122 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "123 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "124 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "125 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "126 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "127 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "128 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "129 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "130 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "131 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "132 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "133 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "134 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "135 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "136 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "137 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "138 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "139 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "140 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "141 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "142 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "143 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "144 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "145 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "146 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "147 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "148 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "149 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "150 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "151 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "152 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "153 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "154 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "155 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "156 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "157 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "158 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "159 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "160 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "161 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "162 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "163 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "164 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "165 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "166 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "167 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "168 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "169 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "170 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "171 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "172 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "173 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "174 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "175 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "176 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "177 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "178 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "179 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "180 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "181 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "182 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "183 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "184 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "185 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "186 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "187 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "188 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "189 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "190 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "191 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "192 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "193 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "194 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "195 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "196 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "197 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "198 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "199 Train Loss 2739.03 Test RE 0.0027967677795608836 c 0.0071729184 k 0.032422226 m -0.00010809209\n",
      "Training time: 69.14\n",
      "Training time: 69.14\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 6846092.5 Test RE 0.9958538707270588 c 0.00021054185 k 0.0015020698 m -2.1681498e-07\n",
      "1 Train Loss 6639928.5 Test RE 0.9807288663283062 c 0.00068521645 k 0.009332575 m -4.2334377e-06\n",
      "2 Train Loss 6091730.0 Test RE 0.9393416267531698 c 0.0015376605 k 0.014638382 m -8.577028e-06\n",
      "3 Train Loss 5459017.0 Test RE 0.8892014298284826 c 0.0023444532 k 0.019806128 m -1.5371073e-05\n",
      "4 Train Loss 4439665.5 Test RE 0.8019526964934542 c 0.0036680745 k 0.028910477 m -2.4010786e-05\n",
      "5 Train Loss 3601831.8 Test RE 0.7223998385492411 c 0.004436936 k 0.034199033 m -2.8797593e-05\n",
      "6 Train Loss nan Test RE nan c nan k nan m nan\n",
      "7 Train Loss nan Test RE nan c nan k nan m nan\n",
      "8 Train Loss nan Test RE nan c nan k nan m nan\n",
      "9 Train Loss nan Test RE nan c nan k nan m nan\n",
      "10 Train Loss nan Test RE nan c nan k nan m nan\n",
      "11 Train Loss nan Test RE nan c nan k nan m nan\n",
      "12 Train Loss nan Test RE nan c nan k nan m nan\n",
      "13 Train Loss nan Test RE nan c nan k nan m nan\n",
      "14 Train Loss nan Test RE nan c nan k nan m nan\n",
      "15 Train Loss nan Test RE nan c nan k nan m nan\n",
      "16 Train Loss nan Test RE nan c nan k nan m nan\n",
      "17 Train Loss nan Test RE nan c nan k nan m nan\n",
      "18 Train Loss nan Test RE nan c nan k nan m nan\n",
      "19 Train Loss nan Test RE nan c nan k nan m nan\n",
      "20 Train Loss nan Test RE nan c nan k nan m nan\n",
      "21 Train Loss nan Test RE nan c nan k nan m nan\n",
      "22 Train Loss nan Test RE nan c nan k nan m nan\n",
      "23 Train Loss nan Test RE nan c nan k nan m nan\n",
      "24 Train Loss nan Test RE nan c nan k nan m nan\n",
      "25 Train Loss nan Test RE nan c nan k nan m nan\n",
      "26 Train Loss nan Test RE nan c nan k nan m nan\n",
      "27 Train Loss nan Test RE nan c nan k nan m nan\n",
      "28 Train Loss nan Test RE nan c nan k nan m nan\n",
      "29 Train Loss nan Test RE nan c nan k nan m nan\n",
      "30 Train Loss nan Test RE nan c nan k nan m nan\n",
      "31 Train Loss nan Test RE nan c nan k nan m nan\n",
      "32 Train Loss nan Test RE nan c nan k nan m nan\n",
      "33 Train Loss nan Test RE nan c nan k nan m nan\n",
      "34 Train Loss nan Test RE nan c nan k nan m nan\n",
      "35 Train Loss nan Test RE nan c nan k nan m nan\n",
      "36 Train Loss nan Test RE nan c nan k nan m nan\n",
      "37 Train Loss nan Test RE nan c nan k nan m nan\n",
      "38 Train Loss nan Test RE nan c nan k nan m nan\n",
      "39 Train Loss nan Test RE nan c nan k nan m nan\n",
      "40 Train Loss nan Test RE nan c nan k nan m nan\n",
      "41 Train Loss nan Test RE nan c nan k nan m nan\n",
      "42 Train Loss nan Test RE nan c nan k nan m nan\n",
      "43 Train Loss nan Test RE nan c nan k nan m nan\n",
      "44 Train Loss nan Test RE nan c nan k nan m nan\n",
      "45 Train Loss nan Test RE nan c nan k nan m nan\n",
      "46 Train Loss nan Test RE nan c nan k nan m nan\n",
      "47 Train Loss nan Test RE nan c nan k nan m nan\n",
      "48 Train Loss nan Test RE nan c nan k nan m nan\n",
      "49 Train Loss nan Test RE nan c nan k nan m nan\n",
      "50 Train Loss nan Test RE nan c nan k nan m nan\n",
      "51 Train Loss nan Test RE nan c nan k nan m nan\n",
      "52 Train Loss nan Test RE nan c nan k nan m nan\n",
      "53 Train Loss nan Test RE nan c nan k nan m nan\n",
      "54 Train Loss nan Test RE nan c nan k nan m nan\n",
      "55 Train Loss nan Test RE nan c nan k nan m nan\n",
      "56 Train Loss nan Test RE nan c nan k nan m nan\n",
      "57 Train Loss nan Test RE nan c nan k nan m nan\n",
      "58 Train Loss nan Test RE nan c nan k nan m nan\n",
      "59 Train Loss nan Test RE nan c nan k nan m nan\n",
      "60 Train Loss nan Test RE nan c nan k nan m nan\n",
      "61 Train Loss nan Test RE nan c nan k nan m nan\n",
      "62 Train Loss nan Test RE nan c nan k nan m nan\n",
      "63 Train Loss nan Test RE nan c nan k nan m nan\n",
      "64 Train Loss nan Test RE nan c nan k nan m nan\n",
      "65 Train Loss nan Test RE nan c nan k nan m nan\n",
      "66 Train Loss nan Test RE nan c nan k nan m nan\n",
      "67 Train Loss nan Test RE nan c nan k nan m nan\n",
      "68 Train Loss nan Test RE nan c nan k nan m nan\n",
      "69 Train Loss nan Test RE nan c nan k nan m nan\n",
      "70 Train Loss nan Test RE nan c nan k nan m nan\n",
      "71 Train Loss nan Test RE nan c nan k nan m nan\n",
      "72 Train Loss nan Test RE nan c nan k nan m nan\n",
      "73 Train Loss nan Test RE nan c nan k nan m nan\n",
      "74 Train Loss nan Test RE nan c nan k nan m nan\n",
      "75 Train Loss nan Test RE nan c nan k nan m nan\n",
      "76 Train Loss nan Test RE nan c nan k nan m nan\n",
      "77 Train Loss nan Test RE nan c nan k nan m nan\n",
      "78 Train Loss nan Test RE nan c nan k nan m nan\n",
      "79 Train Loss nan Test RE nan c nan k nan m nan\n",
      "80 Train Loss nan Test RE nan c nan k nan m nan\n",
      "81 Train Loss nan Test RE nan c nan k nan m nan\n",
      "82 Train Loss nan Test RE nan c nan k nan m nan\n",
      "83 Train Loss nan Test RE nan c nan k nan m nan\n",
      "84 Train Loss nan Test RE nan c nan k nan m nan\n",
      "85 Train Loss nan Test RE nan c nan k nan m nan\n",
      "86 Train Loss nan Test RE nan c nan k nan m nan\n",
      "87 Train Loss nan Test RE nan c nan k nan m nan\n",
      "88 Train Loss nan Test RE nan c nan k nan m nan\n",
      "89 Train Loss nan Test RE nan c nan k nan m nan\n",
      "90 Train Loss nan Test RE nan c nan k nan m nan\n",
      "91 Train Loss nan Test RE nan c nan k nan m nan\n",
      "92 Train Loss nan Test RE nan c nan k nan m nan\n",
      "93 Train Loss nan Test RE nan c nan k nan m nan\n",
      "94 Train Loss nan Test RE nan c nan k nan m nan\n",
      "95 Train Loss nan Test RE nan c nan k nan m nan\n",
      "96 Train Loss nan Test RE nan c nan k nan m nan\n",
      "97 Train Loss nan Test RE nan c nan k nan m nan\n",
      "98 Train Loss nan Test RE nan c nan k nan m nan\n",
      "99 Train Loss nan Test RE nan c nan k nan m nan\n",
      "100 Train Loss nan Test RE nan c nan k nan m nan\n",
      "101 Train Loss nan Test RE nan c nan k nan m nan\n",
      "102 Train Loss nan Test RE nan c nan k nan m nan\n",
      "103 Train Loss nan Test RE nan c nan k nan m nan\n",
      "104 Train Loss nan Test RE nan c nan k nan m nan\n",
      "105 Train Loss nan Test RE nan c nan k nan m nan\n",
      "106 Train Loss nan Test RE nan c nan k nan m nan\n",
      "107 Train Loss nan Test RE nan c nan k nan m nan\n",
      "108 Train Loss nan Test RE nan c nan k nan m nan\n",
      "109 Train Loss nan Test RE nan c nan k nan m nan\n",
      "110 Train Loss nan Test RE nan c nan k nan m nan\n",
      "111 Train Loss nan Test RE nan c nan k nan m nan\n",
      "112 Train Loss nan Test RE nan c nan k nan m nan\n",
      "113 Train Loss nan Test RE nan c nan k nan m nan\n",
      "114 Train Loss nan Test RE nan c nan k nan m nan\n",
      "115 Train Loss nan Test RE nan c nan k nan m nan\n",
      "116 Train Loss nan Test RE nan c nan k nan m nan\n",
      "117 Train Loss nan Test RE nan c nan k nan m nan\n",
      "118 Train Loss nan Test RE nan c nan k nan m nan\n",
      "119 Train Loss nan Test RE nan c nan k nan m nan\n",
      "120 Train Loss nan Test RE nan c nan k nan m nan\n",
      "121 Train Loss nan Test RE nan c nan k nan m nan\n",
      "122 Train Loss nan Test RE nan c nan k nan m nan\n",
      "123 Train Loss nan Test RE nan c nan k nan m nan\n",
      "124 Train Loss nan Test RE nan c nan k nan m nan\n",
      "125 Train Loss nan Test RE nan c nan k nan m nan\n",
      "126 Train Loss nan Test RE nan c nan k nan m nan\n",
      "127 Train Loss nan Test RE nan c nan k nan m nan\n",
      "128 Train Loss nan Test RE nan c nan k nan m nan\n",
      "129 Train Loss nan Test RE nan c nan k nan m nan\n",
      "130 Train Loss nan Test RE nan c nan k nan m nan\n",
      "131 Train Loss nan Test RE nan c nan k nan m nan\n",
      "132 Train Loss nan Test RE nan c nan k nan m nan\n",
      "133 Train Loss nan Test RE nan c nan k nan m nan\n",
      "134 Train Loss nan Test RE nan c nan k nan m nan\n",
      "135 Train Loss nan Test RE nan c nan k nan m nan\n",
      "136 Train Loss nan Test RE nan c nan k nan m nan\n",
      "137 Train Loss nan Test RE nan c nan k nan m nan\n",
      "138 Train Loss nan Test RE nan c nan k nan m nan\n",
      "139 Train Loss nan Test RE nan c nan k nan m nan\n",
      "140 Train Loss nan Test RE nan c nan k nan m nan\n",
      "141 Train Loss nan Test RE nan c nan k nan m nan\n",
      "142 Train Loss nan Test RE nan c nan k nan m nan\n",
      "143 Train Loss nan Test RE nan c nan k nan m nan\n",
      "144 Train Loss nan Test RE nan c nan k nan m nan\n",
      "145 Train Loss nan Test RE nan c nan k nan m nan\n",
      "146 Train Loss nan Test RE nan c nan k nan m nan\n",
      "147 Train Loss nan Test RE nan c nan k nan m nan\n",
      "148 Train Loss nan Test RE nan c nan k nan m nan\n",
      "149 Train Loss nan Test RE nan c nan k nan m nan\n",
      "150 Train Loss nan Test RE nan c nan k nan m nan\n",
      "151 Train Loss nan Test RE nan c nan k nan m nan\n",
      "152 Train Loss nan Test RE nan c nan k nan m nan\n",
      "153 Train Loss nan Test RE nan c nan k nan m nan\n",
      "154 Train Loss nan Test RE nan c nan k nan m nan\n",
      "155 Train Loss nan Test RE nan c nan k nan m nan\n",
      "156 Train Loss nan Test RE nan c nan k nan m nan\n",
      "157 Train Loss nan Test RE nan c nan k nan m nan\n",
      "158 Train Loss nan Test RE nan c nan k nan m nan\n",
      "159 Train Loss nan Test RE nan c nan k nan m nan\n",
      "160 Train Loss nan Test RE nan c nan k nan m nan\n",
      "161 Train Loss nan Test RE nan c nan k nan m nan\n",
      "162 Train Loss nan Test RE nan c nan k nan m nan\n",
      "163 Train Loss nan Test RE nan c nan k nan m nan\n",
      "164 Train Loss nan Test RE nan c nan k nan m nan\n",
      "165 Train Loss nan Test RE nan c nan k nan m nan\n",
      "166 Train Loss nan Test RE nan c nan k nan m nan\n",
      "167 Train Loss nan Test RE nan c nan k nan m nan\n",
      "168 Train Loss nan Test RE nan c nan k nan m nan\n",
      "169 Train Loss nan Test RE nan c nan k nan m nan\n",
      "170 Train Loss nan Test RE nan c nan k nan m nan\n",
      "171 Train Loss nan Test RE nan c nan k nan m nan\n",
      "172 Train Loss nan Test RE nan c nan k nan m nan\n",
      "173 Train Loss nan Test RE nan c nan k nan m nan\n",
      "174 Train Loss nan Test RE nan c nan k nan m nan\n",
      "175 Train Loss nan Test RE nan c nan k nan m nan\n",
      "176 Train Loss nan Test RE nan c nan k nan m nan\n",
      "177 Train Loss nan Test RE nan c nan k nan m nan\n",
      "178 Train Loss nan Test RE nan c nan k nan m nan\n",
      "179 Train Loss nan Test RE nan c nan k nan m nan\n",
      "180 Train Loss nan Test RE nan c nan k nan m nan\n",
      "181 Train Loss nan Test RE nan c nan k nan m nan\n",
      "182 Train Loss nan Test RE nan c nan k nan m nan\n",
      "183 Train Loss nan Test RE nan c nan k nan m nan\n",
      "184 Train Loss nan Test RE nan c nan k nan m nan\n",
      "185 Train Loss nan Test RE nan c nan k nan m nan\n",
      "186 Train Loss nan Test RE nan c nan k nan m nan\n",
      "187 Train Loss nan Test RE nan c nan k nan m nan\n",
      "188 Train Loss nan Test RE nan c nan k nan m nan\n",
      "189 Train Loss nan Test RE nan c nan k nan m nan\n",
      "190 Train Loss nan Test RE nan c nan k nan m nan\n",
      "191 Train Loss nan Test RE nan c nan k nan m nan\n",
      "192 Train Loss nan Test RE nan c nan k nan m nan\n",
      "193 Train Loss nan Test RE nan c nan k nan m nan\n",
      "194 Train Loss nan Test RE nan c nan k nan m nan\n",
      "195 Train Loss nan Test RE nan c nan k nan m nan\n",
      "196 Train Loss nan Test RE nan c nan k nan m nan\n",
      "197 Train Loss nan Test RE nan c nan k nan m nan\n",
      "198 Train Loss nan Test RE nan c nan k nan m nan\n",
      "199 Train Loss nan Test RE nan c nan k nan m nan\n",
      "Training time: 154.27\n",
      "Training time: 154.27\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 200\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "m_full = []\n",
    "k_full = []\n",
    "c_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "n_val = 5.0\n",
    "rowdy_terms = 2\n",
    "\n",
    "for reps in range(max_reps):   \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []   \n",
    "    beta_val = []\n",
    "    m_val = []\n",
    "    k_val = []\n",
    "    c_val = []\n",
    "\n",
    "    torch.manual_seed(reps*36)\n",
    "    N_f = 10000 #Total number of collocation points\n",
    "\n",
    "    layers = np.array([1,50,50,50,1]) #9 hidden layers\n",
    "    PINN = Sequentialmodel(layers,n_val,rowdy_terms)\n",
    "\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.25, \n",
    "                                max_iter = 10, \n",
    "                                max_eval = 15, \n",
    "                                tolerance_grad = 1e-6, \n",
    "                                tolerance_change = 1e-6, \n",
    "                                history_size = 100, \n",
    "                                line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "    train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    beta_full.append(beta_val)\n",
    "    m_full.append(m_val)\n",
    "    k_full.append(k_val)\n",
    "    c_full.append(c_val)\n",
    "    \n",
    "\n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"beta\": beta_full,\"m\": m_full,\"k\": k_full,\"c\": c_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "pmHEeBpzfFQh",
    "outputId": "990b2054-f35e-4c8e-c378-69ff2eb19f4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f19640bc590>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_pred = PINN.test()\n",
    "plt.plot(t,x_true,'b')\n",
    "plt.plot(t,x_pred,'r--')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "stan_1D_FODE_tune.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
