{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def true_2D_1(xt): #True function for 2D_1 Klein Gordon Equation x \\in [-50,50] , t \\in [0,10]\n",
    "#     y = xt[:,0]*np.cos(xt[:,1])\n",
    "#     return y.reshape(-1,1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_thresh = 25000\n",
    "level = \"_low\"\n",
    "label = \"Burgers_stan\" + level\n",
    "\n",
    "x = np.linspace(-1,1,256).reshape(-1,1)\n",
    "t = np.linspace(0,1,100).reshape(-1,1)\n",
    "\n",
    "X,T = np.meshgrid(x,t)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "T = T.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xt = np.hstack((X,T))\n",
    "\n",
    "# bound_pts_1 = (X == 0).reshape(-1,)\n",
    "# bound_pts_2 = np.logical_and(Y == 0,X != 0).reshape(-1,)\n",
    "# bound_pts_3 = np.logical_and(X == 1,Y != 0).reshape(-1,) \n",
    "# bound_pts_4 = np.logical_and(Y == 1,X != 1).reshape(-1,) \n",
    "\n",
    "# xy_bound_1 = xy[bound_pts_1,:]\n",
    "# xy_bound_2 = xy[bound_pts_2,:]\n",
    "# xy_bound_3 = xy[bound_pts_3,:]\n",
    "# xy_bound_4 = xy[bound_pts_4,:]\n",
    "\n",
    "# u_bound_1 = 1000*np.ones((np.shape(xy_bound_1)[0],1))\n",
    "# u_bound_2 = 800*np.ones((np.shape(xy_bound_2)[0],1))\n",
    "# u_bound_3 = 500*np.ones((np.shape(xy_bound_3)[0],1))\n",
    "# u_bound_4 = np.zeros((np.shape(xy_bound_4)[0],1))\n",
    "\n",
    "# xy_bound = np.vstack((xy_bound_1,xy_bound_2,xy_bound_3,xy_bound_4))\n",
    "# u_bound = np.vstack((u_bound_1,u_bound_2,u_bound_3,u_bound_4))\n",
    "\n",
    "# xy_test_tensor = torch.from_numpy(xy).float().to(device)\n",
    "\n",
    "lb_xt = xt[0]\n",
    "ub_xt = xt[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat('../burgers_shock_3.mat') \n",
    "\n",
    "pi = torch.from_numpy(np.array(np.pi)).double().to(device)\n",
    "\n",
    "x = np.array(data['x'])\n",
    "t = np.array(data['t'])\n",
    "\n",
    "X,T = np.meshgrid(x,t)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "T = T.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xt = np.hstack((X,T))\n",
    "\n",
    "u_true = np.array(data['usol'][:])\n",
    "\n",
    "xt_test_tensor = torch.from_numpy(xt).float().to(device)\n",
    "u_true_norm = np.linalg.norm(u_true,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_T,N_f,seed):\n",
    "    '''Boundary Conditions''' \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    x_l = x[0]*np.ones((N_T,1))\n",
    "    t_l = np.random.uniform(t[0],t[-1],(N_T,1))\n",
    "    xt_l = np.hstack((x_l,t_l))\n",
    "    u_l = 0.0*np.ones((N_T,1))\n",
    "    \n",
    "    x_r = x[-1]*np.ones((N_T,1))\n",
    "    t_r = np.random.uniform(t[0],t[-1],(N_T,1))\n",
    "    xt_r = np.hstack((x_r,t_r))\n",
    "    u_r = 0.0*np.ones((N_T,1))\n",
    "    \n",
    "    x_0 = np.random.uniform(x[0],x[-1],(N_T,1))\n",
    "    t_0 = t[0]*np.ones((N_T,1))\n",
    "    xt_0 = np.hstack((x_0,t_0))\n",
    "    u_0 = -1.0*np.sin(np.pi*x_0)\n",
    "    \n",
    "    xt_BC = np.vstack((xt_l,xt_r,xt_0,xt)) #choose indices from  set 'idx' (x,t)\n",
    "    u_BC = np.vstack((u_l,u_r,u_0,u_true.reshape(-1,1)))\n",
    "    \n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    samples = sampling(N_f)\n",
    "    \n",
    "    xt_coll = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    \n",
    "    xt_coll = np.vstack((xt_coll, xt_BC)) # append training points to collocation points \n",
    "\n",
    "    return xt_coll, xt_BC, u_BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        self.iter = 0\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)   \n",
    "        \n",
    "        \n",
    "        self.p = Parameter(torch.ones(1))\n",
    "        self.p.requiresGrad = True\n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xy):\n",
    "        if torch.is_tensor(xy) != True:         \n",
    "            xy = torch.from_numpy(xy)                \n",
    "        \n",
    "        ubxy = torch.from_numpy(ub_xt).float().to(device)\n",
    "        lbxy = torch.from_numpy(lb_xt).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xy = 2.0*(xy - lbxy)/(ubxy - lbxy) - 1.0\n",
    "        \n",
    "        #convert to float\n",
    "        a = xy.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a =self.activation(z)\n",
    "            \n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC(self,xy,u):\n",
    "                \n",
    "        loss_bc = self.loss_function(self.forward(xy), u)\n",
    "                \n",
    "        return loss_bc\n",
    "    \n",
    "    def loss_PDE(self, xy_coll, f_hat):\n",
    "        \n",
    "        g = xy_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_y = autograd.grad(u,g,torch.ones([xy_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_xx_yy = autograd.grad(u_x_y,g,torch.ones(xy_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        du_dx = u_x_y[:,[0]]\n",
    "        du_dt = u_x_y[:,[1]]\n",
    "        \n",
    "        d2u_dx2 = u_xx_yy[:,[0]]\n",
    "        \n",
    "\n",
    "        f = du_dt + u*du_dx - self.p*d2u_dx2/pi\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xy_BC,u_BC,xy_coll,f_hat):\n",
    "\n",
    "        loss_BC = self.loss_BC(xy_BC,u_BC)\n",
    "        loss_f = self.loss_PDE(xy_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_BC + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "         \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        u_pred = self.forward(xt_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "        \n",
    "        \n",
    "        return test_mse, test_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(xy_BC,u_BC,xy_coll,f_hat,seed):\n",
    "\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(xy_BC,u_BC,xy_coll,f_hat)\n",
    "        loss.backward()\n",
    "        #print(loss.cpu().detach().numpy())\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*9)\n",
    "    start_time = time.time() \n",
    "    thresh_flag = 0\n",
    "\n",
    "    xy_coll_np_array, xy_BC_np_array, u_BC_np_array = trainingdata(N_T,N_f,rep*22)\n",
    "        \n",
    "    xy_coll = torch.from_numpy(xy_coll_np_array).float().to(device)\n",
    "    xy_BC = torch.from_numpy(xy_BC_np_array).float().to(device)\n",
    "    u_BC = torch.from_numpy(u_BC_np_array).float().to(device)\n",
    "        \n",
    "    f_hat = torch.zeros(xy_coll.shape[0],1).to(device)\n",
    "    \n",
    "\n",
    "    for i in range(max_iter):\n",
    "        train_step(xy_BC,u_BC,xy_coll,f_hat,i)\n",
    "        loss_np = PINN.loss(xy_BC,u_BC,xy_coll,f_hat).cpu().detach().numpy()\n",
    "        \n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1          \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        \n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test RE\",test_re_loss[-1],\"p:\",PINN.p.cpu().detach().numpy())   \n",
    "        \n",
    "    elapsed_time[rep] = time.time() - start_time  \n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Burgers_stan_low\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 0.07835406 Test RE 0.42502290072556 p: [0.22832176]\n",
      "1 Train Loss 0.027563158 Test RE 0.2474792544506492 p: [0.10217235]\n",
      "2 Train Loss 0.01766327 Test RE 0.18315312924194185 p: [0.10396785]\n",
      "3 Train Loss 0.012499251 Test RE 0.16780414727855164 p: [0.11663058]\n",
      "4 Train Loss 0.009832122 Test RE 0.14318220955546032 p: [0.13545582]\n",
      "5 Train Loss 0.008073669 Test RE 0.127568570550043 p: [0.12195039]\n",
      "6 Train Loss 0.006203779 Test RE 0.1149602180060312 p: [0.10888404]\n",
      "7 Train Loss 0.0043513123 Test RE 0.08962275508374988 p: [0.09868878]\n",
      "8 Train Loss 0.0033969672 Test RE 0.08099848298265418 p: [0.10266455]\n",
      "9 Train Loss 0.0025362617 Test RE 0.06139455713938018 p: [0.10169037]\n",
      "10 Train Loss 0.0018799524 Test RE 0.05500658175055795 p: [0.10323656]\n",
      "11 Train Loss 0.0013649691 Test RE 0.04518150955715208 p: [0.10304209]\n",
      "12 Train Loss 0.0009819029 Test RE 0.04161168850611346 p: [0.10090591]\n",
      "13 Train Loss 0.0007959886 Test RE 0.03971040299313216 p: [0.09661458]\n",
      "14 Train Loss 0.00069453893 Test RE 0.037757821196008706 p: [0.09681378]\n",
      "15 Train Loss 0.0006291699 Test RE 0.03718095392471993 p: [0.10061558]\n",
      "16 Train Loss 0.0005945662 Test RE 0.03683867887063836 p: [0.09710845]\n",
      "17 Train Loss 0.0005659093 Test RE 0.03660364647576475 p: [0.09826914]\n",
      "18 Train Loss 0.0005485207 Test RE 0.03625805756278837 p: [0.09779069]\n",
      "19 Train Loss 0.0005246304 Test RE 0.03593729993247766 p: [0.09784075]\n",
      "20 Train Loss 0.00050337147 Test RE 0.03619006008711569 p: [0.09764821]\n",
      "21 Train Loss 0.0004919801 Test RE 0.03577805515102147 p: [0.09743059]\n",
      "22 Train Loss 0.00048661305 Test RE 0.03577706320441693 p: [0.09715186]\n",
      "23 Train Loss 0.00048349932 Test RE 0.03582522212440172 p: [0.0971678]\n",
      "24 Train Loss 0.00047821034 Test RE 0.03592412989665246 p: [0.09677582]\n",
      "25 Train Loss 0.00047149247 Test RE 0.035690302763466485 p: [0.09708645]\n",
      "26 Train Loss 0.00046673114 Test RE 0.035703872957117876 p: [0.09697074]\n",
      "27 Train Loss 0.00046340996 Test RE 0.035705167132990503 p: [0.09710113]\n",
      "28 Train Loss 0.00046081073 Test RE 0.035682303846470455 p: [0.09688262]\n",
      "29 Train Loss 0.0004589334 Test RE 0.03567990568319813 p: [0.09690419]\n",
      "30 Train Loss 0.00045753943 Test RE 0.03566447710483866 p: [0.09688026]\n",
      "31 Train Loss 0.0004548471 Test RE 0.03560295183725862 p: [0.09686881]\n",
      "32 Train Loss 0.0004524904 Test RE 0.03565177205525775 p: [0.096721]\n",
      "33 Train Loss 0.00045162052 Test RE 0.03562287973430859 p: [0.09676512]\n",
      "34 Train Loss 0.00045161176 Test RE 0.03562360591152294 p: [0.09676279]\n",
      "35 Train Loss 0.00045160294 Test RE 0.03562390587679648 p: [0.0967738]\n",
      "36 Train Loss 0.00045159686 Test RE 0.03563292979905598 p: [0.09677599]\n",
      "37 Train Loss 0.00045138906 Test RE 0.03560277131940334 p: [0.09694927]\n",
      "38 Train Loss 0.00044931727 Test RE 0.035635125918834276 p: [0.09685344]\n",
      "39 Train Loss 0.0004492766 Test RE 0.035628569693027835 p: [0.09686252]\n",
      "40 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "41 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "42 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "43 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "44 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "45 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "46 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "47 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "48 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "49 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "50 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "51 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "52 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "53 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "54 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "55 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "56 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "57 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "58 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "59 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "60 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "61 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "62 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "63 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "64 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "65 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "66 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "67 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "68 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "69 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "70 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "71 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "72 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "73 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "74 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "75 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "76 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "77 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "78 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "79 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "80 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "81 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "82 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "83 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "84 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "85 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "86 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "87 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "88 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "89 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "90 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "91 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "92 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "93 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "94 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "95 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "96 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "97 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "98 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "99 Train Loss 0.0004492697 Test RE 0.03563433575933251 p: [0.0968724]\n",
      "Training time: 15.62\n",
      "Training time: 15.62\n",
      "Burgers_stan_low\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 0.15998055 Test RE 0.6989960089188524 p: [0.9837389]\n",
      "1 Train Loss 0.039992817 Test RE 0.27835410801443705 p: [0.2730884]\n",
      "2 Train Loss 0.018743265 Test RE 0.20037372795423036 p: [0.13513835]\n",
      "3 Train Loss 0.01500391 Test RE 0.1807223492177462 p: [0.12755835]\n",
      "4 Train Loss 0.013020484 Test RE 0.17038978984884218 p: [0.12666422]\n",
      "5 Train Loss 0.011229107 Test RE 0.16353919278659854 p: [0.12377028]\n",
      "6 Train Loss 0.0098482575 Test RE 0.14847529091887388 p: [0.13128224]\n",
      "7 Train Loss 0.008322086 Test RE 0.12942990174272723 p: [0.13241023]\n",
      "8 Train Loss 0.0065289484 Test RE 0.0989337367488544 p: [0.10764047]\n",
      "9 Train Loss 0.005157958 Test RE 0.09137136453133919 p: [0.09562804]\n",
      "10 Train Loss 0.0041795215 Test RE 0.08305952316743359 p: [0.09788001]\n",
      "11 Train Loss 0.0035335622 Test RE 0.07338264424808262 p: [0.10492739]\n",
      "12 Train Loss 0.0027982532 Test RE 0.06644592186734065 p: [0.1151991]\n",
      "13 Train Loss 0.0021171225 Test RE 0.05765889283719471 p: [0.11186256]\n",
      "14 Train Loss 0.0016833544 Test RE 0.05118516238758796 p: [0.10739607]\n",
      "15 Train Loss 0.0014474986 Test RE 0.04902291703929028 p: [0.10337022]\n",
      "16 Train Loss 0.0013361258 Test RE 0.04739186414732714 p: [0.10194672]\n",
      "17 Train Loss 0.0011982198 Test RE 0.04440031553989864 p: [0.09757806]\n",
      "18 Train Loss 0.0010769096 Test RE 0.0434579935876867 p: [0.10019311]\n",
      "19 Train Loss 0.00096438813 Test RE 0.041082670742200525 p: [0.09520166]\n",
      "20 Train Loss 0.00086770253 Test RE 0.039921147032178625 p: [0.09786251]\n",
      "21 Train Loss 0.00080795365 Test RE 0.04021439809942223 p: [0.09901923]\n",
      "22 Train Loss 0.00074556354 Test RE 0.03853181677740027 p: [0.09936655]\n",
      "23 Train Loss 0.0006971312 Test RE 0.03840203678687206 p: [0.10213472]\n",
      "24 Train Loss 0.0006525996 Test RE 0.03720697314748871 p: [0.0990022]\n",
      "25 Train Loss 0.00062733056 Test RE 0.037214577342384265 p: [0.0987073]\n",
      "26 Train Loss 0.00061059895 Test RE 0.03713314118431537 p: [0.09741628]\n",
      "27 Train Loss 0.00059971807 Test RE 0.037063069125077834 p: [0.09806634]\n",
      "28 Train Loss 0.0005813495 Test RE 0.03667804470506392 p: [0.09801274]\n",
      "29 Train Loss 0.0005656953 Test RE 0.036393964060476305 p: [0.09827899]\n",
      "30 Train Loss 0.00055834267 Test RE 0.03633114872240882 p: [0.09814386]\n",
      "31 Train Loss 0.00054271414 Test RE 0.036401379054002876 p: [0.0979266]\n",
      "32 Train Loss 0.000529428 Test RE 0.03621627243145364 p: [0.09861454]\n",
      "33 Train Loss 0.00052216416 Test RE 0.03616817055051956 p: [0.0980792]\n",
      "34 Train Loss 0.0005063927 Test RE 0.035971950359600095 p: [0.09836493]\n",
      "35 Train Loss 0.00050256815 Test RE 0.036004851570303475 p: [0.09783474]\n",
      "36 Train Loss 0.0004983777 Test RE 0.03591717808812811 p: [0.09787722]\n",
      "37 Train Loss 0.00049468677 Test RE 0.03580379590625096 p: [0.09747217]\n",
      "38 Train Loss 0.00049195305 Test RE 0.03580868191149843 p: [0.09720931]\n",
      "39 Train Loss 0.000489167 Test RE 0.035728081726807255 p: [0.09699523]\n",
      "40 Train Loss 0.00048795078 Test RE 0.035787682338991926 p: [0.09728844]\n",
      "41 Train Loss 0.00048288924 Test RE 0.03585404312624153 p: [0.09775519]\n",
      "42 Train Loss 0.00047923805 Test RE 0.03567935079347902 p: [0.09726972]\n",
      "43 Train Loss 0.0004759435 Test RE 0.03568634109516468 p: [0.09714088]\n",
      "44 Train Loss 0.0004714862 Test RE 0.03566961000189562 p: [0.09681482]\n",
      "45 Train Loss 0.00046966545 Test RE 0.03574978157333552 p: [0.09700634]\n",
      "46 Train Loss 0.00046691552 Test RE 0.03575297852245647 p: [0.09697636]\n",
      "47 Train Loss 0.00046411384 Test RE 0.035765097772273324 p: [0.09716205]\n",
      "48 Train Loss 0.00046259724 Test RE 0.03564453257580964 p: [0.09691747]\n",
      "49 Train Loss 0.00046009163 Test RE 0.03571796669227294 p: [0.0969557]\n",
      "50 Train Loss 0.00045919168 Test RE 0.035675048777120445 p: [0.09681997]\n",
      "51 Train Loss 0.00045813268 Test RE 0.0356413935572868 p: [0.09713423]\n",
      "52 Train Loss 0.00045671526 Test RE 0.035586438977688206 p: [0.09704729]\n",
      "53 Train Loss 0.00045575586 Test RE 0.03548696521164882 p: [0.09699771]\n",
      "54 Train Loss 0.00045515836 Test RE 0.03552502863627792 p: [0.09701344]\n",
      "55 Train Loss 0.00045404822 Test RE 0.035517797860137985 p: [0.09687699]\n",
      "56 Train Loss 0.00045194442 Test RE 0.03548724514681525 p: [0.09716501]\n",
      "57 Train Loss 0.00045097817 Test RE 0.03553124601209174 p: [0.09715527]\n",
      "58 Train Loss 0.00045012223 Test RE 0.03551666348228351 p: [0.09723689]\n",
      "59 Train Loss 0.0004495779 Test RE 0.03549413028800208 p: [0.09727808]\n",
      "60 Train Loss 0.00044747855 Test RE 0.035540040044746674 p: [0.09707426]\n",
      "61 Train Loss 0.00044624895 Test RE 0.035461503624515556 p: [0.09687249]\n",
      "62 Train Loss 0.0004452857 Test RE 0.03543159566759218 p: [0.09688856]\n",
      "63 Train Loss 0.0004445832 Test RE 0.035401069644638794 p: [0.09678131]\n",
      "64 Train Loss 0.00044384983 Test RE 0.03541034035557252 p: [0.0969223]\n",
      "65 Train Loss 0.00044347308 Test RE 0.03538570389473377 p: [0.09691685]\n",
      "66 Train Loss 0.00044347308 Test RE 0.03538570389473377 p: [0.09691685]\n",
      "67 Train Loss 0.00044347308 Test RE 0.03538570389473377 p: [0.09691685]\n",
      "68 Train Loss 0.00044347308 Test RE 0.03538570389473377 p: [0.09691685]\n",
      "69 Train Loss 0.00044347308 Test RE 0.03538570389473377 p: [0.09691685]\n",
      "70 Train Loss 0.00044347308 Test RE 0.03538570389473377 p: [0.09691685]\n",
      "71 Train Loss 0.00044347308 Test RE 0.03538570389473377 p: [0.09691685]\n",
      "72 Train Loss 0.00044347308 Test RE 0.03538570389473377 p: [0.09691685]\n",
      "73 Train Loss 0.00044347308 Test RE 0.03538570389473377 p: [0.09691685]\n",
      "74 Train Loss 0.00044347308 Test RE 0.03538570389473377 p: [0.09691685]\n",
      "75 Train Loss 0.00044347308 Test RE 0.03538570389473377 p: [0.09691685]\n",
      "76 Train Loss 0.00044347308 Test RE 0.03538570389473377 p: [0.09691685]\n",
      "77 Train Loss 0.00044347308 Test RE 0.03538570389473377 p: [0.09691685]\n",
      "78 Train Loss 0.00044347308 Test RE 0.03538570389473377 p: [0.09691685]\n",
      "79 Train Loss 0.00044347308 Test RE 0.03538570389473377 p: [0.09691685]\n",
      "80 Train Loss 0.00044347308 Test RE 0.03538570389473377 p: [0.09691685]\n",
      "81 Train Loss 0.00044347308 Test RE 0.03538570389473377 p: [0.09691685]\n",
      "82 Train Loss 0.00044347308 Test RE 0.03538570389473377 p: [0.09691685]\n",
      "83 Train Loss 0.00044347308 Test RE 0.03538570389473377 p: [0.09691685]\n",
      "84 Train Loss 0.00044347308 Test RE 0.03538570389473377 p: [0.09691685]\n",
      "85 Train Loss 0.00044347308 Test RE 0.03538570389473377 p: [0.09691685]\n",
      "86 Train Loss 0.00044347308 Test RE 0.03538570389473377 p: [0.09691685]\n",
      "87 Train Loss 0.00044347308 Test RE 0.03538570389473377 p: [0.09691685]\n",
      "88 Train Loss 0.00044347308 Test RE 0.03538570389473377 p: [0.09691685]\n",
      "89 Train Loss 0.00044347308 Test RE 0.03538570389473377 p: [0.09691685]\n",
      "90 Train Loss 0.00044347308 Test RE 0.03538570389473377 p: [0.09691685]\n",
      "91 Train Loss 0.00044347308 Test RE 0.03538570389473377 p: [0.09691685]\n",
      "92 Train Loss 0.00044347308 Test RE 0.03538570389473377 p: [0.09691685]\n",
      "93 Train Loss 0.00044347308 Test RE 0.03538570389473377 p: [0.09691685]\n",
      "94 Train Loss 0.00044347308 Test RE 0.03538570389473377 p: [0.09691685]\n",
      "95 Train Loss 0.00044347308 Test RE 0.03538570389473377 p: [0.09691685]\n",
      "96 Train Loss 0.00044347308 Test RE 0.03538570389473377 p: [0.09691685]\n",
      "97 Train Loss 0.00044347308 Test RE 0.03538570389473377 p: [0.09691685]\n",
      "98 Train Loss 0.00044347308 Test RE 0.03538570389473377 p: [0.09691685]\n",
      "99 Train Loss 0.00044347308 Test RE 0.03538570389473377 p: [0.09691685]\n",
      "Training time: 27.17\n",
      "Training time: 27.17\n",
      "Burgers_stan_low\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 0.13636541 Test RE 0.6454677204143785 p: [0.7216733]\n",
      "1 Train Loss 0.025946472 Test RE 0.22876447130822006 p: [0.14153521]\n",
      "2 Train Loss 0.016101748 Test RE 0.1962252309667034 p: [0.10164397]\n",
      "3 Train Loss 0.013017793 Test RE 0.16907701837736866 p: [0.13472104]\n",
      "4 Train Loss 0.010611023 Test RE 0.15231232720848684 p: [0.12860082]\n",
      "5 Train Loss 0.008507641 Test RE 0.13362776071970847 p: [0.12043809]\n",
      "6 Train Loss 0.0059451284 Test RE 0.1151326351584227 p: [0.11998234]\n",
      "7 Train Loss 0.0043743774 Test RE 0.08674422191313597 p: [0.10818222]\n",
      "8 Train Loss 0.0032111842 Test RE 0.07377714300434039 p: [0.11173426]\n",
      "9 Train Loss 0.002185993 Test RE 0.05361186287926124 p: [0.10293607]\n",
      "10 Train Loss 0.0015767864 Test RE 0.04958250856861077 p: [0.09919121]\n",
      "11 Train Loss 0.0012179862 Test RE 0.04615417595755406 p: [0.10289013]\n",
      "12 Train Loss 0.001039099 Test RE 0.04410431515076249 p: [0.10396577]\n",
      "13 Train Loss 0.0009261286 Test RE 0.04048110823706995 p: [0.09962364]\n",
      "14 Train Loss 0.0008441794 Test RE 0.040113538168476987 p: [0.09748434]\n",
      "15 Train Loss 0.0007646083 Test RE 0.03778690477578768 p: [0.09850411]\n",
      "16 Train Loss 0.0007101025 Test RE 0.03813220519898413 p: [0.09900866]\n",
      "17 Train Loss 0.00066737295 Test RE 0.03741453415010882 p: [0.09665731]\n",
      "18 Train Loss 0.0006318172 Test RE 0.037655967369726005 p: [0.09762344]\n",
      "19 Train Loss 0.0005969524 Test RE 0.03677076148603581 p: [0.0987341]\n",
      "20 Train Loss 0.0005691603 Test RE 0.03662606432930736 p: [0.09793859]\n",
      "21 Train Loss 0.00055025634 Test RE 0.036739731076510386 p: [0.09733236]\n",
      "22 Train Loss 0.0005356508 Test RE 0.03658200683961329 p: [0.09832276]\n",
      "23 Train Loss 0.0005270534 Test RE 0.03675584176993959 p: [0.0979851]\n",
      "24 Train Loss 0.0005177358 Test RE 0.03663757229254543 p: [0.09721797]\n",
      "25 Train Loss 0.00051183254 Test RE 0.03657062469008709 p: [0.09683432]\n",
      "26 Train Loss 0.00050501624 Test RE 0.036629258894576 p: [0.09704031]\n",
      "27 Train Loss 0.0004999021 Test RE 0.03641984093078095 p: [0.09748908]\n",
      "28 Train Loss 0.0004940148 Test RE 0.036356629296780456 p: [0.09723125]\n",
      "29 Train Loss 0.00049072213 Test RE 0.03639345746133909 p: [0.09660277]\n",
      "30 Train Loss 0.0004862172 Test RE 0.03625716078176903 p: [0.09669132]\n",
      "31 Train Loss 0.00048619835 Test RE 0.0362612334022267 p: [0.09666263]\n",
      "32 Train Loss 0.00048618868 Test RE 0.03625542528438602 p: [0.09663849]\n",
      "33 Train Loss 0.00048242934 Test RE 0.036177646562520635 p: [0.09695428]\n",
      "34 Train Loss 0.0004787234 Test RE 0.036070797738117964 p: [0.09676852]\n",
      "35 Train Loss 0.0004766602 Test RE 0.03603718626629717 p: [0.09697261]\n",
      "36 Train Loss 0.00047322523 Test RE 0.035991332359348344 p: [0.09708769]\n",
      "37 Train Loss 0.0004697378 Test RE 0.03588872677589075 p: [0.09702715]\n",
      "38 Train Loss 0.0004681413 Test RE 0.035808565190009904 p: [0.09741295]\n",
      "39 Train Loss 0.000466927 Test RE 0.03575993728393389 p: [0.09711006]\n",
      "40 Train Loss 0.000465144 Test RE 0.03580796346977821 p: [0.09698037]\n",
      "41 Train Loss 0.0004638424 Test RE 0.03591019971486808 p: [0.09684157]\n",
      "42 Train Loss 0.00046257678 Test RE 0.03593054890526385 p: [0.09688538]\n",
      "43 Train Loss 0.00046101885 Test RE 0.035838353183262506 p: [0.09693724]\n",
      "44 Train Loss 0.00045991552 Test RE 0.03587137413552004 p: [0.09684387]\n",
      "45 Train Loss 0.0004593418 Test RE 0.03582636774246453 p: [0.09681024]\n",
      "46 Train Loss 0.0004582726 Test RE 0.03585954027310189 p: [0.0967568]\n",
      "47 Train Loss 0.00045721862 Test RE 0.03588559385596153 p: [0.09679044]\n",
      "48 Train Loss 0.00045643226 Test RE 0.035848710101504075 p: [0.09675628]\n",
      "49 Train Loss 0.00045590958 Test RE 0.035871230759873814 p: [0.09682633]\n",
      "50 Train Loss 0.00045465553 Test RE 0.03582346616057528 p: [0.09672047]\n",
      "51 Train Loss 0.0004539135 Test RE 0.03582325592422427 p: [0.09673233]\n",
      "52 Train Loss 0.00045388367 Test RE 0.03581592913250066 p: [0.09675787]\n",
      "53 Train Loss 0.00045286788 Test RE 0.03581497462692406 p: [0.09662588]\n",
      "54 Train Loss 0.00045216098 Test RE 0.035815523457247166 p: [0.09669501]\n",
      "55 Train Loss 0.00045213322 Test RE 0.03581373747608508 p: [0.09673091]\n",
      "56 Train Loss 0.00045212527 Test RE 0.035810961543965035 p: [0.09672818]\n",
      "57 Train Loss 0.00045211747 Test RE 0.03581621571064753 p: [0.09672265]\n",
      "58 Train Loss 0.00045211177 Test RE 0.03580329285770609 p: [0.09671742]\n",
      "59 Train Loss 0.00045210338 Test RE 0.03580452035212824 p: [0.09671866]\n",
      "60 Train Loss 0.0004520942 Test RE 0.035807026365638005 p: [0.09671821]\n",
      "61 Train Loss 0.00045207268 Test RE 0.03580206537493601 p: [0.09670693]\n",
      "62 Train Loss 0.00045153225 Test RE 0.03574075830175911 p: [0.09676617]\n",
      "63 Train Loss 0.0004515119 Test RE 0.035756511738084654 p: [0.09675267]\n",
      "64 Train Loss 0.000450939 Test RE 0.035758376939540044 p: [0.09669192]\n",
      "65 Train Loss 0.0004501373 Test RE 0.03574665448122985 p: [0.09687962]\n",
      "66 Train Loss 0.00044956346 Test RE 0.035741712424571255 p: [0.09687836]\n",
      "67 Train Loss 0.00044905496 Test RE 0.03570156940583131 p: [0.09685498]\n",
      "68 Train Loss 0.00044869317 Test RE 0.03569070783908277 p: [0.09677026]\n",
      "69 Train Loss 0.0004476762 Test RE 0.03573134166554622 p: [0.09656439]\n",
      "70 Train Loss 0.0004476675 Test RE 0.03573044610045383 p: [0.09656136]\n",
      "71 Train Loss 0.00044752186 Test RE 0.03570000456218336 p: [0.09663378]\n",
      "72 Train Loss 0.00044751246 Test RE 0.03569386414436252 p: [0.09662551]\n",
      "73 Train Loss 0.00044692846 Test RE 0.03564031736704037 p: [0.0967012]\n",
      "74 Train Loss 0.00044631324 Test RE 0.03568438109619363 p: [0.09653591]\n",
      "75 Train Loss 0.0004452092 Test RE 0.0356147219183812 p: [0.09661953]\n",
      "76 Train Loss 0.00044462868 Test RE 0.03558026680161845 p: [0.09670407]\n",
      "77 Train Loss 0.00044460915 Test RE 0.03557960211602257 p: [0.0966844]\n",
      "78 Train Loss 0.00044460027 Test RE 0.03557462402607408 p: [0.09668246]\n",
      "79 Train Loss 0.00044457964 Test RE 0.03553257432118749 p: [0.09666972]\n",
      "80 Train Loss 0.00044398778 Test RE 0.035530017710181466 p: [0.09674675]\n",
      "81 Train Loss 0.0004433121 Test RE 0.035497533116750066 p: [0.09666569]\n",
      "82 Train Loss 0.0004412587 Test RE 0.035496698902468586 p: [0.09652354]\n",
      "83 Train Loss 0.00044047486 Test RE 0.03547209697665724 p: [0.09657519]\n",
      "84 Train Loss 0.00043983123 Test RE 0.03544942287343556 p: [0.09660818]\n",
      "85 Train Loss 0.0004398094 Test RE 0.03545237643725081 p: [0.09663189]\n",
      "86 Train Loss 0.0004397903 Test RE 0.03545551079153828 p: [0.09662272]\n",
      "87 Train Loss 0.00043978042 Test RE 0.03545892076262054 p: [0.09662796]\n",
      "88 Train Loss 0.00043978042 Test RE 0.03545892076262054 p: [0.09662796]\n",
      "89 Train Loss 0.00043978042 Test RE 0.03545892076262054 p: [0.09662796]\n",
      "90 Train Loss 0.00043978042 Test RE 0.03545892076262054 p: [0.09662796]\n",
      "91 Train Loss 0.00043978042 Test RE 0.03545892076262054 p: [0.09662796]\n",
      "92 Train Loss 0.00043978042 Test RE 0.03545892076262054 p: [0.09662796]\n",
      "93 Train Loss 0.00043978042 Test RE 0.03545892076262054 p: [0.09662796]\n",
      "94 Train Loss 0.00043978042 Test RE 0.03545892076262054 p: [0.09662796]\n",
      "95 Train Loss 0.00043978042 Test RE 0.03545892076262054 p: [0.09662796]\n",
      "96 Train Loss 0.00043978042 Test RE 0.03545892076262054 p: [0.09662796]\n",
      "97 Train Loss 0.00043978042 Test RE 0.03545892076262054 p: [0.09662796]\n",
      "98 Train Loss 0.00043978042 Test RE 0.03545892076262054 p: [0.09662796]\n",
      "99 Train Loss 0.00043978042 Test RE 0.03545892076262054 p: [0.09662796]\n",
      "Training time: 28.37\n",
      "Training time: 28.37\n",
      "Burgers_stan_low\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 0.15368341 Test RE 0.6620027367217222 p: [0.8831998]\n",
      "1 Train Loss 0.034362435 Test RE 0.25874686056075336 p: [0.1626572]\n",
      "2 Train Loss 0.019192243 Test RE 0.18789216422687852 p: [0.11293671]\n",
      "3 Train Loss 0.012367237 Test RE 0.15518613212846197 p: [0.11567064]\n",
      "4 Train Loss 0.009728418 Test RE 0.13817670165577206 p: [0.11716147]\n",
      "5 Train Loss 0.007361441 Test RE 0.11615251174956662 p: [0.12278145]\n",
      "6 Train Loss 0.0053085526 Test RE 0.09847826512885775 p: [0.1191071]\n",
      "7 Train Loss 0.0030234943 Test RE 0.0707899806407636 p: [0.10110974]\n",
      "8 Train Loss 0.0021855128 Test RE 0.05753782478922649 p: [0.0968508]\n",
      "9 Train Loss 0.0014970932 Test RE 0.04731556731690689 p: [0.10174929]\n",
      "10 Train Loss 0.0011533615 Test RE 0.042248079408328705 p: [0.10173233]\n",
      "11 Train Loss 0.0009264051 Test RE 0.03838701673489693 p: [0.09898686]\n",
      "12 Train Loss 0.00077874755 Test RE 0.03854069466287323 p: [0.09848622]\n",
      "13 Train Loss 0.000684702 Test RE 0.03748198474009203 p: [0.09760919]\n",
      "14 Train Loss 0.00063071493 Test RE 0.03686455257814684 p: [0.09767742]\n",
      "15 Train Loss 0.00060175784 Test RE 0.03718688403305741 p: [0.0992138]\n",
      "16 Train Loss 0.0005679193 Test RE 0.03675245590976866 p: [0.09887942]\n",
      "17 Train Loss 0.0005323279 Test RE 0.03640686024648033 p: [0.09701703]\n",
      "18 Train Loss 0.0005097458 Test RE 0.036130847600532985 p: [0.09691343]\n",
      "19 Train Loss 0.00049767084 Test RE 0.03620386963106584 p: [0.09648614]\n",
      "20 Train Loss 0.00048881274 Test RE 0.0360596844257888 p: [0.09723745]\n",
      "21 Train Loss 0.00048243243 Test RE 0.03603864775376654 p: [0.09675386]\n",
      "22 Train Loss 0.00047636076 Test RE 0.03594270336847517 p: [0.09666691]\n",
      "23 Train Loss 0.00047187088 Test RE 0.035807987875055486 p: [0.09698186]\n",
      "24 Train Loss 0.00046716098 Test RE 0.03573801306312974 p: [0.0968865]\n",
      "25 Train Loss 0.00046434067 Test RE 0.03572364295239304 p: [0.09698679]\n",
      "26 Train Loss 0.00046012772 Test RE 0.035732329185528114 p: [0.09670498]\n",
      "27 Train Loss 0.0004585863 Test RE 0.03571719785787592 p: [0.09658387]\n",
      "28 Train Loss 0.00045575114 Test RE 0.03568260055616811 p: [0.09632999]\n",
      "29 Train Loss 0.0004526393 Test RE 0.03562384792077404 p: [0.09638507]\n",
      "30 Train Loss 0.00045062441 Test RE 0.03570721377292226 p: [0.09662069]\n",
      "31 Train Loss 0.00044655218 Test RE 0.03563364355764389 p: [0.09693706]\n",
      "32 Train Loss 0.0004447011 Test RE 0.03563873553456979 p: [0.09661811]\n",
      "33 Train Loss 0.00044398676 Test RE 0.03573358200484592 p: [0.09672702]\n",
      "34 Train Loss 0.00044309822 Test RE 0.03564293402380601 p: [0.09672341]\n",
      "35 Train Loss 0.0004422931 Test RE 0.03565984966055633 p: [0.09673207]\n",
      "36 Train Loss 0.00044186524 Test RE 0.03560176992712101 p: [0.09667187]\n",
      "37 Train Loss 0.0004418578 Test RE 0.0356003899341404 p: [0.0966652]\n",
      "38 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "39 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "40 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "41 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "42 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "43 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "44 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "45 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "46 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "47 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "48 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "49 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "50 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "51 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "52 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "53 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "54 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "55 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "56 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "57 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "58 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "59 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "60 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "61 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "62 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "63 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "64 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "65 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "66 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "67 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "68 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "69 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "70 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "71 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "72 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "73 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "74 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "75 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "76 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "77 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "78 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "79 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "80 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "81 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "82 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "83 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "84 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "85 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "86 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "87 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "88 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "89 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "90 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "91 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "92 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "93 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "94 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "95 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "96 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "97 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "98 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "99 Train Loss 0.00044185336 Test RE 0.035592570925952964 p: [0.09665811]\n",
      "Training time: 15.86\n",
      "Training time: 15.86\n",
      "Burgers_stan_low\n",
      "4\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 0.1489634 Test RE 0.6854985037122557 p: [0.9325926]\n",
      "1 Train Loss 0.03614626 Test RE 0.27512295917532026 p: [0.18622486]\n",
      "2 Train Loss 0.0217929 Test RE 0.21316809406656817 p: [0.12270192]\n",
      "3 Train Loss 0.01603531 Test RE 0.1834777899480941 p: [0.11288133]\n",
      "4 Train Loss 0.013593269 Test RE 0.17773788142528812 p: [0.12146597]\n",
      "5 Train Loss 0.012066116 Test RE 0.1680564518782378 p: [0.14187261]\n",
      "6 Train Loss 0.010671776 Test RE 0.1547064496649346 p: [0.1337738]\n",
      "7 Train Loss 0.009452992 Test RE 0.13904134484230213 p: [0.11831389]\n",
      "8 Train Loss 0.008132763 Test RE 0.1273255229614308 p: [0.11904056]\n",
      "9 Train Loss 0.005977701 Test RE 0.10573008137258505 p: [0.1165644]\n",
      "10 Train Loss 0.0043895873 Test RE 0.08885243764348978 p: [0.11771122]\n",
      "11 Train Loss 0.0035757932 Test RE 0.07861411117342954 p: [0.11554987]\n",
      "12 Train Loss 0.0027601481 Test RE 0.06720237954664784 p: [0.1085762]\n",
      "13 Train Loss 0.0022709249 Test RE 0.05955049173673695 p: [0.10458277]\n",
      "14 Train Loss 0.0018175491 Test RE 0.05588944487164992 p: [0.10287016]\n",
      "15 Train Loss 0.0014548708 Test RE 0.0489512230110642 p: [0.10076856]\n",
      "16 Train Loss 0.0012059554 Test RE 0.04570488793762235 p: [0.10388381]\n",
      "17 Train Loss 0.0010602063 Test RE 0.04329853683933292 p: [0.10148008]\n",
      "18 Train Loss 0.0009110551 Test RE 0.04209036018399959 p: [0.10299158]\n",
      "19 Train Loss 0.0008327542 Test RE 0.041041736479464616 p: [0.10299379]\n",
      "20 Train Loss 0.0007534032 Test RE 0.03970503776778459 p: [0.10122379]\n",
      "21 Train Loss 0.0007049368 Test RE 0.03912527274406431 p: [0.09961415]\n",
      "22 Train Loss 0.00067477603 Test RE 0.03837594842043 p: [0.09915448]\n",
      "23 Train Loss 0.00062850997 Test RE 0.03755830553988688 p: [0.09899596]\n",
      "24 Train Loss 0.00060217216 Test RE 0.03716094321927767 p: [0.09851055]\n",
      "25 Train Loss 0.00057755515 Test RE 0.03699782660773172 p: [0.09626859]\n",
      "26 Train Loss 0.000562545 Test RE 0.036850631745232294 p: [0.09700873]\n",
      "27 Train Loss 0.00055349903 Test RE 0.03682693003502082 p: [0.09814787]\n",
      "28 Train Loss 0.00054361013 Test RE 0.03652226376181088 p: [0.09798077]\n",
      "29 Train Loss 0.0005257855 Test RE 0.03644568870869694 p: [0.09748663]\n",
      "30 Train Loss 0.0005166009 Test RE 0.036587218371367775 p: [0.09679536]\n",
      "31 Train Loss 0.0005095299 Test RE 0.036429991650824264 p: [0.09624171]\n",
      "32 Train Loss 0.0004993043 Test RE 0.03620884404313035 p: [0.09671305]\n",
      "33 Train Loss 0.0004952351 Test RE 0.036287785123588566 p: [0.09661702]\n",
      "34 Train Loss 0.0004826997 Test RE 0.03618792131739581 p: [0.09655996]\n",
      "35 Train Loss 0.00047787197 Test RE 0.035819832083419566 p: [0.09661523]\n",
      "36 Train Loss 0.00047140665 Test RE 0.03591075585841746 p: [0.09673747]\n",
      "37 Train Loss 0.00046973434 Test RE 0.03585374873426482 p: [0.09664541]\n",
      "38 Train Loss 0.00046638568 Test RE 0.035823263141923696 p: [0.09672598]\n",
      "39 Train Loss 0.0004634322 Test RE 0.03587241890699145 p: [0.09676216]\n",
      "40 Train Loss 0.0004620665 Test RE 0.03582951921795874 p: [0.09670837]\n",
      "41 Train Loss 0.0004595465 Test RE 0.03584818806803943 p: [0.09645265]\n",
      "42 Train Loss 0.0004588368 Test RE 0.03579703033968271 p: [0.09633747]\n",
      "43 Train Loss 0.0004565714 Test RE 0.035773375565122716 p: [0.09630805]\n",
      "44 Train Loss 0.0004553297 Test RE 0.035782349538703705 p: [0.09620302]\n",
      "45 Train Loss 0.00045435745 Test RE 0.03579370248847955 p: [0.09621485]\n",
      "46 Train Loss 0.00045396917 Test RE 0.03576445051893244 p: [0.09628934]\n",
      "47 Train Loss 0.00045396917 Test RE 0.03576445051893244 p: [0.09628934]\n",
      "48 Train Loss 0.00045396917 Test RE 0.03576445051893244 p: [0.09628934]\n",
      "49 Train Loss 0.00045396917 Test RE 0.03576445051893244 p: [0.09628934]\n",
      "50 Train Loss 0.00045396917 Test RE 0.03576445051893244 p: [0.09628934]\n",
      "51 Train Loss 0.00045396917 Test RE 0.03576445051893244 p: [0.09628934]\n",
      "52 Train Loss 0.00045396917 Test RE 0.03576445051893244 p: [0.09628934]\n",
      "53 Train Loss 0.00045396917 Test RE 0.03576445051893244 p: [0.09628934]\n",
      "54 Train Loss 0.00045396917 Test RE 0.03576445051893244 p: [0.09628934]\n",
      "55 Train Loss 0.00045396917 Test RE 0.03576445051893244 p: [0.09628934]\n",
      "56 Train Loss 0.00045396917 Test RE 0.03576445051893244 p: [0.09628934]\n",
      "57 Train Loss 0.00045396917 Test RE 0.03576445051893244 p: [0.09628934]\n",
      "58 Train Loss 0.00045396917 Test RE 0.03576445051893244 p: [0.09628934]\n",
      "59 Train Loss 0.00045396917 Test RE 0.03576445051893244 p: [0.09628934]\n",
      "60 Train Loss 0.00045396917 Test RE 0.03576445051893244 p: [0.09628934]\n",
      "61 Train Loss 0.00045396917 Test RE 0.03576445051893244 p: [0.09628934]\n",
      "62 Train Loss 0.00045396917 Test RE 0.03576445051893244 p: [0.09628934]\n",
      "63 Train Loss 0.00045396917 Test RE 0.03576445051893244 p: [0.09628934]\n",
      "64 Train Loss 0.00045396917 Test RE 0.03576445051893244 p: [0.09628934]\n",
      "65 Train Loss 0.00045396917 Test RE 0.03576445051893244 p: [0.09628934]\n",
      "66 Train Loss 0.00045396917 Test RE 0.03576445051893244 p: [0.09628934]\n",
      "67 Train Loss 0.00045396917 Test RE 0.03576445051893244 p: [0.09628934]\n",
      "68 Train Loss 0.00045396917 Test RE 0.03576445051893244 p: [0.09628934]\n",
      "69 Train Loss 0.00045396917 Test RE 0.03576445051893244 p: [0.09628934]\n",
      "70 Train Loss 0.00045396917 Test RE 0.03576445051893244 p: [0.09628934]\n",
      "71 Train Loss 0.00045396917 Test RE 0.03576445051893244 p: [0.09628934]\n",
      "72 Train Loss 0.00045396917 Test RE 0.03576445051893244 p: [0.09628934]\n",
      "73 Train Loss 0.00045396917 Test RE 0.03576445051893244 p: [0.09628934]\n",
      "74 Train Loss 0.00045396917 Test RE 0.03576445051893244 p: [0.09628934]\n",
      "75 Train Loss 0.00045396917 Test RE 0.03576445051893244 p: [0.09628934]\n",
      "76 Train Loss 0.00045396917 Test RE 0.03576445051893244 p: [0.09628934]\n",
      "77 Train Loss 0.00045396917 Test RE 0.03576445051893244 p: [0.09628934]\n",
      "78 Train Loss 0.00045396917 Test RE 0.03576445051893244 p: [0.09628934]\n",
      "79 Train Loss 0.00045396917 Test RE 0.03576445051893244 p: [0.09628934]\n",
      "80 Train Loss 0.00045396917 Test RE 0.03576445051893244 p: [0.09628934]\n",
      "81 Train Loss 0.00045396917 Test RE 0.03576445051893244 p: [0.09628934]\n",
      "82 Train Loss 0.00045396917 Test RE 0.03576445051893244 p: [0.09628934]\n",
      "83 Train Loss 0.00045396917 Test RE 0.03576445051893244 p: [0.09628934]\n",
      "84 Train Loss 0.00045396917 Test RE 0.03576445051893244 p: [0.09628934]\n",
      "85 Train Loss 0.00045396917 Test RE 0.03576445051893244 p: [0.09628934]\n",
      "86 Train Loss 0.00045396917 Test RE 0.03576445051893244 p: [0.09628934]\n",
      "87 Train Loss 0.00045396917 Test RE 0.03576445051893244 p: [0.09628934]\n",
      "88 Train Loss 0.00045396917 Test RE 0.03576445051893244 p: [0.09628934]\n",
      "89 Train Loss 0.00045396917 Test RE 0.03576445051893244 p: [0.09628934]\n",
      "90 Train Loss 0.00045396917 Test RE 0.03576445051893244 p: [0.09628934]\n",
      "91 Train Loss 0.00045396917 Test RE 0.03576445051893244 p: [0.09628934]\n",
      "92 Train Loss 0.00045396917 Test RE 0.03576445051893244 p: [0.09628934]\n",
      "93 Train Loss 0.00045396917 Test RE 0.03576445051893244 p: [0.09628934]\n",
      "94 Train Loss 0.00045396917 Test RE 0.03576445051893244 p: [0.09628934]\n",
      "95 Train Loss 0.00045396917 Test RE 0.03576445051893244 p: [0.09628934]\n",
      "96 Train Loss 0.00045396917 Test RE 0.03576445051893244 p: [0.09628934]\n",
      "97 Train Loss 0.00045396917 Test RE 0.03576445051893244 p: [0.09628934]\n",
      "98 Train Loss 0.00045396917 Test RE 0.03576445051893244 p: [0.09628934]\n",
      "99 Train Loss 0.00045396917 Test RE 0.03576445051893244 p: [0.09628934]\n",
      "Training time: 19.55\n",
      "Training time: 19.55\n"
     ]
    }
   ],
   "source": [
    "max_reps = 5\n",
    "max_iter = 100 #200\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "\n",
    "\n",
    "N_T = 500 #Total number of data points for 'y'\n",
    "N_f = 10000 #Total number of collocation points \n",
    "\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(label)\n",
    "    print(reps)\n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []\n",
    " \n",
    "    \n",
    "    torch.manual_seed(reps*36)\n",
    "    \n",
    "    layers = np.array([2,50,50,50,50,1]) #9 hidden layers\n",
    "\n",
    "    PINN = Sequentialmodel(layers)\n",
    "   \n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "    \n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=1, \n",
    "                              max_iter = 20, \n",
    "                              max_eval = 30, \n",
    "                              tolerance_grad = 1e-08, \n",
    "                              tolerance_change = 1e-08, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "    \n",
    "    nan_flag = train_model(max_iter,reps)\n",
    "    \n",
    "    \n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    #elapsed_time[reps] = time.time() - start_time\n",
    "\n",
    "\n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "    \n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-2,2,500).reshape(-1,1)\n",
    "y = np.linspace(0,1,500).reshape(-1,1)\n",
    "\n",
    "X,Y = np.meshgrid(x,y)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "Y = Y.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xy = np.hstack((X,Y))\n",
    "\n",
    "xy_test_tensor = torch.from_numpy(xy).float().to(device)\n",
    "\n",
    "u_pred = PINN.forward(xy_test_tensor).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "cmap = plt.cm.jet\n",
    "\n",
    "img3 = ax.imshow(np.flip(u_pred.reshape(500,500),axis = 0),cmap = cmap,extent=[0,1,0,1],aspect = 0.75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_loss_full' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2390/1704525943.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtest_loss_full\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_loss_full' is not defined"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + test_loss_full[i][-1]\n",
    "print(a/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + train_loss_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
