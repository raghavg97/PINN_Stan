{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def true_2D_1(xt): #True function for 2D_1 Klein Gordon Equation x \\in [-50,50] , t \\in [0,10]\n",
    "#     y = xt[:,0]*np.cos(xt[:,1])\n",
    "#     return y.reshape(-1,1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_thresh = 25000\n",
    "level = \"_low\"\n",
    "label = \"Burgers_stan\" + level\n",
    "\n",
    "x = np.linspace(-1,1,256).reshape(-1,1)\n",
    "t = np.linspace(0,1,100).reshape(-1,1)\n",
    "\n",
    "X,T = np.meshgrid(x,t)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "T = T.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xt = np.hstack((X,T))\n",
    "\n",
    "# bound_pts_1 = (X == 0).reshape(-1,)\n",
    "# bound_pts_2 = np.logical_and(Y == 0,X != 0).reshape(-1,)\n",
    "# bound_pts_3 = np.logical_and(X == 1,Y != 0).reshape(-1,) \n",
    "# bound_pts_4 = np.logical_and(Y == 1,X != 1).reshape(-1,) \n",
    "\n",
    "# xy_bound_1 = xy[bound_pts_1,:]\n",
    "# xy_bound_2 = xy[bound_pts_2,:]\n",
    "# xy_bound_3 = xy[bound_pts_3,:]\n",
    "# xy_bound_4 = xy[bound_pts_4,:]\n",
    "\n",
    "# u_bound_1 = 1000*np.ones((np.shape(xy_bound_1)[0],1))\n",
    "# u_bound_2 = 800*np.ones((np.shape(xy_bound_2)[0],1))\n",
    "# u_bound_3 = 500*np.ones((np.shape(xy_bound_3)[0],1))\n",
    "# u_bound_4 = np.zeros((np.shape(xy_bound_4)[0],1))\n",
    "\n",
    "# xy_bound = np.vstack((xy_bound_1,xy_bound_2,xy_bound_3,xy_bound_4))\n",
    "# u_bound = np.vstack((u_bound_1,u_bound_2,u_bound_3,u_bound_4))\n",
    "\n",
    "# xy_test_tensor = torch.from_numpy(xy).float().to(device)\n",
    "\n",
    "lb_xt = xt[0]\n",
    "ub_xt = xt[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat('../burgers_shock_3.mat') \n",
    "\n",
    "pi = torch.from_numpy(np.array(np.pi)).double().to(device)\n",
    "\n",
    "x = np.array(data['x'])\n",
    "t = np.array(data['t'])\n",
    "\n",
    "X,T = np.meshgrid(x,t)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "T = T.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xt = np.hstack((X,T))\n",
    "\n",
    "u_true = np.array(data['usol'][:])\n",
    "\n",
    "xt_test_tensor = torch.from_numpy(xt).float().to(device)\n",
    "u_true_norm = np.linalg.norm(u_true,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_T,N_f,seed):\n",
    "    '''Boundary Conditions''' \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    x_l = x[0]*np.ones((N_T,1))\n",
    "    t_l = np.random.uniform(t[0],t[-1],(N_T,1))\n",
    "    xt_l = np.hstack((x_l,t_l))\n",
    "    u_l = 0.0*np.ones((N_T,1))\n",
    "    \n",
    "    x_r = x[-1]*np.ones((N_T,1))\n",
    "    t_r = np.random.uniform(t[0],t[-1],(N_T,1))\n",
    "    xt_r = np.hstack((x_r,t_r))\n",
    "    u_r = 0.0*np.ones((N_T,1))\n",
    "    \n",
    "    x_0 = np.random.uniform(x[0],x[-1],(N_T,1))\n",
    "    t_0 = t[0]*np.ones((N_T,1))\n",
    "    xt_0 = np.hstack((x_0,t_0))\n",
    "    u_0 = -1.0*np.sin(np.pi*x_0)\n",
    "    \n",
    "    xt_BC = np.vstack((xt_l,xt_r,xt_0,xt)) #choose indices from  set 'idx' (x,t)\n",
    "    u_BC = np.vstack((u_l,u_r,u_0,u_true.reshape(-1,1)))\n",
    "    \n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    samples = sampling(N_f)\n",
    "    \n",
    "    xt_coll = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    \n",
    "    xt_coll = np.vstack((xt_coll, xt_BC)) # append training points to collocation points \n",
    "\n",
    "    return xt_coll, xt_BC, u_BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        self.iter = 0\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)   \n",
    "        \n",
    "        \n",
    "        self.p = Parameter(torch.ones(1))\n",
    "        self.p.requiresGrad = True\n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xy):\n",
    "        if torch.is_tensor(xy) != True:         \n",
    "            xy = torch.from_numpy(xy)                \n",
    "        \n",
    "        ubxy = torch.from_numpy(ub_xt).float().to(device)\n",
    "        lbxy = torch.from_numpy(lb_xt).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xy = 2.0*(xy - lbxy)/(ubxy - lbxy) - 1.0\n",
    "        \n",
    "        #convert to float\n",
    "        a = xy.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a =self.activation(z)\n",
    "            \n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC(self,xy,u):\n",
    "                \n",
    "        loss_bc = self.loss_function(self.forward(xy), u)\n",
    "                \n",
    "        return loss_bc\n",
    "    \n",
    "    def loss_PDE(self, xy_coll, f_hat):\n",
    "        \n",
    "        g = xy_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_y = autograd.grad(u,g,torch.ones([xy_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_xx_yy = autograd.grad(u_x_y,g,torch.ones(xy_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        du_dx = u_x_y[:,[0]]\n",
    "        du_dt = u_x_y[:,[1]]\n",
    "        \n",
    "        d2u_dx2 = u_xx_yy[:,[0]]\n",
    "        \n",
    "\n",
    "        f = du_dt + u*du_dx - self.p*d2u_dx2/pi\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xy_BC,u_BC,xy_coll,f_hat):\n",
    "\n",
    "        loss_BC = self.loss_BC(xy_BC,u_BC)\n",
    "        loss_f = self.loss_PDE(xy_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_BC + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "         \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        u_pred = self.forward(xt_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "        \n",
    "        \n",
    "        return test_mse, test_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(xy_BC,u_BC,xy_coll,f_hat,seed):\n",
    "\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(xy_BC,u_BC,xy_coll,f_hat)\n",
    "        loss.backward()\n",
    "        #print(loss.cpu().detach().numpy())\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*9)\n",
    "    start_time = time.time() \n",
    "    thresh_flag = 0\n",
    "\n",
    "    xy_coll_np_array, xy_BC_np_array, u_BC_np_array = trainingdata(N_T,N_f,rep*22)\n",
    "        \n",
    "    xy_coll = torch.from_numpy(xy_coll_np_array).float().to(device)\n",
    "    xy_BC = torch.from_numpy(xy_BC_np_array).float().to(device)\n",
    "    u_BC = torch.from_numpy(u_BC_np_array).float().to(device)\n",
    "        \n",
    "    f_hat = torch.zeros(xy_coll.shape[0],1).to(device)\n",
    "    \n",
    "\n",
    "    for i in range(max_iter):\n",
    "        train_step(xy_BC,u_BC,xy_coll,f_hat,i)\n",
    "        loss_np = PINN.loss(xy_BC,u_BC,xy_coll,f_hat).cpu().detach().numpy()\n",
    "        \n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1          \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        \n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test RE\",test_re_loss[-1],\"p:\",PINN.p.cpu().detach().numpy())   \n",
    "        \n",
    "    elapsed_time[rep] = time.time() - start_time  \n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Burgers_stan_low\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 0.09995562 Test RE 0.48192801117700157 p: [0.21314022]\n",
      "1 Train Loss 0.05385707 Test RE 0.3393800959651805 p: [0.08219989]\n",
      "2 Train Loss 0.03493817 Test RE 0.2586578547507704 p: [0.06096001]\n",
      "3 Train Loss 0.028443256 Test RE 0.23277101634990327 p: [0.05428765]\n",
      "4 Train Loss 0.024162691 Test RE 0.21310648808709923 p: [0.04801868]\n",
      "5 Train Loss 0.021455854 Test RE 0.20444621267144694 p: [0.04375328]\n",
      "6 Train Loss 0.019642089 Test RE 0.19358250172729147 p: [0.04015851]\n",
      "7 Train Loss 0.017770056 Test RE 0.18300366260954146 p: [0.03925996]\n",
      "8 Train Loss 0.016539903 Test RE 0.1742894741305266 p: [0.03872482]\n",
      "9 Train Loss 0.015303884 Test RE 0.17400387440560497 p: [0.04244541]\n",
      "10 Train Loss 0.0140976785 Test RE 0.167853538413061 p: [0.04214666]\n",
      "11 Train Loss 0.0126437675 Test RE 0.15640304782956352 p: [0.03898537]\n",
      "12 Train Loss 0.011363365 Test RE 0.15356430675646943 p: [0.03761887]\n",
      "13 Train Loss 0.0100774765 Test RE 0.14513906979491523 p: [0.0380548]\n",
      "14 Train Loss 0.00894615 Test RE 0.13694587957185297 p: [0.03855713]\n",
      "15 Train Loss 0.00808612 Test RE 0.1302207782065877 p: [0.03879654]\n",
      "16 Train Loss 0.006998867 Test RE 0.11827600906398611 p: [0.0360948]\n",
      "17 Train Loss 0.0063112453 Test RE 0.11446005703561225 p: [0.03658057]\n",
      "18 Train Loss 0.0056929095 Test RE 0.1076118871165814 p: [0.03531978]\n",
      "19 Train Loss 0.0049716355 Test RE 0.09824611912191339 p: [0.03469367]\n",
      "20 Train Loss 0.004229647 Test RE 0.09168101837506053 p: [0.03178407]\n",
      "21 Train Loss 0.0037557562 Test RE 0.08654932269358609 p: [0.03013741]\n",
      "22 Train Loss 0.0034825425 Test RE 0.08044129104898987 p: [0.02773743]\n",
      "23 Train Loss 0.0033542626 Test RE 0.07945608974482667 p: [0.02705689]\n",
      "24 Train Loss 0.0030655265 Test RE 0.07548977514578063 p: [0.0263369]\n",
      "25 Train Loss 0.0029328899 Test RE 0.07378108067246344 p: [0.02546128]\n",
      "26 Train Loss 0.0027179169 Test RE 0.07255877289047566 p: [0.02576658]\n",
      "27 Train Loss 0.00257233 Test RE 0.06783351197152834 p: [0.02458794]\n",
      "28 Train Loss 0.002352532 Test RE 0.06472239681055146 p: [0.02396718]\n",
      "29 Train Loss 0.0022932382 Test RE 0.06438255516522463 p: [0.02390817]\n",
      "30 Train Loss 0.0021677269 Test RE 0.06236789777276835 p: [0.0231135]\n",
      "31 Train Loss 0.0019368058 Test RE 0.05732855030189538 p: [0.02186552]\n",
      "32 Train Loss 0.0018211398 Test RE 0.05483298023258718 p: [0.02122298]\n",
      "33 Train Loss 0.0017046304 Test RE 0.05343779332570569 p: [0.0206924]\n",
      "34 Train Loss 0.0015382951 Test RE 0.05197387255969896 p: [0.01982336]\n",
      "35 Train Loss 0.0014679313 Test RE 0.049847725822168 p: [0.01902776]\n",
      "36 Train Loss 0.0013831737 Test RE 0.04914651305764677 p: [0.018842]\n",
      "37 Train Loss 0.0013283959 Test RE 0.04772285645342366 p: [0.0184316]\n",
      "38 Train Loss 0.0012643232 Test RE 0.04478084811981374 p: [0.01744577]\n",
      "39 Train Loss 0.00120838 Test RE 0.045022757344162 p: [0.01773756]\n",
      "40 Train Loss 0.0011575797 Test RE 0.04377100638492297 p: [0.01746176]\n",
      "41 Train Loss 0.0011293249 Test RE 0.04350078016739159 p: [0.0175898]\n",
      "42 Train Loss 0.0010739244 Test RE 0.042741971241287716 p: [0.01742228]\n",
      "43 Train Loss 0.0010478994 Test RE 0.04179332177320969 p: [0.01717564]\n",
      "44 Train Loss 0.0010299645 Test RE 0.04128497508960723 p: [0.01696079]\n",
      "45 Train Loss 0.0009877691 Test RE 0.04006456079856267 p: [0.01684609]\n",
      "46 Train Loss 0.0009353199 Test RE 0.038183796476287904 p: [0.01649239]\n",
      "47 Train Loss 0.0009182249 Test RE 0.03706717399505213 p: [0.01612646]\n",
      "48 Train Loss 0.0008871079 Test RE 0.036539684534425285 p: [0.01587312]\n",
      "49 Train Loss 0.00085011614 Test RE 0.03510893953670395 p: [0.01549347]\n",
      "50 Train Loss 0.0008337942 Test RE 0.03486372943677541 p: [0.01530219]\n",
      "51 Train Loss 0.00081838266 Test RE 0.03518674463411639 p: [0.01555745]\n",
      "52 Train Loss 0.00078593433 Test RE 0.0342620640594445 p: [0.01547404]\n",
      "53 Train Loss 0.00077684387 Test RE 0.034258339966878805 p: [0.01541483]\n",
      "54 Train Loss 0.00075806293 Test RE 0.033847089334988666 p: [0.01532297]\n",
      "55 Train Loss 0.0007440069 Test RE 0.033732197808771014 p: [0.0152832]\n",
      "56 Train Loss 0.0007258611 Test RE 0.032248757909446334 p: [0.01489218]\n",
      "57 Train Loss 0.00070605346 Test RE 0.031472196239181655 p: [0.01463813]\n",
      "58 Train Loss 0.0006873398 Test RE 0.03128937375384834 p: [0.01450311]\n",
      "59 Train Loss 0.0006716038 Test RE 0.03160576328587655 p: [0.0145874]\n",
      "60 Train Loss 0.000659224 Test RE 0.031242313575244464 p: [0.01444295]\n",
      "61 Train Loss 0.0006338577 Test RE 0.030064915250096972 p: [0.01423851]\n",
      "62 Train Loss 0.00062121206 Test RE 0.030488240285491255 p: [0.01427973]\n",
      "63 Train Loss 0.00061534974 Test RE 0.030508095152564925 p: [0.01430595]\n",
      "64 Train Loss 0.0005960793 Test RE 0.02934838451134412 p: [0.0140758]\n",
      "65 Train Loss 0.00058900344 Test RE 0.028997896670405313 p: [0.01396945]\n",
      "66 Train Loss 0.0005777379 Test RE 0.02835096371927663 p: [0.0137548]\n",
      "67 Train Loss 0.0005603684 Test RE 0.027749819340128195 p: [0.0135766]\n",
      "68 Train Loss 0.00054848497 Test RE 0.026911979664899464 p: [0.01335711]\n",
      "69 Train Loss 0.0005377298 Test RE 0.02643407117261656 p: [0.01318702]\n",
      "70 Train Loss 0.00052127504 Test RE 0.026178359825173546 p: [0.01314342]\n",
      "71 Train Loss 0.0005085728 Test RE 0.026050347388422466 p: [0.01300141]\n",
      "72 Train Loss 0.0005008366 Test RE 0.025545349444144497 p: [0.01286247]\n",
      "73 Train Loss 0.0004909007 Test RE 0.02542074766350432 p: [0.01280685]\n",
      "74 Train Loss 0.00047064794 Test RE 0.024421797572615665 p: [0.01238145]\n",
      "75 Train Loss 0.0004588502 Test RE 0.024164998848465673 p: [0.01235371]\n",
      "76 Train Loss 0.00045078382 Test RE 0.02393207291001611 p: [0.01232513]\n",
      "77 Train Loss 0.0004443539 Test RE 0.023735871952278654 p: [0.01223905]\n",
      "78 Train Loss 0.00043231112 Test RE 0.02402044202947347 p: [0.01228695]\n",
      "79 Train Loss 0.00041828264 Test RE 0.02344867275935516 p: [0.01220231]\n",
      "80 Train Loss 0.00040864633 Test RE 0.023178179840602203 p: [0.01219511]\n",
      "81 Train Loss 0.00039688637 Test RE 0.022941336345714474 p: [0.01211835]\n",
      "82 Train Loss 0.0003806401 Test RE 0.02304335174351283 p: [0.01215057]\n",
      "83 Train Loss 0.00036777905 Test RE 0.022567232792032547 p: [0.01205459]\n",
      "84 Train Loss 0.00036079943 Test RE 0.022455632408914464 p: [0.01208179]\n",
      "85 Train Loss 0.0003548151 Test RE 0.022385317950240294 p: [0.01206148]\n",
      "86 Train Loss 0.00035125762 Test RE 0.02224343448754377 p: [0.01205932]\n",
      "87 Train Loss 0.00034467678 Test RE 0.022075107311072827 p: [0.0120205]\n",
      "88 Train Loss 0.00033700978 Test RE 0.021765172083736888 p: [0.01198894]\n",
      "89 Train Loss 0.00032750313 Test RE 0.021641059388663095 p: [0.0119603]\n",
      "90 Train Loss 0.00032299774 Test RE 0.021520743899384676 p: [0.01193807]\n",
      "91 Train Loss 0.00032032613 Test RE 0.021373370754895038 p: [0.01189316]\n",
      "92 Train Loss 0.00031568302 Test RE 0.021247291756486566 p: [0.01189145]\n",
      "93 Train Loss 0.00030694826 Test RE 0.020659871187452314 p: [0.01180855]\n",
      "94 Train Loss 0.0003027842 Test RE 0.02004757541858692 p: [0.01175222]\n",
      "95 Train Loss 0.000299137 Test RE 0.019838461141972023 p: [0.01174196]\n",
      "96 Train Loss 0.0002951427 Test RE 0.019466735972268147 p: [0.01169395]\n",
      "97 Train Loss 0.00029091438 Test RE 0.019613698878067683 p: [0.01180214]\n",
      "98 Train Loss 0.00028647415 Test RE 0.019455192009719117 p: [0.01176606]\n",
      "99 Train Loss 0.00028054765 Test RE 0.019343879052081042 p: [0.01169774]\n",
      "100 Train Loss 0.00027687912 Test RE 0.01896347013991194 p: [0.01163224]\n",
      "101 Train Loss 0.00027311555 Test RE 0.018873205242261885 p: [0.01164614]\n",
      "102 Train Loss 0.0002700795 Test RE 0.01860846479215019 p: [0.01161002]\n",
      "103 Train Loss 0.00026578375 Test RE 0.018502207508518116 p: [0.01164689]\n",
      "104 Train Loss 0.0002619364 Test RE 0.01848382569724625 p: [0.01159936]\n",
      "105 Train Loss 0.00025762088 Test RE 0.01829032390759141 p: [0.01155775]\n",
      "106 Train Loss 0.00025404402 Test RE 0.01836030413931068 p: [0.01157491]\n",
      "107 Train Loss 0.00024794714 Test RE 0.018068858095122067 p: [0.01146572]\n",
      "108 Train Loss 0.00024423026 Test RE 0.01776447833753405 p: [0.01140296]\n",
      "109 Train Loss 0.0002419715 Test RE 0.017723090149936036 p: [0.01141442]\n",
      "110 Train Loss 0.00023947944 Test RE 0.017447297756653393 p: [0.01137697]\n",
      "111 Train Loss 0.0002367727 Test RE 0.017319312164143844 p: [0.01134544]\n",
      "112 Train Loss 0.00023154527 Test RE 0.016723236490649433 p: [0.01133453]\n",
      "113 Train Loss 0.0002261977 Test RE 0.01671711932130026 p: [0.01131138]\n",
      "114 Train Loss 0.00022336055 Test RE 0.01658375976591429 p: [0.01130665]\n",
      "115 Train Loss 0.00022151944 Test RE 0.01659706034534811 p: [0.01128552]\n",
      "116 Train Loss 0.0002191464 Test RE 0.016559148672882613 p: [0.01128686]\n",
      "117 Train Loss 0.00021713201 Test RE 0.016427626306965756 p: [0.01127317]\n",
      "118 Train Loss 0.0002154425 Test RE 0.01647753908168559 p: [0.01129628]\n",
      "119 Train Loss 0.00021434383 Test RE 0.016389831436459855 p: [0.01126615]\n",
      "120 Train Loss 0.00021421124 Test RE 0.0163744402792257 p: [0.01126833]\n",
      "121 Train Loss 0.00021155865 Test RE 0.016319411315114607 p: [0.0112501]\n",
      "122 Train Loss 0.00021143875 Test RE 0.0162691435215965 p: [0.01125006]\n",
      "123 Train Loss 0.00020933908 Test RE 0.016112846975680224 p: [0.01123529]\n",
      "124 Train Loss 0.00020720236 Test RE 0.015908484933507418 p: [0.01116897]\n",
      "125 Train Loss 0.00020539548 Test RE 0.01582395198077767 p: [0.01110393]\n",
      "126 Train Loss 0.00020372766 Test RE 0.015992084009390426 p: [0.011093]\n",
      "127 Train Loss 0.00020363655 Test RE 0.015990357137449725 p: [0.01109235]\n",
      "128 Train Loss 0.00020260917 Test RE 0.01590113935307972 p: [0.01107722]\n",
      "129 Train Loss 0.00020125124 Test RE 0.01581979742210116 p: [0.01105994]\n",
      "130 Train Loss 0.00019940849 Test RE 0.015761221357707373 p: [0.01104677]\n",
      "131 Train Loss 0.00019724955 Test RE 0.015672612730762977 p: [0.01104795]\n",
      "132 Train Loss 0.00019615772 Test RE 0.01554452519838255 p: [0.01103605]\n",
      "133 Train Loss 0.00019439516 Test RE 0.015392074146814289 p: [0.01101585]\n",
      "134 Train Loss 0.0001924636 Test RE 0.01540919588109505 p: [0.0109949]\n",
      "135 Train Loss 0.0001904401 Test RE 0.015410544783746668 p: [0.01100029]\n",
      "136 Train Loss 0.00018960345 Test RE 0.015372224958388999 p: [0.01100596]\n",
      "137 Train Loss 0.00018942163 Test RE 0.015357543640978927 p: [0.01101078]\n",
      "138 Train Loss 0.00018920071 Test RE 0.015333188875975061 p: [0.01100619]\n",
      "139 Train Loss 0.00018849205 Test RE 0.015236449715097194 p: [0.0109874]\n",
      "140 Train Loss 0.00018685154 Test RE 0.01506099862496612 p: [0.01094668]\n",
      "141 Train Loss 0.0001840097 Test RE 0.014811556117298478 p: [0.01093131]\n",
      "142 Train Loss 0.00018371565 Test RE 0.014844503066118652 p: [0.01093041]\n",
      "143 Train Loss 0.00018234216 Test RE 0.014905082001954029 p: [0.01095261]\n",
      "144 Train Loss 0.00018172711 Test RE 0.01493639222623066 p: [0.01095269]\n",
      "145 Train Loss 0.00018042483 Test RE 0.015029231011487947 p: [0.01094732]\n",
      "146 Train Loss 0.00017956665 Test RE 0.014924015071756967 p: [0.01091999]\n",
      "147 Train Loss 0.00017853355 Test RE 0.014957119411245584 p: [0.01091917]\n",
      "148 Train Loss 0.00017745655 Test RE 0.01488893667964781 p: [0.01090963]\n",
      "149 Train Loss 0.00017497232 Test RE 0.014622629949392448 p: [0.0108731]\n",
      "150 Train Loss 0.00017390231 Test RE 0.014588031855878933 p: [0.01085086]\n",
      "151 Train Loss 0.00017266818 Test RE 0.014465433192307574 p: [0.0108266]\n",
      "152 Train Loss 0.00017130373 Test RE 0.014437642973572015 p: [0.01081584]\n",
      "153 Train Loss 0.00017103468 Test RE 0.014415407054835539 p: [0.01081151]\n",
      "154 Train Loss 0.0001696544 Test RE 0.014354780162014264 p: [0.01082546]\n",
      "155 Train Loss 0.00016699132 Test RE 0.014146484603040544 p: [0.01077589]\n",
      "156 Train Loss 0.00016548819 Test RE 0.014108331167287729 p: [0.01078858]\n",
      "157 Train Loss 0.00016417552 Test RE 0.014152231362238251 p: [0.01080194]\n",
      "158 Train Loss 0.0001638484 Test RE 0.014153836135568542 p: [0.01079339]\n",
      "159 Train Loss 0.0001636745 Test RE 0.014119359011543941 p: [0.01078993]\n",
      "160 Train Loss 0.00016338713 Test RE 0.014129445354448078 p: [0.0107964]\n",
      "161 Train Loss 0.00016239895 Test RE 0.014249367406543631 p: [0.01079801]\n",
      "162 Train Loss 0.00016101063 Test RE 0.014217734793120905 p: [0.01077903]\n",
      "163 Train Loss 0.00015926996 Test RE 0.014231324735399502 p: [0.01078409]\n",
      "164 Train Loss 0.00015755609 Test RE 0.014291223423668826 p: [0.01083349]\n",
      "165 Train Loss 0.00015643178 Test RE 0.014267132444274862 p: [0.01084435]\n",
      "166 Train Loss 0.00015542179 Test RE 0.014270674980693267 p: [0.01086301]\n",
      "167 Train Loss 0.00015535115 Test RE 0.014286766784611871 p: [0.01086271]\n",
      "168 Train Loss 0.00015491975 Test RE 0.01428711662159961 p: [0.01085973]\n",
      "169 Train Loss 0.00015446644 Test RE 0.014263727970349386 p: [0.01085534]\n",
      "170 Train Loss 0.0001543384 Test RE 0.01426226113429853 p: [0.01085031]\n",
      "171 Train Loss 0.00015420449 Test RE 0.01428131846910628 p: [0.010848]\n",
      "172 Train Loss 0.0001529073 Test RE 0.01410929609790166 p: [0.01084747]\n",
      "173 Train Loss 0.00015181751 Test RE 0.014093701269818076 p: [0.01085031]\n",
      "174 Train Loss 0.0001516414 Test RE 0.01407821465538061 p: [0.01084381]\n",
      "175 Train Loss 0.00015158943 Test RE 0.014091195211787064 p: [0.01084376]\n",
      "176 Train Loss 0.00015060385 Test RE 0.013987049007457985 p: [0.01084671]\n",
      "177 Train Loss 0.00014979027 Test RE 0.013843572358387117 p: [0.01083086]\n",
      "178 Train Loss 0.00014968787 Test RE 0.013827036609489782 p: [0.01082892]\n",
      "179 Train Loss 0.00014959753 Test RE 0.01383901287050434 p: [0.01083709]\n",
      "180 Train Loss 0.00014946883 Test RE 0.013821818175721316 p: [0.01082741]\n",
      "181 Train Loss 0.00014854409 Test RE 0.013742554955488453 p: [0.01083268]\n",
      "182 Train Loss 0.00014786495 Test RE 0.013652999315856446 p: [0.01081582]\n",
      "183 Train Loss 0.0001477495 Test RE 0.013654766826837587 p: [0.01081237]\n",
      "184 Train Loss 0.00014624916 Test RE 0.013610208898248061 p: [0.01078238]\n",
      "185 Train Loss 0.00014528519 Test RE 0.013495835312874726 p: [0.01078256]\n",
      "186 Train Loss 0.00014445503 Test RE 0.013431671835248704 p: [0.01077449]\n",
      "187 Train Loss 0.00014286168 Test RE 0.013215596224778841 p: [0.01077348]\n",
      "188 Train Loss 0.00014211104 Test RE 0.013188468696858463 p: [0.01077536]\n",
      "189 Train Loss 0.00014093546 Test RE 0.013016633537131677 p: [0.01077306]\n",
      "190 Train Loss 0.00013977874 Test RE 0.012895578042683724 p: [0.01075275]\n",
      "191 Train Loss 0.00013900925 Test RE 0.012826597511879161 p: [0.01074383]\n",
      "192 Train Loss 0.0001380424 Test RE 0.012819814330992418 p: [0.01073776]\n",
      "193 Train Loss 0.0001373706 Test RE 0.012779676843802183 p: [0.0107279]\n",
      "194 Train Loss 0.00013690341 Test RE 0.012827621759182619 p: [0.01071613]\n",
      "195 Train Loss 0.00013659491 Test RE 0.012797366099951342 p: [0.01071471]\n",
      "196 Train Loss 0.00013636507 Test RE 0.012784224989061913 p: [0.01070662]\n",
      "197 Train Loss 0.00013634429 Test RE 0.012793726400842259 p: [0.01070536]\n",
      "198 Train Loss 0.00013626809 Test RE 0.012778117139408872 p: [0.01070025]\n",
      "199 Train Loss 0.00013540301 Test RE 0.012734267464916611 p: [0.01069314]\n",
      "200 Train Loss 0.00013523338 Test RE 0.012705165281219021 p: [0.010679]\n",
      "201 Train Loss 0.00013515362 Test RE 0.012691711225040755 p: [0.01067875]\n",
      "202 Train Loss 0.00013512482 Test RE 0.012693298919237673 p: [0.01067842]\n",
      "203 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "204 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "205 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "206 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "207 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "208 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "209 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "210 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "211 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "212 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "213 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "214 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "215 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "216 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "217 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "218 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "219 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "220 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "221 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "222 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "223 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "224 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "225 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "226 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "227 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "228 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "229 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "230 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "231 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "232 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "233 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "234 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "235 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "236 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "237 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "238 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "239 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "240 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "241 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "242 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "243 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "244 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "245 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "246 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "247 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "248 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "249 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "250 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "251 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "252 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "253 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "254 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "255 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "256 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "257 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "258 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "259 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "260 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "261 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "262 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "263 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "264 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "265 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "266 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "267 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "268 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "269 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "270 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "271 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "272 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "273 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "274 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "275 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "276 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "277 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "278 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "279 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "280 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "281 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "282 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "283 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "284 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "285 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "286 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "287 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "288 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "289 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "290 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "291 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "292 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "293 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "294 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "295 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "296 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "297 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "298 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "299 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "300 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "301 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "302 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "303 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "304 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "305 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "306 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "307 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "308 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "309 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "310 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "311 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "312 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "313 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "314 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "315 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "316 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "317 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "318 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "319 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "320 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "321 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "322 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "323 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "324 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "325 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "326 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "327 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "328 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "329 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "330 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "331 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "332 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "333 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "334 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "335 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "336 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "337 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "338 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "339 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "340 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "341 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "342 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "343 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "344 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "345 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "346 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "347 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "348 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "349 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "350 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "351 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "352 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "353 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "354 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "355 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "356 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "357 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "358 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "359 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "360 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "361 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "362 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "363 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "364 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "365 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "366 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "367 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "368 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "369 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "370 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "371 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "372 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "373 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "374 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "375 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "376 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "377 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "378 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "379 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "380 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "381 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "382 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "383 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "384 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "385 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "386 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "387 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "388 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "389 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "390 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "391 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "392 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "393 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "394 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "395 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "396 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "397 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "398 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "399 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "400 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "401 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "402 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "403 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "404 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "405 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "406 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "407 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "408 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "409 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "410 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "411 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "412 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "413 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "414 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "415 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "416 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "417 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "418 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "419 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "420 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "421 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "422 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "423 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "424 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "425 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "426 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "427 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "428 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "429 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "430 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "431 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "432 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "433 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "434 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "435 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "436 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "437 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "438 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "439 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "440 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "441 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "442 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "443 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "444 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "445 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "446 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "447 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "448 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "449 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "450 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "451 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "452 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "453 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "454 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "455 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "456 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "457 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "458 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "459 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "460 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "461 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "462 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "463 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "464 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "465 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "466 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "467 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "468 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "469 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "470 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "471 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "472 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "473 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "474 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "475 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "476 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "477 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "478 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "479 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "480 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "481 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "482 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "483 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "484 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "485 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "486 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "487 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "488 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "489 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "490 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "491 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "492 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "493 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "494 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "495 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "496 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "497 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "498 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "499 Train Loss 0.0001351173 Test RE 0.012693744861631738 p: [0.01067876]\n",
      "Training time: 83.09\n",
      "Training time: 83.09\n"
     ]
    }
   ],
   "source": [
    "max_reps = 1\n",
    "max_iter = 500 #200\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "\n",
    "\n",
    "N_T = 500 #Total number of data points for 'y'\n",
    "N_f = 10000 #Total number of collocation points \n",
    "\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(label)\n",
    "    print(reps)\n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []\n",
    " \n",
    "    \n",
    "    torch.manual_seed(reps*36)\n",
    "    \n",
    "    layers = np.array([2,50,50,50,50,1]) #9 hidden layers\n",
    "\n",
    "    PINN = Sequentialmodel(layers)\n",
    "   \n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "    \n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=1, \n",
    "                              max_iter = 20, \n",
    "                              max_eval = 30, \n",
    "                              tolerance_grad = 1e-08, \n",
    "                              tolerance_change = 1e-08, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "    \n",
    "    nan_flag = train_model(max_iter,reps)\n",
    "    \n",
    "    \n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    #elapsed_time[reps] = time.time() - start_time\n",
    "\n",
    "\n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "    \n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-2,2,500).reshape(-1,1)\n",
    "y = np.linspace(0,1,500).reshape(-1,1)\n",
    "\n",
    "X,Y = np.meshgrid(x,y)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "Y = Y.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xy = np.hstack((X,Y))\n",
    "\n",
    "xy_test_tensor = torch.from_numpy(xy).float().to(device)\n",
    "\n",
    "u_pred = PINN.forward(xy_test_tensor).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "cmap = plt.cm.jet\n",
    "\n",
    "img3 = ax.imshow(np.flip(u_pred.reshape(500,500),axis = 0),cmap = cmap,extent=[0,1,0,1],aspect = 0.75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'test_loss_full' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_30906/1704525943.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0ma\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mtest_loss_full\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'test_loss_full' is not defined"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + test_loss_full[i][-1]\n",
    "print(a/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + train_loss_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
