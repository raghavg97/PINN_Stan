{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def true_2D_1(xt): #True function for 2D_1 Klein Gordon Equation x \\in [-50,50] , t \\in [0,10]\n",
    "#     y = xt[:,0]*np.cos(xt[:,1])\n",
    "#     return y.reshape(-1,1)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_thresh = 25000\n",
    "level = \"_low\"\n",
    "label = \"Burgers_stan\" + level\n",
    "\n",
    "x = np.linspace(-1,1,256).reshape(-1,1)\n",
    "t = np.linspace(0,1,100).reshape(-1,1)\n",
    "\n",
    "X,T = np.meshgrid(x,t)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "T = T.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xt = np.hstack((X,T))\n",
    "\n",
    "# bound_pts_1 = (X == 0).reshape(-1,)\n",
    "# bound_pts_2 = np.logical_and(Y == 0,X != 0).reshape(-1,)\n",
    "# bound_pts_3 = np.logical_and(X == 1,Y != 0).reshape(-1,) \n",
    "# bound_pts_4 = np.logical_and(Y == 1,X != 1).reshape(-1,) \n",
    "\n",
    "# xy_bound_1 = xy[bound_pts_1,:]\n",
    "# xy_bound_2 = xy[bound_pts_2,:]\n",
    "# xy_bound_3 = xy[bound_pts_3,:]\n",
    "# xy_bound_4 = xy[bound_pts_4,:]\n",
    "\n",
    "# u_bound_1 = 1000*np.ones((np.shape(xy_bound_1)[0],1))\n",
    "# u_bound_2 = 800*np.ones((np.shape(xy_bound_2)[0],1))\n",
    "# u_bound_3 = 500*np.ones((np.shape(xy_bound_3)[0],1))\n",
    "# u_bound_4 = np.zeros((np.shape(xy_bound_4)[0],1))\n",
    "\n",
    "# xy_bound = np.vstack((xy_bound_1,xy_bound_2,xy_bound_3,xy_bound_4))\n",
    "# u_bound = np.vstack((u_bound_1,u_bound_2,u_bound_3,u_bound_4))\n",
    "\n",
    "# xy_test_tensor = torch.from_numpy(xy).float().to(device)\n",
    "\n",
    "lb_xt = xt[0]\n",
    "ub_xt = xt[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat('../burgers_shock_3.mat') \n",
    "\n",
    "pi = torch.from_numpy(np.array(np.pi)).double().to(device)\n",
    "\n",
    "x = np.array(data['x'])\n",
    "t = np.array(data['t'])\n",
    "\n",
    "X,T = np.meshgrid(x,t)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "T = T.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xt = np.hstack((X,T))\n",
    "\n",
    "u_true = np.array(data['usol'][:])\n",
    "\n",
    "xt_test_tensor = torch.from_numpy(xt).float().to(device)\n",
    "u_true_norm = np.linalg.norm(u_true,2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_T,N_f,seed):\n",
    "    '''Boundary Conditions''' \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    x_l = x[0]*np.ones((N_T,1))\n",
    "    t_l = np.random.uniform(t[0],t[-1],(N_T,1))\n",
    "    xt_l = np.hstack((x_l,t_l))\n",
    "    u_l = 0.0*np.ones((N_T,1))\n",
    "    \n",
    "    x_r = x[-1]*np.ones((N_T,1))\n",
    "    t_r = np.random.uniform(t[0],t[-1],(N_T,1))\n",
    "    xt_r = np.hstack((x_r,t_r))\n",
    "    u_r = 0.0*np.ones((N_T,1))\n",
    "    \n",
    "    x_0 = np.random.uniform(x[0],x[-1],(N_T,1))\n",
    "    t_0 = t[0]*np.ones((N_T,1))\n",
    "    xt_0 = np.hstack((x_0,t_0))\n",
    "    u_0 = -1.0*np.sin(np.pi*x_0)\n",
    "    \n",
    "    xt_BC = np.vstack((xt_l,xt_r,xt_0,xt)) #choose indices from  set 'idx' (x,t)\n",
    "    u_BC = np.vstack((u_l,u_r,u_0,u_true.reshape(-1,1)))\n",
    "    \n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    samples = sampling(N_f)\n",
    "    \n",
    "    xt_coll = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    \n",
    "    xt_coll = np.vstack((xt_coll, xt_BC)) # append training points to collocation points \n",
    "\n",
    "    return xt_coll, xt_BC, u_BC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        self.iter = 0\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)   \n",
    "        \n",
    "        beta_mean = 0.0*torch.ones((50,len(layers)-2))\n",
    "        beta_std = 0.1*torch.ones((50,len(layers)-2))\n",
    "        \n",
    "        self.beta = Parameter(torch.normal(beta_mean,beta_std))\n",
    "        self.beta.requiresGrad = True\n",
    "        \n",
    "        self.p = Parameter(torch.ones(1))\n",
    "        self.p.requiresGrad = True\n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xy):\n",
    "        if torch.is_tensor(xy) != True:         \n",
    "            xy = torch.from_numpy(xy)                \n",
    "        \n",
    "        ubxy = torch.from_numpy(ub_xt).float().to(device)\n",
    "        lbxy = torch.from_numpy(lb_xt).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xy = 2.0*(xy - lbxy)/(ubxy - lbxy) - 1.0\n",
    "        \n",
    "        #convert to float\n",
    "        a = xy.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            z1 =self.activation(z)\n",
    "            a = z1 + self.beta[:,i]*z*z1\n",
    "       \n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC(self,xy,u):\n",
    "                \n",
    "        loss_bc = self.loss_function(self.forward(xy), u)\n",
    "                \n",
    "        return loss_bc\n",
    "    \n",
    "    def loss_PDE(self, xy_coll, f_hat):\n",
    "        \n",
    "        g = xy_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_y = autograd.grad(u,g,torch.ones([xy_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_xx_yy = autograd.grad(u_x_y,g,torch.ones(xy_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        du_dx = u_x_y[:,[0]]\n",
    "        du_dt = u_x_y[:,[1]]\n",
    "        \n",
    "        d2u_dx2 = u_xx_yy[:,[0]]\n",
    "        \n",
    "\n",
    "        f = du_dt + u*du_dx - self.p*d2u_dx2/pi\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xy_BC,u_BC,xy_coll,f_hat):\n",
    "\n",
    "        loss_BC = self.loss_BC(xy_BC,u_BC)\n",
    "        loss_f = self.loss_PDE(xy_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_BC + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "         \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        u_pred = self.forward(xt_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "        \n",
    "        \n",
    "        return test_mse, test_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(xy_BC,u_BC,xy_coll,f_hat,seed):\n",
    "\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(xy_BC,u_BC,xy_coll,f_hat)\n",
    "        loss.backward()\n",
    "        #print(loss.cpu().detach().numpy())\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*9)\n",
    "    start_time = time.time() \n",
    "    thresh_flag = 0\n",
    "\n",
    "    xy_coll_np_array, xy_BC_np_array, u_BC_np_array = trainingdata(N_T,N_f,rep*22)\n",
    "        \n",
    "    xy_coll = torch.from_numpy(xy_coll_np_array).float().to(device)\n",
    "    xy_BC = torch.from_numpy(xy_BC_np_array).float().to(device)\n",
    "    u_BC = torch.from_numpy(u_BC_np_array).float().to(device)\n",
    "        \n",
    "    f_hat = torch.zeros(xy_coll.shape[0],1).to(device)\n",
    "    \n",
    "\n",
    "    for i in range(max_iter):\n",
    "        train_step(xy_BC,u_BC,xy_coll,f_hat,i)\n",
    "        loss_np = PINN.loss(xy_BC,u_BC,xy_coll,f_hat).cpu().detach().numpy()\n",
    "        \n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1          \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        \n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test RE\",test_re_loss[-1],\"p:\",PINN.p.cpu().detach().numpy())   \n",
    "        \n",
    "    elapsed_time[rep] = time.time() - start_time  \n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Burgers_stan_low\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 0.11332945 Test RE 0.5521822952808414 p: [0.73137116]\n",
      "1 Train Loss 0.038305953 Test RE 0.26822364740147014 p: [0.17748964]\n",
      "2 Train Loss 0.024784988 Test RE 0.21584947345789499 p: [0.11186743]\n",
      "3 Train Loss 0.016275708 Test RE 0.17150088599238505 p: [0.11441453]\n",
      "4 Train Loss 0.013581163 Test RE 0.15668482002843837 p: [0.12426602]\n",
      "5 Train Loss 0.011085884 Test RE 0.14998284117290486 p: [0.11336147]\n",
      "6 Train Loss 0.00853205 Test RE 0.12499735572708931 p: [0.12109609]\n",
      "7 Train Loss 0.0069304286 Test RE 0.10900569470938984 p: [0.12367035]\n",
      "8 Train Loss 0.0055008763 Test RE 0.0999860646069215 p: [0.10765075]\n",
      "9 Train Loss 0.0043302025 Test RE 0.08963094066838069 p: [0.10303266]\n",
      "10 Train Loss 0.0037334757 Test RE 0.07675124400403024 p: [0.1122355]\n",
      "11 Train Loss 0.0028593624 Test RE 0.07033582778862853 p: [0.11438446]\n",
      "12 Train Loss 0.0024752924 Test RE 0.061697919327861346 p: [0.11280723]\n",
      "13 Train Loss 0.0021427092 Test RE 0.05775180673235038 p: [0.10921862]\n",
      "14 Train Loss 0.0016843781 Test RE 0.052469119727748495 p: [0.1070475]\n",
      "15 Train Loss 0.001460572 Test RE 0.05001144030682451 p: [0.10479712]\n",
      "16 Train Loss 0.0012600495 Test RE 0.044735353942313594 p: [0.10570242]\n",
      "17 Train Loss 0.0011542665 Test RE 0.04288647682455631 p: [0.1024226]\n",
      "18 Train Loss 0.0009767602 Test RE 0.04025084959577405 p: [0.09901612]\n",
      "19 Train Loss 0.0009365581 Test RE 0.039320009971603224 p: [0.0980972]\n",
      "20 Train Loss 0.00088376424 Test RE 0.03943318889392137 p: [0.10032693]\n",
      "21 Train Loss 0.00081753806 Test RE 0.038547511994849445 p: [0.10120402]\n",
      "22 Train Loss 0.00078518555 Test RE 0.03813997870469823 p: [0.10077807]\n",
      "23 Train Loss 0.0007547231 Test RE 0.03831408910077699 p: [0.09816899]\n",
      "24 Train Loss 0.0007249107 Test RE 0.03831435694636709 p: [0.09871478]\n",
      "25 Train Loss 0.0007019219 Test RE 0.03779237322890584 p: [0.0992323]\n",
      "26 Train Loss 0.00067096914 Test RE 0.037271398615482655 p: [0.09821934]\n",
      "27 Train Loss 0.0006577757 Test RE 0.036942002080237596 p: [0.09876695]\n",
      "28 Train Loss 0.0006355175 Test RE 0.037011472574952124 p: [0.09876852]\n",
      "29 Train Loss 0.00060461205 Test RE 0.03716793595643716 p: [0.09982671]\n",
      "30 Train Loss 0.00058942963 Test RE 0.036732099838209895 p: [0.09872829]\n",
      "31 Train Loss 0.0005779398 Test RE 0.0365549262098713 p: [0.09705447]\n",
      "32 Train Loss 0.00056285795 Test RE 0.03670495061564925 p: [0.09643078]\n",
      "33 Train Loss 0.0005509511 Test RE 0.0367287724695357 p: [0.0963817]\n",
      "34 Train Loss 0.0005457057 Test RE 0.036687695334792124 p: [0.09635288]\n",
      "35 Train Loss 0.0005299955 Test RE 0.0365803185857373 p: [0.09702345]\n",
      "36 Train Loss 0.00052330334 Test RE 0.036581368329981344 p: [0.0971166]\n",
      "37 Train Loss 0.0005173711 Test RE 0.03650825822852203 p: [0.09700422]\n",
      "38 Train Loss 0.0005116035 Test RE 0.036397925207064974 p: [0.09715234]\n",
      "39 Train Loss 0.0005049254 Test RE 0.036361003022193805 p: [0.09719891]\n",
      "40 Train Loss 0.0005024249 Test RE 0.036294102969718085 p: [0.09746592]\n",
      "41 Train Loss 0.0004987255 Test RE 0.03611231598545062 p: [0.09700509]\n",
      "42 Train Loss 0.000491419 Test RE 0.03622196465768726 p: [0.09659898]\n",
      "43 Train Loss 0.0004893188 Test RE 0.036329717909540364 p: [0.09637135]\n",
      "44 Train Loss 0.00048338235 Test RE 0.03626727934613087 p: [0.09638304]\n",
      "45 Train Loss 0.00048335726 Test RE 0.03626046576021298 p: [0.09635608]\n",
      "46 Train Loss 0.00048126554 Test RE 0.03615016617677607 p: [0.09654746]\n",
      "47 Train Loss 0.0004774903 Test RE 0.036084149365037115 p: [0.09674767]\n",
      "48 Train Loss 0.0004762414 Test RE 0.036124311067002694 p: [0.09667502]\n",
      "49 Train Loss 0.00047528953 Test RE 0.036097354242554405 p: [0.09664551]\n",
      "50 Train Loss 0.0004728526 Test RE 0.036015420780227236 p: [0.09690185]\n",
      "51 Train Loss 0.00047087 Test RE 0.03597315261164085 p: [0.09690624]\n",
      "52 Train Loss 0.00046967258 Test RE 0.03595712590317244 p: [0.09701097]\n",
      "53 Train Loss 0.00046691377 Test RE 0.03591993429676795 p: [0.09714784]\n",
      "54 Train Loss 0.00046605727 Test RE 0.035860283614121795 p: [0.09711479]\n",
      "55 Train Loss 0.00046604773 Test RE 0.03586345096012939 p: [0.09713]\n",
      "56 Train Loss 0.00046604773 Test RE 0.03586345096012939 p: [0.09713]\n",
      "57 Train Loss 0.00046604773 Test RE 0.03586345096012939 p: [0.09713]\n",
      "58 Train Loss 0.00046604773 Test RE 0.03586345096012939 p: [0.09713]\n",
      "59 Train Loss 0.00046604773 Test RE 0.03586345096012939 p: [0.09713]\n",
      "60 Train Loss 0.00046604773 Test RE 0.03586345096012939 p: [0.09713]\n",
      "61 Train Loss 0.00046604773 Test RE 0.03586345096012939 p: [0.09713]\n",
      "62 Train Loss 0.00046604773 Test RE 0.03586345096012939 p: [0.09713]\n",
      "63 Train Loss 0.00046604773 Test RE 0.03586345096012939 p: [0.09713]\n",
      "64 Train Loss 0.00046604773 Test RE 0.03586345096012939 p: [0.09713]\n",
      "65 Train Loss 0.00046604773 Test RE 0.03586345096012939 p: [0.09713]\n",
      "66 Train Loss 0.00046604773 Test RE 0.03586345096012939 p: [0.09713]\n",
      "67 Train Loss 0.00046604773 Test RE 0.03586345096012939 p: [0.09713]\n",
      "68 Train Loss 0.00046604773 Test RE 0.03586345096012939 p: [0.09713]\n",
      "69 Train Loss 0.00046604773 Test RE 0.03586345096012939 p: [0.09713]\n",
      "70 Train Loss 0.00046604773 Test RE 0.03586345096012939 p: [0.09713]\n",
      "71 Train Loss 0.00046604773 Test RE 0.03586345096012939 p: [0.09713]\n",
      "72 Train Loss 0.00046604773 Test RE 0.03586345096012939 p: [0.09713]\n",
      "73 Train Loss 0.00046604773 Test RE 0.03586345096012939 p: [0.09713]\n",
      "74 Train Loss 0.00046604773 Test RE 0.03586345096012939 p: [0.09713]\n",
      "75 Train Loss 0.00046604773 Test RE 0.03586345096012939 p: [0.09713]\n",
      "76 Train Loss 0.00046604773 Test RE 0.03586345096012939 p: [0.09713]\n",
      "77 Train Loss 0.00046604773 Test RE 0.03586345096012939 p: [0.09713]\n",
      "78 Train Loss 0.00046604773 Test RE 0.03586345096012939 p: [0.09713]\n",
      "79 Train Loss 0.00046604773 Test RE 0.03586345096012939 p: [0.09713]\n",
      "80 Train Loss 0.00046604773 Test RE 0.03586345096012939 p: [0.09713]\n",
      "81 Train Loss 0.00046604773 Test RE 0.03586345096012939 p: [0.09713]\n",
      "82 Train Loss 0.00046604773 Test RE 0.03586345096012939 p: [0.09713]\n",
      "83 Train Loss 0.00046604773 Test RE 0.03586345096012939 p: [0.09713]\n",
      "84 Train Loss 0.00046604773 Test RE 0.03586345096012939 p: [0.09713]\n",
      "85 Train Loss 0.00046604773 Test RE 0.03586345096012939 p: [0.09713]\n",
      "86 Train Loss 0.00046604773 Test RE 0.03586345096012939 p: [0.09713]\n",
      "87 Train Loss 0.00046604773 Test RE 0.03586345096012939 p: [0.09713]\n",
      "88 Train Loss 0.00046604773 Test RE 0.03586345096012939 p: [0.09713]\n",
      "89 Train Loss 0.00046604773 Test RE 0.03586345096012939 p: [0.09713]\n",
      "90 Train Loss 0.00046604773 Test RE 0.03586345096012939 p: [0.09713]\n",
      "91 Train Loss 0.00046604773 Test RE 0.03586345096012939 p: [0.09713]\n",
      "92 Train Loss 0.00046604773 Test RE 0.03586345096012939 p: [0.09713]\n",
      "93 Train Loss 0.00046604773 Test RE 0.03586345096012939 p: [0.09713]\n",
      "94 Train Loss 0.00046604773 Test RE 0.03586345096012939 p: [0.09713]\n",
      "95 Train Loss 0.00046604773 Test RE 0.03586345096012939 p: [0.09713]\n",
      "96 Train Loss 0.00046604773 Test RE 0.03586345096012939 p: [0.09713]\n",
      "97 Train Loss 0.00046604773 Test RE 0.03586345096012939 p: [0.09713]\n",
      "98 Train Loss 0.00046604773 Test RE 0.03586345096012939 p: [0.09713]\n",
      "99 Train Loss 0.00046604773 Test RE 0.03586345096012939 p: [0.09713]\n",
      "Training time: 38.88\n",
      "Training time: 38.88\n",
      "Burgers_stan_low\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 0.15637328 Test RE 0.6951244808743261 p: [0.94010735]\n",
      "1 Train Loss 0.039815586 Test RE 0.29257704725334244 p: [0.21522436]\n",
      "2 Train Loss 0.018882357 Test RE 0.1961558286025702 p: [0.12898749]\n",
      "3 Train Loss 0.014591295 Test RE 0.17538416798380257 p: [0.12408648]\n",
      "4 Train Loss 0.012180636 Test RE 0.1564406031493355 p: [0.13382001]\n",
      "5 Train Loss 0.009587824 Test RE 0.13554808241511282 p: [0.12274269]\n",
      "6 Train Loss 0.007944767 Test RE 0.12379672337468976 p: [0.11832875]\n",
      "7 Train Loss 0.0059587983 Test RE 0.11002276373422357 p: [0.11070213]\n",
      "8 Train Loss 0.00420018 Test RE 0.08273902871914805 p: [0.11372538]\n",
      "9 Train Loss 0.0033053865 Test RE 0.07015077364088695 p: [0.10392679]\n",
      "10 Train Loss 0.0025977304 Test RE 0.060115295825797124 p: [0.10399229]\n",
      "11 Train Loss 0.0018756897 Test RE 0.05191399724140194 p: [0.10845111]\n",
      "12 Train Loss 0.0014458378 Test RE 0.04961787278744037 p: [0.10650261]\n",
      "13 Train Loss 0.0011220456 Test RE 0.04253220207236603 p: [0.10060147]\n",
      "14 Train Loss 0.0009187044 Test RE 0.041883516548133076 p: [0.09962008]\n",
      "15 Train Loss 0.000812941 Test RE 0.03880314518628601 p: [0.09808032]\n",
      "16 Train Loss 0.0006969557 Test RE 0.037652981504886225 p: [0.09638468]\n",
      "17 Train Loss 0.0006535911 Test RE 0.037139622162467446 p: [0.09608826]\n",
      "18 Train Loss 0.00059855566 Test RE 0.03679871122348697 p: [0.09892157]\n",
      "19 Train Loss 0.0005582775 Test RE 0.03628543930569214 p: [0.09719266]\n",
      "20 Train Loss 0.00053645857 Test RE 0.036331130239493595 p: [0.09688786]\n",
      "21 Train Loss 0.0005128557 Test RE 0.03617710890180582 p: [0.09745289]\n",
      "22 Train Loss 0.0005027866 Test RE 0.03609535149151243 p: [0.09736501]\n",
      "23 Train Loss 0.00049648347 Test RE 0.03597796504350315 p: [0.09681041]\n",
      "24 Train Loss 0.0004869981 Test RE 0.03606931502729365 p: [0.09774873]\n",
      "25 Train Loss 0.0004798502 Test RE 0.03593943501164168 p: [0.0969667]\n",
      "26 Train Loss 0.00047339764 Test RE 0.03585283549473238 p: [0.09732103]\n",
      "27 Train Loss 0.0004698317 Test RE 0.03577323105750666 p: [0.09698314]\n",
      "28 Train Loss 0.00046388683 Test RE 0.035800638147272607 p: [0.09699754]\n",
      "29 Train Loss 0.00046005868 Test RE 0.03577145385382586 p: [0.09679407]\n",
      "30 Train Loss 0.00045692213 Test RE 0.0357093451139634 p: [0.09696998]\n",
      "31 Train Loss 0.0004550099 Test RE 0.035639764563665036 p: [0.09635873]\n",
      "32 Train Loss 0.00045500539 Test RE 0.035625172093530603 p: [0.09636776]\n",
      "33 Train Loss 0.0004515565 Test RE 0.03561983380768545 p: [0.09655289]\n",
      "34 Train Loss 0.0004493661 Test RE 0.03547192698063431 p: [0.09673157]\n",
      "35 Train Loss 0.00044644275 Test RE 0.03548033126207658 p: [0.09717496]\n",
      "36 Train Loss 0.00044476998 Test RE 0.0355800113594427 p: [0.09702622]\n",
      "37 Train Loss 0.0004433933 Test RE 0.03556402397353577 p: [0.09689166]\n",
      "38 Train Loss 0.00044330748 Test RE 0.03555717364602779 p: [0.09692395]\n",
      "39 Train Loss 0.0004427284 Test RE 0.03551325059748177 p: [0.09669694]\n",
      "40 Train Loss 0.00044140598 Test RE 0.03549519247619773 p: [0.09675872]\n",
      "41 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "42 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "43 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "44 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "45 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "46 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "47 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "48 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "49 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "50 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "51 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "52 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "53 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "54 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "55 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "56 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "57 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "58 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "59 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "60 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "61 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "62 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "63 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "64 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "65 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "66 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "67 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "68 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "69 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "70 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "71 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "72 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "73 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "74 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "75 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "76 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "77 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "78 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "79 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "80 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "81 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "82 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "83 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "84 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "85 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "86 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "87 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "88 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "89 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "90 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "91 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "92 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "93 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "94 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "95 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "96 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "97 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "98 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "99 Train Loss 0.00044139242 Test RE 0.03550456691896229 p: [0.09674966]\n",
      "Training time: 28.45\n",
      "Training time: 28.45\n",
      "Burgers_stan_low\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 0.11691889 Test RE 0.5333549188463973 p: [0.71809566]\n",
      "1 Train Loss 0.027805012 Test RE 0.2607739832151009 p: [0.18490364]\n",
      "2 Train Loss 0.01628002 Test RE 0.18668771911941365 p: [0.12870684]\n",
      "3 Train Loss 0.012967828 Test RE 0.16868970475813308 p: [0.12858553]\n",
      "4 Train Loss 0.011001492 Test RE 0.1522815699797809 p: [0.11991613]\n",
      "5 Train Loss 0.007948327 Test RE 0.11297993766468303 p: [0.10285991]\n",
      "6 Train Loss 0.0054615666 Test RE 0.0852657331196804 p: [0.10623073]\n",
      "7 Train Loss 0.0039971173 Test RE 0.08197000252551978 p: [0.10829663]\n",
      "8 Train Loss 0.0025850069 Test RE 0.05734885578799252 p: [0.10110522]\n",
      "9 Train Loss 0.0021457174 Test RE 0.053250607767924556 p: [0.09839932]\n",
      "10 Train Loss 0.0016769348 Test RE 0.05027727875908618 p: [0.10024951]\n",
      "11 Train Loss 0.0013003602 Test RE 0.04636066416052187 p: [0.09514105]\n",
      "12 Train Loss 0.0010147556 Test RE 0.04290952210359303 p: [0.09746113]\n",
      "13 Train Loss 0.00086159393 Test RE 0.039926994647247055 p: [0.09757217]\n",
      "14 Train Loss 0.0007844941 Test RE 0.03936151069267299 p: [0.09958643]\n",
      "15 Train Loss 0.00068829354 Test RE 0.037973065679327664 p: [0.0986236]\n",
      "16 Train Loss 0.000633132 Test RE 0.037049366313753676 p: [0.09766664]\n",
      "17 Train Loss 0.0005934782 Test RE 0.03673126191132856 p: [0.09695353]\n",
      "18 Train Loss 0.0005720845 Test RE 0.0368445631539195 p: [0.09673501]\n",
      "19 Train Loss 0.0005512663 Test RE 0.0369168463876298 p: [0.0970935]\n",
      "20 Train Loss 0.0005331301 Test RE 0.036541700675094234 p: [0.09647623]\n",
      "21 Train Loss 0.0005169473 Test RE 0.03667649248782274 p: [0.09732771]\n",
      "22 Train Loss 0.0005079097 Test RE 0.03653092513904356 p: [0.0970076]\n",
      "23 Train Loss 0.0004980152 Test RE 0.03634182294459387 p: [0.09709227]\n",
      "24 Train Loss 0.0004917474 Test RE 0.03624556748953904 p: [0.09750144]\n",
      "25 Train Loss 0.00048535044 Test RE 0.036133137870046336 p: [0.09726962]\n",
      "26 Train Loss 0.00047900662 Test RE 0.0360376012477212 p: [0.09669597]\n",
      "27 Train Loss 0.00047269868 Test RE 0.03586817945565342 p: [0.09673705]\n",
      "28 Train Loss 0.00046613786 Test RE 0.0358586765837647 p: [0.09727703]\n",
      "29 Train Loss 0.00046429058 Test RE 0.035820273778450734 p: [0.09696598]\n",
      "30 Train Loss 0.0004608235 Test RE 0.03588483107007192 p: [0.09652499]\n",
      "31 Train Loss 0.00045688887 Test RE 0.035722086079493294 p: [0.09663864]\n",
      "32 Train Loss 0.0004533721 Test RE 0.035652832599618836 p: [0.09669887]\n",
      "33 Train Loss 0.00045070075 Test RE 0.03566899457032913 p: [0.09640899]\n",
      "34 Train Loss 0.00044822713 Test RE 0.03558679425249124 p: [0.09658535]\n",
      "35 Train Loss 0.00044560136 Test RE 0.035580557054954894 p: [0.09639707]\n",
      "36 Train Loss 0.00044470685 Test RE 0.03551974672885424 p: [0.09656854]\n",
      "37 Train Loss 0.0004429086 Test RE 0.03550126512253261 p: [0.09667189]\n",
      "38 Train Loss 0.00044280995 Test RE 0.03546656437140547 p: [0.09671336]\n",
      "39 Train Loss 0.000440356 Test RE 0.035428449528262115 p: [0.09658215]\n",
      "40 Train Loss 0.00044005146 Test RE 0.03548021948400262 p: [0.09654503]\n",
      "41 Train Loss 0.00044004878 Test RE 0.03546122292314139 p: [0.09653794]\n",
      "42 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "43 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "44 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "45 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "46 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "47 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "48 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "49 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "50 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "51 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "52 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "53 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "54 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "55 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "56 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "57 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "58 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "59 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "60 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "61 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "62 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "63 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "64 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "65 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "66 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "67 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "68 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "69 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "70 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "71 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "72 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "73 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "74 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "75 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "76 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "77 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "78 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "79 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "80 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "81 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "82 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "83 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "84 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "85 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "86 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "87 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "88 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "89 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "90 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "91 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "92 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "93 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "94 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "95 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "96 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "97 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "98 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "99 Train Loss 0.00044004107 Test RE 0.035466668094449795 p: [0.09653488]\n",
      "Training time: 28.87\n",
      "Training time: 28.87\n",
      "Burgers_stan_low\n",
      "3\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 0.1375114 Test RE 0.6379977985691601 p: [0.9355143]\n",
      "1 Train Loss 0.04681971 Test RE 0.2926113467567527 p: [0.2441064]\n",
      "2 Train Loss 0.025140218 Test RE 0.2439304296431926 p: [0.13421664]\n",
      "3 Train Loss 0.017541528 Test RE 0.19372828863735592 p: [0.12674941]\n",
      "4 Train Loss 0.014414276 Test RE 0.17184548313156292 p: [0.13836175]\n",
      "5 Train Loss 0.011800633 Test RE 0.15963435783088603 p: [0.12413678]\n",
      "6 Train Loss 0.010208759 Test RE 0.1477796891806235 p: [0.13494883]\n",
      "7 Train Loss 0.0091042165 Test RE 0.1349660866497784 p: [0.13256498]\n",
      "8 Train Loss 0.007475333 Test RE 0.1251635186604182 p: [0.13047883]\n",
      "9 Train Loss 0.0061016306 Test RE 0.10003427203194223 p: [0.11434442]\n",
      "10 Train Loss 0.0046516024 Test RE 0.08551202227300464 p: [0.11735801]\n",
      "11 Train Loss 0.0036669122 Test RE 0.07043007999812013 p: [0.10715269]\n",
      "12 Train Loss 0.0026987535 Test RE 0.06436549710658372 p: [0.11030638]\n",
      "13 Train Loss 0.0022974259 Test RE 0.0600820705219044 p: [0.1151768]\n",
      "14 Train Loss 0.0017604247 Test RE 0.05133934841975282 p: [0.10835534]\n",
      "15 Train Loss 0.0014050263 Test RE 0.044102459328451304 p: [0.10105171]\n",
      "16 Train Loss 0.0011816903 Test RE 0.04211481353967542 p: [0.10216571]\n",
      "17 Train Loss 0.0009976283 Test RE 0.04158265414132649 p: [0.10019284]\n",
      "18 Train Loss 0.00081975834 Test RE 0.04033256538811 p: [0.09482545]\n",
      "19 Train Loss 0.0007273428 Test RE 0.03891801539597659 p: [0.09828261]\n",
      "20 Train Loss 0.00066089694 Test RE 0.03785200982040339 p: [0.0983182]\n",
      "21 Train Loss 0.0006162115 Test RE 0.03721084032611881 p: [0.0982888]\n",
      "22 Train Loss 0.00057622825 Test RE 0.036922681825516564 p: [0.09738059]\n",
      "23 Train Loss 0.0005540258 Test RE 0.03694465828166241 p: [0.09743494]\n",
      "24 Train Loss 0.0005328899 Test RE 0.036441038164110984 p: [0.09653861]\n",
      "25 Train Loss 0.00051883556 Test RE 0.03634969944252125 p: [0.09714596]\n",
      "26 Train Loss 0.000510475 Test RE 0.03635115649386726 p: [0.09684885]\n",
      "27 Train Loss 0.0005016771 Test RE 0.03611221406029478 p: [0.09704605]\n",
      "28 Train Loss 0.0004893546 Test RE 0.03617585859945656 p: [0.09783049]\n",
      "29 Train Loss 0.00048476033 Test RE 0.036022820071986726 p: [0.09737563]\n",
      "30 Train Loss 0.00047704892 Test RE 0.03615165139915623 p: [0.0975363]\n",
      "31 Train Loss 0.00047075577 Test RE 0.03608001834738812 p: [0.09648828]\n",
      "32 Train Loss 0.0004686591 Test RE 0.03610437294241858 p: [0.09660815]\n",
      "33 Train Loss 0.00046456206 Test RE 0.03595058919648575 p: [0.0967223]\n",
      "34 Train Loss 0.00046100805 Test RE 0.03587409655325174 p: [0.09638423]\n",
      "35 Train Loss 0.00045943278 Test RE 0.03590158233032219 p: [0.09635669]\n",
      "36 Train Loss 0.00045758951 Test RE 0.035815447122661256 p: [0.09614098]\n",
      "37 Train Loss 0.00045561898 Test RE 0.035776415322945766 p: [0.09625126]\n",
      "38 Train Loss 0.0004530656 Test RE 0.035785668515367075 p: [0.0962689]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_2180/2957329489.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 52\u001b[0;31m     \u001b[0mnan_flag\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2180/1213486050.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(max_iter, rep)\u001b[0m\n\u001b[1;32m     15\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     16\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 17\u001b[0;31m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy_BC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu_BC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxy_coll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_hat\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     18\u001b[0m         \u001b[0mloss_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPINN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy_BC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu_BC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxy_coll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2180/896708044.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(xy_BC, u_BC, xy_coll, f_hat, seed)\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 10\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                     loss, flat_grad, t, ls_func_evals = _strong_wolfe(\n\u001b[0;32m--> 426\u001b[0;31m                         obj_func, x_init, t, d, loss, flat_grad, gtd)\n\u001b[0m\u001b[1;32m    427\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0mopt_cond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflat_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mtolerance_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36m_strong_wolfe\u001b[0;34m(obj_func, x, t, d, f, g, gtd, c1, c2, tolerance_change, max_ls)\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0;31m# evaluate objective and gradient using initial step\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m     \u001b[0mf_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m     \u001b[0mls_func_evals\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m     \u001b[0mgtd_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mobj_func\u001b[0;34m(x, t, d)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m                     \u001b[0;32mdef\u001b[0m \u001b[0mobj_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m                         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_directional_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                     loss, flat_grad, t, ls_func_evals = _strong_wolfe(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36m_directional_evaluate\u001b[0;34m(self, closure, x, t, d)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_directional_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0mflat_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gather_flat_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2180/896708044.py\u001b[0m in \u001b[0;36mclosure\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPINN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy_BC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu_BC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mxy_coll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;31m#print(loss.cpu().detach().numpy())\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2180/1410752823.py\u001b[0m in \u001b[0;36mloss\u001b[0;34m(self, xy_BC, u_BC, xy_coll, f_hat)\u001b[0m\n\u001b[1;32m     86\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m         \u001b[0mloss_BC\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_BC\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy_BC\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mu_BC\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m         \u001b[0mloss_f\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss_PDE\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy_coll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0mloss_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mloss_BC\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss_f\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_2180/1410752823.py\u001b[0m in \u001b[0;36mloss_PDE\u001b[0;34m(self, xy_coll, f_hat)\u001b[0m\n\u001b[1;32m     71\u001b[0m         \u001b[0mu_xx_yy\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgrad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mu_x_y\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mg\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mones\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxy_coll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mallow_unused\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 73\u001b[0;31m         \u001b[0mdu_dx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu_x_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     74\u001b[0m         \u001b[0mdu_dt\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mu_x_y\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_reps = 5\n",
    "max_iter = 100 #200\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "\n",
    "\n",
    "N_T = 500 #Total number of data points for 'y'\n",
    "N_f = 10000 #Total number of collocation points \n",
    "\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(label)\n",
    "    print(reps)\n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []\n",
    " \n",
    "    \n",
    "    torch.manual_seed(reps*36)\n",
    "    \n",
    "    layers = np.array([2,50,50,50,50,1]) #9 hidden layers\n",
    "\n",
    "    PINN = Sequentialmodel(layers)\n",
    "   \n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "    \n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=1, \n",
    "                              max_iter = 20, \n",
    "                              max_eval = 30, \n",
    "                              tolerance_grad = 1e-08, \n",
    "                              tolerance_change = 1e-08, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "    \n",
    "    nan_flag = train_model(max_iter,reps)\n",
    "    \n",
    "    \n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    #elapsed_time[reps] = time.time() - start_time\n",
    "\n",
    "\n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "    \n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-2,2,500).reshape(-1,1)\n",
    "y = np.linspace(0,1,500).reshape(-1,1)\n",
    "\n",
    "X,Y = np.meshgrid(x,y)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "Y = Y.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xy = np.hstack((X,Y))\n",
    "\n",
    "xy_test_tensor = torch.from_numpy(xy).float().to(device)\n",
    "\n",
    "u_pred = PINN.forward(xy_test_tensor).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "u_true.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "cmap = plt.cm.jet\n",
    "\n",
    "img3 = ax.imshow(np.flip(u_pred.reshape(500,500),axis = 0),cmap = cmap,extent=[0,1,0,1],aspect = 0.75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + test_loss_full[i][-1]\n",
    "print(a/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + train_loss_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
