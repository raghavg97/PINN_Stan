{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8byrnUmNKGR",
    "outputId": "510f67c6-b225-41ba-e78a-d0a6e54ded85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uoNYpDzlNKGV"
   },
   "outputs": [],
   "source": [
    "tstart= 0 \n",
    "tstop= 60 \n",
    "\n",
    "increment = 0.1 \n",
    "# Initial condition\n",
    "x_init= [0,0] \n",
    "\n",
    "t = np.arange(tstart,tstop+1,increment)\n",
    "\n",
    "c = 1 # Damping constant \n",
    "k = 0.05 # Stiffness of the spring\n",
    "m = 5 # Mass \n",
    "F0 = 100\n",
    "\n",
    "def mydiff(x, t):    \n",
    "    F =F0\n",
    "    \n",
    "    dx1dt = x[1] \n",
    "    dx2dt = (F -c*x[1] -k*x[0])/m\n",
    "    dxdt= [dx1dt, dx2dt] \n",
    "    \n",
    "    return dxdt \n",
    "\n",
    "x_full_sol = odeint(mydiff, x_init, t) \n",
    "\n",
    "x_sol = x_full_sol[:,0]\n",
    "v_sol = x_full_sol[:,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BR02v-fkNKGV"
   },
   "outputs": [],
   "source": [
    "level = \"medium\"\n",
    "label = \"1D_SMD_swish_\" + level\n",
    "\n",
    "loss_thresh = 0.005\n",
    "\n",
    "t_test = t.reshape(-1,1)\n",
    "t_test_tensor = torch.from_numpy(t_test).float().to(device)\n",
    "\n",
    "# bc1_t = t[0].reshape(-1,1)\n",
    "# bc1_x = x_sol[0].reshape(-1,1)\n",
    "# t_bc1_train = torch.from_numpy(bc1_t).float().to(device)\n",
    "# x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "\n",
    "bc1_t = t.reshape(-1,1)\n",
    "bc1_x = x_sol.reshape(-1,1)\n",
    "t_bc1_train = torch.from_numpy(bc1_t).float().to(device)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "\n",
    "\n",
    "bc2_t = t[0].reshape(-1,1)\n",
    "t_bc2_train = torch.from_numpy(bc2_t).float().to(device)\n",
    "bc2_val = v_sol[0].reshape(-1,1)\n",
    "bc2_val =torch.from_numpy(bc2_val).float().to(device)\n",
    "\n",
    "t_test = t.reshape(-1,1)\n",
    "t_test_tensor = torch.from_numpy(t_test).float().to(device)\n",
    "x_true = x_sol\n",
    "x_true_norm = np.linalg.norm(x_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(t[0]) \n",
    "ub = np.array(t[-1]) \n",
    "\n",
    "#torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SyyktBKBXRo1"
   },
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    \n",
    "    t01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=t01,random_state =seed)\n",
    "\n",
    "    t_coll = lb + (ub-lb)*sampling(N_f)\n",
    "    # t_coll = np.vstack((t_coll,)) # append training points to collocation points \n",
    "\n",
    "    return t_coll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "o1b21zLnNKGW"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "        \n",
    "        self.beta = Parameter(torch.ones((layers[1],len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "        \n",
    "        self.k = Parameter(torch.tensor(0.0))\n",
    "        self.k.requiresGrad = True\n",
    "        self.c = Parameter(torch.tensor(0.0))\n",
    "        self.c.requiresGrad = True\n",
    "        self.m = Parameter(torch.tensor(0.0))\n",
    "        self.m.requiresGrad = True\n",
    "        \n",
    "    'forward pass'\n",
    "    def forward(self,t):\n",
    "        if torch.is_tensor(t) != True:         \n",
    "            t = torch.from_numpy(t)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        t = 2.0*(t - l_b)/(u_b - l_b) - 1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = t.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = z*self.activation(self.beta[:,i]*z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,t,x):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(t), x)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,t_bc2,bc2_val):\n",
    "        g = t_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        x = self.forward(g)    \n",
    "            \n",
    "        x_t = autograd.grad(x,g,torch.ones([t_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dx_dt = x_t[:,[0]]\n",
    "        \n",
    "        bc2 = dx_dt\n",
    "        \n",
    "        loss_bc2= self.loss_function(bc2,bc2_val)\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self,t_coll,f_hat):\n",
    "             \n",
    "        g = t_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        x = self.forward(g) \n",
    "\n",
    "        x_t = autograd.grad(x,g,torch.ones([t_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        x_tt = autograd.grad(x_t,g,torch.ones(t_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dx_dt = x_t[:,[0]]\n",
    "        \n",
    "        dx2_d2t = x_tt[:,[0]]\n",
    "        \n",
    "        f = self.m*dx2_d2t + self.c*dx_dt + self.k*x - F0\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,t_bc1,x_bc1,t_bc2,bc2_val,t_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(t_bc1,x_bc1)\n",
    "        loss_bc2 = self.loss_BC2(t_bc2,bc2_val)\n",
    "        loss_f = self.loss_PDE(t_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self):\n",
    "        x_pred = self.forward(t_test_tensor)\n",
    "        x_pred = x_pred.cpu().detach().numpy()\n",
    "\n",
    "        return x_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        x_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(x_pred.reshape(-1,1) - x_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(x_pred.reshape(-1,1) - x_true.reshape(-1,1),2)/x_true_norm\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fLY2mT5BOgjD"
   },
   "outputs": [],
   "source": [
    "def train_step(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8srA5uGuObil"
   },
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    m_val.append(PINN.m.cpu().detach().numpy())\n",
    "    k_val.append(PINN.k.cpu().detach().numpy())\n",
    "    c_val.append(PINN.c.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0SezTZ_racQB"
   },
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "\n",
    "    t_coll = colloc_pts(N_f,rep*11)\n",
    "    t_coll =  torch.from_numpy(t_coll).float().to(device)\n",
    "    \n",
    "    f_hat = torch.zeros(t_coll.shape[0],1).to(device)\n",
    "    for i in range(max_iter):      \n",
    "        train_step(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat)\n",
    "         \n",
    "        loss_np = PINN.loss(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test RE\",test_re_loss[-1],\"c\",PINN.c.cpu().detach().numpy(),\"k\",PINN.k.cpu().detach().numpy(),\"m\",PINN.m.cpu().detach().numpy())\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I9BYbcJ0NKGX",
    "outputId": "42720d97-b37a-4c42-b7e5-b77a0ef391f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 1413494.0 Test RE 0.745011806009658 c -0.009978067 k 1.2392292 m -8.8687484e-05\n",
      "1 Train Loss 275070.1 Test RE 0.35214412974721937 c -0.028456535 k 0.06988286 m -0.00022546465\n",
      "2 Train Loss 110410.37 Test RE 0.2220479940116448 c -0.025385715 k 0.062851965 m -0.00022102895\n",
      "3 Train Loss 39855.824 Test RE 0.1301213075038518 c -0.012520003 k 0.06276738 m -0.00020862746\n",
      "4 Train Loss 12640.885 Test RE 0.06446333295602634 c 0.034357432 k 0.058572736 m 1.0596337e-05\n",
      "5 Train Loss 4479.3228 Test RE 0.016383117282127498 c 0.07589658 k 0.05767747 m 0.00029315424\n",
      "6 Train Loss 4109.8467 Test RE 0.014940407676640297 c 0.0861925 k 0.062205844 m 0.0004149003\n",
      "7 Train Loss 3495.9268 Test RE 0.012136610487724905 c 0.1570767 k 0.058216266 m 0.0013218886\n",
      "8 Train Loss 2644.2002 Test RE 0.013814404927882457 c 0.21908437 k 0.056801245 m 0.002249608\n",
      "9 Train Loss 2506.1406 Test RE 0.011433154431883046 c 0.23662509 k 0.057436626 m 0.0026019693\n",
      "10 Train Loss 2216.4924 Test RE 0.008426101292964745 c 0.3575461 k 0.05496639 m 0.005503257\n",
      "11 Train Loss 1720.8021 Test RE 0.0072794166762107735 c 0.69614834 k 0.050460394 m 0.015404714\n",
      "12 Train Loss 1609.1342 Test RE 0.007361440410370532 c 0.7931302 k 0.049676996 m 0.018620925\n",
      "13 Train Loss 1180.456 Test RE 0.010846001765783611 c 0.75340235 k 0.05201834 m 0.021033246\n",
      "14 Train Loss 770.45776 Test RE 0.007363276207514168 c 1.1098565 k 0.044671733 m 0.033329573\n",
      "15 Train Loss 748.0973 Test RE 0.00726341647911634 c 1.1544251 k 0.0446959 m 0.03511435\n",
      "16 Train Loss 628.89764 Test RE 0.007984187304163014 c 1.2121123 k 0.044194944 m 0.043302648\n",
      "17 Train Loss 563.74097 Test RE 0.007204462689407839 c 1.2037251 k 0.04394151 m 0.049071833\n",
      "18 Train Loss 524.5041 Test RE 0.007745842864514356 c 1.1799221 k 0.043928936 m 0.06966056\n",
      "19 Train Loss 483.96356 Test RE 0.0070822202317659955 c 1.2112589 k 0.04393393 m 0.08634465\n",
      "20 Train Loss 451.40448 Test RE 0.006857499574974192 c 1.1803058 k 0.04450878 m 0.09044016\n",
      "21 Train Loss 438.45584 Test RE 0.007032988632388159 c 1.1176417 k 0.044911675 m 0.098537534\n",
      "22 Train Loss 397.45004 Test RE 0.006641722900225406 c 1.0393331 k 0.045871004 m 0.14410123\n",
      "23 Train Loss 354.95258 Test RE 0.006051174741082187 c 1.1204916 k 0.045070324 m 0.1783884\n",
      "24 Train Loss 350.12683 Test RE 0.005711430405862408 c 1.124656 k 0.044504285 m 0.17497817\n",
      "25 Train Loss 345.6894 Test RE 0.005337566058917447 c 1.101525 k 0.044845495 m 0.1824615\n",
      "26 Train Loss 326.02478 Test RE 0.0051621538367906915 c 1.0789833 k 0.045590267 m 0.23211883\n",
      "27 Train Loss 316.1278 Test RE 0.005512152184555572 c 1.1174355 k 0.044899974 m 0.24944516\n",
      "28 Train Loss 313.0036 Test RE 0.00575521083659671 c 1.1139956 k 0.044877585 m 0.25518233\n",
      "29 Train Loss 311.4104 Test RE 0.0057801186963994545 c 1.1005799 k 0.04504021 m 0.26097524\n",
      "30 Train Loss 306.41318 Test RE 0.005638078480525064 c 1.0783656 k 0.045349065 m 0.2929183\n",
      "31 Train Loss 291.48077 Test RE 0.005874011474054729 c 1.0760542 k 0.045425184 m 0.42627087\n",
      "32 Train Loss 287.2695 Test RE 0.005939234284943654 c 1.1122738 k 0.04509024 m 0.4389707\n",
      "33 Train Loss 286.74652 Test RE 0.005971048099817618 c 1.1258094 k 0.04494332 m 0.4459182\n",
      "34 Train Loss 283.28265 Test RE 0.00530302838114709 c 1.1192183 k 0.0451215 m 0.45967922\n",
      "35 Train Loss 279.58463 Test RE 0.004638067493159231 c 1.1022768 k 0.045160368 m 0.51211226\n",
      "36 Train Loss 262.41205 Test RE 0.004577605355817401 c 1.0771948 k 0.045930214 m 0.91716236\n",
      "37 Train Loss 252.85358 Test RE 0.004876649868694817 c 1.0732528 k 0.045912243 m 1.0551323\n",
      "38 Train Loss 246.51898 Test RE 0.005345299311159854 c 1.0712378 k 0.04598492 m 1.202869\n",
      "39 Train Loss 240.95935 Test RE 0.005165574917292248 c 1.0758375 k 0.046227578 m 1.4206235\n",
      "40 Train Loss 239.6098 Test RE 0.005110999553673892 c 1.0864061 k 0.046089914 m 1.4383876\n",
      "41 Train Loss 235.43533 Test RE 0.004849512369272683 c 1.1094084 k 0.04573503 m 1.439065\n",
      "42 Train Loss 229.3801 Test RE 0.005485348325093303 c 1.1031793 k 0.045854136 m 1.5233456\n",
      "43 Train Loss 224.95732 Test RE 0.0056409157269441046 c 1.114892 k 0.04568783 m 1.6029284\n",
      "44 Train Loss 214.45158 Test RE 0.005394373268575733 c 1.0860606 k 0.04625326 m 1.6681433\n",
      "45 Train Loss 206.81421 Test RE 0.005159600822203158 c 1.0688517 k 0.04648373 m 1.7849917\n",
      "46 Train Loss 198.72835 Test RE 0.0052421557791283956 c 1.0967966 k 0.046314213 m 1.8501345\n",
      "47 Train Loss 185.88017 Test RE 0.004141781195430794 c 1.0778561 k 0.046538018 m 1.933043\n",
      "48 Train Loss 181.8085 Test RE 0.003946656842792524 c 1.0439293 k 0.0469231 m 2.003365\n",
      "49 Train Loss 170.2377 Test RE 0.004052523319526857 c 1.0491431 k 0.047215153 m 2.2030115\n",
      "50 Train Loss 161.06212 Test RE 0.003997701085811962 c 1.0566531 k 0.04702157 m 2.388906\n",
      "51 Train Loss 151.41388 Test RE 0.004233615665196988 c 1.0512716 k 0.04717277 m 2.5153856\n",
      "52 Train Loss 141.40094 Test RE 0.004037939508495604 c 1.0220622 k 0.04762346 m 2.5963786\n",
      "53 Train Loss 127.74024 Test RE 0.003843659959832388 c 1.0336546 k 0.04775328 m 2.7652984\n",
      "54 Train Loss 121.79077 Test RE 0.003787241807585737 c 1.0391271 k 0.047632676 m 2.968942\n",
      "55 Train Loss 117.45477 Test RE 0.003583743094107514 c 1.0162524 k 0.048359133 m 3.2231147\n",
      "56 Train Loss 110.363556 Test RE 0.0035802468093944976 c 1.0389409 k 0.048005093 m 3.4064023\n",
      "57 Train Loss 108.25815 Test RE 0.003486505088493607 c 1.0498297 k 0.047797117 m 3.321268\n",
      "58 Train Loss 102.02873 Test RE 0.003211598975272092 c 1.042247 k 0.047999598 m 3.3027995\n",
      "59 Train Loss 93.129036 Test RE 0.0031718423379210295 c 1.0345099 k 0.048169356 m 3.3733764\n",
      "60 Train Loss 68.90706 Test RE 0.00261562645025543 c 1.0549351 k 0.04806879 m 3.4012063\n",
      "61 Train Loss 56.487835 Test RE 0.002128086766653404 c 1.0271765 k 0.048545327 m 3.4554899\n",
      "62 Train Loss 54.04625 Test RE 0.002140473180702086 c 1.0372427 k 0.048311666 m 3.466212\n",
      "63 Train Loss 52.235886 Test RE 0.0022404556833279707 c 1.0320354 k 0.048427504 m 3.5068934\n",
      "64 Train Loss 50.073917 Test RE 0.0022003552836313113 c 1.0350009 k 0.048452146 m 3.6266742\n",
      "65 Train Loss 49.43734 Test RE 0.0021943337141147254 c 1.034965 k 0.048440974 m 3.7054477\n",
      "66 Train Loss 48.92395 Test RE 0.0022412065817452235 c 1.0323434 k 0.048521373 m 3.777282\n",
      "67 Train Loss 48.063126 Test RE 0.0022681649554697017 c 1.0324887 k 0.048515614 m 3.8042035\n",
      "68 Train Loss 47.41232 Test RE 0.002216825564763556 c 1.0327306 k 0.04842925 m 3.7578766\n",
      "69 Train Loss 46.76796 Test RE 0.0022004136503192785 c 1.0325034 k 0.04838862 m 3.7790384\n",
      "70 Train Loss 43.469543 Test RE 0.0021415748127498577 c 1.0262259 k 0.04871862 m 3.890177\n",
      "71 Train Loss 34.04737 Test RE 0.001654370570647268 c 1.0193996 k 0.048972085 m 4.0993333\n",
      "72 Train Loss 29.437973 Test RE 0.0014398277383627332 c 1.0181307 k 0.04919224 m 4.349172\n",
      "73 Train Loss 27.155878 Test RE 0.0014973997210006628 c 1.009379 k 0.04960229 m 4.653767\n",
      "74 Train Loss 23.000353 Test RE 0.001542651439783243 c 0.9947265 k 0.050155997 m 4.9588375\n",
      "75 Train Loss 21.392605 Test RE 0.0014708294908932508 c 1.0011989 k 0.05004982 m 4.877512\n",
      "76 Train Loss 20.63397 Test RE 0.0014684372736299604 c 0.9965106 k 0.0502584 m 4.992843\n",
      "77 Train Loss 20.098284 Test RE 0.0014178889287706743 c 0.99056715 k 0.050414957 m 5.0804653\n",
      "78 Train Loss 19.617954 Test RE 0.001373894826578635 c 0.9869528 k 0.05054578 m 5.148908\n",
      "79 Train Loss 19.268219 Test RE 0.001343387382823268 c 0.9886259 k 0.050565638 m 5.1300273\n",
      "80 Train Loss 11.389402 Test RE 0.00158294097012352 c 0.99637055 k 0.050404876 m 4.9024677\n",
      "81 Train Loss 7.0236926 Test RE 0.001135814965149709 c 1.0091201 k 0.049678456 m 4.872265\n",
      "82 Train Loss 5.6207924 Test RE 0.0009905097953440576 c 1.009688 k 0.04956099 m 4.7800107\n",
      "83 Train Loss 4.9023147 Test RE 0.0008839026022980374 c 1.0008712 k 0.049864624 m 4.7611313\n",
      "84 Train Loss 4.1929507 Test RE 0.0007947177092667683 c 1.0035559 k 0.04968425 m 4.752457\n",
      "85 Train Loss 3.965544 Test RE 0.0007598573173791779 c 1.0089974 k 0.049566746 m 4.7573934\n",
      "86 Train Loss 3.7781148 Test RE 0.0007430682521127028 c 1.0104016 k 0.049605433 m 4.7733655\n",
      "87 Train Loss 3.160894 Test RE 0.0005913231659776376 c 1.0059501 k 0.049738783 m 4.810978\n",
      "88 Train Loss 2.9535806 Test RE 0.0005332446296934069 c 1.0052679 k 0.049692933 m 4.800047\n",
      "89 Train Loss 2.7139626 Test RE 0.0004979914456889692 c 1.0064821 k 0.049614277 m 4.7954965\n",
      "90 Train Loss 2.6202986 Test RE 0.0004984607220449917 c 1.007262 k 0.049642164 m 4.8034544\n",
      "91 Train Loss 2.613505 Test RE 0.0005014345017426515 c 1.0071431 k 0.049659483 m 4.809062\n",
      "92 Train Loss 2.5841894 Test RE 0.0005021358736956415 c 1.0067946 k 0.049681805 m 4.8315215\n",
      "93 Train Loss 2.512419 Test RE 0.000506512444714156 c 1.0059894 k 0.049696952 m 4.8668356\n",
      "94 Train Loss 2.4021835 Test RE 0.0005021257530300873 c 1.004371 k 0.0497476 m 4.8930492\n",
      "95 Train Loss 2.2925768 Test RE 0.00048158242240810327 c 1.0037992 k 0.04981245 m 4.9302087\n",
      "96 Train Loss 2.121894 Test RE 0.00043724586037796614 c 1.0037707 k 0.049839072 m 4.9876537\n",
      "97 Train Loss 1.8881443 Test RE 0.00036653206271387287 c 1.0047183 k 0.049800776 m 4.9766445\n",
      "98 Train Loss 1.7162865 Test RE 0.0003420959061156391 c 1.0035337 k 0.049808033 m 4.9282985\n",
      "99 Train Loss 1.6868222 Test RE 0.0003407935614530991 c 1.0037286 k 0.049805 m 4.923502\n",
      "100 Train Loss 1.6521789 Test RE 0.0003321339776741591 c 1.0035257 k 0.04983021 m 4.9449134\n",
      "101 Train Loss 1.5883288 Test RE 0.00031134763557128935 c 1.002462 k 0.049869906 m 4.986179\n",
      "102 Train Loss 1.4975097 Test RE 0.0003123158956055013 c 1.0013726 k 0.04990421 m 5.00082\n",
      "103 Train Loss 1.4377322 Test RE 0.00029442031176074855 c 1.0019307 k 0.04989898 m 4.996852\n",
      "104 Train Loss 1.4139879 Test RE 0.00028673851498124534 c 1.0022097 k 0.049910996 m 4.996096\n",
      "105 Train Loss 1.387974 Test RE 0.000282705945998137 c 1.0019126 k 0.049911328 m 4.999044\n",
      "106 Train Loss 1.3777205 Test RE 0.00028543543300599435 c 1.0018306 k 0.049900383 m 4.995497\n",
      "107 Train Loss 1.3709491 Test RE 0.0002874255473104939 c 1.0021737 k 0.04988126 m 4.985424\n",
      "108 Train Loss 1.3689772 Test RE 0.0002872839023064872 c 1.0022562 k 0.0498816 m 4.983964\n",
      "109 Train Loss 1.3669969 Test RE 0.00028677089040578106 c 1.0022572 k 0.049887765 m 4.986607\n",
      "110 Train Loss 1.3647592 Test RE 0.0002871584716834735 c 1.0021142 k 0.049894683 m 4.991206\n",
      "111 Train Loss 1.3621551 Test RE 0.0002885897894607948 c 1.0019337 k 0.049900748 m 4.996212\n",
      "112 Train Loss 1.35211 Test RE 0.00028854918253984357 c 1.0012729 k 0.04991353 m 5.0059175\n",
      "113 Train Loss 1.2989564 Test RE 0.0002695800240252896 c 1.0005369 k 0.049934443 m 5.0237684\n",
      "114 Train Loss 1.2558988 Test RE 0.0002556361818327426 c 1.0007459 k 0.049953237 m 5.040705\n",
      "115 Train Loss 1.2469465 Test RE 0.00024827285363246946 c 1.0010226 k 0.049948893 m 5.0392256\n",
      "116 Train Loss 1.2394074 Test RE 0.00024573989630232877 c 1.0008974 k 0.04994715 m 5.0396414\n",
      "117 Train Loss 1.2233429 Test RE 0.00024339223563147488 c 1.0013518 k 0.04994276 m 5.034795\n",
      "118 Train Loss 1.2176595 Test RE 0.00024272766853199849 c 1.0013587 k 0.04994335 m 5.027745\n",
      "119 Train Loss 1.2163835 Test RE 0.00024459848808928135 c 1.00121 k 0.04994093 m 5.0224156\n",
      "120 Train Loss 1.2152739 Test RE 0.0002453310261030031 c 1.0011426 k 0.04993776 m 5.018807\n",
      "121 Train Loss 1.2152678 Test RE 0.00024532545820460025 c 1.0011426 k 0.04993776 m 5.018807\n",
      "122 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "123 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "124 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "125 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "126 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "127 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "128 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "129 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "130 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "131 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "132 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "133 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "134 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "135 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "136 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "137 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "138 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "139 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "140 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "141 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "142 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "143 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "144 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "145 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "146 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "147 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "148 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "149 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "150 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "151 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "152 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "153 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "154 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "155 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "156 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "157 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "158 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "159 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "160 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "161 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "162 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "163 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "164 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "165 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "166 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "167 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "168 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "169 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "170 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "171 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "172 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "173 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "174 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "175 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "176 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "177 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "178 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "179 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "180 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "181 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "182 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "183 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "184 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "185 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "186 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "187 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "188 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "189 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "190 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "191 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "192 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "193 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "194 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "195 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "196 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "197 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "198 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "199 Train Loss 1.2152667 Test RE 0.0002453248148159603 c 1.0011426 k 0.04993776 m 5.018807\n",
      "Training time: 54.18\n",
      "Training time: 54.18\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 449731.66 Test RE 0.43395508945449696 c -0.02143701 k -0.0745895 m -0.00045064968\n",
      "1 Train Loss 302443.5 Test RE 0.36902910758212254 c -0.021524709 k 0.086575866 m -0.00046325644\n",
      "2 Train Loss 65176.066 Test RE 0.16932186945036698 c -0.003740767 k 0.064013645 m -6.1369814e-05\n",
      "3 Train Loss 48442.188 Test RE 0.14475978584022287 c 0.05307947 k 0.05760314 m 0.0035264785\n",
      "4 Train Loss 38429.67 Test RE 0.12772108025774043 c 0.12628572 k 0.061563153 m 0.007322844\n",
      "5 Train Loss 27487.92 Test RE 0.10746590122059423 c 0.43988642 k 0.069819756 m 0.0177466\n",
      "6 Train Loss 10629.785 Test RE 0.06667497059837939 c 0.6817125 k 0.044426642 m 0.029244069\n",
      "7 Train Loss 2274.088 Test RE 0.027281724729405137 c 1.151302 k 0.043566212 m 0.06173175\n",
      "8 Train Loss 1753.6298 Test RE 0.022318268713853038 c 1.2665145 k 0.040977806 m 0.08292428\n",
      "9 Train Loss 1200.8267 Test RE 0.017423468471507813 c 1.1343653 k 0.04282881 m 0.12847315\n",
      "10 Train Loss 736.9791 Test RE 0.01106127902948694 c 1.1392529 k 0.04654076 m 0.23100299\n",
      "11 Train Loss 560.1821 Test RE 0.00861729099468231 c 1.0578597 k 0.043759298 m 0.28107807\n",
      "12 Train Loss 451.19214 Test RE 0.007047270428966952 c 0.9623978 k 0.04906745 m 0.30348784\n",
      "13 Train Loss 417.74228 Test RE 0.006869554338324465 c 0.93564963 k 0.04673593 m 0.32044917\n",
      "14 Train Loss 393.13977 Test RE 0.007323014053279532 c 0.99008226 k 0.046138335 m 0.34914044\n",
      "15 Train Loss 374.94202 Test RE 0.006392679746321316 c 1.0357537 k 0.046985295 m 0.35287964\n",
      "16 Train Loss 339.54413 Test RE 0.005640153435426167 c 1.1426342 k 0.043525256 m 0.39989677\n",
      "17 Train Loss 302.66205 Test RE 0.005411379915800907 c 1.1383463 k 0.045221996 m 0.47288513\n",
      "18 Train Loss 268.0753 Test RE 0.005705417980542375 c 1.1129428 k 0.045329057 m 0.59142846\n",
      "19 Train Loss 260.64084 Test RE 0.005631452347785197 c 1.1227852 k 0.044238992 m 0.60962623\n",
      "20 Train Loss 249.18036 Test RE 0.005190372318162636 c 1.1611235 k 0.044453427 m 0.65706617\n",
      "21 Train Loss 238.33524 Test RE 0.004999456919759754 c 1.1549282 k 0.046002444 m 0.7429512\n",
      "22 Train Loss 224.00488 Test RE 0.004573671980753744 c 1.118404 k 0.04520835 m 0.7869126\n",
      "23 Train Loss 216.88397 Test RE 0.003803671101729322 c 1.0905647 k 0.045144737 m 0.8916275\n",
      "24 Train Loss 185.02983 Test RE 0.0030221817216164243 c 1.0928302 k 0.0462797 m 1.3674861\n",
      "25 Train Loss 162.33951 Test RE 0.004023682813282473 c 1.1161877 k 0.04623627 m 1.6991358\n",
      "26 Train Loss 141.1454 Test RE 0.004163519738104479 c 1.0603198 k 0.04652491 m 1.9442908\n",
      "27 Train Loss 112.992584 Test RE 0.003895323285786212 c 1.0766501 k 0.047091622 m 2.2293446\n",
      "28 Train Loss 106.75552 Test RE 0.004028054698059765 c 1.0774657 k 0.04650749 m 2.3232183\n",
      "29 Train Loss 95.23595 Test RE 0.003934667291630369 c 1.0747863 k 0.04699585 m 2.4604552\n",
      "30 Train Loss 83.97703 Test RE 0.003525803958065472 c 1.0434482 k 0.047829982 m 2.6857643\n",
      "31 Train Loss 73.33038 Test RE 0.003007824804225377 c 1.0685868 k 0.047282696 m 2.8944616\n",
      "32 Train Loss 65.53535 Test RE 0.0027925325905448404 c 1.0568821 k 0.0480097 m 3.000713\n",
      "33 Train Loss 61.58472 Test RE 0.002333222465198476 c 1.0441833 k 0.04777762 m 3.0850604\n",
      "34 Train Loss 59.849396 Test RE 0.002023131042132573 c 1.0495209 k 0.047976673 m 3.1669366\n",
      "35 Train Loss 45.28609 Test RE 0.0021432359005329167 c 1.0536463 k 0.04803492 m 3.5862327\n",
      "36 Train Loss 32.120445 Test RE 0.0019280116568364512 c 1.0394058 k 0.04825796 m 3.8045628\n",
      "37 Train Loss 24.442514 Test RE 0.0018612734211854772 c 1.0150833 k 0.04933349 m 4.1724467\n",
      "38 Train Loss 19.109177 Test RE 0.0010641776039594375 c 1.0129524 k 0.049470138 m 4.4668555\n",
      "39 Train Loss 15.24503 Test RE 0.001073735021844932 c 1.0182775 k 0.04949446 m 4.684804\n",
      "40 Train Loss 9.593537 Test RE 0.0010119754309696654 c 1.0054868 k 0.049630694 m 4.5900183\n",
      "41 Train Loss 5.90382 Test RE 0.0008224504705392862 c 1.0023234 k 0.049857937 m 4.852029\n",
      "42 Train Loss 5.0242023 Test RE 0.0007852662689211592 c 1.0029129 k 0.050028212 m 4.9618597\n",
      "43 Train Loss 4.3872557 Test RE 0.0006942707309563868 c 1.0015553 k 0.049942546 m 4.957554\n",
      "44 Train Loss 4.084418 Test RE 0.0006448279172588129 c 1.00129 k 0.04990505 m 4.917895\n",
      "45 Train Loss 3.2473278 Test RE 0.00048352020875384866 c 1.0050651 k 0.049738295 m 4.831978\n",
      "46 Train Loss 2.8443985 Test RE 0.0004602318744780238 c 1.0046244 k 0.04978922 m 4.8640995\n",
      "47 Train Loss 2.4754913 Test RE 0.0004231697190365118 c 1.0055383 k 0.049746867 m 4.9197655\n",
      "48 Train Loss 2.3700578 Test RE 0.00042399410726214336 c 1.0061715 k 0.049717166 m 4.943496\n",
      "49 Train Loss 2.1421747 Test RE 0.0003738310819406507 c 1.0051441 k 0.049809106 m 4.9707646\n",
      "50 Train Loss 2.0452614 Test RE 0.0003603713058020786 c 1.0033596 k 0.049848437 m 4.986014\n",
      "51 Train Loss 1.9354467 Test RE 0.00033913402880288726 c 1.0029302 k 0.04985197 m 4.976877\n",
      "52 Train Loss 1.8172134 Test RE 0.00033279563062862374 c 1.002128 k 0.049885195 m 4.983446\n",
      "53 Train Loss 1.6837745 Test RE 0.0003437178570653357 c 1.0010744 k 0.049964547 m 5.024237\n",
      "54 Train Loss 1.5710821 Test RE 0.00033049285082885143 c 1.0007683 k 0.049977187 m 5.0616283\n",
      "55 Train Loss 1.5230416 Test RE 0.0003329264420980266 c 1.0006907 k 0.049976848 m 5.061255\n",
      "56 Train Loss 1.3789248 Test RE 0.0003197460551113397 c 1.000846 k 0.04996638 m 5.0197163\n",
      "57 Train Loss 1.2423718 Test RE 0.0002817224923063123 c 1.0004683 k 0.049953196 m 5.004536\n",
      "58 Train Loss 1.1584289 Test RE 0.0002747664627775941 c 1.001783 k 0.04990525 m 5.004868\n",
      "59 Train Loss 1.0431085 Test RE 0.00025212019594417895 c 1.0018152 k 0.049929205 m 5.0360503\n",
      "60 Train Loss 0.90572494 Test RE 0.0002338298288879814 c 1.0003669 k 0.049962338 m 5.0190845\n",
      "61 Train Loss 0.8623411 Test RE 0.0002346093674958838 c 1.0018213 k 0.04991874 m 4.9850225\n",
      "62 Train Loss 0.8410877 Test RE 0.00023166253881163836 c 1.0022897 k 0.049899016 m 4.972755\n",
      "63 Train Loss 0.8050475 Test RE 0.00021768978684176833 c 1.0023237 k 0.049884472 m 4.9718056\n",
      "64 Train Loss 0.75076103 Test RE 0.00018606946801383203 c 1.0020568 k 0.04990651 m 4.9738107\n",
      "65 Train Loss 0.73512083 Test RE 0.00018827444106948396 c 1.0016996 k 0.04993019 m 4.9736886\n",
      "66 Train Loss 0.7135069 Test RE 0.0002030435996022097 c 1.0007422 k 0.04996727 m 4.976367\n",
      "67 Train Loss 0.67526364 Test RE 0.00021627057158575097 c 0.9998114 k 0.050012797 m 4.982269\n",
      "68 Train Loss 0.6120681 Test RE 0.00017347006642065785 c 1.0000942 k 0.050002553 m 4.9785523\n",
      "69 Train Loss 0.5607277 Test RE 0.0001562442510037645 c 1.0017115 k 0.049945753 m 4.972995\n",
      "70 Train Loss 0.5021875 Test RE 0.00014185346031576575 c 1.0000063 k 0.049986146 m 4.985091\n",
      "71 Train Loss 0.42649618 Test RE 9.948904726267446e-05 c 1.0009403 k 0.049969316 m 4.9860563\n",
      "72 Train Loss 0.40017608 Test RE 8.064234040605312e-05 c 1.0013345 k 0.04996547 m 4.984097\n",
      "73 Train Loss 0.37169832 Test RE 9.937392544590409e-05 c 1.0006407 k 0.049972884 m 4.980421\n",
      "74 Train Loss 0.34994224 Test RE 8.263120364234728e-05 c 1.000833 k 0.049964506 m 4.9813886\n",
      "75 Train Loss 0.33988148 Test RE 8.066847666108608e-05 c 1.0009499 k 0.04996852 m 4.989234\n",
      "76 Train Loss 0.31927243 Test RE 8.991650438273775e-05 c 1.0002214 k 0.049990524 m 4.999385\n",
      "77 Train Loss 0.29888436 Test RE 9.832347421020974e-05 c 0.99962544 k 0.04999501 m 5.004162\n",
      "78 Train Loss 0.27380946 Test RE 9.819959026445053e-05 c 1.0007414 k 0.049972497 m 5.0082664\n",
      "79 Train Loss 0.24959578 Test RE 8.759930585120633e-05 c 1.001266 k 0.049964465 m 5.0061884\n",
      "80 Train Loss 0.23353647 Test RE 7.50613326002774e-05 c 1.0008483 k 0.04995547 m 4.997045\n",
      "81 Train Loss 0.22114047 Test RE 6.783134063904393e-05 c 1.001192 k 0.049942866 m 4.9859624\n",
      "82 Train Loss 0.20619842 Test RE 5.7560747562207807e-05 c 1.0012189 k 0.049954753 m 4.9901996\n",
      "83 Train Loss 0.1933334 Test RE 5.662045514429308e-05 c 1.0003403 k 0.049982235 m 5.006673\n",
      "84 Train Loss 0.1769482 Test RE 5.2804201501478864e-05 c 0.9998164 k 0.04999172 m 5.012291\n",
      "85 Train Loss 0.15408538 Test RE 5.119910845615503e-05 c 1.0005274 k 0.04997885 m 5.0025034\n",
      "86 Train Loss 0.1438232 Test RE 4.0010639625885736e-05 c 1.0003705 k 0.049987853 m 5.001263\n",
      "87 Train Loss 0.13811295 Test RE 3.9285785834814444e-05 c 1.0000763 k 0.04999477 m 4.9995594\n",
      "88 Train Loss 0.13472062 Test RE 3.945084565316209e-05 c 0.99999774 k 0.049995262 m 4.9981513\n",
      "89 Train Loss 0.12855023 Test RE 3.431118349641858e-05 c 0.99981034 k 0.05000432 m 4.999545\n",
      "90 Train Loss 0.12529379 Test RE 3.3353613818775075e-05 c 0.99987847 k 0.050005488 m 5.001975\n",
      "91 Train Loss 0.12448078 Test RE 3.5784262750382434e-05 c 0.99996936 k 0.050003793 m 5.0030594\n",
      "92 Train Loss 0.12278194 Test RE 3.774265083706669e-05 c 1.0001546 k 0.04999865 m 5.002362\n",
      "93 Train Loss 0.119968355 Test RE 3.315976953835562e-05 c 0.99976057 k 0.050003994 m 5.005116\n",
      "94 Train Loss 0.114266805 Test RE 2.9078776760252576e-05 c 0.99952847 k 0.050007097 m 5.005473\n",
      "95 Train Loss 0.10064565 Test RE 4.017861671702884e-05 c 1.0000277 k 0.049988825 m 4.9984674\n",
      "96 Train Loss 0.093875475 Test RE 4.9052803406736745e-05 c 1.0004649 k 0.049983416 m 4.9989214\n",
      "97 Train Loss 0.08604286 Test RE 5.607283104617132e-05 c 1.0002252 k 0.049989965 m 5.007172\n",
      "98 Train Loss 0.08040321 Test RE 6.202146304880954e-05 c 0.9998338 k 0.04999324 m 5.0088887\n",
      "99 Train Loss 0.06888058 Test RE 6.562486757230532e-05 c 1.0002309 k 0.04997967 m 4.9985633\n",
      "100 Train Loss 0.06256205 Test RE 6.329686463993582e-05 c 1.0007228 k 0.049971964 m 4.996821\n",
      "101 Train Loss 0.05663349 Test RE 6.025197343592689e-05 c 1.0001363 k 0.049986202 m 5.004857\n",
      "102 Train Loss 0.052053437 Test RE 5.7546303744531874e-05 c 1.0000354 k 0.049985353 m 5.0029325\n",
      "103 Train Loss 0.04969093 Test RE 5.735875949269651e-05 c 1.000213 k 0.049981568 m 4.9979343\n",
      "104 Train Loss 0.0455164 Test RE 5.394373718981316e-05 c 1.0005162 k 0.049978253 m 4.9967394\n",
      "105 Train Loss 0.043820575 Test RE 4.851997778472174e-05 c 1.0004067 k 0.04998281 m 5.000765\n",
      "106 Train Loss 0.042434677 Test RE 4.703368708017822e-05 c 1.0001608 k 0.049986683 m 5.0011873\n",
      "107 Train Loss 0.041254457 Test RE 4.4840840072402576e-05 c 1.0003085 k 0.049984816 m 5.001029\n",
      "108 Train Loss 0.039764307 Test RE 3.951660338425757e-05 c 1.0002859 k 0.049986992 m 5.0028825\n",
      "109 Train Loss 0.038070403 Test RE 3.0681764386915004e-05 c 1.000193 k 0.049990587 m 5.0020895\n",
      "110 Train Loss 0.037535403 Test RE 2.6903275327530245e-05 c 1.0001515 k 0.04999142 m 5.0005856\n",
      "111 Train Loss 0.037116226 Test RE 2.1626595602120594e-05 c 1.0001398 k 0.049992688 m 5.0000396\n",
      "112 Train Loss 0.036598843 Test RE 1.5675118079187495e-05 c 1.0001342 k 0.04999583 m 5.0005608\n",
      "113 Train Loss 0.03598098 Test RE 1.364503948197397e-05 c 1.000105 k 0.04999905 m 5.0018945\n",
      "114 Train Loss 0.034886282 Test RE 1.1823185465193267e-05 c 1.0000256 k 0.050000604 m 5.002379\n",
      "115 Train Loss 0.034701593 Test RE 1.2576646007781704e-05 c 0.9999903 k 0.050000366 m 5.0013847\n",
      "116 Train Loss 0.034581464 Test RE 1.2696869827388534e-05 c 0.9999743 k 0.05000071 m 5.0009804\n",
      "117 Train Loss 0.034044046 Test RE 1.2597800031340194e-05 c 0.9999584 k 0.050000746 m 5.000429\n",
      "118 Train Loss 0.033522252 Test RE 1.3354921371743003e-05 c 1.0001352 k 0.04999906 m 5.001216\n",
      "119 Train Loss 0.033331025 Test RE 1.3902995452444375e-05 c 1.0001005 k 0.0499999 m 5.001986\n",
      "120 Train Loss 0.032972947 Test RE 1.3364884679396934e-05 c 0.99985343 k 0.05000479 m 5.002823\n",
      "121 Train Loss 0.03266372 Test RE 1.3014717789996817e-05 c 0.99985015 k 0.050005004 m 5.0017333\n",
      "122 Train Loss 0.03251282 Test RE 1.3356282595188988e-05 c 0.99997616 k 0.050002348 m 5.0003757\n",
      "123 Train Loss 0.03247646 Test RE 1.3232847165977561e-05 c 1.000004 k 0.050002113 m 4.9998937\n",
      "124 Train Loss 0.032399245 Test RE 1.2818141512548577e-05 c 1.0000366 k 0.050000723 m 4.9993677\n",
      "125 Train Loss 0.032132193 Test RE 1.1992674080049033e-05 c 1.0000392 k 0.049999017 m 4.9989877\n",
      "126 Train Loss 0.03182971 Test RE 1.3003040280893778e-05 c 1.0000007 k 0.049999345 m 4.9996157\n",
      "127 Train Loss 0.031801395 Test RE 1.3132174715009996e-05 c 0.9999987 k 0.049999256 m 4.9996705\n",
      "128 Train Loss 0.031688727 Test RE 1.3271379499743608e-05 c 1.000006 k 0.049998205 m 4.999333\n",
      "129 Train Loss 0.03158542 Test RE 1.3363988963215354e-05 c 1.0000262 k 0.049997427 m 4.999101\n",
      "130 Train Loss 0.030787319 Test RE 1.3177549290339509e-05 c 1.0001019 k 0.04999426 m 4.99728\n",
      "131 Train Loss 0.02949208 Test RE 1.1124219723292889e-05 c 1.000096 k 0.04999354 m 4.996455\n",
      "132 Train Loss 0.028676694 Test RE 1.121590991027671e-05 c 1.0001181 k 0.049996484 m 4.999173\n",
      "133 Train Loss 0.028135668 Test RE 1.3322078107668274e-05 c 1.0000534 k 0.04999976 m 5.001177\n",
      "134 Train Loss 0.027758343 Test RE 1.3052421697398483e-05 c 0.9999936 k 0.050000384 m 5.000997\n",
      "135 Train Loss 0.02754863 Test RE 1.2461321337010918e-05 c 1.0000092 k 0.049999394 m 4.9995375\n",
      "136 Train Loss 0.027350381 Test RE 1.2841923973151253e-05 c 1.0000483 k 0.049997896 m 4.9983673\n",
      "137 Train Loss 0.027240178 Test RE 1.4213595835196606e-05 c 1.0000815 k 0.049997028 m 4.9983015\n",
      "138 Train Loss 0.027187588 Test RE 1.4824487624340375e-05 c 1.0000947 k 0.049996532 m 4.99838\n",
      "139 Train Loss 0.027117692 Test RE 1.4319835768288023e-05 c 1.0001032 k 0.049995854 m 4.9981933\n",
      "140 Train Loss 0.026973834 Test RE 1.1295605683245e-05 c 1.0001038 k 0.049995083 m 4.9977856\n",
      "141 Train Loss 0.026863556 Test RE 9.304111821795612e-06 c 1.0001175 k 0.04999518 m 4.9978228\n",
      "142 Train Loss 0.026698813 Test RE 8.486117009376785e-06 c 1.0001447 k 0.049995154 m 4.9985003\n",
      "143 Train Loss 0.026313964 Test RE 1.136509755178863e-05 c 1.0001446 k 0.04999434 m 4.999535\n",
      "144 Train Loss 0.025803559 Test RE 1.2641601828752659e-05 c 1.0000798 k 0.04999483 m 4.998975\n",
      "145 Train Loss 0.024942214 Test RE 9.778155772602523e-06 c 1.0001149 k 0.049995407 m 4.9978123\n",
      "146 Train Loss 0.024405189 Test RE 9.385241923633921e-06 c 1.000163 k 0.04999507 m 4.998006\n",
      "147 Train Loss 0.023904286 Test RE 8.178993221789575e-06 c 1.0000074 k 0.049997926 m 4.9999323\n",
      "148 Train Loss 0.023420222 Test RE 6.749935434788246e-06 c 0.99991596 k 0.049998887 m 4.999193\n",
      "149 Train Loss 0.023026811 Test RE 9.391749315251747e-06 c 1.0000793 k 0.049995776 m 4.9983225\n",
      "150 Train Loss 0.022890612 Test RE 9.438962363533336e-06 c 1.0001476 k 0.04999453 m 4.998623\n",
      "151 Train Loss 0.022862105 Test RE 9.182059684933034e-06 c 1.0001478 k 0.04999438 m 4.998713\n",
      "152 Train Loss 0.022862105 Test RE 9.182059684933034e-06 c 1.0001478 k 0.04999438 m 4.998713\n",
      "153 Train Loss 0.022862105 Test RE 9.182059684933034e-06 c 1.0001478 k 0.04999438 m 4.998713\n",
      "154 Train Loss 0.022862105 Test RE 9.182059684933034e-06 c 1.0001478 k 0.04999438 m 4.998713\n",
      "155 Train Loss 0.022862105 Test RE 9.182059684933034e-06 c 1.0001478 k 0.04999438 m 4.998713\n",
      "156 Train Loss 0.022862105 Test RE 9.182059684933034e-06 c 1.0001478 k 0.04999438 m 4.998713\n",
      "157 Train Loss 0.022862105 Test RE 9.182059684933034e-06 c 1.0001478 k 0.04999438 m 4.998713\n",
      "158 Train Loss 0.022862105 Test RE 9.182059684933034e-06 c 1.0001478 k 0.04999438 m 4.998713\n",
      "159 Train Loss 0.022862105 Test RE 9.182059684933034e-06 c 1.0001478 k 0.04999438 m 4.998713\n",
      "160 Train Loss 0.022862105 Test RE 9.182059684933034e-06 c 1.0001478 k 0.04999438 m 4.998713\n",
      "161 Train Loss 0.022862105 Test RE 9.182059684933034e-06 c 1.0001478 k 0.04999438 m 4.998713\n",
      "162 Train Loss 0.022862105 Test RE 9.182059684933034e-06 c 1.0001478 k 0.04999438 m 4.998713\n",
      "163 Train Loss 0.022862105 Test RE 9.182059684933034e-06 c 1.0001478 k 0.04999438 m 4.998713\n",
      "164 Train Loss 0.022862105 Test RE 9.182059684933034e-06 c 1.0001478 k 0.04999438 m 4.998713\n",
      "165 Train Loss 0.022862105 Test RE 9.182059684933034e-06 c 1.0001478 k 0.04999438 m 4.998713\n",
      "166 Train Loss 0.022862105 Test RE 9.182059684933034e-06 c 1.0001478 k 0.04999438 m 4.998713\n",
      "167 Train Loss 0.022862105 Test RE 9.182059684933034e-06 c 1.0001478 k 0.04999438 m 4.998713\n",
      "168 Train Loss 0.022862105 Test RE 9.182059684933034e-06 c 1.0001478 k 0.04999438 m 4.998713\n",
      "169 Train Loss 0.022862105 Test RE 9.182059684933034e-06 c 1.0001478 k 0.04999438 m 4.998713\n",
      "170 Train Loss 0.022862105 Test RE 9.182059684933034e-06 c 1.0001478 k 0.04999438 m 4.998713\n",
      "171 Train Loss 0.022862105 Test RE 9.182059684933034e-06 c 1.0001478 k 0.04999438 m 4.998713\n",
      "172 Train Loss 0.022862105 Test RE 9.182059684933034e-06 c 1.0001478 k 0.04999438 m 4.998713\n",
      "173 Train Loss 0.022862105 Test RE 9.182059684933034e-06 c 1.0001478 k 0.04999438 m 4.998713\n",
      "174 Train Loss 0.022862105 Test RE 9.182059684933034e-06 c 1.0001478 k 0.04999438 m 4.998713\n",
      "175 Train Loss 0.022862105 Test RE 9.182059684933034e-06 c 1.0001478 k 0.04999438 m 4.998713\n",
      "176 Train Loss 0.022862105 Test RE 9.182059684933034e-06 c 1.0001478 k 0.04999438 m 4.998713\n",
      "177 Train Loss 0.022862105 Test RE 9.182059684933034e-06 c 1.0001478 k 0.04999438 m 4.998713\n",
      "178 Train Loss 0.022862105 Test RE 9.182059684933034e-06 c 1.0001478 k 0.04999438 m 4.998713\n",
      "179 Train Loss 0.022862105 Test RE 9.182059684933034e-06 c 1.0001478 k 0.04999438 m 4.998713\n",
      "180 Train Loss 0.022862105 Test RE 9.182059684933034e-06 c 1.0001478 k 0.04999438 m 4.998713\n",
      "181 Train Loss 0.022862105 Test RE 9.182059684933034e-06 c 1.0001478 k 0.04999438 m 4.998713\n",
      "182 Train Loss 0.022862105 Test RE 9.182059684933034e-06 c 1.0001478 k 0.04999438 m 4.998713\n",
      "183 Train Loss 0.022862105 Test RE 9.182059684933034e-06 c 1.0001478 k 0.04999438 m 4.998713\n",
      "184 Train Loss 0.022862105 Test RE 9.182059684933034e-06 c 1.0001478 k 0.04999438 m 4.998713\n",
      "185 Train Loss 0.022862105 Test RE 9.182059684933034e-06 c 1.0001478 k 0.04999438 m 4.998713\n",
      "186 Train Loss 0.022862105 Test RE 9.182059684933034e-06 c 1.0001478 k 0.04999438 m 4.998713\n",
      "187 Train Loss 0.022862105 Test RE 9.182059684933034e-06 c 1.0001478 k 0.04999438 m 4.998713\n",
      "188 Train Loss 0.022862105 Test RE 9.182059684933034e-06 c 1.0001478 k 0.04999438 m 4.998713\n",
      "189 Train Loss 0.022862105 Test RE 9.182059684933034e-06 c 1.0001478 k 0.04999438 m 4.998713\n",
      "190 Train Loss 0.022862105 Test RE 9.182059684933034e-06 c 1.0001478 k 0.04999438 m 4.998713\n",
      "191 Train Loss 0.022862105 Test RE 9.182059684933034e-06 c 1.0001478 k 0.04999438 m 4.998713\n",
      "192 Train Loss 0.022862105 Test RE 9.182059684933034e-06 c 1.0001478 k 0.04999438 m 4.998713\n",
      "193 Train Loss 0.022862105 Test RE 9.182059684933034e-06 c 1.0001478 k 0.04999438 m 4.998713\n",
      "194 Train Loss 0.022862105 Test RE 9.182059684933034e-06 c 1.0001478 k 0.04999438 m 4.998713\n",
      "195 Train Loss 0.022862105 Test RE 9.182059684933034e-06 c 1.0001478 k 0.04999438 m 4.998713\n",
      "196 Train Loss 0.022862105 Test RE 9.182059684933034e-06 c 1.0001478 k 0.04999438 m 4.998713\n",
      "197 Train Loss 0.022862105 Test RE 9.182059684933034e-06 c 1.0001478 k 0.04999438 m 4.998713\n",
      "198 Train Loss 0.022862105 Test RE 9.182059684933034e-06 c 1.0001478 k 0.04999438 m 4.998713\n",
      "199 Train Loss 0.022862105 Test RE 9.182059684933034e-06 c 1.0001478 k 0.04999438 m 4.998713\n",
      "Training time: 59.33\n",
      "Training time: 59.33\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 275553.38 Test RE 0.35245244713229806 c 0.00527917 k 0.070358045 m -0.00036860703\n",
      "1 Train Loss 255417.2 Test RE 0.3392525785921396 c 0.0049455175 k 0.08048679 m -0.00033059364\n",
      "2 Train Loss 78917.02 Test RE 0.18673244629243294 c 0.005672862 k 0.055267826 m -0.00031104533\n",
      "3 Train Loss 41964.17 Test RE 0.13314378561962723 c 0.02535612 k 0.06757374 m -3.2804972e-05\n",
      "4 Train Loss 8958.132 Test RE 0.0494666781929491 c 0.100641824 k 0.048118103 m 0.0011896362\n",
      "5 Train Loss 4366.532 Test RE 0.02317856231289388 c 0.12148443 k 0.060732257 m 0.0015520798\n",
      "6 Train Loss 3401.9346 Test RE 0.022605393514798025 c 0.14974119 k 0.05845388 m 0.0024880215\n",
      "7 Train Loss 2878.9692 Test RE 0.02127006931100454 c 0.23911211 k 0.057851266 m 0.0060632955\n",
      "8 Train Loss 2004.4006 Test RE 0.022689064977264802 c 0.59535295 k 0.050467037 m 0.021697363\n",
      "9 Train Loss 1565.9718 Test RE 0.022475681735784226 c 0.8651745 k 0.049129065 m 0.034427974\n",
      "10 Train Loss 1223.8931 Test RE 0.020120820779311665 c 1.2378529 k 0.041397214 m 0.05461881\n",
      "11 Train Loss 988.16846 Test RE 0.017830328540902628 c 1.1772938 k 0.043183535 m 0.056403108\n",
      "12 Train Loss 856.86005 Test RE 0.016249446206354306 c 1.2499858 k 0.042384885 m 0.077068254\n",
      "13 Train Loss 722.0223 Test RE 0.013849554879334708 c 1.3645768 k 0.040729467 m 0.13963525\n",
      "14 Train Loss 536.3567 Test RE 0.012010393552323077 c 1.3088769 k 0.04141276 m 0.27147862\n",
      "15 Train Loss 491.68423 Test RE 0.011917859203342363 c 1.1716568 k 0.043977764 m 0.26533362\n",
      "16 Train Loss 464.29364 Test RE 0.011636927717161858 c 1.1230658 k 0.04436729 m 0.25203848\n",
      "17 Train Loss 453.04883 Test RE 0.011515861473982071 c 1.1110973 k 0.044792436 m 0.25977653\n",
      "18 Train Loss 445.02466 Test RE 0.011407525619148895 c 1.1132575 k 0.044717222 m 0.2734735\n",
      "19 Train Loss 442.2483 Test RE 0.011328338224236782 c 1.1213874 k 0.044575296 m 0.27722183\n",
      "20 Train Loss 440.4736 Test RE 0.01122951363807282 c 1.1237456 k 0.04456597 m 0.27541006\n",
      "21 Train Loss 436.51068 Test RE 0.011122985502906309 c 1.1114703 k 0.04477947 m 0.26914385\n",
      "22 Train Loss 433.74365 Test RE 0.011084337533199472 c 1.1121182 k 0.044684846 m 0.27219924\n",
      "23 Train Loss 431.54535 Test RE 0.01104581647024396 c 1.1187834 k 0.044655442 m 0.27445257\n",
      "24 Train Loss 429.70438 Test RE 0.011009462886297373 c 1.1228733 k 0.044576503 m 0.26537332\n",
      "25 Train Loss 427.7467 Test RE 0.011001925266883029 c 1.1208218 k 0.044628512 m 0.25333288\n",
      "26 Train Loss 422.8999 Test RE 0.010998276979368315 c 1.1193328 k 0.04465461 m 0.22978541\n",
      "27 Train Loss 403.17337 Test RE 0.010750874493500236 c 1.135735 k 0.044342954 m 0.16273437\n",
      "28 Train Loss 378.25275 Test RE 0.01003774258205553 c 1.1525433 k 0.04407215 m 0.12948608\n",
      "29 Train Loss 366.50546 Test RE 0.00976724730001693 c 1.1464514 k 0.044179052 m 0.10475188\n",
      "30 Train Loss 356.54553 Test RE 0.009334612997283654 c 1.1174641 k 0.044624176 m 0.070914134\n",
      "31 Train Loss 354.19623 Test RE 0.009173842773973803 c 1.1201404 k 0.04452822 m 0.04035768\n",
      "32 Train Loss 346.10675 Test RE 0.007979860574935086 c 1.1398724 k 0.04422812 m -0.040807374\n",
      "33 Train Loss 340.27405 Test RE 0.007650724345784367 c 1.1175319 k 0.044618152 m -0.042405777\n",
      "34 Train Loss 328.41693 Test RE 0.007157150149531641 c 1.0954076 k 0.044856533 m -0.01802782\n",
      "35 Train Loss 317.2964 Test RE 0.006975773387750235 c 1.1102093 k 0.044526104 m 0.012491483\n",
      "36 Train Loss 310.91473 Test RE 0.007046323922811151 c 1.1144456 k 0.04464098 m 0.025853762\n",
      "37 Train Loss 309.9063 Test RE 0.0070195547836161285 c 1.1263883 k 0.044469744 m 0.028473424\n",
      "38 Train Loss 306.91772 Test RE 0.006977232797582534 c 1.1416978 k 0.04419806 m 0.052585185\n",
      "39 Train Loss 301.85992 Test RE 0.006986831769621277 c 1.1196928 k 0.044579268 m 0.117200404\n",
      "40 Train Loss 300.05032 Test RE 0.0069720734904025585 c 1.1195376 k 0.0446718 m 0.14644395\n",
      "41 Train Loss 298.51175 Test RE 0.0069343726917714035 c 1.1287287 k 0.044530183 m 0.16817297\n",
      "42 Train Loss 295.9049 Test RE 0.00692167324049329 c 1.1251234 k 0.044565156 m 0.20239548\n",
      "43 Train Loss 289.089 Test RE 0.0066879917972722325 c 1.1189196 k 0.044688735 m 0.25342393\n",
      "44 Train Loss 284.10312 Test RE 0.006507998240989475 c 1.1326517 k 0.04448378 m 0.3041277\n",
      "45 Train Loss 282.78607 Test RE 0.006524144823068097 c 1.1220889 k 0.04473007 m 0.31148034\n",
      "46 Train Loss 282.20972 Test RE 0.006542408218279315 c 1.1194787 k 0.044807903 m 0.3214202\n",
      "47 Train Loss 280.81845 Test RE 0.00657710902819526 c 1.1271675 k 0.0446838 m 0.36627504\n",
      "48 Train Loss 278.20236 Test RE 0.006423480860429112 c 1.1364692 k 0.04461774 m 0.44344345\n",
      "49 Train Loss 275.05545 Test RE 0.00614473463321212 c 1.111995 k 0.04500469 m 0.50902444\n",
      "50 Train Loss 274.142 Test RE 0.0061162297562669474 c 1.1075885 k 0.045095146 m 0.55268544\n",
      "51 Train Loss 273.5529 Test RE 0.006100645226525372 c 1.111537 k 0.04506206 m 0.58655554\n",
      "52 Train Loss 269.45264 Test RE 0.006029758698879761 c 1.125391 k 0.044884626 m 0.6581136\n",
      "53 Train Loss 244.23894 Test RE 0.005227768327905712 c 1.1257294 k 0.044809602 m 0.6360289\n",
      "54 Train Loss 240.11317 Test RE 0.0050895755932152025 c 1.131035 k 0.044988528 m 0.66942996\n",
      "55 Train Loss 232.28635 Test RE 0.005085038857959768 c 1.1176281 k 0.045089714 m 0.74880296\n",
      "56 Train Loss 220.06133 Test RE 0.0049123160087858496 c 1.1017591 k 0.045362618 m 0.9465761\n",
      "57 Train Loss 212.90524 Test RE 0.004750025669300745 c 1.1192459 k 0.045371633 m 1.0709059\n",
      "58 Train Loss 206.43604 Test RE 0.004753561566021852 c 1.1149383 k 0.045567594 m 1.1562368\n",
      "59 Train Loss 193.60677 Test RE 0.005036386014056789 c 1.1208973 k 0.045103755 m 1.345531\n",
      "60 Train Loss 173.1436 Test RE 0.004471989589708532 c 1.1111428 k 0.046301626 m 1.6724821\n",
      "61 Train Loss 155.37267 Test RE 0.0042708224536052285 c 1.0721108 k 0.046667915 m 1.8786657\n",
      "62 Train Loss 141.28116 Test RE 0.0042005543394706395 c 1.0875263 k 0.0467708 m 1.9339026\n",
      "63 Train Loss 123.9386 Test RE 0.0031732672516278987 c 1.0657883 k 0.04670874 m 1.9803897\n",
      "64 Train Loss 94.11204 Test RE 0.0033304503997607744 c 1.0804801 k 0.046828087 m 2.431304\n",
      "65 Train Loss 67.015686 Test RE 0.0035171733642093157 c 1.0648385 k 0.047598716 m 3.2408626\n",
      "66 Train Loss 51.324913 Test RE 0.0032664860717033723 c 0.99960876 k 0.049047433 m 4.138406\n",
      "67 Train Loss 39.274048 Test RE 0.0028498305143156767 c 1.0280927 k 0.049509767 m 4.787167\n",
      "68 Train Loss 30.274868 Test RE 0.0024388948598807318 c 1.0031346 k 0.049617223 m 4.892987\n",
      "69 Train Loss 28.512794 Test RE 0.0023553860149545048 c 1.008209 k 0.04948593 m 4.801145\n",
      "70 Train Loss 23.68069 Test RE 0.00209703948617559 c 1.016092 k 0.04952898 m 4.692196\n",
      "71 Train Loss 18.274397 Test RE 0.0017474321988313459 c 0.9976966 k 0.049754243 m 4.7097673\n",
      "72 Train Loss 15.202611 Test RE 0.0014017801729149602 c 1.003177 k 0.049782265 m 4.9266214\n",
      "73 Train Loss 14.057239 Test RE 0.0013526136218094467 c 1.0138097 k 0.049579576 m 4.9435987\n",
      "74 Train Loss 8.514234 Test RE 0.0009772598210950497 c 1.011525 k 0.049486984 m 4.7807817\n",
      "75 Train Loss 6.6726794 Test RE 0.0007421587268612994 c 1.0063261 k 0.049611583 m 4.719097\n",
      "76 Train Loss 6.130975 Test RE 0.000735406891743249 c 1.0079474 k 0.049639657 m 4.7383327\n",
      "77 Train Loss 5.866105 Test RE 0.0006725933418099382 c 1.0062529 k 0.049679212 m 4.805702\n",
      "78 Train Loss 5.6929607 Test RE 0.0006445530602316512 c 1.0026555 k 0.049769685 m 4.887868\n",
      "79 Train Loss 5.213668 Test RE 0.0006874906057169072 c 0.99955964 k 0.049906597 m 4.957315\n",
      "80 Train Loss 4.495905 Test RE 0.0006080317862900598 c 1.0025065 k 0.04983329 m 4.8851733\n",
      "81 Train Loss 3.5871782 Test RE 0.000544730352294581 c 1.0047206 k 0.04971259 m 4.848302\n",
      "82 Train Loss 2.8545413 Test RE 0.0005559037094068655 c 1.0038791 k 0.04982548 m 4.944897\n",
      "83 Train Loss 2.331773 Test RE 0.0004902066340205546 c 0.9995434 k 0.050009307 m 5.0398407\n",
      "84 Train Loss 2.0627725 Test RE 0.0004048232588723592 c 1.0009063 k 0.04993818 m 5.0160193\n",
      "85 Train Loss 1.9891111 Test RE 0.0004129907724130302 c 1.0009099 k 0.04993849 m 5.0323715\n",
      "86 Train Loss 1.9264717 Test RE 0.00046118202798698396 c 1.0010068 k 0.04992477 m 5.01339\n",
      "87 Train Loss 1.7971797 Test RE 0.0004558962231381984 c 1.002103 k 0.049891036 m 4.9818945\n",
      "88 Train Loss 1.5341699 Test RE 0.0003916221101296084 c 1.000707 k 0.049930096 m 4.989863\n",
      "89 Train Loss 1.4384036 Test RE 0.0003904351485489842 c 1.0002745 k 0.0499775 m 5.0032105\n",
      "90 Train Loss 1.4235797 Test RE 0.00038437514233419993 c 1.0002834 k 0.049978003 m 5.0004845\n",
      "91 Train Loss 1.4095935 Test RE 0.00037321038806397176 c 1.000301 k 0.049960185 m 4.989114\n",
      "92 Train Loss 1.3676528 Test RE 0.000370644090057572 c 1.0008408 k 0.049937863 m 4.9786506\n",
      "93 Train Loss 1.3512366 Test RE 0.0003786556863107606 c 1.0010071 k 0.049948316 m 4.9815907\n",
      "94 Train Loss 1.343779 Test RE 0.0003815664835502812 c 1.0011561 k 0.049947426 m 4.9828315\n",
      "95 Train Loss 1.3054247 Test RE 0.0003765159841460775 c 1.0009882 k 0.049944554 m 4.9731994\n",
      "96 Train Loss 1.2286515 Test RE 0.0003553151664927118 c 0.99997413 k 0.04997489 m 4.996364\n",
      "97 Train Loss 1.1986012 Test RE 0.00034421291624008645 c 0.9997056 k 0.05000536 m 5.0264635\n",
      "98 Train Loss 1.1810356 Test RE 0.0003274549953859867 c 0.99965316 k 0.050014447 m 5.0403924\n",
      "99 Train Loss 1.1532748 Test RE 0.0003214655327638594 c 0.99931186 k 0.050010543 m 5.0246606\n",
      "100 Train Loss 1.1084694 Test RE 0.00031944003529211064 c 0.99959916 k 0.050013587 m 5.005608\n",
      "101 Train Loss 1.0792848 Test RE 0.00030579395197567966 c 0.9999621 k 0.05001572 m 5.0030293\n",
      "102 Train Loss 1.0683854 Test RE 0.0003011956386471389 c 0.9992072 k 0.05003674 m 5.0108185\n",
      "103 Train Loss 1.0554335 Test RE 0.0002964723518256528 c 0.99807364 k 0.0500564 m 5.0212936\n",
      "104 Train Loss 1.0314417 Test RE 0.00027963678119648764 c 0.9988579 k 0.05003785 m 5.018418\n",
      "105 Train Loss 0.9989567 Test RE 0.000262245928614591 c 1.0003425 k 0.05001143 m 5.0105524\n",
      "106 Train Loss 0.93779206 Test RE 0.00026967847098960103 c 0.99896944 k 0.050034154 m 5.010706\n",
      "107 Train Loss 0.9078429 Test RE 0.0002809554477948771 c 0.99700063 k 0.05005449 m 5.012566\n",
      "108 Train Loss 0.838362 Test RE 0.00028344534892269337 c 0.9992307 k 0.050009944 m 5.017319\n",
      "109 Train Loss 0.7930068 Test RE 0.0002640403526006014 c 1.002023 k 0.0499669 m 5.02391\n",
      "110 Train Loss 0.673681 Test RE 0.00022350536860702767 c 1.0020505 k 0.04996157 m 5.0283704\n",
      "111 Train Loss 0.6065849 Test RE 0.00022375292071216888 c 0.99991405 k 0.049972657 m 5.013923\n",
      "112 Train Loss 0.5838055 Test RE 0.0002187472676141489 c 0.9998303 k 0.049964283 m 5.0048113\n",
      "113 Train Loss 0.57626 Test RE 0.00021670300363986012 c 1.0010501 k 0.049938265 m 4.996523\n",
      "114 Train Loss 0.5694184 Test RE 0.0002215311079799462 c 1.0016369 k 0.04992465 m 4.9892716\n",
      "115 Train Loss 0.5647547 Test RE 0.0002193533587931676 c 1.0011822 k 0.049931232 m 4.990345\n",
      "116 Train Loss 0.5593668 Test RE 0.0002101351475712257 c 1.0009063 k 0.049934417 m 4.9921203\n",
      "117 Train Loss 0.53447086 Test RE 0.00018885527146888796 c 1.0010924 k 0.049934026 m 4.998863\n",
      "118 Train Loss 0.4898365 Test RE 0.00018846843165374125 c 1.0018429 k 0.049920045 m 4.986591\n",
      "119 Train Loss 0.46640232 Test RE 0.00018807975300287244 c 1.0009705 k 0.049931914 m 4.9783015\n",
      "120 Train Loss 0.4334924 Test RE 0.00017780205102668688 c 1.0007591 k 0.04993983 m 4.9830036\n",
      "121 Train Loss 0.41802964 Test RE 0.0001778838208289099 c 1.0010816 k 0.049936224 m 4.9888473\n",
      "122 Train Loss 0.41371262 Test RE 0.00017761984463001532 c 1.0012406 k 0.049935013 m 4.9886184\n",
      "123 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "124 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "125 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "126 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "127 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "128 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "129 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "130 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "131 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "132 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "133 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "134 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "135 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "136 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "137 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "138 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "139 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "140 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "141 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "142 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "143 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "144 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "145 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "146 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "147 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "148 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "149 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "150 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "151 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "152 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "153 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "154 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "155 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "156 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "157 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "158 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "159 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "160 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "161 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "162 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "163 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "164 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "165 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "166 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "167 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "168 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "169 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "170 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "171 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "172 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "173 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "174 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "175 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "176 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "177 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "178 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "179 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "180 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "181 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "182 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "183 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "184 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "185 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "186 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "187 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "188 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "189 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "190 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "191 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "192 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "193 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "194 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "195 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "196 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "197 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "198 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "199 Train Loss 0.41174948 Test RE 0.00017032404666870776 c 1.0012352 k 0.04994066 m 4.9900203\n",
      "Training time: 51.33\n",
      "Training time: 51.33\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 151648.89 Test RE 0.2589090845989961 c -0.00762904 k 0.10085673 m 0.0010021233\n",
      "1 Train Loss 45370.625 Test RE 0.13710281807016425 c -0.005859352 k 0.041762933 m 0.0013854716\n",
      "2 Train Loss 30117.867 Test RE 0.11230026476097253 c -0.0011777665 k 0.07707973 m 0.0015611406\n",
      "3 Train Loss 18877.14 Test RE 0.08643219611092817 c 0.014432335 k 0.04572625 m 0.0027561928\n",
      "4 Train Loss 5869.921 Test RE 0.0424279248939285 c 0.0813044 k 0.06473545 m 0.006949348\n",
      "5 Train Loss 2570.085 Test RE 0.01751940002721113 c 0.13024193 k 0.057496604 m 0.01015345\n",
      "6 Train Loss 1674.0244 Test RE 0.008024278235871528 c 0.15015936 k 0.05863001 m 0.01140908\n",
      "7 Train Loss 1403.6741 Test RE 0.0049006955361158054 c 0.17742813 k 0.058423027 m 0.013457266\n",
      "8 Train Loss 1307.0001 Test RE 0.004133064765259414 c 0.22326846 k 0.05765597 m 0.01702736\n",
      "9 Train Loss 1063.383 Test RE 0.00580348320354788 c 0.38113174 k 0.05529463 m 0.029723728\n",
      "10 Train Loss 850.7864 Test RE 0.006940520024311601 c 0.5199503 k 0.053547923 m 0.041476976\n",
      "11 Train Loss 611.52734 Test RE 0.005930383419434814 c 0.80102086 k 0.0497468 m 0.066084035\n",
      "12 Train Loss 497.4278 Test RE 0.004578405763752184 c 0.88557845 k 0.048001483 m 0.07383717\n",
      "13 Train Loss 458.963 Test RE 0.005259383325619704 c 0.97468835 k 0.04710126 m 0.08349323\n",
      "14 Train Loss 384.06277 Test RE 0.005265420381411517 c 1.0349752 k 0.04599056 m 0.09419765\n",
      "15 Train Loss 367.6107 Test RE 0.004308923650704623 c 1.0467477 k 0.04578517 m 0.09735379\n",
      "16 Train Loss 354.0821 Test RE 0.004541538501025528 c 1.0693148 k 0.045417253 m 0.1028709\n",
      "17 Train Loss 346.70117 Test RE 0.004427261788981507 c 1.1020277 k 0.044955306 m 0.11050359\n",
      "18 Train Loss 341.88147 Test RE 0.004484207735178937 c 1.1159577 k 0.044711497 m 0.11877487\n",
      "19 Train Loss 339.41605 Test RE 0.004326827353740204 c 1.1095282 k 0.044845305 m 0.123665\n",
      "20 Train Loss 334.10468 Test RE 0.004167325050255049 c 1.1075543 k 0.04496232 m 0.14509974\n",
      "21 Train Loss 323.24808 Test RE 0.004582400182359726 c 1.1313108 k 0.04463147 m 0.19178808\n",
      "22 Train Loss 311.27698 Test RE 0.00456891411104838 c 1.1375084 k 0.04438797 m 0.25658396\n",
      "23 Train Loss 296.62445 Test RE 0.004442722381269766 c 1.1570652 k 0.0442579 m 0.34126505\n",
      "24 Train Loss 286.21237 Test RE 0.004392293372433561 c 1.1340308 k 0.044719823 m 0.39130065\n",
      "25 Train Loss 271.9582 Test RE 0.004405830474593217 c 1.0990914 k 0.045345858 m 0.4805517\n",
      "26 Train Loss 256.0216 Test RE 0.004664592234722755 c 1.0964135 k 0.04550539 m 0.585078\n",
      "27 Train Loss 249.87108 Test RE 0.005052588276652803 c 1.1009796 k 0.04543189 m 0.6298784\n",
      "28 Train Loss 229.19673 Test RE 0.0046769416586039 c 1.0940206 k 0.0456291 m 0.8122578\n",
      "29 Train Loss 217.85417 Test RE 0.004649796078475046 c 1.1008308 k 0.04557727 m 0.94188994\n",
      "30 Train Loss 214.926 Test RE 0.004464216042662134 c 1.0934569 k 0.045646906 m 0.9806809\n",
      "31 Train Loss 206.45393 Test RE 0.004315251993489422 c 1.0923699 k 0.045725122 m 1.1623548\n",
      "32 Train Loss 201.52522 Test RE 0.004328279459694271 c 1.0930274 k 0.04568163 m 1.2145498\n",
      "33 Train Loss 191.44946 Test RE 0.004028533577053196 c 1.0566733 k 0.0464399 m 1.3046992\n",
      "34 Train Loss 184.10458 Test RE 0.003960447192853951 c 1.0616199 k 0.046589803 m 1.4638283\n",
      "35 Train Loss 179.57149 Test RE 0.004013598486950822 c 1.0945417 k 0.046155043 m 1.5906985\n",
      "36 Train Loss 175.6393 Test RE 0.003971604790042911 c 1.0941912 k 0.0461273 m 1.6360178\n",
      "37 Train Loss 172.1373 Test RE 0.003826634273464976 c 1.0795575 k 0.04655687 m 1.7351545\n",
      "38 Train Loss 168.51169 Test RE 0.003967300954081706 c 1.0701056 k 0.046676744 m 1.8684026\n",
      "39 Train Loss 164.49866 Test RE 0.0042335213486447264 c 1.0634798 k 0.046794333 m 1.9687406\n",
      "40 Train Loss 160.71835 Test RE 0.004057805346544203 c 1.0667856 k 0.046718497 m 1.9552134\n",
      "41 Train Loss 159.33456 Test RE 0.004220237825487066 c 1.0769396 k 0.04657497 m 1.9539207\n",
      "42 Train Loss 157.0057 Test RE 0.004188813374895224 c 1.0842092 k 0.04652922 m 1.9982405\n",
      "43 Train Loss 151.07812 Test RE 0.003813597981999789 c 1.0607747 k 0.046846658 m 2.040311\n",
      "44 Train Loss 146.3511 Test RE 0.004000175438540407 c 1.0431836 k 0.04720553 m 2.1099827\n",
      "45 Train Loss 136.09769 Test RE 0.0037912462160488527 c 1.0747656 k 0.046902448 m 2.2783\n",
      "46 Train Loss 132.95105 Test RE 0.0036763587153989955 c 1.0694537 k 0.046885908 m 2.261636\n",
      "47 Train Loss 130.90218 Test RE 0.0036896447560412385 c 1.0779942 k 0.046714697 m 2.2627745\n",
      "48 Train Loss 129.00418 Test RE 0.003661200570889023 c 1.0868416 k 0.04663992 m 2.311976\n",
      "49 Train Loss 127.0616 Test RE 0.003692531527389215 c 1.0725446 k 0.04691385 m 2.3678112\n",
      "50 Train Loss 123.65458 Test RE 0.0036460886079697457 c 1.0411797 k 0.04736574 m 2.444525\n",
      "51 Train Loss 116.19008 Test RE 0.0033969708032455664 c 1.0476286 k 0.047374524 m 2.5811646\n",
      "52 Train Loss 113.38086 Test RE 0.0034875531201323277 c 1.0591958 k 0.047412876 m 2.6168191\n",
      "53 Train Loss 111.36691 Test RE 0.003423357379938457 c 1.0558183 k 0.047343746 m 2.623025\n",
      "54 Train Loss 109.05338 Test RE 0.003422116737245564 c 1.0677869 k 0.047167063 m 2.666533\n",
      "55 Train Loss 106.910515 Test RE 0.0034396953617079912 c 1.0824689 k 0.047067173 m 2.6981032\n",
      "56 Train Loss 104.97516 Test RE 0.0033340059176548203 c 1.0750288 k 0.04709418 m 2.6880016\n",
      "57 Train Loss 103.53958 Test RE 0.0032309601750053485 c 1.0602764 k 0.04726135 m 2.6947103\n",
      "58 Train Loss 101.70743 Test RE 0.003150383122073545 c 1.0546788 k 0.047371782 m 2.7473855\n",
      "59 Train Loss 98.91194 Test RE 0.003156727984924249 c 1.0667331 k 0.047303263 m 2.8314517\n",
      "60 Train Loss 94.10701 Test RE 0.002892305087783912 c 1.0642151 k 0.04739726 m 2.928108\n",
      "61 Train Loss 92.6729 Test RE 0.0027800845365065783 c 1.0479267 k 0.047643762 m 2.9432719\n",
      "62 Train Loss 91.65483 Test RE 0.0028153979490936907 c 1.0423315 k 0.04775506 m 2.9475603\n",
      "63 Train Loss 90.44915 Test RE 0.002997154616044153 c 1.0528661 k 0.04761845 m 2.9595833\n",
      "64 Train Loss 89.54547 Test RE 0.0030712579216751203 c 1.0617692 k 0.04748319 m 2.9503138\n",
      "65 Train Loss 87.570145 Test RE 0.00302117718187599 c 1.0629221 k 0.047439225 m 2.9556854\n",
      "66 Train Loss 86.792336 Test RE 0.002967932951180375 c 1.05647 k 0.047562353 m 2.9756203\n",
      "67 Train Loss 86.49505 Test RE 0.002984908287485152 c 1.0528904 k 0.047632642 m 2.9879463\n",
      "68 Train Loss 85.40404 Test RE 0.00301367135501812 c 1.0484439 k 0.04772973 m 3.035703\n",
      "69 Train Loss 83.91077 Test RE 0.0028698840617205 c 1.0470723 k 0.047777534 m 3.1121716\n",
      "70 Train Loss 82.42401 Test RE 0.0027324079589243514 c 1.0487905 k 0.04780507 m 3.1683455\n",
      "71 Train Loss 80.7276 Test RE 0.002761583922601309 c 1.0545771 k 0.047752712 m 3.2019212\n",
      "72 Train Loss 78.979485 Test RE 0.0028961940729982727 c 1.0531868 k 0.04775513 m 3.1747391\n",
      "73 Train Loss 78.23519 Test RE 0.00290862354707643 c 1.0482666 k 0.047809865 m 3.173535\n",
      "74 Train Loss 77.994804 Test RE 0.002851030821418673 c 1.0455868 k 0.047862012 m 3.1943476\n",
      "75 Train Loss 77.39253 Test RE 0.0028590854322969902 c 1.0549186 k 0.04781999 m 3.264393\n",
      "76 Train Loss 76.259 Test RE 0.002828542427032732 c 1.0560943 k 0.04778505 m 3.2684011\n",
      "77 Train Loss 75.680405 Test RE 0.002816840275102267 c 1.0465006 k 0.04789208 m 3.259924\n",
      "78 Train Loss 73.2688 Test RE 0.0027896283581055363 c 1.0380815 k 0.04806393 m 3.3804274\n",
      "79 Train Loss 70.50054 Test RE 0.002726682279052207 c 1.0471996 k 0.04787135 m 3.391806\n",
      "80 Train Loss 66.41243 Test RE 0.0027787280165010287 c 1.0514004 k 0.04786366 m 3.3979185\n",
      "81 Train Loss 61.95514 Test RE 0.002552949334193144 c 1.0404519 k 0.048108768 m 3.441105\n",
      "82 Train Loss 58.22869 Test RE 0.0023154588706443173 c 1.0308354 k 0.048223656 m 3.4634702\n",
      "83 Train Loss 56.496906 Test RE 0.0021548755203818566 c 1.0336869 k 0.048223868 m 3.491584\n",
      "84 Train Loss 53.59072 Test RE 0.0020403335533100368 c 1.0413733 k 0.048196945 m 3.5681045\n",
      "85 Train Loss 49.521847 Test RE 0.002033858238730034 c 1.0334483 k 0.048326235 m 3.5992982\n",
      "86 Train Loss 47.61674 Test RE 0.0019358517886475847 c 1.0215763 k 0.048446737 m 3.6285086\n",
      "87 Train Loss 45.24336 Test RE 0.0018381588996211 c 1.0226935 k 0.048486408 m 3.6860454\n",
      "88 Train Loss 44.121803 Test RE 0.0018256326134067457 c 1.0344042 k 0.048383158 m 3.705687\n",
      "89 Train Loss 43.412296 Test RE 0.0018819598807358758 c 1.0390601 k 0.048287693 m 3.7059448\n",
      "90 Train Loss 41.82678 Test RE 0.0018902555043688104 c 1.0313642 k 0.04839841 m 3.661155\n",
      "91 Train Loss 40.176613 Test RE 0.0018068984704208326 c 1.0323725 k 0.048326716 m 3.6826055\n",
      "92 Train Loss 38.565414 Test RE 0.0018518156643189726 c 1.0379674 k 0.048408348 m 3.7678037\n",
      "93 Train Loss 37.64718 Test RE 0.0018230256886838704 c 1.033339 k 0.048586972 m 3.8273025\n",
      "94 Train Loss 36.814625 Test RE 0.0017175012397038214 c 1.0269153 k 0.048615813 m 3.8271859\n",
      "95 Train Loss 35.388203 Test RE 0.0016222175366678929 c 1.0321753 k 0.04856203 m 3.8453429\n",
      "96 Train Loss 33.61591 Test RE 0.0015738304177060703 c 1.0304438 k 0.04863689 m 3.893418\n",
      "97 Train Loss 31.87727 Test RE 0.001454679356566956 c 1.0235301 k 0.048746493 m 3.9447167\n",
      "98 Train Loss 31.115665 Test RE 0.0014196389686869345 c 1.0296179 k 0.048659097 m 3.939759\n",
      "99 Train Loss 30.301363 Test RE 0.0013689334462253374 c 1.034248 k 0.048586752 m 3.9293914\n",
      "100 Train Loss 29.604376 Test RE 0.0013734935676970126 c 1.0333697 k 0.048572093 m 3.95237\n",
      "101 Train Loss 29.256092 Test RE 0.0013958850727370466 c 1.032899 k 0.048601806 m 3.9634185\n",
      "102 Train Loss 28.211813 Test RE 0.0012344093552796927 c 1.038087 k 0.048524335 m 3.959148\n",
      "103 Train Loss 27.828886 Test RE 0.001220463216872085 c 1.0379792 k 0.048516072 m 3.9418435\n",
      "104 Train Loss 27.200619 Test RE 0.001253568914149377 c 1.0325602 k 0.048627798 m 3.9616377\n",
      "105 Train Loss 26.373686 Test RE 0.001190452579154019 c 1.0237218 k 0.048830133 m 4.008966\n",
      "106 Train Loss 25.975576 Test RE 0.0011438698916058532 c 1.0234038 k 0.048867267 m 4.0234346\n",
      "107 Train Loss 25.585875 Test RE 0.0011331266607474903 c 1.0278081 k 0.048807785 m 4.016971\n",
      "108 Train Loss 25.38228 Test RE 0.0011524735305680999 c 1.0313659 k 0.04870912 m 3.9969213\n",
      "109 Train Loss 25.045315 Test RE 0.0011907000812695547 c 1.0326531 k 0.048638057 m 3.984535\n",
      "110 Train Loss 24.360285 Test RE 0.0011686094617597831 c 1.0293795 k 0.04869585 m 3.981118\n",
      "111 Train Loss 23.65831 Test RE 0.00107142724334486 c 1.0270989 k 0.048747003 m 3.9800134\n",
      "112 Train Loss 22.123236 Test RE 0.0009661412797516236 c 1.0301169 k 0.048706252 m 3.9915476\n",
      "113 Train Loss 20.667606 Test RE 0.0009948851415089615 c 1.0311018 k 0.048720732 m 4.0165386\n",
      "114 Train Loss 19.178654 Test RE 0.0009376123216857924 c 1.0169328 k 0.049040727 m 4.092248\n",
      "115 Train Loss 18.693737 Test RE 0.0008978012693456273 c 1.0097967 k 0.04917261 m 4.149667\n",
      "116 Train Loss 18.273245 Test RE 0.0009038056807685125 c 1.0119822 k 0.049109776 m 4.175816\n",
      "117 Train Loss 17.44664 Test RE 0.000967545235657801 c 1.0199921 k 0.048967857 m 4.1912413\n",
      "118 Train Loss 14.599607 Test RE 0.0009876851370148616 c 1.0256509 k 0.048940197 m 4.3218718\n",
      "119 Train Loss 13.41275 Test RE 0.000810591569427619 c 1.0187732 k 0.049127866 m 4.3726416\n",
      "120 Train Loss 13.114909 Test RE 0.000775516692854113 c 1.0169926 k 0.04915441 m 4.373253\n",
      "121 Train Loss 12.654724 Test RE 0.0007978686124572932 c 1.0164306 k 0.04910934 m 4.355992\n",
      "122 Train Loss 12.122307 Test RE 0.00079687629589234 c 1.0157797 k 0.049188517 m 4.385602\n",
      "123 Train Loss 11.4160595 Test RE 0.0007136093874424738 c 1.0167264 k 0.04922346 m 4.4305215\n",
      "124 Train Loss 11.205508 Test RE 0.0007108643625110883 c 1.0181133 k 0.049169775 m 4.436056\n",
      "125 Train Loss 11.146699 Test RE 0.0007386754586236132 c 1.0178416 k 0.049167942 m 4.444018\n",
      "126 Train Loss 11.059045 Test RE 0.0007514695883136419 c 1.0155706 k 0.049219828 m 4.471315\n",
      "127 Train Loss 11.030888 Test RE 0.0007298353978159403 c 1.0152025 k 0.049230862 m 4.480681\n",
      "128 Train Loss 10.962173 Test RE 0.0006867097060516143 c 1.0167431 k 0.049218036 m 4.4807906\n",
      "129 Train Loss 10.804329 Test RE 0.0006772696555712558 c 1.0186851 k 0.049192708 m 4.4675007\n",
      "130 Train Loss 10.657355 Test RE 0.0006979198915432269 c 1.0174599 k 0.049210373 m 4.4676604\n",
      "131 Train Loss 10.43531 Test RE 0.0006766184619832454 c 1.0138723 k 0.04927433 m 4.4930453\n",
      "132 Train Loss 10.3011465 Test RE 0.0006570411625128343 c 1.0149822 k 0.04927506 m 4.505488\n",
      "133 Train Loss 10.14341 Test RE 0.0006659976890708665 c 1.0170989 k 0.04925202 m 4.4987187\n",
      "134 Train Loss 9.944044 Test RE 0.0006628042216705229 c 1.0176395 k 0.04925068 m 4.4970703\n",
      "135 Train Loss 9.624448 Test RE 0.0005967904453171788 c 1.0165186 k 0.049277496 m 4.5267906\n",
      "136 Train Loss 9.312191 Test RE 0.0005884510286859016 c 1.0149435 k 0.049330045 m 4.564164\n",
      "137 Train Loss 9.042148 Test RE 0.0006060122816325487 c 1.0176283 k 0.049276732 m 4.5562415\n",
      "138 Train Loss 8.806441 Test RE 0.0006060472464353021 c 1.018097 k 0.049255062 m 4.5348544\n",
      "139 Train Loss 8.540275 Test RE 0.0005922055480276782 c 1.012129 k 0.049347404 m 4.5486307\n",
      "140 Train Loss 8.09735 Test RE 0.0005242765385416676 c 1.0062486 k 0.04948399 m 4.6044693\n",
      "141 Train Loss 6.9953966 Test RE 0.00046750429668340206 c 1.0103714 k 0.049519554 m 4.707277\n",
      "142 Train Loss 5.701573 Test RE 0.000457417848439855 c 1.0137347 k 0.049514625 m 4.865476\n",
      "143 Train Loss 5.2178893 Test RE 0.0004327043603389795 c 1.0145739 k 0.049552485 m 4.9102736\n",
      "144 Train Loss 4.784098 Test RE 0.0003201133447428292 c 1.0145682 k 0.04960458 m 4.9294505\n",
      "145 Train Loss 4.612986 Test RE 0.00032247295268935007 c 1.0098047 k 0.049681008 m 4.9327335\n",
      "146 Train Loss 4.3381624 Test RE 0.0003385713022348344 c 1.0034462 k 0.04980751 m 4.96181\n",
      "147 Train Loss 4.067407 Test RE 0.0003793738411792539 c 1.0008426 k 0.04986081 m 4.985226\n",
      "148 Train Loss 3.8943448 Test RE 0.000350146455675181 c 1.0012026 k 0.049858566 m 4.9917746\n",
      "149 Train Loss 3.8053858 Test RE 0.00032843401646360083 c 1.003633 k 0.049827818 m 4.998425\n",
      "150 Train Loss 3.7466135 Test RE 0.00032199696015307257 c 1.0061076 k 0.049797736 m 5.003553\n",
      "151 Train Loss 3.704551 Test RE 0.00031417181258061426 c 1.0046188 k 0.0498325 m 5.0091085\n",
      "152 Train Loss 3.6022162 Test RE 0.0003133181669403141 c 1.001058 k 0.049927276 m 5.030429\n",
      "153 Train Loss 3.3665955 Test RE 0.00034045129730139826 c 1.0041603 k 0.04985386 m 5.025298\n",
      "154 Train Loss 3.1651073 Test RE 0.00032462010430462984 c 1.0077152 k 0.049757693 m 5.0056605\n",
      "155 Train Loss 3.076237 Test RE 0.000283055939826941 c 1.0040156 k 0.04984678 m 5.0162344\n",
      "156 Train Loss 3.0000012 Test RE 0.0002572957677617494 c 0.99947107 k 0.049947813 m 5.054481\n",
      "157 Train Loss 2.9397652 Test RE 0.00026031965269261356 c 1.0010183 k 0.049916867 m 5.0704794\n",
      "158 Train Loss 2.9061005 Test RE 0.00025061786331909754 c 1.0015545 k 0.049911514 m 5.071463\n",
      "159 Train Loss 2.8813415 Test RE 0.0002367553682824034 c 0.9999394 k 0.04995037 m 5.070345\n",
      "160 Train Loss 2.877599 Test RE 0.00023416267600768836 c 0.9993965 k 0.049958758 m 5.0677447\n",
      "161 Train Loss 2.8770244 Test RE 0.0002341135873740649 c 0.9993518 k 0.04995902 m 5.0672054\n",
      "162 Train Loss 2.8748462 Test RE 0.00023475366391136044 c 0.99919575 k 0.04995885 m 5.064904\n",
      "163 Train Loss 2.8727736 Test RE 0.00023672232325809312 c 0.9992417 k 0.04995405 m 5.062648\n",
      "164 Train Loss 2.8704693 Test RE 0.00023977591479388391 c 0.9994474 k 0.049947567 m 5.061386\n",
      "165 Train Loss 2.861551 Test RE 0.00024493869670287805 c 0.999546 k 0.049940597 m 5.0636406\n",
      "166 Train Loss 2.8356817 Test RE 0.00024770373489184643 c 0.9995273 k 0.049944907 m 5.073824\n",
      "167 Train Loss 2.746978 Test RE 0.00022169500872951735 c 1.0000503 k 0.049948506 m 5.077677\n",
      "168 Train Loss 2.6006403 Test RE 0.00018563569342336438 c 1.002153 k 0.04991837 m 5.0585723\n",
      "169 Train Loss 2.524356 Test RE 0.00018490239412466105 c 1.004472 k 0.049890652 m 5.058336\n",
      "170 Train Loss 2.4326534 Test RE 0.00020341678418948732 c 1.0048895 k 0.04989277 m 5.0628247\n",
      "171 Train Loss 2.2263813 Test RE 0.00018246591005257533 c 1.002316 k 0.049943503 m 5.060396\n",
      "172 Train Loss 1.987639 Test RE 0.00018540080402555927 c 1.0018468 k 0.04995164 m 5.03283\n",
      "173 Train Loss 1.8729595 Test RE 0.00019347562406530895 c 1.0020224 k 0.04991737 m 5.022524\n",
      "174 Train Loss 1.8138837 Test RE 0.00018871919154624624 c 1.0021354 k 0.049885787 m 5.0040884\n",
      "175 Train Loss 1.7852397 Test RE 0.00018479807569767395 c 1.0028882 k 0.049855173 m 4.9850645\n",
      "176 Train Loss 1.782159 Test RE 0.00018514152287707472 c 1.0031586 k 0.049848482 m 4.9806824\n",
      "177 Train Loss 1.7615142 Test RE 0.00019287023997009397 c 1.0041062 k 0.049824312 m 4.967722\n",
      "178 Train Loss 1.7262676 Test RE 0.00021477961749956064 c 1.0039538 k 0.049820554 m 4.9517374\n",
      "179 Train Loss 1.7085091 Test RE 0.0002196975205391461 c 1.0033478 k 0.049827714 m 4.946652\n",
      "180 Train Loss 1.7043653 Test RE 0.00021644235502649446 c 1.0032558 k 0.04983164 m 4.9478865\n",
      "181 Train Loss 1.701061 Test RE 0.00021128083928038363 c 1.0032861 k 0.049835302 m 4.9508715\n",
      "182 Train Loss 1.6984403 Test RE 0.00020741553098605885 c 1.0033565 k 0.049838535 m 4.9532013\n",
      "183 Train Loss 1.6958417 Test RE 0.00020633852226983597 c 1.003299 k 0.049841437 m 4.953557\n",
      "184 Train Loss 1.6920493 Test RE 0.000206894461356471 c 1.0030684 k 0.04984302 m 4.9515753\n",
      "185 Train Loss 1.6765344 Test RE 0.00020246943747594603 c 1.0021956 k 0.04984619 m 4.9445686\n",
      "186 Train Loss 1.6560467 Test RE 0.00019336425957947607 c 1.002825 k 0.049828142 m 4.941269\n",
      "187 Train Loss 1.6418623 Test RE 0.00018374634200974237 c 1.0032059 k 0.04983147 m 4.946056\n",
      "188 Train Loss 1.6218424 Test RE 0.00018306560325672346 c 1.0023106 k 0.049851403 m 4.9515758\n",
      "189 Train Loss 1.5865331 Test RE 0.00019739546004517223 c 1.0013562 k 0.049859665 m 4.9459343\n",
      "190 Train Loss 1.5578204 Test RE 0.00020372341759218547 c 1.0022601 k 0.04984566 m 4.9404902\n",
      "191 Train Loss 1.5367569 Test RE 0.000207484427245586 c 1.0034262 k 0.04983477 m 4.946376\n",
      "192 Train Loss 1.5197921 Test RE 0.00021773930708747228 c 1.0039972 k 0.04982936 m 4.948505\n",
      "193 Train Loss 1.4915588 Test RE 0.00022736003690680692 c 1.0039552 k 0.049821734 m 4.9354744\n",
      "194 Train Loss 1.4560127 Test RE 0.00020701236638704182 c 1.0038376 k 0.04981279 m 4.9223504\n",
      "195 Train Loss 1.4140939 Test RE 0.00018130178381666173 c 1.0028467 k 0.049826484 m 4.9280043\n",
      "196 Train Loss 1.3880147 Test RE 0.00018649677334580582 c 1.0027734 k 0.04983214 m 4.9270563\n",
      "197 Train Loss 1.3697577 Test RE 0.0001982537318908336 c 1.0038667 k 0.049815387 m 4.917815\n",
      "198 Train Loss 1.3508022 Test RE 0.0001917251072837426 c 1.0042337 k 0.049816944 m 4.9206557\n",
      "199 Train Loss 1.3318999 Test RE 0.00018622277446260458 c 1.0040389 k 0.049822573 m 4.9322143\n",
      "Training time: 69.96\n",
      "Training time: 69.96\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 259493.16 Test RE 0.3417412312604611 c 0.0006626567 k 0.07554062 m -4.8430426e-05\n",
      "1 Train Loss 194547.19 Test RE 0.28941983475558064 c 0.0023675896 k 0.005380692 m -5.5028766e-05\n",
      "2 Train Loss 46967.848 Test RE 0.14051772329591194 c 0.045922205 k 0.051142372 m -0.0001508958\n",
      "3 Train Loss 45347.05 Test RE 0.13847104186451428 c 0.052988898 k 0.060448468 m -0.00015587291\n",
      "4 Train Loss 30572.893 Test RE 0.11241401946957535 c 0.5381166 k 0.0500986 m 7.20123e-05\n",
      "5 Train Loss 9383.968 Test RE 0.05210649436954044 c 1.580214 k 0.03659519 m 0.0006392733\n",
      "6 Train Loss 3944.9463 Test RE 0.03121198696839705 c 1.5046475 k 0.03863718 m 0.0034877479\n",
      "7 Train Loss 2947.974 Test RE 0.02255408655217653 c 1.4182996 k 0.038138594 m 0.009813782\n",
      "8 Train Loss 2420.1838 Test RE 0.02133921029699299 c 1.1363314 k 0.04396303 m 0.021433456\n",
      "9 Train Loss 2250.7605 Test RE 0.020488388562677273 c 1.0700724 k 0.04461182 m 0.031846326\n",
      "10 Train Loss 1990.0085 Test RE 0.019844926131279052 c 0.94069743 k 0.0465118 m 0.061765175\n",
      "11 Train Loss 1530.4435 Test RE 0.020484435530529498 c 0.9938986 k 0.046609424 m 0.18531798\n",
      "12 Train Loss 1216.6805 Test RE 0.019114021701854753 c 1.2670232 k 0.04146901 m 0.28172636\n",
      "13 Train Loss 963.82715 Test RE 0.01771429548490168 c 1.1539724 k 0.043924402 m 0.41513073\n",
      "14 Train Loss 808.81537 Test RE 0.015816621329274538 c 1.0076084 k 0.046000883 m 0.5121678\n",
      "15 Train Loss 548.81506 Test RE 0.01107979257402335 c 1.03181 k 0.046029978 m 0.67867017\n",
      "16 Train Loss 406.19946 Test RE 0.008710405693288294 c 1.0203679 k 0.046396755 m 0.8274901\n",
      "17 Train Loss 328.05573 Test RE 0.007482722341566459 c 1.0490291 k 0.046334967 m 1.0169358\n",
      "18 Train Loss 274.3146 Test RE 0.005942713864845516 c 1.0636258 k 0.04616388 m 1.0588136\n",
      "19 Train Loss 258.3058 Test RE 0.005319973354254152 c 1.1002918 k 0.045665942 m 1.1232864\n",
      "20 Train Loss 251.62512 Test RE 0.0055797523211583815 c 1.0884948 k 0.04596951 m 1.1700066\n",
      "21 Train Loss 247.54828 Test RE 0.005566994436119496 c 1.1186314 k 0.04551834 m 1.1814665\n",
      "22 Train Loss 239.43015 Test RE 0.005277759951033192 c 1.1321758 k 0.04526939 m 1.1856004\n",
      "23 Train Loss 220.58853 Test RE 0.004727617898726034 c 1.0794333 k 0.046120644 m 1.281668\n",
      "24 Train Loss 214.92545 Test RE 0.004493715052320998 c 1.0991024 k 0.0457855 m 1.3651606\n",
      "25 Train Loss 204.484 Test RE 0.004582337016867016 c 1.0929779 k 0.04591939 m 1.5007948\n",
      "26 Train Loss 198.89471 Test RE 0.0044736898814647106 c 1.1031069 k 0.04578827 m 1.5777217\n",
      "27 Train Loss 190.48596 Test RE 0.00439113139499432 c 1.0891627 k 0.04602897 m 1.6230496\n",
      "28 Train Loss 185.58939 Test RE 0.004144059576742294 c 1.0584711 k 0.046525672 m 1.6347755\n",
      "29 Train Loss 172.64398 Test RE 0.0037903781623921068 c 1.085032 k 0.046137396 m 1.7094593\n",
      "30 Train Loss 169.16437 Test RE 0.004066512262617522 c 1.0918729 k 0.04613142 m 1.7484324\n",
      "31 Train Loss 165.09708 Test RE 0.004104048069748485 c 1.086037 k 0.046203848 m 1.8116292\n",
      "32 Train Loss 156.52596 Test RE 0.003131972174396957 c 1.084813 k 0.0462957 m 2.0029874\n",
      "33 Train Loss 146.93872 Test RE 0.003351711241456554 c 1.0781333 k 0.046631098 m 2.0437393\n",
      "34 Train Loss 132.84755 Test RE 0.003218792977418685 c 1.082385 k 0.046725318 m 2.282284\n",
      "35 Train Loss 123.27722 Test RE 0.0027281075868542783 c 1.0736332 k 0.04692991 m 2.3655505\n",
      "36 Train Loss 117.23177 Test RE 0.002741578425685855 c 1.0604006 k 0.04720486 m 2.3847215\n",
      "37 Train Loss 105.92979 Test RE 0.0029925350851370777 c 1.0632393 k 0.047191314 m 2.4982328\n",
      "38 Train Loss 97.18119 Test RE 0.003038782481363458 c 1.0731779 k 0.047602624 m 2.6868486\n",
      "39 Train Loss 69.3685 Test RE 0.002504577194157331 c 1.0426781 k 0.047981225 m 2.9548676\n",
      "40 Train Loss 65.05765 Test RE 0.0025377692480421945 c 1.053124 k 0.048060562 m 3.0143666\n",
      "41 Train Loss 61.606384 Test RE 0.002346733613336039 c 1.0453657 k 0.048011627 m 3.0178156\n",
      "42 Train Loss 60.538548 Test RE 0.0022041316127176555 c 1.045546 k 0.04805362 m 3.0192158\n",
      "43 Train Loss 55.280273 Test RE 0.002266929279571253 c 1.0447756 k 0.047980838 m 3.070859\n",
      "44 Train Loss 46.2623 Test RE 0.0019345493833827115 c 1.0432746 k 0.048177026 m 3.1973686\n",
      "45 Train Loss 41.012814 Test RE 0.0017291994257409567 c 1.0440731 k 0.04808884 m 3.3026848\n",
      "46 Train Loss 37.67763 Test RE 0.0015194069168388766 c 1.039902 k 0.048176777 m 3.4035769\n",
      "47 Train Loss 34.061768 Test RE 0.0016342895675921499 c 1.0423709 k 0.048090167 m 3.4756365\n",
      "48 Train Loss 30.298847 Test RE 0.0017100761026007914 c 1.0319958 k 0.04842714 m 3.5781927\n",
      "49 Train Loss 22.411608 Test RE 0.0013705216039186375 c 1.0394188 k 0.048497405 m 3.8355062\n",
      "50 Train Loss 7.9397173 Test RE 0.001127621245151861 c 1.0200994 k 0.0491712 m 4.478737\n",
      "51 Train Loss 4.631896 Test RE 0.0009413065583144126 c 1.0077816 k 0.049519513 m 4.80666\n",
      "52 Train Loss 3.4409494 Test RE 0.0008008020243457566 c 1.0081202 k 0.04963357 m 4.9709907\n",
      "53 Train Loss 2.9077516 Test RE 0.0006897355639343258 c 1.0053744 k 0.04972538 m 4.970734\n",
      "54 Train Loss 2.263155 Test RE 0.0004841092074830374 c 1.0040115 k 0.04981646 m 4.988269\n",
      "55 Train Loss 2.1140566 Test RE 0.0004718703053840343 c 1.0057902 k 0.049723323 m 4.9490757\n",
      "56 Train Loss 2.0239944 Test RE 0.00047008416883747244 c 1.0058811 k 0.04971438 m 4.9162292\n",
      "57 Train Loss 1.8361411 Test RE 0.00044863451450490006 c 1.003442 k 0.049806748 m 4.9497914\n",
      "58 Train Loss 1.6357265 Test RE 0.00040766721094204976 c 1.002894 k 0.0498132 m 5.0051885\n",
      "59 Train Loss 1.3509077 Test RE 0.000384175093760027 c 1.0042024 k 0.04978036 m 4.9893494\n",
      "60 Train Loss 1.2559369 Test RE 0.000390348363295111 c 1.0031233 k 0.049838822 m 4.981214\n",
      "61 Train Loss 1.2296221 Test RE 0.0003983157456211775 c 1.0028921 k 0.049849246 m 4.9885\n",
      "62 Train Loss 1.1958472 Test RE 0.00039043881109996216 c 1.0032037 k 0.04983274 m 4.987746\n",
      "63 Train Loss 1.132587 Test RE 0.00036920330205817694 c 1.0032752 k 0.04980083 m 4.9667583\n",
      "64 Train Loss 1.0879598 Test RE 0.0003647303322589725 c 1.0033654 k 0.0498039 m 4.95891\n",
      "65 Train Loss 1.0614018 Test RE 0.00035950031221914134 c 1.0036757 k 0.04980014 m 4.955739\n",
      "66 Train Loss 0.9841907 Test RE 0.00034589102729427315 c 1.0047503 k 0.049773283 m 4.936664\n",
      "67 Train Loss 0.8966423 Test RE 0.00032911102292887374 c 1.0048373 k 0.0497747 m 4.941958\n",
      "68 Train Loss 0.8776555 Test RE 0.000327859093100704 c 1.0039903 k 0.049787804 m 4.9476557\n",
      "69 Train Loss 0.8599108 Test RE 0.00032648186498000683 c 1.0034225 k 0.04979845 m 4.9513383\n",
      "70 Train Loss 0.83416325 Test RE 0.00032350613284479825 c 1.0035648 k 0.04980997 m 4.9567046\n",
      "71 Train Loss 0.8201137 Test RE 0.00032173656768110404 c 1.0037313 k 0.04981886 m 4.9621735\n",
      "72 Train Loss 0.8109706 Test RE 0.00031984340398877786 c 1.0035614 k 0.049819447 m 4.968377\n",
      "73 Train Loss 0.7997905 Test RE 0.00031502753864060445 c 1.0030955 k 0.049827486 m 4.9751105\n",
      "74 Train Loss 0.7426784 Test RE 0.0002986091762035913 c 1.000973 k 0.049886145 m 4.996609\n",
      "75 Train Loss 0.65746427 Test RE 0.00028935313672313575 c 1.0018337 k 0.049882866 m 4.9939857\n",
      "76 Train Loss 0.61323434 Test RE 0.00026369319286608786 c 1.0025438 k 0.0498587 m 4.975235\n",
      "77 Train Loss 0.5823807 Test RE 0.0002498993801409092 c 1.0015488 k 0.049883757 m 4.9821157\n",
      "78 Train Loss 0.544045 Test RE 0.0002441907480881991 c 1.0012281 k 0.049911767 m 4.997332\n",
      "79 Train Loss 0.52954656 Test RE 0.00024749655703158144 c 1.0015061 k 0.04991232 m 4.9975863\n",
      "80 Train Loss 0.5246036 Test RE 0.0002462808510691134 c 1.0016062 k 0.049906157 m 4.9932623\n",
      "81 Train Loss 0.5205518 Test RE 0.0002387060559805943 c 1.0017551 k 0.049901005 m 4.99379\n",
      "82 Train Loss 0.51468813 Test RE 0.00023553149498093512 c 1.0019126 k 0.049897373 m 4.9976006\n",
      "83 Train Loss 0.5058688 Test RE 0.00024106006957487518 c 1.0021266 k 0.04990248 m 4.997382\n",
      "84 Train Loss 0.48739702 Test RE 0.00023901019070047923 c 1.0014414 k 0.04992319 m 5.0021434\n",
      "85 Train Loss 0.44304836 Test RE 0.00021570281049361636 c 1.0008929 k 0.049935058 m 5.0146074\n",
      "86 Train Loss 0.39696336 Test RE 0.00019982028275703227 c 1.0013765 k 0.049932756 m 5.0138125\n",
      "87 Train Loss 0.3729766 Test RE 0.0002030700607136459 c 1.0012802 k 0.04993526 m 5.004108\n",
      "88 Train Loss 0.3648334 Test RE 0.00019738711727617286 c 1.0008795 k 0.049945623 m 5.0090384\n",
      "89 Train Loss 0.35896772 Test RE 0.0001952512900335359 c 1.0010899 k 0.04994512 m 5.0149713\n",
      "90 Train Loss 0.34801444 Test RE 0.00019954393222639162 c 1.0012578 k 0.049939595 m 5.0109634\n",
      "91 Train Loss 0.30947733 Test RE 0.0001932712096384003 c 1.0011705 k 0.049929053 m 4.998282\n",
      "92 Train Loss 0.25063258 Test RE 0.0001540311135737474 c 1.0015371 k 0.049911883 m 4.994807\n",
      "93 Train Loss 0.22907406 Test RE 0.0001341389518875163 c 1.0016161 k 0.049911655 m 4.9929566\n",
      "94 Train Loss 0.22053991 Test RE 0.00013509124131586524 c 1.0019078 k 0.049901623 m 4.9874444\n",
      "95 Train Loss 0.21511236 Test RE 0.0001353386097993558 c 1.0020561 k 0.04990288 m 4.9909444\n",
      "96 Train Loss 0.21284778 Test RE 0.00013569306730187492 c 1.0017619 k 0.04990534 m 4.9903784\n",
      "97 Train Loss 0.21146095 Test RE 0.000136169784380662 c 1.0018399 k 0.04990663 m 4.9873147\n",
      "98 Train Loss 0.20760939 Test RE 0.00013672337651826955 c 1.0016745 k 0.049906373 m 4.9839764\n",
      "99 Train Loss 0.20031153 Test RE 0.00013315662572318697 c 1.0016294 k 0.04990914 m 4.985673\n",
      "100 Train Loss 0.19777338 Test RE 0.00012991881115073488 c 1.0017233 k 0.049911708 m 4.9885345\n",
      "101 Train Loss 0.19168358 Test RE 0.00012728328379175473 c 1.0015004 k 0.04992117 m 4.992493\n",
      "102 Train Loss 0.18290064 Test RE 0.0001307346669365503 c 1.0012289 k 0.0499288 m 4.990042\n",
      "103 Train Loss 0.17361641 Test RE 0.0001248907371290388 c 1.0014758 k 0.04992823 m 4.988509\n",
      "104 Train Loss 0.1681274 Test RE 0.00011631013672260186 c 1.0015717 k 0.049928434 m 4.988233\n",
      "105 Train Loss 0.16213407 Test RE 0.00010427883491878558 c 1.0010117 k 0.049929213 m 4.985592\n",
      "106 Train Loss 0.15676653 Test RE 9.500000670671895e-05 c 1.0008833 k 0.049928878 m 4.9856353\n",
      "107 Train Loss 0.14924608 Test RE 8.713151018276103e-05 c 1.0014784 k 0.049923446 m 4.98287\n",
      "108 Train Loss 0.14390709 Test RE 8.470467726786367e-05 c 1.0015134 k 0.04992998 m 4.979511\n",
      "109 Train Loss 0.13597324 Test RE 8.347761020903121e-05 c 1.0009118 k 0.049944274 m 4.9814024\n",
      "110 Train Loss 0.12614916 Test RE 8.36951584436128e-05 c 1.0008273 k 0.049951866 m 4.9863358\n",
      "111 Train Loss 0.12049583 Test RE 7.765111546123334e-05 c 1.001116 k 0.049953047 m 4.9924283\n",
      "112 Train Loss 0.11667671 Test RE 7.653914349163464e-05 c 1.0006766 k 0.049956605 m 4.9940715\n",
      "113 Train Loss 0.11393442 Test RE 7.960674073664375e-05 c 1.0007136 k 0.049952716 m 4.9867187\n",
      "114 Train Loss 0.11077684 Test RE 8.242697933742862e-05 c 1.0008638 k 0.049952667 m 4.9849434\n",
      "115 Train Loss 0.104720235 Test RE 7.790894081146878e-05 c 1.0008006 k 0.049961485 m 4.9920483\n",
      "116 Train Loss 0.09433298 Test RE 6.622297005826942e-05 c 1.0004164 k 0.04996659 m 4.9954743\n",
      "117 Train Loss 0.083767734 Test RE 6.144766241190485e-05 c 1.0005485 k 0.049962875 m 4.9878755\n",
      "118 Train Loss 0.07459587 Test RE 5.649307015261957e-05 c 1.0010728 k 0.049952134 m 4.992121\n",
      "119 Train Loss 0.068837896 Test RE 5.6413697500320715e-05 c 1.0006617 k 0.049965717 m 4.9992037\n",
      "120 Train Loss 0.063411295 Test RE 5.923084502092164e-05 c 1.0003158 k 0.04997485 m 4.9973183\n",
      "121 Train Loss 0.060841538 Test RE 5.5885380456472124e-05 c 1.0004519 k 0.049971923 m 4.998792\n",
      "122 Train Loss 0.05785446 Test RE 4.795867765401191e-05 c 1.000373 k 0.049974587 m 5.00346\n",
      "123 Train Loss 0.055452712 Test RE 4.377800758760528e-05 c 1.0003155 k 0.04997904 m 5.004617\n",
      "124 Train Loss 0.05054384 Test RE 4.2458358771077204e-05 c 1.0002304 k 0.04998321 m 5.0009527\n",
      "125 Train Loss 0.044754926 Test RE 3.782203166493263e-05 c 1.0004207 k 0.049975954 m 4.9983673\n",
      "126 Train Loss 0.042606976 Test RE 3.7324696212373e-05 c 1.0003577 k 0.04998089 m 5.0009484\n",
      "127 Train Loss 0.038897738 Test RE 3.946071201431936e-05 c 1.0002133 k 0.049990546 m 5.0050235\n",
      "128 Train Loss 0.035845533 Test RE 3.579032200469441e-05 c 1.0003382 k 0.049990933 m 5.003201\n",
      "129 Train Loss 0.032845795 Test RE 2.5761691292831456e-05 c 1.0002738 k 0.049988054 m 5.000369\n",
      "130 Train Loss 0.030867347 Test RE 2.3492294957286864e-05 c 1.000079 k 0.049991056 m 5.001798\n",
      "131 Train Loss 0.028919995 Test RE 2.415678437564065e-05 c 1.0000986 k 0.04999296 m 5.001029\n",
      "132 Train Loss 0.027248958 Test RE 2.46873430701217e-05 c 1.0002768 k 0.049988877 m 4.9974327\n",
      "133 Train Loss 0.025105298 Test RE 3.0151239826327532e-05 c 1.000115 k 0.04998713 m 4.9961877\n",
      "134 Train Loss 0.022836683 Test RE 2.8737008828334767e-05 c 1.0000361 k 0.049993612 m 4.999412\n",
      "135 Train Loss 0.021857927 Test RE 2.955899087068066e-05 c 1.0001953 k 0.049991187 m 4.999337\n",
      "136 Train Loss 0.02073208 Test RE 3.168956075197972e-05 c 1.0003175 k 0.049987547 m 4.9950175\n",
      "137 Train Loss 0.017866658 Test RE 2.8429005994039343e-05 c 0.999931 k 0.0499944 m 4.9941053\n",
      "138 Train Loss 0.015579596 Test RE 2.8332248034791832e-05 c 0.9997749 k 0.050003562 m 5.0017457\n",
      "139 Train Loss 0.013410985 Test RE 2.5507271648867058e-05 c 1.0000118 k 0.050004635 m 5.0031\n",
      "140 Train Loss 0.012473328 Test RE 2.3291879836074952e-05 c 0.9999966 k 0.050002486 m 4.9996734\n",
      "141 Train Loss 0.011693453 Test RE 2.176477249780442e-05 c 0.9999921 k 0.04999962 m 4.998874\n",
      "142 Train Loss 0.011346048 Test RE 2.340733880779696e-05 c 0.9999807 k 0.05000055 m 5.0002646\n",
      "143 Train Loss 0.010958569 Test RE 2.420957810648486e-05 c 0.999979 k 0.050001297 m 5.0006814\n",
      "144 Train Loss 0.010530569 Test RE 2.325413643219555e-05 c 0.9999154 k 0.05000299 m 4.9999247\n",
      "145 Train Loss 0.010118892 Test RE 2.296133814839479e-05 c 0.9998328 k 0.050005957 m 5.000107\n",
      "146 Train Loss 0.009119294 Test RE 2.207165404282054e-05 c 0.99999374 k 0.05000447 m 5.0005116\n",
      "147 Train Loss 0.008652215 Test RE 2.0032941151836024e-05 c 1.0000105 k 0.050004028 m 5.000498\n",
      "148 Train Loss 0.008386906 Test RE 2.0217970664896964e-05 c 0.9998636 k 0.050005917 m 5.0007305\n",
      "149 Train Loss 0.008266966 Test RE 2.0220021462544144e-05 c 0.99982846 k 0.05000648 m 5.0009594\n",
      "150 Train Loss 0.0080774715 Test RE 1.9461124432677346e-05 c 0.99985665 k 0.050006285 m 5.0008388\n",
      "151 Train Loss 0.0073868656 Test RE 1.9935962688399468e-05 c 0.9999055 k 0.05000553 m 4.999805\n",
      "152 Train Loss 0.0069238506 Test RE 2.168866922993078e-05 c 0.9999361 k 0.050003916 m 4.9990854\n",
      "153 Train Loss 0.006745569 Test RE 2.2322490268556297e-05 c 1.0000006 k 0.050002318 m 4.998681\n",
      "154 Train Loss 0.0066814553 Test RE 2.2277726945588568e-05 c 0.99999195 k 0.0500024 m 4.998956\n",
      "155 Train Loss 0.006602682 Test RE 2.2296949197961135e-05 c 0.999933 k 0.050003905 m 4.9996743\n",
      "156 Train Loss 0.0063903164 Test RE 2.2531493515257464e-05 c 0.99983555 k 0.050007265 m 5.001296\n",
      "157 Train Loss 0.005747377 Test RE 2.070689555993235e-05 c 0.9998976 k 0.05000735 m 5.0018177\n",
      "158 Train Loss 0.0047810692 Test RE 1.6831768335119542e-05 c 1.0000198 k 0.050005134 m 5.00112\n",
      "159 Train Loss 0.0037460653 Test RE 1.3097731221361456e-05 c 0.9998515 k 0.050004326 m 5.001132\n",
      "160 Train Loss 0.0032891105 Test RE 1.1041165127083383e-05 c 0.99982023 k 0.05000396 m 5.001128\n",
      "161 Train Loss 0.0031297402 Test RE 1.0174594933769556e-05 c 0.9999046 k 0.050002012 m 4.9999948\n",
      "162 Train Loss 0.003037697 Test RE 9.402561720664873e-06 c 1.0000429 k 0.049999032 m 4.999283\n",
      "163 Train Loss 0.0029284586 Test RE 8.54939105850442e-06 c 1.0000765 k 0.049998965 m 4.999934\n",
      "164 Train Loss 0.0029167815 Test RE 8.512906664941312e-06 c 1.0000502 k 0.049999475 m 5.000093\n",
      "165 Train Loss 0.002915801 Test RE 8.510545914224485e-06 c 1.0000478 k 0.049999513 m 5.0001016\n",
      "166 Train Loss 0.0029068107 Test RE 8.470344169448966e-06 c 1.0000116 k 0.049999975 m 5.000198\n",
      "167 Train Loss 0.0029063104 Test RE 8.464373568146603e-06 c 1.0000106 k 0.049999993 m 5.000198\n",
      "168 Train Loss 0.0029059444 Test RE 8.45912299386468e-06 c 1.0000086 k 0.05000002 m 5.000196\n",
      "169 Train Loss 0.0029054335 Test RE 8.4493822895988e-06 c 1.0000068 k 0.050000012 m 5.0001874\n",
      "170 Train Loss 0.0029031767 Test RE 8.419902725501467e-06 c 1.0000017 k 0.050000023 m 5.0001597\n",
      "171 Train Loss 0.0029031767 Test RE 8.419902725501467e-06 c 1.0000017 k 0.050000023 m 5.0001597\n",
      "172 Train Loss 0.0029031767 Test RE 8.419902725501467e-06 c 1.0000017 k 0.050000023 m 5.0001597\n",
      "173 Train Loss 0.0029031767 Test RE 8.419902725501467e-06 c 1.0000017 k 0.050000023 m 5.0001597\n",
      "174 Train Loss 0.0029031767 Test RE 8.419902725501467e-06 c 1.0000017 k 0.050000023 m 5.0001597\n",
      "175 Train Loss 0.0029031767 Test RE 8.419902725501467e-06 c 1.0000017 k 0.050000023 m 5.0001597\n",
      "176 Train Loss 0.0029031767 Test RE 8.419902725501467e-06 c 1.0000017 k 0.050000023 m 5.0001597\n",
      "177 Train Loss 0.0029031767 Test RE 8.419902725501467e-06 c 1.0000017 k 0.050000023 m 5.0001597\n",
      "178 Train Loss 0.0029031767 Test RE 8.419902725501467e-06 c 1.0000017 k 0.050000023 m 5.0001597\n",
      "179 Train Loss 0.0029031767 Test RE 8.419902725501467e-06 c 1.0000017 k 0.050000023 m 5.0001597\n",
      "180 Train Loss 0.0029031767 Test RE 8.419902725501467e-06 c 1.0000017 k 0.050000023 m 5.0001597\n",
      "181 Train Loss 0.0029031767 Test RE 8.419902725501467e-06 c 1.0000017 k 0.050000023 m 5.0001597\n",
      "182 Train Loss 0.0029031767 Test RE 8.419902725501467e-06 c 1.0000017 k 0.050000023 m 5.0001597\n",
      "183 Train Loss 0.0029031767 Test RE 8.419902725501467e-06 c 1.0000017 k 0.050000023 m 5.0001597\n",
      "184 Train Loss 0.0029031767 Test RE 8.419902725501467e-06 c 1.0000017 k 0.050000023 m 5.0001597\n",
      "185 Train Loss 0.0029031767 Test RE 8.419902725501467e-06 c 1.0000017 k 0.050000023 m 5.0001597\n",
      "186 Train Loss 0.0029031767 Test RE 8.419902725501467e-06 c 1.0000017 k 0.050000023 m 5.0001597\n",
      "187 Train Loss 0.0029031767 Test RE 8.419902725501467e-06 c 1.0000017 k 0.050000023 m 5.0001597\n",
      "188 Train Loss 0.0029031767 Test RE 8.419902725501467e-06 c 1.0000017 k 0.050000023 m 5.0001597\n",
      "189 Train Loss 0.0029031767 Test RE 8.419902725501467e-06 c 1.0000017 k 0.050000023 m 5.0001597\n",
      "190 Train Loss 0.0029031767 Test RE 8.419902725501467e-06 c 1.0000017 k 0.050000023 m 5.0001597\n",
      "191 Train Loss 0.0029031767 Test RE 8.419902725501467e-06 c 1.0000017 k 0.050000023 m 5.0001597\n",
      "192 Train Loss 0.0029031767 Test RE 8.419902725501467e-06 c 1.0000017 k 0.050000023 m 5.0001597\n",
      "193 Train Loss 0.0029031767 Test RE 8.419902725501467e-06 c 1.0000017 k 0.050000023 m 5.0001597\n",
      "194 Train Loss 0.0029031767 Test RE 8.419902725501467e-06 c 1.0000017 k 0.050000023 m 5.0001597\n",
      "195 Train Loss 0.0029031767 Test RE 8.419902725501467e-06 c 1.0000017 k 0.050000023 m 5.0001597\n",
      "196 Train Loss 0.0029031767 Test RE 8.419902725501467e-06 c 1.0000017 k 0.050000023 m 5.0001597\n",
      "197 Train Loss 0.0029031767 Test RE 8.419902725501467e-06 c 1.0000017 k 0.050000023 m 5.0001597\n",
      "198 Train Loss 0.0029031767 Test RE 8.419902725501467e-06 c 1.0000017 k 0.050000023 m 5.0001597\n",
      "199 Train Loss 0.0029031767 Test RE 8.419902725501467e-06 c 1.0000017 k 0.050000023 m 5.0001597\n",
      "Training time: 59.56\n",
      "Training time: 59.56\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 165367.72 Test RE 0.27075178486538154 c -0.07040392 k 0.034504257 m 0.000701567\n",
      "1 Train Loss 51953.15 Test RE 0.14986456672096418 c -0.08141308 k 0.07948276 m 0.00089241535\n",
      "2 Train Loss 46038.758 Test RE 0.1405531521392348 c -0.074932866 k 0.06328473 m 0.0011929299\n",
      "3 Train Loss 43880.746 Test RE 0.13770944669730723 c 0.05363027 k 0.0618189 m 0.0048698573\n",
      "4 Train Loss 24151.87 Test RE 0.10184006494387862 c 0.6880589 k 0.04993444 m 0.023520393\n",
      "5 Train Loss 6971.1733 Test RE 0.049809732376571936 c 1.8144975 k 0.01631385 m 0.05693826\n",
      "6 Train Loss 3729.58 Test RE 0.03043874720023595 c 2.0552268 k 0.030307332 m 0.06441421\n",
      "7 Train Loss 2949.6143 Test RE 0.029390409765802865 c 1.8302683 k 0.03035462 m 0.06107222\n",
      "8 Train Loss 2156.019 Test RE 0.025048527633940724 c 1.5692103 k 0.035150565 m 0.060383562\n",
      "9 Train Loss 1195.755 Test RE 0.01719617645818854 c 1.1410239 k 0.043980855 m 0.06291057\n",
      "10 Train Loss 847.40283 Test RE 0.012966059485667931 c 1.032857 k 0.044561017 m 0.06619633\n",
      "11 Train Loss 668.14685 Test RE 0.010931970409839505 c 0.93441105 k 0.04686739 m 0.07523534\n",
      "12 Train Loss 552.96075 Test RE 0.008488444594633063 c 0.93143463 k 0.04674666 m 0.09052091\n",
      "13 Train Loss 414.19745 Test RE 0.005631742658197162 c 0.97517824 k 0.04662996 m 0.12281181\n",
      "14 Train Loss 363.37958 Test RE 0.005752668152831825 c 1.0608408 k 0.04526329 m 0.138976\n",
      "15 Train Loss 330.4662 Test RE 0.005764107170182865 c 1.12251 k 0.04472507 m 0.16961966\n",
      "16 Train Loss 304.43164 Test RE 0.004897560216368085 c 1.1199701 k 0.044567134 m 0.21006988\n",
      "17 Train Loss 284.6435 Test RE 0.004940606849935311 c 1.1162579 k 0.044939145 m 0.26797923\n",
      "18 Train Loss 272.05682 Test RE 0.004328813910071696 c 1.1180696 k 0.04504311 m 0.30820367\n",
      "19 Train Loss 258.998 Test RE 0.004531156054160429 c 1.1449772 k 0.044710428 m 0.40523437\n",
      "20 Train Loss 237.64844 Test RE 0.004679707589017086 c 1.1500276 k 0.044782523 m 0.6292385\n",
      "21 Train Loss 227.02281 Test RE 0.004584984361194247 c 1.0873203 k 0.045770437 m 0.7331813\n",
      "22 Train Loss 212.2606 Test RE 0.00455325932790842 c 1.1224358 k 0.0458084 m 0.8889845\n",
      "23 Train Loss 189.52011 Test RE 0.004186328686739441 c 1.1260636 k 0.045386735 m 1.2869469\n",
      "24 Train Loss 162.39647 Test RE 0.00470778327742328 c 1.0924352 k 0.0468085 m 1.6095774\n",
      "25 Train Loss 145.63564 Test RE 0.004224605994679597 c 1.0638155 k 0.047076564 m 1.8614964\n",
      "26 Train Loss 129.55127 Test RE 0.003867073238601376 c 1.0818609 k 0.047116973 m 2.0645971\n",
      "27 Train Loss 109.51799 Test RE 0.004219788684617145 c 1.0660992 k 0.047099095 m 2.4067824\n",
      "28 Train Loss 86.7081 Test RE 0.0034763619125974233 c 1.0632004 k 0.047790155 m 2.662801\n",
      "29 Train Loss 80.04826 Test RE 0.003245311627328689 c 1.0578939 k 0.047922093 m 2.8122005\n",
      "30 Train Loss 70.67233 Test RE 0.0030945403318860167 c 1.0499281 k 0.04822541 m 3.0106409\n",
      "31 Train Loss 58.20302 Test RE 0.0027497374240716826 c 1.0525997 k 0.04830559 m 3.3108668\n",
      "32 Train Loss 50.295677 Test RE 0.0025736585125299065 c 1.0419359 k 0.048677895 m 3.5494118\n",
      "33 Train Loss 44.31026 Test RE 0.0021625771727695237 c 1.0213225 k 0.049070522 m 3.7190793\n",
      "34 Train Loss 39.148094 Test RE 0.0020951088164582947 c 1.0302064 k 0.04900146 m 3.8198423\n",
      "35 Train Loss 37.611694 Test RE 0.002054407760744932 c 1.029354 k 0.04913384 m 3.9402943\n",
      "36 Train Loss 34.9268 Test RE 0.0018211358489434054 c 1.022562 k 0.049334157 m 4.112261\n",
      "37 Train Loss 33.74754 Test RE 0.0017443018758111206 c 1.0063232 k 0.049583137 m 4.220618\n",
      "38 Train Loss 32.134285 Test RE 0.0017164303478250775 c 0.9902127 k 0.04991634 m 4.2354293\n",
      "39 Train Loss 30.229519 Test RE 0.0016276975210393558 c 0.9937885 k 0.049740843 m 4.121128\n",
      "40 Train Loss 29.188694 Test RE 0.001578471538233472 c 0.9993613 k 0.049624626 m 4.1491017\n",
      "41 Train Loss 27.548145 Test RE 0.00163873365963198 c 1.0087105 k 0.049529146 m 4.1591377\n",
      "42 Train Loss 26.864328 Test RE 0.0016006396578159218 c 1.0102814 k 0.049537845 m 4.150731\n",
      "43 Train Loss 24.970722 Test RE 0.0014692786982393943 c 1.0199984 k 0.04932659 m 4.3100142\n",
      "44 Train Loss 23.651022 Test RE 0.0014888854353267795 c 1.0158151 k 0.049566276 m 4.347037\n",
      "45 Train Loss 20.724064 Test RE 0.001316647033902948 c 1.0033417 k 0.04983826 m 4.5365677\n",
      "46 Train Loss 17.795681 Test RE 0.0011307216618717437 c 1.0113881 k 0.04966502 m 4.683895\n",
      "47 Train Loss 15.898683 Test RE 0.0010719194774468843 c 1.0251421 k 0.049526 m 4.6655746\n",
      "48 Train Loss 13.613548 Test RE 0.0009612525404028559 c 1.018565 k 0.049496103 m 4.6164985\n",
      "49 Train Loss 11.339416 Test RE 0.0009349638004044163 c 1.0102339 k 0.049713735 m 4.765724\n",
      "50 Train Loss 9.928312 Test RE 0.0008685925959854381 c 1.0107998 k 0.04984669 m 4.864822\n",
      "51 Train Loss 8.134232 Test RE 0.0007987878716151547 c 1.0003743 k 0.05000075 m 4.8835044\n",
      "52 Train Loss 7.469233 Test RE 0.0007452987594146211 c 0.99783206 k 0.0501146 m 5.003576\n",
      "53 Train Loss 7.134567 Test RE 0.0007794981297717537 c 1.001972 k 0.050085217 m 5.066346\n",
      "54 Train Loss 6.0146303 Test RE 0.0008939097475623605 c 1.0025434 k 0.049991876 m 5.083247\n",
      "55 Train Loss 5.2992954 Test RE 0.0008248109645310842 c 0.99776757 k 0.04997307 m 5.032322\n",
      "56 Train Loss 5.158206 Test RE 0.0008194880357241484 c 1.0012643 k 0.049874216 m 4.97902\n",
      "57 Train Loss 5.023775 Test RE 0.000799718503120605 c 1.0046717 k 0.049796086 m 4.936327\n",
      "58 Train Loss 4.915165 Test RE 0.0007620171114858734 c 1.0048071 k 0.049775276 m 4.92493\n",
      "59 Train Loss 4.785504 Test RE 0.0007353008376130351 c 1.0056363 k 0.04977279 m 4.925441\n",
      "60 Train Loss 4.72731 Test RE 0.0007123337503898888 c 1.0059605 k 0.049771253 m 4.9109645\n",
      "61 Train Loss 4.6385846 Test RE 0.0006566152774556281 c 1.0048305 k 0.04978531 m 4.8878164\n",
      "62 Train Loss 4.5431275 Test RE 0.0006509442615935189 c 1.0053625 k 0.049761735 m 4.8811593\n",
      "63 Train Loss 4.3234243 Test RE 0.0007265446690898681 c 1.0065907 k 0.049744133 m 4.888851\n",
      "64 Train Loss 4.1940413 Test RE 0.0007848768786037838 c 1.004038 k 0.049812023 m 4.9243875\n",
      "65 Train Loss 3.5694883 Test RE 0.000731336317869678 c 0.99962217 k 0.049896687 m 5.0606346\n",
      "66 Train Loss 2.743364 Test RE 0.0005106836693513743 c 1.0060408 k 0.04989102 m 5.0925937\n",
      "67 Train Loss 2.3791652 Test RE 0.0005178574812153605 c 1.0039229 k 0.049846813 m 4.985935\n",
      "68 Train Loss 2.3060687 Test RE 0.000491191875223587 c 1.0037903 k 0.049820837 m 4.951756\n",
      "69 Train Loss 2.2688704 Test RE 0.0004678613938047841 c 1.0032815 k 0.049833458 m 4.9547257\n",
      "70 Train Loss 2.177939 Test RE 0.00046470466600535886 c 1.0013524 k 0.04987854 m 4.9776936\n",
      "71 Train Loss 2.111469 Test RE 0.0004845791643709657 c 1.0037458 k 0.049841974 m 4.9726057\n",
      "72 Train Loss 2.0742817 Test RE 0.00048142627859510957 c 1.0059946 k 0.049798317 m 4.9602017\n",
      "73 Train Loss 2.0197446 Test RE 0.00046564771905601267 c 1.0056708 k 0.049785096 m 4.946628\n",
      "74 Train Loss 1.9859148 Test RE 0.00046351970811405504 c 1.0039299 k 0.049810644 m 4.951167\n",
      "75 Train Loss 1.9712672 Test RE 0.0004673904212384111 c 1.0030686 k 0.049834147 m 4.9650993\n",
      "76 Train Loss 1.9510024 Test RE 0.00046394000175831857 c 1.0033001 k 0.049838435 m 4.964962\n",
      "77 Train Loss 1.93849 Test RE 0.0004616948360943842 c 1.0041429 k 0.04982036 m 4.9599643\n",
      "78 Train Loss 1.8728335 Test RE 0.0004513781357104032 c 1.0046757 k 0.049817815 m 4.9710183\n",
      "79 Train Loss 1.788096 Test RE 0.00044795547722825603 c 1.0021869 k 0.049853306 m 4.963573\n",
      "80 Train Loss 1.6109159 Test RE 0.00039859911737225154 c 1.001369 k 0.049859766 m 4.9555273\n",
      "81 Train Loss 1.5145139 Test RE 0.0003868930787338522 c 1.0031466 k 0.049837656 m 4.957944\n",
      "82 Train Loss 1.4965595 Test RE 0.0003832436051090992 c 1.004061 k 0.049833633 m 4.9701514\n",
      "83 Train Loss 1.4671538 Test RE 0.0003701549731274682 c 1.0034295 k 0.049861446 m 4.988703\n",
      "84 Train Loss 1.4008739 Test RE 0.0003544132424373639 c 1.0019876 k 0.049887072 m 5.0023646\n",
      "85 Train Loss 1.3415265 Test RE 0.00036026760770529803 c 1.0019308 k 0.049891096 m 5.0003605\n",
      "86 Train Loss 1.3039415 Test RE 0.00035878345765286363 c 1.0019743 k 0.04990429 m 5.0043025\n",
      "87 Train Loss 1.2062678 Test RE 0.00033723419281142487 c 1.0019332 k 0.04990543 m 5.00862\n",
      "88 Train Loss 1.1214359 Test RE 0.0003223176969018171 c 1.0025196 k 0.04989715 m 5.0045033\n",
      "89 Train Loss 1.0821891 Test RE 0.0003115246409006245 c 1.0024033 k 0.04989745 m 5.003705\n",
      "90 Train Loss 1.0640368 Test RE 0.0002997908367833595 c 1.0021142 k 0.04990005 m 5.0025835\n",
      "91 Train Loss 1.0548742 Test RE 0.0002966501469808094 c 1.0024793 k 0.04989058 m 5.0001802\n",
      "92 Train Loss 1.0420175 Test RE 0.00029722659592090506 c 1.0024763 k 0.049895138 m 4.998717\n",
      "93 Train Loss 1.0289143 Test RE 0.000291985093689234 c 1.0013614 k 0.049910933 m 4.99843\n",
      "94 Train Loss 1.018596 Test RE 0.0002846066884098105 c 1.0012772 k 0.04990348 m 4.9951944\n",
      "95 Train Loss 1.0075881 Test RE 0.00028410126647400826 c 1.0011512 k 0.049907003 m 4.99723\n",
      "96 Train Loss 0.9884949 Test RE 0.00029062367653438423 c 1.002112 k 0.049906094 m 5.00894\n",
      "97 Train Loss 0.9694952 Test RE 0.00029220569472539654 c 1.0024258 k 0.04990877 m 5.0149775\n",
      "98 Train Loss 0.956457 Test RE 0.0002923475202276592 c 1.0017697 k 0.049905855 m 5.0049043\n",
      "99 Train Loss 0.93532205 Test RE 0.0002899128121857515 c 1.0011148 k 0.049899187 m 4.9888964\n",
      "100 Train Loss 0.87211066 Test RE 0.0002797091711845318 c 1.0012485 k 0.049905684 m 4.9915886\n",
      "101 Train Loss 0.7266358 Test RE 0.0002652012476947499 c 1.0023506 k 0.049903724 m 5.0048733\n",
      "102 Train Loss 0.5867476 Test RE 0.0002514743354203279 c 1.0018655 k 0.049906746 m 5.0008216\n",
      "103 Train Loss 0.55230176 Test RE 0.00024996816424014995 c 1.0010923 k 0.049919747 m 4.995455\n",
      "104 Train Loss 0.53285897 Test RE 0.0002520455036822792 c 1.0011066 k 0.049922556 m 4.9991875\n",
      "105 Train Loss 0.5069324 Test RE 0.00024975576133165565 c 1.0017325 k 0.049919188 m 5.0005054\n",
      "106 Train Loss 0.47871828 Test RE 0.00024330236598535228 c 1.0017523 k 0.049916007 m 5.0025764\n",
      "107 Train Loss 0.44895425 Test RE 0.00023666659316404502 c 1.0014204 k 0.04991854 m 4.9957485\n",
      "108 Train Loss 0.4316995 Test RE 0.00023059350325400945 c 1.0017176 k 0.04990534 m 4.981695\n",
      "109 Train Loss 0.42892236 Test RE 0.0002299301155486841 c 1.0017247 k 0.049903292 m 4.9800467\n",
      "110 Train Loss 0.42674506 Test RE 0.0002306808288902294 c 1.0018103 k 0.049902357 m 4.9808\n",
      "111 Train Loss 0.4236709 Test RE 0.0002329900708666855 c 1.0018821 k 0.0499071 m 4.9846373\n",
      "112 Train Loss 0.4224035 Test RE 0.000233730637791211 c 1.001614 k 0.049911954 m 4.985722\n",
      "113 Train Loss 0.42176023 Test RE 0.00023341466052266067 c 1.0014812 k 0.04991291 m 4.985674\n",
      "114 Train Loss 0.41782838 Test RE 0.0002306852643564492 c 1.0011455 k 0.04991446 m 4.98813\n",
      "115 Train Loss 0.40505207 Test RE 0.00022417494018604092 c 1.0015404 k 0.04991853 m 4.9990754\n",
      "116 Train Loss 0.39362884 Test RE 0.0002210693436058129 c 1.001534 k 0.049926426 m 5.0011883\n",
      "117 Train Loss 0.37708497 Test RE 0.0002200562294777124 c 1.0011598 k 0.04992411 m 4.989775\n",
      "118 Train Loss 0.35934684 Test RE 0.000220235003664557 c 1.0017006 k 0.04989965 m 4.974789\n",
      "119 Train Loss 0.35066974 Test RE 0.00022300948843521654 c 1.0021796 k 0.04989252 m 4.9712567\n",
      "120 Train Loss 0.34275258 Test RE 0.0002256027190763402 c 1.001869 k 0.049902968 m 4.974779\n",
      "121 Train Loss 0.331926 Test RE 0.00022428197695469965 c 1.0014805 k 0.049917784 m 4.9873233\n",
      "122 Train Loss 0.32208353 Test RE 0.00022461442064054655 c 1.0014783 k 0.049915016 m 4.993076\n",
      "123 Train Loss 0.31699228 Test RE 0.00022724792162067512 c 1.0016415 k 0.04991155 m 4.993169\n",
      "124 Train Loss 0.31495085 Test RE 0.00023051332386425974 c 1.0016692 k 0.04991274 m 4.994886\n",
      "125 Train Loss 0.31210133 Test RE 0.0002339491665032569 c 1.0015355 k 0.04991462 m 4.994672\n",
      "126 Train Loss 0.31039816 Test RE 0.00023596883462257938 c 1.0015868 k 0.049910206 m 4.991146\n",
      "127 Train Loss 0.30984867 Test RE 0.0002362256899530468 c 1.0017368 k 0.049906176 m 4.9893727\n",
      "128 Train Loss 0.308789 Test RE 0.00023582654111670232 c 1.0019349 k 0.049903706 m 4.9893537\n",
      "129 Train Loss 0.3069498 Test RE 0.00023377649503161517 c 1.0019904 k 0.04990388 m 4.9915485\n",
      "130 Train Loss 0.30622602 Test RE 0.00023317257298814253 c 1.001866 k 0.04990652 m 4.9924455\n",
      "131 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "132 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "133 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "134 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "135 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "136 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "137 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "138 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "139 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "140 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "141 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "142 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "143 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "144 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "145 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "146 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "147 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "148 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "149 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "150 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "151 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "152 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "153 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "154 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "155 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "156 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "157 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "158 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "159 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "160 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "161 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "162 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "163 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "164 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "165 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "166 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "167 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "168 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "169 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "170 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "171 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "172 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "173 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "174 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "175 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "176 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "177 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "178 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "179 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "180 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "181 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "182 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "183 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "184 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "185 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "186 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "187 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "188 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "189 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "190 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "191 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "192 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "193 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "194 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "195 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "196 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "197 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "198 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "199 Train Loss 0.30595782 Test RE 0.00023310601371403052 c 1.001813 k 0.04990749 m 4.992662\n",
      "Training time: 54.27\n",
      "Training time: 54.27\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 210871.94 Test RE 0.3081621522802761 c 0.030930603 k 0.066628434 m -8.138393e-05\n",
      "1 Train Loss 158585.7 Test RE 0.25328968053287704 c 0.03515098 k 0.17359552 m -7.605473e-05\n",
      "2 Train Loss 54677.215 Test RE 0.15380518766534412 c 0.024212971 k 0.06571986 m -4.4292803e-05\n",
      "3 Train Loss 53516.844 Test RE 0.15193336084069756 c 0.03137509 k 0.05177374 m 0.00010311097\n",
      "4 Train Loss 49961.953 Test RE 0.1464862201837503 c 0.06242788 k 0.06554305 m 0.00065866695\n",
      "5 Train Loss 48235.93 Test RE 0.14362018753966782 c 0.11720699 k 0.060051244 m 0.0015795634\n",
      "6 Train Loss 48081.496 Test RE 0.143337508830946 c 0.13788839 k 0.05838175 m 0.0018729381\n",
      "7 Train Loss 47888.598 Test RE 0.1430011515578305 c 0.23610488 k 0.061357968 m 0.0028227435\n",
      "8 Train Loss 47363.56 Test RE 0.1423398358976483 c 0.33739078 k 0.055425595 m 0.0035597796\n",
      "9 Train Loss 46826.01 Test RE 0.14149453478684754 c 0.47506925 k 0.043200392 m 0.0039577624\n",
      "10 Train Loss 46250.562 Test RE 0.14093184422706628 c 0.4959689 k 0.05248234 m 0.0036419835\n",
      "11 Train Loss 45932.86 Test RE 0.1403009687076048 c 0.48361173 k 0.057962645 m 0.0031677296\n",
      "12 Train Loss 45085.68 Test RE 0.13927791159622718 c 0.58525527 k 0.044241734 m 0.00305611\n",
      "13 Train Loss 41088.582 Test RE 0.13463083965706985 c 1.8548285 k 0.025591455 m 0.0064950697\n",
      "14 Train Loss 27262.25 Test RE 0.10786819951965465 c 2.7883863 k 0.005001437 m 0.010459949\n",
      "15 Train Loss 18931.979 Test RE 0.0813824460617299 c 3.7955956 k -0.007096134 m 0.013998886\n",
      "16 Train Loss 13297.248 Test RE 0.0537239013765235 c 3.9028752 k -0.0066012833 m 0.0139457565\n",
      "17 Train Loss 12369.4 Test RE 0.05021588048566553 c 3.8119614 k -0.0017762516 m 0.013965055\n",
      "18 Train Loss 11025.807 Test RE 0.04594223192515885 c 3.5251303 k -0.0035091161 m 0.014431901\n",
      "19 Train Loss 7911.2393 Test RE 0.03649602220093427 c 2.8641498 k 0.020985685 m 0.012837222\n",
      "20 Train Loss 5064.051 Test RE 0.029503455441311943 c 1.5491405 k 0.030991606 m 0.010052181\n",
      "21 Train Loss 3482.7327 Test RE 0.025649673965130237 c 1.014588 k 0.045742184 m 0.009264683\n",
      "22 Train Loss 3195.321 Test RE 0.025291793095340756 c 1.1150209 k 0.04414367 m 0.00941828\n",
      "23 Train Loss 3190.4556 Test RE 0.02515553960286327 c 1.0925106 k 0.04422455 m 0.009423775\n",
      "24 Train Loss 3188.9885 Test RE 0.02510099787370959 c 1.0791726 k 0.044423107 m 0.009575996\n",
      "25 Train Loss 3170.0586 Test RE 0.025070590075596606 c 1.0428658 k 0.044975925 m 0.013086753\n",
      "26 Train Loss 3016.7388 Test RE 0.025606563314409077 c 1.1186519 k 0.043469835 m 0.033184245\n",
      "27 Train Loss 2877.1733 Test RE 0.026202323380346813 c 1.1892427 k 0.0426351 m 0.05625973\n",
      "28 Train Loss 2613.1472 Test RE 0.026062598325501386 c 1.0583787 k 0.044735394 m 0.10105019\n",
      "29 Train Loss 1961.2893 Test RE 0.02463533383615662 c 0.9922457 k 0.046089906 m 0.21613073\n",
      "30 Train Loss 1643.8367 Test RE 0.022478016726584657 c 0.93880785 k 0.046533898 m 0.33534977\n",
      "31 Train Loss 1495.9009 Test RE 0.022052865706159854 c 1.1262767 k 0.0445954 m 0.34467742\n",
      "32 Train Loss 1223.7925 Test RE 0.019775086702666048 c 1.0761251 k 0.04468391 m 0.42923766\n",
      "33 Train Loss 1148.4155 Test RE 0.01894394046426336 c 1.1231651 k 0.044313826 m 0.4465345\n",
      "34 Train Loss 1088.6091 Test RE 0.018784533278484193 c 1.0316461 k 0.045311697 m 0.45997146\n",
      "35 Train Loss 1011.54517 Test RE 0.0187135371358176 c 1.1204371 k 0.044527687 m 0.4726091\n",
      "36 Train Loss 977.9947 Test RE 0.018476902220657947 c 1.1518946 k 0.043480314 m 0.5068587\n",
      "37 Train Loss 951.4883 Test RE 0.017986070114451285 c 1.1515446 k 0.043866728 m 0.5437818\n",
      "38 Train Loss 872.73584 Test RE 0.017298610547339468 c 1.1185983 k 0.04465712 m 0.6504985\n",
      "39 Train Loss 813.3892 Test RE 0.016656522716078054 c 1.1566712 k 0.043840967 m 0.76160234\n",
      "40 Train Loss 723.9697 Test RE 0.01527815783762433 c 1.0621227 k 0.04586673 m 0.9383316\n",
      "41 Train Loss 687.92883 Test RE 0.014204766512491652 c 1.0887365 k 0.04516582 m 0.999799\n",
      "42 Train Loss 645.1089 Test RE 0.013326289712757157 c 1.05822 k 0.045932204 m 1.0315306\n",
      "43 Train Loss 558.5373 Test RE 0.011605191671250676 c 1.1253989 k 0.044959337 m 1.0958601\n",
      "44 Train Loss 490.33322 Test RE 0.010308570798550262 c 1.0437042 k 0.04662553 m 1.2080544\n",
      "45 Train Loss 404.53687 Test RE 0.007914436715228667 c 1.0409033 k 0.046368316 m 1.2889661\n",
      "46 Train Loss 306.46472 Test RE 0.007353334966618083 c 1.091261 k 0.046392057 m 1.3251709\n",
      "47 Train Loss 279.27164 Test RE 0.006142766545692315 c 1.0825001 k 0.045864422 m 1.3662903\n",
      "48 Train Loss 262.43405 Test RE 0.005491281782697572 c 1.0842805 k 0.04650774 m 1.4042599\n",
      "49 Train Loss 241.44148 Test RE 0.004901047038303115 c 1.1244401 k 0.045546528 m 1.4328046\n",
      "50 Train Loss 220.99004 Test RE 0.00474353585285633 c 1.0828432 k 0.046425816 m 1.4764581\n",
      "51 Train Loss 214.02827 Test RE 0.004651203760236078 c 1.0723082 k 0.046490613 m 1.4782168\n",
      "52 Train Loss 206.7771 Test RE 0.004435659796930109 c 1.0880728 k 0.04618882 m 1.4764948\n",
      "53 Train Loss 202.06383 Test RE 0.00434550961953176 c 1.0753658 k 0.046757117 m 1.4756278\n",
      "54 Train Loss 198.85835 Test RE 0.0043394133582625736 c 1.0823566 k 0.046299417 m 1.4666698\n",
      "55 Train Loss 193.79547 Test RE 0.0042601065735711485 c 1.0881511 k 0.04642412 m 1.4384602\n",
      "56 Train Loss 189.55333 Test RE 0.004063991915056874 c 1.074754 k 0.04669326 m 1.411434\n",
      "57 Train Loss 180.91998 Test RE 0.0043399987128962016 c 1.092279 k 0.046139345 m 1.3576934\n",
      "58 Train Loss 177.9249 Test RE 0.004485007419783259 c 1.0966548 k 0.04612184 m 1.3327829\n",
      "59 Train Loss 174.61215 Test RE 0.004236344952077804 c 1.0946552 k 0.04611163 m 1.3215873\n",
      "60 Train Loss 172.05695 Test RE 0.0037993162589907282 c 1.0877357 k 0.046132654 m 1.3296322\n",
      "61 Train Loss 167.22743 Test RE 0.0036588681783461794 c 1.0958631 k 0.04610309 m 1.3496028\n",
      "62 Train Loss 163.75882 Test RE 0.0038368377253750275 c 1.0797517 k 0.046263035 m 1.3827817\n",
      "63 Train Loss 160.30508 Test RE 0.0035790007829980316 c 1.0922761 k 0.046100594 m 1.410508\n",
      "64 Train Loss 157.79855 Test RE 0.0036070298215023933 c 1.0898135 k 0.046114717 m 1.4506447\n",
      "65 Train Loss 156.67775 Test RE 0.0034728671490830243 c 1.0864983 k 0.046286017 m 1.4811141\n",
      "66 Train Loss 150.94618 Test RE 0.0033193929190005154 c 1.0808272 k 0.046506044 m 1.6211658\n",
      "67 Train Loss 141.13467 Test RE 0.003229838501272862 c 1.0886896 k 0.046202105 m 1.693859\n",
      "68 Train Loss 125.9772 Test RE 0.002951178579359287 c 1.0655234 k 0.046759967 m 1.9813521\n",
      "69 Train Loss 116.98608 Test RE 0.002564547727686985 c 1.0738105 k 0.046943847 m 2.212327\n",
      "70 Train Loss 88.86638 Test RE 0.00337415490138506 c 1.0290303 k 0.048468195 m 3.2709398\n",
      "71 Train Loss 56.496708 Test RE 0.0030919599491345427 c 1.0650531 k 0.048650924 m 3.9944975\n",
      "72 Train Loss 36.513115 Test RE 0.0024598471097194777 c 1.0103617 k 0.04946799 m 4.267144\n",
      "73 Train Loss 20.954355 Test RE 0.0011395077230857689 c 1.0180373 k 0.049263638 m 4.488945\n",
      "74 Train Loss 20.113323 Test RE 0.0012704856839493802 c 1.020789 k 0.049107738 m 4.4547353\n",
      "75 Train Loss 18.404282 Test RE 0.0012328487900315143 c 1.0089793 k 0.049503297 m 4.667826\n",
      "76 Train Loss 14.642624 Test RE 0.0011043382986160798 c 1.0131481 k 0.04967943 m 4.912764\n",
      "77 Train Loss 10.266178 Test RE 0.0010604498690236276 c 1.0048418 k 0.04979481 m 5.0365276\n",
      "78 Train Loss 9.378937 Test RE 0.0010351655441858674 c 1.003444 k 0.049725555 m 4.996102\n",
      "79 Train Loss 8.664798 Test RE 0.001004482542598302 c 1.0033836 k 0.04990303 m 4.9697027\n",
      "80 Train Loss 7.479379 Test RE 0.0009667650445305402 c 1.004526 k 0.04975301 m 5.078509\n",
      "81 Train Loss 7.1197834 Test RE 0.0009737066662252493 c 1.0016536 k 0.04988811 m 5.15299\n",
      "82 Train Loss 6.806326 Test RE 0.0009870052387693253 c 1.0004236 k 0.049936853 m 5.1828732\n",
      "83 Train Loss 6.37313 Test RE 0.0009942681173514135 c 1.0041834 k 0.04987169 m 5.1795635\n",
      "84 Train Loss 5.6364284 Test RE 0.0009319811565230834 c 1.008462 k 0.049685124 m 5.014804\n",
      "85 Train Loss 5.0412803 Test RE 0.000884308090090161 c 1.0008049 k 0.049803734 m 4.9719067\n",
      "86 Train Loss 4.8172355 Test RE 0.0008515121210780405 c 1.0059553 k 0.04969776 m 4.955561\n",
      "87 Train Loss 3.9958832 Test RE 0.0007184617563513432 c 1.006951 k 0.04964828 m 4.8626833\n",
      "88 Train Loss 2.6602802 Test RE 0.0006487445080313711 c 1.0055703 k 0.049728766 m 4.830135\n",
      "89 Train Loss 2.463581 Test RE 0.0006214631261598454 c 1.0061935 k 0.0497159 m 4.830391\n",
      "90 Train Loss 2.122108 Test RE 0.0005774458284200882 c 1.0026952 k 0.04974919 m 4.8458266\n",
      "91 Train Loss 1.6533986 Test RE 0.0005388508434113874 c 1.006223 k 0.04976337 m 4.9447093\n",
      "92 Train Loss 1.4496161 Test RE 0.0005098510317744671 c 1.0021781 k 0.049889516 m 5.010264\n",
      "93 Train Loss 1.3649002 Test RE 0.0004839648455767768 c 1.0006847 k 0.049905565 m 5.008389\n",
      "94 Train Loss 1.2935874 Test RE 0.0004562324912516419 c 1.0027487 k 0.04988823 m 4.986799\n",
      "95 Train Loss 1.2518272 Test RE 0.00044272777925945863 c 1.0029421 k 0.049871083 m 4.971827\n",
      "96 Train Loss 1.2268498 Test RE 0.00043410292734405063 c 1.0019194 k 0.049892224 m 4.9814897\n",
      "97 Train Loss 1.2003391 Test RE 0.00043203975089281445 c 1.0016222 k 0.049918078 m 5.0077753\n",
      "98 Train Loss 1.1584085 Test RE 0.0004260079450414635 c 1.0018421 k 0.049922675 m 5.0170326\n",
      "99 Train Loss 1.1446723 Test RE 0.00042464302083850664 c 1.0012443 k 0.04992547 m 5.0162673\n",
      "100 Train Loss 1.1243236 Test RE 0.00042039769243668864 c 1.0003755 k 0.049932986 m 5.0155735\n",
      "101 Train Loss 1.0758253 Test RE 0.0004001024369601446 c 1.001764 k 0.049925085 m 5.01402\n",
      "102 Train Loss 1.0020622 Test RE 0.00036518573876589315 c 1.0021951 k 0.04991026 m 5.006289\n",
      "103 Train Loss 0.96834683 Test RE 0.0003432479696385833 c 1.0009043 k 0.049932983 m 5.010373\n",
      "104 Train Loss 0.9238923 Test RE 0.00030884579181859927 c 1.0006988 k 0.04994232 m 5.02111\n",
      "105 Train Loss 0.80640227 Test RE 0.00024055542941546008 c 1.0004492 k 0.049955506 m 5.0295444\n",
      "106 Train Loss 0.788584 Test RE 0.0002254685970629516 c 1.0003798 k 0.049973663 m 5.0350375\n",
      "107 Train Loss 0.78386986 Test RE 0.00022263825549071283 c 1.0001111 k 0.049990624 m 5.045571\n",
      "108 Train Loss 0.75601655 Test RE 0.0002200812709851414 c 0.9990168 k 0.05001573 m 5.0782995\n",
      "109 Train Loss 0.61715245 Test RE 0.00021066610356750793 c 0.9991014 k 0.049989402 m 5.052694\n",
      "110 Train Loss 0.5561515 Test RE 0.00019738521664221232 c 0.9997751 k 0.049999297 m 5.0366025\n",
      "111 Train Loss 0.49046612 Test RE 0.00018593459762144594 c 1.0002015 k 0.049962014 m 5.015253\n",
      "112 Train Loss 0.44807625 Test RE 0.00015676098644718168 c 1.0009496 k 0.04994284 m 5.0018916\n",
      "113 Train Loss 0.44013256 Test RE 0.00014905082190898813 c 1.0011921 k 0.049932834 m 5.0014534\n",
      "114 Train Loss 0.43207768 Test RE 0.00014978239390059907 c 1.0009707 k 0.049938526 m 5.006349\n",
      "115 Train Loss 0.41162422 Test RE 0.0001565618344363538 c 1.0006876 k 0.049940955 m 5.0169826\n",
      "116 Train Loss 0.39530304 Test RE 0.0001559271783863082 c 1.0010747 k 0.049931116 m 5.0184436\n",
      "117 Train Loss 0.38271233 Test RE 0.00015444747024230275 c 1.001596 k 0.04991723 m 5.0102053\n",
      "118 Train Loss 0.35797393 Test RE 0.0001573586434806786 c 1.001433 k 0.0499118 m 4.9890165\n",
      "119 Train Loss 0.34951708 Test RE 0.00016125757204739997 c 1.00138 k 0.049907126 m 4.9799337\n",
      "120 Train Loss 0.3476634 Test RE 0.00016486383648587388 c 1.0016986 k 0.04990061 m 4.9769125\n",
      "121 Train Loss 0.3465716 Test RE 0.0001652156139547748 c 1.0020554 k 0.049898952 m 4.9795427\n",
      "122 Train Loss 0.3452452 Test RE 0.00016564782862650847 c 1.0020533 k 0.049900815 m 4.9836154\n",
      "123 Train Loss 0.34016517 Test RE 0.00017173836043430008 c 1.0015409 k 0.049910985 m 4.9903016\n",
      "124 Train Loss 0.33544666 Test RE 0.00017188584525869877 c 1.0016758 k 0.049909074 m 4.9900017\n",
      "125 Train Loss 0.33260405 Test RE 0.00017237429031003452 c 1.0020095 k 0.049899634 m 4.986578\n",
      "126 Train Loss 0.33120978 Test RE 0.0001745301044511528 c 1.0018936 k 0.04989983 m 4.983849\n",
      "127 Train Loss 0.3305146 Test RE 0.00017592546332051437 c 1.0017141 k 0.049901623 m 4.9819956\n",
      "128 Train Loss 0.3264592 Test RE 0.00017663718105979665 c 1.0008854 k 0.049914367 m 4.9786143\n",
      "129 Train Loss 0.30781916 Test RE 0.00016892975706540454 c 1.0005026 k 0.049919777 m 4.981275\n",
      "130 Train Loss 0.29828498 Test RE 0.00016299787190123295 c 1.0014373 k 0.0499108 m 4.9913144\n",
      "131 Train Loss 0.29591188 Test RE 0.00015993316357645402 c 1.0017933 k 0.04990928 m 4.994619\n",
      "132 Train Loss 0.2951022 Test RE 0.00015910631648526145 c 1.0018764 k 0.04990811 m 4.99463\n",
      "133 Train Loss 0.28984553 Test RE 0.0001548380081400428 c 1.0019062 k 0.04990951 m 4.9896426\n",
      "134 Train Loss 0.282813 Test RE 0.0001471290528345606 c 1.0016439 k 0.049912084 m 4.985151\n",
      "135 Train Loss 0.2796575 Test RE 0.00014091112872876486 c 1.0014399 k 0.049917646 m 4.990806\n",
      "136 Train Loss 0.27922404 Test RE 0.00014002288634479764 c 1.0014173 k 0.049918752 m 4.9921303\n",
      "137 Train Loss 0.2772946 Test RE 0.00013684201640115705 c 1.0014045 k 0.04992183 m 4.996949\n",
      "138 Train Loss 0.2753942 Test RE 0.00013506749437770132 c 1.0015011 k 0.049920708 m 4.994675\n",
      "139 Train Loss 0.27430627 Test RE 0.0001325479292995798 c 1.0012708 k 0.04992304 m 4.9933634\n",
      "140 Train Loss 0.273796 Test RE 0.000130968690718729 c 1.0012306 k 0.049924348 m 4.994501\n",
      "141 Train Loss 0.27303368 Test RE 0.00012851781672668886 c 1.0013329 k 0.049925365 m 4.9965906\n",
      "142 Train Loss 0.27179486 Test RE 0.0001255902692335689 c 1.0015844 k 0.049924206 m 4.9978666\n",
      "143 Train Loss 0.2708855 Test RE 0.00012418566945100707 c 1.0016712 k 0.049923085 m 4.9971147\n",
      "144 Train Loss 0.2684677 Test RE 0.00012196421600713175 c 1.0013869 k 0.049928013 m 4.9956436\n",
      "145 Train Loss 0.2664017 Test RE 0.00012166083276982764 c 1.0009013 k 0.049938656 m 4.9977913\n",
      "146 Train Loss 0.2643006 Test RE 0.00012066014819779598 c 1.0008616 k 0.049940575 m 5.001256\n",
      "147 Train Loss 0.26256102 Test RE 0.00011877401013247198 c 1.0010473 k 0.049936827 m 5.001203\n",
      "148 Train Loss 0.26256102 Test RE 0.00011877401013247198 c 1.0010473 k 0.049936827 m 5.001203\n",
      "149 Train Loss 0.26256102 Test RE 0.00011877401013247198 c 1.0010473 k 0.049936827 m 5.001203\n",
      "150 Train Loss 0.26256102 Test RE 0.00011877401013247198 c 1.0010473 k 0.049936827 m 5.001203\n",
      "151 Train Loss 0.26256102 Test RE 0.00011877401013247198 c 1.0010473 k 0.049936827 m 5.001203\n",
      "152 Train Loss 0.26256102 Test RE 0.00011877401013247198 c 1.0010473 k 0.049936827 m 5.001203\n",
      "153 Train Loss 0.26256102 Test RE 0.00011877401013247198 c 1.0010473 k 0.049936827 m 5.001203\n",
      "154 Train Loss 0.26256102 Test RE 0.00011877401013247198 c 1.0010473 k 0.049936827 m 5.001203\n",
      "155 Train Loss 0.26256102 Test RE 0.00011877401013247198 c 1.0010473 k 0.049936827 m 5.001203\n",
      "156 Train Loss 0.26256102 Test RE 0.00011877401013247198 c 1.0010473 k 0.049936827 m 5.001203\n",
      "157 Train Loss 0.26256102 Test RE 0.00011877401013247198 c 1.0010473 k 0.049936827 m 5.001203\n",
      "158 Train Loss 0.26256102 Test RE 0.00011877401013247198 c 1.0010473 k 0.049936827 m 5.001203\n",
      "159 Train Loss 0.26256102 Test RE 0.00011877401013247198 c 1.0010473 k 0.049936827 m 5.001203\n",
      "160 Train Loss 0.26256102 Test RE 0.00011877401013247198 c 1.0010473 k 0.049936827 m 5.001203\n",
      "161 Train Loss 0.26256102 Test RE 0.00011877401013247198 c 1.0010473 k 0.049936827 m 5.001203\n",
      "162 Train Loss 0.26256102 Test RE 0.00011877401013247198 c 1.0010473 k 0.049936827 m 5.001203\n",
      "163 Train Loss 0.26256102 Test RE 0.00011877401013247198 c 1.0010473 k 0.049936827 m 5.001203\n",
      "164 Train Loss 0.26256102 Test RE 0.00011877401013247198 c 1.0010473 k 0.049936827 m 5.001203\n",
      "165 Train Loss 0.26256102 Test RE 0.00011877401013247198 c 1.0010473 k 0.049936827 m 5.001203\n",
      "166 Train Loss 0.26256102 Test RE 0.00011877401013247198 c 1.0010473 k 0.049936827 m 5.001203\n",
      "167 Train Loss 0.26256102 Test RE 0.00011877401013247198 c 1.0010473 k 0.049936827 m 5.001203\n",
      "168 Train Loss 0.26256102 Test RE 0.00011877401013247198 c 1.0010473 k 0.049936827 m 5.001203\n",
      "169 Train Loss 0.26256102 Test RE 0.00011877401013247198 c 1.0010473 k 0.049936827 m 5.001203\n",
      "170 Train Loss 0.26256102 Test RE 0.00011877401013247198 c 1.0010473 k 0.049936827 m 5.001203\n",
      "171 Train Loss 0.26256102 Test RE 0.00011877401013247198 c 1.0010473 k 0.049936827 m 5.001203\n",
      "172 Train Loss 0.26256102 Test RE 0.00011877401013247198 c 1.0010473 k 0.049936827 m 5.001203\n",
      "173 Train Loss 0.26256102 Test RE 0.00011877401013247198 c 1.0010473 k 0.049936827 m 5.001203\n",
      "174 Train Loss 0.26256102 Test RE 0.00011877401013247198 c 1.0010473 k 0.049936827 m 5.001203\n",
      "175 Train Loss 0.26256102 Test RE 0.00011877401013247198 c 1.0010473 k 0.049936827 m 5.001203\n",
      "176 Train Loss 0.26256102 Test RE 0.00011877401013247198 c 1.0010473 k 0.049936827 m 5.001203\n",
      "177 Train Loss 0.26256102 Test RE 0.00011877401013247198 c 1.0010473 k 0.049936827 m 5.001203\n",
      "178 Train Loss 0.26256102 Test RE 0.00011877401013247198 c 1.0010473 k 0.049936827 m 5.001203\n",
      "179 Train Loss 0.26256102 Test RE 0.00011877401013247198 c 1.0010473 k 0.049936827 m 5.001203\n",
      "180 Train Loss 0.26256102 Test RE 0.00011877401013247198 c 1.0010473 k 0.049936827 m 5.001203\n",
      "181 Train Loss 0.26256102 Test RE 0.00011877401013247198 c 1.0010473 k 0.049936827 m 5.001203\n",
      "182 Train Loss 0.26256102 Test RE 0.00011877401013247198 c 1.0010473 k 0.049936827 m 5.001203\n",
      "183 Train Loss 0.26256102 Test RE 0.00011877401013247198 c 1.0010473 k 0.049936827 m 5.001203\n",
      "184 Train Loss 0.26256102 Test RE 0.00011877401013247198 c 1.0010473 k 0.049936827 m 5.001203\n",
      "185 Train Loss 0.26256102 Test RE 0.00011877401013247198 c 1.0010473 k 0.049936827 m 5.001203\n",
      "186 Train Loss 0.26256102 Test RE 0.00011877401013247198 c 1.0010473 k 0.049936827 m 5.001203\n",
      "187 Train Loss 0.26256102 Test RE 0.00011877401013247198 c 1.0010473 k 0.049936827 m 5.001203\n",
      "188 Train Loss 0.26256102 Test RE 0.00011877401013247198 c 1.0010473 k 0.049936827 m 5.001203\n",
      "189 Train Loss 0.26256102 Test RE 0.00011877401013247198 c 1.0010473 k 0.049936827 m 5.001203\n",
      "190 Train Loss 0.26256102 Test RE 0.00011877401013247198 c 1.0010473 k 0.049936827 m 5.001203\n",
      "191 Train Loss 0.26256102 Test RE 0.00011877401013247198 c 1.0010473 k 0.049936827 m 5.001203\n",
      "192 Train Loss 0.26256102 Test RE 0.00011877401013247198 c 1.0010473 k 0.049936827 m 5.001203\n",
      "193 Train Loss 0.26256102 Test RE 0.00011877401013247198 c 1.0010473 k 0.049936827 m 5.001203\n",
      "194 Train Loss 0.26256102 Test RE 0.00011877401013247198 c 1.0010473 k 0.049936827 m 5.001203\n",
      "195 Train Loss 0.26256102 Test RE 0.00011877401013247198 c 1.0010473 k 0.049936827 m 5.001203\n",
      "196 Train Loss 0.26256102 Test RE 0.00011877401013247198 c 1.0010473 k 0.049936827 m 5.001203\n",
      "197 Train Loss 0.26256102 Test RE 0.00011877401013247198 c 1.0010473 k 0.049936827 m 5.001203\n",
      "198 Train Loss 0.26256102 Test RE 0.00011877401013247198 c 1.0010473 k 0.049936827 m 5.001203\n",
      "199 Train Loss 0.26256102 Test RE 0.00011877401013247198 c 1.0010473 k 0.049936827 m 5.001203\n",
      "Training time: 56.65\n",
      "Training time: 56.65\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 239607.23 Test RE 0.3284182529584119 c 0.02713845 k 0.057574626 m 0.00015862098\n",
      "1 Train Loss 50050.25 Test RE 0.1451718830623551 c 0.035018932 k 0.04121733 m 0.00013745308\n",
      "2 Train Loss 48555.46 Test RE 0.14426725495759998 c 0.040830955 k 0.06550646 m 0.00018057389\n",
      "3 Train Loss 45264.336 Test RE 0.13852750771472125 c 0.12909982 k 0.07147547 m 0.0009884022\n",
      "4 Train Loss 24359.824 Test RE 0.10005453846400843 c 0.99708414 k 0.043594252 m 0.008604088\n",
      "5 Train Loss 9964.407 Test RE 0.05480690993829381 c 1.892274 k 0.02599546 m 0.016276928\n",
      "6 Train Loss 9284.879 Test RE 0.051772347061628476 c 1.9626269 k 0.027126927 m 0.016950227\n",
      "7 Train Loss 7807.402 Test RE 0.04361178940762672 c 1.938329 k 0.02977987 m 0.018494662\n",
      "8 Train Loss 4241.5903 Test RE 0.017085816826226227 c 1.4607142 k 0.040394403 m 0.026363593\n",
      "9 Train Loss 4068.965 Test RE 0.01578483144920232 c 1.3256347 k 0.040880386 m 0.028175635\n",
      "10 Train Loss 3890.978 Test RE 0.01489393580082233 c 1.1772326 k 0.044637155 m 0.03081338\n",
      "11 Train Loss 3810.767 Test RE 0.013046181254508018 c 1.0330815 k 0.045256246 m 0.03430168\n",
      "12 Train Loss 3779.0447 Test RE 0.01378915911869812 c 0.9837606 k 0.047037225 m 0.036635935\n",
      "13 Train Loss 3625.4934 Test RE 0.012692191557020163 c 1.1190339 k 0.04396369 m 0.050374836\n",
      "14 Train Loss 3580.646 Test RE 0.012984422421267098 c 1.1424043 k 0.04461685 m 0.0538967\n",
      "15 Train Loss 3456.144 Test RE 0.014054489885770197 c 1.0750316 k 0.046795495 m 0.068931565\n",
      "16 Train Loss 3365.1792 Test RE 0.012844002291433287 c 1.0392777 k 0.045862332 m 0.07215117\n",
      "17 Train Loss 3155.297 Test RE 0.010756808670324244 c 1.0091752 k 0.04399913 m 0.09420344\n",
      "18 Train Loss 2760.4495 Test RE 0.014806341596411745 c 1.0559143 k 0.04756046 m 0.12636802\n",
      "19 Train Loss 2433.9622 Test RE 0.013492742551795103 c 1.2747015 k 0.041242898 m 0.15946083\n",
      "20 Train Loss 2318.976 Test RE 0.01244490179303571 c 1.3842049 k 0.04126456 m 0.17408231\n",
      "21 Train Loss 2249.5706 Test RE 0.01275729236970365 c 1.3827976 k 0.040827252 m 0.18696795\n",
      "22 Train Loss 1985.5293 Test RE 0.01231868928386112 c 1.0944836 k 0.045037 m 0.21914265\n",
      "23 Train Loss 1898.8466 Test RE 0.011853096313664163 c 0.9467221 k 0.0463146 m 0.24195743\n",
      "24 Train Loss 1775.1543 Test RE 0.012558235348317816 c 1.0171531 k 0.0470518 m 0.2568558\n",
      "25 Train Loss 1606.4019 Test RE 0.014501850749807083 c 1.1585839 k 0.04342974 m 0.29298064\n",
      "26 Train Loss 872.27264 Test RE 0.011903782592065327 c 1.2713885 k 0.04223874 m 0.32289934\n",
      "27 Train Loss 706.2173 Test RE 0.008676043669699316 c 1.2126806 k 0.04371654 m 0.3244362\n",
      "28 Train Loss 674.5281 Test RE 0.008465332745077037 c 1.1353915 k 0.044652004 m 0.3142764\n",
      "29 Train Loss 649.4486 Test RE 0.008760573757798333 c 1.1082567 k 0.04484603 m 0.32316244\n",
      "30 Train Loss 534.1432 Test RE 0.0073453176396294 c 1.014254 k 0.04646155 m 0.38682616\n",
      "31 Train Loss 426.95233 Test RE 0.007206529343504071 c 1.044528 k 0.04542289 m 0.42925125\n",
      "32 Train Loss 359.9001 Test RE 0.005614334409215116 c 1.0573353 k 0.04571161 m 0.44219866\n",
      "33 Train Loss 351.44714 Test RE 0.00510013639307667 c 1.0895674 k 0.045484494 m 0.45105195\n",
      "34 Train Loss 340.50372 Test RE 0.004803542514200141 c 1.1235111 k 0.045012806 m 0.47057047\n",
      "35 Train Loss 313.80533 Test RE 0.005321324353887147 c 1.1940012 k 0.043732915 m 0.5360857\n",
      "36 Train Loss 300.38022 Test RE 0.005577333221630425 c 1.1587962 k 0.044530336 m 0.5317312\n",
      "37 Train Loss 284.02646 Test RE 0.00615745667131741 c 1.1790663 k 0.044043716 m 0.5755369\n",
      "38 Train Loss 273.11642 Test RE 0.005726853335382332 c 1.2023642 k 0.043801066 m 0.59854054\n",
      "39 Train Loss 270.8208 Test RE 0.0054855754226124445 c 1.1959473 k 0.04392977 m 0.59846115\n",
      "40 Train Loss 256.62604 Test RE 0.004735926930259884 c 1.1130791 k 0.04538408 m 0.60058653\n",
      "41 Train Loss 242.99423 Test RE 0.005204488917636599 c 1.1053609 k 0.045328148 m 0.63037866\n",
      "42 Train Loss 237.54341 Test RE 0.005263037532723503 c 1.1300749 k 0.045129675 m 0.6695714\n",
      "43 Train Loss 226.00525 Test RE 0.004938798496475033 c 1.1381258 k 0.045070443 m 0.7044762\n",
      "44 Train Loss 223.63658 Test RE 0.0048959973770774 c 1.1172097 k 0.045296464 m 0.71469617\n",
      "45 Train Loss 223.44705 Test RE 0.004872420836164674 c 1.1094325 k 0.04540466 m 0.71866775\n",
      "46 Train Loss 222.74216 Test RE 0.004715816767695438 c 1.0985413 k 0.04558718 m 0.72547364\n",
      "47 Train Loss 221.20804 Test RE 0.004750381826289971 c 1.1139212 k 0.0453369 m 0.7277742\n",
      "48 Train Loss 219.97395 Test RE 0.004765134761135002 c 1.1132357 k 0.045418724 m 0.7382949\n",
      "49 Train Loss 214.04147 Test RE 0.004624461836849682 c 1.0955582 k 0.045735944 m 0.7940404\n",
      "50 Train Loss 212.37785 Test RE 0.004876630933038794 c 1.1092181 k 0.04556558 m 0.82209164\n",
      "51 Train Loss 210.94432 Test RE 0.004787791259699653 c 1.1003723 k 0.04562902 m 0.8229682\n",
      "52 Train Loss 209.78484 Test RE 0.004673728016134322 c 1.1116762 k 0.045507606 m 0.84252125\n",
      "53 Train Loss 204.05194 Test RE 0.004543818884030024 c 1.1161524 k 0.04551075 m 0.9196895\n",
      "54 Train Loss 201.38014 Test RE 0.004441290341444766 c 1.1015931 k 0.04573566 m 0.9398995\n",
      "55 Train Loss 200.01933 Test RE 0.00437138729137622 c 1.1039333 k 0.045732435 m 0.97410417\n",
      "56 Train Loss 199.70274 Test RE 0.004266854220541317 c 1.1042721 k 0.04570727 m 0.98984134\n",
      "57 Train Loss 199.61258 Test RE 0.004217595841504054 c 1.1037258 k 0.045714792 m 0.99788934\n",
      "58 Train Loss 199.30377 Test RE 0.004157862411847515 c 1.1011347 k 0.04574987 m 1.0167285\n",
      "59 Train Loss 198.95055 Test RE 0.004184965228429357 c 1.0998011 k 0.045789 m 1.0247052\n",
      "60 Train Loss 198.87532 Test RE 0.004177610785690141 c 1.100296 k 0.045785815 m 1.0263965\n",
      "61 Train Loss 198.85345 Test RE 0.004166882742253601 c 1.1002258 k 0.045786623 m 1.0278913\n",
      "62 Train Loss 198.82634 Test RE 0.004145790938132462 c 1.0988939 k 0.045799837 m 1.0314858\n",
      "63 Train Loss 198.7191 Test RE 0.004118149561063966 c 1.0962101 k 0.045849532 m 1.0370859\n",
      "64 Train Loss 198.2746 Test RE 0.004150878361880044 c 1.0941354 k 0.045896947 m 1.0331522\n",
      "65 Train Loss 196.66501 Test RE 0.004240036764994541 c 1.0979898 k 0.045777127 m 1.0254027\n",
      "66 Train Loss 195.67526 Test RE 0.004154448820323211 c 1.1021305 k 0.045724757 m 1.0243727\n",
      "67 Train Loss 195.58769 Test RE 0.004125059609975639 c 1.0997603 k 0.045790993 m 1.0281537\n",
      "68 Train Loss 195.4103 Test RE 0.004094814293284723 c 1.0951781 k 0.045826975 m 1.0350602\n",
      "69 Train Loss 194.21109 Test RE 0.0041677637198643 c 1.1009839 k 0.0456418 m 1.0535738\n",
      "70 Train Loss 189.41965 Test RE 0.004056832526848919 c 1.1110598 k 0.045485917 m 1.0607243\n",
      "71 Train Loss 188.5876 Test RE 0.003988042884433441 c 1.1038349 k 0.045682445 m 1.0590861\n",
      "72 Train Loss 188.27347 Test RE 0.003939980629953684 c 1.1029791 k 0.045731254 m 1.0500852\n",
      "73 Train Loss 187.90773 Test RE 0.003958209689567657 c 1.1074108 k 0.0456333 m 1.046388\n",
      "74 Train Loss 186.61044 Test RE 0.00398063046606539 c 1.1095618 k 0.045688946 m 1.0937182\n",
      "75 Train Loss 181.10233 Test RE 0.003929327599990468 c 1.0833094 k 0.045825776 m 1.1400181\n",
      "76 Train Loss 178.67389 Test RE 0.004027736970691614 c 1.10271 k 0.045655556 m 1.1480027\n",
      "77 Train Loss 178.39078 Test RE 0.003995085973830734 c 1.1057557 k 0.045734424 m 1.1432252\n",
      "78 Train Loss 177.89023 Test RE 0.003950290999830449 c 1.1068943 k 0.045693234 m 1.1380677\n",
      "79 Train Loss 177.05038 Test RE 0.003923500278092264 c 1.1007259 k 0.0456721 m 1.1615719\n",
      "80 Train Loss 176.6341 Test RE 0.0039334538351165245 c 1.095755 k 0.045814607 m 1.1827799\n",
      "81 Train Loss 176.36508 Test RE 0.00393994343096512 c 1.0919439 k 0.04595714 m 1.2071869\n",
      "82 Train Loss 175.90549 Test RE 0.003909197568627801 c 1.0890243 k 0.046078876 m 1.249818\n",
      "83 Train Loss 175.09962 Test RE 0.0038613073999315965 c 1.092986 k 0.04605786 m 1.2996067\n",
      "84 Train Loss 173.07104 Test RE 0.0038689658313372814 c 1.1063522 k 0.04570376 m 1.3131632\n",
      "85 Train Loss 166.40811 Test RE 0.0035720069030126893 c 1.1045835 k 0.04580561 m 1.3220084\n",
      "86 Train Loss 161.13292 Test RE 0.0034548277413885904 c 1.1051514 k 0.045833103 m 1.3425986\n",
      "87 Train Loss 160.09154 Test RE 0.0034764502711108385 c 1.1108522 k 0.045586985 m 1.356624\n",
      "88 Train Loss 147.73227 Test RE 0.0035486703557354856 c 1.0973955 k 0.046386816 m 1.5440947\n",
      "89 Train Loss 136.61899 Test RE 0.003397591048042354 c 1.1032778 k 0.04566335 m 1.7541656\n",
      "90 Train Loss 127.99737 Test RE 0.0036026414092266397 c 1.0961826 k 0.046224613 m 1.8908426\n",
      "91 Train Loss 112.03583 Test RE 0.0037158766923095434 c 1.0831493 k 0.04671394 m 2.1296294\n",
      "92 Train Loss 101.886 Test RE 0.0034895349792931096 c 1.0611109 k 0.047073063 m 2.3801205\n",
      "93 Train Loss 97.27722 Test RE 0.0034841228615539507 c 1.0672939 k 0.04710149 m 2.4434013\n",
      "94 Train Loss 96.827835 Test RE 0.003439753980257077 c 1.067515 k 0.04704477 m 2.4406555\n",
      "95 Train Loss 96.744675 Test RE 0.003415030395517259 c 1.0655729 k 0.047076005 m 2.4388266\n",
      "96 Train Loss 96.43365 Test RE 0.0033461878314615407 c 1.059693 k 0.04718604 m 2.4573598\n",
      "97 Train Loss 94.22853 Test RE 0.003293141154246843 c 1.0598664 k 0.047261767 m 2.552426\n",
      "98 Train Loss 91.72683 Test RE 0.0035789310530545673 c 1.0660347 k 0.047097344 m 2.6019473\n",
      "99 Train Loss 90.398506 Test RE 0.003558133411951161 c 1.0656222 k 0.0472209 m 2.6277733\n",
      "100 Train Loss 89.6399 Test RE 0.0034814293163724524 c 1.060335 k 0.04726305 m 2.6602302\n",
      "101 Train Loss 89.541504 Test RE 0.0034513389472164376 c 1.0569543 k 0.047303148 m 2.6534116\n",
      "102 Train Loss 89.51313 Test RE 0.0034437590447275643 c 1.0571262 k 0.04732221 m 2.649605\n",
      "103 Train Loss 89.39322 Test RE 0.0034428542409403037 c 1.0590605 k 0.047307797 m 2.6561573\n",
      "104 Train Loss 88.84893 Test RE 0.0034372523707260102 c 1.0592284 k 0.047344334 m 2.6908238\n",
      "105 Train Loss 87.71869 Test RE 0.0034080799468020704 c 1.0504969 k 0.04751275 m 2.7261548\n",
      "106 Train Loss 87.3315 Test RE 0.0034431163846894487 c 1.0478799 k 0.047521945 m 2.7546923\n",
      "107 Train Loss 86.820045 Test RE 0.003473481898470337 c 1.0508476 k 0.047486726 m 2.7878983\n",
      "108 Train Loss 86.02113 Test RE 0.0034772262091073767 c 1.0504419 k 0.047577877 m 2.8015127\n",
      "109 Train Loss 85.5179 Test RE 0.0034010703051524944 c 1.0488231 k 0.047629576 m 2.8005393\n",
      "110 Train Loss 85.23623 Test RE 0.0033771760710247516 c 1.0477148 k 0.04767702 m 2.811702\n",
      "111 Train Loss 83.95518 Test RE 0.0034421057103329196 c 1.0477707 k 0.047731303 m 2.8865464\n",
      "112 Train Loss 83.41499 Test RE 0.003502721064016523 c 1.0468326 k 0.047731355 m 2.9292855\n",
      "113 Train Loss 82.67552 Test RE 0.0034953911596909155 c 1.0442085 k 0.047764413 m 2.9588342\n",
      "114 Train Loss 81.668015 Test RE 0.0033864823430782387 c 1.0433518 k 0.04787064 m 2.957964\n",
      "115 Train Loss 81.00153 Test RE 0.0034046100605638705 c 1.0428748 k 0.047937557 m 2.9851255\n",
      "116 Train Loss 80.255356 Test RE 0.003495123778791517 c 1.0427142 k 0.047957852 m 3.0531018\n",
      "117 Train Loss 79.58351 Test RE 0.0034608013315252013 c 1.0421016 k 0.047943886 m 3.0820343\n",
      "118 Train Loss 78.75433 Test RE 0.003395704602932826 c 1.0411351 k 0.048024267 m 3.0659542\n",
      "119 Train Loss 78.1991 Test RE 0.0034049712005468718 c 1.0397187 k 0.048102647 m 3.051776\n",
      "120 Train Loss 76.469215 Test RE 0.003386761432932113 c 1.0362637 k 0.04821954 m 3.1369934\n",
      "121 Train Loss 74.31078 Test RE 0.0034538849766276207 c 1.0419116 k 0.048169438 m 3.2598495\n",
      "122 Train Loss 71.54347 Test RE 0.0034924417665625025 c 1.044005 k 0.047960892 m 3.2854986\n",
      "123 Train Loss 69.92869 Test RE 0.0034057259633758514 c 1.036733 k 0.048303075 m 3.2949786\n",
      "124 Train Loss 68.13391 Test RE 0.0032703294444939753 c 1.0338099 k 0.048447285 m 3.365664\n",
      "125 Train Loss 66.61882 Test RE 0.003384588068495394 c 1.0379848 k 0.048348226 m 3.435358\n",
      "126 Train Loss 65.45853 Test RE 0.003409082473113747 c 1.0389504 k 0.048283547 m 3.481389\n",
      "127 Train Loss 61.591156 Test RE 0.0029742411085636167 c 1.022148 k 0.049118765 m 3.5726967\n",
      "128 Train Loss 59.069862 Test RE 0.002995572488331061 c 1.0231698 k 0.04870776 m 3.6223085\n",
      "129 Train Loss 58.537964 Test RE 0.0030463348424202195 c 1.0225186 k 0.048625868 m 3.616696\n",
      "130 Train Loss 56.443604 Test RE 0.0029581308627976534 c 1.0139227 k 0.048771117 m 3.615483\n",
      "131 Train Loss 53.282543 Test RE 0.002803927083144641 c 1.0173512 k 0.048816405 m 3.6509213\n",
      "132 Train Loss 51.645744 Test RE 0.002774095368921523 c 1.0231982 k 0.04873725 m 3.680946\n",
      "133 Train Loss 50.94069 Test RE 0.002817876818960496 c 1.0246041 k 0.048595566 m 3.6961021\n",
      "134 Train Loss 49.425545 Test RE 0.0028120236064318 c 1.02277 k 0.0486324 m 3.7292\n",
      "135 Train Loss 46.16527 Test RE 0.0025932430246837024 c 1.0198826 k 0.048724554 m 3.7290432\n",
      "136 Train Loss 44.941765 Test RE 0.0025806859475504414 c 1.0262113 k 0.048658714 m 3.7050192\n",
      "137 Train Loss 38.44686 Test RE 0.002406935925054003 c 1.0236565 k 0.04905961 m 3.9354167\n",
      "138 Train Loss 28.676348 Test RE 0.001883642899892036 c 1.018287 k 0.049148988 m 4.122202\n",
      "139 Train Loss 23.440144 Test RE 0.0020054884256174893 c 1.0181034 k 0.0491637 m 4.338652\n",
      "140 Train Loss 16.970058 Test RE 0.0017161786125456675 c 1.0162703 k 0.04919073 m 4.3226624\n",
      "141 Train Loss 10.886159 Test RE 0.0013024010345365372 c 1.0062895 k 0.049538728 m 4.5631976\n",
      "142 Train Loss 5.291172 Test RE 0.0009541742604758008 c 1.0071055 k 0.049665876 m 4.845904\n",
      "143 Train Loss 4.529785 Test RE 0.0008045228773587808 c 1.009349 k 0.049618986 m 4.83128\n",
      "144 Train Loss 4.4200687 Test RE 0.0007569444330868254 c 1.0065321 k 0.049655274 m 4.8196335\n",
      "145 Train Loss 4.3024173 Test RE 0.000698036806571154 c 1.0043719 k 0.049699984 m 4.834914\n",
      "146 Train Loss 4.171872 Test RE 0.0006547921621178742 c 1.0051435 k 0.049715552 m 4.862367\n",
      "147 Train Loss 4.092815 Test RE 0.0006467396301578159 c 1.0041302 k 0.049758546 m 4.881995\n",
      "148 Train Loss 3.9736605 Test RE 0.0006485987658914729 c 1.0036852 k 0.049784645 m 4.876896\n",
      "149 Train Loss 3.8721387 Test RE 0.0006728415446609095 c 1.0049853 k 0.049748875 m 4.855702\n",
      "150 Train Loss 3.7503495 Test RE 0.0006910133423068695 c 1.0055009 k 0.049723648 m 4.863981\n",
      "151 Train Loss 3.4662776 Test RE 0.0006828806046093824 c 1.0052022 k 0.049731273 m 4.903707\n",
      "152 Train Loss 3.3180208 Test RE 0.0006874466403048188 c 1.0059512 k 0.04972918 m 4.9255443\n",
      "153 Train Loss 3.2409034 Test RE 0.0006943547182242873 c 1.0038537 k 0.049784105 m 4.951373\n",
      "154 Train Loss 3.1369781 Test RE 0.0006595200182531665 c 1.0041728 k 0.04980374 m 4.9878464\n",
      "155 Train Loss 3.0545444 Test RE 0.0006140028979322917 c 1.0037705 k 0.049816485 m 4.994658\n",
      "156 Train Loss 2.9825397 Test RE 0.0006083305108856492 c 1.0031575 k 0.0497987 m 4.9789624\n",
      "157 Train Loss 2.8953142 Test RE 0.0006181111802733763 c 1.0031573 k 0.049803454 m 4.9742866\n",
      "158 Train Loss 2.7380314 Test RE 0.0005914247223752362 c 1.0042158 k 0.04979353 m 4.9628954\n",
      "159 Train Loss 2.625224 Test RE 0.0005560019997233415 c 1.0033476 k 0.049771257 m 4.9345117\n",
      "160 Train Loss 2.5155792 Test RE 0.000556074335152967 c 1.0038307 k 0.049750965 m 4.9375124\n",
      "161 Train Loss 2.490206 Test RE 0.0005599082921736398 c 1.0053747 k 0.049722616 m 4.9384747\n",
      "162 Train Loss 2.481033 Test RE 0.0005560385549939963 c 1.0063707 k 0.049696643 m 4.9284244\n",
      "163 Train Loss 2.4667158 Test RE 0.000548569424809962 c 1.0066594 k 0.049683694 m 4.909767\n",
      "164 Train Loss 2.441462 Test RE 0.0005417168336272466 c 1.0060955 k 0.04967864 m 4.8822107\n",
      "165 Train Loss 2.4093864 Test RE 0.0005475096324685253 c 1.0056534 k 0.049682923 m 4.8866043\n",
      "166 Train Loss 2.379295 Test RE 0.0005501831382202928 c 1.006658 k 0.04966936 m 4.8939834\n",
      "167 Train Loss 2.3356047 Test RE 0.0005389035556156988 c 1.00715 k 0.049663905 m 4.8853307\n",
      "168 Train Loss 2.2908382 Test RE 0.0005247868329296043 c 1.0064873 k 0.049676467 m 4.8821564\n",
      "169 Train Loss 2.2818847 Test RE 0.000521711306598207 c 1.0061091 k 0.0496822 m 4.884299\n",
      "170 Train Loss 2.2766244 Test RE 0.0005165745523866156 c 1.0058067 k 0.0496859 m 4.883618\n",
      "171 Train Loss 2.273717 Test RE 0.0005142525341780306 c 1.0059724 k 0.04967885 m 4.8804703\n",
      "172 Train Loss 2.2664542 Test RE 0.0005126346279373501 c 1.0071521 k 0.049654502 m 4.8748107\n",
      "173 Train Loss 2.231546 Test RE 0.0005210881395264202 c 1.0079023 k 0.049645748 m 4.879865\n",
      "174 Train Loss 2.1800435 Test RE 0.0005228050430886091 c 1.0061449 k 0.049658693 m 4.877028\n",
      "175 Train Loss 2.134027 Test RE 0.0005207601054572382 c 1.0070982 k 0.04962084 m 4.853736\n",
      "176 Train Loss 2.0059714 Test RE 0.0004979855276245894 c 1.0087091 k 0.04959405 m 4.8262234\n",
      "177 Train Loss 1.9137164 Test RE 0.0004985626982429564 c 1.0067827 k 0.049644813 m 4.8567653\n",
      "178 Train Loss 1.8725247 Test RE 0.000494695096406372 c 1.0060822 k 0.049681712 m 4.87706\n",
      "179 Train Loss 1.8094957 Test RE 0.0004746548816267123 c 1.0062354 k 0.04967957 m 4.8849664\n",
      "180 Train Loss 1.7278979 Test RE 0.000454676639143976 c 1.0064787 k 0.04967164 m 4.875528\n",
      "181 Train Loss 1.6565231 Test RE 0.00044853803956592606 c 1.004403 k 0.04972942 m 4.8838463\n",
      "182 Train Loss 1.6167642 Test RE 0.00044539832932805106 c 1.0043893 k 0.04974552 m 4.9017777\n",
      "183 Train Loss 1.5944936 Test RE 0.0004329263362889735 c 1.0050716 k 0.049736958 m 4.909901\n",
      "184 Train Loss 1.5563138 Test RE 0.00041241709207481527 c 1.0051684 k 0.049743 m 4.907384\n",
      "185 Train Loss 1.5285963 Test RE 0.00040344763047510683 c 1.0051558 k 0.04974855 m 4.9055305\n",
      "186 Train Loss 1.4942193 Test RE 0.0003996487148820284 c 1.0047213 k 0.04976576 m 4.91337\n",
      "187 Train Loss 1.4475656 Test RE 0.0004024593482887239 c 1.0043724 k 0.049787022 m 4.925763\n",
      "188 Train Loss 1.4253262 Test RE 0.0004028870585787719 c 1.005082 k 0.04977369 m 4.912739\n",
      "189 Train Loss 1.4049804 Test RE 0.0004066510415113622 c 1.0041375 k 0.04978424 m 4.8995314\n",
      "190 Train Loss 1.3784325 Test RE 0.0004117404097402476 c 1.0022674 k 0.049824428 m 4.9059463\n",
      "191 Train Loss 1.3117841 Test RE 0.0003975249101121285 c 1.0016972 k 0.04986274 m 4.9529395\n",
      "192 Train Loss 1.291269 Test RE 0.0003818225077467714 c 1.0020628 k 0.049865093 m 4.970558\n",
      "193 Train Loss 1.2684748 Test RE 0.00037310905686484047 c 1.0027057 k 0.049861208 m 4.968766\n",
      "194 Train Loss 1.2576103 Test RE 0.00037456492066994574 c 1.0026803 k 0.04986497 m 4.968473\n",
      "195 Train Loss 1.2497468 Test RE 0.00038074394635937987 c 1.0018493 k 0.049879376 m 4.9790626\n",
      "196 Train Loss 1.240971 Test RE 0.0003863236068502344 c 1.0018072 k 0.049883533 m 4.9812555\n",
      "197 Train Loss 1.2337959 Test RE 0.0003904691202779775 c 1.0020734 k 0.0498895 m 4.9827065\n",
      "198 Train Loss 1.2225574 Test RE 0.0003943194437150624 c 1.0017582 k 0.049902763 m 4.9849467\n",
      "199 Train Loss 1.2133276 Test RE 0.00039664903757559905 c 1.001558 k 0.049900968 m 4.9851913\n",
      "Training time: 70.69\n",
      "Training time: 70.69\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 281216.47 Test RE 0.3549936842690116 c 0.01501929 k 0.040737078 m 1.2636997e-05\n",
      "1 Train Loss 261320.23 Test RE 0.3414899244177762 c 0.013049903 k 0.10996948 m 1.076174e-05\n",
      "2 Train Loss 32531.836 Test RE 0.11129270043192854 c -0.011044735 k 0.10084046 m 0.00017567369\n",
      "3 Train Loss 26756.465 Test RE 0.10521413219018999 c -0.0084673455 k 0.05160058 m 0.0007033799\n",
      "4 Train Loss 22095.328 Test RE 0.0957924671752719 c 0.10279034 k 0.061717257 m 0.005946706\n",
      "5 Train Loss 12492.938 Test RE 0.07045060064674617 c 0.5396422 k 0.060867842 m 0.02640172\n",
      "6 Train Loss 3831.8738 Test RE 0.033543122466054516 c 1.080041 k 0.041229084 m 0.052079074\n",
      "7 Train Loss 2315.8054 Test RE 0.019097772248021613 c 1.3749453 k 0.039618526 m 0.06944235\n",
      "8 Train Loss 1592.2676 Test RE 0.013422066700823378 c 1.4128042 k 0.038976707 m 0.080851\n",
      "9 Train Loss 1211.9827 Test RE 0.015785405933377055 c 1.2744902 k 0.041729413 m 0.088723205\n",
      "10 Train Loss 1063.4569 Test RE 0.014314919305007686 c 1.3038219 k 0.041477382 m 0.09815806\n",
      "11 Train Loss 987.1283 Test RE 0.012872331144084592 c 1.3126745 k 0.04045433 m 0.10646782\n",
      "12 Train Loss 882.0867 Test RE 0.011708118835517142 c 1.2796791 k 0.04185616 m 0.12616363\n",
      "13 Train Loss 759.9576 Test RE 0.010719419613665616 c 1.214487 k 0.04312857 m 0.16158895\n",
      "14 Train Loss 663.7783 Test RE 0.010593260822102176 c 1.0834326 k 0.044679113 m 0.20226067\n",
      "15 Train Loss 569.31085 Test RE 0.00937631210921571 c 0.9572235 k 0.04724565 m 0.25261384\n",
      "16 Train Loss 507.87567 Test RE 0.008838367296801585 c 0.9066539 k 0.047423683 m 0.33346108\n",
      "17 Train Loss 446.3023 Test RE 0.008867613130844562 c 1.0340456 k 0.046227284 m 0.38117853\n",
      "18 Train Loss 357.9616 Test RE 0.009002702260759653 c 1.0762017 k 0.046769742 m 0.54539317\n",
      "19 Train Loss 288.67487 Test RE 0.008109695282567905 c 1.1425996 k 0.044905037 m 0.68715024\n",
      "20 Train Loss 241.9128 Test RE 0.005770209762229791 c 1.0601414 k 0.046565685 m 0.7947935\n",
      "21 Train Loss 209.63504 Test RE 0.004496205783140675 c 1.1073897 k 0.045688353 m 0.88587916\n",
      "22 Train Loss 202.67038 Test RE 0.004130739530087229 c 1.122124 k 0.045399953 m 0.90703374\n",
      "23 Train Loss 196.94885 Test RE 0.0037249805546403325 c 1.0819746 k 0.045825467 m 0.95635647\n",
      "24 Train Loss 187.16624 Test RE 0.00318964555742055 c 1.0703464 k 0.04630602 m 1.084311\n",
      "25 Train Loss 175.71973 Test RE 0.003564624391046276 c 1.1007712 k 0.045783326 m 1.2072686\n",
      "26 Train Loss 170.10307 Test RE 0.0037399266466772475 c 1.0844249 k 0.046124298 m 1.2659554\n",
      "27 Train Loss 165.71022 Test RE 0.0036157778943354565 c 1.0992142 k 0.045921043 m 1.3212308\n",
      "28 Train Loss 165.07266 Test RE 0.0037469438652195206 c 1.0976378 k 0.045957573 m 1.334739\n",
      "29 Train Loss 164.06647 Test RE 0.003704624127811633 c 1.0992907 k 0.046053268 m 1.3570961\n",
      "30 Train Loss 161.94057 Test RE 0.00336850995597574 c 1.0955427 k 0.046004336 m 1.394929\n",
      "31 Train Loss 159.89645 Test RE 0.003524197817988011 c 1.0829269 k 0.046157017 m 1.4402851\n",
      "32 Train Loss 157.9561 Test RE 0.0034940814968705547 c 1.0920216 k 0.046143506 m 1.4916517\n",
      "33 Train Loss 155.79135 Test RE 0.0034682544166429728 c 1.0945429 k 0.04614159 m 1.5354426\n",
      "34 Train Loss 154.01816 Test RE 0.0035118676281135127 c 1.0807986 k 0.046393357 m 1.5837957\n",
      "35 Train Loss 152.04836 Test RE 0.0035460004712830205 c 1.0799025 k 0.046406712 m 1.6450003\n",
      "36 Train Loss 149.82437 Test RE 0.003556735752459612 c 1.0859697 k 0.046414085 m 1.693406\n",
      "37 Train Loss 147.46982 Test RE 0.003591466827724901 c 1.073923 k 0.04658481 m 1.7586756\n",
      "38 Train Loss 143.95107 Test RE 0.0034646092144327693 c 1.0708601 k 0.04677864 m 1.8563105\n",
      "39 Train Loss 140.83363 Test RE 0.003291340125105154 c 1.0883261 k 0.046606686 m 1.9576765\n",
      "40 Train Loss 138.23943 Test RE 0.003360409306705345 c 1.0742295 k 0.046855196 m 2.0671258\n",
      "41 Train Loss 136.68744 Test RE 0.00337168181543848 c 1.0504341 k 0.047251314 m 2.1012564\n",
      "42 Train Loss 132.15187 Test RE 0.003339113760757043 c 1.0475683 k 0.047317695 m 2.1630597\n",
      "43 Train Loss 130.75623 Test RE 0.0034111164665279257 c 1.0686713 k 0.04704614 m 2.1917984\n",
      "44 Train Loss 129.41582 Test RE 0.003627342996267169 c 1.0844797 k 0.046792213 m 2.235695\n",
      "45 Train Loss 126.45899 Test RE 0.003519274471244341 c 1.0612117 k 0.047242247 m 2.2951946\n",
      "46 Train Loss 120.31744 Test RE 0.0034357962268851032 c 1.059016 k 0.047166243 m 2.4155016\n",
      "47 Train Loss 118.23099 Test RE 0.003565877713386883 c 1.0720286 k 0.047005393 m 2.4540684\n",
      "48 Train Loss 117.13093 Test RE 0.0034899010543576934 c 1.0714897 k 0.047004025 m 2.4717505\n",
      "49 Train Loss 115.69499 Test RE 0.0033382726922004673 c 1.0678626 k 0.047153383 m 2.51623\n",
      "50 Train Loss 107.87305 Test RE 0.0032619395612217016 c 1.0670484 k 0.047262102 m 2.7255845\n",
      "51 Train Loss 96.85295 Test RE 0.0035175372215043126 c 1.0775071 k 0.047306344 m 2.9662201\n",
      "52 Train Loss 86.09774 Test RE 0.00293633612346718 c 1.038259 k 0.04824169 m 3.1659932\n",
      "53 Train Loss 80.40695 Test RE 0.0031004665502827786 c 1.0346795 k 0.04839628 m 3.3420014\n",
      "54 Train Loss 75.568344 Test RE 0.003142155358536778 c 1.0362846 k 0.048613563 m 3.519999\n",
      "55 Train Loss 68.556885 Test RE 0.0027133755884119278 c 1.0316975 k 0.04872065 m 3.7167473\n",
      "56 Train Loss 63.197567 Test RE 0.002486003693760374 c 1.0486696 k 0.048700213 m 4.0195484\n",
      "57 Train Loss 59.43094 Test RE 0.0022022739772796372 c 1.0383247 k 0.049131706 m 4.2019467\n",
      "58 Train Loss 55.1705 Test RE 0.0023070923632078928 c 0.9996369 k 0.04960065 m 4.2859564\n",
      "59 Train Loss 50.36026 Test RE 0.0022324077630200313 c 1.0350835 k 0.049440082 m 4.4218864\n",
      "60 Train Loss 47.40941 Test RE 0.001964709578032597 c 1.0150602 k 0.04969889 m 4.509173\n",
      "61 Train Loss 40.658714 Test RE 0.001986644004300635 c 0.9807567 k 0.05029456 m 4.7285123\n",
      "62 Train Loss 36.141037 Test RE 0.0020805083791329776 c 1.0052328 k 0.0499686 m 4.8894005\n",
      "63 Train Loss 34.659256 Test RE 0.002087517477713003 c 1.0044616 k 0.05003955 m 5.006323\n",
      "64 Train Loss 33.854107 Test RE 0.0019951717525612863 c 1.0019277 k 0.050062276 m 5.0667653\n",
      "65 Train Loss 33.673584 Test RE 0.0019774059668902486 c 1.0019069 k 0.05002876 m 5.083113\n",
      "66 Train Loss 32.486744 Test RE 0.0019930618056921384 c 1.0011944 k 0.04993958 m 5.0937433\n",
      "67 Train Loss 31.365593 Test RE 0.0020166998614651534 c 1.0111145 k 0.04978907 m 5.064065\n",
      "68 Train Loss 30.80867 Test RE 0.0019767574836735352 c 1.0170811 k 0.049566478 m 5.006237\n",
      "69 Train Loss 30.678734 Test RE 0.0019435942590847798 c 1.0164396 k 0.049570363 m 4.985671\n",
      "70 Train Loss 30.37941 Test RE 0.0019000353850186999 c 1.0139434 k 0.049629472 m 4.967407\n",
      "71 Train Loss 29.412281 Test RE 0.0018204594935815343 c 1.011993 k 0.049686085 m 4.9939637\n",
      "72 Train Loss 28.167437 Test RE 0.0017260779039945677 c 1.015344 k 0.049614403 m 4.985513\n",
      "73 Train Loss 25.927608 Test RE 0.001594535342335238 c 1.0106112 k 0.04992458 m 5.0278597\n",
      "74 Train Loss 24.740322 Test RE 0.0015403664796728059 c 1.0082666 k 0.049926125 m 5.0832734\n",
      "75 Train Loss 23.926893 Test RE 0.0015051661606137882 c 1.0072184 k 0.049919568 m 5.122087\n",
      "76 Train Loss 22.500315 Test RE 0.001422282979358173 c 1.0022883 k 0.050006226 m 5.123417\n",
      "77 Train Loss 20.308628 Test RE 0.0012413469816682072 c 0.9980911 k 0.05010977 m 5.084933\n",
      "78 Train Loss 18.942747 Test RE 0.0011161954604638432 c 1.0053749 k 0.049945258 m 5.059738\n",
      "79 Train Loss 17.713148 Test RE 0.0010785720027236007 c 1.005072 k 0.050086096 m 5.0322876\n",
      "80 Train Loss 16.522379 Test RE 0.0010350498963978042 c 1.0035317 k 0.04999836 m 5.0178494\n",
      "81 Train Loss 15.585298 Test RE 0.0009848907566044817 c 1.0078982 k 0.049815755 m 5.0078635\n",
      "82 Train Loss 14.739122 Test RE 0.0009759661842546323 c 1.0087473 k 0.049853425 m 4.9930973\n",
      "83 Train Loss 13.931601 Test RE 0.0009726487427668304 c 1.0024105 k 0.049976002 m 4.9769073\n",
      "84 Train Loss 13.082654 Test RE 0.0008704375541892907 c 1.0068944 k 0.04985551 m 4.9694157\n",
      "85 Train Loss 12.2651825 Test RE 0.000816608413749144 c 1.0084091 k 0.04984342 m 4.963668\n",
      "86 Train Loss 11.753181 Test RE 0.000829983634792433 c 1.0024252 k 0.049961742 m 4.961869\n",
      "87 Train Loss 11.298761 Test RE 0.0007608786719831193 c 1.0012358 k 0.049984217 m 4.9631743\n",
      "88 Train Loss 10.932081 Test RE 0.000722990768326693 c 1.001006 k 0.050010197 m 4.955333\n",
      "89 Train Loss 10.669516 Test RE 0.0007278934153910372 c 1.0006984 k 0.049997993 m 4.941557\n",
      "90 Train Loss 10.264134 Test RE 0.000678793433701776 c 0.99989194 k 0.050015718 m 4.9335213\n",
      "91 Train Loss 9.374316 Test RE 0.0006847324346880197 c 0.99973315 k 0.05004911 m 4.919888\n",
      "92 Train Loss 8.708661 Test RE 0.0007149513713818194 c 1.0032817 k 0.049890485 m 4.903928\n",
      "93 Train Loss 8.272873 Test RE 0.0006457525746557544 c 1.0043302 k 0.049896795 m 4.899109\n",
      "94 Train Loss 8.044527 Test RE 0.0006503174380007571 c 1.0048932 k 0.049877595 m 4.889718\n",
      "95 Train Loss 7.7501807 Test RE 0.0007155835349861877 c 1.0051281 k 0.049877148 m 4.890709\n",
      "96 Train Loss 7.4035788 Test RE 0.000714272811394285 c 1.004473 k 0.049899664 m 4.926631\n",
      "97 Train Loss 7.1792235 Test RE 0.0006357515004290965 c 1.0022166 k 0.04995728 m 4.964099\n",
      "98 Train Loss 7.0225024 Test RE 0.0006090254097162226 c 1.002791 k 0.049984943 m 4.9969883\n",
      "99 Train Loss 6.932835 Test RE 0.0006501307651843109 c 1.0046991 k 0.04999117 m 5.0199103\n",
      "100 Train Loss 6.8553424 Test RE 0.0006562068938631857 c 1.0047163 k 0.0499768 m 5.0428624\n",
      "101 Train Loss 6.7582674 Test RE 0.000606672679749213 c 1.0023936 k 0.050015774 m 5.0588365\n",
      "102 Train Loss 6.499926 Test RE 0.0005413900639314341 c 0.9992075 k 0.050078884 m 5.046965\n",
      "103 Train Loss 6.1131396 Test RE 0.0005913332240951683 c 0.99629605 k 0.050130434 m 5.007481\n",
      "104 Train Loss 5.8112564 Test RE 0.0006397771079623262 c 0.99881154 k 0.049990505 m 4.9540057\n",
      "105 Train Loss 5.6253977 Test RE 0.0006348103637306528 c 1.0008694 k 0.049946114 m 4.905108\n",
      "106 Train Loss 5.463781 Test RE 0.0006534847033814842 c 1.0015683 k 0.04999152 m 4.8839498\n",
      "107 Train Loss 5.0860977 Test RE 0.0006645527840852423 c 0.9984833 k 0.049991656 m 4.8543544\n",
      "108 Train Loss 4.625421 Test RE 0.0006212190799815339 c 0.9996792 k 0.049883123 m 4.904921\n",
      "109 Train Loss 4.336829 Test RE 0.0005879495357113796 c 1.0079098 k 0.049846284 m 4.922065\n",
      "110 Train Loss 4.223442 Test RE 0.0005766003243361121 c 1.0063909 k 0.049883593 m 4.930632\n",
      "111 Train Loss 4.1072187 Test RE 0.0005601537632053662 c 1.0041971 k 0.049865548 m 4.9557366\n",
      "112 Train Loss 4.0075507 Test RE 0.0005402239338951797 c 1.0031837 k 0.049912743 m 4.9668126\n",
      "113 Train Loss 3.9466755 Test RE 0.0005400872415117387 c 1.0031034 k 0.04992904 m 4.972133\n",
      "114 Train Loss 3.9086442 Test RE 0.0005424478535204686 c 1.0041703 k 0.049921114 m 4.9581356\n",
      "115 Train Loss 3.8756866 Test RE 0.0005490546174555086 c 1.004995 k 0.049879655 m 4.9409046\n",
      "116 Train Loss 3.854467 Test RE 0.0005636386067239971 c 1.0037991 k 0.04988642 m 4.9381185\n",
      "117 Train Loss 3.8204393 Test RE 0.0005767472303035292 c 1.003763 k 0.04989103 m 4.938826\n",
      "118 Train Loss 3.7321472 Test RE 0.0005617859556724973 c 1.0043632 k 0.04991109 m 4.947684\n",
      "119 Train Loss 3.628435 Test RE 0.0005487018233527014 c 1.0025598 k 0.049952302 m 4.955012\n",
      "120 Train Loss 3.560978 Test RE 0.0005298958268756251 c 1.0011985 k 0.049967334 m 4.9543443\n",
      "121 Train Loss 3.5327775 Test RE 0.0005238581465327645 c 1.0015166 k 0.049981155 m 4.958974\n",
      "122 Train Loss 3.5204318 Test RE 0.000527938396231467 c 1.0014439 k 0.05000849 m 4.9723797\n",
      "123 Train Loss 3.4987051 Test RE 0.0005337091939694174 c 1.0022026 k 0.050004166 m 4.98471\n",
      "124 Train Loss 3.4666097 Test RE 0.000535310203042367 c 1.0034034 k 0.049972165 m 4.981319\n",
      "125 Train Loss 3.4185543 Test RE 0.0005162025798778492 c 1.0014522 k 0.049981844 m 4.972158\n",
      "126 Train Loss 3.370909 Test RE 0.0005069620031799359 c 1.0003501 k 0.050001867 m 4.974504\n",
      "127 Train Loss 3.3383987 Test RE 0.0005091130121871321 c 1.0010067 k 0.050006874 m 4.974623\n",
      "128 Train Loss 3.317369 Test RE 0.0005131749635722475 c 1.0014696 k 0.05001302 m 4.972273\n",
      "129 Train Loss 3.271551 Test RE 0.0005163186667060814 c 1.0013729 k 0.050002974 m 4.970315\n",
      "130 Train Loss 3.235277 Test RE 0.000508966045526616 c 1.0008755 k 0.049994882 m 4.9760075\n",
      "131 Train Loss 3.2175665 Test RE 0.0005034488601597873 c 1.000975 k 0.050018564 m 4.974077\n",
      "132 Train Loss 3.2034335 Test RE 0.000505160928236751 c 1.0012268 k 0.050023243 m 4.9670815\n",
      "133 Train Loss 3.1958995 Test RE 0.0005118380152803567 c 1.0009515 k 0.05001636 m 4.968695\n",
      "134 Train Loss 3.192141 Test RE 0.0005127467334327526 c 1.0007693 k 0.05002285 m 4.9694705\n",
      "135 Train Loss 3.1895328 Test RE 0.0005125068985224428 c 1.0008705 k 0.050025202 m 4.968468\n",
      "136 Train Loss 3.1735792 Test RE 0.0005120089692911733 c 1.0009714 k 0.050025992 m 4.965523\n",
      "137 Train Loss 3.1229675 Test RE 0.0005065594896584598 c 1.0013032 k 0.050017014 m 4.9724593\n",
      "138 Train Loss 3.0751393 Test RE 0.0004933943425837885 c 1.001783 k 0.050004758 m 4.9829235\n",
      "139 Train Loss 3.0191941 Test RE 0.0004705082524074471 c 1.0005363 k 0.050014272 m 4.982291\n",
      "140 Train Loss 2.9992013 Test RE 0.0004552123353160372 c 1.0012956 k 0.049991358 m 4.9714336\n",
      "141 Train Loss 2.9909174 Test RE 0.0004484081693566251 c 1.0020871 k 0.049974747 m 4.9630647\n",
      "142 Train Loss 2.9790194 Test RE 0.00045267748937527415 c 1.002204 k 0.04997401 m 4.9583735\n",
      "143 Train Loss 2.955696 Test RE 0.00046400376124406664 c 1.0012748 k 0.049994845 m 4.9602685\n",
      "144 Train Loss 2.9089851 Test RE 0.0004496639143821489 c 1.0006863 k 0.050013192 m 4.955467\n",
      "145 Train Loss 2.8546486 Test RE 0.00043202934352234756 c 1.0011859 k 0.04998069 m 4.941876\n",
      "146 Train Loss 2.8296618 Test RE 0.00043749057403371126 c 1.0008548 k 0.049993563 m 4.9464974\n",
      "147 Train Loss 2.8080416 Test RE 0.0004477758965899589 c 1.0003327 k 0.05001316 m 4.957652\n",
      "148 Train Loss 2.7742364 Test RE 0.0004618668113753648 c 1.0004995 k 0.050034713 m 4.974207\n",
      "149 Train Loss 2.6919243 Test RE 0.00046068370887081386 c 1.0003006 k 0.050034024 m 4.976777\n",
      "150 Train Loss 2.5725968 Test RE 0.00043848324252065914 c 0.998998 k 0.05005731 m 4.967662\n",
      "151 Train Loss 2.4460437 Test RE 0.0004006613909316183 c 0.9998892 k 0.050042186 m 4.958364\n",
      "152 Train Loss 2.3513196 Test RE 0.00036972520187142214 c 0.99992883 k 0.05001806 m 4.956671\n",
      "153 Train Loss 2.3092964 Test RE 0.0003323332102669111 c 1.000236 k 0.04998958 m 4.9551015\n",
      "154 Train Loss 2.2996879 Test RE 0.00031341208184015434 c 1.0004506 k 0.04997931 m 4.9565206\n",
      "155 Train Loss 2.2915783 Test RE 0.00030458062607193555 c 1.0005834 k 0.049983148 m 4.966505\n",
      "156 Train Loss 2.2836719 Test RE 0.00031026530439601805 c 1.0003383 k 0.04999654 m 4.980322\n",
      "157 Train Loss 2.2813714 Test RE 0.00031441995614499665 c 1.0003275 k 0.049997944 m 4.9835563\n",
      "158 Train Loss 2.278746 Test RE 0.00031784902937976407 c 1.0004268 k 0.049993042 m 4.983863\n",
      "159 Train Loss 2.2704036 Test RE 0.00031678299633585533 c 1.0008395 k 0.049970783 m 4.9721065\n",
      "160 Train Loss 2.2536879 Test RE 0.00030001753346262024 c 1.0007372 k 0.049972646 m 4.9624295\n",
      "161 Train Loss 2.237364 Test RE 0.000293366677924054 c 1.0002722 k 0.049986873 m 4.9643784\n",
      "162 Train Loss 2.2193239 Test RE 0.00030230664849935184 c 0.9997961 k 0.049993467 m 4.968009\n",
      "163 Train Loss 2.191643 Test RE 0.0003045394120551831 c 1.0002913 k 0.049974848 m 4.9658346\n",
      "164 Train Loss 2.1627011 Test RE 0.00029276091151721504 c 1.0007392 k 0.049978565 m 4.9597363\n",
      "165 Train Loss 2.1483157 Test RE 0.000287227709755764 c 1.0006268 k 0.04996693 m 4.957254\n",
      "166 Train Loss 2.1404593 Test RE 0.0002936256543260923 c 1.0004385 k 0.049958665 m 4.9549565\n",
      "167 Train Loss 2.1254876 Test RE 0.0002983465844068493 c 1.0011798 k 0.04996176 m 4.959094\n",
      "168 Train Loss 2.124133 Test RE 0.00029872283301498784 c 1.0012137 k 0.04996481 m 4.9604244\n",
      "169 Train Loss 2.1169848 Test RE 0.0003045367339433797 c 1.0012395 k 0.049969245 m 4.96517\n",
      "170 Train Loss 2.1119514 Test RE 0.00030901715848505237 c 1.0013545 k 0.049956594 m 4.9683022\n",
      "171 Train Loss 2.1017537 Test RE 0.0003038436672975125 c 1.001216 k 0.049945287 m 4.9711123\n",
      "172 Train Loss 2.0933409 Test RE 0.00029308049056529567 c 1.0015701 k 0.049949806 m 4.9682546\n",
      "173 Train Loss 2.0833426 Test RE 0.0002907675872217853 c 1.0025402 k 0.04993303 m 4.9616456\n",
      "174 Train Loss 2.041395 Test RE 0.0002784425484295866 c 1.002627 k 0.049905904 m 4.9404454\n",
      "175 Train Loss 1.9742925 Test RE 0.000270992124507311 c 1.0004288 k 0.049953487 m 4.929075\n",
      "176 Train Loss 1.8125817 Test RE 0.0002850447274055418 c 1.0011404 k 0.049929384 m 4.9413347\n",
      "177 Train Loss 1.7152586 Test RE 0.0002593848055901976 c 1.002609 k 0.049922716 m 4.9653378\n",
      "178 Train Loss 1.6808622 Test RE 0.00026179840242008157 c 1.0007458 k 0.049958408 m 4.97554\n",
      "179 Train Loss 1.6375376 Test RE 0.0002805695750322998 c 0.99915034 k 0.049985275 m 4.9689026\n",
      "180 Train Loss 1.5780094 Test RE 0.00028508016643930505 c 1.0015092 k 0.049916808 m 4.9519286\n",
      "181 Train Loss 1.5225902 Test RE 0.0002750696236473785 c 1.0022863 k 0.049886342 m 4.9409738\n",
      "182 Train Loss 1.4714774 Test RE 0.0002603291094482848 c 1.0019314 k 0.04990206 m 4.9371643\n",
      "183 Train Loss 1.4441874 Test RE 0.00025449369000901347 c 1.002462 k 0.04988608 m 4.938511\n",
      "184 Train Loss 1.4016609 Test RE 0.00024937488082533163 c 1.0019706 k 0.04989198 m 4.9440465\n",
      "185 Train Loss 1.3539786 Test RE 0.0002582131947891632 c 1.000123 k 0.049953464 m 4.9504414\n",
      "186 Train Loss 1.3309748 Test RE 0.0002620732067661595 c 0.99999785 k 0.049966924 m 4.954821\n",
      "187 Train Loss 1.3074956 Test RE 0.00026019393175392015 c 1.0009259 k 0.049925875 m 4.9585795\n",
      "188 Train Loss 1.2824049 Test RE 0.0002538456146246748 c 1.0020456 k 0.049932104 m 4.9593425\n",
      "189 Train Loss 1.2690109 Test RE 0.00025530954797612347 c 1.0016701 k 0.04995587 m 4.957933\n",
      "190 Train Loss 1.2547839 Test RE 0.0002681038858028638 c 1.0008852 k 0.04996175 m 4.9575763\n",
      "191 Train Loss 1.2354393 Test RE 0.0002772221033917797 c 1.0005625 k 0.049953334 m 4.9613895\n",
      "192 Train Loss 1.2059017 Test RE 0.0002727990920027583 c 1.0004388 k 0.04997439 m 4.9645305\n",
      "193 Train Loss 1.1981492 Test RE 0.0002648599857235455 c 1.0003247 k 0.04998873 m 4.9624915\n",
      "194 Train Loss 1.1921415 Test RE 0.0002644358278279111 c 1.0004855 k 0.049977135 m 4.9620543\n",
      "195 Train Loss 1.1817377 Test RE 0.00026890470790017266 c 1.0004417 k 0.049973927 m 4.9634438\n",
      "196 Train Loss 1.1593032 Test RE 0.0002720567997107635 c 1.0005604 k 0.049985085 m 4.9628086\n",
      "197 Train Loss 1.1469283 Test RE 0.00027271490311112887 c 1.0008044 k 0.049978595 m 4.9625664\n",
      "198 Train Loss 1.140316 Test RE 0.0002705433349387547 c 1.0011235 k 0.049977556 m 4.9605575\n",
      "199 Train Loss 1.1299601 Test RE 0.0002657936278670251 c 1.0007885 k 0.04998418 m 4.9581676\n",
      "Training time: 70.43\n",
      "Training time: 70.43\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 271023.5 Test RE 0.3495244913895521 c 0.018474707 k 0.06837772 m -0.00012453854\n",
      "1 Train Loss 51086.395 Test RE 0.14891467068142292 c 0.026840989 k 0.05928034 m -1.1251417e-05\n",
      "2 Train Loss 35109.793 Test RE 0.12183985992461174 c 0.07090328 k 0.06392405 m 0.0014781591\n",
      "3 Train Loss 8336.468 Test RE 0.03285970507522974 c 0.5161407 k 0.052966345 m 0.013176168\n",
      "4 Train Loss 4803.0435 Test RE 0.017647381857812634 c 0.68021923 k 0.04932051 m 0.015721126\n",
      "5 Train Loss 4488.281 Test RE 0.016901614849160078 c 0.74905914 k 0.050963685 m 0.016534178\n",
      "6 Train Loss 4311.47 Test RE 0.015851803683522174 c 0.88154393 k 0.04855088 m 0.018507546\n",
      "7 Train Loss 4179.7153 Test RE 0.01620399466183055 c 1.0039598 k 0.04456284 m 0.020057954\n",
      "8 Train Loss 4146.8384 Test RE 0.015161079222523436 c 0.9984133 k 0.047003284 m 0.020055238\n",
      "9 Train Loss 4083.2788 Test RE 0.014992595796611479 c 1.1442261 k 0.045764692 m 0.022896564\n",
      "10 Train Loss 3792.114 Test RE 0.015807528878087118 c 1.4197041 k 0.03898801 m 0.032561656\n",
      "11 Train Loss 3518.848 Test RE 0.014122462809836945 c 1.2277719 k 0.043918557 m 0.037545625\n",
      "12 Train Loss 2992.0361 Test RE 0.014150604538804479 c 1.0687622 k 0.04506759 m 0.05871339\n",
      "13 Train Loss 2295.3176 Test RE 0.011469530157202215 c 0.97567415 k 0.04644796 m 0.08341804\n",
      "14 Train Loss 752.8751 Test RE 0.006627597466394222 c 1.089271 k 0.04758846 m 0.17708507\n",
      "15 Train Loss 385.26932 Test RE 0.005149395867405668 c 1.0165118 k 0.045779992 m 0.21326293\n",
      "16 Train Loss 365.96738 Test RE 0.006384139739629931 c 1.1086959 k 0.045156024 m 0.22949772\n",
      "17 Train Loss 362.14453 Test RE 0.0066866770457350375 c 1.1001109 k 0.04512072 m 0.23918188\n",
      "18 Train Loss 354.32452 Test RE 0.006266945183466911 c 1.0946288 k 0.04489191 m 0.2811781\n",
      "19 Train Loss 326.43 Test RE 0.006391548563909337 c 1.1024386 k 0.04509607 m 0.43132865\n",
      "20 Train Loss 309.44016 Test RE 0.006138130975986715 c 1.1010417 k 0.045216825 m 0.54786205\n",
      "21 Train Loss 295.56726 Test RE 0.006156552066466816 c 1.0928293 k 0.045357496 m 0.65463763\n",
      "22 Train Loss 286.2749 Test RE 0.006780034535050154 c 1.1005346 k 0.04542132 m 0.75768214\n",
      "23 Train Loss 263.6217 Test RE 0.007839780353695848 c 1.1155512 k 0.045527346 m 1.0479138\n",
      "24 Train Loss 257.80945 Test RE 0.008298064757991351 c 1.0964687 k 0.04554231 m 1.1922491\n",
      "25 Train Loss 193.789 Test RE 0.007640384088222789 c 1.0394039 k 0.046683073 m 2.1275394\n",
      "26 Train Loss 129.66547 Test RE 0.0060431041896292295 c 1.0989814 k 0.047569726 m 3.3478985\n",
      "27 Train Loss 100.534836 Test RE 0.004811761358422382 c 1.0212376 k 0.048032288 m 3.7126389\n",
      "28 Train Loss 73.92922 Test RE 0.003600371768253693 c 1.0326709 k 0.0488048 m 4.393305\n",
      "29 Train Loss 46.95101 Test RE 0.0030384696202025462 c 0.99590033 k 0.049461495 m 5.0003853\n",
      "30 Train Loss 31.843958 Test RE 0.002217027091105386 c 0.98867816 k 0.050238166 m 5.433996\n",
      "31 Train Loss 26.849072 Test RE 0.0017736689318388415 c 0.9769285 k 0.05062434 m 5.5278025\n",
      "32 Train Loss 24.039742 Test RE 0.0015349145748435843 c 0.9888493 k 0.050534 m 5.5663037\n",
      "33 Train Loss 16.760912 Test RE 0.001133790176970603 c 0.9922951 k 0.050143056 m 5.473598\n",
      "34 Train Loss 13.2017565 Test RE 0.0012208420118592267 c 0.99592495 k 0.050190806 m 5.364394\n",
      "35 Train Loss 8.096451 Test RE 0.0009550408639213546 c 0.992887 k 0.05031731 m 5.2511477\n",
      "36 Train Loss 6.537125 Test RE 0.0009057462400042255 c 0.98485893 k 0.05032794 m 5.198798\n",
      "37 Train Loss 5.1904173 Test RE 0.0009542051237058588 c 1.0003285 k 0.05019399 m 5.0783796\n",
      "38 Train Loss 3.9184346 Test RE 0.0008831248327373697 c 0.98712176 k 0.050343584 m 5.075836\n",
      "39 Train Loss 3.3516662 Test RE 0.0007586868425206407 c 0.99290586 k 0.050303303 m 5.1320834\n",
      "40 Train Loss 2.4013567 Test RE 0.00043498682839079286 c 0.9984894 k 0.050195534 m 5.1691666\n",
      "41 Train Loss 1.3887998 Test RE 0.00025238090155722554 c 0.9937232 k 0.050112262 m 5.079909\n",
      "42 Train Loss 1.0617149 Test RE 0.0002953278596222653 c 0.9990248 k 0.05002351 m 5.0630293\n",
      "43 Train Loss 0.67073625 Test RE 0.0002510402985562312 c 1.0007524 k 0.049945816 m 5.026742\n",
      "44 Train Loss 0.63684714 Test RE 0.0002320564460520914 c 1.0003464 k 0.049960118 m 5.0190864\n",
      "45 Train Loss 0.6302935 Test RE 0.00022556011143961313 c 1.0000713 k 0.049974766 m 5.0214148\n",
      "46 Train Loss 0.61154795 Test RE 0.0002165700827966201 c 0.9993433 k 0.05000673 m 5.0405564\n",
      "47 Train Loss 0.59432185 Test RE 0.0002163269967892879 c 0.9991932 k 0.050012887 m 5.056568\n",
      "48 Train Loss 0.56904536 Test RE 0.0002064453282415445 c 0.99895906 k 0.050033018 m 5.0715804\n",
      "49 Train Loss 0.54519516 Test RE 0.00018122887612489114 c 0.99854004 k 0.05003953 m 5.072917\n",
      "50 Train Loss 0.5234654 Test RE 0.00015919799615728527 c 0.9982891 k 0.05003664 m 5.06149\n",
      "51 Train Loss 0.515439 Test RE 0.00015820136251505038 c 0.9990172 k 0.050021894 m 5.0506353\n",
      "52 Train Loss 0.5142159 Test RE 0.0001587132290852445 c 0.9992685 k 0.050015308 m 5.0458755\n",
      "53 Train Loss 0.5108275 Test RE 0.0001586429316131851 c 0.9994881 k 0.050002284 m 5.0363684\n",
      "54 Train Loss 0.4826417 Test RE 0.00016182621294005345 c 0.9995305 k 0.049975235 m 5.008114\n",
      "55 Train Loss 0.35497984 Test RE 0.0001595994315527801 c 0.998137 k 0.050020684 m 5.022922\n",
      "56 Train Loss 0.33624083 Test RE 0.00015046600196807534 c 0.9997641 k 0.05000096 m 5.0315914\n",
      "57 Train Loss 0.3229381 Test RE 0.00015119640662571865 c 1.0005963 k 0.049974788 m 5.0204635\n",
      "58 Train Loss 0.30748862 Test RE 0.00015402724304941937 c 0.9996869 k 0.04997898 m 5.007241\n",
      "59 Train Loss 0.27793908 Test RE 0.00014975599445617586 c 0.99886245 k 0.050007224 m 5.0197425\n",
      "60 Train Loss 0.27085808 Test RE 0.00014085109211122988 c 0.99986196 k 0.049997054 m 5.0255322\n",
      "61 Train Loss 0.2679716 Test RE 0.00013764084752303132 c 1.0005281 k 0.04997898 m 5.0181065\n",
      "62 Train Loss 0.26677406 Test RE 0.00013771718846428675 c 1.0005561 k 0.049974993 m 5.012158\n",
      "63 Train Loss 0.25107312 Test RE 0.00013587067802753007 c 1.0000497 k 0.049973648 m 4.986998\n",
      "64 Train Loss 0.21368799 Test RE 0.0001180540542532666 c 1.0009878 k 0.049959615 m 4.9824204\n",
      "65 Train Loss 0.18658845 Test RE 0.00010696207006029777 c 1.0018501 k 0.049961388 m 5.004448\n",
      "66 Train Loss 0.16072497 Test RE 0.00010060052225716497 c 1.0002303 k 0.049989935 m 5.0051317\n",
      "67 Train Loss 0.1530258 Test RE 9.546864487895257e-05 c 0.9998555 k 0.049996007 m 4.9979067\n",
      "68 Train Loss 0.14893183 Test RE 9.124248322028505e-05 c 0.99990225 k 0.049997322 m 4.9970737\n",
      "69 Train Loss 0.14751315 Test RE 8.83895877663053e-05 c 0.9999862 k 0.04999827 m 4.998719\n",
      "70 Train Loss 0.14689563 Test RE 8.776773399316241e-05 c 1.0000198 k 0.04999922 m 4.9982796\n",
      "71 Train Loss 0.14545001 Test RE 8.929308488370463e-05 c 0.99990225 k 0.050003793 m 4.9967813\n",
      "72 Train Loss 0.14144102 Test RE 9.14838354816497e-05 c 0.9997758 k 0.05000572 m 4.994115\n",
      "73 Train Loss 0.13695106 Test RE 8.861198405885117e-05 c 0.9996317 k 0.050009534 m 4.9952073\n",
      "74 Train Loss 0.13592333 Test RE 8.835306866319412e-05 c 0.9999117 k 0.050006013 m 4.9949694\n",
      "75 Train Loss 0.13560382 Test RE 8.875563231720568e-05 c 1.0000354 k 0.050005283 m 4.994825\n",
      "76 Train Loss 0.13431413 Test RE 9.018471311915292e-05 c 0.9999774 k 0.050008673 m 4.9955654\n",
      "77 Train Loss 0.13012044 Test RE 9.312037359729517e-05 c 0.9994807 k 0.05001966 m 4.9963512\n",
      "78 Train Loss 0.11244637 Test RE 8.947473718683145e-05 c 0.9986865 k 0.050037276 m 4.9984264\n",
      "79 Train Loss 0.10393488 Test RE 8.321193728517177e-05 c 0.9990511 k 0.050037526 m 5.0016623\n",
      "80 Train Loss 0.09988183 Test RE 8.351634665328033e-05 c 0.999641 k 0.050030477 m 5.0040812\n",
      "81 Train Loss 0.09142631 Test RE 9.211589957048576e-05 c 0.9995662 k 0.050035134 m 5.0042343\n",
      "82 Train Loss 0.08105253 Test RE 8.208025285571641e-05 c 0.9984541 k 0.050053183 m 5.0111694\n",
      "83 Train Loss 0.07489761 Test RE 6.906875301181033e-05 c 0.9988497 k 0.05004558 m 5.0108685\n",
      "84 Train Loss 0.07052782 Test RE 6.432371818123578e-05 c 0.99984676 k 0.050026372 m 5.005933\n",
      "85 Train Loss 0.061930515 Test RE 6.758735408774877e-05 c 1.0000427 k 0.05002112 m 5.0022492\n",
      "86 Train Loss 0.059151225 Test RE 6.593452152849664e-05 c 0.99944496 k 0.050025906 m 4.999422\n",
      "87 Train Loss 0.05752907 Test RE 6.143519562792973e-05 c 0.9995256 k 0.0500212 m 4.9954433\n",
      "88 Train Loss 0.056430932 Test RE 6.0624344868464414e-05 c 0.9996877 k 0.050017443 m 4.9971814\n",
      "89 Train Loss 0.05570571 Test RE 6.087745910941705e-05 c 0.9996209 k 0.05002054 m 5.000813\n",
      "90 Train Loss 0.055068694 Test RE 6.0187168502664836e-05 c 0.9994607 k 0.050025128 m 5.001945\n",
      "91 Train Loss 0.05460764 Test RE 5.907414679073356e-05 c 0.9994628 k 0.050024193 m 5.0008435\n",
      "92 Train Loss 0.054324552 Test RE 5.8299235419060384e-05 c 0.999514 k 0.050022144 m 5.000241\n",
      "93 Train Loss 0.054178916 Test RE 5.7724502468375614e-05 c 0.99958205 k 0.050019722 m 5.0002623\n",
      "94 Train Loss 0.054062907 Test RE 5.778629995608156e-05 c 0.99956983 k 0.05002003 m 5.0004053\n",
      "95 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "96 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "97 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "98 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "99 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "100 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "101 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "102 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "103 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "104 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "105 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "106 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "107 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "108 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "109 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "110 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "111 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "112 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "113 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "114 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "115 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "116 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "117 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "118 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "119 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "120 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "121 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "122 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "123 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "124 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "125 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "126 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "127 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "128 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "129 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "130 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "131 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "132 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "133 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "134 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "135 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "136 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "137 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "138 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "139 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "140 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "141 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "142 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "143 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "144 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "145 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "146 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "147 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "148 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "149 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "150 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "151 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "152 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "153 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "154 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "155 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "156 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "157 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "158 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "159 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "160 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "161 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "162 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "163 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "164 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "165 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "166 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "167 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "168 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "169 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "170 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "171 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "172 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "173 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "174 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "175 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "176 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "177 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "178 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "179 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "180 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "181 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "182 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "183 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "184 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "185 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "186 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "187 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "188 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "189 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "190 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "191 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "192 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "193 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "194 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "195 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "196 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "197 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "198 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "199 Train Loss 0.05400484 Test RE 5.792508087078589e-05 c 0.99955434 k 0.0500204 m 5.0005517\n",
      "Training time: 43.54\n",
      "Training time: 43.54\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 200\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "m_full = []\n",
    "k_full = []\n",
    "c_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "\n",
    "for reps in range(max_reps):   \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []   \n",
    "    beta_val = []\n",
    "    m_val = []\n",
    "    k_val = []\n",
    "    c_val = []\n",
    "\n",
    "    torch.manual_seed(reps*36)\n",
    "    N_f = 10000 #Total number of collocation points\n",
    "\n",
    "    layers = np.array([1,50,50,50,1]) #9 hidden layers\n",
    "    PINN = Sequentialmodel(layers)\n",
    "\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.05, \n",
    "                                max_iter = 10, \n",
    "                                max_eval = 15, \n",
    "                                tolerance_grad = 1e-6, \n",
    "                                tolerance_change = 1e-6, \n",
    "                                history_size = 100, \n",
    "                                line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "    train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    beta_full.append(beta_val)\n",
    "    m_full.append(m_val)\n",
    "    k_full.append(k_val)\n",
    "    c_full.append(c_val)\n",
    "    \n",
    "\n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"beta\": beta_full,\"m\": m_full,\"k\": k_full,\"c\": c_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "pmHEeBpzfFQh",
    "outputId": "990b2054-f35e-4c8e-c378-69ff2eb19f4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd5c0760090>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_pred = PINN.test()\n",
    "plt.plot(t,x_true,'b')\n",
    "plt.plot(t,x_pred,'r--')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "stan_1D_FODE_tune.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
