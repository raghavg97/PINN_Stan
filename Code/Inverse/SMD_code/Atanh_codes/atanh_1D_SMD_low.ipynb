{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8byrnUmNKGR",
    "outputId": "510f67c6-b225-41ba-e78a-d0a6e54ded85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uoNYpDzlNKGV"
   },
   "outputs": [],
   "source": [
    "tstart= 0 \n",
    "tstop= 60 \n",
    "\n",
    "increment = 0.1 \n",
    "# Initial condition\n",
    "x_init= [0,0] \n",
    "\n",
    "t = np.arange(tstart,tstop+1,increment)\n",
    "\n",
    "c = 1 # Damping constant \n",
    "k = 1 # Stiffness of the spring\n",
    "m = 5 # Mass \n",
    "F0 = 100\n",
    "\n",
    "def mydiff(x, t):    \n",
    "    F =F0\n",
    "    \n",
    "    dx1dt = x[1] \n",
    "    dx2dt = (F -c*x[1] -k*x[0])/m\n",
    "    dxdt= [dx1dt, dx2dt] \n",
    "    \n",
    "    return dxdt \n",
    "\n",
    "x_full_sol = odeint(mydiff, x_init, t) \n",
    "\n",
    "x_sol = x_full_sol[:,0]\n",
    "v_sol = x_full_sol[:,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BR02v-fkNKGV"
   },
   "outputs": [],
   "source": [
    "level = \"low\"\n",
    "label = \"1D_SMD_atanh_\" + level\n",
    "\n",
    "loss_thresh = 0.005\n",
    "\n",
    "t_test = t.reshape(-1,1)\n",
    "t_test_tensor = torch.from_numpy(t_test).float().to(device)\n",
    "\n",
    "# bc1_t = t[0].reshape(-1,1)\n",
    "# bc1_x = x_sol[0].reshape(-1,1)\n",
    "# t_bc1_train = torch.from_numpy(bc1_t).float().to(device)\n",
    "# x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "\n",
    "bc1_t = t.reshape(-1,1)\n",
    "bc1_x = x_sol.reshape(-1,1)\n",
    "t_bc1_train = torch.from_numpy(bc1_t).float().to(device)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "\n",
    "\n",
    "bc2_t = t[0].reshape(-1,1)\n",
    "t_bc2_train = torch.from_numpy(bc2_t).float().to(device)\n",
    "bc2_val = v_sol[0].reshape(-1,1)\n",
    "bc2_val =torch.from_numpy(bc2_val).float().to(device)\n",
    "\n",
    "t_test = t.reshape(-1,1)\n",
    "t_test_tensor = torch.from_numpy(t_test).float().to(device)\n",
    "x_true = x_sol\n",
    "x_true_norm = np.linalg.norm(x_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(t[0]) \n",
    "ub = np.array(t[-1]) \n",
    "\n",
    "#torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SyyktBKBXRo1"
   },
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    \n",
    "    t01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=t01,random_state =seed)\n",
    "\n",
    "    t_coll = lb + (ub-lb)*sampling(N_f)\n",
    "    # t_coll = np.vstack((t_coll,)) # append training points to collocation points \n",
    "\n",
    "    return t_coll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "o1b21zLnNKGW"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "        \n",
    "        \n",
    "        self.k = Parameter(torch.tensor(0.0))\n",
    "        self.k.requiresGrad = True\n",
    "        self.c = Parameter(torch.tensor(0.0))\n",
    "        self.c.requiresGrad = True\n",
    "        self.m = Parameter(torch.tensor(0.0))\n",
    "        self.m.requiresGrad = True\n",
    "        \n",
    "        self.alpha = Parameter(torch.ones((50,len(layers)-2)))\n",
    "        self.alpha.requiresGrad = True\n",
    "        \n",
    "        self.n = torch.tensor(1.0)\n",
    "        \n",
    "    'forward pass'\n",
    "    def forward(self,t):\n",
    "        if torch.is_tensor(t) != True:         \n",
    "            t = torch.from_numpy(t)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        t = 2.0*(t - l_b)/(u_b - l_b) - 1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = t.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(self.n*self.alpha[:,i]*z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,t,x):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(t), x)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,t_bc2,bc2_val):\n",
    "        g = t_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        x = self.forward(g)    \n",
    "            \n",
    "        x_t = autograd.grad(x,g,torch.ones([t_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dx_dt = x_t[:,[0]]\n",
    "        \n",
    "        bc2 = dx_dt\n",
    "        \n",
    "        loss_bc2= self.loss_function(bc2,bc2_val)\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self,t_coll,f_hat):\n",
    "             \n",
    "        g = t_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        x = self.forward(g) \n",
    "\n",
    "        x_t = autograd.grad(x,g,torch.ones([t_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        x_tt = autograd.grad(x_t,g,torch.ones(t_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dx_dt = x_t[:,[0]]\n",
    "        \n",
    "        dx2_d2t = x_tt[:,[0]]\n",
    "        \n",
    "        f = self.m*dx2_d2t + self.c*dx_dt + self.k*x - F0\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,t_bc1,x_bc1,t_bc2,bc2_val,t_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(t_bc1,x_bc1)\n",
    "        loss_bc2 = self.loss_BC2(t_bc2,bc2_val)\n",
    "        loss_f = self.loss_PDE(t_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self):\n",
    "        x_pred = self.forward(t_test_tensor)\n",
    "        x_pred = x_pred.cpu().detach().numpy()\n",
    "\n",
    "        return x_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        x_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(x_pred.reshape(-1,1) - x_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(x_pred.reshape(-1,1) - x_true.reshape(-1,1),2)/x_true_norm\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fLY2mT5BOgjD"
   },
   "outputs": [],
   "source": [
    "def train_step(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8srA5uGuObil"
   },
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    m_val.append(PINN.m.cpu().detach().numpy())\n",
    "    k_val.append(PINN.k.cpu().detach().numpy())\n",
    "    c_val.append(PINN.c.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0SezTZ_racQB"
   },
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "\n",
    "    t_coll = colloc_pts(N_f,rep*11)\n",
    "    t_coll =  torch.from_numpy(t_coll).float().to(device)\n",
    "    \n",
    "    f_hat = torch.zeros(t_coll.shape[0],1).to(device)\n",
    "    for i in range(max_iter):      \n",
    "        train_step(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat)\n",
    "         \n",
    "        loss_np = PINN.loss(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test RE\",test_re_loss[-1],\"c\",PINN.c.cpu().detach().numpy(),\"k\",PINN.k.cpu().detach().numpy(),\"m\",PINN.m.cpu().detach().numpy())\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I9BYbcJ0NKGX",
    "outputId": "42720d97-b37a-4c42-b7e5-b77a0ef391f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 940.9352 Test RE 0.2878743983824057 c -0.20319524 k 1.1294433 m -0.0002806618\n",
      "1 Train Loss 496.99927 Test RE 0.22123423142127221 c -0.24208802 k 1.0177344 m -0.00034054837\n",
      "2 Train Loss 496.437 Test RE 0.22107320158679689 c -0.2433119 k 1.0141305 m -0.00034240852\n",
      "3 Train Loss 495.99908 Test RE 0.22100087987974418 c -0.24468724 k 1.0207372 m -0.00034438606\n",
      "4 Train Loss 494.13992 Test RE 0.2204674106662908 c -0.24390823 k 1.0064884 m -0.000339039\n",
      "5 Train Loss 455.0497 Test RE 0.2109920220199771 c -0.23518322 k 1.0155346 m 0.00020895002\n",
      "6 Train Loss 418.80057 Test RE 0.1997598810513011 c -0.18662547 k 1.013932 m -0.0021239326\n",
      "7 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "8 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "9 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "10 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "11 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "12 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "13 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "14 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "15 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "16 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "17 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "18 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "19 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "20 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "21 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "22 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "23 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "24 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "25 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "26 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "27 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "28 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "29 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "30 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "31 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "32 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "33 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "34 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "35 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "36 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "37 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "38 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "39 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "40 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "41 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "42 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "43 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "44 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "45 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "46 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "47 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "48 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "49 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "50 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "51 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "52 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "53 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "54 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "55 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "56 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "57 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "58 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "59 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "60 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "61 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "62 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "63 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "64 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "65 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "66 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "67 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "68 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "69 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "70 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "71 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "72 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "73 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "74 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "75 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "76 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "77 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "78 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "79 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "80 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "81 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "82 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "83 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "84 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "85 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "86 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "87 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "88 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "89 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "90 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "91 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "92 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "93 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "94 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "95 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "96 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "97 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "98 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "99 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "100 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "101 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "102 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "103 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "104 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "105 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "106 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "107 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "108 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "109 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "110 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "111 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "112 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "113 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "114 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "115 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "116 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "117 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "118 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "119 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "120 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "121 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "122 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "123 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "124 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "125 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "126 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "127 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "128 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "129 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "130 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "131 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "132 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "133 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "134 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "135 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "136 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "137 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "138 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "139 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "140 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "141 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "142 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "143 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "144 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "145 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "146 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "147 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "148 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "149 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "150 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "151 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "152 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "153 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "154 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "155 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "156 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "157 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "158 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "159 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "160 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "161 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "162 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "163 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "164 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "165 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "166 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "167 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "168 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "169 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "170 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "171 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "172 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "173 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "174 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "175 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "176 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "177 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "178 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "179 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "180 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "181 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "182 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "183 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "184 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "185 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "186 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "187 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "188 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "189 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "190 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "191 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "192 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "193 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "194 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "195 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "196 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "197 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "198 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "199 Train Loss 409.03967 Test RE 0.19590142882471992 c -0.1554468 k 1.0188799 m 0.0003495037\n",
      "Training time: 25.86\n",
      "Training time: 25.86\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 1533.6829 Test RE 0.3860330194228373 c -0.294935 k 1.4380515 m -0.00045916523\n",
      "1 Train Loss 496.9945 Test RE 0.22123317762805006 c -0.40469855 k 1.0177988 m -0.00064732035\n",
      "2 Train Loss 496.7755 Test RE 0.2211539385660784 c -0.40758663 k 1.0130833 m -0.0006517147\n",
      "3 Train Loss 495.98688 Test RE 0.22099799952469495 c -0.4148085 k 1.0202929 m -0.0006615167\n",
      "4 Train Loss 493.3297 Test RE 0.2202904191787065 c -0.41466632 k 1.0230598 m -0.00065857597\n",
      "5 Train Loss 482.66226 Test RE 0.21706822615654145 c -0.40567377 k 0.992095 m -0.00054162595\n",
      "6 Train Loss 400.15283 Test RE 0.19493354750961625 c -0.38072067 k 1.0038022 m 0.0035616155\n",
      "7 Train Loss 356.55966 Test RE 0.17181810629250258 c -0.30785275 k 1.0092258 m 0.023867099\n",
      "8 Train Loss 351.033 Test RE 0.1662023580942132 c -0.28359872 k 1.0061853 m 0.03047598\n",
      "9 Train Loss 336.15283 Test RE 0.1644788404104612 c 0.15113707 k 1.005165 m 0.22481427\n",
      "10 Train Loss 304.81198 Test RE 0.1636500582832446 c 1.6150004 k 0.99816936 m 0.8749151\n",
      "11 Train Loss 293.72934 Test RE 0.16431249183204175 c 2.9678965 k 0.9943476 m 1.4808754\n",
      "12 Train Loss 290.2679 Test RE 0.1634266990401088 c 2.9692824 k 0.9844748 m 1.4887539\n",
      "13 Train Loss 276.94107 Test RE 0.1596569341326109 c 3.1257288 k 0.98808134 m 1.57325\n",
      "14 Train Loss 274.83643 Test RE 0.15871759285596274 c 2.5566475 k 0.9910052 m 1.3465353\n",
      "15 Train Loss 265.37048 Test RE 0.1555317399959782 c 2.6496952 k 0.98546076 m 1.4943911\n",
      "16 Train Loss 254.24774 Test RE 0.15147089600134428 c 2.6645973 k 0.98891735 m 1.6678077\n",
      "17 Train Loss 246.85417 Test RE 0.1487068625063962 c 2.1303747 k 0.98728746 m 1.5496309\n",
      "18 Train Loss 236.11252 Test RE 0.1448206189144328 c 2.5290675 k 0.9809916 m 1.8167359\n",
      "19 Train Loss 231.90044 Test RE 0.1421383800495635 c 2.317194 k 0.9841043 m 1.7913604\n",
      "20 Train Loss 230.1842 Test RE 0.1406776765229514 c 2.2813544 k 0.9788684 m 1.8045675\n",
      "21 Train Loss 228.53442 Test RE 0.14077412998256436 c 2.3772612 k 0.97922325 m 1.8082865\n",
      "22 Train Loss 225.82526 Test RE 0.13893403889728925 c 2.1996095 k 0.9821447 m 1.7353181\n",
      "23 Train Loss 221.20766 Test RE 0.13681439966865958 c 2.3261657 k 0.978537 m 1.7990834\n",
      "24 Train Loss 218.2537 Test RE 0.13556842853477705 c 2.3050902 k 0.9755735 m 1.7853854\n",
      "25 Train Loss 215.8535 Test RE 0.13518460507351093 c 2.207124 k 0.97673213 m 1.7197765\n",
      "26 Train Loss 214.32047 Test RE 0.13450891757684125 c 2.1762166 k 0.9851109 m 1.7043308\n",
      "27 Train Loss 213.65854 Test RE 0.13412177214312979 c 2.1943817 k 0.9765301 m 1.7174536\n",
      "28 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "29 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "30 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "31 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "32 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "33 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "34 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "35 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "36 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "37 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "38 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "39 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "40 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "41 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "42 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "43 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "44 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "45 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "46 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "47 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "48 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "49 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "50 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "51 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "52 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "53 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "54 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "55 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "56 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "57 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "58 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "59 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "60 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "61 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "62 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "63 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "64 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "65 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "66 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "67 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "68 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "69 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "70 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "71 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "72 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "73 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "74 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "75 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "76 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "77 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "78 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "79 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "80 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "81 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "82 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "83 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "84 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "85 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "86 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "87 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "88 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "89 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "90 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "91 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "92 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "93 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "94 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "95 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "96 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "97 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "98 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "99 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "100 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "101 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "102 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "103 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "104 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "105 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "106 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "107 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "108 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "109 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "110 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "111 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "112 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "113 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "114 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "115 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "116 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "117 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "118 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "119 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "120 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "121 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "122 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "123 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "124 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "125 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "126 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "127 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "128 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "129 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "130 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "131 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "132 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "133 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "134 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "135 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "136 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "137 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "138 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "139 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "140 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "141 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "142 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "143 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "144 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "145 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "146 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "147 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "148 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "149 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "150 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "151 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "152 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "153 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "154 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "155 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "156 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "157 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "158 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "159 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "160 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "161 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "162 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "163 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "164 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "165 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "166 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "167 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "168 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "169 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "170 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "171 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "172 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "173 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "174 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "175 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "176 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "177 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "178 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "179 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "180 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "181 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "182 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "183 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "184 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "185 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "186 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "187 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "188 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "189 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "190 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "191 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "192 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "193 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "194 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "195 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "196 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "197 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "198 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "199 Train Loss 213.65552 Test RE 0.13411537195689344 c 2.1970074 k 0.97633684 m 1.7184474\n",
      "Training time: 31.35\n",
      "Training time: 31.35\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 696.6009 Test RE 0.25819467618171454 c 0.022713112 k 1.126174 m -0.0001912988\n",
      "1 Train Loss 497.0124 Test RE 0.221237180332454 c 0.025642566 k 1.0176849 m -0.00021834321\n",
      "2 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "3 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "4 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "5 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "6 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "7 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "8 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "9 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "10 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "11 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "12 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "13 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "14 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "15 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "16 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "17 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "18 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "19 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "20 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "21 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "22 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "23 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "24 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "25 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "26 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "27 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "28 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "29 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "30 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "31 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "32 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "33 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "34 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "35 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "36 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "37 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "38 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "39 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "40 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "41 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "42 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "43 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "44 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "45 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "46 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "47 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "48 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "49 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "50 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "51 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "52 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "53 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "54 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "55 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "56 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "57 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "58 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "59 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "60 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "61 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "62 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "63 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "64 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "65 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "66 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "67 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "68 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "69 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "70 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "71 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "72 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "73 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "74 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "75 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "76 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "77 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "78 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "79 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "80 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "81 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "82 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "83 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "84 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "85 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "86 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "87 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "88 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "89 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "90 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "91 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "92 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "93 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "94 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "95 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "96 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "97 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "98 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "99 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "100 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "101 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "102 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "103 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "104 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "105 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "106 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "107 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "108 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "109 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "110 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "111 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "112 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "113 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "114 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "115 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "116 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "117 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "118 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "119 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "120 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "121 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "122 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "123 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "124 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "125 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "126 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "127 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "128 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "129 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "130 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "131 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "132 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "133 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "134 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "135 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "136 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "137 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "138 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "139 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "140 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "141 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "142 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "143 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "144 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "145 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "146 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "147 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "148 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "149 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "150 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "151 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "152 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "153 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "154 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "155 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "156 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "157 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "158 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "159 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "160 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "161 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "162 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "163 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "164 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "165 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "166 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "167 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "168 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "169 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "170 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "171 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "172 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "173 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "174 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "175 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "176 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "177 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "178 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "179 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "180 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "181 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "182 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "183 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "184 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "185 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "186 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "187 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "188 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "189 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "190 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "191 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "192 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "193 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "194 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "195 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "196 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "197 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "198 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "199 Train Loss 497.0122 Test RE 0.22123713127686048 c 0.025644721 k 1.0176115 m -0.00021836309\n",
      "Training time: 30.95\n",
      "Training time: 30.95\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 1295.8608 Test RE 0.33683608259183445 c -0.010155349 k 1.2109678 m -0.00022940531\n",
      "1 Train Loss 497.0122 Test RE 0.22123712218190866 c -0.012904987 k 1.0176704 m -0.0003018759\n",
      "2 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "3 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "4 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "5 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "6 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "7 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "8 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "9 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "10 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "11 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "12 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "13 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "14 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "15 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "16 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "17 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "18 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "19 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "20 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "21 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "22 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "23 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "24 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "25 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "26 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "27 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "28 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "29 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "30 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "31 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "32 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "33 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "34 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "35 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "36 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "37 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "38 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "39 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "40 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "41 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "42 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "43 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "44 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "45 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "46 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "47 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "48 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "49 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "50 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "51 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "52 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "53 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "54 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "55 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "56 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "57 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "58 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "59 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "60 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "61 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "62 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "63 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "64 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "65 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "66 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "67 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "68 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "69 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "70 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "71 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "72 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "73 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "74 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "75 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "76 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "77 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "78 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "79 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "80 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "81 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "82 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "83 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "84 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "85 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "86 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "87 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "88 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "89 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "90 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "91 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "92 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "93 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "94 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "95 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "96 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "97 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "98 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "99 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "100 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "101 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "102 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "103 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "104 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "105 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "106 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "107 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "108 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "109 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "110 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "111 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "112 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "113 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "114 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "115 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "116 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "117 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "118 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "119 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "120 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "121 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "122 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "123 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "124 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "125 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "126 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "127 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "128 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "129 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "130 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "131 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "132 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "133 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "134 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "135 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "136 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "137 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "138 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "139 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "140 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "141 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "142 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "143 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "144 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "145 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "146 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "147 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "148 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "149 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "150 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "151 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "152 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "153 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "154 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "155 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "156 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "157 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "158 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "159 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "160 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "161 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "162 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "163 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "164 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "165 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "166 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "167 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "168 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "169 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "170 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "171 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "172 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "173 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "174 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "175 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "176 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "177 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "178 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "179 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "180 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "181 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "182 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "183 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "184 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "185 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "186 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "187 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "188 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "189 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "190 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "191 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "192 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "193 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "194 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "195 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "196 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "197 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "198 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "199 Train Loss 497.012 Test RE 0.2212370935730342 c -0.012905898 k 1.0176077 m -0.0003018999\n",
      "Training time: 26.76\n",
      "Training time: 26.76\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 514.39905 Test RE 0.22489015274606486 c 0.060322426 k 1.0712496 m -0.00036576772\n",
      "1 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "2 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "3 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "4 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "5 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "6 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "7 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "8 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "9 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "10 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "11 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "12 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "13 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "14 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "15 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "16 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "17 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "18 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "19 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "20 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "21 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "22 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "23 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "24 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "25 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "26 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "27 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "28 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "29 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "30 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "31 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "32 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "33 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "34 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "35 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "36 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "37 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "38 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "39 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "40 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "41 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "42 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "43 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "44 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "45 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "46 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "47 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "48 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "49 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "50 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "51 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "52 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "53 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "54 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "55 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "56 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "57 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "58 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "59 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "60 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "61 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "62 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "63 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "64 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "65 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "66 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "67 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "68 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "69 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "70 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "71 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "72 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "73 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "74 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "75 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "76 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "77 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "78 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "79 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "80 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "81 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "82 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "83 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "84 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "85 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "86 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "87 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "88 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "89 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "90 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "91 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "92 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "93 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "94 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "95 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "96 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "97 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "98 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "99 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "100 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "101 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "102 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "103 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "104 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "105 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "106 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "107 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "108 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "109 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "110 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "111 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "112 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "113 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "114 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "115 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "116 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "117 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "118 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "119 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "120 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "121 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "122 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "123 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "124 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "125 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "126 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "127 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "128 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "129 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "130 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "131 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "132 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "133 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "134 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "135 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "136 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "137 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "138 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "139 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "140 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "141 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "142 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "143 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "144 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "145 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "146 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "147 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "148 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "149 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "150 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "151 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "152 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "153 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "154 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "155 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "156 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "157 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "158 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "159 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "160 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "161 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "162 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "163 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "164 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "165 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "166 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "167 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "168 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "169 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "170 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "171 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "172 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "173 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "174 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "175 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "176 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "177 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "178 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "179 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "180 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "181 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "182 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "183 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "184 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "185 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "186 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "187 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "188 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "189 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "190 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "191 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "192 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "193 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "194 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "195 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "196 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "197 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "198 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "199 Train Loss 497.01154 Test RE 0.22123698249355203 c 0.06235 k 1.0176477 m -0.0003796023\n",
      "Training time: 27.99\n",
      "Training time: 27.99\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 998.2946 Test RE 0.3135102836742076 c -0.35662344 k 1.3112907 m -0.0005302951\n",
      "1 Train Loss 497.01474 Test RE 0.22123768435063648 c -0.44519076 k 1.0177485 m -0.000673127\n",
      "2 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "3 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "4 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "5 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "6 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "7 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "8 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "9 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "10 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "11 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "12 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "13 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "14 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "15 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "16 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "17 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "18 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "19 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "20 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "21 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "22 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "23 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "24 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "25 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "26 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "27 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "28 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "29 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "30 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "31 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "32 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "33 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "34 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "35 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "36 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "37 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "38 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "39 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "40 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "41 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "42 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "43 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "44 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "45 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "46 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "47 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "48 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "49 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "50 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "51 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "52 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "53 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "54 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "55 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "56 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "57 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "58 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "59 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "60 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "61 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "62 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "63 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "64 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "65 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "66 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "67 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "68 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "69 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "70 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "71 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "72 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "73 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "74 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "75 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "76 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "77 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "78 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "79 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "80 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "81 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "82 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "83 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "84 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "85 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "86 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "87 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "88 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "89 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "90 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "91 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "92 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "93 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "94 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "95 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "96 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "97 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "98 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "99 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "100 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "101 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "102 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "103 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "104 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "105 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "106 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "107 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "108 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "109 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "110 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "111 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "112 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "113 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "114 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "115 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "116 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "117 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "118 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "119 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "120 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "121 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "122 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "123 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "124 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "125 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "126 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "127 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "128 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "129 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "130 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "131 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "132 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "133 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "134 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "135 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "136 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "137 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "138 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "139 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "140 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "141 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "142 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "143 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "144 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "145 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "146 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "147 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "148 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "149 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "150 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "151 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "152 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "153 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "154 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "155 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "156 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "157 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "158 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "159 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "160 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "161 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "162 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "163 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "164 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "165 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "166 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "167 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "168 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "169 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "170 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "171 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "172 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "173 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "174 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "175 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "176 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "177 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "178 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "179 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "180 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "181 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "182 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "183 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "184 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "185 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "186 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "187 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "188 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "189 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "190 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "191 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "192 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "193 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "194 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "195 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "196 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "197 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "198 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "199 Train Loss 497.01413 Test RE 0.22123757244734404 c -0.44527337 k 1.0175939 m -0.0006732544\n",
      "Training time: 22.81\n",
      "Training time: 22.81\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 647.86304 Test RE 0.2476540174021378 c 0.34509695 k 1.0911627 m -0.0006079936\n",
      "1 Train Loss 497.00952 Test RE 0.22123652739071686 c 0.3828611 k 1.0176606 m -0.0006759576\n",
      "2 Train Loss 497.00934 Test RE 0.2212364930804287 c 0.38290492 k 1.0175897 m -0.00067603553\n",
      "3 Train Loss 497.00934 Test RE 0.22123649536949958 c 0.38290712 k 1.0175855 m -0.00067603955\n",
      "4 Train Loss 497.00934 Test RE 0.22123649172317586 c 0.3829094 k 1.0175811 m -0.0006760436\n",
      "5 Train Loss 497.00934 Test RE 0.22123648632220191 c 0.38291204 k 1.0175761 m -0.00067604845\n",
      "6 Train Loss 497.00934 Test RE 0.2212364856400673 c 0.38291574 k 1.0175694 m -0.00067605515\n",
      "7 Train Loss 497.00934 Test RE 0.22123648300383994 c 0.3829209 k 1.01756 m -0.0006760645\n",
      "8 Train Loss 497.00934 Test RE 0.22123647590840126 c 0.38292846 k 1.0175472 m -0.00067607826\n",
      "9 Train Loss 496.96497 Test RE 0.22122302445305034 c 0.38388056 k 1.0165313 m -0.0006778086\n",
      "10 Train Loss 496.87476 Test RE 0.22120642604808813 c 0.38581526 k 1.0175219 m -0.0006809895\n",
      "11 Train Loss 496.02994 Test RE 0.2210116351488675 c 0.38839892 k 1.018334 m -0.00068491325\n",
      "12 Train Loss 483.73843 Test RE 0.218069640506949 c 0.38833177 k 1.0130689 m -0.0006120728\n",
      "13 Train Loss 438.7321 Test RE 0.2067360644727883 c 0.3931413 k 1.022496 m -0.0005142599\n",
      "14 Train Loss 399.48206 Test RE 0.19537334038911527 c 0.39693257 k 1.0119934 m 0.004636099\n",
      "15 Train Loss 374.7987 Test RE 0.18621323015403898 c 0.3814559 k 1.0147899 m 0.059974205\n",
      "16 Train Loss 353.09662 Test RE 0.17784455242637698 c 0.3790855 k 1.0053531 m 0.11192692\n",
      "17 Train Loss 350.08124 Test RE 0.1761904843900023 c 0.37734437 k 1.006406 m 0.12145934\n",
      "18 Train Loss 347.25305 Test RE 0.17487483375995333 c 0.37626046 k 1.0177469 m 0.14121896\n",
      "19 Train Loss 338.0137 Test RE 0.17000797417821742 c 0.38410544 k 0.995816 m 0.20260245\n",
      "20 Train Loss 333.97806 Test RE 0.16754138661309287 c 0.3813553 k 1.0062996 m 0.22618613\n",
      "21 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "22 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "23 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "24 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "25 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "26 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "27 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "28 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "29 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "30 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "31 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "32 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "33 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "34 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "35 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "36 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "37 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "38 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "39 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "40 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "41 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "42 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "43 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "44 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "45 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "46 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "47 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "48 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "49 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "50 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "51 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "52 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "53 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "54 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "55 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "56 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "57 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "58 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "59 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "60 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "61 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "62 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "63 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "64 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "65 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "66 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "67 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "68 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "69 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "70 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "71 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "72 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "73 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "74 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "75 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "76 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "77 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "78 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "79 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "80 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "81 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "82 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "83 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "84 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "85 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "86 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "87 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "88 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "89 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "90 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "91 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "92 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "93 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "94 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "95 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "96 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "97 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "98 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "99 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "100 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "101 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "102 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "103 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "104 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "105 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "106 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "107 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "108 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "109 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "110 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "111 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "112 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "113 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "114 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "115 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "116 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "117 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "118 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "119 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "120 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "121 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "122 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "123 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "124 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "125 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "126 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "127 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "128 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "129 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "130 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "131 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "132 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "133 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "134 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "135 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "136 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "137 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "138 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "139 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "140 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "141 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "142 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "143 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "144 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "145 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "146 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "147 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "148 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "149 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "150 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "151 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "152 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "153 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "154 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "155 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "156 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "157 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "158 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "159 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "160 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "161 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "162 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "163 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "164 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "165 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "166 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "167 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "168 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "169 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "170 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "171 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "172 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "173 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "174 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "175 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "176 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "177 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "178 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "179 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "180 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "181 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "182 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "183 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "184 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "185 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "186 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "187 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "188 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "189 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "190 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "191 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "192 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "193 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "194 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "195 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "196 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "197 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "198 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "199 Train Loss 332.28375 Test RE 0.16641610599633813 c 0.3842628 k 1.0012921 m 0.25107518\n",
      "Training time: 22.44\n",
      "Training time: 22.44\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 1292.473 Test RE 0.3555816291992008 c 0.38706148 k 1.3823925 m -0.0004319619\n",
      "1 Train Loss 497.002 Test RE 0.2212348356101933 c 0.51284564 k 1.0178344 m -0.0005867666\n",
      "2 Train Loss 497.00076 Test RE 0.22123458203701513 c 0.51299626 k 1.017583 m -0.00058694294\n",
      "3 Train Loss 496.9332 Test RE 0.22121837110928713 c 0.5150422 k 1.0168306 m -0.00058938976\n",
      "4 Train Loss 496.88602 Test RE 0.2212085998560241 c 0.5205911 k 1.0175161 m -0.00059565814\n",
      "5 Train Loss 495.82434 Test RE 0.22088627130050906 c 0.51656526 k 1.0118918 m -0.0005899947\n",
      "6 Train Loss 493.17038 Test RE 0.22014675607011772 c 0.51802707 k 1.0187378 m -0.0005632305\n",
      "7 Train Loss 476.1397 Test RE 0.2157844247489218 c 0.49694696 k 1.035745 m 0.0018540662\n",
      "8 Train Loss 415.95117 Test RE 0.20065191429274087 c 0.5006053 k 1.0084114 m 0.005396938\n",
      "9 Train Loss 401.17944 Test RE 0.19591174040923293 c 0.52426094 k 1.0126626 m 0.009513642\n",
      "10 Train Loss 388.00714 Test RE 0.19194585217005825 c 0.60194784 k 1.0126897 m 0.01944162\n",
      "11 Train Loss 366.17545 Test RE 0.18188181207501244 c 0.6596719 k 0.9966663 m 0.0563258\n",
      "12 Train Loss 357.66998 Test RE 0.17968341520221393 c 0.6655417 k 1.008385 m 0.06770748\n",
      "13 Train Loss 346.51346 Test RE 0.1728116837835612 c 0.7010825 k 1.0066732 m 0.11177627\n",
      "14 Train Loss 338.91837 Test RE 0.16761579105969418 c 0.74588436 k 0.9990019 m 0.17300324\n",
      "15 Train Loss 335.82172 Test RE 0.16436991353467253 c 0.7640853 k 0.9994826 m 0.1991258\n",
      "16 Train Loss 322.0401 Test RE 0.16015258626319162 c 0.7781884 k 0.9958178 m 0.23979765\n",
      "17 Train Loss 320.70624 Test RE 0.16055044257385065 c 0.7764752 k 0.9973313 m 0.24621285\n",
      "18 Train Loss 319.58145 Test RE 0.16013677913204125 c 0.77659774 k 0.9966699 m 0.2501582\n",
      "19 Train Loss 314.20966 Test RE 0.15882894178454096 c 0.7815651 k 0.9955533 m 0.2524549\n",
      "20 Train Loss 311.7542 Test RE 0.15864057101858492 c 0.7840515 k 0.99760497 m 0.252069\n",
      "21 Train Loss 309.986 Test RE 0.15786452066011333 c 0.7897932 k 0.9982823 m 0.25292704\n",
      "22 Train Loss 308.18298 Test RE 0.15794846627791767 c 0.79671246 k 0.9971521 m 0.25502217\n",
      "23 Train Loss 306.5245 Test RE 0.15744538202485578 c 0.810574 k 0.9945223 m 0.26266456\n",
      "24 Train Loss 305.698 Test RE 0.15651805692058451 c 0.82023627 k 0.9978472 m 0.26788688\n",
      "25 Train Loss 305.11658 Test RE 0.15514305447087173 c 0.8303673 k 0.9942386 m 0.27459234\n",
      "26 Train Loss 304.02444 Test RE 0.15526962539377948 c 0.83825994 k 0.99401027 m 0.28394178\n",
      "27 Train Loss 302.57867 Test RE 0.15530467364664433 c 0.8530331 k 0.997349 m 0.30480516\n",
      "28 Train Loss 301.23105 Test RE 0.15568090272512122 c 0.87759995 k 0.99375504 m 0.3285667\n",
      "29 Train Loss 299.8477 Test RE 0.15582204291716817 c 0.9007864 k 0.9925466 m 0.35241613\n",
      "30 Train Loss 298.0157 Test RE 0.15389386496807048 c 0.91182745 k 0.99546385 m 0.37154204\n",
      "31 Train Loss 297.93555 Test RE 0.1539310302199546 c 0.911389 k 0.99310344 m 0.37295434\n",
      "32 Train Loss 296.59424 Test RE 0.1535181058397583 c 0.9266227 k 0.99434 m 0.40273693\n",
      "33 Train Loss 293.95535 Test RE 0.15374525883289777 c 0.96303713 k 0.9910231 m 0.45117396\n",
      "34 Train Loss 290.1369 Test RE 0.15263371595971537 c 1.0155852 k 0.98817945 m 0.5269454\n",
      "35 Train Loss 285.06726 Test RE 0.15114019567484072 c 1.0908396 k 0.9910036 m 0.6367839\n",
      "36 Train Loss 279.1368 Test RE 0.1514894435906797 c 1.1644388 k 0.991916 m 0.7449805\n",
      "37 Train Loss 274.01202 Test RE 0.1522198479343321 c 1.2237135 k 0.9923563 m 0.8221858\n",
      "38 Train Loss 264.67746 Test RE 0.15191875860291867 c 1.3239647 k 0.9905624 m 0.9400911\n",
      "39 Train Loss 262.51657 Test RE 0.15078364009657627 c 1.3570125 k 0.9890392 m 0.98194426\n",
      "40 Train Loss 260.2504 Test RE 0.15031942855402972 c 1.4558281 k 0.99980104 m 1.1014434\n",
      "41 Train Loss 260.2503 Test RE 0.15031960290722668 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "42 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "43 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "44 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "45 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "46 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "47 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "48 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "49 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "50 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "51 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "52 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "53 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "54 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "55 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "56 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "57 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "58 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "59 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "60 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "61 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "62 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "63 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "64 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "65 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "66 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "67 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "68 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "69 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "70 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "71 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "72 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "73 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "74 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "75 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "76 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "77 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "78 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "79 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "80 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "81 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "82 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "83 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "84 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "85 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "86 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "87 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "88 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "89 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "90 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "91 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "92 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "93 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "94 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "95 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "96 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "97 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "98 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "99 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "100 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "101 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "102 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "103 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "104 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "105 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "106 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "107 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "108 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "109 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "110 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "111 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "112 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "113 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "114 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "115 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "116 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "117 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "118 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "119 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "120 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "121 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "122 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "123 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "124 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "125 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "126 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "127 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "128 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "129 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "130 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "131 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "132 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "133 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "134 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "135 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "136 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "137 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "138 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "139 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "140 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "141 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "142 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "143 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "144 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "145 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "146 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "147 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "148 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "149 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "150 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "151 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "152 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "153 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "154 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "155 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "156 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "157 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "158 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "159 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "160 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "161 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "162 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "163 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "164 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "165 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "166 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "167 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "168 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "169 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "170 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "171 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "172 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "173 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "174 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "175 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "176 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "177 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "178 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "179 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "180 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "181 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "182 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "183 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "184 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "185 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "186 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "187 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "188 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "189 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "190 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "191 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "192 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "193 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "194 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "195 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "196 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "197 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "198 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "199 Train Loss 260.25003 Test RE 0.15031956200043997 c 1.4558282 k 0.99980104 m 1.1014435\n",
      "Training time: 30.10\n",
      "Training time: 30.10\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 683.51056 Test RE 0.25373113002653686 c 0.1368511 k 1.1024954 m -0.00020763662\n",
      "1 Train Loss 497.0127 Test RE 0.2212372394363811 c 0.15366605 k 1.0176172 m -0.00023404366\n",
      "2 Train Loss 497.0126 Test RE 0.22123721535457264 c 0.15367535 k 1.0175797 m -0.00023405797\n",
      "3 Train Loss 496.97156 Test RE 0.22122786157336344 c 0.15404883 k 1.0169146 m -0.00023449247\n",
      "4 Train Loss 496.96622 Test RE 0.2212268769683855 c 0.15388888 k 1.0178045 m -0.00023424394\n",
      "5 Train Loss 496.75668 Test RE 0.2211481742681172 c 0.15282118 k 1.0146959 m -0.0002323868\n",
      "6 Train Loss 487.58813 Test RE 0.21846975366953425 c 0.15007414 k 1.0122358 m -0.00022298607\n",
      "7 Train Loss 344.56277 Test RE 0.16983576168813955 c 0.13023725 k 1.0034007 m 0.00028424032\n",
      "8 Train Loss 339.3999 Test RE 0.16381729746918972 c 0.12734672 k 1.0020899 m 0.000740769\n",
      "9 Train Loss 334.93506 Test RE 0.16104809475973259 c 0.14766337 k 1.0015211 m 0.050193947\n",
      "10 Train Loss 330.62164 Test RE 0.15991378402499587 c 0.17098893 k 1.0011986 m 0.107056536\n",
      "11 Train Loss 328.39212 Test RE 0.16096699867873904 c 0.18645625 k 1.0016798 m 0.14075011\n",
      "12 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "13 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "14 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "15 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "16 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "17 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "18 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "19 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "20 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "21 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "22 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "23 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "24 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "25 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "26 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "27 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "28 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "29 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "30 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "31 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "32 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "33 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "34 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "35 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "36 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "37 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "38 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "39 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "40 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "41 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "42 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "43 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "44 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "45 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "46 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "47 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "48 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "49 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "50 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "51 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "52 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "53 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "54 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "55 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "56 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "57 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "58 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "59 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "60 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "61 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "62 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "63 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "64 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "65 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "66 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "67 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "68 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "69 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "70 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "71 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "72 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "73 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "74 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "75 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "76 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "77 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "78 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "79 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "80 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "81 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "82 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "83 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "84 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "85 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "86 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "87 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "88 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "89 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "90 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "91 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "92 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "93 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "94 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "95 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "96 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "97 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "98 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "99 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "100 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "101 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "102 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "103 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "104 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "105 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "106 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "107 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "108 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "109 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "110 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "111 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "112 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "113 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "114 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "115 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "116 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "117 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "118 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "119 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "120 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "121 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "122 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "123 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "124 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "125 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "126 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "127 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "128 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "129 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "130 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "131 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "132 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "133 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "134 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "135 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "136 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "137 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "138 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "139 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "140 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "141 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "142 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "143 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "144 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "145 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "146 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "147 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "148 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "149 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "150 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "151 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "152 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "153 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "154 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "155 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "156 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "157 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "158 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "159 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "160 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "161 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "162 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "163 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "164 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "165 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "166 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "167 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "168 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "169 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "170 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "171 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "172 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "173 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "174 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "175 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "176 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "177 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "178 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "179 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "180 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "181 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "182 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "183 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "184 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "185 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "186 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "187 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "188 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "189 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "190 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "191 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "192 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "193 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "194 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "195 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "196 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "197 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "198 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "199 Train Loss 328.38876 Test RE 0.16099380284899273 c 0.1865574 k 1.0016811 m 0.14100836\n",
      "Training time: 21.66\n",
      "Training time: 21.66\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 1209.4587 Test RE 0.33809831613531655 c 0.17789464 k 1.282884 m -0.00035382397\n",
      "1 Train Loss 497.02664 Test RE 0.22124030704946024 c 0.22722296 k 1.0179679 m -0.00045384892\n",
      "2 Train Loss 497.01123 Test RE 0.22123690398960855 c 0.22725055 k 1.0168949 m -0.00045387863\n",
      "3 Train Loss 496.99133 Test RE 0.2212324660545154 c 0.22684047 k 1.0176928 m -0.00045297973\n",
      "4 Train Loss 496.8577 Test RE 0.22119308153527875 c 0.22872344 k 1.0139849 m -0.0004561548\n",
      "5 Train Loss 496.53867 Test RE 0.22110862848364554 c 0.23006308 k 1.020439 m -0.00045824476\n",
      "6 Train Loss 493.84473 Test RE 0.22038512339314104 c 0.24092604 k 1.0160234 m -0.00045908746\n",
      "7 Train Loss 437.61356 Test RE 0.20602990230988838 c 0.26018062 k 1.0086473 m 0.00043513707\n",
      "8 Train Loss 353.307 Test RE 0.17230318281823687 c 0.32031155 k 1.0083201 m 0.041641146\n",
      "9 Train Loss 326.65485 Test RE 0.16314887130080719 c 0.38635215 k 0.9995951 m 0.10121383\n",
      "10 Train Loss 313.6941 Test RE 0.15774866718894887 c 0.46481678 k 1.0028687 m 0.18855014\n",
      "11 Train Loss 306.84912 Test RE 0.15819943935364422 c 0.5744349 k 1.0030966 m 0.28874227\n",
      "12 Train Loss 283.56262 Test RE 0.15337565348810428 c 0.9199006 k 1.0017174 m 0.52816397\n",
      "13 Train Loss 267.63623 Test RE 0.1489442035888534 c 1.0809139 k 0.9907856 m 0.6515379\n",
      "14 Train Loss 257.8436 Test RE 0.1468567101548411 c 1.2433639 k 0.97807014 m 0.7800755\n",
      "15 Train Loss 243.83804 Test RE 0.1439116084677808 c 1.4711133 k 0.99088687 m 0.97928923\n",
      "16 Train Loss 234.24272 Test RE 0.14101655848476466 c 1.6881369 k 0.9857749 m 1.19465\n",
      "17 Train Loss 229.55551 Test RE 0.13881421851519 c 1.8283651 k 0.97917056 m 1.3464034\n",
      "18 Train Loss 226.79587 Test RE 0.1394747054045736 c 1.8399276 k 0.98500556 m 1.3579675\n",
      "19 Train Loss 223.24065 Test RE 0.1376611260532206 c 1.9896194 k 0.98081785 m 1.4766672\n",
      "20 Train Loss 217.67737 Test RE 0.13684697986249694 c 2.06303 k 0.9750444 m 1.5460762\n",
      "21 Train Loss 214.53831 Test RE 0.13542214370306704 c 2.1680214 k 0.98041 m 1.6368859\n",
      "22 Train Loss 214.52528 Test RE 0.13540540051465264 c 2.1700048 k 0.97950125 m 1.6387565\n",
      "23 Train Loss 213.7571 Test RE 0.13578056569037245 c 2.1718276 k 0.9800072 m 1.6435143\n",
      "24 Train Loss 213.12653 Test RE 0.13553787776099577 c 2.2001793 k 0.97873783 m 1.6720958\n",
      "25 Train Loss 212.10988 Test RE 0.1354128895056638 c 2.2481303 k 0.98030466 m 1.7224506\n",
      "26 Train Loss 211.20407 Test RE 0.13611987185778457 c 2.2280831 k 0.9808996 m 1.7153026\n",
      "27 Train Loss 210.04646 Test RE 0.13665601435159463 c 2.244121 k 0.9815567 m 1.7442408\n",
      "28 Train Loss 209.8109 Test RE 0.13642796529357243 c 2.282339 k 0.9814161 m 1.782266\n",
      "29 Train Loss 209.71376 Test RE 0.1362140783999365 c 2.3011553 k 0.9811098 m 1.7973838\n",
      "30 Train Loss 209.11893 Test RE 0.13628687243391022 c 2.2891486 k 0.980901 m 1.8065442\n",
      "31 Train Loss 208.57217 Test RE 0.1366807863835611 c 2.2928061 k 0.98146796 m 1.8370314\n",
      "32 Train Loss 208.31435 Test RE 0.1368223118132182 c 2.3269956 k 0.98124003 m 1.8819677\n",
      "33 Train Loss 208.28473 Test RE 0.13672714418730283 c 2.351996 k 0.98072547 m 1.9078826\n",
      "34 Train Loss 208.12201 Test RE 0.13672002297469266 c 2.3487132 k 0.98082894 m 1.9131376\n",
      "35 Train Loss 207.79263 Test RE 0.13681835561361824 c 2.3388917 k 0.98128384 m 1.9241976\n",
      "36 Train Loss 207.67488 Test RE 0.13658993830264088 c 2.3547766 k 0.98047704 m 1.9458907\n",
      "37 Train Loss 207.01326 Test RE 0.13678500466287594 c 2.285053 k 0.98069936 m 1.9045514\n",
      "38 Train Loss 206.04361 Test RE 0.13600643511048002 c 2.2635286 k 0.9807412 m 1.9248402\n",
      "39 Train Loss 205.47015 Test RE 0.1351490205901348 c 2.2764952 k 0.97976106 m 1.9629564\n",
      "40 Train Loss 205.22134 Test RE 0.13489195432098225 c 2.2257788 k 0.9801879 m 1.9281634\n",
      "41 Train Loss 205.06653 Test RE 0.13520601664163057 c 2.1779437 k 0.98046887 m 1.8959371\n",
      "42 Train Loss 204.22816 Test RE 0.13453370177092328 c 2.164571 k 0.98055136 m 1.9809126\n",
      "43 Train Loss 203.52766 Test RE 0.13414734831192165 c 2.2080076 k 0.97872144 m 2.0756476\n",
      "44 Train Loss 202.12608 Test RE 0.13401307393312412 c 2.234338 k 0.9788787 m 2.1881497\n",
      "45 Train Loss 200.7575 Test RE 0.13417299806906538 c 2.2269752 k 0.9789502 m 2.2389529\n",
      "46 Train Loss 200.592 Test RE 0.13435694626928096 c 2.1917949 k 0.97886467 m 2.2033322\n",
      "47 Train Loss 200.41418 Test RE 0.13443886729012858 c 2.1987998 k 0.9791051 m 2.2210884\n",
      "48 Train Loss 200.23769 Test RE 0.1344054461966402 c 2.203801 k 0.97950405 m 2.2539785\n",
      "49 Train Loss 200.14665 Test RE 0.13436999756363063 c 2.2138822 k 0.9796299 m 2.2858489\n",
      "50 Train Loss 200.00365 Test RE 0.13398849327076362 c 2.2301567 k 0.9780388 m 2.3205616\n",
      "51 Train Loss 199.92862 Test RE 0.13409493310489665 c 2.2142134 k 0.9789184 m 2.2857406\n",
      "52 Train Loss 199.76387 Test RE 0.13408557895043427 c 2.180901 k 0.979241 m 2.284854\n",
      "53 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "54 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "55 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "56 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "57 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "58 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "59 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "60 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "61 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "62 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "63 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "64 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "65 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "66 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "67 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "68 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "69 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "70 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "71 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "72 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "73 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "74 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "75 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "76 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "77 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "78 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "79 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "80 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "81 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "82 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "83 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "84 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "85 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "86 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "87 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "88 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "89 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "90 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "91 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "92 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "93 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "94 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "95 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "96 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "97 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "98 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "99 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "100 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "101 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "102 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "103 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "104 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "105 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "106 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "107 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "108 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "109 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "110 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "111 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "112 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "113 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "114 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "115 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "116 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "117 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "118 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "119 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "120 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "121 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "122 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "123 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "124 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "125 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "126 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "127 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "128 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "129 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "130 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "131 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "132 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "133 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "134 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "135 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "136 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "137 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "138 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "139 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "140 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "141 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "142 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "143 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "144 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "145 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "146 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "147 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "148 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "149 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "150 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "151 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "152 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "153 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "154 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "155 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "156 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "157 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "158 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "159 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "160 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "161 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "162 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "163 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "164 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "165 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "166 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "167 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "168 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "169 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "170 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "171 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "172 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "173 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "174 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "175 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "176 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "177 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "178 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "179 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "180 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "181 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "182 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "183 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "184 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "185 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "186 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "187 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "188 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "189 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "190 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "191 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "192 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "193 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "194 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "195 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "196 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "197 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "198 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "199 Train Loss 199.75676 Test RE 0.13413772998252615 c 2.1790361 k 0.97949207 m 2.2783809\n",
      "Training time: 26.41\n",
      "Training time: 26.41\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 200\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "m_full = []\n",
    "k_full = []\n",
    "c_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "\n",
    "for reps in range(max_reps):   \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []   \n",
    "    beta_val = []\n",
    "    m_val = []\n",
    "    k_val = []\n",
    "    c_val = []\n",
    "\n",
    "    torch.manual_seed(reps*36)\n",
    "    N_f = 10000 #Total number of collocation points\n",
    "\n",
    "    layers = np.array([1,50,50,50,1]) #9 hidden layers\n",
    "    PINN = Sequentialmodel(layers)\n",
    "\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.5, \n",
    "                                max_iter = 10, \n",
    "                                max_eval = 15, \n",
    "                                tolerance_grad = 1e-6, \n",
    "                                tolerance_change = 1e-6, \n",
    "                                history_size = 100, \n",
    "                                line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "    train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    beta_full.append(beta_val)\n",
    "    m_full.append(m_val)\n",
    "    k_full.append(k_val)\n",
    "    c_full.append(c_val)\n",
    "    \n",
    "\n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"beta\": beta_full,\"m\": m_full,\"k\": k_full,\"c\": c_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "pmHEeBpzfFQh",
    "outputId": "990b2054-f35e-4c8e-c378-69ff2eb19f4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f4fb06b1f90>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_pred = PINN.test()\n",
    "plt.plot(t,x_true,'b')\n",
    "plt.plot(t,x_pred,'r--')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "stan_1D_FODE_tune.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
