{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8byrnUmNKGR",
    "outputId": "510f67c6-b225-41ba-e78a-d0a6e54ded85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uoNYpDzlNKGV"
   },
   "outputs": [],
   "source": [
    "tstart= 0 \n",
    "tstop= 60 \n",
    "\n",
    "increment = 0.1 \n",
    "# Initial condition\n",
    "x_init= [0,0] \n",
    "\n",
    "t = np.arange(tstart,tstop+1,increment)\n",
    "\n",
    "c = 1 # Damping constant \n",
    "k = 1 # Stiffness of the spring\n",
    "m = 5 # Mass \n",
    "F0 = 100\n",
    "\n",
    "def mydiff(x, t):    \n",
    "    F =F0\n",
    "    \n",
    "    dx1dt = x[1] \n",
    "    dx2dt = (F -c*x[1] -k*x[0])/m\n",
    "    dxdt= [dx1dt, dx2dt] \n",
    "    \n",
    "    return dxdt \n",
    "\n",
    "x_full_sol = odeint(mydiff, x_init, t) \n",
    "\n",
    "x_sol = x_full_sol[:,0]\n",
    "v_sol = x_full_sol[:,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BR02v-fkNKGV"
   },
   "outputs": [],
   "source": [
    "level = \"low\"\n",
    "label = \"1D_SMD_tanh_\" + level\n",
    "\n",
    "loss_thresh = 0.005\n",
    "\n",
    "t_test = t.reshape(-1,1)\n",
    "t_test_tensor = torch.from_numpy(t_test).float().to(device)\n",
    "\n",
    "# bc1_t = t[0].reshape(-1,1)\n",
    "# bc1_x = x_sol[0].reshape(-1,1)\n",
    "# t_bc1_train = torch.from_numpy(bc1_t).float().to(device)\n",
    "# x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "\n",
    "bc1_t = t.reshape(-1,1)\n",
    "bc1_x = x_sol.reshape(-1,1)\n",
    "t_bc1_train = torch.from_numpy(bc1_t).float().to(device)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "\n",
    "\n",
    "bc2_t = t[0].reshape(-1,1)\n",
    "t_bc2_train = torch.from_numpy(bc2_t).float().to(device)\n",
    "bc2_val = v_sol[0].reshape(-1,1)\n",
    "bc2_val =torch.from_numpy(bc2_val).float().to(device)\n",
    "\n",
    "t_test = t.reshape(-1,1)\n",
    "t_test_tensor = torch.from_numpy(t_test).float().to(device)\n",
    "x_true = x_sol\n",
    "x_true_norm = np.linalg.norm(x_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(t[0]) \n",
    "ub = np.array(t[-1]) \n",
    "\n",
    "#torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SyyktBKBXRo1"
   },
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    \n",
    "    t01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=t01,random_state =seed)\n",
    "\n",
    "    t_coll = lb + (ub-lb)*sampling(N_f)\n",
    "    # t_coll = np.vstack((t_coll,)) # append training points to collocation points \n",
    "\n",
    "    return t_coll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "o1b21zLnNKGW"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "        \n",
    "        \n",
    "        self.k = Parameter(torch.tensor(0.0))\n",
    "        self.k.requiresGrad = True\n",
    "        self.c = Parameter(torch.tensor(0.0))\n",
    "        self.c.requiresGrad = True\n",
    "        self.m = Parameter(torch.tensor(0.0))\n",
    "        self.m.requiresGrad = True\n",
    "        \n",
    "    'forward pass'\n",
    "    def forward(self,t):\n",
    "        if torch.is_tensor(t) != True:         \n",
    "            t = torch.from_numpy(t)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        t = 2.0*(t - l_b)/(u_b - l_b) - 1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = t.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,t,x):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(t), x)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,t_bc2,bc2_val):\n",
    "        g = t_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        x = self.forward(g)    \n",
    "            \n",
    "        x_t = autograd.grad(x,g,torch.ones([t_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dx_dt = x_t[:,[0]]\n",
    "        \n",
    "        bc2 = dx_dt\n",
    "        \n",
    "        loss_bc2= self.loss_function(bc2,bc2_val)\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self,t_coll,f_hat):\n",
    "             \n",
    "        g = t_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        x = self.forward(g) \n",
    "\n",
    "        x_t = autograd.grad(x,g,torch.ones([t_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        x_tt = autograd.grad(x_t,g,torch.ones(t_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dx_dt = x_t[:,[0]]\n",
    "        \n",
    "        dx2_d2t = x_tt[:,[0]]\n",
    "        \n",
    "        f = self.m*dx2_d2t + self.c*dx_dt + self.k*x - F0\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,t_bc1,x_bc1,t_bc2,bc2_val,t_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(t_bc1,x_bc1)\n",
    "        loss_bc2 = self.loss_BC2(t_bc2,bc2_val)\n",
    "        loss_f = self.loss_PDE(t_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self):\n",
    "        x_pred = self.forward(t_test_tensor)\n",
    "        x_pred = x_pred.cpu().detach().numpy()\n",
    "\n",
    "        return x_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        x_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(x_pred.reshape(-1,1) - x_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(x_pred.reshape(-1,1) - x_true.reshape(-1,1),2)/x_true_norm\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fLY2mT5BOgjD"
   },
   "outputs": [],
   "source": [
    "def train_step(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8srA5uGuObil"
   },
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    m_val.append(PINN.m.cpu().detach().numpy())\n",
    "    k_val.append(PINN.k.cpu().detach().numpy())\n",
    "    c_val.append(PINN.c.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0SezTZ_racQB"
   },
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "\n",
    "    t_coll = colloc_pts(N_f,rep*11)\n",
    "    t_coll =  torch.from_numpy(t_coll).float().to(device)\n",
    "    \n",
    "    f_hat = torch.zeros(t_coll.shape[0],1).to(device)\n",
    "    for i in range(max_iter):      \n",
    "        train_step(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat)\n",
    "         \n",
    "        loss_np = PINN.loss(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test RE\",test_re_loss[-1],\"c\",PINN.c.cpu().detach().numpy(),\"k\",PINN.k.cpu().detach().numpy(),\"m\",PINN.m.cpu().detach().numpy())\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I9BYbcJ0NKGX",
    "outputId": "42720d97-b37a-4c42-b7e5-b77a0ef391f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 924.83276 Test RE 0.30147765070033833 c -0.19921543 k 1.2702584 m -0.00026414532\n",
      "1 Train Loss 498.37244 Test RE 0.221536372873564 c -0.23485205 k 1.0286171 m -0.00031475726\n",
      "2 Train Loss 496.98285 Test RE 0.22123057789444237 c -0.2367709 k 1.0181048 m -0.00031747587\n",
      "3 Train Loss 496.51785 Test RE 0.22107392646358665 c -0.2384539 k 1.0109837 m -0.0003196672\n",
      "4 Train Loss 495.95953 Test RE 0.22099194188223875 c -0.23954898 k 1.0202805 m -0.00032102197\n",
      "5 Train Loss 494.77393 Test RE 0.22068625144729287 c -0.24278326 k 1.0148457 m -0.0003202577\n",
      "6 Train Loss 465.46387 Test RE 0.21217027463727028 c -0.23818564 k 1.0434229 m -6.7072404e-05\n",
      "7 Train Loss 370.2082 Test RE 0.18033201333072793 c -0.20863757 k 1.0193597 m 0.020179326\n",
      "8 Train Loss 338.69128 Test RE 0.1647518399634131 c 0.11721027 k 0.9918153 m 0.046444952\n",
      "9 Train Loss 290.40295 Test RE 0.14113206727072158 c 1.4255023 k 0.99058074 m 0.0013797088\n",
      "10 Train Loss 257.46494 Test RE 0.14270770907998362 c 1.76203 k 0.9695494 m 0.2846212\n",
      "11 Train Loss 227.48438 Test RE 0.13768197581180397 c 2.0572155 k 0.97387236 m 0.87003714\n",
      "12 Train Loss 221.90079 Test RE 0.134648507325609 c 2.2395084 k 0.9812635 m 1.0941634\n",
      "13 Train Loss 215.52908 Test RE 0.13391420084901912 c 2.0138636 k 0.9672845 m 1.6413798\n",
      "14 Train Loss 214.20929 Test RE 0.1338878668850422 c 1.9277405 k 0.9808175 m 1.8003392\n",
      "15 Train Loss 212.57916 Test RE 0.13407307248109196 c 2.007138 k 0.99208665 m 1.9760059\n",
      "16 Train Loss 207.85997 Test RE 0.1342107768827498 c 2.2871304 k 0.9816907 m 2.7209415\n",
      "17 Train Loss 207.17513 Test RE 0.1345274667280741 c 2.3206015 k 0.9750609 m 2.8831453\n",
      "18 Train Loss 205.95389 Test RE 0.13480540829303067 c 2.4300752 k 0.9827449 m 2.9142158\n",
      "19 Train Loss 201.35272 Test RE 0.13325642847510225 c 2.3799105 k 0.9809323 m 2.8534307\n",
      "20 Train Loss 199.23299 Test RE 0.13239692131621586 c 2.1750045 k 0.9764471 m 2.4411933\n",
      "21 Train Loss 199.10867 Test RE 0.13232804986411012 c 2.1675742 k 0.9773023 m 2.3336632\n",
      "22 Train Loss 197.05104 Test RE 0.13210920077469343 c 2.2491176 k 0.9769193 m 2.3468692\n",
      "23 Train Loss 193.79028 Test RE 0.13166382947244562 c 2.2173805 k 0.9787775 m 2.7174125\n",
      "24 Train Loss 192.18036 Test RE 0.13166917079129592 c 2.275487 k 0.9804368 m 2.691326\n",
      "25 Train Loss 191.69955 Test RE 0.13151829594761416 c 2.2493978 k 0.97652596 m 2.6930358\n",
      "26 Train Loss 190.6172 Test RE 0.13106140339049047 c 2.1303923 k 0.97847843 m 2.4429178\n",
      "27 Train Loss 190.04996 Test RE 0.13071738913332082 c 2.1669023 k 0.9776882 m 2.239555\n",
      "28 Train Loss 186.981 Test RE 0.12946276768512469 c 2.1192904 k 0.9748587 m 2.273779\n",
      "29 Train Loss 184.67813 Test RE 0.12835155882560823 c 2.0037634 k 0.98143804 m 2.2991881\n",
      "30 Train Loss 181.69324 Test RE 0.12720465558810481 c 2.076695 k 0.9773247 m 2.44412\n",
      "31 Train Loss 178.83241 Test RE 0.12599419550034097 c 2.0097213 k 0.97567666 m 2.3706806\n",
      "32 Train Loss 176.99371 Test RE 0.1253264122637728 c 2.0540695 k 0.9793078 m 2.4487398\n",
      "33 Train Loss 175.50275 Test RE 0.12482325252265788 c 2.0373745 k 0.97854924 m 2.6111367\n",
      "34 Train Loss 174.20694 Test RE 0.1242037944027426 c 2.0436273 k 0.97754025 m 2.61433\n",
      "35 Train Loss 173.59724 Test RE 0.1238687984168784 c 2.0764322 k 0.97962224 m 2.8271403\n",
      "36 Train Loss 171.98131 Test RE 0.12317473655367651 c 2.0129595 k 0.9755289 m 3.1197848\n",
      "37 Train Loss 166.49016 Test RE 0.11988799162881235 c 1.8590041 k 0.9800735 m 2.7241902\n",
      "38 Train Loss 159.51486 Test RE 0.1153294086045348 c 2.033534 k 0.985665 m 3.1053224\n",
      "39 Train Loss 153.54967 Test RE 0.1107039320692878 c 1.8902675 k 0.96856546 m 2.749288\n",
      "40 Train Loss 146.67693 Test RE 0.10455733483030823 c 1.7519565 k 0.9813526 m 2.56692\n",
      "41 Train Loss 145.16959 Test RE 0.10236376287108331 c 1.7706071 k 0.97876364 m 2.6376507\n",
      "42 Train Loss 140.72491 Test RE 0.09797752591537517 c 1.6618675 k 0.9784021 m 2.5820866\n",
      "43 Train Loss 139.2778 Test RE 0.09752828279492891 c 1.6082515 k 0.9844232 m 2.5112398\n",
      "44 Train Loss 135.22435 Test RE 0.10081623381743589 c 1.6886437 k 0.9853826 m 2.7407176\n",
      "45 Train Loss 132.8956 Test RE 0.10105140534631227 c 1.824169 k 0.9780656 m 3.1803863\n",
      "46 Train Loss 126.163086 Test RE 0.09825293779634482 c 1.8820926 k 0.9750619 m 3.8120809\n",
      "47 Train Loss 117.33335 Test RE 0.09438924817172761 c 1.7785846 k 0.9756071 m 3.0342326\n",
      "48 Train Loss 115.282234 Test RE 0.09310489655886592 c 1.725368 k 0.9800335 m 3.0924485\n",
      "49 Train Loss 112.21449 Test RE 0.08959402898118077 c 1.8034908 k 0.97641194 m 3.3871512\n",
      "50 Train Loss 106.77487 Test RE 0.08480753834911195 c 1.7008437 k 0.984072 m 3.4362314\n",
      "51 Train Loss 105.916245 Test RE 0.0846921847616145 c 1.6071966 k 0.98226863 m 3.47104\n",
      "52 Train Loss 103.98797 Test RE 0.08278769958896823 c 1.5857096 k 0.9828824 m 3.6077542\n",
      "53 Train Loss 102.32001 Test RE 0.0803573954156393 c 1.5826726 k 0.9845261 m 3.5615232\n",
      "54 Train Loss 101.71104 Test RE 0.07929518527760775 c 1.5338535 k 0.9834573 m 3.514086\n",
      "55 Train Loss 101.46059 Test RE 0.07948338021217424 c 1.535522 k 0.9844909 m 3.4633708\n",
      "56 Train Loss 100.81966 Test RE 0.08143318931658851 c 1.572676 k 0.98584896 m 3.516502\n",
      "57 Train Loss 99.59683 Test RE 0.08191859036261896 c 1.5818871 k 0.9814167 m 3.5339649\n",
      "58 Train Loss 98.846565 Test RE 0.08106681762966043 c 1.5557925 k 0.98554194 m 3.5519497\n",
      "59 Train Loss 98.70214 Test RE 0.0804509868829326 c 1.5224555 k 0.9863323 m 3.5547972\n",
      "60 Train Loss 98.6035 Test RE 0.07965791452160378 c 1.5235837 k 0.98561627 m 3.558227\n",
      "61 Train Loss 98.47438 Test RE 0.07962335622619054 c 1.5235038 k 0.9857037 m 3.4934933\n",
      "62 Train Loss 98.24031 Test RE 0.07980318070144914 c 1.5249894 k 0.9858566 m 3.52824\n",
      "63 Train Loss 98.20337 Test RE 0.07941947298644463 c 1.5149908 k 0.98588717 m 3.5030868\n",
      "64 Train Loss 98.15166 Test RE 0.07887264547260067 c 1.5035399 k 0.98602813 m 3.4677913\n",
      "65 Train Loss 97.90607 Test RE 0.07878595556951576 c 1.4918479 k 0.9864052 m 3.5157776\n",
      "66 Train Loss 96.39055 Test RE 0.07923109007035468 c 1.5222098 k 0.9860432 m 3.5566306\n",
      "67 Train Loss 96.08685 Test RE 0.07983645136337524 c 1.5365729 k 0.9856828 m 3.5661557\n",
      "68 Train Loss 95.79303 Test RE 0.08076277962108337 c 1.5586143 k 0.9864283 m 3.6187258\n",
      "69 Train Loss 95.60901 Test RE 0.0812678424996331 c 1.5631484 k 0.9861307 m 3.6289485\n",
      "70 Train Loss 95.2007 Test RE 0.08160192226026405 c 1.5610957 k 0.98552024 m 3.5536454\n",
      "71 Train Loss 94.8423 Test RE 0.08099076209121384 c 1.5622708 k 0.9859406 m 3.5634305\n",
      "72 Train Loss 92.72186 Test RE 0.0791336414149812 c 1.5188035 k 0.98584515 m 3.6327806\n",
      "73 Train Loss 89.22096 Test RE 0.07789728114109197 c 1.5186608 k 0.98615295 m 3.811681\n",
      "74 Train Loss 88.38553 Test RE 0.07654181325789981 c 1.4955298 k 0.9874205 m 3.6939356\n",
      "75 Train Loss 88.17008 Test RE 0.07627595279130173 c 1.5041403 k 0.9870402 m 3.7468016\n",
      "76 Train Loss 87.28766 Test RE 0.0759443744779251 c 1.4940733 k 0.9868824 m 3.7620115\n",
      "77 Train Loss 85.77542 Test RE 0.07532778030322634 c 1.4679306 k 0.9884973 m 3.7608294\n",
      "78 Train Loss 85.049446 Test RE 0.07549919607820983 c 1.4941827 k 0.9869119 m 3.8579607\n",
      "79 Train Loss 85.010506 Test RE 0.07566982008155317 c 1.4936887 k 0.9871339 m 3.8580534\n",
      "80 Train Loss 84.85422 Test RE 0.07546024568407605 c 1.484477 k 0.9873751 m 3.9171853\n",
      "81 Train Loss 84.741684 Test RE 0.07469028202688233 c 1.4688269 k 0.98740935 m 3.9210908\n",
      "82 Train Loss 84.6888 Test RE 0.07431735756292374 c 1.4525862 k 0.98794127 m 3.8893604\n",
      "83 Train Loss 84.28392 Test RE 0.07336488920565763 c 1.4397714 k 0.9878763 m 3.9329917\n",
      "84 Train Loss 84.15749 Test RE 0.07357833104198913 c 1.4455028 k 0.9881249 m 3.947752\n",
      "85 Train Loss 83.93141 Test RE 0.07364336123859101 c 1.4467167 k 0.9879498 m 3.926735\n",
      "86 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "87 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "88 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "89 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "90 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "91 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "92 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "93 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "94 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "95 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "96 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "97 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "98 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "99 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "100 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "101 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "102 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "103 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "104 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "105 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "106 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "107 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "108 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "109 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "110 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "111 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "112 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "113 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "114 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "115 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "116 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "117 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "118 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "119 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "120 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "121 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "122 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "123 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "124 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "125 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "126 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "127 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "128 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "129 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "130 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "131 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "132 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "133 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "134 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "135 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "136 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "137 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "138 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "139 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "140 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "141 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "142 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "143 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "144 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "145 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "146 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "147 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "148 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "149 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "150 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "151 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "152 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "153 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "154 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "155 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "156 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "157 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "158 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "159 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "160 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "161 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "162 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "163 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "164 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "165 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "166 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "167 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "168 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "169 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "170 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "171 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "172 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "173 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "174 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "175 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "176 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "177 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "178 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "179 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "180 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "181 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "182 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "183 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "184 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "185 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "186 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "187 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "188 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "189 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "190 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "191 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "192 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "193 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "194 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "195 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "196 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "197 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "198 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "199 Train Loss 83.8354 Test RE 0.07339036078501822 c 1.4419186 k 0.9883788 m 3.9838874\n",
      "Training time: 30.89\n",
      "Training time: 30.89\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 1818.4888 Test RE 0.42151584646231455 c -0.25621915 k 1.5489872 m -0.00032296713\n",
      "1 Train Loss 501.7807 Test RE 0.22225936459462883 c -0.36234665 k 1.0361204 m -0.00048594078\n",
      "2 Train Loss 497.00815 Test RE 0.22123609020656196 c -0.36911404 k 1.0185863 m -0.0004959982\n",
      "3 Train Loss 496.9919 Test RE 0.22123261013278692 c -0.36953342 k 1.0175699 m -0.0004966194\n",
      "4 Train Loss 496.83682 Test RE 0.22117610296372475 c -0.37201574 k 1.0139776 m -0.0005001633\n",
      "5 Train Loss 496.47357 Test RE 0.221113479765703 c -0.37417343 k 1.0227052 m -0.0005029242\n",
      "6 Train Loss 494.71573 Test RE 0.22066738402742328 c -0.38960576 k 1.0212939 m -0.0005245972\n",
      "7 Train Loss 493.99567 Test RE 0.2204376064458924 c -0.39318466 k 1.0108027 m -0.00052947825\n",
      "8 Train Loss 490.73907 Test RE 0.21958015226399916 c -0.39343527 k 1.0213966 m -0.00051316875\n",
      "9 Train Loss 473.59628 Test RE 0.2151704396808343 c -0.43008825 k 1.0143008 m -0.0004734392\n",
      "10 Train Loss 436.00775 Test RE 0.20437183254890676 c -0.45141268 k 0.98774606 m 0.0011090691\n",
      "11 Train Loss 359.00665 Test RE 0.17488907471554727 c -0.37810174 k 1.0258975 m 0.028246598\n",
      "12 Train Loss 333.3278 Test RE 0.15664089764566758 c -0.21593286 k 0.9905597 m 0.071841426\n",
      "13 Train Loss 313.50076 Test RE 0.1469025547678614 c 0.28696918 k 1.00017 m 0.050976627\n",
      "14 Train Loss 288.09323 Test RE 0.15035077392455773 c 1.1577027 k 0.9978526 m 0.083112136\n",
      "15 Train Loss 262.87946 Test RE 0.14636829250483702 c 2.0134468 k 0.9790018 m 0.35048723\n",
      "16 Train Loss 255.46979 Test RE 0.1434039885972578 c 2.2714722 k 0.99163705 m 0.5592749\n",
      "17 Train Loss 244.56532 Test RE 0.14008664337149734 c 2.477288 k 0.97823393 m 1.0124205\n",
      "18 Train Loss 239.36655 Test RE 0.13685736570555732 c 2.2340288 k 0.97508514 m 1.2112647\n",
      "19 Train Loss 235.14594 Test RE 0.13593767643667734 c 1.9397091 k 0.98009795 m 1.381316\n",
      "20 Train Loss 233.4718 Test RE 0.1367134325666302 c 2.0117784 k 0.9790449 m 1.5579561\n",
      "21 Train Loss 231.63313 Test RE 0.13673490833981378 c 2.0733705 k 0.9765334 m 1.8309847\n",
      "22 Train Loss 226.63316 Test RE 0.13407988937833398 c 2.2018101 k 0.9778538 m 2.7606711\n",
      "23 Train Loss 224.8938 Test RE 0.13397283380250577 c 2.1707227 k 0.9832486 m 2.6303203\n",
      "24 Train Loss 224.14005 Test RE 0.13330550140935493 c 2.1440942 k 0.98118234 m 2.6087763\n",
      "25 Train Loss 223.36119 Test RE 0.13213481840374358 c 2.1322432 k 0.9801585 m 2.560682\n",
      "26 Train Loss 219.62341 Test RE 0.12756206329325492 c 1.9137918 k 0.9800006 m 1.9707338\n",
      "27 Train Loss 215.29172 Test RE 0.12598442296942772 c 1.9189883 k 0.98100966 m 2.2729917\n",
      "28 Train Loss 214.35046 Test RE 0.12721622727890652 c 1.9484221 k 0.9807735 m 2.1431665\n",
      "29 Train Loss 213.29906 Test RE 0.1290416819061757 c 1.9601506 k 0.9827665 m 2.1859255\n",
      "30 Train Loss 212.00519 Test RE 0.1289632891588828 c 2.0722299 k 0.98344237 m 2.5650048\n",
      "31 Train Loss 211.29553 Test RE 0.12792013420248977 c 2.018279 k 0.98026675 m 2.4023728\n",
      "32 Train Loss 209.97606 Test RE 0.1267279424614097 c 1.8750373 k 0.9813557 m 2.3566775\n",
      "33 Train Loss 207.53288 Test RE 0.12540298850659432 c 1.9835577 k 0.97963375 m 2.2911375\n",
      "34 Train Loss 207.34737 Test RE 0.12518837582306222 c 1.996751 k 0.97944546 m 2.2038434\n",
      "35 Train Loss 207.24391 Test RE 0.12508143166768318 c 1.9763083 k 0.9789943 m 2.172172\n",
      "36 Train Loss 206.6836 Test RE 0.12505160728000142 c 2.004851 k 0.978757 m 2.3271515\n",
      "37 Train Loss 206.40189 Test RE 0.12468887038893219 c 2.0241764 k 0.97952586 m 2.3057642\n",
      "38 Train Loss 206.08089 Test RE 0.12444521933786666 c 2.0046287 k 0.97946054 m 2.2363787\n",
      "39 Train Loss 205.65356 Test RE 0.12454324171667677 c 1.995338 k 0.97947013 m 2.3113587\n",
      "40 Train Loss 205.40118 Test RE 0.12408968356190374 c 1.9940276 k 0.9790009 m 2.257053\n",
      "41 Train Loss 205.29877 Test RE 0.12406372484232345 c 1.9979016 k 0.97893107 m 2.2633874\n",
      "42 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "43 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "44 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "45 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "46 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "47 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "48 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "49 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "50 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "51 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "52 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "53 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "54 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "55 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "56 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "57 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "58 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "59 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "60 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "61 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "62 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "63 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "64 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "65 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "66 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "67 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "68 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "69 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "70 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "71 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "72 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "73 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "74 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "75 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "76 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "77 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "78 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "79 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "80 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "81 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "82 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "83 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "84 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "85 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "86 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "87 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "88 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "89 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "90 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "91 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "92 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "93 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "94 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "95 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "96 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "97 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "98 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "99 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "100 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "101 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "102 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "103 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "104 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "105 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "106 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "107 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "108 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "109 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "110 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "111 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "112 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "113 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "114 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "115 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "116 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "117 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "118 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "119 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "120 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "121 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "122 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "123 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "124 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "125 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "126 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "127 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "128 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "129 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "130 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "131 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "132 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "133 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "134 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "135 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "136 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "137 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "138 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "139 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "140 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "141 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "142 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "143 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "144 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "145 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "146 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "147 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "148 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "149 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "150 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "151 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "152 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "153 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "154 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "155 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "156 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "157 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "158 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "159 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "160 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "161 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "162 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "163 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "164 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "165 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "166 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "167 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "168 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "169 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "170 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "171 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "172 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "173 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "174 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "175 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "176 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "177 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "178 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "179 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "180 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "181 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "182 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "183 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "184 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "185 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "186 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "187 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "188 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "189 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "190 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "191 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "192 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "193 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "194 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "195 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "196 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "197 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "198 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "199 Train Loss 205.2827 Test RE 0.12404117196660511 c 1.9954505 k 0.9788657 m 2.26647\n",
      "Training time: 22.29\n",
      "Training time: 22.29\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 709.8265 Test RE 0.26076319950932914 c 0.025169682 k 1.1332018 m -0.00019051059\n",
      "1 Train Loss 497.7451 Test RE 0.22139097693243934 c 0.028232843 k 1.0241786 m -0.00021353927\n",
      "2 Train Loss 497.01532 Test RE 0.22123778361754576 c 0.028418234 k 1.0179442 m -0.00021493349\n",
      "3 Train Loss 497.013 Test RE 0.22123731169144684 c 0.0284278 k 1.0176345 m -0.00021500546\n",
      "4 Train Loss 497.013 Test RE 0.22123731356555135 c 0.028428173 k 1.0176225 m -0.00021500826\n",
      "5 Train Loss 497.013 Test RE 0.22123730875629433 c 0.028428456 k 1.0176133 m -0.00021501038\n",
      "6 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "7 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "8 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "9 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "10 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "11 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "12 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "13 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "14 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "15 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "16 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "17 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "18 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "19 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "20 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "21 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "22 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "23 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "24 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "25 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "26 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "27 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "28 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "29 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "30 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "31 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "32 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "33 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "34 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "35 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "36 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "37 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "38 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "39 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "40 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "41 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "42 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "43 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "44 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "45 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "46 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "47 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "48 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "49 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "50 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "51 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "52 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "53 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "54 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "55 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "56 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "57 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "58 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "59 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "60 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "61 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "62 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "63 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "64 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "65 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "66 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "67 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "68 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "69 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "70 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "71 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "72 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "73 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "74 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "75 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "76 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "77 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "78 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "79 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "80 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "81 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "82 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "83 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "84 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "85 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "86 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "87 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "88 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "89 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "90 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "91 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "92 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "93 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "94 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "95 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "96 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "97 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "98 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "99 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "100 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "101 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "102 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "103 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "104 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "105 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "106 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "107 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "108 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "109 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "110 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "111 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "112 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "113 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "114 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "115 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "116 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "117 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "118 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "119 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "120 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "121 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "122 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "123 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "124 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "125 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "126 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "127 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "128 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "129 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "130 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "131 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "132 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "133 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "134 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "135 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "136 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "137 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "138 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "139 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "140 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "141 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "142 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "143 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "144 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "145 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "146 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "147 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "148 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "149 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "150 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "151 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "152 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "153 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "154 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "155 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "156 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "157 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "158 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "159 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "160 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "161 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "162 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "163 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "164 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "165 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "166 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "167 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "168 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "169 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "170 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "171 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "172 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "173 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "174 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "175 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "176 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "177 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "178 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "179 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "180 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "181 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "182 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "183 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "184 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "185 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "186 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "187 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "188 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "189 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "190 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "191 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "192 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "193 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "194 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "195 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "196 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "197 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "198 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "199 Train Loss 497.0129 Test RE 0.22123729346860949 c 0.028428672 k 1.0176064 m -0.000215012\n",
      "Training time: 15.78\n",
      "Training time: 15.78\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 1874.4646 Test RE 0.40778406787444954 c -0.011282389 k 1.3546727 m -0.00035270175\n",
      "1 Train Loss 506.60333 Test RE 0.22328513582170104 c -0.015681565 k 1.0438942 m -0.00048622678\n",
      "2 Train Loss 497.0341 Test RE 0.22124182993127714 c -0.016063811 k 1.0188836 m -0.0004976467\n",
      "3 Train Loss 497.01035 Test RE 0.2212367033321255 c -0.01608031 k 1.0176578 m -0.00049811555\n",
      "4 Train Loss 497.01025 Test RE 0.2212366943882228 c -0.01608073 k 1.0176235 m -0.0004981274\n",
      "5 Train Loss 497.01025 Test RE 0.22123669549398886 c -0.016080867 k 1.0176119 m -0.0004981313\n",
      "6 Train Loss 497.01025 Test RE 0.22123669510345104 c -0.016080977 k 1.0176024 m -0.0004981344\n",
      "7 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "8 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "9 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "10 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "11 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "12 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "13 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "14 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "15 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "16 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "17 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "18 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "19 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "20 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "21 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "22 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "23 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "24 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "25 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "26 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "27 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "28 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "29 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "30 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "31 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "32 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "33 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "34 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "35 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "36 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "37 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "38 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "39 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "40 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "41 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "42 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "43 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "44 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "45 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "46 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "47 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "48 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "49 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "50 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "51 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "52 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "53 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "54 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "55 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "56 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "57 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "58 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "59 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "60 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "61 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "62 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "63 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "64 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "65 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "66 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "67 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "68 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "69 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "70 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "71 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "72 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "73 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "74 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "75 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "76 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "77 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "78 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "79 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "80 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "81 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "82 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "83 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "84 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "85 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "86 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "87 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "88 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "89 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "90 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "91 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "92 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "93 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "94 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "95 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "96 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "97 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "98 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "99 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "100 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "101 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "102 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "103 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "104 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "105 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "106 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "107 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "108 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "109 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "110 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "111 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "112 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "113 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "114 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "115 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "116 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "117 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "118 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "119 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "120 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "121 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "122 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "123 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "124 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "125 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "126 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "127 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "128 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "129 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "130 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "131 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "132 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "133 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "134 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "135 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "136 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "137 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "138 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "139 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "140 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "141 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "142 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "143 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "144 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "145 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "146 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "147 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "148 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "149 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "150 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "151 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "152 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "153 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "154 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "155 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "156 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "157 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "158 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "159 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "160 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "161 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "162 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "163 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "164 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "165 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "166 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "167 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "168 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "169 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "170 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "171 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "172 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "173 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "174 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "175 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "176 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "177 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "178 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "179 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "180 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "181 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "182 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "183 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "184 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "185 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "186 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "187 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "188 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "189 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "190 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "191 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "192 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "193 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "194 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "195 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "196 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "197 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "198 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "199 Train Loss 497.01013 Test RE 0.22123667996536445 c -0.016081067 k 1.0175943 m -0.0004981369\n",
      "Training time: 9.05\n",
      "Training time: 9.05\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 890.78705 Test RE 0.28946930206331944 c 0.056892477 k 1.1789571 m -0.00034886543\n",
      "1 Train Loss 498.36533 Test RE 0.22152052457597474 c 0.06756189 k 1.0265326 m -0.00042091208\n",
      "2 Train Loss 497.01337 Test RE 0.22123733921599403 c 0.068212435 k 1.0180945 m -0.0004252989\n",
      "3 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "4 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "5 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "6 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "7 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "8 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "9 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "10 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "11 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "12 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "13 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "14 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "15 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "16 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "17 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "18 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "19 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "20 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "21 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "22 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "23 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "24 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "25 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "26 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "27 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "28 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "29 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "30 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "31 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "32 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "33 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "34 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "35 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "36 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "37 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "38 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "39 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "40 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "41 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "42 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "43 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "44 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "45 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "46 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "47 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "48 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "49 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "50 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "51 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "52 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "53 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "54 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "55 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "56 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "57 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "58 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "59 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "60 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "61 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "62 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "63 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "64 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "65 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "66 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "67 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "68 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "69 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "70 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "71 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "72 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "73 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "74 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "75 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "76 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "77 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "78 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "79 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "80 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "81 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "82 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "83 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "84 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "85 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "86 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "87 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "88 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "89 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "90 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "91 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "92 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "93 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "94 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "95 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "96 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "97 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "98 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "99 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "100 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "101 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "102 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "103 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "104 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "105 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "106 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "107 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "108 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "109 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "110 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "111 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "112 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "113 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "114 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "115 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "116 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "117 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "118 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "119 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "120 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "121 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "122 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "123 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "124 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "125 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "126 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "127 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "128 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "129 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "130 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "131 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "132 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "133 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "134 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "135 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "136 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "137 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "138 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "139 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "140 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "141 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "142 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "143 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "144 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "145 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "146 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "147 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "148 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "149 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "150 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "151 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "152 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "153 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "154 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "155 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "156 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "157 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "158 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "159 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "160 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "161 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "162 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "163 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "164 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "165 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "166 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "167 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "168 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "169 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "170 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "171 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "172 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "173 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "174 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "175 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "176 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "177 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "178 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "179 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "180 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "181 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "182 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "183 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "184 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "185 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "186 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "187 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "188 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "189 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "190 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "191 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "192 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "193 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "194 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "195 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "196 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "197 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "198 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "199 Train Loss 497.00912 Test RE 0.2212364547060056 c 0.068246484 k 1.0176545 m -0.0004255284\n",
      "Training time: 12.81\n",
      "Training time: 12.81\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 641.07635 Test RE 0.24785768786641182 c -0.33921042 k 1.197027 m -0.0004240304\n",
      "1 Train Loss 497.31027 Test RE 0.22130041142235576 c -0.36908436 k 1.0243118 m -0.00046105147\n",
      "2 Train Loss 497.0151 Test RE 0.2212377717142455 c -0.37054053 k 1.017953 m -0.0004628647\n",
      "3 Train Loss 497.01373 Test RE 0.22123746547311585 c -0.37064856 k 1.0174782 m -0.00046300323\n",
      "4 Train Loss 497.00516 Test RE 0.22123550193068062 c -0.37079376 k 1.0164397 m -0.00046329724\n",
      "5 Train Loss 496.86954 Test RE 0.22120522641300752 c -0.3693368 k 1.0169358 m -0.0004621335\n",
      "6 Train Loss 493.08493 Test RE 0.22018931770677408 c -0.36311653 k 1.0232553 m -0.00044171268\n",
      "7 Train Loss 389.15268 Test RE 0.189346422630274 c -0.29948094 k 1.0046937 m 0.004938168\n",
      "8 Train Loss 342.63855 Test RE 0.16543063445608203 c 0.099150546 k 1.0079412 m 0.17220773\n",
      "9 Train Loss 319.3467 Test RE 0.1589703506351797 c 0.63772875 k 0.9932824 m 0.26006892\n",
      "10 Train Loss 309.6381 Test RE 0.15905011078221531 c 1.2946839 k 0.99465555 m 0.41412058\n",
      "11 Train Loss 295.12454 Test RE 0.1557374620852694 c 2.2362797 k 0.9871059 m 0.64673567\n",
      "12 Train Loss 293.48105 Test RE 0.1571445697911584 c 2.2080014 k 0.98786676 m 0.6513204\n",
      "13 Train Loss 290.3223 Test RE 0.1549734221439004 c 1.7894742 k 0.9928508 m 0.60382485\n",
      "14 Train Loss 274.37778 Test RE 0.14948961114462944 c 1.8436898 k 0.9737216 m 0.8674399\n",
      "15 Train Loss 266.9743 Test RE 0.14847282719721402 c 2.2120838 k 0.9826211 m 1.1254296\n",
      "16 Train Loss 261.53143 Test RE 0.1481937782987971 c 2.1557906 k 0.9835686 m 1.4620795\n",
      "17 Train Loss 257.64386 Test RE 0.14584626682666121 c 1.7905887 k 0.9901277 m 1.9041152\n",
      "18 Train Loss 253.25635 Test RE 0.1441407835855677 c 2.2213004 k 0.98406416 m 2.5687664\n",
      "19 Train Loss 235.26143 Test RE 0.1396742309849295 c 2.3238077 k 0.9883513 m 3.5801082\n",
      "20 Train Loss 231.97188 Test RE 0.1391282973953864 c 2.5792058 k 0.98149157 m 3.8922067\n",
      "21 Train Loss 231.21318 Test RE 0.13865866010233951 c 2.4171448 k 0.98523074 m 4.0041304\n",
      "22 Train Loss 230.4934 Test RE 0.13878645177380125 c 2.3268564 k 0.98714936 m 3.957516\n",
      "23 Train Loss 227.87411 Test RE 0.138862143206769 c 2.5063076 k 0.9811688 m 4.011823\n",
      "24 Train Loss 225.53311 Test RE 0.13892193299636818 c 2.5705788 k 0.9795841 m 4.1420712\n",
      "25 Train Loss 225.01218 Test RE 0.1387722842384291 c 2.515342 k 0.9812783 m 4.216875\n",
      "26 Train Loss 223.11789 Test RE 0.1382801815084682 c 2.3487253 k 0.98690397 m 4.0200515\n",
      "27 Train Loss 219.24585 Test RE 0.1374107896164108 c 2.3600311 k 0.978571 m 3.5665047\n",
      "28 Train Loss 218.38815 Test RE 0.1375217118046087 c 2.3034103 k 0.98188555 m 3.2655067\n",
      "29 Train Loss 217.12859 Test RE 0.13734375333658244 c 2.3534355 k 0.9785249 m 3.2137609\n",
      "30 Train Loss 215.38165 Test RE 0.1364991787463401 c 2.345192 k 0.98326665 m 3.3986387\n",
      "31 Train Loss 214.34972 Test RE 0.13558666338538544 c 2.331197 k 0.9800732 m 3.483028\n",
      "32 Train Loss 212.58548 Test RE 0.1352449387389975 c 2.2809818 k 0.9824001 m 3.383687\n",
      "33 Train Loss 211.92014 Test RE 0.1348611429157455 c 2.2760296 k 0.9836241 m 3.2614267\n",
      "34 Train Loss 210.85518 Test RE 0.1342731376204698 c 2.2502162 k 0.9824686 m 3.084412\n",
      "35 Train Loss 208.08328 Test RE 0.13341189954101104 c 2.1868784 k 0.98379034 m 2.9291112\n",
      "36 Train Loss 204.68863 Test RE 0.13247585596978903 c 2.2106009 k 0.9833124 m 3.0865204\n",
      "37 Train Loss 203.14365 Test RE 0.13235257985630752 c 2.2420323 k 0.98189116 m 3.3173969\n",
      "38 Train Loss 202.59468 Test RE 0.13237948121575666 c 2.2636216 k 0.98171705 m 3.4951956\n",
      "39 Train Loss 202.40274 Test RE 0.13225177739690563 c 2.2671497 k 0.9824037 m 3.5680814\n",
      "40 Train Loss 200.19702 Test RE 0.13118099705569036 c 2.358722 k 0.974047 m 3.5103555\n",
      "41 Train Loss 196.77693 Test RE 0.13025239110560513 c 2.2176843 k 0.979581 m 3.2477896\n",
      "42 Train Loss 195.67628 Test RE 0.12953300056636044 c 2.281471 k 0.98194665 m 3.3717875\n",
      "43 Train Loss 193.39238 Test RE 0.1272505540473322 c 2.1359384 k 0.9829858 m 3.4355698\n",
      "44 Train Loss 191.14058 Test RE 0.12619728483898796 c 2.053145 k 0.98207015 m 3.2437649\n",
      "45 Train Loss 190.50484 Test RE 0.12559344574904432 c 2.0511353 k 0.98305696 m 3.1421585\n",
      "46 Train Loss 185.55641 Test RE 0.12426476024325647 c 2.0464158 k 0.9833045 m 2.8993268\n",
      "47 Train Loss 183.817 Test RE 0.12431147609644995 c 2.0363789 k 0.98397523 m 2.636466\n",
      "48 Train Loss 176.62198 Test RE 0.12027886682825834 c 1.9532372 k 0.98793536 m 2.6229277\n",
      "49 Train Loss 168.61465 Test RE 0.11740425668559402 c 2.0117176 k 0.9788423 m 2.653521\n",
      "50 Train Loss 163.04254 Test RE 0.11454099212686597 c 1.8690112 k 0.98780394 m 2.6447015\n",
      "51 Train Loss 160.36328 Test RE 0.11101493168571057 c 1.7629253 k 0.9866201 m 2.6295626\n",
      "52 Train Loss 158.09525 Test RE 0.10846148816018027 c 1.673983 k 0.9861815 m 2.6218562\n",
      "53 Train Loss 146.50957 Test RE 0.10570909479914063 c 1.6812302 k 0.9832347 m 2.891639\n",
      "54 Train Loss 144.22377 Test RE 0.10467305422135988 c 1.7515608 k 0.9846463 m 2.8444924\n",
      "55 Train Loss 139.73065 Test RE 0.10161478437715936 c 1.6760583 k 0.98067886 m 2.8587773\n",
      "56 Train Loss 134.15294 Test RE 0.09889886876603972 c 1.6447737 k 0.9825904 m 2.934625\n",
      "57 Train Loss 127.13074 Test RE 0.0989994462929528 c 1.7664907 k 0.98332024 m 2.95883\n",
      "58 Train Loss 121.36694 Test RE 0.09657229606309681 c 1.7190801 k 0.9789081 m 2.7725644\n",
      "59 Train Loss 115.50338 Test RE 0.09376199232737974 c 1.7563298 k 0.9823581 m 2.907665\n",
      "60 Train Loss 114.10726 Test RE 0.09452384221698078 c 1.6784575 k 0.98077226 m 3.0459504\n",
      "61 Train Loss 111.92343 Test RE 0.09461698601021618 c 1.733284 k 0.9839368 m 3.3522427\n",
      "62 Train Loss 110.34332 Test RE 0.09257872358428276 c 1.7328943 k 0.98559594 m 3.4884353\n",
      "63 Train Loss 108.67957 Test RE 0.09066969010163786 c 1.663183 k 0.98998964 m 3.4974365\n",
      "64 Train Loss 106.72131 Test RE 0.08901436727527687 c 1.6480812 k 0.9884146 m 3.6723545\n",
      "65 Train Loss 105.44105 Test RE 0.0881636087363818 c 1.5898019 k 0.988105 m 3.7443438\n",
      "66 Train Loss 100.48142 Test RE 0.08617774321226618 c 1.583876 k 0.98613983 m 3.455929\n",
      "67 Train Loss 98.34386 Test RE 0.08411374226173424 c 1.5095813 k 0.98677 m 3.3254297\n",
      "68 Train Loss 97.22093 Test RE 0.08349235502617987 c 1.4796251 k 0.9863742 m 3.419273\n",
      "69 Train Loss 94.6986 Test RE 0.08304474454416572 c 1.5753905 k 0.9865382 m 3.6123455\n",
      "70 Train Loss 93.10751 Test RE 0.08325851172108953 c 1.521581 k 0.9870168 m 3.6732507\n",
      "71 Train Loss 91.47719 Test RE 0.08318230488984986 c 1.5552491 k 0.9873688 m 3.7298741\n",
      "72 Train Loss 89.94845 Test RE 0.08192079216139063 c 1.4919593 k 0.9884873 m 3.6094658\n",
      "73 Train Loss 89.201904 Test RE 0.08241027904060726 c 1.4789244 k 0.9877729 m 3.6406152\n",
      "74 Train Loss 88.85994 Test RE 0.08262913117449164 c 1.4935167 k 0.98853385 m 3.7145572\n",
      "75 Train Loss 88.279274 Test RE 0.08235384668363736 c 1.454757 k 0.98849833 m 3.689181\n",
      "76 Train Loss 87.21265 Test RE 0.08160666107574392 c 1.4723563 k 0.98684615 m 3.6751719\n",
      "77 Train Loss 86.44398 Test RE 0.08102255472471746 c 1.4013612 k 0.98893666 m 3.6794128\n",
      "78 Train Loss 85.6261 Test RE 0.08063939486925903 c 1.4137173 k 0.98918295 m 3.7793417\n",
      "79 Train Loss 84.909966 Test RE 0.0811934882645132 c 1.4802461 k 0.9898096 m 3.8732154\n",
      "80 Train Loss 84.36885 Test RE 0.08101329696002016 c 1.4501964 k 0.9885657 m 3.9180217\n",
      "81 Train Loss 83.99737 Test RE 0.08041112758724962 c 1.4353286 k 0.9890521 m 3.92621\n",
      "82 Train Loss 83.71049 Test RE 0.08069931542898343 c 1.4551209 k 0.99048585 m 3.903676\n",
      "83 Train Loss 83.35858 Test RE 0.08031584925574664 c 1.431924 k 0.9897037 m 3.9230783\n",
      "84 Train Loss 82.14664 Test RE 0.07802967986872526 c 1.3634186 k 0.99147844 m 3.893966\n",
      "85 Train Loss 80.06395 Test RE 0.07676520291609877 c 1.3303037 k 0.9910076 m 3.9362512\n",
      "86 Train Loss 77.97512 Test RE 0.07630169194664221 c 1.3552992 k 0.9929493 m 4.05865\n",
      "87 Train Loss 76.519516 Test RE 0.07540588955589257 c 1.3426434 k 0.9960306 m 4.009313\n",
      "88 Train Loss 75.78989 Test RE 0.07483527997394193 c 1.3532635 k 0.9898795 m 3.9533215\n",
      "89 Train Loss 74.604416 Test RE 0.07501284014990116 c 1.3551724 k 0.98920894 m 3.9571621\n",
      "90 Train Loss 73.91868 Test RE 0.07434547082167355 c 1.3487376 k 0.99331194 m 4.0877385\n",
      "91 Train Loss 73.66369 Test RE 0.07410899421753923 c 1.3710729 k 0.99174553 m 4.1781282\n",
      "92 Train Loss 73.35469 Test RE 0.07409679834448814 c 1.401847 k 0.9906828 m 4.1049404\n",
      "93 Train Loss 72.83003 Test RE 0.07364376589828062 c 1.3989315 k 0.9902664 m 4.0040674\n",
      "94 Train Loss 72.607056 Test RE 0.07382356877992852 c 1.3727163 k 0.98821133 m 4.032163\n",
      "95 Train Loss 72.01805 Test RE 0.07376010183104602 c 1.366295 k 0.9900024 m 4.0487995\n",
      "96 Train Loss 71.27198 Test RE 0.07281527465888689 c 1.3764327 k 0.9909083 m 4.059557\n",
      "97 Train Loss 70.626945 Test RE 0.0724746478051577 c 1.3883651 k 0.9884949 m 4.084059\n",
      "98 Train Loss 70.02431 Test RE 0.07171280992605003 c 1.3751307 k 0.99028826 m 4.093168\n",
      "99 Train Loss 69.73543 Test RE 0.07188063403479238 c 1.3839294 k 0.9903342 m 4.107197\n",
      "100 Train Loss 69.4184 Test RE 0.07176539022469762 c 1.3824033 k 0.99366987 m 4.068509\n",
      "101 Train Loss 67.97092 Test RE 0.07076861342260422 c 1.2994195 k 0.9957146 m 4.111953\n",
      "102 Train Loss 67.23379 Test RE 0.0712109126944016 c 1.3355842 k 0.99033356 m 4.1333256\n",
      "103 Train Loss 66.6188 Test RE 0.07095116901902833 c 1.3803084 k 0.98947257 m 4.13886\n",
      "104 Train Loss 65.926765 Test RE 0.07025142022769956 c 1.3611898 k 0.9921654 m 4.236168\n",
      "105 Train Loss 65.37328 Test RE 0.07003019526543916 c 1.3477225 k 0.9931711 m 4.226796\n",
      "106 Train Loss 64.97559 Test RE 0.06969364048415527 c 1.3568819 k 0.99182516 m 4.254035\n",
      "107 Train Loss 64.60674 Test RE 0.06979004527977203 c 1.3614631 k 0.9924535 m 4.3088965\n",
      "108 Train Loss 64.36613 Test RE 0.07001539549821098 c 1.3571675 k 0.9931782 m 4.2553105\n",
      "109 Train Loss 64.233215 Test RE 0.06987355205740205 c 1.3442765 k 0.9907392 m 4.241422\n",
      "110 Train Loss 63.954895 Test RE 0.06994025226085063 c 1.34183 k 0.99012834 m 4.2526174\n",
      "111 Train Loss 63.75776 Test RE 0.07023329977246007 c 1.3589747 k 0.991513 m 4.205971\n",
      "112 Train Loss 63.49533 Test RE 0.07026448154740811 c 1.3682225 k 0.99018073 m 4.1848454\n",
      "113 Train Loss 63.13781 Test RE 0.07001658756873311 c 1.3493396 k 0.99012834 m 4.170258\n",
      "114 Train Loss 62.877487 Test RE 0.07009456909677333 c 1.34355 k 0.99111545 m 4.1437135\n",
      "115 Train Loss 62.685356 Test RE 0.06993580928415813 c 1.3457115 k 0.99019307 m 4.145428\n",
      "116 Train Loss 62.485577 Test RE 0.06957964440796852 c 1.3548471 k 0.9927173 m 4.139426\n",
      "117 Train Loss 62.15061 Test RE 0.0691568788155474 c 1.315377 k 0.99194163 m 4.0854983\n",
      "118 Train Loss 61.854107 Test RE 0.06922164604336473 c 1.3131855 k 0.9906846 m 4.096773\n",
      "119 Train Loss 61.461502 Test RE 0.06894995778154961 c 1.368345 k 0.9908077 m 4.1567903\n",
      "120 Train Loss 61.218613 Test RE 0.06876797492074782 c 1.3583798 k 0.9910868 m 4.173347\n",
      "121 Train Loss 61.068333 Test RE 0.06863436099577915 c 1.3499227 k 0.9935005 m 4.211337\n",
      "122 Train Loss 60.47491 Test RE 0.06814836844867803 c 1.3440742 k 0.9936499 m 4.2928095\n",
      "123 Train Loss 59.948666 Test RE 0.06768599737122605 c 1.3434811 k 0.9906353 m 4.253699\n",
      "124 Train Loss 59.725014 Test RE 0.0671404586759944 c 1.3231647 k 0.99208724 m 4.2562957\n",
      "125 Train Loss 59.617302 Test RE 0.066967483140894 c 1.3147953 k 0.9916745 m 4.2845297\n",
      "126 Train Loss 59.469406 Test RE 0.06699188976599348 c 1.3321158 k 0.9918565 m 4.284523\n",
      "127 Train Loss 59.214123 Test RE 0.06644673846345915 c 1.3278368 k 0.9925376 m 4.2860417\n",
      "128 Train Loss 58.793804 Test RE 0.06645923221474535 c 1.3276527 k 0.98663884 m 4.3001566\n",
      "129 Train Loss 58.040745 Test RE 0.06722144077698104 c 1.3440913 k 0.98848593 m 4.2709126\n",
      "130 Train Loss 57.813866 Test RE 0.06715834369687997 c 1.3199497 k 0.99175215 m 4.253834\n",
      "131 Train Loss 57.649532 Test RE 0.06694323535087714 c 1.3249409 k 0.9917543 m 4.3373895\n",
      "132 Train Loss 57.490578 Test RE 0.06673670225992866 c 1.3205553 k 0.99210536 m 4.3900785\n",
      "133 Train Loss 57.368668 Test RE 0.06650814356369374 c 1.3078446 k 0.9924178 m 4.3568664\n",
      "134 Train Loss 57.187996 Test RE 0.0665752884613514 c 1.3112226 k 0.99129635 m 4.3062224\n",
      "135 Train Loss 57.032333 Test RE 0.06629462043546543 c 1.3027177 k 0.99185216 m 4.297934\n",
      "136 Train Loss 56.965252 Test RE 0.06601453140591516 c 1.3055847 k 0.9921651 m 4.3259907\n",
      "137 Train Loss 56.912746 Test RE 0.06620478571318428 c 1.3097762 k 0.991729 m 4.3304677\n",
      "138 Train Loss 56.844093 Test RE 0.06614365697645917 c 1.2975945 k 0.99172103 m 4.3256354\n",
      "139 Train Loss 56.792152 Test RE 0.06591886356572303 c 1.2974538 k 0.992021 m 4.3339405\n",
      "140 Train Loss 56.728966 Test RE 0.06625068672528504 c 1.306455 k 0.9920008 m 4.344025\n",
      "141 Train Loss 56.712585 Test RE 0.06627201377509886 c 1.3076098 k 0.9919305 m 4.3451767\n",
      "142 Train Loss 56.668087 Test RE 0.06615023211921216 c 1.306955 k 0.9913712 m 4.328438\n",
      "143 Train Loss 56.5204 Test RE 0.06615187655099874 c 1.3097466 k 0.9917821 m 4.3389316\n",
      "144 Train Loss 56.435574 Test RE 0.06608493188925324 c 1.3171456 k 0.991932 m 4.357363\n",
      "145 Train Loss 56.363224 Test RE 0.06599358007595309 c 1.3149178 k 0.991589 m 4.333093\n",
      "146 Train Loss 56.281933 Test RE 0.06601371934085501 c 1.3164184 k 0.9924587 m 4.337154\n",
      "147 Train Loss 56.165085 Test RE 0.06575671808714896 c 1.2983112 k 0.9924776 m 4.347239\n",
      "148 Train Loss 56.115402 Test RE 0.06558738067851436 c 1.2948204 k 0.9924224 m 4.329487\n",
      "149 Train Loss 56.050835 Test RE 0.06574691899234937 c 1.3115689 k 0.99190146 m 4.334097\n",
      "150 Train Loss 56.00071 Test RE 0.06572548683531104 c 1.3088604 k 0.99184346 m 4.3606205\n",
      "151 Train Loss 55.961018 Test RE 0.06572684200710659 c 1.3086792 k 0.9922896 m 4.356949\n",
      "152 Train Loss 55.940475 Test RE 0.06561771765267539 c 1.3103957 k 0.99222094 m 4.358376\n",
      "153 Train Loss 55.932236 Test RE 0.06564267063482228 c 1.3096453 k 0.99180734 m 4.358451\n",
      "154 Train Loss 55.922863 Test RE 0.06571528890807976 c 1.308919 k 0.99201584 m 4.35209\n",
      "155 Train Loss 55.911247 Test RE 0.06559884555232028 c 1.3044239 k 0.99228525 m 4.354077\n",
      "156 Train Loss 55.872517 Test RE 0.06559324701771299 c 1.3090814 k 0.99192613 m 4.3642726\n",
      "157 Train Loss 55.82064 Test RE 0.06568546906100825 c 1.3133138 k 0.99241793 m 4.3741546\n",
      "158 Train Loss 55.784454 Test RE 0.06547947681749558 c 1.2967079 k 0.9922697 m 4.3782554\n",
      "159 Train Loss 55.742622 Test RE 0.06559508110958552 c 1.2981164 k 0.99125266 m 4.3464074\n",
      "160 Train Loss 55.69436 Test RE 0.06578418364766188 c 1.3069865 k 0.992081 m 4.347296\n",
      "161 Train Loss 55.65845 Test RE 0.06576384941061578 c 1.3134538 k 0.99254256 m 4.340449\n",
      "162 Train Loss 55.630936 Test RE 0.06567555749215151 c 1.3142531 k 0.99183357 m 4.337509\n",
      "163 Train Loss 55.57643 Test RE 0.06567378349087395 c 1.312047 k 0.9918043 m 4.3630443\n",
      "164 Train Loss 55.52356 Test RE 0.06565104093152321 c 1.3120072 k 0.99133193 m 4.3916974\n",
      "165 Train Loss 55.44048 Test RE 0.06557865589119725 c 1.3022915 k 0.9911327 m 4.4020696\n",
      "166 Train Loss 55.361496 Test RE 0.06555228461373913 c 1.3015103 k 0.99197966 m 4.417178\n",
      "167 Train Loss 55.30555 Test RE 0.06533159252276056 c 1.305298 k 0.99195206 m 4.398893\n",
      "168 Train Loss 55.28466 Test RE 0.06538986402721989 c 1.3036944 k 0.9919989 m 4.3786664\n",
      "169 Train Loss 55.263535 Test RE 0.06541796794309176 c 1.2974548 k 0.99191856 m 4.375556\n",
      "170 Train Loss 55.24169 Test RE 0.06533363828452429 c 1.3004754 k 0.9922537 m 4.3901114\n",
      "171 Train Loss 55.177086 Test RE 0.0653381198091663 c 1.3061763 k 0.9925472 m 4.409352\n",
      "172 Train Loss 55.04272 Test RE 0.06529287061234741 c 1.3098943 k 0.9925451 m 4.398526\n",
      "173 Train Loss 54.828598 Test RE 0.06491766189779612 c 1.3031068 k 0.9918278 m 4.393674\n",
      "174 Train Loss 54.562927 Test RE 0.06453529127528078 c 1.2961545 k 0.9926764 m 4.379395\n",
      "175 Train Loss 54.43672 Test RE 0.06480329987721599 c 1.2979596 k 0.99213076 m 4.353397\n",
      "176 Train Loss 54.337708 Test RE 0.06479569159074247 c 1.3092363 k 0.99061805 m 4.3369427\n",
      "177 Train Loss 54.214516 Test RE 0.0646855918479196 c 1.3217491 k 0.9899335 m 4.3600416\n",
      "178 Train Loss 53.563324 Test RE 0.0647475648420373 c 1.3587689 k 0.9881647 m 4.3357253\n",
      "179 Train Loss 52.767178 Test RE 0.06391183108168029 c 1.3281999 k 0.9889358 m 4.353394\n",
      "180 Train Loss 51.830948 Test RE 0.06296349415376357 c 1.3046725 k 0.9882248 m 4.3069506\n",
      "181 Train Loss 50.95105 Test RE 0.06262378248476573 c 1.3265457 k 0.9906353 m 4.3709555\n",
      "182 Train Loss 50.54777 Test RE 0.061971726258991305 c 1.2945803 k 0.9933421 m 4.450826\n",
      "183 Train Loss 50.186573 Test RE 0.061769955362415536 c 1.271701 k 0.9944142 m 4.438375\n",
      "184 Train Loss 49.6728 Test RE 0.06147404973311494 c 1.2449659 k 0.997841 m 4.493084\n",
      "185 Train Loss 48.128113 Test RE 0.06123105191823912 c 1.255158 k 0.99911726 m 4.465218\n",
      "186 Train Loss 47.176426 Test RE 0.060547713598088984 c 1.2749964 k 0.9943181 m 4.3830085\n",
      "187 Train Loss 45.487274 Test RE 0.05936090892397389 c 1.272438 k 0.98768187 m 4.3172007\n",
      "188 Train Loss 43.289936 Test RE 0.057997823515865535 c 1.2839943 k 0.99191415 m 4.4139147\n",
      "189 Train Loss 42.925262 Test RE 0.05766662675255803 c 1.2930204 k 0.9908535 m 4.4267554\n",
      "190 Train Loss 42.42244 Test RE 0.05718470569664968 c 1.267873 k 0.9926017 m 4.4921412\n",
      "191 Train Loss 41.961708 Test RE 0.05653334373541097 c 1.2594928 k 0.9959623 m 4.5569334\n",
      "192 Train Loss 41.0998 Test RE 0.05661475595884418 c 1.2673805 k 0.9932985 m 4.4634933\n",
      "193 Train Loss 40.84264 Test RE 0.056324187797080844 c 1.2456776 k 0.9951555 m 4.4694376\n",
      "194 Train Loss 40.155098 Test RE 0.055368958528111946 c 1.2522967 k 0.99432015 m 4.501385\n",
      "195 Train Loss 39.533424 Test RE 0.05482819738217901 c 1.269055 k 0.9914742 m 4.5382276\n",
      "196 Train Loss 38.809547 Test RE 0.053800998232226824 c 1.2358518 k 0.9914147 m 4.556305\n",
      "197 Train Loss 36.51316 Test RE 0.05090001041879678 c 1.2033801 k 0.9924995 m 4.479888\n",
      "198 Train Loss 34.14524 Test RE 0.049414024859828216 c 1.2083935 k 0.99217165 m 4.374367\n",
      "199 Train Loss 32.957207 Test RE 0.04870943298166267 c 1.177118 k 0.9905902 m 4.407751\n",
      "Training time: 39.16\n",
      "Training time: 39.16\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 1004.2865 Test RE 0.3093206145909224 c 0.26159966 k 1.2326063 m -0.00034628445\n",
      "1 Train Loss 498.83432 Test RE 0.22162410519145828 c 0.31369126 k 1.0284722 m -0.00042264984\n",
      "2 Train Loss 497.0206 Test RE 0.22123894767377167 c 0.31692278 k 1.018212 m -0.00042733335\n",
      "3 Train Loss 497.01474 Test RE 0.2212376978996983 c 0.3170986 k 1.0176394 m -0.0004275891\n",
      "4 Train Loss 497.01465 Test RE 0.22123768222446277 c 0.3171095 k 1.0176033 m -0.00042760515\n",
      "5 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "6 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "7 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "8 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "9 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "10 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "11 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "12 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "13 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "14 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "15 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "16 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "17 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "18 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "19 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "20 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "21 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "22 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "23 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "24 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "25 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "26 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "27 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "28 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "29 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "30 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "31 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "32 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "33 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "34 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "35 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "36 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "37 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "38 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "39 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "40 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "41 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "42 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "43 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "44 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "45 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "46 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "47 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "48 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "49 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "50 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "51 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "52 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "53 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "54 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "55 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "56 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "57 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "58 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "59 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "60 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "61 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "62 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "63 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "64 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "65 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "66 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "67 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "68 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "69 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "70 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "71 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "72 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "73 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "74 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "75 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "76 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "77 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "78 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "79 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "80 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "81 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "82 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "83 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "84 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "85 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "86 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "87 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "88 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "89 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "90 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "91 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "92 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "93 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "94 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "95 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "96 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "97 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "98 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "99 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "100 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "101 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "102 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "103 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "104 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "105 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "106 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "107 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "108 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "109 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "110 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "111 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "112 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "113 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "114 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "115 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "116 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "117 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "118 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "119 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "120 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "121 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "122 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "123 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "124 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "125 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "126 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "127 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "128 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "129 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "130 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "131 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "132 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "133 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "134 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "135 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "136 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "137 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "138 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "139 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "140 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "141 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "142 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "143 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "144 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "145 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "146 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "147 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "148 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "149 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "150 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "151 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "152 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "153 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "154 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "155 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "156 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "157 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "158 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "159 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "160 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "161 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "162 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "163 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "164 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "165 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "166 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "167 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "168 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "169 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "170 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "171 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "172 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "173 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "174 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "175 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "176 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "177 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "178 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "179 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "180 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "181 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "182 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "183 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "184 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "185 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "186 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "187 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "188 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "189 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "190 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "191 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "192 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "193 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "194 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "195 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "196 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "197 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "198 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "199 Train Loss 497.0145 Test RE 0.2212376442275862 c 0.31712294 k 1.0175577 m -0.00042762523\n",
      "Training time: 11.62\n",
      "Training time: 11.62\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 1500.1293 Test RE 0.3669110981021036 c 0.28678977 k 1.2862421 m -0.00021440604\n",
      "1 Train Loss 501.34546 Test RE 0.22214614659724669 c 0.37018335 k 1.0338819 m -0.00028144842\n",
      "2 Train Loss 497.0247 Test RE 0.2212397425723203 c 0.37595785 k 1.0185106 m -0.0002860581\n",
      "3 Train Loss 497.01028 Test RE 0.22123670345729243 c 0.37629682 k 1.0176138 m -0.00028632797\n",
      "4 Train Loss 497.01004 Test RE 0.2212366411067523 c 0.37632895 k 1.017528 m -0.000286353\n",
      "5 Train Loss 496.9986 Test RE 0.22123396481459803 c 0.3766304 k 1.0167285 m -0.0002865508\n",
      "6 Train Loss 496.24792 Test RE 0.22105995302462975 c 0.3792165 k 1.016376 m -0.00028794436\n",
      "7 Train Loss 496.11948 Test RE 0.2210314888042756 c 0.38038206 k 1.0198358 m -0.00028867446\n",
      "8 Train Loss 494.4856 Test RE 0.22063580612052966 c 0.38238677 k 1.0097904 m -0.00028636874\n",
      "9 Train Loss 483.68674 Test RE 0.2180506633728003 c 0.3688961 k 1.0362518 m -0.00016353888\n",
      "10 Train Loss 409.71597 Test RE 0.1979391672412325 c 0.3611036 k 1.0033892 m 0.00047683966\n",
      "11 Train Loss 364.0388 Test RE 0.18190382762819174 c 0.3630919 k 1.0107819 m 0.04806924\n",
      "12 Train Loss 338.26184 Test RE 0.16482666974716764 c 0.3802576 k 1.0002248 m 0.10745327\n",
      "13 Train Loss 335.37482 Test RE 0.16233343678751153 c 0.4034459 k 1.0001281 m 0.124285564\n",
      "14 Train Loss 326.73456 Test RE 0.16348342094939344 c 0.47764063 k 1.0024735 m 0.16520754\n",
      "15 Train Loss 320.39328 Test RE 0.16259351213890555 c 0.56251884 k 1.0033157 m 0.23763378\n",
      "16 Train Loss 299.59644 Test RE 0.1550994140104534 c 0.61740524 k 0.99677074 m 0.29424775\n",
      "17 Train Loss 297.28836 Test RE 0.15250154134767213 c 0.6612337 k 0.9981717 m 0.34902063\n",
      "18 Train Loss 294.86417 Test RE 0.1519660346608781 c 0.7342115 k 1.0011916 m 0.42789644\n",
      "19 Train Loss 286.4809 Test RE 0.15163456120069618 c 1.042056 k 1.0028235 m 0.72136647\n",
      "20 Train Loss 282.49548 Test RE 0.14995200814205845 c 1.2998633 k 0.99436855 m 0.9749597\n",
      "21 Train Loss 278.06128 Test RE 0.1486272288724762 c 1.4185998 k 0.9904354 m 1.1057798\n",
      "22 Train Loss 273.4559 Test RE 0.14810476678690848 c 1.514474 k 0.98566896 m 1.20225\n",
      "23 Train Loss 266.90726 Test RE 0.14716858581021228 c 1.6478785 k 0.97511584 m 1.3139285\n",
      "24 Train Loss 264.52115 Test RE 0.14580776333969664 c 1.6281216 k 0.98497164 m 1.2795212\n",
      "25 Train Loss 261.40173 Test RE 0.14553547809877587 c 1.6936246 k 1.0027623 m 1.316059\n",
      "26 Train Loss 257.40497 Test RE 0.14508263913847733 c 1.6936151 k 0.9871667 m 1.2962085\n",
      "27 Train Loss 256.87732 Test RE 0.14474092468120067 c 1.7324139 k 0.9832529 m 1.3174502\n",
      "28 Train Loss 255.92862 Test RE 0.14499868305449784 c 1.7924504 k 0.9935621 m 1.3644018\n",
      "29 Train Loss 249.40436 Test RE 0.14313516535789017 c 2.036128 k 0.9854277 m 1.5988753\n",
      "30 Train Loss 243.67538 Test RE 0.14043324567557525 c 2.3588831 k 0.97476697 m 1.9120016\n",
      "31 Train Loss 242.09286 Test RE 0.13949829248891302 c 2.577564 k 0.97147197 m 2.100103\n",
      "32 Train Loss 241.00546 Test RE 0.1388470442650405 c 2.6777601 k 0.97563016 m 2.2076018\n",
      "33 Train Loss 235.89777 Test RE 0.13775900311776526 c 2.5943732 k 0.9769664 m 2.2558622\n",
      "34 Train Loss 234.51079 Test RE 0.13676854358900428 c 2.522954 k 0.9771217 m 2.2620852\n",
      "35 Train Loss 230.40245 Test RE 0.13602673938440749 c 2.438415 k 0.9743592 m 2.2168984\n",
      "36 Train Loss 227.51532 Test RE 0.13533466077500325 c 2.486225 k 0.9732332 m 2.2643065\n",
      "37 Train Loss 225.81044 Test RE 0.134229994489762 c 2.3864238 k 0.9732304 m 2.2330728\n",
      "38 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "39 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "40 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "41 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "42 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "43 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "44 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "45 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "46 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "47 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "48 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "49 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "50 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "51 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "52 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "53 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "54 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "55 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "56 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "57 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "58 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "59 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "60 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "61 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "62 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "63 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "64 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "65 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "66 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "67 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "68 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "69 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "70 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "71 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "72 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "73 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "74 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "75 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "76 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "77 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "78 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "79 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "80 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "81 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "82 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "83 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "84 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "85 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "86 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "87 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "88 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "89 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "90 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "91 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "92 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "93 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "94 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "95 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "96 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "97 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "98 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "99 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "100 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "101 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "102 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "103 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "104 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "105 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "106 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "107 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "108 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "109 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "110 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "111 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "112 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "113 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "114 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "115 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "116 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "117 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "118 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "119 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "120 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "121 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "122 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "123 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "124 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "125 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "126 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "127 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "128 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "129 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "130 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "131 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "132 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "133 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "134 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "135 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "136 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "137 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "138 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "139 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "140 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "141 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "142 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "143 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "144 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "145 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "146 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "147 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "148 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "149 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "150 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "151 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "152 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "153 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "154 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "155 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "156 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "157 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "158 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "159 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "160 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "161 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "162 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "163 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "164 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "165 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "166 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "167 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "168 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "169 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "170 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "171 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "172 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "173 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "174 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "175 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "176 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "177 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "178 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "179 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "180 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "181 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "182 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "183 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "184 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "185 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "186 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "187 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "188 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "189 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "190 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "191 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "192 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "193 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "194 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "195 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "196 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "197 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "198 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "199 Train Loss 225.45529 Test RE 0.13391489450686123 c 2.304688 k 0.9742113 m 2.195913\n",
      "Training time: 24.46\n",
      "Training time: 24.46\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 764.8028 Test RE 0.2707654379967019 c 0.105258495 k 1.1568624 m -0.00012416286\n",
      "1 Train Loss 498.114 Test RE 0.22146741965934993 c 0.12019779 k 1.0256089 m -0.00014364599\n",
      "2 Train Loss 497.009 Test RE 0.2212363948207681 c 0.121204644 k 1.0180252 m -0.00014499741\n",
      "3 Train Loss 497.0059 Test RE 0.22123571671621445 c 0.12126141 k 1.017567 m -0.00014507644\n",
      "4 Train Loss 497.0016 Test RE 0.22123472092612498 c 0.12131431 k 1.0170424 m -0.00014516189\n",
      "5 Train Loss 441.07965 Test RE 0.20500718641539373 c 0.11249861 k 1.0668507 m -0.00013723421\n",
      "6 Train Loss 353.0041 Test RE 0.1731867602118376 c 0.12370317 k 0.9787828 m 0.015087297\n",
      "7 Train Loss 333.8498 Test RE 0.16230531061435388 c 0.26306757 k 1.0037227 m 0.12833048\n",
      "8 Train Loss 316.00012 Test RE 0.16244524487589684 c 1.2122403 k 1.0112897 m 0.41170624\n",
      "9 Train Loss 290.15393 Test RE 0.15754630857627466 c 1.9792517 k 0.96008444 m 0.83246464\n",
      "10 Train Loss 259.27725 Test RE 0.14447938728784065 c 2.6592689 k 0.97460335 m 1.3796465\n",
      "11 Train Loss 250.94093 Test RE 0.14155801813821867 c 2.1516066 k 0.9964899 m 1.5557435\n",
      "12 Train Loss 245.03596 Test RE 0.13631449882678828 c 2.0086963 k 0.9781596 m 1.8710713\n",
      "13 Train Loss 240.93335 Test RE 0.13523411220222276 c 1.8531561 k 0.9751973 m 2.1090782\n",
      "14 Train Loss 239.84204 Test RE 0.1352280731017498 c 1.9915577 k 0.98304474 m 2.1356647\n",
      "15 Train Loss 238.47226 Test RE 0.13498303775867254 c 2.0813243 k 0.9816483 m 2.3625553\n",
      "16 Train Loss 237.24129 Test RE 0.13574782175882943 c 2.01972 k 0.9842234 m 2.6437454\n",
      "17 Train Loss 234.60289 Test RE 0.13467962017121388 c 2.088864 k 0.98411065 m 2.818517\n",
      "18 Train Loss 231.66348 Test RE 0.13450310892054831 c 2.141757 k 0.98177415 m 3.1338027\n",
      "19 Train Loss 229.36293 Test RE 0.13628695808121982 c 2.2616613 k 0.9843084 m 3.2695737\n",
      "20 Train Loss 226.02733 Test RE 0.13751537230498925 c 2.0830631 k 0.9763947 m 3.5731697\n",
      "21 Train Loss 211.4758 Test RE 0.13052084537684608 c 1.7840034 k 0.99710464 m 3.6745126\n",
      "22 Train Loss 195.49068 Test RE 0.12422193041066916 c 2.0819426 k 0.9897915 m 4.0929704\n",
      "23 Train Loss 182.18947 Test RE 0.12130386559861132 c 1.9414803 k 0.9845386 m 4.5349345\n",
      "24 Train Loss 172.45815 Test RE 0.11842357199512289 c 1.9124854 k 0.9887371 m 4.856708\n",
      "25 Train Loss 165.08627 Test RE 0.11495430450216801 c 1.9866065 k 0.991457 m 5.238872\n",
      "26 Train Loss 163.57349 Test RE 0.1136130038846274 c 1.998488 k 0.9931252 m 5.1598806\n",
      "27 Train Loss 162.23007 Test RE 0.11269206346769545 c 2.0052989 k 0.98695517 m 4.983855\n",
      "28 Train Loss 160.01459 Test RE 0.11127708501776283 c 1.9945939 k 0.98886514 m 4.8582954\n",
      "29 Train Loss 157.51233 Test RE 0.10964836130146276 c 2.0057452 k 0.9901036 m 4.687966\n",
      "30 Train Loss 154.52321 Test RE 0.1077121319134362 c 1.9684252 k 0.984976 m 4.4388247\n",
      "31 Train Loss 151.81906 Test RE 0.10496927855802217 c 1.9625404 k 0.9864378 m 4.116574\n",
      "32 Train Loss 151.02347 Test RE 0.10316005448020255 c 1.9445001 k 0.9852734 m 3.939039\n",
      "33 Train Loss 150.5056 Test RE 0.10251288444719321 c 1.9062692 k 0.98280156 m 3.7365267\n",
      "34 Train Loss 150.03601 Test RE 0.10315256308701554 c 1.9324975 k 0.9822456 m 3.6843872\n",
      "35 Train Loss 149.27077 Test RE 0.10261230148748726 c 1.9160607 k 0.98108333 m 3.697035\n",
      "36 Train Loss 148.32681 Test RE 0.10173476879342976 c 1.8459268 k 0.9861848 m 3.7832317\n",
      "37 Train Loss 147.1719 Test RE 0.10127142133109339 c 1.8949509 k 0.9857624 m 3.8483036\n",
      "38 Train Loss 145.7387 Test RE 0.10030557653231054 c 1.8925173 k 0.9778506 m 3.7704144\n",
      "39 Train Loss 144.13467 Test RE 0.09833703929460418 c 1.7901369 k 0.97909606 m 3.6719744\n",
      "40 Train Loss 134.56313 Test RE 0.0937782975273621 c 1.6726874 k 0.99294853 m 3.620022\n",
      "41 Train Loss 131.5784 Test RE 0.09322873735592278 c 1.6886338 k 0.9875811 m 3.714939\n",
      "42 Train Loss 130.90224 Test RE 0.09355766821055574 c 1.6906887 k 0.98521405 m 3.7447183\n",
      "43 Train Loss 130.57957 Test RE 0.09373828963098477 c 1.700521 k 0.9858849 m 3.7019186\n",
      "44 Train Loss 128.1749 Test RE 0.09396631943179594 c 1.6650774 k 0.991029 m 3.6508105\n",
      "45 Train Loss 124.16148 Test RE 0.09225156522635138 c 1.6728053 k 0.98502594 m 3.6717803\n",
      "46 Train Loss 121.22171 Test RE 0.09122829013244795 c 1.7406157 k 0.9853102 m 3.7582796\n",
      "47 Train Loss 118.68155 Test RE 0.08830661077959699 c 1.6377814 k 0.9858192 m 3.7323956\n",
      "48 Train Loss 117.68043 Test RE 0.08753879955210327 c 1.6254232 k 0.99031794 m 3.6962733\n",
      "49 Train Loss 114.945045 Test RE 0.08685531458198219 c 1.626186 k 0.9862646 m 3.6918993\n",
      "50 Train Loss 112.46458 Test RE 0.08700371477811979 c 1.5640525 k 0.9879826 m 3.6161778\n",
      "51 Train Loss 109.81984 Test RE 0.08667173942525255 c 1.5953594 k 0.984814 m 3.5618315\n",
      "52 Train Loss 106.29082 Test RE 0.0845548681231334 c 1.5110453 k 0.98792696 m 3.6663985\n",
      "53 Train Loss 105.64626 Test RE 0.0848061589842509 c 1.4694906 k 0.9910564 m 3.806494\n",
      "54 Train Loss 104.90339 Test RE 0.0836890351581549 c 1.4771223 k 0.99360466 m 3.8230617\n",
      "55 Train Loss 102.99701 Test RE 0.08161828367637465 c 1.3966324 k 0.9888131 m 3.5522525\n",
      "56 Train Loss 101.873955 Test RE 0.08177005307649042 c 1.3735057 k 0.990934 m 3.4823747\n",
      "57 Train Loss 99.7063 Test RE 0.08168614321021148 c 1.3453664 k 0.98921824 m 3.6516993\n",
      "58 Train Loss 96.97263 Test RE 0.08053704751004888 c 1.3962905 k 0.99009246 m 3.687467\n",
      "59 Train Loss 91.82856 Test RE 0.07820901924027444 c 1.3389606 k 0.9913098 m 3.9498477\n",
      "60 Train Loss 78.6921 Test RE 0.06985754014267828 c 1.5478457 k 0.99672717 m 4.0246844\n",
      "61 Train Loss 73.56979 Test RE 0.06076427156723014 c 1.3521914 k 0.9858247 m 3.8277414\n",
      "62 Train Loss 60.21644 Test RE 0.055004676363603934 c 1.326394 k 0.9906867 m 4.0266047\n",
      "63 Train Loss 51.78881 Test RE 0.04690572091301549 c 1.3252625 k 0.99775326 m 4.679347\n",
      "64 Train Loss 48.246197 Test RE 0.04402740145714317 c 1.2043424 k 0.9919854 m 4.8303742\n",
      "65 Train Loss 37.91893 Test RE 0.03799372219845742 c 1.129734 k 1.0054928 m 4.7288246\n",
      "66 Train Loss 28.512533 Test RE 0.033883928852192394 c 1.0851107 k 0.99509454 m 4.629898\n",
      "67 Train Loss 27.489494 Test RE 0.03343801856582009 c 1.1009843 k 0.996326 m 4.7617955\n",
      "68 Train Loss 26.951796 Test RE 0.03274701176810401 c 1.0814621 k 0.99755734 m 4.8500233\n",
      "69 Train Loss 25.947697 Test RE 0.03268562936763718 c 1.1117746 k 0.9956762 m 4.863035\n",
      "70 Train Loss 25.11902 Test RE 0.032530818359507795 c 1.1075948 k 0.9971989 m 4.9599023\n",
      "71 Train Loss 24.852367 Test RE 0.03270713326241703 c 1.1104507 k 0.9974575 m 4.957788\n",
      "72 Train Loss 24.662159 Test RE 0.03301949507280769 c 1.1106738 k 0.9976505 m 4.9425054\n",
      "73 Train Loss 24.527708 Test RE 0.032670641295789865 c 1.087149 k 0.9971817 m 4.9433627\n",
      "74 Train Loss 23.86236 Test RE 0.032593979957464166 c 1.0858073 k 0.99827427 m 4.9317822\n",
      "75 Train Loss 23.505695 Test RE 0.032707021958856324 c 1.091792 k 0.99844795 m 4.975652\n",
      "76 Train Loss 23.435165 Test RE 0.03272075110521846 c 1.0933529 k 0.9980736 m 5.024639\n",
      "77 Train Loss 23.327278 Test RE 0.03269844882491364 c 1.0895853 k 0.9969367 m 4.9913282\n",
      "78 Train Loss 23.07668 Test RE 0.03288436003339919 c 1.1103613 k 0.9992134 m 4.9868035\n",
      "79 Train Loss 22.578999 Test RE 0.03235973327249716 c 1.0876677 k 0.9984129 m 4.9765387\n",
      "80 Train Loss 22.321106 Test RE 0.03240187172925535 c 1.0845919 k 0.998332 m 4.93053\n",
      "81 Train Loss 22.22403 Test RE 0.03243437694951473 c 1.0896524 k 0.99794847 m 4.947593\n",
      "82 Train Loss 22.085587 Test RE 0.03233972721794286 c 1.0855197 k 0.99822634 m 4.9685397\n",
      "83 Train Loss 21.805231 Test RE 0.032566767586425883 c 1.0888762 k 0.9978594 m 4.9147844\n",
      "84 Train Loss 21.757317 Test RE 0.03275034561460899 c 1.0866596 k 0.9980914 m 4.9195995\n",
      "85 Train Loss 21.718746 Test RE 0.03283849251076166 c 1.0904523 k 0.9979762 m 4.9406004\n",
      "86 Train Loss 21.70226 Test RE 0.03274385002647319 c 1.0854166 k 0.9983163 m 4.931333\n",
      "87 Train Loss 21.674107 Test RE 0.03270436606582107 c 1.084033 k 0.9980147 m 4.916347\n",
      "88 Train Loss 21.526814 Test RE 0.03280169196320149 c 1.0863107 k 0.9977272 m 4.904382\n",
      "89 Train Loss 21.343798 Test RE 0.0328009201179501 c 1.0910656 k 0.99881303 m 4.9502926\n",
      "90 Train Loss 21.155615 Test RE 0.032594038626695236 c 1.0923841 k 0.99765396 m 4.9289727\n",
      "91 Train Loss 21.10241 Test RE 0.03248944702651518 c 1.083688 k 0.998069 m 4.940579\n",
      "92 Train Loss 20.981369 Test RE 0.032648954324930105 c 1.0751762 k 0.9983103 m 4.9664474\n",
      "93 Train Loss 20.860504 Test RE 0.03252657168641253 c 1.0897574 k 0.9978332 m 4.9273977\n",
      "94 Train Loss 20.841398 Test RE 0.03248917923546881 c 1.0903226 k 0.9977553 m 4.928162\n",
      "95 Train Loss 20.78077 Test RE 0.032474770808871185 c 1.0927763 k 0.9975295 m 4.9322886\n",
      "96 Train Loss 20.326794 Test RE 0.032783864584377556 c 1.1156993 k 0.99767256 m 4.894073\n",
      "97 Train Loss 19.635582 Test RE 0.032767184262901664 c 1.0764736 k 0.998162 m 4.915379\n",
      "98 Train Loss 19.312752 Test RE 0.0326712931599522 c 1.0800468 k 0.99762374 m 4.900952\n",
      "99 Train Loss 19.164497 Test RE 0.03223548513896556 c 1.0932269 k 0.99824345 m 4.9231596\n",
      "100 Train Loss 19.113415 Test RE 0.032188224664906305 c 1.0989604 k 0.9983977 m 4.900927\n",
      "101 Train Loss 18.989742 Test RE 0.032025036072061724 c 1.0985329 k 0.99725574 m 4.8876085\n",
      "102 Train Loss 18.538765 Test RE 0.03123103213340413 c 1.0894316 k 0.9977841 m 4.912953\n",
      "103 Train Loss 18.18022 Test RE 0.031011313072563416 c 1.0944895 k 0.9977719 m 4.9419665\n",
      "104 Train Loss 18.063591 Test RE 0.030875627000363778 c 1.0849077 k 0.9981393 m 4.945965\n",
      "105 Train Loss 17.944042 Test RE 0.03071666832553861 c 1.0947943 k 0.9986265 m 4.932998\n",
      "106 Train Loss 17.839981 Test RE 0.030873194422673826 c 1.1038895 k 0.9976123 m 4.963336\n",
      "107 Train Loss 17.694656 Test RE 0.03059783968740274 c 1.0849427 k 0.99888825 m 4.996452\n",
      "108 Train Loss 17.594181 Test RE 0.03047725088337553 c 1.0720237 k 0.99864376 m 4.9531345\n",
      "109 Train Loss 17.563347 Test RE 0.03043511207968089 c 1.0794544 k 0.99789935 m 4.9487834\n",
      "110 Train Loss 17.522938 Test RE 0.030368944305642617 c 1.0881537 k 0.9985019 m 4.9606237\n",
      "111 Train Loss 17.469986 Test RE 0.03018854646608849 c 1.0740968 k 0.9985278 m 4.9773865\n",
      "112 Train Loss 17.259771 Test RE 0.029579623467291143 c 1.0559016 k 0.9978793 m 4.9965425\n",
      "113 Train Loss 16.747393 Test RE 0.029156119924147402 c 1.0898291 k 0.9981427 m 4.906858\n",
      "114 Train Loss 16.278585 Test RE 0.029347195337419918 c 1.0912375 k 0.997847 m 4.8749776\n",
      "115 Train Loss 16.080286 Test RE 0.029111092382742277 c 1.0845566 k 0.99839175 m 4.9149947\n",
      "116 Train Loss 16.012268 Test RE 0.02894867639442397 c 1.0868242 k 0.9979411 m 4.901796\n",
      "117 Train Loss 15.948847 Test RE 0.028810770205607777 c 1.085048 k 0.997708 m 4.8579154\n",
      "118 Train Loss 15.872381 Test RE 0.02856508146829098 c 1.0785921 k 0.9983496 m 4.8885126\n",
      "119 Train Loss 15.8242855 Test RE 0.028481719896005956 c 1.0794438 k 0.9978772 m 4.9207177\n",
      "120 Train Loss 15.440588 Test RE 0.027884241944892148 c 1.1117806 k 0.9920819 m 4.878596\n",
      "121 Train Loss 14.38744 Test RE 0.02694023216041804 c 1.0833035 k 0.99742234 m 4.863205\n",
      "122 Train Loss 14.136979 Test RE 0.026655483096206314 c 1.0580025 k 0.9986358 m 4.905275\n",
      "123 Train Loss 13.908016 Test RE 0.02625705446603427 c 1.0657142 k 0.99793893 m 4.904756\n",
      "124 Train Loss 13.692215 Test RE 0.025866008107604352 c 1.0874488 k 0.9989676 m 4.9406295\n",
      "125 Train Loss 13.534466 Test RE 0.025918860560362394 c 1.0805175 k 0.9993552 m 4.9763303\n",
      "126 Train Loss 13.187565 Test RE 0.026143873658952734 c 1.0656509 k 0.9983345 m 4.924708\n",
      "127 Train Loss 13.021101 Test RE 0.0257850450546942 c 1.0647719 k 0.99842614 m 4.924225\n",
      "128 Train Loss 12.957176 Test RE 0.02570475942019557 c 1.0605255 k 0.99812806 m 4.921364\n",
      "129 Train Loss 12.744911 Test RE 0.02603837700104106 c 1.0612851 k 0.9974862 m 4.858496\n",
      "130 Train Loss 12.47011 Test RE 0.025805477848263272 c 1.0637655 k 0.9998668 m 4.89229\n",
      "131 Train Loss 12.30742 Test RE 0.025595686367278114 c 1.0735413 k 0.9986971 m 4.9051223\n",
      "132 Train Loss 12.261163 Test RE 0.02540898776180871 c 1.0648036 k 0.99806494 m 4.891828\n",
      "133 Train Loss 12.222519 Test RE 0.025203119030870993 c 1.047824 k 0.99855226 m 4.912989\n",
      "134 Train Loss 12.058748 Test RE 0.02491230932310947 c 1.0483286 k 0.9970774 m 4.9068327\n",
      "135 Train Loss 11.9197445 Test RE 0.024744380747435166 c 1.0711837 k 0.9984618 m 4.8993325\n",
      "136 Train Loss 11.827599 Test RE 0.024848619597880355 c 1.0771242 k 0.9984456 m 4.9262133\n",
      "137 Train Loss 11.7949 Test RE 0.02467642097030164 c 1.0661134 k 0.9986114 m 4.9321003\n",
      "138 Train Loss 11.772245 Test RE 0.024642785788031443 c 1.0616481 k 0.9988488 m 4.9404893\n",
      "139 Train Loss 11.737806 Test RE 0.024711674065063815 c 1.0581441 k 0.9989776 m 4.9584646\n",
      "140 Train Loss 11.654348 Test RE 0.024571373924027796 c 1.066486 k 0.9990515 m 4.963862\n",
      "141 Train Loss 11.538797 Test RE 0.024168145101771392 c 1.0663905 k 0.9982482 m 4.94415\n",
      "142 Train Loss 11.512119 Test RE 0.024018056398866738 c 1.0630682 k 0.9988297 m 4.9571924\n",
      "143 Train Loss 11.501875 Test RE 0.024069768864285006 c 1.0617054 k 0.9991574 m 4.96378\n",
      "144 Train Loss 11.465481 Test RE 0.02399306143716749 c 1.0578915 k 0.9992246 m 4.963598\n",
      "145 Train Loss 11.412112 Test RE 0.023723043528001368 c 1.0616468 k 0.9994544 m 4.979147\n",
      "146 Train Loss 11.350458 Test RE 0.02358086344839291 c 1.0557035 k 0.9984286 m 4.97406\n",
      "147 Train Loss 11.317551 Test RE 0.02348418174143521 c 1.0552429 k 0.9987697 m 4.9528456\n",
      "148 Train Loss 11.312162 Test RE 0.023503455492126547 c 1.0560229 k 0.99888206 m 4.9477553\n",
      "149 Train Loss 11.299768 Test RE 0.023529765065794926 c 1.0550034 k 0.99849105 m 4.9462943\n",
      "150 Train Loss 11.253834 Test RE 0.023482761961046174 c 1.0598181 k 0.9987694 m 4.947698\n",
      "151 Train Loss 11.2116165 Test RE 0.023503690714678217 c 1.0628158 k 0.99916273 m 4.94224\n",
      "152 Train Loss 11.12422 Test RE 0.023186683045802205 c 1.0459839 k 0.99856144 m 4.9015584\n",
      "153 Train Loss 10.890152 Test RE 0.022430017046505817 c 1.0382442 k 1.0011883 m 4.9214544\n",
      "154 Train Loss 9.71464 Test RE 0.020296190988613516 c 1.0893341 k 1.0029947 m 5.055558\n",
      "155 Train Loss 8.639016 Test RE 0.01880831019234592 c 1.0419043 k 1.0009425 m 4.994157\n",
      "156 Train Loss 8.194151 Test RE 0.0179839893593737 c 1.0428133 k 0.9983082 m 4.983475\n",
      "157 Train Loss 8.050812 Test RE 0.017711389347696473 c 1.0491229 k 0.99902433 m 4.9547715\n",
      "158 Train Loss 7.851475 Test RE 0.017801189158947732 c 1.0304106 k 0.9995291 m 4.962235\n",
      "159 Train Loss 7.589249 Test RE 0.01775213778114545 c 1.045814 k 0.9990067 m 4.9466777\n",
      "160 Train Loss 7.507582 Test RE 0.017572005551252583 c 1.0485855 k 0.99940675 m 4.9547577\n",
      "161 Train Loss 7.4406996 Test RE 0.01749661652945944 c 1.0359311 k 0.9993731 m 4.983156\n",
      "162 Train Loss 7.404259 Test RE 0.01739004378415559 c 1.0470148 k 0.9990187 m 4.968249\n",
      "163 Train Loss 7.3849263 Test RE 0.01730398911990251 c 1.0433136 k 0.9993848 m 4.96567\n",
      "164 Train Loss 7.3498583 Test RE 0.01718872478685366 c 1.0403317 k 0.99966145 m 4.9785986\n",
      "165 Train Loss 7.3055105 Test RE 0.017228345691265818 c 1.0466231 k 0.99972594 m 4.979444\n",
      "166 Train Loss 7.264617 Test RE 0.017061969332497758 c 1.0407822 k 0.9997064 m 5.001008\n",
      "167 Train Loss 7.07485 Test RE 0.017181596012764193 c 1.0352075 k 0.99935347 m 4.9904647\n",
      "168 Train Loss 6.8781595 Test RE 0.01700121804588638 c 1.0367789 k 0.99958384 m 4.9447064\n",
      "169 Train Loss 6.7422323 Test RE 0.016630078597049278 c 1.0356804 k 0.99939233 m 4.957942\n",
      "170 Train Loss 6.7155943 Test RE 0.016696634995325317 c 1.0375223 k 0.9991491 m 4.951896\n",
      "171 Train Loss 6.7078805 Test RE 0.016702452384299345 c 1.0356184 k 0.99903303 m 4.9483953\n",
      "172 Train Loss 6.6910114 Test RE 0.01660994672476936 c 1.0321932 k 0.9991339 m 4.9536195\n",
      "173 Train Loss 6.6807165 Test RE 0.01660558991221392 c 1.0360444 k 0.9993142 m 4.9467664\n",
      "174 Train Loss 6.6201515 Test RE 0.016655238973604866 c 1.0325081 k 0.9993352 m 4.9233336\n",
      "175 Train Loss 6.4000187 Test RE 0.016622061473522867 c 1.0307511 k 0.9976725 m 4.914588\n",
      "176 Train Loss 5.845049 Test RE 0.016276474775066718 c 1.0360736 k 0.99955183 m 4.888343\n",
      "177 Train Loss 5.7920256 Test RE 0.01616456400134614 c 1.0245489 k 0.9994186 m 4.8780794\n",
      "178 Train Loss 5.6851788 Test RE 0.01631767679562243 c 1.0319796 k 0.9983784 m 4.8851404\n",
      "179 Train Loss 5.4881687 Test RE 0.0166828028211071 c 1.0413556 k 0.99934196 m 4.8988314\n",
      "180 Train Loss 5.4056854 Test RE 0.016565741112904906 c 1.0313878 k 0.9988615 m 4.876343\n",
      "181 Train Loss 5.2302437 Test RE 0.016506413305135408 c 1.021804 k 0.9989217 m 4.860101\n",
      "182 Train Loss 5.072172 Test RE 0.016619924739383722 c 1.0333787 k 0.99951804 m 4.883614\n",
      "183 Train Loss 4.803815 Test RE 0.016202665130383567 c 1.0277029 k 0.999919 m 4.8842883\n",
      "184 Train Loss 4.5012994 Test RE 0.016236470024675913 c 1.0213044 k 0.99924004 m 4.919259\n",
      "185 Train Loss 4.322946 Test RE 0.016086884350184377 c 1.0388521 k 0.9988639 m 4.910852\n",
      "186 Train Loss 4.205462 Test RE 0.015943110089550804 c 1.025382 k 0.99961954 m 4.9165273\n",
      "187 Train Loss 4.1636066 Test RE 0.015979590523015504 c 1.0207205 k 0.99939585 m 4.919671\n",
      "188 Train Loss 4.112596 Test RE 0.01589008298526116 c 1.0293963 k 1.0000373 m 4.9149175\n",
      "189 Train Loss 4.0004096 Test RE 0.015600187866084775 c 1.0188308 k 0.99897766 m 4.944613\n",
      "190 Train Loss 3.7647424 Test RE 0.015427155025665397 c 1.0051862 k 0.99996805 m 4.9724064\n",
      "191 Train Loss 3.6134021 Test RE 0.015205721975544373 c 1.0183449 k 0.9994195 m 4.9451795\n",
      "192 Train Loss 3.5841918 Test RE 0.015148583453121216 c 1.0208452 k 0.99948394 m 4.9601526\n",
      "193 Train Loss 3.5758796 Test RE 0.01516907611386368 c 1.0158955 k 0.9996804 m 4.974196\n",
      "194 Train Loss 3.5639067 Test RE 0.015204935689130545 c 1.0169938 k 0.99960864 m 4.961636\n",
      "195 Train Loss 3.549334 Test RE 0.015299884877094648 c 1.022031 k 0.99946517 m 4.952216\n",
      "196 Train Loss 3.5172172 Test RE 0.01516873643895334 c 1.0200394 k 0.99953043 m 4.9554625\n",
      "197 Train Loss 3.2413797 Test RE 0.013814309414574628 c 1.0254649 k 0.9992743 m 4.933205\n",
      "198 Train Loss 3.1013074 Test RE 0.013591789199133102 c 1.0201179 k 0.99935275 m 4.9315085\n",
      "199 Train Loss 3.056696 Test RE 0.013320438842303292 c 1.0243678 k 0.9993049 m 4.946762\n",
      "Training time: 47.62\n",
      "Training time: 47.62\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 696.5103 Test RE 0.2599813233274529 c 0.15725015 k 1.1455027 m -0.00022222589\n",
      "1 Train Loss 497.64902 Test RE 0.22137387809962092 c 0.1759793 k 1.0239996 m -0.0002508198\n",
      "2 Train Loss 497.04517 Test RE 0.2212444396655829 c 0.1770117 k 1.0178888 m -0.0002524291\n",
      "3 Train Loss 497.0279 Test RE 0.2212405896443661 c 0.17709976 k 1.0169454 m -0.00025257465\n",
      "4 Train Loss 496.95886 Test RE 0.2212252432511757 c 0.1770053 k 1.018251 m -0.00025247186\n",
      "5 Train Loss 496.8595 Test RE 0.22120199797901932 c 0.17797646 k 1.0184569 m -0.00025409038\n",
      "6 Train Loss 496.04498 Test RE 0.2210170040417794 c 0.177204 k 1.0170889 m -0.0002538614\n",
      "7 Train Loss 494.44266 Test RE 0.2202643693858696 c 0.17708838 k 1.0375746 m -0.00025647285\n",
      "8 Train Loss 471.74414 Test RE 0.21369162957117074 c 0.17456192 k 1.01589 m -0.00025230533\n",
      "9 Train Loss 426.53348 Test RE 0.19994407250141474 c 0.16732311 k 0.99338216 m 0.00023506029\n",
      "10 Train Loss 377.72495 Test RE 0.18367235238935792 c 0.18789735 k 1.0388875 m 0.0034693228\n",
      "11 Train Loss 345.9859 Test RE 0.17202006213374132 c 0.2770441 k 0.99289155 m 0.0350408\n",
      "12 Train Loss 313.84357 Test RE 0.1567436763703938 c 0.757731 k 1.0106014 m 0.3403852\n",
      "13 Train Loss 303.26123 Test RE 0.14750206399775367 c 1.2849828 k 0.9753762 m 0.6898642\n",
      "14 Train Loss 267.89648 Test RE 0.13886333107688112 c 1.8028163 k 0.96682954 m 1.0194618\n",
      "15 Train Loss 264.54996 Test RE 0.13710665464512842 c 1.8873659 k 0.98702973 m 1.0834686\n",
      "16 Train Loss 258.4143 Test RE 0.1333746122538581 c 2.1920877 k 0.98117346 m 1.3877819\n",
      "17 Train Loss 255.09946 Test RE 0.13388255219342077 c 2.1561522 k 0.96017134 m 1.4819839\n",
      "18 Train Loss 239.25456 Test RE 0.13526911411095963 c 1.8754482 k 0.9673609 m 1.9751081\n",
      "19 Train Loss 233.97263 Test RE 0.13373701975851737 c 1.9924862 k 0.9886177 m 2.3407845\n",
      "20 Train Loss 232.2112 Test RE 0.1334236098993936 c 2.0643291 k 0.9783072 m 2.614306\n",
      "21 Train Loss 230.27246 Test RE 0.1336645842870651 c 2.0462854 k 0.978273 m 2.7681167\n",
      "22 Train Loss 228.50685 Test RE 0.13377876156603455 c 2.1037421 k 0.97995377 m 2.7569282\n",
      "23 Train Loss 226.71579 Test RE 0.13386849907056236 c 2.1981978 k 0.98435724 m 3.1142561\n",
      "24 Train Loss 220.96843 Test RE 0.13354154719579564 c 2.3071003 k 0.984943 m 3.252975\n",
      "25 Train Loss 219.40273 Test RE 0.13394315200993362 c 2.271869 k 0.98069656 m 2.9873874\n",
      "26 Train Loss 213.149 Test RE 0.13216728092547583 c 2.4303448 k 0.98082405 m 2.3571117\n",
      "27 Train Loss 204.76913 Test RE 0.1312748793581188 c 2.5243547 k 0.9756822 m 2.6594052\n",
      "28 Train Loss 197.24446 Test RE 0.12859003483884546 c 2.590348 k 0.98568326 m 3.3514938\n",
      "29 Train Loss 189.3137 Test RE 0.12576780875383078 c 2.6607912 k 0.9866333 m 3.8725402\n",
      "30 Train Loss 180.77939 Test RE 0.12379741652642749 c 2.4343257 k 0.9813741 m 4.1492205\n",
      "31 Train Loss 170.80437 Test RE 0.11990051024098043 c 2.3828175 k 0.98276705 m 3.9640012\n",
      "32 Train Loss 161.78563 Test RE 0.12001155167917972 c 2.333035 k 0.9915458 m 4.6310844\n",
      "33 Train Loss 156.54803 Test RE 0.11862668570150803 c 2.2645876 k 0.98135215 m 4.991701\n",
      "34 Train Loss 144.48045 Test RE 0.11203069031094996 c 2.0678933 k 0.98927075 m 3.8801696\n",
      "35 Train Loss 135.3122 Test RE 0.11081013965929613 c 1.9789935 k 0.9781961 m 3.5058167\n",
      "36 Train Loss 131.14438 Test RE 0.10722549765198978 c 1.6927004 k 0.9841457 m 3.275599\n",
      "37 Train Loss 125.76994 Test RE 0.10503616916593903 c 1.7702087 k 0.9804223 m 2.7119846\n",
      "38 Train Loss 122.61356 Test RE 0.10341972957120463 c 1.8925714 k 0.97866 m 2.8568537\n",
      "39 Train Loss 113.60308 Test RE 0.09796022087475144 c 1.6171558 k 0.973043 m 2.856862\n",
      "40 Train Loss 103.33569 Test RE 0.09299585898219727 c 1.7629352 k 0.9845658 m 3.2535794\n",
      "41 Train Loss 94.52852 Test RE 0.08873608543024304 c 1.6829827 k 0.98968214 m 4.375942\n",
      "42 Train Loss 89.53373 Test RE 0.08605322060611138 c 1.6437713 k 0.9920181 m 4.6954403\n",
      "43 Train Loss 82.46682 Test RE 0.08101155018859794 c 1.5640075 k 0.988891 m 4.359435\n",
      "44 Train Loss 78.93183 Test RE 0.0762197186685433 c 1.4400036 k 0.9884024 m 4.2420835\n",
      "45 Train Loss 75.16331 Test RE 0.0716331401565977 c 1.4645634 k 0.99158335 m 4.5131536\n",
      "46 Train Loss 73.80592 Test RE 0.06881530895411331 c 1.343139 k 0.9924751 m 4.674581\n",
      "47 Train Loss 73.18744 Test RE 0.06856548843612692 c 1.3199985 k 0.9912447 m 4.4631505\n",
      "48 Train Loss 72.63582 Test RE 0.06905670180946065 c 1.3488939 k 0.9925458 m 4.478292\n",
      "49 Train Loss 72.44784 Test RE 0.0692064984635761 c 1.3233769 k 0.9926076 m 4.484129\n",
      "50 Train Loss 72.19853 Test RE 0.06979754426192572 c 1.3422244 k 0.99217606 m 4.3892684\n",
      "51 Train Loss 70.96605 Test RE 0.07053549838195862 c 1.3818355 k 0.9927512 m 4.363087\n",
      "52 Train Loss 68.056015 Test RE 0.06997919414094539 c 1.3709083 k 0.99011225 m 4.44215\n",
      "53 Train Loss 64.96138 Test RE 0.06941289534271784 c 1.3666661 k 0.9900004 m 4.4240127\n",
      "54 Train Loss 64.10821 Test RE 0.06896146577808522 c 1.3619941 k 0.99119383 m 4.5303187\n",
      "55 Train Loss 62.414402 Test RE 0.06912495712989608 c 1.3391798 k 0.9911327 m 4.2466016\n",
      "56 Train Loss 60.976685 Test RE 0.06827354969796325 c 1.3296015 k 0.9920264 m 4.4079666\n",
      "57 Train Loss 59.021114 Test RE 0.06844832851337772 c 1.3362118 k 0.991761 m 4.330677\n",
      "58 Train Loss 58.367004 Test RE 0.06820932734914435 c 1.3371266 k 0.9912599 m 4.4092236\n",
      "59 Train Loss 58.067772 Test RE 0.06801283273868432 c 1.3304263 k 0.9915758 m 4.3943744\n",
      "60 Train Loss 57.827034 Test RE 0.06810732563731256 c 1.3355871 k 0.9913742 m 4.297022\n",
      "61 Train Loss 57.566544 Test RE 0.06781542247281205 c 1.3252738 k 0.99116033 m 4.3442817\n",
      "62 Train Loss 57.423172 Test RE 0.06763031608778967 c 1.3191638 k 0.9919557 m 4.3770823\n",
      "63 Train Loss 57.399918 Test RE 0.06768402502430601 c 1.323639 k 0.99195725 m 4.3588223\n",
      "64 Train Loss 57.397797 Test RE 0.06768410891924129 c 1.3226433 k 0.99180955 m 4.3573227\n",
      "65 Train Loss 57.393246 Test RE 0.0676369209659011 c 1.321162 k 0.99189466 m 4.359772\n",
      "66 Train Loss 57.36926 Test RE 0.06763534389463484 c 1.3167447 k 0.9920101 m 4.362336\n",
      "67 Train Loss 57.252136 Test RE 0.06763692434387576 c 1.3185912 k 0.99144536 m 4.3264613\n",
      "68 Train Loss 57.14986 Test RE 0.06754465232844596 c 1.3158895 k 0.9922222 m 4.3646045\n",
      "69 Train Loss 57.107758 Test RE 0.06753476720432841 c 1.3169161 k 0.99187803 m 4.355349\n",
      "70 Train Loss 57.07892 Test RE 0.06761329049089367 c 1.3197434 k 0.9919158 m 4.351386\n",
      "71 Train Loss 57.07512 Test RE 0.06757121117880018 c 1.3179471 k 0.9919308 m 4.3553677\n",
      "72 Train Loss 57.06952 Test RE 0.06758635720702984 c 1.3182194 k 0.9919137 m 4.3519454\n",
      "73 Train Loss 57.052387 Test RE 0.06750878367995895 c 1.3164612 k 0.9918603 m 4.3566866\n",
      "74 Train Loss 57.032204 Test RE 0.0674647375923149 c 1.3102366 k 0.9917505 m 4.345931\n",
      "75 Train Loss 57.002235 Test RE 0.0674606337755776 c 1.3074033 k 0.99206024 m 4.3523116\n",
      "76 Train Loss 56.991386 Test RE 0.06747261380686685 c 1.3168168 k 0.9919482 m 4.356023\n",
      "77 Train Loss 56.986702 Test RE 0.06751420213207163 c 1.3181806 k 0.9916492 m 4.346279\n",
      "78 Train Loss 56.97656 Test RE 0.06748949843141888 c 1.3146025 k 0.99199665 m 4.3533583\n",
      "79 Train Loss 56.964394 Test RE 0.0674180290668718 c 1.3132075 k 0.9919107 m 4.351061\n",
      "80 Train Loss 56.946976 Test RE 0.06748719407247802 c 1.3167821 k 0.991599 m 4.349004\n",
      "81 Train Loss 56.93131 Test RE 0.06749133165794122 c 1.3159643 k 0.9919922 m 4.352155\n",
      "82 Train Loss 56.915688 Test RE 0.06748202795908591 c 1.3198097 k 0.9916179 m 4.357763\n",
      "83 Train Loss 56.874 Test RE 0.06746552377039672 c 1.3123488 k 0.9916675 m 4.3545556\n",
      "84 Train Loss 56.852505 Test RE 0.06747751227346135 c 1.3184755 k 0.9918638 m 4.35392\n",
      "85 Train Loss 56.846943 Test RE 0.0674424319790221 c 1.3161663 k 0.9916939 m 4.352293\n",
      "86 Train Loss 56.830734 Test RE 0.06741359895003439 c 1.3106838 k 0.99191767 m 4.363429\n",
      "87 Train Loss 56.798916 Test RE 0.0673747473745536 c 1.3177564 k 0.99187577 m 4.363779\n",
      "88 Train Loss 56.79312 Test RE 0.06734292846837378 c 1.3165607 k 0.99200445 m 4.3585925\n",
      "89 Train Loss 56.78768 Test RE 0.06735931798412367 c 1.3164891 k 0.99203974 m 4.3570914\n",
      "90 Train Loss 56.783066 Test RE 0.06734213297316165 c 1.3145484 k 0.9918687 m 4.3620667\n",
      "91 Train Loss 56.77355 Test RE 0.06736168302136619 c 1.314301 k 0.9918768 m 4.3603873\n",
      "92 Train Loss 56.76894 Test RE 0.06738371867522056 c 1.3142896 k 0.99181867 m 4.355631\n",
      "93 Train Loss 56.76693 Test RE 0.06736989538255655 c 1.3141943 k 0.9919171 m 4.354743\n",
      "94 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "95 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "96 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "97 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "98 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "99 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "100 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "101 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "102 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "103 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "104 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "105 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "106 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "107 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "108 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "109 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "110 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "111 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "112 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "113 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "114 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "115 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "116 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "117 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "118 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "119 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "120 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "121 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "122 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "123 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "124 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "125 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "126 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "127 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "128 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "129 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "130 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "131 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "132 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "133 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "134 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "135 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "136 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "137 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "138 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "139 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "140 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "141 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "142 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "143 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "144 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "145 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "146 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "147 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "148 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "149 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "150 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "151 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "152 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "153 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "154 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "155 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "156 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "157 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "158 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "159 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "160 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "161 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "162 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "163 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "164 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "165 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "166 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "167 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "168 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "169 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "170 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "171 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "172 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "173 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "174 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "175 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "176 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "177 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "178 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "179 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "180 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "181 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "182 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "183 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "184 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "185 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "186 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "187 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "188 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "189 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "190 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "191 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "192 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "193 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "194 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "195 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "196 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "197 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "198 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "199 Train Loss 56.76657 Test RE 0.06736139239047038 c 1.3143471 k 0.9919585 m 4.3550763\n",
      "Training time: 36.77\n",
      "Training time: 36.77\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 200\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "m_full = []\n",
    "k_full = []\n",
    "c_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "\n",
    "for reps in range(max_reps):   \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []   \n",
    "    beta_val = []\n",
    "    m_val = []\n",
    "    k_val = []\n",
    "    c_val = []\n",
    "\n",
    "    torch.manual_seed(reps*36)\n",
    "    N_f = 10000 #Total number of collocation points\n",
    "\n",
    "    layers = np.array([1,50,50,50,1]) #9 hidden layers\n",
    "    PINN = Sequentialmodel(layers)\n",
    "\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.25, \n",
    "                                max_iter = 10, \n",
    "                                max_eval = 15, \n",
    "                                tolerance_grad = 1e-6, \n",
    "                                tolerance_change = 1e-6, \n",
    "                                history_size = 100, \n",
    "                                line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "    train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    beta_full.append(beta_val)\n",
    "    m_full.append(m_val)\n",
    "    k_full.append(k_val)\n",
    "    c_full.append(c_val)\n",
    "    \n",
    "\n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"beta\": beta_full,\"m\": m_full,\"k\": k_full,\"c\": c_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "pmHEeBpzfFQh",
    "outputId": "990b2054-f35e-4c8e-c378-69ff2eb19f4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa4e04c7090>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAigAAAGdCAYAAAA44ojeAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAABPT0lEQVR4nO3deVxVdf7H8ddlFRBQUFkUFZVyX9Iyl9JSKUvbptW2sWV00oqyzZzKmknMymz0lzNWY7Y4NmWaNTVJm1a2KGqi5o6KC5KmgKis5/fHVzZFAr33nnvh/Xw8zuNezj3c++GI9775bsdhWZaFiIiIiAfxsbsAERERkRMpoIiIiIjHUUARERERj6OAIiIiIh5HAUVEREQ8jgKKiIiIeBwFFBEREfE4CigiIiLicfzsLuB0lJSUsGfPHkJDQ3E4HHaXIyIiIjVgWRa5ubnExsbi41N9G4lXBpQ9e/YQFxdndxkiIiJyGjIyMmjRokW1x3hlQAkNDQXMDxgWFmZzNSIiIlITOTk5xMXFlX2OV8crA0ppt05YWJgCioiIiJepyfAMDZIVERERj6OAIiIiIh5HAUVEREQ8jgKKiIiIeBwFFBEREfE4CigiIiLicRRQRERExOMooIiIiIjHUUARERERj6OAIiIiIh5HAUVEREQ8jgKKiIiIeByvvFig1NyOHfCf/8CBA3DeeTB8OPj7212ViIhI9RRQ6rDp02HcOCgsLN/XrRvMnw9t29pXl4iIyO9RF08dNX063HefCScDBsDo0RARAT//DBdcYFpWREREPJUCSh3000/wwAPm/sSJ8NVXMHMmrF0LnTvD3r1wzTWVW1ZEREQ8iQJKHWNZcO+9UFwM118PTz4JDod5LCYG/vtf05KyciW8/LK9tYqIiJyKAkodM3++aUEJCTEBpDSclGrZEp5/3tx/6inYvt3tJYqIiPwuBZQ6pLgYJkww98eNg+joqo8bORIuvBCOHIG//tV99YmIiNSUAkodsngxbNoEjRqZgHIqDgdMnmzuv/km7NzplvJERERqTAGlDvnnP83t7bdDWFj1x/bpAxddBEVF5V0+IiIinkIBpY7YvRs+/tjcHzWqZt9T2h30r39BTo5r6hIRETkdCih1xJw5ZgzKBRdAhw41+56LLzbHHjkCc+e6tj4REZHaUECpI+bPN7e33Vbz73E44E9/MvdnzXJ+TSIiIqdLAaUO2L7drGvi4wNXXlm77731VggIgFWrIDXVJeWJiIjUmgJKHbBggbm98EJo2rR23xsZaVaVBXj7befWJSIicroUUOqADz4wt6VBo7Zuusnc/uc/UFLinJpERETOhAKKl/vtN/juO3P/qqtO7zkuuQTCw2HPHvj2W6eVJiIictpqHVCWLl3K8OHDiY2NxeFwsHDhwlMeO2rUKBwOB9OmTau0Pz8/n3vvvZcmTZoQEhLCFVdcwa5du2pbimAuBGhZ0LEjxMVVcUB+/u82iwQGwtVXm/vz5jm/RhERkdqqdUDJy8ujW7duzJgxo9rjFi5cyI8//khsbOxJjyUlJbFgwQLmzZvHt99+y+HDhxk2bBjFxcW1Lafe+/xzczt48AkPLFgAPXtCUBA0bAiHDlX7PDfeaG4/+EDdPCIiYj+/2n7D0KFDGTp0aLXH7N69m7Fjx/LZZ59x+eWXV3osOzub119/nbfeeovBxz9V3377beLi4vj888+55JJLaltSvXZSQLEseOwxmDKl/KBzzzXr31fjoovM6rP79sHy5dC7t0vKFRERqRGnj0EpKSnh1ltv5eGHH6ZTp04nPZ6amkphYSGJiYll+2JjY+ncuTPLli1zdjl12vbtsGUL+PrCgAHHd06dWh5OHn3ULDH74Yfl33TsGKSlnfRcAQFQmjsXLXJp2SIiIr/L6QHlueeew8/Pj/vuu6/KxzMzMwkICKBx48aV9kdFRZGZmVnl9+Tn55OTk1NpE/jyS3N73nnHr72zbp1pPQF46SVzRcDY2PLWk1274PzzTXNLFed6+HBzq4AiIiJ2c2pASU1N5eWXX+aNN97A4XDU6nstyzrl9yQnJxMeHl62xVU5GrT++eYbc3vRRcd3TJ1qrv535ZVw//0nf0NkpBlgkpVlrihoWZUeHjrUtMasXQvp6a6tXUREpDpODSjffPMNWVlZtGzZEj8/P/z8/NixYwfjxo2jdevWAERHR1NQUMDBgwcrfW9WVhZRUVFVPu/48ePJzs4u2zIyMpxZttcq7RHr1+/4jpkzTffOtGlmHfsTBQXBu++aaTuLF1fu+gEiIqB/f3P/k09cVraIiMjvcmpAufXWW1mzZg2rV68u22JjY3n44Yf57LPPAOjZsyf+/v6kpKSUfd/evXtZu3Ytffv2rfJ5AwMDCQsLq7TVd/v3w6ZN5v755x/fGRAADz8Mx8NglTp0gHHjzP2HH4aCgkoPl45RrvDPIyIi4na1nsVz+PBhtmzZUvZ1eno6q1evJiIigpYtWxIZGVnpeH9/f6Kjozn77LMBCA8P584772TcuHFERkYSERHBQw89RJcuXcpm9cjv+/57c9uhA0TwG5Q0MhfjqYnHHoPXXzcjbGfNgrFjyx4aMgQef9ysr1JUBH61/g0RERE5c7VuQVmxYgU9evSgR48eADz44IP06NGDJ598ssbP8dJLL3HVVVdx/fXX069fP4KDg/noo4/w9fWtbTn1Vmn3Tt++wB13QPv28PXXNfvm0FB44glz/4UXoLCw7KEePUxXT04O/PSTU0sWERGpMYdlnTBS0gvk5OQQHh5OdnZ2ve3uGTAAli6Ff0/dy40PtTCDX3/5xQSVmjh6FFq1gk6d4J13zGyf466/Ht57D556CiZOdE39IiJS/9Tm81vX4vFCRUVmMTWAi349foW/Pn1qHk7ADJhNSzN9OSes9lu6RI3GoYiIiF0UULzQL7+YBpDQUGj2xVyzc8SI2j/RKWZNDRlibn/8EbKzT7NIERGRM6CA4oVSU83t8A5bcPz0k1m85LrrTv8Js7Iqrc7WqhUkJEBxcc2HtYiIiDiTAooXKg0oI/zfM3cGDTpla8jv2rEDWrQwAefXX8t2l7aiqJtHRETsoIDihUoDyrm//tfcufrq03+yVq2ge3ezHsqbb5btLl2ddunS039qERGR06WA4mWKimD1anM//6G/wJgxcMIVo2tt5Ehz+/bbZbsuvNDcpqXBgQNn9vQiIiK1pYDiZTZuNANkGzaE5ndeCjNmwJlem+j668Hf3ySftWsBaNasfFLQt9+e2dOLiIjUlgKKlynt3uneveYLx/6uyEi47DJz/513ynYPGGBulyxx0uuIiIjUkAKKl1mzBsBifP5Ecznj4mLnPPEtt5jbd94x66pQ3s2jcSgiIuJuCiheZu1a6Mh6Llv+tLmyn7MCyrBhEB4OmZmwbh1Q3oKyapXWQxEREfdSQPEya9fChRxv0ujb11zB2BkaNICFC2HvXujSBYDmzaFtW9OgUnrtHxEREXdQQPEiBw/C7t0wgOODQkqbOJxl4EAzHqWC0m4ejUMRERF3UkDxIqbnxeJiHxcFlIqOj0MpfQmNQxEREXdSQPEia9dCAptpVpIJgYFw3nnOf5HFi6FfP0hKAqB/f7M7NRXy853/ciIiIlVRQPEiaWkVxp/07m3GjThbYaEZcPL++1BSQps20LSpWWh25Urnv5yIiEhVFFC8yNq10IsV5ot+/VzzIoMHQ1iYGSz7ww84HGYsLmigrIiIuI8CipewLBNQxjKDDfNWw5/+5JoXCgw0U44B5s8HFFBERMT9FFC8RGYm/PYbWD5+tL6yG7Ru7boX+8MfzO38+WBZ9Oljvly2zAQlERERV1NA8RLHL5FDQoJrhp5UcumlEBwMO3bAypX06gV+fiYk7djh4tcWERFBAcVrrF0LtzGHV46NNDNtXCk4GIYONffnzycoCM45x3ypbh4REXEHBRQvsXYtDOcjLt7xhrnqsKvddBNcdRWcfz5AWTfP99+7/qVFREQUULzEhg1wHj+ZL1yx/smJ/vAHWLAArrgC0EBZERFxLwUUL7F/w35akmG+KO1vcaPSgPLzz5CX5/aXFxGRekYBxQv89hu0+O1nAEratDXrlLjL1q3w1lu0aAEtWpiLJy9f7r6XFxGR+kkBxQts2gTdWQ2AT4/u7nvhrCxo1w5uuw327lU3j4iIuI0CihfYuLE8oNC9u/teuFkzs6Q+wIcfKqCIiIjbKKB4gU2boBGHKMHh3oACZiYPwMKFpRN6+OknLdgmIiKupYDiBTZuhCv4iJmTc8y1ctzp6qvN7Zdf0q11Nv7+8OuvsH27e8sQEZH6RQHFC2zaZG7bdG3ohmVkT3D22dC+PRQW0uCrT8sacH76yb1liIhI/aKA4uFKSmDzZnP/rLNsKqJCN0/pEiw//mhTLSIiUi8ooHi4jAx45NjTfOO4gPjl/7GniNJunq+/pnevYkAtKCIi4loKKB5u40Y4nx/ob32LT262PUX06gUffghbt3JeH18AVq6EwkJ7yhERkbpPAcXDbdoEnVhnvujY0Z4ifHzMkvchISQkQHg4HD1afoVlERERZ6t1QFm6dCnDhw8nNjYWh8PBwoULyx4rLCzk0UcfpUuXLoSEhBAbG8ttt93Gnj17Kj1Hfn4+9957L02aNCEkJIQrrriCXbt2nfEPUxftSMspX+K+Qwd7i8FklXPPNffVzSMiIq5S64CSl5dHt27dmDFjxkmPHTlyhJUrV/LEE0+wcuVKPvjgAzZt2sQVxy84VyopKYkFCxYwb948vv32Ww4fPsywYcMoLi4+/Z+kjsr/eQMAR8KjISLC3mJeeAG6duW62O8ABRQREXEdv9p+w9ChQxk6dGiVj4WHh5OSklJp3/Tp0znvvPPYuXMnLVu2JDs7m9dff5233nqLwcfX9Hj77beJi4vj888/55JLLjmNH6PuCtiyHoCCth0JtrkWVq2CtDQubrsQ6KeZPCIi4jIuH4OSnZ2Nw+GgUaNGAKSmplJYWEhiYmLZMbGxsXTu3JllWkO9koICiDpgAop/N5vGn1R0fDZP69ULAIv16yE3196SRESkbnJpQDl27BiPPfYYI0aMIOz4FXgzMzMJCAigcePGlY6NiooiMzOzyufJz88nJyen0lYf7NwJxwhkl6MFwed2srscuOQSCAzEb/tWBkWvx7IgNdXuokREpC5yWUApLCzkxhtvpKSkhFdeeeV3j7csC4fDUeVjycnJhIeHl21xcXHOLtcjbdsGT/JXLu2YgWP0KLvLgdDQsqX274xcCGjBNhERcQ2XBJTCwkKuv/560tPTSUlJKWs9AYiOjqagoICDBw9W+p6srCyioqKqfL7x48eTnZ1dtmVkZLiibI+zdau5bdMGOEV4c7vjq8oOzF4IaKCsiIi4htMDSmk42bx5M59//jmRkZGVHu/Zsyf+/v6VBtPu3buXtWvX0rdv3yqfMzAwkLCwsEpbfbBtq7lkcJs2NhdS0RVXgMNBzK4VtCBDAUVERFyi1rN4Dh8+zJYtW8q+Tk9PZ/Xq1URERBAbG8u1117LypUr+fjjjykuLi4bVxIREUFAQADh4eHceeedjBs3jsjISCIiInjooYfo0qVL2aweMeKWvM1uHmXfmpuAF+0ux2jWDIYPpzAknKB5+WzeBXv2QGys3YWJiEhdUuuAsmLFCi666KKyrx988EEAbr/9diZOnMiiRYsA6F562dvjvvrqKwYOHAjASy+9hJ+fH9dffz1Hjx5l0KBBvPHGG/j6+p7mj1E3hexcTyx7KQw6ancplX34If5Ag7VAmunmKb2eoIiIiDPUOqAMHDgQy7JO+Xh1j5Vq0KAB06dPZ/r06bV9+XrDsqDpbxsBCOzW3uZqqta7N6QpoIiIiAvoWjwe6sABiC/aDEDE+WfZXE0VLItLY1bTjdWaySMiIk6ngOKhtm4uoR1mrE9Ax3Y2V1OF55/nD3/twZM8w/LlUFJid0EiIlKXKKB4qMzU3QRxjCKHH7RubXc5JxsyBIBL+R9FuUfYsMHmekREpE5RQPFQuStN987+0Hjwq/VQIdfr3h1atiSYowzmc003FhERp1JA8VC7Mn1ZwoXsa1P12jC2czjKRsZexUIFFBERcSoFFA/1vyMDGMgS1j70ht2lnNrxiwdewSJW/FBkczEiIlKXKKB4qG3bzG3btvbWUa3+/SluFEETDtBwzXcc9bDlWkRExHspoHig/HzYl1EAeNgy9yfy88PnyuEADC3+mJUrba5HRETqDAUUD7Rzewm/0ZidjpY0LdprdznVciQl8WSfFB5nktZDERERp/HA6SGSmbqLBI4QYBXgaNbU7nKq1707DYZB0fcooIiIiNOoBcUDlU4x3hfcxjOnGJ+gd29zq4AiIiLO4vmffvVQ0S8moPzWJIEWNtdSE+eencMLPMOgHZ+zb9dyolr4212SV9m2DVJSYN06+PVXsypvdDR06AADBkD79mZWt4hIfaKA4oH8t5sl7o/FJdhcSc2ERQfzR9+3iCzO4odZi4l65nK7S/J4xcXw3nswdSosX179sR07wt13w5/+BMHB7qlPRMRu6uLxQKH7TAuKI8E7Agp+fqw6+0YAgua/Y3Mxni81Fc4/H266yYQTX1/TUvLQQzBtmtkefhguvhgCA2H9enjgATOja/p0KNKSMyJSD6gFxQM1yzEtKMFdPfAigaeQe8XNsP7vnLXhQzh8GBo2tLskj2NZ8PLL8MgjUFgIYWEwbhz8+c/Q9BRjobOzYd48mDwZtm+H++6Dt96C2bOhUye3li8i4lZqQfEwxcXwVdGFLOFCIs4/y+5yaqzNDeeykbMIKjlCyb/ftbscj1NUZLppHnjAhJM//AE2boQnnzx1OAEID4dRo8yxr7xivl6+HM45B2bMMKFHRKQuUkDxMHv2wGhrJoP9ltDsvNZ2l1NjnTo7eNP/LgCOTZ9lczWepagIbrwRXn8dfHxMK8p775mBsDUVEGBaWtatg2HDoKAA7r0X/vhHtIKviNRJCigeZscOcxsXZ8YmeAs/P1jb83YK8Cc47Sf4+We7S/IIlgWjR8P8+WY8yfz5ppvmdGflNG8OixbBCy+YsPPmmzBoEPz2m3PrFhGxmwKKh9m1MQ9/CmjVyu5Kau/sC5oxm5F80fk+aNTI7nI8wuOPl7eczJtXdgHoM+JwmLErKSnQuDF8/z1ceCHs3n3mzy0i4ikUUDxM9LxpHKMB4/cl2V1KrZ1/Pozmn4zzfRmvTFhO9u9/m8GtAK++6pxwUtHFF8M330BsrOn66dcPNm927muIiNhFAcXD+GWk44OFf7PGdpdSa6UryqalQV6evbXY7ZdfzKBYgAkT4I47XPM6nTrBd99BQoLpHrzoIti61TWvJSLiTgooHibk13QAfNvF21xJ7TVvbraSEovNs7+FMWPMsqj1TF4eXHutub34Ynj6ade+XuvW8O23Jqzs3m1es3Qsk4iIt1JA8TCRudsBCOnsfQEFTCtKCHl0ePhyMy/2v/+1uyS3mzDBLK4WEwNz57pnsHOzZvD553DWWbBzp2lJychw/euKiLiKAooHsYqKiSncCUBET+8NKHk05JOWfzY7nn/e3oLc7Lvv4O9/N/dnz4aoKPe9dnQ0fPkltG0L6elwySWa3SMi3ksBxYMcTNuFP0XkE0Bsr1i7yzktpeNQns29D/z9zSjOH36wtyg3OXrUjDWxLBg50gQEd2ve3ISUFi3MOJhhw+DIEffXISJyphRQPMivP5nxJ7t8WxEY5J3/ND17mim1qXtjybvmVrPzmWfsLcpNnn0WNm0yXTtTp9pXR8uW8L//mZne338PN9yg6/eIiPfxzk/BOmpXbjhvcis/NhtudymnrWFD6NLF3P/2gvFmBbdPP4WlS+0tzMW2bTOLp4FZgt7uZWA6dYKPP4YGDcztn/6kZfFFxLsooHiQNb49uJ03WXThi3aXckZKu3m+3NkO7jLL3zN+fJ3+hBw3DvLzYfBguPpqu6sx+vWDd981LVqzZ5vBuyIi3kIBxYOUTg319jXOSgPKjz8CTzwBbdqYC8nUUSkpsHChma3z8sunv4y9K1xxBfzzn+Z+cnL5AF4REU/nZ3cBUi53w278aUqrVgF2l3JGSgPKihVQHBWL76ZN3nVhoVooLjatJwBjx0LHjvbWU5W77oLMTJMVk5LMlOQbb7S7KhGR6imgeJBJX/ZmFntZVrQC6GF3OaetQwcIC4OcHLOqbPfuFcJJbi6EhtpXnJP9+9/mZ2zUCJ580u5qTm3CBBNS/u//4LbboEkT0x0lNVNcDHv3mmnbOTnm17ikxORuX18ID4emTc15bdjQs1rRRLyVAoqnyM+naeEefLBo0r2F3dWcER8fc12exYvNuiDdux9/YOFCs/777Nlm/quXKygwrRIAjz4KERH21lMdh8N0P2VlwXvvmXEyS5bAOefYXZnn2bEDli+H1FRYtQq2bDGL3xUW1uz7GzQwa9G0a2cuQdChgznPnTqZmfciUjMKKB4ib+MuQrDII5jm3ZrYXc4Z69+/PKCMGXN859KlsH+/WSxk+XKvH2wzaxZs324WSLvvPrur+X2+vvDWW3DggFkrZehQ8+/Trp3dldnr2DH47DMzNTsl5dTXMvLzMyE0LMxsPj6mFaWoCA4dMr/aR46Y51u3zmwVBQZC167Qq5cZwDxggFmvRkSq5rAs75takZOTQ3h4ONnZ2YSFhdldjlPsnPMVLf94MZt8zuas4g12l3PGvvwSBg2CuDjz1ydg3rn79jV/lnbtai4g46XdPUePQnw87NtnVvT3pjHAOTnmw3H1ajN++bvvTMiqT4qLTSD5979h0SLTZVPK19e0+vXsaVo+OnQw/9axsb8/lOrIEdMVtHWrubL05s2mCzA1FbKzTz6+TRvzbzFgAAwc6PWZXeR31erz26qlJUuWWMOGDbNiYmIswFqwYEGlx0tKSqynnnrKiomJsRo0aGANGDDAWrt2baVjjh07Zo0dO9aKjIy0goODreHDh1sZGRk1riE7O9sCrOzs7NqW77HSHnrDssD6LmSI3aU4xeHDluXra1lgWTt3Vnhg507LiooyD1xwgWV56b/hyy+bH6FVK8vKz7e7mtrbu9ey2rQxP0P37l77z1Bru3db1jPPWFZcnPnZS7cWLSxrzBjL+vBD15yLkhLL2rzZsubNs6wHHrCsnj0ty8encg1gWa1bW9bIkZb15puWVYu3RBGvUZvP71p38eTl5dGtWzdGjhzJH/7wh5MenzJlClOnTuWNN97grLPO4m9/+xtDhgxh48aNhB7/azkpKYmPPvqIefPmERkZybhx4xg2bBipqan41tHZHr+nYItpZshp3NLmSpwjJAR69DAzeb77rsKskbg4+OgjGDLELIN/8cVmUES891x7KD8fpkwx9x97DAK8cNJVdLTpguvb17SkDBtm1tMLCbG7MtdYvRomT4b33zetJwCRkXDzzeZ3s3dv02XjKg6H6Upr186s7AumRWXZMjMWaMkS839l+3YzRGv2bHNMu3bmwo+lW31r6SpVXGy6JrOy4Ndfzf3Swcq5ueX3jx41XW6Fhea24uZwmDFAfn6Vb/39ze99w4bltyfeDw01W1iYuQ0K0kBodzijLh6Hw8GCBQu46qqrALAsi9jYWJKSknj00UcByM/PJyoqiueee45Ro0aRnZ1N06ZNeeutt7jh+P/UPXv2EBcXxyeffMIlNbiASV3s4lnZ627OSX2ND895mitTPXg6SC0kJZmBmWPGmNVVK0lNNRerOXDATH95+mk7Sjwts2bBqFGmyX/bNjO2wFutXGk++HJyTBfDf/8LwcF2V+UclmUycHKy6c4p1a+f6ZL7wx/MgFZPcfiw6fX86iuzpaaaMS4VtW9fHlYGDjQzh7zdsWPmyts7dpht505zm5FhulCzssz4Hk8ajODrWx5YSkNLdbfVPRYY6Blhx7LMwP+CAvNH2LFjZr+zx0nV5vPbqYNk09PTyczMJDExsWxfYGAgAwYMYNmyZYwaNYrU1FQKCwsrHRMbG0vnzp1ZtmxZlQElPz+f/Pz8sq9zcnKcWbZH+Llhf9aSj2/H8+wuxWn69zcB5bvvqniwZ0/zDvzCC+VTYcCMVvTxMc0vkZGe8T+3gsJC84EH8Mgj3h1OwIyxWLzYNGh9/TUMH24auLw9pPz0k1m8+Msvzdc+Pqbl4pFHKswq8zANG8Kll5oNTAvLN9+Yn+Grr+Dnn2HDBrPNnGmO6dzZtIL17Gm2zp0973fy0KHy8FFxKw0i+/bV7HkcDjNIuVkz89Zw4od9aKj5vS1tHanYUuLraz6AS1tWCgvL7+fnm7FDhw9DXl7l29KttKWmdKxScbH5uQ4dOvPz4+d38s8SGFj+M5Ruvr6V74MJsMXF5raqrfTnKw0dJ24V9xcUnFxbx44nD/Z2J6cGlMzMTACiTrjGfFRUFDuOL5OamZlJQEAAjRs3PumY0u8/UXJyMk970V/Yp+O94Nv5lNt5/SK7K3Gefv3M7Zo15i/0k8Jyq1YwfXr51yUlpmmidEndhg3NlfdCQszWrZtZyKPUyJGmBcbhKA8ypffj4+HFCpcMGDvWjF6sSkxM5SaeceNMW3sV0vc3Zvv212jWzMyY5vHHYePGqp83ONhMmyn1zDPmU6Yqvr7wn/+Uf/3cc+ZT9lTmzSufszptmvkkO5U33yzvu5k5Ez7/vNLDvYH0c8xf73d8OYsrr4xk0SII+ve/TJPKqUyfbpqRAObOhfnzT33sCy+Ud+PNn2+OP5VnnzVNBWAuJFTa31GVJ54oTxwpKeQ8/w/Wr4Pde+AeYKwDWrWGs86Chvc/At2PryL4zTfmvJ3K/ffDhRea+z/+WN6nV5XRo03CA/PvW93FMUeOLJ9iv2FDtdcfCL/pJoZde605PD2dgvse4tf9povj11/N/ynWmu0DrmEUN+PvDwPP3sOknHtpGHq8e6IhhDQEv9Le88svNzPpwCzscvfdp6538ODyEeB5eWYRnQqKSyD/GBzLh4yW/fmi6wNs3w4Z6UWMWXoDR45AYYULVcYe33w5j/d5tGz/e743EBZcRHAw5VuQaeGyunSFp54iMtJ8OHPrreWX6C4EDhzfAM4+GyZNKn/Bu+82P2NVWrc++T3i0PH3iMDjW+Txx46/R5SUmNPAQ+MgfXt54CntSiqEXP/GvDv4tbKupyt+eJxmhzZSVPHYQigqhiMEc1vRW/z2mynzCZ6hG1W/RxTjyw2Uv0c8wnOcx6nfI25kHkWY94j7mcYFnPo94jbepADzHjGamQzmc7b7tOFNv+dP+T1ucSaDXThhkOx3331nAdaePXsqHXfXXXdZl1xyiWVZlvXOO+9YAQEBJz3X4MGDrVGjRlX5OseOHbOys7PLtoyMjDo3SLZzZzNI7rPP7K7EueLja/FzHThgWbfcYkYKnjh6ECyrX7/Kx8fEVH0cWFa3bpWPTUg49bHt2lU+tlu3Ux67zzfGAst67rnjx/bte+rnDQ+v/LxDhpz6WD+/ysdeeeWpj4XKI3NHjKj+2IMHy4+9++5qj20XtMsCy+rf37KOjrq/+ufduLH8eR9/vPpjV60qP/bZZ6s/9ttvy4996aXqjz3+i7Vjh2W90W9W9cdWHND/zjvVH/v22+XHLlhQ/bH//Gf5sYsXV3/sSy+VH/vdd9Uf+7e/lR+7alW1x85tPd5q3Pj4rzObqj32/Rb3WyNGmEHBf7tnd7XHrjr3Luvxxy3r3nst6883Haz22He4qfzXmYJqj93S+Urrgw8sKzXVsvbvt6wSf/9THz94cOX/G+Hhpz62T5/Kx8bGnvpYF71HWDExlY+t5j2iJDzcysiwrPXrLeuHHyxr/zmnfo8o8vGzXn7Zsl580bz/bOp0ZbXn+PWZ+dbrr5tf5R39q3+P2LnmoJWZad4qCkcef4845xzLFVw6SLY60cdHcGVmZhITE1O2Pysrq6xVJTo6moKCAg4ePFipFSUrK4u+fftW+byBgYEEelq7pTMVFOCzYzf+NCcuzgtHXFajf39ITzfdPBV69aoWEVHe4nDsmGlJycoyf7Lk5ZnlOit68UWz37LM16X/3Uqfq6Knn656niec/LwTJpiWmROsSYMXXgkmPLzCtOKHHjp1G/WJo2fvuw+uuabqY08coTl6dHlbf1UqDiYfORIuuODUxwYFld+/+eZqV2eb07YRl11nWlPu3n0df09uT+NGpzi4WbPy+1dcYQZAn0rFjuxLL61+Vbu2bcvvX3xxeX9GFX6L6sCz40zDWpv8fnzPTLp3M+WUNu6U6dat/P6551b7vJxXoau1W7fqj+3fv/x++/bVH1vxPa5Nm5rX0KJFtcfe1KMHN55n/susWdqUT96fyb4syNpnfj2PHC0/Nm1XF5Ydb8AKJpwMTv28vyzvwNLl5n4AQZRUcayvj2kdzYtpxw1dTcNE6zgf1v4yk8hIaBwBDU54+27bujVtK/56z5hx8oCbij97RVOnVt0fAXBC6z3JyeWtLSeKjKz8tRPeI4CT+0ereY9wBARU/vGevg92Vf0e4evjw31/qrCj62jYfur3iDvu9oXSt4mokbDl1O8RcWcFmRYjgNtvhvPOMcsi28wlg2QfeOABHnnkEQAKCgpo1qzZSYNk3377ba6//noA9u7dS4sWLertINkj3/9McN/u7KMZwTn7vHVpkCr985/ms/bii+GLL+yu5swMGmTGAzz0EDxvc8unK6WlmfHLe/eazPHxx2bZGk9y+LDpoXn++eNdHZi1RCZPNqsYSznLMmMl9u41lzvYu9dsOTnlYyzy8swMGIfDZOXSntLg4PLxHaVbZKTJAdHR5rZxY48bKiYezKWDZA8fPsyWLVvKvk5PT2f16tVERETQsmVLkpKSmDRpEgkJCSQkJDBp0iSCg4MZMWIEAOHh4dx5552MGzeOyMhIIiIieOihh+jSpQuD6+nFQX5bvZNgYI9vHD3qUDiB8j8uf/jBDErz1qW+16wx4cTXF+691+5qXKtLFzP99ZJLYNMm6NMHXnsNbrrJ7srMH86vvgp//Wv5H6Xdu5tgkpioD8qqOBwmRDRu7JkXsxQ5pdr2H3311VcWcNJ2++23W5ZVvlBbdHS0FRgYaF144YVWWlpapec4evSoNXbsWCsiIsIKCgqyhg0bZu2stJpX9eraQm2/jJ1hWWClhF5tdylOV1xsWRERpu/lhx/srub0jRxpfobrr7e7EvfZv7/ysJmkJPsWpSsutqy5c8sXlwNzf+5c85iIeIfafH5rqXsPkHbZo3T5dAoLW97HVTtetrscp7v6anOdwMmTzUX1vM2+fdCypfnr/fvv61cXQnGxmShTOrW6WzeYM6fycA5Xv/5775mJGWlpZl9UlFk65667vHORPJH6rDaf3y5cO1FqyrErA4CC6LqxiuyJLjo+dfqrr+yt43TNnGnCSe/e9SucgOnSmjQJFiwwYw9+/tlc7G78+PKxH66Qnw//+pe5Ds5NN5lwEhpquna2bIF77lE4EanrFFA8QIMss8y9o1XdDCgDB5rbb7+t+SXrPcWxY+ZigAAPPGBvLXa66iqzYNNVV5m1HCZPhoQEM/niVJMkTsf27WZpmbg4uPNOc7G9iAgzwWLHDvjLX8zaHiJS9ymgeICwbBNQGpxVNwNK587mr++8PLN4rDeZO9csihUXZ5ZGr8+iouCDD+DDD004ycoyA4ZbtTKzLn/55fSed+9eM0144EAz8zY52Zzz2FgzS2fHDtOlc8LajiJSxzl1HRQ5PR+EjST4WDpxXdvYXYpL+PiYKaAffGC6ebylm8Syyhcbvffe46tY1nMOh1lj5NJLzcyeF14w69xMmmS2zp1N0OjXzyzqGR9vumZ8fU032cGDJnCsX2/C6pIl5WNLSg0aZLpwhg/33llfInLmNEjWAzRubNYpWLeu7k4DnD7drFOWmGgut+MNPv/crGAeEgK7dkGjRnZX5HmKiswA6DlzzAX5ioqqPs7Xt/wqwlXp3Ruuuw6uvda0yIhI3WTbxQKl9g4fLr/gVHULcXq7iuNQCgq8Y4DjSy+Z25EjFU5Oxc/PhIprrzULa375pbnMzfLl5krPWVnmuNJw4nCYrqJOncx2wQXmkjcVF6UVEQG1oNhu83dZJPbPIze0OftzvOBT+zSVlJgPpv37TUgpvZCgp9qwwcwgcTjMYmXt2tldkXc6etSMPcrPNyvuh4dXXqVfROoXTTP2JrNnk04bZvvcaXclLuXjUz7d+ISL6Xqkv//d3A4frnByJoKCzCU9mjc3s3EUTkSkphRQbFa0zczgORxZ9zveSy8W6OljUH77zYypAEhKsrUUEZF6SwHFZr67TUApjKnDA1COKw0oP/5YPu7GE82aZdb26NatfOyMiIi4lwKKzYIOmFVkfVvXzTVQKmrZ0lyNvqTEDKb0RIWFZvExMAuz6eJzIiL2UECxWaOcur1I24k8vZvnvfdg924zoPfGG+2uRkSk/lJAsVNeHqGFBwFo1KXud/EAXHKJuf3sM7MQmiexrPKpxWPGQGCgvfWIiNRnCih22rMHgFwaEnO2d0+XrqkBA8waKDt2mIu+eZLvvoMVK0wwGT3a7mpEROo3BRQbHXGE8CyP8wr30KKF3dW4R0gI9O9v7v/vf/bWcqLS1pNbb4WmTe2tRUSkvlNAsdFuK5a/8Cx/a/gcXr7eXK1ceqm5/fhje+uoaNs2s2Q7aGqxiIgnUECx0e7d5jY21t463O2KK8ztV19Bdra9tZR6+WUzu+iSS8wS7CIiYi8FFBtlr04nnm20jsm3uxS3OvtssxUWekY3z8GD8Prr5v64cfbWIiIihgKKjc6e/RjbaMsth2faXYrbXXmluf3wQ3vrAHj1VXO9mC5dYPBgu6sRERFQQLFVwH7Tx+No3tzmStyvNKB88olpSbFLYWH5dXcefFALs4mIeAoFFBuFZJtpxoHx9WwQCtC7NzRrZsagLF1qXx3/+Y8ZCxQdDTfdZF8dIiJSmQKKXSyLRkdMQAk5q/61oPj6misFg33dPJYFU6ea+2PHamE2ERFPooBil99+I9Ayg2MjOsXYXIw9rrrK3L7/PhQXu//1lyyBlSshKEgLs4mIeBoFFJtYu8z4k19pQkzr+vmne2IiNG4Me/fC11+7//UnTza3f/wjREa6//VFROTUFFBskrPBdO/sIZaY+tmAQkAAXHeduT93rntfe/lycz0gX194+GH3vraIiPw+BRSb7A1oxd+YwPyQ2wkIsLsa+4wYYW7ffx+OHXPf6z77rLm9+WaIj3ff64qISM0ooNgkvUEHnuBvfJTwoN2l2OqCC6BFC8jJMVOO3SEtzQzMdThg/Hj3vKaIiNSOAopN6usy9yfy8Smf3vvOO+55zUmTzO1110H79u55TRERqR0FFJsUrF5PG7bSMrrA7lJsd/PN5vbjj2H/fte+1qZN8O675v6ECa59LREROX0KKDa5/N3b2Eo7Lji62O5SbNetG/TsCQUF8MYbrn2tp54y658MHw5du7r2tURE5PQpoNikYa6ZxdOgTT3v4zmudB2Sf/7TXFXYFVauhHnzzNiTv/7VNa8hIiLOoYBih6IiGuXvAyCsvQIKwI03QlgYbNkCX3zhmtd47DFzO2KEabURERHPpYBih3378KWEInxp0rGZ3dV4hIYN4bbbzP0XX3T+83/+OaSkgL+/Wk9ERLyB0wNKUVERf/nLX4iPjycoKIg2bdrwzDPPUFKh3d6yLCZOnEhsbCxBQUEMHDiQdevWObsUj1W0w0zh2UsMzeOUEUs98ICZ1fPZZ7B6tfOet6jIXKkY4M9/1ronIiLewOmfjs899xz/+Mc/mDFjBr/88gtTpkzh+eefZ/r06WXHTJkyhalTpzJjxgyWL19OdHQ0Q4YMITc319nleKSD68z4k72OWC2xXkGbNnDDDeb+lCnOe97p083aJxER8MQTznteERFxHacHlO+//54rr7ySyy+/nNatW3PttdeSmJjIihUrANN6Mm3aNCZMmMA111xD586dmTNnDkeOHGGuu9c7t8nhjaYF5bcGzfFRA0oljzxibt99F9auPfPn27ULnnzS3J8yBZo0OfPnFBER13P6x2P//v354osv2LRpEwA///wz3377LZdddhkA6enpZGZmkpiYWPY9gYGBDBgwgGXLllX5nPn5+eTk5FTavFl6ZC/+yl/4sfk1dpficbp3NwuolZSUD2o9XZYFSUlw+DD06QMjRzqjQhERcQenB5RHH32Um266ifbt2+Pv70+PHj1ISkripuPLhWZmZgIQFRVV6fuioqLKHjtRcnIy4eHhZVtcXJyzy3ardQ178yR/ZW33W+wuxSM9+yz4+cF//3tmM3rmzoX5880FAWfORK1VIiJexOlv2e+++y5vv/02c+fOZeXKlcyZM4cXXniBOXPmVDrO4XBU+tqyrJP2lRo/fjzZ2dllW0ZGhrPLdistc1+9hITydVFGjYIjR2r/HOnpcM895v7EiZpWLCLibZweUB5++GEee+wxbrzxRrp06cKtt97KAw88QHJyMgDR0dEAJ7WWZGVlndSqUiowMJCwsLBKmzcLTFtBG7bSIrrI7lI81rPPmosIbt1a+yXpDx+GK680FyDs2/fMu4pERMT9nB5Qjhw5gs8Jbem+vr5l04zj4+OJjo4mJSWl7PGCggKWLFlC3759nV2ORxq3OJGttKO97ya7S/FYYWFmVVmAadPKr5/zewoKzEJsaWkQHW2+z8/PZWWKiIiLOD2gDB8+nGeffZb//ve/bN++nQULFjB16lSuvvpqwHTtJCUlMWnSJBYsWMDatWv54x//SHBwMCNGjHB2OZ7n6FHCig4C0KiD+niqc9ll8PDD5v7IkbBkSfXH5+ebFWk/+ggCA2HBAtMKIyIi3sfpf1tOnz6dJ554gnvuuYesrCxiY2MZNWoUT5bO9QQeeeQRjh49yj333MPBgwfp3bs3ixcvJjQ01NnleJ49Zg2UPIKJOivc5mI8X3IyrFsHn3wCl1wCs2ebEHLicKVt28waKitWmHCycCGcf74tJYuIiBM4LMuy7C6itnJycggPDyc7O9vrxqMc+d9SgocOYDPtiMndTMOGdlfk+Y4eNd02CxearxMT4Y474KyzICsLPvwQXn/ddO9ERJhuncGDbS1ZRESqUJvPb/XOu9mh9XsIBvb5xpKgcFIjQUHw3nvmGjqTJ8PixWY70cUXw7/+Ba1aub9GERFxLgUUN8vbZOYYHwppbnMl3sXPD55+Gm65BV57DT79FA4cMOGlf3+zf9Cgk7t+RETEOymguFnhTjMG5UgjDZA9HQkJ8NxzZhMRkbpLAcXN1sQOZT7BBCZcYHcpIiIiHksBxc2+DxnM3xnMY+faXYmIiIjn0tVJ3EzL3IuIiPw+BRR3siyabVhKW7bQPKbE7mpEREQ8lrp43OnQIV5ZNwCAn5odBRrYW4+IiIiHUguKG5VkmP6dA0QQE69wIiIicioKKG6Us8FMMd5Nc45f1FlERESqoIDiRtm/mICyPyAWf3+bixEREfFgCihudGyr6eLJDdUUHhERkeoooLhR0fFVZI9GaJl7ERGR6iiguJFvpmlBKYlSC4qIiEh1NM3Yjb5scRv/2diV6M7n212KiIiIR1NAcaP/Bl7DJ1zDa73srkRERMSzqYvHjUqXuW+uISgiIiLVUguKu+Tl0Wr7Cg7TnNjYdnZXIyIi4tEUUNyk4Odf+DB7ILuJpUHz3XaXIyIi4tHUxeMmB9eZKcZ7HbFERNhcjIiIiIdTQHGTw5tMq8lvQc1xOGwuRkRExMMpoLhJ/jbTgpIXpjVQREREfo8CiptYu01AyW+iKTwiIiK/RwHFTfz2mS4eK0YtKCIiIr9HAcVNgg6ZFhS/VmpBERER+T2aZuwmb0c/TOGhLXTv1sHuUkRERDyeAoqbzCm5lU3A113srkRERMTzqYvHDSyrfJn7WA1BERER+V0KKG6QuzWLXnlf04rtug6PiIhIDSiguEHOoq/4mouY63sbwcF2VyMiIuL5FFDc4MhmM4PnUIiaT0RERGpCAcUNCrebAShHGmkAioiISE0ooLjDHtOCUtRMAUVERKQmXBJQdu/ezS233EJkZCTBwcF0796d1NTUsscty2LixInExsYSFBTEwIEDWbdunStK8QgB+01A0QhZERGRmnF6QDl48CD9+vXD39+fTz/9lPXr1/Piiy/SqFGjsmOmTJnC1KlTmTFjBsuXLyc6OpohQ4aQm5vr7HI8Qki26eIJjFcLioiISE04faG25557jri4OGbPnl22r3Xr1mX3Lcti2rRpTJgwgWuuuQaAOXPmEBUVxdy5cxk1apSzS7KXZdH4qGlBCT1bAUVERKQmnN6CsmjRInr16sV1111Hs2bN6NGjB6+++mrZ4+np6WRmZpKYmFi2LzAwkAEDBrBs2bIqnzM/P5+cnJxKm9coKWFi2Es8zZM06qQuHhERkZpwekDZtm0bM2fOJCEhgc8++4zRo0dz33338eabbwKQmZkJQFRUVKXvi4qKKnvsRMnJyYSHh5dtcXFxzi7bZYrx5cXcPzGRp4ltG2R3OSIiIl7B6QGlpKSEc845h0mTJtGjRw9GjRrF3XffzcyZMysd53A4Kn1tWdZJ+0qNHz+e7Ozssi0jI8PZZbvMvn1QXAw+PnBCJhMREZFTcHpAiYmJoWPHjpX2dejQgZ07dwIQHR0NcFJrSVZW1kmtKqUCAwMJCwurtHmL/cvTGcDXnNM0A19fu6sRERHxDk4PKP369WPjxo2V9m3atIlWrVoBEB8fT3R0NCkpKWWPFxQUsGTJEvr27evscmzn9/48vuYinix+0u5SREREvIbTZ/E88MAD9O3bl0mTJnH99dfz008/MWvWLGbNmgWYrp2kpCQmTZpEQkICCQkJTJo0ieDgYEaMGOHscmxXlGFm8ORHaAaPiIhITTk9oJx77rksWLCA8ePH88wzzxAfH8+0adO4+eaby4555JFHOHr0KPfccw8HDx6kd+/eLF68mNDQUGeXYzvfTLMGSnGUAoqIiEhNOSzLsuwuorZycnIIDw8nOzvb48ejbG16Pm33/8j7Ny/g2revsrscERER29Tm81vX4nGxsFzTghLUVi0oIiIiNaWA4kolJTTO3wtAaHst0iYiIlJTCiiulJWFH8WU4KBJJy2CIiIiUlNOHyQr5Y4QTBL/pBGHmNBKp1pERKSm9KnpQnsOh/EqfyI4GJ7z7LG8IiIiHkVdPC6024yPpXlzOMUq/iIiIlIFtaC4UN73axjIASKatAdi7C5HRETEa6gFxYVaLJjOV1zMjYdfs7sUERERr6KA4kJ+WaaPx2quKcYiIiK1oYDiQsEHzXV4AlppkTYREZHaUEBxofC846vIJqgFRUREpDYUUFwlP5/GRfsBaNxRLSgiIiK1oYDiItYes8T9MQKJ6hBhczUiIiLeRQHFRbJ/MeNP9hBLTKwWQREREakNrYPiIrsD4nmIVwlp6MPLAXZXIyIi4l0UUFxkR0EMr3MX3dvZXYmIiIj3URePi5Qucx+r8bEiIiK1phYUF/H94TsuIp+zI7sCTewuR0RExKuoBcVF+n36F75kEP3yFttdioiIiNdRQHGRkBwziycwXn08IiIitaWA4gqWRcRRMwil4dlaRVZERKS2FFBcITeX4JI8ACI6xdhcjIiIiPdRQHGBgnTTepJNGDEJDW2uRkRExPsooLjAwXWlq8g2p4km8IiIiNSaAooL5PxiWlAONIjFoVXuRUREak3roLjA5mb9eI5Xadoqkv52FyMiIuKFFFBcYFNxW16nLdd1tbsSERER76QuHhfIyDC3cXH21iEiIuKt1ILiAk2Wf8LFBNK26blAmN3liIiIeB21oLjAnT/+iS8YzNlstLsUERERr6SA4mzFxUQUZALQuEsLm4sRERHxTgooTla8Zx9+FFOEL1FdmtldjoiIiFdSQHGyAz/vAmAvMUQ397W5GhEREe/k8oCSnJyMw+EgKSmpbJ9lWUycOJHY2FiCgoIYOHAg69atc3UpbnFwrVmk7deA5vgqn4iIiJwWlwaU5cuXM2vWLLp2rbwgyJQpU5g6dSozZsxg+fLlREdHM2TIEHJzc11Zjlsc2WwCSk5DXcVYRETkdLksoBw+fJibb76ZV199lcaNG5fttyyLadOmMWHCBK655ho6d+7MnDlzOHLkCHPnznVVOW5TuN0ElKORCigiIiKny2UBZcyYMVx++eUMHjy40v709HQyMzNJTEws2xcYGMiAAQNYtmxZlc+Vn59PTk5Opc1TLYm9iTt5ja09b7C7FBEREa/lkoXa5s2bx8qVK1m+fPlJj2Vmmim4UVFRlfZHRUWxY8eOKp8vOTmZp59+2vmFusDy/K68R1emnW93JSIiIt7L6S0oGRkZ3H///bz99ts0aNDglMc5TrjMr2VZJ+0rNX78eLKzs8u2jNK15D1QaWkttASKiIjIaXN6C0pqaipZWVn07NmzbF9xcTFLly5lxowZbNxoVlfNzMwkJiam7JisrKyTWlVKBQYGEhgY6OxSnc+yOHfjO4QQTcvoC4EAuysSERHxSk4PKIMGDSItLa3SvpEjR9K+fXseffRR2rRpQ3R0NCkpKfTo0QOAgoIClixZwnPPPefsctyq6Lcc/n7wVgAyow+jgCIiInJ6nB5QQkND6dy5c6V9ISEhREZGlu1PSkpi0qRJJCQkkJCQwKRJkwgODmbEiBHOLset9v+8m2jgEOE0bR1idzkiIiJey5arGT/yyCMcPXqUe+65h4MHD9K7d28WL15MaGioHeU4zW9pJqBk+TenkRZpExEROW1uCShff/11pa8dDgcTJ05k4sSJ7nh5tzm8yayBcihEa6CIiIicCV2Lx4kK0k1AOdJYAUVERORMKKA4kWO3CShFUZpjLCIiciYUUJwo8FdzJWOfOLWgiIiInAkFFCf6Z8R47uQ1ii+8yO5SREREvJots3jqqk8P9WE3fRjd2+5KREREvJtaUJykqAj27jX34+LsrUVERMTbqQXFSTI3HOKWkg/Z49uSZs3UxSMiInImFFCc5OD3G5jDH9lNS3x8qr4qs4iIiNSMunicJPcXM4Pnt2DN4BERETlTCihOcmxzBgBHGmsNFBERkTOlgOIkVoYJKAUxLW2uRERExPspoDhJ4D4TUHxbawqPiIjImVJAcZLQbBNQgs9SQBERETlTCihOYFnQ9JgJKI27KqCIiIicKU0zdoKDB+E2aw6t2MGM/mfZXY6IiIjXU0Bxgh074AsGExUFr0fbXY2IiIj3UxePE+w4vi5bq1b21iEiIlJXqAXFCXK/X8vtrCAqrAvQ0+5yREREvJ5aUJyg0bJPeIOR3LD7JbtLERERqRMUUJzAd4+ZwWO10AweERERZ1BAcYKgAyagBLRVQBEREXEGBRQnaHzYBJTQjgooIiIizqCAcoZycyG22ASUJj0UUERERJxBAeUM7dx0jGb8CkDDDgooIiIizqCAcoayVu4C4KgjCCIibK5GRESkblBAOUObD8eQyGfM6PkGOBx2lyMiIlInaKG2M7Q1M4QUEunQ1+5KRERE6g61oJyhrVvNbZs29tYhIiJSl6gF5Qw1X/kRt3OALiEXAkopIiIizqCAcgYsC67YOYNBLGbPvn+hgCIiIuIc6uI5A/v3Q4vi7QA06dXa1lpERETqEgWUM7B1cwmt2AFAwFmt7S1GRESkDlFAOQO7V2XRgHyK8YEWLewuR0REpM5wekBJTk7m3HPPJTQ0lGbNmnHVVVexcePGSsdYlsXEiROJjY0lKCiIgQMHsm7dOmeX4nKHVm83tyEtwN/f3mJERETqEKcHlCVLljBmzBh++OEHUlJSKCoqIjExkby8vLJjpkyZwtSpU5kxYwbLly8nOjqaIUOGkJub6+xyXCp/43YA8pq2trUOERGRusbps3j+97//Vfp69uzZNGvWjNTUVC688EIsy2LatGlMmDCBa665BoA5c+YQFRXF3LlzGTVqlLNLchmfndsBKG7Z2tY6RERE6hqXj0HJzs4GIOL4dWrS09PJzMwkMTGx7JjAwEAGDBjAsmXLqnyO/Px8cnJyKm2eYOaR2xnCYo7edZ/dpYiIiNQpLg0olmXx4IMP0r9/fzp37gxAZmYmAFFRUZWOjYqKKnvsRMnJyYSHh5dtcXH2XzU4Lw/W/BrD5wwhZlhPu8sRERGpU1waUMaOHcuaNWv497//fdJjjhMurGdZ1kn7So0fP57s7OyyLSMjwyX11sa2bea2cWOziYiIiPO4bCXZe++9l0WLFrF06VJaVJiCGx0dDZiWlJiYmLL9WVlZJ7WqlAoMDCQwMNBVpZ6WrVssnuCv+IS3gvwbwcPqExER8WZOb0GxLIuxY8fywQcf8OWXXxIfH1/p8fj4eKKjo0lJSSnbV1BQwJIlS+jb13suCbx39T6e4Sme3HEHnKLlR0RERE6P01tQxowZw9y5c/nwww8JDQ0tG1cSHh5OUFAQDoeDpKQkJk2aREJCAgkJCUyaNIng4GBGjBjh7HJcJjdtOwA5oc1pFBBgbzEiIiJ1jNMDysyZMwEYOHBgpf2zZ8/mj3/8IwCPPPIIR48e5Z577uHgwYP07t2bxYsXExoa6uxyXKZoy3YAjkW1trUOERGRusjpAcWyrN89xuFwMHHiRCZOnOjsl3cb34ztADhat7K3EBERkTpI1+I5DUePQvghc5HAhp1b21uMiIhIHaSAcho2b4ZWbAcguGNrW2sRERGpixRQTsOGDdAGsxCKI761vcWIiIjUQQoop2HDBhjKp7yY+D/o1cvuckREROocly3UVpdt3AjptKHo4jYQbnc1IiIidY9aUE7Dhg3mtn17e+sQERGpq9SCUkslJRC17kue5Bt6ZA8ABtpdkoiISJ2jFpRa2r0bLs7/hKeZSPPUD+0uR0REpE5SQKmlDRugLVsB8D2rnc3ViIiI1E0KKLWUlgbt2GK+aNvW3mJERETqKAWUWkr7uaSsBYV2akERERFxBQWUWtq7ci/BHKXExxda6To8IiIirqCAUgtFRVC80XTvFDdvBf7+NlckIiJSNymg1MKWLRBfuBEAv05n21yNiIhI3aV1UGohLQ3+xR381vUi3n+hwO5yRERE6iwFlFpYswaK8aPRuQnQye5qRERE6i518dRCWpq57dLF3jpERETqOgWUWlj70xHe4hauSHvWjJgVERERl1BAqaE9e6Dh3k3cwju0/mAq+PraXZKIiEidpYBSQ8uXQ3vMZYwdHdqDw2FzRSIiInWXAkoNVQwodOhgbzEiIiJ1nAJKDf30U4WA0r69vcWIiIjUcQooNWBZpgWlC8en8agFRURExKUUUGpg82Y4eugYZ2NWkaV7d1vrERERqeu0UFsNLFmCuYKxwwERkRAba3dJIiIidZpaUGrgiy9gPZ2YNP4w/PijZvCIiIi4mALK7ygpgS+/NPcHXhIIbdvaW5CIiEg9oIDyO9auhV9/heBgOP98u6sRERGpHxRQfscXXwBYLAu8iIA/3wkHDthdkoiISJ2ngPI7Fi+Gluyk28Gv4a23IDTU7pJERETqPAWUahw6ZFpQzmW52dGpEwQE2FqTiIhIfaCAUo1Fi6CwEIZHLDM7+va1tyAREZF6QgGlGrNnm9vBQd+ZOwooIiIibmFrQHnllVeIj4+nQYMG9OzZk2+++cbOcipZvx6+/hqCOErsvpVmZ79+ttYkIiJSX9gWUN59912SkpKYMGECq1at4oILLmDo0KHs3LnTrpIq+etfze0D/ZfjKCqCmBho1creokREROoJ2wLK1KlTufPOO7nrrrvo0KED06ZNIy4ujpkzZ9pVUpmFC2HePHP/jmtzoF0703qiFWRFRETcwpaAUlBQQGpqKomJiZX2JyYmsmzZspOOz8/PJycnp9LmCmvWwCWXwDXXmK+TkqDt/cPM1QLnznXJa4qIiMjJbAko+/fvp7i4mKioqEr7o6KiyMzMPOn45ORkwsPDy7a4uDiX1BUWZtY9sSwYMQImT67woL+/S15TRERETmbrIFnHCV0mlmWdtA9g/PjxZGdnl20ZGRkuqad1a5g1ywyQfecdCDx6CAoKXPJaIiIicmq2BJQmTZrg6+t7UmtJVlbWSa0qAIGBgYSFhVXaXOXuu6FDh+NfTJgAkZEwY4bLXk9EREROZktACQgIoGfPnqSkpFTan5KSQl9PWWukqAjefx8OHzaDZEVERMRt/Ox64QcffJBbb72VXr160adPH2bNmsXOnTsZPXq0XSVV9r//QVaWaUEZNMjuakREROoV2wLKDTfcwIEDB3jmmWfYu3cvnTt35pNPPqGVp6w1Mm2auR05UgNkRURE3MxhWZZldxG1lZOTQ3h4ONnZ2a4Zj/Lpp3DZZeDnZ6YYt27t/NcQERGpZ2rz+a1r8Zxo2TK44QZz/777FE5ERERsoIByopYtzcDYfv3gmWfsrkZERKResm0Misdq0QI+/tgMjA0MtLsaERGRekkBpSqXXWZ3BSIiIvWaunhERETE4yigiIiIiMdRQBERERGPo4AiIiIiHkcBRURERDyOAoqIiIh4HAUUERER8TgKKCIiIuJxFFBERETE4yigiIiIiMdRQBERERGPo4AiIiIiHkcBRURERDyOV17N2LIsAHJycmyuRERERGqq9HO79HO8Ol4ZUHJzcwGIi4uzuRIRERGprdzcXMLDw6s9xmHVJMZ4mJKSEvbs2UNoaCgOh8Opz52Tk0NcXBwZGRmEhYU59bnrEp2nmtF5qhmdp5rReaoZnaeaseM8WZZFbm4usbGx+PhUP8rEK1tQfHx8aNGihUtfIywsTL/YNaDzVDM6TzWj81QzOk81o/NUM+4+T7/XclJKg2RFRETE4yigiIiIiMdRQDlBYGAgTz31FIGBgXaX4tF0nmpG56lmdJ5qRuepZnSeasbTz5NXDpIVERGRuk0tKCIiIuJxFFBERETE4yigiIiIiMdRQBERERGPo4BSwSuvvEJ8fDwNGjSgZ8+efPPNN3aXZLulS5cyfPhwYmNjcTgcLFy4sNLjlmUxceJEYmNjCQoKYuDAgaxbt86eYm2SnJzMueeeS2hoKM2aNeOqq65i48aNlY7ReYKZM2fStWvXskWh+vTpw6efflr2uM5R1ZKTk3E4HCQlJZXt07mCiRMn4nA4Km3R0dFlj+scldu9eze33HILkZGRBAcH0717d1JTU8se99RzpYBy3LvvvktSUhITJkxg1apVXHDBBQwdOpSdO3faXZqt8vLy6NatGzNmzKjy8SlTpjB16lRmzJjB8uXLiY6OZsiQIWXXS6oPlixZwpgxY/jhhx9ISUmhqKiIxMRE8vLyyo7ReYIWLVowefJkVqxYwYoVK7j44ou58sory94IdY5Otnz5cmbNmkXXrl0r7de5Mjp16sTevXvLtrS0tLLHdI6MgwcP0q9fP/z9/fn0009Zv349L774Io0aNSo7xmPPlSWWZVnWeeedZ40ePbrSvvbt21uPPfaYTRV5HsBasGBB2dclJSVWdHS0NXny5LJ9x44ds8LDw61//OMfNlToGbKysizAWrJkiWVZOk/Vady4sfXaa6/pHFUhNzfXSkhIsFJSUqwBAwZY999/v2VZ+n0q9dRTT1ndunWr8jGdo3KPPvqo1b9//1M+7snnSi0oQEFBAampqSQmJlban5iYyLJly2yqyvOlp6eTmZlZ6bwFBgYyYMCAen3esrOzAYiIiAB0nqpSXFzMvHnzyMvLo0+fPjpHVRgzZgyXX345gwcPrrRf56rc5s2biY2NJT4+nhtvvJFt27YBOkcVLVq0iF69enHdddfRrFkzevTowauvvlr2uCefKwUUYP/+/RQXFxMVFVVpf1RUFJmZmTZV5flKz43OWznLsnjwwQfp378/nTt3BnSeKkpLS6Nhw4YEBgYyevRoFixYQMeOHXWOTjBv3jxWrlxJcnLySY/pXBm9e/fmzTff5LPPPuPVV18lMzOTvn37cuDAAZ2jCrZt28bMmTNJSEjgs88+Y/To0dx33328+eabgGf/Pnnl1YxdxeFwVPrasqyT9snJdN7KjR07ljVr1vDtt9+e9JjOE5x99tmsXr2aQ4cOMX/+fG6//XaWLFlS9rjOEWRkZHD//fezePFiGjRocMrj6vu5Gjp0aNn9Ll260KdPH9q2bcucOXM4//zzAZ0jgJKSEnr16sWkSZMA6NGjB+vWrWPmzJncdtttZcd54rlSCwrQpEkTfH19T0qLWVlZJ6VKKVc6Yl7nzbj33ntZtGgRX331FS1atCjbr/NULiAggHbt2tGrVy+Sk5Pp1q0bL7/8ss5RBampqWRlZdGzZ0/8/Pzw8/NjyZIl/P3vf8fPz6/sfOhcVRYSEkKXLl3YvHmzfp8qiImJoWPHjpX2dejQoWwCiCefKwUUzJtmz549SUlJqbQ/JSWFvn372lSV54uPjyc6OrrSeSsoKGDJkiX16rxZlsXYsWP54IMP+PLLL4mPj6/0uM7TqVmWRX5+vs5RBYMGDSItLY3Vq1eXbb169eLmm29m9erVtGnTRueqCvn5+fzyyy/ExMTo96mCfv36nbTswaZNm2jVqhXg4e9Pdo3O9TTz5s2z/P39rddff91av369lZSUZIWEhFjbt2+3uzRb5ebmWqtWrbJWrVplAdbUqVOtVatWWTt27LAsy7ImT55shYeHWx988IGVlpZm3XTTTVZMTIyVk5Njc+Xu8+c//9kKDw+3vv76a2vv3r1l25EjR8qO0XmyrPHjx1tLly610tPTrTVr1liPP/645ePjYy1evNiyLJ2j6lScxWNZOleWZVnjxo2zvv76a2vbtm3WDz/8YA0bNswKDQ0te8/WOTJ++ukny8/Pz3r22WetzZs3W++8844VHBxsvf3222XHeOq5UkCp4P/+7/+sVq1aWQEBAdY555xTNk20Pvvqq68s4KTt9ttvtyzLTFF76qmnrOjoaCswMNC68MILrbS0NHuLdrOqzg9gzZ49u+wYnSfLuuOOO8r+fzVt2tQaNGhQWTixLJ2j6pwYUHSuLOuGG26wYmJiLH9/fys2Nta65pprrHXr1pU9rnNU7qOPPrI6d+5sBQYGWu3bt7dmzZpV6XFPPVcOy7Ise9puRERERKqmMSgiIiLicRRQRERExOMooIiIiIjHUUARERERj6OAIiIiIh5HAUVEREQ8jgKKiIiIeBwFFBEREfE4CigiIiLicRRQRERExOMooIiIiIjHUUARERERj/P/uLE4MSdjgw4AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_pred = PINN.test()\n",
    "plt.plot(t,x_true,'b')\n",
    "plt.plot(t,x_pred,'r--')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "stan_1D_FODE_tune.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
