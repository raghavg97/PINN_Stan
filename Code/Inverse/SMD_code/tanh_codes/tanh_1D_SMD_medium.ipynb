{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8byrnUmNKGR",
    "outputId": "510f67c6-b225-41ba-e78a-d0a6e54ded85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uoNYpDzlNKGV"
   },
   "outputs": [],
   "source": [
    "tstart= 0 \n",
    "tstop= 60 \n",
    "\n",
    "increment = 0.1 \n",
    "# Initial condition\n",
    "x_init= [0,0] \n",
    "\n",
    "t = np.arange(tstart,tstop+1,increment)\n",
    "\n",
    "c = 1 # Damping constant \n",
    "k = 0.05 # Stiffness of the spring\n",
    "m = 5 # Mass \n",
    "F0 = 100\n",
    "\n",
    "def mydiff(x, t):    \n",
    "    F =F0\n",
    "    \n",
    "    dx1dt = x[1] \n",
    "    dx2dt = (F -c*x[1] -k*x[0])/m\n",
    "    dxdt= [dx1dt, dx2dt] \n",
    "    \n",
    "    return dxdt \n",
    "\n",
    "x_full_sol = odeint(mydiff, x_init, t) \n",
    "\n",
    "x_sol = x_full_sol[:,0]\n",
    "v_sol = x_full_sol[:,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BR02v-fkNKGV"
   },
   "outputs": [],
   "source": [
    "level = \"medium\"\n",
    "label = \"1D_SMD_tanh_\" + level\n",
    "\n",
    "loss_thresh = 0.005\n",
    "\n",
    "t_test = t.reshape(-1,1)\n",
    "t_test_tensor = torch.from_numpy(t_test).float().to(device)\n",
    "\n",
    "# bc1_t = t[0].reshape(-1,1)\n",
    "# bc1_x = x_sol[0].reshape(-1,1)\n",
    "# t_bc1_train = torch.from_numpy(bc1_t).float().to(device)\n",
    "# x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "\n",
    "bc1_t = t.reshape(-1,1)\n",
    "bc1_x = x_sol.reshape(-1,1)\n",
    "t_bc1_train = torch.from_numpy(bc1_t).float().to(device)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "\n",
    "\n",
    "bc2_t = t[0].reshape(-1,1)\n",
    "t_bc2_train = torch.from_numpy(bc2_t).float().to(device)\n",
    "bc2_val = v_sol[0].reshape(-1,1)\n",
    "bc2_val =torch.from_numpy(bc2_val).float().to(device)\n",
    "\n",
    "t_test = t.reshape(-1,1)\n",
    "t_test_tensor = torch.from_numpy(t_test).float().to(device)\n",
    "x_true = x_sol\n",
    "x_true_norm = np.linalg.norm(x_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(t[0]) \n",
    "ub = np.array(t[-1]) \n",
    "\n",
    "#torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SyyktBKBXRo1"
   },
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    \n",
    "    t01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=t01,random_state =seed)\n",
    "\n",
    "    t_coll = lb + (ub-lb)*sampling(N_f)\n",
    "    # t_coll = np.vstack((t_coll,)) # append training points to collocation points \n",
    "\n",
    "    return t_coll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "o1b21zLnNKGW"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "        \n",
    "        \n",
    "        self.k = Parameter(torch.tensor(0.0))\n",
    "        self.k.requiresGrad = True\n",
    "        self.c = Parameter(torch.tensor(0.0))\n",
    "        self.c.requiresGrad = True\n",
    "        self.m = Parameter(torch.tensor(0.0))\n",
    "        self.m.requiresGrad = True\n",
    "        \n",
    "    'forward pass'\n",
    "    def forward(self,t):\n",
    "        if torch.is_tensor(t) != True:         \n",
    "            t = torch.from_numpy(t)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        t = 2.0*(t - l_b)/(u_b - l_b) - 1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = t.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,t,x):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(t), x)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,t_bc2,bc2_val):\n",
    "        g = t_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        x = self.forward(g)    \n",
    "            \n",
    "        x_t = autograd.grad(x,g,torch.ones([t_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dx_dt = x_t[:,[0]]\n",
    "        \n",
    "        bc2 = dx_dt\n",
    "        \n",
    "        loss_bc2= self.loss_function(bc2,bc2_val)\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self,t_coll,f_hat):\n",
    "             \n",
    "        g = t_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        x = self.forward(g) \n",
    "\n",
    "        x_t = autograd.grad(x,g,torch.ones([t_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        x_tt = autograd.grad(x_t,g,torch.ones(t_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dx_dt = x_t[:,[0]]\n",
    "        \n",
    "        dx2_d2t = x_tt[:,[0]]\n",
    "        \n",
    "        f = self.m*dx2_d2t + self.c*dx_dt + self.k*x - F0\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,t_bc1,x_bc1,t_bc2,bc2_val,t_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(t_bc1,x_bc1)\n",
    "        loss_bc2 = self.loss_BC2(t_bc2,bc2_val)\n",
    "        loss_f = self.loss_PDE(t_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self):\n",
    "        x_pred = self.forward(t_test_tensor)\n",
    "        x_pred = x_pred.cpu().detach().numpy()\n",
    "\n",
    "        return x_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        x_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(x_pred.reshape(-1,1) - x_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(x_pred.reshape(-1,1) - x_true.reshape(-1,1),2)/x_true_norm\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fLY2mT5BOgjD"
   },
   "outputs": [],
   "source": [
    "def train_step(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8srA5uGuObil"
   },
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    m_val.append(PINN.m.cpu().detach().numpy())\n",
    "    k_val.append(PINN.k.cpu().detach().numpy())\n",
    "    c_val.append(PINN.c.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0SezTZ_racQB"
   },
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "\n",
    "    t_coll = colloc_pts(N_f,rep*11)\n",
    "    t_coll =  torch.from_numpy(t_coll).float().to(device)\n",
    "    \n",
    "    f_hat = torch.zeros(t_coll.shape[0],1).to(device)\n",
    "    for i in range(max_iter):      \n",
    "        train_step(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat)\n",
    "         \n",
    "        loss_np = PINN.loss(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test RE\",test_re_loss[-1],\"c\",PINN.c.cpu().detach().numpy(),\"k\",PINN.k.cpu().detach().numpy(),\"m\",PINN.m.cpu().detach().numpy())\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I9BYbcJ0NKGX",
    "outputId": "42720d97-b37a-4c42-b7e5-b77a0ef391f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 797724.6 Test RE 0.5829969977915818 c -0.07730792 k 0.41185433 m -0.0006190172\n",
      "1 Train Loss 399271.88 Test RE 0.424275542627007 c -0.13594832 k 0.06656182 m -0.0010905418\n",
      "2 Train Loss 398541.84 Test RE 0.4239618788632821 c -0.13830674 k 0.07374815 m -0.0011095104\n",
      "3 Train Loss 398539.66 Test RE 0.42396091654016815 c -0.13842763 k 0.0740958 m -0.0011104827\n",
      "4 Train Loss 398539.6 Test RE 0.4239609048160057 c -0.13843569 k 0.07411985 m -0.0011105476\n",
      "5 Train Loss 398539.6 Test RE 0.4239609024559566 c -0.13843827 k 0.074128136 m -0.0011105683\n",
      "6 Train Loss 398539.6 Test RE 0.42396090108419426 c -0.13844025 k 0.07413336 m -0.0011105842\n",
      "7 Train Loss 398539.6 Test RE 0.423960900341782 c -0.13844168 k 0.074138016 m -0.0011105958\n",
      "8 Train Loss 398539.6 Test RE 0.4239608999194192 c -0.13844278 k 0.07414096 m -0.0011106046\n",
      "9 Train Loss 398539.6 Test RE 0.42396089967593015 c -0.1384436 k 0.074143514 m -0.0011106113\n",
      "10 Train Loss 398539.6 Test RE 0.42396089954290633 c -0.13844422 k 0.07414525 m -0.0011106161\n",
      "11 Train Loss 398539.6 Test RE 0.4239608994711913 c -0.13844466 k 0.07414663 m -0.0011106197\n",
      "12 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "13 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "14 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "15 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "16 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "17 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "18 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "19 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "20 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "21 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "22 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "23 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "24 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "25 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "26 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "27 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "28 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "29 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "30 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "31 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "32 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "33 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "34 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "35 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "36 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "37 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "38 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "39 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "40 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "41 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "42 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "43 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "44 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "45 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "46 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "47 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "48 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "49 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "50 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "51 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "52 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "53 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "54 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "55 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "56 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "57 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "58 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "59 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "60 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "61 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "62 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "63 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "64 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "65 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "66 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "67 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "68 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "69 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "70 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "71 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "72 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "73 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "74 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "75 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "76 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "77 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "78 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "79 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "80 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "81 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "82 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "83 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "84 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "85 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "86 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "87 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "88 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "89 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "90 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "91 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "92 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "93 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "94 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "95 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "96 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "97 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "98 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "99 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "100 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "101 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "102 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "103 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "104 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "105 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "106 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "107 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "108 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "109 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "110 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "111 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "112 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "113 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "114 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "115 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "116 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "117 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "118 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "119 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "120 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "121 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "122 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "123 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "124 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "125 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "126 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "127 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "128 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "129 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "130 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "131 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "132 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "133 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "134 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "135 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "136 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "137 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "138 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "139 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "140 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "141 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "142 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "143 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "144 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "145 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "146 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "147 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "148 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "149 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "150 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "151 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "152 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "153 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "154 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "155 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "156 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "157 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "158 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "159 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "160 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "161 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "162 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "163 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "164 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "165 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "166 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "167 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "168 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "169 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "170 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "171 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "172 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "173 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "174 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "175 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "176 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "177 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "178 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "179 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "180 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "181 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "182 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "183 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "184 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "185 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "186 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "187 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "188 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "189 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "190 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "191 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "192 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "193 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "194 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "195 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "196 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "197 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "198 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "199 Train Loss 398539.56 Test RE 0.4239608994251766 c -0.13844502 k 0.07414759 m -0.0011106227\n",
      "Training time: 17.48\n",
      "Training time: 17.48\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 1027067.56 Test RE 0.6664227343684503 c -0.08870236 k 0.52462345 m -0.0009972327\n",
      "1 Train Loss 397104.16 Test RE 0.4231110350114596 c -0.2012627 k 0.06578338 m -0.0022635038\n",
      "2 Train Loss 393877.06 Test RE 0.4214599711214901 c -0.20682146 k 0.077275105 m -0.0023272177\n",
      "3 Train Loss 391053.53 Test RE 0.41996010121720156 c -0.20559475 k 0.07406851 m -0.0023075582\n",
      "4 Train Loss 359154.56 Test RE 0.4024510940193197 c -0.20552999 k 0.07474852 m 0.0004470667\n",
      "5 Train Loss 350213.8 Test RE 0.39674932903263227 c -0.2005329 k 0.09101732 m 0.0011728472\n",
      "6 Train Loss 229652.1 Test RE 0.29626330062450107 c 0.21041773 k -0.073904574 m -0.05517841\n",
      "7 Train Loss 78826.97 Test RE 0.18665764607052293 c 0.6053218 k 0.06674338 m -0.11667994\n",
      "8 Train Loss 78636.84 Test RE 0.18653170079631862 c 0.58874065 k 0.06324982 m -0.11414211\n",
      "9 Train Loss 78636.22 Test RE 0.18653425110530528 c 0.58784235 k 0.06305683 m -0.11400514\n",
      "10 Train Loss 78636.22 Test RE 0.18653436009749308 c 0.5878144 k 0.06305069 m -0.11400095\n",
      "11 Train Loss 78636.2 Test RE 0.18653449249166645 c 0.5877713 k 0.06304119 m -0.11399455\n",
      "12 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "13 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "14 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "15 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "16 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "17 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "18 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "19 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "20 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "21 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "22 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "23 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "24 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "25 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "26 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "27 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "28 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "29 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "30 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "31 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "32 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "33 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "34 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "35 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "36 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "37 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "38 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "39 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "40 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "41 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "42 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "43 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "44 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "45 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "46 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "47 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "48 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "49 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "50 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "51 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "52 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "53 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "54 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "55 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "56 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "57 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "58 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "59 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "60 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "61 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "62 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "63 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "64 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "65 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "66 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "67 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "68 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "69 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "70 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "71 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "72 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "73 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "74 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "75 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "76 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "77 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "78 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "79 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "80 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "81 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "82 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "83 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "84 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "85 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "86 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "87 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "88 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "89 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "90 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "91 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "92 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "93 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "94 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "95 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "96 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "97 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "98 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "99 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "100 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "101 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "102 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "103 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "104 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "105 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "106 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "107 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "108 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "109 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "110 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "111 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "112 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "113 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "114 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "115 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "116 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "117 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "118 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "119 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "120 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "121 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "122 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "123 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "124 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "125 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "126 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "127 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "128 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "129 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "130 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "131 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "132 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "133 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "134 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "135 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "136 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "137 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "138 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "139 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "140 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "141 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "142 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "143 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "144 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "145 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "146 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "147 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "148 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "149 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "150 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "151 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "152 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "153 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "154 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "155 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "156 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "157 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "158 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "159 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "160 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "161 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "162 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "163 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "164 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "165 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "166 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "167 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "168 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "169 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "170 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "171 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "172 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "173 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "174 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "175 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "176 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "177 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "178 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "179 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "180 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "181 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "182 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "183 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "184 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "185 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "186 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "187 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "188 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "189 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "190 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "191 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "192 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "193 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "194 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "195 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "196 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "197 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "198 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "199 Train Loss 78636.18 Test RE 0.1865346063854582 c 0.58773506 k 0.06303297 m -0.113989316\n",
      "Training time: 16.59\n",
      "Training time: 16.59\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 486638.72 Test RE 0.46817233200860126 c 0.05636858 k 0.07087082 m -0.0010608581\n",
      "1 Train Loss 398732.34 Test RE 0.4240633131295436 c 0.07145076 k 0.074608594 m -0.0013447063\n",
      "2 Train Loss 398540.12 Test RE 0.4239611862027786 c 0.07215425 k 0.07415727 m -0.0013579484\n",
      "3 Train Loss 398539.6 Test RE 0.4239609044925287 c 0.07218833 k 0.07415149 m -0.0013585895\n",
      "4 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "5 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "6 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "7 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "8 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "9 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "10 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "11 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "12 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "13 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "14 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "15 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "16 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "17 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "18 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "19 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "20 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "21 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "22 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "23 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "24 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "25 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "26 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "27 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "28 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "29 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "30 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "31 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "32 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "33 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "34 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "35 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "36 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "37 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "38 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "39 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "40 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "41 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "42 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "43 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "44 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "45 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "46 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "47 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "48 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "49 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "50 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "51 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "52 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "53 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "54 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "55 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "56 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "57 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "58 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "59 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "60 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "61 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "62 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "63 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "64 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "65 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "66 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "67 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "68 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "69 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "70 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "71 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "72 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "73 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "74 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "75 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "76 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "77 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "78 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "79 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "80 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "81 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "82 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "83 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "84 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "85 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "86 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "87 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "88 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "89 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "90 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "91 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "92 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "93 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "94 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "95 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "96 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "97 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "98 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "99 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "100 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "101 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "102 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "103 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "104 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "105 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "106 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "107 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "108 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "109 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "110 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "111 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "112 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "113 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "114 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "115 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "116 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "117 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "118 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "119 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "120 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "121 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "122 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "123 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "124 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "125 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "126 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "127 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "128 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "129 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "130 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "131 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "132 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "133 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "134 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "135 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "136 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "137 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "138 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "139 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "140 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "141 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "142 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "143 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "144 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "145 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "146 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "147 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "148 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "149 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "150 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "151 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "152 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "153 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "154 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "155 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "156 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "157 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "158 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "159 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "160 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "161 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "162 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "163 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "164 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "165 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "166 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "167 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "168 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "169 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "170 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "171 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "172 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "173 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "174 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "175 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "176 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "177 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "178 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "179 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "180 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "181 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "182 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "183 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "184 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "185 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "186 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "187 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "188 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "189 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "190 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "191 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "192 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "193 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "194 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "195 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "196 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "197 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "198 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "199 Train Loss 398539.6 Test RE 0.4239609022327336 c 0.07218965 k 0.074151285 m -0.0013586143\n",
      "Training time: 21.15\n",
      "Training time: 21.15\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 410915.2 Test RE 0.4244051116018075 c 0.03607089 k -0.0056259483 m -0.001197524\n",
      "1 Train Loss 398569.9 Test RE 0.42396113649921074 c 0.036849204 k 0.07013299 m -0.0012233625\n",
      "2 Train Loss 398539.7 Test RE 0.4239609000515084 c 0.036866836 k 0.0739247 m -0.0012239467\n",
      "3 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "4 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "5 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "6 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "7 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "8 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "9 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "10 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "11 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "12 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "13 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "14 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "15 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "16 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "17 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "18 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "19 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "20 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "21 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "22 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "23 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "24 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "25 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "26 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "27 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "28 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "29 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "30 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "31 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "32 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "33 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "34 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "35 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "36 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "37 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "38 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "39 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "40 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "41 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "42 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "43 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "44 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "45 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "46 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "47 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "48 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "49 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "50 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "51 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "52 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "53 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "54 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "55 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "56 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "57 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "58 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "59 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "60 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "61 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "62 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "63 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "64 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "65 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "66 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "67 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "68 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "69 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "70 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "71 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "72 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "73 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "74 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "75 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "76 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "77 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "78 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "79 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "80 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "81 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "82 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "83 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "84 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "85 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "86 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "87 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "88 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "89 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "90 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "91 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "92 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "93 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "94 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "95 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "96 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "97 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "98 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "99 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "100 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "101 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "102 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "103 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "104 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "105 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "106 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "107 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "108 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "109 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "110 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "111 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "112 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "113 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "114 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "115 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "116 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "117 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "118 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "119 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "120 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "121 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "122 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "123 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "124 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "125 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "126 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "127 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "128 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "129 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "130 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "131 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "132 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "133 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "134 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "135 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "136 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "137 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "138 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "139 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "140 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "141 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "142 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "143 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "144 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "145 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "146 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "147 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "148 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "149 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "150 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "151 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "152 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "153 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "154 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "155 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "156 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "157 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "158 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "159 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "160 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "161 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "162 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "163 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "164 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "165 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "166 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "167 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "168 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "169 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "170 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "171 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "172 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "173 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "174 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "175 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "176 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "177 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "178 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "179 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "180 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "181 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "182 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "183 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "184 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "185 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "186 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "187 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "188 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "189 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "190 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "191 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "192 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "193 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "194 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "195 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "196 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "197 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "198 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "199 Train Loss 398539.6 Test RE 0.42396089943892973 c 0.036867518 k 0.07407917 m -0.0012239693\n",
      "Training time: 19.30\n",
      "Training time: 19.30\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 873456.8 Test RE 0.5526929204124326 c 0.07383279 k 0.66155595 m -0.001428378\n",
      "1 Train Loss 400237.22 Test RE 0.42484016612087594 c 0.11767156 k 0.07145293 m -0.0022778017\n",
      "2 Train Loss 398545.0 Test RE 0.4239636895797772 c 0.12113178 k 0.07402015 m -0.0023448442\n",
      "3 Train Loss 398539.66 Test RE 0.4239609151241851 c 0.121322714 k 0.07414095 m -0.0023485434\n",
      "4 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "5 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "6 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "7 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "8 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "9 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "10 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "11 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "12 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "13 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "14 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "15 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "16 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "17 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "18 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "19 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "20 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "21 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "22 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "23 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "24 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "25 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "26 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "27 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "28 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "29 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "30 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "31 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "32 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "33 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "34 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "35 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "36 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "37 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "38 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "39 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "40 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "41 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "42 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "43 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "44 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "45 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "46 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "47 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "48 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "49 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "50 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "51 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "52 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "53 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "54 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "55 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "56 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "57 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "58 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "59 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "60 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "61 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "62 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "63 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "64 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "65 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "66 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "67 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "68 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "69 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "70 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "71 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "72 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "73 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "74 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "75 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "76 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "77 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "78 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "79 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "80 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "81 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "82 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "83 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "84 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "85 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "86 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "87 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "88 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "89 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "90 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "91 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "92 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "93 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "94 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "95 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "96 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "97 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "98 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "99 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "100 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "101 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "102 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "103 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "104 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "105 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "106 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "107 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "108 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "109 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "110 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "111 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "112 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "113 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "114 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "115 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "116 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "117 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "118 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "119 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "120 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "121 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "122 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "123 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "124 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "125 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "126 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "127 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "128 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "129 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "130 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "131 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "132 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "133 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "134 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "135 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "136 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "137 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "138 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "139 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "140 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "141 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "142 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "143 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "144 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "145 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "146 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "147 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "148 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "149 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "150 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "151 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "152 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "153 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "154 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "155 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "156 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "157 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "158 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "159 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "160 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "161 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "162 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "163 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "164 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "165 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "166 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "167 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "168 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "169 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "170 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "171 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "172 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "173 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "174 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "175 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "176 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "177 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "178 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "179 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "180 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "181 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "182 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "183 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "184 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "185 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "186 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "187 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "188 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "189 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "190 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "191 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "192 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "193 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "194 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "195 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "196 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "197 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "198 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "199 Train Loss 398539.53 Test RE 0.423960875572303 c 0.1213295 k 0.074145205 m -0.002348675\n",
      "Training time: 9.10\n",
      "Training time: 9.10\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 857022.5 Test RE 0.607994974154872 c -0.12698425 k 0.4192748 m -0.00057534047\n",
      "1 Train Loss 399360.66 Test RE 0.4243661263424712 c -0.23990719 k 0.069893286 m -0.0010862671\n",
      "2 Train Loss 398542.16 Test RE 0.42396216082414256 c -0.24464859 k 0.07392124 m -0.0011077242\n",
      "3 Train Loss 398539.7 Test RE 0.4239609392033646 c -0.24487892 k 0.07410967 m -0.0011087664\n",
      "4 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "5 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "6 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "7 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "8 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "9 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "10 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "11 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "12 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "13 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "14 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "15 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "16 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "17 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "18 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "19 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "20 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "21 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "22 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "23 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "24 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "25 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "26 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "27 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "28 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "29 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "30 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "31 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "32 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "33 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "34 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "35 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "36 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "37 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "38 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "39 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "40 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "41 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "42 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "43 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "44 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "45 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "46 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "47 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "48 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "49 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "50 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "51 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "52 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "53 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "54 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "55 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "56 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "57 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "58 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "59 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "60 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "61 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "62 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "63 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "64 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "65 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "66 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "67 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "68 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "69 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "70 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "71 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "72 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "73 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "74 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "75 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "76 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "77 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "78 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "79 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "80 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "81 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "82 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "83 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "84 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "85 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "86 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "87 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "88 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "89 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "90 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "91 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "92 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "93 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "94 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "95 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "96 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "97 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "98 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "99 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "100 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "101 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "102 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "103 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "104 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "105 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "106 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "107 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "108 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "109 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "110 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "111 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "112 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "113 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "114 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "115 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "116 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "117 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "118 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "119 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "120 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "121 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "122 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "123 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "124 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "125 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "126 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "127 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "128 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "129 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "130 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "131 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "132 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "133 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "134 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "135 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "136 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "137 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "138 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "139 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "140 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "141 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "142 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "143 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "144 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "145 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "146 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "147 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "148 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "149 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "150 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "151 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "152 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "153 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "154 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "155 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "156 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "157 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "158 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "159 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "160 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "161 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "162 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "163 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "164 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "165 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "166 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "167 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "168 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "169 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "170 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "171 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "172 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "173 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "174 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "175 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "176 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "177 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "178 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "179 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "180 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "181 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "182 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "183 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "184 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "185 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "186 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "187 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "188 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "189 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "190 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "191 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "192 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "193 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "194 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "195 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "196 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "197 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "198 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "199 Train Loss 398539.6 Test RE 0.42396091197043784 c -0.24490072 k 0.07412757 m -0.001108865\n",
      "Training time: 14.64\n",
      "Training time: 14.64\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 863868.6 Test RE 0.6060116824417189 c 0.1438734 k 0.45843178 m -0.0012150878\n",
      "1 Train Loss 399232.28 Test RE 0.424200343983373 c 0.2712112 k 0.06360773 m -0.0022915269\n",
      "2 Train Loss 398541.7 Test RE 0.42396163786655255 c 0.2753097 k 0.0735863 m -0.0023261993\n",
      "3 Train Loss 398539.62 Test RE 0.423960906749265 c 0.2755266 k 0.074094296 m -0.0023280338\n",
      "4 Train Loss 398539.62 Test RE 0.42396090352397886 c 0.2755326 k 0.07410842 m -0.0023280845\n",
      "5 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "6 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "7 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "8 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "9 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "10 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "11 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "12 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "13 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "14 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "15 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "16 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "17 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "18 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "19 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "20 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "21 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "22 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "23 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "24 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "25 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "26 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "27 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "28 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "29 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "30 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "31 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "32 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "33 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "34 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "35 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "36 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "37 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "38 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "39 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "40 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "41 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "42 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "43 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "44 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "45 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "46 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "47 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "48 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "49 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "50 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "51 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "52 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "53 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "54 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "55 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "56 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "57 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "58 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "59 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "60 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "61 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "62 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "63 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "64 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "65 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "66 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "67 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "68 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "69 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "70 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "71 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "72 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "73 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "74 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "75 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "76 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "77 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "78 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "79 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "80 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "81 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "82 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "83 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "84 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "85 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "86 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "87 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "88 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "89 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "90 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "91 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "92 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "93 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "94 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "95 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "96 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "97 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "98 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "99 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "100 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "101 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "102 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "103 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "104 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "105 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "106 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "107 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "108 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "109 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "110 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "111 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "112 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "113 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "114 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "115 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "116 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "117 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "118 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "119 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "120 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "121 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "122 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "123 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "124 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "125 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "126 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "127 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "128 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "129 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "130 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "131 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "132 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "133 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "134 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "135 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "136 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "137 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "138 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "139 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "140 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "141 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "142 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "143 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "144 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "145 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "146 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "147 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "148 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "149 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "150 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "151 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "152 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "153 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "154 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "155 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "156 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "157 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "158 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "159 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "160 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "161 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "162 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "163 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "164 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "165 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "166 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "167 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "168 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "169 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "170 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "171 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "172 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "173 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "174 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "175 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "176 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "177 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "178 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "179 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "180 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "181 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "182 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "183 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "184 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "185 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "186 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "187 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "188 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "189 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "190 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "191 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "192 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "193 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "194 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "195 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "196 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "197 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "198 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "199 Train Loss 398539.6 Test RE 0.42396090171567047 c 0.2755371 k 0.07411897 m -0.0023281227\n",
      "Training time: 22.60\n",
      "Training time: 22.60\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 1123164.5 Test RE 0.6889549179747066 c 0.14002804 k 0.6776841 m -0.00035706794\n",
      "1 Train Loss 399736.06 Test RE 0.42459652172909623 c 0.33880773 k 0.07553621 m -0.0008636446\n",
      "2 Train Loss 398543.38 Test RE 0.4239629118658895 c 0.347227 k 0.074236184 m -0.00088504876\n",
      "3 Train Loss 398539.7 Test RE 0.4239609340513547 c 0.34766257 k 0.07415732 m -0.0008861535\n",
      "4 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "5 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "6 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "7 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "8 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "9 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "10 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "11 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "12 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "13 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "14 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "15 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "16 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "17 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "18 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "19 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "20 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "21 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "22 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "23 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "24 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "25 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "26 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "27 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "28 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "29 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "30 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "31 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "32 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "33 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "34 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "35 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "36 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "37 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "38 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "39 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "40 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "41 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "42 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "43 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "44 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "45 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "46 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "47 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "48 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "49 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "50 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "51 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "52 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "53 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "54 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "55 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "56 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "57 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "58 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "59 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "60 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "61 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "62 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "63 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "64 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "65 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "66 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "67 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "68 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "69 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "70 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "71 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "72 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "73 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "74 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "75 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "76 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "77 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "78 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "79 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "80 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "81 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "82 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "83 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "84 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "85 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "86 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "87 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "88 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "89 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "90 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "91 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "92 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "93 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "94 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "95 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "96 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "97 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "98 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "99 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "100 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "101 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "102 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "103 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "104 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "105 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "106 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "107 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "108 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "109 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "110 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "111 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "112 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "113 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "114 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "115 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "116 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "117 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "118 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "119 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "120 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "121 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "122 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "123 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "124 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "125 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "126 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "127 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "128 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "129 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "130 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "131 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "132 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "133 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "134 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "135 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "136 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "137 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "138 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "139 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "140 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "141 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "142 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "143 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "144 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "145 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "146 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "147 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "148 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "149 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "150 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "151 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "152 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "153 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "154 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "155 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "156 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "157 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "158 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "159 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "160 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "161 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "162 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "163 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "164 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "165 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "166 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "167 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "168 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "169 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "170 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "171 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "172 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "173 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "174 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "175 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "176 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "177 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "178 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "179 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "180 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "181 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "182 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "183 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "184 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "185 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "186 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "187 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "188 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "189 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "190 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "191 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "192 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "193 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "194 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "195 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "196 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "197 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "198 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "199 Train Loss 398539.6 Test RE 0.42396091018037 c 0.3476916 k 0.074154645 m -0.0008862271\n",
      "Training time: 21.94\n",
      "Training time: 21.94\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 901398.4 Test RE 0.6358151270754389 c 0.07752499 k 0.04510676 m -0.00044281996\n",
      "1 Train Loss 398719.3 Test RE 0.42396940218162726 c 0.16258046 k 0.08388741 m -0.00093134015\n",
      "2 Train Loss 398540.16 Test RE 0.42396092229833937 c 0.16303779 k 0.07469389 m -0.00093396055\n",
      "3 Train Loss 398539.62 Test RE 0.4239609000891416 c 0.1630584 k 0.07424731 m -0.00093407853\n",
      "4 Train Loss 398539.6 Test RE 0.4239608996006065 c 0.16306034 k 0.07420502 m -0.0009340897\n",
      "5 Train Loss 398539.6 Test RE 0.4239608994995486 c 0.16306096 k 0.074191436 m -0.0009340933\n",
      "6 Train Loss 398539.6 Test RE 0.4239608994433648 c 0.16306143 k 0.074181244 m -0.000934096\n",
      "7 Train Loss 398539.6 Test RE 0.4239608994141542 c 0.16306177 k 0.0741736 m -0.000934098\n",
      "8 Train Loss 398539.6 Test RE 0.4239608993951012 c 0.16306205 k 0.07416785 m -0.00093409955\n",
      "9 Train Loss 398539.6 Test RE 0.4239608993847239 c 0.16306224 k 0.07416356 m -0.0009341007\n",
      "10 Train Loss 398539.6 Test RE 0.4239608993792136 c 0.1630624 k 0.07416034 m -0.0009341015\n",
      "11 Train Loss 398539.6 Test RE 0.4239608993759468 c 0.1630625 k 0.07415792 m -0.00093410217\n",
      "12 Train Loss 398539.6 Test RE 0.42396089937469383 c 0.16306257 k 0.07415611 m -0.0009341026\n",
      "13 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "14 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "15 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "16 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "17 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "18 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "19 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "20 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "21 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "22 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "23 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "24 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "25 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "26 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "27 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "28 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "29 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "30 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "31 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "32 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "33 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "34 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "35 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "36 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "37 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "38 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "39 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "40 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "41 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "42 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "43 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "44 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "45 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "46 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "47 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "48 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "49 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "50 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "51 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "52 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "53 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "54 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "55 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "56 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "57 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "58 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "59 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "60 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "61 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "62 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "63 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "64 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "65 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "66 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "67 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "68 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "69 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "70 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "71 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "72 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "73 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "74 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "75 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "76 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "77 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "78 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "79 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "80 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "81 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "82 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "83 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "84 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "85 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "86 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "87 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "88 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "89 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "90 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "91 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "92 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "93 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "94 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "95 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "96 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "97 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "98 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "99 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "100 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "101 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "102 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "103 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "104 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "105 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "106 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "107 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "108 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "109 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "110 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "111 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "112 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "113 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "114 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "115 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "116 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "117 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "118 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "119 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "120 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "121 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "122 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "123 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "124 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "125 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "126 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "127 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "128 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "129 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "130 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "131 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "132 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "133 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "134 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "135 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "136 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "137 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "138 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "139 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "140 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "141 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "142 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "143 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "144 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "145 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "146 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "147 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "148 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "149 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "150 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "151 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "152 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "153 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "154 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "155 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "156 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "157 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "158 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "159 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "160 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "161 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "162 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "163 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "164 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "165 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "166 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "167 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "168 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "169 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "170 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "171 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "172 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "173 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "174 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "175 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "176 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "177 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "178 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "179 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "180 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "181 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "182 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "183 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "184 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "185 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "186 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "187 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "188 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "189 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "190 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "191 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "192 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "193 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "194 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "195 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "196 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "197 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "198 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "199 Train Loss 398539.6 Test RE 0.4239608993734842 c 0.16306265 k 0.074154735 m -0.000934103\n",
      "Training time: 9.79\n",
      "Training time: 9.79\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 479258.06 Test RE 0.4590739545332077 c 0.12399369 k 0.19275051 m -0.0010282687\n",
      "1 Train Loss 398707.66 Test RE 0.4240421481028121 c 0.15236427 k 0.0777609 m -0.0012637515\n",
      "2 Train Loss 398540.06 Test RE 0.42396114701790444 c 0.15369114 k 0.07433267 m -0.0012747678\n",
      "3 Train Loss 398539.7 Test RE 0.4239609241682592 c 0.15374413 k 0.07420806 m -0.0012752078\n",
      "4 Train Loss 398539.6 Test RE 0.4239609037684032 c 0.15375833 k 0.07417484 m -0.0012753258\n",
      "5 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "6 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "7 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "8 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "9 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "10 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "11 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "12 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "13 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "14 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "15 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "16 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "17 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "18 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "19 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "20 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "21 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "22 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "23 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "24 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "25 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "26 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "27 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "28 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "29 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "30 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "31 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "32 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "33 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "34 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "35 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "36 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "37 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "38 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "39 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "40 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "41 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "42 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "43 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "44 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "45 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "46 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "47 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "48 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "49 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "50 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "51 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "52 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "53 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "54 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "55 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "56 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "57 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "58 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "59 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "60 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "61 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "62 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "63 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "64 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "65 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "66 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "67 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "68 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "69 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "70 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "71 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "72 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "73 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "74 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "75 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "76 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "77 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "78 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "79 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "80 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "81 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "82 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "83 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "84 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "85 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "86 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "87 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "88 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "89 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "90 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "91 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "92 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "93 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "94 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "95 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "96 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "97 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "98 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "99 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "100 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "101 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "102 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "103 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "104 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "105 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "106 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "107 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "108 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "109 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "110 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "111 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "112 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "113 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "114 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "115 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "116 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "117 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "118 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "119 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "120 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "121 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "122 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "123 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "124 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "125 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "126 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "127 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "128 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "129 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "130 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "131 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "132 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "133 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "134 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "135 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "136 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "137 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "138 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "139 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "140 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "141 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "142 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "143 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "144 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "145 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "146 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "147 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "148 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "149 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "150 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "151 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "152 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "153 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "154 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "155 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "156 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "157 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "158 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "159 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "160 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "161 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "162 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "163 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "164 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "165 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "166 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "167 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "168 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "169 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "170 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "171 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "172 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "173 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "174 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "175 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "176 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "177 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "178 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "179 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "180 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "181 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "182 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "183 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "184 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "185 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "186 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "187 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "188 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "189 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "190 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "191 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "192 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "193 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "194 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "195 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "196 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "197 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "198 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "199 Train Loss 398539.6 Test RE 0.4239609018556115 c 0.1537609 k 0.07416878 m -0.0012753471\n",
      "Training time: 21.03\n",
      "Training time: 21.03\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 200\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "m_full = []\n",
    "k_full = []\n",
    "c_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "\n",
    "for reps in range(max_reps):   \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []   \n",
    "    beta_val = []\n",
    "    m_val = []\n",
    "    k_val = []\n",
    "    c_val = []\n",
    "\n",
    "    torch.manual_seed(reps*36)\n",
    "    N_f = 10000 #Total number of collocation points\n",
    "\n",
    "    layers = np.array([1,50,50,50,1]) #9 hidden layers\n",
    "    PINN = Sequentialmodel(layers)\n",
    "\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.25, \n",
    "                                max_iter = 10, \n",
    "                                max_eval = 15, \n",
    "                                tolerance_grad = 1e-6, \n",
    "                                tolerance_change = 1e-6, \n",
    "                                history_size = 100, \n",
    "                                line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "    train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    beta_full.append(beta_val)\n",
    "    m_full.append(m_val)\n",
    "    k_full.append(k_val)\n",
    "    c_full.append(c_val)\n",
    "    \n",
    "\n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"beta\": beta_full,\"m\": m_full,\"k\": k_full,\"c\": c_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "pmHEeBpzfFQh",
    "outputId": "990b2054-f35e-4c8e-c378-69ff2eb19f4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1b304d3550>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_pred = PINN.test()\n",
    "plt.plot(t,x_true,'b')\n",
    "plt.plot(t,x_pred,'r--')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "stan_1D_FODE_tune.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
