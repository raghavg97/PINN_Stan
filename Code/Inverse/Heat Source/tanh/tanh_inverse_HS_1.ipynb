{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "import matplotlib as mpl\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "from scipy.io import savemat\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "rho = 4.43\n",
    "q = 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_thresh = 25000\n",
    "level = \"_high\"\n",
    "label = \"Inverse_HS\" + level"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = scipy.io.loadmat('../inverse_HS_test5_v1.mat') \n",
    "\n",
    "pi = torch.from_numpy(np.array(np.pi)).double().to(device)\n",
    "\n",
    "x = np.array(data['x'])\n",
    "t = np.array(data['t'])\n",
    "\n",
    "X,T = np.meshgrid(x,t)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "T = T.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xt = np.hstack((X,T))\n",
    "\n",
    "u_true = np.array(data['sol'][:]).reshape(-1,1)/300.0\n",
    "\n",
    "xt_test_tensor = torch.from_numpy(xt).float().to(device)\n",
    "u_true_norm = np.linalg.norm(u_true,2)\n",
    "\n",
    "lb_xt = xt[0]\n",
    "ub_xt = xt[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def trainingdata(N_I,N_N,N_f,seed):\n",
    "    '''Boundary Conditions''' \n",
    "    \n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    x_l = x[0]*np.ones((N_N,1))\n",
    "    t_l = np.random.uniform(t[0],t[-1],(N_N,1))\n",
    "    xt_l = np.hstack((x_l,t_l))\n",
    "    \n",
    "    x_r = x[-1]*np.ones((N_N,1))\n",
    "    t_r = np.random.uniform(t[0],t[-1],(N_N,1))\n",
    "    xt_r = np.hstack((x_r,t_r))\n",
    "\n",
    "    \n",
    "    x_0 = np.random.uniform(x[0],x[-1],(N_I,1))\n",
    "    t_0 = t[0]*np.ones((N_I,1))\n",
    "    xt_0 = np.hstack((x_0,t_0))\n",
    "    u_0 = 300.0*np.ones((N_I,1))/300.0\n",
    "    \n",
    "    xt_data = np.vstack((xt,xt_0))\n",
    "    u_data = np.vstack((u_true.reshape(-1,1),u_0))\n",
    "    \n",
    "#     xt_data = xt\n",
    "#     u_data = u_true.reshape(-1,1)\n",
    "    \n",
    "   \n",
    "    '''Collocation Points'''\n",
    "\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,t)\n",
    "    x01 = np.array([[0.0,1.0],[0.0,1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    samples = sampling(N_f)\n",
    "    \n",
    "    xt_coll = lb_xt + (ub_xt - lb_xt)*samples\n",
    "    \n",
    "    xt_coll = np.vstack((xt_coll, xt_r,xt_l,xt_0)) # append training points to collocation points \n",
    "\n",
    "    return xt_coll, xt_l,xt_r, xt_data,u_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "        'activation function'\n",
    "        self.activation = nn.Tanh()\n",
    "\n",
    "     \n",
    "        'loss function'\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        self.iter = 0\n",
    "        \n",
    "        # std = gain * sqrt(2/(input_dim+output_dim))\n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data)   \n",
    "        \n",
    "        \n",
    "        self.cp = Parameter(torch.ones(1,1))\n",
    "        self.cp.requiresGrad = True\n",
    "        \n",
    "        self.k = Parameter(torch.ones(1,1))\n",
    "        self.k.requiresGrad = True\n",
    "            \n",
    "    'foward pass'\n",
    "    def forward(self,xy):\n",
    "        if torch.is_tensor(xy) != True:         \n",
    "            xy = torch.from_numpy(xy)                \n",
    "        \n",
    "        ubxy = torch.from_numpy(ub_xt).float().to(device)\n",
    "        lbxy = torch.from_numpy(lb_xt).float().to(device)\n",
    "    \n",
    "                      \n",
    "        #preprocessing input \n",
    "        xy = 2.0*(xy - lbxy)/(ubxy - lbxy) - 1.0\n",
    "        \n",
    "        #convert to float\n",
    "        a = xy.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a =self.activation(z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_IC(self,xt,u):\n",
    "                \n",
    "        loss_ic = self.loss_function(self.forward(xt), u)\n",
    "                \n",
    "        return loss_ic\n",
    "    \n",
    "    def loss_NBC_L(self,xt_NBC,N_hat):\n",
    "        \n",
    "        g = xt_NBC.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_t = autograd.grad(u,g,torch.ones([xt_NBC.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]       \n",
    "        \n",
    "        du_dx = u_x_t[:,[0]]\n",
    "        \n",
    "        f = torch.abs(self.k)*du_dx + q/300.0\n",
    "        \n",
    "        loss_NBC = self.loss_function(f,N_hat)\n",
    "                \n",
    "        return loss_NBC\n",
    "    \n",
    "    def loss_NBC_R(self,xt_NBC,N_hat):\n",
    "        \n",
    "        g = xt_NBC.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_t = autograd.grad(u,g,torch.ones([xt_NBC.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]       \n",
    "        \n",
    "        du_dx = u_x_t[:,[0]]\n",
    "        \n",
    "        f = torch.abs(self.k)*du_dx\n",
    "        \n",
    "        loss_NBC = self.loss_function(f,N_hat)\n",
    "                \n",
    "        return loss_NBC\n",
    "    \n",
    "    def loss_PDE(self, xt_coll, f_hat):\n",
    "        \n",
    "        g = xt_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "        u = self.forward(g) \n",
    "        \n",
    "        u_x_t = autograd.grad(u,g,torch.ones([xt_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        u_xx_tt = autograd.grad(u_x_t,g,torch.ones(xt_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        du_dt = u_x_t[:,[1]]\n",
    "        \n",
    "        d2u_dx2 = u_xx_tt[:,[0]]\n",
    "        \n",
    "\n",
    "        f = rho*torch.abs(self.cp)*du_dt - torch.abs(self.k)*d2u_dx2\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    def loss(self,xt_IC,u_IC,xt_NBC_L,xt_NBC_R,N_hat,xt_coll,f_hat):\n",
    "\n",
    "        loss_IC = self.loss_IC(xt_IC,u_IC)\n",
    "        loss_NBC_L = self.loss_NBC_L(xt_NBC_L,N_hat)\n",
    "        loss_NBC_R = self.loss_NBC_R(xt_NBC_R,N_hat)\n",
    "        loss_f = self.loss_PDE(xt_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_IC + loss_NBC_L + loss_NBC_R + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "         \n",
    "    'test neural network'\n",
    "    def test(self):\n",
    "        u_pred = self.forward(xt_test_tensor)\n",
    "        u_pred = u_pred.cpu().detach().numpy()\n",
    "   \n",
    "        return u_pred\n",
    "\n",
    "    def test_loss(self):\n",
    "        u_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(u_pred.reshape(-1,1) - u_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(u_pred.reshape(-1,1) - u_true.reshape(-1,1),2)/u_true_norm\n",
    "        \n",
    "        \n",
    "        \n",
    "        return test_mse, test_re "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(xt_IC,u_IC,xt_NBC_L,xt_NBC_R,N_hat,xt_coll,f_hat,seed):\n",
    "\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(xt_IC,u_IC,xt_NBC_L,xt_NBC_R,N_hat,xt_coll,f_hat)\n",
    "        loss.backward()\n",
    "        #print(loss.cpu().detach().numpy())\n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep): \n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*9)\n",
    "    start_time = time.time() \n",
    "    thresh_flag = 0\n",
    "\n",
    "    xt_coll, xt_NBC_L,xt_NBC_R, xt_IC,u_IC = trainingdata(N_I,N_N,N_f,rep*22)\n",
    "        \n",
    "    xt_coll = torch.from_numpy(xt_coll).float().to(device)\n",
    "    xt_NBC_L = torch.from_numpy(xt_NBC_L).float().to(device)\n",
    "    xt_NBC_R = torch.from_numpy(xt_NBC_R).float().to(device)\n",
    "    xt_IC = torch.from_numpy(xt_IC).float().to(device)\n",
    "    u_IC = torch.from_numpy(u_IC).float().to(device)\n",
    "        \n",
    "    f_hat = torch.zeros(xt_coll.shape[0],1).to(device)\n",
    "    N_hat = torch.zeros(xt_NBC_L.shape[0],1).to(device)\n",
    "    \n",
    "\n",
    "    for i in range(max_iter):\n",
    "        train_step(xt_IC,u_IC,xt_NBC_L,xt_NBC_R,N_hat,xt_coll,f_hat,i)\n",
    "        loss_np = PINN.loss(xt_IC,u_IC,xt_NBC_L,xt_NBC_R,N_hat,xt_coll,f_hat).cpu().detach().numpy()\n",
    "        \n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1          \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        \n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test RE\",test_re_loss[-1],\"cp:\",PINN.cp.cpu().detach().numpy(),\"K:\",PINN.k.cpu().detach().numpy())   \n",
    "        \n",
    "    elapsed_time[rep] = time.time() - start_time  \n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inverse_HS_high\n",
      "0\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 0.088634826 Test RE 0.03822648340183399 cp: [[-0.03963499]] K: [[0.9696396]]\n",
      "1 Train Loss 0.011071675 Test RE 0.018055768283680846 cp: [[-0.04777173]] K: [[1.5245544]]\n",
      "2 Train Loss 0.0034033293 Test RE 0.00768680315734431 cp: [[-0.05069447]] K: [[2.7583866]]\n",
      "3 Train Loss 0.0016303043 Test RE 0.007244953759918037 cp: [[-0.05129617]] K: [[3.0514808]]\n",
      "4 Train Loss 0.0008712879 Test RE 0.004479023052000035 cp: [[-0.04980858]] K: [[3.8120432]]\n",
      "5 Train Loss 0.0005245064 Test RE 0.0034350119974046 cp: [[-0.05028655]] K: [[4.411655]]\n",
      "6 Train Loss 0.00041901838 Test RE 0.0035819836711329677 cp: [[-0.05023164]] K: [[4.2166977]]\n",
      "7 Train Loss 0.0002743109 Test RE 0.0026685148906286558 cp: [[-0.04988528]] K: [[4.6580577]]\n",
      "8 Train Loss 0.00018870132 Test RE 0.0017929432569575988 cp: [[-0.04964335]] K: [[5.2577505]]\n",
      "9 Train Loss 0.0001373617 Test RE 0.0015319638862220197 cp: [[-0.04997409]] K: [[5.4692926]]\n",
      "10 Train Loss 0.00010598737 Test RE 0.0015543001404691652 cp: [[-0.05002919]] K: [[5.4432764]]\n",
      "11 Train Loss 9.0561865e-05 Test RE 0.0015158082703913183 cp: [[-0.05001421]] K: [[5.441975]]\n",
      "12 Train Loss 7.635253e-05 Test RE 0.0013880439016334214 cp: [[-0.0498853]] K: [[5.5582876]]\n",
      "13 Train Loss 5.213803e-05 Test RE 0.001073085947220498 cp: [[-0.05020755]] K: [[5.831992]]\n",
      "14 Train Loss 4.1697724e-05 Test RE 0.0009148404941506637 cp: [[-0.05009435]] K: [[5.9237714]]\n",
      "15 Train Loss 3.0037707e-05 Test RE 0.00084622107106691 cp: [[-0.05005161]] K: [[5.9501762]]\n",
      "16 Train Loss 2.3857378e-05 Test RE 0.0005280259104268503 cp: [[-0.05007863]] K: [[6.2842517]]\n",
      "17 Train Loss 2.1856711e-05 Test RE 0.0005655000361729508 cp: [[-0.05008456]] K: [[6.2415266]]\n",
      "18 Train Loss 1.4763513e-05 Test RE 0.0005235631986594718 cp: [[-0.05008807]] K: [[6.241136]]\n",
      "19 Train Loss 1.2574836e-05 Test RE 0.00037993910989497163 cp: [[-0.05004653]] K: [[6.4121785]]\n",
      "20 Train Loss 1.1112423e-05 Test RE 0.00038208139893064973 cp: [[-0.05006455]] K: [[6.420191]]\n",
      "21 Train Loss 9.8147775e-06 Test RE 0.00036328724355698714 cp: [[-0.0500596]] K: [[6.4372396]]\n",
      "22 Train Loss 6.855166e-06 Test RE 0.0002737580259147468 cp: [[-0.05006451]] K: [[6.603658]]\n",
      "23 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "24 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "25 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "26 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "27 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "28 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "29 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "30 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "31 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "32 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "33 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "34 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "35 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "36 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "37 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "38 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "39 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "40 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "41 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "42 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "43 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "44 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "45 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "46 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "47 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "48 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "49 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "50 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "51 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "52 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "53 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "54 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "55 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "56 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "57 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "58 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "59 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "60 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "61 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "62 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "63 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "64 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "65 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "66 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "67 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "68 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "69 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "70 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "71 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "72 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "73 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "74 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "75 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "76 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "77 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "78 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "79 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "80 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "81 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "82 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "83 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "84 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "85 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "86 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "87 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "88 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "89 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "90 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "91 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "92 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "93 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "94 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "95 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "96 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "97 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "98 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "99 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "100 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "101 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "102 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "103 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "104 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "105 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "106 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "107 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "108 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "109 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "110 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "111 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "112 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "113 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "114 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "115 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "116 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "117 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "118 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "119 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "120 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "121 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "122 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "123 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "124 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "125 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "126 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "127 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "128 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "129 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "130 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "131 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "132 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "133 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "134 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "135 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "136 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "137 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "138 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "139 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "140 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "141 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "142 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "143 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "144 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "145 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "146 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "147 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "148 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "149 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "150 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "151 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "152 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "153 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "154 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "155 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "156 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "157 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "158 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "159 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "160 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "161 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "162 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "163 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "164 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "165 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "166 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "167 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "168 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "169 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "170 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "171 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "172 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "173 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "174 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "175 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "176 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "177 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "178 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "179 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "180 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "181 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "182 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "183 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "184 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "185 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "186 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "187 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "188 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "189 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "190 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "191 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "192 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "193 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "194 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "195 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "196 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "197 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "198 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "199 Train Loss 6.5767317e-06 Test RE 0.00025734889175363346 cp: [[-0.05006567]] K: [[6.650454]]\n",
      "Training time: 20.91\n",
      "Training time: 20.91\n",
      "Inverse_HS_high\n",
      "1\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 0.059651982 Test RE 0.03242778456443632 cp: [[0.04102109]] K: [[1.0097712]]\n",
      "1 Train Loss 0.012258652 Test RE 0.01861990848184638 cp: [[0.04869867]] K: [[1.4459263]]\n",
      "2 Train Loss 0.004540442 Test RE 0.01068528607399517 cp: [[0.04800725]] K: [[2.2219605]]\n",
      "3 Train Loss 0.0016045325 Test RE 0.007137177494900548 cp: [[0.05034804]] K: [[2.9272788]]\n",
      "4 Train Loss 0.0012385488 Test RE 0.005972932409375388 cp: [[0.05000589]] K: [[3.2227628]]\n",
      "5 Train Loss 0.0009075803 Test RE 0.004819836966042975 cp: [[0.05058846]] K: [[3.6120617]]\n",
      "6 Train Loss 0.00067765644 Test RE 0.004463557515211437 cp: [[0.04988581]] K: [[3.7426805]]\n",
      "7 Train Loss 0.00049605384 Test RE 0.003591726109610418 cp: [[0.05006683]] K: [[4.1108356]]\n",
      "8 Train Loss 0.0003129507 Test RE 0.0026876919360042024 cp: [[0.04961371]] K: [[4.5759916]]\n",
      "9 Train Loss 0.00024018122 Test RE 0.0024769755657542236 cp: [[0.05007439]] K: [[4.6817975]]\n",
      "10 Train Loss 0.00020853964 Test RE 0.0022828151281555015 cp: [[0.05009839]] K: [[4.790059]]\n",
      "11 Train Loss 0.00018647237 Test RE 0.002249131738158598 cp: [[0.04998587]] K: [[4.805158]]\n",
      "12 Train Loss 0.00014479575 Test RE 0.001936272108507918 cp: [[0.05005623]] K: [[5.007014]]\n",
      "13 Train Loss 0.00011261605 Test RE 0.0016529965008458623 cp: [[0.0500487]] K: [[5.209956]]\n",
      "14 Train Loss 9.951409e-05 Test RE 0.0013485833265731832 cp: [[0.04999397]] K: [[5.4481835]]\n",
      "15 Train Loss 9.243963e-05 Test RE 0.0012233482527970298 cp: [[0.04976092]] K: [[5.579735]]\n",
      "16 Train Loss 8.256108e-05 Test RE 0.0011169260447870804 cp: [[0.04982274]] K: [[5.682973]]\n",
      "17 Train Loss 7.154825e-05 Test RE 0.0012235760592805237 cp: [[0.04986922]] K: [[5.567041]]\n",
      "18 Train Loss 6.951805e-05 Test RE 0.0012855983376161403 cp: [[0.0499267]] K: [[5.50531]]\n",
      "19 Train Loss 5.9570142e-05 Test RE 0.0012137094272248834 cp: [[0.0498803]] K: [[5.571003]]\n",
      "20 Train Loss 5.4410077e-05 Test RE 0.0010209648529365286 cp: [[0.04984435]] K: [[5.7558374]]\n",
      "21 Train Loss 5.0915623e-05 Test RE 0.0009024253470437912 cp: [[0.04981284]] K: [[5.863425]]\n",
      "22 Train Loss 3.2895157e-05 Test RE 0.0007746746605221023 cp: [[0.04986853]] K: [[5.944919]]\n",
      "23 Train Loss 2.0047099e-05 Test RE 0.0007058456859090242 cp: [[0.05001739]] K: [[6.0163445]]\n",
      "24 Train Loss 1.707393e-05 Test RE 0.0006279400423351827 cp: [[0.05009203]] K: [[6.10233]]\n",
      "25 Train Loss 1.3930679e-05 Test RE 0.0005428809933915934 cp: [[0.05005134]] K: [[6.2060504]]\n",
      "26 Train Loss 1.1222542e-05 Test RE 0.0004439855758571122 cp: [[0.0500453]] K: [[6.3451133]]\n",
      "27 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "28 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "29 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "30 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "31 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "32 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "33 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "34 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "35 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "36 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "37 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "38 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "39 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "40 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "41 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "42 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "43 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "44 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "45 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "46 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "47 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "48 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "49 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "50 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "51 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "52 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "53 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "54 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "55 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "56 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "57 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "58 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "59 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "60 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "61 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "62 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "63 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "64 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "65 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "66 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "67 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "68 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "69 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "70 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "71 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "72 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "73 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "74 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "75 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "76 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "77 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "78 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "79 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "80 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "81 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "82 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "83 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "84 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "85 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "86 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "87 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "88 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "89 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "90 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "91 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "92 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "93 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "94 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "95 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "96 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "97 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "98 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "99 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "100 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "101 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "102 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "103 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "104 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "105 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "106 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "107 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "108 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "109 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "110 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "111 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "112 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "113 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "114 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "115 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "116 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "117 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "118 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "119 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "120 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "121 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "122 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "123 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "124 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "125 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "126 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "127 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "128 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "129 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "130 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "131 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "132 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "133 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "134 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "135 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "136 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "137 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "138 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "139 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "140 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "141 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "142 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "143 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "144 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "145 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "146 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "147 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "148 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "149 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "150 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "151 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "152 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "153 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "154 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "155 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "156 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "157 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "158 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "159 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "160 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "161 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "162 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "163 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "164 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "165 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "166 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "167 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "168 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "169 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "170 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "171 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "172 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "173 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "174 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "175 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "176 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "177 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "178 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "179 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "180 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "181 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "182 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "183 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "184 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "185 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "186 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "187 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "188 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "189 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "190 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "191 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "192 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "193 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "194 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "195 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "196 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "197 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "198 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "199 Train Loss 1.1209172e-05 Test RE 0.0004440705565325112 cp: [[0.05004422]] K: [[6.3451824]]\n",
      "Training time: 23.67\n",
      "Training time: 23.67\n",
      "Inverse_HS_high\n",
      "2\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=2, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 0.05804184 Test RE 0.03411638702074833 cp: [[0.04305683]] K: [[0.9389783]]\n",
      "1 Train Loss 0.011134143 Test RE 0.018290364354361693 cp: [[0.04973414]] K: [[1.6472284]]\n",
      "2 Train Loss 0.0027427082 Test RE 0.008976907688460236 cp: [[0.05158523]] K: [[2.615019]]\n",
      "3 Train Loss 0.0015867956 Test RE 0.0059552172708952165 cp: [[0.04963058]] K: [[3.2584212]]\n",
      "4 Train Loss 0.00081490545 Test RE 0.004762050659787241 cp: [[0.04964308]] K: [[3.6195977]]\n",
      "5 Train Loss 0.0006484763 Test RE 0.004050770059063596 cp: [[0.04984688]] K: [[3.9792454]]\n",
      "6 Train Loss 0.0004420407 Test RE 0.0036194215166485406 cp: [[0.04986516]] K: [[4.1169963]]\n",
      "7 Train Loss 0.00026289263 Test RE 0.00255155380040688 cp: [[0.05023802]] K: [[4.7216573]]\n",
      "8 Train Loss 0.00019130141 Test RE 0.0021978364566980083 cp: [[0.05003718]] K: [[4.962601]]\n",
      "9 Train Loss 0.00012278529 Test RE 0.0016470175649712152 cp: [[0.05007903]] K: [[5.2396455]]\n",
      "10 Train Loss 0.00010148606 Test RE 0.0015885435724011256 cp: [[0.05001787]] K: [[5.29469]]\n",
      "11 Train Loss 5.9550577e-05 Test RE 0.00104593181140426 cp: [[0.05016638]] K: [[5.7354345]]\n",
      "12 Train Loss 4.9217437e-05 Test RE 0.0009864478465949466 cp: [[0.05015229]] K: [[5.8290944]]\n",
      "13 Train Loss 3.951825e-05 Test RE 0.0007843935339939591 cp: [[0.0501462]] K: [[6.001832]]\n",
      "14 Train Loss 3.007007e-05 Test RE 0.0007291233289105312 cp: [[0.05013144]] K: [[6.0466323]]\n",
      "15 Train Loss 2.67254e-05 Test RE 0.0007460145173089286 cp: [[0.05007323]] K: [[5.9950104]]\n",
      "16 Train Loss 2.175024e-05 Test RE 0.0006580344882337083 cp: [[0.05002776]] K: [[6.0969357]]\n",
      "17 Train Loss 1.7720202e-05 Test RE 0.00048480873202253307 cp: [[0.05002546]] K: [[6.3209248]]\n",
      "18 Train Loss 1.7710538e-05 Test RE 0.00048373086229007174 cp: [[0.05002448]] K: [[6.3232565]]\n",
      "19 Train Loss 1.7701212e-05 Test RE 0.0004817647353718611 cp: [[0.05002177]] K: [[6.3266006]]\n",
      "20 Train Loss 1.7665145e-05 Test RE 0.0004746640654240019 cp: [[0.05001418]] K: [[6.338422]]\n",
      "21 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "22 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "23 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "24 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "25 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "26 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "27 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "28 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "29 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "30 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "31 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "32 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "33 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "34 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "35 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "36 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "37 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "38 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "39 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "40 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "41 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "42 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "43 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "44 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "45 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "46 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "47 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "48 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "49 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "50 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "51 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "52 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "53 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "54 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "55 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "56 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "57 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "58 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "59 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "60 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "61 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "62 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "63 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "64 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "65 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "66 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "67 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "68 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "69 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "70 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "71 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "72 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "73 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "74 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "75 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "76 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "77 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "78 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "79 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "80 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "81 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "82 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "83 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "84 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "85 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "86 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "87 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "88 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "89 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "90 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "91 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "92 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "93 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "94 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "95 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "96 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "97 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "98 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "99 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "100 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "101 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "102 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "103 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "104 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "105 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "106 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "107 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "108 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "109 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "110 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "111 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "112 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "113 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "114 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "115 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "116 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "117 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "118 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "119 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "120 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "121 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "122 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "123 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "124 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "125 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "126 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "127 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "128 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "129 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "130 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "131 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "132 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "133 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "134 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "135 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "136 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "137 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "138 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "139 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "140 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "141 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "142 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "143 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "144 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "145 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "146 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "147 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "148 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "149 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "150 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "151 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "152 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "153 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "154 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "155 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "156 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "157 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "158 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "159 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "160 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "161 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "162 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "163 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "164 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "165 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "166 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "167 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "168 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "169 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "170 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "171 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "172 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "173 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "174 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "175 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "176 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "177 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "178 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "179 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "180 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "181 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "182 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "183 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "184 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "185 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "186 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "187 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "188 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "189 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "190 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "191 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "192 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "193 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "194 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "195 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "196 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "197 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "198 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "199 Train Loss 1.765354e-05 Test RE 0.00047250574334380797 cp: [[0.05001704]] K: [[6.3422847]]\n",
      "Training time: 17.50\n",
      "Training time: 17.50\n"
     ]
    }
   ],
   "source": [
    "max_reps = 3\n",
    "max_iter = 200 #200\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "\n",
    "\n",
    "N_I = 500 #Total number of data points for 'y'\n",
    "N_N = 500\n",
    "N_f = 10000 #Total number of collocation points \n",
    "\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    print(label)\n",
    "    print(reps)\n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []\n",
    " \n",
    "    \n",
    "    torch.manual_seed(reps*36)\n",
    "    \n",
    "    layers = np.array([2,50,50,50,1]) #9 hidden layers\n",
    "\n",
    "    PINN = Sequentialmodel(layers)\n",
    "   \n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "    \n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=1, \n",
    "                              max_iter = 20, \n",
    "                              max_eval = 30, \n",
    "                              tolerance_grad = 1e-08, \n",
    "                              tolerance_change = 1e-08, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "   \n",
    "    \n",
    "    nan_flag = train_model(max_iter,reps)\n",
    "    \n",
    "    \n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    #elapsed_time[reps] = time.time() - start_time\n",
    "\n",
    "\n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "    \n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.linspace(-2,2,500).reshape(-1,1)\n",
    "y = np.linspace(0,1,500).reshape(-1,1)\n",
    "\n",
    "X,Y = np.meshgrid(x,y)\n",
    "\n",
    "X = X.flatten('F').reshape(-1,1)\n",
    "Y = Y.flatten('F').reshape(-1,1)\n",
    "  \n",
    "xy = np.hstack((X,Y))\n",
    "\n",
    "xy_test_tensor = torch.from_numpy(xy).float().to(device)\n",
    "\n",
    "u_pred = PINN.forward(xy_test_tensor).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1,1)\n",
    "cmap = plt.cm.jet\n",
    "\n",
    "img3 = ax.imshow(np.flip(u_pred.reshape(500,500),axis = 0),cmap = cmap,extent=[0,1,0,1],aspect = 0.75)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + test_loss_full[i][-1]\n",
    "print(a/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + train_loss_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
