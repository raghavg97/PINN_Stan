{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8byrnUmNKGR",
    "outputId": "510f67c6-b225-41ba-e78a-d0a6e54ded85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uoNYpDzlNKGV"
   },
   "outputs": [],
   "source": [
    "tstart= 0 \n",
    "tstop= 60 \n",
    "\n",
    "increment = 0.1 \n",
    "# Initial condition\n",
    "x_init= [0,0] \n",
    "\n",
    "t = np.arange(tstart,tstop+1,increment)\n",
    "\n",
    "c = 1 # Damping constant \n",
    "k = 0.05 # Stiffness of the spring\n",
    "m = 5 # Mass \n",
    "F0 = 100\n",
    "\n",
    "def mydiff(x, t):    \n",
    "    F =F0\n",
    "    \n",
    "    dx1dt = x[1] \n",
    "    dx2dt = (F -c*x[1] -k*x[0])/m\n",
    "    dxdt= [dx1dt, dx2dt] \n",
    "    \n",
    "    return dxdt \n",
    "\n",
    "x_full_sol = odeint(mydiff, x_init, t) \n",
    "\n",
    "x_sol = x_full_sol[:,0]\n",
    "v_sol = x_full_sol[:,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BR02v-fkNKGV"
   },
   "outputs": [],
   "source": [
    "level = \"medium\"\n",
    "label = \"1D_SMD_atanh_\" + level\n",
    "\n",
    "loss_thresh = 0.005\n",
    "\n",
    "t_test = t.reshape(-1,1)\n",
    "t_test_tensor = torch.from_numpy(t_test).float().to(device)\n",
    "\n",
    "# bc1_t = t[0].reshape(-1,1)\n",
    "# bc1_x = x_sol[0].reshape(-1,1)\n",
    "# t_bc1_train = torch.from_numpy(bc1_t).float().to(device)\n",
    "# x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "\n",
    "bc1_t = t.reshape(-1,1)\n",
    "bc1_x = x_sol.reshape(-1,1)\n",
    "t_bc1_train = torch.from_numpy(bc1_t).float().to(device)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "\n",
    "\n",
    "bc2_t = t[0].reshape(-1,1)\n",
    "t_bc2_train = torch.from_numpy(bc2_t).float().to(device)\n",
    "bc2_val = v_sol[0].reshape(-1,1)\n",
    "bc2_val =torch.from_numpy(bc2_val).float().to(device)\n",
    "\n",
    "t_test = t.reshape(-1,1)\n",
    "t_test_tensor = torch.from_numpy(t_test).float().to(device)\n",
    "x_true = x_sol\n",
    "x_true_norm = np.linalg.norm(x_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(t[0]) \n",
    "ub = np.array(t[-1]) \n",
    "\n",
    "#torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SyyktBKBXRo1"
   },
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    \n",
    "    t01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=t01,random_state =seed)\n",
    "\n",
    "    t_coll = lb + (ub-lb)*sampling(N_f)\n",
    "    # t_coll = np.vstack((t_coll,)) # append training points to collocation points \n",
    "\n",
    "    return t_coll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "o1b21zLnNKGW"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "        \n",
    "        \n",
    "        self.k = Parameter(torch.tensor(0.0))\n",
    "        self.k.requiresGrad = True\n",
    "        self.c = Parameter(torch.tensor(0.0))\n",
    "        self.c.requiresGrad = True\n",
    "        self.m = Parameter(torch.tensor(0.0))\n",
    "        self.m.requiresGrad = True\n",
    "        \n",
    "        self.alpha = Parameter(torch.ones((50,len(layers)-2)))\n",
    "        self.alpha.requiresGrad = True\n",
    "        \n",
    "        self.n = torch.tensor(1.0)\n",
    "        \n",
    "    'forward pass'\n",
    "    def forward(self,t):\n",
    "        if torch.is_tensor(t) != True:         \n",
    "            t = torch.from_numpy(t)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        t = 2.0*(t - l_b)/(u_b - l_b) - 1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = t.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(self.n*self.alpha[:,i]*z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,t,x):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(t), x)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,t_bc2,bc2_val):\n",
    "        g = t_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        x = self.forward(g)    \n",
    "            \n",
    "        x_t = autograd.grad(x,g,torch.ones([t_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dx_dt = x_t[:,[0]]\n",
    "        \n",
    "        bc2 = dx_dt\n",
    "        \n",
    "        loss_bc2= self.loss_function(bc2,bc2_val)\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self,t_coll,f_hat):\n",
    "             \n",
    "        g = t_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        x = self.forward(g) \n",
    "\n",
    "        x_t = autograd.grad(x,g,torch.ones([t_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        x_tt = autograd.grad(x_t,g,torch.ones(t_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dx_dt = x_t[:,[0]]\n",
    "        \n",
    "        dx2_d2t = x_tt[:,[0]]\n",
    "        \n",
    "        f = self.m*dx2_d2t + self.c*dx_dt + self.k*x - F0\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,t_bc1,x_bc1,t_bc2,bc2_val,t_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(t_bc1,x_bc1)\n",
    "        loss_bc2 = self.loss_BC2(t_bc2,bc2_val)\n",
    "        loss_f = self.loss_PDE(t_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self):\n",
    "        x_pred = self.forward(t_test_tensor)\n",
    "        x_pred = x_pred.cpu().detach().numpy()\n",
    "\n",
    "        return x_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        x_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(x_pred.reshape(-1,1) - x_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(x_pred.reshape(-1,1) - x_true.reshape(-1,1),2)/x_true_norm\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fLY2mT5BOgjD"
   },
   "outputs": [],
   "source": [
    "def train_step(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8srA5uGuObil"
   },
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    m_val.append(PINN.m.cpu().detach().numpy())\n",
    "    k_val.append(PINN.k.cpu().detach().numpy())\n",
    "    c_val.append(PINN.c.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0SezTZ_racQB"
   },
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "\n",
    "    t_coll = colloc_pts(N_f,rep*11)\n",
    "    t_coll =  torch.from_numpy(t_coll).float().to(device)\n",
    "    \n",
    "    f_hat = torch.zeros(t_coll.shape[0],1).to(device)\n",
    "    for i in range(max_iter):      \n",
    "        train_step(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat)\n",
    "         \n",
    "        loss_np = PINN.loss(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test RE\",test_re_loss[-1],\"c\",PINN.c.cpu().detach().numpy(),\"k\",PINN.k.cpu().detach().numpy(),\"m\",PINN.m.cpu().detach().numpy())\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I9BYbcJ0NKGX",
    "outputId": "42720d97-b37a-4c42-b7e5-b77a0ef391f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 1093841.0 Test RE 0.6298636558695572 c -0.06780614 k 0.8592778 m -0.0004900986\n",
      "1 Train Loss 398540.53 Test RE 0.42396137203949663 c -0.14796048 k 0.074310124 m -0.0010687247\n",
      "2 Train Loss 398540.38 Test RE 0.4239613323463336 c -0.14798363 k 0.07419038 m -0.0010688921\n",
      "3 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "4 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "5 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "6 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "7 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "8 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "9 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "10 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "11 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "12 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "13 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "14 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "15 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "16 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "17 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "18 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "19 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "20 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "21 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "22 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "23 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "24 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "25 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "26 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "27 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "28 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "29 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "30 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "31 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "32 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "33 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "34 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "35 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "36 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "37 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "38 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "39 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "40 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "41 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "42 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "43 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "44 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "45 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "46 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "47 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "48 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "49 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "50 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "51 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "52 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "53 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "54 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "55 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "56 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "57 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "58 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "59 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "60 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "61 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 200\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "m_full = []\n",
    "k_full = []\n",
    "c_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "\n",
    "for reps in range(max_reps):   \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []   \n",
    "    beta_val = []\n",
    "    m_val = []\n",
    "    k_val = []\n",
    "    c_val = []\n",
    "\n",
    "    torch.manual_seed(reps*36)\n",
    "    N_f = 10000 #Total number of collocation points\n",
    "\n",
    "    layers = np.array([1,50,50,50,1]) #9 hidden layers\n",
    "    PINN = Sequentialmodel(layers)\n",
    "\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.5, \n",
    "                                max_iter = 10, \n",
    "                                max_eval = 15, \n",
    "                                tolerance_grad = 1e-6, \n",
    "                                tolerance_change = 1e-6, \n",
    "                                history_size = 100, \n",
    "                                line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "    train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    beta_full.append(beta_val)\n",
    "    m_full.append(m_val)\n",
    "    k_full.append(k_val)\n",
    "    c_full.append(c_val)\n",
    "    \n",
    "\n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"beta\": beta_full,\"m\": m_full,\"k\": k_full,\"c\": c_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "pmHEeBpzfFQh",
    "outputId": "990b2054-f35e-4c8e-c378-69ff2eb19f4f"
   },
   "outputs": [],
   "source": [
    "x_pred = PINN.test()\n",
    "plt.plot(t,x_true,'b')\n",
    "plt.plot(t,x_pred,'r--')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "stan_1D_FODE_tune.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
