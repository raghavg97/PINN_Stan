{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8byrnUmNKGR",
    "outputId": "510f67c6-b225-41ba-e78a-d0a6e54ded85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uoNYpDzlNKGV"
   },
   "outputs": [],
   "source": [
    "tstart= 0 \n",
    "tstop= 60 \n",
    "\n",
    "increment = 0.1 \n",
    "# Initial condition\n",
    "x_init= [0,0] \n",
    "\n",
    "t = np.arange(tstart,tstop+1,increment)\n",
    "\n",
    "c = 1 # Damping constant \n",
    "k = 0.05 # Stiffness of the spring\n",
    "m = 5 # Mass \n",
    "F0 = 100\n",
    "\n",
    "def mydiff(x, t):    \n",
    "    F =F0\n",
    "    \n",
    "    dx1dt = x[1] \n",
    "    dx2dt = (F -c*x[1] -k*x[0])/m\n",
    "    dxdt= [dx1dt, dx2dt] \n",
    "    \n",
    "    return dxdt \n",
    "\n",
    "x_full_sol = odeint(mydiff, x_init, t) \n",
    "\n",
    "x_sol = x_full_sol[:,0]\n",
    "v_sol = x_full_sol[:,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BR02v-fkNKGV"
   },
   "outputs": [],
   "source": [
    "level = \"medium\"\n",
    "label = \"1D_SMD_atanh_\" + level\n",
    "\n",
    "loss_thresh = 0.005\n",
    "\n",
    "t_test = t.reshape(-1,1)\n",
    "t_test_tensor = torch.from_numpy(t_test).float().to(device)\n",
    "\n",
    "# bc1_t = t[0].reshape(-1,1)\n",
    "# bc1_x = x_sol[0].reshape(-1,1)\n",
    "# t_bc1_train = torch.from_numpy(bc1_t).float().to(device)\n",
    "# x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "\n",
    "bc1_t = t.reshape(-1,1)\n",
    "bc1_x = x_sol.reshape(-1,1)\n",
    "t_bc1_train = torch.from_numpy(bc1_t).float().to(device)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "\n",
    "\n",
    "bc2_t = t[0].reshape(-1,1)\n",
    "t_bc2_train = torch.from_numpy(bc2_t).float().to(device)\n",
    "bc2_val = v_sol[0].reshape(-1,1)\n",
    "bc2_val =torch.from_numpy(bc2_val).float().to(device)\n",
    "\n",
    "t_test = t.reshape(-1,1)\n",
    "t_test_tensor = torch.from_numpy(t_test).float().to(device)\n",
    "x_true = x_sol\n",
    "x_true_norm = np.linalg.norm(x_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(t[0]) \n",
    "ub = np.array(t[-1]) \n",
    "\n",
    "#torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SyyktBKBXRo1"
   },
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    \n",
    "    t01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=t01,random_state =seed)\n",
    "\n",
    "    t_coll = lb + (ub-lb)*sampling(N_f)\n",
    "    # t_coll = np.vstack((t_coll,)) # append training points to collocation points \n",
    "\n",
    "    return t_coll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "o1b21zLnNKGW"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "        \n",
    "        \n",
    "        self.k = Parameter(torch.tensor(0.0))\n",
    "        self.k.requiresGrad = True\n",
    "        self.c = Parameter(torch.tensor(0.0))\n",
    "        self.c.requiresGrad = True\n",
    "        self.m = Parameter(torch.tensor(0.0))\n",
    "        self.m.requiresGrad = True\n",
    "        \n",
    "        self.alpha = Parameter(torch.ones((50,len(layers)-2)))\n",
    "        self.alpha.requiresGrad = True\n",
    "        \n",
    "        self.n = torch.tensor(1.0)\n",
    "        \n",
    "    'forward pass'\n",
    "    def forward(self,t):\n",
    "        if torch.is_tensor(t) != True:         \n",
    "            t = torch.from_numpy(t)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        t = 2.0*(t - l_b)/(u_b - l_b) - 1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = t.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(self.n*self.alpha[:,i]*z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,t,x):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(t), x)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,t_bc2,bc2_val):\n",
    "        g = t_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        x = self.forward(g)    \n",
    "            \n",
    "        x_t = autograd.grad(x,g,torch.ones([t_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dx_dt = x_t[:,[0]]\n",
    "        \n",
    "        bc2 = dx_dt\n",
    "        \n",
    "        loss_bc2= self.loss_function(bc2,bc2_val)\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self,t_coll,f_hat):\n",
    "             \n",
    "        g = t_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        x = self.forward(g) \n",
    "\n",
    "        x_t = autograd.grad(x,g,torch.ones([t_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        x_tt = autograd.grad(x_t,g,torch.ones(t_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dx_dt = x_t[:,[0]]\n",
    "        \n",
    "        dx2_d2t = x_tt[:,[0]]\n",
    "        \n",
    "        f = self.m*dx2_d2t + self.c*dx_dt + self.k*x - F0\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,t_bc1,x_bc1,t_bc2,bc2_val,t_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(t_bc1,x_bc1)\n",
    "        loss_bc2 = self.loss_BC2(t_bc2,bc2_val)\n",
    "        loss_f = self.loss_PDE(t_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self):\n",
    "        x_pred = self.forward(t_test_tensor)\n",
    "        x_pred = x_pred.cpu().detach().numpy()\n",
    "\n",
    "        return x_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        x_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(x_pred.reshape(-1,1) - x_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(x_pred.reshape(-1,1) - x_true.reshape(-1,1),2)/x_true_norm\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fLY2mT5BOgjD"
   },
   "outputs": [],
   "source": [
    "def train_step(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8srA5uGuObil"
   },
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    m_val.append(PINN.m.cpu().detach().numpy())\n",
    "    k_val.append(PINN.k.cpu().detach().numpy())\n",
    "    c_val.append(PINN.c.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0SezTZ_racQB"
   },
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "\n",
    "    t_coll = colloc_pts(N_f,rep*11)\n",
    "    t_coll =  torch.from_numpy(t_coll).float().to(device)\n",
    "    \n",
    "    f_hat = torch.zeros(t_coll.shape[0],1).to(device)\n",
    "    for i in range(max_iter):      \n",
    "        train_step(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat)\n",
    "         \n",
    "        loss_np = PINN.loss(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test RE\",test_re_loss[-1],\"c\",PINN.c.cpu().detach().numpy(),\"k\",PINN.k.cpu().detach().numpy(),\"m\",PINN.m.cpu().detach().numpy())\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I9BYbcJ0NKGX",
    "outputId": "42720d97-b37a-4c42-b7e5-b77a0ef391f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 1093841.0 Test RE 0.6298636558695572 c -0.06780614 k 0.8592778 m -0.0004900986\n",
      "1 Train Loss 398540.53 Test RE 0.42396137203949663 c -0.14796048 k 0.074310124 m -0.0010687247\n",
      "2 Train Loss 398540.38 Test RE 0.4239613323463336 c -0.14798363 k 0.07419038 m -0.0010688921\n",
      "3 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "4 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "5 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "6 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "7 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "8 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "9 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "10 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "11 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "12 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "13 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "14 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "15 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "16 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "17 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "18 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "19 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "20 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "21 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "22 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "23 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "24 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "25 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "26 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "27 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "28 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "29 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "30 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "31 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "32 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "33 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "34 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "35 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "36 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "37 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "38 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "39 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "40 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "41 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "42 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "43 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "44 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "45 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "46 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "47 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "48 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "49 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "50 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "51 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "52 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "53 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "54 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "55 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "56 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "57 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "58 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "59 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "60 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "61 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "62 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "63 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "64 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "65 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "66 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "67 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "68 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "69 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "70 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "71 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "72 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "73 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "74 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "75 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "76 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "77 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "78 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "79 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "80 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "81 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "82 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "83 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "84 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "85 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "86 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "87 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "88 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "89 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "90 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "91 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "92 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "93 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "94 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "95 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "96 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "97 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "98 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "99 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "100 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "101 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "102 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "103 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "104 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "105 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "106 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "107 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "108 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "109 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "110 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "111 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "112 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "113 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "114 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "115 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "116 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "117 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "118 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "119 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "120 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "121 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "122 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "123 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "124 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "125 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "126 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "127 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "128 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "129 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "130 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "131 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "132 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "133 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "134 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "135 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "136 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "137 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "138 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "139 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "140 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "141 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "142 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "143 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "144 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "145 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "146 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "147 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "148 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "149 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "150 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "151 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "152 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "153 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "154 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "155 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "156 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "157 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "158 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "159 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "160 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "161 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "162 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "163 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "164 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "165 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "166 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "167 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "168 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "169 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "170 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "171 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "172 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "173 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "174 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "175 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "176 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "177 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "178 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "179 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "180 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "181 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "182 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "183 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "184 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "185 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "186 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "187 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "188 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "189 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "190 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "191 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "192 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "193 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "194 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "195 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "196 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "197 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "198 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "199 Train Loss 398540.38 Test RE 0.4239613276404677 c -0.14798753 k 0.07417022 m -0.0010689202\n",
      "Training time: 33.20\n",
      "Training time: 33.20\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 567327.1 Test RE 0.43754987303595927 c -0.12678564 k 0.40246943 m -0.0020983058\n",
      "1 Train Loss 398539.6 Test RE 0.42396090346681303 c -0.14395167 k 0.07411043 m -0.0023823741\n",
      "2 Train Loss 398539.6 Test RE 0.42396090038664347 c -0.14395636 k 0.07413055 m -0.0023824521\n",
      "3 Train Loss 398539.6 Test RE 0.42396089962830896 c -0.14395869 k 0.074140616 m -0.0023824908\n",
      "4 Train Loss 398539.6 Test RE 0.42396089943463733 c -0.14395986 k 0.07414564 m -0.0023825103\n",
      "5 Train Loss 398539.6 Test RE 0.42396089938737175 c -0.14396045 k 0.074148156 m -0.00238252\n",
      "6 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "7 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "8 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "9 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "10 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "11 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "12 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "13 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "14 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "15 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "16 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "17 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "18 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "19 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "20 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "21 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "22 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "23 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "24 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "25 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "26 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "27 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "28 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "29 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "30 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "31 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "32 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "33 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "34 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "35 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "36 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "37 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "38 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "39 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "40 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "41 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "42 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "43 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "44 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "45 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "46 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "47 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "48 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "49 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "50 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "51 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "52 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "53 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "54 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "55 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "56 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "57 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "58 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "59 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "60 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "61 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "62 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "63 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "64 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "65 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "66 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "67 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "68 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "69 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "70 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "71 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "72 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "73 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "74 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "75 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "76 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "77 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "78 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "79 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "80 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "81 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "82 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "83 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "84 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "85 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "86 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "87 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "88 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "89 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "90 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "91 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "92 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "93 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "94 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "95 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "96 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "97 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "98 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "99 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "100 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "101 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "102 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "103 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "104 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "105 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "106 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "107 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "108 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "109 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "110 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "111 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "112 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "113 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "114 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "115 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "116 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "117 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "118 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "119 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "120 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "121 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "122 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "123 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "124 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "125 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "126 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "127 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "128 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "129 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "130 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "131 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "132 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "133 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "134 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "135 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "136 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "137 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "138 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "139 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "140 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "141 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "142 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "143 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "144 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "145 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "146 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "147 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "148 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "149 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "150 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "151 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "152 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "153 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "154 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "155 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "156 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "157 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "158 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "159 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "160 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "161 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "162 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "163 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "164 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "165 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "166 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "167 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "168 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "169 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "170 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "171 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "172 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "173 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "174 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "175 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "176 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "177 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "178 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "179 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "180 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "181 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "182 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "183 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "184 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "185 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "186 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "187 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "188 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "189 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "190 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "191 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "192 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "193 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "194 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "195 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "196 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "197 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "198 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "199 Train Loss 398539.53 Test RE 0.4239608646689406 c -0.14396073 k 0.074149415 m -0.0023825245\n",
      "Training time: 15.01\n",
      "Training time: 15.01\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 462728.25 Test RE 0.44387442121447906 c 0.058531944 k -0.05277589 m -0.0011045431\n",
      "1 Train Loss 398539.66 Test RE 0.42396092861311974 c 0.0685466 k 0.074114256 m -0.0012929752\n",
      "2 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "3 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "4 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "5 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "6 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "7 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "8 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "9 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "10 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "11 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "12 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "13 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "14 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "15 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "16 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "17 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "18 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "19 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "20 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "21 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "22 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "23 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "24 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "25 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "26 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "27 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "28 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "29 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "30 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "31 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "32 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "33 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "34 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "35 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "36 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "37 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "38 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "39 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "40 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "41 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "42 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "43 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "44 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "45 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "46 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "47 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "48 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "49 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "50 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "51 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "52 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "53 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "54 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "55 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "56 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "57 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "58 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "59 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "60 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "61 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "62 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "63 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "64 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "65 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "66 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "67 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "68 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "69 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "70 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "71 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "72 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "73 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "74 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "75 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "76 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "77 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "78 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "79 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "80 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "81 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "82 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "83 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "84 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "85 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "86 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "87 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "88 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "89 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "90 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "91 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "92 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "93 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "94 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "95 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "96 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "97 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "98 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "99 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "100 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "101 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "102 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "103 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "104 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "105 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "106 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "107 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "108 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "109 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "110 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "111 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "112 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "113 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "114 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "115 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "116 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "117 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "118 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "119 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "120 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "121 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "122 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "123 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "124 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "125 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "126 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "127 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "128 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "129 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "130 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "131 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "132 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "133 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "134 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "135 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "136 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "137 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "138 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "139 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "140 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "141 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "142 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "143 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "144 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "145 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "146 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "147 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "148 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "149 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "150 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "151 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "152 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "153 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "154 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "155 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "156 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "157 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "158 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "159 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "160 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "161 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "162 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "163 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "164 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "165 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "166 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "167 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "168 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "169 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "170 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "171 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "172 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "173 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "174 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "175 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "176 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "177 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "178 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "179 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "180 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "181 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "182 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "183 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "184 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "185 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "186 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "187 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "188 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "189 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "190 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "191 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "192 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "193 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "194 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "195 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "196 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "197 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "198 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "199 Train Loss 398539.6 Test RE 0.423960906657792 c 0.06855255 k 0.07413249 m -0.0012930873\n",
      "Training time: 35.24\n",
      "Training time: 35.24\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 827374.6 Test RE 0.60567790311393 c 0.023430852 k 0.30974734 m -0.00061028043\n",
      "1 Train Loss 398541.12 Test RE 0.4239612473957202 c 0.045591913 k 0.07485323 m -0.0011870712\n",
      "2 Train Loss 398540.16 Test RE 0.42396119671900306 c 0.045601334 k 0.07419471 m -0.0011873167\n",
      "3 Train Loss 398540.16 Test RE 0.4239611996860259 c 0.045601655 k 0.07417205 m -0.0011873251\n",
      "4 Train Loss 398540.16 Test RE 0.4239611959879172 c 0.045601834 k 0.07415959 m -0.0011873298\n",
      "5 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "6 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "7 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "8 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "9 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "10 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "11 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "12 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "13 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "14 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "15 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "16 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "17 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "18 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "19 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "20 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "21 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "22 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "23 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "24 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "25 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "26 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "27 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "28 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "29 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "30 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "31 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "32 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "33 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "34 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "35 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "36 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "37 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "38 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "39 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "40 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "41 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "42 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "43 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "44 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "45 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "46 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "47 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "48 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "49 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "50 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "51 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "52 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "53 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "54 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "55 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "56 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "57 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "58 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "59 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "60 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "61 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "62 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "63 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "64 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "65 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "66 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "67 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "68 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "69 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "70 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "71 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "72 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "73 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "74 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "75 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "76 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "77 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "78 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "79 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "80 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "81 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "82 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "83 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "84 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "85 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "86 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "87 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "88 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "89 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "90 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "91 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "92 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "93 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "94 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "95 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "96 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "97 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "98 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "99 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "100 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "101 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "102 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "103 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "104 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "105 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "106 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "107 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "108 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "109 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "110 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "111 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "112 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "113 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "114 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "115 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "116 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "117 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "118 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "119 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "120 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "121 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "122 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "123 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "124 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "125 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "126 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "127 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "128 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "129 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "130 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "131 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "132 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "133 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "134 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "135 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "136 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "137 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "138 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "139 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "140 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "141 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "142 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "143 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "144 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "145 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "146 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "147 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "148 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "149 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "150 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "151 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "152 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "153 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "154 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "155 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "156 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "157 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "158 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "159 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "160 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "161 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "162 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "163 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "164 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "165 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "166 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "167 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "168 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "169 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "170 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "171 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "172 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "173 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "174 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "175 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "176 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "177 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "178 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "179 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "180 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "181 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "182 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "183 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "184 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "185 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "186 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "187 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "188 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "189 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "190 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "191 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "192 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "193 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "194 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "195 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "196 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "197 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "198 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "199 Train Loss 398540.12 Test RE 0.42396119572079 c 0.045601945 k 0.07415122 m -0.0011873327\n",
      "Training time: 24.63\n",
      "Training time: 24.63\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 1035249.4 Test RE 0.6828059502895338 c 0.039402407 k 0.111031294 m -0.0010593159\n",
      "1 Train Loss 398540.53 Test RE 0.42396137892898544 c 0.09359639 k 0.07409313 m -0.0025428452\n",
      "2 Train Loss 398538.06 Test RE 0.4239600937001565 c 0.09367781 k 0.07415333 m -0.0025450764\n",
      "3 Train Loss 398538.06 Test RE 0.42396008996783635 c 0.09367537 k 0.07415031 m -0.0025450091\n",
      "4 Train Loss 398538.03 Test RE 0.42396006842372946 c 0.09367164 k 0.07414522 m -0.0025449062\n",
      "5 Train Loss 398538.03 Test RE 0.42396005358248906 c 0.09366965 k 0.074141875 m -0.0025448503\n",
      "6 Train Loss 398533.9 Test RE 0.4239578619926832 c 0.09387287 k 0.07418221 m -0.0025502872\n",
      "7 Train Loss 398316.88 Test RE 0.4238423055082587 c 0.09493463 k 0.07386691 m -0.002578911\n",
      "8 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "9 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "10 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "11 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "12 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "13 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "14 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "15 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "16 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "17 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "18 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "19 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "20 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "21 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "22 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "23 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "24 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "25 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "26 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "27 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "28 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "29 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "30 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "31 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "32 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "33 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "34 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "35 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "36 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "37 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "38 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "39 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "40 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "41 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "42 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "43 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "44 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "45 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "46 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "47 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "48 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "49 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "50 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "51 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "52 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "53 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "54 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "55 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "56 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "57 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "58 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "59 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "60 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "61 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "62 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "63 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "64 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "65 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "66 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "67 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "68 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "69 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "70 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "71 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "72 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "73 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "74 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "75 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "76 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "77 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "78 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "79 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "80 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "81 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "82 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "83 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "84 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "85 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "86 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "87 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "88 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "89 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "90 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "91 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "92 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "93 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "94 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "95 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "96 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "97 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "98 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "99 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "100 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "101 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "102 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "103 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "104 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "105 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "106 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "107 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "108 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "109 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "110 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "111 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "112 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "113 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "114 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "115 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "116 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "117 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "118 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "119 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "120 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "121 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "122 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "123 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "124 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "125 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "126 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "127 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "128 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "129 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "130 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "131 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "132 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "133 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "134 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "135 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "136 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "137 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "138 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "139 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "140 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "141 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "142 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "143 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "144 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "145 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "146 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "147 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "148 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "149 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "150 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "151 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "152 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "153 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "154 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "155 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "156 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "157 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "158 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "159 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "160 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "161 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "162 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "163 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "164 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "165 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "166 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "167 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "168 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "169 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "170 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "171 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "172 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "173 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "174 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "175 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "176 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "177 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "178 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "179 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "180 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "181 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "182 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "183 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "184 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "185 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "186 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "187 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "188 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "189 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "190 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "191 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "192 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "193 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "194 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "195 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "196 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "197 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "198 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "199 Train Loss 397381.84 Test RE 0.4233446491067489 c 0.0955682 k 0.07446498 m -0.0025958824\n",
      "Training time: 24.21\n",
      "Training time: 24.21\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 399107.6 Test RE 0.42413625030143665 c -0.21666814 k 0.06356122 m -0.0010705375\n",
      "1 Train Loss 398539.6 Test RE 0.4239608999618638 c -0.21962003 k 0.07413192 m -0.0010850984\n",
      "2 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "3 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "4 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "5 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "6 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "7 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "8 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "9 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "10 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "11 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "12 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "13 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "14 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "15 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "16 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "17 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "18 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "19 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "20 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "21 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "22 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "23 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "24 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "25 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "26 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "27 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "28 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "29 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "30 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "31 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "32 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "33 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "34 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "35 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "36 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "37 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "38 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "39 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "40 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "41 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "42 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "43 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "44 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "45 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "46 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "47 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "48 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "49 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "50 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "51 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "52 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "53 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "54 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "55 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "56 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "57 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "58 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "59 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "60 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "61 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "62 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "63 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "64 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "65 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "66 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "67 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "68 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "69 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "70 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "71 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "72 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "73 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "74 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "75 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "76 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "77 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "78 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "79 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "80 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "81 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "82 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "83 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "84 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "85 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "86 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "87 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "88 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "89 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "90 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "91 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "92 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "93 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "94 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "95 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "96 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "97 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "98 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "99 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "100 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "101 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "102 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "103 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "104 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "105 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "106 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "107 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "108 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "109 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "110 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "111 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "112 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "113 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "114 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "115 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "116 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "117 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "118 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "119 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "120 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "121 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "122 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "123 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "124 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "125 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "126 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "127 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "128 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "129 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "130 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "131 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "132 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "133 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "134 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "135 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "136 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "137 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "138 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "139 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "140 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "141 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "142 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "143 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "144 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "145 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "146 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "147 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "148 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "149 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "150 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "151 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "152 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "153 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "154 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "155 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "156 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "157 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "158 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "159 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "160 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "161 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "162 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "163 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "164 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "165 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "166 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "167 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "168 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "169 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "170 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "171 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "172 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "173 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "174 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "175 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "176 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "177 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "178 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "179 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "180 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "181 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "182 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "183 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "184 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "185 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "186 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "187 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "188 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "189 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "190 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "191 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "192 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "193 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "194 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "195 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "196 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "197 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "198 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "199 Train Loss 398539.6 Test RE 0.423960899520435 c -0.21962273 k 0.074141294 m -0.0010851116\n",
      "Training time: 23.19\n",
      "Training time: 23.19\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 773582.25 Test RE 0.575040861522534 c 0.14791879 k -0.13111725 m 0.0005119895\n",
      "1 Train Loss 398540.03 Test RE 0.423961096259235 c 0.275946 k 0.07397431 m 0.000962804\n",
      "2 Train Loss 398539.97 Test RE 0.4239610927838502 c 0.27593678 k 0.07411744 m 0.0009627689\n",
      "3 Train Loss 398539.97 Test RE 0.4239610896797556 c 0.27593338 k 0.07414869 m 0.00096275564\n",
      "4 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "5 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "6 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "7 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "8 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "9 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "10 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "11 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "12 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "13 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "14 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "15 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "16 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "17 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "18 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "19 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "20 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "21 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "22 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "23 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "24 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "25 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "26 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "27 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "28 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "29 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "30 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "31 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "32 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "33 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "34 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "35 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "36 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "37 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "38 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "39 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "40 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "41 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "42 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "43 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "44 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "45 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "46 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "47 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "48 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "49 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "50 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "51 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "52 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "53 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "54 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "55 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "56 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "57 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "58 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "59 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "60 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "61 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "62 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "63 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "64 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "65 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "66 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "67 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "68 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "69 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "70 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "71 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "72 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "73 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "74 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "75 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "76 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "77 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "78 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "79 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "80 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "81 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "82 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "83 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "84 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "85 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "86 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "87 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "88 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "89 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "90 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "91 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "92 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "93 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "94 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "95 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "96 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "97 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "98 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "99 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "100 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "101 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "102 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "103 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "104 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "105 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "106 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "107 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "108 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "109 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "110 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "111 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "112 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "113 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "114 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "115 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "116 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "117 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "118 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "119 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "120 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "121 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "122 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "123 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "124 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "125 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "126 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "127 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "128 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "129 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "130 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "131 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "132 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "133 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "134 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "135 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "136 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "137 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "138 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "139 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "140 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "141 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "142 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "143 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "144 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "145 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "146 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "147 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "148 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "149 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "150 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "151 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "152 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "153 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "154 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "155 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "156 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "157 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "158 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "159 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "160 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "161 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "162 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "163 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "164 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "165 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "166 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "167 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "168 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "169 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "170 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "171 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "172 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "173 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "174 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "175 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "176 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "177 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "178 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "179 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "180 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "181 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "182 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "183 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "184 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "185 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "186 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "187 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "188 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "189 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "190 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "191 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "192 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "193 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "194 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "195 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "196 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "197 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "198 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "199 Train Loss 398539.9 Test RE 0.4239610807471833 c 0.2759292 k 0.074174955 m 0.00096273917\n",
      "Training time: 16.68\n",
      "Training time: 16.68\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 515566.66 Test RE 0.46621690546053685 c 0.30792516 k 0.2673705 m -0.00047492495\n",
      "1 Train Loss 398559.84 Test RE 0.42397166342289977 c 0.38283452 k 0.07411386 m -0.0005893536\n",
      "2 Train Loss 398544.34 Test RE 0.4239632359996831 c 0.3837637 k 0.07448336 m -0.0005909451\n",
      "3 Train Loss 398538.53 Test RE 0.42396033027290014 c 0.3834255 k 0.074142896 m -0.00059049815\n",
      "4 Train Loss 398538.53 Test RE 0.4239603184883839 c 0.38340434 k 0.07413101 m -0.000590464\n",
      "5 Train Loss 398510.16 Test RE 0.4239449817672897 c 0.38104874 k 0.07374754 m -0.000585903\n",
      "6 Train Loss 397963.88 Test RE 0.423649251742382 c 0.3752403 k 0.07219295 m -0.0005737845\n",
      "7 Train Loss 388863.66 Test RE 0.4187806121982916 c 0.3720265 k 0.07387708 m -0.0005566516\n",
      "8 Train Loss 373475.53 Test RE 0.41039147383908453 c 0.3590937 k 0.07438405 m 0.00095782615\n",
      "9 Train Loss 368714.53 Test RE 0.4077510367012401 c -0.029330157 k 0.07697113 m 0.005248792\n",
      "10 Train Loss 363111.38 Test RE 0.4045524505464079 c -0.56088245 k 0.06464017 m -0.018147893\n",
      "11 Train Loss 339178.78 Test RE 0.38628922997041554 c -3.0075283 k 0.144444 m 0.031512413\n",
      "12 Train Loss 110531.06 Test RE 0.22242615734607327 c -28.382055 k 0.07378097 m 0.3976364\n",
      "13 Train Loss nan Test RE nan c nan k nan m nan\n",
      "14 Train Loss nan Test RE nan c nan k nan m nan\n",
      "15 Train Loss nan Test RE nan c nan k nan m nan\n",
      "16 Train Loss nan Test RE nan c nan k nan m nan\n",
      "17 Train Loss nan Test RE nan c nan k nan m nan\n",
      "18 Train Loss nan Test RE nan c nan k nan m nan\n",
      "19 Train Loss nan Test RE nan c nan k nan m nan\n",
      "20 Train Loss nan Test RE nan c nan k nan m nan\n",
      "21 Train Loss nan Test RE nan c nan k nan m nan\n",
      "22 Train Loss nan Test RE nan c nan k nan m nan\n",
      "23 Train Loss nan Test RE nan c nan k nan m nan\n",
      "24 Train Loss nan Test RE nan c nan k nan m nan\n",
      "25 Train Loss nan Test RE nan c nan k nan m nan\n",
      "26 Train Loss nan Test RE nan c nan k nan m nan\n",
      "27 Train Loss nan Test RE nan c nan k nan m nan\n",
      "28 Train Loss nan Test RE nan c nan k nan m nan\n",
      "29 Train Loss nan Test RE nan c nan k nan m nan\n",
      "30 Train Loss nan Test RE nan c nan k nan m nan\n",
      "31 Train Loss nan Test RE nan c nan k nan m nan\n",
      "32 Train Loss nan Test RE nan c nan k nan m nan\n",
      "33 Train Loss nan Test RE nan c nan k nan m nan\n",
      "34 Train Loss nan Test RE nan c nan k nan m nan\n",
      "35 Train Loss nan Test RE nan c nan k nan m nan\n",
      "36 Train Loss nan Test RE nan c nan k nan m nan\n",
      "37 Train Loss nan Test RE nan c nan k nan m nan\n",
      "38 Train Loss nan Test RE nan c nan k nan m nan\n",
      "39 Train Loss nan Test RE nan c nan k nan m nan\n",
      "40 Train Loss nan Test RE nan c nan k nan m nan\n",
      "41 Train Loss nan Test RE nan c nan k nan m nan\n",
      "42 Train Loss nan Test RE nan c nan k nan m nan\n",
      "43 Train Loss nan Test RE nan c nan k nan m nan\n",
      "44 Train Loss nan Test RE nan c nan k nan m nan\n",
      "45 Train Loss nan Test RE nan c nan k nan m nan\n",
      "46 Train Loss nan Test RE nan c nan k nan m nan\n",
      "47 Train Loss nan Test RE nan c nan k nan m nan\n",
      "48 Train Loss nan Test RE nan c nan k nan m nan\n",
      "49 Train Loss nan Test RE nan c nan k nan m nan\n",
      "50 Train Loss nan Test RE nan c nan k nan m nan\n",
      "51 Train Loss nan Test RE nan c nan k nan m nan\n",
      "52 Train Loss nan Test RE nan c nan k nan m nan\n",
      "53 Train Loss nan Test RE nan c nan k nan m nan\n",
      "54 Train Loss nan Test RE nan c nan k nan m nan\n",
      "55 Train Loss nan Test RE nan c nan k nan m nan\n",
      "56 Train Loss nan Test RE nan c nan k nan m nan\n",
      "57 Train Loss nan Test RE nan c nan k nan m nan\n",
      "58 Train Loss nan Test RE nan c nan k nan m nan\n",
      "59 Train Loss nan Test RE nan c nan k nan m nan\n",
      "60 Train Loss nan Test RE nan c nan k nan m nan\n",
      "61 Train Loss nan Test RE nan c nan k nan m nan\n",
      "62 Train Loss nan Test RE nan c nan k nan m nan\n",
      "63 Train Loss nan Test RE nan c nan k nan m nan\n",
      "64 Train Loss nan Test RE nan c nan k nan m nan\n",
      "65 Train Loss nan Test RE nan c nan k nan m nan\n",
      "66 Train Loss nan Test RE nan c nan k nan m nan\n",
      "67 Train Loss nan Test RE nan c nan k nan m nan\n",
      "68 Train Loss nan Test RE nan c nan k nan m nan\n",
      "69 Train Loss nan Test RE nan c nan k nan m nan\n",
      "70 Train Loss nan Test RE nan c nan k nan m nan\n",
      "71 Train Loss nan Test RE nan c nan k nan m nan\n",
      "72 Train Loss nan Test RE nan c nan k nan m nan\n",
      "73 Train Loss nan Test RE nan c nan k nan m nan\n",
      "74 Train Loss nan Test RE nan c nan k nan m nan\n",
      "75 Train Loss nan Test RE nan c nan k nan m nan\n",
      "76 Train Loss nan Test RE nan c nan k nan m nan\n",
      "77 Train Loss nan Test RE nan c nan k nan m nan\n",
      "78 Train Loss nan Test RE nan c nan k nan m nan\n",
      "79 Train Loss nan Test RE nan c nan k nan m nan\n",
      "80 Train Loss nan Test RE nan c nan k nan m nan\n",
      "81 Train Loss nan Test RE nan c nan k nan m nan\n",
      "82 Train Loss nan Test RE nan c nan k nan m nan\n",
      "83 Train Loss nan Test RE nan c nan k nan m nan\n",
      "84 Train Loss nan Test RE nan c nan k nan m nan\n",
      "85 Train Loss nan Test RE nan c nan k nan m nan\n",
      "86 Train Loss nan Test RE nan c nan k nan m nan\n",
      "87 Train Loss nan Test RE nan c nan k nan m nan\n",
      "88 Train Loss nan Test RE nan c nan k nan m nan\n",
      "89 Train Loss nan Test RE nan c nan k nan m nan\n",
      "90 Train Loss nan Test RE nan c nan k nan m nan\n",
      "91 Train Loss nan Test RE nan c nan k nan m nan\n",
      "92 Train Loss nan Test RE nan c nan k nan m nan\n",
      "93 Train Loss nan Test RE nan c nan k nan m nan\n",
      "94 Train Loss nan Test RE nan c nan k nan m nan\n",
      "95 Train Loss nan Test RE nan c nan k nan m nan\n",
      "96 Train Loss nan Test RE nan c nan k nan m nan\n",
      "97 Train Loss nan Test RE nan c nan k nan m nan\n",
      "98 Train Loss nan Test RE nan c nan k nan m nan\n",
      "99 Train Loss nan Test RE nan c nan k nan m nan\n",
      "100 Train Loss nan Test RE nan c nan k nan m nan\n",
      "101 Train Loss nan Test RE nan c nan k nan m nan\n",
      "102 Train Loss nan Test RE nan c nan k nan m nan\n",
      "103 Train Loss nan Test RE nan c nan k nan m nan\n",
      "104 Train Loss nan Test RE nan c nan k nan m nan\n",
      "105 Train Loss nan Test RE nan c nan k nan m nan\n",
      "106 Train Loss nan Test RE nan c nan k nan m nan\n",
      "107 Train Loss nan Test RE nan c nan k nan m nan\n",
      "108 Train Loss nan Test RE nan c nan k nan m nan\n",
      "109 Train Loss nan Test RE nan c nan k nan m nan\n",
      "110 Train Loss nan Test RE nan c nan k nan m nan\n",
      "111 Train Loss nan Test RE nan c nan k nan m nan\n",
      "112 Train Loss nan Test RE nan c nan k nan m nan\n",
      "113 Train Loss nan Test RE nan c nan k nan m nan\n",
      "114 Train Loss nan Test RE nan c nan k nan m nan\n",
      "115 Train Loss nan Test RE nan c nan k nan m nan\n",
      "116 Train Loss nan Test RE nan c nan k nan m nan\n",
      "117 Train Loss nan Test RE nan c nan k nan m nan\n",
      "118 Train Loss nan Test RE nan c nan k nan m nan\n",
      "119 Train Loss nan Test RE nan c nan k nan m nan\n",
      "120 Train Loss nan Test RE nan c nan k nan m nan\n",
      "121 Train Loss nan Test RE nan c nan k nan m nan\n",
      "122 Train Loss nan Test RE nan c nan k nan m nan\n",
      "123 Train Loss nan Test RE nan c nan k nan m nan\n",
      "124 Train Loss nan Test RE nan c nan k nan m nan\n",
      "125 Train Loss nan Test RE nan c nan k nan m nan\n",
      "126 Train Loss nan Test RE nan c nan k nan m nan\n",
      "127 Train Loss nan Test RE nan c nan k nan m nan\n",
      "128 Train Loss nan Test RE nan c nan k nan m nan\n",
      "129 Train Loss nan Test RE nan c nan k nan m nan\n",
      "130 Train Loss nan Test RE nan c nan k nan m nan\n",
      "131 Train Loss nan Test RE nan c nan k nan m nan\n",
      "132 Train Loss nan Test RE nan c nan k nan m nan\n",
      "133 Train Loss nan Test RE nan c nan k nan m nan\n",
      "134 Train Loss nan Test RE nan c nan k nan m nan\n",
      "135 Train Loss nan Test RE nan c nan k nan m nan\n",
      "136 Train Loss nan Test RE nan c nan k nan m nan\n",
      "137 Train Loss nan Test RE nan c nan k nan m nan\n",
      "138 Train Loss nan Test RE nan c nan k nan m nan\n",
      "139 Train Loss nan Test RE nan c nan k nan m nan\n",
      "140 Train Loss nan Test RE nan c nan k nan m nan\n",
      "141 Train Loss nan Test RE nan c nan k nan m nan\n",
      "142 Train Loss nan Test RE nan c nan k nan m nan\n",
      "143 Train Loss nan Test RE nan c nan k nan m nan\n",
      "144 Train Loss nan Test RE nan c nan k nan m nan\n",
      "145 Train Loss nan Test RE nan c nan k nan m nan\n",
      "146 Train Loss nan Test RE nan c nan k nan m nan\n",
      "147 Train Loss nan Test RE nan c nan k nan m nan\n",
      "148 Train Loss nan Test RE nan c nan k nan m nan\n",
      "149 Train Loss nan Test RE nan c nan k nan m nan\n",
      "150 Train Loss nan Test RE nan c nan k nan m nan\n",
      "151 Train Loss nan Test RE nan c nan k nan m nan\n",
      "152 Train Loss nan Test RE nan c nan k nan m nan\n",
      "153 Train Loss nan Test RE nan c nan k nan m nan\n",
      "154 Train Loss nan Test RE nan c nan k nan m nan\n",
      "155 Train Loss nan Test RE nan c nan k nan m nan\n",
      "156 Train Loss nan Test RE nan c nan k nan m nan\n",
      "157 Train Loss nan Test RE nan c nan k nan m nan\n",
      "158 Train Loss nan Test RE nan c nan k nan m nan\n",
      "159 Train Loss nan Test RE nan c nan k nan m nan\n",
      "160 Train Loss nan Test RE nan c nan k nan m nan\n",
      "161 Train Loss nan Test RE nan c nan k nan m nan\n",
      "162 Train Loss nan Test RE nan c nan k nan m nan\n",
      "163 Train Loss nan Test RE nan c nan k nan m nan\n",
      "164 Train Loss nan Test RE nan c nan k nan m nan\n",
      "165 Train Loss nan Test RE nan c nan k nan m nan\n",
      "166 Train Loss nan Test RE nan c nan k nan m nan\n",
      "167 Train Loss nan Test RE nan c nan k nan m nan\n",
      "168 Train Loss nan Test RE nan c nan k nan m nan\n",
      "169 Train Loss nan Test RE nan c nan k nan m nan\n",
      "170 Train Loss nan Test RE nan c nan k nan m nan\n",
      "171 Train Loss nan Test RE nan c nan k nan m nan\n",
      "172 Train Loss nan Test RE nan c nan k nan m nan\n",
      "173 Train Loss nan Test RE nan c nan k nan m nan\n",
      "174 Train Loss nan Test RE nan c nan k nan m nan\n",
      "175 Train Loss nan Test RE nan c nan k nan m nan\n",
      "176 Train Loss nan Test RE nan c nan k nan m nan\n",
      "177 Train Loss nan Test RE nan c nan k nan m nan\n",
      "178 Train Loss nan Test RE nan c nan k nan m nan\n",
      "179 Train Loss nan Test RE nan c nan k nan m nan\n",
      "180 Train Loss nan Test RE nan c nan k nan m nan\n",
      "181 Train Loss nan Test RE nan c nan k nan m nan\n",
      "182 Train Loss nan Test RE nan c nan k nan m nan\n",
      "183 Train Loss nan Test RE nan c nan k nan m nan\n",
      "184 Train Loss nan Test RE nan c nan k nan m nan\n",
      "185 Train Loss nan Test RE nan c nan k nan m nan\n",
      "186 Train Loss nan Test RE nan c nan k nan m nan\n",
      "187 Train Loss nan Test RE nan c nan k nan m nan\n",
      "188 Train Loss nan Test RE nan c nan k nan m nan\n",
      "189 Train Loss nan Test RE nan c nan k nan m nan\n",
      "190 Train Loss nan Test RE nan c nan k nan m nan\n",
      "191 Train Loss nan Test RE nan c nan k nan m nan\n",
      "192 Train Loss nan Test RE nan c nan k nan m nan\n",
      "193 Train Loss nan Test RE nan c nan k nan m nan\n",
      "194 Train Loss nan Test RE nan c nan k nan m nan\n",
      "195 Train Loss nan Test RE nan c nan k nan m nan\n",
      "196 Train Loss nan Test RE nan c nan k nan m nan\n",
      "197 Train Loss nan Test RE nan c nan k nan m nan\n",
      "198 Train Loss nan Test RE nan c nan k nan m nan\n",
      "199 Train Loss nan Test RE nan c nan k nan m nan\n",
      "Training time: 88.45\n",
      "Training time: 88.45\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 398713.56 Test RE 0.42404079642088266 c 0.14280313 k 0.07848884 m -0.0007447498\n",
      "1 Train Loss 398459.12 Test RE 0.42390782715033487 c 0.14382102 k 0.07052311 m -0.0007500763\n",
      "2 Train Loss 397337.75 Test RE 0.4231144300873478 c 0.1394594 k 0.09171702 m -0.0007274123\n",
      "3 Train Loss 368003.44 Test RE 0.407247994029562 c 0.14758909 k 0.06048532 m -0.0007688981\n",
      "4 Train Loss 363685.5 Test RE 0.4049809619031385 c 0.14775965 k 0.076820314 m -0.0006696665\n",
      "5 Train Loss 361227.6 Test RE 0.40351260951458057 c 0.12790005 k 0.06448811 m 0.00016836631\n",
      "6 Train Loss 358244.44 Test RE 0.4015781143746253 c 0.09320388 k 0.05704027 m 0.009217143\n",
      "7 Train Loss 356937.1 Test RE 0.4011180363195932 c 0.08627747 k 0.08152487 m -0.0007980865\n",
      "8 Train Loss 355714.62 Test RE 0.4004662613270759 c 0.07347061 k 0.07854827 m -0.00038541792\n",
      "9 Train Loss 355522.2 Test RE 0.4004118982397991 c 0.07184612 k 0.07243922 m -0.00016889705\n",
      "10 Train Loss 353773.12 Test RE 0.3993841078807755 c 0.026564501 k 0.075176366 m -0.00045661075\n",
      "11 Train Loss 353050.78 Test RE 0.39901695647808716 c 0.011190929 k 0.07525647 m 0.0003839632\n",
      "12 Train Loss 351458.34 Test RE 0.3980492705141124 c -0.016359854 k 0.068008274 m -0.0006903232\n",
      "13 Train Loss 349974.06 Test RE 0.39721179860663125 c -0.044402037 k 0.06537488 m 7.515546e-05\n",
      "14 Train Loss 349412.3 Test RE 0.39693923828475 c -0.053477336 k 0.077486336 m -0.0002346807\n",
      "15 Train Loss 349064.12 Test RE 0.396753493382578 c -0.058698937 k 0.071977526 m -9.840869e-05\n",
      "16 Train Loss 348442.8 Test RE 0.3963915114646502 c -0.06356888 k 0.07175242 m -0.0006677183\n",
      "17 Train Loss 346756.84 Test RE 0.3954174216845508 c -0.07965609 k 0.071915954 m 0.0003369297\n",
      "18 Train Loss 345809.47 Test RE 0.3948316575638808 c -0.09364562 k 0.07799543 m 0.0006320073\n",
      "19 Train Loss 344667.75 Test RE 0.39408428303871196 c -0.10349952 k 0.06656808 m 0.00039241815\n",
      "20 Train Loss 343088.5 Test RE 0.39326303995628803 c -0.11372756 k 0.075530626 m 3.8381917e-05\n",
      "21 Train Loss 342149.28 Test RE 0.3927877936834958 c -0.12082227 k 0.0720235 m 4.3964334e-05\n",
      "22 Train Loss 341307.94 Test RE 0.39231682776549415 c -0.12683368 k 0.072964825 m 3.666845e-05\n",
      "23 Train Loss 340981.12 Test RE 0.3921261152151521 c -0.12863326 k 0.07006611 m 3.206918e-05\n",
      "24 Train Loss 340710.78 Test RE 0.39194735344132253 c -0.13069388 k 0.07345772 m 3.1776613e-05\n",
      "25 Train Loss 340369.66 Test RE 0.391782059964282 c -0.13112201 k 0.074487515 m 2.8938584e-05\n",
      "26 Train Loss 340220.5 Test RE 0.3917038253916659 c -0.13258557 k 0.07371946 m 2.808311e-05\n",
      "27 Train Loss 340098.06 Test RE 0.39162447830283736 c -0.13246617 k 0.07415802 m 2.5271775e-05\n",
      "28 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "29 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "30 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "31 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "32 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "33 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "34 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "35 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "36 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "37 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "38 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "39 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "40 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "41 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "42 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "43 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "44 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "45 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "46 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "47 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "48 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "49 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "50 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "51 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "52 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "53 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "54 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "55 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "56 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "57 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "58 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "59 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "60 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "61 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "62 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "63 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "64 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "65 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "66 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "67 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "68 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "69 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "70 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "71 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "72 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "73 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "74 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "75 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "76 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "77 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "78 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "79 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "80 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "81 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "82 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "83 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "84 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "85 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "86 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "87 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "88 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "89 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "90 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "91 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "92 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "93 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "94 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "95 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "96 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "97 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "98 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "99 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "100 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "101 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "102 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "103 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "104 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "105 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "106 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "107 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "108 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "109 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "110 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "111 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "112 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "113 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "114 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "115 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "116 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "117 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "118 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "119 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "120 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "121 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "122 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "123 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "124 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "125 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "126 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "127 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "128 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "129 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "130 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "131 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "132 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "133 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "134 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "135 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "136 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "137 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "138 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "139 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "140 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "141 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "142 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "143 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "144 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "145 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "146 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "147 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "148 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "149 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "150 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "151 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "152 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "153 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "154 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "155 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "156 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "157 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "158 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "159 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "160 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "161 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "162 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "163 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "164 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "165 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "166 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "167 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "168 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "169 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "170 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "171 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "172 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "173 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "174 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "175 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "176 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "177 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "178 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "179 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "180 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "181 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "182 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "183 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "184 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "185 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "186 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "187 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "188 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "189 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "190 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "191 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "192 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "193 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "194 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "195 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "196 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "197 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "198 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "199 Train Loss 339894.94 Test RE 0.39150798490547367 c -0.13330902 k 0.0735121 m 2.6336038e-05\n",
      "Training time: 43.75\n",
      "Training time: 43.75\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 1071276.0 Test RE 0.6850520645920603 c 0.068014875 k 0.5029024 m -0.00019189133\n",
      "1 Train Loss 396934.34 Test RE 0.4229098387110534 c 0.18552966 k 0.08681887 m -0.0005211471\n",
      "2 Train Loss 390316.34 Test RE 0.4193805622094772 c 0.1910096 k 0.08625088 m -0.00053546\n",
      "3 Train Loss 375089.75 Test RE 0.4112962334944968 c 0.18528597 k 0.072089374 m -0.00050386303\n",
      "4 Train Loss 373828.7 Test RE 0.41060532667608485 c 0.18067683 k 0.07404053 m -0.00034518732\n",
      "5 Train Loss 372227.6 Test RE 0.40971482322485775 c 0.1715103 k 0.07704269 m 0.0004048917\n",
      "6 Train Loss 372028.9 Test RE 0.40961070590991594 c 0.17154562 k 0.07585287 m 0.00043894272\n",
      "7 Train Loss 371646.78 Test RE 0.40939626045381144 c 0.17025913 k 0.070645064 m 0.00055306166\n",
      "8 Train Loss 369746.4 Test RE 0.4083128375445503 c 0.16315539 k 0.079839185 m 0.0015663716\n",
      "9 Train Loss 368463.44 Test RE 0.40764295630537406 c 0.1581522 k 0.07068047 m 0.00243477\n",
      "10 Train Loss 366827.88 Test RE 0.40673390911281965 c 0.14868546 k 0.072842166 m 0.0032565405\n",
      "11 Train Loss 364430.28 Test RE 0.4054058177338099 c 0.13813682 k 0.07540783 m 0.0028432372\n",
      "12 Train Loss 362236.5 Test RE 0.40416017747805716 c 0.12720393 k 0.07929509 m 0.003016188\n",
      "13 Train Loss 357219.62 Test RE 0.4012550232161178 c 0.10480672 k 0.08531301 m 0.0036856758\n",
      "14 Train Loss 354328.44 Test RE 0.3997323363599091 c 0.10148058 k 0.069457345 m 0.0041759918\n",
      "15 Train Loss 353245.72 Test RE 0.39913413384963287 c 0.10370319 k 0.07594198 m 0.0042456533\n",
      "16 Train Loss 352475.12 Test RE 0.39869413751227717 c 0.10613546 k 0.07100459 m 0.00428401\n",
      "17 Train Loss 352389.3 Test RE 0.3986518757468924 c 0.106144965 k 0.07280603 m 0.0042889747\n",
      "18 Train Loss 352082.4 Test RE 0.3984764353411426 c 0.10527546 k 0.07283383 m 0.004190787\n",
      "19 Train Loss 350935.0 Test RE 0.39779901694390635 c 0.10227566 k 0.07543579 m 0.0040835026\n",
      "20 Train Loss 349967.84 Test RE 0.3972518700139175 c 0.098908424 k 0.07505306 m 0.0040551033\n",
      "21 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "22 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "23 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "24 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "25 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "26 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "27 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "28 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "29 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "30 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "31 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "32 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "33 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "34 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "35 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "36 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "37 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "38 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "39 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "40 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "41 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "42 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "43 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "44 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "45 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "46 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "47 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "48 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "49 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "50 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "51 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "52 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "53 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "54 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "55 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "56 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "57 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "58 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "59 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "60 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "61 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "62 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "63 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "64 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "65 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "66 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "67 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "68 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "69 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "70 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "71 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "72 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "73 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "74 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "75 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "76 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "77 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "78 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "79 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "80 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "81 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "82 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "83 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "84 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "85 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "86 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "87 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "88 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "89 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "90 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "91 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "92 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "93 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "94 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "95 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "96 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "97 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "98 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "99 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "100 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "101 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "102 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "103 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "104 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "105 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "106 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "107 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "108 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "109 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "110 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "111 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "112 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "113 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "114 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "115 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "116 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "117 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "118 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "119 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "120 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "121 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "122 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "123 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "124 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "125 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "126 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "127 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "128 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "129 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "130 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "131 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "132 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "133 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "134 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "135 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "136 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "137 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "138 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "139 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "140 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "141 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "142 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "143 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "144 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "145 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "146 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "147 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "148 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "149 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "150 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "151 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "152 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "153 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "154 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "155 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "156 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "157 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "158 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "159 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "160 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "161 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "162 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "163 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "164 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "165 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "166 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "167 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "168 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "169 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "170 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "171 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "172 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "173 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "174 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "175 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "176 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "177 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "178 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "179 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "180 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "181 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "182 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "183 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "184 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "185 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "186 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "187 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "188 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "189 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "190 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "191 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "192 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "193 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "194 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "195 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "196 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "197 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "198 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "199 Train Loss 349237.84 Test RE 0.3968471304815841 c 0.09495093 k 0.07552779 m 0.0040286435\n",
      "Training time: 38.20\n",
      "Training time: 38.20\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 200\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "m_full = []\n",
    "k_full = []\n",
    "c_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "\n",
    "for reps in range(max_reps):   \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []   \n",
    "    beta_val = []\n",
    "    m_val = []\n",
    "    k_val = []\n",
    "    c_val = []\n",
    "\n",
    "    torch.manual_seed(reps*36)\n",
    "    N_f = 10000 #Total number of collocation points\n",
    "\n",
    "    layers = np.array([1,50,50,50,1]) #9 hidden layers\n",
    "    PINN = Sequentialmodel(layers)\n",
    "\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.5, \n",
    "                                max_iter = 10, \n",
    "                                max_eval = 15, \n",
    "                                tolerance_grad = 1e-6, \n",
    "                                tolerance_change = 1e-6, \n",
    "                                history_size = 100, \n",
    "                                line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "    train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    beta_full.append(beta_val)\n",
    "    m_full.append(m_val)\n",
    "    k_full.append(k_val)\n",
    "    c_full.append(c_val)\n",
    "    \n",
    "\n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"beta\": beta_full,\"m\": m_full,\"k\": k_full,\"c\": c_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "pmHEeBpzfFQh",
    "outputId": "990b2054-f35e-4c8e-c378-69ff2eb19f4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7faf440d5d90>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_pred = PINN.test()\n",
    "plt.plot(t,x_true,'b')\n",
    "plt.plot(t,x_pred,'r--')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "stan_1D_FODE_tune.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
