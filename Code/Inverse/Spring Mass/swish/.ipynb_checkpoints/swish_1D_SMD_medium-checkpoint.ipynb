{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8byrnUmNKGR",
    "outputId": "510f67c6-b225-41ba-e78a-d0a6e54ded85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uoNYpDzlNKGV"
   },
   "outputs": [],
   "source": [
    "tstart= 0 \n",
    "tstop= 60 \n",
    "\n",
    "increment = 0.1 \n",
    "# Initial condition\n",
    "x_init= [0,0] \n",
    "\n",
    "t = np.arange(tstart,tstop+1,increment)\n",
    "\n",
    "c = 1 # Damping constant \n",
    "k = 0.05 # Stiffness of the spring\n",
    "m = 5 # Mass \n",
    "F0 = 100\n",
    "\n",
    "def mydiff(x, t):    \n",
    "    F =F0\n",
    "    \n",
    "    dx1dt = x[1] \n",
    "    dx2dt = (F -c*x[1] -k*x[0])/m\n",
    "    dxdt= [dx1dt, dx2dt] \n",
    "    \n",
    "    return dxdt \n",
    "\n",
    "x_full_sol = odeint(mydiff, x_init, t) \n",
    "\n",
    "x_sol = x_full_sol[:,0]\n",
    "v_sol = x_full_sol[:,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BR02v-fkNKGV"
   },
   "outputs": [],
   "source": [
    "level = \"medium\"\n",
    "label = \"1D_SMD_swish_\" + level\n",
    "\n",
    "loss_thresh = 0.005\n",
    "\n",
    "t_test = t.reshape(-1,1)\n",
    "t_test_tensor = torch.from_numpy(t_test).float().to(device)\n",
    "\n",
    "# bc1_t = t[0].reshape(-1,1)\n",
    "# bc1_x = x_sol[0].reshape(-1,1)\n",
    "# t_bc1_train = torch.from_numpy(bc1_t).float().to(device)\n",
    "# x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "\n",
    "bc1_t = t.reshape(-1,1)\n",
    "bc1_x = x_sol.reshape(-1,1)\n",
    "t_bc1_train = torch.from_numpy(bc1_t).float().to(device)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "\n",
    "\n",
    "bc2_t = t[0].reshape(-1,1)\n",
    "t_bc2_train = torch.from_numpy(bc2_t).float().to(device)\n",
    "bc2_val = v_sol[0].reshape(-1,1)\n",
    "bc2_val =torch.from_numpy(bc2_val).float().to(device)\n",
    "\n",
    "t_test = t.reshape(-1,1)\n",
    "t_test_tensor = torch.from_numpy(t_test).float().to(device)\n",
    "x_true = x_sol\n",
    "x_true_norm = np.linalg.norm(x_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(t[0]) \n",
    "ub = np.array(t[-1]) \n",
    "\n",
    "#torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SyyktBKBXRo1"
   },
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    \n",
    "    t01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=t01,random_state =seed)\n",
    "\n",
    "    t_coll = lb + (ub-lb)*sampling(N_f)\n",
    "    # t_coll = np.vstack((t_coll,)) # append training points to collocation points \n",
    "\n",
    "    return t_coll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "o1b21zLnNKGW"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "        \n",
    "        self.beta = Parameter(torch.ones((layers[1],len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "        \n",
    "        self.k = Parameter(torch.tensor(0.0))\n",
    "        self.k.requiresGrad = True\n",
    "        self.c = Parameter(torch.tensor(0.0))\n",
    "        self.c.requiresGrad = True\n",
    "        self.m = Parameter(torch.tensor(0.0))\n",
    "        self.m.requiresGrad = True\n",
    "        \n",
    "    'forward pass'\n",
    "    def forward(self,t):\n",
    "        if torch.is_tensor(t) != True:         \n",
    "            t = torch.from_numpy(t)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        t = 2.0*(t - l_b)/(u_b - l_b) - 1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = t.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = z*self.activation(self.beta[:,i]*z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,t,x):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(t), x)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,t_bc2,bc2_val):\n",
    "        g = t_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        x = self.forward(g)    \n",
    "            \n",
    "        x_t = autograd.grad(x,g,torch.ones([t_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dx_dt = x_t[:,[0]]\n",
    "        \n",
    "        bc2 = dx_dt\n",
    "        \n",
    "        loss_bc2= self.loss_function(bc2,bc2_val)\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self,t_coll,f_hat):\n",
    "             \n",
    "        g = t_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        x = self.forward(g) \n",
    "\n",
    "        x_t = autograd.grad(x,g,torch.ones([t_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        x_tt = autograd.grad(x_t,g,torch.ones(t_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dx_dt = x_t[:,[0]]\n",
    "        \n",
    "        dx2_d2t = x_tt[:,[0]]\n",
    "        \n",
    "        f = self.m*dx2_d2t + self.c*dx_dt + self.k*x - F0\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,t_bc1,x_bc1,t_bc2,bc2_val,t_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(t_bc1,x_bc1)\n",
    "        loss_bc2 = self.loss_BC2(t_bc2,bc2_val)\n",
    "        loss_f = self.loss_PDE(t_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self):\n",
    "        x_pred = self.forward(t_test_tensor)\n",
    "        x_pred = x_pred.cpu().detach().numpy()\n",
    "\n",
    "        return x_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        x_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(x_pred.reshape(-1,1) - x_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(x_pred.reshape(-1,1) - x_true.reshape(-1,1),2)/x_true_norm\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fLY2mT5BOgjD"
   },
   "outputs": [],
   "source": [
    "def train_step(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8srA5uGuObil"
   },
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    m_val.append(PINN.m.cpu().detach().numpy())\n",
    "    k_val.append(PINN.k.cpu().detach().numpy())\n",
    "    c_val.append(PINN.c.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0SezTZ_racQB"
   },
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "\n",
    "    t_coll = colloc_pts(N_f,rep*11)\n",
    "    t_coll =  torch.from_numpy(t_coll).float().to(device)\n",
    "    \n",
    "    f_hat = torch.zeros(t_coll.shape[0],1).to(device)\n",
    "    for i in range(max_iter):      \n",
    "        train_step(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat)\n",
    "         \n",
    "        loss_np = PINN.loss(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test RE\",test_re_loss[-1],\"c\",PINN.c.cpu().detach().numpy(),\"k\",PINN.k.cpu().detach().numpy(),\"m\",PINN.m.cpu().detach().numpy())\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I9BYbcJ0NKGX",
    "outputId": "42720d97-b37a-4c42-b7e5-b77a0ef391f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 877721.2 Test RE 0.34284771571626443 c -0.0016169231 k 0.14177355 m -0.00031479885\n",
      "1 Train Loss 114791.3 Test RE 0.03968195042523559 c 0.0070831585 k -0.0910734 m -0.0004100765\n",
      "2 Train Loss 6862.821 Test RE 0.024294037861103988 c 0.022695435 k 0.02651645 m -0.00020125337\n",
      "3 Train Loss 3737.6416 Test RE 0.012693727434949228 c 0.03788879 k 0.03433399 m 0.0009297146\n",
      "4 Train Loss 2949.177 Test RE 0.008931978115497397 c 0.08160361 k 0.030258471 m 0.0042049456\n",
      "5 Train Loss 2571.6458 Test RE 0.007806779663316648 c 0.15768486 k 0.028867476 m 0.009854333\n",
      "6 Train Loss 1223.0212 Test RE 0.004751087488218097 c 0.5372532 k 0.01980039 m 0.038495798\n",
      "7 Train Loss 772.8898 Test RE 0.00400119275909384 c 0.7247562 k 0.01568233 m 0.05339105\n",
      "8 Train Loss 436.74326 Test RE 0.004880648481780175 c 1.0091356 k 0.008878766 m 0.07797842\n",
      "9 Train Loss 316.60492 Test RE 0.0037618171428674767 c 1.0584619 k 0.007908646 m 0.08484429\n",
      "10 Train Loss 271.60693 Test RE 0.0029392128925463997 c 1.0744966 k 0.007419137 m 0.088186234\n",
      "11 Train Loss 243.30463 Test RE 0.003113712132859729 c 1.1417593 k 0.0057974323 m 0.097150534\n",
      "12 Train Loss 229.75443 Test RE 0.0029216674313145364 c 1.1707311 k 0.0051031 m 0.10399077\n",
      "13 Train Loss 226.98657 Test RE 0.0027669175416966106 c 1.1827734 k 0.004786211 m 0.1084757\n",
      "14 Train Loss 224.2944 Test RE 0.0026589406710724383 c 1.1939294 k 0.0044845883 m 0.116726644\n",
      "15 Train Loss 222.60759 Test RE 0.0025756807596327325 c 1.1909759 k 0.0046057366 m 0.12579691\n",
      "16 Train Loss 221.59282 Test RE 0.002594684562048467 c 1.1869391 k 0.0046832673 m 0.13617513\n",
      "17 Train Loss 220.89963 Test RE 0.0027205591894248395 c 1.1852574 k 0.00472278 m 0.13917662\n",
      "18 Train Loss 220.70605 Test RE 0.002728828138629929 c 1.1793901 k 0.0048635197 m 0.13809395\n",
      "19 Train Loss 220.1991 Test RE 0.0026936128710145363 c 1.173656 k 0.005003602 m 0.14674062\n",
      "20 Train Loss 218.93942 Test RE 0.00267269719021086 c 1.1877211 k 0.004685331 m 0.18358557\n",
      "21 Train Loss 217.06244 Test RE 0.0026174332403105863 c 1.1926591 k 0.0045917095 m 0.21842647\n",
      "22 Train Loss 216.01869 Test RE 0.0025438753023647553 c 1.1752981 k 0.0049746446 m 0.22557728\n",
      "23 Train Loss 213.07515 Test RE 0.0024891563518945063 c 1.1804944 k 0.0048548523 m 0.29043445\n",
      "24 Train Loss 209.29787 Test RE 0.0023901717513345167 c 1.1911433 k 0.0046415385 m 0.40321288\n",
      "25 Train Loss 207.47964 Test RE 0.0023181852017576834 c 1.1706182 k 0.005155481 m 0.47334695\n",
      "26 Train Loss 205.24966 Test RE 0.0024825192680622147 c 1.168685 k 0.005171337 m 0.53753203\n",
      "27 Train Loss 199.61623 Test RE 0.0024827252593772105 c 1.1794423 k 0.004982672 m 0.65687084\n",
      "28 Train Loss 192.28651 Test RE 0.001980135953277867 c 1.1512966 k 0.0056820163 m 0.7670473\n",
      "29 Train Loss 183.66786 Test RE 0.0019202364613378215 c 1.1737016 k 0.005156308 m 0.9453986\n",
      "30 Train Loss 180.17062 Test RE 0.0020379351406654795 c 1.1582134 k 0.005482496 m 0.9927208\n",
      "31 Train Loss 168.96126 Test RE 0.0021420219694026845 c 1.1438751 k 0.00588674 m 1.0976262\n",
      "32 Train Loss 159.30318 Test RE 0.0018414008314959493 c 1.1440529 k 0.0058971643 m 1.2958847\n",
      "33 Train Loss 146.03491 Test RE 0.0016230238958912582 c 1.1373711 k 0.006104569 m 1.5032367\n",
      "34 Train Loss 131.92155 Test RE 0.0014728920602124695 c 1.1415559 k 0.0060315765 m 1.6927081\n",
      "35 Train Loss 123.62025 Test RE 0.0014554139113865216 c 1.1326592 k 0.0062539442 m 1.8290513\n",
      "36 Train Loss 119.615555 Test RE 0.0013939578955327757 c 1.1205233 k 0.0064624744 m 1.8167361\n",
      "37 Train Loss 117.103424 Test RE 0.0014203671129392457 c 1.1343466 k 0.006278427 m 1.811738\n",
      "38 Train Loss 109.35219 Test RE 0.001351831429462075 c 1.1226021 k 0.006582197 m 1.9709466\n",
      "39 Train Loss 105.200554 Test RE 0.0011866825288994443 c 1.1087939 k 0.0068091657 m 2.0603247\n",
      "40 Train Loss 99.483894 Test RE 0.0012358575535884883 c 1.1105078 k 0.006854484 m 2.1160266\n",
      "41 Train Loss 93.65553 Test RE 0.0011616883857999785 c 1.1051528 k 0.0069053243 m 2.2146664\n",
      "42 Train Loss 77.580246 Test RE 0.0012420950040212238 c 1.0958325 k 0.007285809 m 2.5273056\n",
      "43 Train Loss 68.751366 Test RE 0.001216031958677342 c 1.1024135 k 0.0072027077 m 2.7333238\n",
      "44 Train Loss 50.493748 Test RE 0.000884371038641187 c 1.0531157 k 0.008318033 m 3.0817933\n",
      "45 Train Loss 39.452793 Test RE 0.0009531758303179672 c 1.066475 k 0.008083935 m 3.387143\n",
      "46 Train Loss 35.955376 Test RE 0.00092914369340129 c 1.073691 k 0.0077599185 m 3.5172992\n",
      "47 Train Loss 30.484173 Test RE 0.0008218067059535776 c 1.0607594 k 0.008213135 m 3.6927714\n",
      "48 Train Loss 22.587263 Test RE 0.0007542575005319174 c 1.0570501 k 0.008235133 m 3.9351811\n",
      "49 Train Loss 18.682957 Test RE 0.0006284086356139414 c 1.043129 k 0.008673706 m 4.079229\n",
      "50 Train Loss 14.393571 Test RE 0.0004214928108863913 c 1.0234554 k 0.009288856 m 4.414182\n",
      "51 Train Loss 12.2820635 Test RE 0.0003101394013602942 c 1.0208095 k 0.009321311 m 4.504272\n",
      "52 Train Loss 10.67365 Test RE 0.00031752058855475784 c 1.0216593 k 0.009378692 m 4.547722\n",
      "53 Train Loss 9.327526 Test RE 0.0003140411059982906 c 1.0116899 k 0.009611691 m 4.7197776\n",
      "54 Train Loss 7.696949 Test RE 0.0003011073747571939 c 1.0072223 k 0.00976857 m 4.814451\n",
      "55 Train Loss 6.0704317 Test RE 0.0003624181872866249 c 1.0037049 k 0.009940077 m 4.8974056\n",
      "56 Train Loss 3.9482737 Test RE 0.00020703245903974613 c 1.0006698 k 0.009946161 m 4.956316\n",
      "57 Train Loss 2.2293665 Test RE 0.00018284306895786123 c 1.0048656 k 0.009798021 m 4.950247\n",
      "58 Train Loss 1.7057879 Test RE 0.00020987483224879088 c 1.0054116 k 0.009783353 m 4.970689\n",
      "59 Train Loss 1.3486946 Test RE 0.0001710618611526063 c 1.0033951 k 0.009852478 m 5.0350857\n",
      "60 Train Loss 1.2156684 Test RE 0.00015963507150897294 c 1.0016491 k 0.009906188 m 5.042257\n",
      "61 Train Loss 1.1692079 Test RE 0.00015381447103991053 c 1.0023814 k 0.009878143 m 5.006481\n",
      "62 Train Loss 1.084979 Test RE 0.0001411020555076061 c 1.0013274 k 0.00989475 m 4.9863605\n",
      "63 Train Loss 0.85735625 Test RE 0.00011116337611872229 c 1.0023646 k 0.009879645 m 4.960543\n",
      "64 Train Loss 0.7902599 Test RE 0.0001116480225345563 c 1.0041219 k 0.009841689 m 4.945894\n",
      "65 Train Loss 0.7503613 Test RE 0.00011188935435347023 c 1.0036308 k 0.009851883 m 4.939693\n",
      "66 Train Loss 0.6617853 Test RE 0.00010559077360797898 c 1.0033476 k 0.00986393 m 4.9674387\n",
      "67 Train Loss 0.62070876 Test RE 0.00011068869998963411 c 1.0033089 k 0.009859551 m 4.971755\n",
      "68 Train Loss 0.60616714 Test RE 0.0001225221428721469 c 1.0034668 k 0.009849504 m 4.962451\n",
      "69 Train Loss 0.5812145 Test RE 0.00012788435183445256 c 1.0029318 k 0.00986898 m 4.9799156\n",
      "70 Train Loss 0.5322015 Test RE 0.00012017334911656773 c 1.0017346 k 0.009903691 m 4.991734\n",
      "71 Train Loss 0.4054641 Test RE 0.00011204306643487454 c 1.0037055 k 0.009856196 m 4.9553494\n",
      "72 Train Loss 0.37035215 Test RE 9.935127599090388e-05 c 1.0028464 k 0.009878529 m 4.9668007\n",
      "73 Train Loss 0.29006305 Test RE 8.082922011659633e-05 c 1.0014018 k 0.009929599 m 4.9878254\n",
      "74 Train Loss 0.2570668 Test RE 6.835612940494613e-05 c 1.0022618 k 0.009899991 m 4.9867697\n",
      "75 Train Loss 0.2488604 Test RE 6.723329144753631e-05 c 1.0018767 k 0.009911549 m 4.9926805\n",
      "76 Train Loss 0.2403158 Test RE 6.861237034216038e-05 c 1.0018188 k 0.009913666 m 4.987663\n",
      "77 Train Loss 0.21806876 Test RE 6.878953452748351e-05 c 1.0027916 k 0.009887351 m 4.965168\n",
      "78 Train Loss 0.20764558 Test RE 6.895198741959299e-05 c 1.0029061 k 0.009886385 m 4.9623394\n",
      "79 Train Loss 0.18974285 Test RE 7.016187593362856e-05 c 1.0024446 k 0.0099021755 m 4.9642606\n",
      "80 Train Loss 0.17702028 Test RE 6.371848796270775e-05 c 1.0026008 k 0.009897967 m 4.971847\n",
      "81 Train Loss 0.16495241 Test RE 6.458934338138657e-05 c 1.001998 k 0.009911324 m 4.9937897\n",
      "82 Train Loss 0.1377697 Test RE 6.664902801522936e-05 c 1.0019021 k 0.009913309 m 4.9866085\n",
      "83 Train Loss 0.12772714 Test RE 6.210797542206615e-05 c 1.0021404 k 0.00991189 m 4.980473\n",
      "84 Train Loss 0.12628926 Test RE 6.165241080690828e-05 c 1.0020671 k 0.009912924 m 4.9805694\n",
      "85 Train Loss 0.12544706 Test RE 6.113954504062517e-05 c 1.002115 k 0.009912775 m 4.981778\n",
      "86 Train Loss 0.12479402 Test RE 6.0477505567362555e-05 c 1.0021558 k 0.0099121 m 4.980787\n",
      "87 Train Loss 0.12287334 Test RE 6.145225452256032e-05 c 1.0021676 k 0.009910435 m 4.9762526\n",
      "88 Train Loss 0.120741725 Test RE 6.226078521830149e-05 c 1.002217 k 0.009911087 m 4.976875\n",
      "89 Train Loss 0.11875783 Test RE 6.295760835356139e-05 c 1.0019515 k 0.009918727 m 4.9801927\n",
      "90 Train Loss 0.1178217 Test RE 6.384686592702445e-05 c 1.0017292 k 0.009923415 m 4.979764\n",
      "91 Train Loss 0.117414616 Test RE 6.439693379119047e-05 c 1.0019205 k 0.009918519 m 4.979602\n",
      "92 Train Loss 0.11517785 Test RE 6.556653845109403e-05 c 1.0023565 k 0.009910033 m 4.9842587\n",
      "93 Train Loss 0.10809542 Test RE 6.545770322747447e-05 c 1.0021372 k 0.009917865 m 4.992521\n",
      "94 Train Loss 0.08432882 Test RE 5.8358908917830295e-05 c 1.0010163 k 0.0099478895 m 4.9958844\n",
      "95 Train Loss 0.07406317 Test RE 5.4623725798196116e-05 c 1.0012 k 0.009946207 m 4.993498\n",
      "96 Train Loss 0.072727 Test RE 5.565386278275256e-05 c 1.0010759 k 0.009951807 m 4.9954095\n",
      "97 Train Loss 0.070711486 Test RE 5.624782763051961e-05 c 1.0007739 k 0.0099609075 m 4.9989214\n",
      "98 Train Loss 0.06981194 Test RE 5.540056001608446e-05 c 1.0009763 k 0.009956395 m 4.9962683\n",
      "99 Train Loss 0.06886986 Test RE 5.476581843616565e-05 c 1.00113 k 0.00995229 m 4.9948993\n",
      "100 Train Loss 0.0673404 Test RE 5.495569799307089e-05 c 1.0007685 k 0.009962213 m 4.9958305\n",
      "101 Train Loss 0.06522736 Test RE 5.2480431885621435e-05 c 1.0007824 k 0.009961115 m 4.9941387\n",
      "102 Train Loss 0.063147016 Test RE 5.044043268590966e-05 c 1.0009104 k 0.009959287 m 4.9943204\n",
      "103 Train Loss 0.060481124 Test RE 4.912418859358954e-05 c 1.0007687 k 0.00996384 m 4.9945755\n",
      "104 Train Loss 0.05958983 Test RE 4.831308643117894e-05 c 1.0007808 k 0.00996561 m 4.9963436\n",
      "105 Train Loss 0.059113525 Test RE 4.744161981138796e-05 c 1.0006249 k 0.009969734 m 4.9960575\n",
      "106 Train Loss 0.058437016 Test RE 4.592582529298317e-05 c 1.000611 k 0.0099692745 m 4.9957128\n",
      "107 Train Loss 0.057870127 Test RE 4.506588512788671e-05 c 1.000833 k 0.009964366 m 4.9954042\n",
      "108 Train Loss 0.057606068 Test RE 4.4383414442706436e-05 c 1.0007061 k 0.0099679185 m 4.9953637\n",
      "109 Train Loss 0.05748363 Test RE 4.357247276211761e-05 c 1.0007155 k 0.009967954 m 4.9959455\n",
      "110 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "111 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "112 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "113 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "114 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "115 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "116 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "117 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "118 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "119 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "120 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "121 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "122 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "123 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "124 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "125 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "126 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "127 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "128 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "129 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "130 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "131 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "132 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "133 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "134 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "135 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "136 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "137 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "138 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "139 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "140 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "141 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "142 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "143 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "144 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "145 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "146 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "147 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "148 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "149 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "150 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "151 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "152 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "153 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "154 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "155 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "156 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "157 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "158 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "159 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "160 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "161 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "162 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "163 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "164 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "165 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "166 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "167 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "168 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "169 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "170 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "171 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "172 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "173 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "174 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "175 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "176 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "177 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "178 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "179 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "180 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "181 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "182 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "183 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "184 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "185 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "186 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "187 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "188 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "189 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "190 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "191 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "192 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "193 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "194 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "195 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "196 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "197 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "198 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "199 Train Loss 0.057447042 Test RE 4.332319836534105e-05 c 1.0007452 k 0.009967443 m 4.996012\n",
      "Training time: 45.89\n",
      "Training time: 45.89\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 2771503.5 Test RE 0.5036941512122293 c -0.00021464875 k 0.6777272 m -0.0006783937\n",
      "1 Train Loss 28610.568 Test RE 0.058548766886622264 c -0.009047472 k 0.05397287 m -0.0006903279\n",
      "2 Train Loss 8357.624 Test RE 0.02858645199122196 c -0.0031237514 k 0.030912079 m -0.00020025058\n",
      "3 Train Loss 3626.415 Test RE 0.013757524917514133 c 0.117702946 k 0.029707992 m 0.009473013\n",
      "4 Train Loss 1985.4883 Test RE 0.007600795678070209 c 0.3780949 k 0.02338348 m 0.031175243\n",
      "5 Train Loss 884.61804 Test RE 0.006684863735268417 c 0.91368264 k 0.010793266 m 0.07972309\n",
      "6 Train Loss 502.02603 Test RE 0.0036538982584186207 c 1.0481484 k 0.007872552 m 0.09560895\n",
      "7 Train Loss 363.33566 Test RE 0.002643875882456331 c 1.196999 k 0.0042211744 m 0.11801021\n",
      "8 Train Loss 324.07443 Test RE 0.0022481576294258437 c 1.1888765 k 0.004499919 m 0.12804572\n",
      "9 Train Loss 293.99548 Test RE 0.0023036763582373354 c 1.1974872 k 0.004239673 m 0.14863734\n",
      "10 Train Loss 259.6174 Test RE 0.0020336842100490767 c 1.2253451 k 0.0036099472 m 0.1872196\n",
      "11 Train Loss 251.11322 Test RE 0.002113969407401703 c 1.2057431 k 0.0041607986 m 0.19501752\n",
      "12 Train Loss 238.57095 Test RE 0.00234395588309705 c 1.1881894 k 0.0045202817 m 0.2335629\n",
      "13 Train Loss 232.75632 Test RE 0.002374654446824736 c 1.1926675 k 0.0044553517 m 0.2601042\n",
      "14 Train Loss 219.57497 Test RE 0.0023430510268238037 c 1.1737958 k 0.004938314 m 0.3586168\n",
      "15 Train Loss 211.54344 Test RE 0.002370679967948452 c 1.1842498 k 0.0047455546 m 0.4286471\n",
      "16 Train Loss 202.58575 Test RE 0.002102324382012279 c 1.1776718 k 0.0050025317 m 0.5673663\n",
      "17 Train Loss 195.83405 Test RE 0.002160141156157756 c 1.165885 k 0.0052198344 m 0.5849859\n",
      "18 Train Loss 192.48734 Test RE 0.0022364026923621925 c 1.1571789 k 0.005446614 m 0.63200074\n",
      "19 Train Loss 190.70587 Test RE 0.0023529205333200254 c 1.1665027 k 0.005235724 m 0.66055167\n",
      "20 Train Loss 188.87566 Test RE 0.002497013706591335 c 1.1684746 k 0.005190428 m 0.7042035\n",
      "21 Train Loss 185.14578 Test RE 0.002526997756389385 c 1.1523523 k 0.005593007 m 0.7579563\n",
      "22 Train Loss 183.5149 Test RE 0.0025285283257460447 c 1.1421518 k 0.005774113 m 0.7995245\n",
      "23 Train Loss 179.28696 Test RE 0.00256265408139252 c 1.1546613 k 0.005600978 m 0.8658367\n",
      "24 Train Loss 176.79533 Test RE 0.0025252497376610345 c 1.159723 k 0.0054290593 m 0.88689524\n",
      "25 Train Loss 170.27086 Test RE 0.002434997793437626 c 1.1296903 k 0.0061282413 m 1.023344\n",
      "26 Train Loss 165.74338 Test RE 0.0024907148670893164 c 1.1448334 k 0.005803702 m 1.1291307\n",
      "27 Train Loss 161.93944 Test RE 0.0024137822927368614 c 1.1529944 k 0.0056240032 m 1.1612189\n",
      "28 Train Loss 157.27089 Test RE 0.002263134079813538 c 1.1411065 k 0.0059091323 m 1.2011591\n",
      "29 Train Loss 154.42029 Test RE 0.0021232216628793705 c 1.1509236 k 0.005658162 m 1.20518\n",
      "30 Train Loss 151.2051 Test RE 0.0022688587470592113 c 1.1571862 k 0.005547939 m 1.2633226\n",
      "31 Train Loss 143.96115 Test RE 0.00217210429821552 c 1.1336191 k 0.0061383364 m 1.3957783\n",
      "32 Train Loss 134.61894 Test RE 0.001796648381268923 c 1.162919 k 0.0054702726 m 1.6495638\n",
      "33 Train Loss 130.37508 Test RE 0.00179273061378182 c 1.136873 k 0.0061480133 m 1.7182177\n",
      "34 Train Loss 124.74293 Test RE 0.001988900810077746 c 1.1076055 k 0.006876707 m 1.8560318\n",
      "35 Train Loss 117.90852 Test RE 0.0019022713408694354 c 1.108976 k 0.00695694 m 2.0192387\n",
      "36 Train Loss 107.92877 Test RE 0.0015040025749831288 c 1.0767355 k 0.007771611 m 2.237299\n",
      "37 Train Loss 101.0203 Test RE 0.001684116041185062 c 1.0885429 k 0.0076482533 m 2.330425\n",
      "38 Train Loss 77.4385 Test RE 0.0014160069177639567 c 1.0941384 k 0.007530543 m 2.7209108\n",
      "39 Train Loss 62.764313 Test RE 0.001505666567332153 c 1.0727338 k 0.008024649 m 2.942712\n",
      "40 Train Loss 41.92025 Test RE 0.001086934388658991 c 1.0723101 k 0.007972503 m 3.1766858\n",
      "41 Train Loss 36.233833 Test RE 0.0009578914611985437 c 1.0645351 k 0.008230736 m 3.322925\n",
      "42 Train Loss 32.260475 Test RE 0.0008888044761051405 c 1.0529641 k 0.008500463 m 3.4520314\n",
      "43 Train Loss 26.796574 Test RE 0.0008595030767593382 c 1.048644 k 0.008699559 m 3.6628773\n",
      "44 Train Loss 21.140518 Test RE 0.0006584120487261216 c 1.0407114 k 0.008765142 m 3.8709152\n",
      "45 Train Loss 17.474037 Test RE 0.0006835440817741016 c 1.049337 k 0.008563007 m 3.9107084\n",
      "46 Train Loss 14.033252 Test RE 0.0006606839340153795 c 1.0354791 k 0.0089537455 m 4.002398\n",
      "47 Train Loss 8.999119 Test RE 0.0005011904059520315 c 1.0339135 k 0.008966052 m 4.2727194\n",
      "48 Train Loss 6.045788 Test RE 0.00043527289681900394 c 1.0185336 k 0.00946119 m 4.504327\n",
      "49 Train Loss 4.67503 Test RE 0.00032723923781988575 c 1.0085468 k 0.00972025 m 4.6754932\n",
      "50 Train Loss 3.4198895 Test RE 0.00018458568849463885 c 1.0057809 k 0.00980418 m 4.8167806\n",
      "51 Train Loss 2.8281174 Test RE 0.00022973593328576593 c 1.0041386 k 0.009849789 m 4.8470845\n",
      "52 Train Loss 2.2064455 Test RE 0.00018260091394934743 c 1.0051725 k 0.009803458 m 4.955681\n",
      "53 Train Loss 1.7346938 Test RE 0.00014470220130076837 c 1.0008962 k 0.009937869 m 4.9864287\n",
      "54 Train Loss 1.4805416 Test RE 0.00015435681633523202 c 1.001169 k 0.009981615 m 4.9637914\n",
      "55 Train Loss 1.2939786 Test RE 0.00014481479162690313 c 1.001798 k 0.009974917 m 4.9832726\n",
      "56 Train Loss 1.0386393 Test RE 0.00012908292507082851 c 0.9979708 k 0.010076651 m 4.9856634\n",
      "57 Train Loss 0.8843593 Test RE 0.00014022137974924846 c 0.9998085 k 0.010001028 m 4.9905453\n",
      "58 Train Loss 0.750568 Test RE 0.00013131028560269187 c 1.0006362 k 0.009974972 m 5.024801\n",
      "59 Train Loss 0.6861171 Test RE 0.00011905221366764275 c 0.99779165 k 0.010059888 m 5.0493865\n",
      "60 Train Loss 0.60607225 Test RE 9.238211142152085e-05 c 0.9983695 k 0.010043438 m 5.053459\n",
      "61 Train Loss 0.57052106 Test RE 8.640159435030044e-05 c 0.99926555 k 0.010011701 m 5.0286474\n",
      "62 Train Loss 0.5417484 Test RE 8.31757036686908e-05 c 1.0002972 k 0.0099818725 m 5.001017\n",
      "63 Train Loss 0.5250724 Test RE 8.162221609170929e-05 c 1.0008947 k 0.009966645 m 4.988764\n",
      "64 Train Loss 0.4942981 Test RE 8.340439791738574e-05 c 1.0005102 k 0.00997214 m 4.9941325\n",
      "65 Train Loss 0.46380478 Test RE 9.892879637419513e-05 c 1.0004776 k 0.009971722 m 4.9960246\n",
      "66 Train Loss 0.44375122 Test RE 0.00010942206817881333 c 1.0015395 k 0.009944707 m 4.9912906\n",
      "67 Train Loss 0.36361516 Test RE 8.592291102608492e-05 c 1.0015576 k 0.009943265 m 4.9755282\n",
      "68 Train Loss 0.3459822 Test RE 7.77203692590433e-05 c 1.0009239 k 0.00996238 m 4.982836\n",
      "69 Train Loss 0.3183177 Test RE 6.581433234251367e-05 c 1.0004983 k 0.009976864 m 4.994899\n",
      "70 Train Loss 0.304557 Test RE 6.025002606725139e-05 c 1.000821 k 0.009973777 m 4.9911423\n",
      "71 Train Loss 0.27034453 Test RE 4.680066465408969e-05 c 1.000221 k 0.009998162 m 5.0008597\n",
      "72 Train Loss 0.23839745 Test RE 4.4127920396171434e-05 c 0.9987883 k 0.010028542 m 5.005751\n",
      "73 Train Loss 0.21564536 Test RE 4.642992446119759e-05 c 1.0007668 k 0.009977787 m 4.988312\n",
      "74 Train Loss 0.20431918 Test RE 4.701639376503737e-05 c 1.0008152 k 0.009978206 m 4.9897356\n",
      "75 Train Loss 0.18608929 Test RE 4.817537144532983e-05 c 0.99924797 k 0.010016804 m 4.9965477\n",
      "76 Train Loss 0.17468303 Test RE 4.720751345464288e-05 c 1.0001136 k 0.009996274 m 4.984755\n",
      "77 Train Loss 0.15792784 Test RE 4.082704605447836e-05 c 0.9999075 k 0.010008176 m 4.9858623\n",
      "78 Train Loss 0.14132199 Test RE 2.879018692749487e-05 c 1.0001535 k 0.010002628 m 5.000628\n",
      "79 Train Loss 0.13633484 Test RE 2.6669096319404328e-05 c 1.0000741 k 0.010000243 m 5.0000253\n",
      "80 Train Loss 0.13410819 Test RE 2.5874645552545214e-05 c 0.9998745 k 0.010004818 m 4.9986286\n",
      "81 Train Loss 0.1234472 Test RE 2.330306340598739e-05 c 1.0002413 k 0.009996831 m 4.994584\n",
      "82 Train Loss 0.116698064 Test RE 2.3690933084522942e-05 c 1.000199 k 0.009997603 m 4.989203\n",
      "83 Train Loss 0.11131851 Test RE 2.3042716097478326e-05 c 0.9999811 k 0.010002152 m 4.988151\n",
      "84 Train Loss 0.1086325 Test RE 2.683561867084574e-05 c 1.0003474 k 0.009991879 m 4.9872065\n",
      "85 Train Loss 0.10449773 Test RE 2.9795945502216165e-05 c 1.0003517 k 0.0099952025 m 4.9913845\n",
      "86 Train Loss 0.10284843 Test RE 2.7982405736520994e-05 c 1.0000865 k 0.010000036 m 4.990038\n",
      "87 Train Loss 0.09945894 Test RE 2.9003253448353094e-05 c 0.9998195 k 0.010007512 m 4.9823422\n",
      "88 Train Loss 0.09102231 Test RE 3.251862964674412e-05 c 0.9999227 k 0.010003712 m 4.992259\n",
      "89 Train Loss 0.080495894 Test RE 2.591679216023839e-05 c 0.99903435 k 0.010027634 m 5.0042424\n",
      "90 Train Loss 0.07168829 Test RE 2.056262556046304e-05 c 0.999498 k 0.010016591 m 5.0049415\n",
      "91 Train Loss 0.07044455 Test RE 2.3846343385919473e-05 c 0.99993086 k 0.010007525 m 5.005161\n",
      "92 Train Loss 0.06894335 Test RE 2.571129646456855e-05 c 0.9996817 k 0.010013371 m 5.0049043\n",
      "93 Train Loss 0.066893965 Test RE 2.183832297691829e-05 c 0.9992814 k 0.010023546 m 5.0073624\n",
      "94 Train Loss 0.0626312 Test RE 2.009213944181991e-05 c 0.9995299 k 0.010018064 m 5.000233\n",
      "95 Train Loss 0.061940793 Test RE 1.994012982181789e-05 c 0.9997688 k 0.0100108925 m 4.998422\n",
      "96 Train Loss 0.061820887 Test RE 1.9880243221426834e-05 c 0.9998079 k 0.010009461 m 4.9983826\n",
      "97 Train Loss 0.060726117 Test RE 2.0472550360295176e-05 c 0.9996278 k 0.010014529 m 4.999871\n",
      "98 Train Loss 0.057154365 Test RE 2.197551756038703e-05 c 0.999615 k 0.010014903 m 5.0021353\n",
      "99 Train Loss 0.056387775 Test RE 2.057672485068461e-05 c 0.999799 k 0.0100096585 m 5.0005946\n",
      "100 Train Loss 0.05542852 Test RE 2.0296273719963985e-05 c 0.9998813 k 0.010007673 m 4.9988527\n",
      "101 Train Loss 0.05431776 Test RE 1.9942080766626815e-05 c 0.9998225 k 0.01000825 m 4.999616\n",
      "102 Train Loss 0.053922083 Test RE 2.0253515934129604e-05 c 0.99988526 k 0.010007151 m 4.999064\n",
      "103 Train Loss 0.053593025 Test RE 2.062943743514922e-05 c 0.9997786 k 0.010010012 m 4.9991784\n",
      "104 Train Loss 0.05251737 Test RE 2.0003081176361736e-05 c 0.9997212 k 0.010011033 m 5.000116\n",
      "105 Train Loss 0.051819786 Test RE 1.9922886735730975e-05 c 0.99993575 k 0.010005993 m 4.9998894\n",
      "106 Train Loss 0.051604908 Test RE 1.99178696559782e-05 c 0.9998386 k 0.010008106 m 5.0003915\n",
      "107 Train Loss 0.051369112 Test RE 1.9922317218050876e-05 c 0.99981 k 0.010008608 m 5.000059\n",
      "108 Train Loss 0.051012628 Test RE 2.0382659674862627e-05 c 0.9999133 k 0.010005552 m 5.00037\n",
      "109 Train Loss 0.049379002 Test RE 2.139989080105026e-05 c 0.99997747 k 0.01000387 m 4.9991355\n",
      "110 Train Loss 0.04748092 Test RE 2.1833792041091143e-05 c 0.999871 k 0.010006368 m 4.998734\n",
      "111 Train Loss 0.046922658 Test RE 2.171052669345476e-05 c 0.99975646 k 0.010009167 m 5.0010266\n",
      "112 Train Loss 0.04688598 Test RE 2.154348863029791e-05 c 0.99978685 k 0.0100086415 m 5.0009375\n",
      "113 Train Loss 0.04678346 Test RE 2.1331085416519638e-05 c 0.99987596 k 0.010006703 m 4.9999757\n",
      "114 Train Loss 0.046531357 Test RE 2.1889521043444545e-05 c 0.99983406 k 0.010007086 m 5.000523\n",
      "115 Train Loss 0.044555116 Test RE 2.3561328936535264e-05 c 0.9998036 k 0.010006305 m 5.0003495\n",
      "116 Train Loss 0.04199145 Test RE 2.2396756392731615e-05 c 1.000058 k 0.010000927 m 4.9972773\n",
      "117 Train Loss 0.040009588 Test RE 2.209279404120018e-05 c 0.99999726 k 0.010003909 m 5.000281\n",
      "118 Train Loss 0.038109813 Test RE 2.2576430817851156e-05 c 0.99973416 k 0.010007235 m 4.999431\n",
      "119 Train Loss 0.036274523 Test RE 2.183889262937495e-05 c 0.9999896 k 0.010003168 m 4.9978595\n",
      "120 Train Loss 0.03569641 Test RE 2.204864080595222e-05 c 1.0000447 k 0.010001031 m 4.997563\n",
      "121 Train Loss 0.035490118 Test RE 2.2014133507948698e-05 c 0.9999761 k 0.01000325 m 4.9987197\n",
      "122 Train Loss 0.03522762 Test RE 2.200205138983052e-05 c 0.99993616 k 0.010004883 m 4.9996176\n",
      "123 Train Loss 0.03314746 Test RE 2.307303512496735e-05 c 0.999848 k 0.010005553 m 5.0002484\n",
      "124 Train Loss 0.03226354 Test RE 2.470713563858995e-05 c 0.9999416 k 0.010002927 m 4.9985347\n",
      "125 Train Loss 0.03184629 Test RE 2.5333793676876783e-05 c 0.9999749 k 0.010002481 m 4.9993825\n",
      "126 Train Loss 0.031159742 Test RE 2.42684239551733e-05 c 0.9998529 k 0.010005856 m 5.001081\n",
      "127 Train Loss 0.030688107 Test RE 2.4051506545102986e-05 c 0.99993503 k 0.0100033665 m 4.998713\n",
      "128 Train Loss 0.03020773 Test RE 2.429353870597549e-05 c 0.99992347 k 0.010004321 m 4.9975686\n",
      "129 Train Loss 0.029021628 Test RE 2.4789785172841945e-05 c 0.99991745 k 0.01000457 m 4.998182\n",
      "130 Train Loss 0.027302023 Test RE 2.5873049197855114e-05 c 1.0001233 k 0.009997904 m 4.9974685\n",
      "131 Train Loss 0.02677969 Test RE 2.6589096626004214e-05 c 0.9999765 k 0.010001641 m 4.9991035\n",
      "132 Train Loss 0.02636857 Test RE 2.6443993289147697e-05 c 0.99987876 k 0.010003666 m 4.9996533\n",
      "133 Train Loss 0.026231483 Test RE 2.663515260174067e-05 c 0.9999932 k 0.010000799 m 4.9990363\n",
      "134 Train Loss 0.026107349 Test RE 2.688695174120569e-05 c 1.0000149 k 0.010000466 m 4.9986534\n",
      "135 Train Loss 0.025668003 Test RE 2.7506724235302616e-05 c 1.0000961 k 0.0099981725 m 4.996959\n",
      "136 Train Loss 0.024817068 Test RE 2.8266380268609543e-05 c 1.0002654 k 0.009992846 m 4.9963784\n",
      "137 Train Loss 0.024050022 Test RE 2.8299155148167735e-05 c 0.999949 k 0.0100022275 m 4.9997807\n",
      "138 Train Loss 0.02382109 Test RE 2.8673047932805445e-05 c 0.9999466 k 0.010001811 m 4.9991903\n",
      "139 Train Loss 0.02367429 Test RE 2.8784602957634665e-05 c 1.0000547 k 0.00999899 m 4.9983974\n",
      "140 Train Loss 0.023594145 Test RE 2.853825137161995e-05 c 1.0000765 k 0.0099982405 m 4.998031\n",
      "141 Train Loss 0.023559432 Test RE 2.8596412101829402e-05 c 1.0000384 k 0.009999138 m 4.9983525\n",
      "142 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "143 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "144 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "145 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "146 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "147 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "148 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "149 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "150 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "151 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "152 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "153 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "154 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "155 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "156 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "157 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "158 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "159 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "160 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "161 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "162 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "163 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "164 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "165 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "166 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "167 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "168 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "169 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "170 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "171 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "172 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "173 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "174 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "175 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "176 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "177 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "178 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "179 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "180 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "181 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "182 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "183 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "184 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "185 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "186 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "187 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "188 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "189 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "190 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "191 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "192 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "193 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "194 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "195 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "196 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "197 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "198 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "199 Train Loss 0.023536894 Test RE 2.8634086012367707e-05 c 1.0000068 k 0.009999902 m 4.998631\n",
      "Training time: 52.42\n",
      "Training time: 52.42\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 744884.6 Test RE 0.3273289140783502 c 0.0016419601 k 0.011609953 m -0.00011226959\n",
      "1 Train Loss 17027.785 Test RE 0.0450597364404622 c 0.010682946 k 0.034773633 m 0.0001706639\n",
      "2 Train Loss 4155.715 Test RE 0.01564988701512036 c 0.065751426 k 0.030277438 m 0.0032505754\n",
      "3 Train Loss 2316.3906 Test RE 0.011638927610472772 c 0.41415578 k 0.023420053 m 0.028136108\n",
      "4 Train Loss 1573.6602 Test RE 0.010684253698943012 c 0.67348415 k 0.01582464 m 0.04742306\n",
      "5 Train Loss 1189.6163 Test RE 0.01072281201336068 c 1.0112725 k 0.0074287485 m 0.07347132\n",
      "6 Train Loss 1037.0812 Test RE 0.010259160459280006 c 1.2000986 k 0.0035145413 m 0.089817554\n",
      "7 Train Loss 834.313 Test RE 0.00944914336952718 c 1.177534 k 0.0038223115 m 0.098145775\n",
      "8 Train Loss 723.57465 Test RE 0.008798983683428406 c 1.2633613 k 0.0019984944 m 0.11622002\n",
      "9 Train Loss 607.3793 Test RE 0.007693078356014489 c 1.2299316 k 0.0026467138 m 0.1278103\n",
      "10 Train Loss 556.30896 Test RE 0.007310218173590855 c 1.270879 k 0.0019824863 m 0.1418926\n",
      "11 Train Loss 523.6606 Test RE 0.007128451059022434 c 1.3175769 k 0.00078877906 m 0.1630145\n",
      "12 Train Loss 431.10394 Test RE 0.0060323206122957634 c 1.3007472 k 0.0014591706 m 0.21689144\n",
      "13 Train Loss 370.6853 Test RE 0.005450627338074573 c 1.2068156 k 0.00366959 m 0.24740613\n",
      "14 Train Loss 316.78598 Test RE 0.004332300171209504 c 1.246605 k 0.0029941066 m 0.29745877\n",
      "15 Train Loss 293.60016 Test RE 0.004132782102028624 c 1.2011164 k 0.003995006 m 0.31995702\n",
      "16 Train Loss 270.87497 Test RE 0.003614565609876391 c 1.2042357 k 0.0039669117 m 0.36361858\n",
      "17 Train Loss 254.64001 Test RE 0.0031809755325517847 c 1.1800128 k 0.004679306 m 0.4109756\n",
      "18 Train Loss 247.10138 Test RE 0.0030501192635518076 c 1.1498495 k 0.0053854394 m 0.42562413\n",
      "19 Train Loss 241.8453 Test RE 0.0030715216777871543 c 1.1589906 k 0.0052056056 m 0.4555544\n",
      "20 Train Loss 238.06061 Test RE 0.003171231272678508 c 1.1747454 k 0.004841393 m 0.46238503\n",
      "21 Train Loss 235.1373 Test RE 0.00327472302034238 c 1.1685256 k 0.0049553732 m 0.463238\n",
      "22 Train Loss 230.44113 Test RE 0.0032409616578084584 c 1.1667874 k 0.0050231256 m 0.4798425\n",
      "23 Train Loss 227.60312 Test RE 0.00328519278400443 c 1.1662288 k 0.0050671543 m 0.4973679\n",
      "24 Train Loss 224.12509 Test RE 0.0033347762956457056 c 1.1868684 k 0.0045693773 m 0.5223823\n",
      "25 Train Loss 221.37392 Test RE 0.0031935965839443798 c 1.1738029 k 0.0048601893 m 0.54121614\n",
      "26 Train Loss 216.48233 Test RE 0.003040023058371424 c 1.1823057 k 0.0046787253 m 0.5842689\n",
      "27 Train Loss 212.35321 Test RE 0.0030717229187122597 c 1.2102132 k 0.003968404 m 0.631509\n",
      "28 Train Loss 203.48769 Test RE 0.0027585675541329634 c 1.1810055 k 0.0048464336 m 0.6718589\n",
      "29 Train Loss 191.2822 Test RE 0.002704967353795346 c 1.139239 k 0.0058432035 m 0.8245727\n",
      "30 Train Loss 176.33899 Test RE 0.0027170600070769383 c 1.181987 k 0.004761102 m 0.9784467\n",
      "31 Train Loss 171.09424 Test RE 0.0026927098960408684 c 1.1813898 k 0.00487996 m 1.0317936\n",
      "32 Train Loss 163.76425 Test RE 0.002545230482364652 c 1.1579648 k 0.0054512853 m 1.127014\n",
      "33 Train Loss 155.32074 Test RE 0.00231426723622633 c 1.1450562 k 0.0057625356 m 1.1995423\n",
      "34 Train Loss 151.37714 Test RE 0.0024015398028309123 c 1.1285222 k 0.006226367 m 1.3209223\n",
      "35 Train Loss 138.49533 Test RE 0.002161448246959359 c 1.124996 k 0.0063006096 m 1.4834026\n",
      "36 Train Loss 124.83597 Test RE 0.001959537115076394 c 1.1299294 k 0.006242647 m 1.6992183\n",
      "37 Train Loss 110.17717 Test RE 0.0023107785630825935 c 1.1020288 k 0.006685828 m 2.0362592\n",
      "38 Train Loss 94.4277 Test RE 0.0016496095160420325 c 1.1180629 k 0.0066631665 m 2.153779\n",
      "39 Train Loss 83.11674 Test RE 0.0016187383420054708 c 1.1013399 k 0.0070430366 m 2.386188\n",
      "40 Train Loss 77.69531 Test RE 0.001603067238585885 c 1.0935802 k 0.0073341043 m 2.567971\n",
      "41 Train Loss 64.765015 Test RE 0.0014715535026805944 c 1.0932313 k 0.007318077 m 2.8820488\n",
      "42 Train Loss 46.251286 Test RE 0.001298865316466009 c 1.0580257 k 0.008480399 m 3.3029568\n",
      "43 Train Loss 38.07606 Test RE 0.0014645316101295548 c 1.0492774 k 0.008658037 m 3.5375443\n",
      "44 Train Loss 28.791233 Test RE 0.001382729261728515 c 1.0408366 k 0.008904165 m 3.874537\n",
      "45 Train Loss 23.933556 Test RE 0.00112724611008703 c 1.030762 k 0.009224272 m 4.126437\n",
      "46 Train Loss 19.641312 Test RE 0.0010157996356080033 c 1.0204375 k 0.0094611915 m 4.177738\n",
      "47 Train Loss 17.581745 Test RE 0.0008941316927414686 c 1.025177 k 0.009339315 m 4.235905\n",
      "48 Train Loss 14.057723 Test RE 0.0007130611964199623 c 1.016498 k 0.009573686 m 4.3872833\n",
      "49 Train Loss 12.071911 Test RE 0.0006237497860380283 c 1.0181584 k 0.00953023 m 4.419492\n",
      "50 Train Loss 11.728756 Test RE 0.0006031320390539686 c 1.0187869 k 0.009495038 m 4.4062653\n",
      "51 Train Loss 11.588394 Test RE 0.0005959246958352969 c 1.0204198 k 0.009446216 m 4.397913\n",
      "52 Train Loss 11.310113 Test RE 0.0005805262029507218 c 1.0205605 k 0.009451773 m 4.4034147\n",
      "53 Train Loss 11.2075 Test RE 0.0005678685163470685 c 1.0186766 k 0.009495606 m 4.4046416\n",
      "54 Train Loss 10.788313 Test RE 0.0005404652002987942 c 1.0182021 k 0.009513315 m 4.41997\n",
      "55 Train Loss 8.796007 Test RE 0.0004981417331350426 c 1.016404 k 0.009549202 m 4.577547\n",
      "56 Train Loss 6.223104 Test RE 0.0003754170900828147 c 1.0090027 k 0.009806007 m 4.7891345\n",
      "57 Train Loss 4.0363116 Test RE 0.00022055264831311798 c 1.0043463 k 0.009844941 m 4.9062343\n",
      "58 Train Loss 3.5575342 Test RE 0.00017525502857743106 c 1.0011308 k 0.009933442 m 5.026085\n",
      "59 Train Loss 3.462944 Test RE 0.00017381119953092014 c 1.000914 k 0.009930163 m 5.060726\n",
      "60 Train Loss 3.394066 Test RE 0.00016270682073438387 c 0.99901676 k 0.009973481 m 5.0539837\n",
      "61 Train Loss 3.2373464 Test RE 0.00014691431401008909 c 1.0002427 k 0.009942486 m 4.997317\n",
      "62 Train Loss 3.166905 Test RE 0.00014583460533427057 c 1.0039167 k 0.009846784 m 4.990434\n",
      "63 Train Loss 2.8976471 Test RE 0.00016897361514781544 c 1.0045811 k 0.009826246 m 5.0172253\n",
      "64 Train Loss 2.3367233 Test RE 0.00017659973483305418 c 1.0051883 k 0.009798921 m 4.930332\n",
      "65 Train Loss 2.1734846 Test RE 0.00018254486415982544 c 1.0073124 k 0.009738912 m 4.908463\n",
      "66 Train Loss 2.1088371 Test RE 0.00018192598326043625 c 1.005132 k 0.009795877 m 4.9176345\n",
      "67 Train Loss 2.0892818 Test RE 0.00018077936879653522 c 1.0057678 k 0.009781433 m 4.912246\n",
      "68 Train Loss 2.0260162 Test RE 0.00018520959564183496 c 1.0060972 k 0.009773411 m 4.9224577\n",
      "69 Train Loss 1.9078672 Test RE 0.00017116455723629998 c 1.004253 k 0.009829012 m 4.931307\n",
      "70 Train Loss 1.788106 Test RE 0.00016297894447861357 c 1.0049678 k 0.009804442 m 4.935756\n",
      "71 Train Loss 1.5955799 Test RE 0.00016318304671020034 c 1.0025553 k 0.009871933 m 4.9749722\n",
      "72 Train Loss 1.441059 Test RE 0.00016669491438936555 c 1.0020345 k 0.00991111 m 4.9750757\n",
      "73 Train Loss 1.4063257 Test RE 0.00017364387167537084 c 1.0020436 k 0.009898453 m 4.9930387\n",
      "74 Train Loss 1.3668162 Test RE 0.00017074600977408903 c 1.0002475 k 0.009962171 m 5.0377674\n",
      "75 Train Loss 1.3480355 Test RE 0.00017441696749005497 c 0.9999763 k 0.009973965 m 5.0498266\n",
      "76 Train Loss 1.3260556 Test RE 0.0001815835238641747 c 0.9990661 k 0.0100017795 m 5.0595474\n",
      "77 Train Loss 1.235602 Test RE 0.00018108497346643924 c 0.99691963 k 0.010063072 m 5.0867295\n",
      "78 Train Loss 1.1686368 Test RE 0.00016239189532212388 c 0.9990045 k 0.010003309 m 5.0886903\n",
      "79 Train Loss 1.0575701 Test RE 0.0001304599350708107 c 1.002297 k 0.009904787 m 5.0430837\n",
      "80 Train Loss 0.95433295 Test RE 0.00012230922126482075 c 1.002445 k 0.0098937685 m 4.9800963\n",
      "81 Train Loss 0.93372995 Test RE 0.00011679709453065404 c 1.0020947 k 0.009899753 m 4.9786143\n",
      "82 Train Loss 0.9232486 Test RE 0.00011474640365584684 c 1.002006 k 0.009897863 m 4.9845705\n",
      "83 Train Loss 0.9114655 Test RE 0.0001172658662707104 c 1.0022684 k 0.009895308 m 4.9780784\n",
      "84 Train Loss 0.8717265 Test RE 0.00010736311390813329 c 1.0018917 k 0.009903615 m 4.9640675\n",
      "85 Train Loss 0.8598641 Test RE 0.00010568570292868642 c 1.0023929 k 0.009889444 m 4.976172\n",
      "86 Train Loss 0.8501778 Test RE 9.993014180072688e-05 c 1.0020522 k 0.009904555 m 4.986353\n",
      "87 Train Loss 0.8381111 Test RE 0.0001002767535031148 c 1.0014576 k 0.009920365 m 4.9887795\n",
      "88 Train Loss 0.81524044 Test RE 0.00010541546486804859 c 1.0029966 k 0.009884604 m 4.9953313\n",
      "89 Train Loss 0.76108557 Test RE 9.985213968792557e-05 c 1.0031812 k 0.0098825125 m 4.9919453\n",
      "90 Train Loss 0.7351989 Test RE 9.454710531506139e-05 c 1.0015657 k 0.009915671 m 4.983137\n",
      "91 Train Loss 0.69892126 Test RE 0.00010051987042503182 c 1.0012943 k 0.009920609 m 4.9839587\n",
      "92 Train Loss 0.671484 Test RE 9.264883910527392e-05 c 1.0017259 k 0.009916323 m 4.9841695\n",
      "93 Train Loss 0.6585911 Test RE 8.36713763969261e-05 c 1.001895 k 0.009915219 m 4.973123\n",
      "94 Train Loss 0.6490067 Test RE 7.777693350020404e-05 c 1.0030084 k 0.009894505 m 4.969361\n",
      "95 Train Loss 0.58649254 Test RE 9.878799732517823e-05 c 1.0050801 k 0.009850414 m 4.9715633\n",
      "96 Train Loss 0.51541966 Test RE 7.934442716652644e-05 c 1.0019134 k 0.009928486 m 4.996182\n",
      "97 Train Loss 0.486687 Test RE 7.85000817852121e-05 c 1.0013033 k 0.009943056 m 5.010067\n",
      "98 Train Loss 0.471675 Test RE 7.945899295659334e-05 c 1.0012181 k 0.009943122 m 5.0018926\n",
      "99 Train Loss 0.4675905 Test RE 7.895012914951544e-05 c 1.0016235 k 0.009931047 m 4.995086\n",
      "100 Train Loss 0.46526432 Test RE 7.998788037298527e-05 c 1.001772 k 0.0099243475 m 4.9950447\n",
      "101 Train Loss 0.46053353 Test RE 8.309737853583432e-05 c 1.0018094 k 0.009923536 m 4.9934754\n",
      "102 Train Loss 0.44405183 Test RE 8.719431592052269e-05 c 1.0008888 k 0.009950083 m 4.9942093\n",
      "103 Train Loss 0.4155037 Test RE 8.855850717035715e-05 c 1.000013 k 0.009974113 m 5.0079074\n",
      "104 Train Loss 0.38540137 Test RE 9.941723325238683e-05 c 1.0004827 k 0.009968626 m 5.0180035\n",
      "105 Train Loss 0.35306638 Test RE 9.491200519073614e-05 c 1.0006852 k 0.009962795 m 5.009596\n",
      "106 Train Loss 0.34339085 Test RE 9.021606187775398e-05 c 1.000948 k 0.009951075 m 4.99841\n",
      "107 Train Loss 0.34141153 Test RE 9.272115836672167e-05 c 1.0012178 k 0.009945558 m 4.99428\n",
      "108 Train Loss 0.33814937 Test RE 9.220652004149043e-05 c 1.0008056 k 0.009957138 m 5.0017467\n",
      "109 Train Loss 0.3267609 Test RE 8.629958135726735e-05 c 1.0005018 k 0.009962264 m 5.0085993\n",
      "110 Train Loss 0.27903998 Test RE 6.966570037554129e-05 c 1.0023354 k 0.009911634 m 4.984653\n",
      "111 Train Loss 0.26748914 Test RE 6.867860018442331e-05 c 1.0018449 k 0.009924 m 4.9815516\n",
      "112 Train Loss 0.26056603 Test RE 6.585559779054117e-05 c 1.0011421 k 0.009942587 m 4.987899\n",
      "113 Train Loss 0.25503305 Test RE 6.328640675322393e-05 c 1.0014851 k 0.009934892 m 4.9897203\n",
      "114 Train Loss 0.25152397 Test RE 6.323271849219226e-05 c 1.0017532 k 0.009924917 m 4.9831038\n",
      "115 Train Loss 0.24666482 Test RE 6.158611892667873e-05 c 1.0018898 k 0.009920932 m 4.9830365\n",
      "116 Train Loss 0.24437222 Test RE 6.006277802964171e-05 c 1.0017816 k 0.009924454 m 4.9864883\n",
      "117 Train Loss 0.24372573 Test RE 5.9919344777025506e-05 c 1.0016657 k 0.009927468 m 4.985919\n",
      "118 Train Loss 0.24269624 Test RE 6.0741070038719846e-05 c 1.0014817 k 0.009932179 m 4.987678\n",
      "119 Train Loss 0.241987 Test RE 6.091690975190752e-05 c 1.0016974 k 0.009925765 m 4.9895496\n",
      "120 Train Loss 0.24117826 Test RE 6.137805199531084e-05 c 1.0017245 k 0.009924857 m 4.9886394\n",
      "121 Train Loss 0.2400168 Test RE 6.290558001629767e-05 c 1.0017323 k 0.009924486 m 4.983854\n",
      "122 Train Loss 0.23887488 Test RE 6.41738540541344e-05 c 1.0019178 k 0.009918456 m 4.9850054\n",
      "123 Train Loss 0.23590395 Test RE 6.483092227649801e-05 c 1.0017673 k 0.009922729 m 4.9875073\n",
      "124 Train Loss 0.2303214 Test RE 6.730046676973473e-05 c 1.0018222 k 0.009919291 m 4.986877\n",
      "125 Train Loss 0.21888813 Test RE 6.92392157154417e-05 c 1.0014806 k 0.009929326 m 4.9906754\n",
      "126 Train Loss 0.21140185 Test RE 7.366844108178019e-05 c 1.0013748 k 0.009933285 m 4.993539\n",
      "127 Train Loss 0.19952986 Test RE 7.379033327174517e-05 c 1.0016487 k 0.009925814 m 4.991111\n",
      "128 Train Loss 0.19496189 Test RE 7.402548225524568e-05 c 1.0017134 k 0.009922733 m 4.9881835\n",
      "129 Train Loss 0.19381176 Test RE 7.483896283275174e-05 c 1.0018499 k 0.009919084 m 4.9884944\n",
      "130 Train Loss 0.19318399 Test RE 7.561622572373703e-05 c 1.0017959 k 0.009921252 m 4.989091\n",
      "131 Train Loss 0.19279052 Test RE 7.565297894052025e-05 c 1.0017781 k 0.009921658 m 4.9882593\n",
      "132 Train Loss 0.19267741 Test RE 7.568274846644901e-05 c 1.0017854 k 0.009921535 m 4.9880066\n",
      "133 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "134 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "135 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "136 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "137 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "138 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "139 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "140 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "141 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "142 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "143 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "144 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "145 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "146 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "147 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "148 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "149 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "150 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "151 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "152 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "153 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "154 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "155 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "156 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "157 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "158 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "159 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "160 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "161 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "162 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "163 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "164 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "165 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "166 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "167 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "168 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "169 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "170 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "171 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "172 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "173 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "174 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "175 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "176 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "177 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "178 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "179 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "180 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "181 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "182 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "183 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "184 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "185 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "186 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "187 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "188 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "189 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "190 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "191 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "192 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "193 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "194 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "195 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "196 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "197 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "198 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "199 Train Loss 0.19261263 Test RE 7.566884251689683e-05 c 1.0017629 k 0.009922093 m 4.9880714\n",
      "Training time: 51.06\n",
      "Training time: 51.06\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 137023.5 Test RE 0.06399946690155474 c -0.0008409377 k 0.15750182 m 0.00063517666\n",
      "1 Train Loss 8097.3984 Test RE 0.026696059048475326 c -0.001118712 k 0.031721618 m 0.0008997534\n",
      "2 Train Loss 5196.3135 Test RE 0.019077715831571466 c 0.03256862 k 0.031750556 m 0.0034235616\n",
      "3 Train Loss 3692.1426 Test RE 0.013668162723789734 c 0.07491699 k 0.0309659 m 0.006334255\n",
      "4 Train Loss 2662.0984 Test RE 0.007163375187898342 c 0.11847493 k 0.029524622 m 0.009379864\n",
      "5 Train Loss 2044.138 Test RE 0.004981413295611797 c 0.22865891 k 0.027097996 m 0.017320748\n",
      "6 Train Loss 1712.596 Test RE 0.004755994060317759 c 0.31934705 k 0.024871347 m 0.023874208\n",
      "7 Train Loss 766.89136 Test RE 0.004038444739437228 c 0.7270358 k 0.015761752 m 0.05362639\n",
      "8 Train Loss 449.60712 Test RE 0.003936443537670397 c 0.9825811 k 0.0095407795 m 0.07322996\n",
      "9 Train Loss 367.626 Test RE 0.0038131860957260717 c 1.0954741 k 0.0071352134 m 0.08280554\n",
      "10 Train Loss 334.7277 Test RE 0.003285488565129436 c 1.1033667 k 0.0065638144 m 0.085757695\n",
      "11 Train Loss 290.7583 Test RE 0.0026988048642309836 c 1.1653792 k 0.005077465 m 0.09797603\n",
      "12 Train Loss 285.74307 Test RE 0.002678063203038334 c 1.1890134 k 0.0046386896 m 0.10151766\n",
      "13 Train Loss 274.693 Test RE 0.00278894096835503 c 1.1949224 k 0.0044197403 m 0.1118419\n",
      "14 Train Loss 266.07382 Test RE 0.0024895579341370957 c 1.1806072 k 0.0047088275 m 0.119659066\n",
      "15 Train Loss 259.58847 Test RE 0.0025366454319158405 c 1.2008045 k 0.0042584906 m 0.15952808\n",
      "16 Train Loss 253.11281 Test RE 0.002705306064878817 c 1.18481 k 0.004626548 m 0.18938632\n",
      "17 Train Loss 247.03896 Test RE 0.0025291930668398226 c 1.1807787 k 0.0048105177 m 0.22278117\n",
      "18 Train Loss 240.33475 Test RE 0.002789869462964927 c 1.1672966 k 0.005083863 m 0.27496582\n",
      "19 Train Loss 223.43341 Test RE 0.0027514443125479762 c 1.1558737 k 0.005448153 m 0.37590018\n",
      "20 Train Loss 210.06436 Test RE 0.002423732312758856 c 1.1868439 k 0.004688573 m 0.4683493\n",
      "21 Train Loss 198.55199 Test RE 0.002653678618165879 c 1.1887459 k 0.004553906 m 0.656453\n",
      "22 Train Loss 189.84781 Test RE 0.0024517928167450127 c 1.1804448 k 0.0047633993 m 0.7786474\n",
      "23 Train Loss 184.68422 Test RE 0.0026122019497535867 c 1.1626256 k 0.0051948167 m 0.84361905\n",
      "24 Train Loss 178.3239 Test RE 0.0023434981587764567 c 1.1806893 k 0.004904721 m 0.90676093\n",
      "25 Train Loss 175.79984 Test RE 0.002263092916640177 c 1.166033 k 0.005176221 m 0.94147843\n",
      "26 Train Loss 174.44003 Test RE 0.002287791439954596 c 1.1628759 k 0.005282613 m 0.9908058\n",
      "27 Train Loss 173.09698 Test RE 0.0022236214016443665 c 1.1717726 k 0.0050791404 m 1.0550071\n",
      "28 Train Loss 171.86111 Test RE 0.0021956915185908092 c 1.1575458 k 0.005390624 m 1.0669894\n",
      "29 Train Loss 170.63087 Test RE 0.002120014706185321 c 1.1528233 k 0.005548295 m 1.1118013\n",
      "30 Train Loss 169.9352 Test RE 0.002026095062448087 c 1.1547295 k 0.0055102264 m 1.1750805\n",
      "31 Train Loss 164.76212 Test RE 0.0018874551624009806 c 1.1259866 k 0.0062740576 m 1.3747971\n",
      "32 Train Loss 161.69301 Test RE 0.0018922071804966527 c 1.1385449 k 0.006014395 m 1.3843485\n",
      "33 Train Loss 160.87779 Test RE 0.0019902414795972467 c 1.1430659 k 0.0058858674 m 1.3770521\n",
      "34 Train Loss 158.66724 Test RE 0.0019296073009690978 c 1.1367888 k 0.0060001635 m 1.3736542\n",
      "35 Train Loss 150.48651 Test RE 0.002013119881646133 c 1.1473583 k 0.0058178487 m 1.4607996\n",
      "36 Train Loss 147.29665 Test RE 0.001979271021627581 c 1.1490407 k 0.005848545 m 1.5352724\n",
      "37 Train Loss 139.09756 Test RE 0.001597783522729032 c 1.1196188 k 0.0065599964 m 1.749659\n",
      "38 Train Loss 132.9549 Test RE 0.0014599035390329125 c 1.1110784 k 0.006838015 m 1.9030632\n",
      "39 Train Loss 79.11924 Test RE 0.001668868014347554 c 1.0985677 k 0.007305726 m 2.6708019\n",
      "40 Train Loss 67.03082 Test RE 0.0014877921064430492 c 1.0786812 k 0.007679024 m 2.904031\n",
      "41 Train Loss 59.52444 Test RE 0.001464937647135511 c 1.074873 k 0.007827566 m 3.039579\n",
      "42 Train Loss 52.398758 Test RE 0.0012078744945865471 c 1.0667626 k 0.008025186 m 3.2202518\n",
      "43 Train Loss 50.076218 Test RE 0.001127410493360088 c 1.0539156 k 0.00827085 m 3.3296418\n",
      "44 Train Loss 48.005337 Test RE 0.0011543607129622904 c 1.0564626 k 0.008264691 m 3.4053476\n",
      "45 Train Loss 45.031036 Test RE 0.0010458956368371416 c 1.0600032 k 0.008169162 m 3.5013008\n",
      "46 Train Loss 41.616203 Test RE 0.001004735054541772 c 1.0475733 k 0.008452557 m 3.673652\n",
      "47 Train Loss 38.58909 Test RE 0.0009773624700747268 c 1.045935 k 0.008530748 m 3.7546434\n",
      "48 Train Loss 35.85084 Test RE 0.0008881641954295123 c 1.0441129 k 0.008559418 m 3.7845871\n",
      "49 Train Loss 33.258186 Test RE 0.0008776328916993315 c 1.0491097 k 0.008475959 m 3.8594744\n",
      "50 Train Loss 27.398964 Test RE 0.0009255812677760659 c 1.0478274 k 0.0085225925 m 4.1042686\n",
      "51 Train Loss 26.131283 Test RE 0.0008766550878400501 c 1.032358 k 0.008897166 m 4.1903214\n",
      "52 Train Loss 24.951132 Test RE 0.0008579128899629737 c 1.0306323 k 0.009033743 m 4.283117\n",
      "53 Train Loss 21.976864 Test RE 0.0008805015423109657 c 1.0229235 k 0.009128001 m 4.446386\n",
      "54 Train Loss 20.658031 Test RE 0.0008693372119374848 c 1.0280243 k 0.00908164 m 4.4689026\n",
      "55 Train Loss 19.852396 Test RE 0.0007739045458286148 c 1.0222616 k 0.009177505 m 4.494864\n",
      "56 Train Loss 17.030264 Test RE 0.000761350217384002 c 1.0133841 k 0.009455344 m 4.6752276\n",
      "57 Train Loss 15.83641 Test RE 0.0007688480053769298 c 1.0248853 k 0.009164227 m 4.6066566\n",
      "58 Train Loss 14.631511 Test RE 0.0007381566006580954 c 1.0211078 k 0.009268625 m 4.610274\n",
      "59 Train Loss 14.118234 Test RE 0.000746158383722438 c 1.0199219 k 0.00928056 m 4.6054764\n",
      "60 Train Loss 13.80522 Test RE 0.0007655839606361869 c 1.0222026 k 0.009241817 m 4.591155\n",
      "61 Train Loss 11.565002 Test RE 0.0006573531842513917 c 1.0175686 k 0.009365902 m 4.684742\n",
      "62 Train Loss 7.741026 Test RE 0.0005347128159353134 c 1.0075876 k 0.009588606 m 4.9833255\n",
      "63 Train Loss 6.0460825 Test RE 0.0005251424024263041 c 1.0059787 k 0.00968832 m 5.1149783\n",
      "64 Train Loss 5.5243473 Test RE 0.0005161649201794547 c 1.0030407 k 0.00976748 m 5.1317215\n",
      "65 Train Loss 5.20346 Test RE 0.0005099939008117213 c 1.0022643 k 0.009774975 m 5.0780954\n",
      "66 Train Loss 5.101099 Test RE 0.0005081823433918976 c 1.0065013 k 0.00967692 m 5.019602\n",
      "67 Train Loss 5.067626 Test RE 0.0005083597778419224 c 1.0066614 k 0.009663551 m 4.9866357\n",
      "68 Train Loss 5.0261335 Test RE 0.0005145462041359812 c 1.0067977 k 0.009658355 m 4.9544044\n",
      "69 Train Loss 4.9978704 Test RE 0.0005112424210179605 c 1.0086504 k 0.009620888 m 4.9572806\n",
      "70 Train Loss 4.8515015 Test RE 0.0004868410811401178 c 1.0059023 k 0.009693406 m 5.015576\n",
      "71 Train Loss 4.2345834 Test RE 0.0004663690421876449 c 1.0038496 k 0.009721661 m 4.9743752\n",
      "72 Train Loss 3.6601474 Test RE 0.0004279341561243195 c 1.0114015 k 0.0095725 m 4.8810706\n",
      "73 Train Loss 3.2821414 Test RE 0.00039228239948697544 c 1.0124036 k 0.009540995 m 4.7896953\n",
      "74 Train Loss 3.1923957 Test RE 0.0003843584172549707 c 1.0118104 k 0.009551457 m 4.7878838\n",
      "75 Train Loss 3.0704021 Test RE 0.00037292330114915584 c 1.0129018 k 0.009522477 m 4.822441\n",
      "76 Train Loss 2.773201 Test RE 0.0003583400289346408 c 1.0101662 k 0.009616691 m 4.8958\n",
      "77 Train Loss 2.5209162 Test RE 0.00035268347044168193 c 1.0065365 k 0.009712714 m 4.9430995\n",
      "78 Train Loss 2.3173804 Test RE 0.0003295284934720133 c 1.0056603 k 0.009744632 m 4.990352\n",
      "79 Train Loss 2.169416 Test RE 0.0003207508942618556 c 1.0036587 k 0.009798603 m 5.0228944\n",
      "80 Train Loss 2.0737197 Test RE 0.00030653143068265655 c 1.0021801 k 0.009834121 m 5.0299125\n",
      "81 Train Loss 1.9641936 Test RE 0.0002936631268129984 c 1.0003586 k 0.0098999785 m 5.0616965\n",
      "82 Train Loss 1.8265288 Test RE 0.00029729584886232087 c 1.0015539 k 0.009885565 m 5.100836\n",
      "83 Train Loss 1.6419502 Test RE 0.00026097509746923086 c 0.9994192 k 0.009932844 m 5.115772\n",
      "84 Train Loss 1.4450148 Test RE 0.0002401239401038337 c 0.99907964 k 0.009950048 m 5.1280684\n",
      "85 Train Loss 1.4003986 Test RE 0.00024213563337077354 c 0.998647 k 0.009967553 m 5.138354\n",
      "86 Train Loss 1.3492799 Test RE 0.00022287687981012224 c 0.99697024 k 0.010018894 m 5.157685\n",
      "87 Train Loss 1.2661033 Test RE 0.00020444824062166897 c 0.9966964 k 0.010025308 m 5.1658053\n",
      "88 Train Loss 1.1629052 Test RE 0.0001935575075695302 c 0.9970772 k 0.010016535 m 5.1722302\n",
      "89 Train Loss 1.0946565 Test RE 0.0001734001589370766 c 0.99607706 k 0.01005396 m 5.1694093\n",
      "90 Train Loss 1.0190979 Test RE 0.00016662246121771306 c 0.9969809 k 0.010025835 m 5.142735\n",
      "91 Train Loss 0.7036736 Test RE 0.00016464214773771613 c 1.0021659 k 0.009871877 m 5.016315\n",
      "92 Train Loss 0.610008 Test RE 0.00014941834474700545 c 1.0035654 k 0.009848215 m 4.999565\n",
      "93 Train Loss 0.5770297 Test RE 0.0001513561298519722 c 1.0037291 k 0.009841925 m 4.9816437\n",
      "94 Train Loss 0.5645563 Test RE 0.00014976212877461479 c 1.0036253 k 0.009847015 m 4.969163\n",
      "95 Train Loss 0.56076986 Test RE 0.00014692337270463094 c 1.0034192 k 0.009847347 m 4.9720345\n",
      "96 Train Loss 0.55295944 Test RE 0.00015043552646847434 c 1.0031177 k 0.009858912 m 4.9758058\n",
      "97 Train Loss 0.5477731 Test RE 0.00015470141047147838 c 1.003177 k 0.009858788 m 4.971357\n",
      "98 Train Loss 0.5249442 Test RE 0.00015158995068181586 c 1.002748 k 0.009871223 m 4.9821243\n",
      "99 Train Loss 0.5126203 Test RE 0.00014665907652358258 c 1.0031703 k 0.009858892 m 4.973811\n",
      "100 Train Loss 0.5063721 Test RE 0.0001486058050842904 c 1.0037842 k 0.00984421 m 4.967365\n",
      "101 Train Loss 0.50298023 Test RE 0.00015127774359673308 c 1.0037401 k 0.009844524 m 4.9664755\n",
      "102 Train Loss 0.4979413 Test RE 0.00014903426137200201 c 1.003255 k 0.009853235 m 4.964631\n",
      "103 Train Loss 0.49073464 Test RE 0.000147764813143403 c 1.0033761 k 0.009854929 m 4.9615164\n",
      "104 Train Loss 0.463474 Test RE 0.00014698528403580866 c 1.0038029 k 0.009839993 m 4.9600487\n",
      "105 Train Loss 0.4509862 Test RE 0.00014465476500878077 c 1.0035692 k 0.009851995 m 4.962929\n",
      "106 Train Loss 0.4267125 Test RE 0.00013744277494888947 c 1.0033613 k 0.009852704 m 4.983193\n",
      "107 Train Loss 0.4180176 Test RE 0.00013318911757751156 c 1.0030698 k 0.009864396 m 4.982069\n",
      "108 Train Loss 0.41110262 Test RE 0.00013606905515000555 c 1.0027956 k 0.0098708905 m 4.9853916\n",
      "109 Train Loss 0.39894047 Test RE 0.0001348029745061222 c 1.0033586 k 0.009855467 m 4.9804287\n",
      "110 Train Loss 0.37856713 Test RE 0.00012822577368646742 c 1.0027062 k 0.009875688 m 4.982402\n",
      "111 Train Loss 0.37127623 Test RE 0.00012851945206863083 c 1.0029919 k 0.009865466 m 4.9719777\n",
      "112 Train Loss 0.36900425 Test RE 0.00012725844549571158 c 1.0032109 k 0.009861354 m 4.967952\n",
      "113 Train Loss 0.35986507 Test RE 0.00012904981431898584 c 1.0032753 k 0.009860822 m 4.9717755\n",
      "114 Train Loss 0.32333162 Test RE 0.00012117879366625171 c 1.0025272 k 0.009879775 m 4.9705863\n",
      "115 Train Loss 0.28517297 Test RE 0.00010093776023486513 c 1.0036194 k 0.009857351 m 4.9858727\n",
      "116 Train Loss 0.2565508 Test RE 8.883357659808713e-05 c 1.0012026 k 0.0099189365 m 5.0180826\n",
      "117 Train Loss 0.24264967 Test RE 8.58660463713748e-05 c 1.0005668 k 0.009945428 m 5.036157\n",
      "118 Train Loss 0.23081411 Test RE 8.06668772487325e-05 c 1.0015148 k 0.009925243 m 5.0363007\n",
      "119 Train Loss 0.2142777 Test RE 7.65443798682126e-05 c 1.000126 k 0.0099544795 m 5.0273952\n",
      "120 Train Loss 0.19665715 Test RE 7.28164947414264e-05 c 1.0008818 k 0.00994036 m 5.026466\n",
      "121 Train Loss 0.1815903 Test RE 6.697059985442015e-05 c 1.0021013 k 0.009910719 m 5.0157404\n",
      "122 Train Loss 0.16138864 Test RE 6.427042701028785e-05 c 1.0013356 k 0.009931488 m 4.996647\n",
      "123 Train Loss 0.14283869 Test RE 6.280377006250649e-05 c 1.001539 k 0.009930036 m 4.9949865\n",
      "124 Train Loss 0.13859785 Test RE 6.062823072547205e-05 c 1.0016583 k 0.009924723 m 4.9907875\n",
      "125 Train Loss 0.13773575 Test RE 6.09063370882414e-05 c 1.0017797 k 0.009922328 m 4.9880133\n",
      "126 Train Loss 0.13631347 Test RE 6.013375308992378e-05 c 1.0015357 k 0.009929094 m 4.993409\n",
      "127 Train Loss 0.13573317 Test RE 5.971792919727323e-05 c 1.0014088 k 0.009933222 m 4.9956503\n",
      "128 Train Loss 0.13504264 Test RE 5.894371089596488e-05 c 1.0016512 k 0.009927858 m 4.9947443\n",
      "129 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "130 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "131 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "132 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "133 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "134 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "135 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "136 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "137 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "138 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "139 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "140 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "141 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "142 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "143 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "144 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "145 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "146 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "147 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "148 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "149 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "150 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "151 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "152 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "153 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "154 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "155 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "156 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "157 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "158 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "159 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "160 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "161 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "162 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "163 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "164 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "165 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "166 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "167 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "168 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "169 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "170 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "171 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "172 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "173 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "174 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "175 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "176 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "177 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "178 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "179 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "180 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "181 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "182 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "183 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "184 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "185 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "186 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "187 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "188 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "189 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "190 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "191 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "192 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "193 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "194 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "195 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "196 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "197 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "198 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "199 Train Loss 0.13451956 Test RE 5.796355204432866e-05 c 1.0014564 k 0.009931377 m 4.9968963\n",
      "Training time: 51.55\n",
      "Training time: 51.55\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 629770.4 Test RE 0.3006928763971639 c 0.002268795 k 0.011114423 m -6.914172e-05\n",
      "1 Train Loss 61566.348 Test RE 0.04546353861807885 c -0.008615306 k 0.11513393 m -5.3204985e-05\n",
      "2 Train Loss 12069.316 Test RE 0.036622346478639184 c -0.018797254 k 0.035349876 m 0.00013281085\n",
      "3 Train Loss 8994.69 Test RE 0.030063614518418866 c -0.00018121256 k 0.030116165 m 0.0012672835\n",
      "4 Train Loss 3927.4717 Test RE 0.013187492583007273 c 0.1767754 k 0.038235683 m 0.012432507\n",
      "5 Train Loss 2325.369 Test RE 0.009555001886656343 c 0.29967478 k 0.022212068 m 0.020501653\n",
      "6 Train Loss 1674.4246 Test RE 0.008254260760932415 c 0.44303873 k 0.022063714 m 0.029894603\n",
      "7 Train Loss 1136.1804 Test RE 0.007042035267022499 c 0.59484774 k 0.018834736 m 0.04023593\n",
      "8 Train Loss 432.6742 Test RE 0.005612971111721067 c 1.0305314 k 0.008312941 m 0.07295611\n",
      "9 Train Loss 365.82913 Test RE 0.004981357055477074 c 1.1054966 k 0.0069219153 m 0.08137338\n",
      "10 Train Loss 323.91312 Test RE 0.00392444406667129 c 1.1073148 k 0.006725363 m 0.093593866\n",
      "11 Train Loss 262.87668 Test RE 0.0033177196268453497 c 1.1799855 k 0.0048978017 m 0.1326128\n",
      "12 Train Loss 246.798 Test RE 0.0028251306245513515 c 1.1578183 k 0.0054111774 m 0.14529435\n",
      "13 Train Loss 237.62152 Test RE 0.00276067937564673 c 1.1924717 k 0.004461298 m 0.17039636\n",
      "14 Train Loss 234.8474 Test RE 0.002826772054035982 c 1.1869973 k 0.0046062274 m 0.17882039\n",
      "15 Train Loss 233.38138 Test RE 0.0028411437876301494 c 1.1865222 k 0.0045792875 m 0.19015764\n",
      "16 Train Loss 230.61935 Test RE 0.0029001467346159737 c 1.1919037 k 0.0044792295 m 0.23324046\n",
      "17 Train Loss 226.82593 Test RE 0.0029128038494439196 c 1.1870203 k 0.00458423 m 0.24587014\n",
      "18 Train Loss 225.83485 Test RE 0.0029345544376164915 c 1.1910995 k 0.0045226444 m 0.25712693\n",
      "19 Train Loss 224.60294 Test RE 0.0028988006459096773 c 1.1885598 k 0.0046031354 m 0.27465934\n",
      "20 Train Loss 221.46732 Test RE 0.0028529356771040135 c 1.172657 k 0.004994175 m 0.33588442\n",
      "21 Train Loss 220.48912 Test RE 0.0028245024125982125 c 1.172449 k 0.005032982 m 0.3587368\n",
      "22 Train Loss 218.34744 Test RE 0.0026729903567629947 c 1.1758453 k 0.0049849353 m 0.38312125\n",
      "23 Train Loss 217.31508 Test RE 0.002603958471748025 c 1.1751287 k 0.004970477 m 0.37548992\n",
      "24 Train Loss 215.09373 Test RE 0.0025988730724120076 c 1.171101 k 0.00505438 m 0.40426284\n",
      "25 Train Loss 212.35684 Test RE 0.002757706265562377 c 1.1690536 k 0.0051913313 m 0.5050293\n",
      "26 Train Loss 210.04865 Test RE 0.0025737341975999983 c 1.1738667 k 0.005057487 m 0.5076512\n",
      "27 Train Loss 207.47623 Test RE 0.002323336451458028 c 1.1708065 k 0.0051172306 m 0.53186274\n",
      "28 Train Loss 206.578 Test RE 0.0023082339559881595 c 1.169039 k 0.0051861396 m 0.5691974\n",
      "29 Train Loss 204.8544 Test RE 0.0024175775250233557 c 1.1714011 k 0.0051433076 m 0.62974995\n",
      "30 Train Loss 201.77411 Test RE 0.0022336460078586917 c 1.1535416 k 0.0055985083 m 0.6782232\n",
      "31 Train Loss 199.04652 Test RE 0.0022153170006135635 c 1.1514966 k 0.0056827264 m 0.7597774\n",
      "32 Train Loss 196.3328 Test RE 0.002257155265870763 c 1.1621796 k 0.005383657 m 0.75297904\n",
      "33 Train Loss 194.86847 Test RE 0.0020921510839015988 c 1.1608715 k 0.0054160403 m 0.74830616\n",
      "34 Train Loss 191.23166 Test RE 0.0019313848498616435 c 1.1423407 k 0.005777956 m 0.8563959\n",
      "35 Train Loss 183.12982 Test RE 0.002401397516298699 c 1.1320673 k 0.0060942983 m 0.9911746\n",
      "36 Train Loss 176.92064 Test RE 0.0023641755440345228 c 1.1405898 k 0.005957059 m 1.0897669\n",
      "37 Train Loss 169.12894 Test RE 0.0024164027549530022 c 1.1493198 k 0.0055969344 m 1.2541186\n",
      "38 Train Loss 142.93246 Test RE 0.0020507678355304133 c 1.1120766 k 0.006637451 m 1.6200857\n",
      "39 Train Loss 107.324036 Test RE 0.0018283177021359614 c 1.1177827 k 0.006457924 m 2.0542688\n",
      "40 Train Loss 97.210724 Test RE 0.001986501757253782 c 1.1107708 k 0.0068064947 m 2.3072083\n",
      "41 Train Loss 91.24986 Test RE 0.0017707079866180928 c 1.0845495 k 0.0073846355 m 2.39268\n",
      "42 Train Loss 82.2709 Test RE 0.0018147852833921233 c 1.0899773 k 0.007315405 m 2.5760598\n",
      "43 Train Loss 73.0549 Test RE 0.0018961875275207098 c 1.1034291 k 0.0069953613 m 2.7800994\n",
      "44 Train Loss 69.19802 Test RE 0.0017940689172035245 c 1.0900035 k 0.007311136 m 2.785418\n",
      "45 Train Loss 61.13096 Test RE 0.001535767318548516 c 1.0876821 k 0.0073120994 m 2.8605309\n",
      "46 Train Loss 52.579773 Test RE 0.0016391277745773428 c 1.0911592 k 0.007316115 m 3.1455066\n",
      "47 Train Loss 42.92047 Test RE 0.0016146740288705708 c 1.065377 k 0.007878794 m 3.484034\n",
      "48 Train Loss 34.911667 Test RE 0.0016562858073324751 c 1.0434577 k 0.008446625 m 3.9128382\n",
      "49 Train Loss 33.759293 Test RE 0.001663136631286237 c 1.0464212 k 0.008317115 m 4.0602856\n",
      "50 Train Loss 28.902332 Test RE 0.0015229209659508665 c 1.0439891 k 0.008454456 m 4.2836075\n",
      "51 Train Loss 22.475521 Test RE 0.001267351928214942 c 1.0082239 k 0.009313329 m 4.598641\n",
      "52 Train Loss 15.755739 Test RE 0.0008740708385565996 c 1.0209268 k 0.00916579 m 4.7199945\n",
      "53 Train Loss 13.385952 Test RE 0.0007627404176351263 c 1.0295964 k 0.008948043 m 4.615333\n",
      "54 Train Loss 10.6403 Test RE 0.0006162237265685563 c 1.0130142 k 0.009423074 m 4.6512394\n",
      "55 Train Loss 9.4796 Test RE 0.0004901998922591642 c 1.0209547 k 0.009269658 m 4.660625\n",
      "56 Train Loss 9.085021 Test RE 0.0004619910412839799 c 1.0169871 k 0.009361029 m 4.6524467\n",
      "57 Train Loss 8.755457 Test RE 0.0004236555049531455 c 1.0178376 k 0.009375886 m 4.6607575\n",
      "58 Train Loss 8.575769 Test RE 0.00039919272777964675 c 1.0193088 k 0.009346601 m 4.6238017\n",
      "59 Train Loss 8.26623 Test RE 0.0003945811732389863 c 1.0186168 k 0.009383906 m 4.5978146\n",
      "60 Train Loss 7.61098 Test RE 0.00035476881608207295 c 1.0187951 k 0.009374808 m 4.6425385\n",
      "61 Train Loss 6.770935 Test RE 0.00035599663565491767 c 1.014103 k 0.009535228 m 4.7184196\n",
      "62 Train Loss 6.6572647 Test RE 0.0003695866258215874 c 1.0135465 k 0.009550193 m 4.7099657\n",
      "63 Train Loss 6.4689837 Test RE 0.000362393858239497 c 1.0168896 k 0.009467249 m 4.6638694\n",
      "64 Train Loss 5.9734073 Test RE 0.00037728038410716856 c 1.0101433 k 0.009666106 m 4.689832\n",
      "65 Train Loss 5.132726 Test RE 0.00035800503782265245 c 1.0045669 k 0.00985203 m 4.899476\n",
      "66 Train Loss 4.4330845 Test RE 0.00026826261701375185 c 1.0039351 k 0.009841212 m 4.9532623\n",
      "67 Train Loss 4.086181 Test RE 0.00023647806147019534 c 1.0056995 k 0.009785246 m 4.9078836\n",
      "68 Train Loss 3.9935305 Test RE 0.0002404953859695674 c 1.0048034 k 0.009795039 m 4.950313\n",
      "69 Train Loss 3.887001 Test RE 0.00022873756324381345 c 1.001192 k 0.009889597 m 4.9749312\n",
      "70 Train Loss 3.7537906 Test RE 0.00023155806692426572 c 1.0038743 k 0.009837477 m 4.940771\n",
      "71 Train Loss 3.67462 Test RE 0.00023457460480604602 c 1.0052155 k 0.009797125 m 4.9625425\n",
      "72 Train Loss 3.0700736 Test RE 0.0002492687565748982 c 1.0017984 k 0.009920706 m 4.9858613\n",
      "73 Train Loss 2.4492526 Test RE 0.0001919110663861809 c 1.0026865 k 0.009864066 m 4.965894\n",
      "74 Train Loss 2.3922389 Test RE 0.00018937299497867886 c 1.0035282 k 0.009843861 m 4.959948\n",
      "75 Train Loss 2.3524659 Test RE 0.00019767751585813358 c 1.0044228 k 0.00983436 m 4.955401\n",
      "76 Train Loss 2.198567 Test RE 0.00018625419530012683 c 1.0034438 k 0.009835456 m 4.9667816\n",
      "77 Train Loss 2.097382 Test RE 0.0001843727319735989 c 1.0037746 k 0.009847639 m 4.9394093\n",
      "78 Train Loss 1.9520593 Test RE 0.00019551352120399017 c 1.0035489 k 0.00987349 m 4.949073\n",
      "79 Train Loss 1.7421429 Test RE 0.00016524260705116778 c 1.0026075 k 0.009875088 m 4.950711\n",
      "80 Train Loss 1.6226968 Test RE 0.00014822252373618527 c 1.0028386 k 0.009881295 m 4.9305863\n",
      "81 Train Loss 1.4550983 Test RE 0.00014667715370332443 c 1.000331 k 0.009959836 m 4.9810185\n",
      "82 Train Loss 1.1845927 Test RE 0.00014012892567936665 c 1.002708 k 0.009917079 m 4.9894743\n",
      "83 Train Loss 1.066881 Test RE 0.00013161235443791454 c 1.0045456 k 0.009864127 m 4.9300256\n",
      "84 Train Loss 1.0299779 Test RE 0.00013677061489024309 c 1.001972 k 0.009929441 m 4.947109\n",
      "85 Train Loss 1.0032939 Test RE 0.0001252516789525871 c 1.0014713 k 0.009944807 m 4.9519496\n",
      "86 Train Loss 0.9887909 Test RE 0.00011157000716787776 c 1.0023533 k 0.009920727 m 4.949931\n",
      "87 Train Loss 0.9784862 Test RE 0.00011007906218053434 c 1.0027913 k 0.009914471 m 4.963191\n",
      "88 Train Loss 0.8743045 Test RE 0.00012777622625350927 c 1.0038898 k 0.009904198 m 4.9787827\n",
      "89 Train Loss 0.75291765 Test RE 0.00013238241320200018 c 1.0028249 k 0.009928384 m 4.937655\n",
      "90 Train Loss 0.6678294 Test RE 0.00013028390368186472 c 1.0033523 k 0.00991976 m 4.956715\n",
      "91 Train Loss 0.5309958 Test RE 0.00012168805936180547 c 1.0016073 k 0.00998635 m 4.984574\n",
      "92 Train Loss 0.46676597 Test RE 9.874849760020298e-05 c 1.0001527 k 0.010001396 m 4.98791\n",
      "93 Train Loss 0.45544648 Test RE 9.910922884456986e-05 c 1.0003762 k 0.009999781 m 4.988826\n",
      "94 Train Loss 0.45254886 Test RE 9.683481762509235e-05 c 1.0002608 k 0.01000052 m 4.9859934\n",
      "95 Train Loss 0.43681324 Test RE 8.669574240984539e-05 c 0.999748 k 0.010008143 m 4.9818187\n",
      "96 Train Loss 0.41855317 Test RE 8.764993402348048e-05 c 1.0003788 k 0.009996361 m 4.9877563\n",
      "97 Train Loss 0.40931037 Test RE 8.813045946365477e-05 c 1.0002408 k 0.009995221 m 4.9869447\n",
      "98 Train Loss 0.40495113 Test RE 8.339365658392665e-05 c 1.0004466 k 0.009987326 m 4.9875674\n",
      "99 Train Loss 0.40184873 Test RE 8.180601046425019e-05 c 1.0007068 k 0.009980609 m 4.9865427\n",
      "100 Train Loss 0.39942545 Test RE 8.103275168236149e-05 c 1.0000395 k 0.009992566 m 4.9864993\n",
      "101 Train Loss 0.39293724 Test RE 8.319525316940469e-05 c 0.9999998 k 0.009993495 m 4.988388\n",
      "102 Train Loss 0.35752082 Test RE 8.539115596454007e-05 c 1.0015122 k 0.009952738 m 4.977446\n",
      "103 Train Loss 0.31330162 Test RE 9.292168399789729e-05 c 1.001491 k 0.00995115 m 4.971844\n",
      "104 Train Loss 0.29542693 Test RE 8.717421828884645e-05 c 1.0005242 k 0.009977178 m 4.9875755\n",
      "105 Train Loss 0.29156232 Test RE 8.01281791073748e-05 c 1.000873 k 0.009968944 m 4.9884157\n",
      "106 Train Loss 0.28971645 Test RE 7.780244292090357e-05 c 1.0007992 k 0.00996903 m 4.99329\n",
      "107 Train Loss 0.2836308 Test RE 8.03932550007193e-05 c 1.0005913 k 0.009980252 m 4.9944377\n",
      "108 Train Loss 0.2771986 Test RE 8.477152307274937e-05 c 1.0006385 k 0.009978064 m 4.9839115\n",
      "109 Train Loss 0.27340806 Test RE 8.51036182660198e-05 c 1.0005664 k 0.009981249 m 4.987153\n",
      "110 Train Loss 0.26920015 Test RE 8.391566946017502e-05 c 1.0005287 k 0.009981415 m 4.9876857\n",
      "111 Train Loss 0.26277217 Test RE 8.434750937512693e-05 c 1.0003546 k 0.00998827 m 4.985222\n",
      "112 Train Loss 0.2563336 Test RE 8.498641945844751e-05 c 0.99992895 k 0.0099979015 m 4.9910374\n",
      "113 Train Loss 0.24451588 Test RE 7.879388396180308e-05 c 1.0005069 k 0.009978701 m 4.9868627\n",
      "114 Train Loss 0.23898335 Test RE 7.908124093871358e-05 c 1.000595 k 0.009982468 m 4.984603\n",
      "115 Train Loss 0.23613255 Test RE 8.08912173013133e-05 c 1.0005156 k 0.009981941 m 4.986643\n",
      "116 Train Loss 0.23291123 Test RE 7.862737895261368e-05 c 1.0005674 k 0.009982 m 4.9865746\n",
      "117 Train Loss 0.2279119 Test RE 7.56478099135668e-05 c 1.0006021 k 0.009982517 m 4.984751\n",
      "118 Train Loss 0.22156942 Test RE 7.42981461841925e-05 c 1.0002595 k 0.009992569 m 4.9875216\n",
      "119 Train Loss 0.21637309 Test RE 6.811715677756867e-05 c 0.9996368 k 0.010007981 m 4.995759\n",
      "120 Train Loss 0.21176523 Test RE 6.635417556197117e-05 c 1.0000397 k 0.010001078 m 4.99628\n",
      "121 Train Loss 0.2092729 Test RE 6.685812322233341e-05 c 1.0005299 k 0.009989468 m 4.991687\n",
      "122 Train Loss 0.20461415 Test RE 6.990832363107991e-05 c 1.0001352 k 0.009999168 m 4.997464\n",
      "123 Train Loss 0.20344785 Test RE 7.236439500357823e-05 c 0.9999422 k 0.0100042205 m 4.9977484\n",
      "124 Train Loss 0.20254529 Test RE 7.114284158312482e-05 c 0.99983567 k 0.010007102 m 4.9946303\n",
      "125 Train Loss 0.20114784 Test RE 6.944680820128437e-05 c 0.99976313 k 0.0100086825 m 4.997258\n",
      "126 Train Loss 0.20046157 Test RE 6.90123931971874e-05 c 0.9999797 k 0.01000286 m 4.998684\n",
      "127 Train Loss 0.19958647 Test RE 6.864623654872075e-05 c 1.0001765 k 0.00999777 m 4.9947\n",
      "128 Train Loss 0.19839096 Test RE 6.951754097556879e-05 c 0.99982435 k 0.010006843 m 4.9953666\n",
      "129 Train Loss 0.1961692 Test RE 6.981477475965505e-05 c 0.9997127 k 0.01000844 m 5.001308\n",
      "130 Train Loss 0.18568096 Test RE 6.512873970437508e-05 c 1.0001798 k 0.009998064 m 4.998673\n",
      "131 Train Loss 0.16638723 Test RE 6.602150299885878e-05 c 0.9993161 k 0.0100214705 m 5.0066714\n",
      "132 Train Loss 0.14058799 Test RE 6.0667245789684985e-05 c 0.999572 k 0.010015375 m 5.013948\n",
      "133 Train Loss 0.11976857 Test RE 5.7021045389760976e-05 c 0.9997404 k 0.010007892 m 5.0103087\n",
      "134 Train Loss 0.103164874 Test RE 5.594673825253578e-05 c 1.0000498 k 0.009996336 m 4.998373\n",
      "135 Train Loss 0.09069783 Test RE 5.664522130376417e-05 c 1.0002782 k 0.009991825 m 4.9972467\n",
      "136 Train Loss 0.07686924 Test RE 4.7656818415086385e-05 c 0.9995624 k 0.010008251 m 5.006819\n",
      "137 Train Loss 0.073810294 Test RE 4.496178677347567e-05 c 0.9996776 k 0.0100115705 m 5.0067077\n",
      "138 Train Loss 0.07245502 Test RE 4.460036283090694e-05 c 0.99952465 k 0.010016273 m 5.009958\n",
      "139 Train Loss 0.07115321 Test RE 4.270364149408473e-05 c 0.9993858 k 0.010019408 m 5.0107307\n",
      "140 Train Loss 0.07067544 Test RE 4.2517183174248616e-05 c 0.9995911 k 0.010014152 m 5.008152\n",
      "141 Train Loss 0.07050905 Test RE 4.269498148762909e-05 c 0.99960697 k 0.010012802 m 5.0080934\n",
      "142 Train Loss 0.07031989 Test RE 4.2543660805181315e-05 c 0.9995816 k 0.01001382 m 5.0100985\n",
      "143 Train Loss 0.06897066 Test RE 4.29156056256577e-05 c 0.9994154 k 0.010021779 m 5.0131717\n",
      "144 Train Loss 0.066568546 Test RE 4.099084316625718e-05 c 0.9996092 k 0.010012375 m 5.005035\n",
      "145 Train Loss 0.06482906 Test RE 3.827545141467878e-05 c 0.9995747 k 0.010013109 m 5.0049577\n",
      "146 Train Loss 0.06193515 Test RE 3.792878700291427e-05 c 0.9993865 k 0.010019785 m 5.0100236\n",
      "147 Train Loss 0.060461555 Test RE 3.723678429789363e-05 c 0.9996516 k 0.010011826 m 5.005965\n",
      "148 Train Loss 0.058938753 Test RE 3.7142788168802614e-05 c 0.9996325 k 0.010012399 m 5.003004\n",
      "149 Train Loss 0.054988146 Test RE 3.90463541757624e-05 c 0.9995011 k 0.01001758 m 5.0073557\n",
      "150 Train Loss 0.053321615 Test RE 3.978660095030389e-05 c 0.99994624 k 0.01000454 m 5.004853\n",
      "151 Train Loss 0.051607817 Test RE 3.9562934182127696e-05 c 1.0001955 k 0.009999968 m 5.003645\n",
      "152 Train Loss 0.049890354 Test RE 3.930788497400002e-05 c 0.9997565 k 0.010009719 m 5.0048294\n",
      "153 Train Loss 0.04797725 Test RE 3.8622324784648513e-05 c 0.9995014 k 0.01001448 m 5.003728\n",
      "154 Train Loss 0.04577461 Test RE 3.935133356395832e-05 c 0.9997105 k 0.010010542 m 5.0032835\n",
      "155 Train Loss 0.043895908 Test RE 3.805626373284501e-05 c 0.99976754 k 0.010011455 m 5.004939\n",
      "156 Train Loss 0.041919533 Test RE 3.694303499909467e-05 c 0.9995098 k 0.010018048 m 5.002197\n",
      "157 Train Loss 0.03996277 Test RE 3.66762031049353e-05 c 0.99941385 k 0.010020539 m 5.002368\n",
      "158 Train Loss 0.0388033 Test RE 3.4345103999390436e-05 c 0.9995797 k 0.0100166425 m 5.0023527\n",
      "159 Train Loss 0.038632862 Test RE 3.340588633807083e-05 c 0.9996505 k 0.010015939 m 5.0015774\n",
      "160 Train Loss 0.0385083 Test RE 3.3322722728287275e-05 c 0.99962485 k 0.010016544 m 5.0013123\n",
      "161 Train Loss 0.038317643 Test RE 3.333036370342396e-05 c 0.99958867 k 0.01001652 m 5.002234\n",
      "162 Train Loss 0.03800471 Test RE 3.3661966433979036e-05 c 0.999757 k 0.010009913 m 5.000632\n",
      "163 Train Loss 0.03725244 Test RE 3.510746126348433e-05 c 1.0000175 k 0.009999345 m 4.9968214\n",
      "164 Train Loss 0.035309095 Test RE 3.455575687232423e-05 c 1.0003735 k 0.009986994 m 4.997691\n",
      "165 Train Loss 0.03263165 Test RE 3.1373564345055245e-05 c 1.000335 k 0.009985633 m 4.996892\n",
      "166 Train Loss 0.030719753 Test RE 2.8234944265529196e-05 c 1.0003026 k 0.00998744 m 4.996839\n",
      "167 Train Loss 0.029331831 Test RE 2.586406361747879e-05 c 1.0002309 k 0.009990905 m 5.003277\n",
      "168 Train Loss 0.027408993 Test RE 2.316404930250553e-05 c 0.99994427 k 0.01000122 m 5.0045824\n",
      "169 Train Loss 0.025975186 Test RE 2.3477879294244074e-05 c 1.0000548 k 0.009997115 m 5.000494\n",
      "170 Train Loss 0.024670158 Test RE 2.3772042172337544e-05 c 1.0000207 k 0.00999569 m 5.0012794\n",
      "171 Train Loss 0.024034332 Test RE 2.3152488777205143e-05 c 0.9999835 k 0.009997182 m 5.0008755\n",
      "172 Train Loss 0.023451012 Test RE 2.2772203825068695e-05 c 1.000034 k 0.009995799 m 5.00045\n",
      "173 Train Loss 0.022715895 Test RE 2.042470570086608e-05 c 1.0001364 k 0.009992826 m 5.0013337\n",
      "174 Train Loss 0.022503737 Test RE 1.9048628506005393e-05 c 1.0002402 k 0.009989535 m 4.999773\n",
      "175 Train Loss 0.02233916 Test RE 1.7788450784302786e-05 c 1.0002695 k 0.009988768 m 4.9997716\n",
      "176 Train Loss 0.021913113 Test RE 1.4949450937114154e-05 c 1.0000101 k 0.009995743 m 5.000223\n",
      "177 Train Loss 0.021223184 Test RE 1.4115882143908503e-05 c 0.9999292 k 0.009998656 m 5.0007224\n",
      "178 Train Loss 0.02105201 Test RE 1.3656327512873141e-05 c 1.000109 k 0.009994531 m 5.000085\n",
      "179 Train Loss 0.020868244 Test RE 1.1914509641851405e-05 c 1.0001137 k 0.009994377 m 4.9996095\n",
      "180 Train Loss 0.02077388 Test RE 1.1671962537830963e-05 c 1.0000615 k 0.009995998 m 4.9995027\n",
      "181 Train Loss 0.020691346 Test RE 1.1356167762560621e-05 c 1.000055 k 0.009996889 m 4.9999986\n",
      "182 Train Loss 0.020508738 Test RE 1.014619542924026e-05 c 0.9999771 k 0.009999827 m 5.0002675\n",
      "183 Train Loss 0.020399773 Test RE 9.733944208403777e-06 c 1.0000305 k 0.009998486 m 4.999214\n",
      "184 Train Loss 0.020304069 Test RE 9.382450129094239e-06 c 1.0000407 k 0.009998627 m 4.9990144\n",
      "185 Train Loss 0.020185735 Test RE 8.327851782253588e-06 c 0.9999633 k 0.010001702 m 4.9997253\n",
      "186 Train Loss 0.02006252 Test RE 8.05349844289027e-06 c 0.99995106 k 0.010002784 m 4.9996486\n",
      "187 Train Loss 0.019865263 Test RE 8.316933252780182e-06 c 0.9999482 k 0.010003176 m 4.9991693\n",
      "188 Train Loss 0.019627629 Test RE 7.3344771877562016e-06 c 0.9999038 k 0.010003948 m 4.999196\n",
      "189 Train Loss 0.01894596 Test RE 5.5873135415095205e-06 c 0.9999455 k 0.010002939 m 4.9981775\n",
      "190 Train Loss 0.018529566 Test RE 5.888184613851217e-06 c 1.0000192 k 0.01000208 m 4.9979024\n",
      "191 Train Loss 0.017596208 Test RE 7.098239578857542e-06 c 1.0000072 k 0.010001518 m 4.998571\n",
      "192 Train Loss 0.016903322 Test RE 6.8139102191248535e-06 c 1.0000361 k 0.009999809 m 4.9993963\n",
      "193 Train Loss 0.016390296 Test RE 6.479280746871593e-06 c 1.0001359 k 0.00999649 m 4.998575\n",
      "194 Train Loss 0.01628983 Test RE 5.536642295542666e-06 c 1.0000801 k 0.009997076 m 4.9983683\n",
      "195 Train Loss 0.016257215 Test RE 5.217853305624123e-06 c 1.0000607 k 0.009997555 m 4.99868\n",
      "196 Train Loss 0.016213492 Test RE 5.379777136567737e-06 c 1.0001155 k 0.009995969 m 4.997861\n",
      "197 Train Loss 0.016209451 Test RE 5.369228074886393e-06 c 1.0001249 k 0.009995651 m 4.9976835\n",
      "198 Train Loss 0.016198933 Test RE 5.3041817775676335e-06 c 1.0001374 k 0.009995241 m 4.9975033\n",
      "199 Train Loss 0.016174112 Test RE 5.644971209733996e-06 c 1.000139 k 0.009995031 m 4.99766\n",
      "Training time: 58.72\n",
      "Training time: 58.72\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 154333.36 Test RE 0.11408288584812812 c -0.018542113 k 0.12870929 m 0.0005832378\n",
      "1 Train Loss 6735.381 Test RE 0.023827861691552047 c -0.015478699 k 0.03209928 m 0.0007660593\n",
      "2 Train Loss 3178.8203 Test RE 0.008856841925356657 c 0.014295603 k 0.032142576 m 0.0026937528\n",
      "3 Train Loss 2582.3452 Test RE 0.0056243756322007545 c 0.09560804 k 0.030308628 m 0.008513311\n",
      "4 Train Loss 1931.8597 Test RE 0.003962300063000698 c 0.25526935 k 0.026591685 m 0.019919148\n",
      "5 Train Loss 1285.0295 Test RE 0.004298201844441691 c 0.5001347 k 0.021024993 m 0.037166163\n",
      "6 Train Loss 829.66455 Test RE 0.007280569119386316 c 0.83073723 k 0.01338192 m 0.060659897\n",
      "7 Train Loss 547.3661 Test RE 0.0061834769723869585 c 1.2008626 k 0.0045087496 m 0.087665364\n",
      "8 Train Loss 365.98425 Test RE 0.004140353124524817 c 1.2600851 k 0.0032194261 m 0.09379891\n",
      "9 Train Loss 295.07065 Test RE 0.002989031580260919 c 1.1918596 k 0.004556579 m 0.09161025\n",
      "10 Train Loss 281.6537 Test RE 0.002933630579890369 c 1.1510158 k 0.005750391 m 0.093537144\n",
      "11 Train Loss 262.6112 Test RE 0.002994924994895764 c 1.1641985 k 0.005193009 m 0.10398589\n",
      "12 Train Loss 245.98581 Test RE 0.002847961747170697 c 1.1755328 k 0.005035746 m 0.121367626\n",
      "13 Train Loss 241.90384 Test RE 0.0029109843937799493 c 1.1799147 k 0.004922685 m 0.13311617\n",
      "14 Train Loss 237.53474 Test RE 0.0029375890415957724 c 1.1820494 k 0.0048007714 m 0.14018616\n",
      "15 Train Loss 235.6993 Test RE 0.0027383075613391923 c 1.179992 k 0.004855738 m 0.1403876\n",
      "16 Train Loss 232.84927 Test RE 0.0024537479986398948 c 1.1791437 k 0.004892643 m 0.15313746\n",
      "17 Train Loss 231.18661 Test RE 0.0025579098001158713 c 1.1904874 k 0.00460895 m 0.16545889\n",
      "18 Train Loss 230.24023 Test RE 0.0025411765118683056 c 1.1926471 k 0.00455368 m 0.1658877\n",
      "19 Train Loss 227.94507 Test RE 0.0024808527884789336 c 1.1764513 k 0.004976017 m 0.19246346\n",
      "20 Train Loss 226.90675 Test RE 0.002548805850121066 c 1.1803337 k 0.0048570605 m 0.21042286\n",
      "21 Train Loss 225.07025 Test RE 0.0025063639881950386 c 1.1868422 k 0.0047316067 m 0.23054649\n",
      "22 Train Loss 224.2741 Test RE 0.0024464943369531048 c 1.1749583 k 0.004990381 m 0.2348625\n",
      "23 Train Loss 221.88567 Test RE 0.0024912125322040534 c 1.1754713 k 0.0050175535 m 0.2632093\n",
      "24 Train Loss 221.40466 Test RE 0.0024427539744037567 c 1.1772885 k 0.0049393363 m 0.27133378\n",
      "25 Train Loss 221.23886 Test RE 0.0023660664014006918 c 1.1761237 k 0.0049827294 m 0.27372634\n",
      "26 Train Loss 220.77151 Test RE 0.002393402896952814 c 1.1765755 k 0.0049472204 m 0.27944604\n",
      "27 Train Loss 219.69603 Test RE 0.0025035776204392385 c 1.1809525 k 0.0048273215 m 0.30532116\n",
      "28 Train Loss 218.98587 Test RE 0.0024232549749401543 c 1.1778103 k 0.004973121 m 0.32634735\n",
      "29 Train Loss 217.37782 Test RE 0.002392817899228516 c 1.1683053 k 0.005183684 m 0.38696522\n",
      "30 Train Loss 216.55853 Test RE 0.0023438930072405763 c 1.1766306 k 0.004969537 m 0.40845382\n",
      "31 Train Loss 214.64906 Test RE 0.0023419951052281486 c 1.1730809 k 0.0051668384 m 0.4347771\n",
      "32 Train Loss 213.25336 Test RE 0.0023258512352301425 c 1.1635771 k 0.0052682487 m 0.45674482\n",
      "33 Train Loss 212.05898 Test RE 0.002326178384509035 c 1.1739589 k 0.005059143 m 0.46092862\n",
      "34 Train Loss 211.69147 Test RE 0.0024086356268302614 c 1.1704726 k 0.005130367 m 0.45535153\n",
      "35 Train Loss 210.98428 Test RE 0.002463467055615892 c 1.1622298 k 0.0052843047 m 0.47349495\n",
      "36 Train Loss 209.38478 Test RE 0.002474954003767969 c 1.1649369 k 0.0052863737 m 0.52628446\n",
      "37 Train Loss 208.79585 Test RE 0.00255670355541014 c 1.1623316 k 0.00531925 m 0.5602656\n",
      "38 Train Loss 207.97473 Test RE 0.0025379746157479822 c 1.1575038 k 0.005429499 m 0.60609996\n",
      "39 Train Loss 206.27798 Test RE 0.002469631831379415 c 1.1519104 k 0.0055964724 m 0.6498751\n",
      "40 Train Loss 205.01605 Test RE 0.0024192941226393178 c 1.1586815 k 0.0054611266 m 0.6345688\n",
      "41 Train Loss 203.68536 Test RE 0.0023954072707468195 c 1.1640853 k 0.005214029 m 0.6404108\n",
      "42 Train Loss 201.68161 Test RE 0.0023183500990329423 c 1.1618689 k 0.0053460216 m 0.648806\n",
      "43 Train Loss 200.53513 Test RE 0.002350160286014852 c 1.1634866 k 0.0052679908 m 0.68086576\n",
      "44 Train Loss 199.2807 Test RE 0.002448723220301924 c 1.1637554 k 0.0053294254 m 0.73145604\n",
      "45 Train Loss 197.67284 Test RE 0.0023499064030400134 c 1.1563286 k 0.0054621575 m 0.8106675\n",
      "46 Train Loss 193.97246 Test RE 0.002252778057427564 c 1.1513592 k 0.005678848 m 0.8851186\n",
      "47 Train Loss 188.43796 Test RE 0.0021571796941113547 c 1.1537026 k 0.0055520977 m 0.8990557\n",
      "48 Train Loss 182.53593 Test RE 0.0021344373502622995 c 1.1657034 k 0.0052425093 m 1.0016507\n",
      "49 Train Loss 174.13472 Test RE 0.002147660666793709 c 1.155657 k 0.005468391 m 1.216171\n",
      "50 Train Loss 169.14594 Test RE 0.0022127343972664446 c 1.1582935 k 0.005393244 m 1.4062738\n",
      "51 Train Loss 150.04636 Test RE 0.0025252987874620743 c 1.163796 k 0.005285167 m 1.6106851\n",
      "52 Train Loss 130.7062 Test RE 0.0019224254004279733 c 1.1296208 k 0.006024224 m 1.6997569\n",
      "53 Train Loss 125.96296 Test RE 0.0020203607953851173 c 1.1556851 k 0.0054589836 m 1.7394072\n",
      "54 Train Loss 119.02487 Test RE 0.0019856571560251794 c 1.1438626 k 0.0058618463 m 1.7843206\n",
      "55 Train Loss 113.862144 Test RE 0.0016903515878107034 c 1.125031 k 0.006395306 m 1.8882251\n",
      "56 Train Loss 104.20562 Test RE 0.0015523548445922227 c 1.1190658 k 0.006364666 m 2.1122637\n",
      "57 Train Loss 96.95356 Test RE 0.0014985346721723635 c 1.1356881 k 0.0060673403 m 2.2635217\n",
      "58 Train Loss 83.08286 Test RE 0.0016234411275949355 c 1.0975982 k 0.0070656235 m 2.486572\n",
      "59 Train Loss 78.80178 Test RE 0.0016093981550230256 c 1.1055214 k 0.0069511523 m 2.573815\n",
      "60 Train Loss 71.25892 Test RE 0.0014619541644065321 c 1.0708172 k 0.007870296 m 2.887788\n",
      "61 Train Loss 58.496517 Test RE 0.0015116676099811142 c 1.0773813 k 0.0076381345 m 3.1360426\n",
      "62 Train Loss 50.43015 Test RE 0.0013680157741943427 c 1.0878901 k 0.007333834 m 3.3713262\n",
      "63 Train Loss 42.642643 Test RE 0.0013703275400415477 c 1.0379597 k 0.008575822 m 3.641998\n",
      "64 Train Loss 36.2403 Test RE 0.001205874744359947 c 1.0445411 k 0.008463983 m 3.8589787\n",
      "65 Train Loss 32.43123 Test RE 0.0011097743144730143 c 1.0442007 k 0.008607283 m 4.0063114\n",
      "66 Train Loss 28.613186 Test RE 0.0010068983909933854 c 1.0329399 k 0.008785586 m 4.067601\n",
      "67 Train Loss 23.401777 Test RE 0.0009214917423571302 c 1.0360694 k 0.008966502 m 4.2013836\n",
      "68 Train Loss 21.504108 Test RE 0.0008596154103078172 c 1.0220734 k 0.009314845 m 4.3123083\n",
      "69 Train Loss 19.711088 Test RE 0.0007611328700386375 c 1.0176244 k 0.009369804 m 4.430414\n",
      "70 Train Loss 16.862331 Test RE 0.0006735485224864853 c 1.0283272 k 0.009133407 m 4.5939474\n",
      "71 Train Loss 14.576569 Test RE 0.0005755861236746138 c 1.0175383 k 0.0093685165 m 4.612606\n",
      "72 Train Loss 13.663918 Test RE 0.0005620794681099413 c 1.0218271 k 0.009207903 m 4.636008\n",
      "73 Train Loss 11.645291 Test RE 0.0005220076528509536 c 1.0102518 k 0.009585051 m 4.7485976\n",
      "74 Train Loss 9.793035 Test RE 0.0004616215670798874 c 1.0135043 k 0.009485129 m 4.787196\n",
      "75 Train Loss 7.6692367 Test RE 0.00040586018142095944 c 1.0170538 k 0.009381687 m 4.801266\n",
      "76 Train Loss 6.5310993 Test RE 0.00035722882290233196 c 1.015858 k 0.0094087105 m 4.7426996\n",
      "77 Train Loss 5.594866 Test RE 0.0003499435681423904 c 1.0206667 k 0.009319469 m 4.752739\n",
      "78 Train Loss 4.7814693 Test RE 0.0003462596465471263 c 1.0125195 k 0.0095076775 m 4.816016\n",
      "79 Train Loss 4.2202983 Test RE 0.00033380377594710613 c 1.0067933 k 0.009705635 m 4.8974075\n",
      "80 Train Loss 3.7978632 Test RE 0.0003175275114284328 c 1.0083702 k 0.009700213 m 4.9626064\n",
      "81 Train Loss 3.0544832 Test RE 0.0002947080425488039 c 1.0023375 k 0.009859966 m 5.1119933\n",
      "82 Train Loss 2.7266312 Test RE 0.00028089110138810377 c 0.9962419 k 0.010069545 m 5.1620812\n",
      "83 Train Loss 2.462019 Test RE 0.0002702867117332222 c 0.9950978 k 0.010152885 m 5.1605067\n",
      "84 Train Loss 2.1607618 Test RE 0.00022695909645123504 c 0.9962756 k 0.010078244 m 5.1295466\n",
      "85 Train Loss 1.7083426 Test RE 0.0001558341998260695 c 0.9960681 k 0.010079895 m 5.0814567\n",
      "86 Train Loss 1.5017334 Test RE 0.00012782883389667618 c 0.9998333 k 0.009997044 m 5.072886\n",
      "87 Train Loss 1.2820503 Test RE 8.874733777337388e-05 c 0.99793506 k 0.010065824 m 5.0887346\n",
      "88 Train Loss 1.161679 Test RE 8.193634499026335e-05 c 0.9980609 k 0.010047096 m 5.0398293\n",
      "89 Train Loss 1.0829419 Test RE 8.767730358100871e-05 c 0.9996798 k 0.010015864 m 5.0283\n",
      "90 Train Loss 1.0218425 Test RE 8.687262466764017e-05 c 0.99959666 k 0.010015251 m 5.049233\n",
      "91 Train Loss 0.97148734 Test RE 8.432728277145527e-05 c 0.9981076 k 0.01004665 m 5.044163\n",
      "92 Train Loss 0.94875604 Test RE 8.098233142441697e-05 c 0.99780375 k 0.010071668 m 5.0424232\n",
      "93 Train Loss 0.9211881 Test RE 7.81089039274531e-05 c 0.99903905 k 0.010027998 m 5.029648\n",
      "94 Train Loss 0.8819049 Test RE 8.376824285913725e-05 c 1.0002038 k 0.009988001 m 5.0060177\n",
      "95 Train Loss 0.84545887 Test RE 8.120549420220137e-05 c 1.0007907 k 0.009967846 m 4.9844184\n",
      "96 Train Loss 0.7645863 Test RE 8.102271228995948e-05 c 1.0011003 k 0.00995743 m 4.9857984\n",
      "97 Train Loss 0.7247055 Test RE 8.03237878279596e-05 c 0.9999795 k 0.010004442 m 4.9968753\n",
      "98 Train Loss 0.70261276 Test RE 8.54314149591033e-05 c 0.9998359 k 0.010010327 m 4.9992623\n",
      "99 Train Loss 0.6812636 Test RE 7.923437379165248e-05 c 1.0000921 k 0.010003419 m 4.9966908\n",
      "100 Train Loss 0.6688681 Test RE 8.235638456902389e-05 c 0.9995426 k 0.010018966 m 4.990284\n",
      "101 Train Loss 0.6237384 Test RE 8.43419805926336e-05 c 0.9996953 k 0.010007885 m 4.9893575\n",
      "102 Train Loss 0.57891315 Test RE 8.477852734941101e-05 c 0.9999924 k 0.010005728 m 5.004631\n",
      "103 Train Loss 0.50395054 Test RE 7.917828744109448e-05 c 0.99753517 k 0.010078493 m 5.003475\n",
      "104 Train Loss 0.40491804 Test RE 6.410760588268417e-05 c 0.9992585 k 0.01002423 m 5.012475\n",
      "105 Train Loss 0.3441094 Test RE 6.159639437388317e-05 c 0.9992165 k 0.010021199 m 5.017635\n",
      "106 Train Loss 0.31443703 Test RE 6.137331887125665e-05 c 0.9998737 k 0.010004172 m 5.0083303\n",
      "107 Train Loss 0.28756067 Test RE 4.9031088384445506e-05 c 1.0007821 k 0.009982058 m 5.006856\n",
      "108 Train Loss 0.27300474 Test RE 4.274863598833801e-05 c 1.000072 k 0.009997486 m 5.001384\n",
      "109 Train Loss 0.2640689 Test RE 4.336477890937201e-05 c 0.9999513 k 0.00999866 m 4.998148\n",
      "110 Train Loss 0.25178334 Test RE 4.119409225204475e-05 c 0.99958503 k 0.010013228 m 5.009134\n",
      "111 Train Loss 0.24404828 Test RE 4.1029941629226343e-05 c 1.0000564 k 0.009998639 m 5.0061865\n",
      "112 Train Loss 0.22763684 Test RE 4.062712938673605e-05 c 0.99977815 k 0.010006456 m 5.0049706\n",
      "113 Train Loss 0.21510185 Test RE 3.968026087257409e-05 c 0.99974495 k 0.01000523 m 5.0015435\n",
      "114 Train Loss 0.2092062 Test RE 3.9019051907574796e-05 c 1.0005165 k 0.009989284 m 4.996223\n",
      "115 Train Loss 0.20445172 Test RE 3.942783836961419e-05 c 1.0002164 k 0.009996086 m 4.999551\n",
      "116 Train Loss 0.19696407 Test RE 3.7540122033564016e-05 c 0.9998645 k 0.01000213 m 4.998952\n",
      "117 Train Loss 0.19323245 Test RE 3.777856853691744e-05 c 1.0001698 k 0.009993611 m 4.996919\n",
      "118 Train Loss 0.19271012 Test RE 3.8347673137268206e-05 c 1.0000166 k 0.009997576 m 4.999367\n",
      "119 Train Loss 0.1920958 Test RE 3.828551713828032e-05 c 0.99995273 k 0.010000436 m 4.9994216\n",
      "120 Train Loss 0.18955894 Test RE 3.8645227148776154e-05 c 1.0002502 k 0.009992995 m 4.99767\n",
      "121 Train Loss 0.18689717 Test RE 3.901588698183125e-05 c 1.0001473 k 0.0099964235 m 5.0027747\n",
      "122 Train Loss 0.18506444 Test RE 3.793727988029425e-05 c 0.99992245 k 0.010001549 m 5.0020084\n",
      "123 Train Loss 0.18372041 Test RE 3.6651867549141004e-05 c 0.9999563 k 0.010000504 m 4.9985805\n",
      "124 Train Loss 0.18329287 Test RE 3.667010006707967e-05 c 0.999963 k 0.010001315 m 5.0004067\n",
      "125 Train Loss 0.18201303 Test RE 3.657615462852665e-05 c 0.9998305 k 0.010004174 m 5.0039535\n",
      "126 Train Loss 0.17986062 Test RE 3.594827132662774e-05 c 0.99979746 k 0.010003856 m 5.0016556\n",
      "127 Train Loss 0.17651145 Test RE 3.571860467234675e-05 c 0.99989814 k 0.010006472 m 5.001643\n",
      "128 Train Loss 0.17392963 Test RE 3.5487585924324203e-05 c 1.0001235 k 0.009996435 m 4.999088\n",
      "129 Train Loss 0.17126556 Test RE 3.732443408094407e-05 c 1.0005738 k 0.009985803 m 4.997719\n",
      "130 Train Loss 0.16700116 Test RE 3.807431744129681e-05 c 1.0002916 k 0.009993563 m 4.9981694\n",
      "131 Train Loss 0.16084093 Test RE 3.6654877908624525e-05 c 0.9997536 k 0.010006806 m 4.9986854\n",
      "132 Train Loss 0.14852554 Test RE 4.202510258525402e-05 c 1.0000614 k 0.010003055 m 4.9982734\n",
      "133 Train Loss 0.14314926 Test RE 4.2216218910001634e-05 c 0.9998819 k 0.010006047 m 4.9988885\n",
      "134 Train Loss 0.14042446 Test RE 4.062411987171312e-05 c 1.0000987 k 0.009997864 m 4.998612\n",
      "135 Train Loss 0.13690612 Test RE 3.8728410340463444e-05 c 1.0004963 k 0.00998599 m 4.9965215\n",
      "136 Train Loss 0.13239859 Test RE 3.868123153882228e-05 c 1.0000933 k 0.009994598 m 4.9923\n",
      "137 Train Loss 0.12181703 Test RE 3.616084851096937e-05 c 0.9997023 k 0.0100060105 m 4.994068\n",
      "138 Train Loss 0.11061409 Test RE 3.452293253484343e-05 c 1.00057 k 0.009987258 m 4.99347\n",
      "139 Train Loss 0.10083243 Test RE 3.676447830633328e-05 c 1.0005969 k 0.009988101 m 4.9929476\n",
      "140 Train Loss 0.093076885 Test RE 3.084051198432961e-05 c 1.0003778 k 0.009985986 m 4.9975185\n",
      "141 Train Loss 0.08650907 Test RE 2.904572055543055e-05 c 1.0000927 k 0.009997932 m 5.0004516\n",
      "142 Train Loss 0.079849906 Test RE 2.5568500719711848e-05 c 0.9999017 k 0.010003565 m 4.9999347\n",
      "143 Train Loss 0.07662679 Test RE 2.3785229598243433e-05 c 0.9999089 k 0.010001581 m 4.996065\n",
      "144 Train Loss 0.075345874 Test RE 2.3397307396445026e-05 c 0.9999182 k 0.010001254 m 4.9988513\n",
      "145 Train Loss 0.07456207 Test RE 2.186622828517933e-05 c 1.0000247 k 0.00999981 m 5.0024824\n",
      "146 Train Loss 0.07383145 Test RE 2.075293836816849e-05 c 0.999871 k 0.010004407 m 5.0048194\n",
      "147 Train Loss 0.07243769 Test RE 1.963234909761448e-05 c 0.9999378 k 0.010002891 m 5.0033064\n",
      "148 Train Loss 0.068004645 Test RE 2.0626058771821975e-05 c 1.0002903 k 0.009991781 m 5.002695\n",
      "149 Train Loss 0.0604893 Test RE 2.1505358872506783e-05 c 1.0000572 k 0.010000711 m 5.00787\n",
      "150 Train Loss 0.05692207 Test RE 2.1814433471155263e-05 c 0.99960375 k 0.010011561 m 5.0067496\n",
      "151 Train Loss 0.053372085 Test RE 1.8568419937831767e-05 c 0.99923027 k 0.01002073 m 5.008012\n",
      "152 Train Loss 0.048943855 Test RE 1.8671722972267073e-05 c 0.9997402 k 0.010006848 m 5.0043716\n",
      "153 Train Loss 0.04519038 Test RE 1.8594954662713274e-05 c 1.0003207 k 0.009991266 m 4.9974236\n",
      "154 Train Loss 0.0400997 Test RE 2.0844430115299584e-05 c 0.99979776 k 0.010006041 m 4.99977\n",
      "155 Train Loss 0.034790315 Test RE 2.116532498483131e-05 c 0.99958396 k 0.010009641 m 5.0026937\n",
      "156 Train Loss 0.032960407 Test RE 1.9704554019372225e-05 c 0.9999381 k 0.010002006 m 5.002464\n",
      "157 Train Loss 0.032755222 Test RE 1.9432565900000318e-05 c 0.9999605 k 0.010001273 m 5.0020375\n",
      "158 Train Loss 0.032422375 Test RE 1.9631399440498692e-05 c 0.9999016 k 0.010003024 m 5.002101\n",
      "159 Train Loss 0.031230416 Test RE 1.7932143376900688e-05 c 1.0000006 k 0.010000059 m 5.0003376\n",
      "160 Train Loss 0.030807637 Test RE 1.825549895206129e-05 c 1.0001568 k 0.009995388 m 4.9976735\n",
      "161 Train Loss 0.03068753 Test RE 1.8465174123797998e-05 c 1.0001658 k 0.009995048 m 4.9981065\n",
      "162 Train Loss 0.030548466 Test RE 1.8240332591601353e-05 c 1.0000609 k 0.009998189 m 4.9998183\n",
      "163 Train Loss 0.030376572 Test RE 1.9012774728587937e-05 c 1.0000149 k 0.009998933 m 4.999632\n",
      "164 Train Loss 0.03029755 Test RE 1.951133745133086e-05 c 1.0000057 k 0.009999373 m 4.9998546\n",
      "165 Train Loss 0.030292537 Test RE 1.9539491442575315e-05 c 0.9999956 k 0.009999793 m 5.000132\n",
      "166 Train Loss 0.030291658 Test RE 1.9527842003264527e-05 c 0.9999939 k 0.009999875 m 5.0001616\n",
      "167 Train Loss 0.030279592 Test RE 1.947797011393737e-05 c 0.9999856 k 0.010000196 m 5.0002327\n",
      "168 Train Loss 0.030278137 Test RE 1.947242522025766e-05 c 0.9999856 k 0.010000196 m 5.0002327\n",
      "169 Train Loss 0.030278137 Test RE 1.947242522025766e-05 c 0.9999856 k 0.010000196 m 5.0002327\n",
      "170 Train Loss 0.030278137 Test RE 1.947242522025766e-05 c 0.9999856 k 0.010000196 m 5.0002327\n",
      "171 Train Loss 0.030278137 Test RE 1.947242522025766e-05 c 0.9999856 k 0.010000196 m 5.0002327\n",
      "172 Train Loss 0.030278137 Test RE 1.947242522025766e-05 c 0.9999856 k 0.010000196 m 5.0002327\n",
      "173 Train Loss 0.030278137 Test RE 1.947242522025766e-05 c 0.9999856 k 0.010000196 m 5.0002327\n",
      "174 Train Loss 0.030278137 Test RE 1.947242522025766e-05 c 0.9999856 k 0.010000196 m 5.0002327\n",
      "175 Train Loss 0.030278137 Test RE 1.947242522025766e-05 c 0.9999856 k 0.010000196 m 5.0002327\n",
      "176 Train Loss 0.030278137 Test RE 1.947242522025766e-05 c 0.9999856 k 0.010000196 m 5.0002327\n",
      "177 Train Loss 0.030278137 Test RE 1.947242522025766e-05 c 0.9999856 k 0.010000196 m 5.0002327\n",
      "178 Train Loss 0.030278137 Test RE 1.947242522025766e-05 c 0.9999856 k 0.010000196 m 5.0002327\n",
      "179 Train Loss 0.030278137 Test RE 1.947242522025766e-05 c 0.9999856 k 0.010000196 m 5.0002327\n",
      "180 Train Loss 0.030278137 Test RE 1.947242522025766e-05 c 0.9999856 k 0.010000196 m 5.0002327\n",
      "181 Train Loss 0.030278137 Test RE 1.947242522025766e-05 c 0.9999856 k 0.010000196 m 5.0002327\n",
      "182 Train Loss 0.030278137 Test RE 1.947242522025766e-05 c 0.9999856 k 0.010000196 m 5.0002327\n",
      "183 Train Loss 0.030278137 Test RE 1.947242522025766e-05 c 0.9999856 k 0.010000196 m 5.0002327\n",
      "184 Train Loss 0.030278137 Test RE 1.947242522025766e-05 c 0.9999856 k 0.010000196 m 5.0002327\n",
      "185 Train Loss 0.030278137 Test RE 1.947242522025766e-05 c 0.9999856 k 0.010000196 m 5.0002327\n",
      "186 Train Loss 0.030278137 Test RE 1.947242522025766e-05 c 0.9999856 k 0.010000196 m 5.0002327\n",
      "187 Train Loss 0.030278137 Test RE 1.947242522025766e-05 c 0.9999856 k 0.010000196 m 5.0002327\n",
      "188 Train Loss 0.030278137 Test RE 1.947242522025766e-05 c 0.9999856 k 0.010000196 m 5.0002327\n",
      "189 Train Loss 0.030278137 Test RE 1.947242522025766e-05 c 0.9999856 k 0.010000196 m 5.0002327\n",
      "190 Train Loss 0.030278137 Test RE 1.947242522025766e-05 c 0.9999856 k 0.010000196 m 5.0002327\n",
      "191 Train Loss 0.030278137 Test RE 1.947242522025766e-05 c 0.9999856 k 0.010000196 m 5.0002327\n",
      "192 Train Loss 0.030278137 Test RE 1.947242522025766e-05 c 0.9999856 k 0.010000196 m 5.0002327\n",
      "193 Train Loss 0.030278137 Test RE 1.947242522025766e-05 c 0.9999856 k 0.010000196 m 5.0002327\n",
      "194 Train Loss 0.030278137 Test RE 1.947242522025766e-05 c 0.9999856 k 0.010000196 m 5.0002327\n",
      "195 Train Loss 0.030278137 Test RE 1.947242522025766e-05 c 0.9999856 k 0.010000196 m 5.0002327\n",
      "196 Train Loss 0.030278137 Test RE 1.947242522025766e-05 c 0.9999856 k 0.010000196 m 5.0002327\n",
      "197 Train Loss 0.030278137 Test RE 1.947242522025766e-05 c 0.9999856 k 0.010000196 m 5.0002327\n",
      "198 Train Loss 0.030278137 Test RE 1.947242522025766e-05 c 0.9999856 k 0.010000196 m 5.0002327\n",
      "199 Train Loss 0.030278137 Test RE 1.947242522025766e-05 c 0.9999856 k 0.010000196 m 5.0002327\n",
      "Training time: 46.19\n",
      "Training time: 46.19\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 487633.66 Test RE 0.26468583838094667 c 0.010367812 k 0.05503324 m -8.9825335e-05\n",
      "1 Train Loss 6287.6436 Test RE 0.0193749287557936 c 0.022086194 k 0.03021185 m 6.0541384e-05\n",
      "2 Train Loss 5561.5107 Test RE 0.016783584925843437 c 0.03427604 k 0.03231218 m 0.0005125625\n",
      "3 Train Loss 3865.5737 Test RE 0.012222664608003376 c 0.22107746 k 0.02750451 m 0.007941322\n",
      "4 Train Loss 2988.0012 Test RE 0.010844939008369492 c 0.35255557 k 0.024466615 m 0.0133703975\n",
      "5 Train Loss 1919.1047 Test RE 0.010156709800989799 c 0.80579084 k 0.012299426 m 0.03306592\n",
      "6 Train Loss 1219.9185 Test RE 0.0076815945173827 c 1.2987183 k 0.0012832102 m 0.05568452\n",
      "7 Train Loss 741.9876 Test RE 0.006101157288158709 c 1.3501732 k -3.3101387e-05 m 0.0706447\n",
      "8 Train Loss 501.55453 Test RE 0.0041732077163924 c 1.2979639 k 0.0015771269 m 0.08680286\n",
      "9 Train Loss 409.48624 Test RE 0.0033626090900802175 c 1.290332 k 0.0019793843 m 0.10005239\n",
      "10 Train Loss 305.04807 Test RE 0.003276888047341073 c 1.228719 k 0.0033796986 m 0.13341989\n",
      "11 Train Loss 252.38939 Test RE 0.0022692021216211347 c 1.1804866 k 0.0047647255 m 0.1911731\n",
      "12 Train Loss 242.10971 Test RE 0.0021923512214433952 c 1.1695828 k 0.0051095486 m 0.22941345\n",
      "13 Train Loss 235.23697 Test RE 0.0023339766340912714 c 1.149085 k 0.005588729 m 0.265851\n",
      "14 Train Loss 227.35017 Test RE 0.002060001837380225 c 1.1732187 k 0.0050583077 m 0.3009782\n",
      "15 Train Loss 218.08987 Test RE 0.0024039937389962775 c 1.1732625 k 0.005150516 m 0.3767381\n",
      "16 Train Loss 214.75096 Test RE 0.0025976190486411586 c 1.1700722 k 0.0052010603 m 0.39140737\n",
      "17 Train Loss 212.17189 Test RE 0.002579375931451895 c 1.1794103 k 0.0049830154 m 0.39761093\n",
      "18 Train Loss 209.07585 Test RE 0.002847869900963722 c 1.166886 k 0.005278276 m 0.44561908\n",
      "19 Train Loss 202.72949 Test RE 0.002805917296490057 c 1.1673509 k 0.005292701 m 0.56600046\n",
      "20 Train Loss 195.44116 Test RE 0.0026544873544243337 c 1.1666989 k 0.0054012784 m 0.7360654\n",
      "21 Train Loss 192.81018 Test RE 0.002587686525818566 c 1.149378 k 0.0057611167 m 0.74929684\n",
      "22 Train Loss 188.89418 Test RE 0.0023792898792784037 c 1.147257 k 0.0058062603 m 0.8563589\n",
      "23 Train Loss 185.56944 Test RE 0.0021653193405278865 c 1.1495987 k 0.0058321483 m 0.91826355\n",
      "24 Train Loss 174.91092 Test RE 0.002179355045955327 c 1.1566176 k 0.0055982736 m 0.99638546\n",
      "25 Train Loss 163.25272 Test RE 0.0025112124757663 c 1.1532519 k 0.0057025 m 1.1092073\n",
      "26 Train Loss 151.4263 Test RE 0.0023013546039961498 c 1.121402 k 0.0064645917 m 1.4179307\n",
      "27 Train Loss 128.78027 Test RE 0.0016676503080490551 c 1.1393591 k 0.0060838456 m 2.0112443\n",
      "28 Train Loss 122.11139 Test RE 0.0014087015146692949 c 1.127773 k 0.006434883 m 2.0613163\n",
      "29 Train Loss 120.59852 Test RE 0.0013864060013189206 c 1.1030688 k 0.0070068687 m 2.0997083\n",
      "30 Train Loss 116.11154 Test RE 0.001395628468801293 c 1.0957187 k 0.0071338103 m 2.4057806\n",
      "31 Train Loss 107.61777 Test RE 0.0013553963362833426 c 1.1102109 k 0.006912774 m 2.8027697\n",
      "32 Train Loss 91.80831 Test RE 0.0014431766657989876 c 1.080054 k 0.0075839274 m 3.1604514\n",
      "33 Train Loss 84.32715 Test RE 0.0016303116103323794 c 1.1013881 k 0.0068815397 m 3.0573602\n",
      "34 Train Loss 80.900925 Test RE 0.0015761816746899992 c 1.0910842 k 0.00723034 m 3.027168\n",
      "35 Train Loss 63.534462 Test RE 0.0014298228057584728 c 1.0889486 k 0.0072386106 m 3.4665549\n",
      "36 Train Loss 44.549118 Test RE 0.0009066186445402782 c 1.0610211 k 0.008084248 m 3.76633\n",
      "37 Train Loss 38.696564 Test RE 0.000592945846403811 c 1.0462309 k 0.008567062 m 3.876809\n",
      "38 Train Loss 37.664303 Test RE 0.0005501653274622019 c 1.0437406 k 0.008647165 m 4.0083327\n",
      "39 Train Loss 36.313465 Test RE 0.0005985957827392078 c 1.0330889 k 0.008962631 m 4.1847887\n",
      "40 Train Loss 33.44053 Test RE 0.0007419119033139518 c 1.0230557 k 0.009220596 m 4.5355315\n",
      "41 Train Loss 29.559278 Test RE 0.0008462392612411173 c 1.0258532 k 0.00909691 m 4.619772\n",
      "42 Train Loss 24.87558 Test RE 0.0010219155825035291 c 1.0105677 k 0.009501772 m 4.57757\n",
      "43 Train Loss 23.128506 Test RE 0.0008156846513392888 c 1.019381 k 0.0092724 m 4.5147414\n",
      "44 Train Loss 19.793762 Test RE 0.000669407488395048 c 1.0251803 k 0.009070911 m 4.5095015\n",
      "45 Train Loss 15.7908745 Test RE 0.0004569401654781297 c 1.0186294 k 0.009328429 m 4.6895075\n",
      "46 Train Loss 14.65629 Test RE 0.0004061834660378306 c 1.0128943 k 0.009497458 m 4.851625\n",
      "47 Train Loss 13.764606 Test RE 0.0003779889539491591 c 1.013111 k 0.00952688 m 4.805064\n",
      "48 Train Loss 13.535816 Test RE 0.000375687497295378 c 1.0111928 k 0.00958762 m 4.8433166\n",
      "49 Train Loss 13.440336 Test RE 0.0003868056414110332 c 1.0101151 k 0.00962385 m 4.837785\n",
      "50 Train Loss 13.115134 Test RE 0.00038303033132707143 c 1.0111085 k 0.009603297 m 4.749341\n",
      "51 Train Loss 12.615662 Test RE 0.0004243546197979531 c 1.0137691 k 0.009529012 m 4.8079967\n",
      "52 Train Loss 12.042286 Test RE 0.0004132619415298722 c 1.0090775 k 0.009651013 m 4.9111633\n",
      "53 Train Loss 11.489232 Test RE 0.0003635650861402562 c 1.0122015 k 0.009556574 m 4.799101\n",
      "54 Train Loss 10.808123 Test RE 0.0003656444885757954 c 1.0178572 k 0.009417748 m 4.7200994\n",
      "55 Train Loss 10.030163 Test RE 0.00039118339549343273 c 1.0096291 k 0.009638399 m 4.866249\n",
      "56 Train Loss 9.134019 Test RE 0.00040256428640781023 c 1.0086254 k 0.009787102 m 4.8874154\n",
      "57 Train Loss 7.774081 Test RE 0.00033099532005808886 c 1.010891 k 0.009581177 m 4.804556\n",
      "58 Train Loss 7.0591555 Test RE 0.00026421227680128225 c 1.0077407 k 0.009717574 m 4.8579283\n",
      "59 Train Loss 6.916128 Test RE 0.0002711282441158818 c 1.0119836 k 0.009601347 m 4.814489\n",
      "60 Train Loss 6.662825 Test RE 0.0002711731401039758 c 1.0106798 k 0.009620347 m 4.8382297\n",
      "61 Train Loss 6.3887496 Test RE 0.00026467784215921834 c 1.0062002 k 0.0097505385 m 4.898331\n",
      "62 Train Loss 5.956041 Test RE 0.0002649554215536854 c 1.0146827 k 0.009565972 m 4.867661\n",
      "63 Train Loss 5.357424 Test RE 0.00018493816792982713 c 1.00833 k 0.009732209 m 4.8875027\n",
      "64 Train Loss 5.14242 Test RE 0.00019318793422653917 c 1.0047545 k 0.009818327 m 4.9004717\n",
      "65 Train Loss 4.8878527 Test RE 0.00018157895026106556 c 1.0071901 k 0.009764755 m 4.8753476\n",
      "66 Train Loss 4.8336205 Test RE 0.00017177743575992212 c 1.0055925 k 0.009815048 m 4.908056\n",
      "67 Train Loss 4.7972403 Test RE 0.00016243709776127073 c 1.0055445 k 0.009803019 m 4.93434\n",
      "68 Train Loss 4.4006963 Test RE 0.00021952505497410538 c 1.0089263 k 0.009660212 m 4.9228435\n",
      "69 Train Loss 3.4871573 Test RE 0.00016458747645296926 c 1.0083241 k 0.009707977 m 4.8488607\n",
      "70 Train Loss 3.3670416 Test RE 0.0001411128481130422 c 1.0071261 k 0.009719869 m 4.888407\n",
      "71 Train Loss 3.2819185 Test RE 0.00013392241498285564 c 1.0079011 k 0.009710058 m 4.889999\n",
      "72 Train Loss 3.2398095 Test RE 0.00013285179667151328 c 1.0082549 k 0.009713846 m 4.868068\n",
      "73 Train Loss 2.6556187 Test RE 0.00015225689332608734 c 1.0044357 k 0.009831869 m 4.901697\n",
      "74 Train Loss 2.1613474 Test RE 0.000102303033089487 c 1.0039012 k 0.009873675 m 4.948357\n",
      "75 Train Loss 2.0929751 Test RE 0.00012592655652024086 c 1.0036784 k 0.009886503 m 4.9239497\n",
      "76 Train Loss 2.0458653 Test RE 0.00014478914498851616 c 1.0034993 k 0.009890854 m 4.9366817\n",
      "77 Train Loss 1.9314104 Test RE 0.00014567634902034455 c 1.0033414 k 0.009892561 m 4.9669113\n",
      "78 Train Loss 1.657509 Test RE 0.00013331499790176178 c 1.0042236 k 0.009894628 m 4.945546\n",
      "79 Train Loss 1.4512069 Test RE 0.00010504687784742298 c 1.0026422 k 0.009910189 m 4.9610023\n",
      "80 Train Loss 1.391514 Test RE 0.0001131542593115177 c 1.0018889 k 0.009916522 m 4.971852\n",
      "81 Train Loss 1.322121 Test RE 0.00011914370089896159 c 1.0022537 k 0.009921853 m 4.9970336\n",
      "82 Train Loss 1.2978196 Test RE 0.00011520519966279206 c 1.0012983 k 0.00994749 m 5.0059037\n",
      "83 Train Loss 1.27439 Test RE 0.00013053235333809624 c 0.9996886 k 0.0099911075 m 5.0136914\n",
      "84 Train Loss 1.2594396 Test RE 0.00013885280341487607 c 0.9999692 k 0.009982779 m 5.0263376\n",
      "85 Train Loss 1.239294 Test RE 0.00014918537189541459 c 1.0011414 k 0.0099597 m 5.0257816\n",
      "86 Train Loss 1.2280275 Test RE 0.00015064068842181986 c 1.0004268 k 0.009968094 m 5.014462\n",
      "87 Train Loss 1.2186852 Test RE 0.00015433907811992684 c 1.0004468 k 0.009967084 m 5.00902\n",
      "88 Train Loss 1.2063493 Test RE 0.0001590222862637425 c 1.0000774 k 0.009984612 m 5.008046\n",
      "89 Train Loss 1.1832565 Test RE 0.0001531191432897528 c 0.99847937 k 0.010025462 m 5.0079246\n",
      "90 Train Loss 1.1094848 Test RE 0.00012853322840759764 c 1.0011402 k 0.0099474015 m 5.0066953\n",
      "91 Train Loss 1.0439636 Test RE 0.00010249570743546518 c 1.0011233 k 0.009961572 m 4.985831\n",
      "92 Train Loss 1.0001754 Test RE 9.413568408043576e-05 c 1.001003 k 0.009965157 m 4.9950247\n",
      "93 Train Loss 0.9823768 Test RE 9.90899669895572e-05 c 1.0011021 k 0.009961718 m 4.9958606\n",
      "94 Train Loss 0.9460184 Test RE 0.00010015128439188114 c 0.99952906 k 0.009997678 m 5.009456\n",
      "95 Train Loss 0.8448202 Test RE 0.00011546038758953173 c 0.999127 k 0.010018238 m 5.035426\n",
      "96 Train Loss 0.7377062 Test RE 0.00012187730391206711 c 1.0028511 k 0.009915542 m 4.9932985\n",
      "97 Train Loss 0.61481035 Test RE 0.00011579078731759739 c 0.99855626 k 0.010039141 m 5.016912\n",
      "98 Train Loss 0.48525545 Test RE 0.00011371225565146045 c 0.9986994 k 0.010047629 m 5.037448\n",
      "99 Train Loss 0.40291995 Test RE 0.0001137880110243702 c 0.9993047 k 0.010027956 m 5.034961\n",
      "100 Train Loss 0.3734239 Test RE 0.00010733930578418768 c 0.9984527 k 0.01004937 m 5.033927\n",
      "101 Train Loss 0.36779466 Test RE 0.00011038102951860079 c 0.9985767 k 0.010043565 m 5.032759\n",
      "102 Train Loss 0.3612697 Test RE 0.00011340923414958108 c 0.99889594 k 0.010034283 m 5.0270033\n",
      "103 Train Loss 0.34860584 Test RE 0.00010542219279734992 c 0.99967355 k 0.010012966 m 5.0180655\n",
      "104 Train Loss 0.33940178 Test RE 9.223245885943892e-05 c 1.0003753 k 0.009991526 m 5.014757\n",
      "105 Train Loss 0.33716598 Test RE 8.773805628729058e-05 c 0.99982965 k 0.010003764 m 5.0138564\n",
      "106 Train Loss 0.33593196 Test RE 8.654369776236643e-05 c 0.9995035 k 0.010010905 m 5.0102353\n",
      "107 Train Loss 0.32789034 Test RE 9.07879817531511e-05 c 0.9992843 k 0.010014888 m 5.0039697\n",
      "108 Train Loss 0.31407276 Test RE 8.278322021231878e-05 c 0.99936986 k 0.010011104 m 5.000487\n",
      "109 Train Loss 0.30250528 Test RE 6.813021363674583e-05 c 1.0002722 k 0.009982949 m 4.9980884\n",
      "110 Train Loss 0.2946934 Test RE 7.223473238652412e-05 c 1.0002503 k 0.009983109 m 5.0072756\n",
      "111 Train Loss 0.29192376 Test RE 7.08391630336535e-05 c 0.9997758 k 0.009996599 m 5.0084\n",
      "112 Train Loss 0.28886715 Test RE 6.285526613463078e-05 c 0.9997513 k 0.009991829 m 5.0047936\n",
      "113 Train Loss 0.2811653 Test RE 5.865171550015408e-05 c 1.0004447 k 0.009974434 m 5.004243\n",
      "114 Train Loss 0.2759686 Test RE 5.509614035771676e-05 c 1.000631 k 0.009969057 m 5.000832\n",
      "115 Train Loss 0.26204672 Test RE 5.8744873165970624e-05 c 1.000633 k 0.00996827 m 5.002295\n",
      "116 Train Loss 0.22598591 Test RE 5.834666843088634e-05 c 0.99959373 k 0.010003564 m 5.032273\n",
      "117 Train Loss 0.19302146 Test RE 5.446310228465464e-05 c 0.9988128 k 0.010016543 m 5.0209475\n",
      "118 Train Loss 0.17044108 Test RE 5.2642387930069214e-05 c 1.0009562 k 0.009957663 m 5.007076\n",
      "119 Train Loss 0.15927964 Test RE 6.11501469879687e-05 c 1.0007577 k 0.009966447 m 5.0057197\n",
      "120 Train Loss 0.14621371 Test RE 5.1407997447626404e-05 c 1.0001415 k 0.009985655 m 5.0065465\n",
      "121 Train Loss 0.14320438 Test RE 4.6515128304682475e-05 c 1.0005683 k 0.009973853 m 5.0057135\n",
      "122 Train Loss 0.14102392 Test RE 4.8585052910505136e-05 c 1.0003917 k 0.0099766 m 5.004963\n",
      "123 Train Loss 0.14015687 Test RE 4.932530029809743e-05 c 1.0004381 k 0.009976989 m 5.003229\n",
      "124 Train Loss 0.13833146 Test RE 5.113446006779655e-05 c 1.0008541 k 0.009963718 m 4.995903\n",
      "125 Train Loss 0.12908992 Test RE 6.290338425702616e-05 c 1.0012265 k 0.009951833 m 4.9823165\n",
      "126 Train Loss 0.11195321 Test RE 6.643759653099004e-05 c 1.0008746 k 0.009971779 m 4.9811745\n",
      "127 Train Loss 0.09754677 Test RE 5.722485610665633e-05 c 1.0004462 k 0.009983005 m 4.9855466\n",
      "128 Train Loss 0.091329396 Test RE 5.7654950956584014e-05 c 1.0007818 k 0.009975016 m 4.985873\n",
      "129 Train Loss 0.08529826 Test RE 5.481593901277725e-05 c 1.0008279 k 0.009972085 m 4.984076\n",
      "130 Train Loss 0.080068566 Test RE 5.252122646305697e-05 c 1.0005503 k 0.009979703 m 4.985972\n",
      "131 Train Loss 0.07870151 Test RE 5.1231493222326484e-05 c 1.0006206 k 0.009978153 m 4.990355\n",
      "132 Train Loss 0.078060105 Test RE 5.029488381863165e-05 c 1.0003774 k 0.009985965 m 4.9946504\n",
      "133 Train Loss 0.07721466 Test RE 4.9337709239678955e-05 c 1.0003631 k 0.009986021 m 4.9948373\n",
      "134 Train Loss 0.07689366 Test RE 4.633027979641118e-05 c 1.0004454 k 0.0099827945 m 4.9944596\n",
      "135 Train Loss 0.076629095 Test RE 4.514465119604018e-05 c 1.0002918 k 0.009988188 m 4.9969263\n",
      "136 Train Loss 0.07559814 Test RE 4.272537502215925e-05 c 1.0001825 k 0.009991011 m 4.9997067\n",
      "137 Train Loss 0.074735135 Test RE 4.0415527564921726e-05 c 1.0002718 k 0.009988521 m 5.0010004\n",
      "138 Train Loss 0.07312123 Test RE 4.3414884860023315e-05 c 1.0002378 k 0.009990565 m 5.0007257\n",
      "139 Train Loss 0.071606345 Test RE 4.57113759097874e-05 c 1.0002718 k 0.0099882055 m 4.99772\n",
      "140 Train Loss 0.069893755 Test RE 4.550324846817247e-05 c 1.0002906 k 0.009989018 m 4.997037\n",
      "141 Train Loss 0.06520461 Test RE 3.92852131653404e-05 c 1.0001752 k 0.009990304 m 5.002045\n",
      "142 Train Loss 0.059592903 Test RE 3.796620552252536e-05 c 0.99988174 k 0.00999767 m 5.008181\n",
      "143 Train Loss 0.053888977 Test RE 3.742083944759124e-05 c 1.0001171 k 0.009990325 m 4.997439\n",
      "144 Train Loss 0.048758768 Test RE 3.0849270269817555e-05 c 1.0008038 k 0.009971368 m 4.994583\n",
      "145 Train Loss 0.042760067 Test RE 2.958371508468516e-05 c 1.0006189 k 0.009973692 m 5.002373\n",
      "146 Train Loss 0.03862152 Test RE 3.2805884458101974e-05 c 1.0004554 k 0.009978394 m 4.9997563\n",
      "147 Train Loss 0.037923336 Test RE 3.189381130682682e-05 c 1.0004871 k 0.0099771 m 4.997953\n",
      "148 Train Loss 0.037573904 Test RE 3.1771644177339466e-05 c 1.000575 k 0.009974339 m 4.99759\n",
      "149 Train Loss 0.037374213 Test RE 3.149622853826891e-05 c 1.000592 k 0.009973852 m 4.996743\n",
      "150 Train Loss 0.036178403 Test RE 3.134867215267427e-05 c 1.0006227 k 0.009973623 m 4.995927\n",
      "151 Train Loss 0.035161704 Test RE 3.23567683095785e-05 c 1.000515 k 0.009977049 m 4.997798\n",
      "152 Train Loss 0.0341305 Test RE 3.1808626467196204e-05 c 1.0006007 k 0.009974148 m 4.996817\n",
      "153 Train Loss 0.033171095 Test RE 3.287980041557084e-05 c 1.0004779 k 0.0099792695 m 4.996975\n",
      "154 Train Loss 0.0324261 Test RE 3.16538740702367e-05 c 1.0002983 k 0.009983387 m 4.9992323\n",
      "155 Train Loss 0.03155754 Test RE 2.974634121587843e-05 c 1.0003364 k 0.009981909 m 4.9999733\n",
      "156 Train Loss 0.029887022 Test RE 2.7467564213149037e-05 c 1.0004332 k 0.009978487 m 4.9980645\n",
      "157 Train Loss 0.028817201 Test RE 2.586548286298098e-05 c 1.0005064 k 0.009975511 m 4.998065\n",
      "158 Train Loss 0.027777115 Test RE 2.695543493421393e-05 c 1.0005777 k 0.009973987 m 4.9982386\n",
      "159 Train Loss 0.027343404 Test RE 2.780169751794363e-05 c 1.0006729 k 0.009972031 m 4.99704\n",
      "160 Train Loss 0.027078612 Test RE 2.662135957670617e-05 c 1.0006642 k 0.009971054 m 4.996714\n",
      "161 Train Loss 0.027045863 Test RE 2.6675158810867097e-05 c 1.000639 k 0.009971578 m 4.9964523\n",
      "162 Train Loss 0.027036518 Test RE 2.6685482901825918e-05 c 1.0006353 k 0.0099716745 m 4.996397\n",
      "163 Train Loss 0.027036518 Test RE 2.6685482901825918e-05 c 1.0006353 k 0.0099716745 m 4.996397\n",
      "164 Train Loss 0.027036518 Test RE 2.6685482901825918e-05 c 1.0006353 k 0.0099716745 m 4.996397\n",
      "165 Train Loss 0.027036518 Test RE 2.6685482901825918e-05 c 1.0006353 k 0.0099716745 m 4.996397\n",
      "166 Train Loss 0.027036518 Test RE 2.6685482901825918e-05 c 1.0006353 k 0.0099716745 m 4.996397\n",
      "167 Train Loss 0.027036518 Test RE 2.6685482901825918e-05 c 1.0006353 k 0.0099716745 m 4.996397\n",
      "168 Train Loss 0.027036518 Test RE 2.6685482901825918e-05 c 1.0006353 k 0.0099716745 m 4.996397\n",
      "169 Train Loss 0.027036518 Test RE 2.6685482901825918e-05 c 1.0006353 k 0.0099716745 m 4.996397\n",
      "170 Train Loss 0.027036518 Test RE 2.6685482901825918e-05 c 1.0006353 k 0.0099716745 m 4.996397\n",
      "171 Train Loss 0.027036518 Test RE 2.6685482901825918e-05 c 1.0006353 k 0.0099716745 m 4.996397\n",
      "172 Train Loss 0.027036518 Test RE 2.6685482901825918e-05 c 1.0006353 k 0.0099716745 m 4.996397\n",
      "173 Train Loss 0.027036518 Test RE 2.6685482901825918e-05 c 1.0006353 k 0.0099716745 m 4.996397\n",
      "174 Train Loss 0.027036518 Test RE 2.6685482901825918e-05 c 1.0006353 k 0.0099716745 m 4.996397\n",
      "175 Train Loss 0.027036518 Test RE 2.6685482901825918e-05 c 1.0006353 k 0.0099716745 m 4.996397\n",
      "176 Train Loss 0.027036518 Test RE 2.6685482901825918e-05 c 1.0006353 k 0.0099716745 m 4.996397\n",
      "177 Train Loss 0.027036518 Test RE 2.6685482901825918e-05 c 1.0006353 k 0.0099716745 m 4.996397\n",
      "178 Train Loss 0.027036518 Test RE 2.6685482901825918e-05 c 1.0006353 k 0.0099716745 m 4.996397\n",
      "179 Train Loss 0.027036518 Test RE 2.6685482901825918e-05 c 1.0006353 k 0.0099716745 m 4.996397\n",
      "180 Train Loss 0.027036518 Test RE 2.6685482901825918e-05 c 1.0006353 k 0.0099716745 m 4.996397\n",
      "181 Train Loss 0.027036518 Test RE 2.6685482901825918e-05 c 1.0006353 k 0.0099716745 m 4.996397\n",
      "182 Train Loss 0.027036518 Test RE 2.6685482901825918e-05 c 1.0006353 k 0.0099716745 m 4.996397\n",
      "183 Train Loss 0.027036518 Test RE 2.6685482901825918e-05 c 1.0006353 k 0.0099716745 m 4.996397\n",
      "184 Train Loss 0.027036518 Test RE 2.6685482901825918e-05 c 1.0006353 k 0.0099716745 m 4.996397\n",
      "185 Train Loss 0.027036518 Test RE 2.6685482901825918e-05 c 1.0006353 k 0.0099716745 m 4.996397\n",
      "186 Train Loss 0.027036518 Test RE 2.6685482901825918e-05 c 1.0006353 k 0.0099716745 m 4.996397\n",
      "187 Train Loss 0.027036518 Test RE 2.6685482901825918e-05 c 1.0006353 k 0.0099716745 m 4.996397\n",
      "188 Train Loss 0.027036518 Test RE 2.6685482901825918e-05 c 1.0006353 k 0.0099716745 m 4.996397\n",
      "189 Train Loss 0.027036518 Test RE 2.6685482901825918e-05 c 1.0006353 k 0.0099716745 m 4.996397\n",
      "190 Train Loss 0.027036518 Test RE 2.6685482901825918e-05 c 1.0006353 k 0.0099716745 m 4.996397\n",
      "191 Train Loss 0.027036518 Test RE 2.6685482901825918e-05 c 1.0006353 k 0.0099716745 m 4.996397\n",
      "192 Train Loss 0.027036518 Test RE 2.6685482901825918e-05 c 1.0006353 k 0.0099716745 m 4.996397\n",
      "193 Train Loss 0.027036518 Test RE 2.6685482901825918e-05 c 1.0006353 k 0.0099716745 m 4.996397\n",
      "194 Train Loss 0.027036518 Test RE 2.6685482901825918e-05 c 1.0006353 k 0.0099716745 m 4.996397\n",
      "195 Train Loss 0.027036518 Test RE 2.6685482901825918e-05 c 1.0006353 k 0.0099716745 m 4.996397\n",
      "196 Train Loss 0.027036518 Test RE 2.6685482901825918e-05 c 1.0006353 k 0.0099716745 m 4.996397\n",
      "197 Train Loss 0.027036518 Test RE 2.6685482901825918e-05 c 1.0006353 k 0.0099716745 m 4.996397\n",
      "198 Train Loss 0.027036518 Test RE 2.6685482901825918e-05 c 1.0006353 k 0.0099716745 m 4.996397\n",
      "199 Train Loss 0.027036518 Test RE 2.6685482901825918e-05 c 1.0006353 k 0.0099716745 m 4.996397\n",
      "Training time: 49.32\n",
      "Training time: 49.32\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 603792.44 Test RE 0.29436615399171323 c 0.009092499 k 0.012876965 m 0.00022985596\n",
      "1 Train Loss 8409.274 Test RE 0.027857295546672142 c 0.01891019 k 0.039437857 m 0.00035131353\n",
      "2 Train Loss 5776.535 Test RE 0.020873736478311315 c 0.03071035 k 0.02700169 m 0.0010791398\n",
      "3 Train Loss 3614.84 Test RE 0.012776508595362012 c 0.106702246 k 0.033483103 m 0.005643799\n",
      "4 Train Loss 1935.7291 Test RE 0.00715531072769723 c 0.33212018 k 0.024578178 m 0.01953151\n",
      "5 Train Loss 984.0243 Test RE 0.007018174164188203 c 0.7489012 k 0.01339993 m 0.04620229\n",
      "6 Train Loss 848.156 Test RE 0.007526934808398463 c 0.88968694 k 0.012106107 m 0.05568497\n",
      "7 Train Loss 610.0825 Test RE 0.004664417047140435 c 0.97832155 k 0.009925658 m 0.06540358\n",
      "8 Train Loss 548.056 Test RE 0.0035782335017607134 c 1.0432526 k 0.007877307 m 0.074504085\n",
      "9 Train Loss 521.7196 Test RE 0.0036320357778595164 c 1.1396122 k 0.005900959 m 0.08954986\n",
      "10 Train Loss 430.28595 Test RE 0.0037944296476968994 c 1.1693581 k 0.0050902274 m 0.1259535\n",
      "11 Train Loss 365.74347 Test RE 0.00537006105354544 c 1.1642652 k 0.0046526412 m 0.16347897\n",
      "12 Train Loss 313.25653 Test RE 0.004909689803718742 c 1.1817172 k 0.0047009583 m 0.15680146\n",
      "13 Train Loss 298.4422 Test RE 0.004603437509467464 c 1.2239332 k 0.0036555966 m 0.16345732\n",
      "14 Train Loss 267.75104 Test RE 0.0035805938866266497 c 1.196342 k 0.0039437898 m 0.17926842\n",
      "15 Train Loss 253.17699 Test RE 0.0029901519361149034 c 1.1640803 k 0.00519201 m 0.19409877\n",
      "16 Train Loss 247.61629 Test RE 0.002550864227534192 c 1.1592989 k 0.005355071 m 0.23294137\n",
      "17 Train Loss 229.16771 Test RE 0.0024078969879915966 c 1.1962554 k 0.004477926 m 0.3369929\n",
      "18 Train Loss 224.57587 Test RE 0.0024140354853347586 c 1.1705669 k 0.005030505 m 0.35080695\n",
      "19 Train Loss 219.98723 Test RE 0.00237083402663436 c 1.1685184 k 0.0052185464 m 0.3892561\n",
      "20 Train Loss 216.69879 Test RE 0.0023831296410236785 c 1.1702305 k 0.0051553356 m 0.4135854\n",
      "21 Train Loss 210.82999 Test RE 0.0024186978887709328 c 1.1579318 k 0.005378884 m 0.48047101\n",
      "22 Train Loss 206.94954 Test RE 0.0024997517655414325 c 1.1703439 k 0.0051182583 m 0.5282928\n",
      "23 Train Loss 206.36737 Test RE 0.0025075775106586347 c 1.1687793 k 0.005181315 m 0.5535531\n",
      "24 Train Loss 205.58337 Test RE 0.002495398031135329 c 1.1708392 k 0.005132764 m 0.5577855\n",
      "25 Train Loss 205.44174 Test RE 0.002519453584812364 c 1.1729264 k 0.0050919326 m 0.55127776\n",
      "26 Train Loss 203.53534 Test RE 0.0026330049954499785 c 1.1733052 k 0.0050602993 m 0.5991923\n",
      "27 Train Loss 189.2217 Test RE 0.00221123159542707 c 1.1670376 k 0.0052103037 m 0.984976\n",
      "28 Train Loss 179.49268 Test RE 0.0021518778950352143 c 1.1192901 k 0.0064851884 m 1.2227864\n",
      "29 Train Loss 164.80862 Test RE 0.0018523162065622663 c 1.1233971 k 0.006310735 m 1.5784401\n",
      "30 Train Loss 134.9249 Test RE 0.0014516945218893448 c 1.1281526 k 0.006455633 m 1.9126297\n",
      "31 Train Loss 122.04143 Test RE 0.0015822390652574945 c 1.1150789 k 0.006676225 m 2.1292622\n",
      "32 Train Loss 114.405685 Test RE 0.0017227252343645306 c 1.0984349 k 0.0071017365 m 2.2516534\n",
      "33 Train Loss 107.018234 Test RE 0.0016692576522264176 c 1.0994202 k 0.0071063675 m 2.363386\n",
      "34 Train Loss 105.701035 Test RE 0.0017620064963522772 c 1.0931487 k 0.007203835 m 2.4408445\n",
      "35 Train Loss 104.00164 Test RE 0.0016835723487650099 c 1.1013445 k 0.007050302 m 2.4404206\n",
      "36 Train Loss 100.688934 Test RE 0.001539501912714849 c 1.1141034 k 0.0067153196 m 2.3515477\n",
      "37 Train Loss 98.27715 Test RE 0.0015297919389281073 c 1.1001263 k 0.007050647 m 2.3187668\n",
      "38 Train Loss 84.39539 Test RE 0.001112592421972265 c 1.0962703 k 0.007266902 m 2.5363822\n",
      "39 Train Loss 46.085106 Test RE 0.0010157888976843777 c 1.0718428 k 0.007951536 m 3.35329\n",
      "40 Train Loss 24.233307 Test RE 0.0009865865370903202 c 1.0185541 k 0.009296479 m 4.370872\n",
      "41 Train Loss 21.155481 Test RE 0.000996083818142843 c 1.0101314 k 0.0096509 m 4.6685843\n",
      "42 Train Loss 16.165674 Test RE 0.0009973919200109021 c 1.0148455 k 0.009482849 m 4.676122\n",
      "43 Train Loss 14.239235 Test RE 0.0009568872139346789 c 1.0160313 k 0.009474367 m 4.51951\n",
      "44 Train Loss 13.615374 Test RE 0.0009694266494527797 c 1.0186799 k 0.009424474 m 4.5478163\n",
      "45 Train Loss 13.280727 Test RE 0.0009909643377185354 c 1.0131534 k 0.009564288 m 4.6825676\n",
      "46 Train Loss 12.603657 Test RE 0.000979352304157198 c 1.0007175 k 0.009927284 m 4.8090677\n",
      "47 Train Loss 8.003714 Test RE 0.0007283849212623397 c 0.9985354 k 0.0099303415 m 5.0000505\n",
      "48 Train Loss 6.757182 Test RE 0.0006701718863687861 c 1.0130408 k 0.009495177 m 4.8730755\n",
      "49 Train Loss 6.63429 Test RE 0.000666724814532702 c 1.0109779 k 0.009558032 m 4.870631\n",
      "50 Train Loss 6.570286 Test RE 0.0006635346972471855 c 1.0101353 k 0.009580671 m 4.8837514\n",
      "51 Train Loss 5.765562 Test RE 0.0006035044530162618 c 1.0139378 k 0.009485499 m 4.855305\n",
      "52 Train Loss 3.183568 Test RE 0.0004059393618469334 c 1.0081054 k 0.009681841 m 4.892501\n",
      "53 Train Loss 2.2617102 Test RE 0.0003107284657268232 c 1.0043638 k 0.009819334 m 4.9660473\n",
      "54 Train Loss 2.0159922 Test RE 0.00025539393721677085 c 1.0019586 k 0.0098777795 m 5.0284004\n",
      "55 Train Loss 1.9779372 Test RE 0.00023553437337600326 c 1.0022599 k 0.009871319 m 5.010523\n",
      "56 Train Loss 1.9546876 Test RE 0.00022725145396283877 c 1.0026516 k 0.0098702 m 4.995872\n",
      "57 Train Loss 1.9171063 Test RE 0.00022606997898158248 c 1.0036912 k 0.009835203 m 4.981152\n",
      "58 Train Loss 1.896759 Test RE 0.0002274808809301011 c 1.0030003 k 0.009858551 m 4.990797\n",
      "59 Train Loss 1.860326 Test RE 0.0002299264344350659 c 1.0016665 k 0.009907068 m 4.9984717\n",
      "60 Train Loss 1.5794809 Test RE 0.00019862159754255172 c 1.0001713 k 0.009959164 m 5.012842\n",
      "61 Train Loss 1.4881992 Test RE 0.00015736754342054156 c 1.0008758 k 0.009943213 m 5.016564\n",
      "62 Train Loss 1.4525495 Test RE 0.0001514413450379417 c 1.0009302 k 0.0099345 m 5.022899\n",
      "63 Train Loss 1.2864172 Test RE 0.00016083165559863303 c 0.99985534 k 0.009939657 m 5.0331955\n",
      "64 Train Loss 0.82081914 Test RE 0.00011042900124592927 c 1.0001887 k 0.009985185 m 5.018487\n",
      "65 Train Loss 0.67118436 Test RE 0.00011873335720228762 c 0.9999751 k 0.010013638 m 5.004079\n",
      "66 Train Loss 0.5214094 Test RE 9.001086254316107e-05 c 0.998922 k 0.010018178 m 4.9589906\n",
      "67 Train Loss 0.4285489 Test RE 8.269555465225759e-05 c 1.0017521 k 0.00995242 m 4.96752\n",
      "68 Train Loss 0.41488504 Test RE 8.119979632711271e-05 c 1.0010372 k 0.009969883 m 4.976926\n",
      "69 Train Loss 0.41049644 Test RE 8.029734466623204e-05 c 1.0009183 k 0.009969939 m 4.972083\n",
      "70 Train Loss 0.40899417 Test RE 8.095780981795997e-05 c 1.001221 k 0.009963823 m 4.9728975\n",
      "71 Train Loss 0.40272057 Test RE 8.697673087747482e-05 c 1.0008149 k 0.00998046 m 4.9763722\n",
      "72 Train Loss 0.32759893 Test RE 0.0001006152468086379 c 1.0000696 k 0.009993655 m 4.9571066\n",
      "73 Train Loss 0.25052157 Test RE 9.995675880210613e-05 c 1.001256 k 0.009972296 m 4.9793043\n",
      "74 Train Loss 0.2117172 Test RE 9.300194781848751e-05 c 0.998969 k 0.0100261085 m 5.0127664\n",
      "75 Train Loss 0.20468765 Test RE 9.147549258180477e-05 c 0.9998698 k 0.010002456 m 4.9999976\n",
      "76 Train Loss 0.20034647 Test RE 8.882772687120936e-05 c 1.0004127 k 0.009991949 m 5.0036\n",
      "77 Train Loss 0.18855946 Test RE 7.661904780552494e-05 c 0.99982834 k 0.0100054 m 5.0145497\n",
      "78 Train Loss 0.18255618 Test RE 6.915937215858918e-05 c 1.0000159 k 0.009996542 m 5.0025206\n",
      "79 Train Loss 0.17994797 Test RE 6.336994960630007e-05 c 1.0001538 k 0.009990598 m 5.006282\n",
      "80 Train Loss 0.17674993 Test RE 5.8060371711816475e-05 c 0.99984705 k 0.009997876 m 5.0108643\n",
      "81 Train Loss 0.167493 Test RE 5.493740071693365e-05 c 1.0007137 k 0.009974364 m 4.997568\n",
      "82 Train Loss 0.1554079 Test RE 5.5340060739953766e-05 c 1.0002805 k 0.009986167 m 4.9996176\n",
      "83 Train Loss 0.15322454 Test RE 5.56338216919501e-05 c 1.000431 k 0.009983054 m 5.0018177\n",
      "84 Train Loss 0.15005997 Test RE 5.096008787201331e-05 c 1.000348 k 0.009983081 m 4.998777\n",
      "85 Train Loss 0.14851363 Test RE 4.683535951077185e-05 c 1.0004473 k 0.00997982 m 4.9996605\n",
      "86 Train Loss 0.14693752 Test RE 4.51050275985607e-05 c 1.0004948 k 0.009977588 m 4.998758\n",
      "87 Train Loss 0.14422238 Test RE 4.1299528870901776e-05 c 1.0003635 k 0.009979785 m 4.996068\n",
      "88 Train Loss 0.14355248 Test RE 4.064538892924215e-05 c 1.0006078 k 0.009972693 m 4.9962144\n",
      "89 Train Loss 0.14338304 Test RE 4.003226130793349e-05 c 1.0006914 k 0.009971058 m 4.9967957\n",
      "90 Train Loss 0.14261618 Test RE 3.821682872041933e-05 c 1.0005988 k 0.009971517 m 4.997172\n",
      "91 Train Loss 0.14111221 Test RE 3.724554082915656e-05 c 1.0006936 k 0.009968674 m 4.9936075\n",
      "92 Train Loss 0.13778502 Test RE 3.7279718154025654e-05 c 1.0009024 k 0.009965373 m 4.9964833\n",
      "93 Train Loss 0.13446991 Test RE 3.564321339004497e-05 c 1.0005345 k 0.009971428 m 4.998481\n",
      "94 Train Loss 0.13082391 Test RE 3.631061647389109e-05 c 1.0005857 k 0.009972402 m 4.9946404\n",
      "95 Train Loss 0.12470031 Test RE 3.826341991635863e-05 c 1.0005522 k 0.009975673 m 4.999388\n",
      "96 Train Loss 0.12011441 Test RE 3.7467622743486544e-05 c 1.000373 k 0.009978467 m 4.998657\n",
      "97 Train Loss 0.11814058 Test RE 3.659403336722239e-05 c 1.0006386 k 0.009971925 m 4.998441\n",
      "98 Train Loss 0.1159395 Test RE 3.597532305992719e-05 c 1.0006511 k 0.009971728 m 4.998544\n",
      "99 Train Loss 0.11310572 Test RE 3.495300670506033e-05 c 1.0003489 k 0.009978649 m 4.995894\n",
      "100 Train Loss 0.10943309 Test RE 3.339440822826145e-05 c 1.0007063 k 0.00996961 m 4.99965\n",
      "101 Train Loss 0.1049711 Test RE 3.1911050101181965e-05 c 1.0007651 k 0.009966407 m 4.998618\n",
      "102 Train Loss 0.099893294 Test RE 3.0931667245291496e-05 c 1.000736 k 0.009966593 m 4.9948425\n",
      "103 Train Loss 0.09467414 Test RE 3.008577214821045e-05 c 1.0010067 k 0.009957076 m 4.9939284\n",
      "104 Train Loss 0.090969615 Test RE 3.122699365068773e-05 c 1.0009223 k 0.0099556595 m 4.992915\n",
      "105 Train Loss 0.08804411 Test RE 3.2915105895022066e-05 c 1.0010971 k 0.009952078 m 4.9926\n",
      "106 Train Loss 0.086257935 Test RE 3.344988351500345e-05 c 1.0009246 k 0.0099574085 m 4.994365\n",
      "107 Train Loss 0.08035076 Test RE 3.165807750775775e-05 c 1.000196 k 0.009975053 m 4.9954324\n",
      "108 Train Loss 0.07538506 Test RE 3.0813337174707064e-05 c 1.0009596 k 0.009955942 m 4.9952087\n",
      "109 Train Loss 0.07421539 Test RE 3.1763651316906266e-05 c 1.0009851 k 0.009955673 m 4.9946985\n",
      "110 Train Loss 0.07391428 Test RE 3.1494054408434816e-05 c 1.0008968 k 0.009957338 m 4.994304\n",
      "111 Train Loss 0.07355062 Test RE 3.1333421313605646e-05 c 1.0009769 k 0.0099560805 m 4.9960246\n",
      "112 Train Loss 0.073361374 Test RE 3.08491066951739e-05 c 1.0009186 k 0.009958506 m 4.996121\n",
      "113 Train Loss 0.07315806 Test RE 3.0520137460422025e-05 c 1.0008409 k 0.009960684 m 4.996628\n",
      "114 Train Loss 0.07315159 Test RE 3.054794384974063e-05 c 1.0008465 k 0.009960534 m 4.996669\n",
      "115 Train Loss 0.073141925 Test RE 3.056136569756792e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "116 Train Loss 0.07314091 Test RE 3.05582267012565e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "117 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "118 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "119 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "120 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "121 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "122 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "123 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "124 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "125 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "126 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "127 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "128 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "129 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "130 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "131 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "132 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "133 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "134 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "135 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "136 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "137 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "138 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "139 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "140 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "141 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "142 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "143 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "144 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "145 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "146 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "147 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "148 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "149 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "150 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "151 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "152 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "153 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "154 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "155 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "156 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "157 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "158 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "159 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "160 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "161 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "162 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "163 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "164 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "165 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "166 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "167 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "168 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "169 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "170 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "171 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "172 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "173 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "174 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "175 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "176 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "177 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "178 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "179 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "180 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "181 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "182 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "183 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "184 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "185 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "186 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "187 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "188 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "189 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "190 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "191 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "192 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "193 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "194 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "195 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "196 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "197 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "198 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "199 Train Loss 0.07314033 Test RE 3.055786192632027e-05 c 1.0008543 k 0.009960323 m 4.9967084\n",
      "Training time: 39.04\n",
      "Training time: 39.04\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 901990.25 Test RE 0.35550700051782635 c 0.0068248776 k -0.02963236 m -0.00021790226\n",
      "1 Train Loss 11512.955 Test RE 0.03275973084274824 c 0.0060590063 k 0.045049667 m -0.00026194792\n",
      "2 Train Loss 5095.9424 Test RE 0.018137676994768186 c 0.015514338 k 0.030194087 m 0.00030207174\n",
      "3 Train Loss 3625.4822 Test RE 0.013495288622287546 c 0.09042879 k 0.030993985 m 0.004938931\n",
      "4 Train Loss 1922.5603 Test RE 0.012729399734025535 c 0.5787454 k 0.017382791 m 0.03645587\n",
      "5 Train Loss 582.6335 Test RE 0.008202637919002461 c 1.2791643 k 0.0031213076 m 0.083321735\n",
      "6 Train Loss 436.9589 Test RE 0.006927707366564033 c 1.2072477 k 0.003771167 m 0.07943731\n",
      "7 Train Loss 353.4297 Test RE 0.0058793518350548 c 1.1472871 k 0.0056177843 m 0.07769826\n",
      "8 Train Loss 319.4364 Test RE 0.00526745369013066 c 1.1260219 k 0.006093408 m 0.07878025\n",
      "9 Train Loss 287.2954 Test RE 0.004546617445700569 c 1.1382611 k 0.005615678 m 0.08336904\n",
      "10 Train Loss 280.13477 Test RE 0.004419939789023884 c 1.1509452 k 0.0055297622 m 0.08554951\n",
      "11 Train Loss 269.50473 Test RE 0.004189921372705896 c 1.1406883 k 0.0058211433 m 0.094967574\n",
      "12 Train Loss 253.2016 Test RE 0.0038761605453155033 c 1.1803256 k 0.004621424 m 0.12191171\n",
      "13 Train Loss 247.15149 Test RE 0.003743083272969047 c 1.2082555 k 0.0040139193 m 0.12912028\n",
      "14 Train Loss 242.319 Test RE 0.0035734609794726897 c 1.2054303 k 0.0041598123 m 0.13674152\n",
      "15 Train Loss 241.61267 Test RE 0.003523929545588972 c 1.2046931 k 0.004092135 m 0.14094259\n",
      "16 Train Loss 240.84937 Test RE 0.003493310844543647 c 1.2060435 k 0.0040353443 m 0.14581534\n",
      "17 Train Loss 228.41425 Test RE 0.0029782065413588277 c 1.190416 k 0.004497637 m 0.23138508\n",
      "18 Train Loss 223.39436 Test RE 0.002672198220578649 c 1.1728927 k 0.005025406 m 0.27270466\n",
      "19 Train Loss 223.05313 Test RE 0.002593329353219606 c 1.1735786 k 0.004954169 m 0.27421743\n",
      "20 Train Loss 222.77968 Test RE 0.0026077717552643676 c 1.1745293 k 0.004936184 m 0.28269657\n",
      "21 Train Loss 222.39479 Test RE 0.002585400278652749 c 1.1742355 k 0.004950869 m 0.29707715\n",
      "22 Train Loss 221.38577 Test RE 0.0025676909923463874 c 1.1746435 k 0.0049471264 m 0.30402115\n",
      "23 Train Loss 220.7667 Test RE 0.0026679549175155836 c 1.1757752 k 0.004922651 m 0.30883887\n",
      "24 Train Loss 220.27042 Test RE 0.002654124633340028 c 1.1760157 k 0.0049294597 m 0.32618323\n",
      "25 Train Loss 220.20557 Test RE 0.002658015732908736 c 1.1773549 k 0.0048909076 m 0.3253004\n",
      "26 Train Loss 220.18193 Test RE 0.002673390000876392 c 1.1781766 k 0.004873468 m 0.32421446\n",
      "27 Train Loss 219.78012 Test RE 0.002728359827187816 c 1.1801292 k 0.0048345407 m 0.31480962\n",
      "28 Train Loss 218.63745 Test RE 0.002714737323849744 c 1.1780977 k 0.0048817075 m 0.30723754\n",
      "29 Train Loss 218.18927 Test RE 0.0027234241800019247 c 1.1773319 k 0.004898379 m 0.2993392\n",
      "30 Train Loss 217.97073 Test RE 0.0027552236959939758 c 1.179714 k 0.004845437 m 0.29392377\n",
      "31 Train Loss 217.83235 Test RE 0.0027533360046391032 c 1.1792734 k 0.0048524137 m 0.29163572\n",
      "32 Train Loss 217.69484 Test RE 0.002758895466965237 c 1.1795808 k 0.0048504025 m 0.28869084\n",
      "33 Train Loss 217.30579 Test RE 0.002778908150802215 c 1.1794885 k 0.004865934 m 0.28814393\n",
      "34 Train Loss 216.25499 Test RE 0.0027442430301217826 c 1.1771984 k 0.0049446207 m 0.30621836\n",
      "35 Train Loss 216.08762 Test RE 0.0026881359670465907 c 1.1760612 k 0.004981053 m 0.31338024\n",
      "36 Train Loss 215.82074 Test RE 0.0026212818657055563 c 1.1747559 k 0.005024608 m 0.32128778\n",
      "37 Train Loss 215.39917 Test RE 0.0025846918702962076 c 1.1715361 k 0.0051128645 m 0.34189007\n",
      "38 Train Loss 214.93602 Test RE 0.0025237506525510964 c 1.1728859 k 0.0050824597 m 0.37265566\n",
      "39 Train Loss 214.8084 Test RE 0.0024917504338508433 c 1.1724024 k 0.005103306 m 0.389717\n",
      "40 Train Loss 214.59473 Test RE 0.002505974140385193 c 1.1699383 k 0.005161284 m 0.38481587\n",
      "41 Train Loss 214.2345 Test RE 0.0024578368124194266 c 1.1718289 k 0.0051047015 m 0.4008143\n",
      "42 Train Loss 213.14464 Test RE 0.0022771614555132367 c 1.1736345 k 0.0050863093 m 0.49321747\n",
      "43 Train Loss 211.46332 Test RE 0.0023274978984396447 c 1.1637062 k 0.005343048 m 0.50654715\n",
      "44 Train Loss 209.99739 Test RE 0.0024879412555320764 c 1.1685762 k 0.0052003143 m 0.51152396\n",
      "45 Train Loss 209.49582 Test RE 0.0024800964923740176 c 1.168865 k 0.005198218 m 0.5301843\n",
      "46 Train Loss 209.22356 Test RE 0.0024887695727855087 c 1.1607718 k 0.0053887344 m 0.5370651\n",
      "47 Train Loss 207.78381 Test RE 0.002491178325504641 c 1.1648571 k 0.005277559 m 0.5444579\n",
      "48 Train Loss 206.55899 Test RE 0.002486131861834984 c 1.1715547 k 0.0051983683 m 0.57431644\n",
      "49 Train Loss 205.32024 Test RE 0.0024765186101191844 c 1.163581 k 0.005300285 m 0.645423\n",
      "50 Train Loss 204.825 Test RE 0.0025041483956144595 c 1.1624577 k 0.005386782 m 0.66816646\n",
      "51 Train Loss 204.62027 Test RE 0.002535842640928325 c 1.1645222 k 0.0053255917 m 0.68274236\n",
      "52 Train Loss 202.99742 Test RE 0.0025155532401500793 c 1.1679057 k 0.005233309 m 0.6942704\n",
      "53 Train Loss 201.6661 Test RE 0.0025767539564225273 c 1.1610849 k 0.0053918366 m 0.66031337\n",
      "54 Train Loss 200.96098 Test RE 0.002612989795093578 c 1.165735 k 0.0052864607 m 0.64881015\n",
      "55 Train Loss 200.09338 Test RE 0.0026498939166414307 c 1.1604204 k 0.0053974236 m 0.6603439\n",
      "56 Train Loss 196.6806 Test RE 0.0025946827870420588 c 1.1613511 k 0.0054711546 m 0.7777565\n",
      "57 Train Loss 188.81606 Test RE 0.0022974286002827382 c 1.1522393 k 0.005538569 m 0.9721687\n",
      "58 Train Loss 176.80814 Test RE 0.0023200310445173995 c 1.1443775 k 0.0059959604 m 1.1728051\n",
      "59 Train Loss 163.64647 Test RE 0.002195255877030565 c 1.1463369 k 0.005785198 m 1.2909104\n",
      "60 Train Loss 158.47438 Test RE 0.0020679153826816637 c 1.1462953 k 0.005765648 m 1.362201\n",
      "61 Train Loss 129.51215 Test RE 0.0016445838211782277 c 1.0972707 k 0.0068812408 m 1.8780022\n",
      "62 Train Loss 113.5143 Test RE 0.0014655108274083625 c 1.1065221 k 0.0068735676 m 2.223402\n",
      "63 Train Loss 108.91495 Test RE 0.0015252887140333965 c 1.0996522 k 0.0070252875 m 2.4056554\n",
      "64 Train Loss 105.43788 Test RE 0.0015294252868354556 c 1.1098368 k 0.00672801 m 2.5025709\n",
      "65 Train Loss 98.78677 Test RE 0.001469276454966476 c 1.0966016 k 0.0071306443 m 2.7176788\n",
      "66 Train Loss 94.226036 Test RE 0.0013999692378187875 c 1.0806217 k 0.0074897422 m 2.9562042\n",
      "67 Train Loss 89.80209 Test RE 0.0014400054128396985 c 1.080172 k 0.007399502 m 3.047393\n",
      "68 Train Loss 88.70831 Test RE 0.0015568227440212645 c 1.0810584 k 0.0074183433 m 3.089851\n",
      "69 Train Loss 87.98735 Test RE 0.0016063161857243827 c 1.077333 k 0.007514815 m 3.15929\n",
      "70 Train Loss 82.0625 Test RE 0.001455798221643711 c 1.0780504 k 0.007547683 m 3.1143384\n",
      "71 Train Loss 69.72256 Test RE 0.001439398872116351 c 1.0788074 k 0.0074571543 m 3.294878\n",
      "72 Train Loss 62.899174 Test RE 0.0014050496158700655 c 1.0685643 k 0.007808362 m 3.5288508\n",
      "73 Train Loss 54.73515 Test RE 0.001314260354387612 c 1.0600295 k 0.007954772 m 3.554636\n",
      "74 Train Loss 36.092484 Test RE 0.0011045087923329045 c 1.0515894 k 0.00830501 m 3.7590308\n",
      "75 Train Loss 20.198431 Test RE 0.0007490703663308633 c 1.0392513 k 0.00876616 m 4.0461926\n",
      "76 Train Loss 11.960062 Test RE 0.0005913580060185829 c 1.0251387 k 0.009207213 m 4.3893266\n",
      "77 Train Loss 10.983534 Test RE 0.0005584116685603173 c 1.0215915 k 0.009327281 m 4.50612\n",
      "78 Train Loss 10.46887 Test RE 0.0005273253202787362 c 1.0138607 k 0.009506032 m 4.6073065\n",
      "79 Train Loss 9.966076 Test RE 0.0004889841231427118 c 1.0181084 k 0.009394016 m 4.6190376\n",
      "80 Train Loss 8.923519 Test RE 0.0005104028048065394 c 1.0215493 k 0.009353145 m 4.618487\n",
      "81 Train Loss 8.659345 Test RE 0.000535707847966687 c 1.0134556 k 0.009547098 m 4.6514273\n",
      "82 Train Loss 8.261883 Test RE 0.0005200609930081871 c 1.004822 k 0.0097699445 m 4.7152424\n",
      "83 Train Loss 7.6640654 Test RE 0.0004623956885623647 c 1.0114008 k 0.009604264 m 4.7627826\n",
      "84 Train Loss 7.400366 Test RE 0.0004510672230424818 c 1.0108868 k 0.009628243 m 4.846707\n",
      "85 Train Loss 7.250342 Test RE 0.00045434953467281866 c 1.0068284 k 0.0097317165 m 4.8923535\n",
      "86 Train Loss 7.0743527 Test RE 0.0004763334644692726 c 1.0067933 k 0.0097414395 m 4.8830886\n",
      "87 Train Loss 6.5738163 Test RE 0.00043808904656347314 c 1.0084028 k 0.009682515 m 4.846034\n",
      "88 Train Loss 6.1782913 Test RE 0.0003879885574674494 c 1.0095872 k 0.009619706 m 4.8019276\n",
      "89 Train Loss 5.447103 Test RE 0.00039008227259072894 c 1.0141021 k 0.009523664 m 4.804456\n",
      "90 Train Loss 5.028622 Test RE 0.00038951770871125887 c 1.0120884 k 0.009562171 m 4.76997\n",
      "91 Train Loss 4.8139343 Test RE 0.0003651507957755772 c 1.0117576 k 0.009548001 m 4.739796\n",
      "92 Train Loss 3.8174362 Test RE 0.0003422921920354917 c 1.0101366 k 0.009614223 m 4.871634\n",
      "93 Train Loss 3.309973 Test RE 0.00035812198608020586 c 1.0043304 k 0.009773897 m 4.9945264\n",
      "94 Train Loss 3.172822 Test RE 0.0003666514777581396 c 1.0039334 k 0.009788666 m 5.003229\n",
      "95 Train Loss 3.0824268 Test RE 0.000369162663876541 c 1.0075685 k 0.009696585 m 4.947384\n",
      "96 Train Loss 3.0125356 Test RE 0.00036294048805445066 c 1.0073248 k 0.009687011 m 4.935868\n",
      "97 Train Loss 2.9597793 Test RE 0.00035489588500622145 c 1.0064185 k 0.009712238 m 4.954428\n",
      "98 Train Loss 2.947666 Test RE 0.0003564071951935293 c 1.0069321 k 0.0097012855 m 4.9590926\n",
      "99 Train Loss 2.910823 Test RE 0.0003590305996595159 c 1.0067632 k 0.009701323 m 4.962037\n",
      "100 Train Loss 2.6288414 Test RE 0.0003306261925451285 c 1.007162 k 0.009696248 m 4.93097\n",
      "101 Train Loss 2.538562 Test RE 0.0003206478728320424 c 1.0075405 k 0.009683184 m 4.930933\n",
      "102 Train Loss 2.5018435 Test RE 0.0003290158998664327 c 1.0072173 k 0.009695629 m 4.9351797\n",
      "103 Train Loss 2.4433608 Test RE 0.00033100801189980095 c 1.0067301 k 0.009721077 m 4.937466\n",
      "104 Train Loss 2.3471732 Test RE 0.0003146951695884876 c 1.0061568 k 0.009714023 m 4.9527254\n",
      "105 Train Loss 2.2035363 Test RE 0.00031449881383784496 c 1.0056129 k 0.00972853 m 4.939783\n",
      "106 Train Loss 2.081184 Test RE 0.00031247008802577734 c 1.0054071 k 0.00975243 m 4.9543123\n",
      "107 Train Loss 1.9855735 Test RE 0.00029329504416325097 c 1.0059464 k 0.0097412905 m 4.979326\n",
      "108 Train Loss 1.8958759 Test RE 0.0002668554365637273 c 1.0063277 k 0.009729305 m 4.964886\n",
      "109 Train Loss 1.8375506 Test RE 0.00026471074131234336 c 1.0055875 k 0.009763417 m 4.966734\n",
      "110 Train Loss 1.7499437 Test RE 0.00024268807642502824 c 1.0038013 k 0.009796914 m 4.976251\n",
      "111 Train Loss 1.6844956 Test RE 0.00023255121517517765 c 1.001593 k 0.009859427 m 4.982439\n",
      "112 Train Loss 1.6412833 Test RE 0.0002464267457922004 c 1.0033207 k 0.009835547 m 4.9742346\n",
      "113 Train Loss 1.6242397 Test RE 0.0002365954463801523 c 1.0048529 k 0.009793556 m 4.979039\n",
      "114 Train Loss 1.6056867 Test RE 0.00023563322419157105 c 1.0041288 k 0.009825071 m 5.0037327\n",
      "115 Train Loss 1.5816537 Test RE 0.00023879417684253876 c 1.0035222 k 0.009846067 m 5.007032\n",
      "116 Train Loss 1.5302918 Test RE 0.00022066068854833425 c 1.0036122 k 0.009850608 m 4.97313\n",
      "117 Train Loss 1.4648805 Test RE 0.00020362490776537872 c 1.0014051 k 0.009916416 m 4.960857\n",
      "118 Train Loss 1.4135178 Test RE 0.00021565104304931848 c 1.0022719 k 0.009898146 m 4.975668\n",
      "119 Train Loss 1.2925923 Test RE 0.00020703631210702532 c 1.0014772 k 0.0099465605 m 4.989629\n",
      "120 Train Loss 1.2262373 Test RE 0.00018748731888224284 c 1.0008147 k 0.009963344 m 4.9859567\n",
      "121 Train Loss 1.1999445 Test RE 0.0001932101354578812 c 1.000374 k 0.009974553 m 4.985186\n",
      "122 Train Loss 1.0956059 Test RE 0.00017783029353208112 c 1.0007703 k 0.009972987 m 4.9807515\n",
      "123 Train Loss 1.0844687 Test RE 0.000179461999856418 c 1.0004631 k 0.009975794 m 4.9875207\n",
      "124 Train Loss 1.078214 Test RE 0.00018125898632960561 c 1.0000623 k 0.009989996 m 4.99314\n",
      "125 Train Loss 1.0498792 Test RE 0.00017540431462299152 c 1.000585 k 0.009974025 m 4.997469\n",
      "126 Train Loss 1.0083051 Test RE 0.00017433571748790981 c 1.0025724 k 0.009915836 m 4.989003\n",
      "127 Train Loss 0.973073 Test RE 0.00017585032025493557 c 1.0017067 k 0.00993797 m 4.991407\n",
      "128 Train Loss 0.9537603 Test RE 0.00016624772721987686 c 1.0000194 k 0.009973127 m 5.0005875\n",
      "129 Train Loss 0.9430614 Test RE 0.00016775454066072788 c 1.0008022 k 0.009952265 m 5.0007524\n",
      "130 Train Loss 0.9367348 Test RE 0.00017104721554193927 c 1.0015259 k 0.009935458 m 4.994941\n",
      "131 Train Loss 0.93319666 Test RE 0.00017000124208270905 c 1.0016466 k 0.009928496 m 4.9921665\n",
      "132 Train Loss 0.9283932 Test RE 0.00016857837160216618 c 1.0016303 k 0.009927663 m 4.994226\n",
      "133 Train Loss 0.92339945 Test RE 0.00017014193640800686 c 1.0016264 k 0.009927173 m 4.9886336\n",
      "134 Train Loss 0.91671 Test RE 0.00016524990596855635 c 1.0017773 k 0.009919691 m 4.991556\n",
      "135 Train Loss 0.9094 Test RE 0.0001628858674305853 c 1.0020423 k 0.009910351 m 4.9898186\n",
      "136 Train Loss 0.89349705 Test RE 0.0001663925919183538 c 1.0022738 k 0.009901936 m 4.988155\n",
      "137 Train Loss 0.8673089 Test RE 0.00015705050578065994 c 1.0020511 k 0.009912323 m 4.999977\n",
      "138 Train Loss 0.81678987 Test RE 0.00015155132312043027 c 1.003225 k 0.009889288 m 4.98711\n",
      "139 Train Loss 0.71455693 Test RE 0.00014288864667855733 c 1.0041902 k 0.009857567 m 4.965893\n",
      "140 Train Loss 0.6508101 Test RE 0.00013839310404234744 c 1.0028517 k 0.009892782 m 4.988811\n",
      "141 Train Loss 0.63792294 Test RE 0.00014133627776417358 c 1.0018622 k 0.009920344 m 4.9895782\n",
      "142 Train Loss 0.6343408 Test RE 0.00014159526535782014 c 1.0018895 k 0.0099187 m 4.979844\n",
      "143 Train Loss 0.6230168 Test RE 0.00013372397533840973 c 1.0017012 k 0.009920493 m 4.9800673\n",
      "144 Train Loss 0.6088225 Test RE 0.00013610097302452264 c 1.002113 k 0.009909489 m 4.982392\n",
      "145 Train Loss 0.5954925 Test RE 0.00013658475621861941 c 1.0024334 k 0.009899556 m 4.9761496\n",
      "146 Train Loss 0.5722193 Test RE 0.000132865896445747 c 1.0025357 k 0.009901243 m 4.986732\n",
      "147 Train Loss 0.5571095 Test RE 0.00013269005281400773 c 1.0024124 k 0.009909825 m 4.987235\n",
      "148 Train Loss 0.54498214 Test RE 0.00013471337195615762 c 1.001184 k 0.009944236 m 4.9841924\n",
      "149 Train Loss 0.5340899 Test RE 0.00013468350752965326 c 1.0002662 k 0.009972185 m 4.99508\n",
      "150 Train Loss 0.5278188 Test RE 0.000137930068944994 c 1.0009053 k 0.009960588 m 4.992789\n",
      "151 Train Loss 0.5265849 Test RE 0.00013785801323337797 c 1.001165 k 0.0099549545 m 4.988501\n",
      "152 Train Loss 0.5244041 Test RE 0.00013654988781456183 c 1.0010359 k 0.009960696 m 4.992823\n",
      "153 Train Loss 0.52135766 Test RE 0.00013622820226358195 c 1.0008918 k 0.009967128 m 4.9965563\n",
      "154 Train Loss 0.5164422 Test RE 0.00013864693173301819 c 1.0003225 k 0.009982001 m 4.994309\n",
      "155 Train Loss 0.5048802 Test RE 0.00013788387086358892 c 1.0002983 k 0.0099899005 m 4.9933515\n",
      "156 Train Loss 0.4926234 Test RE 0.0001339194049748709 c 1.0001298 k 0.009990249 m 4.9986057\n",
      "157 Train Loss 0.47236103 Test RE 0.000133633076289565 c 0.9994131 k 0.010001519 m 4.9966\n",
      "158 Train Loss 0.45971692 Test RE 0.0001264376467847218 c 1.0005745 k 0.009975099 m 4.994286\n",
      "159 Train Loss 0.45635808 Test RE 0.00012504614097654443 c 1.0004836 k 0.009976707 m 4.9905195\n",
      "160 Train Loss 0.45260894 Test RE 0.00012466604059977005 c 1.0003469 k 0.009979321 m 4.9904\n",
      "161 Train Loss 0.44899967 Test RE 0.00012394583916003468 c 1.0007341 k 0.009969487 m 4.996079\n",
      "162 Train Loss 0.44169784 Test RE 0.00012794509902532933 c 1.0003 k 0.0099848565 m 5.0019665\n",
      "163 Train Loss 0.43479678 Test RE 0.00012493620734173704 c 1.0001034 k 0.009993483 m 5.006728\n",
      "164 Train Loss 0.42826572 Test RE 0.00012031683041547403 c 1.0007143 k 0.00997808 m 4.993038\n",
      "165 Train Loss 0.41904536 Test RE 0.00011671239913972323 c 1.0003654 k 0.009982448 m 4.9900293\n",
      "166 Train Loss 0.4097551 Test RE 0.00011513835179445657 c 1.0000558 k 0.009997091 m 5.0005484\n",
      "167 Train Loss 0.38572788 Test RE 0.00011153063658434227 c 1.0000128 k 0.010001151 m 4.99915\n",
      "168 Train Loss 0.36791146 Test RE 0.0001060323862894093 c 0.9991431 k 0.010025368 m 5.015461\n",
      "169 Train Loss 0.35118952 Test RE 0.00010641851787582447 c 0.99917144 k 0.010030237 m 5.0180144\n",
      "170 Train Loss 0.34562925 Test RE 0.00010492492006881431 c 0.9992545 k 0.010024835 m 5.0090528\n",
      "171 Train Loss 0.34356037 Test RE 0.00010345094061452432 c 0.9996279 k 0.010014286 m 5.006272\n",
      "172 Train Loss 0.34238318 Test RE 0.00010329111701081977 c 0.99961096 k 0.010015003 m 5.0040035\n",
      "173 Train Loss 0.34161493 Test RE 0.0001033562490875946 c 0.99944276 k 0.01002035 m 5.006901\n",
      "174 Train Loss 0.33830518 Test RE 0.00010438285647391256 c 0.9992799 k 0.01002499 m 5.009014\n",
      "175 Train Loss 0.32021224 Test RE 0.00010336004272640542 c 0.99941546 k 0.010021125 m 4.997729\n",
      "176 Train Loss 0.28512874 Test RE 9.193892074981031e-05 c 0.99951726 k 0.010016813 m 5.012831\n",
      "177 Train Loss 0.24134958 Test RE 7.186539891299482e-05 c 0.9992117 k 0.010021078 m 5.0202804\n",
      "178 Train Loss 0.19890924 Test RE 5.375539344022616e-05 c 1.0004212 k 0.0099889245 m 4.9933295\n",
      "179 Train Loss 0.18092121 Test RE 4.8946443638979206e-05 c 1.0003092 k 0.009987841 m 4.9896555\n",
      "180 Train Loss 0.17747337 Test RE 4.7073381273581786e-05 c 1.0000361 k 0.009993894 m 4.9939513\n",
      "181 Train Loss 0.16962652 Test RE 4.2522501197049755e-05 c 1.0003446 k 0.009985688 m 4.993254\n",
      "182 Train Loss 0.16682608 Test RE 4.1572730350748745e-05 c 1.000248 k 0.009990933 m 4.998573\n",
      "183 Train Loss 0.16455612 Test RE 4.119860689646123e-05 c 1.0002446 k 0.009987677 m 4.996581\n",
      "184 Train Loss 0.1628605 Test RE 3.975285945404238e-05 c 1.0003469 k 0.009984978 m 4.99246\n",
      "185 Train Loss 0.15810758 Test RE 3.8107951284173666e-05 c 1.0001963 k 0.009994728 m 5.001253\n",
      "186 Train Loss 0.15049268 Test RE 4.049782008367646e-05 c 1.0003121 k 0.009992655 m 5.00956\n",
      "187 Train Loss 0.14572082 Test RE 4.3471193031713285e-05 c 1.0004683 k 0.009989932 m 4.9995046\n",
      "188 Train Loss 0.1435831 Test RE 4.318073210261811e-05 c 0.9996861 k 0.010009634 m 5.001773\n",
      "189 Train Loss 0.14016157 Test RE 4.173476257479897e-05 c 0.99930286 k 0.01001901 m 5.003177\n",
      "190 Train Loss 0.13546506 Test RE 3.7486833438276286e-05 c 0.9996479 k 0.010011487 m 5.00341\n",
      "191 Train Loss 0.1316668 Test RE 3.8050543656580166e-05 c 0.99942213 k 0.0100209 m 5.0088415\n",
      "192 Train Loss 0.12561229 Test RE 3.579681498314139e-05 c 1.000107 k 0.009999689 m 4.9994664\n",
      "193 Train Loss 0.12119404 Test RE 3.1275665930271485e-05 c 1.000368 k 0.009993955 m 4.9956255\n",
      "194 Train Loss 0.11808655 Test RE 3.175767106042913e-05 c 1.0002527 k 0.009996029 m 5.00153\n",
      "195 Train Loss 0.11278428 Test RE 3.247585252360656e-05 c 0.99997157 k 0.01000386 m 5.000659\n",
      "196 Train Loss 0.109265916 Test RE 2.7777943788404665e-05 c 0.999559 k 0.010015765 m 5.00139\n",
      "197 Train Loss 0.10809 Test RE 2.8602857687487694e-05 c 0.9995172 k 0.010013992 m 5.004288\n",
      "198 Train Loss 0.10702733 Test RE 2.8363486898399728e-05 c 0.99933225 k 0.0100203175 m 5.006098\n",
      "199 Train Loss 0.104372725 Test RE 2.878651197296647e-05 c 0.99968725 k 0.01001039 m 5.001789\n",
      "Training time: 64.62\n",
      "Training time: 64.62\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 433842.56 Test RE 0.2500672172272982 c 0.006850499 k 0.038046755 m -2.5641577e-05\n",
      "1 Train Loss 12552.975 Test RE 0.037304348323329926 c 0.011404049 k 0.025502773 m 1.2687129e-05\n",
      "2 Train Loss 6114.392 Test RE 0.02240439974503704 c 0.019506749 k 0.03437425 m 0.0005701341\n",
      "3 Train Loss 4766.8555 Test RE 0.01811524729725153 c 0.057402484 k 0.030714612 m 0.003231808\n",
      "4 Train Loss 2169.5872 Test RE 0.008771307542651412 c 0.3084576 k 0.024447322 m 0.020901639\n",
      "5 Train Loss 1213.8198 Test RE 0.00575970043446493 c 0.53246146 k 0.020049224 m 0.036906924\n",
      "6 Train Loss 839.16125 Test RE 0.0067880831001286375 c 0.79612607 k 0.0141121615 m 0.05731012\n",
      "7 Train Loss 493.53818 Test RE 0.0055726482430349085 c 1.216487 k 0.0038837525 m 0.09416269\n",
      "8 Train Loss 381.92596 Test RE 0.004019957043738182 c 1.3270289 k 0.0013561356 m 0.10640262\n",
      "9 Train Loss 305.88513 Test RE 0.0038004818800171074 c 1.2044774 k 0.0041318918 m 0.104425296\n",
      "10 Train Loss 269.5349 Test RE 0.003428825989036655 c 1.1731175 k 0.004847199 m 0.1087196\n",
      "11 Train Loss 266.15628 Test RE 0.0034309248644170437 c 1.1851943 k 0.00473242 m 0.11280006\n",
      "12 Train Loss 261.5685 Test RE 0.0034111865256404358 c 1.1931959 k 0.0043625743 m 0.12074198\n",
      "13 Train Loss 253.36209 Test RE 0.003501038754815205 c 1.1922991 k 0.004349031 m 0.13965471\n",
      "14 Train Loss 250.59717 Test RE 0.003528362053800833 c 1.1909829 k 0.0045266524 m 0.14981474\n",
      "15 Train Loss 241.68579 Test RE 0.003056342896762055 c 1.182589 k 0.0046964237 m 0.17818521\n",
      "16 Train Loss 239.86414 Test RE 0.0029123147687362995 c 1.1791725 k 0.00472851 m 0.18407358\n",
      "17 Train Loss 236.75334 Test RE 0.0030234981598898573 c 1.1821729 k 0.0046226494 m 0.20008625\n",
      "18 Train Loss 234.85059 Test RE 0.0030379273477644784 c 1.1871154 k 0.0045778304 m 0.21291596\n",
      "19 Train Loss 232.99521 Test RE 0.0030216682066365446 c 1.1900681 k 0.0044789645 m 0.23357815\n",
      "20 Train Loss 230.4822 Test RE 0.0030355850009260244 c 1.184761 k 0.0046112165 m 0.27507544\n",
      "21 Train Loss 221.05186 Test RE 0.0029817241318331974 c 1.1787521 k 0.004736842 m 0.36048588\n",
      "22 Train Loss 219.58423 Test RE 0.0030617322029646817 c 1.1813105 k 0.004699037 m 0.385579\n",
      "23 Train Loss 216.87213 Test RE 0.0029652678840568067 c 1.1822001 k 0.0047247373 m 0.41490275\n",
      "24 Train Loss 213.93962 Test RE 0.00278946608614926 c 1.1705433 k 0.004997519 m 0.4311125\n",
      "25 Train Loss 212.37758 Test RE 0.002782383090978872 c 1.1633589 k 0.005195207 m 0.4538996\n",
      "26 Train Loss 209.8602 Test RE 0.0027636154411190654 c 1.179012 k 0.00482753 m 0.47109118\n",
      "27 Train Loss 208.62048 Test RE 0.002612141184096301 c 1.1806875 k 0.0047746995 m 0.47499022\n",
      "28 Train Loss 206.8533 Test RE 0.002577150428938899 c 1.1800057 k 0.0048192497 m 0.47243264\n",
      "29 Train Loss 206.02402 Test RE 0.002599728751564286 c 1.174912 k 0.004933301 m 0.47575122\n",
      "30 Train Loss 205.1102 Test RE 0.0025050767771030475 c 1.1723758 k 0.0049999887 m 0.49568778\n",
      "31 Train Loss 203.97612 Test RE 0.0025386186224606666 c 1.1754271 k 0.004940352 m 0.51737595\n",
      "32 Train Loss 203.78656 Test RE 0.002568175062270413 c 1.1741413 k 0.004969701 m 0.52600557\n",
      "33 Train Loss 201.71149 Test RE 0.002413632355475649 c 1.1585934 k 0.0053484524 m 0.62391484\n",
      "34 Train Loss 195.43092 Test RE 0.0021494975325756158 c 1.1647277 k 0.0052353684 m 0.686429\n",
      "35 Train Loss 193.42825 Test RE 0.002028480192059594 c 1.167225 k 0.0052375495 m 0.7499578\n",
      "36 Train Loss 191.51315 Test RE 0.002048723752903091 c 1.1607721 k 0.0053477013 m 0.8236122\n",
      "37 Train Loss 189.91516 Test RE 0.0021207407021259475 c 1.161491 k 0.0053358404 m 0.8830597\n",
      "38 Train Loss 188.5158 Test RE 0.002085526506686755 c 1.1572878 k 0.005467409 m 0.8963761\n",
      "39 Train Loss 186.05092 Test RE 0.0021672931234874232 c 1.1537145 k 0.0055648233 m 0.9140824\n",
      "40 Train Loss 184.0209 Test RE 0.0020994727327294147 c 1.1545856 k 0.0055554975 m 0.973759\n",
      "41 Train Loss 176.91264 Test RE 0.001981059603227645 c 1.149769 k 0.0056787427 m 1.0621922\n",
      "42 Train Loss 170.6809 Test RE 0.0021094111116033796 c 1.1458455 k 0.0057159024 m 1.0854306\n",
      "43 Train Loss 164.24 Test RE 0.0019993402342587084 c 1.1497314 k 0.00569703 m 1.1117321\n",
      "44 Train Loss 158.48628 Test RE 0.0020137725021976494 c 1.1480294 k 0.0056382404 m 1.1364568\n",
      "45 Train Loss 157.00401 Test RE 0.0019220106458499209 c 1.150994 k 0.0056763147 m 1.1411365\n",
      "46 Train Loss 155.48631 Test RE 0.001983446975359129 c 1.1544935 k 0.005564081 m 1.1698477\n",
      "47 Train Loss 153.0115 Test RE 0.0019144520045281776 c 1.1480855 k 0.005698792 m 1.2056125\n",
      "48 Train Loss 148.98804 Test RE 0.0018732440343446936 c 1.1471989 k 0.005767746 m 1.289066\n",
      "49 Train Loss 144.68643 Test RE 0.002025669224686071 c 1.150664 k 0.005658127 m 1.3594569\n",
      "50 Train Loss 137.95647 Test RE 0.0019260801526907688 c 1.1308953 k 0.00613344 m 1.4833871\n",
      "51 Train Loss 128.48804 Test RE 0.001663359161854598 c 1.1379005 k 0.0061241062 m 1.6620015\n",
      "52 Train Loss 117.70621 Test RE 0.0016388034937933663 c 1.1453242 k 0.005961049 m 1.8153281\n",
      "53 Train Loss 112.97102 Test RE 0.0014352909015070356 c 1.1237202 k 0.006484422 m 1.923259\n",
      "54 Train Loss 106.79492 Test RE 0.001617231191266493 c 1.1016449 k 0.0070378506 m 2.0956273\n",
      "55 Train Loss 101.82656 Test RE 0.0016561557840743255 c 1.1232493 k 0.006567223 m 2.2235916\n",
      "56 Train Loss 98.75663 Test RE 0.001633682407585854 c 1.09767 k 0.0071249106 m 2.3107033\n",
      "57 Train Loss 92.09775 Test RE 0.0016166224544687247 c 1.0860589 k 0.0074755377 m 2.440622\n",
      "58 Train Loss 88.31241 Test RE 0.0016052155906861158 c 1.1047267 k 0.007093415 m 2.509552\n",
      "59 Train Loss 84.905014 Test RE 0.0014397763964624857 c 1.0820806 k 0.0076358034 m 2.543939\n",
      "60 Train Loss 80.40436 Test RE 0.0012713318808992133 c 1.0873646 k 0.007568137 m 2.5854762\n",
      "61 Train Loss 76.333336 Test RE 0.0014388139238411337 c 1.0890007 k 0.0076360237 m 2.706376\n",
      "62 Train Loss 73.43063 Test RE 0.0012676752755924564 c 1.0792794 k 0.007919771 m 2.8076618\n",
      "63 Train Loss 69.45691 Test RE 0.0011611787137139286 c 1.0740292 k 0.007984295 m 2.8989172\n",
      "64 Train Loss 66.35834 Test RE 0.001209658381070237 c 1.0611244 k 0.0083053075 m 3.001709\n",
      "65 Train Loss 58.189167 Test RE 0.0011023958525765265 c 1.0607308 k 0.008303443 m 3.2281673\n",
      "66 Train Loss 53.32129 Test RE 0.0011612985805372626 c 1.0526935 k 0.008614419 m 3.348094\n",
      "67 Train Loss 49.977436 Test RE 0.0009923250777439348 c 1.0515221 k 0.008686102 m 3.4722316\n",
      "68 Train Loss 45.83462 Test RE 0.001107052190902478 c 1.0378747 k 0.00899505 m 3.640128\n",
      "69 Train Loss 33.288425 Test RE 0.000885767345465739 c 1.0161489 k 0.0094108535 m 4.046083\n",
      "70 Train Loss 28.715576 Test RE 0.0006241473752179965 c 1.0328728 k 0.009165792 m 4.075949\n",
      "71 Train Loss 26.24202 Test RE 0.0006733669812160792 c 1.0246062 k 0.009290679 m 4.121101\n",
      "72 Train Loss 18.948124 Test RE 0.0006247436252073935 c 1.0184894 k 0.009440187 m 4.339394\n",
      "73 Train Loss 11.794722 Test RE 0.0003731164160330517 c 1.0296626 k 0.009185349 m 4.42993\n",
      "74 Train Loss 9.427487 Test RE 0.0004970706666557176 c 1.0130093 k 0.009604229 m 4.485625\n",
      "75 Train Loss 6.248767 Test RE 0.0005832928486756737 c 1.0058213 k 0.009854041 m 4.715038\n",
      "76 Train Loss 4.066032 Test RE 0.0005346382917648142 c 1.0016778 k 0.009979291 m 4.853327\n",
      "77 Train Loss 2.9369721 Test RE 0.0004117798074714789 c 1.0021597 k 0.009945871 m 4.990367\n",
      "78 Train Loss 2.1348174 Test RE 0.00029142748268736406 c 0.9934464 k 0.010153861 m 5.084426\n",
      "79 Train Loss 1.6713473 Test RE 0.0001425550218498004 c 0.9962568 k 0.010059246 m 5.15136\n",
      "80 Train Loss 1.3787996 Test RE 0.00012789444749013667 c 0.99727434 k 0.0100547355 m 5.157229\n",
      "81 Train Loss 1.1959548 Test RE 0.00013851572308644627 c 0.9921344 k 0.010162149 m 5.1440444\n",
      "82 Train Loss 1.0192735 Test RE 0.0001260514907218506 c 0.99830836 k 0.010020132 m 5.1209035\n",
      "83 Train Loss 0.84405625 Test RE 9.128418885976522e-05 c 1.000588 k 0.009960446 m 5.0560455\n",
      "84 Train Loss 0.80296636 Test RE 8.08635057282413e-05 c 0.9988862 k 0.010000931 m 5.040037\n",
      "85 Train Loss 0.76682377 Test RE 7.164155376327323e-05 c 1.0008392 k 0.009952631 m 5.0242343\n",
      "86 Train Loss 0.730221 Test RE 6.778671125130955e-05 c 1.0018742 k 0.0099265585 m 4.9966717\n",
      "87 Train Loss 0.70229083 Test RE 6.245841601468504e-05 c 1.0010778 k 0.009943425 m 4.984145\n",
      "88 Train Loss 0.69041604 Test RE 6.101617699275162e-05 c 1.0021586 k 0.009912346 m 4.981007\n",
      "89 Train Loss 0.67786705 Test RE 5.947996885172607e-05 c 1.0016162 k 0.009929661 m 4.9745183\n",
      "90 Train Loss 0.6590266 Test RE 6.070333795241515e-05 c 1.0019186 k 0.009922611 m 4.9658275\n",
      "91 Train Loss 0.6462587 Test RE 6.0030941436658685e-05 c 1.0015688 k 0.009933469 m 4.9867454\n",
      "92 Train Loss 0.63481516 Test RE 5.779442021591848e-05 c 1.0011159 k 0.009945198 m 4.986085\n",
      "93 Train Loss 0.61875683 Test RE 5.386484367020082e-05 c 1.0018665 k 0.009923578 m 4.972299\n",
      "94 Train Loss 0.59422517 Test RE 5.87508894342061e-05 c 1.0018994 k 0.0099235065 m 4.9772677\n",
      "95 Train Loss 0.58615535 Test RE 7.064566450524053e-05 c 1.0023235 k 0.009913356 m 4.9744844\n",
      "96 Train Loss 0.56843084 Test RE 7.870605875943571e-05 c 1.0028118 k 0.009896931 m 4.9691844\n",
      "97 Train Loss 0.55508643 Test RE 7.278278198269088e-05 c 1.0019187 k 0.009920705 m 4.977037\n",
      "98 Train Loss 0.51628643 Test RE 7.586844047888447e-05 c 1.0022613 k 0.009915637 m 4.9929543\n",
      "99 Train Loss 0.4705609 Test RE 5.9083079893422434e-05 c 1.0018551 k 0.0099253105 m 4.9811077\n",
      "100 Train Loss 0.4460176 Test RE 5.425283133579028e-05 c 1.0009468 k 0.009947904 m 4.975991\n",
      "101 Train Loss 0.42336577 Test RE 5.409042952876468e-05 c 1.001536 k 0.009937604 m 4.9876413\n",
      "102 Train Loss 0.41973937 Test RE 5.433564635112294e-05 c 1.0016453 k 0.00993644 m 4.989642\n",
      "103 Train Loss 0.4147225 Test RE 5.310103240305092e-05 c 1.0009643 k 0.009951587 m 4.994721\n",
      "104 Train Loss 0.40637776 Test RE 5.39874222539883e-05 c 0.9999295 k 0.009979457 m 5.0027413\n",
      "105 Train Loss 0.38939986 Test RE 5.379773101786423e-05 c 1.0002365 k 0.0099677965 m 4.9930487\n",
      "106 Train Loss 0.37224218 Test RE 5.074723000266441e-05 c 1.0010047 k 0.009954387 m 4.9922705\n",
      "107 Train Loss 0.36249304 Test RE 5.0288736965369024e-05 c 1.0015873 k 0.00994119 m 4.9874096\n",
      "108 Train Loss 0.34603256 Test RE 4.944319186979635e-05 c 1.001037 k 0.00995299 m 4.993512\n",
      "109 Train Loss 0.33998686 Test RE 4.769652280131399e-05 c 1.000927 k 0.009956631 m 4.996327\n",
      "110 Train Loss 0.33427182 Test RE 4.611333586328842e-05 c 1.0009809 k 0.009954372 m 4.991604\n",
      "111 Train Loss 0.31712234 Test RE 4.78953600312805e-05 c 1.0012572 k 0.009942479 m 4.987978\n",
      "112 Train Loss 0.29988435 Test RE 5.27020966506323e-05 c 1.0012459 k 0.00994927 m 4.9882913\n",
      "113 Train Loss 0.2931985 Test RE 5.068947776271848e-05 c 1.0016772 k 0.0099372445 m 4.9811773\n",
      "114 Train Loss 0.28459254 Test RE 4.375449341861222e-05 c 1.00089 k 0.009957004 m 4.985815\n",
      "115 Train Loss 0.2582933 Test RE 3.734000778402778e-05 c 1.000054 k 0.009988389 m 4.9953065\n",
      "116 Train Loss 0.22006284 Test RE 3.546217347075266e-05 c 1.0015835 k 0.009946928 m 4.980564\n",
      "117 Train Loss 0.21012454 Test RE 3.6018282111601874e-05 c 1.001033 k 0.009962057 m 4.9859576\n",
      "118 Train Loss 0.20291355 Test RE 3.386446118962685e-05 c 1.000337 k 0.009982598 m 4.9913073\n",
      "119 Train Loss 0.19681211 Test RE 2.9879501470701278e-05 c 1.0003394 k 0.009984335 m 4.9929943\n",
      "120 Train Loss 0.19383715 Test RE 2.8589155352196734e-05 c 1.0002521 k 0.009988157 m 4.998485\n",
      "121 Train Loss 0.18929781 Test RE 3.172427920293562e-05 c 1.0003543 k 0.009986152 m 4.996206\n",
      "122 Train Loss 0.18627355 Test RE 2.985236544264858e-05 c 1.0003684 k 0.009984393 m 4.9934454\n",
      "123 Train Loss 0.18441974 Test RE 2.8561089894906714e-05 c 1.0004864 k 0.009979669 m 4.9945884\n",
      "124 Train Loss 0.18231186 Test RE 2.8492735568145925e-05 c 1.0008723 k 0.009969786 m 4.9929247\n",
      "125 Train Loss 0.18016213 Test RE 2.8985474619997032e-05 c 1.0006119 k 0.009974593 m 4.9962015\n",
      "126 Train Loss 0.17797434 Test RE 3.153047689854773e-05 c 1.0002995 k 0.009982999 m 5.0025315\n",
      "127 Train Loss 0.17142884 Test RE 3.3627468069004144e-05 c 1.0003798 k 0.009980504 m 5.0045366\n",
      "128 Train Loss 0.16244115 Test RE 2.7753938585650122e-05 c 1.000618 k 0.009974754 m 5.001354\n",
      "129 Train Loss 0.14587791 Test RE 3.0793296116678305e-05 c 0.99982184 k 0.009995267 m 5.0073237\n",
      "130 Train Loss 0.14183968 Test RE 3.280036786450229e-05 c 1.0002096 k 0.009983957 m 4.9970007\n",
      "131 Train Loss 0.13870479 Test RE 3.402027185737055e-05 c 1.0003779 k 0.00998187 m 4.9944987\n",
      "132 Train Loss 0.12867966 Test RE 2.8850967746892174e-05 c 1.0004485 k 0.0099818995 m 5.0040026\n",
      "133 Train Loss 0.124185145 Test RE 2.6759336220378208e-05 c 1.0005263 k 0.009980495 m 5.0005975\n",
      "134 Train Loss 0.11495651 Test RE 2.3545837008269622e-05 c 0.9997773 k 0.009998252 m 4.9933386\n",
      "135 Train Loss 0.09656981 Test RE 2.4211378333927655e-05 c 1.0001098 k 0.00998799 m 4.997725\n",
      "136 Train Loss 0.0831602 Test RE 2.378116601343285e-05 c 1.0011175 k 0.009965758 m 5.0037103\n",
      "137 Train Loss 0.06575531 Test RE 2.375241035666401e-05 c 0.99989027 k 0.010000541 m 5.0008802\n",
      "138 Train Loss 0.06162375 Test RE 2.2727993254914658e-05 c 0.9997782 k 0.00999755 m 4.99852\n",
      "139 Train Loss 0.059235655 Test RE 2.2332501109260826e-05 c 1.0003378 k 0.00998447 m 5.0003295\n",
      "140 Train Loss 0.05807305 Test RE 2.3187793020771368e-05 c 1.0002414 k 0.009988648 m 5.0008955\n",
      "141 Train Loss 0.0574697 Test RE 2.3469217584347876e-05 c 1.0004255 k 0.009983926 m 5.0001273\n",
      "142 Train Loss 0.05597412 Test RE 2.6476753093310093e-05 c 1.0003434 k 0.009987916 m 5.000769\n",
      "143 Train Loss 0.055554662 Test RE 2.7095737710676276e-05 c 1.0001327 k 0.0099924505 m 4.9991612\n",
      "144 Train Loss 0.055109818 Test RE 2.6645097733934464e-05 c 1.000298 k 0.009987001 m 4.998781\n",
      "145 Train Loss 0.05459352 Test RE 2.474941599660231e-05 c 1.0001572 k 0.009990878 m 5.0014877\n",
      "146 Train Loss 0.05441844 Test RE 2.4374277586003334e-05 c 1.0001193 k 0.009991921 m 5.000074\n",
      "147 Train Loss 0.053737212 Test RE 2.421751316609457e-05 c 1.0001183 k 0.009991342 m 5.000215\n",
      "148 Train Loss 0.05267411 Test RE 2.4500790047998135e-05 c 1.000078 k 0.0099958135 m 5.0046487\n",
      "149 Train Loss 0.050702628 Test RE 2.553381951848103e-05 c 0.999965 k 0.009999753 m 5.0061364\n",
      "150 Train Loss 0.049573272 Test RE 2.4055137823025463e-05 c 0.9999623 k 0.009997947 m 5.0036626\n",
      "151 Train Loss 0.048475064 Test RE 2.2416620194797655e-05 c 1.0000383 k 0.009995654 m 5.003372\n",
      "152 Train Loss 0.04788209 Test RE 2.2866749020800398e-05 c 1.0000039 k 0.009996216 m 5.004363\n",
      "153 Train Loss 0.047559194 Test RE 2.2532316979811774e-05 c 0.9998554 k 0.010000051 m 5.0054107\n",
      "154 Train Loss 0.046738524 Test RE 1.997391314936325e-05 c 1.0000496 k 0.009995625 m 5.0045686\n",
      "155 Train Loss 0.04245938 Test RE 1.9483853174482658e-05 c 1.0005008 k 0.009985106 m 5.001844\n",
      "156 Train Loss 0.030369783 Test RE 1.9566069766636247e-05 c 1.0000358 k 0.009999079 m 4.9990144\n",
      "157 Train Loss 0.023760863 Test RE 1.5758633450498873e-05 c 1.0002394 k 0.009992311 m 4.997541\n",
      "158 Train Loss 0.022632642 Test RE 1.610133710716748e-05 c 1.0002981 k 0.0099913 m 4.994367\n",
      "159 Train Loss 0.02177634 Test RE 1.7440969336494473e-05 c 1.0000304 k 0.009997647 m 4.9959846\n",
      "160 Train Loss 0.020621613 Test RE 1.5681049525413102e-05 c 1.0000962 k 0.00999646 m 4.9987783\n",
      "161 Train Loss 0.019956358 Test RE 1.4594287604917937e-05 c 1.000176 k 0.0099944305 m 4.9989223\n",
      "162 Train Loss 0.019573987 Test RE 1.4834921878280569e-05 c 1.000078 k 0.0099954065 m 5.000191\n",
      "163 Train Loss 0.018632669 Test RE 1.6427608966944217e-05 c 1.0000972 k 0.009995779 m 4.9998465\n",
      "164 Train Loss 0.017753307 Test RE 1.7436660771493095e-05 c 1.0001894 k 0.009994219 m 5.000375\n",
      "165 Train Loss 0.017063089 Test RE 1.8660204769187803e-05 c 1.0000411 k 0.009997534 m 4.999452\n",
      "166 Train Loss 0.016709844 Test RE 1.8499194901031997e-05 c 1.0001022 k 0.009995376 m 4.9992194\n",
      "167 Train Loss 0.01637649 Test RE 1.8648518943077192e-05 c 1.0000626 k 0.00999649 m 5.000352\n",
      "168 Train Loss 0.016287275 Test RE 1.917009729608557e-05 c 1.000044 k 0.009997346 m 5.000572\n",
      "169 Train Loss 0.016204696 Test RE 1.9186604693027e-05 c 1.0000943 k 0.009995928 m 5.0000105\n",
      "170 Train Loss 0.016097663 Test RE 1.9025840229009113e-05 c 1.0000776 k 0.009996861 m 5.0005016\n",
      "171 Train Loss 0.016063137 Test RE 1.887909037256133e-05 c 1.0000261 k 0.009997675 m 5.0010905\n",
      "172 Train Loss 0.016053503 Test RE 1.885351892220743e-05 c 1.0000252 k 0.009997703 m 5.001082\n",
      "173 Train Loss 0.016052565 Test RE 1.8816145891886002e-05 c 1.0000243 k 0.009997763 m 5.0010304\n",
      "174 Train Loss 0.016052362 Test RE 1.8824687026357914e-05 c 1.0000267 k 0.009997686 m 5.0010023\n",
      "175 Train Loss 0.016051415 Test RE 1.880679530866358e-05 c 1.0000292 k 0.009997597 m 5.0009747\n",
      "176 Train Loss 0.016051415 Test RE 1.880679530866358e-05 c 1.0000292 k 0.009997597 m 5.0009747\n",
      "177 Train Loss 0.016051415 Test RE 1.880679530866358e-05 c 1.0000292 k 0.009997597 m 5.0009747\n",
      "178 Train Loss 0.016051415 Test RE 1.880679530866358e-05 c 1.0000292 k 0.009997597 m 5.0009747\n",
      "179 Train Loss 0.016051415 Test RE 1.880679530866358e-05 c 1.0000292 k 0.009997597 m 5.0009747\n",
      "180 Train Loss 0.016051415 Test RE 1.880679530866358e-05 c 1.0000292 k 0.009997597 m 5.0009747\n",
      "181 Train Loss 0.016051415 Test RE 1.880679530866358e-05 c 1.0000292 k 0.009997597 m 5.0009747\n",
      "182 Train Loss 0.016051415 Test RE 1.880679530866358e-05 c 1.0000292 k 0.009997597 m 5.0009747\n",
      "183 Train Loss 0.016051415 Test RE 1.880679530866358e-05 c 1.0000292 k 0.009997597 m 5.0009747\n",
      "184 Train Loss 0.016051415 Test RE 1.880679530866358e-05 c 1.0000292 k 0.009997597 m 5.0009747\n",
      "185 Train Loss 0.016051415 Test RE 1.880679530866358e-05 c 1.0000292 k 0.009997597 m 5.0009747\n",
      "186 Train Loss 0.016051415 Test RE 1.880679530866358e-05 c 1.0000292 k 0.009997597 m 5.0009747\n",
      "187 Train Loss 0.016051415 Test RE 1.880679530866358e-05 c 1.0000292 k 0.009997597 m 5.0009747\n",
      "188 Train Loss 0.016051415 Test RE 1.880679530866358e-05 c 1.0000292 k 0.009997597 m 5.0009747\n",
      "189 Train Loss 0.016051415 Test RE 1.880679530866358e-05 c 1.0000292 k 0.009997597 m 5.0009747\n",
      "190 Train Loss 0.016051415 Test RE 1.880679530866358e-05 c 1.0000292 k 0.009997597 m 5.0009747\n",
      "191 Train Loss 0.016051415 Test RE 1.880679530866358e-05 c 1.0000292 k 0.009997597 m 5.0009747\n",
      "192 Train Loss 0.016051415 Test RE 1.880679530866358e-05 c 1.0000292 k 0.009997597 m 5.0009747\n",
      "193 Train Loss 0.016051415 Test RE 1.880679530866358e-05 c 1.0000292 k 0.009997597 m 5.0009747\n",
      "194 Train Loss 0.016051415 Test RE 1.880679530866358e-05 c 1.0000292 k 0.009997597 m 5.0009747\n",
      "195 Train Loss 0.016051415 Test RE 1.880679530866358e-05 c 1.0000292 k 0.009997597 m 5.0009747\n",
      "196 Train Loss 0.016051415 Test RE 1.880679530866358e-05 c 1.0000292 k 0.009997597 m 5.0009747\n",
      "197 Train Loss 0.016051415 Test RE 1.880679530866358e-05 c 1.0000292 k 0.009997597 m 5.0009747\n",
      "198 Train Loss 0.016051415 Test RE 1.880679530866358e-05 c 1.0000292 k 0.009997597 m 5.0009747\n",
      "199 Train Loss 0.016051415 Test RE 1.880679530866358e-05 c 1.0000292 k 0.009997597 m 5.0009747\n",
      "Training time: 71.79\n",
      "Training time: 71.79\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 200\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "m_full = []\n",
    "k_full = []\n",
    "c_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "\n",
    "for reps in range(max_reps):   \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []   \n",
    "    beta_val = []\n",
    "    m_val = []\n",
    "    k_val = []\n",
    "    c_val = []\n",
    "\n",
    "    torch.manual_seed(reps*36)\n",
    "    N_f = 10000 #Total number of collocation points\n",
    "\n",
    "    layers = np.array([1,50,50,50,1]) #9 hidden layers\n",
    "    PINN = Sequentialmodel(layers)\n",
    "\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.05, \n",
    "                                max_iter = 10, \n",
    "                                max_eval = 15, \n",
    "                                tolerance_grad = 1e-6, \n",
    "                                tolerance_change = 1e-6, \n",
    "                                history_size = 100, \n",
    "                                line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "    train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    beta_full.append(beta_val)\n",
    "    m_full.append(m_val)\n",
    "    k_full.append(k_val)\n",
    "    c_full.append(c_val)\n",
    "    \n",
    "\n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"beta\": beta_full,\"m\": m_full,\"k\": k_full,\"c\": c_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "pmHEeBpzfFQh",
    "outputId": "990b2054-f35e-4c8e-c378-69ff2eb19f4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f0d46f7ffd0>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_pred = PINN.test()\n",
    "plt.plot(t,x_true,'b')\n",
    "plt.plot(t,x_pred,'r--')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "stan_1D_FODE_tune.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
