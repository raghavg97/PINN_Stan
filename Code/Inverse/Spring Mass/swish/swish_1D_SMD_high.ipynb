{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8byrnUmNKGR",
    "outputId": "510f67c6-b225-41ba-e78a-d0a6e54ded85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uoNYpDzlNKGV"
   },
   "outputs": [],
   "source": [
    "tstart= 0 \n",
    "tstop= 60 \n",
    "\n",
    "increment = 0.1 \n",
    "# Initial condition\n",
    "x_init= [0,0] \n",
    "\n",
    "t = np.arange(tstart,tstop+1,increment)\n",
    "\n",
    "c = 1 # Damping constant \n",
    "k = 0.01 # Stiffness of the spring\n",
    "m = 5 # Mass \n",
    "F0 = 100\n",
    "\n",
    "def mydiff(x, t):    \n",
    "    F =F0\n",
    "    \n",
    "    dx1dt = x[1] \n",
    "    dx2dt = (F -c*x[1] -k*x[0])/m\n",
    "    dxdt= [dx1dt, dx2dt] \n",
    "    \n",
    "    return dxdt \n",
    "\n",
    "x_full_sol = odeint(mydiff, x_init, t) \n",
    "\n",
    "x_sol = x_full_sol[:,0]\n",
    "v_sol = x_full_sol[:,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BR02v-fkNKGV"
   },
   "outputs": [],
   "source": [
    "level = \"high\"\n",
    "label = \"1D_SMD_swish_\" + level\n",
    "\n",
    "loss_thresh = 0.005\n",
    "\n",
    "t_test = t.reshape(-1,1)\n",
    "t_test_tensor = torch.from_numpy(t_test).float().to(device)\n",
    "\n",
    "# bc1_t = t[0].reshape(-1,1)\n",
    "# bc1_x = x_sol[0].reshape(-1,1)\n",
    "# t_bc1_train = torch.from_numpy(bc1_t).float().to(device)\n",
    "# x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "\n",
    "bc1_t = t.reshape(-1,1)\n",
    "bc1_x = x_sol.reshape(-1,1)\n",
    "t_bc1_train = torch.from_numpy(bc1_t).float().to(device)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "\n",
    "\n",
    "bc2_t = t[0].reshape(-1,1)\n",
    "t_bc2_train = torch.from_numpy(bc2_t).float().to(device)\n",
    "bc2_val = v_sol[0].reshape(-1,1)\n",
    "bc2_val =torch.from_numpy(bc2_val).float().to(device)\n",
    "\n",
    "t_test = t.reshape(-1,1)\n",
    "t_test_tensor = torch.from_numpy(t_test).float().to(device)\n",
    "x_true = x_sol\n",
    "x_true_norm = np.linalg.norm(x_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(t[0]) \n",
    "ub = np.array(t[-1]) \n",
    "\n",
    "#torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SyyktBKBXRo1"
   },
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    \n",
    "    t01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=t01,random_state =seed)\n",
    "\n",
    "    t_coll = lb + (ub-lb)*sampling(N_f)\n",
    "    # t_coll = np.vstack((t_coll,)) # append training points to collocation points \n",
    "\n",
    "    return t_coll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "o1b21zLnNKGW"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Sigmoid()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "        \n",
    "        self.beta = Parameter(torch.ones((layers[1],len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "        \n",
    "        self.k = Parameter(torch.tensor(0.0))\n",
    "        self.k.requiresGrad = True\n",
    "        self.c = Parameter(torch.tensor(0.0))\n",
    "        self.c.requiresGrad = True\n",
    "        self.m = Parameter(torch.tensor(0.0))\n",
    "        self.m.requiresGrad = True\n",
    "        \n",
    "    'forward pass'\n",
    "    def forward(self,t):\n",
    "        if torch.is_tensor(t) != True:         \n",
    "            t = torch.from_numpy(t)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        t = 2.0*(t - l_b)/(u_b - l_b) - 1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = t.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = z*self.activation(self.beta[:,i]*z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,t,x):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(t), x)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,t_bc2,bc2_val):\n",
    "        g = t_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        x = self.forward(g)    \n",
    "            \n",
    "        x_t = autograd.grad(x,g,torch.ones([t_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dx_dt = x_t[:,[0]]\n",
    "        \n",
    "        bc2 = dx_dt\n",
    "        \n",
    "        loss_bc2= self.loss_function(bc2,bc2_val)\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self,t_coll,f_hat):\n",
    "             \n",
    "        g = t_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        x = self.forward(g) \n",
    "\n",
    "        x_t = autograd.grad(x,g,torch.ones([t_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        x_tt = autograd.grad(x_t,g,torch.ones(t_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dx_dt = x_t[:,[0]]\n",
    "        \n",
    "        dx2_d2t = x_tt[:,[0]]\n",
    "        \n",
    "        f = self.m*dx2_d2t + self.c*dx_dt + self.k*x - F0\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,t_bc1,x_bc1,t_bc2,bc2_val,t_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(t_bc1,x_bc1)\n",
    "        loss_bc2 = self.loss_BC2(t_bc2,bc2_val)\n",
    "        loss_f = self.loss_PDE(t_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self):\n",
    "        x_pred = self.forward(t_test_tensor)\n",
    "        x_pred = x_pred.cpu().detach().numpy()\n",
    "\n",
    "        return x_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        x_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(x_pred.reshape(-1,1) - x_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(x_pred.reshape(-1,1) - x_true.reshape(-1,1),2)/x_true_norm\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fLY2mT5BOgjD"
   },
   "outputs": [],
   "source": [
    "def train_step(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8srA5uGuObil"
   },
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    m_val.append(PINN.m.cpu().detach().numpy())\n",
    "    k_val.append(PINN.k.cpu().detach().numpy())\n",
    "    c_val.append(PINN.c.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0SezTZ_racQB"
   },
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "\n",
    "    t_coll = colloc_pts(N_f,rep*11)\n",
    "    t_coll =  torch.from_numpy(t_coll).float().to(device)\n",
    "    \n",
    "    f_hat = torch.zeros(t_coll.shape[0],1).to(device)\n",
    "    for i in range(max_iter):      \n",
    "        train_step(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat)\n",
    "         \n",
    "        loss_np = PINN.loss(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test RE\",test_re_loss[-1],\"c\",PINN.c.cpu().detach().numpy(),\"k\",PINN.k.cpu().detach().numpy(),\"m\",PINN.m.cpu().detach().numpy())\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I9BYbcJ0NKGX",
    "outputId": "42720d97-b37a-4c42-b7e5-b77a0ef391f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 4150401.0 Test RE 0.6978253968633702 c -0.008593233 k 1.0346282 m -7.703335e-05\n",
      "1 Train Loss 993038.94 Test RE 0.3793803446312333 c -0.020016836 k 0.034664646 m -0.00014558688\n",
      "2 Train Loss 210867.03 Test RE 0.17348235731000342 c -0.0040811724 k 0.042751633 m -0.00010017995\n",
      "3 Train Loss 7636.643 Test RE 0.01520043490256002 c 0.013173212 k 0.032509014 m 0.000112871305\n",
      "4 Train Loss 6498.839 Test RE 0.0095720823318121 c 0.059995722 k 0.031361517 m 0.0008728467\n",
      "5 Train Loss 5364.386 Test RE 0.012512255843998798 c 0.2886961 k 0.024418717 m 0.00475637\n",
      "6 Train Loss 3497.062 Test RE 0.01100069720095161 c 0.6966711 k 0.015858773 m 0.0123917\n",
      "7 Train Loss 2380.5042 Test RE 0.008679125105833475 c 1.1272991 k 0.0052017444 m 0.02111624\n",
      "8 Train Loss 1982.645 Test RE 0.006707431390926004 c 1.2351384 k 0.0030243346 m 0.025699133\n",
      "9 Train Loss 1583.3643 Test RE 0.004873614805451 c 1.3267828 k -0.00015190402 m 0.035164665\n",
      "10 Train Loss 1251.267 Test RE 0.004253789900105459 c 1.3937074 k -0.00056557683 m 0.04654056\n",
      "11 Train Loss 963.1798 Test RE 0.003926041270663758 c 1.4504397 k -0.00231998 m 0.059722506\n",
      "12 Train Loss 564.866 Test RE 0.004213534349765329 c 1.3504453 k 0.0009224419 m 0.08973508\n",
      "13 Train Loss 346.36963 Test RE 0.0028974969846663904 c 1.1876385 k 0.0046304106 m 0.12524454\n",
      "14 Train Loss 328.44534 Test RE 0.0030432589669430656 c 1.1766267 k 0.0049465457 m 0.1313214\n",
      "15 Train Loss 309.57697 Test RE 0.0027353155201048493 c 1.1630024 k 0.005152264 m 0.16699293\n",
      "16 Train Loss 295.80765 Test RE 0.002258581022660353 c 1.1496388 k 0.0056348913 m 0.18951003\n",
      "17 Train Loss 281.195 Test RE 0.0024952325843707324 c 1.132557 k 0.0059478036 m 0.24395372\n",
      "18 Train Loss 257.83292 Test RE 0.0029647444119723196 c 1.2029252 k 0.0042559733 m 0.33418003\n",
      "19 Train Loss 250.29044 Test RE 0.0029362855540536153 c 1.1640629 k 0.0051635373 m 0.38053972\n",
      "20 Train Loss 238.47171 Test RE 0.002878910170987307 c 1.188305 k 0.0046388013 m 0.44832802\n",
      "21 Train Loss 231.40906 Test RE 0.002813410916333035 c 1.1592932 k 0.0053409794 m 0.46057916\n",
      "22 Train Loss 228.56 Test RE 0.0029428774660963312 c 1.1693841 k 0.0051158774 m 0.47545424\n",
      "23 Train Loss 225.14542 Test RE 0.0028780066912769643 c 1.188449 k 0.004648944 m 0.51218504\n",
      "24 Train Loss 215.74973 Test RE 0.0029707701098814726 c 1.1487643 k 0.005613858 m 0.5715082\n",
      "25 Train Loss 202.87051 Test RE 0.002541024337562169 c 1.1733141 k 0.004994936 m 0.67920256\n",
      "26 Train Loss 186.53003 Test RE 0.002462273687613544 c 1.1471806 k 0.0058115744 m 0.9175068\n",
      "27 Train Loss 171.81714 Test RE 0.002250610573469508 c 1.147309 k 0.0056809466 m 1.0287513\n",
      "28 Train Loss 161.33255 Test RE 0.0022371435262408847 c 1.1578006 k 0.0054837805 m 1.1686188\n",
      "29 Train Loss 152.14508 Test RE 0.0022989320586637905 c 1.099953 k 0.0069905412 m 1.4089082\n",
      "30 Train Loss 137.12036 Test RE 0.0022061197584847285 c 1.1430924 k 0.0059172544 m 1.7557133\n",
      "31 Train Loss 96.82459 Test RE 0.0021530855492809214 c 1.0867722 k 0.0073559918 m 2.4012709\n",
      "32 Train Loss 84.10976 Test RE 0.0020703105307316436 c 1.086635 k 0.007319468 m 2.6365263\n",
      "33 Train Loss 83.3931 Test RE 0.0020933992013124707 c 1.0918494 k 0.007223306 m 2.6792812\n",
      "34 Train Loss 78.46146 Test RE 0.002029918429890798 c 1.1011779 k 0.007147276 m 2.8312027\n",
      "35 Train Loss 75.2457 Test RE 0.0019232963763625445 c 1.0829346 k 0.00745801 m 2.8186438\n",
      "36 Train Loss 72.367165 Test RE 0.0018256686354786633 c 1.0654688 k 0.007768817 m 2.892802\n",
      "37 Train Loss 63.161186 Test RE 0.0016502539385259466 c 1.0712214 k 0.0078232875 m 3.1882234\n",
      "38 Train Loss 57.96878 Test RE 0.0016394041292399372 c 1.0768553 k 0.007846962 m 3.5115552\n",
      "39 Train Loss 46.745186 Test RE 0.0016201177924967671 c 1.0404483 k 0.008522029 m 3.8493652\n",
      "40 Train Loss 32.990673 Test RE 0.0013691043756915305 c 1.0309687 k 0.0088594295 m 4.0774574\n",
      "41 Train Loss 30.746304 Test RE 0.001363748023987215 c 1.035952 k 0.008824973 m 4.1775904\n",
      "42 Train Loss 29.728905 Test RE 0.0013312481330670807 c 1.0351335 k 0.0088690715 m 4.3018527\n",
      "43 Train Loss 27.683323 Test RE 0.0012234378024742725 c 1.0275484 k 0.009044209 m 4.373228\n",
      "44 Train Loss 24.386986 Test RE 0.0012148077887669333 c 1.0252342 k 0.009097385 m 4.343997\n",
      "45 Train Loss 23.018387 Test RE 0.0012292065688433995 c 1.0229009 k 0.009200217 m 4.473577\n",
      "46 Train Loss 22.755962 Test RE 0.0011988300298400186 c 1.0219197 k 0.009254813 m 4.5431514\n",
      "47 Train Loss 22.567877 Test RE 0.0011484279849844957 c 1.018237 k 0.009341442 m 4.5710263\n",
      "48 Train Loss 21.349783 Test RE 0.0010045918602586133 c 1.0149508 k 0.009415166 m 4.502974\n",
      "49 Train Loss 17.490187 Test RE 0.0010440758573487668 c 1.0091367 k 0.009558877 m 4.6460304\n",
      "50 Train Loss 16.395182 Test RE 0.001032467090584884 c 1.0154483 k 0.009493878 m 4.762787\n",
      "51 Train Loss 13.050465 Test RE 0.0008211363462591211 c 1.0123035 k 0.009681343 m 4.9894357\n",
      "52 Train Loss 10.516097 Test RE 0.0007560797286851404 c 1.0080795 k 0.0096731465 m 4.886324\n",
      "53 Train Loss 9.672794 Test RE 0.0007655154509524316 c 1.0111717 k 0.009594088 m 4.8393397\n",
      "54 Train Loss 8.667864 Test RE 0.000735093597955826 c 1.011322 k 0.009592409 m 4.7911143\n",
      "55 Train Loss 8.119397 Test RE 0.000699751529385062 c 1.0121028 k 0.009551993 m 4.7655096\n",
      "56 Train Loss 7.554947 Test RE 0.0006596028523384993 c 1.0171252 k 0.009401501 m 4.725608\n",
      "57 Train Loss 6.347034 Test RE 0.0005559363154040193 c 1.0175644 k 0.009442724 m 4.733137\n",
      "58 Train Loss 5.587784 Test RE 0.0005248880153628641 c 1.0099599 k 0.009638918 m 4.7708306\n",
      "59 Train Loss 5.3992596 Test RE 0.0005124268047711448 c 1.010192 k 0.009615628 m 4.786517\n",
      "60 Train Loss 5.3221703 Test RE 0.000498193250816418 c 1.0114734 k 0.00959329 m 4.801371\n",
      "61 Train Loss 5.1596823 Test RE 0.00046855630657931577 c 1.0096285 k 0.009661823 m 4.860581\n",
      "62 Train Loss 4.7487354 Test RE 0.0004335891008399916 c 1.0035418 k 0.009818775 m 4.9961157\n",
      "63 Train Loss 4.6093283 Test RE 0.0004470409831070723 c 1.0032737 k 0.0098197935 m 5.0211954\n",
      "64 Train Loss 3.8784733 Test RE 0.0004802369259218844 c 1.0062312 k 0.009755897 m 5.0440135\n",
      "65 Train Loss 3.257741 Test RE 0.00045831585920164407 c 1.0009446 k 0.009853899 m 5.0642476\n",
      "66 Train Loss 3.0233696 Test RE 0.0004294633144728702 c 1.0008532 k 0.009869017 m 5.0662727\n",
      "67 Train Loss 2.7941775 Test RE 0.0004110457130444706 c 1.0037743 k 0.009814479 m 5.053598\n",
      "68 Train Loss 2.3609195 Test RE 0.00038376868573469254 c 1.0018272 k 0.009853538 m 5.0091486\n",
      "69 Train Loss 1.877831 Test RE 0.00032732197135271706 c 1.0020212 k 0.009832636 m 4.917619\n",
      "70 Train Loss 1.5431607 Test RE 0.0002850300174641076 c 1.0088544 k 0.009684735 m 4.8773303\n",
      "71 Train Loss 1.2693238 Test RE 0.000241206790273984 c 1.0089372 k 0.009701444 m 4.874814\n",
      "72 Train Loss 0.81501937 Test RE 0.00019040173627571933 c 1.0011891 k 0.0099179065 m 4.9653945\n",
      "73 Train Loss 0.5651394 Test RE 0.00014554604975405526 c 0.99958986 k 0.009970394 m 5.0129986\n",
      "74 Train Loss 0.49158782 Test RE 0.00012807353570560862 c 1.0007571 k 0.009955888 m 5.0365415\n",
      "75 Train Loss 0.41457447 Test RE 0.00010982428526481947 c 1.0018485 k 0.009928863 m 5.0185065\n",
      "76 Train Loss 0.3910692 Test RE 9.672791753803647e-05 c 1.0012594 k 0.009942819 m 5.0056095\n",
      "77 Train Loss 0.38040537 Test RE 9.042399009209381e-05 c 1.0001857 k 0.009971904 m 5.0130734\n",
      "78 Train Loss 0.32105675 Test RE 8.726244913766222e-05 c 1.0000225 k 0.009989732 m 5.0225205\n",
      "79 Train Loss 0.28721 Test RE 7.91625630552767e-05 c 1.0001025 k 0.009979507 m 5.020053\n",
      "80 Train Loss 0.24217671 Test RE 7.592634924148835e-05 c 0.9999022 k 0.009977802 m 5.017463\n",
      "81 Train Loss 0.18115759 Test RE 7.462760368294305e-05 c 0.99993706 k 0.00997967 m 5.0226555\n",
      "82 Train Loss 0.16613266 Test RE 7.773680829256301e-05 c 1.0006212 k 0.0099632 m 5.015342\n",
      "83 Train Loss 0.15829447 Test RE 7.987982719496205e-05 c 1.0010978 k 0.009948583 m 5.0035887\n",
      "84 Train Loss 0.15503035 Test RE 7.760082723606096e-05 c 1.0009453 k 0.009951865 m 5.004188\n",
      "85 Train Loss 0.14972794 Test RE 7.481331977882974e-05 c 1.0008111 k 0.009956208 m 5.009539\n",
      "86 Train Loss 0.12072568 Test RE 6.530501096053304e-05 c 1.0008733 k 0.0099588465 m 5.0158157\n",
      "87 Train Loss 0.08842714 Test RE 5.267945089301311e-05 c 1.0003995 k 0.009970733 m 5.0108576\n",
      "88 Train Loss 0.072709315 Test RE 4.0386707863159895e-05 c 1.000018 k 0.009983339 m 5.0012784\n",
      "89 Train Loss 0.06517275 Test RE 3.630089359040822e-05 c 1.0001422 k 0.009979492 m 4.993746\n",
      "90 Train Loss 0.06105438 Test RE 3.664909994951188e-05 c 1.0006034 k 0.009966656 m 4.989468\n",
      "91 Train Loss 0.05796794 Test RE 3.387748587898266e-05 c 1.0011917 k 0.009952398 m 4.9871774\n",
      "92 Train Loss 0.052303657 Test RE 3.345903457161025e-05 c 1.001704 k 0.009941423 m 4.9884276\n",
      "93 Train Loss 0.0445924 Test RE 3.6137607564886696e-05 c 1.0016918 k 0.009942147 m 4.996142\n",
      "94 Train Loss 0.041916516 Test RE 3.600073924860823e-05 c 1.0013013 k 0.009952634 m 5.000321\n",
      "95 Train Loss 0.040637616 Test RE 3.580854386522351e-05 c 1.0010161 k 0.00995907 m 4.9984317\n",
      "96 Train Loss 0.03955922 Test RE 3.542957677575806e-05 c 1.0008366 k 0.0099621955 m 4.9946184\n",
      "97 Train Loss 0.03840064 Test RE 3.471877388366288e-05 c 1.000665 k 0.009966696 m 4.994457\n",
      "98 Train Loss 0.0365491 Test RE 3.2922162020125656e-05 c 1.000522 k 0.009972621 m 4.9981656\n",
      "99 Train Loss 0.031299982 Test RE 2.6906073806873756e-05 c 1.000826 k 0.009968653 m 5.0002747\n",
      "100 Train Loss 0.026865955 Test RE 2.290780077276494e-05 c 1.0006272 k 0.009972529 m 4.996147\n",
      "101 Train Loss 0.024140239 Test RE 2.1130103781522634e-05 c 1.0002953 k 0.009981081 m 4.994922\n",
      "102 Train Loss 0.021492895 Test RE 1.8404351873966164e-05 c 1.0003327 k 0.009982378 m 4.997031\n",
      "103 Train Loss 0.020277686 Test RE 1.5805106560965366e-05 c 1.0004532 k 0.009982568 m 4.9997406\n",
      "104 Train Loss 0.019427327 Test RE 1.2597065750448463e-05 c 1.0003079 k 0.009987443 m 4.99999\n",
      "105 Train Loss 0.019234255 Test RE 1.1406549117869838e-05 c 1.0002185 k 0.009990027 m 4.999492\n",
      "106 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "107 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "108 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "109 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "110 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "111 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "112 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "113 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "114 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "115 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "116 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "117 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "118 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "119 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "120 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "121 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "122 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "123 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "124 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "125 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "126 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "127 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "128 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "129 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "130 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "131 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "132 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "133 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "134 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "135 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "136 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "137 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "138 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "139 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "140 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "141 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "142 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "143 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "144 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "145 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "146 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "147 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "148 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "149 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "150 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "151 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "152 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "153 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "154 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "155 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "156 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "157 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "158 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "159 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "160 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "161 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "162 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "163 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "164 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "165 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "166 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "167 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "168 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "169 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "170 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "171 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "172 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "173 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "174 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "175 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "176 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "177 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "178 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "179 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "180 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "181 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "182 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "183 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "184 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "185 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "186 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "187 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "188 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "189 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "190 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "191 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "192 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "193 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "194 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "195 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "196 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "197 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "198 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "199 Train Loss 0.019188916 Test RE 1.0987150273814383e-05 c 1.0001949 k 0.009990621 m 4.99913\n",
      "Training time: 57.98\n",
      "Training time: 57.98\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 5122609.5 Test RE 0.8049349570191159 c -0.007410263 k 1.4880353 m -0.00018307747\n",
      "1 Train Loss 1260931.5 Test RE 0.4275662062063705 c -0.018535929 k 0.049279075 m -0.00037131956\n",
      "2 Train Loss 909188.7 Test RE 0.35896622657684085 c -0.013344149 k -0.018954728 m -0.00033278432\n",
      "3 Train Loss 23589.76 Test RE 0.05395561829490414 c 0.019038288 k 0.037713148 m -0.00032375928\n",
      "4 Train Loss 20603.25 Test RE 0.04941219007612891 c 0.027744815 k 0.021431584 m 6.38275e-05\n",
      "5 Train Loss 7055.451 Test RE 0.025915201944201816 c 0.0908339 k 0.033254944 m 0.0030869015\n",
      "6 Train Loss 6689.4976 Test RE 0.02514548417840299 c 0.10092193 k 0.02992125 m 0.0035666116\n",
      "7 Train Loss 6337.4336 Test RE 0.02421842330020435 c 0.11527031 k 0.029340014 m 0.004381669\n",
      "8 Train Loss 5499.8555 Test RE 0.02167602268372233 c 0.15633947 k 0.03230062 m 0.007300419\n",
      "9 Train Loss 1903.0673 Test RE 0.007170187909041588 c 0.3885926 k 0.019812688 m 0.025675785\n",
      "10 Train Loss 1714.3499 Test RE 0.006211144729099545 c 0.39068794 k 0.023653097 m 0.02607017\n",
      "11 Train Loss 1524.713 Test RE 0.0042103057685599545 c 0.42855233 k 0.024593666 m 0.029561635\n",
      "12 Train Loss 1396.1935 Test RE 0.002989858351221118 c 0.44377616 k 0.021269377 m 0.03075555\n",
      "13 Train Loss 1371.2678 Test RE 0.0027465454140605003 c 0.45854652 k 0.021742322 m 0.03192441\n",
      "14 Train Loss 1362.6198 Test RE 0.003153671678058335 c 0.47482517 k 0.021185514 m 0.033139948\n",
      "15 Train Loss 1259.4885 Test RE 0.005122328173855058 c 0.5674682 k 0.019359292 m 0.039865397\n",
      "16 Train Loss 1027.3376 Test RE 0.004679695336157857 c 0.6411003 k 0.017451301 m 0.045073625\n",
      "17 Train Loss 1008.10144 Test RE 0.004699709081312842 c 0.6469558 k 0.01742807 m 0.045646906\n",
      "18 Train Loss 982.33307 Test RE 0.004387158839941721 c 0.6438785 k 0.017546019 m 0.04571245\n",
      "19 Train Loss 878.11273 Test RE 0.0057234006202088895 c 0.751417 k 0.015070794 m 0.055360235\n",
      "20 Train Loss 591.7614 Test RE 0.004921927521863839 c 1.0072708 k 0.008983773 m 0.07690883\n",
      "21 Train Loss 567.55945 Test RE 0.005291947212506488 c 1.1189697 k 0.006554227 m 0.086661845\n",
      "22 Train Loss 565.11945 Test RE 0.005376308512542328 c 1.1345611 k 0.006250108 m 0.08811564\n",
      "23 Train Loss 507.29904 Test RE 0.0052661020926884705 c 1.1385689 k 0.0059829694 m 0.09035697\n",
      "24 Train Loss 373.331 Test RE 0.003203343977313493 c 1.1351533 k 0.0060413466 m 0.092607066\n",
      "25 Train Loss 362.60062 Test RE 0.0034124210307001353 c 1.1561135 k 0.0054259924 m 0.09480913\n",
      "26 Train Loss 361.13766 Test RE 0.0036074380695114046 c 1.1577927 k 0.0052849255 m 0.095802285\n",
      "27 Train Loss 347.67413 Test RE 0.004398200824983013 c 1.1244633 k 0.0060408316 m 0.09513046\n",
      "28 Train Loss 320.25244 Test RE 0.004292980560029207 c 1.0836941 k 0.0070563895 m 0.09100924\n",
      "29 Train Loss 316.20038 Test RE 0.00446408820469388 c 1.1060338 k 0.0064583807 m 0.0927988\n",
      "30 Train Loss 316.0316 Test RE 0.004482181206027726 c 1.1078932 k 0.006433989 m 0.09301594\n",
      "31 Train Loss 315.9062 Test RE 0.004501698875715947 c 1.1090114 k 0.0064356923 m 0.09321864\n",
      "32 Train Loss 314.86523 Test RE 0.004573843714822858 c 1.1102352 k 0.0064391857 m 0.09384003\n",
      "33 Train Loss 313.49518 Test RE 0.004580455375282868 c 1.1119318 k 0.0063564177 m 0.09414904\n",
      "34 Train Loss 306.29468 Test RE 0.004471571491095535 c 1.131934 k 0.0059820586 m 0.09637258\n",
      "35 Train Loss 289.81653 Test RE 0.004060875699678469 c 1.1454948 k 0.0055577164 m 0.0977331\n",
      "36 Train Loss 281.3743 Test RE 0.003914959186727304 c 1.1344593 k 0.005887106 m 0.097415276\n",
      "37 Train Loss 280.46713 Test RE 0.003965147841540345 c 1.1384646 k 0.0058095693 m 0.09803406\n",
      "38 Train Loss 278.40216 Test RE 0.004021617248204235 c 1.1574637 k 0.005447678 m 0.10048228\n",
      "39 Train Loss 269.36487 Test RE 0.003832353768139374 c 1.174307 k 0.004957307 m 0.1028174\n",
      "40 Train Loss 267.5879 Test RE 0.0037867324975340384 c 1.1822366 k 0.004772394 m 0.10385128\n",
      "41 Train Loss 266.672 Test RE 0.0037794014692709215 c 1.1943846 k 0.0044587096 m 0.105057515\n",
      "42 Train Loss 266.60223 Test RE 0.003775554365760917 c 1.1967531 k 0.0043963944 m 0.10526157\n",
      "43 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "44 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "45 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "46 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "47 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "48 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "49 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "50 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "51 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "52 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "53 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "54 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "55 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "56 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "57 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "58 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "59 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "60 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "61 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "62 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "63 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "64 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "65 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "66 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "67 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "68 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "69 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "70 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "71 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "72 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "73 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "74 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "75 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "76 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "77 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "78 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "79 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "80 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "81 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "82 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "83 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "84 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "85 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "86 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "87 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "88 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "89 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "90 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "91 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "92 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "93 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "94 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "95 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "96 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "97 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "98 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "99 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "100 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "101 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "102 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "103 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "104 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "105 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "106 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "107 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "108 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "109 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "110 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "111 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "112 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "113 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "114 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "115 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "116 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "117 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "118 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "119 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "120 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "121 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "122 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "123 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "124 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "125 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "126 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "127 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "128 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "129 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "130 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "131 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "132 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "133 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "134 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "135 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "136 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "137 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "138 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "139 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "140 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "141 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "142 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "143 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "144 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "145 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "146 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "147 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "148 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "149 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "150 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "151 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "152 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "153 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "154 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "155 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "156 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "157 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "158 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "159 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "160 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "161 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "162 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "163 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "164 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "165 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "166 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "167 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "168 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "169 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "170 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "171 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "172 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "173 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "174 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "175 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "176 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "177 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "178 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "179 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "180 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "181 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "182 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "183 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "184 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "185 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "186 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "187 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "188 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "189 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "190 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "191 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "192 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "193 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "194 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "195 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "196 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "197 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "198 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "199 Train Loss 266.59097 Test RE 0.0037731889346070374 c 1.197647 k 0.0043735043 m 0.10534071\n",
      "Training time: 45.34\n",
      "Training time: 45.34\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 1117397.4 Test RE 0.4008404393779031 c 0.0031708938 k 0.0010453006 m -0.00022868473\n",
      "1 Train Loss 1037852.3 Test RE 0.3840467192517687 c 0.0037098734 k 0.102237955 m -0.00020648388\n",
      "2 Train Loss 77398.92 Test RE 0.09625036715047089 c -0.02128319 k -0.0016788654 m -0.0001890021\n",
      "3 Train Loss 7585.0146 Test RE 0.026480360489150197 c -0.007253047 k 0.03260397 m 0.0004961871\n",
      "4 Train Loss 4616.141 Test RE 0.016960784026861245 c 0.045413967 k 0.034137063 m 0.0046552746\n",
      "5 Train Loss 3433.9653 Test RE 0.01319313967505987 c 0.13018236 k 0.029333346 m 0.011491671\n",
      "6 Train Loss 816.3517 Test RE 0.006036444521281481 c 0.84418184 k 0.011506193 m 0.06848048\n",
      "7 Train Loss 588.17865 Test RE 0.004782313712746402 c 1.1252058 k 0.0059129624 m 0.09136461\n",
      "8 Train Loss 546.80554 Test RE 0.004601825661539816 c 1.2260568 k 0.0026864756 m 0.10230437\n",
      "9 Train Loss 482.57452 Test RE 0.004051863063060737 c 1.2101068 k 0.004008152 m 0.10986266\n",
      "10 Train Loss 432.5334 Test RE 0.0035452062814137425 c 1.2008541 k 0.004079473 m 0.13365667\n",
      "11 Train Loss 414.1468 Test RE 0.0036594189882264948 c 1.21557 k 0.0033591422 m 0.16275173\n",
      "12 Train Loss 378.87503 Test RE 0.004195477761695442 c 1.1416342 k 0.005804441 m 0.20495886\n",
      "13 Train Loss 366.69516 Test RE 0.004345488818234104 c 1.1317511 k 0.005651456 m 0.23125656\n",
      "14 Train Loss 355.10156 Test RE 0.0042002000720665345 c 1.1673659 k 0.0051210406 m 0.26214728\n",
      "15 Train Loss 334.69955 Test RE 0.003983855133713505 c 1.2104951 k 0.0037819764 m 0.35286665\n",
      "16 Train Loss 295.21863 Test RE 0.004048276631365094 c 1.2031381 k 0.004127466 m 0.5377056\n",
      "17 Train Loss 256.4214 Test RE 0.0034097934931883564 c 1.13963 k 0.005478612 m 0.7446574\n",
      "18 Train Loss 216.07022 Test RE 0.002731690109466874 c 1.2136611 k 0.004162942 m 0.8555357\n",
      "19 Train Loss 207.37474 Test RE 0.0026177163828296564 c 1.1935363 k 0.0041574365 m 0.88530904\n",
      "20 Train Loss 203.81209 Test RE 0.002571884600106912 c 1.1711134 k 0.0050330725 m 0.91011125\n",
      "21 Train Loss 197.40785 Test RE 0.002463357268229298 c 1.1612903 k 0.0053409436 m 0.91468275\n",
      "22 Train Loss 194.32477 Test RE 0.002559315324809857 c 1.1614002 k 0.005170436 m 0.8859332\n",
      "23 Train Loss 188.65247 Test RE 0.002684750755657721 c 1.1487112 k 0.005448982 m 0.8962216\n",
      "24 Train Loss 174.56938 Test RE 0.00277393030323428 c 1.1602036 k 0.0054372377 m 1.0404924\n",
      "25 Train Loss 169.92842 Test RE 0.002438725856147859 c 1.1563387 k 0.0054513295 m 1.1333219\n",
      "26 Train Loss 168.03061 Test RE 0.002412675557955924 c 1.1424228 k 0.0058959476 m 1.1848272\n",
      "27 Train Loss 161.65929 Test RE 0.0020415286160308773 c 1.1366931 k 0.006103489 m 1.2514126\n",
      "28 Train Loss 157.85553 Test RE 0.0016628006813066776 c 1.1376449 k 0.0059760534 m 1.2969605\n",
      "29 Train Loss 149.14482 Test RE 0.0020749097425054376 c 1.149449 k 0.0057265065 m 1.4305338\n",
      "30 Train Loss 132.9656 Test RE 0.0026112842770973136 c 1.1162436 k 0.0066856323 m 1.8646215\n",
      "31 Train Loss 117.56031 Test RE 0.0017612368482370973 c 1.1230907 k 0.006402448 m 1.9407533\n",
      "32 Train Loss 102.31586 Test RE 0.0015857220262591447 c 1.1100163 k 0.0067439517 m 2.090662\n",
      "33 Train Loss 98.10254 Test RE 0.0017247520494726133 c 1.1101791 k 0.006629441 m 2.1501925\n",
      "34 Train Loss 93.27873 Test RE 0.0015344506865100644 c 1.1101747 k 0.006746384 m 2.2609737\n",
      "35 Train Loss 85.71599 Test RE 0.0016451546063868093 c 1.1026428 k 0.0068805963 m 2.490668\n",
      "36 Train Loss 78.62764 Test RE 0.001640902969353788 c 1.0964701 k 0.007068851 m 2.6304061\n",
      "37 Train Loss 71.69175 Test RE 0.0013408254599482864 c 1.0892175 k 0.0074026007 m 2.695225\n",
      "38 Train Loss 63.84041 Test RE 0.001139890786047426 c 1.089244 k 0.0072153904 m 2.8555024\n",
      "39 Train Loss 54.58993 Test RE 0.0008250886781183968 c 1.0791386 k 0.0076797246 m 3.1327162\n",
      "40 Train Loss 50.639633 Test RE 0.0008169086975932553 c 1.0776906 k 0.007629137 m 3.186289\n",
      "41 Train Loss 43.831417 Test RE 0.0009553313849027927 c 1.0734428 k 0.0078114634 m 3.2783782\n",
      "42 Train Loss 40.77026 Test RE 0.0009570556307605985 c 1.0555818 k 0.008324482 m 3.4540164\n",
      "43 Train Loss 31.311699 Test RE 0.0008156756213024253 c 1.0513936 k 0.008570184 m 3.7543616\n",
      "44 Train Loss 28.428091 Test RE 0.000639880282024885 c 1.0479648 k 0.008580633 m 3.8617475\n",
      "45 Train Loss 22.894775 Test RE 0.0006934613842792291 c 1.0287019 k 0.009149084 m 4.1771216\n",
      "46 Train Loss 15.479943 Test RE 0.0007030323282863239 c 1.00899 k 0.009708547 m 4.948339\n",
      "47 Train Loss 11.452946 Test RE 0.0003986635334875072 c 1.0069768 k 0.009713352 m 4.840184\n",
      "48 Train Loss 9.028662 Test RE 0.00041767236125981986 c 1.0168453 k 0.009427165 m 4.825125\n",
      "49 Train Loss 7.9928083 Test RE 0.0004185046467341906 c 1.0106741 k 0.00955697 m 4.843637\n",
      "50 Train Loss 7.439517 Test RE 0.00043991859000282245 c 1.0112491 k 0.009515216 m 4.8801875\n",
      "51 Train Loss 5.9722404 Test RE 0.00037927662453224304 c 1.0093039 k 0.009624423 m 4.900924\n",
      "52 Train Loss 5.1109905 Test RE 0.0003468333081666489 c 1.0054021 k 0.00974019 m 4.912993\n",
      "53 Train Loss 3.852749 Test RE 0.00031419048148236927 c 0.9939559 k 0.01014683 m 5.047393\n",
      "54 Train Loss 3.0089152 Test RE 0.000283108864830932 c 0.9976247 k 0.010049634 m 5.0955186\n",
      "55 Train Loss 2.8402696 Test RE 0.00027457485556083467 c 1.0004336 k 0.009980723 m 5.0284452\n",
      "56 Train Loss 2.5959046 Test RE 0.00027203749624849383 c 0.9980656 k 0.01005919 m 4.9923706\n",
      "57 Train Loss 2.0067027 Test RE 0.0002690965759904056 c 0.9962644 k 0.010081639 m 5.051919\n",
      "58 Train Loss 1.766886 Test RE 0.00026215241541702064 c 0.99845845 k 0.010042353 m 5.051555\n",
      "59 Train Loss 1.7324638 Test RE 0.0002598236403022378 c 0.99866265 k 0.01003609 m 5.040367\n",
      "60 Train Loss 1.7022169 Test RE 0.00025923973234070655 c 0.9986142 k 0.010030731 m 5.03425\n",
      "61 Train Loss 1.6259146 Test RE 0.00025755709030467417 c 0.99906206 k 0.0100163985 m 5.0235634\n",
      "62 Train Loss 1.4339669 Test RE 0.0002483809821877984 c 0.9992809 k 0.0100102825 m 5.0277348\n",
      "63 Train Loss 1.2561698 Test RE 0.00024166563692052458 c 0.9986917 k 0.010025503 m 5.0426283\n",
      "64 Train Loss 1.1865773 Test RE 0.00024153503193842478 c 0.9994043 k 0.010016014 m 5.035393\n",
      "65 Train Loss 1.1631026 Test RE 0.0002425041153784841 c 0.99924135 k 0.010013813 m 5.031474\n",
      "66 Train Loss 1.1313081 Test RE 0.00024119134274257507 c 0.99891657 k 0.01002579 m 5.03254\n",
      "67 Train Loss 1.1016206 Test RE 0.00024276930678739125 c 0.99900776 k 0.010024914 m 5.0325575\n",
      "68 Train Loss 1.0710509 Test RE 0.0002415976230032246 c 0.9996221 k 0.010009491 m 5.0263553\n",
      "69 Train Loss 1.0417416 Test RE 0.00023716157998592246 c 0.99951696 k 0.010008381 m 5.026015\n",
      "70 Train Loss 1.0313821 Test RE 0.00023500321320118754 c 0.9994044 k 0.010008542 m 5.026432\n",
      "71 Train Loss 0.9738735 Test RE 0.00023167385304440137 c 1.0000646 k 0.0099942135 m 5.0161977\n",
      "72 Train Loss 0.88220143 Test RE 0.0002335330567989857 c 1.0001667 k 0.01000017 m 5.0113473\n",
      "73 Train Loss 0.8380016 Test RE 0.00023020402561811938 c 0.9998706 k 0.010001965 m 5.0154247\n",
      "74 Train Loss 0.800045 Test RE 0.00022217793503642112 c 1.0000272 k 0.009991575 m 5.002631\n",
      "75 Train Loss 0.76319087 Test RE 0.0002116961760070813 c 1.0009179 k 0.00995854 m 4.9942007\n",
      "76 Train Loss 0.7250596 Test RE 0.00020160174610852003 c 1.00178 k 0.009924707 m 4.9866085\n",
      "77 Train Loss 0.70685303 Test RE 0.00019722088965777135 c 1.0018694 k 0.009918926 m 4.984747\n",
      "78 Train Loss 0.68066585 Test RE 0.00018798232231988145 c 1.0025054 k 0.009897561 m 4.969795\n",
      "79 Train Loss 0.62044847 Test RE 0.00017571601723521867 c 1.0033634 k 0.009874892 m 4.9528127\n",
      "80 Train Loss 0.5330187 Test RE 0.0001768671198448766 c 1.0021107 k 0.00992344 m 4.990834\n",
      "81 Train Loss 0.48383647 Test RE 0.00017690923664549383 c 1.0009594 k 0.009950372 m 5.0080857\n",
      "82 Train Loss 0.4549719 Test RE 0.00017168521231699884 c 1.0015447 k 0.009937129 m 4.9963026\n",
      "83 Train Loss 0.43457615 Test RE 0.0001705039791449262 c 1.00118 k 0.009946985 m 4.9878\n",
      "84 Train Loss 0.410717 Test RE 0.00016706763354645455 c 1.0009508 k 0.00995584 m 4.9796076\n",
      "85 Train Loss 0.38359833 Test RE 0.00016240991310787173 c 1.0010076 k 0.009956879 m 4.995077\n",
      "86 Train Loss 0.34869322 Test RE 0.00015385886735602583 c 1.0006268 k 0.009974805 m 5.0153723\n",
      "87 Train Loss 0.32385635 Test RE 0.00014847519953579622 c 1.0009638 k 0.009964362 m 5.012645\n",
      "88 Train Loss 0.30867085 Test RE 0.00014435210894837782 c 1.000643 k 0.009967389 m 5.002559\n",
      "89 Train Loss 0.299936 Test RE 0.00014167728959892307 c 1.0006043 k 0.009963941 m 5.000989\n",
      "90 Train Loss 0.28306162 Test RE 0.0001320176029853302 c 1.0014708 k 0.009941953 m 5.0016146\n",
      "91 Train Loss 0.26445803 Test RE 0.00012049195483549962 c 1.0016991 k 0.009931994 m 4.997268\n",
      "92 Train Loss 0.2510374 Test RE 0.00011139561801823603 c 1.0012434 k 0.009938756 m 4.989346\n",
      "93 Train Loss 0.23772746 Test RE 9.846689228682468e-05 c 1.0014886 k 0.009933205 m 4.9918256\n",
      "94 Train Loss 0.22776741 Test RE 9.035206031368107e-05 c 1.0012984 k 0.009936505 m 5.001544\n",
      "95 Train Loss 0.22041129 Test RE 8.575905437877071e-05 c 1.0013463 k 0.0099335965 m 4.9952846\n",
      "96 Train Loss 0.21105501 Test RE 8.2226773146709e-05 c 1.0021341 k 0.009910843 m 4.980403\n",
      "97 Train Loss 0.19827583 Test RE 8.036059314856562e-05 c 1.0026428 k 0.009896279 m 4.968206\n",
      "98 Train Loss 0.17368162 Test RE 7.991879559878163e-05 c 1.0019817 k 0.009915689 m 4.977961\n",
      "99 Train Loss 0.14905033 Test RE 7.408383307776804e-05 c 1.001262 k 0.009939693 m 4.9891243\n",
      "100 Train Loss 0.1412345 Test RE 7.158503434635509e-05 c 1.0013855 k 0.009937771 m 4.9902124\n",
      "101 Train Loss 0.13534613 Test RE 6.986600410676232e-05 c 1.0014416 k 0.009936677 m 4.99582\n",
      "102 Train Loss 0.12510367 Test RE 6.421944897919946e-05 c 1.0013895 k 0.0099375695 m 5.000304\n",
      "103 Train Loss 0.11861766 Test RE 6.0608520597542736e-05 c 1.0013316 k 0.00993716 m 4.992749\n",
      "104 Train Loss 0.116523854 Test RE 5.8218439511087786e-05 c 1.0015893 k 0.009929403 m 4.989855\n",
      "105 Train Loss 0.114100344 Test RE 5.4258017359974726e-05 c 1.0015941 k 0.009928016 m 4.991819\n",
      "106 Train Loss 0.113096714 Test RE 5.294688315678472e-05 c 1.0015649 k 0.009927584 m 4.993762\n",
      "107 Train Loss 0.11006282 Test RE 5.022103178577078e-05 c 1.0015035 k 0.009927135 m 4.9969935\n",
      "108 Train Loss 0.103831016 Test RE 4.547824605978909e-05 c 1.0014989 k 0.009926879 m 4.9960017\n",
      "109 Train Loss 0.09917249 Test RE 4.2444865422659386e-05 c 1.0014889 k 0.009926615 m 4.995139\n",
      "110 Train Loss 0.09644598 Test RE 4.1855897799677916e-05 c 1.0016496 k 0.009922849 m 4.9946465\n",
      "111 Train Loss 0.09411065 Test RE 4.108092083208361e-05 c 1.0017812 k 0.0099185165 m 4.991739\n",
      "112 Train Loss 0.092432365 Test RE 3.9960058658471184e-05 c 1.0017711 k 0.009918768 m 4.991617\n",
      "113 Train Loss 0.09174244 Test RE 3.9706676727395884e-05 c 1.0018146 k 0.00991731 m 4.9927397\n",
      "114 Train Loss 0.090933755 Test RE 3.9964395101917084e-05 c 1.0018831 k 0.009915309 m 4.9948745\n",
      "115 Train Loss 0.08879758 Test RE 3.992237935675269e-05 c 1.0017309 k 0.009919549 m 4.9952407\n",
      "116 Train Loss 0.087576576 Test RE 3.919906776304744e-05 c 1.0017512 k 0.009918933 m 4.9936066\n",
      "117 Train Loss 0.08560705 Test RE 3.91304496195176e-05 c 1.0017806 k 0.009918337 m 4.9931602\n",
      "118 Train Loss 0.08191476 Test RE 3.946165200154932e-05 c 1.0016928 k 0.009922677 m 4.9966507\n",
      "119 Train Loss 0.07876743 Test RE 3.913737245868254e-05 c 1.0016581 k 0.009925287 m 4.9964643\n",
      "120 Train Loss 0.07692014 Test RE 3.8956954982103234e-05 c 1.0015093 k 0.009930413 m 4.993084\n",
      "121 Train Loss 0.075399406 Test RE 3.8866664930637965e-05 c 1.00147 k 0.009931136 m 4.9924784\n",
      "122 Train Loss 0.07349788 Test RE 3.9492452690947886e-05 c 1.0016025 k 0.009926842 m 4.991538\n",
      "123 Train Loss 0.07144899 Test RE 3.998009948933699e-05 c 1.0015864 k 0.009927401 m 4.9900765\n",
      "124 Train Loss 0.069145314 Test RE 4.0283335229397566e-05 c 1.0014509 k 0.009933565 m 4.990093\n",
      "125 Train Loss 0.06758319 Test RE 4.031095689314252e-05 c 1.0013595 k 0.009937161 m 4.989675\n",
      "126 Train Loss 0.06488321 Test RE 3.835430223888137e-05 c 1.0016265 k 0.009930585 m 4.98739\n",
      "127 Train Loss 0.062479205 Test RE 3.707772616172466e-05 c 1.0014813 k 0.009933639 m 4.9921823\n",
      "128 Train Loss 0.06078237 Test RE 3.6244427841975e-05 c 1.0012594 k 0.009939563 m 4.995334\n",
      "129 Train Loss 0.0599102 Test RE 3.559363833466137e-05 c 1.0012151 k 0.009940878 m 4.9950857\n",
      "130 Train Loss 0.058749527 Test RE 3.5014256580283245e-05 c 1.0014232 k 0.0099362945 m 4.9919543\n",
      "131 Train Loss 0.05702252 Test RE 3.4732632854035444e-05 c 1.0014731 k 0.009935485 m 4.9907146\n",
      "132 Train Loss 0.055276535 Test RE 3.451864491630059e-05 c 1.001256 k 0.009940717 m 4.99162\n",
      "133 Train Loss 0.05407109 Test RE 3.4228505652450614e-05 c 1.0012146 k 0.009942753 m 4.9926376\n",
      "134 Train Loss 0.052560076 Test RE 3.4155616374002654e-05 c 1.0014414 k 0.009939417 m 4.9925327\n",
      "135 Train Loss 0.05031575 Test RE 3.321530771951154e-05 c 1.001296 k 0.009943888 m 4.9919724\n",
      "136 Train Loss 0.047583736 Test RE 3.287882911280881e-05 c 1.000947 k 0.009952779 m 4.9940586\n",
      "137 Train Loss 0.044264678 Test RE 3.125768347421958e-05 c 1.00093 k 0.009955878 m 4.9963098\n",
      "138 Train Loss 0.043319706 Test RE 3.0092815863339272e-05 c 1.0011346 k 0.009949954 m 4.9937167\n",
      "139 Train Loss 0.04329627 Test RE 3.004243577422597e-05 c 1.0011412 k 0.009949777 m 4.9936423\n",
      "140 Train Loss 0.042794153 Test RE 2.9726237224170276e-05 c 1.0012258 k 0.009948439 m 4.993136\n",
      "141 Train Loss 0.041819505 Test RE 3.0322949505304218e-05 c 1.0009536 k 0.00995475 m 4.9951377\n",
      "142 Train Loss 0.04101209 Test RE 3.057726530990624e-05 c 1.0008234 k 0.009959087 m 4.9956145\n",
      "143 Train Loss 0.04062361 Test RE 3.0392545597653302e-05 c 1.0009563 k 0.009955758 m 4.9946027\n",
      "144 Train Loss 0.04049297 Test RE 3.032148173546688e-05 c 1.001005 k 0.009954541 m 4.9942007\n",
      "145 Train Loss 0.040023297 Test RE 3.0142458328659432e-05 c 1.0010974 k 0.009952033 m 4.9939594\n",
      "146 Train Loss 0.039587524 Test RE 2.9651864455190974e-05 c 1.0010453 k 0.009952976 m 4.9949107\n",
      "147 Train Loss 0.039287277 Test RE 2.907180827097827e-05 c 1.0009692 k 0.009954402 m 4.995565\n",
      "148 Train Loss 0.039201263 Test RE 2.884583161099242e-05 c 1.0009499 k 0.0099551445 m 4.995621\n",
      "149 Train Loss 0.039201263 Test RE 2.884583161099242e-05 c 1.0009499 k 0.0099551445 m 4.995621\n",
      "150 Train Loss 0.039201263 Test RE 2.884583161099242e-05 c 1.0009499 k 0.0099551445 m 4.995621\n",
      "151 Train Loss 0.039201263 Test RE 2.884583161099242e-05 c 1.0009499 k 0.0099551445 m 4.995621\n",
      "152 Train Loss 0.039201263 Test RE 2.884583161099242e-05 c 1.0009499 k 0.0099551445 m 4.995621\n",
      "153 Train Loss 0.039201263 Test RE 2.884583161099242e-05 c 1.0009499 k 0.0099551445 m 4.995621\n",
      "154 Train Loss 0.039201263 Test RE 2.884583161099242e-05 c 1.0009499 k 0.0099551445 m 4.995621\n",
      "155 Train Loss 0.039201263 Test RE 2.884583161099242e-05 c 1.0009499 k 0.0099551445 m 4.995621\n",
      "156 Train Loss 0.039201263 Test RE 2.884583161099242e-05 c 1.0009499 k 0.0099551445 m 4.995621\n",
      "157 Train Loss 0.039201263 Test RE 2.884583161099242e-05 c 1.0009499 k 0.0099551445 m 4.995621\n",
      "158 Train Loss 0.039201263 Test RE 2.884583161099242e-05 c 1.0009499 k 0.0099551445 m 4.995621\n",
      "159 Train Loss 0.039201263 Test RE 2.884583161099242e-05 c 1.0009499 k 0.0099551445 m 4.995621\n",
      "160 Train Loss 0.039201263 Test RE 2.884583161099242e-05 c 1.0009499 k 0.0099551445 m 4.995621\n",
      "161 Train Loss 0.039201263 Test RE 2.884583161099242e-05 c 1.0009499 k 0.0099551445 m 4.995621\n",
      "162 Train Loss 0.039201263 Test RE 2.884583161099242e-05 c 1.0009499 k 0.0099551445 m 4.995621\n",
      "163 Train Loss 0.039201263 Test RE 2.884583161099242e-05 c 1.0009499 k 0.0099551445 m 4.995621\n",
      "164 Train Loss 0.039201263 Test RE 2.884583161099242e-05 c 1.0009499 k 0.0099551445 m 4.995621\n",
      "165 Train Loss 0.039201263 Test RE 2.884583161099242e-05 c 1.0009499 k 0.0099551445 m 4.995621\n",
      "166 Train Loss 0.039201263 Test RE 2.884583161099242e-05 c 1.0009499 k 0.0099551445 m 4.995621\n",
      "167 Train Loss 0.039201263 Test RE 2.884583161099242e-05 c 1.0009499 k 0.0099551445 m 4.995621\n",
      "168 Train Loss 0.039201263 Test RE 2.884583161099242e-05 c 1.0009499 k 0.0099551445 m 4.995621\n",
      "169 Train Loss 0.039201263 Test RE 2.884583161099242e-05 c 1.0009499 k 0.0099551445 m 4.995621\n",
      "170 Train Loss 0.039201263 Test RE 2.884583161099242e-05 c 1.0009499 k 0.0099551445 m 4.995621\n",
      "171 Train Loss 0.039201263 Test RE 2.884583161099242e-05 c 1.0009499 k 0.0099551445 m 4.995621\n",
      "172 Train Loss 0.039201263 Test RE 2.884583161099242e-05 c 1.0009499 k 0.0099551445 m 4.995621\n",
      "173 Train Loss 0.039201263 Test RE 2.884583161099242e-05 c 1.0009499 k 0.0099551445 m 4.995621\n",
      "174 Train Loss 0.039201263 Test RE 2.884583161099242e-05 c 1.0009499 k 0.0099551445 m 4.995621\n",
      "175 Train Loss 0.039201263 Test RE 2.884583161099242e-05 c 1.0009499 k 0.0099551445 m 4.995621\n",
      "176 Train Loss 0.039201263 Test RE 2.884583161099242e-05 c 1.0009499 k 0.0099551445 m 4.995621\n",
      "177 Train Loss 0.039201263 Test RE 2.884583161099242e-05 c 1.0009499 k 0.0099551445 m 4.995621\n",
      "178 Train Loss 0.039201263 Test RE 2.884583161099242e-05 c 1.0009499 k 0.0099551445 m 4.995621\n",
      "179 Train Loss 0.039201263 Test RE 2.884583161099242e-05 c 1.0009499 k 0.0099551445 m 4.995621\n",
      "180 Train Loss 0.039201263 Test RE 2.884583161099242e-05 c 1.0009499 k 0.0099551445 m 4.995621\n",
      "181 Train Loss 0.039201263 Test RE 2.884583161099242e-05 c 1.0009499 k 0.0099551445 m 4.995621\n",
      "182 Train Loss 0.039201263 Test RE 2.884583161099242e-05 c 1.0009499 k 0.0099551445 m 4.995621\n",
      "183 Train Loss 0.039201263 Test RE 2.884583161099242e-05 c 1.0009499 k 0.0099551445 m 4.995621\n",
      "184 Train Loss 0.039201263 Test RE 2.884583161099242e-05 c 1.0009499 k 0.0099551445 m 4.995621\n",
      "185 Train Loss 0.039201263 Test RE 2.884583161099242e-05 c 1.0009499 k 0.0099551445 m 4.995621\n",
      "186 Train Loss 0.039201263 Test RE 2.884583161099242e-05 c 1.0009499 k 0.0099551445 m 4.995621\n",
      "187 Train Loss 0.039201263 Test RE 2.884583161099242e-05 c 1.0009499 k 0.0099551445 m 4.995621\n",
      "188 Train Loss 0.039201263 Test RE 2.884583161099242e-05 c 1.0009499 k 0.0099551445 m 4.995621\n",
      "189 Train Loss 0.039201263 Test RE 2.884583161099242e-05 c 1.0009499 k 0.0099551445 m 4.995621\n",
      "190 Train Loss 0.039201263 Test RE 2.884583161099242e-05 c 1.0009499 k 0.0099551445 m 4.995621\n",
      "191 Train Loss 0.039201263 Test RE 2.884583161099242e-05 c 1.0009499 k 0.0099551445 m 4.995621\n",
      "192 Train Loss 0.039201263 Test RE 2.884583161099242e-05 c 1.0009499 k 0.0099551445 m 4.995621\n",
      "193 Train Loss 0.039201263 Test RE 2.884583161099242e-05 c 1.0009499 k 0.0099551445 m 4.995621\n",
      "194 Train Loss 0.039201263 Test RE 2.884583161099242e-05 c 1.0009499 k 0.0099551445 m 4.995621\n",
      "195 Train Loss 0.039201263 Test RE 2.884583161099242e-05 c 1.0009499 k 0.0099551445 m 4.995621\n",
      "196 Train Loss 0.039201263 Test RE 2.884583161099242e-05 c 1.0009499 k 0.0099551445 m 4.995621\n",
      "197 Train Loss 0.039201263 Test RE 2.884583161099242e-05 c 1.0009499 k 0.0099551445 m 4.995621\n",
      "198 Train Loss 0.039201263 Test RE 2.884583161099242e-05 c 1.0009499 k 0.0099551445 m 4.995621\n",
      "199 Train Loss 0.039201263 Test RE 2.884583161099242e-05 c 1.0009499 k 0.0099551445 m 4.995621\n",
      "Training time: 87.62\n",
      "Training time: 87.62\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 211483.14 Test RE 0.1581840123573232 c -0.009230352 k 0.11255695 m 0.0010359316\n",
      "1 Train Loss 6566.418 Test RE 0.023294279826818535 c -0.009058616 k 0.03216677 m 0.0010905969\n",
      "2 Train Loss 3329.4097 Test RE 0.00863495407098998 c 0.006535895 k 0.030359305 m 0.0019470085\n",
      "3 Train Loss 2708.0623 Test RE 0.004007823840441925 c 0.037803084 k 0.03178538 m 0.003726184\n",
      "4 Train Loss 2094.597 Test RE 0.004718259216324931 c 0.20484424 k 0.027710661 m 0.013871687\n",
      "5 Train Loss 932.4512 Test RE 0.007171500123423049 c 0.74810344 k 0.014071653 m 0.047901124\n",
      "6 Train Loss 348.87018 Test RE 0.005154302206699517 c 1.0659902 k 0.007325394 m 0.06841612\n",
      "7 Train Loss 254.08984 Test RE 0.003485308733327226 c 1.1853651 k 0.0046204077 m 0.076626785\n",
      "8 Train Loss 245.72665 Test RE 0.003325118175388092 c 1.205779 k 0.0042159655 m 0.07900887\n",
      "9 Train Loss 240.19731 Test RE 0.0033011884979584362 c 1.1846836 k 0.004697963 m 0.08001614\n",
      "10 Train Loss 231.73349 Test RE 0.0028200279160491533 c 1.1884843 k 0.0046099825 m 0.086436346\n",
      "11 Train Loss 229.47577 Test RE 0.0026610131833150127 c 1.1921874 k 0.0045006587 m 0.09143252\n",
      "12 Train Loss 228.49338 Test RE 0.0026797303200332074 c 1.1867977 k 0.0046250974 m 0.09608977\n",
      "13 Train Loss 227.6169 Test RE 0.0026976958014674596 c 1.1828884 k 0.004722357 m 0.103559844\n",
      "14 Train Loss 224.76364 Test RE 0.002686871661403458 c 1.1794817 k 0.004831667 m 0.13838182\n",
      "15 Train Loss 221.52232 Test RE 0.0027294066272117697 c 1.1732339 k 0.005007327 m 0.19178611\n",
      "16 Train Loss 218.47354 Test RE 0.002639970796522421 c 1.1781876 k 0.004939543 m 0.20951332\n",
      "17 Train Loss 217.17714 Test RE 0.002695644795207573 c 1.1812786 k 0.004788462 m 0.20902367\n",
      "18 Train Loss 216.54173 Test RE 0.002701786659371042 c 1.1822586 k 0.004801027 m 0.2134766\n",
      "19 Train Loss 214.6808 Test RE 0.0026309736466131207 c 1.1798562 k 0.004878186 m 0.236462\n",
      "20 Train Loss 213.10161 Test RE 0.002561695628065967 c 1.1751221 k 0.0049643144 m 0.25750583\n",
      "21 Train Loss 212.00104 Test RE 0.0025010139490899805 c 1.1657287 k 0.005144788 m 0.29418853\n",
      "22 Train Loss 209.30208 Test RE 0.002409450132533448 c 1.1631577 k 0.0051746434 m 0.33981064\n",
      "23 Train Loss 208.03796 Test RE 0.0024265822006967327 c 1.1725236 k 0.0050010765 m 0.35716727\n",
      "24 Train Loss 207.2734 Test RE 0.002440808552377854 c 1.1818626 k 0.0047924486 m 0.3543743\n",
      "25 Train Loss 206.96439 Test RE 0.0024531781803214933 c 1.1810715 k 0.004795809 m 0.35864764\n",
      "26 Train Loss 204.82944 Test RE 0.0026346513964724035 c 1.17755 k 0.0048615607 m 0.44159588\n",
      "27 Train Loss 203.33463 Test RE 0.0025976938958901844 c 1.1770712 k 0.004880249 m 0.44884288\n",
      "28 Train Loss 202.57932 Test RE 0.002572198317461774 c 1.1740032 k 0.004968299 m 0.45550168\n",
      "29 Train Loss 200.72246 Test RE 0.0025181163951809023 c 1.1732423 k 0.005018678 m 0.48934942\n",
      "30 Train Loss 196.58794 Test RE 0.0023150565251275107 c 1.1741592 k 0.005083121 m 0.578955\n",
      "31 Train Loss 193.34064 Test RE 0.002115710078506514 c 1.1651003 k 0.00524849 m 0.6996877\n",
      "32 Train Loss 189.84613 Test RE 0.0021697811681190148 c 1.1650997 k 0.0052871252 m 0.7762013\n",
      "33 Train Loss 188.35834 Test RE 0.0023423461316144454 c 1.1625485 k 0.0053428784 m 0.810449\n",
      "34 Train Loss 186.6904 Test RE 0.0024188435968369683 c 1.1583229 k 0.005431646 m 0.82117176\n",
      "35 Train Loss 182.04831 Test RE 0.0023290905530171536 c 1.1652935 k 0.005319163 m 0.9449394\n",
      "36 Train Loss 178.55095 Test RE 0.0021273131763217485 c 1.1639005 k 0.0053411466 m 1.021147\n",
      "37 Train Loss 176.95787 Test RE 0.002044344044821624 c 1.1538824 k 0.0055388995 m 1.0352105\n",
      "38 Train Loss 175.66872 Test RE 0.0019286227893598393 c 1.1531135 k 0.005576912 m 1.0560433\n",
      "39 Train Loss 171.93604 Test RE 0.001708556957501985 c 1.1581945 k 0.005441663 m 1.1094631\n",
      "40 Train Loss 167.6682 Test RE 0.0018415336366036155 c 1.1534647 k 0.005577756 m 1.1628864\n",
      "41 Train Loss 151.83116 Test RE 0.0018140942588057544 c 1.1332997 k 0.006005742 m 1.4044267\n",
      "42 Train Loss 136.26257 Test RE 0.001769121469024196 c 1.1007416 k 0.006925883 m 1.7323487\n",
      "43 Train Loss 127.15747 Test RE 0.001436971203201512 c 1.1235108 k 0.0063567194 m 1.8282155\n",
      "44 Train Loss 121.958496 Test RE 0.0015541080097999295 c 1.1114318 k 0.00673558 m 1.9585514\n",
      "45 Train Loss 112.81917 Test RE 0.0017517005499903431 c 1.1014245 k 0.0069741 m 2.2464476\n",
      "46 Train Loss 105.29761 Test RE 0.0015172155755064768 c 1.1362956 k 0.006027282 m 2.3026161\n",
      "47 Train Loss 95.3551 Test RE 0.001525352242843579 c 1.1188732 k 0.0065361573 m 2.308342\n",
      "48 Train Loss 88.29776 Test RE 0.0015486845988320825 c 1.0963184 k 0.0073256344 m 2.4840567\n",
      "49 Train Loss 82.38284 Test RE 0.0012122665012740791 c 1.0893099 k 0.0073439754 m 2.6510057\n",
      "50 Train Loss 73.45928 Test RE 0.0011887247593250496 c 1.0821177 k 0.0076194233 m 2.840617\n",
      "51 Train Loss 67.18818 Test RE 0.0012123166273200394 c 1.0602382 k 0.008239227 m 3.1283467\n",
      "52 Train Loss 63.944748 Test RE 0.0010837189580518334 c 1.0638145 k 0.00811978 m 3.1810052\n",
      "53 Train Loss 61.716537 Test RE 0.0011599744808832712 c 1.0568724 k 0.008371793 m 3.1684783\n",
      "54 Train Loss 54.749153 Test RE 0.0011979138369009083 c 1.0442127 k 0.008690867 m 3.2650166\n",
      "55 Train Loss 52.210106 Test RE 0.0010932715817503014 c 1.0481088 k 0.00850266 m 3.351724\n",
      "56 Train Loss 49.198475 Test RE 0.0010654317097249005 c 1.0472986 k 0.008502818 m 3.479545\n",
      "57 Train Loss 45.884872 Test RE 0.0009971650588907741 c 1.0294989 k 0.00903393 m 3.5745895\n",
      "58 Train Loss 39.91168 Test RE 0.000852507291229336 c 1.0507437 k 0.00850228 m 3.6800988\n",
      "59 Train Loss 34.62097 Test RE 0.0007721333375672796 c 1.048337 k 0.008577955 m 3.768787\n",
      "60 Train Loss 33.781326 Test RE 0.0008728832781784164 c 1.0384423 k 0.008857288 m 3.786811\n",
      "61 Train Loss 31.786554 Test RE 0.0008287215976285416 c 1.0316923 k 0.009082239 m 3.893661\n",
      "62 Train Loss 26.39386 Test RE 0.00060006964893468 c 1.0328827 k 0.009109348 m 4.116014\n",
      "63 Train Loss 25.734615 Test RE 0.000650685450419577 c 1.0311456 k 0.009093155 m 4.1030974\n",
      "64 Train Loss 25.3715 Test RE 0.0006730331414865224 c 1.031587 k 0.00903294 m 4.118956\n",
      "65 Train Loss 24.07938 Test RE 0.0005907854326255412 c 1.0350647 k 0.008915936 m 4.173061\n",
      "66 Train Loss 22.857729 Test RE 0.0005249724601612362 c 1.0332558 k 0.009025094 m 4.1916485\n",
      "67 Train Loss 21.13171 Test RE 0.000537397489340031 c 1.02704 k 0.009237494 m 4.20592\n",
      "68 Train Loss 19.85918 Test RE 0.0005412345843684497 c 1.033519 k 0.009019224 m 4.182145\n",
      "69 Train Loss 18.327906 Test RE 0.0004586985040517177 c 1.0280112 k 0.009198509 m 4.220151\n",
      "70 Train Loss 17.451479 Test RE 0.0004266823337078787 c 1.0178676 k 0.009498585 m 4.3003416\n",
      "71 Train Loss 15.100787 Test RE 0.00043631046109743495 c 1.0063696 k 0.009723271 m 4.461921\n",
      "72 Train Loss 14.118902 Test RE 0.0004980322168816794 c 1.0195073 k 0.009376872 m 4.443688\n",
      "73 Train Loss 13.641352 Test RE 0.00047440935059790527 c 1.0221261 k 0.009300107 m 4.4375563\n",
      "74 Train Loss 13.488361 Test RE 0.00045201137620695246 c 1.0191818 k 0.009362971 m 4.4478445\n",
      "75 Train Loss 13.32939 Test RE 0.0004514562840426301 c 1.0175065 k 0.009393459 m 4.4546065\n",
      "76 Train Loss 12.808021 Test RE 0.0004737771194526155 c 1.0215073 k 0.009341474 m 4.482279\n",
      "77 Train Loss 11.007787 Test RE 0.00041364533704604914 c 1.0219853 k 0.009347999 m 4.5766954\n",
      "78 Train Loss 9.421877 Test RE 0.0004055125606378758 c 1.0130482 k 0.009550904 m 4.6568737\n",
      "79 Train Loss 8.896237 Test RE 0.000424899181926466 c 1.0128949 k 0.009556054 m 4.6801286\n",
      "80 Train Loss 8.472774 Test RE 0.00040025928098006786 c 1.0117424 k 0.009592437 m 4.742671\n",
      "81 Train Loss 7.821677 Test RE 0.00038764893647393343 c 1.0090078 k 0.009635186 m 4.82084\n",
      "82 Train Loss 7.560768 Test RE 0.00039448283918894423 c 1.0101516 k 0.009605769 m 4.8250456\n",
      "83 Train Loss 7.357523 Test RE 0.00039105799309687984 c 1.0075324 k 0.009680922 m 4.8424687\n",
      "84 Train Loss 7.074495 Test RE 0.0003930488403434944 c 1.0029947 k 0.009853135 m 4.88479\n",
      "85 Train Loss 6.907393 Test RE 0.00041012821360410684 c 1.0024647 k 0.00987598 m 4.890476\n",
      "86 Train Loss 6.759279 Test RE 0.0004114271799355749 c 1.0049827 k 0.009787695 m 4.8785343\n",
      "87 Train Loss 6.572856 Test RE 0.0003917168511997421 c 1.0063045 k 0.009756588 m 4.856936\n",
      "88 Train Loss 6.480405 Test RE 0.00038005315016750743 c 1.0054473 k 0.009794824 m 4.846908\n",
      "89 Train Loss 6.4440975 Test RE 0.0003758690897430699 c 1.0053006 k 0.009795887 m 4.8412194\n",
      "90 Train Loss 6.3073907 Test RE 0.0003570119846204882 c 1.0064029 k 0.009756726 m 4.8458257\n",
      "91 Train Loss 6.0099187 Test RE 0.0003340910544195678 c 1.0089924 k 0.009698642 m 4.8636217\n",
      "92 Train Loss 5.64505 Test RE 0.00033388773847855055 c 1.005533 k 0.0097999545 m 4.8762913\n",
      "93 Train Loss 5.530051 Test RE 0.00033185266389103784 c 1.0034611 k 0.009845794 m 4.8753505\n",
      "94 Train Loss 5.393344 Test RE 0.00032592019271358955 c 1.0037287 k 0.009833298 m 4.8704314\n",
      "95 Train Loss 5.271584 Test RE 0.0003224328423672725 c 1.0053115 k 0.009793165 m 4.882153\n",
      "96 Train Loss 5.115717 Test RE 0.0003261796738774334 c 1.0059205 k 0.009794503 m 4.923582\n",
      "97 Train Loss 4.7398095 Test RE 0.00030635966237102736 c 1.006791 k 0.009776099 m 4.9423046\n",
      "98 Train Loss 4.496913 Test RE 0.0002741711513133264 c 1.0061233 k 0.009806058 m 4.926725\n",
      "99 Train Loss 4.3393097 Test RE 0.0002588969516998461 c 1.003201 k 0.00992132 m 4.937758\n",
      "100 Train Loss 4.2302036 Test RE 0.000261888758007339 c 1.0005883 k 0.010001895 m 4.9569817\n",
      "101 Train Loss 4.001294 Test RE 0.00025182581861620586 c 0.9993506 k 0.010025626 m 5.0132775\n",
      "102 Train Loss 3.6127222 Test RE 0.00025108566720001573 c 1.0032401 k 0.009879888 m 5.0260444\n",
      "103 Train Loss 2.9709969 Test RE 0.00020790506789776603 c 0.99897724 k 0.00998819 m 5.01056\n",
      "104 Train Loss 2.8010643 Test RE 0.00016110048884016308 c 0.9972965 k 0.010051072 m 5.0201087\n",
      "105 Train Loss 2.7312546 Test RE 0.00012858106955159482 c 0.99828905 k 0.010028513 m 5.0423937\n",
      "106 Train Loss 2.5436308 Test RE 0.00011721336068016133 c 1.0009328 k 0.009961951 m 5.047298\n",
      "107 Train Loss 2.324609 Test RE 0.0001359717924322846 c 1.0022396 k 0.009894524 m 5.0158596\n",
      "108 Train Loss 2.2620432 Test RE 0.00013457237064494595 c 1.0015341 k 0.009906496 m 5.0049925\n",
      "109 Train Loss 2.2144165 Test RE 0.00013375026753844628 c 1.0006906 k 0.009924417 m 5.009556\n",
      "110 Train Loss 2.0336292 Test RE 0.0001456003372477303 c 1.0014468 k 0.009898537 m 5.0321875\n",
      "111 Train Loss 1.9282609 Test RE 0.0001561117434682798 c 1.0033028 k 0.009851559 m 5.031389\n",
      "112 Train Loss 1.7358377 Test RE 0.00013907662319055807 c 1.0035586 k 0.009841966 m 5.009446\n",
      "113 Train Loss 1.6207023 Test RE 0.00013063569823860375 c 1.0014486 k 0.009890748 m 5.0227995\n",
      "114 Train Loss 1.5780748 Test RE 0.00012656671380798223 c 1.0016104 k 0.009890268 m 5.0227757\n",
      "115 Train Loss 1.5473794 Test RE 0.00012372213960442916 c 1.0037563 k 0.009841047 m 5.0047083\n",
      "116 Train Loss 1.4836661 Test RE 0.00011552265465415182 c 1.0054256 k 0.00980546 m 4.9833474\n",
      "117 Train Loss 1.2770152 Test RE 0.0001008587813768776 c 1.0055664 k 0.00982527 m 4.9733863\n",
      "118 Train Loss 1.1558278 Test RE 0.00010604155491639881 c 1.001451 k 0.009933257 m 5.0029607\n",
      "119 Train Loss 1.1180973 Test RE 0.00010283164158817113 c 1.0012264 k 0.009941122 m 5.007307\n",
      "120 Train Loss 1.102738 Test RE 9.887290816002792e-05 c 1.0012276 k 0.009946829 m 5.0143647\n",
      "121 Train Loss 1.0876945 Test RE 0.00010055650894464935 c 1.0002211 k 0.009970433 m 5.0243177\n",
      "122 Train Loss 1.026998 Test RE 9.10043816977864e-05 c 0.9989463 k 0.010005 m 5.029839\n",
      "123 Train Loss 0.9406717 Test RE 7.651291977694272e-05 c 0.9999773 k 0.009988729 m 5.014422\n",
      "124 Train Loss 0.9114104 Test RE 7.523093080400014e-05 c 0.9993434 k 0.010009892 m 5.019937\n",
      "125 Train Loss 0.9002514 Test RE 7.880598642697147e-05 c 0.99910605 k 0.0100168465 m 5.023767\n",
      "126 Train Loss 0.8888179 Test RE 7.612884688015429e-05 c 0.9996433 k 0.010006004 m 5.01644\n",
      "127 Train Loss 0.8856831 Test RE 7.342267832869851e-05 c 0.9998207 k 0.010002713 m 5.0126934\n",
      "128 Train Loss 0.87817895 Test RE 7.159567009444065e-05 c 0.9996397 k 0.010007355 m 5.0126896\n",
      "129 Train Loss 0.8636394 Test RE 7.811022002941797e-05 c 0.99967074 k 0.010005237 m 5.013797\n",
      "130 Train Loss 0.8478585 Test RE 8.73908298562707e-05 c 1.0002155 k 0.009991041 m 5.0054526\n",
      "131 Train Loss 0.8135692 Test RE 9.393970638475562e-05 c 1.0009279 k 0.009977459 m 4.9971924\n",
      "132 Train Loss 0.7508786 Test RE 8.50408013129959e-05 c 1.0007732 k 0.009976837 m 4.9976573\n",
      "133 Train Loss 0.7165574 Test RE 8.324791858453982e-05 c 1.0008992 k 0.009964122 m 4.996054\n",
      "134 Train Loss 0.7089638 Test RE 8.541007112704867e-05 c 1.0011088 k 0.009958993 m 4.990737\n",
      "135 Train Loss 0.7050022 Test RE 8.413982559252759e-05 c 1.0011425 k 0.009960706 m 4.9870443\n",
      "136 Train Loss 0.6944341 Test RE 8.04963430463709e-05 c 1.0010285 k 0.009957579 m 4.9857836\n",
      "137 Train Loss 0.66920024 Test RE 8.035812142283747e-05 c 1.0009179 k 0.009957909 m 4.9884553\n",
      "138 Train Loss 0.58651423 Test RE 7.648640967226012e-05 c 1.0006287 k 0.0099774785 m 4.991428\n",
      "139 Train Loss 0.49706352 Test RE 6.270509255159218e-05 c 1.0002882 k 0.009990339 m 4.990611\n",
      "140 Train Loss 0.4796074 Test RE 5.296602392266569e-05 c 1.0006793 k 0.009978247 m 4.985517\n",
      "141 Train Loss 0.46773005 Test RE 5.023107699441243e-05 c 1.0008566 k 0.009975287 m 4.9867983\n",
      "142 Train Loss 0.4632006 Test RE 5.073641020450714e-05 c 1.000763 k 0.009978443 m 4.987461\n",
      "143 Train Loss 0.45882455 Test RE 5.4024422042126646e-05 c 1.0004294 k 0.009987606 m 4.984584\n",
      "144 Train Loss 0.45176834 Test RE 5.450925140404142e-05 c 1.0003325 k 0.009987808 m 4.9777007\n",
      "145 Train Loss 0.44978747 Test RE 5.2904799844343813e-05 c 1.0003872 k 0.009984878 m 4.977426\n",
      "146 Train Loss 0.4483857 Test RE 5.119083491235142e-05 c 1.0003884 k 0.009986008 m 4.9786763\n",
      "147 Train Loss 0.44780132 Test RE 5.064554587762714e-05 c 1.0003883 k 0.009986661 m 4.979335\n",
      "148 Train Loss 0.4433765 Test RE 4.966592336336636e-05 c 1.0007684 k 0.009981538 m 4.979931\n",
      "149 Train Loss 0.44097972 Test RE 5.003658913205403e-05 c 1.0010833 k 0.009973327 m 4.9771423\n",
      "150 Train Loss 0.43990067 Test RE 5.008349669943715e-05 c 1.0011858 k 0.009970346 m 4.9757733\n",
      "151 Train Loss 0.43757617 Test RE 4.934562148492415e-05 c 1.0011977 k 0.009968942 m 4.975196\n",
      "152 Train Loss 0.43105963 Test RE 4.590346211366515e-05 c 1.0006139 k 0.009983446 m 4.9832673\n",
      "153 Train Loss 0.42918903 Test RE 4.5839529741475376e-05 c 1.0002477 k 0.009993754 m 4.9883246\n",
      "154 Train Loss 0.427625 Test RE 4.701559951756831e-05 c 1.0000063 k 0.010000583 m 4.99011\n",
      "155 Train Loss 0.42692977 Test RE 4.740845986525538e-05 c 0.9999594 k 0.0100011 m 4.9893312\n",
      "156 Train Loss 0.42482802 Test RE 4.676490814797615e-05 c 0.9999604 k 0.009999274 m 4.9861426\n",
      "157 Train Loss 0.42253467 Test RE 4.505138862534142e-05 c 1.0000778 k 0.009995788 m 4.985136\n",
      "158 Train Loss 0.4202181 Test RE 4.337183103681808e-05 c 1.000264 k 0.009991784 m 4.986581\n",
      "159 Train Loss 0.41808763 Test RE 4.2961038404665486e-05 c 1.0003489 k 0.00999076 m 4.989834\n",
      "160 Train Loss 0.40961435 Test RE 4.6795966136612995e-05 c 1.0006412 k 0.009983376 m 4.991774\n",
      "161 Train Loss 0.40302742 Test RE 4.55133991611816e-05 c 1.0008129 k 0.0099791465 m 4.989048\n",
      "162 Train Loss 0.40034643 Test RE 4.486918970517273e-05 c 1.0009507 k 0.009975368 m 4.9868\n",
      "163 Train Loss 0.39511794 Test RE 4.282460897505865e-05 c 1.0011278 k 0.009971112 m 4.9840193\n",
      "164 Train Loss 0.38867563 Test RE 4.324010271579137e-05 c 1.0008966 k 0.009975988 m 4.98504\n",
      "165 Train Loss 0.38477245 Test RE 4.36446873192988e-05 c 1.0006042 k 0.009981797 m 4.982874\n",
      "166 Train Loss 0.3791143 Test RE 4.409532826711151e-05 c 1.0003973 k 0.009983783 m 4.9788375\n",
      "167 Train Loss 0.3722157 Test RE 4.2534671746701934e-05 c 1.0001928 k 0.009985739 m 4.9777737\n",
      "168 Train Loss 0.3682898 Test RE 3.9508882739293854e-05 c 0.99998575 k 0.00999189 m 4.976616\n",
      "169 Train Loss 0.36325988 Test RE 3.6839684105524966e-05 c 1.0002091 k 0.009983498 m 4.974378\n",
      "170 Train Loss 0.35772952 Test RE 3.865375060770299e-05 c 1.0006571 k 0.0099767735 m 4.975539\n",
      "171 Train Loss 0.35288972 Test RE 4.0807677295637364e-05 c 1.000795 k 0.009975103 m 4.976732\n",
      "172 Train Loss 0.3480157 Test RE 3.765501537174289e-05 c 1.000704 k 0.009978936 m 4.9792347\n",
      "173 Train Loss 0.34666166 Test RE 3.6823759632784744e-05 c 1.0008503 k 0.00997448 m 4.9800735\n",
      "174 Train Loss 0.34578383 Test RE 3.7415298786115193e-05 c 1.0009699 k 0.009971116 m 4.9802084\n",
      "175 Train Loss 0.34330955 Test RE 3.92881534040963e-05 c 1.0010529 k 0.009968575 m 4.9813175\n",
      "176 Train Loss 0.3401454 Test RE 3.955708812442942e-05 c 1.0008771 k 0.009973846 m 4.984482\n",
      "177 Train Loss 0.33565834 Test RE 3.853381329289762e-05 c 1.0005763 k 0.009983951 m 4.9907064\n",
      "178 Train Loss 0.33391818 Test RE 3.7959381487514816e-05 c 1.0006659 k 0.009981371 m 4.991696\n",
      "179 Train Loss 0.33159822 Test RE 3.784359261022621e-05 c 1.0006902 k 0.009980373 m 4.9924064\n",
      "180 Train Loss 0.3291707 Test RE 3.771780080745347e-05 c 1.0005125 k 0.009986479 m 4.9909945\n",
      "181 Train Loss 0.3291707 Test RE 3.771780080745347e-05 c 1.0005125 k 0.009986479 m 4.9909945\n",
      "182 Train Loss 0.3291707 Test RE 3.771780080745347e-05 c 1.0005125 k 0.009986479 m 4.9909945\n",
      "183 Train Loss 0.3291707 Test RE 3.771780080745347e-05 c 1.0005125 k 0.009986479 m 4.9909945\n",
      "184 Train Loss 0.3291707 Test RE 3.771780080745347e-05 c 1.0005125 k 0.009986479 m 4.9909945\n",
      "185 Train Loss 0.3291707 Test RE 3.771780080745347e-05 c 1.0005125 k 0.009986479 m 4.9909945\n",
      "186 Train Loss 0.3291707 Test RE 3.771780080745347e-05 c 1.0005125 k 0.009986479 m 4.9909945\n",
      "187 Train Loss 0.3291707 Test RE 3.771780080745347e-05 c 1.0005125 k 0.009986479 m 4.9909945\n",
      "188 Train Loss 0.3291707 Test RE 3.771780080745347e-05 c 1.0005125 k 0.009986479 m 4.9909945\n",
      "189 Train Loss 0.3291707 Test RE 3.771780080745347e-05 c 1.0005125 k 0.009986479 m 4.9909945\n",
      "190 Train Loss 0.3291707 Test RE 3.771780080745347e-05 c 1.0005125 k 0.009986479 m 4.9909945\n",
      "191 Train Loss 0.3291707 Test RE 3.771780080745347e-05 c 1.0005125 k 0.009986479 m 4.9909945\n",
      "192 Train Loss 0.3291707 Test RE 3.771780080745347e-05 c 1.0005125 k 0.009986479 m 4.9909945\n",
      "193 Train Loss 0.3291707 Test RE 3.771780080745347e-05 c 1.0005125 k 0.009986479 m 4.9909945\n",
      "194 Train Loss 0.3291707 Test RE 3.771780080745347e-05 c 1.0005125 k 0.009986479 m 4.9909945\n",
      "195 Train Loss 0.3291707 Test RE 3.771780080745347e-05 c 1.0005125 k 0.009986479 m 4.9909945\n",
      "196 Train Loss 0.3291707 Test RE 3.771780080745347e-05 c 1.0005125 k 0.009986479 m 4.9909945\n",
      "197 Train Loss 0.3291707 Test RE 3.771780080745347e-05 c 1.0005125 k 0.009986479 m 4.9909945\n",
      "198 Train Loss 0.3291707 Test RE 3.771780080745347e-05 c 1.0005125 k 0.009986479 m 4.9909945\n",
      "199 Train Loss 0.3291707 Test RE 3.771780080745347e-05 c 1.0005125 k 0.009986479 m 4.9909945\n",
      "Training time: 100.91\n",
      "Training time: 100.91\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 884665.44 Test RE 0.3578669550577414 c 0.003774222 k 0.051916312 m -6.141229e-05\n",
      "1 Train Loss 402703.16 Test RE 0.2401928392865737 c 0.0068856524 k 0.02150049 m -9.0920126e-05\n",
      "2 Train Loss 14769.48 Test RE 0.031354911834082906 c 0.0038963736 k 0.03395274 m -0.00011199065\n",
      "3 Train Loss 14700.783 Test RE 0.03124640874634623 c 0.0057625985 k 0.03138816 m -0.0001060783\n",
      "4 Train Loss 14208.0 Test RE 0.030424406011196194 c 0.03754177 k 0.027555244 m -7.3480073e-06\n",
      "5 Train Loss 13087.569 Test RE 0.028308560415304676 c 0.075061336 k 0.032948878 m 0.000106543164\n",
      "6 Train Loss 12882.05 Test RE 0.02787904105409273 c 0.1037067 k 0.029646207 m 0.00021058947\n",
      "7 Train Loss 11685.979 Test RE 0.02733459527462468 c 0.6948219 k 0.005676469 m 0.0023060448\n",
      "8 Train Loss 9663.826 Test RE 0.025874334162834117 c 1.573108 k -0.008240422 m 0.0054170913\n",
      "9 Train Loss 9612.023 Test RE 0.025844199191767784 c 1.5794078 k -0.007002616 m 0.005442937\n",
      "10 Train Loss 9590.206 Test RE 0.025841850057106718 c 1.5741398 k -0.0063529583 m 0.00543595\n",
      "11 Train Loss 9426.479 Test RE 0.02593448878489692 c 1.5355659 k -0.0040045194 m 0.0054278\n",
      "12 Train Loss 9067.0625 Test RE 0.02581239423863031 c 1.4803475 k -0.004538906 m 0.0054085543\n",
      "13 Train Loss 8828.391 Test RE 0.02584229755382799 c 1.810043 k -0.015182542 m 0.006818\n",
      "14 Train Loss 7409.8286 Test RE 0.023174044015386152 c 1.7754192 k -0.007454753 m 0.0070145265\n",
      "15 Train Loss 6626.227 Test RE 0.021782602043853194 c 1.4975355 k -0.0076322565 m 0.0061586043\n",
      "16 Train Loss 3231.0295 Test RE 0.01657661863531863 c 1.6146002 k -0.008405225 m 0.006859308\n",
      "17 Train Loss 2181.8906 Test RE 0.012322014266041997 c 1.8519372 k -0.010774375 m 0.007871536\n",
      "18 Train Loss 1665.7843 Test RE 0.00932795816977931 c 1.9299341 k -0.014564031 m 0.008351024\n",
      "19 Train Loss 1341.1196 Test RE 0.009372782073599666 c 1.7556177 k -0.00931072 m 0.007932745\n",
      "20 Train Loss 722.2022 Test RE 0.007758702239020124 c 1.2654284 k 0.0022867245 m 0.0066353213\n",
      "21 Train Loss 369.65396 Test RE 0.003908357872924972 c 1.1613665 k 0.0049558193 m 0.006863003\n",
      "22 Train Loss 342.85266 Test RE 0.004136414011852603 c 1.1742513 k 0.0048002335 m 0.007736912\n",
      "23 Train Loss 331.58167 Test RE 0.004131954823296634 c 1.1781774 k 0.0049485993 m 0.008994414\n",
      "24 Train Loss 330.34283 Test RE 0.004135218022147768 c 1.1801997 k 0.0048613907 m 0.00946705\n",
      "25 Train Loss 325.4357 Test RE 0.004105337843451656 c 1.1853229 k 0.0046336153 m 0.011348126\n",
      "26 Train Loss 312.6555 Test RE 0.003867983019835563 c 1.1822219 k 0.0045781494 m 0.016105793\n",
      "27 Train Loss 310.9281 Test RE 0.0037657309167186646 c 1.1825218 k 0.004614162 m 0.016781602\n",
      "28 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "29 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "30 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "31 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "32 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "33 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "34 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "35 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "36 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "37 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "38 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "39 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "40 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "41 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "42 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "43 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "44 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "45 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "46 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "47 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "48 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "49 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "50 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "51 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "52 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "53 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "54 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "55 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "56 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "57 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "58 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "59 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "60 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "61 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "62 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "63 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "64 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "65 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "66 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "67 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "68 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "69 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "70 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "71 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "72 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "73 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "74 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "75 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "76 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "77 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "78 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "79 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "80 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "81 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "82 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "83 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "84 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "85 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "86 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "87 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "88 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "89 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "90 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "91 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "92 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "93 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "94 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "95 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "96 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "97 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "98 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "99 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "100 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "101 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "102 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "103 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "104 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "105 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "106 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "107 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "108 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "109 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "110 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "111 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "112 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "113 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "114 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "115 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "116 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "117 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "118 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "119 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "120 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "121 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "122 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "123 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "124 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "125 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "126 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "127 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "128 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "129 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "130 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "131 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "132 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "133 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "134 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "135 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "136 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "137 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "138 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "139 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "140 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "141 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "142 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "143 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "144 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "145 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "146 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "147 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "148 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "149 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "150 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "151 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "152 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "153 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "154 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "155 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "156 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "157 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "158 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "159 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "160 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "161 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "162 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "163 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "164 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "165 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "166 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "167 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "168 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "169 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "170 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "171 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "172 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "173 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "174 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "175 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "176 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "177 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "178 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "179 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "180 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "181 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "182 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "183 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "184 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "185 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "186 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "187 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "188 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "189 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "190 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "191 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "192 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "193 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "194 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "195 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "196 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "197 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "198 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "199 Train Loss 310.8432 Test RE 0.0037542375467771454 c 1.1844014 k 0.0045807306 m 0.01675258\n",
      "Training time: 52.64\n",
      "Training time: 52.64\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 1263819.1 Test RE 0.4272787612065337 c -0.04773984 k 0.07128139 m 0.00050135655\n",
      "1 Train Loss 579175.4 Test RE 0.2579930800476764 c -0.0665315 k -0.12436786 m 0.0007217467\n",
      "2 Train Loss 12384.191 Test RE 0.020016509283577573 c -0.06569894 k 0.035691436 m 0.0007741971\n",
      "3 Train Loss 11997.074 Test RE 0.01948586436218295 c -0.06321058 k 0.03483139 m 0.00076908374\n",
      "4 Train Loss 11956.082 Test RE 0.019491870622088864 c -0.05548166 k 0.03274954 m 0.00076205813\n",
      "5 Train Loss 11608.246 Test RE 0.01951580276151539 c 0.026668178 k 0.027790299 m 0.00068871875\n",
      "6 Train Loss 10238.833 Test RE 0.01750701368296118 c 0.25485724 k 0.029976115 m 0.0004951057\n",
      "7 Train Loss 8075.237 Test RE 0.014438945430532377 c 1.0977032 k 0.0054358416 m -0.00020042408\n",
      "8 Train Loss 8040.6304 Test RE 0.014218650581773767 c 1.1750995 k 0.00431439 m -0.00026393554\n",
      "9 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "10 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "11 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "12 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "13 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "14 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "15 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "16 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "17 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "18 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "19 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "20 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "21 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "22 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "23 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "24 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "25 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "26 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "27 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "28 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "29 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "30 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "31 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "32 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "33 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "34 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "35 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "36 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "37 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "38 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "39 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "40 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "41 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "42 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "43 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "44 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "45 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "46 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "47 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "48 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "49 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "50 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "51 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "52 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "53 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "54 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "55 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "56 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "57 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "58 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "59 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "60 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "61 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "62 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "63 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "64 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "65 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "66 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "67 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "68 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "69 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "70 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "71 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "72 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "73 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "74 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "75 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "76 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "77 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "78 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "79 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "80 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "81 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "82 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "83 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "84 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "85 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "86 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "87 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "88 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "89 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "90 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "91 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "92 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "93 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "94 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "95 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "96 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "97 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "98 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "99 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "100 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "101 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "102 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "103 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "104 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "105 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "106 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "107 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "108 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "109 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "110 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "111 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "112 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "113 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "114 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "115 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "116 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "117 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "118 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "119 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "120 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "121 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "122 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "123 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "124 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "125 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "126 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "127 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "128 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "129 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "130 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "131 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "132 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "133 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "134 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "135 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "136 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "137 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "138 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "139 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "140 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "141 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "142 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "143 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "144 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "145 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "146 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "147 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "148 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "149 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "150 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "151 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "152 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "153 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "154 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "155 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "156 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "157 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "158 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "159 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "160 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "161 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "162 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "163 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "164 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "165 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "166 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "167 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "168 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "169 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "170 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "171 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "172 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "173 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "174 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "175 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "176 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "177 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "178 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "179 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "180 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "181 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "182 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "183 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "184 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "185 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "186 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "187 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "188 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "189 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "190 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "191 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "192 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "193 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "194 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "195 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "196 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "197 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "198 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "199 Train Loss 8039.389 Test RE 0.014197250315209766 c 1.1817786 k 0.0043595484 m -0.0002693758\n",
      "Training time: 39.89\n",
      "Training time: 39.89\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 801755.2 Test RE 0.3407118180347052 c 0.019640356 k 0.049194828 m -4.92284e-05\n",
      "1 Train Loss 22997.723 Test RE 0.0533465236532326 c 0.020135224 k 0.03218046 m 0.00012883259\n",
      "2 Train Loss 17493.525 Test RE 0.04521762027698715 c 0.03229734 k 0.030880524 m 0.000736594\n",
      "3 Train Loss 13120.8955 Test RE 0.03837602030349276 c 0.068624735 k 0.032542773 m 0.002489111\n",
      "4 Train Loss 5481.7783 Test RE 0.021182875959517206 c 0.29433277 k 0.023599286 m 0.013809478\n",
      "5 Train Loss 2558.0989 Test RE 0.011585382035379455 c 0.6631548 k 0.015583527 m 0.032288816\n",
      "6 Train Loss 2061.251 Test RE 0.010103374009015266 c 0.9501657 k 0.009844519 m 0.046692442\n",
      "7 Train Loss 1868.4938 Test RE 0.00836769559965495 c 1.1565201 k 0.004898071 m 0.05724809\n",
      "8 Train Loss 1828.7723 Test RE 0.0070938211692773276 c 1.2381349 k 0.0030286259 m 0.06160628\n",
      "9 Train Loss 1732.2324 Test RE 0.0066035995722936915 c 1.175266 k 0.0043077394 m 0.060517475\n",
      "10 Train Loss 1428.1729 Test RE 0.007603676869655574 c 1.0954748 k 0.0060951337 m 0.06913971\n",
      "11 Train Loss 807.3828 Test RE 0.006300720515315873 c 1.2384115 k 0.0031029284 m 0.09316288\n",
      "12 Train Loss 575.84485 Test RE 0.005295806720219252 c 1.2680676 k 0.0022784614 m 0.10808907\n",
      "13 Train Loss 526.52673 Test RE 0.004743151221481939 c 1.2311497 k 0.0031599614 m 0.11093395\n",
      "14 Train Loss 506.62305 Test RE 0.004891259114723236 c 1.1937253 k 0.0040690443 m 0.1082734\n",
      "15 Train Loss 498.28925 Test RE 0.004634583519182731 c 1.2206215 k 0.0034343468 m 0.11471719\n",
      "16 Train Loss 492.66068 Test RE 0.00442503826793927 c 1.2337837 k 0.0033815925 m 0.12634751\n",
      "17 Train Loss 457.71973 Test RE 0.004021649591687907 c 1.1570867 k 0.00547057 m 0.20361766\n",
      "18 Train Loss 452.4754 Test RE 0.004046101252205222 c 1.1318501 k 0.005802243 m 0.21683507\n",
      "19 Train Loss 451.38812 Test RE 0.004063116166714144 c 1.1230434 k 0.0058919964 m 0.2193401\n",
      "20 Train Loss 444.84802 Test RE 0.004001675348350298 c 1.1097188 k 0.0061635436 m 0.22998834\n",
      "21 Train Loss 441.14816 Test RE 0.003942159072521008 c 1.1237785 k 0.0060674422 m 0.24373896\n",
      "22 Train Loss 440.15958 Test RE 0.0039696499361431955 c 1.1319587 k 0.005924321 m 0.25462124\n",
      "23 Train Loss 434.97876 Test RE 0.004102987855431554 c 1.1444486 k 0.005562463 m 0.29714978\n",
      "24 Train Loss 395.28592 Test RE 0.004120931052729036 c 1.1595687 k 0.0048753056 m 0.545667\n",
      "25 Train Loss 319.26517 Test RE 0.0027508835833170704 c 1.1421384 k 0.005953508 m 0.6118593\n",
      "26 Train Loss 311.97638 Test RE 0.0026235433667412913 c 1.1518003 k 0.005498174 m 0.6211357\n",
      "27 Train Loss 311.3573 Test RE 0.0026468318418184545 c 1.1511134 k 0.0054689837 m 0.6229614\n",
      "28 Train Loss 309.60077 Test RE 0.0026980480589082942 c 1.1526535 k 0.0054654577 m 0.60839576\n",
      "29 Train Loss 307.83987 Test RE 0.0027137352109741155 c 1.1571517 k 0.005433818 m 0.57727534\n",
      "30 Train Loss 289.65775 Test RE 0.0029401295702425833 c 1.1762687 k 0.0048035393 m 0.43468747\n",
      "31 Train Loss 257.6543 Test RE 0.0029095595724306787 c 1.1680102 k 0.005102061 m 0.4262411\n",
      "32 Train Loss 254.94922 Test RE 0.0030343970142466784 c 1.162064 k 0.0051957443 m 0.4299507\n",
      "33 Train Loss 248.36966 Test RE 0.003316005287803869 c 1.1703146 k 0.0049856794 m 0.3236252\n",
      "34 Train Loss 247.40163 Test RE 0.0034029604786859026 c 1.1735381 k 0.004915118 m 0.28629285\n",
      "35 Train Loss 247.07285 Test RE 0.0033988247754602203 c 1.1773856 k 0.004817986 m 0.26063573\n",
      "36 Train Loss 246.61404 Test RE 0.003350351509582431 c 1.1773833 k 0.0047985497 m 0.22162326\n",
      "37 Train Loss 246.11398 Test RE 0.0033017747533636236 c 1.1740644 k 0.004900481 m 0.20748435\n",
      "38 Train Loss 245.92087 Test RE 0.0032913562528814 c 1.1741887 k 0.004932975 m 0.19864354\n",
      "39 Train Loss 245.70795 Test RE 0.0032710115901185407 c 1.1762712 k 0.0048913104 m 0.19229443\n",
      "40 Train Loss 243.96658 Test RE 0.0031960420719335182 c 1.1765218 k 0.0048884144 m 0.21422459\n",
      "41 Train Loss 242.47206 Test RE 0.003226420821415167 c 1.1689283 k 0.0050419257 m 0.2312096\n",
      "42 Train Loss 240.06912 Test RE 0.0032661240765307275 c 1.1779888 k 0.0048409235 m 0.23422815\n",
      "43 Train Loss 237.43744 Test RE 0.0030097664956342247 c 1.1792207 k 0.00476847 m 0.22937265\n",
      "44 Train Loss 235.17044 Test RE 0.002869774748536174 c 1.165432 k 0.0051416946 m 0.2085234\n",
      "45 Train Loss 232.92838 Test RE 0.002873663134476805 c 1.1800995 k 0.0048334855 m 0.19257781\n",
      "46 Train Loss 231.70769 Test RE 0.002851934348537732 c 1.1782508 k 0.0048421673 m 0.18480076\n",
      "47 Train Loss 230.2684 Test RE 0.002794930981467581 c 1.174105 k 0.0049922024 m 0.17135465\n",
      "48 Train Loss 228.58865 Test RE 0.0027865417279324713 c 1.1813803 k 0.004802358 m 0.17838702\n",
      "49 Train Loss 226.91812 Test RE 0.002898276798831609 c 1.1827577 k 0.0047706906 m 0.20242046\n",
      "50 Train Loss 225.52345 Test RE 0.0030596706312858166 c 1.1872354 k 0.004679336 m 0.23390992\n",
      "51 Train Loss 224.41425 Test RE 0.0029239483151044026 c 1.1784238 k 0.0049082185 m 0.25989968\n",
      "52 Train Loss 224.21243 Test RE 0.002897011379261429 c 1.1783181 k 0.0049025356 m 0.27420133\n",
      "53 Train Loss 224.13965 Test RE 0.0029118838126715387 c 1.1807756 k 0.0048424015 m 0.28512746\n",
      "54 Train Loss 223.77759 Test RE 0.002915365055293541 c 1.182999 k 0.004767318 m 0.2965632\n",
      "55 Train Loss 223.66255 Test RE 0.0029019787452682667 c 1.1798611 k 0.0048367167 m 0.29560566\n",
      "56 Train Loss 223.63846 Test RE 0.0029046482470691344 c 1.178927 k 0.0048593846 m 0.29769614\n",
      "57 Train Loss 223.55185 Test RE 0.00293076037755376 c 1.1799393 k 0.0048378035 m 0.3122613\n",
      "58 Train Loss 223.0112 Test RE 0.0029236074401035963 c 1.179174 k 0.0048581036 m 0.35915732\n",
      "59 Train Loss 222.29813 Test RE 0.0028309943005037 c 1.1710415 k 0.005066772 m 0.37369028\n",
      "60 Train Loss 221.82407 Test RE 0.00276306036197719 c 1.17328 k 0.0050096894 m 0.3557713\n",
      "61 Train Loss 220.94524 Test RE 0.0027219132231350103 c 1.1761578 k 0.004947243 m 0.39127925\n",
      "62 Train Loss 220.06006 Test RE 0.002677670934473053 c 1.1732051 k 0.005002228 m 0.41703528\n",
      "63 Train Loss 219.72038 Test RE 0.0026185230618379837 c 1.1709921 k 0.0050497223 m 0.42860207\n",
      "64 Train Loss 219.0571 Test RE 0.0025643880826713176 c 1.1714426 k 0.005069451 m 0.4529289\n",
      "65 Train Loss 217.49948 Test RE 0.0026316909714051233 c 1.1713623 k 0.005100999 m 0.51682246\n",
      "66 Train Loss 216.28755 Test RE 0.002694633533482495 c 1.1708976 k 0.0051163263 m 0.5534922\n",
      "67 Train Loss 213.38795 Test RE 0.0026457219506988744 c 1.1733055 k 0.005088397 m 0.6502853\n",
      "68 Train Loss 209.65482 Test RE 0.002572563757210993 c 1.1540166 k 0.005571086 m 0.78313303\n",
      "69 Train Loss 206.21976 Test RE 0.00235464364851295 c 1.1463946 k 0.005780363 m 0.8763892\n",
      "70 Train Loss 204.762 Test RE 0.0023889205642419788 c 1.1558899 k 0.0055597266 m 0.89842284\n",
      "71 Train Loss 202.56189 Test RE 0.00233382634165491 c 1.1458424 k 0.00583583 m 0.94665235\n",
      "72 Train Loss 195.82408 Test RE 0.0022806640991276145 c 1.1346086 k 0.0062023727 m 1.3051324\n",
      "73 Train Loss 184.82506 Test RE 0.0022819875787024506 c 1.122782 k 0.0065456005 m 2.0219321\n",
      "74 Train Loss 146.69498 Test RE 0.0024335787358511145 c 1.0783412 k 0.007869393 m 3.302492\n",
      "75 Train Loss 136.9074 Test RE 0.002330207768171267 c 1.0809191 k 0.0077777184 m 3.3925524\n",
      "76 Train Loss 131.61378 Test RE 0.0022175413835565628 c 1.0756203 k 0.007835893 m 3.2354066\n",
      "77 Train Loss 124.40953 Test RE 0.0020623618853424642 c 1.0680151 k 0.008035696 m 3.2531896\n",
      "78 Train Loss 114.67227 Test RE 0.0019318063430099263 c 1.0892549 k 0.0073516374 m 3.241093\n",
      "79 Train Loss 104.704025 Test RE 0.002053898369776888 c 1.0634427 k 0.008018159 m 3.3663561\n",
      "80 Train Loss 90.32495 Test RE 0.001871429108206465 c 1.0618362 k 0.007984643 m 3.8990772\n",
      "81 Train Loss 66.90878 Test RE 0.001280646633974451 c 1.0248007 k 0.009105453 m 4.712572\n",
      "82 Train Loss 62.049118 Test RE 0.001356946434649762 c 1.0098891 k 0.009567001 m 4.981599\n",
      "83 Train Loss 44.952026 Test RE 0.0010932366128349575 c 1.0149325 k 0.009433962 m 5.310516\n",
      "84 Train Loss 40.07379 Test RE 0.0010009391444481788 c 0.97645265 k 0.010384372 m 5.5172014\n",
      "85 Train Loss 35.316765 Test RE 0.0008302037801979531 c 0.966089 k 0.010733488 m 5.8958006\n",
      "86 Train Loss 24.369698 Test RE 0.00081982390509978 c 0.9758491 k 0.010607734 m 5.966531\n",
      "87 Train Loss 10.975255 Test RE 0.0005325765748667428 c 0.98783875 k 0.010249931 m 5.3140893\n",
      "88 Train Loss 8.500135 Test RE 0.0004281143667402205 c 0.99888974 k 0.009959695 m 5.055757\n",
      "89 Train Loss 8.012411 Test RE 0.00041931672518981795 c 1.0005385 k 0.009904248 m 5.002035\n",
      "90 Train Loss 7.7096987 Test RE 0.0004449448753780234 c 1.0005769 k 0.009892887 m 4.9933367\n",
      "91 Train Loss 7.32318 Test RE 0.00044281083620160446 c 0.99736285 k 0.010004318 m 5.0509377\n",
      "92 Train Loss 7.2435803 Test RE 0.00044256073387124957 c 0.99643636 k 0.010033152 m 5.0692544\n",
      "93 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "94 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "95 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "96 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "97 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "98 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "99 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "100 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "101 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "102 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "103 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "104 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "105 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "106 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "107 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "108 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "109 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "110 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "111 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "112 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "113 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "114 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "115 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "116 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "117 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "118 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "119 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "120 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "121 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "122 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "123 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "124 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "125 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "126 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "127 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "128 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "129 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "130 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "131 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "132 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "133 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "134 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "135 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "136 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "137 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "138 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "139 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "140 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "141 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "142 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "143 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "144 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "145 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "146 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "147 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "148 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "149 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "150 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "151 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "152 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "153 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "154 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "155 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "156 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "157 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "158 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "159 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "160 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "161 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "162 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "163 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "164 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "165 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "166 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "167 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "168 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "169 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "170 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "171 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "172 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "173 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "174 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "175 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "176 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "177 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "178 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "179 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "180 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "181 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "182 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "183 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "184 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "185 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "186 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "187 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "188 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "189 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "190 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "191 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "192 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "193 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "194 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "195 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "196 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "197 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "198 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "199 Train Loss 7.231442 Test RE 0.0004454150778216353 c 0.9966516 k 0.010029229 m 5.0726376\n",
      "Training time: 74.13\n",
      "Training time: 74.13\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 976930.5 Test RE 0.37598152088164916 c 0.01670226 k 0.057228357 m 9.577464e-05\n",
      "1 Train Loss 132180.28 Test RE 0.04872776714318448 c 0.0309112 k -0.09623973 m 7.1601964e-05\n",
      "2 Train Loss 10368.561 Test RE 0.03345756652076128 c 0.06367341 k 0.028496433 m 0.00043557188\n",
      "3 Train Loss 9963.201 Test RE 0.03274558260545851 c 0.075950235 k 0.032579836 m 0.0011643084\n",
      "4 Train Loss 6064.1855 Test RE 0.024924962665975617 c 0.34809324 k 0.02934135 m 0.017830666\n",
      "5 Train Loss 3658.0796 Test RE 0.020036812667853548 c 0.6227571 k 0.016529959 m 0.03528883\n",
      "6 Train Loss 1638.1204 Test RE 0.014372175176645043 c 1.2318941 k 0.000651994 m 0.080857664\n",
      "7 Train Loss 1475.1748 Test RE 0.013591136811135962 c 1.2342575 k 0.0033671416 m 0.08301455\n",
      "8 Train Loss 1336.6779 Test RE 0.012826792850066979 c 1.2708635 k 0.0011666014 m 0.09121553\n",
      "9 Train Loss 1060.0723 Test RE 0.010673027171333983 c 1.4035734 k -0.0030405899 m 0.12845889\n",
      "10 Train Loss 864.54224 Test RE 0.009446974616495303 c 1.3633367 k 0.000117308984 m 0.14520255\n",
      "11 Train Loss 585.03455 Test RE 0.00658778601759541 c 1.3001695 k 0.0013011494 m 0.18154396\n",
      "12 Train Loss 383.23657 Test RE 0.004087125516204714 c 1.2366521 k 0.0029647055 m 0.2571353\n",
      "13 Train Loss 309.51547 Test RE 0.0026857038612144125 c 1.1897582 k 0.004482752 m 0.3681278\n",
      "14 Train Loss 281.96457 Test RE 0.00285567977494655 c 1.1634064 k 0.00529131 m 0.436998\n",
      "15 Train Loss 259.3403 Test RE 0.002248145460361444 c 1.1374887 k 0.0058444967 m 0.4570504\n",
      "16 Train Loss 245.16573 Test RE 0.0022681296504768553 c 1.1655501 k 0.0051809265 m 0.5059379\n",
      "17 Train Loss 243.57726 Test RE 0.0023339242353309724 c 1.161923 k 0.0052468576 m 0.525695\n",
      "18 Train Loss 241.07509 Test RE 0.0024191797838040803 c 1.162902 k 0.005363863 m 0.546437\n",
      "19 Train Loss 233.40385 Test RE 0.002326296526001453 c 1.1608647 k 0.0052687745 m 0.5850442\n",
      "20 Train Loss 220.68333 Test RE 0.0023246891302283533 c 1.163023 k 0.0051232534 m 0.7217803\n",
      "21 Train Loss 216.93909 Test RE 0.0023278508660645675 c 1.1671857 k 0.0051754704 m 0.8079499\n",
      "22 Train Loss 215.91205 Test RE 0.002259945796139963 c 1.1634556 k 0.0053166677 m 0.84710413\n",
      "23 Train Loss 214.71497 Test RE 0.002219645360350636 c 1.1570481 k 0.0054022386 m 0.8531065\n",
      "24 Train Loss 212.31146 Test RE 0.0023331471147813145 c 1.1614096 k 0.0053980546 m 0.87588143\n",
      "25 Train Loss 208.58128 Test RE 0.0026366009771238283 c 1.1440586 k 0.0059127104 m 0.9833565\n",
      "26 Train Loss 207.4462 Test RE 0.002775785329501811 c 1.1331646 k 0.006085179 m 1.013509\n",
      "27 Train Loss 202.26456 Test RE 0.003110726299927663 c 1.1517549 k 0.0055768453 m 1.1988132\n",
      "28 Train Loss 175.36844 Test RE 0.002824902257613624 c 1.1254424 k 0.006489634 m 1.4519556\n",
      "29 Train Loss 161.82007 Test RE 0.0023372502186208284 c 1.1293206 k 0.0061149523 m 1.4424115\n",
      "30 Train Loss 159.59628 Test RE 0.0022509322536387786 c 1.1438189 k 0.0057931384 m 1.4826919\n",
      "31 Train Loss 149.34424 Test RE 0.0021653296706522435 c 1.122517 k 0.0064875637 m 1.8148754\n",
      "32 Train Loss 129.37683 Test RE 0.0019552049422099237 c 1.1065264 k 0.006729716 m 2.1240635\n",
      "33 Train Loss 124.35859 Test RE 0.0018570059512585323 c 1.1073583 k 0.0067840233 m 2.2644808\n",
      "34 Train Loss 107.88725 Test RE 0.001649163787185101 c 1.098172 k 0.007120918 m 2.651249\n",
      "35 Train Loss 85.68849 Test RE 0.0009626152075607692 c 1.0853608 k 0.007470488 m 3.0456874\n",
      "36 Train Loss 77.75867 Test RE 0.0012331170488451369 c 1.0631074 k 0.0080752075 m 3.3111289\n",
      "37 Train Loss 68.840096 Test RE 0.0014219808296744164 c 1.0653389 k 0.007943054 m 3.317263\n",
      "38 Train Loss 58.180046 Test RE 0.0011686130969291502 c 1.0703157 k 0.007825781 m 3.3214087\n",
      "39 Train Loss 48.82017 Test RE 0.001274481172610215 c 1.062023 k 0.008050402 m 3.4773161\n",
      "40 Train Loss 41.43315 Test RE 0.001253560594465454 c 1.054221 k 0.008225045 m 3.6882262\n",
      "41 Train Loss 31.129635 Test RE 0.0012001788420900187 c 1.051089 k 0.0083400225 m 4.286758\n",
      "42 Train Loss 18.099743 Test RE 0.0008482972485580984 c 1.0265709 k 0.008996584 m 4.5548325\n",
      "43 Train Loss 16.164173 Test RE 0.0006320257482387645 c 1.0221809 k 0.009147809 m 4.6574817\n",
      "44 Train Loss 13.767849 Test RE 0.0004738161403734197 c 1.0152098 k 0.009413478 m 4.831069\n",
      "45 Train Loss 9.876797 Test RE 0.0004907632403767593 c 1.0195117 k 0.009312641 m 4.7424793\n",
      "46 Train Loss 8.283998 Test RE 0.0004100573293161198 c 1.0150933 k 0.00942062 m 4.690487\n",
      "47 Train Loss 7.575403 Test RE 0.00037141058517403084 c 1.0150759 k 0.0094585465 m 4.8051114\n",
      "48 Train Loss 6.1500077 Test RE 0.00037668900947922506 c 1.0088513 k 0.009648148 m 4.92632\n",
      "49 Train Loss 4.822298 Test RE 0.00035250821542104593 c 1.0023133 k 0.009854458 m 4.9855027\n",
      "50 Train Loss 4.3389454 Test RE 0.0003369991601426619 c 1.0017204 k 0.009884481 m 5.0335097\n",
      "51 Train Loss 4.1722817 Test RE 0.00031442815054763567 c 0.99987954 k 0.009967701 m 5.081577\n",
      "52 Train Loss 3.945466 Test RE 0.0003056619622783906 c 0.9993742 k 0.00997895 m 5.074212\n",
      "53 Train Loss 3.8259392 Test RE 0.0003314515320419587 c 1.0005932 k 0.009925923 m 5.047112\n",
      "54 Train Loss 3.6024482 Test RE 0.0003266466504632536 c 1.0012234 k 0.009880846 m 5.049246\n",
      "55 Train Loss 3.0583575 Test RE 0.00025287178222678286 c 0.9971643 k 0.009992666 m 5.112669\n",
      "56 Train Loss 2.3480437 Test RE 0.00024607249735560333 c 0.9992986 k 0.009939571 m 5.1379247\n",
      "57 Train Loss 2.2744694 Test RE 0.00023848750167202966 c 1.0000256 k 0.009905826 m 5.1289515\n",
      "58 Train Loss 2.1633499 Test RE 0.00023137721229780448 c 1.0003706 k 0.009889834 m 5.114009\n",
      "59 Train Loss 1.884932 Test RE 0.00020990319042590231 c 1.0027232 k 0.009817677 m 5.0551915\n",
      "60 Train Loss 1.7071885 Test RE 0.0001815644312256081 c 1.006077 k 0.0097382 m 4.994062\n",
      "61 Train Loss 1.5419629 Test RE 0.00016452862182598225 c 1.0052876 k 0.009755168 m 4.9644294\n",
      "62 Train Loss 1.1785392 Test RE 0.00013317312673952536 c 1.0033636 k 0.009831498 m 4.9669294\n",
      "63 Train Loss 1.1279091 Test RE 0.00012691743112730174 c 1.0040203 k 0.009818259 m 4.9752917\n",
      "64 Train Loss 1.0496458 Test RE 0.00011821029523583791 c 1.0047917 k 0.009798306 m 4.961531\n",
      "65 Train Loss 0.93325377 Test RE 0.00010045592429598143 c 1.002967 k 0.009868821 m 4.9549737\n",
      "66 Train Loss 0.81241006 Test RE 0.00010317572595842979 c 1.0020646 k 0.009920911 m 4.974729\n",
      "67 Train Loss 0.74550784 Test RE 0.00010471902579685334 c 1.0032558 k 0.009881566 m 4.9575305\n",
      "68 Train Loss 0.6493318 Test RE 9.419837074279446e-05 c 1.0018542 k 0.009927401 m 4.934878\n",
      "69 Train Loss 0.55178297 Test RE 8.277959059521894e-05 c 0.9999564 k 0.010000707 m 4.981357\n",
      "70 Train Loss 0.5149183 Test RE 7.751167816994717e-05 c 1.0004954 k 0.0099803 m 5.008135\n",
      "71 Train Loss 0.48448575 Test RE 7.622583641617335e-05 c 1.0000749 k 0.0099910265 m 5.002064\n",
      "72 Train Loss 0.47121325 Test RE 7.398517609485413e-05 c 0.9993958 k 0.010011406 m 5.007634\n",
      "73 Train Loss 0.4541871 Test RE 6.731350626746703e-05 c 1.0002418 k 0.009983618 m 5.0058866\n",
      "74 Train Loss 0.44016108 Test RE 6.117397909572598e-05 c 1.0013404 k 0.009951789 m 4.9856043\n",
      "75 Train Loss 0.42703187 Test RE 6.0201695930333924e-05 c 1.0010237 k 0.009960399 m 4.9818664\n",
      "76 Train Loss 0.420048 Test RE 6.393578171302385e-05 c 1.0005862 k 0.009975286 m 4.990092\n",
      "77 Train Loss 0.4121048 Test RE 6.349603705481618e-05 c 1.0008608 k 0.009967311 m 4.9921107\n",
      "78 Train Loss 0.40102988 Test RE 5.967668431807802e-05 c 1.0016714 k 0.009941649 m 4.9823976\n",
      "79 Train Loss 0.38674214 Test RE 5.8004701024382114e-05 c 1.0014822 k 0.009947637 m 4.9817443\n",
      "80 Train Loss 0.38080376 Test RE 6.107074679147367e-05 c 1.0008379 k 0.009961213 m 4.987644\n",
      "81 Train Loss 0.37126327 Test RE 6.326227421329026e-05 c 1.000372 k 0.009974223 m 4.995338\n",
      "82 Train Loss 0.33751467 Test RE 6.0621800257847464e-05 c 1.0011421 k 0.009959643 m 4.9907885\n",
      "83 Train Loss 0.31187811 Test RE 6.379674991868818e-05 c 1.0017453 k 0.009939102 m 4.9685693\n",
      "84 Train Loss 0.29252514 Test RE 6.291174460325318e-05 c 1.0006702 k 0.009966563 m 4.97816\n",
      "85 Train Loss 0.28494114 Test RE 5.9744854464921916e-05 c 1.0007077 k 0.009965533 m 4.9941897\n",
      "86 Train Loss 0.28224245 Test RE 5.9291468662035584e-05 c 1.0009621 k 0.009958478 m 5.0009184\n",
      "87 Train Loss 0.27020818 Test RE 5.9971929066296456e-05 c 1.0015125 k 0.009943114 m 5.006677\n",
      "88 Train Loss 0.2383236 Test RE 5.520591184888956e-05 c 1.0016457 k 0.0099287685 m 4.990672\n",
      "89 Train Loss 0.23116922 Test RE 5.244053592284745e-05 c 1.0016787 k 0.009926083 m 4.985646\n",
      "90 Train Loss 0.22272097 Test RE 5.1456740953854035e-05 c 1.0018466 k 0.009924519 m 4.985429\n",
      "91 Train Loss 0.21810445 Test RE 5.231192435968612e-05 c 1.001683 k 0.009928256 m 4.9884477\n",
      "92 Train Loss 0.21696718 Test RE 5.252542330525933e-05 c 1.0015612 k 0.009930406 m 4.989222\n",
      "93 Train Loss 0.21688686 Test RE 5.2573397767006005e-05 c 1.0015665 k 0.009930219 m 4.989103\n",
      "94 Train Loss 0.21644132 Test RE 5.284947414652368e-05 c 1.0016241 k 0.009928379 m 4.9886417\n",
      "95 Train Loss 0.21519661 Test RE 5.3992630240446556e-05 c 1.0017037 k 0.009926515 m 4.988217\n",
      "96 Train Loss 0.21334238 Test RE 5.512109046468937e-05 c 1.0017625 k 0.009921792 m 4.9881434\n",
      "97 Train Loss 0.21001795 Test RE 5.4694052756345816e-05 c 1.0016264 k 0.009923997 m 4.986739\n",
      "98 Train Loss 0.19148481 Test RE 5.3017897171053606e-05 c 1.0012782 k 0.009941144 m 4.9827013\n",
      "99 Train Loss 0.17613944 Test RE 5.229105435683991e-05 c 1.0011911 k 0.009950446 m 4.991656\n",
      "100 Train Loss 0.17119852 Test RE 5.279336472122794e-05 c 1.0008203 k 0.009961086 m 4.9982133\n",
      "101 Train Loss 0.15747741 Test RE 5.496321885589737e-05 c 1.0001104 k 0.009980727 m 4.994859\n",
      "102 Train Loss 0.15007955 Test RE 5.381399742769525e-05 c 1.0005767 k 0.0099727195 m 4.993586\n",
      "103 Train Loss 0.14597552 Test RE 5.2270193595981097e-05 c 1.0007885 k 0.009970069 m 4.997789\n",
      "104 Train Loss 0.14148636 Test RE 5.20683295988526e-05 c 1.0005171 k 0.009978691 m 5.0020423\n",
      "105 Train Loss 0.1375212 Test RE 5.3620357287429024e-05 c 1.0000314 k 0.009994067 m 5.001768\n",
      "106 Train Loss 0.1295398 Test RE 5.763871065039238e-05 c 0.99965745 k 0.01000865 m 4.9989543\n",
      "107 Train Loss 0.11887028 Test RE 5.8628774612898735e-05 c 0.99998426 k 0.010004119 m 4.9985943\n",
      "108 Train Loss 0.10586534 Test RE 5.814049378351182e-05 c 1.0002298 k 0.009992076 m 5.0039797\n",
      "109 Train Loss 0.09529008 Test RE 5.1808044995212325e-05 c 0.9998709 k 0.009999772 m 5.0037947\n",
      "110 Train Loss 0.08696701 Test RE 5.0149861902866736e-05 c 1.0000411 k 0.009997755 m 4.9945607\n",
      "111 Train Loss 0.08188783 Test RE 4.847770448762464e-05 c 1.0004624 k 0.009989179 m 4.9936266\n",
      "112 Train Loss 0.07605885 Test RE 4.66401607065003e-05 c 1.0003235 k 0.0099911345 m 5.0035853\n",
      "113 Train Loss 0.07273159 Test RE 4.579855595047135e-05 c 0.9997646 k 0.010005857 m 5.007822\n",
      "114 Train Loss 0.070289 Test RE 4.714351747191801e-05 c 0.9998107 k 0.010003375 m 5.001044\n",
      "115 Train Loss 0.067424476 Test RE 4.809510581485424e-05 c 1.0002593 k 0.0099907955 m 4.9972534\n",
      "116 Train Loss 0.061103925 Test RE 4.665370883301321e-05 c 1.0006089 k 0.009981479 m 4.999509\n",
      "117 Train Loss 0.050062984 Test RE 4.1836390058723165e-05 c 1.0002424 k 0.009991276 m 5.003575\n",
      "118 Train Loss 0.045597296 Test RE 4.061601253771219e-05 c 1.0000377 k 0.009999188 m 5.0005784\n",
      "119 Train Loss 0.043941274 Test RE 4.02485966548596e-05 c 0.9999878 k 0.010000696 m 4.999771\n",
      "120 Train Loss 0.04267104 Test RE 4.090262918872442e-05 c 0.99995273 k 0.010001278 m 5.0014977\n",
      "121 Train Loss 0.041426785 Test RE 4.127826282914328e-05 c 0.9998299 k 0.010003708 m 5.004037\n",
      "122 Train Loss 0.03988931 Test RE 4.078851895857906e-05 c 0.99983203 k 0.010003494 m 5.003616\n",
      "123 Train Loss 0.03893872 Test RE 4.0161043524888064e-05 c 0.9999308 k 0.010000816 m 5.0013876\n",
      "124 Train Loss 0.038356546 Test RE 3.962077455624715e-05 c 0.99998635 k 0.00999936 m 5.0009685\n",
      "125 Train Loss 0.03801056 Test RE 3.976539254608814e-05 c 0.9999712 k 0.009999778 m 5.000937\n",
      "126 Train Loss 0.03714957 Test RE 3.932638503934986e-05 c 0.9999004 k 0.010002069 m 5.0004287\n",
      "127 Train Loss 0.03675212 Test RE 3.870424530193916e-05 c 0.9999083 k 0.010001729 m 5.000086\n",
      "128 Train Loss 0.036586657 Test RE 3.8492196382690454e-05 c 0.99993724 k 0.010000685 m 4.9998393\n",
      "129 Train Loss 0.036418628 Test RE 3.829707125003722e-05 c 1.0000135 k 0.009998133 m 4.999316\n",
      "130 Train Loss 0.03627175 Test RE 3.814520694301226e-05 c 1.0001192 k 0.009994561 m 4.9986234\n",
      "131 Train Loss 0.03615825 Test RE 3.80959873438937e-05 c 1.0001636 k 0.009993343 m 4.99837\n",
      "132 Train Loss 0.035790943 Test RE 3.781537162873169e-05 c 1.0002176 k 0.009992505 m 4.9985805\n",
      "133 Train Loss 0.03537478 Test RE 3.709801777718574e-05 c 1.0001118 k 0.009996333 m 4.999967\n",
      "134 Train Loss 0.034004778 Test RE 3.508527348626543e-05 c 1.0000001 k 0.010000291 m 5.0015554\n",
      "135 Train Loss 0.030311057 Test RE 3.201661924576273e-05 c 1.0004125 k 0.009987521 m 5.0010405\n",
      "136 Train Loss 0.025948998 Test RE 2.8449636397951276e-05 c 1.0004221 k 0.009984831 m 4.999287\n",
      "137 Train Loss 0.023551757 Test RE 2.4222344853375978e-05 c 1.0000881 k 0.009993401 m 4.9988685\n",
      "138 Train Loss 0.022593178 Test RE 2.390704116571088e-05 c 0.99999404 k 0.00999631 m 4.9992805\n",
      "139 Train Loss 0.022346366 Test RE 2.4179833007936044e-05 c 1.0000762 k 0.009994343 m 4.9993396\n",
      "140 Train Loss 0.022116989 Test RE 2.4328026390224615e-05 c 1.0002073 k 0.009990935 m 4.999524\n",
      "141 Train Loss 0.021939244 Test RE 2.4140596379280466e-05 c 1.0002682 k 0.009989234 m 4.999781\n",
      "142 Train Loss 0.02177806 Test RE 2.3809969796859384e-05 c 1.0002311 k 0.009990044 m 4.999994\n",
      "143 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "144 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "145 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "146 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "147 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "148 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "149 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "150 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "151 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "152 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "153 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "154 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "155 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "156 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "157 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "158 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "159 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "160 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "161 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "162 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "163 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "164 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "165 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "166 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "167 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "168 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "169 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "170 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "171 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "172 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "173 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "174 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "175 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "176 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "177 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "178 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "179 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "180 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "181 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "182 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "183 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "184 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "185 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "186 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "187 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "188 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "189 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "190 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "191 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "192 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "193 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "194 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "195 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "196 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "197 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "198 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "199 Train Loss 0.02171908 Test RE 2.379180195075783e-05 c 1.0001926 k 0.009991149 m 4.999833\n",
      "Training time: 86.54\n",
      "Training time: 86.54\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 1108474.9 Test RE 0.4005679749780062 c 0.010621196 k 0.05695317 m 8.710759e-06\n",
      "1 Train Loss 348917.06 Test RE 0.22379401586297104 c 0.0064704707 k 0.042664867 m 3.2641536e-05\n",
      "2 Train Loss 5220.9956 Test RE 0.018439979868907847 c 0.0058153006 k 0.03382076 m 0.00019115988\n",
      "3 Train Loss 2999.2485 Test RE 0.00819538191852556 c 0.07422412 k 0.029201105 m 0.0039241835\n",
      "4 Train Loss 2843.669 Test RE 0.007670628506462343 c 0.09663604 k 0.03132454 m 0.005132283\n",
      "5 Train Loss 1830.7864 Test RE 0.009843582034454354 c 0.5318299 k 0.017811334 m 0.029457793\n",
      "6 Train Loss 1192.6294 Test RE 0.004655832449448021 c 0.5637977 k 0.020173308 m 0.031986427\n",
      "7 Train Loss 897.31335 Test RE 0.0054454921002118324 c 0.75985235 k 0.013564414 m 0.044827197\n",
      "8 Train Loss 643.6602 Test RE 0.004601044476808896 c 0.89641905 k 0.0111018075 m 0.055592995\n",
      "9 Train Loss 617.0054 Test RE 0.004752571464867235 c 0.937997 k 0.011341037 m 0.058937475\n",
      "10 Train Loss 474.3529 Test RE 0.004230906530622118 c 1.1803823 k 0.006434796 m 0.08048063\n",
      "11 Train Loss 426.1141 Test RE 0.00412222453224834 c 1.1914811 k 0.004474917 m 0.0828932\n",
      "12 Train Loss 405.0515 Test RE 0.004217817210973366 c 1.1828635 k 0.0044507775 m 0.08460004\n",
      "13 Train Loss 398.07358 Test RE 0.0042902694327094275 c 1.1893413 k 0.0048238984 m 0.0871186\n",
      "14 Train Loss 379.75122 Test RE 0.0043170201105207145 c 1.1445699 k 0.005593165 m 0.093856625\n",
      "15 Train Loss 364.15112 Test RE 0.003916514937192585 c 1.1483486 k 0.005333635 m 0.10155328\n",
      "16 Train Loss 343.8514 Test RE 0.003638166639714267 c 1.1648232 k 0.005353104 m 0.118305676\n",
      "17 Train Loss 336.73245 Test RE 0.003665287744293176 c 1.1619205 k 0.0051917844 m 0.12794796\n",
      "18 Train Loss 329.89832 Test RE 0.003724105660979434 c 1.1581296 k 0.0051377933 m 0.13584937\n",
      "19 Train Loss 318.1382 Test RE 0.0033227666930538704 c 1.1731772 k 0.0051027145 m 0.14840993\n",
      "20 Train Loss 313.75882 Test RE 0.003070162039885642 c 1.1863829 k 0.004594689 m 0.15743844\n",
      "21 Train Loss 300.1729 Test RE 0.0030539337808173955 c 1.2049663 k 0.004408014 m 0.22409979\n",
      "22 Train Loss 283.76407 Test RE 0.0030222195999408553 c 1.1746122 k 0.0050451187 m 0.31785038\n",
      "23 Train Loss 250.5429 Test RE 0.003196454522936971 c 1.1468766 k 0.005009909 m 0.519318\n",
      "24 Train Loss 240.33261 Test RE 0.0031729901331599363 c 1.1681496 k 0.0051568784 m 0.5646582\n",
      "25 Train Loss 231.15219 Test RE 0.0030513167484474525 c 1.162321 k 0.0051517305 m 0.62152284\n",
      "26 Train Loss 227.74707 Test RE 0.002957727492419156 c 1.1652422 k 0.00510515 m 0.6267741\n",
      "27 Train Loss 225.47581 Test RE 0.0029989349005695265 c 1.1706202 k 0.004988412 m 0.63300055\n",
      "28 Train Loss 224.47537 Test RE 0.003020850718310608 c 1.1710418 k 0.005006214 m 0.6287784\n",
      "29 Train Loss 223.97993 Test RE 0.0030395065534546545 c 1.1709913 k 0.005011937 m 0.6277595\n",
      "30 Train Loss 222.5726 Test RE 0.002977786908357224 c 1.169223 k 0.0050380393 m 0.64290595\n",
      "31 Train Loss 221.11444 Test RE 0.0028248932678973616 c 1.1702996 k 0.0050347284 m 0.6713048\n",
      "32 Train Loss 219.28513 Test RE 0.0027579700133376715 c 1.167638 k 0.0050655985 m 0.71534747\n",
      "33 Train Loss 215.84666 Test RE 0.0026682862958803556 c 1.1641626 k 0.005103475 m 0.76976657\n",
      "34 Train Loss 214.84308 Test RE 0.0026155428400377397 c 1.1709994 k 0.004993149 m 0.78031397\n",
      "35 Train Loss 210.82042 Test RE 0.0026474406529981274 c 1.1705391 k 0.0050071366 m 0.83059126\n",
      "36 Train Loss 207.03125 Test RE 0.0026309679633780025 c 1.1604725 k 0.0052240924 m 0.8586127\n",
      "37 Train Loss 205.66104 Test RE 0.0025924157972749386 c 1.1627767 k 0.0051374887 m 0.83999527\n",
      "38 Train Loss 200.61125 Test RE 0.0024767610178090924 c 1.1662974 k 0.0053137317 m 0.84662837\n",
      "39 Train Loss 187.81502 Test RE 0.0024657810320931244 c 1.1473346 k 0.005657165 m 0.9326716\n",
      "40 Train Loss 184.62538 Test RE 0.0025102710168003143 c 1.1532872 k 0.005542477 m 1.0315347\n",
      "41 Train Loss 184.1515 Test RE 0.0024312538351230722 c 1.1521589 k 0.005579106 m 1.0481648\n",
      "42 Train Loss 183.67712 Test RE 0.002369738016181008 c 1.1523868 k 0.005522392 m 1.0676268\n",
      "43 Train Loss 181.7875 Test RE 0.0022887614375722387 c 1.1542124 k 0.0055240844 m 1.060024\n",
      "44 Train Loss 179.32773 Test RE 0.0022646407197256177 c 1.1496509 k 0.0057598385 m 1.0956478\n",
      "45 Train Loss 177.166 Test RE 0.002159007843998897 c 1.1454221 k 0.005744826 m 1.1531293\n",
      "46 Train Loss 173.34007 Test RE 0.002097496884205509 c 1.1440803 k 0.0058315354 m 1.3306235\n",
      "47 Train Loss 168.18106 Test RE 0.0019198229594162089 c 1.1401954 k 0.005968627 m 1.3521861\n",
      "48 Train Loss 167.17433 Test RE 0.0020104393930702067 c 1.1414968 k 0.0059235403 m 1.345323\n",
      "49 Train Loss 166.29422 Test RE 0.002074134300030451 c 1.1427892 k 0.0058970386 m 1.3215213\n",
      "50 Train Loss 163.5442 Test RE 0.00188441498954997 c 1.1354814 k 0.0060657826 m 1.4476415\n",
      "51 Train Loss 158.63556 Test RE 0.0019942909370893494 c 1.1326762 k 0.0061918334 m 1.6014173\n",
      "52 Train Loss 141.12305 Test RE 0.002252544046957173 c 1.1316427 k 0.0060884347 m 2.289284\n",
      "53 Train Loss 116.64265 Test RE 0.0019861103103419042 c 1.1289496 k 0.0063535585 m 2.7313344\n",
      "54 Train Loss 102.17318 Test RE 0.0018486772325172352 c 1.0904613 k 0.0071214116 m 2.937772\n",
      "55 Train Loss 95.629944 Test RE 0.0016663414046028753 c 1.0919318 k 0.0071188174 m 2.8782992\n",
      "56 Train Loss 91.0661 Test RE 0.0013668706837529616 c 1.0885684 k 0.0072395727 m 2.9254038\n",
      "57 Train Loss 82.48257 Test RE 0.0013711710207661137 c 1.0917423 k 0.00718117 m 2.7481084\n",
      "58 Train Loss 80.48642 Test RE 0.001199780147842518 c 1.0896479 k 0.007327189 m 2.7533374\n",
      "59 Train Loss 77.43796 Test RE 0.0009759022851711763 c 1.0778022 k 0.0076665343 m 2.9150834\n",
      "60 Train Loss 52.352146 Test RE 0.0009427528994016704 c 1.0640146 k 0.008133607 m 3.6200037\n",
      "61 Train Loss 41.267555 Test RE 0.000772001434258802 c 1.039503 k 0.00876999 m 3.8277688\n",
      "62 Train Loss 27.14552 Test RE 0.000833182849920525 c 1.0397235 k 0.008869894 m 4.062071\n",
      "63 Train Loss 22.87288 Test RE 0.0007482983496249444 c 1.0307685 k 0.009005968 m 4.146776\n",
      "64 Train Loss 19.772198 Test RE 0.0007675828006440364 c 1.0323149 k 0.009087176 m 4.224048\n",
      "65 Train Loss 15.979158 Test RE 0.0006563849567393568 c 1.0335082 k 0.0090053165 m 4.245098\n",
      "66 Train Loss 15.581728 Test RE 0.0006542227900700146 c 1.0276754 k 0.009166651 m 4.2641516\n",
      "67 Train Loss 14.186775 Test RE 0.0006369968861731393 c 1.0258578 k 0.009293005 m 4.369252\n",
      "68 Train Loss 13.558281 Test RE 0.0006153758699368167 c 1.0234135 k 0.009265639 m 4.4289265\n",
      "69 Train Loss 13.323087 Test RE 0.0005791984587011011 c 1.0224879 k 0.009296902 m 4.475264\n",
      "70 Train Loss 12.9127655 Test RE 0.0005714811503963712 c 1.0219536 k 0.00932649 m 4.522251\n",
      "71 Train Loss 11.959543 Test RE 0.0005822417905605194 c 1.0152467 k 0.009484418 m 4.628521\n",
      "72 Train Loss 10.134417 Test RE 0.0005010505766615173 c 1.0071132 k 0.009697112 m 4.8599\n",
      "73 Train Loss 8.831852 Test RE 0.00041857902486989914 c 1.0072305 k 0.009713835 m 4.9122596\n",
      "74 Train Loss 8.0688 Test RE 0.0003888563776798519 c 1.0077767 k 0.009691271 m 4.897863\n",
      "75 Train Loss 7.081662 Test RE 0.00034440390233832376 c 1.0107703 k 0.00959196 m 4.835519\n",
      "76 Train Loss 6.8002977 Test RE 0.0003289305005051098 c 1.0113968 k 0.009563934 m 4.818414\n",
      "77 Train Loss 6.6973743 Test RE 0.0003236667986936865 c 1.0118018 k 0.009542597 m 4.823932\n",
      "78 Train Loss 6.5380254 Test RE 0.0003304330860751559 c 1.0110006 k 0.009539537 m 4.832353\n",
      "79 Train Loss 6.021265 Test RE 0.0003208889525139136 c 1.010723 k 0.009588583 m 4.8502774\n",
      "80 Train Loss 4.84692 Test RE 0.0002663891638655376 c 1.0119957 k 0.009544975 m 4.8860154\n",
      "81 Train Loss 4.4617114 Test RE 0.00023296164143729638 c 1.0081645 k 0.009667156 m 4.896216\n",
      "82 Train Loss 4.2276216 Test RE 0.00021489349557662362 c 1.0095206 k 0.009628838 m 4.892924\n",
      "83 Train Loss 3.5609243 Test RE 0.0002403231162665488 c 1.0059658 k 0.0097398665 m 4.910553\n",
      "84 Train Loss 3.1769452 Test RE 0.00022592628296230963 c 1.0061982 k 0.0097125685 m 4.917536\n",
      "85 Train Loss 2.8942876 Test RE 0.00019149328130766896 c 1.0070156 k 0.009735651 m 4.940269\n",
      "86 Train Loss 2.7385569 Test RE 0.00016652996042009765 c 1.0052766 k 0.009775099 m 4.956784\n",
      "87 Train Loss 2.6702125 Test RE 0.00016795423636236642 c 1.005068 k 0.009780789 m 4.955364\n",
      "88 Train Loss 2.4772444 Test RE 0.00018291391874717465 c 1.0048296 k 0.009784615 m 4.9481654\n",
      "89 Train Loss 1.8027079 Test RE 0.00012941765386776448 c 1.001523 k 0.0099061495 m 5.017674\n",
      "90 Train Loss 1.6538583 Test RE 0.00012510581511084175 c 0.99958867 k 0.009983064 m 5.0665674\n",
      "91 Train Loss 1.486774 Test RE 0.00010564874517849644 c 0.9975325 k 0.0100429505 m 5.118855\n",
      "92 Train Loss 1.4202172 Test RE 0.00011746483167105444 c 0.9975298 k 0.010029538 m 5.133416\n",
      "93 Train Loss 1.3397477 Test RE 0.00012549396049539314 c 0.9973514 k 0.01004321 m 5.141628\n",
      "94 Train Loss 1.2922581 Test RE 0.0001136888420504506 c 0.99689764 k 0.010062958 m 5.1407666\n",
      "95 Train Loss 1.2670815 Test RE 0.00011619815244654156 c 0.99694127 k 0.010054009 m 5.1376376\n",
      "96 Train Loss 1.2613608 Test RE 0.00012164430585639216 c 0.9972483 k 0.01004234 m 5.129718\n",
      "97 Train Loss 1.2482892 Test RE 0.00012286577033981537 c 0.99802077 k 0.010020295 m 5.1062803\n",
      "98 Train Loss 1.2296855 Test RE 0.00011524177758497765 c 0.998499 k 0.010005516 m 5.089213\n",
      "99 Train Loss 1.1920054 Test RE 9.966652290175651e-05 c 1.0000877 k 0.009965223 m 5.0677705\n",
      "100 Train Loss 1.14169 Test RE 9.361870566799353e-05 c 1.0012513 k 0.009938765 m 5.055728\n",
      "101 Train Loss 0.98000586 Test RE 7.713540673008875e-05 c 0.99916565 k 0.009995157 m 5.0407977\n",
      "102 Train Loss 0.9220881 Test RE 6.747643102248644e-05 c 0.99883646 k 0.01001348 m 5.040915\n",
      "103 Train Loss 0.88643056 Test RE 7.106124554127289e-05 c 0.99975497 k 0.009997961 m 5.046876\n",
      "104 Train Loss 0.8119302 Test RE 6.95251779452578e-05 c 0.99941635 k 0.010003421 m 5.0545764\n",
      "105 Train Loss 0.62888783 Test RE 7.061493987951962e-05 c 0.9981322 k 0.01002389 m 5.0731516\n",
      "106 Train Loss 0.47738847 Test RE 4.0013098464042934e-05 c 0.9993559 k 0.010018671 m 5.043333\n",
      "107 Train Loss 0.34476477 Test RE 5.4859377946349575e-05 c 0.9993298 k 0.010011044 m 5.0012274\n",
      "108 Train Loss 0.30231148 Test RE 4.2721450369779955e-05 c 0.99982214 k 0.009995389 m 5.0037813\n",
      "109 Train Loss 0.2651384 Test RE 5.064760151345838e-05 c 1.0009801 k 0.009966548 m 5.015762\n",
      "110 Train Loss 0.24619417 Test RE 5.483423721568863e-05 c 0.9995874 k 0.009999493 m 5.01972\n",
      "111 Train Loss 0.23521438 Test RE 5.417263402719362e-05 c 0.9993347 k 0.010004079 m 5.025883\n",
      "112 Train Loss 0.22792757 Test RE 5.252954257282664e-05 c 0.99995536 k 0.009989418 m 5.028161\n",
      "113 Train Loss 0.22251718 Test RE 5.209145348578765e-05 c 0.9999455 k 0.009991595 m 5.023412\n",
      "114 Train Loss 0.22092539 Test RE 5.243931099470129e-05 c 0.9997004 k 0.009997041 m 5.0187726\n",
      "115 Train Loss 0.21837881 Test RE 5.1732682026223934e-05 c 0.9997359 k 0.00999752 m 5.0154967\n",
      "116 Train Loss 0.21259958 Test RE 4.7532417279808436e-05 c 1.0000987 k 0.009990033 m 5.018907\n",
      "117 Train Loss 0.20366785 Test RE 4.303691750607435e-05 c 0.9999508 k 0.00998837 m 5.021236\n",
      "118 Train Loss 0.19654126 Test RE 4.1019760282828876e-05 c 1.0000488 k 0.009984023 m 5.0084004\n",
      "119 Train Loss 0.1918749 Test RE 4.17420095653909e-05 c 1.0006189 k 0.009968681 m 4.995632\n",
      "120 Train Loss 0.1850162 Test RE 4.561154750457359e-05 c 1.0013291 k 0.009948641 m 4.9904184\n",
      "121 Train Loss 0.18050636 Test RE 4.7954646070757333e-05 c 1.0013039 k 0.009947528 m 4.9945674\n",
      "122 Train Loss 0.17245875 Test RE 5.032799137762902e-05 c 1.0011504 k 0.009947341 m 4.9911566\n",
      "123 Train Loss 0.16879411 Test RE 4.994657215922027e-05 c 1.0010355 k 0.009949666 m 4.988759\n",
      "124 Train Loss 0.16777422 Test RE 4.896782967899324e-05 c 1.0012369 k 0.009944558 m 4.98849\n",
      "125 Train Loss 0.16726324 Test RE 4.899511566664363e-05 c 1.0013976 k 0.009938911 m 4.988466\n",
      "126 Train Loss 0.16344962 Test RE 4.820986372406572e-05 c 1.0018265 k 0.0099250255 m 4.9862523\n",
      "127 Train Loss 0.14939684 Test RE 4.0376502371716764e-05 c 1.0017862 k 0.009924793 m 4.983443\n",
      "128 Train Loss 0.14154327 Test RE 3.4141657908504874e-05 c 1.0013962 k 0.009939676 m 4.986383\n",
      "129 Train Loss 0.13877973 Test RE 3.266655395856332e-05 c 1.0012528 k 0.009941048 m 4.992134\n",
      "130 Train Loss 0.13661093 Test RE 3.315160537878465e-05 c 1.001359 k 0.009936069 m 4.993959\n",
      "131 Train Loss 0.1351892 Test RE 3.4323975532006964e-05 c 1.0015491 k 0.009931531 m 4.9920597\n",
      "132 Train Loss 0.13479868 Test RE 3.444221006897771e-05 c 1.0015414 k 0.009931596 m 4.991728\n",
      "133 Train Loss 0.13447317 Test RE 3.399346564224127e-05 c 1.0015093 k 0.009932688 m 4.9923854\n",
      "134 Train Loss 0.13413139 Test RE 3.3702039612331146e-05 c 1.0014511 k 0.009933954 m 4.9936776\n",
      "135 Train Loss 0.13316472 Test RE 3.4643953752588096e-05 c 1.0013791 k 0.009935668 m 4.9958835\n",
      "136 Train Loss 0.13137814 Test RE 3.7456000581473565e-05 c 1.0013248 k 0.009936909 m 4.9963894\n",
      "137 Train Loss 0.12871358 Test RE 3.8846018776311e-05 c 1.0012836 k 0.009938835 m 4.993708\n",
      "138 Train Loss 0.12514663 Test RE 3.8671124850618745e-05 c 1.0011337 k 0.009944388 m 4.9947543\n",
      "139 Train Loss 0.12259534 Test RE 3.8325925702793286e-05 c 1.0011218 k 0.009946968 m 4.9981246\n",
      "140 Train Loss 0.12141232 Test RE 3.810592685941961e-05 c 1.0011374 k 0.009947036 m 4.999423\n",
      "141 Train Loss 0.12109559 Test RE 3.854835776698731e-05 c 1.0011095 k 0.009947714 m 4.999728\n",
      "142 Train Loss 0.12092251 Test RE 3.89534298099994e-05 c 1.0010456 k 0.009950332 m 4.999852\n",
      "143 Train Loss 0.12086591 Test RE 3.913284476766909e-05 c 1.0010045 k 0.009951822 m 4.9999948\n",
      "144 Train Loss 0.12078997 Test RE 3.923448499991173e-05 c 1.0009598 k 0.00995339 m 5.000168\n",
      "145 Train Loss 0.12048841 Test RE 3.9449060790478386e-05 c 1.0008948 k 0.009956094 m 5.000554\n",
      "146 Train Loss 0.119677246 Test RE 3.9681514519242473e-05 c 1.0009112 k 0.009956652 m 5.0002747\n",
      "147 Train Loss 0.118597455 Test RE 3.998011455752806e-05 c 1.0010374 k 0.00995304 m 4.999319\n",
      "148 Train Loss 0.11550555 Test RE 4.272525646874395e-05 c 1.0006033 k 0.0099637 m 5.002567\n",
      "149 Train Loss 0.11174242 Test RE 4.490096877894334e-05 c 1.0006516 k 0.0099649755 m 5.0005965\n",
      "150 Train Loss 0.102600835 Test RE 4.394059280826228e-05 c 1.0011854 k 0.009953449 m 4.996389\n",
      "151 Train Loss 0.09343509 Test RE 4.2802014385051176e-05 c 1.0008906 k 0.00995901 m 4.9980073\n",
      "152 Train Loss 0.076729 Test RE 4.499322116978151e-05 c 1.0005386 k 0.00997315 m 4.997118\n",
      "153 Train Loss 0.06609766 Test RE 4.3487033639978565e-05 c 1.0005121 k 0.009981325 m 4.9940596\n",
      "154 Train Loss 0.061964203 Test RE 4.227806441353795e-05 c 1.0003033 k 0.009989285 m 4.9961247\n",
      "155 Train Loss 0.058894616 Test RE 4.1355486044372155e-05 c 1.0001624 k 0.00999405 m 4.999805\n",
      "156 Train Loss 0.0558207 Test RE 4.1874353070829984e-05 c 1.0000216 k 0.009999092 m 5.0025015\n",
      "157 Train Loss 0.054132063 Test RE 4.076354934123951e-05 c 0.9998794 k 0.010003743 m 5.00127\n",
      "158 Train Loss 0.053448413 Test RE 3.9792400097845663e-05 c 0.99981844 k 0.010006985 m 5.0009694\n",
      "159 Train Loss 0.05251027 Test RE 3.8523184090782207e-05 c 0.99988663 k 0.010004994 m 5.000814\n",
      "160 Train Loss 0.051270477 Test RE 3.7234936806355996e-05 c 0.99991727 k 0.010004085 m 5.0019555\n",
      "161 Train Loss 0.05030442 Test RE 3.6678261917292254e-05 c 0.9998025 k 0.010006517 m 5.004042\n",
      "162 Train Loss 0.04925542 Test RE 3.676658917778942e-05 c 0.99956554 k 0.010013587 m 5.005091\n",
      "163 Train Loss 0.048065297 Test RE 3.632681072466828e-05 c 0.9996463 k 0.01001264 m 5.00524\n",
      "164 Train Loss 0.046956748 Test RE 3.595429211563559e-05 c 0.99985653 k 0.0100082755 m 5.004637\n",
      "165 Train Loss 0.04474068 Test RE 3.564965464014232e-05 c 0.99977005 k 0.010011001 m 5.0044565\n",
      "166 Train Loss 0.044126093 Test RE 3.581635819524724e-05 c 0.9996333 k 0.010014683 m 5.0051026\n",
      "167 Train Loss 0.043745283 Test RE 3.5844326912691204e-05 c 0.9995255 k 0.010017425 m 5.0052843\n",
      "168 Train Loss 0.04326152 Test RE 3.454594492216849e-05 c 0.99964964 k 0.010015053 m 5.004511\n",
      "169 Train Loss 0.042638876 Test RE 3.2768184202123005e-05 c 0.99983674 k 0.01001007 m 5.0031757\n",
      "170 Train Loss 0.04203423 Test RE 3.2349505843057537e-05 c 0.9999261 k 0.010007128 m 5.001964\n",
      "171 Train Loss 0.040894486 Test RE 3.2720762697233866e-05 c 0.9998666 k 0.010007513 m 5.000504\n",
      "172 Train Loss 0.040349215 Test RE 3.33236148329957e-05 c 0.99983937 k 0.010008347 m 4.9996715\n",
      "173 Train Loss 0.039424293 Test RE 3.2500311531336505e-05 c 0.99986774 k 0.0100068245 m 5.0004597\n",
      "174 Train Loss 0.038847975 Test RE 3.1062912137962704e-05 c 0.99978906 k 0.01000843 m 5.0021453\n",
      "175 Train Loss 0.03858658 Test RE 3.054792102362889e-05 c 0.9996979 k 0.010011099 m 5.00325\n",
      "176 Train Loss 0.038459152 Test RE 3.024743187439872e-05 c 0.9996615 k 0.010011781 m 5.0040116\n",
      "177 Train Loss 0.038357925 Test RE 2.9972894024973038e-05 c 0.9996364 k 0.010012248 m 5.0046215\n",
      "178 Train Loss 0.038252223 Test RE 2.977130513798531e-05 c 0.99962515 k 0.010012428 m 5.005053\n",
      "179 Train Loss 0.037986517 Test RE 2.948806560318825e-05 c 0.9996522 k 0.010011626 m 5.0054293\n",
      "180 Train Loss 0.037525054 Test RE 2.9624056721895647e-05 c 0.99979395 k 0.010007843 m 5.00343\n",
      "181 Train Loss 0.03718104 Test RE 2.9634108395694135e-05 c 0.9999886 k 0.010002194 m 5.000354\n",
      "182 Train Loss 0.03704276 Test RE 2.921594404617459e-05 c 1.0000656 k 0.009999706 m 4.9991126\n",
      "183 Train Loss 0.037001215 Test RE 2.9010761051816912e-05 c 1.0000618 k 0.009999628 m 4.9991474\n",
      "184 Train Loss 0.036983944 Test RE 2.8905114576806344e-05 c 1.0000535 k 0.009999794 m 4.9991937\n",
      "185 Train Loss 0.036983944 Test RE 2.8905114576806344e-05 c 1.0000535 k 0.009999794 m 4.9991937\n",
      "186 Train Loss 0.036983944 Test RE 2.8905114576806344e-05 c 1.0000535 k 0.009999794 m 4.9991937\n",
      "187 Train Loss 0.036983944 Test RE 2.8905114576806344e-05 c 1.0000535 k 0.009999794 m 4.9991937\n",
      "188 Train Loss 0.036983944 Test RE 2.8905114576806344e-05 c 1.0000535 k 0.009999794 m 4.9991937\n",
      "189 Train Loss 0.036983944 Test RE 2.8905114576806344e-05 c 1.0000535 k 0.009999794 m 4.9991937\n",
      "190 Train Loss 0.036983944 Test RE 2.8905114576806344e-05 c 1.0000535 k 0.009999794 m 4.9991937\n",
      "191 Train Loss 0.036983944 Test RE 2.8905114576806344e-05 c 1.0000535 k 0.009999794 m 4.9991937\n",
      "192 Train Loss 0.036983944 Test RE 2.8905114576806344e-05 c 1.0000535 k 0.009999794 m 4.9991937\n",
      "193 Train Loss 0.036983944 Test RE 2.8905114576806344e-05 c 1.0000535 k 0.009999794 m 4.9991937\n",
      "194 Train Loss 0.036983944 Test RE 2.8905114576806344e-05 c 1.0000535 k 0.009999794 m 4.9991937\n",
      "195 Train Loss 0.036983944 Test RE 2.8905114576806344e-05 c 1.0000535 k 0.009999794 m 4.9991937\n",
      "196 Train Loss 0.036983944 Test RE 2.8905114576806344e-05 c 1.0000535 k 0.009999794 m 4.9991937\n",
      "197 Train Loss 0.036983944 Test RE 2.8905114576806344e-05 c 1.0000535 k 0.009999794 m 4.9991937\n",
      "198 Train Loss 0.036983944 Test RE 2.8905114576806344e-05 c 1.0000535 k 0.009999794 m 4.9991937\n",
      "199 Train Loss 0.036983944 Test RE 2.8905114576806344e-05 c 1.0000535 k 0.009999794 m 4.9991937\n",
      "Training time: 102.12\n",
      "Training time: 102.12\n",
      "Sequentialmodel(\n",
      "  (activation): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 1158826.8 Test RE 0.4087979885728235 c 0.011655758 k 0.007886761 m -7.9295256e-05\n",
      "1 Train Loss 795097.94 Test RE 0.3278295358043226 c 0.008159693 k 0.14305328 m -7.605427e-05\n",
      "2 Train Loss 13335.357 Test RE 0.038672326531171294 c -0.04590357 k 0.03552087 m 0.00010413234\n",
      "3 Train Loss 11655.271 Test RE 0.034637402870301294 c -0.031889167 k 0.025668671 m 0.0011563463\n",
      "4 Train Loss 3793.0942 Test RE 0.014650044034862501 c 0.14304481 k 0.028362475 m 0.01191149\n",
      "5 Train Loss 2327.685 Test RE 0.01101314066954621 c 0.45567292 k 0.022792647 m 0.03126423\n",
      "6 Train Loss 1606.838 Test RE 0.01011371954261483 c 0.63002664 k 0.017011773 m 0.04266544\n",
      "7 Train Loss 690.20264 Test RE 0.0063420692123653845 c 1.0701059 k 0.008151709 m 0.076240025\n",
      "8 Train Loss 609.5426 Test RE 0.005649709268654586 c 1.200409 k 0.0029602968 m 0.08698384\n",
      "9 Train Loss 456.82092 Test RE 0.003944214102487592 c 1.2212627 k 0.002775822 m 0.09091974\n",
      "10 Train Loss 449.63394 Test RE 0.003955564059365899 c 1.2154484 k 0.003709786 m 0.09072656\n",
      "11 Train Loss 437.7017 Test RE 0.003985552715546611 c 1.210507 k 0.004568799 m 0.092898116\n",
      "12 Train Loss 374.2878 Test RE 0.003871089920350744 c 1.2397501 k 0.0033934251 m 0.113597706\n",
      "13 Train Loss 351.62958 Test RE 0.00381359289689263 c 1.2450125 k 0.0025298023 m 0.13046788\n",
      "14 Train Loss 295.2599 Test RE 0.0033257806251329345 c 1.2089237 k 0.0038764703 m 0.18356965\n",
      "15 Train Loss 291.5099 Test RE 0.003372259190229239 c 1.1905706 k 0.0047318796 m 0.18321936\n",
      "16 Train Loss 273.79965 Test RE 0.0031190893232617522 c 1.1845385 k 0.004679106 m 0.19668391\n",
      "17 Train Loss 259.80295 Test RE 0.0029682524246478154 c 1.183868 k 0.004610453 m 0.23873979\n",
      "18 Train Loss 252.9435 Test RE 0.0029786381212824086 c 1.186048 k 0.0049499073 m 0.28183356\n",
      "19 Train Loss 233.06464 Test RE 0.0030129596968003288 c 1.1814685 k 0.0047923606 m 0.3734813\n",
      "20 Train Loss 229.38937 Test RE 0.0031275019953591336 c 1.1829929 k 0.004626768 m 0.41045955\n",
      "21 Train Loss 213.27524 Test RE 0.0031663345370021813 c 1.1658447 k 0.0052967393 m 0.6156558\n",
      "22 Train Loss 210.2177 Test RE 0.0031236112666588422 c 1.1575328 k 0.005433474 m 0.6892496\n",
      "23 Train Loss 206.75906 Test RE 0.003182892596740161 c 1.1583407 k 0.0055019516 m 0.76702374\n",
      "24 Train Loss 201.33954 Test RE 0.0030980928703227876 c 1.1740922 k 0.0050132275 m 0.75775576\n",
      "25 Train Loss 197.81844 Test RE 0.0028690511518502638 c 1.1580845 k 0.00550392 m 0.82426953\n",
      "26 Train Loss 182.60287 Test RE 0.002038303931234393 c 1.1267573 k 0.0063866978 m 1.2101411\n",
      "27 Train Loss 167.47308 Test RE 0.0020320257673583078 c 1.1289786 k 0.006089574 m 1.42324\n",
      "28 Train Loss 162.4683 Test RE 0.001981208902659891 c 1.1341478 k 0.0062878034 m 1.5260893\n",
      "29 Train Loss 138.83943 Test RE 0.0018718436057190092 c 1.1304336 k 0.0065125297 m 1.713162\n",
      "30 Train Loss 122.20686 Test RE 0.0015949157870811337 c 1.0874672 k 0.007292543 m 2.1257558\n",
      "31 Train Loss 114.28672 Test RE 0.0016686226659468962 c 1.088609 k 0.0073217526 m 2.3512099\n",
      "32 Train Loss 108.11769 Test RE 0.0017896056039398383 c 1.1094478 k 0.00679871 m 2.4277313\n",
      "33 Train Loss 93.83899 Test RE 0.0015210734975913457 c 1.0792999 k 0.007683545 m 2.7385638\n",
      "34 Train Loss 86.72641 Test RE 0.0013361935476714764 c 1.0800755 k 0.007626651 m 2.8844836\n",
      "35 Train Loss 82.56913 Test RE 0.0013524845358924003 c 1.0876255 k 0.0074018566 m 2.8015537\n",
      "36 Train Loss 74.85783 Test RE 0.0011838532096389647 c 1.0876969 k 0.0073675863 m 2.8960986\n",
      "37 Train Loss 68.509315 Test RE 0.001272561773067915 c 1.0891823 k 0.007268064 m 3.125937\n",
      "38 Train Loss 56.323162 Test RE 0.0013648324230814183 c 1.0575607 k 0.008272946 m 3.4510589\n",
      "39 Train Loss 39.32859 Test RE 0.001279799475158428 c 1.0387428 k 0.008738106 m 4.0717854\n",
      "40 Train Loss 31.64254 Test RE 0.0009927823507798724 c 1.0218273 k 0.009153229 m 4.4589877\n",
      "41 Train Loss 30.03627 Test RE 0.0009800351719622907 c 1.0242742 k 0.009126281 m 4.555624\n",
      "42 Train Loss 21.54655 Test RE 0.0008484542688121923 c 1.026428 k 0.009076546 m 4.7018695\n",
      "43 Train Loss 16.12095 Test RE 0.0007376690497955325 c 1.0167621 k 0.00926123 m 4.8161273\n",
      "44 Train Loss 10.755763 Test RE 0.0006362935468455086 c 1.0215797 k 0.009170915 m 4.7168093\n",
      "45 Train Loss 10.135973 Test RE 0.0006469207223334022 c 1.0238559 k 0.009085136 m 4.7259765\n",
      "46 Train Loss 8.715703 Test RE 0.0006130595510465958 c 1.0230727 k 0.0091670025 m 4.8090835\n",
      "47 Train Loss 7.406534 Test RE 0.000574304778589654 c 1.014301 k 0.0093517685 m 4.829581\n",
      "48 Train Loss 6.4671354 Test RE 0.0005416914442953383 c 1.0171499 k 0.009317211 m 4.787248\n",
      "49 Train Loss 5.979372 Test RE 0.0004670932474097526 c 1.0201794 k 0.009287012 m 4.7583303\n",
      "50 Train Loss 5.679701 Test RE 0.0004212054494031017 c 1.016226 k 0.009351925 m 4.761783\n",
      "51 Train Loss 5.1219864 Test RE 0.000377601468398935 c 1.011222 k 0.009499809 m 4.8517346\n",
      "52 Train Loss 4.8608117 Test RE 0.0003874314661988783 c 1.0122536 k 0.009505955 m 4.8966966\n",
      "53 Train Loss 4.5695596 Test RE 0.0004041950070920314 c 1.0146025 k 0.009449378 m 4.902219\n",
      "54 Train Loss 3.9205983 Test RE 0.00036176793299136394 c 1.0118732 k 0.009517469 m 4.9134135\n",
      "55 Train Loss 3.3146245 Test RE 0.00031067778508900675 c 1.0055794 k 0.009694657 m 4.9633493\n",
      "56 Train Loss 2.8283992 Test RE 0.00025708223713477964 c 1.0089132 k 0.0096199205 m 4.9852457\n",
      "57 Train Loss 2.4972897 Test RE 0.00023550223651766256 c 1.0060092 k 0.009698727 m 4.9903917\n",
      "58 Train Loss 2.3404086 Test RE 0.0002289063164005381 c 1.0018678 k 0.009815069 m 5.021248\n",
      "59 Train Loss 2.1156516 Test RE 0.00021533913747704005 c 1.002484 k 0.009821643 m 5.0550346\n",
      "60 Train Loss 1.9289129 Test RE 0.00022563032597340183 c 1.0067348 k 0.009726163 m 5.0196505\n",
      "61 Train Loss 1.3801045 Test RE 0.00019936254364834771 c 1.0083494 k 0.009698151 m 4.9055796\n",
      "62 Train Loss 1.0459374 Test RE 0.0001879532116106261 c 1.0056067 k 0.009789873 m 4.912095\n",
      "63 Train Loss 1.0104632 Test RE 0.00019185972634393425 c 1.0052595 k 0.0098079145 m 4.9326005\n",
      "64 Train Loss 0.9484608 Test RE 0.00019001547501896343 c 1.0039659 k 0.009848588 m 4.9580293\n",
      "65 Train Loss 0.87136745 Test RE 0.00017724647107514755 c 1.0028518 k 0.009883708 m 4.955817\n",
      "66 Train Loss 0.7108854 Test RE 0.00015870993003751967 c 1.0020809 k 0.009915708 m 4.9711018\n",
      "67 Train Loss 0.66371304 Test RE 0.00015418217439142184 c 1.0011802 k 0.009947337 m 4.9943795\n",
      "68 Train Loss 0.65490514 Test RE 0.00015212918721533084 c 1.001212 k 0.009945161 m 4.9989905\n",
      "69 Train Loss 0.64975834 Test RE 0.00015131517645364289 c 1.0014112 k 0.009939511 m 5.0056577\n",
      "70 Train Loss 0.6312053 Test RE 0.0001492638195999712 c 1.0004829 k 0.009968523 m 5.021477\n",
      "71 Train Loss 0.49018776 Test RE 0.0001377022430241498 c 0.9990427 k 0.0100321695 m 5.042806\n",
      "72 Train Loss 0.4016173 Test RE 0.0001276880388902924 c 0.9989249 k 0.010031105 m 5.029001\n",
      "73 Train Loss 0.39014995 Test RE 0.0001224193501865136 c 0.9995011 k 0.010010657 m 5.015793\n",
      "74 Train Loss 0.38910732 Test RE 0.0001220821013919438 c 0.99966264 k 0.010006207 m 5.011766\n",
      "75 Train Loss 0.3858715 Test RE 0.0001222836401557787 c 0.99981964 k 0.010001062 m 5.0083327\n",
      "76 Train Loss 0.38247088 Test RE 0.00012242745968950568 c 0.9996057 k 0.010004515 m 5.0132895\n",
      "77 Train Loss 0.38023794 Test RE 0.00012160938496579911 c 0.9995082 k 0.010010053 m 5.01611\n",
      "78 Train Loss 0.37748393 Test RE 0.00012032446402659502 c 0.9996611 k 0.010007062 m 5.011656\n",
      "79 Train Loss 0.3672797 Test RE 0.00011697635216732883 c 1.0003428 k 0.0099885315 m 4.9936924\n",
      "80 Train Loss 0.3153165 Test RE 9.849981340850283e-05 c 1.0002972 k 0.009980229 m 4.9890513\n",
      "81 Train Loss 0.27567926 Test RE 8.180610481771643e-05 c 0.99984366 k 0.010000907 m 5.0018888\n",
      "82 Train Loss 0.26680523 Test RE 7.740892749655469e-05 c 1.0001326 k 0.009993031 m 5.0009885\n",
      "83 Train Loss 0.26339388 Test RE 7.746997576329292e-05 c 1.0001823 k 0.009991854 m 4.9983068\n",
      "84 Train Loss 0.25825977 Test RE 7.982449870557749e-05 c 1.0000023 k 0.009996104 m 5.0004625\n",
      "85 Train Loss 0.23843294 Test RE 7.530554556997574e-05 c 0.99935234 k 0.010014671 m 5.0130453\n",
      "86 Train Loss 0.20298916 Test RE 5.8785003307521493e-05 c 0.9989426 k 0.010031642 m 5.019647\n",
      "87 Train Loss 0.16762125 Test RE 4.492464016722944e-05 c 0.9999827 k 0.010002492 m 5.0035653\n",
      "88 Train Loss 0.15069918 Test RE 4.468066056304035e-05 c 1.0004627 k 0.009983948 m 4.989646\n",
      "89 Train Loss 0.14331624 Test RE 4.7670820180827825e-05 c 1.0002649 k 0.009989836 m 4.99343\n",
      "90 Train Loss 0.13964178 Test RE 5.122793402152136e-05 c 1.000241 k 0.009990671 m 4.9958744\n",
      "91 Train Loss 0.13561727 Test RE 5.331957081602388e-05 c 1.000557 k 0.009980603 m 4.990726\n",
      "92 Train Loss 0.12448639 Test RE 5.0360428245388606e-05 c 1.0009282 k 0.009970268 m 4.9799604\n",
      "93 Train Loss 0.09535225 Test RE 3.66423726212546e-05 c 1.0004137 k 0.00998629 m 4.990311\n",
      "94 Train Loss 0.08696892 Test RE 3.0685561939650216e-05 c 1.000208 k 0.009992592 m 4.997386\n",
      "95 Train Loss 0.08264172 Test RE 2.7556256367512765e-05 c 1.0001572 k 0.00999408 m 4.9986567\n",
      "96 Train Loss 0.07994591 Test RE 2.5488819697520757e-05 c 1.0001656 k 0.009993929 m 4.998563\n",
      "97 Train Loss 0.07860717 Test RE 2.6106201967553888e-05 c 1.0001144 k 0.009995409 m 4.9991803\n",
      "98 Train Loss 0.077339515 Test RE 2.65513667649652e-05 c 1.0000186 k 0.009998146 m 5.000506\n",
      "99 Train Loss 0.07599406 Test RE 2.6135273124049818e-05 c 1.0001136 k 0.009995069 m 4.9996195\n",
      "100 Train Loss 0.07422047 Test RE 2.6784332007175204e-05 c 1.0003028 k 0.009989177 m 4.9987974\n",
      "101 Train Loss 0.067069195 Test RE 2.71969053167632e-05 c 1.0002618 k 0.009990159 m 4.9995537\n",
      "102 Train Loss 0.063099176 Test RE 2.9005449297658337e-05 c 0.9999649 k 0.009997716 m 5.00113\n",
      "103 Train Loss 0.060163576 Test RE 3.193931541594132e-05 c 1.0000174 k 0.009995409 m 5.0030446\n",
      "104 Train Loss 0.057385594 Test RE 3.426175571186359e-05 c 1.0001705 k 0.009990743 m 5.003804\n",
      "105 Train Loss 0.056568034 Test RE 3.504483146847146e-05 c 1.0001925 k 0.009989376 m 5.0029826\n",
      "106 Train Loss 0.055793095 Test RE 3.544214113719249e-05 c 1.0001643 k 0.009989226 m 5.002529\n",
      "107 Train Loss 0.054779463 Test RE 3.621645298937456e-05 c 1.0002458 k 0.009986435 m 5.0022287\n",
      "108 Train Loss 0.054184705 Test RE 3.635271273149539e-05 c 1.0002754 k 0.009984262 m 5.002168\n",
      "109 Train Loss 0.053135935 Test RE 3.6957443222358824e-05 c 1.0002819 k 0.009983657 m 5.0031004\n",
      "110 Train Loss 0.04958032 Test RE 3.6367003820342676e-05 c 1.0000944 k 0.009987771 m 5.001344\n",
      "111 Train Loss 0.043417823 Test RE 3.258786268250036e-05 c 1.0001839 k 0.009988761 m 5.0003605\n",
      "112 Train Loss 0.041017786 Test RE 3.0477728084047316e-05 c 1.0002708 k 0.009985753 m 5.0014615\n",
      "113 Train Loss 0.0399345 Test RE 2.987126058617305e-05 c 1.000261 k 0.009985606 m 5.0025454\n",
      "114 Train Loss 0.039282136 Test RE 2.9734636320714077e-05 c 1.000254 k 0.009986707 m 5.0017686\n",
      "115 Train Loss 0.03867147 Test RE 2.8919624627509213e-05 c 1.0002766 k 0.009986606 m 4.9999943\n",
      "116 Train Loss 0.03842738 Test RE 2.8286348693430754e-05 c 1.0003105 k 0.009985286 m 4.998946\n",
      "117 Train Loss 0.038237676 Test RE 2.7915852891745306e-05 c 1.0003629 k 0.0099832835 m 4.9982347\n",
      "118 Train Loss 0.03757199 Test RE 2.7845047679740536e-05 c 1.0004514 k 0.0099796625 m 4.997985\n",
      "119 Train Loss 0.03631542 Test RE 2.819988079123257e-05 c 1.0003443 k 0.009983185 m 4.9996634\n",
      "120 Train Loss 0.035873823 Test RE 2.8343031030650657e-05 c 1.0003103 k 0.009984209 m 5.000079\n",
      "121 Train Loss 0.03509582 Test RE 2.778247515638951e-05 c 1.0002552 k 0.009985279 m 5.0007215\n",
      "122 Train Loss 0.03334477 Test RE 2.639412992505996e-05 c 1.0002786 k 0.009984962 m 5.000404\n",
      "123 Train Loss 0.028122118 Test RE 2.4037395611940245e-05 c 1.0003651 k 0.009983523 m 4.9987617\n",
      "124 Train Loss 0.022044724 Test RE 1.9153507661095146e-05 c 1.000051 k 0.009989486 m 5.002117\n",
      "125 Train Loss 0.01960907 Test RE 1.6126160522434362e-05 c 1.0001756 k 0.0099895755 m 5.004991\n",
      "126 Train Loss 0.017191881 Test RE 1.318316684208081e-05 c 1.0002462 k 0.009988215 m 5.0026903\n",
      "127 Train Loss 0.016145226 Test RE 1.2532749324678635e-05 c 1.0002787 k 0.009985893 m 4.9983797\n",
      "128 Train Loss 0.015606259 Test RE 1.1478832253934181e-05 c 1.000391 k 0.009982811 m 4.9974055\n",
      "129 Train Loss 0.015455361 Test RE 1.1505034278646879e-05 c 1.0004289 k 0.009981837 m 4.997513\n",
      "130 Train Loss 0.015207734 Test RE 1.1471164304770038e-05 c 1.0004381 k 0.009981673 m 4.9968724\n",
      "131 Train Loss 0.014958687 Test RE 1.0704141785358071e-05 c 1.0004028 k 0.009982265 m 4.997201\n",
      "132 Train Loss 0.014867929 Test RE 1.0416400728718746e-05 c 1.0003755 k 0.009983148 m 4.997505\n",
      "133 Train Loss 0.014751904 Test RE 9.972069356727476e-06 c 1.000388 k 0.0099830115 m 4.9974227\n",
      "134 Train Loss 0.014678783 Test RE 9.768227276711565e-06 c 1.0003942 k 0.00998315 m 4.9972463\n",
      "135 Train Loss 0.014673617 Test RE 9.757095678511226e-06 c 1.000409 k 0.009982683 m 4.997139\n",
      "136 Train Loss 0.014667172 Test RE 9.732017810038732e-06 c 1.0004001 k 0.009982989 m 4.9971714\n",
      "137 Train Loss 0.01466698 Test RE 9.730943114177037e-06 c 1.0004001 k 0.009982989 m 4.9971714\n",
      "138 Train Loss 0.014666689 Test RE 9.7281531447636e-06 c 1.0004001 k 0.009982989 m 4.997173\n",
      "139 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "140 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "141 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "142 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "143 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "144 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "145 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "146 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "147 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "148 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "149 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "150 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "151 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "152 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "153 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "154 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "155 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "156 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "157 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "158 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "159 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "160 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "161 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "162 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "163 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "164 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "165 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "166 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "167 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "168 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "169 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "170 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "171 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "172 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "173 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "174 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "175 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "176 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "177 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "178 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "179 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "180 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "181 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "182 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "183 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "184 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "185 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "186 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "187 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "188 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "189 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "190 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "191 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "192 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "193 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "194 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "195 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "196 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "197 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "198 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "199 Train Loss 0.014620829 Test RE 9.498715911811227e-06 c 1.0003853 k 0.009983528 m 4.997238\n",
      "Training time: 83.11\n",
      "Training time: 83.11\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 200\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "m_full = []\n",
    "k_full = []\n",
    "c_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "\n",
    "for reps in range(max_reps):   \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []   \n",
    "    beta_val = []\n",
    "    m_val = []\n",
    "    k_val = []\n",
    "    c_val = []\n",
    "\n",
    "    torch.manual_seed(reps*36)\n",
    "    N_f = 10000 #Total number of collocation points\n",
    "\n",
    "    layers = np.array([1,50,50,50,1]) #9 hidden layers\n",
    "    PINN = Sequentialmodel(layers)\n",
    "\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.05, \n",
    "                                max_iter = 10, \n",
    "                                max_eval = 15, \n",
    "                                tolerance_grad = 1e-6, \n",
    "                                tolerance_change = 1e-6, \n",
    "                                history_size = 100, \n",
    "                                line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "    train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    beta_full.append(beta_val)\n",
    "    m_full.append(m_val)\n",
    "    k_full.append(k_val)\n",
    "    c_full.append(c_val)\n",
    "    \n",
    "\n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"beta\": beta_full,\"m\": m_full,\"k\": k_full,\"c\": c_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "pmHEeBpzfFQh",
    "outputId": "990b2054-f35e-4c8e-c378-69ff2eb19f4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f22b6f08d10>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAjEAAAGdCAYAAADjWSL8AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAABCqUlEQVR4nO3deViU9f7G8fewuuHkBkgumZIbrpgILqCmaaHtm0ZaWmkumdpi55TW76QeWzyauVa2WXRaLOskZSW4gQtK7mYuiQviwuoCwnx/f3SaE1kGCj4zcL+ua64rZr7CPd/LmNvP8zwzNmOMQURERMTNeFgdQERERORiqMSIiIiIW1KJEREREbekEiMiIiJuSSVGRERE3JJKjIiIiLgllRgRERFxSyoxIiIi4pa8rA5QVhwOB4cPH8bPzw+bzWZ1HBERESkGYww5OTkEBQXh4XHhWUu5LTGHDx+mfv36VscQERGRi5Camkq9evUuuKbclhg/Pz/gl02oXr26xWlERESkOLKzs6lfv77zdfxCym2J+fUQUvXq1VViRERE3ExxTgXRib0iIiLillRiRERExC2pxIiIiIhbUokRERERt6QSIyIiIm5JJUZERETckkqMiIiIuCWVGBEREXFLKjEiIiLillRiRERExC2pxIiIiIhbUokRERERt6QSIyIiIiVybEsaj95+iC++sDaHSoyIiIgUS2EhfH/PAnxaN6XHJ48wZgwUFFiXRyVGRERE/lJSElx7LXwTewI72TSpcpgPF2Tj5WVdJgt/tIiIiLi6kz8eZ8bTR3n+k5YAHLSPpddNQUQtGIinj6el2TSJERERkfM4ChysvG8BNGvKHZ/chRfnGDwYtv7oQ8+377O8wIAmMSIiIvI7O2NTKHhwGF1z1wJwrFI91rx3hGtva2BxsqJUYkRERASArIM5bOz3LN1SZuKJg2z82HjT83SJHYlXJderDK6XSERERC4rY+CL2alcO7oT3R2HAVjT4C6uXvwKUe2DLE7351RiREREKrA9e2DECPj663p8R1PyvStz4vnZRDzV2+pof0klRkREpALKy85j5Z2vMmD5gxzLt+PrayNl1CIi/laDhldUsjpesajEiIiIVDCbpsdT/anhXJe/k2f4mSXXvcrs2RAcXNfqaCWiEiMiIlJBHNuWzq7+j9Nl7zu/fO3hT+gjEYycCTabxeEugt4nRkREpJxzFDhYEbMAr1bN6LL3HRzYWNFyOD57dhLx6j1uWWBAkxgREZFybfNmSIp+gYdSnwVgZ+W2FM6aS7cHwixOduk0iRERESmHcnPh8cehfXv4e+rDHLA1IOHm6TQ5uZ6W5aDAgCYxIiIi5YsxrH36c3bOWsZLua8B0O02fzxf3E1kIx+Lw5UulRgREZFy4uCaAxy+dSRhR78gDIj3v5E7Ft7ADTcAlK8CAyoxIiIibq8gr5BVd71Kh8//Tj1OkY83qyMe57XPo6hS2+p0ZUclRkRExI3t/GATjqEPEnU6GYAfqneh6rvz6N6/hcXJyp5O7BUREXFDp07BE+MK8RxwJy1OJ5OFnZUx82l1IoEmFaDAgCYxIiIibufrOMOw4Tb27/dkCzN5tv5CGn8xg65t3Osddy+VJjEiIiJu4ti2dFZfNZDFfeexfz80aAAjv+xL+IF/41/BCgxoEiMiIuLyjMOweuhCWr41ns4mg+Ys5YoR9/L3qdWoVs3qdNZRiREREXFh+77+kcy7H6ZLZjzwyzvuOuYuYOp9Fbi9/JdKjIiIiAvKz81nzc3T6PTdP2hEHqeowvobn6PLx2PwqqSXb9A5MSIiIi4nMRHuabuDrt9NpBJ5rK/dh5MJW4n6crwKzG9oJ0RERFxE1okCnn7WizlzwJg2TKvyHN3ub0zEzLuxebjpR02XIU1iREREXMDaJz8lO6AJy2dvxxi4/3546MDf6TzrHhWYP6FJjIiIiIXSN6ex54aRhB/6BIAX/KZi/+wdevSwOJgb0CRGRETEAsZhWDX0LbzbtiD80Cecw4vlEX+jz8/zVWCK6ZJKzJQpU7DZbIwZM8Z5nzGGSZMmERQUROXKlYmKimLbtm1F/lxeXh6jRo2idu3aVK1alf79+3Pw4MEiazIyMoiJicFut2O324mJiSEzM/NS4oqIiLiEgyv3sbHO9XR5435qmAx2VG7P3tj1dF/9DyrXqGR1PLdx0SVm/fr1zJ8/n9atWxe5f9q0abzyyivMmjWL9evXExgYSK9evcjJyXGuGTNmDIsXLyY2NpZVq1aRm5tLdHQ0hYWFzjUDBgwgJSWFuLg44uLiSElJISYm5mLjioiIWK6wEGbOhDk9Pyb05DLOUIn4G6YRfHItTe9qa3U892MuQk5OjgkODjbLli0zkZGR5tFHHzXGGONwOExgYKCZOnWqc+3Zs2eN3W43c+fONcYYk5mZaby9vU1sbKxzzaFDh4yHh4eJi4szxhizfft2A5ikpCTnmsTERAOYnTt3FitjVlaWAUxWVtbFPEUREZFStX3zORMebgwY48k5s7jucLM3bpfVsVxOSV6/L2oSM2LECG688Uauu+66Ivfv27ePtLQ0evfu7bzP19eXyMhI1qxZA0BycjLnzp0rsiYoKIiQkBDnmsTEROx2O2FhYc41nTp1wm63O9f8Xl5eHtnZ2UVuIiIiVjt3Kp/4nv9HfpsObEw8i58fzJrjRf+Ds2l0/TVWx3NrJb46KTY2lo0bN7J+/frzHktLSwMgICCgyP0BAQH8/PPPzjU+Pj7UqFHjvDW//vm0tDT8/f3P+/7+/v7ONb83ZcoUnnvuuZI+HRERkTKz/e31eA0bQtTZLQBMbvMhd3wxiPr1LQ5WTpRoEpOamsqjjz7Ke++9R6VKf37ikc1W9Hp2Y8x59/3e79f80foLfZ8JEyaQlZXlvKWmpl7w54mIiJSV08dPk3DteJoO7sQ1Z7dw3Fab1SPe57GN96nAlKISlZjk5GTS09MJDQ3Fy8sLLy8vEhISmDlzJl5eXs4JzO+nJenp6c7HAgMDyc/PJyMj44Jrjh49et7PP3bs2HlTnl/5+vpSvXr1IjcREZHLLWX6co7XbUXkhpfxxMGqqwbC9h1607oyUKIS07NnT7Zs2UJKSorz1qFDBwYOHEhKSgpXX301gYGBLFu2zPln8vPzSUhIICIiAoDQ0FC8vb2LrDly5Ahbt251rgkPDycrK4t169Y516xdu5asrCznGhEREVeSlQXDhsHRsVNpULCXw571WPfsl3TZ9x61m9W2Ol65VKJzYvz8/AgJCSlyX9WqValVq5bz/jFjxjB58mSCg4MJDg5m8uTJVKlShQEDBgBgt9sZMmQI48aNo1atWtSsWZPx48fTqlUr54nCzZs3p0+fPjz44IPMmzcPgIceeojo6GiaNm16yU9aRESkNH35eSHDRnhy6BDEMY83Q6YT+tX/0bG+jgqUpVL/2IEnnniCM2fO8Mgjj5CRkUFYWBjffPMNfn5+zjXTp0/Hy8uLO++8kzNnztCzZ0/eeustPD09nWsWLVrE6NGjnVcx9e/fn1mzZpV2XBERkYuWseck23qP4eDeKhxiLk2awOuvX0Vk5Ayro1UINmOMsTpEWcjOzsZut5OVlaXzY0REpNSt/dsSrpr6MAGONArw5KWhu3h0ZmMqV7Y6mXsryeu3PgBSRESkBE7uPsGO3o/Sef8iAPb4NOPs7IU8NaSxxckqHn0ApIiISDGtfWoxBc1a0nn/IgrxIL7Tk1x5dBMth3SyOlqFpBIjIiLyF44fhyF3ZNP4nw/i7zjKHp/m7HwzkajEqVS6Qh/YaBUdThIREbmATz+F4cMhPb06p2yzGdZpE52+mqjy4gI0iREREfkDx3ccI7HBXSy67RPS06FlSxi/7k6i1kxRgXERKjEiIiK/kzjuI0zLloSn/puZjObZJ/NIToYOHaxOJr+lw0kiIiL/dWxbOj/1GUH4wY8B2O0bQsHrb/Hcvb4WJ5M/okmMiIhUeMZhSBzzIbZWLQk/+DEFeBLf9RkaHk+m+b2hVseTP6FJjIiIVGhHj8JLAzbx4vd3A7CrUmvMGwuJGtDe4mTyV1RiRESkwvr4418+tPHEifY0sQ2jabcAIr58Gp9qPlZHk2JQiRERkQonY28Gm3o/yZg9z3KCerRpA2ELZ9O2nc3qaFICOidGREQqlHXPLeVscAg99ixgPg/zzDOwbh0qMG5IkxgREakQsg/lkNJjLN1+fB2AvT5NaTjnWW54wOJgctE0iRERkXJv0/R4shq2ptuPr+PARkL7x6h7ZBMtHwizOppcAk1iRESk3Dp9Gt6/8zOG/ucWAFK9ruLky28ROTrS4mRSGlRiRESkXEpMhEGDIHX39XShKektomj/7YvUr+tndTQpJTqcJCIi5Upedh6fXT+Hbp0L2b0bal1ZmYOLN9Bt21yqqcCUK5rEiIhIubHrwxRsg+/j5rNbGM1pjsWMY8YMqFGjmtXRpAyoxIiIiNsrOFvAquipdP7uObwp4JitDnc80ZhOU61OJmVJJUZERNzanv/s5Oxd9xF1aj0ASUG30mTZHDq18Lc4mZQ1nRMjIiJuyeGAr+59n6DodrQ8tZ5M2xWsHv4eYakfU1sFpkLQJEZERNzOgQMweDAcXx7Cehysr92Hektfp3OHK62OJpeRJjEiIuI2jIEvpu2gdWtYvhz2VGnNkr+vo8PRr6irAlPhaBIjIiJuIWNvBtujHqFP6sc0IRHvTh14911o0qSN1dHEIprEiIiIy0ue9h1ng1vROTUWG4bn+m1g5Upo0sTqZGIlTWJERMRlnTl5hqSeT9M95V8A7PcO5vS8d7nxfn3mkajEiIiIi9q+aBM+D9xL9/ztAKwIGU6H71+kSp2qFicTV6HDSSIi4lIKCuCFF2DBfStpkr+dox6BbHj+K7ptma0CI0VoEiMiIi5jz24H9w32YM0asDGSji2yuX7xMDpcU9vqaOKCNIkRERHLGYdhxaA3yG0Wyg9rcqleHd5+x4O7t/6dmiow8ic0iREREUsd25bOnuseolva5wC82GgONyx/nIYNLQ4mLk+TGBERsczaZ7+EVq3olPY5+XiTcMM/eWjnWBUYKRZNYkRE5LI7ffw06yPHEbl9LgC7fUMw775H5B164zopPpUYERG5rDZtgh09xjMg85cCkxA6lrBvX6DSFZUsTibuRoeTRETksnA44KWXICwMxmY+yzavNmz85zIiN7ysAiMXRZMYEREpc0c2HOLTAR/z+O5HAYi4JZDA+ZuoVdtmcTJxZyoxIiJSptZO+Izgfw5hhDnJGp8riZp1O0OHgs2mAiOXRiVGRETKxKn0UyRHjqXbzvkA7Kjcnn98EkKjvhYHk3JD58SIiEip27FoI0frh9Jt53wc2Ijv+ASN0xNp1LeZ1dGkHNEkRkRESo3DAd/ePpeoxaPx4RxHPK4kbdo7RI3rYXU0KYdUYkREpFQcPAiDBkGl7+vTm3MkBd3KNfHzaRdcy+poUk6pxIiIyCX76o0j3Pt4XTIyoEqVG/li9GqiXwjH5qGTd6XsqMSIiMhFy03LZVPkGMJ//ISqbKZxh/osWgTXXBNhdTSpAHRir4iIXJQd7yVzrEF7uv74BnaymN53GatXwzXXWJ1MKgqVGBERKRFHgYP4fi/TOCacRud2c9izHpunf8/tXz2Aj4/V6aQi0eEkEREptmNbj7K/+2CijscBkBR0K01XLKBt45oWJ5OKSJMYEREplm+/hdhO07n2eBxnqMSKAXMJS/2YGiowYhFNYkRE5ILOnYNnnoFp08DXTKRJ9f00eevvdLslxOpoUsGpxIiIyJ9KTdjL6rtn8mLayxg8GfRwZSJfiaVKFauTiajEiIjIn1gzOpaWrz7M3WSzq1IgIe89xW23WZ1K5H9UYkREpIhT6afY1GUUXXYvBOAHv84MWXoP9TpbHEzkd3Rir4iIOO36MIWj9UPpsnshDmws7/IMLdPjqde5odXRRM6jSYyIiGAMfH3/B3R/ezC+5HPEI4i0lxbR/bEoq6OJ/CmVGBGRCu7ECXjgAdi7JIR1eLA2oB9NEt6kXdPaVkcTuSAdThIRqcASPzlMmzawZAn86NOKT59cR8fDn1NLBUbcgEqMiEgFVJhfSHzkRNrdfjVXHlrLNddAUhIMnNpKnzwtbkOHk0REKpi0jYdJ6zGAqKwEAP7e+gu6rw6jWjWLg4mUkCYxIiIVSPLkr/Hs0Ja2WQnkUI3Vjyyi3w//UIERt6RJjIhIBVBwtoBVPZ8las0UAHZVaoPPZ/+m8/XXWJxM5OJpEiMiUs4dPAgvtP3IWWBWtBhGg8NJNFKBETenSYyISDm2dCnExMCJE3fT1OsrGjwSTbcZd1kdS6RUqMSIiJRD506fY9mN07krfji5+NGunY0O/36XJk2sTiZSelRiRETKmcNJBzje+x5uyFnDHDaTNOI9XnoJKlWyOplI6dI5MSIi5ci6Z7+kUkQ7WuesIRM7jcfdwqxZKjBSPmkSIyJSDpw7fY7V3SYQlfwyANurdKDafz4kPOpqi5OJlJ0STWLmzJlD69atqV69OtWrVyc8PJylS5c6HzfGMGnSJIKCgqhcuTJRUVFs27atyPfIy8tj1KhR1K5dm6pVq9K/f38OHjxYZE1GRgYxMTHY7XbsdjsxMTFkZmZe/LMUESnHDiamsjOgm7PAJLR9lMZHVtFABUbKuRKVmHr16jF16lQ2bNjAhg0b6NGjBzfddJOzqEybNo1XXnmFWbNmsX79egIDA+nVqxc5OTnO7zFmzBgWL15MbGwsq1atIjc3l+joaAoLC51rBgwYQEpKCnFxccTFxZGSkkJMTEwpPWURkfLj88/hur7e1MndR6btCtY+tZjITf/Ct7qv1dFEyp65RDVq1DCvv/66cTgcJjAw0EydOtX52NmzZ43dbjdz5841xhiTmZlpvL29TWxsrHPNoUOHjIeHh4mLizPGGLN9+3YDmKSkJOeaxMREA5idO3cWO1dWVpYBTFZW1qU+RRERl5N3usA89pgx8MttSIs1JnXlPqtjiVyykrx+X/SJvYWFhcTGxnLq1CnCw8PZt28faWlp9O7d27nG19eXyMhI1qxZA0BycjLnzp0rsiYoKIiQkBDnmsTEROx2O2FhYc41nTp1wm63O9f8kby8PLKzs4vcRETKo0OJB9jl34WD0/8NwLhxMHtTOPW6XGVtMJHLrMQlZsuWLVSrVg1fX1+GDRvG4sWLadGiBWlpaQAEBAQUWR8QEOB8LC0tDR8fH2rUqHHBNf7+/uf9XH9/f+eaPzJlyhTnOTR2u5369euX9KmJiLi8Df+Io3LndrTKTeJl2+Ms+Tifl14CHx+rk4lcfiUuMU2bNiUlJYWkpCSGDx/OoEGD2L59u/Nxm63oR7gbY8677/d+v+aP1v/V95kwYQJZWVnOW2pqanGfkoiIyyvMLyS+6zO0f+YGapqTbK8SCvHx9LtN7UUqrhJfYu3j40OT/77lY4cOHVi/fj0zZszgySefBH6ZpNStW9e5Pj093TmdCQwMJD8/n4yMjCLTmPT0dCIiIpxrjh49et7PPXbs2HlTnt/y9fXF11cnsolI+XNsWzqpXQcQlfEdAAktH6HTmld08q5UeJf8ZnfGGPLy8mjUqBGBgYEsW7bM+Vh+fj4JCQnOghIaGoq3t3eRNUeOHGHr1q3ONeHh4WRlZbFu3TrnmrVr15KVleVcIyJSUSQuzaSgdXvaZ3xHLlVZ/cgiIre+pgIjQgknMU8//TR9+/alfv365OTkEBsbS3x8PHFxcdhsNsaMGcPkyZMJDg4mODiYyZMnU6VKFQYMGACA3W5nyJAhjBs3jlq1alGzZk3Gjx9Pq1atuO666wBo3rw5ffr04cEHH2TevHkAPPTQQ0RHR9O0adNSfvoiIq7JGHj5ZXjqqSuY6ribW3y+wnz8CZ37Nbc6mojLKFGJOXr0KDExMRw5cgS73U7r1q2Ji4ujV69eADzxxBOcOXOGRx55hIyMDMLCwvjmm2/w8/Nzfo/p06fj5eXFnXfeyZkzZ+jZsydvvfUWnp6ezjWLFi1i9OjRzquY+vfvz6xZs0rj+YqIuLysnzN5bNgZFsb9cmh+891TGDZ9EtUCq1mcTMS12IwxxuoQZSE7Oxu73U5WVhbVq1e3Oo6ISLHsWLSRKoPv4GBBIL2943l5pjcPPwx/cX2ESLlRktdvfXaSiIgLMA7DyvsW0HHRaCqRh4eXg8SPDtL6pkZWRxNxWSoxIiIWO5V+ik3hw+m2910A1vlHc03SO9RvVOMv/qRIxXbJVyeJiMjF2/vVTg43CKPL3ncpxIP4PlPpcOhzrlCBEflLKjEiIhaJ/cCQ3m8IwXnbOOoRyJZ/fU/U0ifx8NKvZpHi0P8pIiKXWX4+jB4N9wywcZ9jIatr9cO2aRNtH420OpqIW1GJERG5jNI2HmZyyPu8+uovX9/21DWEpS3Bv3WgtcFE3JBO7BURuUxSZiQQNPYunnEcY2OVAIZ+0JP+/a1OJeK+VGJERMqYcRgSbplOlyVP4EUhP1ZqxczPG3LVdVYnE3FvKjEiImUo53AOW8OGEHXwIwBWXTWQduvmU7VOFYuTibg/nRMjIlJG9n61k/RGYYQf/IhzeJFwxyw673lXBUaklKjEiIiUgU8+gRm3xtM4fwdHPILYOTeByH+PwOahzw8QKS06nCQiUooKCuDpp+HFFwEeplmjHG77LIZWuvpIpNRpEiMiUkqObUvn2/r3M//FTADGjbMxdNfjunxapIxoEiMiUgq2vp5ErYdvp4/jEAs8z8IHH3DHHVanEinfVGJERC6BcRhWDJhL+IeP4sM59vg0o93Hz9Kkn9XJRMo/lRgRkYt0+vhpNoYNJ3LvOwAkXnkbLdcupPqVfhYnE6kYdE6MiMhFOLDyZ1LrR9Bl7zu/fPr0jS/S6cBHKjAil5FKjIhICcXFQffoqlQ6m8ExWx02v/wtUV+O1+XTIpeZSoyISDEZh2HqVLjhBtibXZu/tfqCgrUbaTe2u9XRRCoknRMjIlIMuWm5bL72AXYevAHDYIYOhVmzWuPra3UykYpLJUZE5C/s/24P5268mYi8rYQQR+TLNzH4sRrYdPRIxFI6nCQicgEb/hHHFb06EJy3laMegeyfvZT7x6rAiLgCTWJERP6AcRjib/gnkV8/jQeGLdU6USfhE1q3D7I6moj8l0qMiMjv5GY72NzyHrof/DcAK5o9SNjaV/GtrhNgRFyJDieJiPzGTz9BpwgPvjnYnHy8WTFwHt12zFeBEXFBmsSIiPxX3BfnuOc+bzIz4WTAs9z80u10uzfE6lgi8ic0iRGRCs84DPG9J1OrfwRnM88QHg4bNnrQVgVGxKVpEiMiFVrO4Ry2dryfqEOfADA76t8MiBuk938RcQMqMSJSYe1ftpuCfjcTnredfLxJGjiL+98bZHUsESkmHU4SkQpp/fNLqdH7WprkbSfNoy675iXQ7b2HrI4lIiWgSYyIVCjGwFd3vU3fj+7HA8NmvwgCVnxMq7Z1rY4mIiWkEiMiFcaZMzB0KKz+KJL11GJHs1vptP5VfKr5WB1NRC6CSoyIVAiHdp/mpnuqkJwMXl5X8dXzPzBogt59V8Sd6ZwYESn3Ns9PxKNZMHWTv6BWLfj2W1RgRMoBTWJEpFxb+cBCOi4chi/5PFdlGjXWRdPoan16o0h5oBIjIuVSwdkCVoePJzJlBgBJQbcSkvw21QJVYETKCx1OEpFyJ2PPSX64sq+zwMRHTaLjzx9RLbCaxclEpDRpEiMi5cqOxEyqdOtIaMEecqnK1sffIWrarVbHEpEyoEmMiJQbX3wBHXtfwRcFfUj1uoojH6+hkwqMSLmlEiMibs84DC9OOsVNN0FuLnzWbTpVtm0g+LbWVkcTkTKkw0ki4tZOHz9NSvsHaJ96DE/ieHiEN9One+PtXcvqaCJSxlRiRMRtHUo8QHbPm4k4s4lzePHx42u5aVoXq2OJyGWiw0ki4pY2z16FT+draX5mE8dsddj+6vcqMCIVjCYxIuJ2Vt7/JmFvDcOHc+ys3JZqyz6jTeeGVscSkctMJUZE3EZhISztPo3olU8CkFjvDlonL6Sqf1WLk4mIFXQ4SUTcQnY29O8PT628gSyqEx81iU4/f6gCI1KBaRIjIi5v/86zRN9eiW3boFKlEOJn/MhNDwVYHUtELKZJjIi4tM2vrcSnZRNqbFtJ3bqwYgUqMCICaBIjIi5s1dC36PjGQ/hwjn9Wf4GG6+O48kqrU4mIq1CJERGXU5hfyMouE4ha/yIAiVfeTtuUt6lS2+JgIuJSVGJExKXkHM5hR/sBRB39EoD4bs/S7buJeHjp6LeIFKUSIyIu40DKSfI6RdIxbytn8SV55FtEvXq31bFExEXpnzYi4hJWr4YOvWqwIS+Eox6B7HlzBZ1VYETkAlRiRMRy775VSI8ecOy4jZlt3qQwaQMt7+9odSwRcXE6nCQilnGcK2RF16eptHYv5/iQW2/14J13KlO1qi5BEpG/phIjIpbITctlW7uBRKUtASBnwHIGv9sTD82HRaSYVGJE5LI7lHiA3B79CDu7+ZcTeIe/wQOze1odS0TcjP7NIyKX1baF6/Du0pGmZzeT7hHATwvi6Tx7oNWxRMQNaRIjIpfNmic+o92L91CZs/xYqRVVv/+SkPAGVscSETelSYyIlDlj4J//hMdfrIMHDtbXuYG6e1ZzpQqMiFwCTWJEpEzl58Mjj8AbbwB05tU7VzHm7XZ4VdKvHxG5NJrEiEiZydyXwborb2bdG5vx8ICZM2H8h9eqwIhIqdBvEhEpEz9/v4fCvjfSJX8XH3rsZu/izdzY39PqWCJSjmgSIyKl7ofZq6l6XSeuzt/FYc96eHzwvgqMiJQ6lRgRKVWrR7xPsxE9qG2Os71KKJ4b1tH0zjZWxxKRcqhEJWbKlClce+21+Pn54e/vz80338yuXbuKrDHGMGnSJIKCgqhcuTJRUVFs27atyJq8vDxGjRpF7dq1qVq1Kv379+fgwYNF1mRkZBATE4PdbsdutxMTE0NmZubFPUsRKXPGYYjv/hydZw/El3yS6t5Cw30JBLSta3U0ESmnSlRiEhISGDFiBElJSSxbtoyCggJ69+7NqVOnnGumTZvGK6+8wqxZs1i/fj2BgYH06tWLnJwc55oxY8awePFiYmNjWbVqFbm5uURHR1NYWOhcM2DAAFJSUoiLiyMuLo6UlBRiYmJK4SmLSGk7exbuG1hIfvxqAJZ3fIKOBz6mqn9Vi5OJSLlmLkF6eroBTEJCgjHGGIfDYQIDA83UqVOda86ePWvsdruZO3euMcaYzMxM4+3tbWJjY51rDh06ZDw8PExcXJwxxpjt27cbwCQlJTnXJCYmGsDs3LmzWNmysrIMYLKysi7lKYrIX0hPN6ZzZ2PAmBoemebboR9YHUlE3FhJXr8v6ZyYrKwsAGrWrAnAvn37SEtLo3fv3s41vr6+REZGsmbNGgCSk5M5d+5ckTVBQUGEhIQ41yQmJmK32wkLC3Ou6dSpE3a73bnm9/Ly8sjOzi5yE5GytXfpLt5r+n+sXm2w2+HfX9vpueBuq2OJSAVx0SXGGMPYsWPp0qULISEhAKSlpQEQEBBQZG1AQIDzsbS0NHx8fKhRo8YF1/j7+5/3M/39/Z1rfm/KlCnO82fsdjv169e/2KcmIsWw6eXvqXljJx7LeJanar1OYiJcd53VqUSkIrnoEjNy5Eg2b97MBx98cN5jNputyNfGmPPu+73fr/mj9Rf6PhMmTCArK8t5S01NLc7TEJGLsPL+NwkZfz1XmEy2VAtnbMJNNG9udSoRqWguqsSMGjWKJUuWsHz5curVq+e8PzAwEOC8aUl6erpzOhMYGEh+fj4ZGRkXXHP06NHzfu6xY8fOm/L8ytfXl+rVqxe5iUjpchQ4iA+fQNe3huBNAWsa3E1w6vfUaXn+5FREpKyVqMQYYxg5ciSffvop33//PY0aNSryeKNGjQgMDGTZsmXO+/Lz80lISCAiIgKA0NBQvL29i6w5cuQIW7duda4JDw8nKyuLdevWOdesXbuWrKws5xoRubzOZuWR1HggUUlTAVje9Vk67X2fSldUsjiZiFRUJfrYgREjRvD+++/z+eef4+fn55y42O12KleujM1mY8yYMUyePJng4GCCg4OZPHkyVapUYcCAAc61Q4YMYdy4cdSqVYuaNWsyfvx4WrVqxXX/PaDevHlz+vTpw4MPPsi8efMAeOihh4iOjqZp06al+fxFpBhOnIBJUYnMOPAh5/AiacjrdH99kNWxRKSiK8llT8Af3hYuXOhc43A4zMSJE01gYKDx9fU13bp1M1u2bCnyfc6cOWNGjhxpatasaSpXrmyio6PNgQMHiqw5ceKEGThwoPHz8zN+fn5m4MCBJiMjo9hZdYm1SOnYs8eYa6755RLqRyvNNcnTvrU6koiUYyV5/bYZY4x1FarsZGdnY7fbycrK0vkxIhdp65vrGPS4PxtPXkX9+vDVV/DfixFFRMpESV6/9dlJIvKHkp76jKuHRPHuyRvo1iqDpCQVGBFxLSU6J0ZEKob4W2bQ7bPH8MBwqk4j/vONN9UCrU4lIlKUSoyIOBXmF7IybBxRKTMASGgxjM7Jr+JVSb8qRMT16DeTiABw+vhpNre+l6gjiwGI7/tPIr98HJvHhd+oUkTEKjonRkRIT4evWzxGpyOLycOHNaNjifrqCRUYEXFpmsSIVHC7dkHfvnD62HM08tyAx8wZRDzSxepYIiJ/SSVGpAJb+8lB+j5Yj4wMuPrqQCr/ZwNNm2n6IiLuQYeTRCqoNaNjaXN7E27IeI+wMEhMRAVGRNyKSoxIBWMchvg+U4l49R4qkcfD9f7D998Z/PUZjiLiZnQ4SaQCKThbwJp2I4jaOR+A+PaP0TXxRTx9NIEREfejEiNSQeQczmFn27vodmwpDmysvG0GUR+PsjqWiMhFU4kRqQDS9p0ho2UU157ZyGkqs2XCB0ROvsnqWCIil0TnxIiUc7t2QXiPynxypi/HbHXY92Y8YSowIlIOqMSIlGNrVhsiImD/fni78f9xavUPtLy/o9WxRERKhUqMSDm1dsJnnO3Wi9Mnz9CxI6xJtHFVeF2rY4mIlBqdEyNSDiXcPZsuH47CEwdzms3kju+fpGpVq1OJiJQulRiRcsQ4DAmdnyYqaSoAK5o9yL2bxuFVyeJgIiJlQCVGpJzIz81nXZuhRO19F4D4Hs8Tuezv+hBHESm3VGJEyoHsg9nsbnMbXU5+SwGeJN6/gKg377c6lohImdKJvSJu7vBhuLd3Og1PbiKXqmx6/ku6qsCISAWgSYyIG9uxA/r0gQMHmhBT4z+8PMOLa2NCrY4lInJZqMSIuKnNc1Yz6YnTHMjtRXAwvBYXxtVXW51KROTyUYkRcUNJT3xK2xcH8DbeVG61hhnft6J2batTiYhcXioxIm4m4Y5ZdP14NB4Yfgi8ngXfN6aKCoyIVEA6sVfETTgKHMSHPUnkx6PwwLCixTBC931CldpVrI4mImIJTWJE3EB+bj7rW91P1P73AYjv9QKRcRP0HjAiUqFpEiPi4rKy4I32s+i8/33O4cWqoW8R9c3TKjAiUuFpEiPiwg4fhr59YfvuUfh7reWq54fQZUJvq2OJiLgElRgRF/XTdz/T6/567E/1JDDQm0b/+ZD27a1OJSLiOnQ4ScQFbX1jLTV6hfJU6iNcE2xYswYVGBGR31GJEXExG/4RR6OhPahlTtC5agqrvj5Fo0ZWpxIRcT0qMSIuZPWwd2nzTD+qcpoNta7nqp++o06jalbHEhFxSSoxIi4ivt/LdJ53H94UsPqqgbTev4RqgSowIiJ/RiVGxGIOB3wb/gxRX44HIL79WMJ3v4NPNR+Lk4mIuDZdnSRioXPnYOhQOJnUkUi8WH3DZCK/fByb3gJGROQvqcSIWOTUKbjjDli6FDw9+7Fkyi5ue1wfQy0iUlw6nCRigZO7T7Ch/s3sXLqXypXh889RgRERKSFNYkQus0OJBzgbdT2R+Tv5yPMI+d8mER6h40ciIiWlSYzIZbR78VY8ukTQOH8nhzzrY1/8lgqMiMhFUokRuUw2z15Fndu6UtdxiJ98W2BbvZom/ZpbHUtExG2pxIhcBuue+YLgEb24wmSy2S+CWttWEhRW3+pYIiJuTSVGpIy9+boDxz8mU5mzrPOPpsneZdRoXNPqWCIibk8lRqSMGANTpsCQBz3oxxK+aP032u1fTJXaVayOJiJSLqjEiJQBR4GDObcu4+mnf/l66FN1iE75B96VdUGgiEhp0W9UkVKWn5PHhpDBPHIglo0sIGT6UMaMsTqViEj5oxIjUopyDufwY6tbiTj5Lfl4c/+IqnQeY3UqEZHySSVGpJQc25bOsY43EHo6mVyqsmvyp3Se0NvqWCIi5ZZKjEgpSE3YS2Gv62lx7ieO22pzdOFSQgd1sDqWiEi5phIjcom2rsygTo/OBDjSSPW8ioL/fE3L66+xOpaISLmnq5NELkF8PHSOrsEsxyPsqtQa7/VraKQCIyJyWajEiFykT/5dyPXXQ3Y2rOj6dwL2JBLYrq7VsUREKgyVGJGLsGLAXOre1RWv/FPccgt8/Y2NK4L0JnYiIpeTzokRKQHjMCT0eI6ohOcAWND5be766BE8PS0OJiJSAanEiBRTYX4hq9uNIGr7PACWd5vIPcuHY9M8U0TEEioxIsVwNvMsKS0H0u3wpziwsfKu1+geO9zqWCIiFZpKjMhfyPo5k31tbqZTVgJ5+LBx7CIiX77d6lgiIhWeBuEiF3DkCAzoc5K6WTvIxo9tL8URrgIjIuISNIkR+RO7d8P118O+fVczsGYcs2ZB+3vaWR1LRET+SyVG5A/seHcDk0adYF/W9TRuDPO+bkfjxlanEhGR31KJEfmd5KnLuGbCrbyJA6+mK3glIZSAAKtTiYjI7+mcGJHfWDM6llYTbsSPXHbVCGfOd9eowIiIuCiVGJH/Srh1BhGv3oMP51hT/y5a7v8P1a/0szqWiIj8CZUYqfCMwxAfMYHIxWMAiG89ik5738e3uq+1wURE5IJUYqRCKyiAN6LeJSpxKgDLr3uByE0z8PDS/xoiIq6uxL+pV6xYQb9+/QgKCsJms/HZZ58VedwYw6RJkwgKCqJy5cpERUWxbdu2Imvy8vIYNWoUtWvXpmrVqvTv35+DBw8WWZORkUFMTAx2ux273U5MTAyZmZklfoIif+b0abj1Vhi2cgAfczsr71tA92VPY/OwWR1NRESKocQl5tSpU7Rp04ZZs2b94ePTpk3jlVdeYdasWaxfv57AwEB69epFTk6Oc82YMWNYvHgxsbGxrFq1itzcXKKjoyksLHSuGTBgACkpKcTFxREXF0dKSgoxMTEX8RRFzpe5P5O+vQr44gvwruSF9+J/0/XtoVbHEhGRkjCXADCLFy92fu1wOExgYKCZOnWq876zZ88au91u5s6da4wxJjMz03h7e5vY2FjnmkOHDhkPDw8TFxdnjDFm+/btBjBJSUnONYmJiQYwO3fuLFa2rKwsA5isrKxLeYpSDh1el2p2+7Ywb3C/sVd3mBUrrE4kIiK/Ksnrd6ke+N+3bx9paWn07t3beZ+vry+RkZGsWbMGgOTkZM6dO1dkTVBQECEhIc41iYmJ2O12wsLCnGs6deqE3W53rhG5GHu+3IEjPIImedvp6/E1iZ8eoWtXq1OJiMjFKNU3u0tLSwMg4HdvrBEQEMDPP//sXOPj40ONGjXOW/Prn09LS8Pf3/+87+/v7+9c83t5eXnk5eU5v87Ozr74JyLl0tY31hL04A3UNCfZ690U7++/pnmXIKtjiYjIRSqTSzBstqInRhpjzrvv936/5o/WX+j7TJkyxXkSsN1up379+heRXMqrDc9/RaOhPahpTrK1akfsW1ZRv0tDq2OJiMglKNUSExgYCHDetCQ9Pd05nQkMDCQ/P5+MjIwLrjl69Oh53//YsWPnTXl+NWHCBLKyspy31NTUS34+Uj6sfPg92k7sT1VOs752H67a8z21mta2OpaIiFyiUi0xjRo1IjAwkGXLljnvy8/PJyEhgYiICABCQ0Px9vYusubIkSNs3brVuSY8PJysrCzWrVvnXLN27VqysrKca37P19eX6tWrF7lJxWYMvPgi/GN+HQw2VjW6l7Y/L6FaQFWro4mISCko8Tkxubm5/PTTT86v9+3bR0pKCjVr1qRBgwaMGTOGyZMnExwcTHBwMJMnT6ZKlSoMGDAAALvdzpAhQxg3bhy1atWiZs2ajB8/nlatWnHdddcB0Lx5c/r06cODDz7IvHnzAHjooYeIjo6madOmpfG8pZxzOGD8eJg+HeB6Xrs3idEL2+lN7EREypOSXvq0fPlyA5x3GzRokDHml8usJ06caAIDA42vr6/p1q2b2bJlS5HvcebMGTNy5EhTs2ZNU7lyZRMdHW0OHDhQZM2JEyfMwIEDjZ+fn/Hz8zMDBw40GRkZxc6pS6wrrrycPPNN8CMmmF0GjHnpJasTiYhIcZXk9dtmjDEWdqgyk52djd1uJysrS4eWKpCcwznsbnUr7U9+yy6uYcPCrQwc7G11LBERKaaSvH6X6iXWIlY6ti2dYx1voP3pZHKpSs4/ZqrAiIiUYyoxUi6kJuylsNf1tDj3E8dttTn65ld0GHyt1bFERKQMqcSI29v1YQo1BvTB33GUVK+rKPjya1pef43VsUREpIypxIhbi4+HcwMn0NRxlF2VWnNFYhz129a1OpaIiFwGut5U3NbHH8P118Pdhe/xZeBQAnetIEAFRkSkwlCJEbf08RPruPNOyM+H7rfV4rp9C7A3sFsdS0RELiOVGHErxmGI7/oMt78YxjAzm+HD4cMPoVIlq5OJiMjlpnNixG0UnC0gse1wona9DsAdPU4S9Rr8xWeLiohIOaUSI27hzMkzbG55N13TllCIB6sHzqH7ew9ZHUtERCykEiMuL3NfBj+37U9Y9irO4ssPT8XSbcrNVscSERGLqcSISzvw41nyWnelTd42Mm1X8PPMJYSN7Gp1LBERcQE6sVdc1g8/QHj3SryeF8MRjytJ/2gFbVRgRETkv1RixCV9900hXbvC4cPwnxZP4EjZzDW3tbI6loiIuBCVGHE5qx9ZhF+fCExODpGRsGq1jStb1bQ6loiIuBiVGHEZxmGI7zOVznPupaNZx9zWc/j6a7jiCquTiYiIK9KJveISCvMLWRU6mqitswGI7zCeexLH46G/oSIi8if0EiGWO3PyDD+EDCDyyGc4sLHiln8R9eloq2OJiIiLU4kRS53YdZzDHfrTKTeRs/iyadwiol66zepYIiLiBnROjFhm7164re9pauXuJ8NWg12zviVcBUZERIpJkxixRHIy3HADpKc34IHApby2wIc20c2tjiUiIm5EJUYuu/XPL+XlyXmk591Mmzbw5ldtCAqyOpWIiLgblRi5rFbct4CId4ezEG+u6JTEtK/bUL261alERMQdqcTIZeEocLCiywSi1k4DIOnqgcxc1hyfahYHExERt6USI2XuzMkz/NA6hqhDnwAQHzWJyO+exeZhsziZiIi4M5UYKVPHtqVztFN/OuWuJR9v1g97k6g591odS0REygFdYi1lZscOeLvrAkJy15Jhq8GOmd/SWQVGRERKiSYxUiaWL4dbb4XszKeoVf0YkbHDadO3qdWxRESkHNEkRkrdt4/9h+heeWRmQqcIT6J/+hdXq8CIiEgpU4mRUmMchuVdn+W6f0Uzp/BB7rrT8N13UKeO1clERKQ80uEkKRVnTp5hY/shdP/5AwAaRNTn/UUGDy9dgSQiImVDJUYu2dGUIxzrcjOdT63jHF4kDZ5H1MIHrI4lIiLlnA4nySXZsWgjjtBrCTm1jgxbDba+/A1dVWBEROQy0CRGLtqnH+QRdm9/6nKIn3ya4/3VEtr1bGJ1LBERqSA0iZESMwaefx5uG+BLDO+QVKcfdXYn0lAFRkRELiOVGCmR08dPM6F3MhMn/vJ1mzE96HB4CfYGdmuDiYhIhaMSI8V2ZMMh9jeM5Klve9LScycLFsD06eClg5IiImIBlRgpli3zE7GFXUuL0xsotHnx7r9OMHSo1alERKQi07+h5S+tuHc+nRaNxIdz/OTbAp+4L2gXdbXVsUREpIJTiZE/lZedx9qwUXTbuQCAxCtvI2TdQvyC/CxOJiIiosNJ8icOHYL5ITPptnMBDmwsv34KnQ58pAIjIiIuQ5MYOc+qVXD77XDy6KM08V5JnYkj6P63662OJSIiUoRKjDgZhyHu4cXcsrA/eYVetGrlwzWLl9C4sdXJREREzqfDSQJAblouq5vcR9/Xb+P/Cidw992QmIgKjIiIuCxNYoSfPt+G7c476JK/gwI8ufbGAMa/DzZ9ALWIiLgwTWIquFUPvk3dmzvSOH8HRzyC2P5aPFFfjleBERERl6dJTAV1+vhpkiNG0XX3mwBsqNWbhgnv0rqlv8XJREREikeTmApo1y64q/NB2u3+kEI8iO/xPO0Of0UdFRgREXEjKjEViDHw1lvQoQN8+eM1jK7+Nj+8uIyo757B08fT6ngiIiIlosNJFUTmvgy2RQ7n7dSHyaU7kZHwwge3Ubeu1clEREQujiYxFUDKv+I5Hdyazqkf8iYPMPX/zvHdd6jAiIiIW9MkphzLz81nTa+JdEv6Jx4Y9nkHc2bBIp4c5G11NBERkUumElNO/fTZVvIHDibqdDIAK68ZQruEf1EtsJrFyUREREqHDieVMwUFMG/sLurfEkqL08lk2GqQOP5juu56XQVGRETKFU1iypFt22DwYNiwoSk16U9D/7M0WDqP8PZBVkcTEREpdSox5cC50+dYcdu/uO+7wRw+V4crroBzL73DtfdXwuaht94VEZHySYeT3NwPs1ayv2Y7esY9wfRzI4iO/mUiM2BIZRUYEREp1zSJcVMndh1ne78n6Lp7IQDHbbUJeqgfS2YblRcREakQNIlxM44CBysHv4GteVNngVnR7EE8du2ky9wYFRgREakwNIlxIwkJsO7eWTx+8FEAdlVqTf6MuXR7KNziZCIiIpefJjFuYPfOQm65BaKi4P8ODmaPrQnx/V+mcUYyrVRgRESkgtIkxoWlbTzMrsFTsG3ZzGfE4+lp496HquP3zE6i6uoDG0VEpGJTiXFBR1OOsHPwVMJ+mEckeQA8FRZPzJvdadECQAVGREREJcaFHFz9M3tGTqdjyjwiOQvAD9W74Hj2OaaM625xOhEREdeiEuMCkpLgk4mbmfJNe+pRCMBmvwgK/v4c7cb31BVHIiIif0AlxiJZqdnEv7qZyQldWLcOoBWDaEZ+jUDM40/Q/sleKi8iIiIX4PJXJ82ePZtGjRpRqVIlQkNDWblypdWRLlp+bj4b/hHHqsaD8G4QSOSL0WxedwYfHxg82IZZnUj7k98SOqG3CoyIiMhfcOlJzIcffsiYMWOYPXs2nTt3Zt68efTt25ft27fToEEDq+MVy8mfTrJr9nc4Pl1MyM//oQPZzseO+FzFa4/+zI3jmhEQAOBnWU4RERF3YzPGGKtD/JmwsDDat2/PnDlznPc1b96cm2++mSlTplzwz2ZnZ2O328nKyqJ69eplHRWAs1l57P1yGynH67Nmdx1WrIDrtrzCK4xzrjnqEciuZjdT87FBtHwgTBMXERGR3yjJ67fLTmLy8/NJTk7mqaeeKnJ/7969WbNmzXnr8/LyyMvLc36dnZ193prSsH07fPbPXXRZ9woeeafxOnuKatmHqXUmlQDHEVpgeJnXeZMhAHjSnZ98W3Cw9Y3UGnoLLR8II8DL5Y/iiYiIuDyXLTHHjx+nsLCQgF+OszgFBASQlpZ23vopU6bw3HPPlXmu1FT46p1jPM38P3z8pK0m7Zqc4rFoiIiAbt3a4e+/jSZlnkxERKRicdkS8yubrejhFmPMefcBTJgwgbFjxzq/zs7Opn79+qWep0kTuHFEI+K3PwdVq+JRrQo+DQKp3rI+dUIbULt5HUbqEJGIiEiZc9kSU7t2bTw9Pc+buqSnp583nQHw9fXF19e3zHM1bgwTZl0JPFvmP0tERET+nMuenOHj40NoaCjLli0rcv+yZcuIiIiwKJWIiIi4CpedxACMHTuWmJgYOnToQHh4OPPnz+fAgQMMGzbM6mgiIiJiMZcuMXfddRcnTpzg+eef58iRI4SEhPDVV1/RsGFDq6OJiIiIxVz6fWIuhRXvEyMiIiKXpiSv3y57ToyIiIjIhajEiIiIiFtSiRERERG3pBIjIiIibkklRkRERNySSoyIiIi4JZUYERERcUsqMSIiIuKWVGJERETELbn0xw5cil/fiDg7O9viJCIiIlJcv75uF+cDBcpticnJyQGgfv36FicRERGRksrJycFut19wTbn97CSHw8Hhw4fx8/PDZrOV6vfOzs6mfv36pKam6nOZLkD7VDzap+LRPhWP9ql4tE/FY8U+GWPIyckhKCgID48Ln/VSbicxHh4e1KtXr0x/RvXq1fWXvxi0T8WjfSoe7VPxaJ+KR/tUPJd7n/5qAvMrndgrIiIibkklRkRERNySSsxF8PX1ZeLEifj6+lodxaVpn4pH+1Q82qfi0T4Vj/apeFx9n8rtib0iIiJSvmkSIyIiIm5JJUZERETckkqMiIiIuCWVGBEREXFLKjElNHv2bBo1akSlSpUIDQ1l5cqVVkey1IoVK+jXrx9BQUHYbDY+++yzIo8bY5g0aRJBQUFUrlyZqKgotm3bZk1YC02ZMoVrr70WPz8//P39ufnmm9m1a1eRNdormDNnDq1bt3a+sVZ4eDhLly51Pq49+mNTpkzBZrMxZswY533aK5g0aRI2m63ILTAw0Pm49uh/Dh06xL333kutWrWoUqUKbdu2JTk52fm4q+6VSkwJfPjhh4wZM4a//e1vbNq0ia5du9K3b18OHDhgdTTLnDp1ijZt2jBr1qw/fHzatGm88sorzJo1i/Xr1xMYGEivXr2cn21VUSQkJDBixAiSkpJYtmwZBQUF9O7dm1OnTjnXaK+gXr16TJ06lQ0bNrBhwwZ69OjBTTfd5PxlqT063/r165k/fz6tW7cucr/26hctW7bkyJEjztuWLVucj2mPfpGRkUHnzp3x9vZm6dKlbN++nZdffpkrrrjCucZl98pIsXXs2NEMGzasyH3NmjUzTz31lEWJXAtgFi9e7Pza4XCYwMBAM3XqVOd9Z8+eNXa73cydO9eChK4jPT3dACYhIcEYo726kBo1apjXX39de/QHcnJyTHBwsFm2bJmJjIw0jz76qDFGf59+NXHiRNOmTZs/fEx79D9PPvmk6dKly58+7sp7pUlMMeXn55OcnEzv3r2L3N+7d2/WrFljUSrXtm/fPtLS0orsma+vL5GRkRV+z7KysgCoWbMmoL36I4WFhcTGxnLq1CnCw8O1R39gxIgR3HjjjVx33XVF7tde/c/u3bsJCgqiUaNG3H333ezduxfQHv3WkiVL6NChA3fccQf+/v60a9eOBQsWOB935b1SiSmm48ePU1hYSEBAQJH7AwICSEtLsyiVa/t1X7RnRRljGDt2LF26dCEkJATQXv3Wli1bqFatGr6+vgwbNozFixfTokUL7dHvxMbGsnHjRqZMmXLeY9qrX4SFhfHOO+/w9ddfs2DBAtLS0oiIiODEiRPao9/Yu3cvc+bMITg4mK+//pphw4YxevRo3nnnHcC1/z6V20+xLis2m63I18aY8+6TorRnRY0cOZLNmzezatWq8x7TXkHTpk1JSUkhMzOTTz75hEGDBpGQkOB8XHsEqampPProo3zzzTdUqlTpT9dV9L3q27ev879btWpFeHg4jRs35u2336ZTp06A9gjA4XDQoUMHJk+eDEC7du3Ytm0bc+bM4b777nOuc8W90iSmmGrXro2np+d5rTM9Pf28diq/+PUqAO3Z/4waNYolS5awfPly6tWr57xfe/U/Pj4+NGnShA4dOjBlyhTatGnDjBkztEe/kZycTHp6OqGhoXh5eeHl5UVCQgIzZ87Ey8vLuR/aq6KqVq1Kq1at2L17t/4+/UbdunVp0aJFkfuaN2/uvGjFlfdKJaaYfHx8CA0NZdmyZUXuX7ZsGRERERalcm2NGjUiMDCwyJ7l5+eTkJBQ4fbMGMPIkSP59NNP+f7772nUqFGRx7VXf84YQ15envboN3r27MmWLVtISUlx3jp06MDAgQNJSUnh6quv1l79gby8PHbs2EHdunX19+k3OnfufN5bPvz44480bNgQcPHfT1adUeyOYmNjjbe3t3njjTfM9u3bzZgxY0zVqlXN/v37rY5mmZycHLNp0yazadMmA5hXXnnFbNq0yfz888/GGGOmTp1q7Ha7+fTTT82WLVvMPffcY+rWrWuys7MtTn55DR8+3NjtdhMfH2+OHDnivJ0+fdq5RntlzIQJE8yKFSvMvn37zObNm83TTz9tPDw8zDfffGOM0R5dyG+vTjJGe2WMMePGjTPx8fFm7969JikpyURHRxs/Pz/n72zt0S/WrVtnvLy8zAsvvGB2795tFi1aZKpUqWLee+895xpX3SuVmBJ67bXXTMOGDY2Pj49p37698xLZimr58uUGOO82aNAgY8wvl+ZNnDjRBAYGGl9fX9OtWzezZcsWa0Nb4I/2CDALFy50rtFeGfPAAw84//+qU6eO6dmzp7PAGKM9upDflxjtlTF33XWXqVu3rvH29jZBQUHm1ltvNdu2bXM+rj36ny+++MKEhIQYX19f06xZMzN//vwij7vqXtmMMcaaGZCIiIjIxdM5MSIiIuKWVGJERETELanEiIiIiFtSiRERERG3pBIjIiIibkklRkRERNySSoyIiIi4JZUYERERcUsqMSIiIuKWVGJERETELanEiIiIiFtSiRERERG39P+odR5v5spLGwAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_pred = PINN.test()\n",
    "plt.plot(t,x_true,'b')\n",
    "plt.plot(t,x_pred,'r--')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "stan_1D_FODE_tune.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
