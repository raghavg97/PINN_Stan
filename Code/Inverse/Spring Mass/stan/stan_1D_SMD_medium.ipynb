{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8byrnUmNKGR",
    "outputId": "510f67c6-b225-41ba-e78a-d0a6e54ded85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uoNYpDzlNKGV"
   },
   "outputs": [],
   "source": [
    "tstart= 0 \n",
    "tstop= 60 \n",
    "\n",
    "increment = 0.1 \n",
    "# Initial condition\n",
    "x_init= [0,0] \n",
    "\n",
    "t = np.arange(tstart,tstop+1,increment)\n",
    "\n",
    "c = 1 # Damping constant \n",
    "k = 0.05 # Stiffness of the spring\n",
    "m = 5 # Mass \n",
    "F0 = 100\n",
    "\n",
    "def mydiff(x, t):    \n",
    "    F =F0\n",
    "    \n",
    "    dx1dt = x[1] \n",
    "    dx2dt = (F -c*x[1] -k*x[0])/m\n",
    "    dxdt= [dx1dt, dx2dt] \n",
    "    \n",
    "    return dxdt \n",
    "\n",
    "x_full_sol = odeint(mydiff, x_init, t) \n",
    "\n",
    "x_sol = x_full_sol[:,0]\n",
    "v_sol = x_full_sol[:,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BR02v-fkNKGV"
   },
   "outputs": [],
   "source": [
    "level = \"medium\"\n",
    "label = \"1D_SMD_stan_\" + level\n",
    "\n",
    "loss_thresh = 0.005\n",
    "\n",
    "t_test = t.reshape(-1,1)\n",
    "t_test_tensor = torch.from_numpy(t_test).float().to(device)\n",
    "\n",
    "# bc1_t = t[0].reshape(-1,1)\n",
    "# bc1_x = x_sol[0].reshape(-1,1)\n",
    "# t_bc1_train = torch.from_numpy(bc1_t).float().to(device)\n",
    "# x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "\n",
    "bc1_t = t.reshape(-1,1)\n",
    "bc1_x = x_sol.reshape(-1,1)\n",
    "t_bc1_train = torch.from_numpy(bc1_t).float().to(device)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "\n",
    "\n",
    "bc2_t = t[0].reshape(-1,1)\n",
    "t_bc2_train = torch.from_numpy(bc2_t).float().to(device)\n",
    "bc2_val = v_sol[0].reshape(-1,1)\n",
    "bc2_val =torch.from_numpy(bc2_val).float().to(device)\n",
    "\n",
    "t_test = t.reshape(-1,1)\n",
    "t_test_tensor = torch.from_numpy(t_test).float().to(device)\n",
    "x_true = x_sol\n",
    "x_true_norm = np.linalg.norm(x_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(t[0]) \n",
    "ub = np.array(t[-1]) \n",
    "\n",
    "#torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SyyktBKBXRo1"
   },
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    \n",
    "    t01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=t01,random_state =seed)\n",
    "\n",
    "    t_coll = lb + (ub-lb)*sampling(N_f)\n",
    "    # t_coll = np.vstack((t_coll,)) # append training points to collocation points \n",
    "\n",
    "    return t_coll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "o1b21zLnNKGW"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "        \n",
    "        self.beta = Parameter(1.0*torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "        \n",
    "        self.k = Parameter(torch.tensor(0.0))\n",
    "        self.k.requiresGrad = True\n",
    "        self.c = Parameter(torch.tensor(0.0))\n",
    "        self.c.requiresGrad = True\n",
    "        self.m = Parameter(torch.tensor(0.0))\n",
    "        self.m.requiresGrad = True\n",
    "        \n",
    "    'forward pass'\n",
    "    def forward(self,t):\n",
    "        if torch.is_tensor(t) != True:         \n",
    "            t = torch.from_numpy(t)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        t = 2.0*(t - l_b)/(u_b - l_b) - 1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = t.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z) + self.beta[:,i]*z*self.activation(z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,t,x):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(t), x)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,t_bc2,bc2_val):\n",
    "        g = t_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        x = self.forward(g)    \n",
    "            \n",
    "        x_t = autograd.grad(x,g,torch.ones([t_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dx_dt = x_t[:,[0]]\n",
    "        \n",
    "        bc2 = dx_dt\n",
    "        \n",
    "        loss_bc2= self.loss_function(bc2,bc2_val)\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self,t_coll,f_hat):\n",
    "             \n",
    "        g = t_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        x = self.forward(g) \n",
    "\n",
    "        x_t = autograd.grad(x,g,torch.ones([t_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        x_tt = autograd.grad(x_t,g,torch.ones(t_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dx_dt = x_t[:,[0]]\n",
    "        \n",
    "        dx2_d2t = x_tt[:,[0]]\n",
    "        \n",
    "        f = self.m*dx2_d2t + self.c*dx_dt + self.k*x - F0\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,t_bc1,x_bc1,t_bc2,bc2_val,t_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(t_bc1,x_bc1)\n",
    "        loss_bc2 = self.loss_BC2(t_bc2,bc2_val)\n",
    "        loss_f = self.loss_PDE(t_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self):\n",
    "        x_pred = self.forward(t_test_tensor)\n",
    "        x_pred = x_pred.cpu().detach().numpy()\n",
    "\n",
    "        return x_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        x_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(x_pred.reshape(-1,1) - x_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(x_pred.reshape(-1,1) - x_true.reshape(-1,1),2)/x_true_norm\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fLY2mT5BOgjD"
   },
   "outputs": [],
   "source": [
    "def train_step(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8srA5uGuObil"
   },
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    m_val.append(PINN.m.cpu().detach().numpy())\n",
    "    k_val.append(PINN.k.cpu().detach().numpy())\n",
    "    c_val.append(PINN.c.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0SezTZ_racQB"
   },
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "\n",
    "    t_coll = colloc_pts(N_f,rep*11)\n",
    "    t_coll =  torch.from_numpy(t_coll).float().to(device)\n",
    "    \n",
    "    f_hat = torch.zeros(t_coll.shape[0],1).to(device)\n",
    "    for i in range(max_iter):      \n",
    "        train_step(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat)\n",
    "         \n",
    "        loss_np = PINN.loss(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test RE\",test_re_loss[-1],\"c\",PINN.c.cpu().detach().numpy(),\"k\",PINN.k.cpu().detach().numpy(),\"m\",PINN.m.cpu().detach().numpy())\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I9BYbcJ0NKGX",
    "outputId": "42720d97-b37a-4c42-b7e5-b77a0ef391f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 589606.5 Test RE 0.3848806089071731 c -0.0018348111 k 0.56555307 m -0.00055219996\n",
      "1 Train Loss 85709.14 Test RE 0.18005549574346122 c -0.008770529 k -0.011641007 m -0.0007487669\n",
      "2 Train Loss 29115.72 Test RE 0.11051738605017701 c 0.002444791 k 0.06839118 m -0.00075350975\n",
      "3 Train Loss 3908.4255 Test RE 0.030396153366617558 c 0.10811652 k 0.048629675 m 0.004183497\n",
      "4 Train Loss 1581.5112 Test RE 0.007772181319229408 c 0.14393556 k 0.059415206 m 0.006282969\n",
      "5 Train Loss 1101.4949 Test RE 0.005883946496362737 c 0.3419388 k 0.054837387 m 0.02350199\n",
      "6 Train Loss 608.6615 Test RE 0.0092223120463433 c 0.77080035 k 0.05041158 m 0.06239933\n",
      "7 Train Loss 457.97577 Test RE 0.009593983968702815 c 0.9750159 k 0.046717536 m 0.082326524\n",
      "8 Train Loss 344.71143 Test RE 0.0076551603699623084 c 1.08903 k 0.045047544 m 0.09856171\n",
      "9 Train Loss 298.4146 Test RE 0.005843321337605668 c 1.109396 k 0.044845928 m 0.111714505\n",
      "10 Train Loss 279.25528 Test RE 0.0046244030012764885 c 1.1372654 k 0.04440788 m 0.12645723\n",
      "11 Train Loss 267.91205 Test RE 0.004897751326696863 c 1.1570776 k 0.04414548 m 0.15189694\n",
      "12 Train Loss 265.79233 Test RE 0.00501066904894879 c 1.1508849 k 0.04420592 m 0.16326551\n",
      "13 Train Loss 259.02966 Test RE 0.005063157806069216 c 1.1509966 k 0.044165194 m 0.18288687\n",
      "14 Train Loss 252.14297 Test RE 0.005009895084756039 c 1.1344947 k 0.04454927 m 0.20029503\n",
      "15 Train Loss 248.4286 Test RE 0.005178815175540725 c 1.109938 k 0.044852555 m 0.22545259\n",
      "16 Train Loss 244.40979 Test RE 0.005311808858769267 c 1.1377001 k 0.044465113 m 0.2540307\n",
      "17 Train Loss 241.9832 Test RE 0.005281674932330065 c 1.126155 k 0.044750486 m 0.28405505\n",
      "18 Train Loss 234.57559 Test RE 0.004737538854095055 c 1.1193964 k 0.044899274 m 0.3644025\n",
      "19 Train Loss 232.31177 Test RE 0.004545596443485955 c 1.123301 k 0.04495876 m 0.40652913\n",
      "20 Train Loss 230.43439 Test RE 0.004305683813289673 c 1.1200993 k 0.04499512 m 0.4462989\n",
      "21 Train Loss 227.12587 Test RE 0.004433927310021296 c 1.1189272 k 0.045083757 m 0.46975988\n",
      "22 Train Loss 225.08667 Test RE 0.00463968450414347 c 1.1247083 k 0.044938367 m 0.47576854\n",
      "23 Train Loss 221.29474 Test RE 0.004562175199529552 c 1.1263034 k 0.04489118 m 0.55752313\n",
      "24 Train Loss 218.332 Test RE 0.004274814988410927 c 1.1203564 k 0.04506726 m 0.59716606\n",
      "25 Train Loss 210.28105 Test RE 0.003673433388635284 c 1.119122 k 0.0452112 m 0.7841987\n",
      "26 Train Loss 202.23402 Test RE 0.003467191410423626 c 1.1091784 k 0.045509845 m 0.98806685\n",
      "27 Train Loss 188.58057 Test RE 0.0036798753819694626 c 1.0952061 k 0.045763284 m 1.1437496\n",
      "28 Train Loss 174.98227 Test RE 0.003941651153236648 c 1.0982577 k 0.045940932 m 1.3232734\n",
      "29 Train Loss 161.02155 Test RE 0.0036004132623565577 c 1.0436738 k 0.046876337 m 1.6388919\n",
      "30 Train Loss 154.48178 Test RE 0.00320781062114819 c 1.0747811 k 0.046456024 m 1.7843488\n",
      "31 Train Loss 144.10353 Test RE 0.0035123882303249683 c 1.1026975 k 0.046143644 m 1.8966419\n",
      "32 Train Loss 132.48575 Test RE 0.004315795596841241 c 1.0738485 k 0.04674633 m 2.1872423\n",
      "33 Train Loss 123.05525 Test RE 0.0040170373748381494 c 1.0719014 k 0.04706574 m 2.3969357\n",
      "34 Train Loss 93.86868 Test RE 0.002446907579290624 c 1.0062224 k 0.04810003 m 2.928907\n",
      "35 Train Loss 64.52728 Test RE 0.0022607749134896157 c 1.0545398 k 0.0479365 m 3.3126595\n",
      "36 Train Loss 56.191185 Test RE 0.002581009973645574 c 1.0393565 k 0.0482533 m 3.489829\n",
      "37 Train Loss 42.396713 Test RE 0.0016446153762662299 c 1.0143949 k 0.048898734 m 3.8018043\n",
      "38 Train Loss 34.072403 Test RE 0.0013943721966675955 c 1.0280993 k 0.048849765 m 4.0982604\n",
      "39 Train Loss 19.286459 Test RE 0.0009883428464892308 c 1.033314 k 0.048988443 m 4.4897428\n",
      "40 Train Loss 15.30978 Test RE 0.0008384012778522199 c 1.0039862 k 0.049606506 m 4.634269\n",
      "41 Train Loss 13.501678 Test RE 0.0007670525503321351 c 1.0061483 k 0.049709525 m 4.821622\n",
      "42 Train Loss 11.033523 Test RE 0.0006645585243041568 c 1.0042499 k 0.049850274 m 5.0228567\n",
      "43 Train Loss 9.195747 Test RE 0.0005920928858355943 c 0.9978683 k 0.050070595 m 5.1250186\n",
      "44 Train Loss 8.754498 Test RE 0.0005693675525502115 c 1.0004715 k 0.049953874 m 5.034391\n",
      "45 Train Loss 8.251945 Test RE 0.0005830494211172629 c 0.99985945 k 0.05002808 m 5.0374084\n",
      "46 Train Loss 7.0274763 Test RE 0.0005432290332915203 c 1.0130992 k 0.049728014 m 4.9724407\n",
      "47 Train Loss 5.7326164 Test RE 0.0004647560748925674 c 1.0018965 k 0.049915295 m 4.998434\n",
      "48 Train Loss 5.120988 Test RE 0.00046129127861093306 c 0.9926851 k 0.050160732 m 5.1179934\n",
      "49 Train Loss 4.3474894 Test RE 0.0003915657044683542 c 1.0004605 k 0.05010111 m 5.228683\n",
      "50 Train Loss 3.9141483 Test RE 0.00033061056118369696 c 0.9935361 k 0.050304174 m 5.302891\n",
      "51 Train Loss 3.5543878 Test RE 0.00033078829617900937 c 0.99549115 k 0.050254785 m 5.2655854\n",
      "52 Train Loss 2.9692194 Test RE 0.00026218846568093956 c 0.9992291 k 0.050125834 m 5.1648464\n",
      "53 Train Loss 2.7045298 Test RE 0.00027213232928292294 c 0.9944021 k 0.050191082 m 5.11928\n",
      "54 Train Loss 2.3412626 Test RE 0.00034892413787376223 c 0.99965405 k 0.050143514 m 5.1110063\n",
      "55 Train Loss 1.8936167 Test RE 0.0003174495239467848 c 0.9972069 k 0.050132953 m 5.123869\n",
      "56 Train Loss 1.574311 Test RE 0.00021459465883150872 c 0.9924402 k 0.050224263 m 5.1153345\n",
      "57 Train Loss 1.4624518 Test RE 0.00017857968969658644 c 0.99718916 k 0.050139494 m 5.115005\n",
      "58 Train Loss 1.4087409 Test RE 0.00017081345179022708 c 0.99701846 k 0.050167657 m 5.117497\n",
      "59 Train Loss 1.1821401 Test RE 0.00018493756135421025 c 0.9941526 k 0.050197016 m 5.1044044\n",
      "60 Train Loss 1.0528147 Test RE 0.00016017376060332704 c 0.99784464 k 0.050113685 m 5.072041\n",
      "61 Train Loss 1.0153008 Test RE 0.00014484012855926502 c 0.9974354 k 0.050102867 m 5.044104\n",
      "62 Train Loss 0.8457089 Test RE 0.00016243182229573074 c 0.995227 k 0.05013597 m 5.034809\n",
      "63 Train Loss 0.6406883 Test RE 0.00020398537434202312 c 0.9961389 k 0.050149802 m 5.061911\n",
      "64 Train Loss 0.59519696 Test RE 0.0002385240534872355 c 0.9964092 k 0.05015697 m 5.070241\n",
      "65 Train Loss 0.55009913 Test RE 0.00025204817654165884 c 0.9969063 k 0.050153587 m 5.04347\n",
      "66 Train Loss 0.5211541 Test RE 0.00023422807380258628 c 0.99707997 k 0.050131682 m 5.0271792\n",
      "67 Train Loss 0.5067724 Test RE 0.00024976792433349426 c 0.9973378 k 0.050129816 m 5.031115\n",
      "68 Train Loss 0.4400313 Test RE 0.00025897920066047055 c 0.99772096 k 0.05012878 m 5.032624\n",
      "69 Train Loss 0.34910235 Test RE 0.0002194979421128892 c 0.9968034 k 0.0501332 m 5.041081\n",
      "70 Train Loss 0.3269987 Test RE 0.0002013619623982847 c 0.9984473 k 0.05008513 m 5.0068307\n",
      "71 Train Loss 0.32232478 Test RE 0.00019570996749347552 c 0.99899274 k 0.050069395 m 4.997114\n",
      "72 Train Loss 0.3138407 Test RE 0.00018876730578589 c 0.99851537 k 0.050077472 m 5.0062747\n",
      "73 Train Loss 0.29686373 Test RE 0.00016761354190669578 c 0.99894243 k 0.050071523 m 5.0229588\n",
      "74 Train Loss 0.27905232 Test RE 0.00015794015307755443 c 0.99937844 k 0.05005836 m 5.007985\n",
      "75 Train Loss 0.26171422 Test RE 0.0001485349092489114 c 0.9980962 k 0.050072413 m 5.011775\n",
      "76 Train Loss 0.24613185 Test RE 0.00014073461070209165 c 0.99869555 k 0.050062146 m 5.015063\n",
      "77 Train Loss 0.24474692 Test RE 0.0001413228521739919 c 0.9989928 k 0.050055087 m 5.011168\n",
      "78 Train Loss 0.24183287 Test RE 0.0001347788070654958 c 0.9987132 k 0.05005851 m 5.015055\n",
      "79 Train Loss 0.22775015 Test RE 0.00012645822369659377 c 0.9976906 k 0.05009029 m 5.036422\n",
      "80 Train Loss 0.1733396 Test RE 6.795475936294765e-05 c 0.99880254 k 0.050058346 m 5.01903\n",
      "81 Train Loss 0.16046764 Test RE 5.6755054439490426e-05 c 0.9991722 k 0.05003485 m 5.005769\n",
      "82 Train Loss 0.15771542 Test RE 5.04197741460861e-05 c 0.99933636 k 0.050032634 m 5.0034976\n",
      "83 Train Loss 0.15462135 Test RE 4.887815788146617e-05 c 0.9996791 k 0.050024632 m 4.999598\n",
      "84 Train Loss 0.15251398 Test RE 4.9671779254195594e-05 c 0.999526 k 0.05002786 m 5.0022936\n",
      "85 Train Loss 0.15070072 Test RE 4.811356198651623e-05 c 0.9994174 k 0.050032612 m 5.006394\n",
      "86 Train Loss 0.14930119 Test RE 4.648222016491256e-05 c 0.99936736 k 0.050030634 m 5.0042806\n",
      "87 Train Loss 0.14733477 Test RE 4.776949672931397e-05 c 0.99942833 k 0.050030872 m 5.0064306\n",
      "88 Train Loss 0.14625508 Test RE 4.8342223011400605e-05 c 0.99940485 k 0.05002877 m 5.0073204\n",
      "89 Train Loss 0.14466617 Test RE 4.5755752060961444e-05 c 0.9994404 k 0.050029103 m 5.0075183\n",
      "90 Train Loss 0.14322825 Test RE 4.240152825593128e-05 c 0.9995933 k 0.050027043 m 5.0076823\n",
      "91 Train Loss 0.1423033 Test RE 4.1176391364974255e-05 c 0.9995457 k 0.05002602 m 5.004509\n",
      "92 Train Loss 0.13985877 Test RE 4.658458135933907e-05 c 0.99940306 k 0.050027702 m 5.0023074\n",
      "93 Train Loss 0.13802353 Test RE 4.735027828902948e-05 c 0.99957865 k 0.050026905 m 5.0022473\n",
      "94 Train Loss 0.13767461 Test RE 4.7507077372279764e-05 c 0.9994938 k 0.05002753 m 5.001792\n",
      "95 Train Loss 0.13712841 Test RE 4.999676448032828e-05 c 0.99951136 k 0.05002818 m 5.0014462\n",
      "96 Train Loss 0.1359825 Test RE 5.834211441905266e-05 c 0.9995299 k 0.05002849 m 5.000478\n",
      "97 Train Loss 0.13455817 Test RE 6.353197722137194e-05 c 0.99964136 k 0.050027262 m 5.0010533\n",
      "98 Train Loss 0.12808844 Test RE 5.679745896800391e-05 c 0.99981105 k 0.050029915 m 5.007539\n",
      "99 Train Loss 0.124593034 Test RE 5.0838330376788315e-05 c 0.999448 k 0.05002898 m 5.002585\n",
      "100 Train Loss 0.12210326 Test RE 5.206500293906391e-05 c 0.9996603 k 0.050022405 m 5.0002937\n",
      "101 Train Loss 0.1181235 Test RE 4.716289034157992e-05 c 0.99989694 k 0.05001818 m 4.998152\n",
      "102 Train Loss 0.11476468 Test RE 4.7258250473673866e-05 c 0.99925876 k 0.05002916 m 5.0008817\n",
      "103 Train Loss 0.10197535 Test RE 4.60025406061155e-05 c 0.9991512 k 0.050025944 m 5.0060434\n",
      "104 Train Loss 0.08807827 Test RE 3.831398983872223e-05 c 1.0001901 k 0.050013427 m 5.003453\n",
      "105 Train Loss 0.08126478 Test RE 3.80966496817062e-05 c 0.9997156 k 0.05001864 m 5.004385\n",
      "106 Train Loss 0.07660561 Test RE 4.082970955053636e-05 c 0.99981034 k 0.050012946 m 5.0017548\n",
      "107 Train Loss 0.07094615 Test RE 3.732204470696815e-05 c 0.99940896 k 0.05002526 m 5.0023947\n",
      "108 Train Loss 0.06553502 Test RE 3.660182215368082e-05 c 0.9992886 k 0.05002984 m 5.0085926\n",
      "109 Train Loss 0.06242666 Test RE 3.322412033136572e-05 c 0.99956733 k 0.050022732 m 5.005605\n",
      "110 Train Loss 0.06154183 Test RE 3.3277899454069466e-05 c 0.99941564 k 0.05002526 m 5.0060077\n",
      "111 Train Loss 0.060867794 Test RE 3.256124616246315e-05 c 0.99946046 k 0.050024513 m 5.007673\n",
      "112 Train Loss 0.06008749 Test RE 3.1229215075025235e-05 c 0.99964386 k 0.050020404 m 5.0059643\n",
      "113 Train Loss 0.0595942 Test RE 3.151209809678632e-05 c 0.9998329 k 0.050014738 m 5.0037284\n",
      "114 Train Loss 0.058784343 Test RE 3.0009988576627734e-05 c 0.9995749 k 0.050020613 m 5.0054445\n",
      "115 Train Loss 0.058250617 Test RE 2.918095715469448e-05 c 0.9996228 k 0.05001952 m 5.0042176\n",
      "116 Train Loss 0.05815873 Test RE 2.9040785068674806e-05 c 0.9996751 k 0.05001879 m 5.0033884\n",
      "117 Train Loss 0.05810398 Test RE 2.869733909303271e-05 c 0.9996226 k 0.050019376 m 5.0032325\n",
      "118 Train Loss 0.05799459 Test RE 2.8233121275925736e-05 c 0.99959105 k 0.050019465 m 5.0023403\n",
      "119 Train Loss 0.057636794 Test RE 2.8700683012984674e-05 c 0.9996418 k 0.050019417 m 5.0036445\n",
      "120 Train Loss 0.0569345 Test RE 2.9400358560260926e-05 c 0.9997821 k 0.05001939 m 5.0052485\n",
      "121 Train Loss 0.05595411 Test RE 3.089719189872919e-05 c 0.9998255 k 0.050019372 m 5.0040903\n",
      "122 Train Loss 0.05259548 Test RE 3.666331819264194e-05 c 0.999487 k 0.050022796 m 5.001345\n",
      "123 Train Loss 0.04007689 Test RE 3.199493766221628e-05 c 0.999816 k 0.05001254 m 4.9997354\n",
      "124 Train Loss 0.034587804 Test RE 2.7101514056767833e-05 c 0.9996701 k 0.050013475 m 4.999557\n",
      "125 Train Loss 0.03385245 Test RE 2.722226455514285e-05 c 0.99981916 k 0.050010547 m 4.998985\n",
      "126 Train Loss 0.031810693 Test RE 2.0454991760596957e-05 c 0.9998603 k 0.050007265 m 4.9967933\n",
      "127 Train Loss 0.030037642 Test RE 1.5915834082447623e-05 c 0.99967784 k 0.050009307 m 4.9982085\n",
      "128 Train Loss 0.029111277 Test RE 1.534954358824366e-05 c 0.99979264 k 0.05000934 m 4.999646\n",
      "129 Train Loss 0.027856726 Test RE 1.9152805578043097e-05 c 0.99994224 k 0.050005224 m 5.001226\n",
      "130 Train Loss 0.027626626 Test RE 1.983235657837475e-05 c 0.99987334 k 0.050005995 m 5.001019\n",
      "131 Train Loss 0.02732816 Test RE 1.978509842977824e-05 c 0.9998972 k 0.05000279 m 5.000539\n",
      "132 Train Loss 0.026672568 Test RE 2.0822960720047334e-05 c 0.999907 k 0.050005578 m 5.0013247\n",
      "133 Train Loss 0.026346026 Test RE 2.0745708224924617e-05 c 0.9999425 k 0.050003454 m 5.0014343\n",
      "134 Train Loss 0.026093021 Test RE 1.9273062264768465e-05 c 0.9998566 k 0.050004415 m 5.0010486\n",
      "135 Train Loss 0.025753075 Test RE 1.7772378639582252e-05 c 0.99991775 k 0.050005358 m 5.0009747\n",
      "136 Train Loss 0.025582515 Test RE 1.9104614064156052e-05 c 1.0000015 k 0.050003238 m 5.000805\n",
      "137 Train Loss 0.025366247 Test RE 2.014737771582497e-05 c 0.99989885 k 0.050004516 m 5.0005593\n",
      "138 Train Loss 0.025278525 Test RE 1.850439868540047e-05 c 0.9998423 k 0.050005127 m 5.000417\n",
      "139 Train Loss 0.025270762 Test RE 1.8116528203697263e-05 c 0.99985 k 0.05000512 m 5.000461\n",
      "140 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "141 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "142 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "143 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "144 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "145 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "146 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "147 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "148 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "149 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "150 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "151 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "152 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "153 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "154 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "155 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "156 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "157 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "158 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "159 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "160 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "161 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "162 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "163 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "164 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "165 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "166 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "167 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "168 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "169 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "170 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "171 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "172 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "173 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "174 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "175 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "176 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "177 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "178 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "179 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "180 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "181 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "182 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "183 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "184 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "185 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "186 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "187 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "188 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "189 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "190 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "191 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "192 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "193 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "194 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "195 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "196 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "197 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "198 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "199 Train Loss 0.025259284 Test RE 1.7960213085945357e-05 c 0.99985754 k 0.050005075 m 5.0004926\n",
      "Training time: 57.02\n",
      "Training time: 57.02\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 1152412.6 Test RE 0.6092010695263397 c -0.0054830513 k 0.97162515 m -0.0009862084\n",
      "1 Train Loss 144239.94 Test RE 0.25168103740877595 c -0.007144965 k 0.10163315 m -0.0010360419\n",
      "2 Train Loss 40224.01 Test RE 0.13084761545228615 c -0.0125042 k 0.063072644 m -0.0010374052\n",
      "3 Train Loss 4388.9463 Test RE 0.03014201915580642 c 0.04870494 k 0.062204123 m 0.0005738166\n",
      "4 Train Loss 2597.4998 Test RE 0.020469523493756206 c 0.062976584 k 0.056537557 m 0.0009780092\n",
      "5 Train Loss 2157.04 Test RE 0.015817282085099537 c 0.0664429 k 0.061022237 m 0.0013366521\n",
      "6 Train Loss 1699.4414 Test RE 0.011495414609262346 c 0.1424224 k 0.057862606 m 0.0073259063\n",
      "7 Train Loss 1281.412 Test RE 0.01021266069585422 c 0.3140301 k 0.057173874 m 0.021397645\n",
      "8 Train Loss 1081.0209 Test RE 0.010364903828621416 c 0.44808316 k 0.053829543 m 0.032711647\n",
      "9 Train Loss 620.1017 Test RE 0.010857985253778997 c 0.8903807 k 0.04755854 m 0.07269092\n",
      "10 Train Loss 480.5879 Test RE 0.009884598752252376 c 1.0719368 k 0.04474458 m 0.09351738\n",
      "11 Train Loss 394.67975 Test RE 0.008048377981885069 c 1.1197377 k 0.044210404 m 0.11054204\n",
      "12 Train Loss 337.17188 Test RE 0.007554202406525506 c 1.1711134 k 0.043988105 m 0.13047303\n",
      "13 Train Loss 310.22604 Test RE 0.006700513314798001 c 1.1727604 k 0.04364841 m 0.1402442\n",
      "14 Train Loss 300.24896 Test RE 0.006441344622226914 c 1.1509138 k 0.044381946 m 0.15260088\n",
      "15 Train Loss 287.34656 Test RE 0.006502771422122838 c 1.1634084 k 0.043903157 m 0.18112351\n",
      "16 Train Loss 253.20865 Test RE 0.005662488524584576 c 1.1673846 k 0.04414654 m 0.2871761\n",
      "17 Train Loss 235.02832 Test RE 0.005036031638781363 c 1.1283627 k 0.044807285 m 0.3753413\n",
      "18 Train Loss 229.36707 Test RE 0.004803065429601029 c 1.1207709 k 0.04499244 m 0.43616632\n",
      "19 Train Loss 226.48578 Test RE 0.004816145522594144 c 1.1193434 k 0.045029953 m 0.4749107\n",
      "20 Train Loss 224.15585 Test RE 0.004374776290151672 c 1.1135556 k 0.045140374 m 0.51787907\n",
      "21 Train Loss 220.12051 Test RE 0.00422844182199869 c 1.1137297 k 0.045210335 m 0.5808978\n",
      "22 Train Loss 216.25414 Test RE 0.004541912615415943 c 1.1224041 k 0.04508872 m 0.66587174\n",
      "23 Train Loss 212.3541 Test RE 0.004386142287989202 c 1.1024324 k 0.04543636 m 0.72572905\n",
      "24 Train Loss 208.02582 Test RE 0.004142851117895717 c 1.1171188 k 0.04521678 m 0.7669836\n",
      "25 Train Loss 196.36255 Test RE 0.0038057316353370257 c 1.0965735 k 0.045653738 m 1.0117003\n",
      "26 Train Loss 174.4618 Test RE 0.0033295222034064775 c 1.0798686 k 0.04641862 m 1.4661709\n",
      "27 Train Loss 154.55188 Test RE 0.00364819152739682 c 1.0908468 k 0.046110038 m 1.6391726\n",
      "28 Train Loss 146.35909 Test RE 0.0032980643376833165 c 1.0757743 k 0.046755504 m 1.7670848\n",
      "29 Train Loss 123.74092 Test RE 0.0024842245133724415 c 1.0603552 k 0.04742298 m 2.3254304\n",
      "30 Train Loss 108.278854 Test RE 0.0029027736089804915 c 1.0618974 k 0.04737369 m 2.3832035\n",
      "31 Train Loss 91.825745 Test RE 0.002463131401997707 c 1.0807674 k 0.047625747 m 2.9535594\n",
      "32 Train Loss 78.06094 Test RE 0.001830935624615861 c 1.010944 k 0.04861791 m 3.2117643\n",
      "33 Train Loss 55.217712 Test RE 0.0016273800040675437 c 1.0106114 k 0.04880109 m 3.5356164\n",
      "34 Train Loss 38.229607 Test RE 0.0016163401972988465 c 1.0406284 k 0.048683688 m 4.0786934\n",
      "35 Train Loss 28.55719 Test RE 0.001038120106930761 c 1.0158429 k 0.049306575 m 4.140526\n",
      "36 Train Loss 25.746605 Test RE 0.0010288617360828982 c 1.0219539 k 0.04920123 m 4.18867\n",
      "37 Train Loss 22.927145 Test RE 0.0009620650139982915 c 1.0088307 k 0.049550343 m 4.3791924\n",
      "38 Train Loss 16.904945 Test RE 0.0008061528523424651 c 1.0145459 k 0.04960048 m 4.545571\n",
      "39 Train Loss 11.766919 Test RE 0.001063649191006946 c 1.0092854 k 0.049771186 m 4.6888614\n",
      "40 Train Loss 9.932813 Test RE 0.0008599556515292104 c 0.9980301 k 0.049945388 m 4.7522306\n",
      "41 Train Loss 8.376563 Test RE 0.0007269887179125984 c 0.99576235 k 0.04997767 m 4.7530375\n",
      "42 Train Loss 7.2607894 Test RE 0.00058509397157868 c 1.005063 k 0.049825754 m 4.7337523\n",
      "43 Train Loss 6.5427322 Test RE 0.0005724996506188256 c 1.0038185 k 0.049920827 m 4.8025627\n",
      "44 Train Loss 5.3081703 Test RE 0.0005660216160339267 c 0.9953721 k 0.050102424 m 4.930344\n",
      "45 Train Loss 4.297701 Test RE 0.0005624188890312485 c 0.9926163 k 0.050165188 m 4.9897904\n",
      "46 Train Loss 4.023035 Test RE 0.0005291224561265178 c 0.99670345 k 0.05014338 m 5.0105066\n",
      "47 Train Loss 3.958152 Test RE 0.0005212337153398509 c 0.9971743 k 0.05013207 m 5.025915\n",
      "48 Train Loss 3.4616895 Test RE 0.00045803822803091617 c 0.99989426 k 0.050101098 m 5.0613284\n",
      "49 Train Loss 3.215028 Test RE 0.0004213271070800009 c 0.9979011 k 0.05013821 m 5.0409946\n",
      "50 Train Loss 3.0659232 Test RE 0.0004154945430961841 c 0.99593383 k 0.050123483 m 5.009937\n",
      "51 Train Loss 2.8206556 Test RE 0.00040576778610923664 c 0.9985166 k 0.05012316 m 5.0358872\n",
      "52 Train Loss 2.3981307 Test RE 0.0004130703822148352 c 0.9968744 k 0.050119467 m 5.040087\n",
      "53 Train Loss 2.0270844 Test RE 0.0004285444918542575 c 0.9974131 k 0.050078638 m 4.9664693\n",
      "54 Train Loss 1.9117242 Test RE 0.0004421499664262334 c 1.000363 k 0.050019678 m 4.9512653\n",
      "55 Train Loss 1.6350776 Test RE 0.00044171156885108013 c 1.0007048 k 0.05001584 m 4.9434166\n",
      "56 Train Loss 1.3423672 Test RE 0.00042713271370119373 c 0.99780947 k 0.050065357 m 4.9484406\n",
      "57 Train Loss 0.95845467 Test RE 0.00033420067377039814 c 1.0022457 k 0.04995278 m 4.913589\n",
      "58 Train Loss 0.7643192 Test RE 0.0002865289896817644 c 0.99959654 k 0.049985252 m 4.9396477\n",
      "59 Train Loss 0.64294755 Test RE 0.00025906163871778276 c 1.0004464 k 0.050005697 m 4.972464\n",
      "60 Train Loss 0.54334205 Test RE 0.00022717390823374874 c 0.99959075 k 0.049992293 m 4.972469\n",
      "61 Train Loss 0.48741475 Test RE 0.00018199644278665602 c 1.0003251 k 0.04999133 m 4.984187\n",
      "62 Train Loss 0.4682588 Test RE 0.00015811647433362108 c 1.0008061 k 0.049963504 m 4.9737926\n",
      "63 Train Loss 0.456256 Test RE 0.000156318929319317 c 1.0004497 k 0.049975082 m 4.9775705\n",
      "64 Train Loss 0.44801453 Test RE 0.0001551460836085786 c 1.0002778 k 0.049983915 m 4.989179\n",
      "65 Train Loss 0.44049963 Test RE 0.00014910426377177997 c 1.0004762 k 0.049976036 m 4.991484\n",
      "66 Train Loss 0.42863995 Test RE 0.0001435235974540139 c 1.0006522 k 0.049974322 m 4.99242\n",
      "67 Train Loss 0.37282982 Test RE 0.000145856957068296 c 1.0001935 k 0.049975898 m 4.999733\n",
      "68 Train Loss 0.29522163 Test RE 0.0001823706254519626 c 1.0018833 k 0.049930114 m 5.003264\n",
      "69 Train Loss 0.20830464 Test RE 0.00012189640107005344 c 1.0001444 k 0.05000352 m 5.036095\n",
      "70 Train Loss 0.14898121 Test RE 9.751801110795124e-05 c 1.0003204 k 0.049978144 m 5.0158935\n",
      "71 Train Loss 0.1222336 Test RE 7.769438849143641e-05 c 1.0007198 k 0.049974587 m 5.000128\n",
      "72 Train Loss 0.11955266 Test RE 7.185452295757455e-05 c 1.000248 k 0.04998541 m 5.003182\n",
      "73 Train Loss 0.116061896 Test RE 6.904224544957988e-05 c 1.0004556 k 0.049984045 m 5.005737\n",
      "74 Train Loss 0.1112846 Test RE 6.544479600641006e-05 c 1.0006386 k 0.049970444 m 4.9942975\n",
      "75 Train Loss 0.107357286 Test RE 6.215118143595579e-05 c 1.000546 k 0.049969807 m 4.993008\n",
      "76 Train Loss 0.10507806 Test RE 5.809336278325593e-05 c 1.0004771 k 0.04996949 m 4.9956713\n",
      "77 Train Loss 0.10384591 Test RE 5.778486230801801e-05 c 1.0004511 k 0.04996838 m 4.9931026\n",
      "78 Train Loss 0.09888123 Test RE 5.942029973674814e-05 c 1.0004694 k 0.04996453 m 4.9865603\n",
      "79 Train Loss 0.08099462 Test RE 5.013715350690024e-05 c 1.0004274 k 0.049979553 m 5.002111\n",
      "80 Train Loss 0.073107205 Test RE 4.7309636125663216e-05 c 1.0002671 k 0.04997969 m 4.9979744\n",
      "81 Train Loss 0.07048009 Test RE 4.593873404252334e-05 c 1.0005084 k 0.04997342 m 4.9925866\n",
      "82 Train Loss 0.06621265 Test RE 3.91291346095933e-05 c 1.0012126 k 0.049965613 m 4.9967012\n",
      "83 Train Loss 0.050258514 Test RE 3.940720723809753e-05 c 1.0000107 k 0.04997595 m 4.995034\n",
      "84 Train Loss 0.039629813 Test RE 3.849789975313644e-05 c 0.99978083 k 0.049986515 m 4.996345\n",
      "85 Train Loss 0.03599657 Test RE 3.429814600124342e-05 c 1.0005519 k 0.049978316 m 4.997945\n",
      "86 Train Loss 0.03555626 Test RE 3.270376589470139e-05 c 1.0003793 k 0.049981415 m 4.997854\n",
      "87 Train Loss 0.035392918 Test RE 3.387162324627908e-05 c 1.000264 k 0.049984314 m 4.998673\n",
      "88 Train Loss 0.03526188 Test RE 3.440360636625442e-05 c 1.0002656 k 0.04998438 m 4.9983544\n",
      "89 Train Loss 0.03475842 Test RE 3.378786567069217e-05 c 1.0002902 k 0.049983263 m 4.9978027\n",
      "90 Train Loss 0.03244776 Test RE 3.3386222777152244e-05 c 1.0005003 k 0.049981486 m 4.9995723\n",
      "91 Train Loss 0.030383257 Test RE 3.5021396953055925e-05 c 1.0004263 k 0.049984425 m 4.999484\n",
      "92 Train Loss 0.030061912 Test RE 3.500838968795254e-05 c 1.0001944 k 0.049987443 m 4.9990034\n",
      "93 Train Loss 0.02975153 Test RE 3.569935895927441e-05 c 1.0001159 k 0.04998689 m 4.9970965\n",
      "94 Train Loss 0.02594927 Test RE 3.786972636432476e-05 c 1.0003424 k 0.049983013 m 4.992645\n",
      "95 Train Loss 0.019259594 Test RE 2.941577251320644e-05 c 1.000433 k 0.049980827 m 4.998942\n",
      "96 Train Loss 0.017503042 Test RE 2.8607120788982348e-05 c 1.0002626 k 0.049986728 m 4.999495\n",
      "97 Train Loss 0.016542645 Test RE 2.7692980438243764e-05 c 1.0003303 k 0.049984906 m 4.9986706\n",
      "98 Train Loss 0.0162951 Test RE 2.6713749858356994e-05 c 1.0002433 k 0.049985804 m 4.9993267\n",
      "99 Train Loss 0.016236586 Test RE 2.69988319459696e-05 c 1.0002089 k 0.049986854 m 4.99957\n",
      "100 Train Loss 0.016195597 Test RE 2.6786630038595416e-05 c 1.0002887 k 0.049985558 m 4.999282\n",
      "101 Train Loss 0.016092515 Test RE 2.6259200411961092e-05 c 1.0002387 k 0.04998519 m 4.998871\n",
      "102 Train Loss 0.015756592 Test RE 2.7038944708808185e-05 c 1.0001532 k 0.049987376 m 4.9994826\n",
      "103 Train Loss 0.015433475 Test RE 2.6707915717430072e-05 c 1.000306 k 0.049985796 m 4.9993043\n",
      "104 Train Loss 0.014987844 Test RE 2.5860342967129075e-05 c 1.0002432 k 0.049984694 m 4.998401\n",
      "105 Train Loss 0.0145095205 Test RE 2.564349403092103e-05 c 1.0002335 k 0.049985375 m 4.998377\n",
      "106 Train Loss 0.014127902 Test RE 2.5503410925020732e-05 c 1.0002632 k 0.049986128 m 4.9994345\n",
      "107 Train Loss 0.014063243 Test RE 2.5006079775429156e-05 c 1.0002481 k 0.049986076 m 4.9992857\n",
      "108 Train Loss 0.014051786 Test RE 2.4993848572866897e-05 c 1.000244 k 0.04998616 m 4.999236\n",
      "109 Train Loss 0.014047029 Test RE 2.500600950596911e-05 c 1.000243 k 0.049986262 m 4.9992733\n",
      "110 Train Loss 0.014046336 Test RE 2.502049308640439e-05 c 1.0002422 k 0.049986307 m 4.999289\n",
      "111 Train Loss 0.014044184 Test RE 2.502298216892515e-05 c 1.000241 k 0.04998639 m 4.9993176\n",
      "112 Train Loss 0.014041848 Test RE 2.501898833282223e-05 c 1.0002397 k 0.04998645 m 4.9993434\n",
      "113 Train Loss 0.014041745 Test RE 2.5025054002030348e-05 c 1.0002389 k 0.049986485 m 4.999351\n",
      "114 Train Loss 0.01404122 Test RE 2.501789272310672e-05 c 1.0002389 k 0.049986485 m 4.999351\n",
      "115 Train Loss 0.014040967 Test RE 2.501915409604143e-05 c 1.000238 k 0.049986508 m 4.999356\n",
      "116 Train Loss 0.014040042 Test RE 2.501160877636336e-05 c 1.000238 k 0.049986508 m 4.999356\n",
      "117 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "118 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "119 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "120 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "121 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "122 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "123 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "124 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "125 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "126 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "127 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "128 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "129 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "130 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "131 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "132 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "133 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "134 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "135 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "136 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "137 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "138 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "139 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "140 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "141 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "142 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "143 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "144 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "145 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "146 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "147 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "148 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "149 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "150 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "151 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "152 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "153 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "154 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "155 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "156 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "157 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "158 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "159 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "160 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "161 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "162 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "163 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "164 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "165 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "166 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "167 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "168 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "169 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "170 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "171 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "172 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "173 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "174 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "175 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "176 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "177 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "178 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "179 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "180 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "181 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "182 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "183 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "184 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "185 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "186 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "187 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "188 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "189 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "190 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "191 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "192 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "193 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "194 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "195 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "196 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "197 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "198 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "199 Train Loss 0.014039594 Test RE 2.4999537898963637e-05 c 1.0002369 k 0.04998653 m 4.999356\n",
      "Training time: 47.04\n",
      "Training time: 47.04\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 176503.17 Test RE 0.28144069577047975 c 0.0026397211 k 0.054568026 m -0.00018181211\n",
      "1 Train Loss 47258.15 Test RE 0.14220543378117084 c 0.00022876597 k 0.07709649 m -0.00018093918\n",
      "2 Train Loss 45044.633 Test RE 0.13872488966742155 c 0.005731455 k 0.050107513 m 2.4512337e-05\n",
      "3 Train Loss 38890.215 Test RE 0.12808149486389717 c 0.0706905 k 0.06134032 m 0.0014797085\n",
      "4 Train Loss 36870.62 Test RE 0.12441422896817902 c 0.16841893 k 0.061897453 m 0.0028672207\n",
      "5 Train Loss 29718.258 Test RE 0.11233245555281125 c 0.5899712 k 0.04638166 m 0.007995556\n",
      "6 Train Loss 16494.795 Test RE 0.08456656910259176 c 1.1224633 k 0.04942426 m 0.015057776\n",
      "7 Train Loss 2685.4146 Test RE 0.02564188419656844 c 1.7767593 k 0.031634953 m 0.026001005\n",
      "8 Train Loss 1410.9971 Test RE 0.016610686349349625 c 1.7025436 k 0.035983436 m 0.028633932\n",
      "9 Train Loss 888.39545 Test RE 0.009890396306291132 c 1.621631 k 0.035849206 m 0.03607044\n",
      "10 Train Loss 521.7826 Test RE 0.008195331281123217 c 1.4499267 k 0.039547097 m 0.05050167\n",
      "11 Train Loss 370.89734 Test RE 0.006926116176401918 c 1.2577095 k 0.042950235 m 0.06852468\n",
      "12 Train Loss 324.6649 Test RE 0.006527769033224304 c 1.1360116 k 0.044374093 m 0.08116385\n",
      "13 Train Loss 310.42844 Test RE 0.0052933415617761525 c 1.1183314 k 0.04456981 m 0.09483825\n",
      "14 Train Loss 296.93155 Test RE 0.004308624914272189 c 1.0863338 k 0.045189288 m 0.14052522\n",
      "15 Train Loss 277.25522 Test RE 0.004332404149118877 c 1.1018732 k 0.044891436 m 0.25828457\n",
      "16 Train Loss 249.9934 Test RE 0.0036861519391108754 c 1.1486902 k 0.044626124 m 0.48646465\n",
      "17 Train Loss 224.09694 Test RE 0.003003383001973868 c 1.1205149 k 0.045061935 m 0.5961359\n",
      "18 Train Loss 211.70224 Test RE 0.003876694004698893 c 1.1414062 k 0.044933803 m 0.69099563\n",
      "19 Train Loss 204.4762 Test RE 0.0036470909992547106 c 1.1448652 k 0.044959176 m 0.77699035\n",
      "20 Train Loss 196.28337 Test RE 0.0034794061376217014 c 1.1298748 k 0.045325786 m 0.95524675\n",
      "21 Train Loss 183.3974 Test RE 0.003938415355613355 c 1.1214484 k 0.045684423 m 1.1295489\n",
      "22 Train Loss 180.3757 Test RE 0.0037051744021249235 c 1.1248217 k 0.04556755 m 1.1836361\n",
      "23 Train Loss 174.1474 Test RE 0.0035520309816667807 c 1.0970792 k 0.045970064 m 1.2394133\n",
      "24 Train Loss 162.89891 Test RE 0.00393052353893673 c 1.068956 k 0.046439283 m 1.296877\n",
      "25 Train Loss 138.17427 Test RE 0.003091291395268624 c 1.0551733 k 0.046995867 m 1.783569\n",
      "26 Train Loss 125.50092 Test RE 0.00360789868353323 c 1.0455705 k 0.04727839 m 2.0110104\n",
      "27 Train Loss 117.57684 Test RE 0.003782998997781382 c 1.0668873 k 0.04693309 m 2.1032763\n",
      "28 Train Loss 103.979454 Test RE 0.003274218560150418 c 1.037684 k 0.047702402 m 2.3773632\n",
      "29 Train Loss 85.12092 Test RE 0.0031274796752874218 c 1.0319688 k 0.048042823 m 2.735069\n",
      "30 Train Loss 69.656204 Test RE 0.0028041535889087696 c 1.0635756 k 0.047878206 m 3.1487517\n",
      "31 Train Loss 58.35732 Test RE 0.0029661579028434615 c 1.0465578 k 0.048296772 m 3.402111\n",
      "32 Train Loss 38.786404 Test RE 0.001911580922114287 c 1.0441203 k 0.04869644 m 3.8486557\n",
      "33 Train Loss 25.923977 Test RE 0.0011264602584942168 c 1.0175887 k 0.04916119 m 4.0460553\n",
      "34 Train Loss 18.546432 Test RE 0.0012549186946699054 c 1.015436 k 0.049350083 m 4.4095044\n",
      "35 Train Loss 12.823979 Test RE 0.0009395201853069816 c 1.0134801 k 0.049510438 m 4.716492\n",
      "36 Train Loss 11.480973 Test RE 0.0006557975745258609 c 1.0115322 k 0.04965596 m 4.8585215\n",
      "37 Train Loss 10.25404 Test RE 0.0006525255247456904 c 1.0025319 k 0.049814224 m 4.867792\n",
      "38 Train Loss 8.623042 Test RE 0.0007431368902675295 c 1.0128964 k 0.049537398 m 4.78485\n",
      "39 Train Loss 5.629077 Test RE 0.0005970557049885025 c 1.020501 k 0.049361896 m 4.764887\n",
      "40 Train Loss 4.3474674 Test RE 0.0005892783332025237 c 1.0025663 k 0.04968942 m 4.7732897\n",
      "41 Train Loss 3.3388617 Test RE 0.00043281005124294294 c 1.002214 k 0.04975813 m 4.8363824\n",
      "42 Train Loss 2.8281293 Test RE 0.00035172660925882613 c 1.0055311 k 0.049798768 m 4.948305\n",
      "43 Train Loss 2.5439386 Test RE 0.00035263951960052257 c 1.0022155 k 0.049896084 m 4.970229\n",
      "44 Train Loss 2.3845258 Test RE 0.0002903823718406931 c 0.99710023 k 0.04999853 m 5.010079\n",
      "45 Train Loss 2.2226422 Test RE 0.00028849904073986765 c 1.0008873 k 0.049968693 m 5.0525002\n",
      "46 Train Loss 1.8019426 Test RE 0.0003071798257628593 c 1.0046355 k 0.049898904 m 5.0047398\n",
      "47 Train Loss 1.6513095 Test RE 0.00027886310633181224 c 0.9999771 k 0.0499428 m 5.0107827\n",
      "48 Train Loss 1.5910089 Test RE 0.00026416391616303675 c 1.0014325 k 0.049911827 m 5.0181518\n",
      "49 Train Loss 1.4592562 Test RE 0.0002605197779156869 c 1.0024943 k 0.049892444 m 4.9951415\n",
      "50 Train Loss 1.3477054 Test RE 0.00026117566331931315 c 1.0023323 k 0.04987781 m 4.9819164\n",
      "51 Train Loss 1.274936 Test RE 0.0002557086663962321 c 1.0027326 k 0.0498699 m 4.9765735\n",
      "52 Train Loss 1.2340357 Test RE 0.0002478391476615539 c 1.0018684 k 0.049896028 m 4.973046\n",
      "53 Train Loss 1.104689 Test RE 0.0002470670481018016 c 1.0018982 k 0.049883302 m 4.970894\n",
      "54 Train Loss 0.9432269 Test RE 0.00023183602963246453 c 1.0038526 k 0.04986129 m 4.9592156\n",
      "55 Train Loss 0.881915 Test RE 0.0002233562490159015 c 1.0016589 k 0.049897674 m 4.97076\n",
      "56 Train Loss 0.8613936 Test RE 0.0002184427911779185 c 1.0017245 k 0.04990959 m 4.9764576\n",
      "57 Train Loss 0.84087247 Test RE 0.00022311089981104522 c 1.003316 k 0.049861237 m 4.9510036\n",
      "58 Train Loss 0.785113 Test RE 0.00022309323827133848 c 1.0003196 k 0.049933173 m 4.951212\n",
      "59 Train Loss 0.73465353 Test RE 0.00019593799778275234 c 1.0030537 k 0.049877256 m 4.9683833\n",
      "60 Train Loss 0.63013023 Test RE 0.00019026145579799573 c 1.0063782 k 0.049832612 m 4.9778724\n",
      "61 Train Loss 0.5337595 Test RE 0.00020255280458188565 c 1.0020559 k 0.04992299 m 4.992352\n",
      "62 Train Loss 0.50286394 Test RE 0.00018898538342471366 c 1.0017128 k 0.04990933 m 4.983415\n",
      "63 Train Loss 0.49548024 Test RE 0.00018730815968979306 c 1.0019231 k 0.04990577 m 4.9864106\n",
      "64 Train Loss 0.43461487 Test RE 0.00019042459487225778 c 1.0010504 k 0.04992914 m 4.995528\n",
      "65 Train Loss 0.32960153 Test RE 0.00018645320199979116 c 1.0013083 k 0.049921185 m 4.9854107\n",
      "66 Train Loss 0.30021554 Test RE 0.00018068760809214986 c 1.0017383 k 0.049913246 m 4.982153\n",
      "67 Train Loss 0.28034765 Test RE 0.0001782627629001692 c 1.0010855 k 0.04992112 m 4.9853473\n",
      "68 Train Loss 0.2667988 Test RE 0.00016906652955738056 c 1.0018237 k 0.04990927 m 4.9800096\n",
      "69 Train Loss 0.25938874 Test RE 0.00016163913128484892 c 1.0011445 k 0.049926642 m 4.9884887\n",
      "70 Train Loss 0.25477287 Test RE 0.00016186274570076858 c 1.0011938 k 0.049932696 m 4.99217\n",
      "71 Train Loss 0.2526533 Test RE 0.00015968284288118425 c 1.0015538 k 0.049922414 m 4.9885454\n",
      "72 Train Loss 0.2429958 Test RE 0.00015610910673334659 c 1.0007832 k 0.049931604 m 4.9916873\n",
      "73 Train Loss 0.22842745 Test RE 0.00015731393312427144 c 1.0010369 k 0.0499388 m 4.9944367\n",
      "74 Train Loss 0.2224427 Test RE 0.00015752869838797453 c 1.0016314 k 0.049923494 m 4.9873896\n",
      "75 Train Loss 0.21435896 Test RE 0.0001576204730835231 c 1.0010134 k 0.04993324 m 4.991546\n",
      "76 Train Loss 0.20893016 Test RE 0.0001599738189361544 c 1.0013541 k 0.049932584 m 4.9941874\n",
      "77 Train Loss 0.2075518 Test RE 0.00015985945794157045 c 1.0011299 k 0.049936336 m 4.9965897\n",
      "78 Train Loss 0.20501247 Test RE 0.00016015724065858604 c 1.0010536 k 0.049939033 m 4.997835\n",
      "79 Train Loss 0.2034164 Test RE 0.0001596894364590345 c 1.0010817 k 0.049936175 m 4.996326\n",
      "80 Train Loss 0.20117836 Test RE 0.0001579790510647601 c 1.0009903 k 0.049944587 m 4.99861\n",
      "81 Train Loss 0.19883335 Test RE 0.00015775201900503443 c 1.0012716 k 0.049933497 m 4.996408\n",
      "82 Train Loss 0.19649154 Test RE 0.0001561802256515845 c 1.0013057 k 0.04993041 m 4.9930096\n",
      "83 Train Loss 0.19480929 Test RE 0.0001531823185940552 c 1.0011226 k 0.049935196 m 4.9919786\n",
      "84 Train Loss 0.19181138 Test RE 0.00015396954853545786 c 1.0010748 k 0.049938675 m 4.9954777\n",
      "85 Train Loss 0.18354991 Test RE 0.00015014513001174813 c 1.0003879 k 0.0499561 m 5.006155\n",
      "86 Train Loss 0.16151783 Test RE 0.00013255439234199996 c 1.0012239 k 0.049943678 m 4.9944963\n",
      "87 Train Loss 0.14774764 Test RE 0.00012336740069144255 c 1.0016838 k 0.049928017 m 4.992303\n",
      "88 Train Loss 0.13973472 Test RE 0.0001225143408058892 c 1.0007519 k 0.049956135 m 4.9962726\n",
      "89 Train Loss 0.1332454 Test RE 0.00011721891041475593 c 1.0003715 k 0.0499578 m 4.9906745\n",
      "90 Train Loss 0.12520972 Test RE 0.00011354841312350786 c 1.0006721 k 0.049956355 m 4.997282\n",
      "91 Train Loss 0.11129855 Test RE 0.00010801916562268673 c 1.001111 k 0.049960926 m 5.0033507\n",
      "92 Train Loss 0.10650181 Test RE 0.00010462931345149849 c 1.000485 k 0.049969655 m 5.0030203\n",
      "93 Train Loss 0.10549574 Test RE 0.00010218876285561405 c 1.0006164 k 0.049965587 m 5.0000324\n",
      "94 Train Loss 0.10440248 Test RE 0.00010465620839762278 c 1.0006506 k 0.049966346 m 4.9977026\n",
      "95 Train Loss 0.10353632 Test RE 0.00010465192120878635 c 1.0004575 k 0.04996745 m 4.995347\n",
      "96 Train Loss 0.10285718 Test RE 0.00010272280209371555 c 1.000501 k 0.04996649 m 4.994937\n",
      "97 Train Loss 0.10233286 Test RE 0.00010390258604266532 c 1.0005801 k 0.04996816 m 4.9972277\n",
      "98 Train Loss 0.1017396 Test RE 0.00010500021044286397 c 1.0004609 k 0.049973406 m 4.998181\n",
      "99 Train Loss 0.09586478 Test RE 0.00010202969020576743 c 1.0001067 k 0.049981426 m 4.995113\n",
      "100 Train Loss 0.08685436 Test RE 9.347702284838936e-05 c 1.0003513 k 0.049977213 m 4.9898453\n",
      "101 Train Loss 0.08110907 Test RE 8.279107854053924e-05 c 1.0008738 k 0.049967606 m 4.9864397\n",
      "102 Train Loss 0.07430003 Test RE 7.39570390926535e-05 c 1.0008003 k 0.049968276 m 4.9917192\n",
      "103 Train Loss 0.065456256 Test RE 6.628124442921176e-05 c 1.0005444 k 0.049968887 m 4.9943957\n",
      "104 Train Loss 0.06308588 Test RE 6.345061797716599e-05 c 1.0005698 k 0.04996939 m 4.9934287\n",
      "105 Train Loss 0.062278852 Test RE 6.050513051803498e-05 c 1.0005945 k 0.04996942 m 4.9939284\n",
      "106 Train Loss 0.061245054 Test RE 5.686135500880659e-05 c 1.0006847 k 0.04996622 m 4.9930215\n",
      "107 Train Loss 0.06072072 Test RE 5.7916103362582056e-05 c 1.0006317 k 0.049968366 m 4.993088\n",
      "108 Train Loss 0.06055663 Test RE 5.7936887036344044e-05 c 1.0005692 k 0.049969185 m 4.993025\n",
      "109 Train Loss 0.060136385 Test RE 5.513132497862098e-05 c 1.0007148 k 0.049967784 m 4.9934173\n",
      "110 Train Loss 0.059230085 Test RE 4.963479235727487e-05 c 1.0006506 k 0.04997075 m 4.995638\n",
      "111 Train Loss 0.058090188 Test RE 4.501402537121809e-05 c 1.0004245 k 0.049974326 m 4.996255\n",
      "112 Train Loss 0.05569417 Test RE 4.070458051378844e-05 c 1.000392 k 0.04997469 m 4.995245\n",
      "113 Train Loss 0.0517908 Test RE 3.430906333684944e-05 c 1.0002781 k 0.04998058 m 4.994036\n",
      "114 Train Loss 0.04974337 Test RE 2.93405064997582e-05 c 1.0001186 k 0.049984764 m 4.9953184\n",
      "115 Train Loss 0.048378527 Test RE 2.628055389034756e-05 c 1.0001608 k 0.049983412 m 4.9955273\n",
      "116 Train Loss 0.047092784 Test RE 2.696717971929618e-05 c 1.0003761 k 0.049979206 m 4.9951725\n",
      "117 Train Loss 0.045171868 Test RE 2.8340874723503137e-05 c 1.000477 k 0.049975984 m 4.994074\n",
      "118 Train Loss 0.0423317 Test RE 2.9973812080174295e-05 c 1.0005125 k 0.049974214 m 4.9905486\n",
      "119 Train Loss 0.03833987 Test RE 2.9043033113520074e-05 c 1.0003488 k 0.049979065 m 4.991598\n",
      "120 Train Loss 0.033937763 Test RE 2.6092113219459944e-05 c 1.0002488 k 0.049980696 m 4.9942594\n",
      "121 Train Loss 0.029610308 Test RE 2.6520279419682698e-05 c 1.0003877 k 0.04997854 m 4.99259\n",
      "122 Train Loss 0.026620897 Test RE 2.3818254249075635e-05 c 0.9999457 k 0.049989328 m 4.995698\n",
      "123 Train Loss 0.026051804 Test RE 2.4499822466584825e-05 c 1.0001172 k 0.049988493 m 4.9970984\n",
      "124 Train Loss 0.025971916 Test RE 2.4466876892372932e-05 c 1.0002936 k 0.049986236 m 4.9974947\n",
      "125 Train Loss 0.025883146 Test RE 2.453560863179248e-05 c 1.0002781 k 0.049987048 m 4.9986987\n",
      "126 Train Loss 0.0256827 Test RE 2.3640820855271244e-05 c 1.0001475 k 0.049989115 m 4.9992914\n",
      "127 Train Loss 0.025462342 Test RE 2.297548016180001e-05 c 1.0001985 k 0.049988244 m 4.9982195\n",
      "128 Train Loss 0.025220592 Test RE 2.3411645195238e-05 c 1.00019 k 0.049987882 m 4.997603\n",
      "129 Train Loss 0.024988547 Test RE 2.240884710580213e-05 c 1.0001477 k 0.049988855 m 4.9965143\n",
      "130 Train Loss 0.024663264 Test RE 2.117388738068966e-05 c 1.0003057 k 0.04998542 m 4.995545\n",
      "131 Train Loss 0.024351852 Test RE 2.0741677943964672e-05 c 1.0002885 k 0.04998753 m 4.997783\n",
      "132 Train Loss 0.024155216 Test RE 1.8894798756301324e-05 c 1.0001864 k 0.049990773 m 4.9990506\n",
      "133 Train Loss 0.024031505 Test RE 1.872596465238112e-05 c 1.0001775 k 0.049991004 m 4.9996934\n",
      "134 Train Loss 0.023821354 Test RE 1.809087584547899e-05 c 1.000123 k 0.049992822 m 4.999912\n",
      "135 Train Loss 0.023379572 Test RE 1.8031533012708803e-05 c 1.0001041 k 0.04999262 m 4.9986687\n",
      "136 Train Loss 0.02303818 Test RE 1.823319734333729e-05 c 1.0000489 k 0.049991947 m 4.9993834\n",
      "137 Train Loss 0.022585895 Test RE 1.773256341986953e-05 c 1.0000863 k 0.04999395 m 4.999372\n",
      "138 Train Loss 0.022095414 Test RE 1.592365040042704e-05 c 1.0003188 k 0.04998734 m 4.996968\n",
      "139 Train Loss 0.021545405 Test RE 1.4316008342431134e-05 c 1.0001808 k 0.049989402 m 4.996933\n",
      "140 Train Loss 0.021123797 Test RE 1.4536709130079064e-05 c 1.000153 k 0.04999206 m 4.9979978\n",
      "141 Train Loss 0.020641426 Test RE 1.437003414988368e-05 c 1.0001471 k 0.04999305 m 4.999409\n",
      "142 Train Loss 0.020028248 Test RE 1.2730086593251853e-05 c 1.0001235 k 0.049992394 m 4.998606\n",
      "143 Train Loss 0.019473393 Test RE 1.3271882542589909e-05 c 1.0000812 k 0.049993377 m 4.999233\n",
      "144 Train Loss 0.019244086 Test RE 1.2534508415010145e-05 c 1.0000353 k 0.04999499 m 4.999587\n",
      "145 Train Loss 0.019150892 Test RE 1.3198752286205303e-05 c 1.0000837 k 0.049993902 m 4.998765\n",
      "146 Train Loss 0.019087704 Test RE 1.3687890485562171e-05 c 1.0000929 k 0.049992967 m 4.998523\n",
      "147 Train Loss 0.018982263 Test RE 1.33773665216886e-05 c 1.0001031 k 0.049994368 m 4.9987345\n",
      "148 Train Loss 0.018840374 Test RE 1.4248765079187807e-05 c 1.0001302 k 0.04999361 m 4.9984984\n",
      "149 Train Loss 0.018742243 Test RE 1.5478118770569612e-05 c 1.0000913 k 0.04999507 m 4.998217\n",
      "150 Train Loss 0.018707715 Test RE 1.5662024688215627e-05 c 1.0001076 k 0.049995378 m 4.998689\n",
      "151 Train Loss 0.01861452 Test RE 1.6118101766855367e-05 c 1.0001897 k 0.04999439 m 4.9986553\n",
      "152 Train Loss 0.018368684 Test RE 1.7754144032783994e-05 c 1.0002854 k 0.049993437 m 4.9984093\n",
      "153 Train Loss 0.016704245 Test RE 1.909313710154606e-05 c 1.0004219 k 0.049988553 m 4.9984603\n",
      "154 Train Loss 0.014965487 Test RE 1.5839558002155626e-05 c 1.0002841 k 0.04999062 m 4.9999685\n",
      "155 Train Loss 0.0137326075 Test RE 1.6883520076328995e-05 c 1.0000479 k 0.04999454 m 4.998124\n",
      "156 Train Loss 0.012212604 Test RE 1.3952946135094873e-05 c 0.99969894 k 0.04999927 m 4.997718\n",
      "157 Train Loss 0.011329431 Test RE 1.2757400060949551e-05 c 1.0000086 k 0.049996287 m 4.997157\n",
      "158 Train Loss 0.010962152 Test RE 1.3509718214229408e-05 c 1.0001347 k 0.04999545 m 4.9988227\n",
      "159 Train Loss 0.010534247 Test RE 1.3643450460724916e-05 c 1.0000871 k 0.049997628 m 4.9999475\n",
      "160 Train Loss 0.009682193 Test RE 1.4176979602210751e-05 c 1.0000317 k 0.049997766 m 4.9986787\n",
      "161 Train Loss 0.009078741 Test RE 1.4181799181733376e-05 c 0.99996454 k 0.05000056 m 5.0007052\n",
      "162 Train Loss 0.008646599 Test RE 1.3512261978695277e-05 c 1.0000384 k 0.049999278 m 5.0001016\n",
      "163 Train Loss 0.008471195 Test RE 1.3943769014903133e-05 c 1.0000675 k 0.049998406 m 4.999603\n",
      "164 Train Loss 0.00831149 Test RE 1.4206207323283711e-05 c 0.99990237 k 0.05000166 m 5.000786\n",
      "165 Train Loss 0.008124187 Test RE 1.2664891529676727e-05 c 0.99998915 k 0.04999978 m 5.000295\n",
      "166 Train Loss 0.008063422 Test RE 1.2233055026379662e-05 c 1.000024 k 0.049999148 m 5.0002685\n",
      "167 Train Loss 0.008047639 Test RE 1.1873161403338983e-05 c 1.0000108 k 0.049999055 m 5.000448\n",
      "168 Train Loss 0.00804672 Test RE 1.181337539104872e-05 c 1.0000104 k 0.049999002 m 5.000423\n",
      "169 Train Loss 0.008029833 Test RE 1.1382509009056218e-05 c 1.000015 k 0.04999886 m 5.000021\n",
      "170 Train Loss 0.008008588 Test RE 1.1547301554508828e-05 c 1.0000165 k 0.04999863 m 4.9999285\n",
      "171 Train Loss 0.007994875 Test RE 1.1588557325520544e-05 c 1.0000293 k 0.04999849 m 5.0000505\n",
      "172 Train Loss 0.007976083 Test RE 1.1588902762807414e-05 c 1.0000387 k 0.0499982 m 4.99997\n",
      "173 Train Loss 0.007912818 Test RE 1.2293229625006291e-05 c 1.0000072 k 0.049999647 m 5.0007467\n",
      "174 Train Loss 0.007867085 Test RE 1.2520591803924492e-05 c 1.0000112 k 0.04999967 m 5.000688\n",
      "175 Train Loss 0.007848213 Test RE 1.288679007278272e-05 c 1.0000081 k 0.049999565 m 5.00017\n",
      "176 Train Loss 0.00784526 Test RE 1.299542076703077e-05 c 1.0000051 k 0.04999962 m 5.0001354\n",
      "177 Train Loss 0.007844928 Test RE 1.3025449461926251e-05 c 1.0000042 k 0.049999654 m 5.0001388\n",
      "178 Train Loss 0.007843191 Test RE 1.3093996117669523e-05 c 1.000002 k 0.04999973 m 5.00016\n",
      "179 Train Loss 0.007842549 Test RE 1.3140305197026933e-05 c 1.0000007 k 0.049999774 m 5.0001807\n",
      "180 Train Loss 0.007842028 Test RE 1.3172814864448628e-05 c 0.9999997 k 0.04999982 m 5.0002003\n",
      "181 Train Loss 0.007841428 Test RE 1.3215685334567016e-05 c 0.99999845 k 0.049999874 m 5.000226\n",
      "182 Train Loss 0.007839298 Test RE 1.3255645449175026e-05 c 0.9999992 k 0.04999981 m 5.0002494\n",
      "183 Train Loss 0.007836879 Test RE 1.32987325779494e-05 c 0.9999986 k 0.04999993 m 5.0002923\n",
      "184 Train Loss 0.007835888 Test RE 1.3322018355800406e-05 c 0.9999984 k 0.049999986 m 5.000312\n",
      "185 Train Loss 0.007831078 Test RE 1.335159572638072e-05 c 0.99999815 k 0.049999774 m 5.000388\n",
      "186 Train Loss 0.007788885 Test RE 1.3127371473771834e-05 c 1.0000279 k 0.050000135 m 5.0005245\n",
      "187 Train Loss 0.0077230353 Test RE 1.2498681865643009e-05 c 1.0001416 k 0.049997523 m 5.000124\n",
      "188 Train Loss 0.007562877 Test RE 1.174082460837901e-05 c 1.0001428 k 0.049996965 m 4.9999094\n",
      "189 Train Loss 0.0074730576 Test RE 1.086482293107244e-05 c 1.0000548 k 0.0499979 m 5.000266\n",
      "190 Train Loss 0.007417915 Test RE 1.0758892937996758e-05 c 1.0000334 k 0.049998447 m 4.9999847\n",
      "191 Train Loss 0.0073310933 Test RE 1.1307257910049795e-05 c 1.0000334 k 0.049998883 m 5.000361\n",
      "192 Train Loss 0.0072023063 Test RE 1.0754365598348984e-05 c 1.0000007 k 0.049999356 m 5.0001636\n",
      "193 Train Loss 0.00708251 Test RE 1.0542985761643936e-05 c 0.99997777 k 0.049999002 m 4.9992614\n",
      "194 Train Loss 0.0070578074 Test RE 1.0621955264595008e-05 c 0.99998057 k 0.04999912 m 4.9995227\n",
      "195 Train Loss 0.0070020366 Test RE 1.088340569803336e-05 c 1.0000235 k 0.049999334 m 5.0003347\n",
      "196 Train Loss 0.0069147176 Test RE 1.0393318511106469e-05 c 1.000027 k 0.049998973 m 5.0002847\n",
      "197 Train Loss 0.006848919 Test RE 1.0016579003906386e-05 c 1.0000097 k 0.04999921 m 5.0003843\n",
      "198 Train Loss 0.006763992 Test RE 9.884019322667263e-06 c 1.0000771 k 0.04999821 m 5.000687\n",
      "199 Train Loss 0.006720424 Test RE 9.764117072424175e-06 c 1.0001005 k 0.049997594 m 5.0004888\n",
      "Training time: 60.55\n",
      "Training time: 60.55\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 250107.78 Test RE 0.2664911992045501 c -0.001534313 k 0.29176962 m 0.0010855943\n",
      "1 Train Loss 31829.566 Test RE 0.11662584633042393 c -0.0032083225 k 0.06585622 m 0.00109995\n",
      "2 Train Loss 20019.781 Test RE 0.09119689398596162 c 0.033695452 k 0.062032007 m 0.003737588\n",
      "3 Train Loss 2309.6685 Test RE 0.019231798197254762 c 0.14467187 k 0.058691055 m 0.011817374\n",
      "4 Train Loss 1520.7891 Test RE 0.008146433017747017 c 0.18846954 k 0.05822303 m 0.015263394\n",
      "5 Train Loss 1012.22156 Test RE 0.011456262668000178 c 0.54144573 k 0.053276166 m 0.044131126\n",
      "6 Train Loss 651.0812 Test RE 0.011233763604254484 c 0.98026425 k 0.046819486 m 0.08290606\n",
      "7 Train Loss 429.0375 Test RE 0.007139519228467136 c 1.094291 k 0.045060195 m 0.09971897\n",
      "8 Train Loss 382.0589 Test RE 0.007176105872110894 c 1.1360006 k 0.04420484 m 0.11008479\n",
      "9 Train Loss 372.66907 Test RE 0.0071615232366239664 c 1.1366055 k 0.044342287 m 0.13139907\n",
      "10 Train Loss 340.38385 Test RE 0.006503134290083838 c 1.1025327 k 0.04506968 m 0.2804257\n",
      "11 Train Loss 294.22153 Test RE 0.005897043542929607 c 1.1311756 k 0.04470267 m 0.4850467\n",
      "12 Train Loss 285.88373 Test RE 0.006146774201747916 c 1.1153659 k 0.045028098 m 0.5551026\n",
      "13 Train Loss 271.19336 Test RE 0.0063679385943865 c 1.0857418 k 0.04543326 m 0.6819738\n",
      "14 Train Loss 260.46783 Test RE 0.005921571959741618 c 1.1182534 k 0.04508433 m 0.77415097\n",
      "15 Train Loss 247.65285 Test RE 0.006018270250109584 c 1.0941687 k 0.04544867 m 0.987344\n",
      "16 Train Loss 241.00746 Test RE 0.005695623452164317 c 1.0836222 k 0.045831516 m 1.0684997\n",
      "17 Train Loss 227.77342 Test RE 0.005537418971578056 c 1.0898694 k 0.04591001 m 1.2990482\n",
      "18 Train Loss 217.6987 Test RE 0.005334554079683991 c 1.1050775 k 0.04574833 m 1.4167129\n",
      "19 Train Loss 208.6781 Test RE 0.004968229874336531 c 1.1036826 k 0.04575007 m 1.440124\n",
      "20 Train Loss 198.4485 Test RE 0.004721817277147836 c 1.1136634 k 0.045676336 m 1.5659206\n",
      "21 Train Loss 159.45953 Test RE 0.003983236703033684 c 1.0975144 k 0.04593115 m 1.4903451\n",
      "22 Train Loss 144.98642 Test RE 0.0034797696903227085 c 1.109134 k 0.04582229 m 1.5569499\n",
      "23 Train Loss 137.12277 Test RE 0.0033784066581273656 c 1.1226875 k 0.04572401 m 1.6985478\n",
      "24 Train Loss 126.83438 Test RE 0.0033850480325125815 c 1.0809973 k 0.04647767 m 1.8216633\n",
      "25 Train Loss 123.40423 Test RE 0.003339259490069699 c 1.0760461 k 0.046517517 m 1.8770293\n",
      "26 Train Loss 121.713394 Test RE 0.0036285674570928515 c 1.0936958 k 0.046410818 m 1.9429519\n",
      "27 Train Loss 119.17349 Test RE 0.003431916724754549 c 1.0859925 k 0.046474088 m 2.02183\n",
      "28 Train Loss 117.24175 Test RE 0.0033611626932917027 c 1.0740244 k 0.046694342 m 2.0784795\n",
      "29 Train Loss 114.685165 Test RE 0.0035056851178436902 c 1.0803883 k 0.04660193 m 2.1140242\n",
      "30 Train Loss 107.372086 Test RE 0.0036814274744087425 c 1.0841165 k 0.04652272 m 2.2961078\n",
      "31 Train Loss 66.4675 Test RE 0.002555856506286052 c 1.0357671 k 0.047894247 m 3.023956\n",
      "32 Train Loss 48.206406 Test RE 0.0016787450170684022 c 1.049012 k 0.048079707 m 3.2316756\n",
      "33 Train Loss 34.475044 Test RE 0.001447306236257745 c 1.0319841 k 0.048490167 m 3.5666296\n",
      "34 Train Loss 32.77391 Test RE 0.0016769062083319005 c 1.0302415 k 0.048678312 m 3.6574533\n",
      "35 Train Loss 29.50376 Test RE 0.0015779564456323595 c 1.0343992 k 0.04868099 m 3.776796\n",
      "36 Train Loss 23.833502 Test RE 0.0011805705401392375 c 1.0177959 k 0.049132753 m 3.9756856\n",
      "37 Train Loss 20.30521 Test RE 0.0012505405172836812 c 1.0131415 k 0.049353298 m 4.1719537\n",
      "38 Train Loss 19.615814 Test RE 0.001297291355015302 c 1.0194974 k 0.049273938 m 4.169633\n",
      "39 Train Loss 18.055157 Test RE 0.0012963373071517145 c 1.0149243 k 0.04936111 m 4.204056\n",
      "40 Train Loss 16.907383 Test RE 0.0012057938683373882 c 1.016802 k 0.049413625 m 4.258059\n",
      "41 Train Loss 15.1794815 Test RE 0.0012229355999518565 c 1.0129293 k 0.04947932 m 4.3327675\n",
      "42 Train Loss 14.091317 Test RE 0.001210304906760487 c 1.0104324 k 0.04951719 m 4.355725\n",
      "43 Train Loss 11.187015 Test RE 0.0011470256140656517 c 1.0103117 k 0.049746767 m 4.6485105\n",
      "44 Train Loss 7.9718695 Test RE 0.0010615831108558404 c 0.9989248 k 0.049978126 m 4.7823267\n",
      "45 Train Loss 6.7987447 Test RE 0.001045023373849146 c 1.0087783 k 0.049823157 m 4.7955155\n",
      "46 Train Loss 5.685175 Test RE 0.0009226836124155472 c 1.0018103 k 0.049861677 m 4.757605\n",
      "47 Train Loss 5.3495636 Test RE 0.0008783037835202027 c 1.0060383 k 0.0497763 m 4.7552657\n",
      "48 Train Loss 4.5899434 Test RE 0.0007541967218839422 c 1.0050153 k 0.049785078 m 4.787329\n",
      "49 Train Loss 4.0496526 Test RE 0.000734937676304199 c 1.0017292 k 0.04982458 m 4.8393507\n",
      "50 Train Loss 3.8537862 Test RE 0.0007092436617313562 c 1.0024309 k 0.049823157 m 4.8720546\n",
      "51 Train Loss 3.7899022 Test RE 0.0007016934786367502 c 1.0037088 k 0.04982605 m 4.8967543\n",
      "52 Train Loss 3.7191615 Test RE 0.000676577015110196 c 1.0031914 k 0.049849235 m 4.911753\n",
      "53 Train Loss 3.513689 Test RE 0.0006567570117591651 c 1.0026023 k 0.049879372 m 4.8963437\n",
      "54 Train Loss 3.361979 Test RE 0.0007285050547994311 c 1.0038071 k 0.049818784 m 4.88473\n",
      "55 Train Loss 3.1955183 Test RE 0.0007609768547206215 c 1.0037397 k 0.049791567 m 4.864224\n",
      "56 Train Loss 3.0427542 Test RE 0.0006994179297404637 c 1.0053264 k 0.049817663 m 4.877218\n",
      "57 Train Loss 2.752069 Test RE 0.0006845423761704827 c 1.0034754 k 0.049847186 m 4.9163566\n",
      "58 Train Loss 2.3080063 Test RE 0.0005650100101631429 c 1.0017098 k 0.049940187 m 4.965493\n",
      "59 Train Loss 2.092015 Test RE 0.0005147259812947195 c 0.9988324 k 0.049926892 m 4.9245944\n",
      "60 Train Loss 1.8447051 Test RE 0.0006002932873896339 c 1.004393 k 0.049786914 m 4.87811\n",
      "61 Train Loss 1.610106 Test RE 0.0006579239198751085 c 1.0018587 k 0.049838975 m 4.9054213\n",
      "62 Train Loss 0.90793735 Test RE 0.0004921267835047257 c 0.99364424 k 0.050001938 m 5.0029798\n",
      "63 Train Loss 0.44991094 Test RE 0.0002698272668985496 c 0.99926406 k 0.050041445 m 5.0382\n",
      "64 Train Loss 0.34944153 Test RE 0.00023936353651685555 c 0.99937016 k 0.049982786 m 5.039074\n",
      "65 Train Loss 0.2767355 Test RE 0.00022871700835899313 c 0.9995839 k 0.04998237 m 5.0153227\n",
      "66 Train Loss 0.25593165 Test RE 0.00021103277036463867 c 1.0014466 k 0.04993995 m 5.0010133\n",
      "67 Train Loss 0.2421329 Test RE 0.00019886895654646776 c 1.0018257 k 0.04991193 m 4.9812236\n",
      "68 Train Loss 0.22680458 Test RE 0.00018455775654654862 c 1.0016763 k 0.049911782 m 4.97297\n",
      "69 Train Loss 0.21675374 Test RE 0.00017756565120886754 c 1.0012885 k 0.049931146 m 4.9904695\n",
      "70 Train Loss 0.21523383 Test RE 0.0001745138070756601 c 1.0012186 k 0.049928676 m 4.9918394\n",
      "71 Train Loss 0.21361911 Test RE 0.0001720938697567926 c 1.00152 k 0.049924947 m 4.9927077\n",
      "72 Train Loss 0.20639437 Test RE 0.00017307901519378156 c 1.0017297 k 0.049921233 m 4.9881725\n",
      "73 Train Loss 0.20137715 Test RE 0.00017591362075264084 c 1.0011653 k 0.049916796 m 4.981352\n",
      "74 Train Loss 0.19931227 Test RE 0.00017625443358011776 c 1.001719 k 0.049917363 m 4.9818597\n",
      "75 Train Loss 0.19558895 Test RE 0.00017104172467794114 c 1.0015959 k 0.049924865 m 4.9924707\n",
      "76 Train Loss 0.19134346 Test RE 0.00016795255926227654 c 1.0007347 k 0.04993275 m 4.994545\n",
      "77 Train Loss 0.18813634 Test RE 0.00016845144877001878 c 1.00108 k 0.049934886 m 5.0024004\n",
      "78 Train Loss 0.18363833 Test RE 0.00016664550396447105 c 1.0014822 k 0.049935263 m 5.010943\n",
      "79 Train Loss 0.17458981 Test RE 0.00016399196925673764 c 1.0011086 k 0.049933195 m 5.0043054\n",
      "80 Train Loss 0.17165947 Test RE 0.00016582291936509294 c 1.0014405 k 0.049937006 m 5.0087085\n",
      "81 Train Loss 0.16125944 Test RE 0.00016379329548604435 c 1.001556 k 0.04993301 m 5.0038614\n",
      "82 Train Loss 0.1560273 Test RE 0.0001601241493551717 c 1.001283 k 0.049927037 m 4.9937267\n",
      "83 Train Loss 0.1549924 Test RE 0.00016049219589390674 c 1.0012798 k 0.049930084 m 4.9960656\n",
      "84 Train Loss 0.15481979 Test RE 0.00016075669798211545 c 1.0012437 k 0.049929567 m 4.995325\n",
      "85 Train Loss 0.15343645 Test RE 0.00016106166095187968 c 1.0014492 k 0.049923923 m 4.990532\n",
      "86 Train Loss 0.1489444 Test RE 0.00016173903228856648 c 1.0012554 k 0.04993071 m 4.987167\n",
      "87 Train Loss 0.146821 Test RE 0.00016107111209060564 c 1.0009676 k 0.04993205 m 4.9860387\n",
      "88 Train Loss 0.13402005 Test RE 0.00014651739168132939 c 1.0021428 k 0.049900714 m 4.967074\n",
      "89 Train Loss 0.12080232 Test RE 0.0001374219411864753 c 1.001976 k 0.049916197 m 4.9824147\n",
      "90 Train Loss 0.11641236 Test RE 0.0001325626367281502 c 1.0009731 k 0.049938295 m 4.994286\n",
      "91 Train Loss 0.10512018 Test RE 0.00011651712828201279 c 1.000355 k 0.049949937 m 5.001029\n",
      "92 Train Loss 0.084448345 Test RE 9.584199438861775e-05 c 1.0009747 k 0.04995183 m 4.9962606\n",
      "93 Train Loss 0.068241686 Test RE 6.853010200507761e-05 c 1.0005723 k 0.049954873 m 4.9909534\n",
      "94 Train Loss 0.04925949 Test RE 5.917841247040749e-05 c 1.0003794 k 0.049976215 m 4.999303\n",
      "95 Train Loss 0.04253767 Test RE 6.51237663885639e-05 c 1.0002798 k 0.049991157 m 5.005036\n",
      "96 Train Loss 0.040819734 Test RE 6.64797811597813e-05 c 1.0001614 k 0.049987283 m 4.9994607\n",
      "97 Train Loss 0.040392958 Test RE 6.59990872505094e-05 c 1.000264 k 0.04998758 m 4.999836\n",
      "98 Train Loss 0.04003504 Test RE 6.78266300943132e-05 c 1.0003413 k 0.049986504 m 5.000176\n",
      "99 Train Loss 0.03953727 Test RE 6.726016694338551e-05 c 1.00024 k 0.049986053 m 4.9982605\n",
      "100 Train Loss 0.039320294 Test RE 6.447728846986159e-05 c 1.0002426 k 0.049984638 m 4.998748\n",
      "101 Train Loss 0.039248496 Test RE 6.494914642913541e-05 c 1.0002759 k 0.049985647 m 4.9997635\n",
      "102 Train Loss 0.0391644 Test RE 6.584288361105883e-05 c 1.0001763 k 0.049988277 m 4.999922\n",
      "103 Train Loss 0.039051767 Test RE 6.596074937933795e-05 c 1.0001107 k 0.049988657 m 4.9995217\n",
      "104 Train Loss 0.038718525 Test RE 6.5612820219865e-05 c 1.0002477 k 0.049985934 m 4.9996715\n",
      "105 Train Loss 0.03763694 Test RE 6.593268303807564e-05 c 1.0002433 k 0.049987454 m 4.9985437\n",
      "106 Train Loss 0.036781702 Test RE 6.36948705307563e-05 c 1.0001826 k 0.049989272 m 5.0004153\n",
      "107 Train Loss 0.035631094 Test RE 6.242197554738855e-05 c 1.0005714 k 0.049982354 m 4.999985\n",
      "108 Train Loss 0.03358522 Test RE 6.251350178891615e-05 c 1.0003316 k 0.049987085 m 4.9978\n",
      "109 Train Loss 0.032512236 Test RE 6.113876514960743e-05 c 1.0001659 k 0.049989548 m 5.000023\n",
      "110 Train Loss 0.032394994 Test RE 5.9946317848342215e-05 c 1.0002276 k 0.049987815 m 4.9998918\n",
      "111 Train Loss 0.03235043 Test RE 5.8926664584715796e-05 c 1.0002419 k 0.049986456 m 4.999413\n",
      "112 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "113 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "114 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "115 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "116 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "117 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "118 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "119 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "120 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "121 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "122 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "123 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "124 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "125 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "126 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "127 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "128 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "129 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "130 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "131 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "132 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "133 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "134 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "135 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "136 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "137 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "138 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "139 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "140 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "141 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "142 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "143 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "144 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "145 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "146 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "147 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "148 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "149 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "150 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "151 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "152 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "153 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "154 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "155 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "156 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "157 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "158 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "159 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "160 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "161 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "162 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "163 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "164 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "165 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "166 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "167 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "168 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "169 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "170 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "171 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "172 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "173 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "174 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "175 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "176 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "177 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "178 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "179 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "180 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "181 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "182 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "183 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "184 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "185 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "186 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "187 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "188 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "189 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "190 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "191 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "192 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "193 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "194 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "195 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "196 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "197 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "198 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "199 Train Loss 0.032337204 Test RE 5.8761651819922705e-05 c 1.0002404 k 0.049986515 m 4.999461\n",
      "Training time: 46.45\n",
      "Training time: 46.45\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 149121.8 Test RE 0.25844185545832427 c 0.0038235944 k 0.054547984 m -9.384535e-05\n",
      "1 Train Loss 50223.266 Test RE 0.1471016231560323 c 0.0045827315 k 0.07265318 m -1.8751403e-05\n",
      "2 Train Loss 41590.32 Test RE 0.13258650315953407 c 0.020335203 k 0.049344447 m 0.0006767606\n",
      "3 Train Loss 8641.2295 Test RE 0.057241570084797015 c 0.23883128 k 0.05931405 m 0.007863575\n",
      "4 Train Loss 1998.4521 Test RE 0.019960367970540394 c 0.32653394 k 0.056328133 m 0.012733632\n",
      "5 Train Loss 933.36145 Test RE 0.012495461794214242 c 0.6231755 k 0.05179558 m 0.04299185\n",
      "6 Train Loss 599.3749 Test RE 0.012164570384174029 c 1.0287542 k 0.045680862 m 0.09445462\n",
      "7 Train Loss 472.237 Test RE 0.010389674083292865 c 1.240443 k 0.04242007 m 0.16424051\n",
      "8 Train Loss 412.5946 Test RE 0.010048355373965544 c 1.2108659 k 0.043176185 m 0.20332883\n",
      "9 Train Loss 360.37848 Test RE 0.00901561459042845 c 1.2230573 k 0.042845365 m 0.28722498\n",
      "10 Train Loss 269.91672 Test RE 0.006777325012117762 c 1.1756588 k 0.044196896 m 0.48182896\n",
      "11 Train Loss 245.0781 Test RE 0.0061028020955164 c 1.0921433 k 0.045452304 m 0.51416445\n",
      "12 Train Loss 230.98442 Test RE 0.005175981182066652 c 1.1095428 k 0.045207985 m 0.53716993\n",
      "13 Train Loss 227.3689 Test RE 0.004742909189414383 c 1.1090416 k 0.0453154 m 0.5469467\n",
      "14 Train Loss 222.01787 Test RE 0.004075873628487259 c 1.1173052 k 0.045170125 m 0.5896712\n",
      "15 Train Loss 210.21985 Test RE 0.003936275310560088 c 1.1237985 k 0.045187276 m 0.74559784\n",
      "16 Train Loss 204.32408 Test RE 0.003939041686642273 c 1.1025732 k 0.045630015 m 0.8622185\n",
      "17 Train Loss 199.81567 Test RE 0.003645186740063973 c 1.1192429 k 0.045445655 m 0.9815544\n",
      "18 Train Loss 192.59058 Test RE 0.0034365332816647816 c 1.1039176 k 0.045698464 m 1.0523514\n",
      "19 Train Loss 183.9606 Test RE 0.0036023558170066275 c 1.0998147 k 0.045827895 m 1.1318198\n",
      "20 Train Loss 175.05916 Test RE 0.0034383509810019044 c 1.1130235 k 0.045755014 m 1.2656531\n",
      "21 Train Loss 164.94067 Test RE 0.003378020476495155 c 1.0820034 k 0.04636619 m 1.4747417\n",
      "22 Train Loss 147.814 Test RE 0.0029047755412034597 c 1.0963699 k 0.046307668 m 1.8719368\n",
      "23 Train Loss 135.32663 Test RE 0.003390187524642714 c 1.0910912 k 0.046799686 m 2.0075405\n",
      "24 Train Loss 98.659996 Test RE 0.002529698369379584 c 1.0210515 k 0.047879074 m 2.5156355\n",
      "25 Train Loss 73.74937 Test RE 0.0018191344986330238 c 1.0803481 k 0.047261465 m 3.0011303\n",
      "26 Train Loss 64.501305 Test RE 0.0023411027909913135 c 1.0437856 k 0.047923163 m 3.2195013\n",
      "27 Train Loss 56.24674 Test RE 0.001893572237841071 c 1.0083239 k 0.048639514 m 3.5969996\n",
      "28 Train Loss 25.167282 Test RE 0.0014913323929851203 c 1.0144302 k 0.049405593 m 4.619848\n",
      "29 Train Loss 21.590557 Test RE 0.0012650039952778365 c 1.0139596 k 0.049520127 m 4.74221\n",
      "30 Train Loss 18.344255 Test RE 0.0010543799221879296 c 1.0033232 k 0.049806844 m 4.990242\n",
      "31 Train Loss 17.293667 Test RE 0.001163733658572026 c 1.0018356 k 0.049832106 m 5.0390477\n",
      "32 Train Loss 15.9303875 Test RE 0.001181958681803495 c 1.005657 k 0.049751595 m 4.9916105\n",
      "33 Train Loss 12.729947 Test RE 0.0008904478652731499 c 1.0048517 k 0.049698927 m 4.8555365\n",
      "34 Train Loss 10.795611 Test RE 0.0009451981427890255 c 1.0008711 k 0.04988176 m 5.0038333\n",
      "35 Train Loss 10.040245 Test RE 0.0009081121242974433 c 0.99560374 k 0.050205465 m 5.171736\n",
      "36 Train Loss 9.3367605 Test RE 0.0009450431075981996 c 0.9938433 k 0.05026011 m 5.1891336\n",
      "37 Train Loss 7.294585 Test RE 0.0006904127830742681 c 0.9944252 k 0.050218217 m 5.211416\n",
      "38 Train Loss 5.274505 Test RE 0.0004614493403913582 c 0.99255264 k 0.050118692 m 5.1186867\n",
      "39 Train Loss 4.8078394 Test RE 0.0003421717755766276 c 1.0007501 k 0.04997972 m 5.0938735\n",
      "40 Train Loss 4.315422 Test RE 0.0003414631087366226 c 0.9993283 k 0.05007809 m 5.170736\n",
      "41 Train Loss 3.6538672 Test RE 0.00032519965314346274 c 0.9938457 k 0.050131276 m 5.1640515\n",
      "42 Train Loss 3.1866748 Test RE 0.00033089869555628237 c 1.0014225 k 0.049912337 m 5.0286784\n",
      "43 Train Loss 2.6630082 Test RE 0.000318591754925768 c 1.0039176 k 0.049840886 m 4.964378\n",
      "44 Train Loss 2.6015346 Test RE 0.00034171027667218034 c 1.0017025 k 0.049908232 m 4.979172\n",
      "45 Train Loss 2.5253053 Test RE 0.0003341944284186015 c 1.0003282 k 0.04995427 m 5.0329466\n",
      "46 Train Loss 2.474337 Test RE 0.0003356031861171219 c 1.0003632 k 0.049947366 m 5.0491605\n",
      "47 Train Loss 2.3278413 Test RE 0.00033260509715149825 c 0.9987416 k 0.049957264 m 5.0332923\n",
      "48 Train Loss 2.266428 Test RE 0.00033811870214630707 c 1.0019501 k 0.049941823 m 5.045534\n",
      "49 Train Loss 2.2113311 Test RE 0.00037744623618876725 c 0.9993774 k 0.050027065 m 5.085076\n",
      "50 Train Loss 2.1358867 Test RE 0.0003908964245672623 c 0.9960098 k 0.050121803 m 5.0910845\n",
      "51 Train Loss 1.8777039 Test RE 0.00034352321358572065 c 0.99900854 k 0.05003432 m 5.0109024\n",
      "52 Train Loss 1.7491636 Test RE 0.00037829225861068833 c 0.99796534 k 0.050071362 m 5.047949\n",
      "53 Train Loss 1.631652 Test RE 0.0004531692812610039 c 0.9995174 k 0.05005272 m 5.0459647\n",
      "54 Train Loss 1.5291244 Test RE 0.000444549151358333 c 0.9975534 k 0.05012124 m 5.0886593\n",
      "55 Train Loss 1.4778038 Test RE 0.00043652179733890664 c 0.9973559 k 0.050111417 m 5.119181\n",
      "56 Train Loss 1.3260548 Test RE 0.00042829511414247534 c 0.9982719 k 0.05008352 m 5.0957284\n",
      "57 Train Loss 1.0747943 Test RE 0.00034715333818665484 c 0.9955975 k 0.05010557 m 5.120682\n",
      "58 Train Loss 0.9763044 Test RE 0.0003338511001453869 c 1.000383 k 0.050016683 m 5.1181927\n",
      "59 Train Loss 0.8920764 Test RE 0.0003545335486031362 c 0.99953586 k 0.05001443 m 5.089381\n",
      "60 Train Loss 0.80810076 Test RE 0.0003550799927388602 c 0.9986843 k 0.04998724 m 5.0571756\n",
      "61 Train Loss 0.68963605 Test RE 0.0003241285267758384 c 0.99953574 k 0.049963333 m 5.0328875\n",
      "62 Train Loss 0.66992563 Test RE 0.0003148107429413284 c 1.0009103 k 0.04994671 m 5.0301275\n",
      "63 Train Loss 0.6589492 Test RE 0.00028340466145290917 c 1.000581 k 0.049958233 m 5.0304666\n",
      "64 Train Loss 0.6177151 Test RE 0.00024635297828759923 c 0.9992826 k 0.04999055 m 5.038781\n",
      "65 Train Loss 0.5695703 Test RE 0.0002574244408656334 c 1.0001009 k 0.04995914 m 5.028681\n",
      "66 Train Loss 0.44412804 Test RE 0.00019858310620699408 c 1.0007888 k 0.050003823 m 5.016954\n",
      "67 Train Loss 0.27741098 Test RE 0.0001775677152481141 c 0.9984379 k 0.05000826 m 5.0328383\n",
      "68 Train Loss 0.17768459 Test RE 0.00012439437416055402 c 1.0003406 k 0.049999323 m 5.0226927\n",
      "69 Train Loss 0.11173889 Test RE 5.725693666331181e-05 c 1.0001136 k 0.050003648 m 5.0172286\n",
      "70 Train Loss 0.094252795 Test RE 5.054784380584487e-05 c 0.99994296 k 0.049996473 m 5.0133677\n",
      "71 Train Loss 0.08670433 Test RE 5.9079003678021166e-05 c 0.9999533 k 0.04999929 m 5.0103507\n",
      "72 Train Loss 0.07933151 Test RE 5.601331589781619e-05 c 1.000186 k 0.04998274 m 5.0073647\n",
      "73 Train Loss 0.07498048 Test RE 4.753896206638695e-05 c 1.0004475 k 0.049968902 m 5.002697\n",
      "74 Train Loss 0.05039475 Test RE 5.6467160163965735e-05 c 1.0000207 k 0.05000051 m 5.001916\n",
      "75 Train Loss 0.036613554 Test RE 6.0076525129738595e-05 c 0.99989295 k 0.05000518 m 5.002125\n",
      "76 Train Loss 0.03369214 Test RE 6.334626850090933e-05 c 0.99989355 k 0.05000732 m 5.001236\n",
      "77 Train Loss 0.03310495 Test RE 6.149928162748403e-05 c 0.9997912 k 0.0500107 m 5.0047207\n",
      "78 Train Loss 0.032171816 Test RE 5.630350811460398e-05 c 0.9998986 k 0.050002888 m 5.0013604\n",
      "79 Train Loss 0.03140397 Test RE 5.753254857612141e-05 c 1.0000027 k 0.04999971 m 4.999236\n",
      "80 Train Loss 0.030696902 Test RE 5.707341655879995e-05 c 0.9999104 k 0.050005022 m 5.001102\n",
      "81 Train Loss 0.028765436 Test RE 4.875317412410302e-05 c 0.9998271 k 0.050003093 m 5.0006027\n",
      "82 Train Loss 0.023513595 Test RE 3.132839652326795e-05 c 1.0000535 k 0.050007112 m 5.003545\n",
      "83 Train Loss 0.01772865 Test RE 3.067360082132466e-05 c 0.9993867 k 0.05002202 m 5.005304\n",
      "84 Train Loss 0.015414587 Test RE 3.236340471241483e-05 c 0.99987173 k 0.050011594 m 5.00594\n",
      "85 Train Loss 0.013347412 Test RE 2.2415723204689577e-05 c 0.99988014 k 0.0500131 m 5.0049195\n",
      "86 Train Loss 0.01217227 Test RE 2.3110521073724066e-05 c 0.99961936 k 0.05001292 m 5.002703\n",
      "87 Train Loss 0.010686075 Test RE 2.3338542604142457e-05 c 0.9997226 k 0.05001544 m 5.0046363\n",
      "88 Train Loss 0.009289824 Test RE 2.2469021394008494e-05 c 0.9996886 k 0.050017424 m 5.00371\n",
      "89 Train Loss 0.007327231 Test RE 1.481972031115945e-05 c 0.9999533 k 0.050002452 m 4.998989\n",
      "90 Train Loss 0.006820859 Test RE 1.1632859345324638e-05 c 0.9998848 k 0.050004758 m 5.0005865\n",
      "91 Train Loss 0.006701928 Test RE 1.1609901937863488e-05 c 0.9999589 k 0.050002012 m 5.00055\n",
      "92 Train Loss 0.0065818676 Test RE 1.3763772207355524e-05 c 0.99996907 k 0.05000137 m 5.00015\n",
      "93 Train Loss 0.006377177 Test RE 1.3583695214742618e-05 c 0.9999204 k 0.050003752 m 5.0013647\n",
      "94 Train Loss 0.006222417 Test RE 1.1473940132107776e-05 c 0.99996996 k 0.050000995 m 5.0005927\n",
      "95 Train Loss 0.0060737585 Test RE 1.2080095291355318e-05 c 1.000034 k 0.049998894 m 4.999537\n",
      "96 Train Loss 0.0060124267 Test RE 1.2492238456958875e-05 c 0.99998635 k 0.050000742 m 5.000709\n",
      "97 Train Loss 0.005971134 Test RE 1.2134446585312844e-05 c 1.00001 k 0.050000466 m 5.001191\n",
      "98 Train Loss 0.005912848 Test RE 1.2400693962645991e-05 c 1.0000234 k 0.04999923 m 5.0003448\n",
      "99 Train Loss 0.005906158 Test RE 1.262455762195501e-05 c 1.000002 k 0.04999985 m 5.000355\n",
      "100 Train Loss 0.0059054988 Test RE 1.2671556326581498e-05 c 0.9999985 k 0.04999996 m 5.000374\n",
      "101 Train Loss 0.005901298 Test RE 1.2862201175503571e-05 c 0.9999869 k 0.050000343 m 5.0004706\n",
      "102 Train Loss 0.0058783987 Test RE 1.3803959769420922e-05 c 1.0000134 k 0.04999987 m 5.0004683\n",
      "103 Train Loss 0.005877723 Test RE 1.3858168255891702e-05 c 1.0000172 k 0.049999684 m 5.0004125\n",
      "104 Train Loss 0.0058770445 Test RE 1.387405048852301e-05 c 1.0000207 k 0.04999952 m 5.0003605\n",
      "105 Train Loss 0.0058749015 Test RE 1.3864935037961798e-05 c 1.0000246 k 0.04999927 m 5.000265\n",
      "106 Train Loss 0.0058742445 Test RE 1.3858800075980907e-05 c 1.0000253 k 0.04999919 m 5.0002303\n",
      "107 Train Loss 0.0058710678 Test RE 1.3795635174696625e-05 c 1.0000222 k 0.049999032 m 5.0001216\n",
      "108 Train Loss 0.005870277 Test RE 1.3789792958097047e-05 c 1.0000192 k 0.04999903 m 5.000095\n",
      "109 Train Loss 0.0058663283 Test RE 1.3697951093442045e-05 c 1.0000061 k 0.049999174 m 5.0000896\n",
      "110 Train Loss 0.005861425 Test RE 1.3663551236364437e-05 c 0.9999881 k 0.049999576 m 5.000175\n",
      "111 Train Loss 0.0058520017 Test RE 1.370387031858727e-05 c 0.99997264 k 0.05000027 m 5.00039\n",
      "112 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "113 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "114 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "115 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "116 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "117 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "118 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "119 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "120 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "121 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "122 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "123 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "124 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "125 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "126 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "127 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "128 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "129 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "130 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "131 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "132 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "133 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "134 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "135 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "136 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "137 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "138 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "139 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "140 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "141 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "142 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "143 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "144 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "145 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "146 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "147 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "148 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "149 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "150 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "151 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "152 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "153 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "154 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "155 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "156 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "157 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "158 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "159 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "160 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "161 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "162 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "163 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "164 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "165 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "166 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "167 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "168 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "169 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "170 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "171 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "172 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "173 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "174 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "175 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "176 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "177 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "178 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "179 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "180 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "181 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "182 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "183 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "184 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "185 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "186 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "187 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "188 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "189 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "190 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "191 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "192 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "193 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "194 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "195 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "196 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "197 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "198 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "199 Train Loss 0.005847863 Test RE 1.3721768104859408e-05 c 0.9999772 k 0.050000325 m 5.0004215\n",
      "Training time: 45.87\n",
      "Training time: 45.87\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 141324.19 Test RE 0.23054725511113855 c -0.030792266 k 0.15787388 m 0.00084128976\n",
      "1 Train Loss 32785.99 Test RE 0.11830430742173474 c -0.029102838 k 0.053593624 m 0.00090573524\n",
      "2 Train Loss 13817.012 Test RE 0.07265373388579595 c -0.0070913425 k 0.06822196 m 0.002367262\n",
      "3 Train Loss 2300.131 Test RE 0.016707086232604567 c 0.055962987 k 0.060474772 m 0.0063378806\n",
      "4 Train Loss 1537.3011 Test RE 0.006045471034273526 c 0.13524757 k 0.05861253 m 0.012311837\n",
      "5 Train Loss 1009.78467 Test RE 0.0055616583982621174 c 0.4248072 k 0.055399984 m 0.035794582\n",
      "6 Train Loss 466.61417 Test RE 0.00687335491995722 c 0.94620585 k 0.04603751 m 0.08080632\n",
      "7 Train Loss 347.9318 Test RE 0.003235841041688985 c 1.0933567 k 0.045451716 m 0.09586491\n",
      "8 Train Loss 331.69836 Test RE 0.003687441132649966 c 1.139932 k 0.0444125 m 0.10933019\n",
      "9 Train Loss 323.7754 Test RE 0.0034206056230603444 c 1.1066629 k 0.044966936 m 0.133137\n",
      "10 Train Loss 301.10352 Test RE 0.004025612715803872 c 1.0620158 k 0.04570615 m 0.27549192\n",
      "11 Train Loss 270.37924 Test RE 0.004132129397945215 c 1.0949408 k 0.045635834 m 0.4993305\n",
      "12 Train Loss 222.94722 Test RE 0.004554980808472947 c 1.1143888 k 0.04527744 m 0.7470036\n",
      "13 Train Loss 214.08472 Test RE 0.003954265374395201 c 1.130429 k 0.045033477 m 0.86409986\n",
      "14 Train Loss 184.91708 Test RE 0.004323923160858931 c 1.1131603 k 0.045829974 m 1.1479042\n",
      "15 Train Loss 168.62729 Test RE 0.003422000090178722 c 1.0896925 k 0.046069503 m 1.2810112\n",
      "16 Train Loss 158.78665 Test RE 0.0034691175027850136 c 1.0812856 k 0.046316575 m 1.4700772\n",
      "17 Train Loss 146.0454 Test RE 0.0033595430619866072 c 1.0827513 k 0.046474364 m 1.6584395\n",
      "18 Train Loss 128.67471 Test RE 0.0030366011692171653 c 1.0520242 k 0.046895485 m 1.9905102\n",
      "19 Train Loss 111.345276 Test RE 0.002905971477581385 c 1.0888293 k 0.046465073 m 2.1828947\n",
      "20 Train Loss 101.33922 Test RE 0.002785481556675723 c 1.093002 k 0.04651916 m 2.3507946\n",
      "21 Train Loss 94.88016 Test RE 0.0026722248992299834 c 1.0505667 k 0.0475149 m 2.4949253\n",
      "22 Train Loss 88.274666 Test RE 0.002786531665938174 c 1.0629525 k 0.047268424 m 2.5824654\n",
      "23 Train Loss 85.89277 Test RE 0.0026747225484532173 c 1.070845 k 0.047193542 m 2.6198173\n",
      "24 Train Loss 80.78824 Test RE 0.0025773032830497485 c 1.0506514 k 0.047579087 m 2.8574126\n",
      "25 Train Loss 78.26309 Test RE 0.002544770493889666 c 1.067499 k 0.04746952 m 2.8835118\n",
      "26 Train Loss 52.32736 Test RE 0.002565035275097458 c 1.0639951 k 0.048000004 m 3.5506804\n",
      "27 Train Loss 34.272675 Test RE 0.001597252178162002 c 1.0128646 k 0.04896251 m 3.928074\n",
      "28 Train Loss 28.453703 Test RE 0.0016665796706338612 c 1.0233206 k 0.048935212 m 4.153837\n",
      "29 Train Loss 25.915995 Test RE 0.001668597949235545 c 1.0186231 k 0.04907233 m 4.2394733\n",
      "30 Train Loss 22.905272 Test RE 0.0015794392303961678 c 1.0205492 k 0.049112752 m 4.2850556\n",
      "31 Train Loss 20.723944 Test RE 0.0015203071382406564 c 1.0134236 k 0.049256597 m 4.367355\n",
      "32 Train Loss 17.660065 Test RE 0.0014231486643055584 c 1.0104965 k 0.04919586 m 4.2805033\n",
      "33 Train Loss 13.851844 Test RE 0.0011656730427890703 c 1.0239639 k 0.04896434 m 4.190108\n",
      "34 Train Loss 11.474371 Test RE 0.0010737563119045362 c 1.015711 k 0.04923206 m 4.354806\n",
      "35 Train Loss 9.478147 Test RE 0.0009556533838901086 c 1.0124942 k 0.049391102 m 4.542235\n",
      "36 Train Loss 8.508507 Test RE 0.0009042723902107186 c 1.0166957 k 0.049365234 m 4.632276\n",
      "37 Train Loss 7.3544254 Test RE 0.0008476697008222794 c 1.0078068 k 0.049619127 m 4.725525\n",
      "38 Train Loss 6.8095613 Test RE 0.000767171079687522 c 1.0078317 k 0.049583696 m 4.731924\n",
      "39 Train Loss 6.153246 Test RE 0.0006291309889508937 c 1.0089751 k 0.049563386 m 4.7231216\n",
      "40 Train Loss 5.522338 Test RE 0.0006378709888773844 c 1.0034467 k 0.049724795 m 4.744174\n",
      "41 Train Loss 4.8825936 Test RE 0.0004907419305287397 c 1.008943 k 0.049621798 m 4.7742076\n",
      "42 Train Loss 3.7976696 Test RE 0.0005023355500945102 c 1.0093983 k 0.04968931 m 4.8692503\n",
      "43 Train Loss 1.8386339 Test RE 0.00035622937938075295 c 1.0007038 k 0.049874153 m 4.9502935\n",
      "44 Train Loss 1.362495 Test RE 0.00026634669148792116 c 1.0042017 k 0.049828082 m 4.927852\n",
      "45 Train Loss 0.8801973 Test RE 0.00022335079405739732 c 1.001281 k 0.049913336 m 4.9579244\n",
      "46 Train Loss 0.7610912 Test RE 0.00017718460939179662 c 1.0010093 k 0.049917422 m 4.957797\n",
      "47 Train Loss 0.7431734 Test RE 0.00015414063957351298 c 1.0017192 k 0.04991767 m 4.9616804\n",
      "48 Train Loss 0.7267027 Test RE 0.00015329510971920596 c 1.0022507 k 0.049904574 m 4.9683037\n",
      "49 Train Loss 0.7099881 Test RE 0.00016049518496339004 c 1.0014586 k 0.04991388 m 4.9678893\n",
      "50 Train Loss 0.6925771 Test RE 0.00015794410883483403 c 1.0008819 k 0.04993632 m 4.984956\n",
      "51 Train Loss 0.6554218 Test RE 0.00014946281495961685 c 1.0007157 k 0.049973097 m 5.0076723\n",
      "52 Train Loss 0.61389476 Test RE 0.00013486443758985812 c 1.0002745 k 0.049973343 m 5.001832\n",
      "53 Train Loss 0.5862268 Test RE 0.0001490090624770281 c 0.9999144 k 0.04999705 m 4.997477\n",
      "54 Train Loss 0.54688984 Test RE 0.00015453148733033458 c 0.9993277 k 0.05002542 m 5.0048532\n",
      "55 Train Loss 0.51775306 Test RE 0.00014192292415797663 c 1.0005258 k 0.04998623 m 4.9805193\n",
      "56 Train Loss 0.47861013 Test RE 0.00010963102183463548 c 1.0018 k 0.049954657 m 4.9909706\n",
      "57 Train Loss 0.45462918 Test RE 0.00011439696134152237 c 1.0002389 k 0.049960025 m 4.9979396\n",
      "58 Train Loss 0.4454099 Test RE 0.00011677383445619712 c 1.0008209 k 0.049959093 m 4.988593\n",
      "59 Train Loss 0.4420556 Test RE 0.0001215522531404478 c 1.0009165 k 0.04995995 m 4.991723\n",
      "60 Train Loss 0.4344857 Test RE 0.0001291497721889826 c 1.0003914 k 0.04997348 m 5.0034547\n",
      "61 Train Loss 0.4074192 Test RE 0.00014403554724804347 c 1.0004208 k 0.049981132 m 5.011265\n",
      "62 Train Loss 0.3464008 Test RE 0.00014159488442280795 c 1.0008299 k 0.0499665 m 4.9969063\n",
      "63 Train Loss 0.3098597 Test RE 0.00010864988355957111 c 1.0004889 k 0.049993347 m 5.0095806\n",
      "64 Train Loss 0.30174035 Test RE 0.00010670085993137968 c 0.99977636 k 0.050001334 m 5.009678\n",
      "65 Train Loss 0.2723859 Test RE 0.00010805064562424371 c 1.0003659 k 0.04998483 m 4.9972925\n",
      "66 Train Loss 0.21492125 Test RE 9.787990240646873e-05 c 1.0004777 k 0.049989533 m 4.993429\n",
      "67 Train Loss 0.20305061 Test RE 9.957855875521712e-05 c 0.9997839 k 0.050000172 m 4.998896\n",
      "68 Train Loss 0.19656397 Test RE 9.774811146021951e-05 c 1.0002354 k 0.04999253 m 5.0008783\n",
      "69 Train Loss 0.18838397 Test RE 0.00010306374523737252 c 1.0004783 k 0.049981683 m 4.9980297\n",
      "70 Train Loss 0.18081895 Test RE 9.945267246119977e-05 c 1.0003628 k 0.049983345 m 4.996885\n",
      "71 Train Loss 0.17652358 Test RE 9.51039233727429e-05 c 1.000753 k 0.049968224 m 4.9896445\n",
      "72 Train Loss 0.17283534 Test RE 9.369788787101997e-05 c 1.0006019 k 0.04997098 m 4.9895287\n",
      "73 Train Loss 0.16540141 Test RE 9.438532921871042e-05 c 1.0003101 k 0.04998605 m 4.9935417\n",
      "74 Train Loss 0.15848696 Test RE 9.052138623591065e-05 c 1.0004838 k 0.04998273 m 4.989775\n",
      "75 Train Loss 0.13935755 Test RE 8.44871180659378e-05 c 0.99982744 k 0.050000865 m 4.9974933\n",
      "76 Train Loss 0.12903778 Test RE 7.828253966993533e-05 c 0.9998571 k 0.050003685 m 5.0057216\n",
      "77 Train Loss 0.12247793 Test RE 8.269284476026662e-05 c 1.0003337 k 0.04999551 m 4.9972215\n",
      "78 Train Loss 0.11938809 Test RE 8.538852330088047e-05 c 0.99984026 k 0.05000408 m 4.999963\n",
      "79 Train Loss 0.11783639 Test RE 8.280021831524803e-05 c 0.99966687 k 0.05000743 m 5.004821\n",
      "80 Train Loss 0.11718023 Test RE 8.226747697497361e-05 c 0.9998747 k 0.050005052 m 5.004827\n",
      "81 Train Loss 0.116810344 Test RE 8.322245895975445e-05 c 0.9999434 k 0.05000283 m 5.005136\n",
      "82 Train Loss 0.11529576 Test RE 7.878063083293537e-05 c 1.0000361 k 0.05000358 m 5.0056376\n",
      "83 Train Loss 0.111646935 Test RE 7.334053947274218e-05 c 0.9998811 k 0.0500037 m 5.0037394\n",
      "84 Train Loss 0.11009531 Test RE 7.169544783212224e-05 c 0.9996554 k 0.050007544 m 5.00527\n",
      "85 Train Loss 0.10612444 Test RE 6.623078273529382e-05 c 0.99909484 k 0.050018296 m 5.01095\n",
      "86 Train Loss 0.08742755 Test RE 5.826079540901813e-05 c 0.9996103 k 0.050008014 m 5.00866\n",
      "87 Train Loss 0.073679104 Test RE 4.4712784537288184e-05 c 1.0005318 k 0.049978174 m 4.995304\n",
      "88 Train Loss 0.06518686 Test RE 3.4858071504444175e-05 c 0.99955314 k 0.050003134 m 5.0001063\n",
      "89 Train Loss 0.06223913 Test RE 3.0805638707075775e-05 c 1.0000823 k 0.04999979 m 4.9998283\n",
      "90 Train Loss 0.06103086 Test RE 2.497007842735323e-05 c 1.0001183 k 0.049996447 m 5.000094\n",
      "91 Train Loss 0.060655914 Test RE 2.2843210204065547e-05 c 1.0000372 k 0.049999595 m 5.001758\n",
      "92 Train Loss 0.060460404 Test RE 2.316266208997235e-05 c 0.99991226 k 0.050001103 m 5.0025992\n",
      "93 Train Loss 0.059828136 Test RE 2.4292894104720082e-05 c 0.99987555 k 0.050000094 m 5.000714\n",
      "94 Train Loss 0.059304364 Test RE 2.257908601695916e-05 c 1.0000095 k 0.050000455 m 5.00201\n",
      "95 Train Loss 0.05918551 Test RE 2.1630258552971152e-05 c 0.9999558 k 0.049999915 m 5.002096\n",
      "96 Train Loss 0.059095133 Test RE 2.1376371068199147e-05 c 0.9999919 k 0.049998023 m 5.0008173\n",
      "97 Train Loss 0.058980606 Test RE 2.189788974045812e-05 c 1.0001489 k 0.049995843 m 5.0009255\n",
      "98 Train Loss 0.058611624 Test RE 2.1946936607383752e-05 c 1.0000969 k 0.04999638 m 5.0015326\n",
      "99 Train Loss 0.05810208 Test RE 2.3086351031646116e-05 c 0.99999297 k 0.0499985 m 5.0020337\n",
      "100 Train Loss 0.05784028 Test RE 2.3567077663157924e-05 c 1.0000402 k 0.04999705 m 5.002506\n",
      "101 Train Loss 0.057205673 Test RE 2.497706488785755e-05 c 1.0002524 k 0.04998748 m 4.9989085\n",
      "102 Train Loss 0.054311976 Test RE 2.4151108048132324e-05 c 1.0006508 k 0.049985897 m 4.9957047\n",
      "103 Train Loss 0.047404755 Test RE 3.188598557792098e-05 c 0.9999544 k 0.049996257 m 4.9997044\n",
      "104 Train Loss 0.039066873 Test RE 2.7871542398698046e-05 c 0.9997594 k 0.05000256 m 5.0028906\n",
      "105 Train Loss 0.032310687 Test RE 2.2967160047089895e-05 c 1.0003281 k 0.04998802 m 4.996809\n",
      "106 Train Loss 0.031149384 Test RE 2.2105263020875573e-05 c 1.0001218 k 0.04998903 m 4.997889\n",
      "107 Train Loss 0.03059829 Test RE 2.3005908477727346e-05 c 1.000275 k 0.049987536 m 4.99731\n",
      "108 Train Loss 0.030092645 Test RE 2.393751966112484e-05 c 1.0001357 k 0.049990427 m 4.99875\n",
      "109 Train Loss 0.028611667 Test RE 2.2211886343004195e-05 c 1.000095 k 0.04999532 m 4.998671\n",
      "110 Train Loss 0.027344484 Test RE 1.732543210638338e-05 c 1.0000972 k 0.04999621 m 4.9974713\n",
      "111 Train Loss 0.026465446 Test RE 1.864418796731343e-05 c 0.99975157 k 0.050004337 m 5.000806\n",
      "112 Train Loss 0.02436194 Test RE 2.0153244646407216e-05 c 1.0001936 k 0.049997933 m 4.9984713\n",
      "113 Train Loss 0.02076751 Test RE 1.666138245140659e-05 c 1.0002633 k 0.049993176 m 4.9987044\n",
      "114 Train Loss 0.017718907 Test RE 1.643632864752954e-05 c 1.0000694 k 0.049995117 m 4.999686\n",
      "115 Train Loss 0.016667949 Test RE 1.3216172131783245e-05 c 1.0001704 k 0.049990527 m 4.998689\n",
      "116 Train Loss 0.016317017 Test RE 1.4300606581540793e-05 c 1.0002165 k 0.049989678 m 4.9984684\n",
      "117 Train Loss 0.016241344 Test RE 1.5050720836010068e-05 c 1.000227 k 0.04998946 m 4.998009\n",
      "118 Train Loss 0.016151872 Test RE 1.5350380494491597e-05 c 1.000109 k 0.049991455 m 4.997838\n",
      "119 Train Loss 0.016028428 Test RE 1.3643762625338862e-05 c 1.0001312 k 0.049991958 m 4.9983754\n",
      "120 Train Loss 0.015950188 Test RE 1.2649806906845357e-05 c 1.0002186 k 0.04999051 m 4.9983764\n",
      "121 Train Loss 0.015827585 Test RE 1.3671885891702872e-05 c 1.0001702 k 0.049990688 m 4.998309\n",
      "122 Train Loss 0.015580296 Test RE 1.2479298151436452e-05 c 1.0000702 k 0.049993 m 4.998028\n",
      "123 Train Loss 0.015364923 Test RE 1.2135612664705093e-05 c 1.0002064 k 0.049991854 m 4.998267\n",
      "124 Train Loss 0.015310347 Test RE 1.1858664322504791e-05 c 1.0001096 k 0.049993064 m 4.9982867\n",
      "125 Train Loss 0.015242259 Test RE 1.1220565664584036e-05 c 1.0000246 k 0.049994368 m 4.9982986\n",
      "126 Train Loss 0.0150684295 Test RE 1.2772154097720179e-05 c 1.0001577 k 0.049993135 m 4.9988174\n",
      "127 Train Loss 0.014551232 Test RE 1.3572631427431329e-05 c 1.0001223 k 0.049994037 m 4.998939\n",
      "128 Train Loss 0.014270765 Test RE 1.2905527548437619e-05 c 1.0001189 k 0.049993563 m 4.9983473\n",
      "129 Train Loss 0.014043539 Test RE 1.3595430279687219e-05 c 1.0001707 k 0.049993187 m 4.998473\n",
      "130 Train Loss 0.0137903355 Test RE 1.2663643760076871e-05 c 0.9999994 k 0.04999682 m 5.0001187\n",
      "131 Train Loss 0.013469164 Test RE 1.3293493814041608e-05 c 1.0001215 k 0.049995426 m 4.9998503\n",
      "132 Train Loss 0.013171153 Test RE 1.3769434384487678e-05 c 1.0001378 k 0.04999361 m 4.9989514\n",
      "133 Train Loss 0.012960324 Test RE 1.252185809461378e-05 c 1.0000396 k 0.049996335 m 5.000424\n",
      "134 Train Loss 0.012777059 Test RE 1.0505563127450661e-05 c 1.0000452 k 0.049996097 m 5.000943\n",
      "135 Train Loss 0.012622625 Test RE 1.0357988218383104e-05 c 1.0001048 k 0.049993455 m 4.998732\n",
      "136 Train Loss 0.012383958 Test RE 1.1044364599591605e-05 c 1.0001397 k 0.04999181 m 4.997806\n",
      "137 Train Loss 0.01202151 Test RE 1.0842910466127042e-05 c 1.0001341 k 0.04999268 m 4.9992175\n",
      "138 Train Loss 0.011853538 Test RE 1.067664060233939e-05 c 1.0002085 k 0.049991082 m 4.997825\n",
      "139 Train Loss 0.011651083 Test RE 1.02382291671161e-05 c 1.0002248 k 0.049992368 m 4.998336\n",
      "140 Train Loss 0.011428352 Test RE 1.0054525270220644e-05 c 1.0000699 k 0.049995773 m 4.9995465\n",
      "141 Train Loss 0.01114816 Test RE 1.0614725717417953e-05 c 0.9999025 k 0.050001163 m 5.0002017\n",
      "142 Train Loss 0.010918964 Test RE 1.0803876359055179e-05 c 0.9999843 k 0.049999606 m 4.999944\n",
      "143 Train Loss 0.010642299 Test RE 1.074940410330557e-05 c 1.0000232 k 0.049999148 m 4.999334\n",
      "144 Train Loss 0.010534234 Test RE 1.0257039305284681e-05 c 0.9999801 k 0.0500006 m 4.999412\n",
      "145 Train Loss 0.010508476 Test RE 1.0533819381622906e-05 c 0.99998367 k 0.050000787 m 4.9998927\n",
      "146 Train Loss 0.010507553 Test RE 1.0534409702191453e-05 c 0.9999851 k 0.050000798 m 4.999952\n",
      "147 Train Loss 0.010499337 Test RE 1.043702646329797e-05 c 0.9999928 k 0.05000083 m 5.000192\n",
      "148 Train Loss 0.010496879 Test RE 1.0395853796586922e-05 c 0.9999953 k 0.050000813 m 5.0002146\n",
      "149 Train Loss 0.010495884 Test RE 1.0347362915937175e-05 c 0.999997 k 0.050000798 m 5.0002165\n",
      "150 Train Loss 0.010493818 Test RE 1.0301941562963329e-05 c 0.99999917 k 0.050000753 m 5.0001955\n",
      "151 Train Loss 0.010489942 Test RE 1.0239527635663342e-05 c 1.0000014 k 0.05000067 m 5.000142\n",
      "152 Train Loss 0.010485746 Test RE 1.0242679500787963e-05 c 1.0000037 k 0.05000059 m 5.0000987\n",
      "153 Train Loss 0.010485591 Test RE 1.0255366826296204e-05 c 1.0000037 k 0.050000586 m 5.000094\n",
      "154 Train Loss 0.010484652 Test RE 1.0256284816789036e-05 c 1.0000035 k 0.05000058 m 5.000095\n",
      "155 Train Loss 0.010484243 Test RE 1.0276735954369286e-05 c 1.0000027 k 0.050000582 m 5.000096\n",
      "156 Train Loss 0.010484235 Test RE 1.0275919056573891e-05 c 1.0000027 k 0.050000582 m 5.000096\n",
      "157 Train Loss 0.010483965 Test RE 1.0281585940159404e-05 c 1.000002 k 0.05000059 m 5.0001035\n",
      "158 Train Loss 0.010483948 Test RE 1.0297521513578419e-05 c 1.0000008 k 0.050000604 m 5.000114\n",
      "159 Train Loss 0.010482959 Test RE 1.0299979180074243e-05 c 0.9999996 k 0.050000627 m 5.000119\n",
      "160 Train Loss 0.0104827 Test RE 1.0317615982626204e-05 c 0.99999803 k 0.050000664 m 5.0001297\n",
      "161 Train Loss 0.0104827 Test RE 1.0317615982626204e-05 c 0.99999803 k 0.050000664 m 5.0001297\n",
      "162 Train Loss 0.0104827 Test RE 1.0317615982626204e-05 c 0.99999803 k 0.050000664 m 5.0001297\n",
      "163 Train Loss 0.0104827 Test RE 1.0317615982626204e-05 c 0.99999803 k 0.050000664 m 5.0001297\n",
      "164 Train Loss 0.0104827 Test RE 1.0317615982626204e-05 c 0.99999803 k 0.050000664 m 5.0001297\n",
      "165 Train Loss 0.0104827 Test RE 1.0317615982626204e-05 c 0.99999803 k 0.050000664 m 5.0001297\n",
      "166 Train Loss 0.0104827 Test RE 1.0317615982626204e-05 c 0.99999803 k 0.050000664 m 5.0001297\n",
      "167 Train Loss 0.0104827 Test RE 1.0317615982626204e-05 c 0.99999803 k 0.050000664 m 5.0001297\n",
      "168 Train Loss 0.0104827 Test RE 1.0317615982626204e-05 c 0.99999803 k 0.050000664 m 5.0001297\n",
      "169 Train Loss 0.0104827 Test RE 1.0317615982626204e-05 c 0.99999803 k 0.050000664 m 5.0001297\n",
      "170 Train Loss 0.0104827 Test RE 1.0317615982626204e-05 c 0.99999803 k 0.050000664 m 5.0001297\n",
      "171 Train Loss 0.0104827 Test RE 1.0317615982626204e-05 c 0.99999803 k 0.050000664 m 5.0001297\n",
      "172 Train Loss 0.0104827 Test RE 1.0317615982626204e-05 c 0.99999803 k 0.050000664 m 5.0001297\n",
      "173 Train Loss 0.0104827 Test RE 1.0317615982626204e-05 c 0.99999803 k 0.050000664 m 5.0001297\n",
      "174 Train Loss 0.0104827 Test RE 1.0317615982626204e-05 c 0.99999803 k 0.050000664 m 5.0001297\n",
      "175 Train Loss 0.0104827 Test RE 1.0317615982626204e-05 c 0.99999803 k 0.050000664 m 5.0001297\n",
      "176 Train Loss 0.0104827 Test RE 1.0317615982626204e-05 c 0.99999803 k 0.050000664 m 5.0001297\n",
      "177 Train Loss 0.0104827 Test RE 1.0317615982626204e-05 c 0.99999803 k 0.050000664 m 5.0001297\n",
      "178 Train Loss 0.0104827 Test RE 1.0317615982626204e-05 c 0.99999803 k 0.050000664 m 5.0001297\n",
      "179 Train Loss 0.0104827 Test RE 1.0317615982626204e-05 c 0.99999803 k 0.050000664 m 5.0001297\n",
      "180 Train Loss 0.0104827 Test RE 1.0317615982626204e-05 c 0.99999803 k 0.050000664 m 5.0001297\n",
      "181 Train Loss 0.0104827 Test RE 1.0317615982626204e-05 c 0.99999803 k 0.050000664 m 5.0001297\n",
      "182 Train Loss 0.0104827 Test RE 1.0317615982626204e-05 c 0.99999803 k 0.050000664 m 5.0001297\n",
      "183 Train Loss 0.0104827 Test RE 1.0317615982626204e-05 c 0.99999803 k 0.050000664 m 5.0001297\n",
      "184 Train Loss 0.0104827 Test RE 1.0317615982626204e-05 c 0.99999803 k 0.050000664 m 5.0001297\n",
      "185 Train Loss 0.0104827 Test RE 1.0317615982626204e-05 c 0.99999803 k 0.050000664 m 5.0001297\n",
      "186 Train Loss 0.0104827 Test RE 1.0317615982626204e-05 c 0.99999803 k 0.050000664 m 5.0001297\n",
      "187 Train Loss 0.0104827 Test RE 1.0317615982626204e-05 c 0.99999803 k 0.050000664 m 5.0001297\n",
      "188 Train Loss 0.0104827 Test RE 1.0317615982626204e-05 c 0.99999803 k 0.050000664 m 5.0001297\n",
      "189 Train Loss 0.0104827 Test RE 1.0317615982626204e-05 c 0.99999803 k 0.050000664 m 5.0001297\n",
      "190 Train Loss 0.0104827 Test RE 1.0317615982626204e-05 c 0.99999803 k 0.050000664 m 5.0001297\n",
      "191 Train Loss 0.0104827 Test RE 1.0317615982626204e-05 c 0.99999803 k 0.050000664 m 5.0001297\n",
      "192 Train Loss 0.0104827 Test RE 1.0317615982626204e-05 c 0.99999803 k 0.050000664 m 5.0001297\n",
      "193 Train Loss 0.0104827 Test RE 1.0317615982626204e-05 c 0.99999803 k 0.050000664 m 5.0001297\n",
      "194 Train Loss 0.0104827 Test RE 1.0317615982626204e-05 c 0.99999803 k 0.050000664 m 5.0001297\n",
      "195 Train Loss 0.0104827 Test RE 1.0317615982626204e-05 c 0.99999803 k 0.050000664 m 5.0001297\n",
      "196 Train Loss 0.0104827 Test RE 1.0317615982626204e-05 c 0.99999803 k 0.050000664 m 5.0001297\n",
      "197 Train Loss 0.0104827 Test RE 1.0317615982626204e-05 c 0.99999803 k 0.050000664 m 5.0001297\n",
      "198 Train Loss 0.0104827 Test RE 1.0317615982626204e-05 c 0.99999803 k 0.050000664 m 5.0001297\n",
      "199 Train Loss 0.0104827 Test RE 1.0317615982626204e-05 c 0.99999803 k 0.050000664 m 5.0001297\n",
      "Training time: 52.04\n",
      "Training time: 52.04\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 102061.82 Test RE 0.21341683011062434 c 0.016364787 k 0.07008706 m -0.00014918274\n",
      "1 Train Loss 42490.11 Test RE 0.1344270237257079 c 0.032135878 k 0.061207343 m -1.4162073e-05\n",
      "2 Train Loss 36321.83 Test RE 0.12357636803518068 c 0.10863268 k 0.059876274 m 0.0010937694\n",
      "3 Train Loss 6827.035 Test RE 0.038871911440609454 c 0.6712018 k 0.049460832 m 0.008656516\n",
      "4 Train Loss 4084.3296 Test RE 0.02242784937352474 c 0.71653277 k 0.05054855 m 0.009337956\n",
      "5 Train Loss 2948.1802 Test RE 0.020122492293889738 c 0.8549898 k 0.04798934 m 0.013706386\n",
      "6 Train Loss 2186.3235 Test RE 0.021271619509505783 c 1.0358961 k 0.04502859 m 0.025205005\n",
      "7 Train Loss 1710.7773 Test RE 0.01862404022653594 c 1.1301291 k 0.043720495 m 0.044334397\n",
      "8 Train Loss 1155.4194 Test RE 0.013983273663908183 c 1.3209733 k 0.041610055 m 0.1056517\n",
      "9 Train Loss 650.44727 Test RE 0.008735877456696451 c 1.4484153 k 0.040004138 m 0.21706778\n",
      "10 Train Loss 402.2004 Test RE 0.005872915577218179 c 1.3325156 k 0.04149794 m 0.30248702\n",
      "11 Train Loss 313.61487 Test RE 0.005646554270618547 c 1.133584 k 0.044601656 m 0.3478567\n",
      "12 Train Loss 286.1975 Test RE 0.005824801241851858 c 1.1330521 k 0.04451843 m 0.35384226\n",
      "13 Train Loss 282.15643 Test RE 0.00619444119704218 c 1.1241153 k 0.044765346 m 0.36024177\n",
      "14 Train Loss 278.56183 Test RE 0.006170362245814939 c 1.1035627 k 0.04510898 m 0.38486648\n",
      "15 Train Loss 272.301 Test RE 0.005717147148845847 c 1.0752244 k 0.045557003 m 0.46051574\n",
      "16 Train Loss 262.65082 Test RE 0.0052730431058865485 c 1.0864022 k 0.045522653 m 0.6023619\n",
      "17 Train Loss 251.12212 Test RE 0.005516930038419015 c 1.0967168 k 0.045540445 m 0.79948\n",
      "18 Train Loss 233.94147 Test RE 0.005741982242569836 c 1.1312703 k 0.045046974 m 1.0863116\n",
      "19 Train Loss 227.46489 Test RE 0.005777546741018722 c 1.1068473 k 0.045560278 m 1.1972115\n",
      "20 Train Loss 222.37595 Test RE 0.005737798771104704 c 1.0922312 k 0.045831952 m 1.3457154\n",
      "21 Train Loss 219.1723 Test RE 0.005869110078733823 c 1.1013403 k 0.045825664 m 1.4707351\n",
      "22 Train Loss 212.8612 Test RE 0.005787649575527652 c 1.0805928 k 0.04621702 m 1.6910615\n",
      "23 Train Loss 194.0567 Test RE 0.005698005153477921 c 1.0494667 k 0.046957918 m 2.2870028\n",
      "24 Train Loss 164.89217 Test RE 0.0054161041210849265 c 0.99706435 k 0.048696063 m 2.8632364\n",
      "25 Train Loss 106.745285 Test RE 0.004711407386912304 c 1.0018846 k 0.048696343 m 4.171779\n",
      "26 Train Loss 68.00177 Test RE 0.003697495871606898 c 1.0371441 k 0.0491657 m 5.011235\n",
      "27 Train Loss 45.68155 Test RE 0.0028524498019399745 c 0.98558056 k 0.04996876 m 5.4255705\n",
      "28 Train Loss 37.29679 Test RE 0.002340485808078937 c 1.0094697 k 0.050049465 m 5.501292\n",
      "29 Train Loss 29.21648 Test RE 0.001565586596436613 c 1.0138711 k 0.049925353 m 5.5628343\n",
      "30 Train Loss 14.223734 Test RE 0.0008247560223583364 c 0.97507626 k 0.05083567 m 5.5922627\n",
      "31 Train Loss 10.909887 Test RE 0.0008556194532530634 c 0.9874696 k 0.050676376 m 5.509234\n",
      "32 Train Loss 9.922022 Test RE 0.0008760087634470605 c 0.99126035 k 0.050609164 m 5.392347\n",
      "33 Train Loss 9.513372 Test RE 0.0009575453433431556 c 0.98622817 k 0.050686736 m 5.3383694\n",
      "34 Train Loss 7.7782288 Test RE 0.0007773096351925325 c 0.9820352 k 0.0506239 m 5.3300576\n",
      "35 Train Loss 5.4382477 Test RE 0.0005180933977536754 c 0.99682444 k 0.05019996 m 5.2092113\n",
      "36 Train Loss 5.156337 Test RE 0.00048826184176424645 c 0.9909079 k 0.050309107 m 5.1848016\n",
      "37 Train Loss 4.542119 Test RE 0.00043059499330483505 c 0.99810594 k 0.050144434 m 5.134994\n",
      "38 Train Loss 3.1940875 Test RE 0.0003223568298368376 c 1.0084972 k 0.049787328 m 4.9858136\n",
      "39 Train Loss 2.844077 Test RE 0.0002742822614360417 c 1.004371 k 0.049810737 m 4.960026\n",
      "40 Train Loss 2.74073 Test RE 0.0003270624229151971 c 1.0041491 k 0.04983317 m 4.9822235\n",
      "41 Train Loss 2.125767 Test RE 0.0004491674376131502 c 1.0038519 k 0.049848266 m 4.9661756\n",
      "42 Train Loss 1.4253821 Test RE 0.0002986209934329292 c 1.0008357 k 0.049875427 m 4.9173656\n",
      "43 Train Loss 1.2821419 Test RE 0.0002870819961018028 c 1.0016351 k 0.049901333 m 4.943643\n",
      "44 Train Loss 1.2458262 Test RE 0.0002882866764609975 c 1.001692 k 0.049888376 m 4.9333878\n",
      "45 Train Loss 1.221956 Test RE 0.0002656923578814704 c 1.003001 k 0.049851943 m 4.916488\n",
      "46 Train Loss 1.051367 Test RE 0.00025750785586041024 c 1.0041233 k 0.049868874 m 4.945663\n",
      "47 Train Loss 0.8724051 Test RE 0.0002488447283067351 c 1.0000534 k 0.049968004 m 4.9817824\n",
      "48 Train Loss 0.8237197 Test RE 0.00021423838254422978 c 1.0006317 k 0.0499479 m 4.9845166\n",
      "49 Train Loss 0.7337563 Test RE 0.00018468485328645455 c 1.0022433 k 0.0499123 m 5.007624\n",
      "50 Train Loss 0.6564108 Test RE 0.00017368356459622939 c 1.0007603 k 0.049941257 m 5.0134664\n",
      "51 Train Loss 0.64730704 Test RE 0.00016673593197144026 c 1.0005587 k 0.049931828 m 5.0028415\n",
      "52 Train Loss 0.6209027 Test RE 0.0001649958660786756 c 1.001728 k 0.04991086 m 4.997472\n",
      "53 Train Loss 0.53281766 Test RE 0.0001595547846481873 c 1.0017706 k 0.04993282 m 4.995003\n",
      "54 Train Loss 0.5234335 Test RE 0.00016495069657275737 c 1.0011724 k 0.04993881 m 4.9896665\n",
      "55 Train Loss 0.51779455 Test RE 0.00016779743938839654 c 1.0012953 k 0.049939618 m 4.9920335\n",
      "56 Train Loss 0.5026863 Test RE 0.00017137685101030852 c 1.0015898 k 0.049939416 m 5.0012097\n",
      "57 Train Loss 0.4136114 Test RE 0.0001812795972286221 c 1.0010573 k 0.049940627 m 4.9918427\n",
      "58 Train Loss 0.38173985 Test RE 0.00017951055826435393 c 1.0018744 k 0.04990855 m 4.971154\n",
      "59 Train Loss 0.3646623 Test RE 0.00017830996742613505 c 1.0021548 k 0.04990112 m 4.975692\n",
      "60 Train Loss 0.26658964 Test RE 0.00015508772331672082 c 1.0008066 k 0.04991661 m 4.9713893\n",
      "61 Train Loss 0.2285109 Test RE 0.00012329204778156015 c 1.0017546 k 0.049904943 m 4.962099\n",
      "62 Train Loss 0.21933325 Test RE 0.00011974887784788343 c 1.0012397 k 0.04992431 m 4.978101\n",
      "63 Train Loss 0.20879331 Test RE 0.00012087591618079423 c 1.0006205 k 0.049943525 m 4.986085\n",
      "64 Train Loss 0.18631214 Test RE 0.00010954404495435932 c 1.0013616 k 0.049938876 m 4.9850655\n",
      "65 Train Loss 0.17411757 Test RE 9.605599557145045e-05 c 1.0007552 k 0.04994863 m 4.9824667\n",
      "66 Train Loss 0.17127393 Test RE 9.874730466907732e-05 c 1.0008533 k 0.049953893 m 4.9829574\n",
      "67 Train Loss 0.16940407 Test RE 0.0001015335095025042 c 1.000644 k 0.049964827 m 4.9867578\n",
      "68 Train Loss 0.16636285 Test RE 0.00010171526405580531 c 1.0002141 k 0.049978245 m 4.992507\n",
      "69 Train Loss 0.1622786 Test RE 9.731010959981519e-05 c 1.0004181 k 0.049967874 m 4.987045\n",
      "70 Train Loss 0.15567124 Test RE 9.175597066696959e-05 c 1.0007143 k 0.049964756 m 4.984094\n",
      "71 Train Loss 0.15338454 Test RE 8.707600117583875e-05 c 1.0006377 k 0.049969498 m 4.9904685\n",
      "72 Train Loss 0.15142198 Test RE 8.330280399085704e-05 c 1.0005141 k 0.04997029 m 4.9886827\n",
      "73 Train Loss 0.1501316 Test RE 8.797684428063835e-05 c 1.0004108 k 0.04996783 m 4.9844646\n",
      "74 Train Loss 0.13572353 Test RE 0.00010156210767035402 c 1.0003129 k 0.04997867 m 4.9891577\n",
      "75 Train Loss 0.093708634 Test RE 7.560302825558906e-05 c 1.0010042 k 0.049986973 m 5.00849\n",
      "76 Train Loss 0.050990514 Test RE 4.9717713176191095e-05 c 1.0011584 k 0.04997223 m 4.9970284\n",
      "77 Train Loss 0.043882236 Test RE 4.718978063016864e-05 c 1.0000395 k 0.049996812 m 4.999367\n",
      "78 Train Loss 0.03814222 Test RE 4.534311723673453e-05 c 0.9997646 k 0.04999985 m 4.9975576\n",
      "79 Train Loss 0.0315446 Test RE 3.8687544951658225e-05 c 0.99999386 k 0.049993876 m 5.000561\n",
      "80 Train Loss 0.02659497 Test RE 3.618612099017668e-05 c 0.9994284 k 0.050009336 m 5.0039268\n",
      "81 Train Loss 0.021053463 Test RE 2.866877451926054e-05 c 1.0001583 k 0.049995825 m 4.9997973\n",
      "82 Train Loss 0.016284756 Test RE 2.4479573449143755e-05 c 1.0003673 k 0.049997784 m 4.9998875\n",
      "83 Train Loss 0.011439387 Test RE 2.143566324900412e-05 c 0.99976575 k 0.05000264 m 5.000497\n",
      "84 Train Loss 0.010238415 Test RE 1.8250280798864832e-05 c 1.000004 k 0.04999757 m 4.9998693\n",
      "85 Train Loss 0.009420614 Test RE 1.5498830815579805e-05 c 1.000109 k 0.049995583 m 5.000093\n",
      "86 Train Loss 0.008968157 Test RE 1.4398359238855282e-05 c 1.0001271 k 0.04999474 m 5.000799\n",
      "87 Train Loss 0.008658188 Test RE 1.4832639663494829e-05 c 1.0001725 k 0.049993232 m 4.999856\n",
      "88 Train Loss 0.008478996 Test RE 1.615253629895393e-05 c 1.0000607 k 0.04999638 m 4.9994407\n",
      "89 Train Loss 0.008203867 Test RE 1.5182683437874634e-05 c 0.99996525 k 0.049997542 m 4.9988227\n",
      "90 Train Loss 0.008112779 Test RE 1.5152505170643396e-05 c 0.999949 k 0.049999624 m 5.0003695\n",
      "91 Train Loss 0.007900948 Test RE 1.3927471869919482e-05 c 0.9998157 k 0.050001744 m 5.0007577\n",
      "92 Train Loss 0.007530879 Test RE 1.3562779447723116e-05 c 0.99987465 k 0.049998682 m 4.9987574\n",
      "93 Train Loss 0.0072903996 Test RE 1.3093932544527889e-05 c 0.99997956 k 0.049999386 m 5.000256\n",
      "94 Train Loss 0.007237351 Test RE 1.2939522347025889e-05 c 0.9999804 k 0.049999144 m 4.9999523\n",
      "95 Train Loss 0.0071846424 Test RE 1.2744401894993917e-05 c 1.000086 k 0.04999746 m 4.9994187\n",
      "96 Train Loss 0.0071037435 Test RE 1.3123441940306053e-05 c 1.0000577 k 0.049998906 m 4.999903\n",
      "97 Train Loss 0.0070686084 Test RE 1.3541074386887252e-05 c 0.99998415 k 0.049999855 m 5.000085\n",
      "98 Train Loss 0.0069008074 Test RE 1.317618515879997e-05 c 0.99996257 k 0.050000116 m 4.9998627\n",
      "99 Train Loss 0.006876947 Test RE 1.3007233999614667e-05 c 0.9999941 k 0.049999848 m 5.0000877\n",
      "100 Train Loss 0.0068727317 Test RE 1.3065553550645217e-05 c 1.0000018 k 0.049999803 m 5.000188\n",
      "101 Train Loss 0.006871923 Test RE 1.3094254366773677e-05 c 1.0000033 k 0.049999822 m 5.000206\n",
      "102 Train Loss 0.0068682255 Test RE 1.3198933039040853e-05 c 1.0000048 k 0.049999774 m 5.0002675\n",
      "103 Train Loss 0.0068395487 Test RE 1.3732394137391727e-05 c 1.0000207 k 0.049999457 m 5.0001755\n",
      "104 Train Loss 0.006811776 Test RE 1.4008666852620582e-05 c 1.0000135 k 0.049999986 m 4.9999347\n",
      "105 Train Loss 0.0068056732 Test RE 1.4085580164284598e-05 c 1.000001 k 0.05000023 m 4.999932\n",
      "106 Train Loss 0.0068045184 Test RE 1.4108439786701317e-05 c 0.99999714 k 0.050000273 m 4.999933\n",
      "107 Train Loss 0.006803528 Test RE 1.4119091082107986e-05 c 0.99999636 k 0.050000273 m 4.9999337\n",
      "108 Train Loss 0.006802671 Test RE 1.413675203925452e-05 c 0.9999971 k 0.050000187 m 4.999925\n",
      "109 Train Loss 0.006802051 Test RE 1.4151826012210199e-05 c 0.99999696 k 0.050000187 m 4.9999266\n",
      "110 Train Loss 0.0068011796 Test RE 1.4172265781203889e-05 c 0.99999714 k 0.050000176 m 4.9999275\n",
      "111 Train Loss 0.0068010613 Test RE 1.4199152534072801e-05 c 0.9999972 k 0.05000015 m 4.999926\n",
      "112 Train Loss 0.0067900005 Test RE 1.4358044583045382e-05 c 0.9999949 k 0.05000017 m 5.000014\n",
      "113 Train Loss 0.0067892554 Test RE 1.4379376511738885e-05 c 0.9999941 k 0.050000187 m 5.0000343\n",
      "114 Train Loss 0.006779116 Test RE 1.430125796437775e-05 c 0.99999475 k 0.050000384 m 5.000175\n",
      "115 Train Loss 0.006776146 Test RE 1.4213123135656815e-05 c 0.9999987 k 0.05000029 m 5.0001836\n",
      "116 Train Loss 0.0067757242 Test RE 1.4174608826978136e-05 c 0.99999994 k 0.050000247 m 5.000186\n",
      "117 Train Loss 0.00677488 Test RE 1.4143664936167178e-05 c 0.99999994 k 0.05000025 m 5.0001917\n",
      "118 Train Loss 0.0067743855 Test RE 1.4118553017983162e-05 c 1.0000001 k 0.050000235 m 5.0001926\n",
      "119 Train Loss 0.0067730024 Test RE 1.4066456849601862e-05 c 0.99999917 k 0.050000194 m 5.000189\n",
      "120 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "121 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "122 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "123 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "124 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "125 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "126 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "127 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "128 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "129 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "130 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "131 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "132 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "133 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "134 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "135 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "136 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "137 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "138 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "139 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "140 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "141 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "142 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "143 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "144 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "145 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "146 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "147 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "148 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "149 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "150 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "151 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "152 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "153 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "154 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "155 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "156 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "157 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "158 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "159 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "160 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "161 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "162 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "163 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "164 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "165 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "166 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "167 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "168 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "169 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "170 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "171 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "172 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "173 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "174 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "175 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "176 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "177 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "178 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "179 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "180 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "181 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "182 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "183 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "184 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "185 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "186 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "187 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "188 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "189 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "190 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "191 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "192 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "193 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "194 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "195 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "196 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "197 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "198 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "199 Train Loss 0.006772215 Test RE 1.4044770436570252e-05 c 0.99999875 k 0.050000176 m 5.0001884\n",
      "Training time: 37.57\n",
      "Training time: 37.57\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 131150.52 Test RE 0.2423474750126833 c 0.015910275 k 0.0719351 m 0.00039685649\n",
      "1 Train Loss 42008.465 Test RE 0.1328768930978019 c 0.028426437 k 0.06947218 m 0.0004616706\n",
      "2 Train Loss 21609.258 Test RE 0.08837922749615491 c 0.15216658 k 0.04512941 m 0.0016904166\n",
      "3 Train Loss 8933.787 Test RE 0.04331502337223524 c 0.23446952 k 0.060725406 m 0.0022359886\n",
      "4 Train Loss 5552.471 Test RE 0.02958191187602526 c 0.2748575 k 0.056905217 m 0.0020940548\n",
      "5 Train Loss 2351.0752 Test RE 0.01064411933648574 c 0.5142147 k 0.053229466 m 0.0043857507\n",
      "6 Train Loss 904.21686 Test RE 0.009856920273536358 c 0.97027254 k 0.047588315 m 0.014720174\n",
      "7 Train Loss 453.79922 Test RE 0.004847808788076158 c 1.1976427 k 0.043852024 m 0.03833379\n",
      "8 Train Loss 402.12543 Test RE 0.0053200905764268155 c 1.2507744 k 0.042552825 m 0.04907454\n",
      "9 Train Loss 326.21957 Test RE 0.005677155381026402 c 1.1267512 k 0.04399995 m 0.16371381\n",
      "10 Train Loss 316.93564 Test RE 0.006269218086127126 c 1.0924611 k 0.045255307 m 0.208153\n",
      "11 Train Loss 296.52142 Test RE 0.007291071175248986 c 1.1174104 k 0.044654272 m 0.3378061\n",
      "12 Train Loss 267.67654 Test RE 0.00721181702751888 c 1.1686758 k 0.04415852 m 0.51700294\n",
      "13 Train Loss 238.16324 Test RE 0.006426949658900479 c 1.1206467 k 0.04548282 m 0.77995515\n",
      "14 Train Loss 228.84421 Test RE 0.005624089000307921 c 1.1591426 k 0.044608254 m 0.7748486\n",
      "15 Train Loss 219.4693 Test RE 0.005657372664109407 c 1.1193204 k 0.045238275 m 0.7375747\n",
      "16 Train Loss 214.85767 Test RE 0.005321211706358441 c 1.0943315 k 0.045692876 m 0.7760925\n",
      "17 Train Loss 212.77232 Test RE 0.004953450755074128 c 1.1141111 k 0.045373544 m 0.79135984\n",
      "18 Train Loss 210.85117 Test RE 0.0050059462159663 c 1.1229601 k 0.045220576 m 0.81115264\n",
      "19 Train Loss 206.87036 Test RE 0.004953525481292527 c 1.0920309 k 0.04582071 m 0.92698544\n",
      "20 Train Loss 194.90477 Test RE 0.004158525974215402 c 1.1201777 k 0.0454544 m 1.1554157\n",
      "21 Train Loss 174.34161 Test RE 0.003636669267364855 c 1.0809431 k 0.046312064 m 1.3189656\n",
      "22 Train Loss 165.516 Test RE 0.0034330515103241756 c 1.0937667 k 0.046088316 m 1.4447564\n",
      "23 Train Loss 141.26816 Test RE 0.0026904913832664448 c 1.0795028 k 0.046759874 m 1.6667166\n",
      "24 Train Loss 128.72409 Test RE 0.002628344685397512 c 1.041418 k 0.04716041 m 1.8067858\n",
      "25 Train Loss 104.74421 Test RE 0.00296877933100986 c 1.0625006 k 0.04737519 m 2.1517637\n",
      "26 Train Loss 74.87087 Test RE 0.0028437051786626274 c 1.0626984 k 0.047717165 m 2.7187278\n",
      "27 Train Loss 67.239845 Test RE 0.0028683012770567076 c 1.0227767 k 0.048507538 m 3.0229666\n",
      "28 Train Loss 63.38626 Test RE 0.00312959908095289 c 1.028689 k 0.04854553 m 3.2461975\n",
      "29 Train Loss 61.17369 Test RE 0.0030461853127884742 c 1.0375874 k 0.04847122 m 3.2521107\n",
      "30 Train Loss 58.777065 Test RE 0.0026530759832486496 c 1.0298339 k 0.04849781 m 3.181481\n",
      "31 Train Loss 50.231102 Test RE 0.0022995566031249076 c 1.0513604 k 0.04826504 m 3.2247944\n",
      "32 Train Loss 32.77607 Test RE 0.0014806557932268672 c 1.0174787 k 0.04861899 m 3.4528916\n",
      "33 Train Loss 23.976576 Test RE 0.0011023768229576506 c 1.040818 k 0.048423465 m 3.6612055\n",
      "34 Train Loss 21.93281 Test RE 0.001299138112896558 c 1.0289781 k 0.04853928 m 3.7864366\n",
      "35 Train Loss 18.419563 Test RE 0.001258097351238195 c 1.0233474 k 0.048904605 m 4.03449\n",
      "36 Train Loss 12.795895 Test RE 0.0013410727517316294 c 1.0157201 k 0.049176887 m 4.344112\n",
      "37 Train Loss 11.46146 Test RE 0.0012586518710127417 c 1.0177345 k 0.04908642 m 4.2644534\n",
      "38 Train Loss 10.180458 Test RE 0.0011274894811219651 c 1.0181336 k 0.04910695 m 4.2556667\n",
      "39 Train Loss 8.545135 Test RE 0.0011361385844505668 c 1.0154995 k 0.049171776 m 4.3358665\n",
      "40 Train Loss 6.25964 Test RE 0.0008071990593938134 c 1.015691 k 0.0492056 m 4.358287\n",
      "41 Train Loss 5.4281063 Test RE 0.0006357535100278193 c 1.0162826 k 0.049314097 m 4.3939047\n",
      "42 Train Loss 3.4910812 Test RE 0.000679936459100953 c 1.0119478 k 0.04948823 m 4.5850954\n",
      "43 Train Loss 2.6139805 Test RE 0.0006374640286875177 c 1.0047392 k 0.049638595 m 4.7169566\n",
      "44 Train Loss 1.8162346 Test RE 0.0004320652962570743 c 1.0048337 k 0.04976866 m 4.8409934\n",
      "45 Train Loss 1.3790783 Test RE 0.00037451239325995484 c 1.0016624 k 0.04993209 m 4.9703345\n",
      "46 Train Loss 1.1625139 Test RE 0.0003581985183868673 c 1.0008458 k 0.049948413 m 5.0018315\n",
      "47 Train Loss 1.1118519 Test RE 0.0003711828527289856 c 0.99937314 k 0.04996691 m 5.01455\n",
      "48 Train Loss 1.0576973 Test RE 0.00036036601792016404 c 1.0003499 k 0.049963873 m 5.0166297\n",
      "49 Train Loss 0.99853134 Test RE 0.0003780134295861421 c 1.0019493 k 0.049917083 m 5.0171013\n",
      "50 Train Loss 0.80978274 Test RE 0.00034217081625392723 c 1.0018063 k 0.049932282 m 5.017299\n",
      "51 Train Loss 0.7109703 Test RE 0.00031565282537084495 c 0.9988815 k 0.04998542 m 5.027764\n",
      "52 Train Loss 0.6805842 Test RE 0.0002879355639502059 c 1.0011922 k 0.049951706 m 5.0276747\n",
      "53 Train Loss 0.65790284 Test RE 0.00027283236491030773 c 1.00207 k 0.04991998 m 5.0063224\n",
      "54 Train Loss 0.6463483 Test RE 0.00027684018002364404 c 1.0018144 k 0.049907736 m 4.9838657\n",
      "55 Train Loss 0.6383081 Test RE 0.0002712750535252959 c 1.0018561 k 0.04989546 m 4.9686\n",
      "56 Train Loss 0.61461973 Test RE 0.00026830040729218453 c 1.0022168 k 0.049886163 m 4.9551663\n",
      "57 Train Loss 0.60799783 Test RE 0.0002750408247673707 c 1.0018737 k 0.049892854 m 4.963405\n",
      "58 Train Loss 0.5623181 Test RE 0.0002764050148304404 c 1.0011666 k 0.049908448 m 4.9880414\n",
      "59 Train Loss 0.4811791 Test RE 0.00025273670182170604 c 1.0010267 k 0.04994595 m 5.011214\n",
      "60 Train Loss 0.35643667 Test RE 0.00021674232167227477 c 1.0021842 k 0.0499085 m 4.9806457\n",
      "61 Train Loss 0.30610475 Test RE 0.00021139466765108958 c 1.00172 k 0.049902532 m 4.975952\n",
      "62 Train Loss 0.28592736 Test RE 0.00019822322700593028 c 1.0017428 k 0.049903836 m 4.9839816\n",
      "63 Train Loss 0.28511104 Test RE 0.00019879090080995415 c 1.0017468 k 0.049905237 m 4.987515\n",
      "64 Train Loss 0.28015068 Test RE 0.0001984915198838463 c 1.0017736 k 0.049913026 m 4.995147\n",
      "65 Train Loss 0.23656954 Test RE 0.00019241518746149978 c 1.0012902 k 0.049924426 m 4.9929466\n",
      "66 Train Loss 0.22805238 Test RE 0.00018953660660383343 c 1.0012115 k 0.049927846 m 4.9919467\n",
      "67 Train Loss 0.22630632 Test RE 0.00018766088155877946 c 1.00148 k 0.04992513 m 4.9941125\n",
      "68 Train Loss 0.22322726 Test RE 0.00018737780527719867 c 1.0010934 k 0.049931034 m 4.9961357\n",
      "69 Train Loss 0.21041262 Test RE 0.000178331220944818 c 1.0006754 k 0.0499466 m 4.99848\n",
      "70 Train Loss 0.20253325 Test RE 0.0001671866748565743 c 1.001064 k 0.049937323 m 4.9979854\n",
      "71 Train Loss 0.19763802 Test RE 0.00016321207066316275 c 1.0016271 k 0.049914084 m 4.982531\n",
      "72 Train Loss 0.18957824 Test RE 0.00016382695703506204 c 1.0015314 k 0.049925264 m 4.985487\n",
      "73 Train Loss 0.18087086 Test RE 0.00016106737712155152 c 1.0003877 k 0.0499453 m 4.9978404\n",
      "74 Train Loss 0.17244205 Test RE 0.00015589359630208607 c 1.00145 k 0.04992715 m 4.989604\n",
      "75 Train Loss 0.16277239 Test RE 0.00014543347540843814 c 1.0019907 k 0.049917053 m 4.981986\n",
      "76 Train Loss 0.15764676 Test RE 0.00014074009641267687 c 1.0009915 k 0.049936228 m 4.9933667\n",
      "77 Train Loss 0.15332453 Test RE 0.00013274242936481682 c 1.0014352 k 0.049931083 m 4.990017\n",
      "78 Train Loss 0.14794803 Test RE 0.0001257951317523661 c 1.0013827 k 0.04992873 m 4.9859242\n",
      "79 Train Loss 0.1439452 Test RE 0.00011679747605838112 c 1.0007614 k 0.04994146 m 4.987985\n",
      "80 Train Loss 0.13920137 Test RE 0.00010806146451992686 c 1.0011337 k 0.049932227 m 4.984019\n",
      "81 Train Loss 0.13584192 Test RE 0.00010510206971811398 c 1.0008945 k 0.049943116 m 4.988628\n",
      "82 Train Loss 0.13259041 Test RE 0.00010441689018521306 c 1.0009727 k 0.04994419 m 4.990423\n",
      "83 Train Loss 0.13143228 Test RE 0.00010585076324891859 c 1.0011568 k 0.049941126 m 4.9890013\n",
      "84 Train Loss 0.13011788 Test RE 0.00010610307462212182 c 1.0009825 k 0.049945988 m 4.991438\n",
      "85 Train Loss 0.122891255 Test RE 0.00010171979298958606 c 1.0007552 k 0.0499478 m 4.9960594\n",
      "86 Train Loss 0.105915494 Test RE 9.249769944467487e-05 c 1.0004388 k 0.0499551 m 4.9899917\n",
      "87 Train Loss 0.084973454 Test RE 8.948863466744476e-05 c 1.0009599 k 0.04994546 m 4.9880724\n",
      "88 Train Loss 0.062374506 Test RE 7.244397088897258e-05 c 1.0009545 k 0.049963284 m 4.991383\n",
      "89 Train Loss 0.055273198 Test RE 6.92850025149144e-05 c 1.0009392 k 0.04995811 m 4.9928317\n",
      "90 Train Loss 0.052367136 Test RE 6.132520016551192e-05 c 1.0006344 k 0.049960237 m 4.987885\n",
      "91 Train Loss 0.047664635 Test RE 5.2129765723083913e-05 c 1.0013522 k 0.049950436 m 4.983929\n",
      "92 Train Loss 0.041804295 Test RE 4.9524611727876425e-05 c 1.000944 k 0.049964998 m 4.9914713\n",
      "93 Train Loss 0.039683893 Test RE 4.465339236994668e-05 c 1.0002592 k 0.049974892 m 4.9951754\n",
      "94 Train Loss 0.037091695 Test RE 4.072748406534831e-05 c 1.0006043 k 0.049971886 m 4.99193\n",
      "95 Train Loss 0.033844043 Test RE 3.362181628867855e-05 c 1.0003369 k 0.049974743 m 4.9928145\n",
      "96 Train Loss 0.028817724 Test RE 2.8152235457072823e-05 c 0.9998364 k 0.049984466 m 4.99727\n",
      "97 Train Loss 0.023873296 Test RE 2.9029738077548253e-05 c 1.000115 k 0.04998873 m 4.999873\n",
      "98 Train Loss 0.02279225 Test RE 3.2615617708768994e-05 c 1.0003369 k 0.04998369 m 4.997659\n",
      "99 Train Loss 0.02244839 Test RE 3.110003983977395e-05 c 1.0003579 k 0.049982373 m 4.9980025\n",
      "100 Train Loss 0.022250766 Test RE 3.230371556121182e-05 c 1.0003211 k 0.049985293 m 4.9993443\n",
      "101 Train Loss 0.021869479 Test RE 3.286813376280145e-05 c 1.0003849 k 0.049983043 m 4.99868\n",
      "102 Train Loss 0.02125429 Test RE 3.090020057792013e-05 c 1.0003234 k 0.049982585 m 4.995631\n",
      "103 Train Loss 0.020500239 Test RE 3.13331544009855e-05 c 1.0001999 k 0.049986765 m 4.997525\n",
      "104 Train Loss 0.02035378 Test RE 3.2650350424934615e-05 c 1.0002732 k 0.049986623 m 4.9984236\n",
      "105 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "106 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "107 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "108 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "109 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "110 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "111 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "112 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "113 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "114 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "115 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "116 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "117 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "118 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "119 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "120 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "121 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "122 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "123 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "124 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "125 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "126 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "127 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "128 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "129 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "130 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "131 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "132 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "133 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "134 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "135 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "136 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "137 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "138 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "139 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "140 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "141 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "142 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "143 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "144 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "145 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "146 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "147 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "148 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "149 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "150 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "151 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "152 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "153 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "154 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "155 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "156 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "157 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "158 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "159 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "160 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "161 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "162 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "163 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "164 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "165 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "166 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "167 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "168 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "169 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "170 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "171 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "172 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "173 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "174 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "175 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "176 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "177 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "178 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "179 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "180 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "181 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "182 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "183 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "184 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "185 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "186 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "187 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "188 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "189 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "190 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "191 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "192 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "193 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "194 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "195 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "196 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "197 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "198 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "199 Train Loss 0.020345107 Test RE 3.277458385071174e-05 c 1.0002884 k 0.049986407 m 4.9983325\n",
      "Training time: 36.97\n",
      "Training time: 36.97\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 213434.9 Test RE 0.3097151245919012 c 0.01134495 k 0.055003516 m -0.00033189112\n",
      "1 Train Loss 42181.81 Test RE 0.13481885536230837 c 0.01777682 k 0.06210015 m -0.00044340864\n",
      "2 Train Loss 32484.559 Test RE 0.11752676516980731 c 0.021554077 k 0.061504345 m -0.00031784084\n",
      "3 Train Loss 27776.033 Test RE 0.1081904121820484 c 0.042298615 k 0.062277257 m 0.00064296875\n",
      "4 Train Loss 2760.5364 Test RE 0.023461281555962447 c 0.15095475 k 0.057655375 m 0.0061748377\n",
      "5 Train Loss 2267.2432 Test RE 0.02125787018387418 c 0.26408774 k 0.057136428 m 0.014214012\n",
      "6 Train Loss 1092.1014 Test RE 0.017487295853233997 c 1.1974335 k 0.043349825 m 0.085176826\n",
      "7 Train Loss 1039.4358 Test RE 0.017395547898342436 c 1.2222409 k 0.042563155 m 0.08848853\n",
      "8 Train Loss 849.103 Test RE 0.01656768704773138 c 1.0960892 k 0.044639822 m 0.1080265\n",
      "9 Train Loss 634.2721 Test RE 0.014016109159479992 c 0.96113807 k 0.046930898 m 0.139583\n",
      "10 Train Loss 571.73145 Test RE 0.012859080044356803 c 0.9188612 k 0.04747326 m 0.15454578\n",
      "11 Train Loss 465.772 Test RE 0.011265562298118014 c 1.1213101 k 0.04414791 m 0.18066233\n",
      "12 Train Loss 401.5194 Test RE 0.00980542235484722 c 1.1004266 k 0.04493087 m 0.18351819\n",
      "13 Train Loss 397.18176 Test RE 0.0097679269489436 c 1.1031868 k 0.04510654 m 0.18605475\n",
      "14 Train Loss 394.64233 Test RE 0.00978856683861555 c 1.1103593 k 0.044774067 m 0.19220336\n",
      "15 Train Loss 385.00528 Test RE 0.009983816166280617 c 1.1035523 k 0.044710357 m 0.21948795\n",
      "16 Train Loss 354.41232 Test RE 0.010105690217831876 c 1.1121165 k 0.044986106 m 0.2666064\n",
      "17 Train Loss 344.10214 Test RE 0.009836385191471184 c 1.1471708 k 0.04411945 m 0.2765279\n",
      "18 Train Loss 298.39746 Test RE 0.008211857524276403 c 1.1130043 k 0.044954628 m 0.2654071\n",
      "19 Train Loss 295.62173 Test RE 0.008166152201826867 c 1.1304374 k 0.044602256 m 0.27061856\n",
      "20 Train Loss 293.69263 Test RE 0.008103728113265114 c 1.1288297 k 0.044564046 m 0.3024191\n",
      "21 Train Loss 291.066 Test RE 0.008041265972532893 c 1.1279397 k 0.04459269 m 0.33771867\n",
      "22 Train Loss 287.2306 Test RE 0.007847436449841486 c 1.1491781 k 0.044427488 m 0.39229363\n",
      "23 Train Loss 282.00485 Test RE 0.007555751009198131 c 1.1490349 k 0.044478662 m 0.42894772\n",
      "24 Train Loss 272.83954 Test RE 0.007205957963038831 c 1.1124938 k 0.044910792 m 0.45098966\n",
      "25 Train Loss 267.81802 Test RE 0.006956651192345286 c 1.1291705 k 0.044743113 m 0.46006086\n",
      "26 Train Loss 265.443 Test RE 0.006748078834052571 c 1.1218894 k 0.04469466 m 0.49155334\n",
      "27 Train Loss 262.50888 Test RE 0.006768031486952516 c 1.1224236 k 0.044875793 m 0.5051527\n",
      "28 Train Loss 261.10355 Test RE 0.006776656375089769 c 1.1353049 k 0.044681255 m 0.5221654\n",
      "29 Train Loss 258.5033 Test RE 0.006703825260750626 c 1.1208156 k 0.04490282 m 0.55158854\n",
      "30 Train Loss 246.39148 Test RE 0.0063717756429329175 c 1.0809846 k 0.045798205 m 0.72941506\n",
      "31 Train Loss 217.2318 Test RE 0.005250326423168047 c 1.1196812 k 0.045234796 m 0.9268086\n",
      "32 Train Loss 201.3317 Test RE 0.004392414382072325 c 1.1024619 k 0.04563312 m 1.0706147\n",
      "33 Train Loss 192.54887 Test RE 0.0034709109147149806 c 1.1011437 k 0.045869485 m 1.3208756\n",
      "34 Train Loss 182.76617 Test RE 0.003403379698060708 c 1.0947114 k 0.04618582 m 1.5776943\n",
      "35 Train Loss 179.13075 Test RE 0.003612049464047956 c 1.075451 k 0.046653476 m 1.6972958\n",
      "36 Train Loss 177.19778 Test RE 0.003490213534195463 c 1.0923157 k 0.04628894 m 1.661911\n",
      "37 Train Loss 174.44086 Test RE 0.0035793090806320745 c 1.0907831 k 0.046194453 m 1.5937259\n",
      "38 Train Loss 172.105 Test RE 0.0033256886575735146 c 1.0862834 k 0.046387695 m 1.6865733\n",
      "39 Train Loss 171.01755 Test RE 0.0033165539275407728 c 1.0898895 k 0.04635051 m 1.6929008\n",
      "40 Train Loss 169.78778 Test RE 0.0033976479845941494 c 1.086269 k 0.046390418 m 1.6766914\n",
      "41 Train Loss 169.2186 Test RE 0.003355378613451758 c 1.0860361 k 0.04642664 m 1.7296684\n",
      "42 Train Loss 162.05652 Test RE 0.003445920440983268 c 1.0838948 k 0.046620674 m 1.9571848\n",
      "43 Train Loss 153.69756 Test RE 0.0032162095084245818 c 1.0739383 k 0.046875555 m 2.082187\n",
      "44 Train Loss 144.36499 Test RE 0.003446464481165076 c 1.0909185 k 0.046591822 m 2.0864074\n",
      "45 Train Loss 141.23761 Test RE 0.0033856316022198856 c 1.0606742 k 0.04714277 m 2.1133153\n",
      "46 Train Loss 135.67508 Test RE 0.003546141084855471 c 1.070618 k 0.04698257 m 2.2025585\n",
      "47 Train Loss 129.3598 Test RE 0.002967473504792901 c 1.0613934 k 0.047125883 m 2.2726302\n",
      "48 Train Loss 118.94301 Test RE 0.003299588992143149 c 1.0587825 k 0.04738082 m 2.4817712\n",
      "49 Train Loss 114.30418 Test RE 0.0034937569542876543 c 1.0843915 k 0.046932854 m 2.4907353\n",
      "50 Train Loss 107.96424 Test RE 0.00322405933653728 c 1.0578032 k 0.04742505 m 2.510454\n",
      "51 Train Loss 95.512405 Test RE 0.003006971635608633 c 1.0724835 k 0.047381464 m 2.8147802\n",
      "52 Train Loss 80.14199 Test RE 0.003182332839671268 c 1.0562183 k 0.047830828 m 2.9764984\n",
      "53 Train Loss 70.85571 Test RE 0.0030656032661302286 c 1.0475118 k 0.04806353 m 3.1797388\n",
      "54 Train Loss 67.323074 Test RE 0.0029535639032779813 c 1.0686836 k 0.04793466 m 3.267929\n",
      "55 Train Loss 58.95375 Test RE 0.0026764920234603412 c 1.0258596 k 0.048556134 m 3.3992403\n",
      "56 Train Loss 55.97711 Test RE 0.0026107035267204162 c 1.0424467 k 0.048375163 m 3.3907797\n",
      "57 Train Loss 54.712475 Test RE 0.0025764625152313115 c 1.0375403 k 0.048419107 m 3.3794835\n",
      "58 Train Loss 44.82655 Test RE 0.0025109550473740153 c 1.0226334 k 0.048534736 m 3.4895144\n",
      "59 Train Loss 40.487537 Test RE 0.0023101177752314628 c 1.0546752 k 0.048103727 m 3.524652\n",
      "60 Train Loss 36.930656 Test RE 0.001975438481474567 c 1.0329803 k 0.048593163 m 3.5662267\n",
      "61 Train Loss 34.986534 Test RE 0.0020395041563391635 c 1.0304339 k 0.048850026 m 3.6117299\n",
      "62 Train Loss 32.38219 Test RE 0.002081971635270457 c 1.0308226 k 0.048792273 m 3.7428248\n",
      "63 Train Loss 31.12627 Test RE 0.0019024193859208374 c 1.023314 k 0.048981648 m 3.8104973\n",
      "64 Train Loss 30.36818 Test RE 0.0018272264063766961 c 1.0283579 k 0.04879864 m 3.8082314\n",
      "65 Train Loss 29.976036 Test RE 0.0016895508623344065 c 1.0299278 k 0.048811387 m 3.7987943\n",
      "66 Train Loss 29.393684 Test RE 0.0016253769811912808 c 1.0283588 k 0.048780166 m 3.773188\n",
      "67 Train Loss 27.42372 Test RE 0.001630903999391626 c 1.02549 k 0.048730325 m 3.8269355\n",
      "68 Train Loss 23.860336 Test RE 0.0013432103352979418 c 1.0272872 k 0.04904793 m 3.982725\n",
      "69 Train Loss 21.2088 Test RE 0.0011592935251572224 c 1.021251 k 0.049012963 m 4.064587\n",
      "70 Train Loss 20.22445 Test RE 0.0009317371699755851 c 1.0213737 k 0.049136452 m 4.196045\n",
      "71 Train Loss 19.801603 Test RE 0.0009364823977890534 c 1.0221729 k 0.049160007 m 4.315245\n",
      "72 Train Loss 19.329823 Test RE 0.0011478250459249783 c 1.0165586 k 0.049231663 m 4.4331985\n",
      "73 Train Loss 13.361797 Test RE 0.0013087716058271816 c 1.020699 k 0.04960306 m 4.5484104\n",
      "74 Train Loss 7.815299 Test RE 0.0006913591690387802 c 1.0129185 k 0.049485054 m 4.5693846\n",
      "75 Train Loss 6.466958 Test RE 0.0006536689982419595 c 1.009072 k 0.049597707 m 4.654551\n",
      "76 Train Loss 6.075921 Test RE 0.0006195561686676131 c 1.0109048 k 0.049560882 m 4.7003813\n",
      "77 Train Loss 5.7124996 Test RE 0.0005321625020130494 c 1.004306 k 0.049761605 m 4.756698\n",
      "78 Train Loss 4.729474 Test RE 0.0005078321990316358 c 1.0065386 k 0.04973206 m 4.8138356\n",
      "79 Train Loss 4.4571075 Test RE 0.0004668314580784063 c 1.0054759 k 0.049801018 m 4.837285\n",
      "80 Train Loss 4.390865 Test RE 0.00047415920798836095 c 1.0041441 k 0.0498369 m 4.85214\n",
      "81 Train Loss 4.266233 Test RE 0.000468370817426202 c 1.0050851 k 0.049827952 m 4.8733177\n",
      "82 Train Loss 4.0584455 Test RE 0.00043200339030562243 c 1.0034927 k 0.049861543 m 4.9027333\n",
      "83 Train Loss 3.808333 Test RE 0.0004789448055409093 c 1.0021867 k 0.049907908 m 4.9250607\n",
      "84 Train Loss 3.665893 Test RE 0.0004931714603508346 c 0.99961144 k 0.049996823 m 4.9681354\n",
      "85 Train Loss 3.4110599 Test RE 0.0004822351290798447 c 1.0010836 k 0.049992 m 5.014949\n",
      "86 Train Loss 3.2080846 Test RE 0.0004891105476816479 c 1.0004344 k 0.050066363 m 5.0444417\n",
      "87 Train Loss 3.1166246 Test RE 0.0005019070143755047 c 0.99922156 k 0.050074965 m 5.0464025\n",
      "88 Train Loss 3.018231 Test RE 0.0005246522717832355 c 0.9996502 k 0.05002855 m 4.985934\n",
      "89 Train Loss 2.9571044 Test RE 0.00045579201960419734 c 1.0014995 k 0.049984552 m 4.973959\n",
      "90 Train Loss 2.9098134 Test RE 0.00043110869891797957 c 1.0011932 k 0.04998133 m 4.9757032\n",
      "91 Train Loss 2.880125 Test RE 0.0004567478875356415 c 1.0009642 k 0.04998303 m 4.9795866\n",
      "92 Train Loss 2.7562983 Test RE 0.0004627083413777107 c 0.9990065 k 0.050081484 m 5.0568\n",
      "93 Train Loss 2.7275665 Test RE 0.0004397875775596482 c 0.9989287 k 0.050092008 m 5.076981\n",
      "94 Train Loss 2.7088778 Test RE 0.0004322434222079067 c 0.9987644 k 0.050100435 m 5.082051\n",
      "95 Train Loss 2.642259 Test RE 0.0004305714601626817 c 0.998333 k 0.05008213 m 5.084953\n",
      "96 Train Loss 2.4889565 Test RE 0.00037542394842422083 c 0.99920636 k 0.0500905 m 5.078484\n",
      "97 Train Loss 2.3986843 Test RE 0.0003847400238941297 c 0.99890995 k 0.050059624 m 5.044037\n",
      "98 Train Loss 2.228393 Test RE 0.0003584444089310518 c 0.9965328 k 0.05010609 m 5.054947\n",
      "99 Train Loss 1.9940195 Test RE 0.00034239020072750654 c 0.9989028 k 0.050155096 m 5.125688\n",
      "100 Train Loss 1.7551082 Test RE 0.00040580966591800057 c 0.998314 k 0.050134264 m 5.153278\n",
      "101 Train Loss 1.6473594 Test RE 0.00036556385516555443 c 0.9962373 k 0.05022165 m 5.1520605\n",
      "102 Train Loss 1.590265 Test RE 0.00032111197185027035 c 0.99594545 k 0.05020892 m 5.1657357\n",
      "103 Train Loss 1.4914947 Test RE 0.00033293830961655393 c 0.9979865 k 0.05014457 m 5.1498156\n",
      "104 Train Loss 1.4105623 Test RE 0.00034683168257583974 c 0.99711126 k 0.05014729 m 5.106749\n",
      "105 Train Loss 1.2767943 Test RE 0.00031467851522244987 c 0.99999356 k 0.050046463 m 5.044063\n",
      "106 Train Loss 1.1519495 Test RE 0.00030902854245047374 c 1.0023232 k 0.050007526 m 5.021026\n",
      "107 Train Loss 1.026899 Test RE 0.00033000252275949403 c 0.9984226 k 0.05007569 m 5.029088\n",
      "108 Train Loss 0.9801638 Test RE 0.0003048183979161146 c 0.9993093 k 0.050080534 m 5.0429068\n",
      "109 Train Loss 0.79211104 Test RE 0.00027203478921601886 c 0.99950373 k 0.05003438 m 5.0296397\n",
      "110 Train Loss 0.6657989 Test RE 0.0002742963970411109 c 0.99859494 k 0.050064605 m 5.0080233\n",
      "111 Train Loss 0.61473733 Test RE 0.0002586516957184895 c 1.000886 k 0.05003351 m 5.018018\n",
      "112 Train Loss 0.53158265 Test RE 0.00023452277575660826 c 0.9995635 k 0.050059706 m 5.0557537\n",
      "113 Train Loss 0.44022366 Test RE 0.00023735907392378737 c 0.997815 k 0.05010913 m 5.078209\n",
      "114 Train Loss 0.40310887 Test RE 0.00023003599407352002 c 0.9992791 k 0.050075192 m 5.0650306\n",
      "115 Train Loss 0.3615214 Test RE 0.00020117443556467158 c 0.9992718 k 0.05005302 m 5.040535\n",
      "116 Train Loss 0.35078645 Test RE 0.00019429416107065092 c 0.998878 k 0.050063603 m 5.047831\n",
      "117 Train Loss 0.30519557 Test RE 0.00018863529983974403 c 0.99877167 k 0.050058033 m 5.02688\n",
      "118 Train Loss 0.26869965 Test RE 0.00018086097244242178 c 1.0003635 k 0.050006222 m 5.0032096\n",
      "119 Train Loss 0.25588113 Test RE 0.00016335047510094078 c 0.99916434 k 0.050021987 m 4.994704\n",
      "120 Train Loss 0.20492402 Test RE 0.0001076423699608588 c 0.9991267 k 0.050018627 m 5.0056877\n",
      "121 Train Loss 0.1769421 Test RE 0.00010477952993656221 c 1.0007606 k 0.049982425 m 4.9973874\n",
      "122 Train Loss 0.17355335 Test RE 0.00010194122502354168 c 1.0002499 k 0.049987048 m 4.9907207\n",
      "123 Train Loss 0.17110121 Test RE 9.545392102086074e-05 c 1.000656 k 0.04997363 m 4.9847927\n",
      "124 Train Loss 0.16322692 Test RE 9.226776124292472e-05 c 1.0008576 k 0.04996884 m 4.9883056\n",
      "125 Train Loss 0.12951648 Test RE 9.397511299353156e-05 c 0.9999376 k 0.04998604 m 4.992702\n",
      "126 Train Loss 0.12302358 Test RE 9.278941834659766e-05 c 1.0004599 k 0.04998345 m 4.9906645\n",
      "127 Train Loss 0.12117945 Test RE 9.315194277392887e-05 c 1.0001489 k 0.049992684 m 4.9995723\n",
      "128 Train Loss 0.12017805 Test RE 9.367674825918522e-05 c 0.9999969 k 0.049988944 m 5.003025\n",
      "129 Train Loss 0.1161459 Test RE 9.680072655393691e-05 c 1.0005462 k 0.049980722 m 4.992981\n",
      "130 Train Loss 0.11377995 Test RE 9.861248878534974e-05 c 1.0003643 k 0.049986746 m 4.9917946\n",
      "131 Train Loss 0.112409286 Test RE 0.00010029501468347433 c 1.0002394 k 0.04998648 m 4.9957027\n",
      "132 Train Loss 0.11125456 Test RE 0.00010202547377371957 c 1.0003188 k 0.049987096 m 4.9920735\n",
      "133 Train Loss 0.110520355 Test RE 0.0001046840479068081 c 1.0002791 k 0.049986385 m 4.993657\n",
      "134 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "135 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "136 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "137 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "138 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "139 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "140 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "141 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "142 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "143 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "144 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "145 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "146 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "147 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "148 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "149 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "150 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "151 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "152 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "153 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "154 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "155 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "156 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "157 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "158 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "159 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "160 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "161 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "162 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "163 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "164 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "165 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "166 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "167 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "168 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "169 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "170 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "171 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "172 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "173 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "174 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "175 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "176 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "177 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "178 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "179 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "180 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "181 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "182 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "183 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "184 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "185 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "186 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "187 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "188 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "189 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "190 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "191 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "192 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "193 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "194 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "195 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "196 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "197 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "198 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "199 Train Loss 0.110311404 Test RE 0.00010487181600523129 c 1.0002847 k 0.04998673 m 4.994618\n",
      "Training time: 45.59\n",
      "Training time: 45.59\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 63673.1 Test RE 0.16492142639939103 c 0.011315656 k 0.04008015 m -4.9009923e-05\n",
      "1 Train Loss 39304.88 Test RE 0.1299556644578571 c 0.017923959 k 0.065121345 m 0.00013642258\n",
      "2 Train Loss 5079.2876 Test RE 0.040265126072253486 c 0.24632458 k 0.05670948 m 0.01182209\n",
      "3 Train Loss 1504.802 Test RE 0.01282343632112779 c 0.31314582 k 0.05534117 m 0.019505681\n",
      "4 Train Loss 1010.47736 Test RE 0.009517935410775577 c 0.4972692 k 0.05443921 m 0.04159709\n",
      "5 Train Loss 620.06177 Test RE 0.008780521810029696 c 0.75568986 k 0.04909223 m 0.07265679\n",
      "6 Train Loss 481.77997 Test RE 0.008868127254009988 c 0.893277 k 0.04920042 m 0.09387696\n",
      "7 Train Loss 315.0133 Test RE 0.007280570137047823 c 1.1763265 k 0.044195116 m 0.13693091\n",
      "8 Train Loss 307.90082 Test RE 0.00675314443532591 c 1.1697417 k 0.043696456 m 0.13858522\n",
      "9 Train Loss 299.80814 Test RE 0.006814355388316428 c 1.1344995 k 0.044082146 m 0.14864196\n",
      "10 Train Loss 296.72412 Test RE 0.007065535306379403 c 1.144206 k 0.044319954 m 0.15774588\n",
      "11 Train Loss 290.99774 Test RE 0.0069622720619335245 c 1.1478087 k 0.044167157 m 0.18223874\n",
      "12 Train Loss 281.16504 Test RE 0.006705918090540638 c 1.1303242 k 0.04405624 m 0.2357369\n",
      "13 Train Loss 269.8528 Test RE 0.006711788013269626 c 1.0937488 k 0.045153838 m 0.27794135\n",
      "14 Train Loss 248.52557 Test RE 0.005579040104856577 c 1.105408 k 0.044947363 m 0.40293097\n",
      "15 Train Loss 234.83888 Test RE 0.004584709279222835 c 1.1512086 k 0.044580378 m 0.49965602\n",
      "16 Train Loss 222.95273 Test RE 0.004057119268872063 c 1.1298398 k 0.045040525 m 0.61938757\n",
      "17 Train Loss 217.77834 Test RE 0.00378467098882875 c 1.1033655 k 0.04531906 m 0.70531726\n",
      "18 Train Loss 210.42366 Test RE 0.003853687872848399 c 1.1057757 k 0.04544704 m 0.7251883\n",
      "19 Train Loss 209.26813 Test RE 0.003954516076475474 c 1.1152984 k 0.04536921 m 0.72489876\n",
      "20 Train Loss 207.14105 Test RE 0.0040050594426583866 c 1.1116729 k 0.045423187 m 0.7710096\n",
      "21 Train Loss 205.48787 Test RE 0.003922898307348245 c 1.1106728 k 0.045457464 m 0.8211636\n",
      "22 Train Loss 204.42499 Test RE 0.003960749767358007 c 1.1135107 k 0.045475665 m 0.83020335\n",
      "23 Train Loss 199.8692 Test RE 0.003626442292878762 c 1.1030247 k 0.04567315 m 0.90002084\n",
      "24 Train Loss 188.75977 Test RE 0.003537102107341299 c 1.0783795 k 0.045973532 m 1.1451896\n",
      "25 Train Loss 157.6182 Test RE 0.0037234250866257828 c 1.1093609 k 0.045896817 m 1.5365313\n",
      "26 Train Loss 139.22212 Test RE 0.003716748758862759 c 1.080623 k 0.04703906 m 1.7739512\n",
      "27 Train Loss 120.234955 Test RE 0.0029748784349397615 c 1.0461951 k 0.04727706 m 1.9630239\n",
      "28 Train Loss 112.82641 Test RE 0.002528053623281842 c 1.086862 k 0.046736937 m 2.054666\n",
      "29 Train Loss 106.7466 Test RE 0.002962386227030966 c 1.0669801 k 0.047108695 m 2.1508603\n",
      "30 Train Loss 100.135124 Test RE 0.003121212753948322 c 1.0699692 k 0.047145348 m 2.287\n",
      "31 Train Loss 78.77666 Test RE 0.003061913430098766 c 1.06452 k 0.047572643 m 2.6944506\n",
      "32 Train Loss 61.7181 Test RE 0.0019674047141245276 c 1.0341098 k 0.04783914 m 2.9422808\n",
      "33 Train Loss 54.033775 Test RE 0.0020104597711664614 c 1.0659028 k 0.048038665 m 3.0622752\n",
      "34 Train Loss 46.01389 Test RE 0.0015861519898202948 c 1.0169166 k 0.048450544 m 3.2853734\n",
      "35 Train Loss 38.999123 Test RE 0.0016929370900945705 c 1.0396134 k 0.04849104 m 3.5591533\n",
      "36 Train Loss 30.056084 Test RE 0.0012928580931540312 c 1.0305973 k 0.048821274 m 3.9824803\n",
      "37 Train Loss 20.654066 Test RE 0.0012979499723687958 c 1.0084796 k 0.04931415 m 4.2550306\n",
      "38 Train Loss 16.435137 Test RE 0.0011920504188706387 c 1.0179256 k 0.04927482 m 4.383361\n",
      "39 Train Loss 15.974575 Test RE 0.0012708228422368848 c 1.0105731 k 0.04938206 m 4.387491\n",
      "40 Train Loss 12.816152 Test RE 0.001227290832030226 c 1.005975 k 0.049537227 m 4.5392227\n",
      "41 Train Loss 11.217112 Test RE 0.0011289730440486073 c 1.0011824 k 0.04984217 m 4.7344475\n",
      "42 Train Loss 10.370761 Test RE 0.0012650175632043772 c 1.0012702 k 0.050037276 m 4.938502\n",
      "43 Train Loss 9.86421 Test RE 0.0013005029405924104 c 0.9943203 k 0.05012709 m 4.97517\n",
      "44 Train Loss 7.366475 Test RE 0.0011537357362457732 c 1.0086215 k 0.049932014 m 4.923413\n",
      "45 Train Loss 5.3595314 Test RE 0.0010181915624660953 c 1.0012164 k 0.04996485 m 4.9029646\n",
      "46 Train Loss 4.5278544 Test RE 0.000828357917290832 c 1.0030485 k 0.049901333 m 4.859287\n",
      "47 Train Loss 4.3542104 Test RE 0.0008032988951923178 c 1.0054932 k 0.049822103 m 4.840595\n",
      "48 Train Loss 3.978632 Test RE 0.0007291805033780241 c 0.9992668 k 0.04990119 m 4.885985\n",
      "49 Train Loss 3.3839257 Test RE 0.0005928709280423091 c 1.0049263 k 0.049847372 m 4.8953\n",
      "50 Train Loss 3.011725 Test RE 0.0005828985469178478 c 1.0037491 k 0.04987805 m 4.9406466\n",
      "51 Train Loss 2.4176326 Test RE 0.0005261455893557579 c 1.0010265 k 0.05001651 m 5.06559\n",
      "52 Train Loss 1.9627587 Test RE 0.0004822464886569505 c 1.0018557 k 0.049975906 m 5.067823\n",
      "53 Train Loss 1.3717868 Test RE 0.0003971385077459631 c 1.0004026 k 0.04988458 m 4.9577293\n",
      "54 Train Loss 1.0005071 Test RE 0.00032953644914860363 c 1.0044978 k 0.049833827 m 4.9491267\n",
      "55 Train Loss 0.9375957 Test RE 0.00030688476372501503 c 1.0024793 k 0.049868684 m 4.96789\n",
      "56 Train Loss 0.87935716 Test RE 0.0002818084249002098 c 1.0016563 k 0.04987437 m 4.9832067\n",
      "57 Train Loss 0.82128066 Test RE 0.00027028081901937345 c 1.004007 k 0.04981617 m 4.958968\n",
      "58 Train Loss 0.7070435 Test RE 0.00022599908484208968 c 1.0034621 k 0.04981671 m 4.917157\n",
      "59 Train Loss 0.64253795 Test RE 0.00020995211785849424 c 1.0043256 k 0.049823858 m 4.9512563\n",
      "60 Train Loss 0.58632094 Test RE 0.00018446382493602113 c 1.0017036 k 0.049891658 m 4.9748955\n",
      "61 Train Loss 0.550065 Test RE 0.00017360546655439493 c 1.002012 k 0.049896006 m 4.965044\n",
      "62 Train Loss 0.5290731 Test RE 0.00016205861970653945 c 1.0020531 k 0.049892597 m 4.9571815\n",
      "63 Train Loss 0.5037192 Test RE 0.00014647492798980716 c 1.0003593 k 0.049938742 m 4.9734764\n",
      "64 Train Loss 0.48438784 Test RE 0.0001408065281141997 c 1.0009695 k 0.049945828 m 4.992301\n",
      "65 Train Loss 0.48047283 Test RE 0.00014147702111892626 c 1.0013264 k 0.049933493 m 4.9909306\n",
      "66 Train Loss 0.46857944 Test RE 0.00012916773061295392 c 1.0010761 k 0.049937043 m 4.991162\n",
      "67 Train Loss 0.444324 Test RE 0.00012022885546630988 c 1.000811 k 0.049942598 m 4.996758\n",
      "68 Train Loss 0.40839404 Test RE 0.00013244899721831773 c 1.0012394 k 0.049943157 m 5.000998\n",
      "69 Train Loss 0.3818637 Test RE 0.0001318691486823023 c 1.0001028 k 0.049951524 m 5.012504\n",
      "70 Train Loss 0.34029153 Test RE 0.0001277473075025957 c 1.0018598 k 0.04992268 m 4.983803\n",
      "71 Train Loss 0.3128436 Test RE 0.00012133976661330307 c 1.0017478 k 0.049925745 m 4.9674587\n",
      "72 Train Loss 0.28787866 Test RE 0.00011123798495962868 c 1.0016346 k 0.04992227 m 4.960979\n",
      "73 Train Loss 0.25825387 Test RE 0.00011617708696615808 c 1.0016509 k 0.049935453 m 4.970247\n",
      "74 Train Loss 0.24388815 Test RE 0.00012507847326059321 c 1.0009573 k 0.04995445 m 4.9832835\n",
      "75 Train Loss 0.24100107 Test RE 0.00012135104495956496 c 1.0008208 k 0.049958598 m 4.986908\n",
      "76 Train Loss 0.2362558 Test RE 0.00012247600457424555 c 1.0007651 k 0.049963273 m 4.9874725\n",
      "77 Train Loss 0.23120499 Test RE 0.0001280000416456796 c 1.000915 k 0.049964614 m 4.9910784\n",
      "78 Train Loss 0.21187662 Test RE 0.00012511513642985209 c 1.0007192 k 0.049959734 m 4.991719\n",
      "79 Train Loss 0.18467051 Test RE 9.583766157506762e-05 c 1.0007286 k 0.04995765 m 4.9842706\n",
      "80 Train Loss 0.17137662 Test RE 9.283256682993449e-05 c 1.00106 k 0.049947605 m 4.9877405\n",
      "81 Train Loss 0.16598281 Test RE 9.080532427012885e-05 c 1.0009019 k 0.04994811 m 4.9847965\n",
      "82 Train Loss 0.16432333 Test RE 8.920918858031156e-05 c 1.0013219 k 0.04993841 m 4.9814816\n",
      "83 Train Loss 0.16026258 Test RE 8.95266466757487e-05 c 1.0020449 k 0.049929243 m 4.979854\n",
      "84 Train Loss 0.14365129 Test RE 8.211172714356875e-05 c 1.0013375 k 0.049937386 m 4.979235\n",
      "85 Train Loss 0.14148656 Test RE 8.195942646352626e-05 c 1.0010006 k 0.049942706 m 4.9847803\n",
      "86 Train Loss 0.13889055 Test RE 8.386598909230393e-05 c 1.0012041 k 0.04994171 m 4.987594\n",
      "87 Train Loss 0.13719971 Test RE 8.42607869513416e-05 c 1.0011319 k 0.049941126 m 4.985074\n",
      "88 Train Loss 0.13671781 Test RE 8.220758714156014e-05 c 1.0010927 k 0.04994326 m 4.9867153\n",
      "89 Train Loss 0.13630852 Test RE 8.191702267789836e-05 c 1.0011417 k 0.049943812 m 4.988061\n",
      "90 Train Loss 0.13479629 Test RE 8.367827599804252e-05 c 1.0010664 k 0.04994279 m 4.985519\n",
      "91 Train Loss 0.13368413 Test RE 8.207340383099265e-05 c 1.0010095 k 0.049940128 m 4.9814076\n",
      "92 Train Loss 0.13191192 Test RE 8.008254691470695e-05 c 1.0013877 k 0.049932405 m 4.9780145\n",
      "93 Train Loss 0.12888534 Test RE 7.608622951996533e-05 c 1.0016899 k 0.04993073 m 4.9787335\n",
      "94 Train Loss 0.12755245 Test RE 7.439045668256046e-05 c 1.0010673 k 0.049940433 m 4.980394\n",
      "95 Train Loss 0.12105824 Test RE 7.104913539759243e-05 c 1.0000409 k 0.049957372 m 4.9868317\n",
      "96 Train Loss 0.09105293 Test RE 7.11173619449715e-05 c 1.0010751 k 0.049951088 m 4.9895897\n",
      "97 Train Loss 0.08734678 Test RE 7.703543381850404e-05 c 1.0005622 k 0.04996732 m 4.996035\n",
      "98 Train Loss 0.08650011 Test RE 7.40736823496127e-05 c 1.0003891 k 0.04997005 m 4.9983077\n",
      "99 Train Loss 0.07967254 Test RE 7.693860168299486e-05 c 1.0012605 k 0.049953043 m 4.9937534\n",
      "100 Train Loss 0.06963321 Test RE 7.679121303565286e-05 c 1.0005985 k 0.04996588 m 4.9995246\n",
      "101 Train Loss 0.060886215 Test RE 6.791353808942887e-05 c 0.99978817 k 0.049980816 m 4.9991264\n",
      "102 Train Loss 0.056603927 Test RE 6.537161794430037e-05 c 1.0005547 k 0.049971044 m 4.9939766\n",
      "103 Train Loss 0.054690007 Test RE 5.842917967937921e-05 c 1.0004518 k 0.049976647 m 4.99769\n",
      "104 Train Loss 0.05331644 Test RE 5.655466376779743e-05 c 1.0003829 k 0.049976997 m 4.9957623\n",
      "105 Train Loss 0.05246488 Test RE 5.2115304478724993e-05 c 1.0003735 k 0.04997475 m 4.9948893\n",
      "106 Train Loss 0.05193499 Test RE 5.0302540139924745e-05 c 1.000494 k 0.049975287 m 4.997566\n",
      "107 Train Loss 0.050930597 Test RE 5.2468357816235056e-05 c 1.0006886 k 0.049974043 m 4.9959903\n",
      "108 Train Loss 0.047092706 Test RE 4.8903109957599086e-05 c 1.0009444 k 0.049967483 m 4.990268\n",
      "109 Train Loss 0.043386158 Test RE 4.206365565412866e-05 c 1.0005369 k 0.04997818 m 4.9969497\n",
      "110 Train Loss 0.04268794 Test RE 4.192682176931807e-05 c 1.0003743 k 0.04998137 m 4.99799\n",
      "111 Train Loss 0.042481244 Test RE 4.1906702151790075e-05 c 1.0003192 k 0.049982753 m 4.997226\n",
      "112 Train Loss 0.04220057 Test RE 4.036226275264674e-05 c 1.0002254 k 0.04998534 m 4.9983745\n",
      "113 Train Loss 0.041963182 Test RE 3.9005874653518636e-05 c 1.0002952 k 0.049983688 m 4.998888\n",
      "114 Train Loss 0.041736167 Test RE 4.036471122113972e-05 c 1.0003686 k 0.04998352 m 4.9983015\n",
      "115 Train Loss 0.041572932 Test RE 4.1027084455768014e-05 c 1.0002365 k 0.049985703 m 4.9987164\n",
      "116 Train Loss 0.041323755 Test RE 3.873234179827051e-05 c 1.0001649 k 0.049985614 m 4.998563\n",
      "117 Train Loss 0.040938877 Test RE 3.811346283939682e-05 c 1.0004001 k 0.04998252 m 4.9960995\n",
      "118 Train Loss 0.039123606 Test RE 3.685103515240483e-05 c 1.0007892 k 0.049977515 m 4.9944425\n",
      "119 Train Loss 0.035932347 Test RE 3.193540519539026e-05 c 1.0002531 k 0.049990173 m 4.9980345\n",
      "120 Train Loss 0.034993283 Test RE 3.2674654552024845e-05 c 1.0000117 k 0.049996186 m 4.9993067\n",
      "121 Train Loss 0.034832463 Test RE 3.325573196800957e-05 c 1.0000768 k 0.049995393 m 4.9993725\n",
      "122 Train Loss 0.034756683 Test RE 3.354018624826487e-05 c 1.0000596 k 0.04999653 m 4.999762\n",
      "123 Train Loss 0.03468749 Test RE 3.370306021735476e-05 c 1.0000614 k 0.049997207 m 5.0003824\n",
      "124 Train Loss 0.03467414 Test RE 3.3723892816459925e-05 c 1.0000793 k 0.049997054 m 5.000425\n",
      "125 Train Loss 0.034636453 Test RE 3.406200721223916e-05 c 1.000091 k 0.049997367 m 5.0003705\n",
      "126 Train Loss 0.03456593 Test RE 3.525131738321241e-05 c 1.0000356 k 0.049998723 m 5.000872\n",
      "127 Train Loss 0.034459304 Test RE 3.588266304694236e-05 c 1.0000249 k 0.049998686 m 5.000246\n",
      "128 Train Loss 0.03435089 Test RE 3.51514447184645e-05 c 1.0000228 k 0.049997896 m 4.999413\n",
      "129 Train Loss 0.034284197 Test RE 3.4750962492022604e-05 c 1.0000858 k 0.04999698 m 5.000359\n",
      "130 Train Loss 0.0342327 Test RE 3.491659302425728e-05 c 1.0001564 k 0.0499958 m 5.000012\n",
      "131 Train Loss 0.034182604 Test RE 3.5009353577533764e-05 c 1.0001119 k 0.04999658 m 4.9996595\n",
      "132 Train Loss 0.03415398 Test RE 3.4895656129934676e-05 c 1.0000795 k 0.049997088 m 4.99971\n",
      "133 Train Loss 0.034077965 Test RE 3.464359101841394e-05 c 1.000008 k 0.04999794 m 4.9999743\n",
      "134 Train Loss 0.034044806 Test RE 3.473821992538257e-05 c 0.9999331 k 0.04999916 m 5.0006037\n",
      "135 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "136 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "137 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "138 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "139 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "140 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "141 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "142 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "143 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "144 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "145 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "146 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "147 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "148 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "149 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "150 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "151 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "152 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "153 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "154 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "155 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "156 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "157 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "158 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "159 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "160 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "161 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "162 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "163 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "164 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "165 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "166 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "167 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "168 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "169 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "170 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "171 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "172 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "173 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "174 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "175 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "176 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "177 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "178 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "179 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "180 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "181 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "182 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "183 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "184 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "185 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "186 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "187 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "188 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "189 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "190 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "191 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "192 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "193 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "194 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "195 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "196 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "197 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "198 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "199 Train Loss 0.034019314 Test RE 3.481928148886694e-05 c 0.9999043 k 0.04999962 m 5.000833\n",
      "Training time: 43.70\n",
      "Training time: 43.70\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 200\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "m_full = []\n",
    "k_full = []\n",
    "c_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "\n",
    "for reps in range(max_reps):   \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []   \n",
    "    beta_val = []\n",
    "    m_val = []\n",
    "    k_val = []\n",
    "    c_val = []\n",
    "\n",
    "    torch.manual_seed(reps*36)\n",
    "    N_f = 10000 #Total number of collocation points\n",
    "\n",
    "    layers = np.array([1,50,50,50,1]) #9 hidden layers\n",
    "    PINN = Sequentialmodel(layers)\n",
    "\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.25, \n",
    "                                max_iter = 10, \n",
    "                                max_eval = 15, \n",
    "                                tolerance_grad = 1e-6, \n",
    "                                tolerance_change = 1e-6, \n",
    "                                history_size = 100, \n",
    "                                line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "    train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    beta_full.append(beta_val)\n",
    "    m_full.append(m_val)\n",
    "    k_full.append(k_val)\n",
    "    c_full.append(c_val)\n",
    "    \n",
    "\n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"beta\": beta_full,\"m\": m_full,\"k\": k_full,\"c\": c_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "pmHEeBpzfFQh",
    "outputId": "990b2054-f35e-4c8e-c378-69ff2eb19f4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f3f186cf910>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_pred = PINN.test()\n",
    "plt.plot(t,x_true,'b')\n",
    "plt.plot(t,x_pred,'r--')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "stan_1D_FODE_tune.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
