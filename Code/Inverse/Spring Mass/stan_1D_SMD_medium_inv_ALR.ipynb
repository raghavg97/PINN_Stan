{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8byrnUmNKGR",
    "outputId": "510f67c6-b225-41ba-e78a-d0a6e54ded85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uoNYpDzlNKGV"
   },
   "outputs": [],
   "source": [
    "tstart= 0 \n",
    "tstop= 60 \n",
    "\n",
    "increment = 0.1 \n",
    "# Initial condition\n",
    "x_init= [0,0] \n",
    "\n",
    "t = np.arange(tstart,tstop+1,increment)\n",
    "\n",
    "c = 1 # Damping constant \n",
    "k = 5 # Stiffness of the spring\n",
    "m = 5 # Mass \n",
    "F0 = 100\n",
    "\n",
    "def mydiff(x, t):    \n",
    "    F =F0\n",
    "    \n",
    "    dx1dt = x[1] \n",
    "    dx2dt = (F -c*x[1] -k*x[0])/m\n",
    "    dxdt= [dx1dt, dx2dt] \n",
    "    \n",
    "    return dxdt \n",
    "\n",
    "x_full_sol = odeint(mydiff, x_init, t) \n",
    "\n",
    "x_sol = x_full_sol[:,0]\n",
    "v_sol = x_full_sol[:,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BR02v-fkNKGV"
   },
   "outputs": [],
   "source": [
    "level = \"low\"\n",
    "label = \"1D_SMD_stan_\" + level\n",
    "\n",
    "loss_thresh = 0.005\n",
    "\n",
    "t_test = t.reshape(-1,1)\n",
    "t_test_tensor = torch.from_numpy(t_test).float().to(device)\n",
    "\n",
    "# bc1_t = t[0].reshape(-1,1)\n",
    "# bc1_x = x_sol[0].reshape(-1,1)\n",
    "# t_bc1_train = torch.from_numpy(bc1_t).float().to(device)\n",
    "# x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "\n",
    "bc1_t = t.reshape(-1,1)\n",
    "bc1_x = x_sol.reshape(-1,1)\n",
    "t_bc1_train = torch.from_numpy(bc1_t).float().to(device)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "\n",
    "\n",
    "bc2_t = t[0].reshape(-1,1)\n",
    "t_bc2_train = torch.from_numpy(bc2_t).float().to(device)\n",
    "bc2_val = v_sol[0].reshape(-1,1)\n",
    "bc2_val =torch.from_numpy(bc2_val).float().to(device)\n",
    "\n",
    "t_test = t.reshape(-1,1)\n",
    "t_test_tensor = torch.from_numpy(t_test).float().to(device)\n",
    "x_true = x_sol\n",
    "x_true_norm = np.linalg.norm(x_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(t[0]) \n",
    "ub = np.array(t[-1]) \n",
    "\n",
    "#torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SyyktBKBXRo1"
   },
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    \n",
    "    t01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=t01,random_state =seed)\n",
    "\n",
    "    t_coll = lb + (ub-lb)*sampling(N_f)\n",
    "    # t_coll = np.vstack((t_coll,)) # append training points to collocation points \n",
    "\n",
    "    return t_coll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "o1b21zLnNKGW"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "        \n",
    "        self.beta = Parameter(0.0*torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "        \n",
    "        self.k = Parameter(torch.tensor(1.0))\n",
    "        self.k.requiresGrad = True\n",
    "        self.c = Parameter(torch.tensor(1.0))\n",
    "        self.c.requiresGrad = True\n",
    "        self.m = Parameter(torch.tensor(1.0))\n",
    "        self.m.requiresGrad = True\n",
    "        \n",
    "        self.lambdas = torch.ones((2,),device = device)\n",
    "        self.lambda_alpha = 0.1\n",
    "        \n",
    "    'forward pass'\n",
    "    def forward(self,t):\n",
    "        if torch.is_tensor(t) != True:         \n",
    "            t = torch.from_numpy(t)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        t = 2.0*(t - l_b)/(u_b - l_b) - 1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = t.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z) + self.beta[:,i]*z*self.activation(z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,t,x):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(t), x)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,t_bc2,bc2_val):\n",
    "        g = t_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        x = self.forward(g)    \n",
    "            \n",
    "        x_t = autograd.grad(x,g,torch.ones([t_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dx_dt = x_t[:,[0]]\n",
    "        \n",
    "        bc2 = dx_dt\n",
    "        \n",
    "        loss_bc2= self.loss_function(bc2,bc2_val)\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self,t_coll,f_hat):\n",
    "             \n",
    "        g = t_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        x = self.forward(g) \n",
    "\n",
    "        x_t = autograd.grad(x,g,torch.ones([t_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        x_tt = autograd.grad(x_t,g,torch.ones(t_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dx_dt = x_t[:,[0]]\n",
    "        \n",
    "        dx2_d2t = x_tt[:,[0]]\n",
    "        \n",
    "        f = torch.abs(self.m)*dx2_d2t + torch.abs(self.c)*dx_dt + torch.abs(self.k)*x - F0\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,t_bc1,x_bc1,t_bc2,bc2_val,t_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.lambdas[0]*self.loss_BC1(t_bc1,x_bc1)\n",
    "        loss_bc2 = self.lambdas[1]*self.loss_BC2(t_bc2,bc2_val)\n",
    "        loss_f = self.loss_PDE(t_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "    \n",
    "    def lambda_update(self,t_bc1,x_bc1,t_bc2,bc2_val,t_coll,f_hat):\n",
    "        loss_bc1 = self.lambdas[0]*self.loss_BC1(t_bc1,x_bc1)\n",
    "        loss_bc1.backward()\n",
    "        bc1_grads = []\n",
    "        for param in self.parameters():\n",
    "            bc1_grads.append(param.grad.view(-1))\n",
    "        bc1_grads = torch.cat(bc1_grads)\n",
    "        bc1_grads = torch.mean(torch.abs(bc1_grads))\n",
    "        \n",
    "        \n",
    "        loss_bc2 = self.lambdas[1]*self.loss_BC2(t_bc2,bc2_val)\n",
    "        loss_bc2.backward()\n",
    "        bc2_grads = []\n",
    "        for param in self.parameters():\n",
    "            bc2_grads.append(param.grad.view(-1))\n",
    "        bc2_grads = torch.cat(bc2_grads)\n",
    "        bc2_grads = torch.mean(torch.abs(bc2_grads))\n",
    "        \n",
    "    \n",
    "        loss_f = self.loss_PDE(t_coll,f_hat)\n",
    "        loss_f.backward()\n",
    "        f_grads = []\n",
    "        for param in self.parameters():\n",
    "            f_grads.append(param.grad.view(-1))   \n",
    "        f_grads = torch.cat(f_grads)\n",
    "        f_grads = torch.max(torch.abs(f_grads))\n",
    "    \n",
    "        self.lambdas[0] = (1.0-self.lambda_alpha)*self.lambdas[0] + self.lambda_alpha*f_grads/bc1_grads\n",
    "        self.lambdas[1] = (1.0-self.lambda_alpha)*self.lambdas[1] + self.lambda_alpha*f_grads/bc2_grads\n",
    "        \n",
    "        return None\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self):\n",
    "        x_pred = self.forward(t_test_tensor)\n",
    "        x_pred = x_pred.cpu().detach().numpy()\n",
    "\n",
    "        return x_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        x_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(x_pred.reshape(-1,1) - x_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(x_pred.reshape(-1,1) - x_true.reshape(-1,1),2)/x_true_norm\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fLY2mT5BOgjD"
   },
   "outputs": [],
   "source": [
    "def train_step(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)\n",
    "    PINN.lambda_update(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8srA5uGuObil"
   },
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0SezTZ_racQB"
   },
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "\n",
    "    t_coll = colloc_pts(N_f,rep*11)\n",
    "    t_coll =  torch.from_numpy(t_coll).float().to(device)\n",
    "    \n",
    "    f_hat = torch.zeros(t_coll.shape[0],1).to(device)\n",
    "    for i in range(max_iter):      \n",
    "        train_step(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat)\n",
    "         \n",
    "        loss_np = PINN.loss(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test RE\",test_re_loss[-1],\"c\",PINN.c.cpu().detach().numpy(),\"k\",PINN.k.cpu().detach().numpy(),\"m\",PINN.m.cpu().detach().numpy())\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I9BYbcJ0NKGX",
    "outputId": "42720d97-b37a-4c42-b7e5-b77a0ef391f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 2814.2231 Test RE 0.23160789671991563 c 0.90604484 k 4.4642177 m 0.999853\n",
      "1 Train Loss 2332.3613 Test RE 0.20469604573569303 c 0.90235573 k 5.0369077 m 0.9998489\n",
      "2 Train Loss 2529.4412 Test RE 0.2045745325117781 c 0.905337 k 5.035246 m 0.9998649\n",
      "3 Train Loss 2561.0554 Test RE 0.20418174994099095 c 0.9183233 k 5.042436 m 0.999924\n",
      "4 Train Loss 2369.828 Test RE 0.20355193720259496 c 0.92708325 k 4.9958124 m 0.9999584\n",
      "5 Train Loss 2316.9355 Test RE 0.2010960175232826 c 0.9357603 k 5.025398 m 0.99987334\n",
      "6 Train Loss 2003.2058 Test RE 0.19369151687716896 c 0.94791526 k 5.0886226 m 0.99952966\n",
      "7 Train Loss 2061.764 Test RE 0.18364509946147212 c 0.95908177 k 5.019445 m 0.999754\n",
      "8 Train Loss 2157.5354 Test RE 0.1731154181903058 c 0.9611165 k 4.757959 m 1.0010322\n",
      "9 Train Loss 2044.112 Test RE 0.16465295535572666 c 0.95661664 k 5.030323 m 1.0024098\n",
      "10 Train Loss 2157.3474 Test RE 0.15983436657124547 c 0.96228963 k 4.7466 m 1.0044515\n",
      "11 Train Loss 2026.7898 Test RE 0.1569434289729204 c 0.9662446 k 4.919307 m 1.0063277\n",
      "12 Train Loss 1977.7795 Test RE 0.1550633651064207 c 0.96741456 k 4.988578 m 1.0066448\n",
      "13 Train Loss 1998.2065 Test RE 0.15331546270349786 c 0.9700466 k 4.900969 m 1.0078588\n",
      "14 Train Loss 1944.0485 Test RE 0.15088724503228382 c 0.97407895 k 4.9535995 m 1.0102308\n",
      "15 Train Loss 2048.949 Test RE 0.15009944743488368 c 0.9770119 k 4.938266 m 1.0122405\n",
      "16 Train Loss 1959.2517 Test RE 0.14716326291188617 c 0.9791578 k 4.906775 m 1.0143014\n",
      "17 Train Loss 1994.241 Test RE 0.14520959608317 c 0.979101 k 4.9986663 m 1.0142037\n",
      "18 Train Loss 1951.6979 Test RE 0.1445717785650061 c 0.97859955 k 5.011365 m 1.0143012\n",
      "19 Train Loss 1870.8171 Test RE 0.14147792514045765 c 0.9797485 k 5.0081587 m 1.016362\n",
      "20 Train Loss 1711.5245 Test RE 0.1401485508561952 c 0.9812781 k 5.1144853 m 1.0181341\n",
      "21 Train Loss 1726.6624 Test RE 0.13860450136573801 c 0.98296857 k 4.96422 m 1.0192083\n",
      "22 Train Loss 1719.8386 Test RE 0.13801591673537733 c 0.9841246 k 4.9728165 m 1.0208648\n",
      "23 Train Loss 1675.7388 Test RE 0.13363781083526688 c 0.9845999 k 4.8547254 m 1.0224214\n",
      "24 Train Loss 1663.726 Test RE 0.1323235024584794 c 0.98457694 k 4.942363 m 1.0229836\n",
      "25 Train Loss 1676.7576 Test RE 0.1316285344785246 c 0.9845038 k 4.9774876 m 1.0233586\n",
      "26 Train Loss 1652.3497 Test RE 0.12905300644561077 c 0.9833935 k 4.9735756 m 1.0245688\n",
      "27 Train Loss 1581.0613 Test RE 0.12750980117110466 c 0.9821107 k 4.8931146 m 1.0252033\n",
      "28 Train Loss 1474.6102 Test RE 0.12708976536379993 c 0.9830106 k 4.8983645 m 1.0269489\n",
      "29 Train Loss 1475.8363 Test RE 0.12478980581992706 c 0.9849552 k 4.938488 m 1.0313625\n",
      "30 Train Loss 1357.6844 Test RE 0.11715079889013519 c 0.99073595 k 4.9440684 m 1.0394437\n",
      "31 Train Loss 1282.9473 Test RE 0.11433188293995122 c 0.994454 k 4.9340515 m 1.042181\n",
      "32 Train Loss 1171.1827 Test RE 0.11239032808514687 c 0.9952385 k 4.941111 m 1.0410235\n",
      "33 Train Loss 1270.5946 Test RE 0.11187832624792331 c 0.9961471 k 4.9290013 m 1.0417405\n",
      "34 Train Loss 1210.4347 Test RE 0.10557548274764351 c 0.9960579 k 4.8503103 m 1.0407809\n",
      "35 Train Loss 1135.1187 Test RE 0.10367478915999462 c 0.99631006 k 4.868259 m 1.040788\n",
      "36 Train Loss 1144.2875 Test RE 0.10312076680466152 c 0.9974619 k 4.8962154 m 1.0415827\n",
      "37 Train Loss 1064.9551 Test RE 0.10208774362092969 c 0.9992105 k 4.909829 m 1.0431367\n",
      "38 Train Loss 1039.4827 Test RE 0.10184333218568216 c 1.0005805 k 4.9167213 m 1.0440065\n",
      "39 Train Loss 993.7985 Test RE 0.10074789815584534 c 1.0023736 k 4.899977 m 1.0451832\n",
      "40 Train Loss 936.4913 Test RE 0.10072772649841415 c 1.0036212 k 4.9353495 m 1.0463487\n",
      "41 Train Loss 862.5425 Test RE 0.1006142640284265 c 1.0033377 k 4.9286633 m 1.0459743\n",
      "42 Train Loss 809.8828 Test RE 0.09689046672929758 c 1.0055326 k 4.8911104 m 1.0483984\n",
      "43 Train Loss 855.61426 Test RE 0.09291072529145769 c 1.0092722 k 4.883876 m 1.0516013\n",
      "44 Train Loss 814.42285 Test RE 0.09133135173280627 c 1.0091808 k 4.815257 m 1.0514926\n",
      "45 Train Loss 769.8624 Test RE 0.08879716583244007 c 1.0081091 k 4.862202 m 1.0513362\n",
      "46 Train Loss 850.5303 Test RE 0.08814052519614331 c 1.0083667 k 4.9191732 m 1.0518184\n",
      "47 Train Loss 911.27295 Test RE 0.08593199914498084 c 1.0061165 k 4.8974857 m 1.0513602\n",
      "48 Train Loss 844.14075 Test RE 0.08543177967389276 c 1.0061164 k 4.8837156 m 1.051522\n",
      "49 Train Loss 827.6116 Test RE 0.08522052742331895 c 1.0045229 k 4.8789344 m 1.0513599\n",
      "50 Train Loss 826.18756 Test RE 0.08511482040903984 c 1.0035391 k 4.898968 m 1.0518275\n",
      "51 Train Loss 852.51355 Test RE 0.08442662008745336 c 1.0007309 k 4.924848 m 1.0502183\n",
      "52 Train Loss 840.2666 Test RE 0.08134229654950151 c 0.99574196 k 4.941127 m 1.0473027\n",
      "53 Train Loss 800.82446 Test RE 0.07905199449422898 c 0.9961895 k 4.940884 m 1.0494686\n",
      "54 Train Loss 959.39777 Test RE 0.07877612425047595 c 0.9950638 k 4.9181542 m 1.0489352\n",
      "55 Train Loss 917.79663 Test RE 0.07753413305582262 c 0.99150383 k 4.9105597 m 1.0474988\n",
      "56 Train Loss 1078.7969 Test RE 0.07688365918970949 c 0.9913075 k 4.891423 m 1.047148\n",
      "57 Train Loss 1040.5316 Test RE 0.07651036874425236 c 0.99194604 k 4.90411 m 1.047246\n",
      "58 Train Loss 989.21564 Test RE 0.07575898081682227 c 0.9925221 k 4.877577 m 1.0466591\n",
      "59 Train Loss 953.43726 Test RE 0.0735757350270701 c 0.9904542 k 4.911478 m 1.0467669\n",
      "60 Train Loss 1032.4314 Test RE 0.07171078020489316 c 0.99153227 k 4.934712 m 1.0482013\n",
      "61 Train Loss 1012.1139 Test RE 0.07096259286761068 c 0.99114525 k 4.9462767 m 1.04861\n",
      "62 Train Loss 950.956 Test RE 0.0690509200008218 c 0.990487 k 4.9193234 m 1.0501034\n",
      "63 Train Loss 933.9127 Test RE 0.06766649847818725 c 0.98863304 k 4.918746 m 1.0500596\n",
      "64 Train Loss 890.0991 Test RE 0.0657166013145205 c 0.9877492 k 4.9930124 m 1.050455\n",
      "65 Train Loss 849.7087 Test RE 0.06259432191070846 c 0.9836775 k 4.963718 m 1.0497609\n",
      "66 Train Loss 906.08905 Test RE 0.06194810694417375 c 0.9831341 k 4.934414 m 1.0499952\n",
      "67 Train Loss 844.9527 Test RE 0.0614467598206179 c 0.9815981 k 4.9204745 m 1.0495337\n",
      "68 Train Loss 781.27655 Test RE 0.06022510994469671 c 0.9793123 k 4.8996487 m 1.0485479\n",
      "69 Train Loss 742.3967 Test RE 0.05950948773229363 c 0.98143214 k 4.803228 m 1.049337\n",
      "70 Train Loss 694.0097 Test RE 0.05926745978042784 c 0.981791 k 4.811495 m 1.0495286\n",
      "71 Train Loss 656.95685 Test RE 0.059252137841617904 c 0.9813453 k 4.8106475 m 1.0495459\n",
      "72 Train Loss 656.7067 Test RE 0.05831248050602517 c 0.97998035 k 4.841572 m 1.0489353\n",
      "73 Train Loss 624.5029 Test RE 0.05794080109387351 c 0.9804189 k 4.8272204 m 1.0493398\n",
      "74 Train Loss 580.6257 Test RE 0.05680951134356263 c 0.98205805 k 4.8311253 m 1.0506723\n",
      "75 Train Loss 538.0288 Test RE 0.05584254183077191 c 0.9828723 k 4.879328 m 1.0520941\n",
      "76 Train Loss 657.3905 Test RE 0.055172070028376 c 0.9826618 k 4.8563876 m 1.0526537\n",
      "77 Train Loss 630.3512 Test RE 0.05454117672659269 c 0.9829908 k 4.8523445 m 1.0533682\n",
      "78 Train Loss 598.5402 Test RE 0.05451270483541307 c 0.9827093 k 4.8935165 m 1.0532789\n",
      "79 Train Loss 576.29535 Test RE 0.05459996017139378 c 0.9820778 k 4.8637514 m 1.0530399\n",
      "80 Train Loss 558.67834 Test RE 0.05428785398668422 c 0.98018754 k 4.8534856 m 1.0525875\n",
      "81 Train Loss 525.12866 Test RE 0.05397919100016678 c 0.9795191 k 4.8926907 m 1.0530714\n",
      "82 Train Loss 876.2454 Test RE 0.05383477903702209 c 0.97884995 k 4.89072 m 1.0530978\n",
      "83 Train Loss 899.1575 Test RE 0.05216842453437393 c 0.97928405 k 4.8888016 m 1.0532478\n",
      "84 Train Loss 827.4133 Test RE 0.05132060478540213 c 0.9795775 k 4.884393 m 1.0534457\n",
      "85 Train Loss 814.4518 Test RE 0.050731877529393656 c 0.9807219 k 4.883587 m 1.0543058\n",
      "86 Train Loss 749.7891 Test RE 0.050269451831320096 c 0.9813049 k 4.866875 m 1.0548742\n",
      "87 Train Loss 697.3648 Test RE 0.0502324528808 c 0.98181564 k 4.9053326 m 1.0552057\n",
      "88 Train Loss 670.2219 Test RE 0.049950751479587066 c 0.98272395 k 4.9022117 m 1.0556165\n",
      "89 Train Loss 658.0756 Test RE 0.04980320884779757 c 0.98365766 k 4.912988 m 1.0562594\n",
      "90 Train Loss 707.9018 Test RE 0.04981474806865926 c 0.98576087 k 4.928124 m 1.057589\n",
      "91 Train Loss 766.5511 Test RE 0.04928371619764816 c 0.9859861 k 4.9513383 m 1.0580988\n",
      "92 Train Loss 740.32513 Test RE 0.04901535333742972 c 0.985244 k 4.941309 m 1.0576051\n",
      "93 Train Loss 741.7129 Test RE 0.04832853533104323 c 0.986451 k 4.8785577 m 1.0584325\n",
      "94 Train Loss 757.5749 Test RE 0.04812936949225862 c 0.9879919 k 4.871834 m 1.059698\n",
      "95 Train Loss 722.1008 Test RE 0.047318428451513045 c 0.9890416 k 4.835621 m 1.0605489\n",
      "96 Train Loss 679.13464 Test RE 0.04704454874640524 c 0.98998284 k 4.831629 m 1.0616667\n",
      "97 Train Loss 652.85754 Test RE 0.04693827028018881 c 0.9891769 k 4.8475213 m 1.0617393\n",
      "98 Train Loss 609.7486 Test RE 0.046799367055663676 c 0.9892816 k 4.8398213 m 1.0620021\n",
      "99 Train Loss 578.0055 Test RE 0.04674896314233583 c 0.9889324 k 4.839611 m 1.0620261\n",
      "100 Train Loss 569.20233 Test RE 0.04530345685583123 c 0.98730004 k 4.8313885 m 1.0640805\n",
      "101 Train Loss 518.1205 Test RE 0.04417620746311018 c 0.98842937 k 4.8444033 m 1.0685959\n",
      "102 Train Loss 492.02185 Test RE 0.04381565097446722 c 0.98896 k 4.804988 m 1.070914\n",
      "103 Train Loss 491.8373 Test RE 0.04395071750344313 c 0.9887482 k 4.776618 m 1.0720555\n",
      "104 Train Loss 468.2838 Test RE 0.04388836452513905 c 0.9889471 k 4.8191614 m 1.0730126\n",
      "105 Train Loss 588.3918 Test RE 0.043283742541567465 c 0.9871937 k 4.8416653 m 1.0735207\n",
      "106 Train Loss 554.2659 Test RE 0.042568646995533754 c 0.9873008 k 4.828998 m 1.0745753\n",
      "107 Train Loss 521.1404 Test RE 0.04225616906325115 c 0.98881495 k 4.822113 m 1.0769719\n",
      "108 Train Loss 489.86224 Test RE 0.042046687358995645 c 0.98990864 k 4.8328714 m 1.078179\n",
      "109 Train Loss 472.15677 Test RE 0.041989057007651716 c 0.9898012 k 4.8526645 m 1.0791317\n",
      "110 Train Loss 513.8679 Test RE 0.041921712359511726 c 0.9888508 k 4.849602 m 1.0788224\n",
      "111 Train Loss 493.38623 Test RE 0.04164966395284787 c 0.98818636 k 4.84292 m 1.0792265\n",
      "112 Train Loss 467.6698 Test RE 0.0416890155111913 c 0.9878976 k 4.8434887 m 1.0793769\n",
      "113 Train Loss 469.5039 Test RE 0.04185380046135152 c 0.9874824 k 4.8459983 m 1.0792224\n",
      "114 Train Loss 445.63733 Test RE 0.04187905147414818 c 0.9868025 k 4.8539486 m 1.0791376\n",
      "115 Train Loss 428.1796 Test RE 0.04214960571323086 c 0.9869935 k 4.8667173 m 1.0798025\n",
      "116 Train Loss 407.4547 Test RE 0.04216293945678088 c 0.98712164 k 4.858438 m 1.0805126\n",
      "117 Train Loss 389.3828 Test RE 0.0422433402809572 c 0.98667324 k 4.8484 m 1.0800155\n",
      "118 Train Loss 430.10056 Test RE 0.04232010430657643 c 0.98753506 k 4.852579 m 1.0809914\n",
      "119 Train Loss 411.66312 Test RE 0.04122330501801709 c 0.989066 k 4.851391 m 1.0832449\n",
      "120 Train Loss 393.15723 Test RE 0.04132374353372293 c 0.99057955 k 4.838459 m 1.0849417\n",
      "121 Train Loss 379.95172 Test RE 0.040760636309556106 c 0.9914394 k 4.846335 m 1.0867316\n",
      "122 Train Loss 396.3847 Test RE 0.040676301362453884 c 0.99307024 k 4.8528333 m 1.0879407\n",
      "123 Train Loss 446.81238 Test RE 0.0398744511354449 c 0.9940665 k 4.861508 m 1.0894352\n",
      "124 Train Loss 428.77667 Test RE 0.03877210489078078 c 0.9959388 k 4.84245 m 1.0921775\n",
      "125 Train Loss 408.65283 Test RE 0.03806773479929619 c 0.9967782 k 4.8649383 m 1.0939387\n",
      "126 Train Loss 391.0321 Test RE 0.037894985367335834 c 0.9970324 k 4.8472414 m 1.0939007\n",
      "127 Train Loss 392.03412 Test RE 0.0376547733819908 c 0.99766487 k 4.852126 m 1.0934676\n",
      "128 Train Loss 374.83905 Test RE 0.03709927682978957 c 0.99845654 k 4.8321066 m 1.0948293\n",
      "129 Train Loss 360.70337 Test RE 0.03719864418011868 c 0.99896395 k 4.847529 m 1.0952371\n",
      "130 Train Loss 347.19452 Test RE 0.03738949108339157 c 0.9991167 k 4.8429837 m 1.0946486\n",
      "131 Train Loss 334.2719 Test RE 0.03768373026520873 c 0.99911183 k 4.843414 m 1.0944977\n",
      "132 Train Loss 321.87473 Test RE 0.03799478026824914 c 0.9991311 k 4.8429403 m 1.0943083\n",
      "133 Train Loss 312.10263 Test RE 0.038413970653211886 c 0.99920195 k 4.8449163 m 1.0940019\n",
      "134 Train Loss 301.10205 Test RE 0.038831588195495356 c 0.999201 k 4.847573 m 1.0937392\n",
      "135 Train Loss 293.09082 Test RE 0.03943637406453935 c 0.99923074 k 4.8479323 m 1.0934873\n",
      "136 Train Loss 284.04535 Test RE 0.04001439292987658 c 0.99903846 k 4.8501954 m 1.0931197\n",
      "137 Train Loss 274.81055 Test RE 0.04082406309822063 c 0.99905425 k 4.8497353 m 1.0930749\n",
      "138 Train Loss 273.10223 Test RE 0.04162419529848231 c 0.9989215 k 4.8536797 m 1.0935327\n",
      "139 Train Loss 264.3137 Test RE 0.04171112889150574 c 0.99906325 k 4.8572416 m 1.0945053\n",
      "140 Train Loss 257.05182 Test RE 0.042763906595745534 c 0.9992482 k 4.859557 m 1.0947907\n",
      "141 Train Loss 249.3874 Test RE 0.04378475630411323 c 0.9997113 k 4.8608866 m 1.0953989\n",
      "142 Train Loss 245.55528 Test RE 0.04404779513530814 c 0.9999416 k 4.8582535 m 1.0956596\n",
      "143 Train Loss 238.36584 Test RE 0.045631653310336424 c 1.0002621 k 4.8565598 m 1.0953476\n",
      "144 Train Loss 234.68768 Test RE 0.04720913275961575 c 1.0008329 k 4.8580565 m 1.0954052\n",
      "145 Train Loss 240.41122 Test RE 0.048900841625168764 c 1.0023618 k 4.861398 m 1.0963326\n",
      "146 Train Loss 235.22128 Test RE 0.046865001804051724 c 1.0040486 k 4.858636 m 1.098102\n",
      "147 Train Loss 227.97287 Test RE 0.048374485761906144 c 1.0064015 k 4.856483 m 1.1001118\n",
      "148 Train Loss 224.2964 Test RE 0.0498377179546927 c 1.0072722 k 4.859582 m 1.1011448\n",
      "149 Train Loss 219.02232 Test RE 0.051378416277872443 c 1.0086627 k 4.863193 m 1.1021435\n",
      "150 Train Loss 212.16106 Test RE 0.05254847069102187 c 1.0102258 k 4.8638926 m 1.1030325\n",
      "151 Train Loss 209.58136 Test RE 0.05458884790442899 c 1.0116552 k 4.8648643 m 1.1042479\n",
      "152 Train Loss 203.56174 Test RE 0.05507913841392525 c 1.0117813 k 4.8647523 m 1.1046215\n",
      "153 Train Loss 200.17819 Test RE 0.05693574205950585 c 1.0125083 k 4.868043 m 1.1054746\n",
      "154 Train Loss 193.65985 Test RE 0.05844784413772934 c 1.0140471 k 4.865048 m 1.1065587\n",
      "155 Train Loss 191.07614 Test RE 0.061377114488377967 c 1.014994 k 4.866753 m 1.1076707\n",
      "156 Train Loss 185.39822 Test RE 0.06256610165637626 c 1.0151846 k 4.8668857 m 1.107998\n",
      "157 Train Loss 179.23236 Test RE 0.0651374908891913 c 1.0155005 k 4.8703856 m 1.10882\n",
      "158 Train Loss 172.50214 Test RE 0.0681518365089161 c 1.0158994 k 4.8714733 m 1.1094203\n",
      "159 Train Loss 166.93317 Test RE 0.07176568833137033 c 1.0161628 k 4.875913 m 1.1091743\n",
      "160 Train Loss 163.04002 Test RE 0.07470920742590108 c 1.0163628 k 4.877184 m 1.1090567\n",
      "161 Train Loss 164.94693 Test RE 0.0771114493715919 c 1.0166123 k 4.878259 m 1.109615\n",
      "162 Train Loss 171.50787 Test RE 0.07522263748948783 c 1.0168253 k 4.877212 m 1.1111437\n",
      "163 Train Loss 166.03026 Test RE 0.07204382711413446 c 1.0169889 k 4.873537 m 1.1147846\n",
      "164 Train Loss 159.48439 Test RE 0.07498395474206147 c 1.0174899 k 4.876281 m 1.1180283\n",
      "165 Train Loss 157.30515 Test RE 0.07907669699324175 c 1.0184942 k 4.877089 m 1.1195009\n",
      "166 Train Loss 151.14896 Test RE 0.0804242185512955 c 1.0189637 k 4.879043 m 1.1227466\n",
      "167 Train Loss 162.5909 Test RE 0.08421473771692485 c 1.0211561 k 4.876227 m 1.1312827\n",
      "168 Train Loss 171.09523 Test RE 0.07715769993396875 c 1.0214161 k 4.875328 m 1.133991\n",
      "169 Train Loss 173.04024 Test RE 0.07254667767571166 c 1.0222008 k 4.8689284 m 1.139025\n",
      "170 Train Loss 167.59915 Test RE 0.07097014396378747 c 1.0247179 k 4.875752 m 1.1494739\n",
      "171 Train Loss 161.71658 Test RE 0.0736884251093369 c 1.0274948 k 4.8746195 m 1.1556845\n",
      "172 Train Loss 182.29678 Test RE 0.07606179785063791 c 1.0326751 k 4.877407 m 1.1688225\n",
      "173 Train Loss 179.93729 Test RE 0.06863679140419966 c 1.0372726 k 4.8567076 m 1.1825551\n",
      "174 Train Loss 177.92111 Test RE 0.06584056171177835 c 1.0382377 k 4.8729296 m 1.1835004\n",
      "175 Train Loss 172.39832 Test RE 0.06559729889291978 c 1.0442917 k 4.86096 m 1.200025\n",
      "176 Train Loss 184.99606 Test RE 0.06751952796463569 c 1.046012 k 4.8699927 m 1.2030635\n",
      "177 Train Loss 182.788 Test RE 0.06176587395330099 c 1.0487434 k 4.8671374 m 1.2097745\n",
      "178 Train Loss 177.19153 Test RE 0.06126543115318667 c 1.05267 k 4.862883 m 1.2181388\n",
      "179 Train Loss 173.51338 Test RE 0.06472884216644706 c 1.0541232 k 4.871373 m 1.218183\n",
      "180 Train Loss 169.35312 Test RE 0.06636918241399939 c 1.0540109 k 4.873203 m 1.217784\n",
      "181 Train Loss 166.94785 Test RE 0.06853054688060788 c 1.055924 k 4.8744774 m 1.222623\n",
      "182 Train Loss 161.86868 Test RE 0.07056006905694871 c 1.056889 k 4.877403 m 1.2254887\n",
      "183 Train Loss 156.3534 Test RE 0.07317406472494543 c 1.0592908 k 4.8677545 m 1.2324407\n",
      "184 Train Loss 157.57819 Test RE 0.0762019113176398 c 1.0618457 k 4.8737845 m 1.2411473\n",
      "185 Train Loss 153.52408 Test RE 0.07430835278695426 c 1.0650102 k 4.87662 m 1.2523956\n",
      "186 Train Loss 149.84262 Test RE 0.07589443097355919 c 1.0652965 k 4.875247 m 1.2532411\n",
      "187 Train Loss 144.04701 Test RE 0.07799395567577186 c 1.0660968 k 4.8761754 m 1.2565947\n",
      "188 Train Loss 138.79755 Test RE 0.08138316581125962 c 1.0657313 k 4.8773203 m 1.2561069\n",
      "189 Train Loss 133.1488 Test RE 0.0852369189605097 c 1.0692976 k 4.8803973 m 1.2691025\n",
      "190 Train Loss 129.45583 Test RE 0.08825973310298525 c 1.0699755 k 4.8842216 m 1.272058\n",
      "191 Train Loss 124.117 Test RE 0.09024193883028234 c 1.0713161 k 4.8841233 m 1.2789035\n",
      "192 Train Loss 148.1329 Test RE 0.09275972572924067 c 1.0730038 k 4.88728 m 1.290034\n",
      "193 Train Loss 165.6012 Test RE 0.08084555808066686 c 1.0699025 k 4.8781605 m 1.2736397\n",
      "194 Train Loss 170.84613 Test RE 0.07026357466495206 c 1.0701287 k 4.872841 m 1.2773186\n",
      "195 Train Loss 166.27896 Test RE 0.06634055390076435 c 1.0721259 k 4.8704553 m 1.2863684\n",
      "196 Train Loss 190.40143 Test RE 0.06790661762369779 c 1.0738447 k 4.876339 m 1.2900591\n",
      "197 Train Loss 194.56854 Test RE 0.05997487097726834 c 1.0727926 k 4.861472 m 1.2884202\n",
      "198 Train Loss 193.25807 Test RE 0.056639150353430624 c 1.0755956 k 4.866753 m 1.2951654\n",
      "199 Train Loss 188.96677 Test RE 0.055162662486647876 c 1.0763849 k 4.8691173 m 1.2989831\n",
      "200 Train Loss 187.4365 Test RE 0.05644240780752933 c 1.0766828 k 4.869323 m 1.2995415\n",
      "201 Train Loss 185.45137 Test RE 0.055593391379261926 c 1.0787405 k 4.868742 m 1.3084756\n",
      "202 Train Loss 179.33345 Test RE 0.057105341549159964 c 1.0792115 k 4.8687725 m 1.311854\n",
      "203 Train Loss 176.35991 Test RE 0.0598152651932967 c 1.079944 k 4.8687196 m 1.3149765\n",
      "204 Train Loss 175.20618 Test RE 0.061114341270924626 c 1.081197 k 4.8681884 m 1.3209027\n",
      "205 Train Loss 172.15732 Test RE 0.06162963454362335 c 1.0812722 k 4.8667703 m 1.324802\n",
      "206 Train Loss 169.41704 Test RE 0.06240701369911264 c 1.0820832 k 4.864564 m 1.329731\n",
      "207 Train Loss 164.14807 Test RE 0.06356204075302002 c 1.0830914 k 4.8674207 m 1.3331435\n",
      "208 Train Loss 158.96835 Test RE 0.0660437004498473 c 1.0844446 k 4.872611 m 1.3400583\n",
      "209 Train Loss 156.16037 Test RE 0.06923143224027543 c 1.0851831 k 4.8695555 m 1.347576\n",
      "210 Train Loss 151.45718 Test RE 0.06994049207704683 c 1.0861459 k 4.8725476 m 1.3527571\n",
      "211 Train Loss 147.8729 Test RE 0.07156484753477103 c 1.0869484 k 4.8750815 m 1.3566554\n",
      "212 Train Loss 146.98605 Test RE 0.07399802490291499 c 1.087463 k 4.875687 m 1.3587775\n",
      "213 Train Loss 149.01343 Test RE 0.07399645011270532 c 1.0865898 k 4.873503 m 1.3614\n",
      "214 Train Loss 152.99481 Test RE 0.0714083307423599 c 1.0877627 k 4.8733463 m 1.3694245\n",
      "215 Train Loss 154.06966 Test RE 0.06827433685661652 c 1.0896877 k 4.875084 m 1.3794934\n",
      "216 Train Loss 149.47351 Test RE 0.06790808385921247 c 1.0897024 k 4.874427 m 1.3786759\n",
      "217 Train Loss 145.67816 Test RE 0.07050099967829837 c 1.0892942 k 4.872146 m 1.3766407\n",
      "218 Train Loss 156.72035 Test RE 0.07214558331824396 c 1.0875818 k 4.874228 m 1.3724802\n",
      "219 Train Loss 151.91321 Test RE 0.0662861300493738 c 1.08933 k 4.871434 m 1.3859453\n",
      "220 Train Loss 159.09615 Test RE 0.06755108262250428 c 1.0884113 k 4.8707485 m 1.3839928\n",
      "221 Train Loss 153.61583 Test RE 0.06326197578263486 c 1.0887153 k 4.8693023 m 1.3868476\n",
      "222 Train Loss 148.9828 Test RE 0.06722586686873386 c 1.0894397 k 4.873644 m 1.3887479\n",
      "223 Train Loss 143.2597 Test RE 0.07099333062520537 c 1.0905094 k 4.8803334 m 1.397282\n",
      "224 Train Loss 142.79062 Test RE 0.07413744823530843 c 1.0911225 k 4.8768115 m 1.405327\n",
      "225 Train Loss 141.04572 Test RE 0.07212435257628245 c 1.0927116 k 4.8709307 m 1.4144505\n",
      "226 Train Loss 147.21349 Test RE 0.07123596342743141 c 1.0949467 k 4.880414 m 1.4302354\n",
      "227 Train Loss 153.98643 Test RE 0.06501074090572777 c 1.0967678 k 4.8709745 m 1.4398277\n",
      "228 Train Loss 157.33476 Test RE 0.05973635400117778 c 1.0971943 k 4.873294 m 1.4429603\n",
      "229 Train Loss 155.19684 Test RE 0.05658899506328617 c 1.0960294 k 4.8655357 m 1.4399319\n",
      "230 Train Loss 157.12009 Test RE 0.057524755888663084 c 1.0956608 k 4.865012 m 1.440279\n",
      "231 Train Loss 152.84096 Test RE 0.056277056362043255 c 1.0957733 k 4.8683414 m 1.4430577\n",
      "232 Train Loss 149.9945 Test RE 0.05794782603610626 c 1.097116 k 4.8693943 m 1.4473431\n",
      "233 Train Loss 145.63367 Test RE 0.05989864420059275 c 1.097769 k 4.868507 m 1.450067\n",
      "234 Train Loss 141.53317 Test RE 0.06229623947234782 c 1.0992559 k 4.8682685 m 1.4551703\n",
      "235 Train Loss 135.90411 Test RE 0.06678343499365051 c 1.097494 k 4.871798 m 1.4587942\n",
      "236 Train Loss 133.23164 Test RE 0.06830958253499536 c 1.0968949 k 4.8748093 m 1.4604683\n",
      "237 Train Loss 135.55171 Test RE 0.06925618510297703 c 1.0969034 k 4.8820877 m 1.4609401\n",
      "238 Train Loss 131.16669 Test RE 0.070362122884361 c 1.0971352 k 4.874585 m 1.4628899\n",
      "239 Train Loss 131.67899 Test RE 0.07311348082312913 c 1.0970845 k 4.874473 m 1.4684178\n",
      "240 Train Loss 144.97336 Test RE 0.07264471573718592 c 1.0971308 k 4.8737793 m 1.4773616\n",
      "241 Train Loss 155.513 Test RE 0.06508623161239244 c 1.097971 k 4.8726044 m 1.4931256\n",
      "242 Train Loss 153.03943 Test RE 0.05729610572284056 c 1.1008036 k 4.8698945 m 1.5138259\n",
      "243 Train Loss 153.70796 Test RE 0.0573683559580247 c 1.1036881 k 4.8663893 m 1.5447469\n",
      "244 Train Loss 151.96236 Test RE 0.05617783486434941 c 1.1074054 k 4.8667965 m 1.5814143\n",
      "245 Train Loss 149.48744 Test RE 0.05661516277186933 c 1.1109307 k 4.868844 m 1.6103746\n",
      "246 Train Loss 189.94368 Test RE 0.054714776887225335 c 1.115496 k 4.86644 m 1.629405\n",
      "247 Train Loss 214.52788 Test RE 0.04901346696513203 c 1.1191752 k 4.8468456 m 1.6463766\n",
      "248 Train Loss 196.88983 Test RE 0.04208716608619777 c 1.1208547 k 4.8573217 m 1.6470227\n",
      "249 Train Loss 189.14075 Test RE 0.03967108464356044 c 1.1202801 k 4.883005 m 1.6234128\n",
      "250 Train Loss 184.85254 Test RE 0.03958394782403846 c 1.1206627 k 4.873819 m 1.6270515\n",
      "251 Train Loss 179.10684 Test RE 0.038930648486545004 c 1.1213137 k 4.8727508 m 1.6267341\n",
      "252 Train Loss 173.72719 Test RE 0.03933236756420444 c 1.120988 k 4.8726754 m 1.6269748\n",
      "253 Train Loss 170.11996 Test RE 0.04022640056667909 c 1.1213849 k 4.872925 m 1.6316794\n",
      "254 Train Loss 167.84958 Test RE 0.040933088268898264 c 1.1214141 k 4.875923 m 1.6279813\n",
      "255 Train Loss 174.5863 Test RE 0.04179045985846301 c 1.1223491 k 4.8708587 m 1.6288635\n",
      "256 Train Loss 170.36154 Test RE 0.03915159538613334 c 1.1228604 k 4.869181 m 1.628796\n",
      "257 Train Loss 167.6484 Test RE 0.040249998909186756 c 1.1247308 k 4.8703146 m 1.632794\n",
      "258 Train Loss 163.3559 Test RE 0.040559551842937414 c 1.1255411 k 4.865181 m 1.6359981\n",
      "259 Train Loss 160.7989 Test RE 0.041850783325543456 c 1.1257105 k 4.869675 m 1.6377727\n",
      "260 Train Loss 157.71658 Test RE 0.04142649079515293 c 1.126148 k 4.870467 m 1.6404468\n",
      "261 Train Loss 153.62776 Test RE 0.04339821661957716 c 1.1279173 k 4.867531 m 1.6543976\n",
      "262 Train Loss 153.21307 Test RE 0.04451377811683699 c 1.1291775 k 4.872293 m 1.6676941\n",
      "263 Train Loss 150.58717 Test RE 0.04462111271235751 c 1.1313121 k 4.875058 m 1.6819609\n",
      "264 Train Loss 146.18916 Test RE 0.045292129461035835 c 1.1319381 k 4.8700385 m 1.6880878\n",
      "265 Train Loss 143.62988 Test RE 0.04843282708919191 c 1.1350806 k 4.8753657 m 1.7142202\n",
      "266 Train Loss 142.61113 Test RE 0.04817464802212549 c 1.138891 k 4.872197 m 1.7402391\n",
      "267 Train Loss 149.12897 Test RE 0.04717351783803057 c 1.1426369 k 4.880458 m 1.7770042\n",
      "268 Train Loss 148.94879 Test RE 0.04330717108747285 c 1.1457782 k 4.8775163 m 1.8071282\n",
      "269 Train Loss 146.72675 Test RE 0.03981201696121792 c 1.1518303 k 4.8705225 m 1.84021\n",
      "270 Train Loss 144.07384 Test RE 0.0392280572843032 c 1.1541524 k 4.869584 m 1.8510975\n",
      "271 Train Loss 142.65222 Test RE 0.04054562456361126 c 1.1573476 k 4.8706 m 1.8707831\n",
      "272 Train Loss 139.44794 Test RE 0.03980431302365997 c 1.1602665 k 4.8680005 m 1.8882335\n",
      "273 Train Loss 141.12212 Test RE 0.038649022846046835 c 1.1612242 k 4.8768225 m 1.8898313\n",
      "274 Train Loss 139.7829 Test RE 0.03736558253938983 c 1.1664447 k 4.8668933 m 1.9200132\n",
      "275 Train Loss 136.27959 Test RE 0.03570800518249998 c 1.168749 k 4.876065 m 1.9356769\n",
      "276 Train Loss 134.18217 Test RE 0.03701325222280562 c 1.1725414 k 4.876203 m 1.9665741\n",
      "277 Train Loss 132.29985 Test RE 0.03693751205443732 c 1.1726159 k 4.8744597 m 1.9683809\n",
      "278 Train Loss 133.08812 Test RE 0.036760881859209345 c 1.173914 k 4.8690853 m 1.9764477\n",
      "279 Train Loss 131.70901 Test RE 0.03590411462055856 c 1.177213 k 4.868353 m 1.9999404\n",
      "280 Train Loss 130.78195 Test RE 0.03598581017319795 c 1.1801496 k 4.868185 m 2.0208197\n",
      "281 Train Loss 129.84628 Test RE 0.036612766651529106 c 1.1805391 k 4.8655443 m 2.025908\n",
      "282 Train Loss 128.78697 Test RE 0.03723518305421294 c 1.1808288 k 4.8688984 m 2.0290375\n",
      "283 Train Loss 126.76919 Test RE 0.03719768997208968 c 1.1821826 k 4.868333 m 2.044592\n",
      "284 Train Loss 124.859024 Test RE 0.03803462387566254 c 1.1840277 k 4.866925 m 2.0709257\n",
      "285 Train Loss 124.944885 Test RE 0.037872867747332624 c 1.1843221 k 4.8690577 m 2.0833113\n",
      "286 Train Loss 122.96761 Test RE 0.035223365514347404 c 1.1850992 k 4.8725114 m 2.1022594\n",
      "287 Train Loss 120.06497 Test RE 0.035835777536397995 c 1.1847905 k 4.8742104 m 2.106879\n",
      "288 Train Loss 119.297905 Test RE 0.03675767653944556 c 1.1855215 k 4.876752 m 2.1158226\n",
      "289 Train Loss 117.3246 Test RE 0.034972081820288145 c 1.1903386 k 4.868895 m 2.1472187\n",
      "290 Train Loss 114.66713 Test RE 0.036383907380348454 c 1.1963124 k 4.862992 m 2.1841726\n",
      "291 Train Loss 111.3273 Test RE 0.036413571778099035 c 1.198428 k 4.865612 m 2.2001536\n",
      "292 Train Loss 109.972595 Test RE 0.038923911726780634 c 1.1977834 k 4.865843 m 2.1984231\n",
      "293 Train Loss 107.85243 Test RE 0.03842922529628298 c 1.1979029 k 4.870565 m 2.199027\n",
      "294 Train Loss 105.61621 Test RE 0.04039467070766434 c 1.197446 k 4.8704515 m 2.1970747\n",
      "295 Train Loss 106.12824 Test RE 0.04334691340844814 c 1.1969622 k 4.87393 m 2.1967676\n",
      "296 Train Loss 107.3485 Test RE 0.04231896849739705 c 1.1986489 k 4.8608685 m 2.2099056\n",
      "297 Train Loss 105.93884 Test RE 0.04001038914778461 c 1.1989632 k 4.868545 m 2.2114716\n",
      "298 Train Loss 103.716324 Test RE 0.04149749188371644 c 1.2001034 k 4.865194 m 2.2221396\n",
      "299 Train Loss 124.17197 Test RE 0.041158973109685214 c 1.2034386 k 4.856386 m 2.260899\n",
      "300 Train Loss 115.95056 Test RE 0.030360504740113196 c 1.2027411 k 4.875161 m 2.2587154\n",
      "301 Train Loss 112.58052 Test RE 0.030519610139052623 c 1.2007478 k 4.8686843 m 2.2482028\n",
      "302 Train Loss 111.087875 Test RE 0.03249316929470723 c 1.1999431 k 4.8695436 m 2.2480936\n",
      "303 Train Loss 109.81961 Test RE 0.033630344028958835 c 1.1991016 k 4.878486 m 2.2450948\n",
      "304 Train Loss 107.48992 Test RE 0.033912840575519104 c 1.1991824 k 4.8807726 m 2.2474258\n",
      "305 Train Loss 105.330925 Test RE 0.03607192284216743 c 1.1994889 k 4.8787465 m 2.2499409\n",
      "306 Train Loss 105.58014 Test RE 0.03778859282842447 c 1.1999177 k 4.878495 m 2.2513607\n",
      "307 Train Loss 105.37464 Test RE 0.03702724263385293 c 1.2038617 k 4.8794093 m 2.264072\n",
      "308 Train Loss 105.44027 Test RE 0.037688477094551 c 1.204957 k 4.8768826 m 2.2726672\n",
      "309 Train Loss 104.23707 Test RE 0.03725671208543553 c 1.2057691 k 4.879374 m 2.2828922\n",
      "310 Train Loss 111.69164 Test RE 0.036548281378960566 c 1.2128551 k 4.8845043 m 2.3282425\n",
      "311 Train Loss 107.64113 Test RE 0.03244227182961922 c 1.2140396 k 4.8837843 m 2.3387988\n",
      "312 Train Loss 104.86598 Test RE 0.03422217425842006 c 1.2156047 k 4.8852296 m 2.3543046\n",
      "313 Train Loss 102.36175 Test RE 0.03551792800824322 c 1.2179505 k 4.888769 m 2.373251\n",
      "314 Train Loss 106.95727 Test RE 0.03621626817467869 c 1.2217491 k 4.880482 m 2.4017906\n",
      "315 Train Loss 108.96985 Test RE 0.032014389897337805 c 1.2250665 k 4.8801584 m 2.4232278\n",
      "316 Train Loss 113.49037 Test RE 0.03109104204777309 c 1.227348 k 4.879205 m 2.4376442\n",
      "317 Train Loss 111.68384 Test RE 0.02841563633116 c 1.2312686 k 4.878778 m 2.4630556\n",
      "318 Train Loss 108.211624 Test RE 0.028258521842235672 c 1.2325852 k 4.8840504 m 2.4722965\n",
      "319 Train Loss 106.72995 Test RE 0.028244458662952058 c 1.233763 k 4.893688 m 2.4835167\n",
      "320 Train Loss 103.62423 Test RE 0.02590776252806526 c 1.2345728 k 4.889139 m 2.489099\n",
      "321 Train Loss 100.134254 Test RE 0.027521687801346045 c 1.2362753 k 4.8842883 m 2.4960132\n",
      "322 Train Loss 98.2335 Test RE 0.027709040086046778 c 1.2392491 k 4.88574 m 2.5091603\n",
      "323 Train Loss 96.51996 Test RE 0.026427818168032077 c 1.2393668 k 4.879854 m 2.5097575\n",
      "324 Train Loss 94.68536 Test RE 0.029535882482124094 c 1.2399696 k 4.8793797 m 2.512688\n",
      "325 Train Loss 92.110596 Test RE 0.02659653553527326 c 1.240056 k 4.881399 m 2.513095\n",
      "326 Train Loss 90.57378 Test RE 0.028186148289915863 c 1.239809 k 4.889958 m 2.5135171\n",
      "327 Train Loss 89.70155 Test RE 0.028252729423387676 c 1.2415141 k 4.8835397 m 2.5259318\n",
      "328 Train Loss 87.82238 Test RE 0.028753764104155744 c 1.243531 k 4.890604 m 2.5430672\n",
      "329 Train Loss 86.69598 Test RE 0.02917145895535118 c 1.2465602 k 4.8808765 m 2.5636933\n",
      "330 Train Loss 84.52586 Test RE 0.029619447809517363 c 1.2512398 k 4.8861203 m 2.592197\n",
      "331 Train Loss 83.08302 Test RE 0.03105511964430435 c 1.2537099 k 4.881431 m 2.6061063\n",
      "332 Train Loss 82.62787 Test RE 0.033088753770323505 c 1.2532121 k 4.8814607 m 2.6028352\n",
      "333 Train Loss 82.126366 Test RE 0.033761788175213844 c 1.2536447 k 4.882442 m 2.6076984\n",
      "334 Train Loss 81.10971 Test RE 0.034263428503162205 c 1.2529063 k 4.8816795 m 2.6057358\n",
      "335 Train Loss 79.84007 Test RE 0.036182911839278846 c 1.2518692 k 4.8815885 m 2.6003022\n",
      "336 Train Loss 79.837265 Test RE 0.03892411383582555 c 1.2530603 k 4.885794 m 2.616637\n",
      "337 Train Loss 78.350136 Test RE 0.03886314797356163 c 1.2516577 k 4.887925 m 2.6140027\n",
      "338 Train Loss 80.45102 Test RE 0.04131045006329731 c 1.2523518 k 4.881515 m 2.621435\n",
      "339 Train Loss 82.852936 Test RE 0.03706317954842779 c 1.2562232 k 4.8922276 m 2.6502094\n",
      "340 Train Loss 82.50022 Test RE 0.032503575811665984 c 1.2578217 k 4.8866615 m 2.661768\n",
      "341 Train Loss 80.56761 Test RE 0.03214476050779697 c 1.2578604 k 4.888668 m 2.6631389\n",
      "342 Train Loss 79.51963 Test RE 0.033531778081503344 c 1.2577798 k 4.8876643 m 2.6639683\n",
      "343 Train Loss 77.71347 Test RE 0.0348704944722031 c 1.2571898 k 4.885163 m 2.6659732\n",
      "344 Train Loss 75.67668 Test RE 0.0364106917464141 c 1.2563727 k 4.886015 m 2.663952\n",
      "345 Train Loss 74.11523 Test RE 0.03898809066693936 c 1.2560478 k 4.887687 m 2.6658592\n",
      "346 Train Loss 72.57005 Test RE 0.040326225189811195 c 1.2560865 k 4.8850393 m 2.6670935\n",
      "347 Train Loss 71.53575 Test RE 0.042468686661765294 c 1.2562727 k 4.884369 m 2.6708157\n",
      "348 Train Loss 70.74611 Test RE 0.04363534257315909 c 1.256469 k 4.8834066 m 2.675371\n",
      "349 Train Loss 71.46015 Test RE 0.04512961389650705 c 1.2567207 k 4.8835764 m 2.6871223\n",
      "350 Train Loss 74.146706 Test RE 0.043289658594872694 c 1.257803 k 4.88809 m 2.7028754\n",
      "351 Train Loss 72.52802 Test RE 0.04058201237331024 c 1.258748 k 4.891455 m 2.7115428\n",
      "352 Train Loss 70.474884 Test RE 0.04248191003228676 c 1.2594167 k 4.8864923 m 2.7165904\n",
      "353 Train Loss 68.96226 Test RE 0.044750314594250114 c 1.260239 k 4.8851767 m 2.727295\n",
      "354 Train Loss 67.0067 Test RE 0.04593325257622872 c 1.261301 k 4.880176 m 2.7372663\n",
      "355 Train Loss 68.09698 Test RE 0.048126531555474715 c 1.2621092 k 4.879403 m 2.7485764\n",
      "356 Train Loss 69.94484 Test RE 0.04703463691855144 c 1.2626461 k 4.8827376 m 2.7561812\n",
      "357 Train Loss 68.3886 Test RE 0.0446054316210236 c 1.2634856 k 4.8891425 m 2.7673843\n",
      "358 Train Loss 66.8808 Test RE 0.046968423100683096 c 1.2660663 k 4.887219 m 2.7911742\n",
      "359 Train Loss 68.26332 Test RE 0.04826520141797161 c 1.2668352 k 4.8851647 m 2.7968948\n",
      "360 Train Loss 66.14306 Test RE 0.04638446427800526 c 1.266967 k 4.887471 m 2.785999\n",
      "361 Train Loss 65.022446 Test RE 0.04927853569965981 c 1.2685177 k 4.884866 m 2.7988126\n",
      "362 Train Loss 65.34413 Test RE 0.04901846094934723 c 1.2702469 k 4.882468 m 2.8069854\n",
      "363 Train Loss 70.55729 Test RE 0.04822892049072534 c 1.2706711 k 4.885428 m 2.8017087\n",
      "364 Train Loss 68.56831 Test RE 0.04784045626697805 c 1.2716072 k 4.882876 m 2.8079453\n",
      "365 Train Loss 72.92325 Test RE 0.044168134844169475 c 1.274804 k 4.8768096 m 2.8215969\n",
      "366 Train Loss 72.37042 Test RE 0.03953867093218607 c 1.276961 k 4.881503 m 2.8333788\n",
      "367 Train Loss 82.44206 Test RE 0.03730700365397456 c 1.2773254 k 4.8925314 m 2.8323689\n",
      "368 Train Loss 79.84747 Test RE 0.034147320226370355 c 1.2762475 k 4.8932295 m 2.8193717\n",
      "369 Train Loss 78.1292 Test RE 0.031251622674680506 c 1.2772064 k 4.896483 m 2.825981\n",
      "370 Train Loss 76.28374 Test RE 0.030905769444265943 c 1.2760805 k 4.897468 m 2.8179975\n",
      "371 Train Loss 75.52836 Test RE 0.03140064099620842 c 1.2768338 k 4.892496 m 2.823696\n",
      "372 Train Loss 75.10991 Test RE 0.03067476279215272 c 1.2782311 k 4.89948 m 2.8352165\n",
      "373 Train Loss 73.35032 Test RE 0.03031331660428718 c 1.2799695 k 4.8997545 m 2.8494458\n",
      "374 Train Loss 71.38489 Test RE 0.030982161744861857 c 1.2823879 k 4.9015193 m 2.870657\n",
      "375 Train Loss 69.63522 Test RE 0.032164910100042486 c 1.2827699 k 4.897624 m 2.8733213\n",
      "376 Train Loss 69.0081 Test RE 0.0334289438873567 c 1.2841902 k 4.8932266 m 2.8857136\n",
      "377 Train Loss 67.34581 Test RE 0.03457542639321464 c 1.2849708 k 4.893402 m 2.8931735\n",
      "378 Train Loss 66.5754 Test RE 0.035975928771706175 c 1.2843206 k 4.894383 m 2.8923154\n",
      "379 Train Loss 66.63941 Test RE 0.03579654878747973 c 1.2833813 k 4.8920083 m 2.8934164\n",
      "380 Train Loss 67.83892 Test RE 0.03709074747566616 c 1.2856858 k 4.8997006 m 2.9279895\n",
      "381 Train Loss 64.87146 Test RE 0.034897430607716835 c 1.2887937 k 4.9077964 m 2.9796443\n",
      "382 Train Loss 63.701187 Test RE 0.035794178918122074 c 1.290525 k 4.8948894 m 3.0103097\n",
      "383 Train Loss 60.314713 Test RE 0.035942458994788436 c 1.2923913 k 4.895123 m 3.0436423\n",
      "384 Train Loss 62.858013 Test RE 0.03808048216138362 c 1.2941588 k 4.909347 m 3.0846715\n",
      "385 Train Loss 66.87574 Test RE 0.036497191615856996 c 1.302009 k 4.9050903 m 3.1801891\n",
      "386 Train Loss 97.305466 Test RE 0.03631958606214182 c 1.3044765 k 4.9099455 m 3.2047043\n",
      "387 Train Loss 100.68649 Test RE 0.036415839147049364 c 1.3048501 k 4.9141684 m 3.2081068\n",
      "388 Train Loss 91.86107 Test RE 0.03295783856029151 c 1.3068975 k 4.9352326 m 3.222816\n",
      "389 Train Loss 86.68733 Test RE 0.028480032444506444 c 1.3083287 k 4.9213276 m 3.2258801\n",
      "390 Train Loss 89.7351 Test RE 0.02755080495536748 c 1.309246 k 4.934482 m 3.2334561\n",
      "391 Train Loss 85.88439 Test RE 0.02658794554866384 c 1.313081 k 4.926943 m 3.2618876\n",
      "392 Train Loss 107.16113 Test RE 0.02678233098487598 c 1.3164548 k 4.917314 m 3.2864387\n",
      "393 Train Loss 116.32218 Test RE 0.025928894435323896 c 1.3172177 k 4.9332833 m 3.2877111\n",
      "394 Train Loss 151.98566 Test RE 0.024909820014875194 c 1.3193821 k 4.924292 m 3.301014\n",
      "395 Train Loss 153.09798 Test RE 0.02410070125895822 c 1.3207855 k 4.9334435 m 3.3084722\n",
      "396 Train Loss 151.5764 Test RE 0.023660436029552246 c 1.3210166 k 4.937024 m 3.3085692\n",
      "397 Train Loss 135.07976 Test RE 0.022870272443938473 c 1.322191 k 4.9615035 m 3.3090627\n",
      "398 Train Loss 119.79919 Test RE 0.021980364945586788 c 1.3240209 k 4.958793 m 3.3203075\n",
      "399 Train Loss 109.628204 Test RE 0.02131716918623962 c 1.3270098 k 4.9259744 m 3.3457732\n",
      "400 Train Loss 103.226456 Test RE 0.021293180877990266 c 1.3268273 k 4.9287915 m 3.34651\n",
      "401 Train Loss 96.98386 Test RE 0.021452600257725482 c 1.3270924 k 4.921755 m 3.3494484\n",
      "402 Train Loss 92.61301 Test RE 0.021353621459519013 c 1.3281349 k 4.928539 m 3.3601441\n",
      "403 Train Loss 90.1402 Test RE 0.021334349249721763 c 1.3277752 k 4.935968 m 3.3609347\n",
      "404 Train Loss 88.47595 Test RE 0.02125214713811187 c 1.3281015 k 4.931114 m 3.3622506\n",
      "405 Train Loss 89.879196 Test RE 0.021248858181647373 c 1.3281009 k 4.9310837 m 3.3622499\n",
      "406 Train Loss 86.68521 Test RE 0.021262411881407244 c 1.3279134 k 4.9254856 m 3.3613741\n",
      "407 Train Loss 84.771614 Test RE 0.021248417302226204 c 1.328085 k 4.9267426 m 3.3633616\n",
      "408 Train Loss 81.933655 Test RE 0.021299080538460472 c 1.3284183 k 4.9255953 m 3.3666506\n",
      "409 Train Loss 78.919525 Test RE 0.021299080538460472 c 1.3284183 k 4.9255953 m 3.3666506\n",
      "410 Train Loss 78.97973 Test RE 0.02130630409855822 c 1.3284274 k 4.9221897 m 3.3670876\n",
      "411 Train Loss 75.10872 Test RE 0.021211402127692534 c 1.3290417 k 4.9209228 m 3.3725266\n",
      "412 Train Loss 78.626144 Test RE 0.0210326041189411 c 1.3301578 k 4.926605 m 3.3864338\n",
      "413 Train Loss 113.57427 Test RE 0.020657781807849704 c 1.3308791 k 4.9300203 m 3.3979616\n",
      "414 Train Loss 106.91871 Test RE 0.020522930835335143 c 1.3311473 k 4.93232 m 3.4019935\n",
      "415 Train Loss 106.79016 Test RE 0.020445862279617262 c 1.3314105 k 4.928999 m 3.405543\n",
      "416 Train Loss 106.880356 Test RE 0.020445862279617262 c 1.3314105 k 4.928999 m 3.405543\n",
      "417 Train Loss 103.57707 Test RE 0.02043425886481153 c 1.331392 k 4.9287786 m 3.4056895\n",
      "418 Train Loss 101.88002 Test RE 0.02043425886481153 c 1.331392 k 4.9287786 m 3.4056895\n",
      "419 Train Loss 101.161194 Test RE 0.02043425886481153 c 1.331392 k 4.9287786 m 3.4056895\n",
      "420 Train Loss 115.377525 Test RE 0.02045372346744958 c 1.3310865 k 4.9310713 m 3.404122\n",
      "421 Train Loss 108.136826 Test RE 0.02041645211138449 c 1.3309848 k 4.9317784 m 3.4048178\n",
      "422 Train Loss 103.059685 Test RE 0.02041645211138449 c 1.3309848 k 4.9317784 m 3.4048178\n",
      "423 Train Loss 99.82568 Test RE 0.02041645211138449 c 1.3309848 k 4.9317784 m 3.4048178\n",
      "424 Train Loss 95.87428 Test RE 0.020460977789508804 c 1.3308705 k 4.931206 m 3.404095\n",
      "425 Train Loss 91.99129 Test RE 0.020460977789508804 c 1.3308705 k 4.931206 m 3.404095\n",
      "426 Train Loss 88.07025 Test RE 0.020460977789508804 c 1.3308705 k 4.931206 m 3.404095\n",
      "427 Train Loss 83.486115 Test RE 0.020518858161845118 c 1.330747 k 4.9293494 m 3.4035645\n",
      "428 Train Loss 80.58081 Test RE 0.020518858161845118 c 1.330747 k 4.9293494 m 3.4035645\n",
      "429 Train Loss 76.8356 Test RE 0.020573123436295387 c 1.3306882 k 4.928504 m 3.4034088\n",
      "430 Train Loss 74.46338 Test RE 0.020585274123123637 c 1.3306619 k 4.928296 m 3.403375\n",
      "431 Train Loss 73.38531 Test RE 0.020585274123123637 c 1.3306619 k 4.928296 m 3.403375\n",
      "432 Train Loss 70.425156 Test RE 0.020707897649101588 c 1.3305327 k 4.927655 m 3.4033306\n",
      "433 Train Loss 72.78834 Test RE 0.020789404122819606 c 1.3310605 k 4.929068 m 3.4128654\n",
      "434 Train Loss 69.69863 Test RE 0.0208211713525314 c 1.3321034 k 4.9364448 m 3.4286447\n",
      "435 Train Loss 68.23443 Test RE 0.02085839070006008 c 1.3324685 k 4.9362216 m 3.4387994\n",
      "436 Train Loss 65.45987 Test RE 0.02086185122617246 c 1.3327107 k 4.933651 m 3.4413607\n",
      "437 Train Loss 63.766396 Test RE 0.02080517864697603 c 1.3329662 k 4.934008 m 3.4499907\n",
      "438 Train Loss 61.775337 Test RE 0.021020230121045574 c 1.333017 k 4.934404 m 3.4548101\n",
      "439 Train Loss 76.0686 Test RE 0.021138052412221366 c 1.3332984 k 4.9318366 m 3.4642391\n",
      "440 Train Loss nan Test RE nan c nan k nan m nan\n",
      "441 Train Loss nan Test RE nan c nan k nan m nan\n",
      "442 Train Loss nan Test RE nan c nan k nan m nan\n",
      "443 Train Loss nan Test RE nan c nan k nan m nan\n",
      "444 Train Loss nan Test RE nan c nan k nan m nan\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_8628/4164427203.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 43\u001b[0;31m   \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mreps\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_8628/4223980623.py\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(max_iter, rep)\u001b[0m\n\u001b[1;32m     10\u001b[0m     \u001b[0mf_hat\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_coll\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmax_iter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m         \u001b[0mtrain_step\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_bc1_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_bc1_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt_bc2_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbc2_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt_coll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m         \u001b[0mloss_np\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPINN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_bc1_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_bc1_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt_bc2_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbc2_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt_coll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcpu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdetach\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnumpy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_8628/4151770434.py\u001b[0m in \u001b[0;36mtrain_step\u001b[0;34m(t_bc1_train, x_bc1_train, t_bc2_train, bc2_val, t_coll, f_hat)\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m     \u001b[0mPINN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlambda_update\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_bc1_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_bc1_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt_bc2_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbc2_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt_coll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/optimizer.py\u001b[0m in \u001b[0;36mwrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     86\u001b[0m                 \u001b[0mprofile_name\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"Optimizer.step#{}.step\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     87\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprofiler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrecord_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprofile_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 88\u001b[0;31m                     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     89\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mwrapper\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mstep\u001b[0;34m(self, closure)\u001b[0m\n\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                     loss, flat_grad, t, ls_func_evals = _strong_wolfe(\n\u001b[0;32m--> 426\u001b[0;31m                         obj_func, x_init, t, d, loss, flat_grad, gtd)\n\u001b[0m\u001b[1;32m    427\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0mopt_cond\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mflat_grad\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mabs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m<=\u001b[0m \u001b[0mtolerance_grad\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36m_strong_wolfe\u001b[0;34m(obj_func, x, t, d, f, g, gtd, c1, c2, tolerance_change, max_ls)\u001b[0m\n\u001b[1;32m     96\u001b[0m         \u001b[0mg_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclone\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmemory_format\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontiguous_format\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m         \u001b[0mgtd_prev\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgtd_new\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 98\u001b[0;31m         \u001b[0mf_new\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mg_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mobj_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     99\u001b[0m         \u001b[0mls_func_evals\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m         \u001b[0mgtd_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mg_new\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36mobj_func\u001b[0;34m(x, t, d)\u001b[0m\n\u001b[1;32m    421\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    422\u001b[0m                     \u001b[0;32mdef\u001b[0m \u001b[0mobj_func\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 423\u001b[0;31m                         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_directional_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    424\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m                     loss, flat_grad, t, ls_func_evals = _strong_wolfe(\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/optim/lbfgs.py\u001b[0m in \u001b[0;36m_directional_evaluate\u001b[0;34m(self, closure, x, t, d)\u001b[0m\n\u001b[1;32m    275\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_directional_evaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mclosure\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    276\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_add_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0md\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 277\u001b[0;31m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfloat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mclosure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    278\u001b[0m         \u001b[0mflat_grad\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_gather_flat_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    279\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_set_param\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/grad_mode.py\u001b[0m in \u001b[0;36mdecorate_context\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     26\u001b[0m         \u001b[0;32mdef\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     27\u001b[0m             \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 28\u001b[0;31m                 \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     29\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mcast\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mF\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdecorate_context\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     30\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_8628/4151770434.py\u001b[0m in \u001b[0;36mclosure\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m         \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPINN\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mt_bc1_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mx_bc1_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt_bc2_train\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mbc2_val\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mt_coll\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mf_hat\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mloss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    253\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 255\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    147\u001b[0m     Variable._execution_engine.run_backward(\n\u001b[1;32m    148\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 149\u001b[0;31m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n\u001b[0m\u001b[1;32m    150\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    151\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_reps = 1\n",
    "max_iter = 500\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "\n",
    "for reps in range(max_reps):   \n",
    "  train_loss = []\n",
    "  test_mse_loss = []\n",
    "  test_re_loss = []   \n",
    "  beta_val = []\n",
    "\n",
    "  torch.manual_seed(reps*36)\n",
    "  N_f = 10000 #Total number of collocation points\n",
    "\n",
    "  layers = np.array([1,50,50,50,1]) #9 hidden layers\n",
    "  PINN = Sequentialmodel(layers)\n",
    "\n",
    "  PINN.to(device)\n",
    "\n",
    "  'Neural Network Summary'\n",
    "  print(PINN)\n",
    "\n",
    "  params = list(PINN.parameters())\n",
    "\n",
    "  optimizer = torch.optim.LBFGS(PINN.parameters(), lr=1, \n",
    "                            max_iter = 10, \n",
    "                            max_eval = 15, \n",
    "                            tolerance_grad = 1e-6, \n",
    "                            tolerance_change = 1e-6, \n",
    "                            history_size = 100, \n",
    "                            line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "  train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "  torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "  train_loss_full.append(train_loss)\n",
    "  test_mse_full.append(test_mse_loss)\n",
    "  test_re_full.append(test_re_loss)\n",
    "  beta_full.append(beta_val)\n",
    "\n",
    "\n",
    "  print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"beta\": beta_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "pmHEeBpzfFQh",
    "outputId": "990b2054-f35e-4c8e-c378-69ff2eb19f4f"
   },
   "outputs": [],
   "source": [
    "x_pred = PINN.test()\n",
    "plt.plot(t,x_true,'b')\n",
    "plt.plot(t,x_pred,'r--')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "stan_1D_FODE_tune.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
