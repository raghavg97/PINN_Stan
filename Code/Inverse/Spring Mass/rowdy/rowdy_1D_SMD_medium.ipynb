{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8byrnUmNKGR",
    "outputId": "510f67c6-b225-41ba-e78a-d0a6e54ded85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uoNYpDzlNKGV"
   },
   "outputs": [],
   "source": [
    "tstart= 0 \n",
    "tstop= 60 \n",
    "\n",
    "increment = 0.1 \n",
    "# Initial condition\n",
    "x_init= [0,0] \n",
    "\n",
    "t = np.arange(tstart,tstop+1,increment)\n",
    "\n",
    "c = 1 # Damping constant \n",
    "k = 0.05 # Stiffness of the spring\n",
    "m = 5 # Mass \n",
    "F0 = 100\n",
    "\n",
    "def mydiff(x, t):    \n",
    "    F =F0\n",
    "    \n",
    "    dx1dt = x[1] \n",
    "    dx2dt = (F -c*x[1] -k*x[0])/m\n",
    "    dxdt= [dx1dt, dx2dt] \n",
    "    \n",
    "    return dxdt \n",
    "\n",
    "x_full_sol = odeint(mydiff, x_init, t) \n",
    "\n",
    "x_sol = x_full_sol[:,0]\n",
    "v_sol = x_full_sol[:,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BR02v-fkNKGV"
   },
   "outputs": [],
   "source": [
    "level = \"medium\"\n",
    "label = \"1D_SMD_rowdy_\" + level\n",
    "\n",
    "loss_thresh = 0.005\n",
    "\n",
    "t_test = t.reshape(-1,1)\n",
    "t_test_tensor = torch.from_numpy(t_test).float().to(device)\n",
    "\n",
    "# bc1_t = t[0].reshape(-1,1)\n",
    "# bc1_x = x_sol[0].reshape(-1,1)\n",
    "# t_bc1_train = torch.from_numpy(bc1_t).float().to(device)\n",
    "# x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "\n",
    "bc1_t = t.reshape(-1,1)\n",
    "bc1_x = x_sol.reshape(-1,1)\n",
    "t_bc1_train = torch.from_numpy(bc1_t).float().to(device)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "\n",
    "\n",
    "bc2_t = t[0].reshape(-1,1)\n",
    "t_bc2_train = torch.from_numpy(bc2_t).float().to(device)\n",
    "bc2_val = v_sol[0].reshape(-1,1)\n",
    "bc2_val =torch.from_numpy(bc2_val).float().to(device)\n",
    "\n",
    "t_test = t.reshape(-1,1)\n",
    "t_test_tensor = torch.from_numpy(t_test).float().to(device)\n",
    "x_true = x_sol\n",
    "x_true_norm = np.linalg.norm(x_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(t[0]) \n",
    "ub = np.array(t[-1]) \n",
    "\n",
    "#torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SyyktBKBXRo1"
   },
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    \n",
    "    t01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=t01,random_state =seed)\n",
    "\n",
    "    t_coll = lb + (ub-lb)*sampling(N_f)\n",
    "    # t_coll = np.vstack((t_coll,)) # append training points to collocation points \n",
    "\n",
    "    return t_coll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "o1b21zLnNKGW"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers,n_val,rowdy_terms):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "            \n",
    "        self.k = Parameter(torch.tensor(0.0))\n",
    "        self.k.requiresGrad = True\n",
    "        self.c = Parameter(torch.tensor(0.0))\n",
    "        self.c.requiresGrad = True\n",
    "        self.m = Parameter(torch.tensor(0.0))\n",
    "        self.m.requiresGrad = True\n",
    "        \n",
    "        \n",
    "        self.omega1 = Parameter(torch.ones((len(layers)-2,1))) \n",
    "        \n",
    "        \n",
    "        self.alpha = Parameter(torch.zeros(rowdy_terms,len(layers)-2))\n",
    "        self.omega = Parameter((1/n_val)*torch.ones(rowdy_terms,len(layers)-2))\n",
    "        \n",
    "        self.n = torch.tensor(n_val)\n",
    "    \n",
    "    'forward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = 2.0*(x - l_b)/(u_b - l_b)-1.0  #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(self.omega1[i,0]*z)\n",
    "            for j in range(rowdy_terms):\n",
    "                a = a + self.alpha[j,i]*self.n*torch.sin((j+1)*self.n*self.omega[j,i]*z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,t,x):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(t), x)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,t_bc2,bc2_val):\n",
    "        g = t_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        x = self.forward(g)    \n",
    "            \n",
    "        x_t = autograd.grad(x,g,torch.ones([t_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dx_dt = x_t[:,[0]]\n",
    "        \n",
    "        bc2 = dx_dt\n",
    "        \n",
    "        loss_bc2= self.loss_function(bc2,bc2_val)\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self,t_coll,f_hat):\n",
    "             \n",
    "        g = t_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        x = self.forward(g) \n",
    "\n",
    "        x_t = autograd.grad(x,g,torch.ones([t_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        x_tt = autograd.grad(x_t,g,torch.ones(t_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dx_dt = x_t[:,[0]]\n",
    "        \n",
    "        dx2_d2t = x_tt[:,[0]]\n",
    "        \n",
    "        f = self.m*dx2_d2t + self.c*dx_dt + self.k*x - F0\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,t_bc1,x_bc1,t_bc2,bc2_val,t_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(t_bc1,x_bc1)\n",
    "        loss_bc2 = self.loss_BC2(t_bc2,bc2_val)\n",
    "        loss_f = self.loss_PDE(t_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self):\n",
    "        x_pred = self.forward(t_test_tensor)\n",
    "        x_pred = x_pred.cpu().detach().numpy()\n",
    "\n",
    "        return x_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        x_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(x_pred.reshape(-1,1) - x_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(x_pred.reshape(-1,1) - x_true.reshape(-1,1),2)/x_true_norm\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fLY2mT5BOgjD"
   },
   "outputs": [],
   "source": [
    "def train_step(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8srA5uGuObil"
   },
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    m_val.append(PINN.m.cpu().detach().numpy())\n",
    "    k_val.append(PINN.k.cpu().detach().numpy())\n",
    "    c_val.append(PINN.c.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0SezTZ_racQB"
   },
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "\n",
    "    t_coll = colloc_pts(N_f,rep*11)\n",
    "    t_coll =  torch.from_numpy(t_coll).float().to(device)\n",
    "    \n",
    "    f_hat = torch.zeros(t_coll.shape[0],1).to(device)\n",
    "    for i in range(max_iter):      \n",
    "        train_step(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat)\n",
    "         \n",
    "        loss_np = PINN.loss(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test RE\",test_re_loss[-1],\"c\",PINN.c.cpu().detach().numpy(),\"k\",PINN.k.cpu().detach().numpy(),\"m\",PINN.m.cpu().detach().numpy())\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I9BYbcJ0NKGX",
    "outputId": "42720d97-b37a-4c42-b7e5-b77a0ef391f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 2138102.5 Test RE 0.9797258699387172 c -0.0011257998 k 0.02692541 m -2.2398044e-06\n",
      "1 Train Loss 1784436.2 Test RE 0.8954575345911722 c -0.0026327078 k 0.11202929 m -9.4383095e-06\n",
      "2 Train Loss 1129638.2 Test RE 0.7134750153215239 c -0.005017601 k 0.26702183 m -1.9384874e-05\n",
      "3 Train Loss 474390.88 Test RE 0.42391824412519813 c -0.006431418 k 0.36398092 m -2.4383784e-05\n",
      "4 Train Loss 280992.62 Test RE 0.27582510748264105 c -0.0068173613 k 0.36656496 m -2.002819e-05\n",
      "5 Train Loss 72950.63 Test RE 0.12692436008848781 c -0.009446953 k 0.20490119 m 0.00010449957\n",
      "6 Train Loss 9201.298 Test RE 0.05851017204605515 c -0.011347314 k 0.06432652 m 0.00023420162\n",
      "7 Train Loss 3957.0583 Test RE 0.03159434645874086 c -0.01147268 k 0.058800124 m 0.0002667721\n",
      "8 Train Loss 2757.7024 Test RE 0.02102997912548838 c -0.011425082 k 0.059862193 m 0.00031012206\n",
      "9 Train Loss 2421.0083 Test RE 0.01553857823840516 c -0.01125447 k 0.05984913 m 0.00035482142\n",
      "10 Train Loss 2270.4297 Test RE 0.01150553994123426 c -0.010957381 k 0.061248142 m 0.0003838553\n",
      "11 Train Loss 2261.9446 Test RE 0.011598202807182097 c -0.010846202 k 0.061534796 m 0.00039074107\n",
      "12 Train Loss 2236.5154 Test RE 0.011617937389315955 c -0.0102504 k 0.06125795 m 0.0004395115\n",
      "13 Train Loss 2162.3691 Test RE 0.01033803361124878 c -0.008358519 k 0.061113432 m 0.00058410136\n",
      "14 Train Loss 2129.9595 Test RE 0.0097096241610174 c -0.0071070446 k 0.06124075 m 0.0006835534\n",
      "15 Train Loss 2112.321 Test RE 0.009266710533487535 c -0.005055275 k 0.06110739 m 0.0008502819\n",
      "16 Train Loss 2085.9429 Test RE 0.008721932897292015 c -0.0022260733 k 0.061206084 m 0.0010643826\n",
      "17 Train Loss 2058.2578 Test RE 0.009047206390625557 c 0.00021244059 k 0.061808437 m 0.0012455485\n",
      "18 Train Loss 2037.9142 Test RE 0.008993803293125689 c 0.0031889966 k 0.06280662 m 0.0014832278\n",
      "19 Train Loss 2008.361 Test RE 0.00821974401978174 c 0.006603695 k 0.061857793 m 0.0017640345\n",
      "20 Train Loss 1980.2916 Test RE 0.008210638163118735 c 0.008639224 k 0.059433073 m 0.0019300566\n",
      "21 Train Loss 1902.8271 Test RE 0.007583340892179595 c 0.014196027 k 0.058656577 m 0.0023381733\n",
      "22 Train Loss 1853.013 Test RE 0.006266964766305033 c 0.017360805 k 0.06097386 m 0.0025614263\n",
      "23 Train Loss 1833.7903 Test RE 0.006013351507963156 c 0.022238988 k 0.0610429 m 0.0029142094\n",
      "24 Train Loss 1805.8776 Test RE 0.005964770940387325 c 0.027466152 k 0.059224017 m 0.003284702\n",
      "25 Train Loss 1785.454 Test RE 0.0056788463293809465 c 0.028907215 k 0.060412224 m 0.0033650373\n",
      "26 Train Loss 1776.5244 Test RE 0.005423876340687319 c 0.029189197 k 0.060556334 m 0.0033658266\n",
      "27 Train Loss 1768.0076 Test RE 0.0053118277642648926 c 0.031908516 k 0.05945796 m 0.0035703024\n",
      "28 Train Loss 1741.7928 Test RE 0.005104956581772236 c 0.038806364 k 0.059660926 m 0.004128435\n",
      "29 Train Loss 1725.9917 Test RE 0.004843026900750946 c 0.042708557 k 0.06124356 m 0.004441933\n",
      "30 Train Loss 1699.5928 Test RE 0.004663894870515727 c 0.050844528 k 0.059484262 m 0.0050833584\n",
      "31 Train Loss 1677.2704 Test RE 0.004307268607933863 c 0.05618046 k 0.06024205 m 0.0054840874\n",
      "32 Train Loss 1656.3801 Test RE 0.003824357838453306 c 0.061030366 k 0.06030792 m 0.0058482247\n",
      "33 Train Loss 1634.322 Test RE 0.003319122777398575 c 0.068533674 k 0.05893118 m 0.006456142\n",
      "34 Train Loss 1615.879 Test RE 0.0030610525939675174 c 0.07485454 k 0.060086094 m 0.0069908053\n",
      "35 Train Loss 1601.5653 Test RE 0.003057176399093485 c 0.08175003 k 0.059081286 m 0.007580654\n",
      "36 Train Loss 1587.8026 Test RE 0.0029849725719658226 c 0.086619034 k 0.059262555 m 0.008019153\n",
      "37 Train Loss 1571.1765 Test RE 0.002542869599955202 c 0.091409825 k 0.0595802 m 0.0084158555\n",
      "38 Train Loss 1569.3741 Test RE 0.0026428784839501577 c 0.09311133 k 0.05935458 m 0.008553795\n",
      "39 Train Loss 1563.4089 Test RE 0.0027942174620977263 c 0.09703795 k 0.059733544 m 0.008894298\n",
      "40 Train Loss 1543.7634 Test RE 0.002735400876136927 c 0.10515081 k 0.059449013 m 0.00958924\n",
      "41 Train Loss 1531.9338 Test RE 0.002822327703892059 c 0.112454645 k 0.058569618 m 0.010179095\n",
      "42 Train Loss 1513.573 Test RE 0.00306382160872468 c 0.11982737 k 0.05929915 m 0.01073302\n",
      "43 Train Loss 1499.6155 Test RE 0.002645906005844489 c 0.122898616 k 0.059296954 m 0.010910658\n",
      "44 Train Loss 1490.7986 Test RE 0.00264470113156746 c 0.126685 k 0.059088916 m 0.011152196\n",
      "45 Train Loss 1484.3015 Test RE 0.003599926566290101 c 0.13371292 k 0.059543308 m 0.011665324\n",
      "46 Train Loss 1469.6523 Test RE 0.003232362277172089 c 0.13984688 k 0.058454428 m 0.0121422885\n",
      "47 Train Loss 1456.4263 Test RE 0.003730738700183691 c 0.14890465 k 0.05853542 m 0.012808478\n",
      "48 Train Loss 1443.7261 Test RE 0.004362230066349536 c 0.15999499 k 0.058722395 m 0.013628281\n",
      "49 Train Loss 1421.7504 Test RE 0.004266273235974657 c 0.1704303 k 0.057725806 m 0.014398696\n",
      "50 Train Loss 1393.4777 Test RE 0.0035465834695010312 c 0.17603916 k 0.058797706 m 0.014780998\n",
      "51 Train Loss 1371.063 Test RE 0.002572905370665072 c 0.17941754 k 0.057924826 m 0.014999538\n",
      "52 Train Loss 1357.5024 Test RE 0.002825552595273024 c 0.18597165 k 0.05828515 m 0.015460616\n",
      "53 Train Loss 1332.1083 Test RE 0.0033880588929733153 c 0.20266125 k 0.058208186 m 0.016705185\n",
      "54 Train Loss 1322.0021 Test RE 0.003987512843162152 c 0.21297128 k 0.05757771 m 0.01749609\n",
      "55 Train Loss 1320.9883 Test RE 0.004450523236701267 c 0.21762581 k 0.057551764 m 0.01785426\n",
      "56 Train Loss 1317.8289 Test RE 0.004916932886311145 c 0.22341129 k 0.057608344 m 0.018300649\n",
      "57 Train Loss 1295.3958 Test RE 0.00394310026418588 c 0.22396974 k 0.057294067 m 0.018346082\n",
      "58 Train Loss 1272.2025 Test RE 0.0023161029376250397 c 0.22482306 k 0.057771236 m 0.018408326\n",
      "59 Train Loss 1268.8547 Test RE 0.0018023228097254777 c 0.22557959 k 0.05766386 m 0.018466363\n",
      "60 Train Loss 1261.3772 Test RE 0.001786223970981521 c 0.23090097 k 0.05707162 m 0.018860621\n",
      "61 Train Loss 1224.4142 Test RE 0.0027344649036041236 c 0.24999283 k 0.057899036 m 0.02021135\n",
      "62 Train Loss 1201.1482 Test RE 0.0023997243150344454 c 0.26236695 k 0.057363763 m 0.021081934\n",
      "63 Train Loss 1172.1167 Test RE 0.003195429856305218 c 0.28252375 k 0.058234602 m 0.022476962\n",
      "64 Train Loss 1156.1989 Test RE 0.0030582428190697596 c 0.288339 k 0.056119233 m 0.022894848\n",
      "65 Train Loss 1137.7646 Test RE 0.002803373299169864 c 0.29644412 k 0.055967603 m 0.023511741\n",
      "66 Train Loss 1125.5487 Test RE 0.0024208566086849465 c 0.3033955 k 0.05688481 m 0.024055814\n",
      "67 Train Loss 1095.1332 Test RE 0.002423026738578092 c 0.3184044 k 0.055768225 m 0.025233453\n",
      "68 Train Loss 1073.234 Test RE 0.0028762810830533776 c 0.3328704 k 0.05572811 m 0.026344828\n",
      "69 Train Loss 1055.3154 Test RE 0.0032662069640077072 c 0.3451951 k 0.056339983 m 0.027262583\n",
      "70 Train Loss 1045.0166 Test RE 0.003995811453535766 c 0.356302 k 0.055342793 m 0.028086025\n",
      "71 Train Loss 1023.98645 Test RE 0.0039624819297026164 c 0.36983773 k 0.056004707 m 0.029098874\n",
      "72 Train Loss 985.7325 Test RE 0.0028294505648961393 c 0.38901508 k 0.055479195 m 0.030575456\n",
      "73 Train Loss 963.3849 Test RE 0.0030855728246221873 c 0.4132513 k 0.055345807 m 0.03241405\n",
      "74 Train Loss 943.1857 Test RE 0.004330159482842638 c 0.4393219 k 0.054827683 m 0.034375813\n",
      "75 Train Loss 928.2032 Test RE 0.005159519033556723 c 0.46014917 k 0.054585364 m 0.035944294\n",
      "76 Train Loss 909.69855 Test RE 0.005447598804237878 c 0.48103935 k 0.05428653 m 0.03752338\n",
      "77 Train Loss 882.86993 Test RE 0.004881136523412019 c 0.4979183 k 0.054007094 m 0.038787577\n",
      "78 Train Loss 875.3614 Test RE 0.004704882874687295 c 0.5049825 k 0.054176986 m 0.03930784\n",
      "79 Train Loss 849.2855 Test RE 0.004697327225854887 c 0.52086425 k 0.05402274 m 0.040475566\n",
      "80 Train Loss 818.28015 Test RE 0.004672558999352227 c 0.531459 k 0.053438444 m 0.041249055\n",
      "81 Train Loss 794.0892 Test RE 0.004635117697337929 c 0.5448672 k 0.05310106 m 0.042262975\n",
      "82 Train Loss 741.76276 Test RE 0.0035744113533504734 c 0.57499015 k 0.052818075 m 0.04447349\n",
      "83 Train Loss 714.85034 Test RE 0.002919736155234404 c 0.5954914 k 0.052538007 m 0.045963585\n",
      "84 Train Loss 662.4447 Test RE 0.0021013142111235977 c 0.6307795 k 0.05188071 m 0.04854834\n",
      "85 Train Loss 640.42786 Test RE 0.001639126669744919 c 0.63433075 k 0.05176169 m 0.048801247\n",
      "86 Train Loss 636.2729 Test RE 0.0014639103148575516 c 0.6349775 k 0.05176978 m 0.048842598\n",
      "87 Train Loss 635.5399 Test RE 0.0014977742698955763 c 0.63594425 k 0.051813804 m 0.048910953\n",
      "88 Train Loss 634.79333 Test RE 0.0017179313841323351 c 0.63895386 k 0.051814485 m 0.0491325\n",
      "89 Train Loss 629.35596 Test RE 0.002281377388642535 c 0.6527269 k 0.05162288 m 0.050139617\n",
      "90 Train Loss 622.46356 Test RE 0.0025342351930307916 c 0.66943896 k 0.051343396 m 0.051345926\n",
      "91 Train Loss 620.1978 Test RE 0.0027893067781362972 c 0.6798656 k 0.051252086 m 0.05209421\n",
      "92 Train Loss 615.4248 Test RE 0.0031237129897565535 c 0.69553274 k 0.050978914 m 0.053238828\n",
      "93 Train Loss 605.2102 Test RE 0.003283853919036612 c 0.7052147 k 0.05062883 m 0.053958323\n",
      "94 Train Loss 593.0292 Test RE 0.0031038865902357745 c 0.70962363 k 0.050829835 m 0.0542819\n",
      "95 Train Loss 588.71265 Test RE 0.003315221735257821 c 0.71134156 k 0.05083992 m 0.054411467\n",
      "96 Train Loss 586.5344 Test RE 0.0035419103194662465 c 0.7142175 k 0.050744183 m 0.054629464\n",
      "97 Train Loss 582.33325 Test RE 0.003260193988614879 c 0.7175854 k 0.05077232 m 0.054880794\n",
      "98 Train Loss 573.8296 Test RE 0.003118433241759913 c 0.72641283 k 0.050553728 m 0.055507842\n",
      "99 Train Loss 558.02277 Test RE 0.00336299046213079 c 0.7439946 k 0.050403077 m 0.05671405\n",
      "100 Train Loss 547.56836 Test RE 0.003591466899765716 c 0.7586023 k 0.05028393 m 0.057712052\n",
      "101 Train Loss 539.98914 Test RE 0.003671171424671531 c 0.76851463 k 0.05016028 m 0.058404297\n",
      "102 Train Loss 526.4625 Test RE 0.003448111784053239 c 0.7851647 k 0.049963366 m 0.05959007\n",
      "103 Train Loss 512.7838 Test RE 0.002923669940533207 c 0.79969865 k 0.04969474 m 0.060629968\n",
      "104 Train Loss 496.6251 Test RE 0.0023331834592452694 c 0.81584704 k 0.049074393 m 0.06178342\n",
      "105 Train Loss 482.81094 Test RE 0.002208460563302932 c 0.82075614 k 0.04946808 m 0.06213289\n",
      "106 Train Loss 476.41278 Test RE 0.002351171126668257 c 0.8224515 k 0.04932854 m 0.062260073\n",
      "107 Train Loss 474.4177 Test RE 0.0026406333915626533 c 0.82286006 k 0.0493294 m 0.062293436\n",
      "108 Train Loss 473.78406 Test RE 0.002921971159309264 c 0.82359457 k 0.049290434 m 0.062349424\n",
      "109 Train Loss 472.18112 Test RE 0.002925393107760274 c 0.8261263 k 0.049221873 m 0.06253464\n",
      "110 Train Loss 471.12582 Test RE 0.002879440087341565 c 0.8287666 k 0.04923666 m 0.06272824\n",
      "111 Train Loss 470.6442 Test RE 0.002829597999147699 c 0.8314064 k 0.04917588 m 0.06292222\n",
      "112 Train Loss 470.0816 Test RE 0.002912526258384595 c 0.83535624 k 0.049096327 m 0.06321649\n",
      "113 Train Loss 468.56598 Test RE 0.0029584635918685053 c 0.8427287 k 0.04875816 m 0.06376589\n",
      "114 Train Loss 462.3095 Test RE 0.0028402914060322425 c 0.85614663 k 0.0490298 m 0.06476534\n",
      "115 Train Loss 456.58328 Test RE 0.002555692775762997 c 0.8619588 k 0.048537116 m 0.0651964\n",
      "116 Train Loss 454.6488 Test RE 0.002337905025689869 c 0.86276805 k 0.04865943 m 0.06525527\n",
      "117 Train Loss 454.06213 Test RE 0.0022051864224750133 c 0.86258817 k 0.048605658 m 0.06524163\n",
      "118 Train Loss 452.85986 Test RE 0.0021592746203743392 c 0.8642044 k 0.04849952 m 0.0653608\n",
      "119 Train Loss 449.0295 Test RE 0.0022952153811944868 c 0.87205243 k 0.048555028 m 0.065943785\n",
      "120 Train Loss 444.42255 Test RE 0.0022206099380972953 c 0.88296664 k 0.04858547 m 0.066777475\n",
      "121 Train Loss 441.50076 Test RE 0.002207070663245086 c 0.8836574 k 0.048397627 m 0.066860355\n",
      "122 Train Loss 437.46805 Test RE 0.001764519943525287 c 0.8842881 k 0.04833156 m 0.06695706\n",
      "123 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "124 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "125 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "126 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "127 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "128 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "129 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "130 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "131 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "132 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "133 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "134 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "135 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "136 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "137 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "138 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "139 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "140 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "141 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "142 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "143 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "144 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "145 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "146 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "147 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "148 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "149 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "150 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "151 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "152 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "153 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "154 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "155 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "156 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "157 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "158 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "159 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "160 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "161 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "162 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "163 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "164 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "165 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "166 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "167 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "168 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "169 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "170 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "171 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "172 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "173 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "174 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "175 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "176 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "177 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "178 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "179 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "180 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "181 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "182 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "183 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "184 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "185 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "186 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "187 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "188 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "189 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "190 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "191 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "192 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "193 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "194 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "195 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "196 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "197 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "198 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "199 Train Loss 436.24414 Test RE 0.0016621261304780716 c 0.8852413 k 0.048293862 m 0.06704393\n",
      "Training time: 74.75\n",
      "Training time: 74.75\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 2084132.6 Test RE 0.967270389371723 c -0.0014683085 k 0.035755694 m -7.0683627e-06\n",
      "1 Train Loss 1837468.9 Test RE 0.9084401416042889 c -0.0031148354 k 0.08294183 m -1.8701063e-05\n",
      "2 Train Loss 1207694.1 Test RE 0.7379564343497759 c -0.006384259 k 0.19210069 m -5.1855e-05\n",
      "3 Train Loss 450456.8 Test RE 0.42478861620174996 c -0.010547643 k 0.34342894 m -0.00010640621\n",
      "4 Train Loss 251792.6 Test RE 0.19694225622622194 c -0.011913713 k 0.39887777 m -0.00013046626\n",
      "5 Train Loss 215770.88 Test RE 0.15204660491862515 c -0.011371236 k 0.3885826 m -0.00013174005\n",
      "6 Train Loss 174524.66 Test RE 0.13499425093115663 c -0.010631383 k 0.3473302 m -0.00011898668\n",
      "7 Train Loss 160706.5 Test RE 0.15436359810504907 c -0.010616941 k 0.3210651 m -0.000104794344\n",
      "8 Train Loss 54996.383 Test RE 0.14208531395890736 c -0.012844699 k 0.14168575 m 1.14427685e-05\n",
      "9 Train Loss 24360.773 Test RE 0.06957595370638489 c -0.013539481 k 0.13672195 m 2.341479e-05\n",
      "10 Train Loss 8379.807 Test RE 0.05319527922666595 c -0.014009449 k 0.056871586 m 7.844806e-05\n",
      "11 Train Loss 4746.958 Test RE 0.029280071521303696 c -0.013589841 k 0.05980636 m 7.8345474e-05\n",
      "12 Train Loss 4577.1846 Test RE 0.024498106136725987 c -0.013472332 k 0.06183894 m 7.888928e-05\n",
      "13 Train Loss 4562.5635 Test RE 0.02343778927021449 c -0.013413264 k 0.06115289 m 8.1798054e-05\n",
      "14 Train Loss 4526.6953 Test RE 0.02328531804870788 c -0.013022971 k 0.060581245 m 0.00010075552\n",
      "15 Train Loss 4204.168 Test RE 0.01901851682873392 c -0.010626682 k 0.061304733 m 0.00020900904\n",
      "16 Train Loss 3956.15 Test RE 0.018226015191658798 c -0.009269257 k 0.06041724 m 0.00027746375\n",
      "17 Train Loss 3702.8472 Test RE 0.01970568056747407 c -0.0073415237 k 0.060711112 m 0.00037460044\n",
      "18 Train Loss 3621.0908 Test RE 0.019282529313249926 c -0.005553337 k 0.0613027 m 0.00045922957\n",
      "19 Train Loss 3560.4526 Test RE 0.019778887330560307 c -0.0033035865 k 0.06015481 m 0.00057006284\n",
      "20 Train Loss 3502.3638 Test RE 0.01931189391236156 c -0.0019470232 k 0.061646804 m 0.0006362162\n",
      "21 Train Loss 3467.1636 Test RE 0.01920884583077852 c -0.0011732014 k 0.06099643 m 0.0006780169\n",
      "22 Train Loss 3401.7173 Test RE 0.016631932158594282 c -0.00079160044 k 0.060254347 m 0.00069872226\n",
      "23 Train Loss 3381.6301 Test RE 0.016785795921744843 c -0.0010042185 k 0.06132563 m 0.0006886644\n",
      "24 Train Loss 3368.4502 Test RE 0.01746483088272733 c -0.00057744444 k 0.06004136 m 0.000703958\n",
      "25 Train Loss 3305.5469 Test RE 0.017615563789648712 c 0.0044463985 k 0.059875626 m 0.0008932082\n",
      "26 Train Loss 3266.669 Test RE 0.01797499061827066 c 0.00910352 k 0.06207441 m 0.0010543268\n",
      "27 Train Loss 3214.2976 Test RE 0.018397106093868767 c 0.011999518 k 0.060511634 m 0.0011444765\n",
      "28 Train Loss 3093.6309 Test RE 0.018783993773024043 c 0.020760424 k 0.060566258 m 0.0014403651\n",
      "29 Train Loss 3053.4976 Test RE 0.018556259076445075 c 0.024900448 k 0.06068912 m 0.0015856302\n",
      "30 Train Loss 3020.687 Test RE 0.0186042301562147 c 0.028182982 k 0.060785227 m 0.0016999055\n",
      "31 Train Loss 2970.7476 Test RE 0.01855628544957796 c 0.030712752 k 0.060017966 m 0.0017908983\n",
      "32 Train Loss 2955.902 Test RE 0.018523427627657775 c 0.033942603 k 0.059868068 m 0.0019126865\n",
      "33 Train Loss 2940.4863 Test RE 0.01852825003815633 c 0.037357707 k 0.06042577 m 0.0020427406\n",
      "34 Train Loss 2626.889 Test RE 0.015470778784161813 c 0.058784906 k 0.05963758 m 0.0029012111\n",
      "35 Train Loss 2307.8904 Test RE 0.014496772859874272 c 0.06845106 k 0.06054732 m 0.0032967883\n",
      "36 Train Loss 2245.6055 Test RE 0.014135354846268553 c 0.0694027 k 0.05965187 m 0.003377953\n",
      "37 Train Loss 1784.771 Test RE 0.008628661288423961 c 0.06697552 k 0.059064783 m 0.0034618555\n",
      "38 Train Loss 1702.4967 Test RE 0.006114824651046265 c 0.06630428 k 0.059961807 m 0.0034714628\n",
      "39 Train Loss 1698.163 Test RE 0.006070832584763829 c 0.06634126 k 0.060011834 m 0.003479344\n",
      "40 Train Loss 1694.9686 Test RE 0.006082398176862044 c 0.06660597 k 0.05992596 m 0.0034945782\n",
      "41 Train Loss 1677.7506 Test RE 0.005416397068784311 c 0.06664139 k 0.059814632 m 0.003505832\n",
      "42 Train Loss 1665.5117 Test RE 0.004610049767657109 c 0.06617853 k 0.059922215 m 0.0034975442\n",
      "43 Train Loss 1661.2849 Test RE 0.0044106757560973524 c 0.067187555 k 0.060345877 m 0.0035439131\n",
      "44 Train Loss 1642.4082 Test RE 0.004083300158394297 c 0.06999204 k 0.06004467 m 0.0036664358\n",
      "45 Train Loss 1629.896 Test RE 0.004065738010883037 c 0.077013515 k 0.059161052 m 0.003937549\n",
      "46 Train Loss 1596.541 Test RE 0.0047538335728293446 c 0.09269294 k 0.059944674 m 0.0045456947\n",
      "47 Train Loss 1591.5875 Test RE 0.004753367905885499 c 0.095256984 k 0.059694182 m 0.0046498617\n",
      "48 Train Loss 1591.1316 Test RE 0.004707900882863648 c 0.09567469 k 0.05953538 m 0.004664356\n",
      "49 Train Loss 1585.7158 Test RE 0.004631745170915952 c 0.09753548 k 0.060168903 m 0.0047149034\n",
      "50 Train Loss 1578.3883 Test RE 0.004333411300770651 c 0.099269405 k 0.059437778 m 0.0047866204\n",
      "51 Train Loss 1561.0693 Test RE 0.00426933608315517 c 0.109957844 k 0.05919999 m 0.0052255383\n",
      "52 Train Loss 1540.1285 Test RE 0.003579660720533617 c 0.110983685 k 0.05930606 m 0.005279506\n",
      "53 Train Loss 1537.3193 Test RE 0.0035974323761534177 c 0.11147414 k 0.05937325 m 0.0053144335\n",
      "54 Train Loss 1532.3838 Test RE 0.0031865329711149603 c 0.11175119 k 0.059252042 m 0.0053666513\n",
      "55 Train Loss 1529.9883 Test RE 0.003152594015052911 c 0.11191174 k 0.059329133 m 0.0053923354\n",
      "56 Train Loss 1529.8307 Test RE 0.0031574957054627912 c 0.111991495 k 0.059367537 m 0.005402186\n",
      "57 Train Loss 1529.3478 Test RE 0.0030721994155583987 c 0.1118153 k 0.059331752 m 0.005403991\n",
      "58 Train Loss 1528.7875 Test RE 0.0030014151578158284 c 0.111638464 k 0.059315637 m 0.005382872\n",
      "59 Train Loss 1528.6652 Test RE 0.0030299723506355364 c 0.111628175 k 0.059373368 m 0.005382415\n",
      "60 Train Loss 1523.6166 Test RE 0.0033861289934301723 c 0.11492481 k 0.0597606 m 0.0056217615\n",
      "61 Train Loss 1442.4106 Test RE 0.0033172503117307557 c 0.15192531 k 0.05941669 m 0.008353686\n",
      "62 Train Loss 1345.6951 Test RE 0.005733514333151159 c 0.2130882 k 0.057104852 m 0.012884666\n",
      "63 Train Loss 1184.6064 Test RE 0.005389103777613924 c 0.2829112 k 0.058185525 m 0.018140594\n",
      "64 Train Loss 1120.0552 Test RE 0.005344596931292534 c 0.3263893 k 0.055955578 m 0.021380037\n",
      "65 Train Loss 1019.47864 Test RE 0.007040500570584286 c 0.40819657 k 0.05498447 m 0.027453791\n",
      "66 Train Loss 946.09265 Test RE 0.0077985747565690105 c 0.4703514 k 0.05450483 m 0.03245135\n",
      "67 Train Loss 854.5818 Test RE 0.007445756950546447 c 0.524556 k 0.05189565 m 0.036723718\n",
      "68 Train Loss 819.88367 Test RE 0.007136489371802981 c 0.55111545 k 0.053035878 m 0.038724333\n",
      "69 Train Loss 811.99554 Test RE 0.007186914814139013 c 0.5731731 k 0.052386865 m 0.040352896\n",
      "70 Train Loss 806.9166 Test RE 0.0072794329195551575 c 0.5889951 k 0.05201129 m 0.04152051\n",
      "71 Train Loss 799.8917 Test RE 0.006987766168277832 c 0.5861411 k 0.053273167 m 0.04124648\n",
      "72 Train Loss 793.61383 Test RE 0.006797692029390351 c 0.580134 k 0.052415922 m 0.04076237\n",
      "73 Train Loss 790.82446 Test RE 0.00676588103431793 c 0.58349776 k 0.052351937 m 0.04099106\n",
      "74 Train Loss 775.4942 Test RE 0.00714143064662318 c 0.61333585 k 0.05324811 m 0.043391984\n",
      "75 Train Loss 742.4864 Test RE 0.006960762303032192 c 0.652086 k 0.05168781 m 0.046527408\n",
      "76 Train Loss 737.6295 Test RE 0.007006345544751806 c 0.6501567 k 0.051541366 m 0.04643054\n",
      "77 Train Loss 737.1638 Test RE 0.006965128464006589 c 0.648823 k 0.05161186 m 0.046339314\n",
      "78 Train Loss 737.0119 Test RE 0.0068651120772144055 c 0.6476307 k 0.05169707 m 0.046240814\n",
      "79 Train Loss 736.87787 Test RE 0.006814076073877224 c 0.6477472 k 0.05169716 m 0.046243884\n",
      "80 Train Loss 735.81964 Test RE 0.006850555050742302 c 0.6464867 k 0.051702388 m 0.04613715\n",
      "81 Train Loss 735.0166 Test RE 0.006953915875155716 c 0.64564914 k 0.05171689 m 0.046077356\n",
      "82 Train Loss 732.76587 Test RE 0.006883856782023197 c 0.64515024 k 0.05171183 m 0.046025924\n",
      "83 Train Loss 731.1292 Test RE 0.006721407221037894 c 0.64444745 k 0.05173757 m 0.04595139\n",
      "84 Train Loss 730.4692 Test RE 0.006742078355752255 c 0.645181 k 0.051727075 m 0.04601431\n",
      "85 Train Loss 730.3999 Test RE 0.00678515087424415 c 0.64519155 k 0.051728707 m 0.046018563\n",
      "86 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "87 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "88 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "89 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "90 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "91 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "92 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "93 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "94 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "95 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "96 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "97 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "98 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "99 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "100 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "101 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "102 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "103 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "104 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "105 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "106 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "107 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "108 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "109 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "110 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "111 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "112 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "113 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "114 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "115 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "116 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "117 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "118 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "119 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "120 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "121 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "122 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "123 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "124 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "125 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "126 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "127 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "128 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "129 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "130 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "131 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "132 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "133 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "134 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "135 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "136 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "137 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "138 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "139 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "140 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "141 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "142 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "143 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "144 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "145 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "146 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "147 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "148 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "149 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "150 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "151 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "152 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "153 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "154 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "155 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "156 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "157 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "158 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "159 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "160 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "161 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "162 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "163 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "164 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "165 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "166 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "167 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "168 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "169 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "170 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "171 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "172 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "173 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "174 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "175 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "176 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "177 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "178 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "179 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "180 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "181 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "182 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "183 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "184 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "185 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "186 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "187 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "188 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "189 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "190 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "191 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "192 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "193 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "194 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "195 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "196 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "197 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "198 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "199 Train Loss 730.396 Test RE 0.006787163023517191 c 0.64504176 k 0.05172858 m 0.046006117\n",
      "Training time: 59.80\n",
      "Training time: 59.80\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 2095297.0 Test RE 0.9698126254377861 c 0.00048830884 k 0.013046281 m -2.750239e-06\n",
      "1 Train Loss 1741120.5 Test RE 0.8839804751419295 c 0.0011337777 k 0.039996255 m -1.0141367e-05\n",
      "2 Train Loss 1032818.06 Test RE 0.6822305456527203 c 0.00286553 k 0.12511565 m -3.8605307e-05\n",
      "3 Train Loss 294212.28 Test RE 0.35985911035390533 c 0.0035632174 k 0.15315132 m -4.8567315e-05\n",
      "4 Train Loss 232180.77 Test RE 0.31550461861366863 c 0.0037904147 k 0.15741442 m -5.0267172e-05\n",
      "5 Train Loss 204138.69 Test RE 0.2918277760693817 c 0.004037091 k 0.16341992 m -5.2631272e-05\n",
      "6 Train Loss 191117.84 Test RE 0.27827370885186076 c 0.004315827 k 0.17074548 m -5.5539866e-05\n",
      "7 Train Loss 165018.05 Test RE 0.24589132160132854 c 0.005091812 k 0.19431685 m -6.459849e-05\n",
      "8 Train Loss 162525.2 Test RE 0.24376438787841415 c 0.005097857 k 0.19357447 m -6.453945e-05\n",
      "9 Train Loss 142061.44 Test RE 0.22178716758223982 c 0.005899556 k 0.19518559 m -7.100584e-05\n",
      "10 Train Loss 124632.875 Test RE 0.20660147035965945 c 0.006370607 k 0.18929726 m -7.390612e-05\n",
      "11 Train Loss 37117.984 Test RE 0.08716118147973098 c 0.0067604305 k 0.15717928 m -7.3137635e-05\n",
      "12 Train Loss 23956.098 Test RE 0.04468114007609805 c 0.006949464 k 0.15001667 m -7.387926e-05\n",
      "13 Train Loss 21241.273 Test RE 0.0429542332207133 c 0.007121168 k 0.1427124 m -7.4448755e-05\n",
      "14 Train Loss 13796.21 Test RE 0.04076098126225454 c 0.0074925236 k 0.11646887 m -7.418898e-05\n",
      "15 Train Loss 11755.765 Test RE 0.0429572821245671 c 0.0076494087 k 0.10568808 m -7.409736e-05\n",
      "16 Train Loss 9743.77 Test RE 0.03782417915653806 c 0.007805543 k 0.09574432 m -7.415572e-05\n",
      "17 Train Loss 9374.662 Test RE 0.04429116938697029 c 0.007902901 k 0.08449822 m -7.36085e-05\n",
      "18 Train Loss 8195.514 Test RE 0.042891063929360894 c 0.007897688 k 0.06878809 m -7.184397e-05\n",
      "19 Train Loss 7538.249 Test RE 0.03886459040878579 c 0.007722053 k 0.06998471 m -7.06659e-05\n",
      "20 Train Loss 7336.761 Test RE 0.03739113358470015 c 0.0075926995 k 0.07144738 m -6.98075e-05\n",
      "21 Train Loss 6786.0864 Test RE 0.03605453938872442 c 0.007514021 k 0.063358866 m -6.817482e-05\n",
      "22 Train Loss 6422.4053 Test RE 0.03436943960158061 c 0.0074551166 k 0.066661514 m -6.807816e-05\n",
      "23 Train Loss 6370.6 Test RE 0.033625195044845214 c 0.0074493065 k 0.06842275 m -6.826045e-05\n",
      "24 Train Loss 6338.7285 Test RE 0.0335985813012542 c 0.0073605194 k 0.0674428 m -6.745408e-05\n",
      "25 Train Loss 6261.7256 Test RE 0.03332114448992361 c 0.0071722497 k 0.06505505 m -6.567206e-05\n",
      "26 Train Loss 6213.791 Test RE 0.03235722004173118 c 0.0070945476 k 0.06726349 m -6.5308785e-05\n",
      "27 Train Loss 6136.4614 Test RE 0.0318201279357247 c 0.006925806 k 0.06625628 m -6.3888445e-05\n",
      "28 Train Loss 6036.867 Test RE 0.030884970394209225 c 0.006699262 k 0.06292313 m -6.186309e-05\n",
      "29 Train Loss 5690.742 Test RE 0.02865141670421922 c 0.00663085 k 0.06525032 m -6.180011e-05\n",
      "30 Train Loss 5493.7905 Test RE 0.026416016715186036 c 0.0064796065 k 0.06571998 m -6.092871e-05\n",
      "31 Train Loss 5471.451 Test RE 0.026178578566228173 c 0.006414209 k 0.06295734 m -6.017739e-05\n",
      "32 Train Loss 5396.425 Test RE 0.02503130966719112 c 0.006275444 k 0.059658315 m -5.8422123e-05\n",
      "33 Train Loss 5225.544 Test RE 0.023527432192710182 c 0.006230243 k 0.06204042 m -5.7623194e-05\n",
      "34 Train Loss 5142.285 Test RE 0.023051524180402353 c 0.0061653294 k 0.06211385 m -5.622975e-05\n",
      "35 Train Loss 5121.1426 Test RE 0.022577847876715077 c 0.0060989833 k 0.06123923 m -5.496744e-05\n",
      "36 Train Loss 5120.163 Test RE 0.02248727997896651 c 0.006086081 k 0.06125997 m -5.4731463e-05\n",
      "37 Train Loss 5120.001 Test RE 0.02249336416376052 c 0.006085032 k 0.061323036 m -5.473958e-05\n",
      "38 Train Loss 5118.949 Test RE 0.022603270550545687 c 0.0060712686 k 0.06128222 m -5.4732234e-05\n",
      "39 Train Loss 5118.5107 Test RE 0.022644388178030918 c 0.006061088 k 0.061232798 m -5.4636184e-05\n",
      "40 Train Loss 5118.2393 Test RE 0.022641711237270528 c 0.006051937 k 0.061252713 m -5.443527e-05\n",
      "41 Train Loss 5117.377 Test RE 0.022665480679202238 c 0.006070002 k 0.061317857 m -5.4187018e-05\n",
      "42 Train Loss 5112.766 Test RE 0.022638597938050533 c 0.006213155 k 0.061231136 m -5.380016e-05\n",
      "43 Train Loss 5111.345 Test RE 0.022583444819066 c 0.006306969 k 0.061179258 m -5.373336e-05\n",
      "44 Train Loss 5110.433 Test RE 0.022537324748740633 c 0.006426482 k 0.06122231 m -5.3301486e-05\n",
      "45 Train Loss 5103.1953 Test RE 0.022640565605173187 c 0.006825823 k 0.061106693 m -4.938336e-05\n",
      "46 Train Loss 5074.4834 Test RE 0.022977736406326055 c 0.0077028065 k 0.06091425 m -3.618954e-05\n",
      "47 Train Loss 5064.023 Test RE 0.023026344020411624 c 0.008209165 k 0.060793757 m -2.7002421e-05\n",
      "48 Train Loss 5061.535 Test RE 0.022697234866650984 c 0.008651535 k 0.06068576 m -1.8462004e-05\n",
      "49 Train Loss 5060.4233 Test RE 0.022423707973283597 c 0.008763155 k 0.060727283 m -1.6329212e-05\n",
      "50 Train Loss 5055.845 Test RE 0.022266423475499765 c 0.008814776 k 0.060816236 m -1.68094e-05\n",
      "51 Train Loss 5003.1074 Test RE 0.021057682660291205 c 0.011840231 k 0.060417313 m 3.087417e-05\n",
      "52 Train Loss 4981.078 Test RE 0.020079421341900144 c 0.014200763 k 0.062016644 m 6.981226e-05\n",
      "53 Train Loss 4974.217 Test RE 0.019253182760114347 c 0.01567937 k 0.061593432 m 9.494671e-05\n",
      "54 Train Loss 4971.9253 Test RE 0.01897042322951752 c 0.016485583 k 0.062040605 m 0.0001083354\n",
      "55 Train Loss 4966.296 Test RE 0.018620573377850152 c 0.017836852 k 0.06229245 m 0.00012995287\n",
      "56 Train Loss 4945.8184 Test RE 0.01797402607376218 c 0.018494146 k 0.062853 m 0.0001379412\n",
      "57 Train Loss 4911.4336 Test RE 0.018977300743515217 c 0.018338835 k 0.06362023 m 0.00013201284\n",
      "58 Train Loss 4794.0366 Test RE 0.018346334940089463 c 0.020639664 k 0.06414218 m 0.00016958964\n",
      "59 Train Loss 4742.676 Test RE 0.017899103170066898 c 0.021562869 k 0.0631434 m 0.00018514028\n",
      "60 Train Loss 4704.4023 Test RE 0.018767085191465915 c 0.021684162 k 0.061980847 m 0.00018639078\n",
      "61 Train Loss 4679.89 Test RE 0.018252867788352035 c 0.020658514 k 0.06264717 m 0.00016993965\n",
      "62 Train Loss 4601.955 Test RE 0.0195521302455085 c 0.018506305 k 0.060963947 m 0.00013270711\n",
      "63 Train Loss 4578.953 Test RE 0.020220535228037222 c 0.017230427 k 0.061998934 m 0.00010972673\n",
      "64 Train Loss 4574.971 Test RE 0.020584239584107593 c 0.016877282 k 0.06173293 m 0.00010350185\n",
      "65 Train Loss 4572.7183 Test RE 0.020753536754703688 c 0.017032674 k 0.06127104 m 0.000105990825\n",
      "66 Train Loss 4559.081 Test RE 0.021206601233064448 c 0.016890898 k 0.0608169 m 0.00010289471\n",
      "67 Train Loss 4539.9453 Test RE 0.02159481239797876 c 0.01585477 k 0.061117653 m 8.516956e-05\n",
      "68 Train Loss 4515.465 Test RE 0.02112312666205358 c 0.016716816 k 0.060044944 m 0.000101243844\n",
      "69 Train Loss 4484.599 Test RE 0.019933435158417306 c 0.018158767 k 0.059730098 m 0.00012836915\n",
      "70 Train Loss 4468.3926 Test RE 0.01905807027127039 c 0.019632364 k 0.058877043 m 0.00015460877\n",
      "71 Train Loss 4448.7725 Test RE 0.01876158962769856 c 0.0195465 k 0.05914924 m 0.00015354087\n",
      "72 Train Loss 4426.7017 Test RE 0.018725301109534655 c 0.019884512 k 0.059367727 m 0.00015888116\n",
      "73 Train Loss 4379.5034 Test RE 0.018974503316332218 c 0.021426925 k 0.059475325 m 0.0001862036\n",
      "74 Train Loss 4348.5933 Test RE 0.019448462260775845 c 0.021739533 k 0.059335817 m 0.00019220295\n",
      "75 Train Loss 4333.8213 Test RE 0.02032701172212651 c 0.021995468 k 0.058770567 m 0.00019563668\n",
      "76 Train Loss 4330.731 Test RE 0.02055073403110557 c 0.021542283 k 0.05859768 m 0.00018741381\n",
      "77 Train Loss 4323.6973 Test RE 0.020606362517629602 c 0.019775761 k 0.05836121 m 0.00015565938\n",
      "78 Train Loss 4312.4556 Test RE 0.020558765217286033 c 0.019871907 k 0.058321834 m 0.00015680079\n",
      "79 Train Loss 4230.474 Test RE 0.020911868332985712 c 0.019076733 k 0.05756439 m 0.00014161543\n",
      "80 Train Loss 4031.2646 Test RE 0.02097824575425093 c 0.018609317 k 0.059542954 m 0.00013263\n",
      "81 Train Loss 3941.5205 Test RE 0.020909519666367712 c 0.022936858 k 0.06037195 m 0.00020711403\n",
      "82 Train Loss 3882.0693 Test RE 0.021252653682412123 c 0.026920583 k 0.060349558 m 0.00027539668\n",
      "83 Train Loss 3844.4883 Test RE 0.020860851270631295 c 0.027704708 k 0.058843497 m 0.00028969257\n",
      "84 Train Loss 3824.842 Test RE 0.02068501553566411 c 0.030187758 k 0.058929697 m 0.00033278772\n",
      "85 Train Loss 3794.8896 Test RE 0.019820125481062743 c 0.029342595 k 0.059944924 m 0.00031804637\n",
      "86 Train Loss 3682.1328 Test RE 0.01830400537676537 c 0.031354308 k 0.05960409 m 0.00035550425\n",
      "87 Train Loss 3629.4897 Test RE 0.018042628447399074 c 0.034509327 k 0.060727075 m 0.0004111198\n",
      "88 Train Loss 3498.017 Test RE 0.0185182808417108 c 0.042142108 k 0.05986382 m 0.0005436358\n",
      "89 Train Loss 3375.3477 Test RE 0.01906931070785221 c 0.04984145 k 0.05985908 m 0.00067752745\n",
      "90 Train Loss 3196.0781 Test RE 0.01829280756553327 c 0.056126405 k 0.060065277 m 0.00078659947\n",
      "91 Train Loss 2822.0532 Test RE 0.014839510223774227 c 0.069485806 k 0.058097735 m 0.0010161496\n",
      "92 Train Loss 2580.5474 Test RE 0.012073034227897452 c 0.07812229 k 0.060361758 m 0.0011631246\n",
      "93 Train Loss 2439.9219 Test RE 0.010843286930669367 c 0.08911699 k 0.059276726 m 0.0013514721\n",
      "94 Train Loss 2355.0593 Test RE 0.010627825377207991 c 0.09657109 k 0.057235215 m 0.0014801751\n",
      "95 Train Loss 2242.903 Test RE 0.01039093642206389 c 0.10060198 k 0.058443222 m 0.0015499442\n",
      "96 Train Loss 2205.1123 Test RE 0.01012873861430072 c 0.10138864 k 0.058941375 m 0.0015633694\n",
      "97 Train Loss 2160.255 Test RE 0.010730691893945894 c 0.09980052 k 0.058997255 m 0.0015355179\n",
      "98 Train Loss 2126.5586 Test RE 0.010546273485768342 c 0.10023404 k 0.05870367 m 0.0015422698\n",
      "99 Train Loss 2082.486 Test RE 0.009853499719639903 c 0.09941562 k 0.059874542 m 0.0015261723\n",
      "100 Train Loss 2033.2089 Test RE 0.009580910129357518 c 0.09851502 k 0.059165224 m 0.0015088032\n",
      "101 Train Loss 1964.3003 Test RE 0.009573278032866945 c 0.098692454 k 0.059734277 m 0.001508933\n",
      "102 Train Loss 1879.3403 Test RE 0.009124780738744677 c 0.10124535 k 0.059200287 m 0.0015492154\n",
      "103 Train Loss 1831.3608 Test RE 0.009343714776629135 c 0.10355048 k 0.059281945 m 0.00158523\n",
      "104 Train Loss 1785.853 Test RE 0.009252582459322563 c 0.10407997 k 0.05935146 m 0.0015919375\n",
      "105 Train Loss 1756.5804 Test RE 0.00932011692940132 c 0.104101226 k 0.059121463 m 0.0015898008\n",
      "106 Train Loss 1733.5719 Test RE 0.009265075348645126 c 0.104486845 k 0.05932036 m 0.0015933261\n",
      "107 Train Loss 1721.8557 Test RE 0.009448303437190594 c 0.10352846 k 0.05934099 m 0.0015744366\n",
      "108 Train Loss 1712.4253 Test RE 0.00942438041290688 c 0.102701135 k 0.05912374 m 0.0015576137\n",
      "109 Train Loss 1701.304 Test RE 0.009296306050963295 c 0.10227898 k 0.059329346 m 0.0015473648\n",
      "110 Train Loss 1692.0961 Test RE 0.009139910265498308 c 0.10141803 k 0.059391063 m 0.0015307275\n",
      "111 Train Loss 1690.1824 Test RE 0.00900968807924785 c 0.10065107 k 0.05930746 m 0.0015173072\n",
      "112 Train Loss 1689.6403 Test RE 0.009005863028772869 c 0.10046938 k 0.059345547 m 0.0015140178\n",
      "113 Train Loss 1686.5076 Test RE 0.008879752543059552 c 0.10146201 k 0.059350617 m 0.0015318752\n",
      "114 Train Loss 1680.3167 Test RE 0.008797753683836939 c 0.10272733 k 0.05942361 m 0.0015544167\n",
      "115 Train Loss 1667.4135 Test RE 0.008663485677547666 c 0.103825815 k 0.05965709 m 0.0015723985\n",
      "116 Train Loss 1653.692 Test RE 0.008745100985156053 c 0.10786805 k 0.059007157 m 0.001641104\n",
      "117 Train Loss 1629.2045 Test RE 0.008268452821115659 c 0.11319482 k 0.05967978 m 0.0017333669\n",
      "118 Train Loss 1601.999 Test RE 0.007992823801638318 c 0.119667254 k 0.05898721 m 0.0018448437\n",
      "119 Train Loss 1588.8204 Test RE 0.007671512687497662 c 0.12337517 k 0.059291102 m 0.001908803\n",
      "120 Train Loss 1572.154 Test RE 0.007240107979896678 c 0.12590846 k 0.059241336 m 0.0019528457\n",
      "121 Train Loss 1570.8253 Test RE 0.007231065445932142 c 0.12648621 k 0.05925289 m 0.0019625367\n",
      "122 Train Loss 1570.7592 Test RE 0.007239401573875 c 0.1266051 k 0.05922488 m 0.0019644925\n",
      "123 Train Loss 1570.4136 Test RE 0.007323509609237492 c 0.12663843 k 0.05929397 m 0.0019646338\n",
      "124 Train Loss 1569.0804 Test RE 0.0073190857826672924 c 0.1263698 k 0.05912634 m 0.0019597511\n",
      "125 Train Loss 1568.3174 Test RE 0.007258259706984396 c 0.12645783 k 0.059228707 m 0.0019609851\n",
      "126 Train Loss 1567.8484 Test RE 0.007294054617445293 c 0.12642117 k 0.05927081 m 0.0019597579\n",
      "127 Train Loss 1566.9839 Test RE 0.007402401227385703 c 0.12692031 k 0.059179086 m 0.0019675898\n",
      "128 Train Loss 1566.5425 Test RE 0.007385264247669235 c 0.12700492 k 0.059174903 m 0.0019694709\n",
      "129 Train Loss 1564.5726 Test RE 0.00740397867522332 c 0.12664388 k 0.059098843 m 0.0019628666\n",
      "130 Train Loss 1557.9409 Test RE 0.007177229925344163 c 0.12847985 k 0.059275743 m 0.0019938257\n",
      "131 Train Loss 1549.8063 Test RE 0.007120322189539012 c 0.1299971 k 0.059327833 m 0.002020222\n",
      "132 Train Loss 1539.2424 Test RE 0.006710928672548364 c 0.13080907 k 0.05901605 m 0.002034015\n",
      "133 Train Loss 1531.7251 Test RE 0.006602170089564498 c 0.1316843 k 0.05897072 m 0.0020482636\n",
      "134 Train Loss 1527.4154 Test RE 0.006584232726378963 c 0.13268137 k 0.05896922 m 0.0020638583\n",
      "135 Train Loss 1525.0883 Test RE 0.006543987779611363 c 0.1332581 k 0.059021115 m 0.002072455\n",
      "136 Train Loss 1524.7251 Test RE 0.006544787993138215 c 0.13309634 k 0.058962837 m 0.002069636\n",
      "137 Train Loss 1524.5615 Test RE 0.006456110246685216 c 0.13271876 k 0.05895441 m 0.0020633568\n",
      "138 Train Loss 1523.4058 Test RE 0.00644839755360812 c 0.1330767 k 0.05911145 m 0.0020689948\n",
      "139 Train Loss 1517.1549 Test RE 0.0062937315510457 c 0.13511859 k 0.05885789 m 0.0021013855\n",
      "140 Train Loss 1511.0529 Test RE 0.006369158696404805 c 0.1375411 k 0.058798578 m 0.002139825\n",
      "141 Train Loss 1508.0322 Test RE 0.006386062708209105 c 0.13921356 k 0.058916543 m 0.0021654859\n",
      "142 Train Loss 1506.1365 Test RE 0.006372474787768639 c 0.13988544 k 0.05878594 m 0.0021751146\n",
      "143 Train Loss 1504.8755 Test RE 0.006421822612064858 c 0.1408012 k 0.058839094 m 0.0021885203\n",
      "144 Train Loss 1503.1285 Test RE 0.006444489474295693 c 0.14165552 k 0.05881684 m 0.0022005886\n",
      "145 Train Loss 1498.8336 Test RE 0.0063049692747927525 c 0.14170614 k 0.05882579 m 0.0022019322\n",
      "146 Train Loss 1495.8119 Test RE 0.006309365662936521 c 0.1425967 k 0.05868432 m 0.0022180798\n",
      "147 Train Loss 1492.2113 Test RE 0.00635652357012883 c 0.14478958 k 0.05860453 m 0.0022549469\n",
      "148 Train Loss 1488.2721 Test RE 0.0064200633059813125 c 0.1478669 k 0.05865763 m 0.002307169\n",
      "149 Train Loss 1484.6102 Test RE 0.006543638612725823 c 0.15069617 k 0.058679454 m 0.0023557171\n",
      "150 Train Loss 1470.9604 Test RE 0.006522470259591686 c 0.15622863 k 0.058796298 m 0.002451648\n",
      "151 Train Loss 1458.2517 Test RE 0.006542458975215907 c 0.16154976 k 0.058425393 m 0.0025445882\n",
      "152 Train Loss 1455.447 Test RE 0.006576762947424732 c 0.1634692 k 0.05834944 m 0.002578377\n",
      "153 Train Loss 1454.258 Test RE 0.006542003525566156 c 0.16384275 k 0.05826942 m 0.0025846919\n",
      "154 Train Loss 1453.4224 Test RE 0.006490347765797372 c 0.16323195 k 0.0582914 m 0.0025741167\n",
      "155 Train Loss 1453.0496 Test RE 0.006492507933916751 c 0.16356614 k 0.058277 m 0.002580162\n",
      "156 Train Loss 1452.8142 Test RE 0.006519072865272088 c 0.16386978 k 0.0583409 m 0.0025855997\n",
      "157 Train Loss 1452.6565 Test RE 0.006506175151064089 c 0.1639883 k 0.058343165 m 0.0025876448\n",
      "158 Train Loss 1452.5334 Test RE 0.0065265548481318325 c 0.1641055 k 0.05835176 m 0.0025899315\n",
      "159 Train Loss 1452.256 Test RE 0.00650954893759751 c 0.16422969 k 0.058357496 m 0.0025929052\n",
      "160 Train Loss 1451.6443 Test RE 0.006449711480694507 c 0.16378263 k 0.05839799 m 0.0025873063\n",
      "161 Train Loss 1450.8167 Test RE 0.006497496709915125 c 0.16413741 k 0.058366597 m 0.002599131\n",
      "162 Train Loss 1450.6609 Test RE 0.0065098541917070225 c 0.16428101 k 0.05835093 m 0.0026033607\n",
      "163 Train Loss 1450.4711 Test RE 0.006549765254348193 c 0.1646128 k 0.058354966 m 0.0026109808\n",
      "164 Train Loss 1450.1547 Test RE 0.006540635184772942 c 0.16485883 k 0.058470566 m 0.0026162944\n",
      "165 Train Loss 1446.218 Test RE 0.006293602728170187 c 0.16386734 k 0.05862748 m 0.0026242004\n",
      "166 Train Loss 1444.9833 Test RE 0.006372674174268782 c 0.16433497 k 0.05838812 m 0.0026350871\n",
      "167 Train Loss 1444.8397 Test RE 0.006381031633712502 c 0.16441193 k 0.05843751 m 0.0026381398\n",
      "168 Train Loss 1444.381 Test RE 0.006363382663186371 c 0.16464564 k 0.058419388 m 0.0026513056\n",
      "169 Train Loss 1443.4993 Test RE 0.006312370187178501 c 0.16473323 k 0.058664206 m 0.0026734618\n",
      "170 Train Loss 1442.8384 Test RE 0.006317192084966778 c 0.16474625 k 0.058431827 m 0.0026810872\n",
      "171 Train Loss 1442.3376 Test RE 0.006283614865550792 c 0.16493115 k 0.05843475 m 0.0026948552\n",
      "172 Train Loss 1441.9198 Test RE 0.006230740574312097 c 0.16473627 k 0.058432672 m 0.002690526\n",
      "173 Train Loss 1441.79 Test RE 0.00622605663414183 c 0.1648822 k 0.058430612 m 0.0026944636\n",
      "174 Train Loss 1441.6697 Test RE 0.006261489382024733 c 0.16537628 k 0.058428127 m 0.0027064756\n",
      "175 Train Loss 1441.6697 Test RE 0.006261489382024733 c 0.16537628 k 0.058428127 m 0.0027064756\n",
      "176 Train Loss 1441.6697 Test RE 0.006261489382024733 c 0.16537628 k 0.058428127 m 0.0027064756\n",
      "177 Train Loss 1441.6697 Test RE 0.006261489382024733 c 0.16537628 k 0.058428127 m 0.0027064756\n",
      "178 Train Loss 1441.6697 Test RE 0.006261489382024733 c 0.16537628 k 0.058428127 m 0.0027064756\n",
      "179 Train Loss 1441.6697 Test RE 0.006261489382024733 c 0.16537628 k 0.058428127 m 0.0027064756\n",
      "180 Train Loss 1441.6697 Test RE 0.006261489382024733 c 0.16537628 k 0.058428127 m 0.0027064756\n",
      "181 Train Loss 1441.6697 Test RE 0.006261489382024733 c 0.16537628 k 0.058428127 m 0.0027064756\n",
      "182 Train Loss 1441.6697 Test RE 0.006261489382024733 c 0.16537628 k 0.058428127 m 0.0027064756\n",
      "183 Train Loss 1441.6697 Test RE 0.006261489382024733 c 0.16537628 k 0.058428127 m 0.0027064756\n",
      "184 Train Loss 1441.6697 Test RE 0.006261489382024733 c 0.16537628 k 0.058428127 m 0.0027064756\n",
      "185 Train Loss 1441.6697 Test RE 0.006261489382024733 c 0.16537628 k 0.058428127 m 0.0027064756\n",
      "186 Train Loss 1441.6697 Test RE 0.006261489382024733 c 0.16537628 k 0.058428127 m 0.0027064756\n",
      "187 Train Loss 1441.6697 Test RE 0.006261489382024733 c 0.16537628 k 0.058428127 m 0.0027064756\n",
      "188 Train Loss 1441.6697 Test RE 0.006261489382024733 c 0.16537628 k 0.058428127 m 0.0027064756\n",
      "189 Train Loss 1441.6697 Test RE 0.006261489382024733 c 0.16537628 k 0.058428127 m 0.0027064756\n",
      "190 Train Loss 1441.6697 Test RE 0.006261489382024733 c 0.16537628 k 0.058428127 m 0.0027064756\n",
      "191 Train Loss 1441.6697 Test RE 0.006261489382024733 c 0.16537628 k 0.058428127 m 0.0027064756\n",
      "192 Train Loss 1441.6697 Test RE 0.006261489382024733 c 0.16537628 k 0.058428127 m 0.0027064756\n",
      "193 Train Loss 1441.6697 Test RE 0.006261489382024733 c 0.16537628 k 0.058428127 m 0.0027064756\n",
      "194 Train Loss 1441.6697 Test RE 0.006261489382024733 c 0.16537628 k 0.058428127 m 0.0027064756\n",
      "195 Train Loss 1441.6697 Test RE 0.006261489382024733 c 0.16537628 k 0.058428127 m 0.0027064756\n",
      "196 Train Loss 1441.6697 Test RE 0.006261489382024733 c 0.16537628 k 0.058428127 m 0.0027064756\n",
      "197 Train Loss 1441.6697 Test RE 0.006261489382024733 c 0.16537628 k 0.058428127 m 0.0027064756\n",
      "198 Train Loss 1441.6697 Test RE 0.006261489382024733 c 0.16537628 k 0.058428127 m 0.0027064756\n",
      "199 Train Loss 1441.6697 Test RE 0.006261489382024733 c 0.16537628 k 0.058428127 m 0.0027064756\n",
      "Training time: 84.89\n",
      "Training time: 84.89\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 2017620.5 Test RE 0.9518312564568756 c -0.0012920811 k 0.08364993 m -4.365778e-06\n",
      "1 Train Loss 1638175.9 Test RE 0.8578019206380438 c -0.0013464841 k 0.09115536 m -4.953583e-06\n",
      "2 Train Loss 949289.6 Test RE 0.6536779130556236 c -0.0021147633 k 0.15432392 m -1.0441726e-05\n",
      "3 Train Loss 547762.4 Test RE 0.4927620081979067 c -0.0025194974 k 0.20453483 m -1.2708011e-05\n",
      "4 Train Loss 361387.4 Test RE 0.3840788249921622 c -0.0026852957 k 0.22957267 m 2.1260323e-05\n",
      "5 Train Loss 261604.66 Test RE 0.3076227871303936 c -0.0027501686 k 0.23855147 m 7.7129946e-05\n",
      "6 Train Loss 165084.9 Test RE 0.22458490260902306 c -0.0028014758 k 0.22875468 m 0.00023685512\n",
      "7 Train Loss 151764.75 Test RE 0.21596990985616493 c -0.0028227847 k 0.22253352 m 0.00029490193\n",
      "8 Train Loss 139822.12 Test RE 0.20531980023404375 c -0.0028971855 k 0.21597299 m 0.00037756484\n",
      "9 Train Loss 106841.516 Test RE 0.1742288773562744 c -0.003086565 k 0.20264402 m 0.0006168605\n",
      "10 Train Loss 92580.89 Test RE 0.16410998568559146 c -0.0031912422 k 0.19271123 m 0.00079073175\n",
      "11 Train Loss 85528.375 Test RE 0.16100696403937623 c -0.0032863615 k 0.17996 m 0.0010043005\n",
      "12 Train Loss 72206.75 Test RE 0.1656045957328953 c -0.0036546632 k 0.13260329 m 0.0020145138\n",
      "13 Train Loss 54718.6 Test RE 0.14911316430554927 c -0.00387461 k 0.10020477 m 0.0028180175\n",
      "14 Train Loss 48483.48 Test RE 0.14448720532983866 c -0.00400916 k 0.06939538 m 0.0037150113\n",
      "15 Train Loss 39959.688 Test RE 0.12732777934095654 c -0.004120548 k 0.030358968 m 0.0049535614\n",
      "16 Train Loss 28442.158 Test RE 0.106077549281197 c -0.0039469474 k 0.033340983 m 0.005643447\n",
      "17 Train Loss 23719.79 Test RE 0.09921821871152428 c -0.0037190176 k 0.06240349 m 0.0054868152\n",
      "18 Train Loss 22226.092 Test RE 0.09489696550161404 c -0.003707474 k 0.075338595 m 0.0053741783\n",
      "19 Train Loss 19931.754 Test RE 0.08898882304753007 c -0.0038098083 k 0.07565628 m 0.005589615\n",
      "20 Train Loss 18324.736 Test RE 0.0861676126813862 c -0.0037132148 k 0.059071895 m 0.005911603\n",
      "21 Train Loss 14875.222 Test RE 0.07582109128001513 c -0.0034345896 k 0.05415384 m 0.005870439\n",
      "22 Train Loss 11890.592 Test RE 0.06341334450307116 c -0.003032216 k 0.03937939 m 0.0063377162\n",
      "23 Train Loss 8483.469 Test RE 0.049345925028202266 c -0.002292703 k 0.03753604 m 0.0068249195\n",
      "24 Train Loss 6062.5605 Test RE 0.0433560631298976 c -0.0019228895 k 0.056097846 m 0.006813812\n",
      "25 Train Loss 5626.51 Test RE 0.04142504433543615 c -0.0018587277 k 0.06158623 m 0.006964749\n",
      "26 Train Loss 4477.0645 Test RE 0.03455958728540023 c -0.0016192956 k 0.058374923 m 0.00782602\n",
      "27 Train Loss 3945.9573 Test RE 0.030791767731510118 c -0.0010849931 k 0.058281653 m 0.008563932\n",
      "28 Train Loss 3217.5881 Test RE 0.02529225697090377 c 0.00019345351 k 0.060810577 m 0.009674623\n",
      "29 Train Loss 3104.3691 Test RE 0.024230530227682033 c 0.00037116828 k 0.060442824 m 0.009800245\n",
      "30 Train Loss 2911.68 Test RE 0.022445647151494105 c 0.00071730226 k 0.05981544 m 0.009778937\n",
      "31 Train Loss 2792.8066 Test RE 0.021223855622912638 c 0.000700526 k 0.059109077 m 0.009566951\n",
      "32 Train Loss 2756.1223 Test RE 0.020855647650682766 c 0.00059798977 k 0.059253927 m 0.009423729\n",
      "33 Train Loss 2722.3953 Test RE 0.0205948334478351 c 0.00078227784 k 0.060732547 m 0.0093573965\n",
      "34 Train Loss 2529.3103 Test RE 0.018351262756334207 c 0.0017521798 k 0.061907742 m 0.009146378\n",
      "35 Train Loss 2404.8032 Test RE 0.01662866684584316 c 0.0015074534 k 0.059143312 m 0.0089781275\n",
      "36 Train Loss 2324.0098 Test RE 0.015639179837038185 c 0.0015642875 k 0.06052339 m 0.008802254\n",
      "37 Train Loss 2265.6277 Test RE 0.014889458641748694 c 0.0020538447 k 0.060731813 m 0.008817745\n",
      "38 Train Loss 2165.2568 Test RE 0.013219765228881623 c 0.002991479 k 0.060268193 m 0.0090413755\n",
      "39 Train Loss 2138.507 Test RE 0.012857750269465316 c 0.0038747564 k 0.060868986 m 0.009203175\n",
      "40 Train Loss 2127.892 Test RE 0.012719102487009668 c 0.004505621 k 0.060687657 m 0.009371213\n",
      "41 Train Loss 2121.1934 Test RE 0.012618804710579875 c 0.005602416 k 0.060520817 m 0.009653022\n",
      "42 Train Loss 2105.643 Test RE 0.012471475814051667 c 0.007894647 k 0.061091322 m 0.010242164\n",
      "43 Train Loss 2076.9268 Test RE 0.011904041234970218 c 0.009272549 k 0.06071374 m 0.010762703\n",
      "44 Train Loss 2059.942 Test RE 0.011664335810215502 c 0.010150412 k 0.06030507 m 0.011106473\n",
      "45 Train Loss 2055.1755 Test RE 0.011627419472579621 c 0.010780481 k 0.06057161 m 0.011277936\n",
      "46 Train Loss 2053.4028 Test RE 0.0115157396429816 c 0.011040095 k 0.0606101 m 0.011372054\n",
      "47 Train Loss 2049.802 Test RE 0.011441973886387492 c 0.01130817 k 0.060188033 m 0.011456315\n",
      "48 Train Loss 2042.1195 Test RE 0.011441663173504833 c 0.012093763 k 0.060462624 m 0.011605919\n",
      "49 Train Loss 2034.8601 Test RE 0.011270691386804976 c 0.012356893 k 0.061747324 m 0.01153634\n",
      "50 Train Loss 2024.1029 Test RE 0.011038551998275095 c 0.012681367 k 0.06144741 m 0.011516217\n",
      "51 Train Loss 1972.1383 Test RE 0.010180701410288253 c 0.016416239 k 0.059750553 m 0.011954026\n",
      "52 Train Loss 1951.584 Test RE 0.009948907300530796 c 0.018216468 k 0.06058726 m 0.012204496\n",
      "53 Train Loss 1949.0826 Test RE 0.009910293856673281 c 0.018510645 k 0.060895465 m 0.012293872\n",
      "54 Train Loss 1948.794 Test RE 0.009911985321102433 c 0.018565252 k 0.060761176 m 0.012315392\n",
      "55 Train Loss 1944.0314 Test RE 0.009863707701412176 c 0.019599902 k 0.06019741 m 0.012486656\n",
      "56 Train Loss 1876.6218 Test RE 0.008786962174434452 c 0.032281376 k 0.059195206 m 0.013887007\n",
      "57 Train Loss 1809.1926 Test RE 0.007746494141883517 c 0.04300442 k 0.061098173 m 0.015012336\n",
      "58 Train Loss 1803.7146 Test RE 0.007853222089601397 c 0.045833997 k 0.06055013 m 0.0153384805\n",
      "59 Train Loss 1802.2303 Test RE 0.007822614507804053 c 0.04585798 k 0.060509734 m 0.015322567\n",
      "60 Train Loss 1801.8873 Test RE 0.00779354415314595 c 0.045532033 k 0.060628574 m 0.015290694\n",
      "61 Train Loss 1799.1044 Test RE 0.007837749941972453 c 0.04719886 k 0.060727295 m 0.015594337\n",
      "62 Train Loss 1795.2883 Test RE 0.00815860426183365 c 0.05343997 k 0.060380537 m 0.01641585\n",
      "63 Train Loss 1787.5514 Test RE 0.008855320194079625 c 0.06812908 k 0.06012107 m 0.018326629\n",
      "64 Train Loss 1779.8474 Test RE 0.009391042681389447 c 0.080953985 k 0.0599967 m 0.019932901\n",
      "65 Train Loss 1778.273 Test RE 0.009677006838495095 c 0.08691571 k 0.059764143 m 0.020629544\n",
      "66 Train Loss 1773.7295 Test RE 0.010106021887031328 c 0.09702129 k 0.059693795 m 0.021672811\n",
      "67 Train Loss 1748.8265 Test RE 0.010314288710129486 c 0.113113694 k 0.059762944 m 0.023143493\n",
      "68 Train Loss 1658.0452 Test RE 0.008577511537290493 c 0.121843025 k 0.058722477 m 0.02330608\n",
      "69 Train Loss 1600.4932 Test RE 0.008792040367777122 c 0.15181154 k 0.058778305 m 0.025802182\n",
      "70 Train Loss 1559.6948 Test RE 0.011527414387125358 c 0.23653823 k 0.057509642 m 0.03352496\n",
      "71 Train Loss 1378.652 Test RE 0.010626149674608837 c 0.29750574 k 0.056716613 m 0.038392633\n",
      "72 Train Loss 1328.0127 Test RE 0.010499850839578596 c 0.32331994 k 0.05650499 m 0.0402793\n",
      "73 Train Loss 1281.0443 Test RE 0.010016361943420348 c 0.33382496 k 0.05605791 m 0.04126204\n",
      "74 Train Loss 1238.4026 Test RE 0.010103950398117872 c 0.380377 k 0.055667177 m 0.04559936\n",
      "75 Train Loss 1159.761 Test RE 0.009210790151798773 c 0.43542445 k 0.05464386 m 0.051059045\n",
      "76 Train Loss 1025.7341 Test RE 0.008676684781262047 c 0.5023765 k 0.054872595 m 0.057743788\n",
      "77 Train Loss 978.5131 Test RE 0.007823589711422992 c 0.51560795 k 0.053419538 m 0.05931294\n",
      "78 Train Loss 933.4083 Test RE 0.00809417702607322 c 0.554642 k 0.053587575 m 0.06331245\n",
      "79 Train Loss 903.14087 Test RE 0.007767817452130907 c 0.563352 k 0.052450426 m 0.06425466\n",
      "80 Train Loss 831.8947 Test RE 0.006997997158535299 c 0.5811733 k 0.05262333 m 0.06670236\n",
      "81 Train Loss 802.1246 Test RE 0.007315670655349008 c 0.59831303 k 0.052753404 m 0.068471715\n",
      "82 Train Loss 792.3338 Test RE 0.007079580423653509 c 0.5962139 k 0.052411728 m 0.06833448\n",
      "83 Train Loss 786.08905 Test RE 0.006573323952966207 c 0.5910375 k 0.05285305 m 0.06781053\n",
      "84 Train Loss 776.3049 Test RE 0.006848037961835395 c 0.60306406 k 0.05233298 m 0.06892563\n",
      "85 Train Loss 763.8386 Test RE 0.007443724035194583 c 0.62518626 k 0.051661078 m 0.071195945\n",
      "86 Train Loss 743.3124 Test RE 0.006922888888501968 c 0.64502805 k 0.05200982 m 0.073361754\n",
      "87 Train Loss 703.2105 Test RE 0.0069409759529597504 c 0.66783786 k 0.051483 m 0.07571474\n",
      "88 Train Loss 680.6605 Test RE 0.0069959892130546895 c 0.684506 k 0.051118597 m 0.077635325\n",
      "89 Train Loss 662.50476 Test RE 0.007236701160691854 c 0.7009069 k 0.05099081 m 0.079362966\n",
      "90 Train Loss 636.62555 Test RE 0.0069848564229635 c 0.70812625 k 0.050683234 m 0.08016819\n",
      "91 Train Loss 621.7627 Test RE 0.00666008777734035 c 0.71389 k 0.05084578 m 0.08082028\n",
      "92 Train Loss 610.9231 Test RE 0.006592633984894694 c 0.7248225 k 0.050739575 m 0.08190168\n",
      "93 Train Loss 606.6591 Test RE 0.006704261902733325 c 0.7232337 k 0.050683785 m 0.081697956\n",
      "94 Train Loss 595.5326 Test RE 0.006247345165577857 c 0.7211986 k 0.05091492 m 0.08135376\n",
      "95 Train Loss 580.72687 Test RE 0.005734303047760693 c 0.73195255 k 0.050661217 m 0.0823766\n",
      "96 Train Loss 523.05695 Test RE 0.005928596633898226 c 0.75369257 k 0.049883705 m 0.084396444\n",
      "97 Train Loss 479.42932 Test RE 0.006074800111800764 c 0.79101837 k 0.050280273 m 0.08775575\n",
      "98 Train Loss 472.6697 Test RE 0.006286474812034604 c 0.7985958 k 0.0495232 m 0.08843281\n",
      "99 Train Loss 469.87943 Test RE 0.006338812030578667 c 0.8064983 k 0.049578495 m 0.089222394\n",
      "100 Train Loss 468.48212 Test RE 0.006305647014128616 c 0.81228137 k 0.04946131 m 0.08981593\n",
      "101 Train Loss 467.91843 Test RE 0.00643099889276209 c 0.81327945 k 0.049275782 m 0.08991576\n",
      "102 Train Loss 467.51566 Test RE 0.006484074191053052 c 0.8152275 k 0.04945424 m 0.09010988\n",
      "103 Train Loss 466.55975 Test RE 0.006510395010096677 c 0.8192193 k 0.049523327 m 0.090453684\n",
      "104 Train Loss 465.25824 Test RE 0.006730866992891276 c 0.8276925 k 0.049068425 m 0.091227725\n",
      "105 Train Loss 457.60934 Test RE 0.00692951577998376 c 0.8529514 k 0.049044803 m 0.093681216\n",
      "106 Train Loss 451.98676 Test RE 0.0065114448290432905 c 0.8620669 k 0.048716776 m 0.09458337\n",
      "107 Train Loss 450.2493 Test RE 0.006523868367008092 c 0.8613779 k 0.04899706 m 0.0944726\n",
      "108 Train Loss 446.65842 Test RE 0.006702383904966763 c 0.86694425 k 0.048755072 m 0.09501696\n",
      "109 Train Loss 439.09183 Test RE 0.006759736344609526 c 0.88137454 k 0.048499875 m 0.09646512\n",
      "110 Train Loss 427.49167 Test RE 0.0062681373820725715 c 0.88162845 k 0.048586503 m 0.09650168\n",
      "111 Train Loss 418.03772 Test RE 0.006125792097562657 c 0.8794625 k 0.04850968 m 0.096350916\n",
      "112 Train Loss 416.74454 Test RE 0.0062056561064821715 c 0.8814027 k 0.048534878 m 0.09655542\n",
      "113 Train Loss 415.07452 Test RE 0.006246027981830314 c 0.879657 k 0.04840736 m 0.09641076\n",
      "114 Train Loss 414.2458 Test RE 0.006157086084660684 c 0.8821638 k 0.04846054 m 0.09667386\n",
      "115 Train Loss 414.0397 Test RE 0.006195018017526505 c 0.88388085 k 0.04843779 m 0.09684493\n",
      "116 Train Loss 414.0088 Test RE 0.006186153957718757 c 0.8829377 k 0.048431616 m 0.09675934\n",
      "117 Train Loss 413.77747 Test RE 0.006074163681352293 c 0.8805128 k 0.04836667 m 0.0965259\n",
      "118 Train Loss 410.6501 Test RE 0.005767091218871081 c 0.889271 k 0.047894385 m 0.097408816\n",
      "119 Train Loss 403.78784 Test RE 0.005643484349641751 c 0.9046275 k 0.047726616 m 0.09905181\n",
      "120 Train Loss 389.26056 Test RE 0.005688225112371254 c 0.92117167 k 0.047941912 m 0.10088073\n",
      "121 Train Loss 377.40622 Test RE 0.005301490002207137 c 0.93816173 k 0.047728505 m 0.10266842\n",
      "122 Train Loss 371.3655 Test RE 0.005212742468392177 c 0.9616961 k 0.047309544 m 0.10516477\n",
      "123 Train Loss 370.31616 Test RE 0.005292353213742691 c 0.960403 k 0.047278576 m 0.10508338\n",
      "124 Train Loss 370.02808 Test RE 0.005258971497662058 c 0.9577361 k 0.047270957 m 0.10480666\n",
      "125 Train Loss 369.92926 Test RE 0.005257150479642307 c 0.95886874 k 0.04733665 m 0.10492566\n",
      "126 Train Loss 369.90448 Test RE 0.005266042755658778 c 0.9596215 k 0.047333155 m 0.105003506\n",
      "127 Train Loss 369.86926 Test RE 0.005280499434884393 c 0.9598081 k 0.047277104 m 0.105029896\n",
      "128 Train Loss 369.7716 Test RE 0.0053077997082301535 c 0.9585226 k 0.047262322 m 0.10490474\n",
      "129 Train Loss 369.50854 Test RE 0.005330771314474208 c 0.9576801 k 0.04742286 m 0.104797885\n",
      "130 Train Loss 368.82153 Test RE 0.0052765940634932566 c 0.95888543 k 0.047481902 m 0.10491972\n",
      "131 Train Loss 361.34396 Test RE 0.004845454456639686 c 0.97172064 k 0.047099635 m 0.106441535\n",
      "132 Train Loss 346.75784 Test RE 0.004967834804371834 c 1.0029603 k 0.046517625 m 0.10990804\n",
      "133 Train Loss 345.7374 Test RE 0.0050920599005300805 c 1.0029889 k 0.046754986 m 0.1098966\n",
      "134 Train Loss 345.4367 Test RE 0.005149468077768154 c 1.0001955 k 0.0468303 m 0.10960062\n",
      "135 Train Loss 345.35867 Test RE 0.005158948670919276 c 0.99836886 k 0.046809223 m 0.109405965\n",
      "136 Train Loss 345.2758 Test RE 0.0051574713329735015 c 0.9976773 k 0.046799 m 0.10933495\n",
      "137 Train Loss 345.2437 Test RE 0.005162620887674857 c 0.9986691 k 0.046758417 m 0.10943993\n",
      "138 Train Loss 345.1748 Test RE 0.00517516771201207 c 0.9990134 k 0.046751067 m 0.10947922\n",
      "139 Train Loss 344.27213 Test RE 0.005186472509075664 c 0.99831635 k 0.046991922 m 0.10946495\n",
      "140 Train Loss 338.4376 Test RE 0.005070395042621172 c 1.0031998 k 0.047563713 m 0.11019246\n",
      "141 Train Loss 330.08636 Test RE 0.0050366689949629204 c 1.0020306 k 0.04666993 m 0.11022163\n",
      "142 Train Loss 328.61783 Test RE 0.005017788316573383 c 1.0025721 k 0.04666254 m 0.1103616\n",
      "143 Train Loss 328.1614 Test RE 0.005055266626740451 c 1.0093873 k 0.046661697 m 0.111060396\n",
      "144 Train Loss 327.6527 Test RE 0.005065343024405198 c 1.0222448 k 0.0464094 m 0.11237458\n",
      "145 Train Loss 327.37143 Test RE 0.00510610002314938 c 1.0234948 k 0.046364028 m 0.112528145\n",
      "146 Train Loss 327.27014 Test RE 0.0051046514294770225 c 1.024518 k 0.046384595 m 0.11264188\n",
      "147 Train Loss 327.2633 Test RE 0.005112152864876397 c 1.0249943 k 0.04636995 m 0.11269362\n",
      "148 Train Loss 327.2618 Test RE 0.005121711293599091 c 1.0253414 k 0.04635885 m 0.11273234\n",
      "149 Train Loss 327.2495 Test RE 0.005132541741164565 c 1.0245916 k 0.046390645 m 0.11265277\n",
      "150 Train Loss 327.2027 Test RE 0.0051499096286446175 c 1.0237758 k 0.046390083 m 0.11256902\n",
      "151 Train Loss 326.5656 Test RE 0.005178356480040287 c 1.0309594 k 0.046194002 m 0.11335987\n",
      "152 Train Loss 323.789 Test RE 0.005271166752065196 c 1.0449429 k 0.046360265 m 0.11509157\n",
      "153 Train Loss 317.32022 Test RE 0.005248878083147498 c 1.0595254 k 0.046386775 m 0.117201\n",
      "154 Train Loss 311.71268 Test RE 0.004741081392809272 c 1.0572199 k 0.04566918 m 0.11718224\n",
      "155 Train Loss 309.09332 Test RE 0.004702822506828479 c 1.0598004 k 0.045837358 m 0.1176562\n",
      "156 Train Loss 308.33203 Test RE 0.004889361071867039 c 1.0682465 k 0.045700233 m 0.11862125\n",
      "157 Train Loss 308.12308 Test RE 0.004912678362411716 c 1.0711982 k 0.04564634 m 0.11889769\n",
      "158 Train Loss 307.82556 Test RE 0.004900239579425124 c 1.0644172 k 0.045788623 m 0.11816593\n",
      "159 Train Loss 307.56082 Test RE 0.004918740156942094 c 1.0630857 k 0.04583122 m 0.118044965\n",
      "160 Train Loss 307.5151 Test RE 0.004913403365989484 c 1.063415 k 0.04582266 m 0.118079886\n",
      "161 Train Loss 307.5013 Test RE 0.004921744607009873 c 1.0618407 k 0.045840755 m 0.11789916\n",
      "162 Train Loss 307.5013 Test RE 0.004921744607009873 c 1.0618407 k 0.045840755 m 0.11789916\n",
      "163 Train Loss 307.5013 Test RE 0.004921744607009873 c 1.0618407 k 0.045840755 m 0.11789916\n",
      "164 Train Loss 307.5013 Test RE 0.004921744607009873 c 1.0618407 k 0.045840755 m 0.11789916\n",
      "165 Train Loss 307.5013 Test RE 0.004921744607009873 c 1.0618407 k 0.045840755 m 0.11789916\n",
      "166 Train Loss 307.5013 Test RE 0.004921744607009873 c 1.0618407 k 0.045840755 m 0.11789916\n",
      "167 Train Loss 307.5013 Test RE 0.004921744607009873 c 1.0618407 k 0.045840755 m 0.11789916\n",
      "168 Train Loss 307.5013 Test RE 0.004921744607009873 c 1.0618407 k 0.045840755 m 0.11789916\n",
      "169 Train Loss 307.5013 Test RE 0.004921744607009873 c 1.0618407 k 0.045840755 m 0.11789916\n",
      "170 Train Loss 307.5013 Test RE 0.004921744607009873 c 1.0618407 k 0.045840755 m 0.11789916\n",
      "171 Train Loss 307.5013 Test RE 0.004921744607009873 c 1.0618407 k 0.045840755 m 0.11789916\n",
      "172 Train Loss 307.5013 Test RE 0.004921744607009873 c 1.0618407 k 0.045840755 m 0.11789916\n",
      "173 Train Loss 307.5013 Test RE 0.004921744607009873 c 1.0618407 k 0.045840755 m 0.11789916\n",
      "174 Train Loss 307.5013 Test RE 0.004921744607009873 c 1.0618407 k 0.045840755 m 0.11789916\n",
      "175 Train Loss 307.5013 Test RE 0.004921744607009873 c 1.0618407 k 0.045840755 m 0.11789916\n",
      "176 Train Loss 307.5013 Test RE 0.004921744607009873 c 1.0618407 k 0.045840755 m 0.11789916\n",
      "177 Train Loss 307.5013 Test RE 0.004921744607009873 c 1.0618407 k 0.045840755 m 0.11789916\n",
      "178 Train Loss 307.5013 Test RE 0.004921744607009873 c 1.0618407 k 0.045840755 m 0.11789916\n",
      "179 Train Loss 307.5013 Test RE 0.004921744607009873 c 1.0618407 k 0.045840755 m 0.11789916\n",
      "180 Train Loss 307.5013 Test RE 0.004921744607009873 c 1.0618407 k 0.045840755 m 0.11789916\n",
      "181 Train Loss 307.5013 Test RE 0.004921744607009873 c 1.0618407 k 0.045840755 m 0.11789916\n",
      "182 Train Loss 307.5013 Test RE 0.004921744607009873 c 1.0618407 k 0.045840755 m 0.11789916\n",
      "183 Train Loss 307.5013 Test RE 0.004921744607009873 c 1.0618407 k 0.045840755 m 0.11789916\n",
      "184 Train Loss 307.5013 Test RE 0.004921744607009873 c 1.0618407 k 0.045840755 m 0.11789916\n",
      "185 Train Loss 307.5013 Test RE 0.004921744607009873 c 1.0618407 k 0.045840755 m 0.11789916\n",
      "186 Train Loss 307.5013 Test RE 0.004921744607009873 c 1.0618407 k 0.045840755 m 0.11789916\n",
      "187 Train Loss 307.5013 Test RE 0.004921744607009873 c 1.0618407 k 0.045840755 m 0.11789916\n",
      "188 Train Loss 307.5013 Test RE 0.004921744607009873 c 1.0618407 k 0.045840755 m 0.11789916\n",
      "189 Train Loss 307.5013 Test RE 0.004921744607009873 c 1.0618407 k 0.045840755 m 0.11789916\n",
      "190 Train Loss 307.5013 Test RE 0.004921744607009873 c 1.0618407 k 0.045840755 m 0.11789916\n",
      "191 Train Loss 307.5013 Test RE 0.004921744607009873 c 1.0618407 k 0.045840755 m 0.11789916\n",
      "192 Train Loss 307.5013 Test RE 0.004921744607009873 c 1.0618407 k 0.045840755 m 0.11789916\n",
      "193 Train Loss 307.5013 Test RE 0.004921744607009873 c 1.0618407 k 0.045840755 m 0.11789916\n",
      "194 Train Loss 307.5013 Test RE 0.004921744607009873 c 1.0618407 k 0.045840755 m 0.11789916\n",
      "195 Train Loss 307.5013 Test RE 0.004921744607009873 c 1.0618407 k 0.045840755 m 0.11789916\n",
      "196 Train Loss 307.5013 Test RE 0.004921744607009873 c 1.0618407 k 0.045840755 m 0.11789916\n",
      "197 Train Loss 307.5013 Test RE 0.004921744607009873 c 1.0618407 k 0.045840755 m 0.11789916\n",
      "198 Train Loss 307.5013 Test RE 0.004921744607009873 c 1.0618407 k 0.045840755 m 0.11789916\n",
      "199 Train Loss 307.5013 Test RE 0.004921744607009873 c 1.0618407 k 0.045840755 m 0.11789916\n",
      "Training time: 76.74\n",
      "Training time: 76.74\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 2188201.5 Test RE 0.9911493394216098 c 0.00044206818 k 0.00051952695 m 4.5735902e-07\n",
      "1 Train Loss 2163200.2 Test RE 0.9854468015236181 c 0.0005308672 k 0.0028514843 m 3.8037424e-06\n",
      "2 Train Loss 2133943.5 Test RE 0.9787314894724136 c 0.0007501683 k 0.0071000196 m -2.5732209e-06\n",
      "3 Train Loss 2080124.9 Test RE 0.9662827534375173 c 0.00113451 k 0.016229117 m -1.3787338e-05\n",
      "4 Train Loss 1985336.5 Test RE 0.9439800179652001 c 0.001706212 k 0.031067574 m -3.493027e-05\n",
      "5 Train Loss 1834998.6 Test RE 0.9075911608946073 c 0.0025159535 k 0.054276653 m -6.3029016e-05\n",
      "6 Train Loss 1319481.2 Test RE 0.7694012240478897 c 0.0042058732 k 0.10245657 m -0.000113483184\n",
      "7 Train Loss 997808.2 Test RE 0.6676326058275871 c 0.0048371092 k 0.12196317 m -0.00011018384\n",
      "8 Train Loss 617035.5 Test RE 0.5260763730703586 c 0.005626449 k 0.14601617 m -0.00013741593\n",
      "9 Train Loss 378461.94 Test RE 0.40888466736132617 c 0.006209747 k 0.1626494 m -0.00015143967\n",
      "10 Train Loss 232242.7 Test RE 0.31604350842986917 c 0.0068257838 k 0.18038899 m -0.0001668456\n",
      "11 Train Loss 127053.25 Test RE 0.21513462978722298 c 0.0074081756 k 0.19659485 m -0.00017076652\n",
      "12 Train Loss 79045.09 Test RE 0.13783522804917822 c 0.0077799265 k 0.20411023 m -0.00016003045\n",
      "13 Train Loss 55891.26 Test RE 0.08272426515126198 c 0.008073073 k 0.20556961 m -0.00013468381\n",
      "14 Train Loss 43533.18 Test RE 0.04956677539932797 c 0.008139585 k 0.19454908 m -0.00010681205\n",
      "15 Train Loss 19148.252 Test RE 0.05709741654742606 c 0.007746795 k 0.1346628 m -1.72277e-05\n",
      "16 Train Loss 5222.3247 Test RE 0.033672387902704756 c 0.007377777 k 0.07920552 m 6.451679e-05\n",
      "17 Train Loss 2876.0234 Test RE 0.020510761202479737 c 0.007342319 k 0.05796793 m 0.000105782055\n",
      "18 Train Loss 2208.0889 Test RE 0.013267832478787274 c 0.0074584624 k 0.061767384 m 0.00010851884\n",
      "19 Train Loss 1900.5771 Test RE 0.008144255912342415 c 0.0077661984 k 0.06169191 m 0.00013195771\n",
      "20 Train Loss 1855.3177 Test RE 0.00630463177596539 c 0.007895227 k 0.06068771 m 0.00014534678\n",
      "21 Train Loss 1829.3538 Test RE 0.005579177179943807 c 0.007959165 k 0.060752016 m 0.00015220267\n",
      "22 Train Loss 1821.9481 Test RE 0.005373856593144779 c 0.008047851 k 0.060966443 m 0.0001602775\n",
      "23 Train Loss 1798.7019 Test RE 0.004116592344341258 c 0.008468984 k 0.06058493 m 0.00019684012\n",
      "24 Train Loss 1787.3292 Test RE 0.003674177596050897 c 0.008562721 k 0.06086933 m 0.00020475681\n",
      "25 Train Loss 1784.7286 Test RE 0.003447945084665214 c 0.008692337 k 0.060656965 m 0.00021616164\n",
      "26 Train Loss 1784.1526 Test RE 0.0032881245025874984 c 0.008817296 k 0.060880434 m 0.00022560873\n",
      "27 Train Loss 1777.0132 Test RE 0.0029783731132780864 c 0.010368077 k 0.06066769 m 0.0003484857\n",
      "28 Train Loss 1762.435 Test RE 0.0032509680272101607 c 0.015299296 k 0.060468156 m 0.00073929556\n",
      "29 Train Loss 1748.9453 Test RE 0.0027120923799282782 c 0.019423814 k 0.060918476 m 0.0010644544\n",
      "30 Train Loss 1744.9701 Test RE 0.0029599255907728993 c 0.021534488 k 0.06051978 m 0.0012320972\n",
      "31 Train Loss 1742.21 Test RE 0.0033485659504684097 c 0.023946242 k 0.060663816 m 0.0014235869\n",
      "32 Train Loss 1740.0757 Test RE 0.0030550554675825977 c 0.023700926 k 0.060566165 m 0.0014054702\n",
      "33 Train Loss 1736.1904 Test RE 0.0027392200456267143 c 0.024725853 k 0.060431186 m 0.0014907159\n",
      "34 Train Loss 1728.1621 Test RE 0.0028909169900894105 c 0.027928887 k 0.06079631 m 0.0017492784\n",
      "35 Train Loss 1724.3026 Test RE 0.003082714136291835 c 0.029722473 k 0.060612973 m 0.0018956825\n",
      "36 Train Loss 1723.0487 Test RE 0.0029082560155040777 c 0.029335467 k 0.060360137 m 0.0018650193\n",
      "37 Train Loss 1720.8958 Test RE 0.003220646071129979 c 0.03193349 k 0.06069282 m 0.0020723518\n",
      "38 Train Loss 1718.9554 Test RE 0.003539260904882395 c 0.03453662 k 0.060401153 m 0.0022806516\n",
      "39 Train Loss 1713.7976 Test RE 0.003748556647719958 c 0.037169248 k 0.06019354 m 0.0024898867\n",
      "40 Train Loss 1702.221 Test RE 0.0033358963646862284 c 0.03956941 k 0.060355235 m 0.0026787797\n",
      "41 Train Loss 1698.5424 Test RE 0.0029859656296951052 c 0.04028871 k 0.060223203 m 0.0027362579\n",
      "42 Train Loss 1693.8243 Test RE 0.0031697849051599935 c 0.042244043 k 0.060132913 m 0.0028907447\n",
      "43 Train Loss 1675.7589 Test RE 0.0035448576055372542 c 0.049947895 k 0.060138654 m 0.0035018842\n",
      "44 Train Loss 1654.0476 Test RE 0.002335234379394942 c 0.055737298 k 0.060309585 m 0.0039628386\n",
      "45 Train Loss 1633.6973 Test RE 0.0026848437733984327 c 0.06622375 k 0.058931153 m 0.0047976826\n",
      "46 Train Loss 1625.8866 Test RE 0.003228576154405476 c 0.0708041 k 0.059492696 m 0.0051603084\n",
      "47 Train Loss 1620.8313 Test RE 0.0031763647505496317 c 0.07268155 k 0.059711054 m 0.005308788\n",
      "48 Train Loss 1617.9756 Test RE 0.003108934452317254 c 0.07422012 k 0.05940987 m 0.005431683\n",
      "49 Train Loss 1611.8229 Test RE 0.003591037684806603 c 0.07977691 k 0.05997837 m 0.005872526\n",
      "50 Train Loss 1607.0211 Test RE 0.00405574649497142 c 0.08571725 k 0.059573583 m 0.006345597\n",
      "51 Train Loss 1590.4595 Test RE 0.004529762881389873 c 0.09859458 k 0.058885407 m 0.0073715663\n",
      "52 Train Loss 1583.7944 Test RE 0.004873629605144083 c 0.10414763 k 0.05932445 m 0.007812835\n",
      "53 Train Loss 1571.8372 Test RE 0.004737684567567138 c 0.10949596 k 0.05819314 m 0.0082418015\n",
      "54 Train Loss 1560.9406 Test RE 0.004472008766009401 c 0.11035778 k 0.05914792 m 0.008309399\n",
      "55 Train Loss 1552.3147 Test RE 0.0041394435816010456 c 0.11070049 k 0.059259657 m 0.008339732\n",
      "56 Train Loss 1548.5137 Test RE 0.003936391435096876 c 0.10995753 k 0.059271205 m 0.008283109\n",
      "57 Train Loss 1539.087 Test RE 0.003576593547032979 c 0.11096531 k 0.059551485 m 0.008369001\n",
      "58 Train Loss 1524.555 Test RE 0.003008305461956526 c 0.11448309 k 0.059462454 m 0.008659291\n",
      "59 Train Loss 1519.0411 Test RE 0.0031032008377367437 c 0.11589137 k 0.059448123 m 0.008777018\n",
      "60 Train Loss 1508.8265 Test RE 0.0031243904874107293 c 0.12064824 k 0.059390746 m 0.009165557\n",
      "61 Train Loss 1496.554 Test RE 0.0032413914101828375 c 0.12790693 k 0.059024505 m 0.009748907\n",
      "62 Train Loss 1487.444 Test RE 0.0034553508779384475 c 0.13280536 k 0.059054226 m 0.010141304\n",
      "63 Train Loss 1483.2351 Test RE 0.0035358499953110074 c 0.13425305 k 0.058942765 m 0.010258158\n",
      "64 Train Loss 1475.5461 Test RE 0.0033779076786166935 c 0.13435006 k 0.058965556 m 0.010271854\n",
      "65 Train Loss 1461.9219 Test RE 0.0029178948385545234 c 0.13817844 k 0.058953956 m 0.010585053\n",
      "66 Train Loss 1455.407 Test RE 0.0037009662713261074 c 0.14643924 k 0.058694173 m 0.011249384\n",
      "67 Train Loss 1431.748 Test RE 0.004381019224910424 c 0.16301551 k 0.05850065 m 0.0125920065\n",
      "68 Train Loss 1410.6582 Test RE 0.003468778296797631 c 0.16393583 k 0.058682628 m 0.01267144\n",
      "69 Train Loss 1404.1503 Test RE 0.0035173019476818206 c 0.1684646 k 0.058208086 m 0.013037685\n",
      "70 Train Loss 1391.4197 Test RE 0.0032290393724173885 c 0.17639491 k 0.05836397 m 0.013679902\n",
      "71 Train Loss 1367.8239 Test RE 0.002899279905145724 c 0.18199363 k 0.058289677 m 0.014135099\n",
      "72 Train Loss 1345.7795 Test RE 0.002874334688935877 c 0.19070427 k 0.058249485 m 0.0148405\n",
      "73 Train Loss 1322.7788 Test RE 0.00279005977793835 c 0.19908495 k 0.057682812 m 0.015516625\n",
      "74 Train Loss 1311.2657 Test RE 0.002514515534358549 c 0.20514503 k 0.058140345 m 0.016001223\n",
      "75 Train Loss 1293.5554 Test RE 0.0024381173850370253 c 0.21220432 k 0.057933655 m 0.016569523\n",
      "76 Train Loss 1289.1333 Test RE 0.0025772134817358726 c 0.21294726 k 0.057961375 m 0.016629688\n",
      "77 Train Loss 1285.6677 Test RE 0.0026456166712221416 c 0.21418984 k 0.058019843 m 0.016728321\n",
      "78 Train Loss 1283.8129 Test RE 0.0027154732203476728 c 0.21571416 k 0.057790596 m 0.016849998\n",
      "79 Train Loss 1273.2747 Test RE 0.0032881753049219947 c 0.22488315 k 0.058034074 m 0.017585678\n",
      "80 Train Loss 1257.6898 Test RE 0.003272431123354736 c 0.23342511 k 0.05810786 m 0.018273816\n",
      "81 Train Loss 1244.1516 Test RE 0.003115599911696314 c 0.2425186 k 0.05697963 m 0.019009434\n",
      "82 Train Loss 1209.3558 Test RE 0.0033019449718417445 c 0.2613674 k 0.05722488 m 0.020531379\n",
      "83 Train Loss 1185.7968 Test RE 0.003906621095786303 c 0.27379665 k 0.056898374 m 0.021540776\n",
      "84 Train Loss 1168.2292 Test RE 0.0036200581275086297 c 0.27795196 k 0.05730742 m 0.02187792\n",
      "85 Train Loss 1165.7572 Test RE 0.0035551380742580353 c 0.27965462 k 0.057090294 m 0.02201614\n",
      "86 Train Loss 1153.1742 Test RE 0.003854991047811161 c 0.28777933 k 0.057268396 m 0.022673527\n",
      "87 Train Loss 1138.8289 Test RE 0.003372208533654832 c 0.29186732 k 0.057414282 m 0.023005093\n",
      "88 Train Loss 1115.403 Test RE 0.0038394178826267463 c 0.30940795 k 0.057084046 m 0.024428133\n",
      "89 Train Loss 1062.2534 Test RE 0.0039059756576745536 c 0.3466549 k 0.056865785 m 0.027451593\n",
      "90 Train Loss 1018.0099 Test RE 0.004126819473146055 c 0.374928 k 0.055878907 m 0.029747255\n",
      "91 Train Loss 983.9152 Test RE 0.003862771046510485 c 0.38457358 k 0.055889107 m 0.030525468\n",
      "92 Train Loss 960.96924 Test RE 0.003867732198308565 c 0.39811462 k 0.055987332 m 0.031623736\n",
      "93 Train Loss 938.76715 Test RE 0.00433404679241325 c 0.41381282 k 0.05417536 m 0.032904424\n",
      "94 Train Loss 912.9304 Test RE 0.004775329099034213 c 0.42496008 k 0.054612346 m 0.03381301\n",
      "95 Train Loss 890.8088 Test RE 0.004105944704762555 c 0.43130177 k 0.05455902 m 0.03432724\n",
      "96 Train Loss 877.28094 Test RE 0.003713258785846286 c 0.43561786 k 0.053968072 m 0.034674205\n",
      "97 Train Loss 870.7073 Test RE 0.0037080617916813437 c 0.4389247 k 0.054408323 m 0.034939192\n",
      "98 Train Loss 859.8618 Test RE 0.0029792395476068282 c 0.44768658 k 0.053845283 m 0.03564798\n",
      "99 Train Loss 838.8994 Test RE 0.0025801440623796877 c 0.4611754 k 0.054123554 m 0.03673801\n",
      "100 Train Loss 834.01025 Test RE 0.0029006880021404135 c 0.46569374 k 0.05354045 m 0.03710504\n",
      "101 Train Loss 828.92084 Test RE 0.0029513686506943675 c 0.47510308 k 0.054163508 m 0.03786776\n",
      "102 Train Loss 821.9198 Test RE 0.003584888171724582 c 0.4855854 k 0.053965695 m 0.038719006\n",
      "103 Train Loss 813.92584 Test RE 0.0036944793721574117 c 0.49537984 k 0.053021714 m 0.03951692\n",
      "104 Train Loss 800.83527 Test RE 0.004165706392206813 c 0.50236344 k 0.05395595 m 0.040080838\n",
      "105 Train Loss 793.3974 Test RE 0.0039487675470784266 c 0.5036871 k 0.05375468 m 0.040184923\n",
      "106 Train Loss 774.066 Test RE 0.003165466235633338 c 0.50964826 k 0.053728826 m 0.040665757\n",
      "107 Train Loss 758.85345 Test RE 0.0032358917410724355 c 0.52005714 k 0.0530933 m 0.04150824\n",
      "108 Train Loss 753.8472 Test RE 0.0032826686104528986 c 0.52227175 k 0.053407177 m 0.041687567\n",
      "109 Train Loss 730.14185 Test RE 0.0031874708584338257 c 0.53711456 k 0.053177726 m 0.042888716\n",
      "110 Train Loss 703.7147 Test RE 0.0033323700786041617 c 0.56494987 k 0.052052397 m 0.045141052\n",
      "111 Train Loss 671.98303 Test RE 0.0038239619865233433 c 0.5887539 k 0.05253774 m 0.047062755\n",
      "112 Train Loss 642.3902 Test RE 0.004590620980547687 c 0.6133219 k 0.052545387 m 0.04903873\n",
      "113 Train Loss 639.17224 Test RE 0.0043175596961629705 c 0.6115194 k 0.052187078 m 0.048894852\n",
      "114 Train Loss 636.98224 Test RE 0.00400345181444788 c 0.6101855 k 0.052775666 m 0.048786387\n",
      "115 Train Loss 629.0228 Test RE 0.004039064153174804 c 0.6216211 k 0.05231699 m 0.04971116\n",
      "116 Train Loss 622.33246 Test RE 0.003427666843943941 c 0.6303537 k 0.05151212 m 0.050417863\n",
      "117 Train Loss 618.8908 Test RE 0.00347507555154432 c 0.63471913 k 0.051957186 m 0.05076859\n",
      "118 Train Loss 612.80804 Test RE 0.0037425119489094077 c 0.63652676 k 0.051724937 m 0.05091437\n",
      "119 Train Loss 608.98486 Test RE 0.004113738754820203 c 0.64167196 k 0.05165118 m 0.051326964\n",
      "120 Train Loss 598.2809 Test RE 0.004377163062114714 c 0.6519595 k 0.05232833 m 0.052148346\n",
      "121 Train Loss 595.10126 Test RE 0.004306147801893905 c 0.6534901 k 0.051526327 m 0.052272037\n",
      "122 Train Loss 593.6158 Test RE 0.004292734579781696 c 0.6544765 k 0.051458433 m 0.05234859\n",
      "123 Train Loss 591.9137 Test RE 0.004383956922434457 c 0.6577057 k 0.051726107 m 0.052603915\n",
      "124 Train Loss 585.7051 Test RE 0.004740538043428909 c 0.6669656 k 0.051414605 m 0.053343922\n",
      "125 Train Loss 581.4509 Test RE 0.00462537711856751 c 0.6667238 k 0.051476646 m 0.05332183\n",
      "126 Train Loss 579.50604 Test RE 0.004438889705610836 c 0.66881025 k 0.05141475 m 0.053485647\n",
      "127 Train Loss 578.7746 Test RE 0.004485438348514338 c 0.67014277 k 0.05132174 m 0.05359048\n",
      "128 Train Loss 578.23846 Test RE 0.004567707597958755 c 0.6705004 k 0.051386025 m 0.0536192\n",
      "129 Train Loss 574.4956 Test RE 0.0043861610546528955 c 0.67310476 k 0.051453266 m 0.053825792\n",
      "130 Train Loss 565.98553 Test RE 0.004706360769624466 c 0.6789263 k 0.051315244 m 0.054289445\n",
      "131 Train Loss 557.8967 Test RE 0.00442093793434558 c 0.681011 k 0.05119316 m 0.05445489\n",
      "132 Train Loss 552.70746 Test RE 0.004546392562634482 c 0.6789889 k 0.05152958 m 0.05428959\n",
      "133 Train Loss 550.9931 Test RE 0.00426653076946141 c 0.6788772 k 0.051320173 m 0.054279033\n",
      "134 Train Loss 549.2758 Test RE 0.004163679144882348 c 0.68072647 k 0.051116414 m 0.054425377\n",
      "135 Train Loss 547.73926 Test RE 0.00428669035480057 c 0.6834487 k 0.050883505 m 0.054643147\n",
      "136 Train Loss 546.3035 Test RE 0.00441030514306026 c 0.6861481 k 0.05087773 m 0.054854795\n",
      "137 Train Loss 539.90155 Test RE 0.00436613204124548 c 0.6870246 k 0.0507695 m 0.05491499\n",
      "138 Train Loss 537.616 Test RE 0.004536784612708389 c 0.688339 k 0.05067462 m 0.0550121\n",
      "139 Train Loss 537.0718 Test RE 0.004578189059724792 c 0.68872637 k 0.050583776 m 0.05504199\n",
      "140 Train Loss 536.92957 Test RE 0.004554918608496702 c 0.68890834 k 0.050557945 m 0.055056073\n",
      "141 Train Loss 535.9842 Test RE 0.004473027713971017 c 0.68900216 k 0.050555278 m 0.05506199\n",
      "142 Train Loss 534.944 Test RE 0.004470909665124424 c 0.69000816 k 0.050580774 m 0.055141687\n",
      "143 Train Loss 534.4659 Test RE 0.004400682527133687 c 0.6903538 k 0.050742596 m 0.055167235\n",
      "144 Train Loss 533.93243 Test RE 0.00426564215616968 c 0.68994117 k 0.050664086 m 0.05513266\n",
      "145 Train Loss 531.7742 Test RE 0.004348439890133714 c 0.69050384 k 0.050652143 m 0.055171907\n",
      "146 Train Loss 530.512 Test RE 0.00426205280912362 c 0.68997 k 0.05073405 m 0.055124354\n",
      "147 Train Loss 529.6268 Test RE 0.004185599645694303 c 0.68985647 k 0.050580233 m 0.055112254\n",
      "148 Train Loss 529.19684 Test RE 0.00428044224275873 c 0.6894088 k 0.050558068 m 0.055076513\n",
      "149 Train Loss 528.67645 Test RE 0.00426243771891315 c 0.69008243 k 0.050492633 m 0.05513044\n",
      "150 Train Loss 522.8273 Test RE 0.004347755407607318 c 0.6959563 k 0.050635327 m 0.055575512\n",
      "151 Train Loss 509.8592 Test RE 0.004090314284489173 c 0.70998585 k 0.050623354 m 0.05661485\n",
      "152 Train Loss 501.18143 Test RE 0.004252617151373157 c 0.71584415 k 0.050450083 m 0.057042852\n",
      "153 Train Loss 494.1073 Test RE 0.005135533052825629 c 0.72473335 k 0.05006067 m 0.057719734\n",
      "154 Train Loss 484.77536 Test RE 0.004853172705641595 c 0.72994196 k 0.050616585 m 0.058120973\n",
      "155 Train Loss 475.3746 Test RE 0.004899534014408282 c 0.7423262 k 0.05055817 m 0.059072543\n",
      "156 Train Loss 469.80615 Test RE 0.005227266421509517 c 0.7546769 k 0.049944688 m 0.060039166\n",
      "157 Train Loss 456.69293 Test RE 0.005344857581500989 c 0.77383906 k 0.050169818 m 0.061483573\n",
      "158 Train Loss 443.80417 Test RE 0.00493397573500859 c 0.7830934 k 0.04965464 m 0.062186867\n",
      "159 Train Loss 440.98547 Test RE 0.004969299699520631 c 0.7883602 k 0.049728878 m 0.06257998\n",
      "160 Train Loss 439.91495 Test RE 0.005199937423834671 c 0.79128397 k 0.049713228 m 0.06278665\n",
      "161 Train Loss 437.9944 Test RE 0.005398407139514616 c 0.7932396 k 0.049517266 m 0.06292197\n",
      "162 Train Loss 436.9494 Test RE 0.0053909354560963945 c 0.7934712 k 0.04966116 m 0.06293826\n",
      "163 Train Loss 436.5454 Test RE 0.005487860521202576 c 0.7943114 k 0.049636148 m 0.06300111\n",
      "164 Train Loss 436.40448 Test RE 0.005401516217804206 c 0.79412705 k 0.049646948 m 0.062988386\n",
      "165 Train Loss 436.29553 Test RE 0.005321779833569275 c 0.79419154 k 0.049715724 m 0.06299118\n",
      "166 Train Loss 435.98096 Test RE 0.005411597542706604 c 0.7952533 k 0.049608227 m 0.06307161\n",
      "167 Train Loss 435.71912 Test RE 0.005460750925490901 c 0.79621 k 0.04959972 m 0.06314805\n",
      "168 Train Loss 434.927 Test RE 0.005419681481515193 c 0.79644746 k 0.04979808 m 0.06316522\n",
      "169 Train Loss 434.29584 Test RE 0.005427352346721812 c 0.79596364 k 0.04960584 m 0.0631315\n",
      "170 Train Loss 433.77087 Test RE 0.00537137446159929 c 0.79743564 k 0.049486257 m 0.0632454\n",
      "171 Train Loss 432.6374 Test RE 0.005318674355679299 c 0.7992382 k 0.04966375 m 0.063376874\n",
      "172 Train Loss 431.3282 Test RE 0.005371195376641654 c 0.79997593 k 0.049587343 m 0.06343813\n",
      "173 Train Loss 426.29218 Test RE 0.005061326770182912 c 0.7984803 k 0.04935109 m 0.063331045\n",
      "174 Train Loss 424.70438 Test RE 0.005206769722972581 c 0.80281293 k 0.049283125 m 0.06364249\n",
      "175 Train Loss 422.57718 Test RE 0.0052577019070406275 c 0.8068298 k 0.049414318 m 0.06393685\n",
      "176 Train Loss 421.02414 Test RE 0.005129168392168519 c 0.8090489 k 0.04939344 m 0.06409731\n",
      "177 Train Loss 418.75058 Test RE 0.0049771660275024065 c 0.81135535 k 0.049350206 m 0.06427557\n",
      "178 Train Loss 414.21674 Test RE 0.004990461410336786 c 0.81386864 k 0.04918842 m 0.06447805\n",
      "179 Train Loss 410.96982 Test RE 0.005342478059045887 c 0.81813395 k 0.049124695 m 0.0648034\n",
      "180 Train Loss 409.2242 Test RE 0.005285922475064131 c 0.82081443 k 0.049112383 m 0.065004416\n",
      "181 Train Loss 405.7985 Test RE 0.005409066155063995 c 0.82737935 k 0.049137294 m 0.06548499\n",
      "182 Train Loss 403.2142 Test RE 0.005291981445547551 c 0.8319478 k 0.04883121 m 0.06582621\n",
      "183 Train Loss 399.76764 Test RE 0.0049877623572404765 c 0.8408657 k 0.049033888 m 0.0665019\n",
      "184 Train Loss 394.55145 Test RE 0.005184626135735528 c 0.85644966 k 0.048777368 m 0.067657575\n",
      "185 Train Loss 391.1674 Test RE 0.005440993342324265 c 0.8654357 k 0.048316672 m 0.068316266\n",
      "186 Train Loss 388.52707 Test RE 0.0056451154202818155 c 0.8660002 k 0.04837863 m 0.06838232\n",
      "187 Train Loss 386.96915 Test RE 0.005598084474431342 c 0.86576056 k 0.048627764 m 0.06836377\n",
      "188 Train Loss 385.03723 Test RE 0.0058031265927906825 c 0.8723589 k 0.04832959 m 0.06885471\n",
      "189 Train Loss 384.32895 Test RE 0.005768362598736837 c 0.8748967 k 0.048346132 m 0.06904156\n",
      "190 Train Loss 384.05713 Test RE 0.005741469790393896 c 0.8742367 k 0.048315793 m 0.06899449\n",
      "191 Train Loss 383.83838 Test RE 0.0058390192102310045 c 0.8740763 k 0.04829717 m 0.06898257\n",
      "192 Train Loss 383.44025 Test RE 0.005863728475487343 c 0.8736267 k 0.048320696 m 0.06894646\n",
      "193 Train Loss 380.9264 Test RE 0.0057353666486027926 c 0.8763766 k 0.048046052 m 0.069155455\n",
      "194 Train Loss 374.7291 Test RE 0.00556195291526107 c 0.8850791 k 0.04821935 m 0.06980547\n",
      "195 Train Loss 372.72357 Test RE 0.005397304676418697 c 0.88713986 k 0.048323024 m 0.06995328\n",
      "196 Train Loss 371.2357 Test RE 0.0054886185396767605 c 0.8867153 k 0.04833037 m 0.06992337\n",
      "197 Train Loss 370.68936 Test RE 0.0054774153012397695 c 0.8866966 k 0.048329756 m 0.069923185\n",
      "198 Train Loss 370.20935 Test RE 0.005369134956700952 c 0.8858896 k 0.048366856 m 0.06986472\n",
      "199 Train Loss 369.4805 Test RE 0.0053883728347388134 c 0.8854151 k 0.048403323 m 0.06983166\n",
      "Training time: 89.81\n",
      "Training time: 89.81\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 2090974.6 Test RE 0.9688478611858972 c -0.0012703706 k 0.031005213 m -7.5885146e-06\n",
      "1 Train Loss 1875062.1 Test RE 0.9175595182614549 c -0.0021434508 k 0.065127335 m -1.2053805e-05\n",
      "2 Train Loss 1031960.9 Test RE 0.6820955842510283 c -0.0061979494 k 0.22103073 m -3.853937e-05\n",
      "3 Train Loss 590170.75 Test RE 0.5052998914585272 c -0.0074730325 k 0.28853968 m -5.2701078e-05\n",
      "4 Train Loss 355240.84 Test RE 0.333514291741582 c -0.008752347 k 0.34807757 m -6.344509e-05\n",
      "5 Train Loss 306497.5 Test RE 0.2982026839263599 c -0.008686536 k 0.34261042 m -5.7787278e-05\n",
      "6 Train Loss 273022.2 Test RE 0.2706960256687393 c -0.008692973 k 0.34089664 m -3.9631857e-05\n",
      "7 Train Loss 241896.9 Test RE 0.24727179772396382 c -0.008828769 k 0.33060864 m 9.246471e-05\n",
      "8 Train Loss 230169.72 Test RE 0.23849665944283904 c -0.008898679 k 0.32690823 m 0.00015228696\n",
      "9 Train Loss 202268.03 Test RE 0.22353942481117597 c -0.009120396 k 0.3067089 m 0.00041652925\n",
      "10 Train Loss 144094.36 Test RE 0.2119767740635317 c -0.010010805 k 0.22652519 m 0.0014792621\n",
      "11 Train Loss 63426.258 Test RE 0.16372914506964545 c -0.0113641415 k 0.10451274 m 0.0029131337\n",
      "12 Train Loss 40147.266 Test RE 0.13014873706632366 c -0.011604838 k 0.09124201 m 0.0030822942\n",
      "13 Train Loss 14111.656 Test RE 0.07437936414094504 c -0.012116761 k 0.051128007 m 0.0035494326\n",
      "14 Train Loss 8103.134 Test RE 0.05383068637169996 c -0.011970886 k 0.058474798 m 0.0034675812\n",
      "15 Train Loss 5116.252 Test RE 0.03933573270784433 c -0.011973299 k 0.05889512 m 0.0035179618\n",
      "16 Train Loss 4218.503 Test RE 0.03370954004849739 c -0.011963694 k 0.05963107 m 0.0035399124\n",
      "17 Train Loss 3505.311 Test RE 0.028386041690012888 c -0.01191542 k 0.05864359 m 0.0035820717\n",
      "18 Train Loss 3010.0889 Test RE 0.02391947020971458 c -0.011863903 k 0.059622895 m 0.003598918\n",
      "19 Train Loss 2735.1445 Test RE 0.020970054120408695 c -0.01178815 k 0.06222978 m 0.0035998002\n",
      "20 Train Loss 2542.2197 Test RE 0.018787187788882935 c -0.01173452 k 0.06138906 m 0.0036122212\n",
      "21 Train Loss 2372.8162 Test RE 0.016440703881043038 c -0.011596133 k 0.060627233 m 0.0036486457\n",
      "22 Train Loss 2188.4731 Test RE 0.013424950846668341 c -0.011249612 k 0.060999528 m 0.0037159799\n",
      "23 Train Loss 2033.2057 Test RE 0.01039557909829657 c -0.011034218 k 0.060278133 m 0.003771229\n",
      "24 Train Loss 1990.5487 Test RE 0.009317351079596888 c -0.010906342 k 0.059874844 m 0.0037997894\n",
      "25 Train Loss 1954.5886 Test RE 0.008371201153308732 c -0.010685124 k 0.061070908 m 0.0038128232\n",
      "26 Train Loss 1934.6466 Test RE 0.007770816482221023 c -0.010360607 k 0.0616327 m 0.0038563118\n",
      "27 Train Loss 1903.2325 Test RE 0.006907953613409718 c -0.009303818 k 0.060798362 m 0.0040075914\n",
      "28 Train Loss 1878.1653 Test RE 0.006151476329613615 c -0.008406326 k 0.06096801 m 0.0041118716\n",
      "29 Train Loss 1864.1167 Test RE 0.005623631279278341 c -0.007433521 k 0.060913745 m 0.004224004\n",
      "30 Train Loss 1845.9819 Test RE 0.004998809324750872 c -0.004784829 k 0.06092057 m 0.004527733\n",
      "31 Train Loss 1831.712 Test RE 0.004516679730725688 c -0.0028255598 k 0.06041953 m 0.0047376775\n",
      "32 Train Loss 1809.609 Test RE 0.0037606643901274546 c -0.00055257103 k 0.060849134 m 0.0049373643\n",
      "33 Train Loss 1802.7147 Test RE 0.003551385272125314 c 0.0009224719 k 0.0610981 m 0.0050698225\n",
      "34 Train Loss 1796.0751 Test RE 0.0034190838165646933 c 0.0028913822 k 0.060809705 m 0.0052578775\n",
      "35 Train Loss 1790.2522 Test RE 0.0032233148620184698 c 0.0038256028 k 0.061081555 m 0.0053383755\n",
      "36 Train Loss 1775.0647 Test RE 0.00354159879101562 c 0.011868137 k 0.060998492 m 0.00603957\n",
      "37 Train Loss 1762.2858 Test RE 0.0035054099574830457 c 0.0172159 k 0.060321327 m 0.0064889886\n",
      "38 Train Loss 1747.5105 Test RE 0.004457144240028561 c 0.029602448 k 0.059849177 m 0.0075586163\n",
      "39 Train Loss 1730.1622 Test RE 0.004604159674462686 c 0.03788567 k 0.06112535 m 0.0082351295\n",
      "40 Train Loss 1696.5323 Test RE 0.00451738993439406 c 0.050867274 k 0.05977466 m 0.009363952\n",
      "41 Train Loss 1667.1028 Test RE 0.004444585600921477 c 0.061078746 k 0.059046622 m 0.0102534825\n",
      "42 Train Loss 1645.0023 Test RE 0.00468458242387334 c 0.07314738 k 0.060437016 m 0.011237796\n",
      "43 Train Loss 1620.3374 Test RE 0.004624448625692871 c 0.082603894 k 0.0595783 m 0.012052164\n",
      "44 Train Loss 1602.0359 Test RE 0.004381284254531953 c 0.08730919 k 0.059611004 m 0.012464937\n",
      "45 Train Loss 1591.911 Test RE 0.004435088077701684 c 0.09188461 k 0.05958067 m 0.012869984\n",
      "46 Train Loss 1572.4329 Test RE 0.004644038661116559 c 0.10260083 k 0.059853394 m 0.013760034\n",
      "47 Train Loss 1548.621 Test RE 0.004780327812729512 c 0.114720315 k 0.060117614 m 0.014790112\n",
      "48 Train Loss 1528.8575 Test RE 0.0050423898508646006 c 0.12391155 k 0.058999676 m 0.015594575\n",
      "49 Train Loss 1503.9844 Test RE 0.005112335239879564 c 0.13727885 k 0.06010093 m 0.016701838\n",
      "50 Train Loss 1483.4893 Test RE 0.005114725535365106 c 0.14551057 k 0.060044706 m 0.017385814\n",
      "51 Train Loss 1464.7666 Test RE 0.005250660955156076 c 0.15121603 k 0.058874886 m 0.017855402\n",
      "52 Train Loss 1449.5935 Test RE 0.004794594538480734 c 0.15526588 k 0.058521464 m 0.018175906\n",
      "53 Train Loss 1420.4749 Test RE 0.004864738169637171 c 0.1689628 k 0.05828331 m 0.019355245\n",
      "54 Train Loss 1382.2615 Test RE 0.004713366962952423 c 0.18651226 k 0.057416767 m 0.020837117\n",
      "55 Train Loss 1355.9069 Test RE 0.004767532599958856 c 0.20079543 k 0.057595044 m 0.021994242\n",
      "56 Train Loss 1304.3367 Test RE 0.005792040889088896 c 0.23765078 k 0.058463458 m 0.025072968\n",
      "57 Train Loss 1245.2473 Test RE 0.006088555777345055 c 0.2809735 k 0.05602893 m 0.028762473\n",
      "58 Train Loss 1207.0884 Test RE 0.006034932073131568 c 0.2967606 k 0.056679264 m 0.030101195\n",
      "59 Train Loss 1157.9279 Test RE 0.004851904920713697 c 0.30690858 k 0.055461865 m 0.031085959\n",
      "60 Train Loss 1129.5991 Test RE 0.005060333738449119 c 0.319094 k 0.056322627 m 0.032127433\n",
      "61 Train Loss 1060.8909 Test RE 0.005405860040079781 c 0.3602659 k 0.057850014 m 0.03562529\n",
      "62 Train Loss 971.95715 Test RE 0.0050342227774641405 c 0.41871193 k 0.053892538 m 0.040579274\n",
      "63 Train Loss 884.5042 Test RE 0.004808486514782521 c 0.4595197 k 0.054945953 m 0.04410259\n",
      "64 Train Loss 858.76624 Test RE 0.0046761220789343656 c 0.47183308 k 0.054210745 m 0.045213215\n",
      "65 Train Loss 838.88696 Test RE 0.004327386793700234 c 0.47997788 k 0.05367261 m 0.045955475\n",
      "66 Train Loss 775.53076 Test RE 0.00505457056640642 c 0.5361965 k 0.054503977 m 0.050800413\n",
      "67 Train Loss 734.6017 Test RE 0.0047509002467466515 c 0.55763227 k 0.05276009 m 0.052700933\n",
      "68 Train Loss 682.75354 Test RE 0.005785055792927594 c 0.6071818 k 0.05279068 m 0.056964263\n",
      "69 Train Loss 618.29083 Test RE 0.004063615930869452 c 0.64228225 k 0.052223075 m 0.059980396\n",
      "70 Train Loss 583.3704 Test RE 0.00507744788871707 c 0.70095026 k 0.049771186 m 0.065038174\n",
      "71 Train Loss 555.5928 Test RE 0.004420920648470073 c 0.7213719 k 0.051298127 m 0.06677532\n",
      "72 Train Loss 543.44995 Test RE 0.0049242219741548335 c 0.74829173 k 0.05047131 m 0.069114774\n",
      "73 Train Loss 528.47675 Test RE 0.004441415681659934 c 0.7651678 k 0.049474865 m 0.07054547\n",
      "74 Train Loss 502.26782 Test RE 0.004362820423342404 c 0.78189427 k 0.05102224 m 0.0719593\n",
      "75 Train Loss 483.35913 Test RE 0.004788636800015792 c 0.80645186 k 0.04982478 m 0.07408736\n",
      "76 Train Loss 470.7807 Test RE 0.004461284310793815 c 0.80593735 k 0.049602926 m 0.074043766\n",
      "77 Train Loss 454.39856 Test RE 0.004270025087634699 c 0.81533444 k 0.049275015 m 0.07486185\n",
      "78 Train Loss 446.2185 Test RE 0.0038330741563050953 c 0.8369416 k 0.048613697 m 0.07670744\n",
      "79 Train Loss 437.35934 Test RE 0.003569440072159102 c 0.8512631 k 0.048400596 m 0.07792345\n",
      "80 Train Loss 420.6487 Test RE 0.003811689074196884 c 0.8578429 k 0.048411142 m 0.078459784\n",
      "81 Train Loss 395.62454 Test RE 0.0035355427070688597 c 0.888534 k 0.048546813 m 0.0811291\n",
      "82 Train Loss 379.22223 Test RE 0.004161296936961348 c 0.9095371 k 0.048023622 m 0.08296933\n",
      "83 Train Loss 373.9206 Test RE 0.004297506841143385 c 0.9273494 k 0.047363117 m 0.08451156\n",
      "84 Train Loss 362.18842 Test RE 0.004189977647190227 c 0.9529512 k 0.047827598 m 0.08669275\n",
      "85 Train Loss 355.86847 Test RE 0.004155803613581757 c 0.9568765 k 0.047159094 m 0.08702564\n",
      "86 Train Loss 350.10568 Test RE 0.003963010177294778 c 0.96617895 k 0.04688223 m 0.08782735\n",
      "87 Train Loss 347.59595 Test RE 0.0039004134155194835 c 0.9702054 k 0.04710524 m 0.088188834\n",
      "88 Train Loss 344.297 Test RE 0.003966038500813629 c 0.9868359 k 0.0467407 m 0.08965489\n",
      "89 Train Loss 342.04486 Test RE 0.004072692488109203 c 0.9991878 k 0.046544764 m 0.09072824\n",
      "90 Train Loss 340.3117 Test RE 0.004035479645974197 c 0.9966457 k 0.0469805 m 0.090509064\n",
      "91 Train Loss 339.05554 Test RE 0.00406204038396628 c 0.9981131 k 0.04709571 m 0.09064924\n",
      "92 Train Loss 337.29556 Test RE 0.004267490512711587 c 1.0046803 k 0.04697197 m 0.0912529\n",
      "93 Train Loss 330.88284 Test RE 0.004092026034488542 c 1.0063772 k 0.04668835 m 0.091518216\n",
      "94 Train Loss 327.73486 Test RE 0.003915366565949357 c 1.0090411 k 0.04650672 m 0.09182091\n",
      "95 Train Loss 326.751 Test RE 0.003909083267358428 c 1.0212562 k 0.046292886 m 0.09292543\n",
      "96 Train Loss 325.6923 Test RE 0.003870784245394081 c 1.0193783 k 0.046232678 m 0.092794895\n",
      "97 Train Loss 323.3112 Test RE 0.003807882864942211 c 1.0225791 k 0.04625063 m 0.09313419\n",
      "98 Train Loss 320.54855 Test RE 0.003886943038841437 c 1.0299872 k 0.04631839 m 0.09378981\n",
      "99 Train Loss 313.12714 Test RE 0.00367798166462212 c 1.0313584 k 0.046304435 m 0.09399423\n",
      "100 Train Loss 311.63068 Test RE 0.003616374854637825 c 1.0363702 k 0.046075426 m 0.09447678\n",
      "101 Train Loss 309.8675 Test RE 0.0037865948894396668 c 1.0405986 k 0.04598252 m 0.09485477\n",
      "102 Train Loss 307.4905 Test RE 0.0037223509019003874 c 1.043128 k 0.04554838 m 0.095127866\n",
      "103 Train Loss 305.52417 Test RE 0.0038265769217292515 c 1.0485194 k 0.045365274 m 0.095680825\n",
      "104 Train Loss 304.5579 Test RE 0.003931900847171768 c 1.052511 k 0.04560346 m 0.096059814\n",
      "105 Train Loss 304.4676 Test RE 0.0038992755886583046 c 1.0522997 k 0.045625083 m 0.09604139\n",
      "106 Train Loss 304.31424 Test RE 0.0038362816784894556 c 1.0526538 k 0.04555093 m 0.09607888\n",
      "107 Train Loss 303.02438 Test RE 0.0038243130731885565 c 1.0615723 k 0.04529182 m 0.09692754\n",
      "108 Train Loss 301.818 Test RE 0.003980404746866288 c 1.0644143 k 0.045423415 m 0.09719805\n",
      "109 Train Loss 300.82736 Test RE 0.003993021682530149 c 1.0632389 k 0.045580994 m 0.09713609\n",
      "110 Train Loss 300.66534 Test RE 0.003948394507801514 c 1.0640981 k 0.045599043 m 0.0972265\n",
      "111 Train Loss 300.6105 Test RE 0.003937997707610173 c 1.0657098 k 0.045624193 m 0.09737401\n",
      "112 Train Loss 300.58682 Test RE 0.003931014825593693 c 1.0662111 k 0.04561648 m 0.09742183\n",
      "113 Train Loss 300.535 Test RE 0.0039002523110994564 c 1.0666658 k 0.04558893 m 0.097472906\n",
      "114 Train Loss 300.43527 Test RE 0.003865707104175209 c 1.0666937 k 0.045608036 m 0.09749349\n",
      "115 Train Loss 300.36176 Test RE 0.0038874006964202085 c 1.0666103 k 0.045635268 m 0.09749799\n",
      "116 Train Loss 300.1435 Test RE 0.0039018016411655077 c 1.0686165 k 0.045626283 m 0.097715095\n",
      "117 Train Loss 299.65927 Test RE 0.003934352057681434 c 1.0721062 k 0.04561793 m 0.09809247\n",
      "118 Train Loss 299.19366 Test RE 0.003970278737390792 c 1.0732255 k 0.04560535 m 0.09823613\n",
      "119 Train Loss 298.3299 Test RE 0.003890078045553734 c 1.07804 k 0.045447987 m 0.098779194\n",
      "120 Train Loss 297.23883 Test RE 0.0038617824142186613 c 1.0889108 k 0.045231182 m 0.09990535\n",
      "121 Train Loss 296.6419 Test RE 0.0038452638877505 c 1.0905286 k 0.045162905 m 0.10007845\n",
      "122 Train Loss 296.41275 Test RE 0.003929670634077367 c 1.0922741 k 0.045115374 m 0.10026259\n",
      "123 Train Loss 296.29645 Test RE 0.003956321005638744 c 1.0939766 k 0.045048732 m 0.10045999\n",
      "124 Train Loss 296.1483 Test RE 0.003933223915828502 c 1.0941776 k 0.0450813 m 0.100490764\n",
      "125 Train Loss 296.077 Test RE 0.0039263981834378086 c 1.0936482 k 0.04509854 m 0.10044876\n",
      "126 Train Loss 295.9606 Test RE 0.0039393818879964035 c 1.0937296 k 0.045143835 m 0.10048352\n",
      "127 Train Loss 295.80878 Test RE 0.00396363256562387 c 1.0943428 k 0.04522478 m 0.100554615\n",
      "128 Train Loss 295.4954 Test RE 0.004035450638940849 c 1.096459 k 0.045163963 m 0.100803204\n",
      "129 Train Loss 295.04816 Test RE 0.004048296236530876 c 1.0971606 k 0.045091767 m 0.10096553\n",
      "130 Train Loss 294.81055 Test RE 0.004060175374696403 c 1.0978678 k 0.045100205 m 0.10110523\n",
      "131 Train Loss 294.72043 Test RE 0.004063348274386704 c 1.0985196 k 0.045088608 m 0.101186246\n",
      "132 Train Loss 294.38672 Test RE 0.004048849783272918 c 1.1007878 k 0.044972334 m 0.1014776\n",
      "133 Train Loss 294.29565 Test RE 0.004062146242566701 c 1.1024016 k 0.044957984 m 0.10167271\n",
      "134 Train Loss 294.1872 Test RE 0.004056140589209622 c 1.1041535 k 0.04492569 m 0.10186253\n",
      "135 Train Loss 293.83633 Test RE 0.004087016972990916 c 1.102411 k 0.04489395 m 0.10179121\n",
      "136 Train Loss 293.32074 Test RE 0.004035486611922072 c 1.1008351 k 0.044923477 m 0.10187975\n",
      "137 Train Loss 292.91977 Test RE 0.003916368626612551 c 1.1032572 k 0.04496516 m 0.10223946\n",
      "138 Train Loss 292.63947 Test RE 0.003913861102643794 c 1.1040084 k 0.044914156 m 0.10236003\n",
      "139 Train Loss 290.93613 Test RE 0.003901637194380452 c 1.1071143 k 0.044896137 m 0.1034285\n",
      "140 Train Loss 290.52472 Test RE 0.003949889758470065 c 1.1056473 k 0.044938117 m 0.1035071\n",
      "141 Train Loss 290.41882 Test RE 0.004013412832543163 c 1.104328 k 0.04495177 m 0.103351116\n",
      "142 Train Loss 290.27966 Test RE 0.003995367091472529 c 1.106089 k 0.044974808 m 0.10361279\n",
      "143 Train Loss 289.6894 Test RE 0.0040374616272237885 c 1.1133934 k 0.045001507 m 0.10472504\n",
      "144 Train Loss 288.97052 Test RE 0.00414804889945128 c 1.114385 k 0.044919454 m 0.10508868\n",
      "145 Train Loss 288.41852 Test RE 0.00415722848869786 c 1.115495 k 0.044781294 m 0.10553477\n",
      "146 Train Loss 288.32065 Test RE 0.004172396645163979 c 1.1176238 k 0.04474855 m 0.105897956\n",
      "147 Train Loss 288.14575 Test RE 0.004210023938906739 c 1.1147227 k 0.04477966 m 0.10564427\n",
      "148 Train Loss 287.93686 Test RE 0.0041602859871691395 c 1.1125573 k 0.04469277 m 0.10552153\n",
      "149 Train Loss 287.84473 Test RE 0.00410445462621926 c 1.1129322 k 0.044673704 m 0.10570166\n",
      "150 Train Loss 287.6935 Test RE 0.004118816249002767 c 1.111713 k 0.044677358 m 0.10571614\n",
      "151 Train Loss 287.52896 Test RE 0.004159125167953818 c 1.1138544 k 0.044651795 m 0.10607548\n",
      "152 Train Loss 286.86078 Test RE 0.004212608783592941 c 1.1182944 k 0.044586424 m 0.10718696\n",
      "153 Train Loss 286.1431 Test RE 0.00431344123255783 c 1.116553 k 0.04469204 m 0.10756557\n",
      "154 Train Loss 285.69717 Test RE 0.004285916861560287 c 1.1160133 k 0.04477849 m 0.107955076\n",
      "155 Train Loss 285.45697 Test RE 0.004325927025440903 c 1.1168008 k 0.044809975 m 0.10821752\n",
      "156 Train Loss 285.30402 Test RE 0.00434702332502826 c 1.1169007 k 0.04483005 m 0.10806784\n",
      "157 Train Loss 285.21896 Test RE 0.004320163525218412 c 1.1149614 k 0.04482803 m 0.10772974\n",
      "158 Train Loss 285.18115 Test RE 0.00430999634936708 c 1.1140922 k 0.044837322 m 0.10760835\n",
      "159 Train Loss 285.13232 Test RE 0.004301979527151431 c 1.1148144 k 0.044900455 m 0.10764002\n",
      "160 Train Loss 285.04163 Test RE 0.004276213077638557 c 1.1168039 k 0.044913936 m 0.10794478\n",
      "161 Train Loss 284.76343 Test RE 0.004250739481871622 c 1.1184485 k 0.0449028 m 0.108548194\n",
      "162 Train Loss 284.60754 Test RE 0.004235397886899686 c 1.1203204 k 0.044852737 m 0.10886787\n",
      "163 Train Loss 284.26453 Test RE 0.004208047505753609 c 1.1218894 k 0.044790175 m 0.10941698\n",
      "164 Train Loss 284.0717 Test RE 0.004205234962111423 c 1.1205914 k 0.044820692 m 0.109621905\n",
      "165 Train Loss 283.98633 Test RE 0.004227087443630422 c 1.1210003 k 0.044786084 m 0.109794386\n",
      "166 Train Loss 283.87955 Test RE 0.004247111174476093 c 1.1216573 k 0.044695545 m 0.10986346\n",
      "167 Train Loss 283.8193 Test RE 0.004257391646861173 c 1.1202445 k 0.04477161 m 0.109777205\n",
      "168 Train Loss 283.7807 Test RE 0.004239855928274741 c 1.1193013 k 0.044795904 m 0.10983751\n",
      "169 Train Loss 283.69446 Test RE 0.0042164859077938885 c 1.1178532 k 0.04476876 m 0.10984598\n",
      "170 Train Loss 283.59137 Test RE 0.004211626478667462 c 1.1159017 k 0.044809975 m 0.10971443\n",
      "171 Train Loss 283.3713 Test RE 0.00421714684418506 c 1.1136469 k 0.04486194 m 0.1098148\n",
      "172 Train Loss 283.16605 Test RE 0.004274652063689713 c 1.1137116 k 0.044822197 m 0.110147834\n",
      "173 Train Loss 282.95468 Test RE 0.004353116302252797 c 1.1126908 k 0.044826563 m 0.11027679\n",
      "174 Train Loss 282.75754 Test RE 0.004285099352385497 c 1.1125636 k 0.04480227 m 0.11051267\n",
      "175 Train Loss 282.666 Test RE 0.004277778375756735 c 1.1116618 k 0.044810023 m 0.11055813\n",
      "176 Train Loss 282.59845 Test RE 0.004271624324874719 c 1.1117219 k 0.044841997 m 0.11062727\n",
      "177 Train Loss 282.54718 Test RE 0.00424948930266126 c 1.1129053 k 0.044845585 m 0.11081589\n",
      "178 Train Loss 282.4809 Test RE 0.004287047134106301 c 1.1121386 k 0.0448705 m 0.11075899\n",
      "179 Train Loss 282.41147 Test RE 0.004306461356921424 c 1.1122195 k 0.044881962 m 0.11073521\n",
      "180 Train Loss 282.1883 Test RE 0.004290968692935771 c 1.1136969 k 0.04481352 m 0.111009285\n",
      "181 Train Loss 281.79587 Test RE 0.004264718976963005 c 1.113528 k 0.04482646 m 0.11112602\n",
      "182 Train Loss 281.45477 Test RE 0.004188054618463225 c 1.1154993 k 0.04484303 m 0.11145961\n",
      "183 Train Loss 280.91095 Test RE 0.004238230351696298 c 1.1195602 k 0.044682935 m 0.11252877\n",
      "184 Train Loss 280.67667 Test RE 0.004341388003246381 c 1.1207294 k 0.04465315 m 0.11308208\n",
      "185 Train Loss 280.518 Test RE 0.004352438919725477 c 1.1202205 k 0.044750113 m 0.11314034\n",
      "186 Train Loss 280.2985 Test RE 0.004325595960152835 c 1.1202536 k 0.044825792 m 0.11325388\n",
      "187 Train Loss 280.14508 Test RE 0.004303480883674543 c 1.119007 k 0.044905324 m 0.11310943\n",
      "188 Train Loss 279.94324 Test RE 0.004287720783516861 c 1.1185985 k 0.044864632 m 0.113278106\n",
      "189 Train Loss 279.85812 Test RE 0.0042860745493009346 c 1.1184192 k 0.044815447 m 0.11343178\n",
      "190 Train Loss 279.8067 Test RE 0.004266486655174716 c 1.1186781 k 0.044831943 m 0.113418\n",
      "191 Train Loss 279.67438 Test RE 0.004277540207888297 c 1.1178085 k 0.04484134 m 0.113466196\n",
      "192 Train Loss 279.48572 Test RE 0.004283288726023976 c 1.1176858 k 0.044822846 m 0.11364293\n",
      "193 Train Loss 279.23846 Test RE 0.004264064172885986 c 1.1172125 k 0.04479184 m 0.11383343\n",
      "194 Train Loss 279.1316 Test RE 0.004289142277940426 c 1.1175 k 0.044811364 m 0.11418975\n",
      "195 Train Loss 279.11304 Test RE 0.004311492819461841 c 1.117405 k 0.044815946 m 0.114228025\n",
      "196 Train Loss 279.0177 Test RE 0.004364380056884397 c 1.1166191 k 0.044791777 m 0.11406011\n",
      "197 Train Loss 278.9132 Test RE 0.00434349712235376 c 1.1173786 k 0.044759437 m 0.1143361\n",
      "198 Train Loss 278.82623 Test RE 0.004341045240636946 c 1.1176773 k 0.04475229 m 0.114675604\n",
      "199 Train Loss 278.74646 Test RE 0.004357533197779979 c 1.117153 k 0.04475734 m 0.1146832\n",
      "Training time: 91.05\n",
      "Training time: 91.05\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 2152799.0 Test RE 0.983067949779825 c 0.00051482266 k 0.006086287 m -8.2942715e-06\n",
      "1 Train Loss 1925649.8 Test RE 0.929599766757276 c 0.0014196374 k 0.018579312 m -1.935022e-05\n",
      "2 Train Loss nan Test RE nan c nan k nan m nan\n",
      "3 Train Loss nan Test RE nan c nan k nan m nan\n",
      "4 Train Loss nan Test RE nan c nan k nan m nan\n",
      "5 Train Loss nan Test RE nan c nan k nan m nan\n",
      "6 Train Loss nan Test RE nan c nan k nan m nan\n",
      "7 Train Loss nan Test RE nan c nan k nan m nan\n",
      "8 Train Loss nan Test RE nan c nan k nan m nan\n",
      "9 Train Loss nan Test RE nan c nan k nan m nan\n",
      "10 Train Loss nan Test RE nan c nan k nan m nan\n",
      "11 Train Loss nan Test RE nan c nan k nan m nan\n",
      "12 Train Loss nan Test RE nan c nan k nan m nan\n",
      "13 Train Loss nan Test RE nan c nan k nan m nan\n",
      "14 Train Loss nan Test RE nan c nan k nan m nan\n",
      "15 Train Loss nan Test RE nan c nan k nan m nan\n",
      "16 Train Loss nan Test RE nan c nan k nan m nan\n",
      "17 Train Loss nan Test RE nan c nan k nan m nan\n",
      "18 Train Loss nan Test RE nan c nan k nan m nan\n",
      "19 Train Loss nan Test RE nan c nan k nan m nan\n",
      "20 Train Loss nan Test RE nan c nan k nan m nan\n",
      "21 Train Loss nan Test RE nan c nan k nan m nan\n",
      "22 Train Loss nan Test RE nan c nan k nan m nan\n",
      "23 Train Loss nan Test RE nan c nan k nan m nan\n",
      "24 Train Loss nan Test RE nan c nan k nan m nan\n",
      "25 Train Loss nan Test RE nan c nan k nan m nan\n",
      "26 Train Loss nan Test RE nan c nan k nan m nan\n",
      "27 Train Loss nan Test RE nan c nan k nan m nan\n",
      "28 Train Loss nan Test RE nan c nan k nan m nan\n",
      "29 Train Loss nan Test RE nan c nan k nan m nan\n",
      "30 Train Loss nan Test RE nan c nan k nan m nan\n",
      "31 Train Loss nan Test RE nan c nan k nan m nan\n",
      "32 Train Loss nan Test RE nan c nan k nan m nan\n",
      "33 Train Loss nan Test RE nan c nan k nan m nan\n",
      "34 Train Loss nan Test RE nan c nan k nan m nan\n",
      "35 Train Loss nan Test RE nan c nan k nan m nan\n",
      "36 Train Loss nan Test RE nan c nan k nan m nan\n",
      "37 Train Loss nan Test RE nan c nan k nan m nan\n",
      "38 Train Loss nan Test RE nan c nan k nan m nan\n",
      "39 Train Loss nan Test RE nan c nan k nan m nan\n",
      "40 Train Loss nan Test RE nan c nan k nan m nan\n",
      "41 Train Loss nan Test RE nan c nan k nan m nan\n",
      "42 Train Loss nan Test RE nan c nan k nan m nan\n",
      "43 Train Loss nan Test RE nan c nan k nan m nan\n",
      "44 Train Loss nan Test RE nan c nan k nan m nan\n",
      "45 Train Loss nan Test RE nan c nan k nan m nan\n",
      "46 Train Loss nan Test RE nan c nan k nan m nan\n",
      "47 Train Loss nan Test RE nan c nan k nan m nan\n",
      "48 Train Loss nan Test RE nan c nan k nan m nan\n",
      "49 Train Loss nan Test RE nan c nan k nan m nan\n",
      "50 Train Loss nan Test RE nan c nan k nan m nan\n",
      "51 Train Loss nan Test RE nan c nan k nan m nan\n",
      "52 Train Loss nan Test RE nan c nan k nan m nan\n",
      "53 Train Loss nan Test RE nan c nan k nan m nan\n",
      "54 Train Loss nan Test RE nan c nan k nan m nan\n",
      "55 Train Loss nan Test RE nan c nan k nan m nan\n",
      "56 Train Loss nan Test RE nan c nan k nan m nan\n",
      "57 Train Loss nan Test RE nan c nan k nan m nan\n",
      "58 Train Loss nan Test RE nan c nan k nan m nan\n",
      "59 Train Loss nan Test RE nan c nan k nan m nan\n",
      "60 Train Loss nan Test RE nan c nan k nan m nan\n",
      "61 Train Loss nan Test RE nan c nan k nan m nan\n",
      "62 Train Loss nan Test RE nan c nan k nan m nan\n",
      "63 Train Loss nan Test RE nan c nan k nan m nan\n",
      "64 Train Loss nan Test RE nan c nan k nan m nan\n",
      "65 Train Loss nan Test RE nan c nan k nan m nan\n",
      "66 Train Loss nan Test RE nan c nan k nan m nan\n",
      "67 Train Loss nan Test RE nan c nan k nan m nan\n",
      "68 Train Loss nan Test RE nan c nan k nan m nan\n",
      "69 Train Loss nan Test RE nan c nan k nan m nan\n",
      "70 Train Loss nan Test RE nan c nan k nan m nan\n",
      "71 Train Loss nan Test RE nan c nan k nan m nan\n",
      "72 Train Loss nan Test RE nan c nan k nan m nan\n",
      "73 Train Loss nan Test RE nan c nan k nan m nan\n",
      "74 Train Loss nan Test RE nan c nan k nan m nan\n",
      "75 Train Loss nan Test RE nan c nan k nan m nan\n",
      "76 Train Loss nan Test RE nan c nan k nan m nan\n",
      "77 Train Loss nan Test RE nan c nan k nan m nan\n",
      "78 Train Loss nan Test RE nan c nan k nan m nan\n",
      "79 Train Loss nan Test RE nan c nan k nan m nan\n",
      "80 Train Loss nan Test RE nan c nan k nan m nan\n",
      "81 Train Loss nan Test RE nan c nan k nan m nan\n",
      "82 Train Loss nan Test RE nan c nan k nan m nan\n",
      "83 Train Loss nan Test RE nan c nan k nan m nan\n",
      "84 Train Loss nan Test RE nan c nan k nan m nan\n",
      "85 Train Loss nan Test RE nan c nan k nan m nan\n",
      "86 Train Loss nan Test RE nan c nan k nan m nan\n",
      "87 Train Loss nan Test RE nan c nan k nan m nan\n",
      "88 Train Loss nan Test RE nan c nan k nan m nan\n",
      "89 Train Loss nan Test RE nan c nan k nan m nan\n",
      "90 Train Loss nan Test RE nan c nan k nan m nan\n",
      "91 Train Loss nan Test RE nan c nan k nan m nan\n",
      "92 Train Loss nan Test RE nan c nan k nan m nan\n",
      "93 Train Loss nan Test RE nan c nan k nan m nan\n",
      "94 Train Loss nan Test RE nan c nan k nan m nan\n",
      "95 Train Loss nan Test RE nan c nan k nan m nan\n",
      "96 Train Loss nan Test RE nan c nan k nan m nan\n",
      "97 Train Loss nan Test RE nan c nan k nan m nan\n",
      "98 Train Loss nan Test RE nan c nan k nan m nan\n",
      "99 Train Loss nan Test RE nan c nan k nan m nan\n",
      "100 Train Loss nan Test RE nan c nan k nan m nan\n",
      "101 Train Loss nan Test RE nan c nan k nan m nan\n",
      "102 Train Loss nan Test RE nan c nan k nan m nan\n",
      "103 Train Loss nan Test RE nan c nan k nan m nan\n",
      "104 Train Loss nan Test RE nan c nan k nan m nan\n",
      "105 Train Loss nan Test RE nan c nan k nan m nan\n",
      "106 Train Loss nan Test RE nan c nan k nan m nan\n",
      "107 Train Loss nan Test RE nan c nan k nan m nan\n",
      "108 Train Loss nan Test RE nan c nan k nan m nan\n",
      "109 Train Loss nan Test RE nan c nan k nan m nan\n",
      "110 Train Loss nan Test RE nan c nan k nan m nan\n",
      "111 Train Loss nan Test RE nan c nan k nan m nan\n",
      "112 Train Loss nan Test RE nan c nan k nan m nan\n",
      "113 Train Loss nan Test RE nan c nan k nan m nan\n",
      "114 Train Loss nan Test RE nan c nan k nan m nan\n",
      "115 Train Loss nan Test RE nan c nan k nan m nan\n",
      "116 Train Loss nan Test RE nan c nan k nan m nan\n",
      "117 Train Loss nan Test RE nan c nan k nan m nan\n",
      "118 Train Loss nan Test RE nan c nan k nan m nan\n",
      "119 Train Loss nan Test RE nan c nan k nan m nan\n",
      "120 Train Loss nan Test RE nan c nan k nan m nan\n",
      "121 Train Loss nan Test RE nan c nan k nan m nan\n",
      "122 Train Loss nan Test RE nan c nan k nan m nan\n",
      "123 Train Loss nan Test RE nan c nan k nan m nan\n",
      "124 Train Loss nan Test RE nan c nan k nan m nan\n",
      "125 Train Loss nan Test RE nan c nan k nan m nan\n",
      "126 Train Loss nan Test RE nan c nan k nan m nan\n",
      "127 Train Loss nan Test RE nan c nan k nan m nan\n",
      "128 Train Loss nan Test RE nan c nan k nan m nan\n",
      "129 Train Loss nan Test RE nan c nan k nan m nan\n",
      "130 Train Loss nan Test RE nan c nan k nan m nan\n",
      "131 Train Loss nan Test RE nan c nan k nan m nan\n",
      "132 Train Loss nan Test RE nan c nan k nan m nan\n",
      "133 Train Loss nan Test RE nan c nan k nan m nan\n",
      "134 Train Loss nan Test RE nan c nan k nan m nan\n",
      "135 Train Loss nan Test RE nan c nan k nan m nan\n",
      "136 Train Loss nan Test RE nan c nan k nan m nan\n",
      "137 Train Loss nan Test RE nan c nan k nan m nan\n",
      "138 Train Loss nan Test RE nan c nan k nan m nan\n",
      "139 Train Loss nan Test RE nan c nan k nan m nan\n",
      "140 Train Loss nan Test RE nan c nan k nan m nan\n",
      "141 Train Loss nan Test RE nan c nan k nan m nan\n",
      "142 Train Loss nan Test RE nan c nan k nan m nan\n",
      "143 Train Loss nan Test RE nan c nan k nan m nan\n",
      "144 Train Loss nan Test RE nan c nan k nan m nan\n",
      "145 Train Loss nan Test RE nan c nan k nan m nan\n",
      "146 Train Loss nan Test RE nan c nan k nan m nan\n",
      "147 Train Loss nan Test RE nan c nan k nan m nan\n",
      "148 Train Loss nan Test RE nan c nan k nan m nan\n",
      "149 Train Loss nan Test RE nan c nan k nan m nan\n",
      "150 Train Loss nan Test RE nan c nan k nan m nan\n",
      "151 Train Loss nan Test RE nan c nan k nan m nan\n",
      "152 Train Loss nan Test RE nan c nan k nan m nan\n",
      "153 Train Loss nan Test RE nan c nan k nan m nan\n",
      "154 Train Loss nan Test RE nan c nan k nan m nan\n",
      "155 Train Loss nan Test RE nan c nan k nan m nan\n",
      "156 Train Loss nan Test RE nan c nan k nan m nan\n",
      "157 Train Loss nan Test RE nan c nan k nan m nan\n",
      "158 Train Loss nan Test RE nan c nan k nan m nan\n",
      "159 Train Loss nan Test RE nan c nan k nan m nan\n",
      "160 Train Loss nan Test RE nan c nan k nan m nan\n",
      "161 Train Loss nan Test RE nan c nan k nan m nan\n",
      "162 Train Loss nan Test RE nan c nan k nan m nan\n",
      "163 Train Loss nan Test RE nan c nan k nan m nan\n",
      "164 Train Loss nan Test RE nan c nan k nan m nan\n",
      "165 Train Loss nan Test RE nan c nan k nan m nan\n",
      "166 Train Loss nan Test RE nan c nan k nan m nan\n",
      "167 Train Loss nan Test RE nan c nan k nan m nan\n",
      "168 Train Loss nan Test RE nan c nan k nan m nan\n",
      "169 Train Loss nan Test RE nan c nan k nan m nan\n",
      "170 Train Loss nan Test RE nan c nan k nan m nan\n",
      "171 Train Loss nan Test RE nan c nan k nan m nan\n",
      "172 Train Loss nan Test RE nan c nan k nan m nan\n",
      "173 Train Loss nan Test RE nan c nan k nan m nan\n",
      "174 Train Loss nan Test RE nan c nan k nan m nan\n",
      "175 Train Loss nan Test RE nan c nan k nan m nan\n",
      "176 Train Loss nan Test RE nan c nan k nan m nan\n",
      "177 Train Loss nan Test RE nan c nan k nan m nan\n",
      "178 Train Loss nan Test RE nan c nan k nan m nan\n",
      "179 Train Loss nan Test RE nan c nan k nan m nan\n",
      "180 Train Loss nan Test RE nan c nan k nan m nan\n",
      "181 Train Loss nan Test RE nan c nan k nan m nan\n",
      "182 Train Loss nan Test RE nan c nan k nan m nan\n",
      "183 Train Loss nan Test RE nan c nan k nan m nan\n",
      "184 Train Loss nan Test RE nan c nan k nan m nan\n",
      "185 Train Loss nan Test RE nan c nan k nan m nan\n",
      "186 Train Loss nan Test RE nan c nan k nan m nan\n",
      "187 Train Loss nan Test RE nan c nan k nan m nan\n",
      "188 Train Loss nan Test RE nan c nan k nan m nan\n",
      "189 Train Loss nan Test RE nan c nan k nan m nan\n",
      "190 Train Loss nan Test RE nan c nan k nan m nan\n",
      "191 Train Loss nan Test RE nan c nan k nan m nan\n",
      "192 Train Loss nan Test RE nan c nan k nan m nan\n",
      "193 Train Loss nan Test RE nan c nan k nan m nan\n",
      "194 Train Loss nan Test RE nan c nan k nan m nan\n",
      "195 Train Loss nan Test RE nan c nan k nan m nan\n",
      "196 Train Loss nan Test RE nan c nan k nan m nan\n",
      "197 Train Loss nan Test RE nan c nan k nan m nan\n",
      "198 Train Loss nan Test RE nan c nan k nan m nan\n",
      "199 Train Loss nan Test RE nan c nan k nan m nan\n",
      "Training time: 154.66\n",
      "Training time: 154.66\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 2167131.8 Test RE 0.9863477176602901 c 0.00060824223 k 0.0047648726 m -4.4491776e-06\n",
      "1 Train Loss 1890949.5 Test RE 0.9211517629948721 c 0.002242796 k 0.017488383 m -1.820909e-05\n",
      "2 Train Loss 964014.94 Test RE 0.6577028771800142 c 0.006752262 k 0.049875654 m -4.9877424e-05\n",
      "3 Train Loss 828485.0 Test RE 0.6099342874919546 c 0.007735324 k 0.05591542 m -5.5785113e-05\n",
      "4 Train Loss 439763.53 Test RE 0.4447698061908033 c 0.010977829 k 0.069119334 m -6.867927e-05\n",
      "5 Train Loss 93396.875 Test RE 0.20086187617413853 c 0.0138553595 k 0.08458409 m -8.4490224e-05\n",
      "6 Train Loss 33475.504 Test RE 0.10969986370036391 c 0.015587606 k 0.094045945 m -9.3849034e-05\n",
      "7 Train Loss 26579.79 Test RE 0.09607542166472449 c 0.016767528 k 0.10040512 m -9.993432e-05\n",
      "8 Train Loss 21956.652 Test RE 0.08417630251001022 c 0.017504558 k 0.10434608 m -0.00010360007\n",
      "9 Train Loss 18101.049 Test RE 0.06943976836765126 c 0.018929923 k 0.11223866 m -0.00011093996\n",
      "10 Train Loss 16902.326 Test RE 0.060521026972646046 c 0.01998726 k 0.11814261 m -0.00011631636\n",
      "11 Train Loss 16840.965 Test RE 0.059554533237960366 c 0.02013872 k 0.118938416 m -0.00011697209\n",
      "12 Train Loss 16780.615 Test RE 0.0588753927441774 c 0.020202303 k 0.11914329 m -0.000116976495\n",
      "13 Train Loss 13956.406 Test RE 0.05211523840702369 c 0.020029744 k 0.113961264 m -0.00010742371\n",
      "14 Train Loss 10880.705 Test RE 0.049068119712779044 c 0.018790869 k 0.103186205 m -9.3276656e-05\n",
      "15 Train Loss 8274.869 Test RE 0.0428193741762875 c 0.017887717 k 0.09379889 m -7.963579e-05\n",
      "16 Train Loss 6315.754 Test RE 0.04057869852256161 c 0.016795415 k 0.0813273 m -6.008968e-05\n",
      "17 Train Loss 5427.1875 Test RE 0.038214922463626615 c 0.016255712 k 0.07487898 m -4.9517916e-05\n",
      "18 Train Loss 4979.66 Test RE 0.03717889315708469 c 0.015816376 k 0.069571614 m -4.0736584e-05\n",
      "19 Train Loss 4634.2915 Test RE 0.036198630446550945 c 0.015262479 k 0.061527576 m -2.6843461e-05\n",
      "20 Train Loss 3885.5894 Test RE 0.030095806702012957 c 0.015185695 k 0.056080606 m -1.704097e-05\n",
      "21 Train Loss 3737.9434 Test RE 0.028255614481876606 c 0.0152830435 k 0.055884983 m -1.6513974e-05\n",
      "22 Train Loss 3650.7278 Test RE 0.02732744165743612 c 0.015437071 k 0.056099486 m -1.6184906e-05\n",
      "23 Train Loss 2977.7275 Test RE 0.017015370774835812 c 0.015999941 k 0.05746268 m -1.4705827e-05\n",
      "24 Train Loss 2890.326 Test RE 0.015500615073821243 c 0.016098728 k 0.057859894 m -1.4316299e-05\n",
      "25 Train Loss 2883.0444 Test RE 0.014472185270620868 c 0.016153011 k 0.058013927 m -1.4083317e-05\n",
      "26 Train Loss 2881.5757 Test RE 0.014285409607972631 c 0.01617218 k 0.058102503 m -1.4200449e-05\n",
      "27 Train Loss 2881.1538 Test RE 0.014401291064541234 c 0.016158454 k 0.057988834 m -1.4079814e-05\n",
      "28 Train Loss 2879.9258 Test RE 0.014323569284878837 c 0.01614343 k 0.057590522 m -1.3087812e-05\n",
      "29 Train Loss 2875.4932 Test RE 0.014441227335707952 c 0.016201632 k 0.057479434 m -1.0521018e-05\n",
      "30 Train Loss 2870.6338 Test RE 0.014586076130687577 c 0.016346334 k 0.058550242 m -8.930915e-06\n",
      "31 Train Loss 2869.8818 Test RE 0.01451456547334989 c 0.016375994 k 0.05864104 m -7.986565e-06\n",
      "32 Train Loss 2839.169 Test RE 0.014962957775019044 c 0.016571637 k 0.058922283 m 1.4006324e-05\n",
      "33 Train Loss 2806.481 Test RE 0.01506844327086512 c 0.016913872 k 0.061846416 m 3.170296e-05\n",
      "34 Train Loss 2801.7893 Test RE 0.01542421101542809 c 0.016843343 k 0.06176503 m 3.272233e-05\n",
      "35 Train Loss 2800.6558 Test RE 0.015627145838565105 c 0.016856523 k 0.061928265 m 3.5044053e-05\n",
      "36 Train Loss 2799.1343 Test RE 0.015818042477843575 c 0.016838223 k 0.062000223 m 3.4986308e-05\n",
      "37 Train Loss 2780.1897 Test RE 0.015601243738262074 c 0.016861133 k 0.0620475 m 2.4060648e-05\n",
      "38 Train Loss 2683.5479 Test RE 0.01610999614764896 c 0.016884165 k 0.062295638 m 1.1639002e-05\n",
      "39 Train Loss 2548.9946 Test RE 0.013215489350865047 c 0.016623637 k 0.0613429 m -2.167551e-06\n",
      "40 Train Loss 2530.6086 Test RE 0.01160699540256303 c 0.016462749 k 0.060807128 m -1.1823136e-05\n",
      "41 Train Loss 2528.0652 Test RE 0.011543554712971022 c 0.016438188 k 0.060676962 m -1.5856904e-05\n",
      "42 Train Loss 2526.1758 Test RE 0.011640755904086713 c 0.01645662 k 0.060689457 m -1.8775685e-05\n",
      "43 Train Loss 2521.6404 Test RE 0.011451848274588184 c 0.016286364 k 0.060762625 m -3.8507507e-05\n",
      "44 Train Loss 2515.374 Test RE 0.011861619087057443 c 0.01598881 k 0.06063924 m -6.3984524e-05\n",
      "45 Train Loss 2509.4575 Test RE 0.011411818497837291 c 0.015741091 k 0.06070551 m -8.612503e-05\n",
      "46 Train Loss 2506.1904 Test RE 0.01110058252585716 c 0.015478953 k 0.060443297 m -0.000107186\n",
      "47 Train Loss 2495.7344 Test RE 0.011191730059039715 c 0.015336151 k 0.06095563 m -0.00012749352\n",
      "48 Train Loss 2485.0852 Test RE 0.011230807221705929 c 0.0153950155 k 0.060764506 m -0.00012674212\n",
      "49 Train Loss 2481.4683 Test RE 0.011261479289013533 c 0.015322571 k 0.06053465 m -0.0001339453\n",
      "50 Train Loss 2475.8882 Test RE 0.01095633604426951 c 0.015111942 k 0.060852587 m -0.0001567473\n",
      "51 Train Loss 2473.2046 Test RE 0.010320736476151183 c 0.014962928 k 0.060954947 m -0.00016832029\n",
      "52 Train Loss 2466.3 Test RE 0.009693138513471347 c 0.0147843165 k 0.060748264 m -0.00017430991\n",
      "53 Train Loss 2459.5493 Test RE 0.009777081309971285 c 0.014795064 k 0.060522404 m -0.00016561802\n",
      "54 Train Loss 2426.1045 Test RE 0.010416050616106272 c 0.014679335 k 0.060750294 m -0.00014589843\n",
      "55 Train Loss 2403.4546 Test RE 0.01028569564161879 c 0.014644975 k 0.06091369 m -0.00014073754\n",
      "56 Train Loss 2395.0776 Test RE 0.010395867387725863 c 0.014554449 k 0.06103651 m -0.0001442702\n",
      "57 Train Loss 2368.989 Test RE 0.010378096772461846 c 0.013930128 k 0.060650542 m -0.00016704826\n",
      "58 Train Loss 2358.0718 Test RE 0.009792605742399067 c 0.013538499 k 0.060791403 m -0.00018257229\n",
      "59 Train Loss 2353.198 Test RE 0.009298102825185457 c 0.013105932 k 0.06080379 m -0.00020512512\n",
      "60 Train Loss 2339.4763 Test RE 0.009430568212306003 c 0.013025963 k 0.06017062 m -0.00020778223\n",
      "61 Train Loss 2320.5518 Test RE 0.010228583753270872 c 0.013884861 k 0.06082888 m -0.00017162233\n",
      "62 Train Loss 2300.571 Test RE 0.009500441655909563 c 0.014878891 k 0.06076557 m -0.00013814711\n",
      "63 Train Loss 2281.4407 Test RE 0.008998085706666836 c 0.015379952 k 0.060442384 m -0.00012611492\n",
      "64 Train Loss 2243.0854 Test RE 0.00975240461458982 c 0.015666708 k 0.060603097 m -0.00012683883\n",
      "65 Train Loss 2215.8418 Test RE 0.008480684124930145 c 0.015110508 k 0.06056254 m -0.00017138172\n",
      "66 Train Loss 2168.4702 Test RE 0.007819289573360722 c 0.013501167 k 0.060880486 m -0.00026974123\n",
      "67 Train Loss 2122.3447 Test RE 0.009367082435216962 c 0.012236718 k 0.06005108 m -0.00033716907\n",
      "68 Train Loss 1941.9554 Test RE 0.006006349476842373 c 0.0067127286 k 0.061458968 m -0.0006446733\n",
      "69 Train Loss 1866.0685 Test RE 0.005803079450060006 c 0.0036132499 k 0.060290787 m -0.00081527577\n",
      "70 Train Loss 1856.7822 Test RE 0.0056802387020421325 c 0.0026330305 k 0.060862374 m -0.0008708831\n",
      "71 Train Loss 1854.1692 Test RE 0.005653686170706199 c 0.0027598846 k 0.06134594 m -0.0008681181\n",
      "72 Train Loss 1850.9685 Test RE 0.005852783299869681 c 0.0028694489 k 0.060925726 m -0.00086459937\n",
      "73 Train Loss 1848.6575 Test RE 0.0058227777344973825 c 0.0026697048 k 0.06053936 m -0.00087857095\n",
      "74 Train Loss 1843.0889 Test RE 0.005565075350649849 c 0.0026563457 k 0.06102493 m -0.00089066307\n",
      "75 Train Loss 1835.0612 Test RE 0.005425026421018434 c 0.0048639425 k 0.061014347 m -0.00078414544\n",
      "76 Train Loss 1829.8822 Test RE 0.005407954909267338 c 0.00669337 k 0.060837667 m -0.00069251773\n",
      "77 Train Loss 1825.6106 Test RE 0.0052899825058984155 c 0.006788717 k 0.06059058 m -0.00069673604\n",
      "78 Train Loss 1822.6545 Test RE 0.005163425240761691 c 0.0067592133 k 0.060867146 m -0.0007052487\n",
      "79 Train Loss 1817.6344 Test RE 0.00518817572277402 c 0.008116291 k 0.060840044 m -0.0006431565\n",
      "80 Train Loss 1809.706 Test RE 0.005531657357847564 c 0.01445116 k 0.06076353 m -0.00032310977\n",
      "81 Train Loss 1808.5656 Test RE 0.005529993544781811 c 0.015200607 k 0.060709953 m -0.00028438866\n",
      "82 Train Loss 1805.4647 Test RE 0.005689924330841546 c 0.016965507 k 0.060706653 m -0.00019929156\n",
      "83 Train Loss 1796.8569 Test RE 0.005307755982558648 c 0.017302597 k 0.061073467 m -0.00019917206\n",
      "84 Train Loss 1793.1039 Test RE 0.005155204433263623 c 0.01761273 k 0.060768053 m -0.0001947651\n",
      "85 Train Loss 1790.8547 Test RE 0.005229177135655294 c 0.019924428 k 0.060505025 m -8.45588e-05\n",
      "86 Train Loss 1780.6388 Test RE 0.0055233226262232414 c 0.025861604 k 0.06069144 m 0.0002169294\n",
      "87 Train Loss 1774.9774 Test RE 0.005477318504656316 c 0.027042586 k 0.059977047 m 0.00027184488\n",
      "88 Train Loss 1767.3088 Test RE 0.005262465901189964 c 0.028544782 k 0.060539808 m 0.0003400583\n",
      "89 Train Loss 1766.7152 Test RE 0.00529939822929769 c 0.029476572 k 0.060417753 m 0.00038756256\n",
      "90 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "91 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "92 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "93 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "94 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "95 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "96 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "97 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "98 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "99 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "100 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "101 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "102 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "103 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "104 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "105 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "106 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "107 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "108 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "109 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "110 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "111 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "112 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "113 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "114 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "115 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "116 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "117 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "118 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "119 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "120 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "121 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "122 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "123 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "124 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "125 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "126 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "127 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "128 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "129 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "130 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "131 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "132 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "133 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "134 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "135 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "136 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "137 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "138 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "139 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "140 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "141 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "142 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "143 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "144 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "145 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "146 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "147 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "148 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "149 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "150 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "151 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "152 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "153 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "154 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "155 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "156 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "157 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "158 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "159 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "160 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "161 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "162 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "163 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "164 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "165 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "166 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "167 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "168 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "169 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "170 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "171 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "172 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "173 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "174 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "175 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "176 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "177 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "178 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "179 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "180 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "181 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "182 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "183 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "184 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "185 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "186 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "187 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "188 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "189 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "190 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "191 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "192 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "193 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "194 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "195 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "196 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "197 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "198 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "199 Train Loss 1765.9346 Test RE 0.00544915774063788 c 0.031195428 k 0.0604575 m 0.00047709508\n",
      "Training time: 57.39\n",
      "Training time: 57.39\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 2121226.5 Test RE 0.9758102820668257 c 0.0006918544 k 0.0106047215 m -8.6197115e-06\n",
      "1 Train Loss 1770246.5 Test RE 0.891265093649375 c 0.0019076278 k 0.03208944 m -2.3678562e-05\n",
      "2 Train Loss 1305794.0 Test RE 0.7656895138184912 c 0.0029601431 k 0.059062105 m -4.0611987e-05\n",
      "3 Train Loss 275549.4 Test RE 0.34670206783438895 c 0.0062195538 k 0.16055699 m -0.00010148746\n",
      "4 Train Loss 83078.766 Test RE 0.14077016820596733 c 0.0073752315 k 0.20138606 m -0.00012526299\n",
      "5 Train Loss 58097.438 Test RE 0.08790817381091882 c 0.0074962443 k 0.20182414 m -0.00011895247\n",
      "6 Train Loss 53213.79 Test RE 0.08092321888069505 c 0.007511313 k 0.20001116 m -0.0001111579\n",
      "7 Train Loss 41681.184 Test RE 0.07502552059600003 c 0.0075556706 k 0.17923827 m -4.631198e-05\n",
      "8 Train Loss 10606.459 Test RE 0.04793710865131074 c 0.007638823 k 0.10352831 m 0.00017730614\n",
      "9 Train Loss 5056.185 Test RE 0.03861774260297971 c 0.00754669 k 0.06179059 m 0.00029158074\n",
      "10 Train Loss 4722.064 Test RE 0.03545704914440802 c 0.007440202 k 0.051898178 m 0.00031182496\n",
      "11 Train Loss 3854.0005 Test RE 0.030397085240949723 c 0.0072335615 k 0.056507483 m 0.00028189862\n",
      "12 Train Loss 3506.0398 Test RE 0.027477327770305815 c 0.007102669 k 0.058690134 m 0.00026518112\n",
      "13 Train Loss 3466.8577 Test RE 0.02711500807023353 c 0.0070770313 k 0.057264794 m 0.00026735791\n",
      "14 Train Loss 3406.3967 Test RE 0.026868535236889222 c 0.007171425 k 0.058578633 m 0.00027162614\n",
      "15 Train Loss 3283.1345 Test RE 0.025019289670133085 c 0.0074297767 k 0.0596221 m 0.0002906086\n",
      "16 Train Loss 2774.1377 Test RE 0.019711855452689758 c 0.00825954 k 0.06098507 m 0.00035605408\n",
      "17 Train Loss 2607.5518 Test RE 0.01837184124268336 c 0.00878002 k 0.060500212 m 0.00040153106\n",
      "18 Train Loss 2473.853 Test RE 0.015687133766928343 c 0.009331302 k 0.060369417 m 0.0004491069\n",
      "19 Train Loss 2469.7598 Test RE 0.015212014372978254 c 0.009432692 k 0.060779724 m 0.00045648456\n",
      "20 Train Loss 2459.8535 Test RE 0.014583916740132761 c 0.009942392 k 0.060416017 m 0.00049798284\n",
      "21 Train Loss 2392.8364 Test RE 0.013325215619541284 c 0.011325183 k 0.060496744 m 0.0005994167\n",
      "22 Train Loss 2353.5896 Test RE 0.011058245824025696 c 0.012531961 k 0.060795333 m 0.00068302767\n",
      "23 Train Loss 2336.6272 Test RE 0.010337158490765145 c 0.013568805 k 0.060321875 m 0.00075461384\n",
      "24 Train Loss 2327.6208 Test RE 0.01050780459352761 c 0.014773636 k 0.060946938 m 0.0008317287\n",
      "25 Train Loss 2265.6624 Test RE 0.009838537853349455 c 0.01807299 k 0.061290856 m 0.0010397666\n",
      "26 Train Loss 2193.6443 Test RE 0.009472353571898292 c 0.020337705 k 0.060445443 m 0.0011814631\n",
      "27 Train Loss 2149.0713 Test RE 0.009451208035022321 c 0.021917656 k 0.06091155 m 0.0012755278\n",
      "28 Train Loss 2108.5579 Test RE 0.010325218067013283 c 0.023335662 k 0.06045272 m 0.0013641409\n",
      "29 Train Loss 2081.502 Test RE 0.011498668001476066 c 0.024396945 k 0.060297117 m 0.0014339834\n",
      "30 Train Loss 2073.3381 Test RE 0.011126056050621422 c 0.024461444 k 0.06015605 m 0.0014392762\n",
      "31 Train Loss 2021.6412 Test RE 0.009455603093923686 c 0.026072629 k 0.060318716 m 0.0015492511\n",
      "32 Train Loss 1977.6049 Test RE 0.0090155721978597 c 0.027588245 k 0.06071597 m 0.0016484167\n",
      "33 Train Loss 1965.9326 Test RE 0.009140525912300381 c 0.028619315 k 0.0600879 m 0.0017188274\n",
      "34 Train Loss 1960.7522 Test RE 0.008867621327041206 c 0.029575067 k 0.060369853 m 0.0017821629\n",
      "35 Train Loss 1956.6055 Test RE 0.009087778384070551 c 0.030876162 k 0.060432836 m 0.0018681685\n",
      "36 Train Loss 1953.387 Test RE 0.009084725291913698 c 0.031870026 k 0.06011736 m 0.0019350699\n",
      "37 Train Loss 1943.737 Test RE 0.008625210696775323 c 0.033709846 k 0.060532134 m 0.0020575654\n",
      "38 Train Loss 1905.206 Test RE 0.00760185716830122 c 0.03585774 k 0.06025892 m 0.002202636\n",
      "39 Train Loss 1900.8502 Test RE 0.007255426927872931 c 0.036620058 k 0.060313128 m 0.002253824\n",
      "40 Train Loss 1896.0599 Test RE 0.0070598416626371915 c 0.037699763 k 0.06030393 m 0.0023247679\n",
      "41 Train Loss 1892.1156 Test RE 0.007538741896558292 c 0.03852656 k 0.060200322 m 0.0023780647\n",
      "42 Train Loss 1891.2922 Test RE 0.007686245043053423 c 0.039018147 k 0.060192447 m 0.0024102589\n",
      "43 Train Loss 1885.4963 Test RE 0.008011460047259868 c 0.041548166 k 0.059851665 m 0.0025801274\n",
      "44 Train Loss 1845.1775 Test RE 0.0081189994638824 c 0.0474968 k 0.06053465 m 0.0029808758\n",
      "45 Train Loss 1797.905 Test RE 0.006986104930831184 c 0.054071188 k 0.060024235 m 0.0034311176\n",
      "46 Train Loss 1776.0747 Test RE 0.006990426750749489 c 0.056984156 k 0.06051315 m 0.003631774\n",
      "47 Train Loss 1756.7435 Test RE 0.0060390296860244625 c 0.057655174 k 0.060148288 m 0.0036810173\n",
      "48 Train Loss 1751.0012 Test RE 0.005929737803956101 c 0.059143204 k 0.060031574 m 0.0037850607\n",
      "49 Train Loss 1749.27 Test RE 0.0058987416557007505 c 0.06044172 k 0.060340703 m 0.0038733766\n",
      "50 Train Loss 1736.0265 Test RE 0.005661097229299465 c 0.063610695 k 0.060451988 m 0.0040829848\n",
      "51 Train Loss 1732.0632 Test RE 0.005499092170216952 c 0.06441212 k 0.060122896 m 0.0041367477\n",
      "52 Train Loss 1730.6465 Test RE 0.005429461104774397 c 0.06597972 k 0.060142577 m 0.0042432277\n",
      "53 Train Loss 1729.6666 Test RE 0.005413655238415502 c 0.06725227 k 0.060140155 m 0.0043285093\n",
      "54 Train Loss 1726.4298 Test RE 0.00548893525302636 c 0.068752736 k 0.059749804 m 0.0044306656\n",
      "55 Train Loss 1697.6055 Test RE 0.005395314066383774 c 0.073578544 k 0.059640545 m 0.004772793\n",
      "56 Train Loss 1688.2605 Test RE 0.00527150674349869 c 0.07568477 k 0.059986215 m 0.0049276142\n",
      "57 Train Loss 1686.8568 Test RE 0.0053067385567009 c 0.07587435 k 0.05974978 m 0.00494243\n",
      "58 Train Loss 1682.7776 Test RE 0.005158162426078102 c 0.076170035 k 0.05997072 m 0.004960748\n",
      "59 Train Loss 1678.3639 Test RE 0.005202874329667628 c 0.078120835 k 0.060007315 m 0.005095719\n",
      "60 Train Loss 1676.63 Test RE 0.005374807372015507 c 0.08017235 k 0.05982554 m 0.005238579\n",
      "61 Train Loss 1671.8613 Test RE 0.005239816202801834 c 0.079764284 k 0.059501916 m 0.005209921\n",
      "62 Train Loss 1654.5358 Test RE 0.004781113079629088 c 0.081135124 k 0.059909448 m 0.005311812\n",
      "63 Train Loss 1648.1044 Test RE 0.004699402321485077 c 0.08431703 k 0.059825454 m 0.005546358\n",
      "64 Train Loss 1628.6638 Test RE 0.0050393607134355065 c 0.09439551 k 0.05962726 m 0.0062715514\n",
      "65 Train Loss 1575.826 Test RE 0.004403096188071951 c 0.1069125 k 0.059456434 m 0.0071748123\n",
      "66 Train Loss 1537.1759 Test RE 0.0053871164452850544 c 0.12536058 k 0.05906518 m 0.008524988\n",
      "67 Train Loss 1518.9841 Test RE 0.0052242238332540205 c 0.13251115 k 0.058662042 m 0.009055884\n",
      "68 Train Loss 1510.3185 Test RE 0.005578427088196448 c 0.1387799 k 0.059275743 m 0.009509157\n",
      "69 Train Loss 1493.5856 Test RE 0.005451284349527648 c 0.14532213 k 0.058729857 m 0.0099808555\n",
      "70 Train Loss 1453.8849 Test RE 0.005210431228854232 c 0.16054307 k 0.058644388 m 0.011091632\n",
      "71 Train Loss 1421.3402 Test RE 0.004177610133263339 c 0.16694856 k 0.058521602 m 0.011574592\n",
      "72 Train Loss 1404.6345 Test RE 0.0039010356310926675 c 0.17362452 k 0.05828451 m 0.012076945\n",
      "73 Train Loss 1384.4226 Test RE 0.004481774247047345 c 0.18797185 k 0.058378294 m 0.01313892\n",
      "74 Train Loss 1373.8809 Test RE 0.004668978035742695 c 0.19400203 k 0.058224555 m 0.013575084\n",
      "75 Train Loss 1362.1041 Test RE 0.003948426924853324 c 0.19431625 k 0.058377434 m 0.013598449\n",
      "76 Train Loss 1353.3342 Test RE 0.00413893311299658 c 0.19832623 k 0.057974864 m 0.013879552\n",
      "77 Train Loss 1348.9204 Test RE 0.004291604934769905 c 0.20287871 k 0.057888217 m 0.01420133\n",
      "78 Train Loss 1344.9543 Test RE 0.004364425687485692 c 0.207095 k 0.05779231 m 0.014507916\n",
      "79 Train Loss 1337.5353 Test RE 0.00446993839564913 c 0.21123368 k 0.05803846 m 0.014805274\n",
      "80 Train Loss 1300.1829 Test RE 0.004339543910182995 c 0.22709924 k 0.057544876 m 0.015920926\n",
      "81 Train Loss 1259.3973 Test RE 0.003984708603445108 c 0.24310012 k 0.05723728 m 0.017028794\n",
      "82 Train Loss 1233.3969 Test RE 0.004649825256200839 c 0.26292533 k 0.05709883 m 0.018416673\n",
      "83 Train Loss 1226.1117 Test RE 0.004671164430711056 c 0.26801515 k 0.057370324 m 0.018772222\n",
      "84 Train Loss 1219.1957 Test RE 0.005010133751909231 c 0.27282846 k 0.056936506 m 0.019111732\n",
      "85 Train Loss 1197.7103 Test RE 0.004893681631920555 c 0.2840844 k 0.056749094 m 0.019918634\n",
      "86 Train Loss 1178.7449 Test RE 0.004460320247493891 c 0.2927594 k 0.056855623 m 0.020531096\n",
      "87 Train Loss 1133.1587 Test RE 0.00453461201116836 c 0.3081778 k 0.056385748 m 0.021626566\n",
      "88 Train Loss 1113.9181 Test RE 0.004991842449943904 c 0.32508844 k 0.056346048 m 0.02283319\n",
      "89 Train Loss 1099.5818 Test RE 0.005794219583360422 c 0.34422112 k 0.056365743 m 0.024198426\n",
      "90 Train Loss 1072.5963 Test RE 0.005872183558857902 c 0.36762398 k 0.054861654 m 0.025883578\n",
      "91 Train Loss 1029.2505 Test RE 0.0057475125135773965 c 0.3898198 k 0.055474523 m 0.027475588\n",
      "92 Train Loss 995.94684 Test RE 0.006611497480996921 c 0.41955242 k 0.05615171 m 0.029596327\n",
      "93 Train Loss 931.2959 Test RE 0.00626405322032202 c 0.44369635 k 0.054478183 m 0.03133745\n",
      "94 Train Loss 923.6377 Test RE 0.0060350369226469264 c 0.44733787 k 0.054109655 m 0.031604912\n",
      "95 Train Loss 917.62555 Test RE 0.006084992713238213 c 0.4491392 k 0.05428708 m 0.03173588\n",
      "96 Train Loss 911.30884 Test RE 0.006119012342592772 c 0.4553634 k 0.054302633 m 0.032183506\n",
      "97 Train Loss 902.9324 Test RE 0.0058908470700926915 c 0.4602988 k 0.054257646 m 0.032536183\n",
      "98 Train Loss 898.2145 Test RE 0.005764182628936657 c 0.46225485 k 0.054170918 m 0.03267444\n",
      "99 Train Loss 897.3854 Test RE 0.005739585447884364 c 0.46321857 k 0.054160755 m 0.032742556\n",
      "100 Train Loss 896.0512 Test RE 0.005663269641879467 c 0.46492007 k 0.05414596 m 0.032867815\n",
      "101 Train Loss 895.81805 Test RE 0.005637324189123881 c 0.46516833 k 0.05411334 m 0.032886345\n",
      "102 Train Loss 895.491 Test RE 0.005570827162908196 c 0.46426967 k 0.054158468 m 0.03282203\n",
      "103 Train Loss 889.4122 Test RE 0.005463031309787053 c 0.46509615 k 0.054447196 m 0.0328919\n",
      "104 Train Loss 865.8369 Test RE 0.0055498760996513016 c 0.4811698 k 0.053843375 m 0.03407867\n",
      "105 Train Loss 839.87665 Test RE 0.0052432776901249045 c 0.5002068 k 0.05381297 m 0.035482816\n",
      "106 Train Loss 807.86005 Test RE 0.005831776340688318 c 0.52391016 k 0.053278305 m 0.037224352\n",
      "107 Train Loss 798.86505 Test RE 0.00610069747941302 c 0.5323666 k 0.05349153 m 0.037849516\n",
      "108 Train Loss 781.9042 Test RE 0.0060320531166244236 c 0.548757 k 0.053456936 m 0.039076503\n",
      "109 Train Loss 765.9993 Test RE 0.00569297630055934 c 0.55901045 k 0.05182503 m 0.039911196\n",
      "110 Train Loss 723.9995 Test RE 0.005116939662898832 c 0.5629605 k 0.053033784 m 0.040237684\n",
      "111 Train Loss 720.13727 Test RE 0.005086887890225083 c 0.5645691 k 0.052661516 m 0.040351756\n",
      "112 Train Loss 719.1676 Test RE 0.005135165039776434 c 0.56487834 k 0.052519314 m 0.040375326\n",
      "113 Train Loss 715.6775 Test RE 0.0049388549089516755 c 0.5633043 k 0.052708093 m 0.040259227\n",
      "114 Train Loss 712.522 Test RE 0.004747038578164954 c 0.56424487 k 0.05266447 m 0.040325377\n",
      "115 Train Loss 707.8078 Test RE 0.004836946403555246 c 0.5714571 k 0.05254938 m 0.040832803\n",
      "116 Train Loss 706.1833 Test RE 0.0046451330071471475 c 0.5713035 k 0.052619774 m 0.04078845\n",
      "117 Train Loss 703.7814 Test RE 0.004412592843653255 c 0.5723657 k 0.052410576 m 0.040820763\n",
      "118 Train Loss 701.6236 Test RE 0.00447034581929499 c 0.5746547 k 0.05267007 m 0.04097537\n",
      "119 Train Loss 699.2195 Test RE 0.004955110169013273 c 0.5784698 k 0.05275492 m 0.041263644\n",
      "120 Train Loss 692.5578 Test RE 0.004902095011120057 c 0.5799478 k 0.05253339 m 0.04136232\n",
      "121 Train Loss 691.9357 Test RE 0.004807149391758908 c 0.57901484 k 0.052503236 m 0.0412923\n",
      "122 Train Loss 691.65594 Test RE 0.004853084355170183 c 0.5797313 k 0.052551847 m 0.041361745\n",
      "123 Train Loss 689.7206 Test RE 0.005081680605931876 c 0.5862731 k 0.05243267 m 0.041890714\n",
      "124 Train Loss 687.886 Test RE 0.005200810544816215 c 0.58793217 k 0.052280843 m 0.042022783\n",
      "125 Train Loss 682.6062 Test RE 0.004996602735632634 c 0.5866063 k 0.05239988 m 0.041971758\n",
      "126 Train Loss 677.22845 Test RE 0.004603059984104892 c 0.58668435 k 0.052371446 m 0.042032905\n",
      "127 Train Loss 675.16077 Test RE 0.004596462396514829 c 0.5896746 k 0.05237456 m 0.04230952\n",
      "128 Train Loss 674.0662 Test RE 0.004644921903189535 c 0.5924432 k 0.052359894 m 0.042533647\n",
      "129 Train Loss 673.9712 Test RE 0.0046136572077422 c 0.5925259 k 0.05222642 m 0.042544425\n",
      "130 Train Loss 673.5739 Test RE 0.004538746161698201 c 0.59313774 k 0.05218529 m 0.042635124\n",
      "131 Train Loss 671.03217 Test RE 0.004361730275389145 c 0.5987002 k 0.052370783 m 0.043181263\n",
      "132 Train Loss 664.6674 Test RE 0.004694834208200923 c 0.612621 k 0.05178947 m 0.044348624\n",
      "133 Train Loss 658.8944 Test RE 0.005199409248482543 c 0.62739223 k 0.0516732 m 0.045686968\n",
      "134 Train Loss 657.53204 Test RE 0.005453154897512399 c 0.6302521 k 0.051764686 m 0.04595686\n",
      "135 Train Loss 654.8011 Test RE 0.005599737228209308 c 0.6308914 k 0.051764876 m 0.045949258\n",
      "136 Train Loss 648.0679 Test RE 0.00551354329037682 c 0.6378195 k 0.05151915 m 0.04658048\n",
      "137 Train Loss 645.4891 Test RE 0.005370982148430246 c 0.6411567 k 0.051510517 m 0.046893187\n",
      "138 Train Loss 624.31006 Test RE 0.004518455974608372 c 0.652243 k 0.051711723 m 0.04783809\n",
      "139 Train Loss 607.7167 Test RE 0.004324615045486635 c 0.65740776 k 0.05134871 m 0.048196066\n",
      "140 Train Loss 598.25757 Test RE 0.0045197047509515096 c 0.6690044 k 0.051017184 m 0.049174514\n",
      "141 Train Loss 571.1182 Test RE 0.004838427508598488 c 0.6954068 k 0.051029935 m 0.051394943\n",
      "142 Train Loss 562.1963 Test RE 0.004868861395772118 c 0.70516336 k 0.050801106 m 0.05218311\n",
      "143 Train Loss 551.6764 Test RE 0.004635735299854598 c 0.711402 k 0.050789796 m 0.0526574\n",
      "144 Train Loss 531.3623 Test RE 0.004335218163010718 c 0.7227179 k 0.050881967 m 0.053522673\n",
      "145 Train Loss 515.1785 Test RE 0.004473107706186663 c 0.7407181 k 0.05022744 m 0.054889243\n",
      "146 Train Loss 503.34198 Test RE 0.005075407395464609 c 0.7702132 k 0.049705744 m 0.057263285\n",
      "147 Train Loss 460.16306 Test RE 0.005940218648560302 c 0.83946186 k 0.047805294 m 0.062884554\n",
      "148 Train Loss 405.39685 Test RE 0.004369628293309845 c 0.8660486 k 0.048723906 m 0.06506091\n",
      "149 Train Loss 368.03763 Test RE 0.0034616984884719795 c 0.89783865 k 0.048054304 m 0.067593694\n",
      "150 Train Loss 357.5406 Test RE 0.0038437926076976907 c 0.9295669 k 0.047220223 m 0.070185475\n",
      "151 Train Loss 349.3693 Test RE 0.0036940793152792136 c 0.9329571 k 0.0474604 m 0.07048284\n",
      "152 Train Loss 340.3223 Test RE 0.00428394062224168 c 0.95758575 k 0.046994194 m 0.07248745\n",
      "153 Train Loss 332.4986 Test RE 0.003959843866932537 c 0.9722068 k 0.046587296 m 0.07375279\n",
      "154 Train Loss 326.80408 Test RE 0.004106176035025279 c 0.9858974 k 0.046541806 m 0.07492723\n",
      "155 Train Loss 322.83777 Test RE 0.004132221726427445 c 1.0061419 k 0.04622446 m 0.07659279\n",
      "156 Train Loss 318.21698 Test RE 0.004004734485447828 c 1.0242509 k 0.045753743 m 0.078117594\n",
      "157 Train Loss 317.40695 Test RE 0.003967502931433501 c 1.0390862 k 0.045739602 m 0.07934892\n",
      "158 Train Loss 314.9986 Test RE 0.003927340588574998 c 1.0631105 k 0.045184873 m 0.08132462\n",
      "159 Train Loss 312.19012 Test RE 0.003965701260727482 c 1.0601643 k 0.045176893 m 0.08108207\n",
      "160 Train Loss 310.3625 Test RE 0.004142121811041969 c 1.0608325 k 0.04522928 m 0.08111306\n",
      "161 Train Loss 310.00827 Test RE 0.004226441952764071 c 1.0637718 k 0.04502483 m 0.081359655\n",
      "162 Train Loss 308.86737 Test RE 0.004131474570716295 c 1.0680304 k 0.045100193 m 0.08174135\n",
      "163 Train Loss 308.744 Test RE 0.004046447448056342 c 1.0677471 k 0.045106996 m 0.08172029\n",
      "164 Train Loss 308.68063 Test RE 0.004031981750948188 c 1.0685996 k 0.04506766 m 0.08179023\n",
      "165 Train Loss 308.62485 Test RE 0.004044200677152414 c 1.0690258 k 0.045105163 m 0.08182427\n",
      "166 Train Loss 308.5739 Test RE 0.004016327792532149 c 1.0690753 k 0.045113016 m 0.08183222\n",
      "167 Train Loss 308.55682 Test RE 0.00401610226747782 c 1.0687908 k 0.04508446 m 0.08180991\n",
      "168 Train Loss 308.33984 Test RE 0.004039873305151023 c 1.0683148 k 0.045045767 m 0.081771016\n",
      "169 Train Loss 305.3231 Test RE 0.003981994028316873 c 1.0814234 k 0.044922158 m 0.08286707\n",
      "170 Train Loss 304.29956 Test RE 0.003909297013987782 c 1.0860462 k 0.04488673 m 0.083259635\n",
      "171 Train Loss 301.84262 Test RE 0.004057036211793181 c 1.0768977 k 0.044938594 m 0.08249759\n",
      "172 Train Loss 301.58975 Test RE 0.004043600644437381 c 1.0734599 k 0.04516632 m 0.082212955\n",
      "173 Train Loss 301.4859 Test RE 0.00406014317558942 c 1.0722374 k 0.045267675 m 0.08211511\n",
      "174 Train Loss 301.294 Test RE 0.004074657085996291 c 1.0710843 k 0.045181587 m 0.08201399\n",
      "175 Train Loss 301.092 Test RE 0.004023841342828474 c 1.0714915 k 0.045154326 m 0.08202702\n",
      "176 Train Loss 300.84717 Test RE 0.0040424765500279725 c 1.0708176 k 0.04524497 m 0.08197431\n",
      "177 Train Loss 300.73547 Test RE 0.00402813346665876 c 1.0719974 k 0.045210186 m 0.08208113\n",
      "178 Train Loss 300.7204 Test RE 0.004017846911812226 c 1.0716008 k 0.04522469 m 0.08204729\n",
      "179 Train Loss 300.66748 Test RE 0.004031519256849835 c 1.0707647 k 0.045241494 m 0.081973806\n",
      "180 Train Loss 300.63443 Test RE 0.004037514017753023 c 1.0714029 k 0.045229346 m 0.08202393\n",
      "181 Train Loss 300.62433 Test RE 0.00403089735707476 c 1.0708834 k 0.045236107 m 0.08198116\n",
      "182 Train Loss 300.62247 Test RE 0.004028160293195683 c 1.0706363 k 0.045240358 m 0.081961185\n",
      "183 Train Loss 300.61896 Test RE 0.004028628405043116 c 1.0704243 k 0.045236003 m 0.08194358\n",
      "184 Train Loss 300.60394 Test RE 0.004034293935795521 c 1.0701464 k 0.04523064 m 0.081920125\n",
      "185 Train Loss 300.54196 Test RE 0.0040714037103644 c 1.0701468 k 0.045277428 m 0.08192127\n",
      "186 Train Loss 300.39993 Test RE 0.004094852100004781 c 1.0681875 k 0.04538782 m 0.081763916\n",
      "187 Train Loss 299.34613 Test RE 0.004064210678466829 c 1.063246 k 0.045274496 m 0.08138262\n",
      "188 Train Loss 296.92795 Test RE 0.004156632183936532 c 1.0675837 k 0.045187224 m 0.08180824\n",
      "189 Train Loss 295.22217 Test RE 0.004144595961702241 c 1.080152 k 0.045186315 m 0.082919\n",
      "190 Train Loss 294.80817 Test RE 0.004157617186680427 c 1.0832553 k 0.045083106 m 0.08319873\n",
      "191 Train Loss 294.71286 Test RE 0.004187516855065235 c 1.0821286 k 0.045113057 m 0.08310921\n",
      "192 Train Loss 294.6898 Test RE 0.004181960909896873 c 1.0828384 k 0.045175094 m 0.08316966\n",
      "193 Train Loss 294.5191 Test RE 0.004178152541827183 c 1.0852249 k 0.0451571 m 0.08336374\n",
      "194 Train Loss 293.82394 Test RE 0.004251116703261795 c 1.0847416 k 0.045018036 m 0.08333574\n",
      "195 Train Loss 293.53003 Test RE 0.004295307277990715 c 1.0860798 k 0.04514705 m 0.08347381\n",
      "196 Train Loss 293.46643 Test RE 0.004308405324817269 c 1.0876403 k 0.045124106 m 0.083606355\n",
      "197 Train Loss 293.45105 Test RE 0.004297125144809694 c 1.0880401 k 0.04510979 m 0.08363734\n",
      "198 Train Loss 293.4507 Test RE 0.00429714182937643 c 1.0880401 k 0.045109805 m 0.08363734\n",
      "199 Train Loss 293.4507 Test RE 0.00429714182937643 c 1.0880401 k 0.045109805 m 0.08363734\n",
      "Training time: 92.26\n",
      "Training time: 92.26\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 2188422.5 Test RE 0.9912021779684673 c 0.00050820364 k 0.004573916 m -7.1956665e-06\n",
      "1 Train Loss 2049460.1 Test RE 0.9591096524939082 c 0.0013473902 k 0.017394584 m -2.8512803e-05\n",
      "2 Train Loss 1891383.8 Test RE 0.9212961658744357 c 0.0022260686 k 0.023852037 m -3.8702896e-05\n",
      "3 Train Loss 1753870.5 Test RE 0.8871142353688787 c 0.0031203132 k 0.029055778 m -4.6515095e-05\n",
      "4 Train Loss 1384391.8 Test RE 0.7880926336514388 c 0.004929208 k 0.041761898 m -6.232168e-05\n",
      "5 Train Loss 839176.6 Test RE 0.614402964692589 c 0.009437171 k 0.075036995 m -0.00010591742\n",
      "6 Train Loss 186408.78 Test RE 0.28804978069180825 c 0.012791185 k 0.09861092 m -0.0001381209\n",
      "7 Train Loss 19595.42 Test RE 0.08009544004709766 c 0.012932114 k 0.09933092 m -0.00013882849\n",
      "8 Train Loss 8466.861 Test RE 0.02649185276575774 c 0.013193623 k 0.10164007 m -0.00014245255\n",
      "9 Train Loss 8187.129 Test RE 0.02116155843013835 c 0.013218169 k 0.10193596 m -0.0001429966\n",
      "10 Train Loss 8058.903 Test RE 0.021305102385566207 c 0.013182005 k 0.1009745 m -0.00014190373\n",
      "11 Train Loss 6004.2227 Test RE 0.026799748087514202 c 0.0130105745 k 0.08169045 m -0.00012163843\n",
      "12 Train Loss 4278.2407 Test RE 0.024778631059261133 c 0.012972017 k 0.061243426 m -0.000101268844\n",
      "13 Train Loss 4137.577 Test RE 0.02324189059713786 c 0.013013753 k 0.062003892 m -0.000102318634\n",
      "14 Train Loss 4107.3433 Test RE 0.023163613285561987 c 0.013043783 k 0.061627135 m -0.0001021031\n",
      "15 Train Loss 4080.624 Test RE 0.02313652044653755 c 0.013078315 k 0.061907295 m -0.00010242607\n",
      "16 Train Loss 4074.2412 Test RE 0.023213043873766975 c 0.013102694 k 0.06186871 m -0.000102321355\n",
      "17 Train Loss 4021.8428 Test RE 0.022270521500489932 c 0.01315961 k 0.06190881 m -0.00010101732\n",
      "18 Train Loss 3908.2832 Test RE 0.020307302871909343 c 0.013297201 k 0.05956185 m -9.419373e-05\n",
      "19 Train Loss 3821.0337 Test RE 0.019314849405832318 c 0.0136772515 k 0.055858813 m -8.22147e-05\n",
      "20 Train Loss 3789.7473 Test RE 0.017732910980180424 c 0.0138992965 k 0.057583738 m -7.917158e-05\n",
      "21 Train Loss 3775.94 Test RE 0.01751682995891239 c 0.0140478695 k 0.059325267 m -7.823502e-05\n",
      "22 Train Loss 3775.2148 Test RE 0.017603110341209687 c 0.014041222 k 0.05940754 m -7.8424026e-05\n",
      "23 Train Loss 3769.2812 Test RE 0.018547493761183986 c 0.013896998 k 0.059428774 m -8.03825e-05\n",
      "24 Train Loss 3761.739 Test RE 0.018984119674224274 c 0.013823401 k 0.060044914 m -8.101001e-05\n",
      "25 Train Loss 3757.3296 Test RE 0.019360778713026983 c 0.013722735 k 0.059975747 m -8.1562306e-05\n",
      "26 Train Loss 3742.8125 Test RE 0.01880656612922502 c 0.013705898 k 0.05991894 m -7.993013e-05\n",
      "27 Train Loss 3741.906 Test RE 0.01860697370913097 c 0.013726398 k 0.059967916 m -7.949763e-05\n",
      "28 Train Loss 3741.9058 Test RE 0.018606967081288343 c 0.013726399 k 0.059967916 m -7.9497615e-05\n",
      "29 Train Loss 3741.8813 Test RE 0.01857083375606692 c 0.01373043 k 0.05997569 m -7.9398524e-05\n",
      "30 Train Loss 3741.538 Test RE 0.018585699005657205 c 0.013736942 k 0.060044773 m -7.914035e-05\n",
      "31 Train Loss 3740.245 Test RE 0.018933668626982557 c 0.01373891 k 0.059896994 m -7.871541e-05\n",
      "32 Train Loss 3711.4414 Test RE 0.018980818059955964 c 0.014228377 k 0.05943777 m -6.0556024e-05\n",
      "33 Train Loss 3702.857 Test RE 0.018829221825866042 c 0.014330012 k 0.059460387 m -5.7734575e-05\n",
      "34 Train Loss 3701.8545 Test RE 0.018966205740018113 c 0.014392851 k 0.059375234 m -5.5805984e-05\n",
      "35 Train Loss 3699.707 Test RE 0.01877861214375354 c 0.014611084 k 0.05948839 m -4.9258233e-05\n",
      "36 Train Loss 3698.942 Test RE 0.018846676741962796 c 0.014724041 k 0.059597135 m -4.5859e-05\n",
      "37 Train Loss 3695.4167 Test RE 0.019205031461319143 c 0.015172821 k 0.0592438 m -3.3104752e-05\n",
      "38 Train Loss 3689.861 Test RE 0.019341418201358675 c 0.0153164435 k 0.059367944 m -2.977889e-05\n",
      "39 Train Loss 3676.9966 Test RE 0.018737008263540994 c 0.0152045265 k 0.06030753 m -3.5529454e-05\n",
      "40 Train Loss 3521.526 Test RE 0.0182873589549711 c 0.014814943 k 0.060872488 m -5.7511057e-05\n",
      "41 Train Loss 3315.3867 Test RE 0.017889418248485146 c 0.015664387 k 0.06072677 m -4.0212588e-05\n",
      "42 Train Loss 3306.9978 Test RE 0.018199671266629257 c 0.01577934 k 0.060699638 m -3.8715603e-05\n",
      "43 Train Loss 3305.0835 Test RE 0.018754501413855696 c 0.015883237 k 0.06067843 m -3.8184553e-05\n",
      "44 Train Loss 3305.082 Test RE 0.01875449435707118 c 0.015883237 k 0.060678437 m -3.818457e-05\n",
      "45 Train Loss 3304.5044 Test RE 0.01897969157777213 c 0.01592158 k 0.06071083 m -3.802311e-05\n",
      "46 Train Loss 3304.0623 Test RE 0.018827625317818807 c 0.01589718 k 0.060723737 m -3.7863258e-05\n",
      "47 Train Loss 3303.929 Test RE 0.018709715864803352 c 0.015873495 k 0.060706813 m -3.7978632e-05\n",
      "48 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "49 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "50 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "51 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "52 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "53 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "54 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "55 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "56 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "57 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "58 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "59 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "60 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "61 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "62 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "63 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "64 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "65 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "66 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "67 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "68 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "69 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "70 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "71 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "72 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "73 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "74 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "75 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "76 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "77 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "78 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "79 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "80 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "81 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "82 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "83 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "84 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "85 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "86 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "87 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "88 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "89 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "90 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "91 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "92 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "93 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "94 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "95 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "96 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "97 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "98 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "99 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "100 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "101 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "102 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "103 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "104 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "105 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "106 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "107 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "108 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "109 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "110 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "111 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "112 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "113 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "114 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "115 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "116 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "117 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "118 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "119 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "120 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "121 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "122 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "123 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "124 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "125 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "126 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "127 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "128 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "129 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "130 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "131 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "132 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "133 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "134 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "135 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "136 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "137 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "138 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "139 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "140 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "141 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "142 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "143 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "144 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "145 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "146 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "147 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "148 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "149 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "150 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "151 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "152 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "153 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "154 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "155 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "156 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "157 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "158 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "159 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "160 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "161 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "162 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "163 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "164 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "165 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "166 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "167 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "168 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "169 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "170 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "171 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "172 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "173 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "174 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "175 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "176 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "177 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "178 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "179 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "180 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "181 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "182 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "183 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "184 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "185 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "186 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "187 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "188 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "189 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "190 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "191 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "192 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "193 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "194 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "195 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "196 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "197 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "198 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "199 Train Loss 3303.905 Test RE 0.018704873674678702 c 0.015871461 k 0.060713716 m -3.800402e-05\n",
      "Training time: 56.72\n",
      "Training time: 56.72\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 200\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "m_full = []\n",
    "k_full = []\n",
    "c_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "n_val = 5.0\n",
    "rowdy_terms = 2\n",
    "\n",
    "for reps in range(max_reps):   \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []   \n",
    "    beta_val = []\n",
    "    m_val = []\n",
    "    k_val = []\n",
    "    c_val = []\n",
    "\n",
    "    torch.manual_seed(reps*36)\n",
    "    N_f = 10000 #Total number of collocation points\n",
    "\n",
    "    layers = np.array([1,50,50,50,1]) #9 hidden layers\n",
    "    PINN = Sequentialmodel(layers,n_val,rowdy_terms)\n",
    "\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.25, \n",
    "                                max_iter = 10, \n",
    "                                max_eval = 15, \n",
    "                                tolerance_grad = 1e-6, \n",
    "                                tolerance_change = 1e-6, \n",
    "                                history_size = 100, \n",
    "                                line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "    train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    beta_full.append(beta_val)\n",
    "    m_full.append(m_val)\n",
    "    k_full.append(k_val)\n",
    "    c_full.append(c_val)\n",
    "    \n",
    "\n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"beta\": beta_full,\"m\": m_full,\"k\": k_full,\"c\": c_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "pmHEeBpzfFQh",
    "outputId": "990b2054-f35e-4c8e-c378-69ff2eb19f4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fdc6056f990>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_pred = PINN.test()\n",
    "plt.plot(t,x_true,'b')\n",
    "plt.plot(t,x_pred,'r--')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "stan_1D_FODE_tune.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
