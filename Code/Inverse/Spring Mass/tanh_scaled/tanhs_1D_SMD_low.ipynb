{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8byrnUmNKGR",
    "outputId": "510f67c6-b225-41ba-e78a-d0a6e54ded85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uoNYpDzlNKGV"
   },
   "outputs": [],
   "source": [
    "tstart= 0 \n",
    "tstop= 60 \n",
    "\n",
    "increment = 0.1 \n",
    "# Initial condition\n",
    "x_init= [0,0] \n",
    "\n",
    "t = np.arange(tstart,tstop+1,increment)\n",
    "\n",
    "c = 1 # Damping constant \n",
    "k = 1 # Stiffness of the spring\n",
    "m = 5 # Mass \n",
    "F0 = 100\n",
    "\n",
    "def mydiff(x, t):    \n",
    "    F =F0\n",
    "    \n",
    "    dx1dt = x[1] \n",
    "    dx2dt = (F -c*x[1] -k*x[0])/m\n",
    "    dxdt= [dx1dt, dx2dt] \n",
    "    \n",
    "    return dxdt \n",
    "\n",
    "x_full_sol = odeint(mydiff, x_init, t) \n",
    "\n",
    "x_sol = x_full_sol[:,0]\n",
    "v_sol = x_full_sol[:,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BR02v-fkNKGV"
   },
   "outputs": [],
   "source": [
    "level = \"low\"\n",
    "label = \"1D_SMD_tanhs_\" + level\n",
    "\n",
    "loss_thresh = 0.005\n",
    "\n",
    "t_test = t.reshape(-1,1)\n",
    "t_test_tensor = torch.from_numpy(t_test).float().to(device)\n",
    "\n",
    "# bc1_t = t[0].reshape(-1,1)\n",
    "# bc1_x = x_sol[0].reshape(-1,1)\n",
    "# t_bc1_train = torch.from_numpy(bc1_t).float().to(device)\n",
    "# x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "x_mean = np.mean(x_sol.reshape(-1,1))\n",
    "x_std  = np.mean(x_sol.reshape(-1,1))                \n",
    "bc1_t = t.reshape(-1,1)\n",
    "bc1_x = (x_sol-x_mean).reshape(-1,1)/x_std\n",
    "t_bc1_train = torch.from_numpy(bc1_t).float().to(device)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "\n",
    "\n",
    "bc2_t = t[0].reshape(-1,1)\n",
    "t_bc2_train = torch.from_numpy(bc2_t).float().to(device)\n",
    "bc2_val = v_sol[0].reshape(-1,1)\n",
    "bc2_val =torch.from_numpy(bc2_val).float().to(device)\n",
    "\n",
    "t_test = t.reshape(-1,1)\n",
    "t_test_tensor = torch.from_numpy(t_test).float().to(device)\n",
    "x_true = x_sol\n",
    "x_true_norm = np.linalg.norm(x_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(t[0]) \n",
    "ub = np.array(t[-1]) \n",
    "\n",
    "#torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SyyktBKBXRo1"
   },
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    \n",
    "    t01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=t01,random_state =seed)\n",
    "\n",
    "    t_coll = lb + (ub-lb)*sampling(N_f)\n",
    "    # t_coll = np.vstack((t_coll,)) # append training points to collocation points \n",
    "\n",
    "    return t_coll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "o1b21zLnNKGW"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "        \n",
    "        \n",
    "        self.k = Parameter(torch.tensor(0.0))\n",
    "        self.k.requiresGrad = True\n",
    "        self.c = Parameter(torch.tensor(0.0))\n",
    "        self.c.requiresGrad = True\n",
    "        self.m = Parameter(torch.tensor(0.0))\n",
    "        self.m.requiresGrad = True\n",
    "        \n",
    "    'forward pass'\n",
    "    def forward(self,t):\n",
    "        if torch.is_tensor(t) != True:         \n",
    "            t = torch.from_numpy(t)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        t = 2.0*(t - l_b)/(u_b - l_b) - 1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = t.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,t,x):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(t), x)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,t_bc2,bc2_val):\n",
    "        g = t_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        x = self.forward(g)    \n",
    "            \n",
    "        x_t = autograd.grad(x,g,torch.ones([t_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dx_dt = x_t[:,[0]]\n",
    "        \n",
    "        bc2 = dx_dt/x_std\n",
    "        \n",
    "        loss_bc2= self.loss_function(bc2,bc2_val)\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self,t_coll,f_hat):\n",
    "             \n",
    "        g = t_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        x = self.forward(g) \n",
    "\n",
    "        x_t = autograd.grad(x,g,torch.ones([t_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        x_tt = autograd.grad(x_t,g,torch.ones(t_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dx_dt = x_t[:,[0]]\n",
    "        \n",
    "        dx2_d2t = x_tt[:,[0]]\n",
    "        \n",
    "        f = self.m*dx2_d2t/x_std + self.c*dx_dt/x_std + self.k*x/x_std - F0\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,t_bc1,x_bc1,t_bc2,bc2_val,t_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(t_bc1,x_bc1)\n",
    "        loss_bc2 = self.loss_BC2(t_bc2,bc2_val)\n",
    "        loss_f = self.loss_PDE(t_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self):\n",
    "        x_pred = self.forward(t_test_tensor)\n",
    "        x_pred = x_pred.cpu().detach().numpy()\n",
    "\n",
    "        x_pred = x_pred*x_std + x_mean\n",
    "        \n",
    "        return x_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        x_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(x_pred.reshape(-1,1) - x_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(x_pred.reshape(-1,1) - x_true.reshape(-1,1),2)/x_true_norm\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fLY2mT5BOgjD"
   },
   "outputs": [],
   "source": [
    "def train_step(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8srA5uGuObil"
   },
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    m_val.append(PINN.m.cpu().detach().numpy())\n",
    "    k_val.append(PINN.k.cpu().detach().numpy())\n",
    "    c_val.append(PINN.c.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0SezTZ_racQB"
   },
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "\n",
    "    t_coll = colloc_pts(N_f,rep*11)\n",
    "    t_coll =  torch.from_numpy(t_coll).float().to(device)\n",
    "    \n",
    "    f_hat = torch.zeros(t_coll.shape[0],1).to(device)\n",
    "    for i in range(max_iter):      \n",
    "        train_step(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat)\n",
    "         \n",
    "        loss_np = PINN.loss(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test RE\",test_re_loss[-1],\"c\",PINN.c.cpu().detach().numpy(),\"k\",PINN.k.cpu().detach().numpy(),\"m\",PINN.m.cpu().detach().numpy())\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I9BYbcJ0NKGX",
    "outputId": "42720d97-b37a-4c42-b7e5-b77a0ef391f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 10000.051 Test RE 0.2194325469732624 c -0.00068137585 k 0.0030394667 m -1.5578928e-06\n",
      "1 Train Loss 10000.051 Test RE 0.2188820791578294 c -0.00018759503 k 0.0039372332 m -2.0509144e-06\n",
      "2 Train Loss 10000.05 Test RE 0.21842178400123224 c 0.00096892606 k 0.007287445 m -3.4351483e-06\n",
      "3 Train Loss 10000.05 Test RE 0.21838363300779232 c 0.0017687983 k 0.0101707205 m -4.517308e-06\n",
      "4 Train Loss 10000.05 Test RE 0.21860398467192382 c 0.0031559833 k 0.01565352 m -6.4942815e-06\n",
      "5 Train Loss 10000.047 Test RE 0.2981189678561285 c 0.036968354 k 0.15831509 m -5.6456553e-05\n",
      "6 Train Loss 9999.281 Test RE 5.872359487987276 c 1.168381 k 5.2275634 m -0.0018128533\n",
      "7 Train Loss 5860.1455 Test RE 49.94832198193275 c 27.145626 k 123.12568 m -0.04278683\n",
      "8 Train Loss 1707.3086 Test RE 37.51134144882524 c 47.679863 k 224.68535 m -0.07719617\n",
      "9 Train Loss 921.66376 Test RE 26.661411784044446 c 70.94851 k 335.84076 m -0.1148574\n",
      "10 Train Loss 664.2291 Test RE 21.381470313508697 c 90.39501 k 429.1265 m -0.14494175\n",
      "11 Train Loss 529.5357 Test RE 20.443901379556504 c 95.366196 k 452.9998 m -0.15105419\n",
      "12 Train Loss 410.37305 Test RE 17.840021075204127 c 109.11817 k 518.8482 m -0.1705117\n",
      "13 Train Loss 335.21948 Test RE 15.8558329231999 c 124.646095 k 593.56396 m -0.19191457\n",
      "14 Train Loss 253.41635 Test RE 14.346661247880403 c 136.92493 k 652.5751 m -0.20891245\n",
      "15 Train Loss 210.12071 Test RE 13.027489872717954 c 151.14177 k 721.00946 m -0.2295634\n",
      "16 Train Loss 183.1836 Test RE 11.861009013533875 c 167.00479 k 797.25726 m -0.25201318\n",
      "17 Train Loss 167.74277 Test RE 11.31041840642837 c 175.07344 k 835.9432 m -0.2633553\n",
      "18 Train Loss 153.6924 Test RE 10.848754098950208 c 182.4765 k 871.41785 m -0.27299705\n",
      "19 Train Loss 137.82983 Test RE 10.467828261857294 c 189.558 k 905.36774 m -0.28147888\n",
      "20 Train Loss 131.67717 Test RE 10.146629584085355 c 195.51738 k 933.9966 m -0.28919962\n",
      "21 Train Loss 125.9858 Test RE 9.925063347020416 c 199.75232 k 954.2644 m -0.29390755\n",
      "22 Train Loss 121.892715 Test RE 9.811876212410697 c 202.56052 k 967.7398 m -0.29646364\n",
      "23 Train Loss 113.61536 Test RE 9.657429894815731 c 205.6372 k 982.6094 m -0.29811513\n",
      "24 Train Loss 106.42952 Test RE 9.507572720954904 c 208.64955 k 997.1739 m -0.30097756\n",
      "25 Train Loss 100.12204 Test RE 8.970494361257884 c 221.89365 k 1060.6881 m -0.3192403\n",
      "26 Train Loss 94.81589 Test RE 8.707625195512978 c 228.52802 k 1092.5153 m -0.32730392\n",
      "27 Train Loss 92.242 Test RE 8.703069546759053 c 228.42253 k 1092.0612 m -0.32657874\n",
      "28 Train Loss 87.664 Test RE 8.410985840525038 c 236.60849 k 1131.4635 m -0.3362652\n",
      "29 Train Loss 82.42463 Test RE 8.043040056636556 c 247.50139 k 1183.7483 m -0.3496487\n",
      "30 Train Loss 79.76007 Test RE 7.8663392648712485 c 253.29138 k 1211.5269 m -0.35635087\n",
      "31 Train Loss 77.90353 Test RE 7.891870911001661 c 252.10786 k 1205.845 m -0.35434413\n",
      "32 Train Loss 75.82351 Test RE 7.825477732332641 c 254.59615 k 1217.7153 m -0.35696098\n",
      "33 Train Loss 72.245155 Test RE 7.561137172841956 c 263.8014 k 1261.8413 m -0.36661413\n",
      "34 Train Loss 70.56699 Test RE 7.490361825266762 c 265.9223 k 1271.9597 m -0.36872092\n",
      "35 Train Loss 68.796234 Test RE 7.248388730969649 c 275.17035 k 1316.2113 m -0.3800878\n",
      "36 Train Loss 67.780815 Test RE 7.21223272649915 c 276.67743 k 1323.4491 m -0.3818427\n",
      "37 Train Loss 66.584785 Test RE 7.225162055432969 c 276.02567 k 1320.2865 m -0.3807165\n",
      "38 Train Loss 65.29222 Test RE 7.194896356995404 c 277.12592 k 1325.5134 m -0.38214833\n",
      "39 Train Loss 64.47374 Test RE 7.140781533092941 c 279.34378 k 1336.1097 m -0.38499933\n",
      "40 Train Loss 63.560276 Test RE 7.135622375391939 c 279.57614 k 1337.1321 m -0.38452125\n",
      "41 Train Loss 62.578537 Test RE 7.06251885578593 c 282.3038 k 1350.1173 m -0.3870705\n",
      "42 Train Loss 61.433617 Test RE 6.972537174271679 c 286.62878 k 1370.7866 m -0.3920278\n",
      "43 Train Loss 59.7895 Test RE 6.929637692170404 c 288.31235 k 1378.8344 m -0.3932051\n",
      "44 Train Loss 59.27336 Test RE 6.968294710882464 c 286.58603 k 1370.5425 m -0.39030123\n",
      "45 Train Loss 58.22992 Test RE 6.870034897100577 c 290.55576 k 1389.5402 m -0.3932206\n",
      "46 Train Loss 56.870255 Test RE 6.7655380071713775 c 294.81656 k 1409.9559 m -0.39666095\n",
      "47 Train Loss 56.230927 Test RE 6.787905883500726 c 293.97226 k 1405.9094 m -0.39546248\n",
      "48 Train Loss 55.65269 Test RE 6.751885661427415 c 295.6756 k 1414.0702 m -0.39727074\n",
      "49 Train Loss 54.661358 Test RE 6.5454234173379024 c 304.9643 k 1458.5818 m -0.40716994\n",
      "50 Train Loss 54.133392 Test RE 6.483812153535859 c 307.83853 k 1472.3451 m -0.40986088\n",
      "51 Train Loss 53.06629 Test RE 6.517164093834186 c 305.8966 k 1463.0077 m -0.40679836\n",
      "52 Train Loss 52.593082 Test RE 6.52992799707168 c 305.5564 k 1461.3801 m -0.40580952\n",
      "53 Train Loss 51.964607 Test RE 6.52032488170938 c 306.28378 k 1464.8972 m -0.4049373\n",
      "54 Train Loss 51.2956 Test RE 6.486552540893838 c 307.70032 k 1471.7009 m -0.40463814\n",
      "55 Train Loss 50.993874 Test RE 6.453494824098396 c 309.32266 k 1479.4648 m -0.40628594\n",
      "56 Train Loss 50.501183 Test RE 6.436138404261258 c 310.3373 k 1484.302 m -0.4064402\n",
      "57 Train Loss 49.714935 Test RE 6.455510957940593 c 309.37042 k 1479.6461 m -0.40170452\n",
      "58 Train Loss 48.902718 Test RE 6.3588803358711585 c 313.93588 k 1501.5177 m -0.40452674\n",
      "59 Train Loss 48.23001 Test RE 6.289953710975248 c 317.47614 k 1518.4833 m -0.40722963\n",
      "60 Train Loss 47.81806 Test RE 6.293375727599964 c 317.38617 k 1518.0537 m -0.40617552\n",
      "61 Train Loss 47.633873 Test RE 6.245810906056322 c 319.7981 k 1529.6136 m -0.40883353\n",
      "62 Train Loss 47.237198 Test RE 6.185446324592985 c 323.25464 k 1546.1763 m -0.41194043\n",
      "63 Train Loss 46.702255 Test RE 6.104482804540313 c 327.30603 k 1565.59 m -0.41310546\n",
      "64 Train Loss 46.373096 Test RE 6.014564466120275 c 331.9988 k 1588.0803 m -0.41543463\n",
      "65 Train Loss 44.90058 Test RE 5.798282577285067 c 344.4458 k 1647.6737 m -0.42147386\n",
      "66 Train Loss 42.279133 Test RE 5.688917751822692 c 350.71393 k 1677.6086 m -0.42206314\n",
      "67 Train Loss 41.697186 Test RE 5.637241396625959 c 354.27713 k 1694.6527 m -0.4222592\n",
      "68 Train Loss 41.20347 Test RE 5.518087706079154 c 362.00052 k 1731.6499 m -0.42715353\n",
      "69 Train Loss 40.562325 Test RE 5.529569330013907 c 361.19913 k 1727.8223 m -0.424949\n",
      "70 Train Loss 39.824036 Test RE 5.497970646932379 c 363.50452 k 1738.8689 m -0.42247033\n",
      "71 Train Loss 39.542442 Test RE 5.442596980043424 c 367.18582 k 1756.4868 m -0.4246648\n",
      "72 Train Loss 38.97502 Test RE 5.437962715963787 c 367.34714 k 1757.2289 m -0.42344347\n",
      "73 Train Loss 38.492725 Test RE 5.451903489528235 c 366.44922 k 1752.9202 m -0.4212201\n",
      "74 Train Loss 37.985577 Test RE 5.386926323967568 c 371.2741 k 1776.0294 m -0.42643526\n",
      "75 Train Loss 37.443893 Test RE 5.363487008713259 c 372.3206 k 1781.0356 m -0.42588598\n",
      "76 Train Loss 36.722603 Test RE 5.318336808483577 c 375.5494 k 1796.4542 m -0.42361543\n",
      "77 Train Loss 36.13347 Test RE 5.253414168974303 c 380.3671 k 1819.4584 m -0.42149276\n",
      "78 Train Loss 35.670715 Test RE 5.263609425447856 c 379.8173 k 1816.8191 m -0.41869923\n",
      "79 Train Loss 35.582886 Test RE 5.261288022935047 c 379.93082 k 1817.3729 m -0.4180983\n",
      "80 Train Loss 35.496574 Test RE 5.251876133249207 c 380.8168 k 1821.6155 m -0.41826978\n",
      "81 Train Loss 35.337936 Test RE 5.294411926505813 c 377.70007 k 1806.7017 m -0.4165911\n",
      "82 Train Loss 35.187782 Test RE 5.282905683079969 c 378.23828 k 1809.2971 m -0.4174089\n",
      "83 Train Loss 34.999546 Test RE 5.218646293643043 c 383.18976 k 1833.0168 m -0.41958192\n",
      "84 Train Loss 34.81205 Test RE 5.20393853272441 c 384.1809 k 1837.768 m -0.41901252\n",
      "85 Train Loss 34.519 Test RE 5.205484980192065 c 383.77893 k 1835.8657 m -0.4196429\n",
      "86 Train Loss 34.411873 Test RE 5.206922295427012 c 384.1571 k 1837.6699 m -0.41919926\n",
      "87 Train Loss 34.159897 Test RE 5.168334663895015 c 386.80814 k 1850.3483 m -0.41737086\n",
      "88 Train Loss 33.716743 Test RE 5.120278135924964 c 390.3309 k 1867.1934 m -0.41801926\n",
      "89 Train Loss 33.212055 Test RE 5.136008742946003 c 389.4494 k 1862.9584 m -0.41488433\n",
      "90 Train Loss 32.987312 Test RE 5.133772705525787 c 389.26926 k 1862.0968 m -0.41228843\n",
      "91 Train Loss 32.44432 Test RE 5.120977222068785 c 390.34186 k 1867.199 m -0.40736815\n",
      "92 Train Loss 32.089977 Test RE 5.0126391922487725 c 399.0113 k 1908.6866 m -0.40810543\n",
      "93 Train Loss 31.3625 Test RE 4.887792340091274 c 408.99994 k 1956.4772 m -0.4023863\n",
      "94 Train Loss 30.230368 Test RE 4.848026758667314 c 412.80057 k 1974.6212 m -0.38990766\n",
      "95 Train Loss 29.920254 Test RE 4.8385849191334716 c 413.2773 k 1976.8936 m -0.3881035\n",
      "96 Train Loss 29.139647 Test RE 4.715550426369222 c 424.16754 k 2029.0549 m -0.3908486\n",
      "97 Train Loss 28.314062 Test RE 4.683044990220523 c 427.9083 k 2046.9618 m -0.39021906\n",
      "98 Train Loss 26.980335 Test RE 4.5756764177294444 c 437.0103 k 2090.5212 m -0.38925812\n",
      "99 Train Loss 26.370087 Test RE 4.528468908220688 c 441.78143 k 2113.3423 m -0.3884489\n",
      "100 Train Loss 26.007336 Test RE 4.477776241195431 c 447.01282 k 2138.4062 m -0.3869738\n",
      "101 Train Loss 25.465683 Test RE 4.404962652322964 c 454.5143 k 2174.3506 m -0.3778666\n",
      "102 Train Loss 25.12775 Test RE 4.3198482643713625 c 463.53232 k 2217.5483 m -0.37562057\n",
      "103 Train Loss 24.904694 Test RE 4.263098575846349 c 469.66165 k 2246.9087 m -0.3793999\n",
      "104 Train Loss 24.662226 Test RE 4.229526989472031 c 473.61664 k 2265.8325 m -0.3785143\n",
      "105 Train Loss 24.529037 Test RE 4.1942206393722525 c 477.48306 k 2284.337 m -0.37533864\n",
      "106 Train Loss 24.449507 Test RE 4.190648054039737 c 477.79758 k 2285.8308 m -0.37608624\n",
      "107 Train Loss 24.402533 Test RE 4.211838664442988 c 475.44595 k 2274.5652 m -0.37664148\n",
      "108 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "109 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "110 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "111 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "112 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "113 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "114 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "115 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "116 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "117 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "118 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "119 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "120 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "121 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "122 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "123 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "124 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "125 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "126 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "127 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "128 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "129 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "130 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "131 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "132 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "133 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "134 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "135 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "136 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "137 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "138 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "139 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "140 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "141 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "142 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "143 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "144 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "145 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "146 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "147 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "148 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "149 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "150 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "151 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "152 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "153 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "154 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "155 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "156 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "157 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "158 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "159 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "160 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "161 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "162 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "163 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "164 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "165 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "166 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "167 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "168 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "169 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "170 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "171 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "172 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "173 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "174 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "175 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "176 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "177 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "178 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "179 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "180 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "181 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "182 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "183 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "184 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "185 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "186 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "187 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "188 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "189 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "190 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "191 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "192 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "193 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "194 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "195 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "196 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "197 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "198 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "199 Train Loss 24.396376 Test RE 4.223129452950124 c 474.1722 k 2268.4656 m -0.37656203\n",
      "Training time: 30.37\n",
      "Training time: 30.37\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 10000.052 Test RE 0.22086802250404866 c -0.0017260106 k 0.0037455377 m -2.5148033e-06\n",
      "1 Train Loss 10000.051 Test RE 0.22016158341485367 c -0.0010520725 k 0.0066937227 m -4.457066e-06\n",
      "2 Train Loss 10000.051 Test RE 0.22003512683460552 c -0.0006017541 k 0.00907628 m -5.904795e-06\n",
      "3 Train Loss 10000.051 Test RE 0.22006084765315811 c 5.058589e-05 k 0.012909884 m -8.13151e-06\n",
      "4 Train Loss 10000.051 Test RE 0.22057325079902568 c 0.0012774817 k 0.020625984 m -1.2478402e-05\n",
      "5 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "6 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "7 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "8 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "9 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "10 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "11 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "12 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "13 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "14 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "15 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "16 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "17 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "18 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "19 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "20 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "21 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "22 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "23 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "24 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "25 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "26 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "27 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "28 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "29 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "30 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "31 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "32 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "33 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "34 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "35 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "36 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "37 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "38 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "39 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "40 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "41 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "42 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "43 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "44 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "45 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "46 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "47 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "48 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "49 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "50 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "51 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "52 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "53 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "54 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "55 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "56 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "57 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "58 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "59 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "60 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "61 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "62 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "63 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "64 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "65 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "66 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "67 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "68 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "69 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "70 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "71 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "72 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "73 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "74 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "75 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "76 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "77 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "78 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "79 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "80 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "81 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "82 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "83 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "84 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "85 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "86 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "87 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "88 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "89 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "90 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "91 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "92 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "93 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "94 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "95 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "96 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "97 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "98 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "99 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "100 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "101 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "102 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "103 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "104 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "105 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "106 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "107 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "108 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "109 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "110 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "111 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "112 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "113 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "114 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "115 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "116 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "117 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "118 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "119 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "120 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "121 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "122 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "123 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "124 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "125 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "126 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "127 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "128 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "129 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "130 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "131 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "132 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "133 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "134 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "135 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "136 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "137 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "138 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "139 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "140 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "141 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "142 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "143 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "144 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "145 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "146 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "147 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "148 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "149 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "150 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "151 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "152 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "153 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "154 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "155 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "156 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "157 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "158 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "159 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "160 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "161 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "162 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "163 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "164 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "165 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "166 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "167 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "168 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "169 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "170 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "171 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "172 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "173 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "174 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "175 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "176 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "177 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "178 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "179 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "180 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "181 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "182 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "183 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "184 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "185 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "186 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "187 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "188 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "189 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "190 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "191 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "192 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "193 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "194 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "195 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "196 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "197 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "198 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "199 Train Loss 4917.8306 Test RE 63.50516399566651 c 17.045687 k 111.69247 m -0.061843228\n",
      "Training time: 29.10\n",
      "Training time: 29.10\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 10000.051 Test RE 0.21906835906870287 c 0.0007603527 k -6.85759e-10 m 4.156675e-13\n",
      "1 Train Loss 10000.051 Test RE 0.21904987881120752 c 0.00081112806 k 7.649608e-05 m -1.0214315e-08\n",
      "2 Train Loss 10000.051 Test RE 0.21903676188829158 c 0.00094054744 k 0.00013253275 m -2.6607728e-08\n",
      "3 Train Loss 10000.051 Test RE 0.2190271168801703 c 0.0012180188 k 0.00026939285 m -6.439175e-08\n",
      "4 Train Loss 10000.051 Test RE 0.2190184808927834 c 0.0016502456 k 0.00045554966 m -1.2930334e-07\n",
      "5 Train Loss 10000.051 Test RE 0.21900979373518592 c 0.0023558275 k 0.0007993221 m -2.524356e-07\n",
      "6 Train Loss 10000.051 Test RE 0.219000021321581 c 0.003581842 k 0.0014328498 m -4.90875e-07\n",
      "7 Train Loss 10000.051 Test RE 0.21899032464994073 c 0.0058886465 k 0.0027211607 m -9.788024e-07\n",
      "8 Train Loss 10000.051 Test RE 0.21900051627961872 c 0.011207007 k 0.0058225314 m -2.1560381e-06\n",
      "9 Train Loss 10000.051 Test RE 0.21936867287274345 c 0.029990794 k 0.016962958 m -6.387825e-06\n",
      "10 Train Loss 7062.9194 Test RE 56.94274686321277 c 168.43588 k 101.19167 m -0.039264694\n",
      "11 Train Loss 1135.8044 Test RE 30.998763668701827 c 463.86337 k 281.20956 m -0.109352715\n",
      "12 Train Loss 316.31445 Test RE 16.134471374765642 c 967.5197 k 561.54663 m -0.21294355\n",
      "13 Train Loss 109.25665 Test RE 9.830375590872652 c 1650.0713 k 952.59686 m -0.35945043\n",
      "14 Train Loss 45.319065 Test RE 6.409540551789992 c 2559.9712 k 1477.0812 m -0.55622613\n",
      "15 Train Loss 21.089027 Test RE 4.401445082494856 c 3748.01 k 2163.5134 m -0.8138342\n",
      "16 Train Loss 12.227417 Test RE 3.3472373114830565 c 4944.525 k 2855.4763 m -1.0735528\n",
      "17 Train Loss 7.4459777 Test RE 2.611999811466016 c 6351.2637 k 3669.3115 m -1.3790383\n",
      "18 Train Loss 4.880274 Test RE 2.124357160105259 c 7823.96 k 4521.4272 m -1.698903\n",
      "19 Train Loss 3.3140993 Test RE 1.751336217246558 c 9515.327 k 5500.131 m -2.0662925\n",
      "20 Train Loss 2.3783877 Test RE 1.4978301069697435 c 11174.61 k 6460.31 m -2.426732\n",
      "21 Train Loss 1.7861936 Test RE 1.2928558267238537 c 12990.265 k 7511.0044 m -2.821152\n",
      "22 Train Loss 1.4012195 Test RE 1.151666900502498 c 14651.873 k 8472.572 m -3.182116\n",
      "23 Train Loss 1.1304948 Test RE 1.0316028288625911 c 16429.252 k 9501.146 m -3.5682354\n",
      "24 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "25 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "26 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "27 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "28 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "29 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "30 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "31 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "32 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "33 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "34 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "35 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "36 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "37 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "38 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "39 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "40 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "41 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "42 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "43 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "44 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "45 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "46 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "47 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "48 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "49 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "50 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "51 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "52 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "53 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "54 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "55 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "56 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "57 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "58 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "59 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "60 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "61 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "62 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "63 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "64 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "65 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "66 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "67 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "68 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "69 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "70 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "71 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "72 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "73 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "74 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "75 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "76 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "77 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "78 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "79 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "80 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "81 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "82 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "83 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "84 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "85 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "86 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "87 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "88 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "89 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "90 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "91 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "92 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "93 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "94 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "95 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "96 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "97 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "98 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "99 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "100 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "101 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "102 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "103 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "104 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "105 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "106 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "107 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "108 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "109 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "110 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "111 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "112 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "113 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "114 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "115 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "116 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "117 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "118 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "119 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "120 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "121 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "122 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "123 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "124 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "125 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "126 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "127 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "128 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "129 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "130 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "131 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "132 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "133 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "134 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "135 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "136 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "137 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "138 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "139 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "140 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "141 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "142 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "143 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "144 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "145 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "146 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "147 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "148 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "149 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "150 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "151 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "152 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "153 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "154 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "155 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "156 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "157 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "158 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "159 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "160 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "161 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "162 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "163 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "164 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "165 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "166 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "167 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "168 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "169 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "170 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "171 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "172 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "173 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "174 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "175 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "176 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "177 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "178 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "179 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "180 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "181 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "182 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "183 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "184 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "185 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "186 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "187 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "188 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "189 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "190 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "191 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "192 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "193 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "194 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "195 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "196 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "197 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "198 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "199 Train Loss 1.0809801 Test RE 1.0104512812379283 c 16793.6 k 9711.997 m -3.647387\n",
      "Training time: 31.05\n",
      "Training time: 31.05\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 10000.051 Test RE 0.21952443761006946 c 0.00030442173 k 0.00016209061 m -6.3667684e-08\n",
      "1 Train Loss 10000.051 Test RE 0.21901354243244692 c 0.0006629146 k 0.00049656176 m -1.5707971e-07\n",
      "2 Train Loss 10000.051 Test RE 0.2187170104127906 c 0.0011045441 k 0.0005597618 m -2.6503798e-07\n",
      "3 Train Loss 10000.05 Test RE 0.21846438884246275 c 0.0020611372 k 0.0013359556 m -6.1006995e-07\n",
      "4 Train Loss 10000.05 Test RE 0.21841692430516757 c 0.0026023004 k 0.0018773604 m -8.554764e-07\n",
      "5 Train Loss 10000.05 Test RE 0.21839133451399872 c 0.0032742014 k 0.0026516137 m -1.202841e-06\n",
      "6 Train Loss 10000.05 Test RE 0.21838453945524466 c 0.0042458167 k 0.003984513 m -1.7620678e-06\n",
      "7 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "8 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "9 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "10 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "11 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "12 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "13 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "14 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "15 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "16 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "17 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "18 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "19 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "20 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "21 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "22 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "23 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "24 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "25 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "26 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "27 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "28 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "29 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "30 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "31 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "32 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "33 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "34 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "35 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "36 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "37 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "38 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "39 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "40 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "41 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "42 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "43 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "44 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "45 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "46 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "47 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "48 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "49 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "50 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "51 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "52 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "53 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "54 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "55 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "56 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "57 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "58 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "59 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "60 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "61 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "62 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "63 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "64 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "65 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "66 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "67 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "68 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "69 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "70 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "71 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "72 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "73 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "74 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "75 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "76 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "77 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "78 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "79 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "80 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "81 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "82 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "83 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "84 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "85 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "86 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "87 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "88 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "89 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "90 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "91 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "92 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "93 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "94 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "95 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "96 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "97 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "98 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "99 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "100 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "101 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "102 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "103 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "104 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "105 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "106 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "107 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "108 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "109 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "110 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "111 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "112 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "113 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "114 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "115 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "116 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "117 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "118 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "119 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "120 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "121 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "122 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "123 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "124 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "125 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "126 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "127 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "128 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "129 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "130 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "131 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "132 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "133 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "134 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "135 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "136 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "137 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "138 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "139 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "140 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "141 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "142 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "143 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "144 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "145 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "146 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "147 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "148 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "149 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "150 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "151 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "152 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "153 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "154 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "155 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "156 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "157 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "158 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "159 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "160 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "161 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "162 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "163 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "164 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "165 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "166 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "167 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "168 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "169 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "170 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "171 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "172 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "173 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "174 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "175 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "176 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "177 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "178 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "179 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "180 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "181 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "182 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "183 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "184 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "185 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "186 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "187 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "188 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "189 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "190 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "191 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "192 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "193 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "194 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "195 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "196 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "197 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "198 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "199 Train Loss 10000.05 Test RE 0.21843874256202778 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "Training time: 23.25\n",
      "Training time: 23.25\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 10000.05 Test RE 0.21792532588758612 c 0.00044432216 k -3.3456826e-10 m -1.573867e-13\n",
      "1 Train Loss 10000.05 Test RE 0.21788359661923 c 0.000604225 k 8.1936894e-05 m -2.0347766e-08\n",
      "2 Train Loss 10000.05 Test RE 0.2178523518495159 c 0.00086153665 k 0.00018793382 m -6.6125864e-08\n",
      "3 Train Loss 10000.05 Test RE 0.21782590876442526 c 0.0012391836 k 0.0003986114 m -1.6388907e-07\n",
      "4 Train Loss 10000.05 Test RE 0.21780077194685882 c 0.0017937608 k 0.0007198268 m -3.4455906e-07\n",
      "5 Train Loss 10000.05 Test RE 0.21777388555600055 c 0.0026391048 k 0.0012929145 m -6.827857e-07\n",
      "6 Train Loss 10000.05 Test RE 0.21774574984110848 c 0.0040141037 k 0.0023125773 m -1.3200021e-06\n",
      "7 Train Loss 10000.05 Test RE 0.21772707838614355 c 0.006493645 k 0.0043161456 m -2.595481e-06\n",
      "8 Train Loss 10000.05 Test RE 0.21781108991018885 c 0.0118285045 k 0.008852769 m -5.510331e-06\n",
      "9 Train Loss 5864.11 Test RE 67.93573507435798 c 106.96792 k 96.24437 m -0.06217754\n",
      "10 Train Loss 2996.4082 Test RE 41.471235222904845 c 167.43031 k 170.03987 m -0.10824562\n",
      "11 Train Loss 993.7522 Test RE 28.226860519438816 c 306.35767 k 318.5863 m -0.20254678\n",
      "12 Train Loss 472.43826 Test RE 20.393961821646624 c 429.35065 k 453.96518 m -0.28744882\n",
      "13 Train Loss 161.49673 Test RE 12.17379713584809 c 715.2114 k 773.0441 m -0.48626074\n",
      "14 Train Loss 68.54826 Test RE 7.846842294747184 c 1101.4143 k 1198.965 m -0.7528556\n",
      "15 Train Loss 33.075356 Test RE 5.536265530273318 c 1574.4325 k 1718.0647 m -1.0799398\n",
      "16 Train Loss 17.6611 Test RE 3.954230525475944 c 2204.236 k 2407.9329 m -1.5155864\n",
      "17 Train Loss 10.261782 Test RE 3.0915661475090697 c 2832.0198 k 3094.888 m -1.9497106\n",
      "18 Train Loss 7.424633 Test RE 2.612652333827384 c 3352.739 k 3664.4531 m -2.3098922\n",
      "19 Train Loss 6.1064973 Test RE 2.3937057452528996 c 3671.714 k 4013.2944 m -2.530637\n",
      "20 Train Loss 5.761066 Test RE 2.3389701202411746 c 3764.2366 k 4114.477 m -2.5946352\n",
      "21 Train Loss nan Test RE nan c nan k nan m nan\n",
      "22 Train Loss nan Test RE nan c nan k nan m nan\n",
      "23 Train Loss nan Test RE nan c nan k nan m nan\n",
      "24 Train Loss nan Test RE nan c nan k nan m nan\n",
      "25 Train Loss nan Test RE nan c nan k nan m nan\n",
      "26 Train Loss nan Test RE nan c nan k nan m nan\n",
      "27 Train Loss nan Test RE nan c nan k nan m nan\n",
      "28 Train Loss nan Test RE nan c nan k nan m nan\n",
      "29 Train Loss nan Test RE nan c nan k nan m nan\n",
      "30 Train Loss nan Test RE nan c nan k nan m nan\n",
      "31 Train Loss nan Test RE nan c nan k nan m nan\n",
      "32 Train Loss nan Test RE nan c nan k nan m nan\n",
      "33 Train Loss nan Test RE nan c nan k nan m nan\n",
      "34 Train Loss nan Test RE nan c nan k nan m nan\n",
      "35 Train Loss nan Test RE nan c nan k nan m nan\n",
      "36 Train Loss nan Test RE nan c nan k nan m nan\n",
      "37 Train Loss nan Test RE nan c nan k nan m nan\n",
      "38 Train Loss nan Test RE nan c nan k nan m nan\n",
      "39 Train Loss nan Test RE nan c nan k nan m nan\n",
      "40 Train Loss nan Test RE nan c nan k nan m nan\n",
      "41 Train Loss nan Test RE nan c nan k nan m nan\n",
      "42 Train Loss nan Test RE nan c nan k nan m nan\n",
      "43 Train Loss nan Test RE nan c nan k nan m nan\n",
      "44 Train Loss nan Test RE nan c nan k nan m nan\n",
      "45 Train Loss nan Test RE nan c nan k nan m nan\n",
      "46 Train Loss nan Test RE nan c nan k nan m nan\n",
      "47 Train Loss nan Test RE nan c nan k nan m nan\n",
      "48 Train Loss nan Test RE nan c nan k nan m nan\n",
      "49 Train Loss nan Test RE nan c nan k nan m nan\n",
      "50 Train Loss nan Test RE nan c nan k nan m nan\n",
      "51 Train Loss nan Test RE nan c nan k nan m nan\n",
      "52 Train Loss nan Test RE nan c nan k nan m nan\n",
      "53 Train Loss nan Test RE nan c nan k nan m nan\n",
      "54 Train Loss nan Test RE nan c nan k nan m nan\n",
      "55 Train Loss nan Test RE nan c nan k nan m nan\n",
      "56 Train Loss nan Test RE nan c nan k nan m nan\n",
      "57 Train Loss nan Test RE nan c nan k nan m nan\n",
      "58 Train Loss nan Test RE nan c nan k nan m nan\n",
      "59 Train Loss nan Test RE nan c nan k nan m nan\n",
      "60 Train Loss nan Test RE nan c nan k nan m nan\n",
      "61 Train Loss nan Test RE nan c nan k nan m nan\n",
      "62 Train Loss nan Test RE nan c nan k nan m nan\n",
      "63 Train Loss nan Test RE nan c nan k nan m nan\n",
      "64 Train Loss nan Test RE nan c nan k nan m nan\n",
      "65 Train Loss nan Test RE nan c nan k nan m nan\n",
      "66 Train Loss nan Test RE nan c nan k nan m nan\n",
      "67 Train Loss nan Test RE nan c nan k nan m nan\n",
      "68 Train Loss nan Test RE nan c nan k nan m nan\n",
      "69 Train Loss nan Test RE nan c nan k nan m nan\n",
      "70 Train Loss nan Test RE nan c nan k nan m nan\n",
      "71 Train Loss nan Test RE nan c nan k nan m nan\n",
      "72 Train Loss nan Test RE nan c nan k nan m nan\n",
      "73 Train Loss nan Test RE nan c nan k nan m nan\n",
      "74 Train Loss nan Test RE nan c nan k nan m nan\n",
      "75 Train Loss nan Test RE nan c nan k nan m nan\n",
      "76 Train Loss nan Test RE nan c nan k nan m nan\n",
      "77 Train Loss nan Test RE nan c nan k nan m nan\n",
      "78 Train Loss nan Test RE nan c nan k nan m nan\n",
      "79 Train Loss nan Test RE nan c nan k nan m nan\n",
      "80 Train Loss nan Test RE nan c nan k nan m nan\n",
      "81 Train Loss nan Test RE nan c nan k nan m nan\n",
      "82 Train Loss nan Test RE nan c nan k nan m nan\n",
      "83 Train Loss nan Test RE nan c nan k nan m nan\n",
      "84 Train Loss nan Test RE nan c nan k nan m nan\n",
      "85 Train Loss nan Test RE nan c nan k nan m nan\n",
      "86 Train Loss nan Test RE nan c nan k nan m nan\n",
      "87 Train Loss nan Test RE nan c nan k nan m nan\n",
      "88 Train Loss nan Test RE nan c nan k nan m nan\n",
      "89 Train Loss nan Test RE nan c nan k nan m nan\n",
      "90 Train Loss nan Test RE nan c nan k nan m nan\n",
      "91 Train Loss nan Test RE nan c nan k nan m nan\n",
      "92 Train Loss nan Test RE nan c nan k nan m nan\n",
      "93 Train Loss nan Test RE nan c nan k nan m nan\n",
      "94 Train Loss nan Test RE nan c nan k nan m nan\n",
      "95 Train Loss nan Test RE nan c nan k nan m nan\n",
      "96 Train Loss nan Test RE nan c nan k nan m nan\n",
      "97 Train Loss nan Test RE nan c nan k nan m nan\n",
      "98 Train Loss nan Test RE nan c nan k nan m nan\n",
      "99 Train Loss nan Test RE nan c nan k nan m nan\n",
      "100 Train Loss nan Test RE nan c nan k nan m nan\n",
      "101 Train Loss nan Test RE nan c nan k nan m nan\n",
      "102 Train Loss nan Test RE nan c nan k nan m nan\n",
      "103 Train Loss nan Test RE nan c nan k nan m nan\n",
      "104 Train Loss nan Test RE nan c nan k nan m nan\n",
      "105 Train Loss nan Test RE nan c nan k nan m nan\n",
      "106 Train Loss nan Test RE nan c nan k nan m nan\n",
      "107 Train Loss nan Test RE nan c nan k nan m nan\n",
      "108 Train Loss nan Test RE nan c nan k nan m nan\n",
      "109 Train Loss nan Test RE nan c nan k nan m nan\n",
      "110 Train Loss nan Test RE nan c nan k nan m nan\n",
      "111 Train Loss nan Test RE nan c nan k nan m nan\n",
      "112 Train Loss nan Test RE nan c nan k nan m nan\n",
      "113 Train Loss nan Test RE nan c nan k nan m nan\n",
      "114 Train Loss nan Test RE nan c nan k nan m nan\n",
      "115 Train Loss nan Test RE nan c nan k nan m nan\n",
      "116 Train Loss nan Test RE nan c nan k nan m nan\n",
      "117 Train Loss nan Test RE nan c nan k nan m nan\n",
      "118 Train Loss nan Test RE nan c nan k nan m nan\n",
      "119 Train Loss nan Test RE nan c nan k nan m nan\n",
      "120 Train Loss nan Test RE nan c nan k nan m nan\n",
      "121 Train Loss nan Test RE nan c nan k nan m nan\n",
      "122 Train Loss nan Test RE nan c nan k nan m nan\n",
      "123 Train Loss nan Test RE nan c nan k nan m nan\n",
      "124 Train Loss nan Test RE nan c nan k nan m nan\n",
      "125 Train Loss nan Test RE nan c nan k nan m nan\n",
      "126 Train Loss nan Test RE nan c nan k nan m nan\n",
      "127 Train Loss nan Test RE nan c nan k nan m nan\n",
      "128 Train Loss nan Test RE nan c nan k nan m nan\n",
      "129 Train Loss nan Test RE nan c nan k nan m nan\n",
      "130 Train Loss nan Test RE nan c nan k nan m nan\n",
      "131 Train Loss nan Test RE nan c nan k nan m nan\n",
      "132 Train Loss nan Test RE nan c nan k nan m nan\n",
      "133 Train Loss nan Test RE nan c nan k nan m nan\n",
      "134 Train Loss nan Test RE nan c nan k nan m nan\n",
      "135 Train Loss nan Test RE nan c nan k nan m nan\n",
      "136 Train Loss nan Test RE nan c nan k nan m nan\n",
      "137 Train Loss nan Test RE nan c nan k nan m nan\n",
      "138 Train Loss nan Test RE nan c nan k nan m nan\n",
      "139 Train Loss nan Test RE nan c nan k nan m nan\n",
      "140 Train Loss nan Test RE nan c nan k nan m nan\n",
      "141 Train Loss nan Test RE nan c nan k nan m nan\n",
      "142 Train Loss nan Test RE nan c nan k nan m nan\n",
      "143 Train Loss nan Test RE nan c nan k nan m nan\n",
      "144 Train Loss nan Test RE nan c nan k nan m nan\n",
      "145 Train Loss nan Test RE nan c nan k nan m nan\n",
      "146 Train Loss nan Test RE nan c nan k nan m nan\n",
      "147 Train Loss nan Test RE nan c nan k nan m nan\n",
      "148 Train Loss nan Test RE nan c nan k nan m nan\n",
      "149 Train Loss nan Test RE nan c nan k nan m nan\n",
      "150 Train Loss nan Test RE nan c nan k nan m nan\n",
      "151 Train Loss nan Test RE nan c nan k nan m nan\n",
      "152 Train Loss nan Test RE nan c nan k nan m nan\n",
      "153 Train Loss nan Test RE nan c nan k nan m nan\n",
      "154 Train Loss nan Test RE nan c nan k nan m nan\n",
      "155 Train Loss nan Test RE nan c nan k nan m nan\n",
      "156 Train Loss nan Test RE nan c nan k nan m nan\n",
      "157 Train Loss nan Test RE nan c nan k nan m nan\n",
      "158 Train Loss nan Test RE nan c nan k nan m nan\n",
      "159 Train Loss nan Test RE nan c nan k nan m nan\n",
      "160 Train Loss nan Test RE nan c nan k nan m nan\n",
      "161 Train Loss nan Test RE nan c nan k nan m nan\n",
      "162 Train Loss nan Test RE nan c nan k nan m nan\n",
      "163 Train Loss nan Test RE nan c nan k nan m nan\n",
      "164 Train Loss nan Test RE nan c nan k nan m nan\n",
      "165 Train Loss nan Test RE nan c nan k nan m nan\n",
      "166 Train Loss nan Test RE nan c nan k nan m nan\n",
      "167 Train Loss nan Test RE nan c nan k nan m nan\n",
      "168 Train Loss nan Test RE nan c nan k nan m nan\n",
      "169 Train Loss nan Test RE nan c nan k nan m nan\n",
      "170 Train Loss nan Test RE nan c nan k nan m nan\n",
      "171 Train Loss nan Test RE nan c nan k nan m nan\n",
      "172 Train Loss nan Test RE nan c nan k nan m nan\n",
      "173 Train Loss nan Test RE nan c nan k nan m nan\n",
      "174 Train Loss nan Test RE nan c nan k nan m nan\n",
      "175 Train Loss nan Test RE nan c nan k nan m nan\n",
      "176 Train Loss nan Test RE nan c nan k nan m nan\n",
      "177 Train Loss nan Test RE nan c nan k nan m nan\n",
      "178 Train Loss nan Test RE nan c nan k nan m nan\n",
      "179 Train Loss nan Test RE nan c nan k nan m nan\n",
      "180 Train Loss nan Test RE nan c nan k nan m nan\n",
      "181 Train Loss nan Test RE nan c nan k nan m nan\n",
      "182 Train Loss nan Test RE nan c nan k nan m nan\n",
      "183 Train Loss nan Test RE nan c nan k nan m nan\n",
      "184 Train Loss nan Test RE nan c nan k nan m nan\n",
      "185 Train Loss nan Test RE nan c nan k nan m nan\n",
      "186 Train Loss nan Test RE nan c nan k nan m nan\n",
      "187 Train Loss nan Test RE nan c nan k nan m nan\n",
      "188 Train Loss nan Test RE nan c nan k nan m nan\n",
      "189 Train Loss nan Test RE nan c nan k nan m nan\n",
      "190 Train Loss nan Test RE nan c nan k nan m nan\n",
      "191 Train Loss nan Test RE nan c nan k nan m nan\n",
      "192 Train Loss nan Test RE nan c nan k nan m nan\n",
      "193 Train Loss nan Test RE nan c nan k nan m nan\n",
      "194 Train Loss nan Test RE nan c nan k nan m nan\n",
      "195 Train Loss nan Test RE nan c nan k nan m nan\n",
      "196 Train Loss nan Test RE nan c nan k nan m nan\n",
      "197 Train Loss nan Test RE nan c nan k nan m nan\n",
      "198 Train Loss nan Test RE nan c nan k nan m nan\n",
      "199 Train Loss nan Test RE nan c nan k nan m nan\n",
      "Training time: 59.39\n",
      "Training time: 59.39\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 10000.052 Test RE 0.22082514898837882 c -0.002379665 k 0.0035942171 m -1.918405e-06\n",
      "1 Train Loss 10000.051 Test RE 0.21950711289651229 c -0.0015494177 k 0.0066002873 m -3.3306878e-06\n",
      "2 Train Loss 10000.051 Test RE 0.21928522103068446 c -0.0010277742 k 0.008844469 m -4.3067494e-06\n",
      "3 Train Loss 10000.051 Test RE 0.21921483274859016 c -0.00034673803 k 0.012397513 m -5.7059215e-06\n",
      "4 Train Loss 10000.051 Test RE 0.21951233560162037 c 0.0008029429 k 0.018918721 m -8.1864255e-06\n",
      "5 Train Loss 10000.034 Test RE 0.6237973956037315 c 0.069627434 k 0.4440734 m -0.00016437571\n",
      "6 Train Loss 9053.603 Test RE 42.65285699954023 c 13.351461 k 89.972115 m -0.032355845\n",
      "7 Train Loss 954.2454 Test RE 28.481337608433865 c 42.180016 k 303.4305 m -0.10684264\n",
      "8 Train Loss 204.93707 Test RE 13.534695946265625 c 93.62284 k 683.5247 m -0.23957591\n",
      "9 Train Loss 80.58269 Test RE 8.488283061050632 c 151.334 k 1107.1917 m -0.38777405\n",
      "10 Train Loss 35.706577 Test RE 5.726972947774572 c 226.5637 k 1658.4105 m -0.58066833\n",
      "11 Train Loss 18.834915 Test RE 4.154665431533862 c 313.72913 k 2296.62 m -0.80394894\n",
      "12 Train Loss 11.067072 Test RE 3.2037991668778307 c 408.08752 k 2987.261 m -1.0455285\n",
      "13 Train Loss 6.673217 Test RE 2.4935868743540377 c 525.8072 k 3848.7393 m -1.3468301\n",
      "14 Train Loss 4.425957 Test RE 2.027188405490391 c 648.74567 k 4748.3267 m -1.6613059\n",
      "15 Train Loss 3.0384035 Test RE 1.6917025050395906 c 780.1438 k 5709.773 m -1.9973046\n",
      "16 Train Loss 2.268681 Test RE 1.4393237391982605 c 918.24207 k 6720.2124 m -2.3502326\n",
      "17 Train Loss 1.741831 Test RE 1.281170296027914 c 1036.5746 k 7586.013 m -2.6526263\n",
      "18 Train Loss 1.3223743 Test RE 1.1198575737201848 c 1192.2606 k 8725.104 m -3.050399\n",
      "19 Train Loss 1.0612218 Test RE 0.9990885935741081 c 1342.7341 k 9826.045 m -3.434784\n",
      "20 Train Loss 0.8491157 Test RE 0.8955140077580243 c 1508.0724 k 11035.738 m -3.8570688\n",
      "21 Train Loss 0.70655566 Test RE 0.8169841964813932 c 1664.1182 k 12177.436 m -4.2555666\n",
      "22 Train Loss 0.58940184 Test RE 0.7452209561044539 c 1839.055 k 13457.34 m -4.7022567\n",
      "23 Train Loss 0.52669096 Test RE 0.7071952030650913 c 1950.3816 k 14271.847 m -4.9865\n",
      "24 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "25 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "26 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "27 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "28 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "29 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "30 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "31 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "32 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "33 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "34 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "35 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "36 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "37 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "38 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "39 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "40 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "41 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "42 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "43 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "44 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "45 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "46 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "47 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "48 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "49 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "50 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "51 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "52 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "53 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "54 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "55 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "56 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "57 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "58 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "59 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "60 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "61 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "62 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "63 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "64 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "65 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "66 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "67 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "68 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "69 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "70 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "71 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "72 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "73 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "74 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "75 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "76 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "77 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "78 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "79 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "80 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "81 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "82 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "83 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "84 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "85 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "86 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "87 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "88 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "89 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "90 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "91 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "92 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "93 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "94 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "95 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "96 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "97 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "98 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "99 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "100 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "101 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "102 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "103 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "104 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "105 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "106 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "107 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "108 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "109 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "110 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "111 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "112 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "113 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "114 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "115 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "116 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "117 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "118 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "119 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "120 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "121 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "122 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "123 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "124 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "125 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "126 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "127 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "128 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "129 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "130 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "131 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "132 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "133 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "134 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "135 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "136 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "137 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "138 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "139 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "140 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "141 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "142 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "143 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "144 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "145 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "146 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "147 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "148 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "149 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "150 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "151 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "152 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "153 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "154 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "155 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "156 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "157 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "158 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "159 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "160 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "161 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "162 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "163 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "164 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "165 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "166 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "167 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "168 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "169 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "170 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "171 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "172 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "173 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "174 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "175 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "176 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "177 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "178 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "179 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "180 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "181 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "182 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "183 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "184 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "185 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "186 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "187 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "188 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "189 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "190 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "191 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "192 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "193 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "194 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "195 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "196 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "197 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "198 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "199 Train Loss 0.48968175 Test RE 0.6772192012106755 c 2044.447 k 14960.063 m -5.226663\n",
      "Training time: 29.55\n",
      "Training time: 29.55\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 10000.051 Test RE 0.21985480360665394 c 0.0052871886 k 0.002623205 m -1.8231195e-06\n",
      "1 Train Loss 10000.051 Test RE 0.21955484442160686 c 0.005752501 k 0.0034425694 m -2.3844968e-06\n",
      "2 Train Loss 10000.051 Test RE 0.21938336148880927 c 0.006234019 k 0.0044752485 m -3.0643566e-06\n",
      "3 Train Loss 10000.051 Test RE 0.2192875454617728 c 0.0067674825 k 0.005868553 m -3.9259485e-06\n",
      "4 Train Loss 10000.051 Test RE 0.21925451351920908 c 0.0074373083 k 0.007905126 m -5.1238067e-06\n",
      "5 Train Loss 10000.051 Test RE 0.21933200295187022 c 0.0084642675 k 0.011395247 m -7.093393e-06\n",
      "6 Train Loss 10000.049 Test RE 0.22496490887664597 c 0.017370041 k 0.04435404 m -2.5126194e-05\n",
      "7 Train Loss 9947.113 Test RE 1.2903916200249195 c 4.894387 k 21.111057 m -0.010910493\n",
      "8 Train Loss 133.38817 Test RE 10.736413878423772 c 199.88132 k 861.7517 m -0.4455285\n",
      "9 Train Loss 31.651974 Test RE 5.485718560696765 c 407.05243 k 1750.1523 m -0.90552497\n",
      "10 Train Loss 14.019907 Test RE 3.6139273167510395 c 615.30115 k 2642.6091 m -1.3676989\n",
      "11 Train Loss 4.3840356 Test RE 2.0321993306132486 c 1108.0587 k 4753.779 m -2.4610872\n",
      "12 Train Loss 2.7180703 Test RE 1.6040580964322344 c 1408.0331 k 6039.06 m -3.126733\n",
      "13 Train Loss 1.9902782 Test RE 1.3728921379986698 c 1650.8673 k 7079.5674 m -3.6656034\n",
      "14 Train Loss 1.5587074 Test RE 1.213657437236814 c 1874.975 k 8039.87 m -4.162932\n",
      "15 Train Loss 1.2224889 Test RE 1.0738981525617666 c 2128.142 k 9124.723 m -4.72476\n",
      "16 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "17 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "18 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "19 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "20 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "21 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "22 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "23 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "24 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "25 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "26 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "27 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "28 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "29 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "30 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "31 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "32 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "33 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "34 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "35 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "36 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "37 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "38 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "39 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "40 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "41 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "42 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "43 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "44 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "45 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "46 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "47 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "48 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "49 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "50 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "51 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "52 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "53 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "54 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "55 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "56 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "57 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "58 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "59 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "60 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "61 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "62 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "63 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "64 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "65 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "66 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "67 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "68 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "69 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "70 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "71 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "72 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "73 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "74 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "75 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "76 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "77 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "78 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "79 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "80 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "81 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "82 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "83 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "84 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "85 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "86 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "87 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "88 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "89 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "90 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "91 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "92 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "93 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "94 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "95 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "96 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "97 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "98 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "99 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "100 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "101 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "102 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "103 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "104 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "105 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "106 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "107 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "108 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "109 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "110 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "111 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "112 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "113 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "114 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "115 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "116 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "117 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "118 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "119 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "120 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "121 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "122 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "123 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "124 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "125 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "126 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "127 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "128 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "129 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "130 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "131 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "132 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "133 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "134 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "135 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "136 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "137 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "138 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "139 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "140 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "141 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "142 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "143 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "144 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "145 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "146 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "147 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "148 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "149 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "150 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "151 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "152 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "153 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "154 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "155 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "156 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "157 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "158 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "159 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "160 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "161 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "162 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "163 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "164 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "165 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "166 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "167 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "168 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "169 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "170 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "171 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "172 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "173 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "174 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "175 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "176 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "177 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "178 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "179 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "180 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "181 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "182 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "183 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "184 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "185 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "186 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "187 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "188 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "189 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "190 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "191 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "192 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "193 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "194 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "195 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "196 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "197 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "198 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "199 Train Loss 1.1313531 Test RE 1.0216672637888917 c 2244.7983 k 9624.617 m -4.983646\n",
      "Training time: 24.30\n",
      "Training time: 24.30\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 10000.052 Test RE 0.22090402051097283 c 0.008818127 k 0.004521011 m -3.417505e-06\n",
      "1 Train Loss 10000.051 Test RE 0.22003984843158142 c 0.010215069 k 0.008191489 m -5.6704766e-06\n",
      "2 Train Loss 10000.051 Test RE 0.2199267248783871 c 0.01108406 k 0.011635719 m -7.4607874e-06\n",
      "3 Train Loss 10000.051 Test RE 0.22015679781920694 c 0.012584386 k 0.018600566 m -1.0833881e-05\n",
      "4 Train Loss 9999.508 Test RE 2.543818463164635 c 0.51422423 k 2.7093177 m -0.0012450751\n",
      "5 Train Loss nan Test RE nan c nan k nan m nan\n",
      "6 Train Loss nan Test RE nan c nan k nan m nan\n",
      "7 Train Loss nan Test RE nan c nan k nan m nan\n",
      "8 Train Loss nan Test RE nan c nan k nan m nan\n",
      "9 Train Loss nan Test RE nan c nan k nan m nan\n",
      "10 Train Loss nan Test RE nan c nan k nan m nan\n",
      "11 Train Loss nan Test RE nan c nan k nan m nan\n",
      "12 Train Loss nan Test RE nan c nan k nan m nan\n",
      "13 Train Loss nan Test RE nan c nan k nan m nan\n",
      "14 Train Loss nan Test RE nan c nan k nan m nan\n",
      "15 Train Loss nan Test RE nan c nan k nan m nan\n",
      "16 Train Loss nan Test RE nan c nan k nan m nan\n",
      "17 Train Loss nan Test RE nan c nan k nan m nan\n",
      "18 Train Loss nan Test RE nan c nan k nan m nan\n",
      "19 Train Loss nan Test RE nan c nan k nan m nan\n",
      "20 Train Loss nan Test RE nan c nan k nan m nan\n",
      "21 Train Loss nan Test RE nan c nan k nan m nan\n",
      "22 Train Loss nan Test RE nan c nan k nan m nan\n",
      "23 Train Loss nan Test RE nan c nan k nan m nan\n",
      "24 Train Loss nan Test RE nan c nan k nan m nan\n",
      "25 Train Loss nan Test RE nan c nan k nan m nan\n",
      "26 Train Loss nan Test RE nan c nan k nan m nan\n",
      "27 Train Loss nan Test RE nan c nan k nan m nan\n",
      "28 Train Loss nan Test RE nan c nan k nan m nan\n",
      "29 Train Loss nan Test RE nan c nan k nan m nan\n",
      "30 Train Loss nan Test RE nan c nan k nan m nan\n",
      "31 Train Loss nan Test RE nan c nan k nan m nan\n",
      "32 Train Loss nan Test RE nan c nan k nan m nan\n",
      "33 Train Loss nan Test RE nan c nan k nan m nan\n",
      "34 Train Loss nan Test RE nan c nan k nan m nan\n",
      "35 Train Loss nan Test RE nan c nan k nan m nan\n",
      "36 Train Loss nan Test RE nan c nan k nan m nan\n",
      "37 Train Loss nan Test RE nan c nan k nan m nan\n",
      "38 Train Loss nan Test RE nan c nan k nan m nan\n",
      "39 Train Loss nan Test RE nan c nan k nan m nan\n",
      "40 Train Loss nan Test RE nan c nan k nan m nan\n",
      "41 Train Loss nan Test RE nan c nan k nan m nan\n",
      "42 Train Loss nan Test RE nan c nan k nan m nan\n",
      "43 Train Loss nan Test RE nan c nan k nan m nan\n",
      "44 Train Loss nan Test RE nan c nan k nan m nan\n",
      "45 Train Loss nan Test RE nan c nan k nan m nan\n",
      "46 Train Loss nan Test RE nan c nan k nan m nan\n",
      "47 Train Loss nan Test RE nan c nan k nan m nan\n",
      "48 Train Loss nan Test RE nan c nan k nan m nan\n",
      "49 Train Loss nan Test RE nan c nan k nan m nan\n",
      "50 Train Loss nan Test RE nan c nan k nan m nan\n",
      "51 Train Loss nan Test RE nan c nan k nan m nan\n",
      "52 Train Loss nan Test RE nan c nan k nan m nan\n",
      "53 Train Loss nan Test RE nan c nan k nan m nan\n",
      "54 Train Loss nan Test RE nan c nan k nan m nan\n",
      "55 Train Loss nan Test RE nan c nan k nan m nan\n",
      "56 Train Loss nan Test RE nan c nan k nan m nan\n",
      "57 Train Loss nan Test RE nan c nan k nan m nan\n",
      "58 Train Loss nan Test RE nan c nan k nan m nan\n",
      "59 Train Loss nan Test RE nan c nan k nan m nan\n",
      "60 Train Loss nan Test RE nan c nan k nan m nan\n",
      "61 Train Loss nan Test RE nan c nan k nan m nan\n",
      "62 Train Loss nan Test RE nan c nan k nan m nan\n",
      "63 Train Loss nan Test RE nan c nan k nan m nan\n",
      "64 Train Loss nan Test RE nan c nan k nan m nan\n",
      "65 Train Loss nan Test RE nan c nan k nan m nan\n",
      "66 Train Loss nan Test RE nan c nan k nan m nan\n",
      "67 Train Loss nan Test RE nan c nan k nan m nan\n",
      "68 Train Loss nan Test RE nan c nan k nan m nan\n",
      "69 Train Loss nan Test RE nan c nan k nan m nan\n",
      "70 Train Loss nan Test RE nan c nan k nan m nan\n",
      "71 Train Loss nan Test RE nan c nan k nan m nan\n",
      "72 Train Loss nan Test RE nan c nan k nan m nan\n",
      "73 Train Loss nan Test RE nan c nan k nan m nan\n",
      "74 Train Loss nan Test RE nan c nan k nan m nan\n",
      "75 Train Loss nan Test RE nan c nan k nan m nan\n",
      "76 Train Loss nan Test RE nan c nan k nan m nan\n",
      "77 Train Loss nan Test RE nan c nan k nan m nan\n",
      "78 Train Loss nan Test RE nan c nan k nan m nan\n",
      "79 Train Loss nan Test RE nan c nan k nan m nan\n",
      "80 Train Loss nan Test RE nan c nan k nan m nan\n",
      "81 Train Loss nan Test RE nan c nan k nan m nan\n",
      "82 Train Loss nan Test RE nan c nan k nan m nan\n",
      "83 Train Loss nan Test RE nan c nan k nan m nan\n",
      "84 Train Loss nan Test RE nan c nan k nan m nan\n",
      "85 Train Loss nan Test RE nan c nan k nan m nan\n",
      "86 Train Loss nan Test RE nan c nan k nan m nan\n",
      "87 Train Loss nan Test RE nan c nan k nan m nan\n",
      "88 Train Loss nan Test RE nan c nan k nan m nan\n",
      "89 Train Loss nan Test RE nan c nan k nan m nan\n",
      "90 Train Loss nan Test RE nan c nan k nan m nan\n",
      "91 Train Loss nan Test RE nan c nan k nan m nan\n",
      "92 Train Loss nan Test RE nan c nan k nan m nan\n",
      "93 Train Loss nan Test RE nan c nan k nan m nan\n",
      "94 Train Loss nan Test RE nan c nan k nan m nan\n",
      "95 Train Loss nan Test RE nan c nan k nan m nan\n",
      "96 Train Loss nan Test RE nan c nan k nan m nan\n",
      "97 Train Loss nan Test RE nan c nan k nan m nan\n",
      "98 Train Loss nan Test RE nan c nan k nan m nan\n",
      "99 Train Loss nan Test RE nan c nan k nan m nan\n",
      "100 Train Loss nan Test RE nan c nan k nan m nan\n",
      "101 Train Loss nan Test RE nan c nan k nan m nan\n",
      "102 Train Loss nan Test RE nan c nan k nan m nan\n",
      "103 Train Loss nan Test RE nan c nan k nan m nan\n",
      "104 Train Loss nan Test RE nan c nan k nan m nan\n",
      "105 Train Loss nan Test RE nan c nan k nan m nan\n",
      "106 Train Loss nan Test RE nan c nan k nan m nan\n",
      "107 Train Loss nan Test RE nan c nan k nan m nan\n",
      "108 Train Loss nan Test RE nan c nan k nan m nan\n",
      "109 Train Loss nan Test RE nan c nan k nan m nan\n",
      "110 Train Loss nan Test RE nan c nan k nan m nan\n",
      "111 Train Loss nan Test RE nan c nan k nan m nan\n",
      "112 Train Loss nan Test RE nan c nan k nan m nan\n",
      "113 Train Loss nan Test RE nan c nan k nan m nan\n",
      "114 Train Loss nan Test RE nan c nan k nan m nan\n",
      "115 Train Loss nan Test RE nan c nan k nan m nan\n",
      "116 Train Loss nan Test RE nan c nan k nan m nan\n",
      "117 Train Loss nan Test RE nan c nan k nan m nan\n",
      "118 Train Loss nan Test RE nan c nan k nan m nan\n",
      "119 Train Loss nan Test RE nan c nan k nan m nan\n",
      "120 Train Loss nan Test RE nan c nan k nan m nan\n",
      "121 Train Loss nan Test RE nan c nan k nan m nan\n",
      "122 Train Loss nan Test RE nan c nan k nan m nan\n",
      "123 Train Loss nan Test RE nan c nan k nan m nan\n",
      "124 Train Loss nan Test RE nan c nan k nan m nan\n",
      "125 Train Loss nan Test RE nan c nan k nan m nan\n",
      "126 Train Loss nan Test RE nan c nan k nan m nan\n",
      "127 Train Loss nan Test RE nan c nan k nan m nan\n",
      "128 Train Loss nan Test RE nan c nan k nan m nan\n",
      "129 Train Loss nan Test RE nan c nan k nan m nan\n",
      "130 Train Loss nan Test RE nan c nan k nan m nan\n",
      "131 Train Loss nan Test RE nan c nan k nan m nan\n",
      "132 Train Loss nan Test RE nan c nan k nan m nan\n",
      "133 Train Loss nan Test RE nan c nan k nan m nan\n",
      "134 Train Loss nan Test RE nan c nan k nan m nan\n",
      "135 Train Loss nan Test RE nan c nan k nan m nan\n",
      "136 Train Loss nan Test RE nan c nan k nan m nan\n",
      "137 Train Loss nan Test RE nan c nan k nan m nan\n",
      "138 Train Loss nan Test RE nan c nan k nan m nan\n",
      "139 Train Loss nan Test RE nan c nan k nan m nan\n",
      "140 Train Loss nan Test RE nan c nan k nan m nan\n",
      "141 Train Loss nan Test RE nan c nan k nan m nan\n",
      "142 Train Loss nan Test RE nan c nan k nan m nan\n",
      "143 Train Loss nan Test RE nan c nan k nan m nan\n",
      "144 Train Loss nan Test RE nan c nan k nan m nan\n",
      "145 Train Loss nan Test RE nan c nan k nan m nan\n",
      "146 Train Loss nan Test RE nan c nan k nan m nan\n",
      "147 Train Loss nan Test RE nan c nan k nan m nan\n",
      "148 Train Loss nan Test RE nan c nan k nan m nan\n",
      "149 Train Loss nan Test RE nan c nan k nan m nan\n",
      "150 Train Loss nan Test RE nan c nan k nan m nan\n",
      "151 Train Loss nan Test RE nan c nan k nan m nan\n",
      "152 Train Loss nan Test RE nan c nan k nan m nan\n",
      "153 Train Loss nan Test RE nan c nan k nan m nan\n",
      "154 Train Loss nan Test RE nan c nan k nan m nan\n",
      "155 Train Loss nan Test RE nan c nan k nan m nan\n",
      "156 Train Loss nan Test RE nan c nan k nan m nan\n",
      "157 Train Loss nan Test RE nan c nan k nan m nan\n",
      "158 Train Loss nan Test RE nan c nan k nan m nan\n",
      "159 Train Loss nan Test RE nan c nan k nan m nan\n",
      "160 Train Loss nan Test RE nan c nan k nan m nan\n",
      "161 Train Loss nan Test RE nan c nan k nan m nan\n",
      "162 Train Loss nan Test RE nan c nan k nan m nan\n",
      "163 Train Loss nan Test RE nan c nan k nan m nan\n",
      "164 Train Loss nan Test RE nan c nan k nan m nan\n",
      "165 Train Loss nan Test RE nan c nan k nan m nan\n",
      "166 Train Loss nan Test RE nan c nan k nan m nan\n",
      "167 Train Loss nan Test RE nan c nan k nan m nan\n",
      "168 Train Loss nan Test RE nan c nan k nan m nan\n",
      "169 Train Loss nan Test RE nan c nan k nan m nan\n",
      "170 Train Loss nan Test RE nan c nan k nan m nan\n",
      "171 Train Loss nan Test RE nan c nan k nan m nan\n",
      "172 Train Loss nan Test RE nan c nan k nan m nan\n",
      "173 Train Loss nan Test RE nan c nan k nan m nan\n",
      "174 Train Loss nan Test RE nan c nan k nan m nan\n",
      "175 Train Loss nan Test RE nan c nan k nan m nan\n",
      "176 Train Loss nan Test RE nan c nan k nan m nan\n",
      "177 Train Loss nan Test RE nan c nan k nan m nan\n",
      "178 Train Loss nan Test RE nan c nan k nan m nan\n",
      "179 Train Loss nan Test RE nan c nan k nan m nan\n",
      "180 Train Loss nan Test RE nan c nan k nan m nan\n",
      "181 Train Loss nan Test RE nan c nan k nan m nan\n",
      "182 Train Loss nan Test RE nan c nan k nan m nan\n",
      "183 Train Loss nan Test RE nan c nan k nan m nan\n",
      "184 Train Loss nan Test RE nan c nan k nan m nan\n",
      "185 Train Loss nan Test RE nan c nan k nan m nan\n",
      "186 Train Loss nan Test RE nan c nan k nan m nan\n",
      "187 Train Loss nan Test RE nan c nan k nan m nan\n",
      "188 Train Loss nan Test RE nan c nan k nan m nan\n",
      "189 Train Loss nan Test RE nan c nan k nan m nan\n",
      "190 Train Loss nan Test RE nan c nan k nan m nan\n",
      "191 Train Loss nan Test RE nan c nan k nan m nan\n",
      "192 Train Loss nan Test RE nan c nan k nan m nan\n",
      "193 Train Loss nan Test RE nan c nan k nan m nan\n",
      "194 Train Loss nan Test RE nan c nan k nan m nan\n",
      "195 Train Loss nan Test RE nan c nan k nan m nan\n",
      "196 Train Loss nan Test RE nan c nan k nan m nan\n",
      "197 Train Loss nan Test RE nan c nan k nan m nan\n",
      "198 Train Loss nan Test RE nan c nan k nan m nan\n",
      "199 Train Loss nan Test RE nan c nan k nan m nan\n",
      "Training time: 71.86\n",
      "Training time: 71.86\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 10000.051 Test RE 0.21929946433252137 c 0.0012197602 k 0.00016411471 m -3.8845197e-08\n",
      "1 Train Loss 10000.051 Test RE 0.2189589606042668 c 0.0020254047 k 0.00022528411 m -1.2397047e-07\n",
      "2 Train Loss 10000.051 Test RE 0.2187535387132468 c 0.0029388354 k 0.00072235067 m -3.0354278e-07\n",
      "3 Train Loss 10000.05 Test RE 0.21855964518088544 c 0.0049548866 k 0.0016703983 m -8.7666876e-07\n",
      "4 Train Loss 10000.05 Test RE 0.21852255151720498 c 0.0060776323 k 0.0025615953 m -1.3307811e-06\n",
      "5 Train Loss 10000.05 Test RE 0.21850554854151416 c 0.00776014 k 0.0043521672 m -2.1300325e-06\n",
      "6 Train Loss 10000.05 Test RE 0.2186089314682377 c 0.011861812 k 0.009266673 m -4.223376e-06\n",
      "7 Train Loss 10000.049 Test RE 0.23694346260273785 c 0.066439144 k 0.077295 m -3.2795877e-05\n",
      "8 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "9 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "10 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "11 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "12 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "13 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "14 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "15 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "16 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "17 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "18 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "19 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "20 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "21 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "22 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "23 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "24 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "25 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "26 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "27 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "28 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "29 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "30 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "31 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "32 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "33 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "34 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "35 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "36 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "37 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "38 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "39 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "40 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "41 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "42 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "43 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "44 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "45 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "46 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "47 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "48 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "49 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "50 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "51 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "52 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "53 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "54 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "55 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "56 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "57 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "58 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "59 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "60 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "61 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "62 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "63 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "64 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "65 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "66 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "67 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "68 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "69 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "70 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "71 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "72 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "73 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "74 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "75 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "76 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "77 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "78 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "79 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "80 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "81 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "82 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "83 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "84 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "85 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "86 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "87 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "88 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "89 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "90 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "91 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "92 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "93 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "94 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "95 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "96 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "97 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "98 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "99 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "100 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "101 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "102 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "103 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "104 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "105 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "106 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "107 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "108 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "109 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "110 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "111 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "112 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "113 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "114 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "115 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "116 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "117 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "118 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "119 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "120 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "121 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "122 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "123 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "124 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "125 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "126 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "127 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "128 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "129 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "130 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "131 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "132 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "133 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "134 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "135 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "136 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "137 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "138 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "139 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "140 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "141 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "142 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "143 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "144 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "145 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "146 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "147 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "148 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "149 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "150 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "151 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "152 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "153 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "154 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "155 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "156 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "157 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "158 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "159 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "160 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "161 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "162 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "163 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "164 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "165 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "166 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "167 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "168 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "169 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "170 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "171 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "172 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "173 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "174 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "175 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "176 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "177 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "178 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "179 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "180 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "181 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "182 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "183 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "184 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "185 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "186 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "187 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "188 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "189 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "190 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "191 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "192 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "193 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "194 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "195 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "196 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "197 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "198 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "199 Train Loss 2046.7698 Test RE 43.10502964193127 c 159.00328 k 200.87712 m -0.08396851\n",
      "Training time: 31.23\n",
      "Training time: 31.23\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 10000.051 Test RE 0.2195397516892676 c 0.003808536 k 0.0011367574 m -4.4900122e-07\n",
      "1 Train Loss 10000.051 Test RE 0.2191020861290155 c 0.004491582 k 0.0015063263 m -6.599321e-07\n",
      "2 Train Loss 10000.051 Test RE 0.21884414083612067 c 0.0051787714 k 0.0022805065 m -9.4874315e-07\n",
      "3 Train Loss 10000.051 Test RE 0.21870824880253387 c 0.0058780774 k 0.0030300056 m -1.2834273e-06\n",
      "4 Train Loss 10000.05 Test RE 0.2186006183092709 c 0.0075160675 k 0.0058743875 m -2.304182e-06\n",
      "5 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "6 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "7 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "8 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "9 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "10 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "11 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "12 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "13 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "14 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "15 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "16 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "17 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "18 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "19 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "20 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "21 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "22 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "23 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "24 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "25 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "26 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "27 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "28 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "29 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "30 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "31 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "32 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "33 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "34 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "35 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "36 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "37 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "38 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "39 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "40 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "41 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "42 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "43 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "44 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "45 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "46 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "47 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "48 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "49 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "50 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "51 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "52 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "53 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "54 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "55 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "56 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "57 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "58 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "59 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "60 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "61 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "62 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "63 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "64 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "65 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "66 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "67 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "68 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "69 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "70 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "71 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "72 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "73 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "74 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "75 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "76 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "77 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "78 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "79 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "80 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "81 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "82 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "83 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "84 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "85 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "86 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "87 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "88 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "89 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "90 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "91 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "92 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "93 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "94 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "95 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "96 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "97 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "98 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "99 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "100 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "101 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "102 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "103 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "104 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "105 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "106 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "107 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "108 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "109 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "110 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "111 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "112 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "113 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "114 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "115 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "116 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "117 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "118 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "119 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "120 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "121 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "122 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "123 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "124 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "125 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "126 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "127 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "128 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "129 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "130 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "131 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "132 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "133 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "134 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "135 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "136 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "137 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "138 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "139 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "140 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "141 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "142 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "143 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "144 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "145 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "146 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "147 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "148 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "149 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "150 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "151 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "152 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "153 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "154 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "155 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "156 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "157 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "158 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "159 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "160 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "161 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "162 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "163 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "164 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "165 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "166 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "167 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "168 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "169 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "170 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "171 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "172 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "173 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "174 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "175 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "176 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "177 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "178 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "179 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "180 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "181 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "182 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "183 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "184 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "185 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "186 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "187 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "188 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "189 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "190 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "191 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "192 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "193 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "194 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "195 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "196 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "197 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "198 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "199 Train Loss 10000.05 Test RE 0.21864834241764572 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "Training time: 34.00\n",
      "Training time: 34.00\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 200\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "m_full = []\n",
    "k_full = []\n",
    "c_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "\n",
    "for reps in range(max_reps):   \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []   \n",
    "    beta_val = []\n",
    "    m_val = []\n",
    "    k_val = []\n",
    "    c_val = []\n",
    "\n",
    "    torch.manual_seed(reps*36)\n",
    "    N_f = 10000 #Total number of collocation points\n",
    "\n",
    "    layers = np.array([1,50,50,50,1]) #9 hidden layers\n",
    "    PINN = Sequentialmodel(layers)\n",
    "\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.25, \n",
    "                                max_iter = 10, \n",
    "                                max_eval = 15, \n",
    "                                tolerance_grad = 1e-6, \n",
    "                                tolerance_change = 1e-6, \n",
    "                                history_size = 100, \n",
    "                                line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "    train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    beta_full.append(beta_val)\n",
    "    m_full.append(m_val)\n",
    "    k_full.append(k_val)\n",
    "    c_full.append(c_val)\n",
    "    \n",
    "\n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"beta\": beta_full,\"m\": m_full,\"k\": k_full,\"c\": c_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "pmHEeBpzfFQh",
    "outputId": "990b2054-f35e-4c8e-c378-69ff2eb19f4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f774c234250>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_pred = PINN.test()\n",
    "plt.plot(t,(x_true-x_mean)/x_std,'b')\n",
    "plt.plot(t,x_pred,'r--')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "stan_1D_FODE_tune.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
