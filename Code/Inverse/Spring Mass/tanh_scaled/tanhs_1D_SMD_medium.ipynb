{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8byrnUmNKGR",
    "outputId": "510f67c6-b225-41ba-e78a-d0a6e54ded85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uoNYpDzlNKGV"
   },
   "outputs": [],
   "source": [
    "tstart= 0 \n",
    "tstop= 60 \n",
    "\n",
    "increment = 0.1 \n",
    "# Initial condition\n",
    "x_init= [0,0] \n",
    "\n",
    "t = np.arange(tstart,tstop+1,increment)\n",
    "\n",
    "c = 1 # Damping constant \n",
    "k = 0.05 # Stiffness of the spring\n",
    "m = 5 # Mass \n",
    "F0 = 100\n",
    "\n",
    "def mydiff(x, t):    \n",
    "    F =F0\n",
    "    \n",
    "    dx1dt = x[1] \n",
    "    dx2dt = (F -c*x[1] -k*x[0])/m\n",
    "    dxdt= [dx1dt, dx2dt] \n",
    "    \n",
    "    return dxdt \n",
    "\n",
    "x_full_sol = odeint(mydiff, x_init, t) \n",
    "\n",
    "x_sol = x_full_sol[:,0]\n",
    "v_sol = x_full_sol[:,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BR02v-fkNKGV"
   },
   "outputs": [],
   "source": [
    "level = \"medium\"\n",
    "label = \"1D_SMD_tanhs_\" + level\n",
    "\n",
    "loss_thresh = 0.005\n",
    "\n",
    "t_test = t.reshape(-1,1)\n",
    "t_test_tensor = torch.from_numpy(t_test).float().to(device)\n",
    "\n",
    "# bc1_t = t[0].reshape(-1,1)\n",
    "# bc1_x = x_sol[0].reshape(-1,1)\n",
    "# t_bc1_train = torch.from_numpy(bc1_t).float().to(device)\n",
    "# x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "x_mean = np.mean(x_sol.reshape(-1,1))\n",
    "x_std  = np.mean(x_sol.reshape(-1,1))                \n",
    "bc1_t = t.reshape(-1,1)\n",
    "bc1_x = (x_sol-x_mean).reshape(-1,1)/x_std\n",
    "t_bc1_train = torch.from_numpy(bc1_t).float().to(device)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "\n",
    "\n",
    "bc2_t = t[0].reshape(-1,1)\n",
    "t_bc2_train = torch.from_numpy(bc2_t).float().to(device)\n",
    "bc2_val = v_sol[0].reshape(-1,1)\n",
    "bc2_val =torch.from_numpy(bc2_val).float().to(device)\n",
    "\n",
    "t_test = t.reshape(-1,1)\n",
    "t_test_tensor = torch.from_numpy(t_test).float().to(device)\n",
    "x_true = x_sol\n",
    "x_true_norm = np.linalg.norm(x_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(t[0]) \n",
    "ub = np.array(t[-1]) \n",
    "\n",
    "#torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SyyktBKBXRo1"
   },
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    \n",
    "    t01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=t01,random_state =seed)\n",
    "\n",
    "    t_coll = lb + (ub-lb)*sampling(N_f)\n",
    "    # t_coll = np.vstack((t_coll,)) # append training points to collocation points \n",
    "\n",
    "    return t_coll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "o1b21zLnNKGW"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "        \n",
    "        \n",
    "        self.k = Parameter(torch.tensor(0.0))\n",
    "        self.k.requiresGrad = True\n",
    "        self.c = Parameter(torch.tensor(0.0))\n",
    "        self.c.requiresGrad = True\n",
    "        self.m = Parameter(torch.tensor(0.0))\n",
    "        self.m.requiresGrad = True\n",
    "        \n",
    "    'forward pass'\n",
    "    def forward(self,t):\n",
    "        if torch.is_tensor(t) != True:         \n",
    "            t = torch.from_numpy(t)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        t = 2.0*(t - l_b)/(u_b - l_b) - 1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = t.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,t,x):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(t), x)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,t_bc2,bc2_val):\n",
    "        g = t_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        x = self.forward(g)    \n",
    "            \n",
    "        x_t = autograd.grad(x,g,torch.ones([t_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dx_dt = x_t[:,[0]]\n",
    "        \n",
    "        bc2 = dx_dt/x_std\n",
    "        \n",
    "        loss_bc2= self.loss_function(bc2,bc2_val)\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self,t_coll,f_hat):\n",
    "             \n",
    "        g = t_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        x = self.forward(g) \n",
    "\n",
    "        x_t = autograd.grad(x,g,torch.ones([t_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        x_tt = autograd.grad(x_t,g,torch.ones(t_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dx_dt = x_t[:,[0]]\n",
    "        \n",
    "        dx2_d2t = x_tt[:,[0]]\n",
    "        \n",
    "        f = self.m*dx2_d2t/x_std + self.c*dx_dt/x_std + self.k*x/x_std - F0\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,t_bc1,x_bc1,t_bc2,bc2_val,t_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(t_bc1,x_bc1)\n",
    "        loss_bc2 = self.loss_BC2(t_bc2,bc2_val)\n",
    "        loss_f = self.loss_PDE(t_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self):\n",
    "        x_pred = self.forward(t_test_tensor)\n",
    "        x_pred = x_pred.cpu().detach().numpy()\n",
    "\n",
    "        x_pred = x_pred*x_std + x_mean\n",
    "        \n",
    "        return x_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        x_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(x_pred.reshape(-1,1) - x_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(x_pred.reshape(-1,1) - x_true.reshape(-1,1),2)/x_true_norm\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fLY2mT5BOgjD"
   },
   "outputs": [],
   "source": [
    "def train_step(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8srA5uGuObil"
   },
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    m_val.append(PINN.m.cpu().detach().numpy())\n",
    "    k_val.append(PINN.k.cpu().detach().numpy())\n",
    "    c_val.append(PINN.c.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0SezTZ_racQB"
   },
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "\n",
    "    t_coll = colloc_pts(N_f,rep*11)\n",
    "    t_coll =  torch.from_numpy(t_coll).float().to(device)\n",
    "    \n",
    "    f_hat = torch.zeros(t_coll.shape[0],1).to(device)\n",
    "    for i in range(max_iter):      \n",
    "        train_step(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat)\n",
    "         \n",
    "        loss_np = PINN.loss(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test RE\",test_re_loss[-1],\"c\",PINN.c.cpu().detach().numpy(),\"k\",PINN.k.cpu().detach().numpy(),\"m\",PINN.m.cpu().detach().numpy())\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I9BYbcJ0NKGX",
    "outputId": "42720d97-b37a-4c42-b7e5-b77a0ef391f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 10000.027 Test RE 0.1485201824262051 c 0.0022491494 k 0.00044633343 m -3.831647e-07\n",
      "1 Train Loss 10000.026 Test RE 0.14651716114014562 c 0.0031223907 k 0.0006254198 m -6.694264e-07\n",
      "2 Train Loss 10000.026 Test RE 0.14598526543711485 c 0.0036249445 k 0.0007305833 m -8.6533123e-07\n",
      "3 Train Loss 10000.025 Test RE 0.1452049041707115 c 0.004914134 k 0.0009977709 m -1.4511784e-06\n",
      "4 Train Loss 10000.025 Test RE 0.144827287386564 c 0.005831625 k 0.0011878194 m -1.9239242e-06\n",
      "5 Train Loss 10000.025 Test RE 0.14438850463954944 c 0.007087622 k 0.0014511476 m -2.6241182e-06\n",
      "6 Train Loss 10000.025 Test RE 0.1438282337677524 c 0.008883633 k 0.0018263429 m -3.7001628e-06\n",
      "7 Train Loss 10000.025 Test RE 0.1430582212106689 c 0.011546083 k 0.0023868552 m -5.406614e-06\n",
      "8 Train Loss 10000.024 Test RE 0.1401705895528467 c 0.02220646 k 0.00464121 m -1.2941761e-05\n",
      "9 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "10 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "11 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "12 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "13 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "14 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "15 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "16 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "17 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "18 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "19 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "20 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "21 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "22 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "23 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "24 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "25 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "26 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "27 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "28 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "29 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "30 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "31 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "32 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "33 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "34 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "35 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "36 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "37 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "38 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "39 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "40 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "41 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "42 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "43 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "44 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "45 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "46 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "47 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "48 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "49 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "50 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "51 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "52 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "53 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "54 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "55 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "56 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "57 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "58 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "59 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "60 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "61 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "62 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "63 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "64 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "65 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "66 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "67 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "68 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "69 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "70 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "71 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "72 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "73 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "74 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "75 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "76 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "77 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "78 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "79 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "80 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "81 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "82 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "83 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "84 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "85 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "86 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "87 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "88 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "89 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "90 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "91 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "92 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "93 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "94 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "95 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "96 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "97 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "98 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "99 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "100 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "101 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "102 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "103 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "104 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "105 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "106 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "107 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "108 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "109 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "110 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "111 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "112 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "113 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "114 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "115 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "116 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "117 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "118 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "119 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "120 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "121 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "122 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "123 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "124 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "125 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "126 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "127 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "128 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "129 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "130 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "131 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "132 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "133 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "134 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "135 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "136 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "137 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "138 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "139 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "140 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "141 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "142 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "143 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "144 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "145 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "146 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "147 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "148 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "149 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "150 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "151 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "152 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "153 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "154 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "155 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "156 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "157 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "158 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "159 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "160 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "161 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "162 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "163 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "164 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "165 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "166 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "167 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "168 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "169 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "170 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "171 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "172 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "173 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "174 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "175 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "176 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "177 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "178 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "179 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "180 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "181 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "182 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "183 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "184 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "185 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "186 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "187 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "188 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "189 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "190 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "191 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "192 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "193 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "194 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "195 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "196 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "197 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "198 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "199 Train Loss 10000.005 Test RE 0.07559244488924616 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "Training time: 18.32\n",
      "Training time: 18.32\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 10000.026 Test RE 0.14638280403855736 c 0.0021821717 k 0.0007513225 m -1.2242381e-06\n",
      "1 Train Loss 10000.025 Test RE 0.14327660423439675 c 0.0030996555 k 0.0010687812 m -2.197315e-06\n",
      "2 Train Loss 10000.024 Test RE 0.14061764943346342 c 0.0045430437 k 0.0015711836 m -4.0831237e-06\n",
      "3 Train Loss 10000.003 Test RE 0.051992204385552014 c 0.06680589 k 0.023053922 m -0.00013363\n",
      "4 Train Loss 10000.003 Test RE 0.051373783148252396 c 0.06768788 k 0.02326682 m -0.00014086376\n",
      "5 Train Loss 10000.003 Test RE 0.0502126534913082 c 0.06929988 k 0.023635924 m -0.00015651643\n",
      "6 Train Loss 10000.003 Test RE 0.04845121746035712 c 0.07169427 k 0.024156123 m -0.00018296398\n",
      "7 Train Loss 10000.003 Test RE 0.04599688580604904 c 0.074929036 k 0.024821538 m -0.00022282488\n",
      "8 Train Loss 10000.002 Test RE 0.03887203246995825 c 0.08346961 k 0.026394766 m -0.0003478871\n",
      "9 Train Loss 10000.001 Test RE 0.028857297449767793 c 0.09364739 k 0.02764984 m -0.00057622226\n",
      "10 Train Loss 10000.001 Test RE 0.024681177177598677 c 0.09799602 k 0.02794501 m -0.00070729264\n",
      "11 Train Loss 10000.001 Test RE 0.022201536759988216 c 0.1011175 k 0.028135872 m -0.00080686127\n",
      "12 Train Loss 10000.001 Test RE 0.020737899104394796 c 0.10335698 k 0.028238967 m -0.00088378653\n",
      "13 Train Loss 10000.0 Test RE 0.019260116886037324 c 0.10655432 k 0.028347543 m -0.0010013026\n",
      "14 Train Loss 10000.0 Test RE 0.01886408402140002 c 0.10782863 k 0.028375586 m -0.0010510349\n",
      "15 Train Loss 10000.0 Test RE 0.018568480296809835 c 0.10903708 k 0.028394682 m -0.0010996052\n",
      "16 Train Loss 10000.0 Test RE 0.018322238749325256 c 0.11028605 k 0.028409377 m -0.001150824\n",
      "17 Train Loss 10000.0 Test RE 0.01809111732307502 c 0.11169291 k 0.028421601 m -0.0012093799\n",
      "18 Train Loss 10000.0 Test RE 0.017846870771454935 c 0.11341504 k 0.028433518 m -0.0012817141\n",
      "19 Train Loss 10000.0 Test RE 0.01756061846278131 c 0.11567794 k 0.028446943 m -0.0013772729\n",
      "20 Train Loss 10000.0 Test RE 0.01719472333440162 c 0.11883502 k 0.028464967 m -0.0015108569\n",
      "21 Train Loss 10000.0 Test RE 0.01669082968048352 c 0.123488165 k 0.028493583 m -0.001707565\n",
      "22 Train Loss 10000.0 Test RE 0.015950066423797704 c 0.13075496 k 0.028547697 m -0.002013438\n",
      "23 Train Loss 10000.0 Test RE 0.01488095598274019 c 0.14247732 k 0.028667519 m -0.0025019927\n",
      "24 Train Loss 10000.0 Test RE 0.014034244896405323 c 0.1572061 k 0.028921638 m -0.0030990802\n",
      "25 Train Loss 10000.0 Test RE 0.01380117005350153 c 0.16564044 k 0.029273909 m -0.0034102106\n",
      "26 Train Loss 10000.0 Test RE 0.013562224991057371 c 0.17182757 k 0.029668769 m -0.00361877\n",
      "27 Train Loss 10000.0 Test RE 0.013308972891148737 c 0.17721002 k 0.0300968 m -0.0037870277\n",
      "28 Train Loss 10000.0 Test RE 0.013058147550699986 c 0.1818464 k 0.030540204 m -0.0039205155\n",
      "29 Train Loss 10000.0 Test RE 0.01285809738240344 c 0.18574566 k 0.030966606 m -0.004024608\n",
      "30 Train Loss 10000.0 Test RE 0.012706523096792172 c 0.1891094 k 0.031380974 m -0.0041076364\n",
      "31 Train Loss 10000.0 Test RE 0.012615287019396721 c 0.19224107 k 0.0317946 m -0.004181109\n",
      "32 Train Loss 10000.0 Test RE 0.012577982750145803 c 0.19535913 k 0.032226548 m -0.004251825\n",
      "33 Train Loss 10000.0 Test RE 0.012609478795873375 c 0.19878533 k 0.032700542 m -0.0043301755\n",
      "34 Train Loss 10000.0 Test RE 0.01271073039151453 c 0.20283858 k 0.03325571 m -0.004424358\n",
      "35 Train Loss 10000.0 Test RE 0.012916038428629165 c 0.20811787 k 0.033953954 m -0.0045512323\n",
      "36 Train Loss 9999.998 Test RE 0.013863933460924368 c 0.22746062 k 0.03640178 m -0.005034476\n",
      "37 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "38 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "39 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "40 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "41 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "42 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "43 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "44 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "45 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "46 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "47 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "48 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "49 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "50 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "51 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "52 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "53 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "54 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "55 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "56 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "57 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "58 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "59 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "60 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "61 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "62 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "63 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "64 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "65 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "66 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "67 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "68 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "69 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "70 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "71 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "72 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "73 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "74 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "75 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "76 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "77 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "78 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "79 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "80 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "81 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "82 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "83 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "84 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "85 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "86 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "87 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "88 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "89 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "90 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "91 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "92 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "93 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "94 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "95 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "96 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "97 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "98 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "99 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "100 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "101 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "102 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "103 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "104 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "105 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "106 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "107 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "108 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "109 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "110 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "111 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "112 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "113 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "114 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "115 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "116 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "117 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "118 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "119 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "120 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "121 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "122 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "123 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "124 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "125 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "126 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "127 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "128 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "129 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "130 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "131 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "132 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "133 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "134 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "135 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "136 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "137 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "138 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "139 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "140 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "141 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "142 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "143 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "144 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "145 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "146 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "147 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "148 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "149 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "150 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "151 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "152 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "153 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "154 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "155 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "156 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "157 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "158 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "159 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "160 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "161 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "162 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "163 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "164 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "165 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "166 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "167 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "168 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "169 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "170 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "171 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "172 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "173 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "174 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "175 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "176 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "177 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "178 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "179 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "180 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "181 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "182 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "183 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "184 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "185 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "186 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "187 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "188 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "189 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "190 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "191 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "192 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "193 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "194 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "195 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "196 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "197 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "198 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "199 Train Loss 9999.998 Test RE 0.015012435991787256 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "Training time: 28.50\n",
      "Training time: 28.50\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 10000.026 Test RE 0.14733516823350498 c 0.0025764382 k 0.00030880206 m -2.5905337e-07\n",
      "1 Train Loss 10000.026 Test RE 0.14682489032540597 c 0.0029322735 k 0.000348664 m -3.292503e-07\n",
      "2 Train Loss 10000.026 Test RE 0.14650370538644186 c 0.0033116892 k 0.00040122133 m -4.1443553e-07\n",
      "3 Train Loss 10000.026 Test RE 0.1462746569350045 c 0.0037390825 k 0.00045537358 m -5.202305e-07\n",
      "4 Train Loss 10000.026 Test RE 0.14608527337026736 c 0.00424962 k 0.0005232223 m -6.593633e-07\n",
      "5 Train Loss 10000.026 Test RE 0.14590383537987825 c 0.004897288 k 0.0006077856 m -8.512737e-07\n",
      "6 Train Loss 10000.026 Test RE 0.14570487580958705 c 0.005767675 k 0.0007227842 m -1.130079e-06\n",
      "7 Train Loss 10000.025 Test RE 0.14514266626209293 c 0.008775503 k 0.0011204364 m -2.206976e-06\n",
      "8 Train Loss 10000.025 Test RE 0.1446969129512629 c 0.011439493 k 0.0014738782 m -3.2492067e-06\n",
      "9 Train Loss 10000.025 Test RE 0.1440446686627074 c 0.015524862 k 0.0020177541 m -4.942881e-06\n",
      "10 Train Loss 10000.025 Test RE 0.1430344009072297 c 0.022041371 k 0.0028881263 m -7.792206e-06\n",
      "11 Train Loss 10000.01 Test RE 0.09632590993446094 c 0.2545293 k 0.03421488 m -0.000118884425\n",
      "12 Train Loss 10000.009 Test RE 0.0949124583425006 c 0.2605435 k 0.035045102 m -0.00012426065\n",
      "13 Train Loss 10000.009 Test RE 0.09438574422639205 c 0.26246795 k 0.035315197 m -0.00012696476\n",
      "14 Train Loss 10000.009 Test RE 0.09390321907623603 c 0.26431185 k 0.03557474 m -0.00013011656\n",
      "15 Train Loss 10000.009 Test RE 0.09341601481161274 c 0.2663961 k 0.03586736 m -0.00013410294\n",
      "16 Train Loss 10000.009 Test RE 0.0928764262946833 c 0.26901802 k 0.036233783 m -0.00013943968\n",
      "17 Train Loss 10000.009 Test RE 0.09223304385467314 c 0.27251017 k 0.036719717 m -0.00014683668\n",
      "18 Train Loss 10000.008 Test RE 0.09036941674867942 c 0.28387815 k 0.038294327 m -0.00017211509\n",
      "19 Train Loss 10000.008 Test RE 0.08895041977462002 c 0.29304144 k 0.03956097 m -0.00019334863\n",
      "20 Train Loss 10000.007 Test RE 0.0841160847385329 c 0.3242936 k 0.043883715 m -0.00026951742\n",
      "21 Train Loss 10000.0 Test RE 0.03749998483854451 c 0.592207 k 0.08158547 m -0.0010768967\n",
      "22 Train Loss 9999.998 Test RE 0.037326636010343604 c 0.6132695 k 0.08459915 m -0.0011583954\n",
      "23 Train Loss 9999.998 Test RE 0.03725462193139463 c 0.6217331 k 0.08582721 m -0.0011996111\n",
      "24 Train Loss 9999.998 Test RE 0.03712941880170975 c 0.630835 k 0.087226726 m -0.0012743489\n",
      "25 Train Loss 9999.998 Test RE 0.037100052325531284 c 0.64037865 k 0.08873598 m -0.0013678849\n",
      "26 Train Loss 9999.998 Test RE 0.03716383913339789 c 0.64952606 k 0.09020965 m -0.0014670868\n",
      "27 Train Loss 9999.998 Test RE 0.03729664796701389 c 0.6584892 k 0.09167659 m -0.0015721755\n",
      "28 Train Loss 9999.998 Test RE 0.03748809407759887 c 0.6679566 k 0.09324805 m -0.0016905303\n",
      "29 Train Loss 9999.998 Test RE 0.03774679213045335 c 0.6789012 k 0.09508774 m -0.0018348478\n",
      "30 Train Loss 9999.998 Test RE 0.03810011119981632 c 0.6926904 k 0.097431056 m -0.0020246971\n",
      "31 Train Loss 9999.998 Test RE 0.038596717158568446 c 0.7113046 k 0.10062302 m -0.0022896226\n",
      "32 Train Loss 9999.998 Test RE 0.039312578107865065 c 0.73765105 k 0.10517216 m -0.0026734103\n",
      "33 Train Loss 9999.998 Test RE 0.04036039533701381 c 0.7759188 k 0.1118097 m -0.0032382759\n",
      "34 Train Loss 9999.998 Test RE 0.04188790358229317 c 0.8312627 k 0.1214277 m -0.004057491\n",
      "35 Train Loss 9999.998 Test RE 0.043980867257337845 c 0.90525234 k 0.13427554 m -0.0051437253\n",
      "36 Train Loss 9999.998 Test RE 0.046335467160003786 c 0.9852254 k 0.14811657 m -0.0062958715\n",
      "37 Train Loss 9999.998 Test RE 0.048557536399633824 c 1.0603809 k 0.16106652 m -0.0073518734\n",
      "38 Train Loss 9999.998 Test RE 0.05088191927167165 c 1.1430383 k 0.17526956 m -0.008489253\n",
      "39 Train Loss 9999.998 Test RE 0.0533170900747818 c 1.2377974 k 0.19155458 m -0.009777608\n",
      "40 Train Loss 9999.997 Test RE 0.05724931395963568 c 1.4505179 k 0.2282271 m -0.012639527\n",
      "41 Train Loss 9999.996 Test RE 0.05783807057506776 c 1.7183415 k 0.2748436 m -0.016241577\n",
      "42 Train Loss 9999.973 Test RE 0.07757630774882374 c 6.073027 k 1.0603175 m -0.077296965\n",
      "43 Train Loss 9999.92 Test RE 0.2087985604372037 c 14.149234 k 2.5012617 m -0.1877582\n",
      "44 Train Loss 9998.9375 Test RE 0.6134948259602141 c 71.283035 k 12.99299 m -0.99197835\n",
      "45 Train Loss 9991.567 Test RE 3.3726489203190284 c 184.99852 k 33.838737 m -2.564899\n",
      "46 Train Loss 9969.296 Test RE 6.005016286237285 c 379.25314 k 69.22024 m -5.1378765\n",
      "47 Train Loss 9870.155 Test RE 10.043799773135087 c 665.1591 k 121.19991 m -8.895419\n",
      "48 Train Loss 9843.146 Test RE 12.553231593125659 c 804.97107 k 146.50903 m -10.716614\n",
      "49 Train Loss 9798.74 Test RE 13.228616680940334 c 897.9265 k 163.3604 m -11.921343\n",
      "50 Train Loss 9785.313 Test RE 13.053485693468577 c 898.3691 k 163.45268 m -11.906228\n",
      "51 Train Loss 9773.686 Test RE 13.546374387865237 c 957.9779 k 174.16487 m -12.640373\n",
      "52 Train Loss 9750.253 Test RE 15.713863840460238 c 1041.4142 k 188.74017 m -13.594074\n",
      "53 Train Loss 9651.824 Test RE 16.375388855490993 c 1260.0424 k 228.04106 m -16.340601\n",
      "54 Train Loss 9629.321 Test RE 18.57666093602174 c 1380.2346 k 249.35033 m -17.741514\n",
      "55 Train Loss 9590.453 Test RE 18.453308313018724 c 1406.9846 k 253.47253 m -17.842121\n",
      "56 Train Loss 9542.022 Test RE 18.838832480261853 c 1416.2761 k 254.47783 m -17.712149\n",
      "57 Train Loss 9530.548 Test RE 19.745824758763646 c 1478.5189 k 265.5292 m -18.455376\n",
      "58 Train Loss 9422.247 Test RE 21.57026677768402 c 1589.4761 k 285.39597 m -19.757715\n",
      "59 Train Loss 9284.611 Test RE 24.627503249679105 c 2063.8467 k 370.83725 m -25.728607\n",
      "60 Train Loss 9246.188 Test RE 24.46830291463341 c 2142.7793 k 384.77106 m -26.643806\n",
      "61 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "62 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "63 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "64 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "65 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "66 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "67 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "68 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "69 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "70 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "71 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "72 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "73 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "74 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "75 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "76 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "77 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "78 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "79 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "80 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "81 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "82 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "83 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "84 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "85 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "86 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "87 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "88 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "89 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "90 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "91 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "92 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "93 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "94 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "95 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "96 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "97 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "98 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "99 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "100 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "101 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "102 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "103 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "104 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "105 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "106 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "107 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "108 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "109 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "110 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "111 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "112 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "113 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "114 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "115 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "116 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "117 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "118 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "119 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "120 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "121 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "122 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "123 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "124 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "125 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "126 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "127 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "128 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "129 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "130 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "131 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "132 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "133 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "134 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "135 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "136 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "137 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "138 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "139 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "140 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "141 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "142 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "143 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "144 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "145 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "146 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "147 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "148 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "149 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "150 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "151 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "152 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "153 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "154 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "155 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "156 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "157 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "158 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "159 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "160 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "161 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "162 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "163 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "164 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "165 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "166 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "167 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "168 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "169 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "170 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "171 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "172 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "173 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "174 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "175 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "176 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "177 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "178 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "179 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "180 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "181 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "182 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "183 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "184 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "185 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "186 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "187 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "188 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "189 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "190 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "191 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "192 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "193 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "194 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "195 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "196 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "197 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "198 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "199 Train Loss 9242.254 Test RE 24.319688930243586 c 2131.721 k 382.73087 m -26.489187\n",
      "Training time: 19.19\n",
      "Training time: 19.19\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 10000.026 Test RE 0.14693000799619055 c 0.0029454017 k 0.00039679636 m -3.720341e-07\n",
      "1 Train Loss 10000.026 Test RE 0.146473784445734 c 0.0033641912 k 0.0004515926 m -4.7149769e-07\n",
      "2 Train Loss 10000.026 Test RE 0.1461687504624805 c 0.003826286 k 0.0005205118 m -5.950999e-07\n",
      "3 Train Loss 10000.026 Test RE 0.1459358035194793 c 0.004363183 k 0.0005964448 m -7.5301034e-07\n",
      "4 Train Loss 10000.026 Test RE 0.14572930841025472 c 0.005024157 k 0.00069271435 m -9.659633e-07\n",
      "5 Train Loss 10000.025 Test RE 0.14527426710546007 c 0.0070733326 k 0.0009896846 m -1.7142384e-06\n",
      "6 Train Loss 10000.025 Test RE 0.14496496048454802 c 0.008772293 k 0.0012366679 m -2.4001347e-06\n",
      "7 Train Loss 10000.025 Test RE 0.14454554026719388 c 0.011278286 k 0.0016022961 m -3.4795312e-06\n",
      "8 Train Loss 10000.025 Test RE 0.14394724263967634 c 0.015065336 k 0.0021560339 m -5.212182e-06\n",
      "9 Train Loss 10000.025 Test RE 0.14304978253798284 c 0.020964669 k 0.003021417 m -8.06613e-06\n",
      "10 Train Loss 10000.006 Test RE 0.07820518523869902 c 0.30928153 k 0.045512553 m -0.00016385065\n",
      "11 Train Loss 10000.005 Test RE 0.07710927608418026 c 0.32798856 k 0.048232205 m -0.00017966235\n",
      "12 Train Loss 9999.999 Test RE 0.028633851707030707 c 0.5025203 k 0.07498308 m -0.0013983565\n",
      "13 Train Loss 9999.999 Test RE 0.026406822214456758 c 0.51692516 k 0.07734435 m -0.0015639908\n",
      "14 Train Loss 9999.999 Test RE 0.025543503298824197 c 0.52905154 k 0.079381295 m -0.0017159043\n",
      "15 Train Loss 9999.999 Test RE 0.025088307183775173 c 0.54018235 k 0.081286505 m -0.0018648716\n",
      "16 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "17 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "18 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "19 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "20 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "21 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "22 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "23 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "24 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "25 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "26 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "27 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "28 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "29 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "30 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "31 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "32 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "33 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "34 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "35 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "36 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "37 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "38 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "39 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "40 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "41 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "42 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "43 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "44 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "45 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "46 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "47 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "48 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "49 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "50 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "51 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "52 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "53 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "54 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "55 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "56 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "57 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "58 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "59 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "60 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "61 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "62 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "63 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "64 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "65 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "66 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "67 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "68 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "69 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "70 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "71 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "72 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "73 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "74 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "75 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "76 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "77 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "78 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "79 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "80 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "81 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "82 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "83 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "84 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "85 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "86 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "87 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "88 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "89 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "90 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "91 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "92 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "93 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "94 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "95 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "96 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "97 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "98 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "99 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "100 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "101 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "102 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "103 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "104 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "105 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "106 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "107 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "108 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "109 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "110 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "111 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "112 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "113 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "114 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "115 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "116 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "117 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "118 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "119 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "120 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "121 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "122 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "123 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "124 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "125 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "126 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "127 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "128 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "129 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "130 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "131 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "132 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "133 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "134 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "135 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "136 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "137 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "138 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "139 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "140 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "141 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "142 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "143 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "144 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "145 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "146 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "147 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "148 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "149 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "150 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "151 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "152 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "153 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "154 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "155 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "156 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "157 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "158 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "159 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "160 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "161 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "162 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "163 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "164 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "165 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "166 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "167 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "168 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "169 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "170 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "171 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "172 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "173 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "174 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "175 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "176 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "177 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "178 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "179 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "180 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "181 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "182 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "183 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "184 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "185 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "186 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "187 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "188 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "189 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "190 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "191 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "192 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "193 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "194 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "195 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "196 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "197 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "198 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "199 Train Loss 9999.998 Test RE 0.0252329656025162 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "Training time: 24.37\n",
      "Training time: 24.37\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 10000.026 Test RE 0.14686851348546381 c 0.0021138075 k 0.00037796583 m -3.7742137e-07\n",
      "1 Train Loss 10000.026 Test RE 0.14627028451946553 c 0.0024471167 k 0.0004399487 m -4.863561e-07\n",
      "2 Train Loss 10000.026 Test RE 0.14584038855554335 c 0.0028183274 k 0.0005092371 m -6.21196e-07\n",
      "3 Train Loss 10000.025 Test RE 0.14516432049495315 c 0.0037866505 k 0.00069055514 m -1.0277785e-06\n",
      "4 Train Loss 10000.025 Test RE 0.1448152621687847 c 0.004488444 k 0.0008223171 m -1.3593452e-06\n",
      "5 Train Loss 10000.025 Test RE 0.1443973876937688 c 0.005458147 k 0.0010047096 m -1.852475e-06\n",
      "6 Train Loss 10000.025 Test RE 0.14385197511828285 c 0.0068543516 k 0.0012677923 m -2.6125922e-06\n",
      "7 Train Loss 10000.025 Test RE 0.14309179393055 c 0.008932483 k 0.0016601558 m -3.817936e-06\n",
      "8 Train Loss 10000.024 Test RE 0.14018218270270277 c 0.017336423 k 0.0032528504 m -9.160563e-06\n",
      "9 Train Loss 10000.008 Test RE 0.07955528308863252 c 0.16163997 k 0.03047682 m -0.0001150659\n",
      "10 Train Loss 10000.008 Test RE 0.07933295634193274 c 0.16362615 k 0.03083956 m -0.00011742108\n",
      "11 Train Loss 10000.008 Test RE 0.0788165288331733 c 0.16652916 k 0.03136563 m -0.00012338658\n",
      "12 Train Loss 10000.007 Test RE 0.07517643027095589 c 0.18059298 k 0.03387446 m -0.00017118167\n",
      "13 Train Loss 9999.999 Test RE 0.027594989044793967 c 0.24449296 k 0.044254486 m -0.00089862815\n",
      "14 Train Loss 9999.999 Test RE 0.026213874750597943 c 0.24474591 k 0.044236384 m -0.0009372501\n",
      "15 Train Loss 9999.999 Test RE 0.025540808075445523 c 0.2447504 k 0.044194557 m -0.00096385187\n",
      "16 Train Loss 9999.999 Test RE 0.025164398165098283 c 0.24470481 k 0.044152968 m -0.000985408\n",
      "17 Train Loss 9999.999 Test RE 0.02493982060685304 c 0.24467707 k 0.044119604 m -0.0010044188\n",
      "18 Train Loss 9999.999 Test RE 0.02479686015982742 c 0.24469307 k 0.044096597 m -0.0010224086\n",
      "19 Train Loss 9999.999 Test RE 0.024698323798393772 c 0.2447649 k 0.04408412 m -0.001040657\n",
      "20 Train Loss 9999.999 Test RE 0.024623443041203955 c 0.24490494 k 0.04408237 m -0.0010605773\n",
      "21 Train Loss 9999.999 Test RE 0.024559866354435855 c 0.24513556 k 0.044092856 m -0.0010840443\n",
      "22 Train Loss 9999.999 Test RE 0.02449955933422085 c 0.24549805 k 0.044119567 m -0.0011137492\n",
      "23 Train Loss 9999.999 Test RE 0.024436432355788514 c 0.24606256 k 0.04417021 m -0.0011536358\n",
      "24 Train Loss 9999.999 Test RE 0.02436502423910044 c 0.2469408 k 0.044257797 m -0.0012094723\n",
      "25 Train Loss 9999.999 Test RE 0.024279482728470386 c 0.2483037 k 0.044402868 m -0.0012896518\n",
      "26 Train Loss 9999.999 Test RE 0.024172742009856273 c 0.25040737 k 0.04463673 m -0.0014063669\n",
      "27 Train Loss 9999.999 Test RE 0.02403536948260412 c 0.25363183 k 0.045006387 m -0.001577316\n",
      "28 Train Loss 9999.999 Test RE 0.02385397376528071 c 0.25854227 k 0.04558233 m -0.0018283846\n",
      "29 Train Loss 9999.999 Test RE 0.023608545209555853 c 0.2659902 k 0.04647149 m -0.0021980007\n",
      "30 Train Loss 9999.999 Test RE 0.023267593869002522 c 0.27729374 k 0.04784045 m -0.0027447606\n",
      "31 Train Loss 9999.999 Test RE 0.022781054276644556 c 0.29455924 k 0.0499574 m -0.0035606055\n",
      "32 Train Loss 9999.999 Test RE 0.02208712118515296 c 0.32101083 k 0.05323836 m -0.004781703\n",
      "33 Train Loss 9999.999 Test RE 0.021274679267168937 c 0.35882303 k 0.057988342 m -0.0064812014\n",
      "34 Train Loss 9999.999 Test RE 0.020801891571946923 c 0.40163302 k 0.06350643 m -0.00832518\n",
      "35 Train Loss 9999.999 Test RE 0.020502419045462933 c 0.42492417 k 0.066623434 m -0.009254797\n",
      "36 Train Loss 9999.998 Test RE 0.01864823718509912 c 0.46056923 k 0.07171914 m -0.010472522\n",
      "37 Train Loss 9999.998 Test RE 0.018048457170528907 c 0.47100464 k 0.073311195 m -0.010776894\n",
      "38 Train Loss 9999.998 Test RE 0.017818503385142667 c 0.47876182 k 0.07452943 m -0.010995245\n",
      "39 Train Loss 9999.998 Test RE 0.01777100521177437 c 0.4852054 k 0.07557098 m -0.011172066\n",
      "40 Train Loss 9999.998 Test RE 0.01782070495637533 c 0.4910784 k 0.076547764 m -0.011330016\n",
      "41 Train Loss 9999.998 Test RE 0.01793236675492758 c 0.4969442 k 0.07754987 m -0.011485421\n",
      "42 Train Loss 9999.998 Test RE 0.01809682076202894 c 0.503337 k 0.07866838 m -0.011653146\n",
      "43 Train Loss 9999.998 Test RE 0.018320811770341502 c 0.5108702 k 0.080013156 m -0.011849857\n",
      "44 Train Loss 9999.998 Test RE 0.018624441841915435 c 0.52036786 k 0.08173607 m -0.012097745\n",
      "45 Train Loss 9999.998 Test RE 0.01904447799531387 c 0.5331087 k 0.0840757 m -0.012431117\n",
      "46 Train Loss 9999.998 Test RE 0.019647663394447675 c 0.5514408 k 0.087471955 m -0.012912691\n",
      "47 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "48 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "49 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "50 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "51 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "52 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "53 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "54 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "55 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "56 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "57 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "58 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "59 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "60 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "61 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "62 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "63 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "64 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "65 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "66 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "67 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "68 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "69 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "70 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "71 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "72 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "73 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "74 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "75 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "76 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "77 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "78 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "79 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "80 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "81 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "82 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "83 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "84 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "85 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "86 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "87 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "88 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "89 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "90 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "91 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "92 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "93 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "94 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "95 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "96 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "97 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "98 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "99 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "100 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "101 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "102 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "103 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "104 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "105 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "106 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "107 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "108 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "109 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "110 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "111 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "112 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "113 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "114 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "115 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "116 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "117 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "118 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "119 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "120 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "121 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "122 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "123 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "124 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "125 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "126 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "127 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "128 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "129 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "130 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "131 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "132 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "133 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "134 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "135 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "136 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "137 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "138 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "139 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "140 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "141 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "142 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "143 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "144 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "145 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "146 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "147 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "148 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "149 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "150 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "151 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "152 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "153 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "154 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "155 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "156 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "157 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "158 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "159 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "160 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "161 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "162 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "163 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "164 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "165 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "166 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "167 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "168 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "169 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "170 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "171 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "172 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "173 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "174 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "175 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "176 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "177 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "178 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "179 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "180 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "181 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "182 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "183 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "184 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "185 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "186 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "187 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "188 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "189 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "190 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "191 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "192 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "193 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "194 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "195 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "196 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "197 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "198 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "199 Train Loss 9999.998 Test RE 0.020571115011458595 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "Training time: 24.81\n",
      "Training time: 24.81\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 10000.026 Test RE 0.14768265784575535 c 0.0024936763 k 0.00059990125 m -6.1741696e-07\n",
      "1 Train Loss 10000.026 Test RE 0.14674265650440924 c 0.0029447856 k 0.0007043573 m -8.120587e-07\n",
      "2 Train Loss 10000.026 Test RE 0.1460841456039391 c 0.0034496952 k 0.0008230733 m -1.0567791e-06\n",
      "3 Train Loss 10000.025 Test RE 0.14508245111239665 c 0.0047694 k 0.0011333607 m -1.8052452e-06\n",
      "4 Train Loss 10000.025 Test RE 0.14457833611823104 c 0.0057260813 k 0.001358891 m -2.421727e-06\n",
      "5 Train Loss 10000.025 Test RE 0.14398025877604273 c 0.00704975 k 0.001672481 m -3.3455967e-06\n",
      "6 Train Loss 10000.025 Test RE 0.14320273106972872 c 0.008960675 k 0.0021258253 m -4.7816593e-06\n",
      "7 Train Loss 10000.024 Test RE 0.1404993072148657 c 0.016264845 k 0.0038698863 m -1.0899503e-05\n",
      "8 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "9 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "10 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "11 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "12 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "13 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "14 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "15 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "16 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "17 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "18 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "19 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "20 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "21 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "22 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "23 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "24 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "25 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "26 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "27 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "28 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "29 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "30 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "31 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "32 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "33 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "34 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "35 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "36 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "37 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "38 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "39 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "40 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "41 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "42 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "43 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "44 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "45 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "46 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "47 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "48 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "49 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "50 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "51 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "52 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "53 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "54 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "55 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "56 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "57 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "58 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "59 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "60 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "61 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "62 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "63 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "64 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "65 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "66 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "67 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "68 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "69 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "70 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "71 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "72 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "73 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "74 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "75 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "76 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "77 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "78 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "79 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "80 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "81 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "82 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "83 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "84 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "85 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "86 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "87 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "88 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "89 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "90 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "91 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "92 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "93 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "94 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "95 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "96 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "97 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "98 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "99 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "100 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "101 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "102 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "103 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "104 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "105 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "106 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "107 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "108 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "109 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "110 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "111 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "112 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "113 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "114 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "115 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "116 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "117 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "118 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "119 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "120 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "121 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "122 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "123 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "124 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "125 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "126 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "127 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "128 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "129 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "130 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "131 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "132 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "133 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "134 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "135 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "136 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "137 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "138 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "139 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "140 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "141 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "142 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "143 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "144 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "145 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "146 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "147 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "148 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "149 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "150 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "151 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "152 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "153 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "154 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "155 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "156 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "157 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "158 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "159 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "160 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "161 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "162 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "163 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "164 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "165 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "166 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "167 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "168 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "169 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "170 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "171 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "172 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "173 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "174 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "175 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "176 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "177 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "178 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "179 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "180 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "181 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "182 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "183 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "184 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "185 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "186 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "187 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "188 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "189 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "190 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "191 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "192 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "193 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "194 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "195 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "196 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "197 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "198 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "199 Train Loss 10000.005 Test RE 0.07434417088868639 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "Training time: 38.20\n",
      "Training time: 38.20\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 10000.026 Test RE 0.14654301384609705 c 0.0021771537 k 0.00041910997 m -5.5470963e-07\n",
      "1 Train Loss 10000.026 Test RE 0.1458290225712244 c 0.002660737 k 0.0005191278 m -7.845425e-07\n",
      "2 Train Loss 10000.025 Test RE 0.14460677357555654 c 0.003989285 k 0.0007962386 m -1.5535038e-06\n",
      "3 Train Loss 10000.025 Test RE 0.14391632310829744 c 0.0050016907 k 0.0010085278 m -2.236799e-06\n",
      "4 Train Loss 10000.025 Test RE 0.1430387868953798 c 0.006454802 k 0.0013141107 m -3.314422e-06\n",
      "5 Train Loss 10000.009 Test RE 0.08357711313112573 c 0.1097061 k 0.022886705 m -0.00011113867\n",
      "6 Train Loss 10000.008 Test RE 0.08110558857306514 c 0.11748814 k 0.024368184 m -0.00013355669\n",
      "7 Train Loss 10000.008 Test RE 0.07956240865395929 c 0.122586474 k 0.02532684 m -0.00015291313\n",
      "8 Train Loss 10000.008 Test RE 0.07774170500893741 c 0.12847732 k 0.026419852 m -0.00017953945\n",
      "9 Train Loss 10000.007 Test RE 0.07276129947038741 c 0.14262238 k 0.028963245 m -0.00026313055\n",
      "10 Train Loss 9999.999 Test RE 0.02565110026201228 c 0.18645644 k 0.034965586 m -0.0009931396\n",
      "11 Train Loss 9999.999 Test RE 0.024459293188921274 c 0.18644986 k 0.034891833 m -0.0010202657\n",
      "12 Train Loss 9999.999 Test RE 0.02365820925354869 c 0.18619636 k 0.034795213 m -0.0010401477\n",
      "13 Train Loss 9999.999 Test RE 0.023113099369376815 c 0.18588462 k 0.034699958 m -0.0010561344\n",
      "14 Train Loss 9999.999 Test RE 0.022748808475259886 c 0.1856145 k 0.034619145 m -0.0010699483\n",
      "15 Train Loss 9999.999 Test RE 0.02250367109082282 c 0.1854222 k 0.034555778 m -0.0010828393\n",
      "16 Train Loss 9999.999 Test RE 0.02233382669932056 c 0.18531589 k 0.034508884 m -0.001095733\n",
      "17 Train Loss 9999.999 Test RE 0.022210261886840825 c 0.18529351 k 0.03447609 m -0.001109447\n",
      "18 Train Loss 9999.999 Test RE 0.02211407312377208 c 0.18535572 k 0.034455378 m -0.001124951\n",
      "19 Train Loss 9999.999 Test RE 0.022032924388354688 c 0.18551417 k 0.03444597 m -0.0011436271\n",
      "20 Train Loss 9999.999 Test RE 0.021958525811791033 c 0.18579958 k 0.03444918 m -0.0011675789\n",
      "21 Train Loss 9999.999 Test RE 0.021885079332172895 c 0.18627056 k 0.034469187 m -0.0012000116\n",
      "22 Train Loss 9999.999 Test RE 0.021808388202794602 c 0.18702522 k 0.034514163 m -0.0012457496\n",
      "23 Train Loss 9999.999 Test RE 0.021725266461738908 c 0.18821919 k 0.034597915 m -0.0013120295\n",
      "24 Train Loss 9999.999 Test RE 0.021633162399673753 c 0.19009477 k 0.03474261 m -0.0014098005\n",
      "25 Train Loss 9999.999 Test RE 0.021529967630541765 c 0.1930324 k 0.034983598 m -0.0015559834\n",
      "26 Train Loss 9999.999 Test RE 0.02141382886990142 c 0.19765346 k 0.035379015 m -0.0017780175\n",
      "27 Train Loss 9999.999 Test RE 0.021282548395418963 c 0.20506506 k 0.03603268 m -0.0021246606\n",
      "28 Train Loss 9999.999 Test RE 0.021131495342493752 c 0.217604 k 0.03716365 m -0.0026988708\n",
      "29 Train Loss 9999.999 Test RE 0.020942791075715565 c 0.24241431 k 0.039440952 m -0.0038158377\n",
      "30 Train Loss 9999.999 Test RE 0.021880688245040372 c 0.32180822 k 0.046840746 m -0.007337576\n",
      "31 Train Loss 9999.999 Test RE 0.02243094761265824 c 0.3491068 k 0.04953069 m -0.008488389\n",
      "32 Train Loss 9999.999 Test RE 0.02145222700327171 c 0.36640468 k 0.051391367 m -0.009153727\n",
      "33 Train Loss 9999.999 Test RE 0.020299883304393185 c 0.3851674 k 0.0534384 m -0.009859016\n",
      "34 Train Loss 9999.998 Test RE 0.019064139283713154 c 0.41452527 k 0.056755118 m -0.01093122\n",
      "35 Train Loss 9999.998 Test RE 0.018947010141152083 c 0.42815647 k 0.05845036 m -0.011409037\n",
      "36 Train Loss 9999.998 Test RE 0.018903248311185374 c 0.4432435 k 0.06047467 m -0.011914857\n",
      "37 Train Loss 9999.998 Test RE 0.019503158393548773 c 0.4673791 k 0.06378141 m -0.012742858\n",
      "38 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "39 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "40 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "41 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "42 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "43 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "44 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "45 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "46 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "47 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "48 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "49 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "50 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "51 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "52 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "53 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "54 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "55 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "56 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "57 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "58 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "59 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "60 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "61 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "62 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "63 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "64 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "65 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "66 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "67 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "68 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "69 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "70 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "71 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "72 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "73 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "74 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "75 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "76 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "77 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "78 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "79 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "80 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "81 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "82 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "83 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "84 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "85 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "86 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "87 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "88 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "89 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "90 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "91 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "92 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "93 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "94 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "95 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "96 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "97 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "98 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "99 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "100 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "101 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "102 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "103 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "104 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "105 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "106 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "107 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "108 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "109 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "110 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "111 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "112 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "113 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "114 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "115 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "116 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "117 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "118 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "119 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "120 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "121 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "122 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "123 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "124 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "125 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "126 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "127 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "128 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "129 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "130 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "131 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "132 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "133 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "134 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "135 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "136 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "137 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "138 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "139 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "140 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "141 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "142 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "143 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "144 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "145 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "146 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "147 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "148 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "149 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "150 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "151 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "152 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "153 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "154 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "155 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "156 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "157 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "158 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "159 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "160 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "161 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "162 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "163 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "164 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "165 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "166 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "167 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "168 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "169 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "170 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "171 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "172 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "173 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "174 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "175 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "176 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "177 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "178 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "179 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "180 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "181 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "182 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "183 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "184 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "185 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "186 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "187 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "188 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "189 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "190 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "191 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "192 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "193 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "194 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "195 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "196 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "197 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "198 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "199 Train Loss 9999.998 Test RE 0.01979143516106618 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "Training time: 25.68\n",
      "Training time: 25.68\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 10000.026 Test RE 0.14609438705442943 c 0.0023809602 k 0.00055672135 m -8.8617446e-07\n",
      "1 Train Loss 10000.025 Test RE 0.14346252768714832 c 0.0039587896 k 0.000948522 m -2.1168241e-06\n",
      "2 Train Loss 10000.024 Test RE 0.14055200043360297 c 0.0068061426 k 0.0016745077 m -5.110834e-06\n",
      "3 Train Loss 10000.006 Test RE 0.07119954266092385 c 0.07600335 k 0.019352814 m -0.00011719109\n",
      "4 Train Loss 10000.006 Test RE 0.07009191328464139 c 0.07781196 k 0.019721374 m -0.00012748897\n",
      "5 Train Loss 10000.006 Test RE 0.06756838699721411 c 0.081446454 k 0.020433757 m -0.000152691\n",
      "6 Train Loss 10000.001 Test RE 0.02740748382733808 c 0.1184233 k 0.02582259 m -0.00072310877\n",
      "7 Train Loss 10000.001 Test RE 0.024994696276645734 c 0.12075518 k 0.025970003 m -0.0007992727\n",
      "8 Train Loss 10000.001 Test RE 0.023711600201507935 c 0.122349896 k 0.02604618 m -0.00085737044\n",
      "9 Train Loss 10000.001 Test RE 0.02294744253118029 c 0.12361682 k 0.02609305 m -0.00090731616\n",
      "10 Train Loss 10000.001 Test RE 0.022449612093434223 c 0.12470362 k 0.026120426 m -0.0009534358\n",
      "11 Train Loss 10000.001 Test RE 0.02208845081391429 c 0.1257248 k 0.02613647 m -0.0009992915\n",
      "12 Train Loss 10000.001 Test RE 0.021791091124762373 c 0.12677401 k 0.0261443 m -0.0010485959\n",
      "13 Train Loss 10000.001 Test RE 0.02150978857640963 c 0.12795919 k 0.026145576 m -0.0011061918\n",
      "14 Train Loss 10000.001 Test RE 0.021205123766676843 c 0.12942345 k 0.02613993 m -0.0011791247\n",
      "15 Train Loss 10000.001 Test RE 0.020832395789000927 c 0.13138555 k 0.026125252 m -0.0012785543\n",
      "16 Train Loss 10000.001 Test RE 0.02032220864217834 c 0.13422765 k 0.026096746 m -0.0014242444\n",
      "17 Train Loss 10000.0 Test RE 0.017940045091742317 c 0.14765957 k 0.025941718 m -0.002117112\n",
      "18 Train Loss 10000.0 Test RE 0.015365802944802439 c 0.17617641 k 0.02570161 m -0.0035694726\n",
      "19 Train Loss 10000.0 Test RE 0.014472799965286392 c 0.17688319 k 0.025881553 m -0.0035689368\n",
      "20 Train Loss 10000.0 Test RE 0.013809563244001453 c 0.1811644 k 0.026159806 m -0.0037241813\n",
      "21 Train Loss 10000.0 Test RE 0.013278774006094549 c 0.18247345 k 0.026364494 m -0.0037462846\n",
      "22 Train Loss 10000.0 Test RE 0.012985921147838695 c 0.1843821 k 0.026559126 m -0.0037991966\n",
      "23 Train Loss 10000.0 Test RE 0.012725751358864706 c 0.18616918 k 0.026770037 m -0.0038418681\n",
      "24 Train Loss 10000.0 Test RE 0.012515528870154061 c 0.18822376 k 0.02701264 m -0.0038903616\n",
      "25 Train Loss 10000.0 Test RE 0.012350768515884125 c 0.19050716 k 0.027290775 m -0.0039421204\n",
      "26 Train Loss 10000.0 Test RE 0.012242041550634087 c 0.19302565 k 0.027602525 m -0.0039980966\n",
      "27 Train Loss 10000.0 Test RE 0.012195691733128898 c 0.1957491 k 0.027943006 m -0.004058079\n",
      "28 Train Loss 10000.0 Test RE 0.01221484076292407 c 0.19866255 k 0.028307216 m -0.00412261\n",
      "29 Train Loss 10000.0 Test RE 0.012298313344198826 c 0.20178717 k 0.028693873 m -0.004193185\n",
      "30 Train Loss 10000.0 Test RE 0.012443463413185075 c 0.20520231 k 0.02910851 m -0.0042726765\n",
      "31 Train Loss 10000.0 Test RE 0.012650248510741943 c 0.20906626 k 0.029566158 m -0.0043658162\n",
      "32 Train Loss 10000.0 Test RE 0.01292559223114052 c 0.21364486 k 0.030094154 m -0.0044800625\n",
      "33 Train Loss 10000.0 Test RE 0.013286829163430833 c 0.21935523 k 0.03073613 m -0.0046269745\n",
      "34 Train Loss 10000.0 Test RE 0.01376506628255399 c 0.22683911 k 0.031559046 m -0.004824427\n",
      "35 Train Loss 10000.0 Test RE 0.014409132692352937 c 0.23708157 k 0.032665115 m -0.005100046\n",
      "36 Train Loss 9999.998 Test RE 0.016520239328627322 c 0.27290535 k 0.0364566 m -0.0060848985\n",
      "37 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "38 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "39 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "40 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "41 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "42 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "43 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "44 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "45 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "46 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "47 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "48 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "49 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "50 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "51 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "52 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "53 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "54 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "55 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "56 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "57 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "58 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "59 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "60 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "61 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "62 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "63 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "64 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "65 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "66 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "67 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "68 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "69 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "70 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "71 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "72 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "73 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "74 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "75 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "76 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "77 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "78 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "79 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "80 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "81 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "82 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "83 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "84 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "85 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "86 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "87 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "88 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "89 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "90 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "91 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "92 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "93 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "94 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "95 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "96 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "97 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "98 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "99 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "100 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "101 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "102 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "103 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "104 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "105 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "106 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "107 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "108 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "109 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "110 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "111 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "112 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "113 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "114 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "115 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "116 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "117 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "118 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "119 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "120 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "121 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "122 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "123 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "124 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "125 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "126 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "127 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "128 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "129 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "130 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "131 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "132 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "133 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "134 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "135 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "136 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "137 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "138 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "139 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "140 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "141 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "142 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "143 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "144 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "145 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "146 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "147 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "148 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "149 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "150 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "151 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "152 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "153 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "154 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "155 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "156 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "157 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "158 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "159 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "160 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "161 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "162 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "163 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "164 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "165 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "166 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "167 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "168 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "169 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "170 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "171 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "172 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "173 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "174 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "175 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "176 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "177 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "178 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "179 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "180 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "181 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "182 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "183 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "184 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "185 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "186 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "187 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "188 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "189 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "190 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "191 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "192 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "193 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "194 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "195 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "196 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "197 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "198 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "199 Train Loss 9999.998 Test RE 0.018260396786185004 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "Training time: 23.18\n",
      "Training time: 23.18\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 10000.026 Test RE 0.14641909083159138 c 0.0039694523 k 0.0006161805 m -8.059683e-07\n",
      "1 Train Loss 10000.026 Test RE 0.14573473282563618 c 0.0046522184 k 0.00074443186 m -1.088078e-06\n",
      "2 Train Loss 10000.025 Test RE 0.1447395413938679 c 0.0064126467 k 0.0010535417 m -1.9507236e-06\n",
      "3 Train Loss 10000.025 Test RE 0.14425690253484325 c 0.0076737623 k 0.0012704494 m -2.6607897e-06\n",
      "4 Train Loss 10000.025 Test RE 0.14368615910475022 c 0.009423531 k 0.0015877225 m -3.7362329e-06\n",
      "5 Train Loss 10000.025 Test RE 0.14293841865406487 c 0.011976239 k 0.0020392418 m -5.432105e-06\n",
      "6 Train Loss 10000.024 Test RE 0.14017293754241117 c 0.022300592 k 0.003902464 m -1.3099194e-05\n",
      "7 Train Loss 10000.007 Test RE 0.0848079615530354 c 0.22483326 k 0.04068251 m -0.00018944597\n",
      "8 Train Loss 10000.007 Test RE 0.08453895794227606 c 0.22803903 k 0.041244518 m -0.0001953676\n",
      "9 Train Loss 10000.007 Test RE 0.08382366089957177 c 0.23478888 k 0.042433802 m -0.0002109658\n",
      "10 Train Loss 10000.006 Test RE 0.08001473783916477 c 0.2645166 k 0.047680743 m -0.00029996442\n",
      "11 Train Loss 10000.005 Test RE 0.07339687889451732 c 0.30925333 k 0.055562068 m -0.00047044814\n",
      "12 Train Loss 9999.999 Test RE 0.029420975978556804 c 0.44736585 k 0.0791982 m -0.0015801387\n",
      "13 Train Loss 9999.999 Test RE 0.028802802354761386 c 0.4530243 k 0.080191016 m -0.0016430791\n",
      "14 Train Loss 9999.999 Test RE 0.028681198238938537 c 0.4572697 k 0.080943264 m -0.0016937656\n",
      "15 Train Loss 9999.999 Test RE 0.0286309637635222 c 0.46221554 k 0.08184651 m -0.0017548423\n",
      "16 Train Loss 9999.999 Test RE 0.028714312442854405 c 0.46889332 k 0.08309122 m -0.0018424725\n",
      "17 Train Loss 9999.999 Test RE 0.028863401197212177 c 0.47908828 k 0.085027136 m -0.0019826521\n",
      "18 Train Loss 9999.999 Test RE 0.029224184326239043 c 0.49611232 k 0.08830627 m -0.0022253566\n",
      "19 Train Loss 9999.999 Test RE 0.029809298696287078 c 0.52287924 k 0.093516335 m -0.0026135994\n",
      "20 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "21 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "22 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "23 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "24 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "25 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "26 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "27 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "28 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "29 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "30 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "31 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "32 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "33 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "34 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "35 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "36 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "37 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "38 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "39 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "40 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "41 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "42 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "43 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "44 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "45 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "46 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "47 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "48 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "49 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "50 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "51 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "52 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "53 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "54 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "55 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "56 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "57 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "58 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "59 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "60 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "61 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "62 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "63 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "64 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "65 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "66 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "67 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "68 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "69 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "70 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "71 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "72 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "73 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "74 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "75 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "76 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "77 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "78 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "79 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "80 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "81 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "82 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "83 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "84 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "85 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "86 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "87 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "88 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "89 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "90 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "91 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "92 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "93 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "94 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "95 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "96 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "97 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "98 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "99 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "100 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "101 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "102 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "103 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "104 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "105 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "106 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "107 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "108 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "109 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "110 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "111 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "112 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "113 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "114 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "115 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "116 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "117 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "118 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "119 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "120 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "121 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "122 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "123 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "124 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "125 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "126 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "127 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "128 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "129 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "130 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "131 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "132 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "133 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "134 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "135 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "136 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "137 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "138 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "139 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "140 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "141 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "142 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "143 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "144 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "145 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "146 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "147 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "148 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "149 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "150 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "151 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "152 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "153 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "154 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "155 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "156 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "157 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "158 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "159 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "160 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "161 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "162 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "163 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "164 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "165 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "166 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "167 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "168 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "169 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "170 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "171 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "172 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "173 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "174 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "175 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "176 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "177 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "178 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "179 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "180 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "181 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "182 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "183 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "184 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "185 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "186 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "187 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "188 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "189 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "190 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "191 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "192 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "193 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "194 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "195 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "196 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "197 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "198 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "199 Train Loss 9999.997 Test RE 0.03416292457181779 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "Training time: 32.35\n",
      "Training time: 32.35\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 10000.026 Test RE 0.14713468680871306 c 0.002988516 k 0.00048480762 m -4.7790945e-07\n",
      "1 Train Loss 10000.026 Test RE 0.14647821170797848 c 0.003489112 k 0.00057509815 m -6.3513056e-07\n",
      "2 Train Loss 10000.026 Test RE 0.14602289087416245 c 0.004042733 k 0.0006714054 m -8.3042687e-07\n",
      "3 Train Loss 10000.025 Test RE 0.14534504374990517 c 0.005464387 k 0.0009249279 m -1.4204628e-06\n",
      "4 Train Loss 10000.025 Test RE 0.14501157930358569 c 0.0064808344 k 0.0011085664 m -1.900997e-06\n",
      "5 Train Loss 10000.025 Test RE 0.14462080835164878 c 0.007877402 k 0.0013579117 m -2.615708e-06\n",
      "6 Train Loss 10000.025 Test RE 0.14411519099013584 c 0.009890292 k 0.0017225329 m -3.7247728e-06\n",
      "7 Train Loss 10000.025 Test RE 0.14341109176116285 c 0.012903993 k 0.0022661479 m -5.5007677e-06\n",
      "8 Train Loss 10000.024 Test RE 0.1405956670003457 c 0.02565309 k 0.0045898343 m -1.3764925e-05\n",
      "9 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "10 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "11 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "12 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "13 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "14 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "15 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "16 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "17 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "18 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "19 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "20 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "21 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "22 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "23 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "24 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "25 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "26 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "27 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "28 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "29 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "30 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "31 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "32 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "33 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "34 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "35 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "36 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "37 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "38 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "39 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "40 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "41 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "42 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "43 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "44 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "45 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "46 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "47 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "48 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "49 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "50 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "51 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "52 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "53 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "54 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "55 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "56 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "57 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "58 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "59 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "60 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "61 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "62 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "63 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "64 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "65 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "66 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "67 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "68 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "69 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "70 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "71 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "72 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "73 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "74 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "75 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "76 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "77 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "78 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "79 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "80 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "81 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "82 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "83 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "84 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "85 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "86 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "87 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "88 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "89 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "90 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "91 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "92 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "93 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "94 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "95 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "96 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "97 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "98 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "99 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "100 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "101 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "102 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "103 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "104 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "105 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "106 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "107 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "108 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "109 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "110 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "111 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "112 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "113 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "114 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "115 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "116 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "117 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "118 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "119 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "120 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "121 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "122 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "123 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "124 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "125 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "126 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "127 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "128 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "129 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "130 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "131 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "132 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "133 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "134 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "135 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "136 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "137 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "138 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "139 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "140 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "141 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "142 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "143 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "144 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "145 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "146 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "147 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "148 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "149 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "150 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "151 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "152 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "153 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "154 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "155 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "156 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "157 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "158 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "159 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "160 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "161 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "162 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "163 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "164 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "165 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "166 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "167 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "168 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "169 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "170 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "171 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "172 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "173 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "174 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "175 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "176 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "177 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "178 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "179 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "180 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "181 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "182 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "183 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "184 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "185 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "186 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "187 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "188 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "189 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "190 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "191 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "192 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "193 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "194 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "195 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "196 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "197 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "198 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "199 Train Loss 10000.007 Test RE 0.08003060743690794 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "Training time: 41.21\n",
      "Training time: 41.21\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 200\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "m_full = []\n",
    "k_full = []\n",
    "c_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "\n",
    "for reps in range(max_reps):   \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []   \n",
    "    beta_val = []\n",
    "    m_val = []\n",
    "    k_val = []\n",
    "    c_val = []\n",
    "\n",
    "    torch.manual_seed(reps*36)\n",
    "    N_f = 10000 #Total number of collocation points\n",
    "\n",
    "    layers = np.array([1,50,50,50,1]) #9 hidden layers\n",
    "    PINN = Sequentialmodel(layers)\n",
    "\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.25, \n",
    "                                max_iter = 10, \n",
    "                                max_eval = 15, \n",
    "                                tolerance_grad = 1e-6, \n",
    "                                tolerance_change = 1e-6, \n",
    "                                history_size = 100, \n",
    "                                line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "    train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    beta_full.append(beta_val)\n",
    "    m_full.append(m_val)\n",
    "    k_full.append(k_val)\n",
    "    c_full.append(c_val)\n",
    "    \n",
    "\n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"beta\": beta_full,\"m\": m_full,\"k\": k_full,\"c\": c_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "pmHEeBpzfFQh",
    "outputId": "990b2054-f35e-4c8e-c378-69ff2eb19f4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f73d84664d0>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_pred = PINN.test()\n",
    "plt.plot(t,(x_true-x_mean)/x_std,'b')\n",
    "plt.plot(t,x_pred,'r--')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "stan_1D_FODE_tune.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
