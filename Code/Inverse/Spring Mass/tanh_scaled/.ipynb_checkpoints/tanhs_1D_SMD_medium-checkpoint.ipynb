{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8byrnUmNKGR",
    "outputId": "510f67c6-b225-41ba-e78a-d0a6e54ded85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:1' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uoNYpDzlNKGV"
   },
   "outputs": [],
   "source": [
    "tstart= 0 \n",
    "tstop= 60 \n",
    "\n",
    "increment = 0.1 \n",
    "# Initial condition\n",
    "x_init= [0,0] \n",
    "\n",
    "t = np.arange(tstart,tstop+1,increment)\n",
    "\n",
    "c = 1 # Damping constant \n",
    "k = 0.05 # Stiffness of the spring\n",
    "m = 5 # Mass \n",
    "F0 = 100\n",
    "\n",
    "def mydiff(x, t):    \n",
    "    F =F0\n",
    "    \n",
    "    dx1dt = x[1] \n",
    "    dx2dt = (F -c*x[1] -k*x[0])/m\n",
    "    dxdt= [dx1dt, dx2dt] \n",
    "    \n",
    "    return dxdt \n",
    "\n",
    "x_full_sol = odeint(mydiff, x_init, t) \n",
    "\n",
    "x_sol = x_full_sol[:,0]\n",
    "v_sol = x_full_sol[:,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BR02v-fkNKGV"
   },
   "outputs": [],
   "source": [
    "level = \"medium\"\n",
    "label = \"1D_SMD_tanhs_\" + level\n",
    "\n",
    "loss_thresh = 0.005\n",
    "\n",
    "t_test = t.reshape(-1,1)\n",
    "t_test_tensor = torch.from_numpy(t_test).float().to(device)\n",
    "\n",
    "# bc1_t = t[0].reshape(-1,1)\n",
    "# bc1_x = x_sol[0].reshape(-1,1)\n",
    "# t_bc1_train = torch.from_numpy(bc1_t).float().to(device)\n",
    "# x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "x_mean = np.mean(x_sol.reshape(-1,1))\n",
    "x_std  = np.mean(x_sol.reshape(-1,1))                \n",
    "bc1_t = t.reshape(-1,1)\n",
    "bc1_x = (x_sol-x_mean).reshape(-1,1)/x_std\n",
    "t_bc1_train = torch.from_numpy(bc1_t).float().to(device)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "\n",
    "\n",
    "bc2_t = t[0].reshape(-1,1)\n",
    "t_bc2_train = torch.from_numpy(bc2_t).float().to(device)\n",
    "bc2_val = v_sol[0].reshape(-1,1)\n",
    "bc2_val =torch.from_numpy(bc2_val).float().to(device)\n",
    "\n",
    "t_test = t.reshape(-1,1)\n",
    "t_test_tensor = torch.from_numpy(t_test).float().to(device)\n",
    "x_true = x_sol\n",
    "x_true_norm = np.linalg.norm(x_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(t[0]) \n",
    "ub = np.array(t[-1]) \n",
    "\n",
    "#torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SyyktBKBXRo1"
   },
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    \n",
    "    t01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=t01,random_state =seed)\n",
    "\n",
    "    t_coll = lb + (ub-lb)*sampling(N_f)\n",
    "    # t_coll = np.vstack((t_coll,)) # append training points to collocation points \n",
    "\n",
    "    return t_coll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "o1b21zLnNKGW"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "        \n",
    "        \n",
    "        self.k = Parameter(torch.tensor(0.0))\n",
    "        self.k.requiresGrad = True\n",
    "        self.c = Parameter(torch.tensor(0.0))\n",
    "        self.c.requiresGrad = True\n",
    "        self.m = Parameter(torch.tensor(0.0))\n",
    "        self.m.requiresGrad = True\n",
    "        \n",
    "    'forward pass'\n",
    "    def forward(self,t):\n",
    "        if torch.is_tensor(t) != True:         \n",
    "            t = torch.from_numpy(t)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        t = 2.0*(t - l_b)/(u_b - l_b) - 1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = t.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,t,x):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(t), x)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,t_bc2,bc2_val):\n",
    "        g = t_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        x = self.forward(g)    \n",
    "            \n",
    "        x_t = autograd.grad(x,g,torch.ones([t_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dx_dt = x_t[:,[0]]\n",
    "        \n",
    "        bc2 = dx_dt/x_std\n",
    "        \n",
    "        loss_bc2= self.loss_function(bc2,bc2_val)\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self,t_coll,f_hat):\n",
    "             \n",
    "        g = t_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        x = self.forward(g) \n",
    "\n",
    "        x_t = autograd.grad(x,g,torch.ones([t_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        x_tt = autograd.grad(x_t,g,torch.ones(t_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dx_dt = x_t[:,[0]]\n",
    "        \n",
    "        dx2_d2t = x_tt[:,[0]]\n",
    "        \n",
    "        f = self.m*dx2_d2t/x_std + self.c*dx_dt/x_std + self.k*x/x_std - F0\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,t_bc1,x_bc1,t_bc2,bc2_val,t_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(t_bc1,x_bc1)\n",
    "        loss_bc2 = self.loss_BC2(t_bc2,bc2_val)\n",
    "        loss_f = self.loss_PDE(t_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self):\n",
    "        x_pred = self.forward(t_test_tensor)\n",
    "        x_pred = x_pred.cpu().detach().numpy()\n",
    "\n",
    "        x_pred = x_pred*x_std + x_mean\n",
    "        \n",
    "        return x_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        x_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(x_pred.reshape(-1,1) - x_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(x_pred.reshape(-1,1) - x_true.reshape(-1,1),2)/x_true_norm\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fLY2mT5BOgjD"
   },
   "outputs": [],
   "source": [
    "def train_step(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8srA5uGuObil"
   },
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    m_val.append(PINN.m.cpu().detach().numpy())\n",
    "    k_val.append(PINN.k.cpu().detach().numpy())\n",
    "    c_val.append(PINN.c.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0SezTZ_racQB"
   },
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "\n",
    "    t_coll = colloc_pts(N_f,rep*11)\n",
    "    t_coll =  torch.from_numpy(t_coll).float().to(device)\n",
    "    \n",
    "    f_hat = torch.zeros(t_coll.shape[0],1).to(device)\n",
    "    for i in range(max_iter):      \n",
    "        train_step(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat)\n",
    "         \n",
    "        loss_np = PINN.loss(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test RE\",test_re_loss[-1],\"c\",PINN.c.cpu().detach().numpy(),\"k\",PINN.k.cpu().detach().numpy(),\"m\",PINN.m.cpu().detach().numpy())\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I9BYbcJ0NKGX",
    "outputId": "42720d97-b37a-4c42-b7e5-b77a0ef391f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 10000.027 Test RE 0.9998872419690179 c 0.0022491494 k 0.00044633343 m -3.831647e-07\n",
      "1 Train Loss 10000.026 Test RE 0.9998833182388037 c 0.0031223907 k 0.0006254198 m -6.694264e-07\n",
      "2 Train Loss 10000.026 Test RE 0.9998819939011967 c 0.0036249445 k 0.0007305833 m -8.6533123e-07\n",
      "3 Train Loss 10000.025 Test RE 0.99987963588309 c 0.004914134 k 0.0009977709 m -1.4511784e-06\n",
      "4 Train Loss 10000.025 Test RE 0.9998784423515648 c 0.005831625 k 0.0011878194 m -1.9239242e-06\n",
      "5 Train Loss 10000.025 Test RE 0.99987727396993 c 0.007087622 k 0.0014511476 m -2.6241182e-06\n",
      "6 Train Loss 10000.025 Test RE 0.9998758093902829 c 0.008883633 k 0.0018263429 m -3.7001628e-06\n",
      "7 Train Loss 10000.025 Test RE 0.9998741634067222 c 0.011546083 k 0.0023868552 m -5.406614e-06\n",
      "8 Train Loss 10000.024 Test RE 0.9998690453843335 c 0.02220646 k 0.00464121 m -1.2941761e-05\n",
      "9 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "10 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "11 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "12 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "13 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "14 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "15 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "16 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "17 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "18 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "19 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "20 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "21 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "22 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "23 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "24 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "25 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "26 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "27 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "28 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "29 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "30 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "31 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "32 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "33 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "34 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "35 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "36 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "37 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "38 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "39 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "40 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "41 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "42 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "43 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "44 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "45 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "46 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "47 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "48 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "49 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "50 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "51 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "52 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "53 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "54 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "55 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "56 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "57 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "58 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "59 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "60 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "61 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "62 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "63 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "64 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "65 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "66 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "67 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "68 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "69 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "70 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "71 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "72 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "73 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "74 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "75 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "76 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "77 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "78 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "79 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "80 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "81 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "82 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "83 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "84 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "85 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "86 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "87 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "88 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "89 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "90 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "91 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "92 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "93 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "94 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "95 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "96 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "97 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "98 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "99 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "100 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "101 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "102 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "103 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "104 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "105 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "106 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "107 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "108 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "109 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "110 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "111 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "112 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "113 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "114 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "115 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "116 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "117 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "118 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "119 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "120 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "121 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "122 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "123 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "124 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "125 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "126 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "127 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "128 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "129 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "130 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "131 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "132 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "133 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "134 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "135 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "136 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "137 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "138 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "139 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "140 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "141 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "142 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "143 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "144 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "145 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "146 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "147 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "148 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "149 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "150 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "151 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "152 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "153 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "154 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "155 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "156 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "157 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "158 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "159 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "160 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "161 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "162 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "163 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "164 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "165 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "166 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "167 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "168 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "169 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "170 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "171 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "172 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "173 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "174 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "175 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "176 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "177 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "178 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "179 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "180 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "181 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "182 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "183 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "184 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "185 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "186 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "187 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "188 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "189 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "190 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "191 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "192 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "193 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "194 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "195 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "196 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "197 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "198 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "199 Train Loss 10000.005 Test RE 0.999868685438873 c 0.20446324 k 0.043706562 m -0.00015813355\n",
      "Training time: 24.34\n",
      "Training time: 24.34\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 10000.026 Test RE 0.999884270468188 c 0.0021821717 k 0.0007513225 m -1.2242381e-06\n",
      "1 Train Loss 10000.025 Test RE 0.999879437030008 c 0.0030996555 k 0.0010687812 m -2.197315e-06\n",
      "2 Train Loss 10000.024 Test RE 0.9998751285495978 c 0.0045430437 k 0.0015711836 m -4.0831237e-06\n",
      "3 Train Loss 10000.003 Test RE 0.9998726623942699 c 0.06680589 k 0.023053922 m -0.00013363\n",
      "4 Train Loss 10000.003 Test RE 0.9998717569001782 c 0.06768788 k 0.02326682 m -0.00014086376\n",
      "5 Train Loss 10000.003 Test RE 0.9998704630207367 c 0.06929988 k 0.023635924 m -0.00015651643\n",
      "6 Train Loss 10000.003 Test RE 0.9998689066334461 c 0.07169427 k 0.024156123 m -0.00018296398\n",
      "7 Train Loss 10000.003 Test RE 0.9998672029388418 c 0.074929036 k 0.024821538 m -0.00022282488\n",
      "8 Train Loss 10000.002 Test RE 0.9998640237328769 c 0.08346961 k 0.026394766 m -0.0003478871\n",
      "9 Train Loss 10000.001 Test RE 0.9998632240519202 c 0.09364739 k 0.02764984 m -0.00057622226\n",
      "10 Train Loss 10000.001 Test RE 0.9998630253612093 c 0.09799602 k 0.02794501 m -0.00070729264\n",
      "11 Train Loss 10000.001 Test RE 0.9998633328396646 c 0.1011175 k 0.028135872 m -0.00080686127\n",
      "12 Train Loss 10000.001 Test RE 0.9998635928898023 c 0.10335698 k 0.028238967 m -0.00088378653\n",
      "13 Train Loss 10000.0 Test RE 0.9998641752954283 c 0.10655432 k 0.028347543 m -0.0010013026\n",
      "14 Train Loss 10000.0 Test RE 0.9998644351827206 c 0.10782863 k 0.028375586 m -0.0010510349\n",
      "15 Train Loss 10000.0 Test RE 0.9998646731333852 c 0.10903708 k 0.028394682 m -0.0010996052\n",
      "16 Train Loss 10000.0 Test RE 0.9998649052998557 c 0.11028605 k 0.028409377 m -0.001150824\n",
      "17 Train Loss 10000.0 Test RE 0.9998651397545701 c 0.11169291 k 0.028421601 m -0.0012093799\n",
      "18 Train Loss 10000.0 Test RE 0.9998653919871929 c 0.11341504 k 0.028433518 m -0.0012817141\n",
      "19 Train Loss 10000.0 Test RE 0.9998656755820299 c 0.11567794 k 0.028446943 m -0.0013772729\n",
      "20 Train Loss 10000.0 Test RE 0.9998660078352478 c 0.11883502 k 0.028464967 m -0.0015108569\n",
      "21 Train Loss 10000.0 Test RE 0.9998664074889626 c 0.123488165 k 0.028493583 m -0.001707565\n",
      "22 Train Loss 10000.0 Test RE 0.9998668903940247 c 0.13075496 k 0.028547697 m -0.002013438\n",
      "23 Train Loss 10000.0 Test RE 0.9998674143287862 c 0.14247732 k 0.028667519 m -0.0025019927\n",
      "24 Train Loss 10000.0 Test RE 0.9998676869146151 c 0.1572061 k 0.028921638 m -0.0030990802\n",
      "25 Train Loss 10000.0 Test RE 0.9998676270253049 c 0.16564044 k 0.029273909 m -0.0034102106\n",
      "26 Train Loss 10000.0 Test RE 0.9998675784471128 c 0.17182757 k 0.029668769 m -0.00361877\n",
      "27 Train Loss 10000.0 Test RE 0.9998672997146542 c 0.17721002 k 0.0300968 m -0.0037870277\n",
      "28 Train Loss 10000.0 Test RE 0.9998668979388772 c 0.1818464 k 0.030540204 m -0.0039205155\n",
      "29 Train Loss 10000.0 Test RE 0.9998664271543958 c 0.18574566 k 0.030966606 m -0.004024608\n",
      "30 Train Loss 10000.0 Test RE 0.9998659735385875 c 0.1891094 k 0.031380974 m -0.0041076364\n",
      "31 Train Loss 10000.0 Test RE 0.9998655431456296 c 0.19224107 k 0.0317946 m -0.004181109\n",
      "32 Train Loss 10000.0 Test RE 0.9998651709267126 c 0.19535913 k 0.032226548 m -0.004251825\n",
      "33 Train Loss 10000.0 Test RE 0.9998648307300368 c 0.19878533 k 0.032700542 m -0.0043301755\n",
      "34 Train Loss 10000.0 Test RE 0.999864540017021 c 0.20283858 k 0.03325571 m -0.004424358\n",
      "35 Train Loss 10000.0 Test RE 0.9998642565117525 c 0.20811787 k 0.033953954 m -0.0045512323\n",
      "36 Train Loss 9999.998 Test RE 0.9998637528423602 c 0.22746062 k 0.03640178 m -0.005034476\n",
      "37 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "38 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "39 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "40 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "41 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "42 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "43 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "44 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "45 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "46 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "47 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "48 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "49 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "50 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "51 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "52 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "53 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "54 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "55 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "56 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "57 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "58 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "59 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "60 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "61 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "62 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "63 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "64 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "65 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "66 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "67 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "68 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "69 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "70 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "71 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "72 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "73 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "74 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "75 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "76 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "77 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "78 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "79 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "80 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "81 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "82 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "83 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "84 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "85 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "86 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "87 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "88 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "89 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "90 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "91 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "92 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "93 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "94 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "95 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "96 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "97 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "98 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "99 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "100 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "101 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "102 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "103 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "104 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "105 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "106 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "107 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "108 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "109 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "110 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "111 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "112 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "113 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "114 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "115 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "116 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "117 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "118 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "119 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "120 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "121 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "122 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "123 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "124 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "125 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "126 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "127 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "128 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "129 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "130 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "131 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "132 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "133 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "134 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "135 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "136 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "137 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "138 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "139 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "140 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "141 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "142 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "143 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "144 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "145 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "146 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "147 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "148 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "149 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "150 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "151 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "152 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "153 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "154 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "155 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "156 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "157 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "158 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "159 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "160 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "161 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "162 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "163 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "164 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "165 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "166 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "167 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "168 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "169 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "170 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "171 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "172 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "173 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "174 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "175 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "176 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "177 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "178 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "179 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "180 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "181 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "182 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "183 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "184 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "185 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "186 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "187 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "188 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "189 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "190 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "191 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "192 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "193 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "194 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "195 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "196 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "197 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "198 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "199 Train Loss 9999.998 Test RE 0.9998637095582923 c 0.2514918 k 0.03934943 m -0.00564943\n",
      "Training time: 34.73\n",
      "Training time: 34.73\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 10000.026 Test RE 0.9998857715075548 c 0.0025764382 k 0.00030880206 m -2.5905337e-07\n",
      "1 Train Loss 10000.026 Test RE 0.9998838309101324 c 0.0029322735 k 0.000348664 m -3.292503e-07\n",
      "2 Train Loss 10000.026 Test RE 0.9998829983046783 c 0.0033116892 k 0.00040122133 m -4.1443553e-07\n",
      "3 Train Loss 10000.026 Test RE 0.9998819499072867 c 0.0037390825 k 0.00045537358 m -5.202305e-07\n",
      "4 Train Loss 10000.026 Test RE 0.9998812221572291 c 0.00424962 k 0.0005232223 m -6.593633e-07\n",
      "5 Train Loss 10000.026 Test RE 0.999880398961045 c 0.004897288 k 0.0006077856 m -8.512737e-07\n",
      "6 Train Loss 10000.026 Test RE 0.9998796194801577 c 0.005767675 k 0.0007227842 m -1.130079e-06\n",
      "7 Train Loss 10000.025 Test RE 0.9998776660749044 c 0.008775503 k 0.0011204364 m -2.206976e-06\n",
      "8 Train Loss 10000.025 Test RE 0.9998763762815602 c 0.011439493 k 0.0014738782 m -3.2492067e-06\n",
      "9 Train Loss 10000.025 Test RE 0.9998747511942649 c 0.015524862 k 0.0020177541 m -4.942881e-06\n",
      "10 Train Loss 10000.025 Test RE 0.999872553039731 c 0.022041371 k 0.0028881263 m -7.792206e-06\n",
      "11 Train Loss 10000.01 Test RE 0.9998645373225015 c 0.2545293 k 0.03421488 m -0.000118884425\n",
      "12 Train Loss 10000.009 Test RE 0.9998702471181197 c 0.2605435 k 0.035045102 m -0.00012426065\n",
      "13 Train Loss 10000.009 Test RE 0.9998724241724353 c 0.26246795 k 0.035315197 m -0.00012696476\n",
      "14 Train Loss 10000.009 Test RE 0.999874337720086 c 0.26431185 k 0.03557474 m -0.00013011656\n",
      "15 Train Loss 10000.009 Test RE 0.9998761088041215 c 0.2663961 k 0.03586736 m -0.00013410294\n",
      "16 Train Loss 10000.009 Test RE 0.9998778233909943 c 0.26901802 k 0.036233783 m -0.00013943968\n",
      "17 Train Loss 10000.009 Test RE 0.9998795274694483 c 0.27251017 k 0.036719717 m -0.00014683668\n",
      "18 Train Loss 10000.008 Test RE 0.999882892306375 c 0.28387815 k 0.038294327 m -0.00017211509\n",
      "19 Train Loss 10000.008 Test RE 0.9998844616567921 c 0.29304144 k 0.03956097 m -0.00019334863\n",
      "20 Train Loss 10000.007 Test RE 0.999886912232769 c 0.3242936 k 0.043883715 m -0.00026951742\n",
      "21 Train Loss 10000.0 Test RE 0.9998652742512643 c 0.592207 k 0.08158547 m -0.0010768967\n",
      "22 Train Loss 9999.998 Test RE 0.9998622841930047 c 0.6132695 k 0.08459915 m -0.0011583954\n",
      "23 Train Loss 9999.998 Test RE 0.999861990288024 c 0.6217331 k 0.08582721 m -0.0011996111\n",
      "24 Train Loss 9999.998 Test RE 0.9998621087531808 c 0.630835 k 0.087226726 m -0.0012743489\n",
      "25 Train Loss 9999.998 Test RE 0.9998623014399088 c 0.64037865 k 0.08873598 m -0.0013678849\n",
      "26 Train Loss 9999.998 Test RE 0.999862484514649 c 0.64952606 k 0.09020965 m -0.0014670868\n",
      "27 Train Loss 9999.998 Test RE 0.9998626348906743 c 0.6584892 k 0.09167659 m -0.0015721755\n",
      "28 Train Loss 9999.998 Test RE 0.9998627559330291 c 0.6679566 k 0.09324805 m -0.0016905303\n",
      "29 Train Loss 9999.998 Test RE 0.99986285331632 c 0.6789012 k 0.09508774 m -0.0018348478\n",
      "30 Train Loss 9999.998 Test RE 0.9998629305743268 c 0.6926904 k 0.097431056 m -0.0020246971\n",
      "31 Train Loss 9999.998 Test RE 0.9998629870459189 c 0.7113046 k 0.10062302 m -0.0022896226\n",
      "32 Train Loss 9999.998 Test RE 0.9998630170455935 c 0.73765105 k 0.10517216 m -0.0026734103\n",
      "33 Train Loss 9999.998 Test RE 0.9998630020116559 c 0.7759188 k 0.1118097 m -0.0032382759\n",
      "34 Train Loss 9999.998 Test RE 0.9998628827252941 c 0.8312627 k 0.1214277 m -0.004057491\n",
      "35 Train Loss 9999.998 Test RE 0.9998624930566897 c 0.90525234 k 0.13427554 m -0.0051437253\n",
      "36 Train Loss 9999.998 Test RE 0.9998616444093453 c 0.9852254 k 0.14811657 m -0.0062958715\n",
      "37 Train Loss 9999.998 Test RE 0.9998603938705339 c 1.0603809 k 0.16106652 m -0.0073518734\n",
      "38 Train Loss 9999.998 Test RE 0.9998586461362633 c 1.1430383 k 0.17526956 m -0.008489253\n",
      "39 Train Loss 9999.998 Test RE 0.9998563870943356 c 1.2377974 k 0.19155458 m -0.009777608\n",
      "40 Train Loss 9999.997 Test RE 0.9998511772966946 c 1.4505179 k 0.2282271 m -0.012639527\n",
      "41 Train Loss 9999.996 Test RE 0.9998473776800243 c 1.7183415 k 0.2748436 m -0.016241577\n",
      "42 Train Loss 9999.973 Test RE 0.9998305872569231 c 6.073027 k 1.0603175 m -0.077296965\n",
      "43 Train Loss 9999.92 Test RE 0.9997378973348875 c 14.149234 k 2.5012617 m -0.1877582\n",
      "44 Train Loss 9998.9375 Test RE 0.99950831648458 c 71.283035 k 12.99299 m -0.99197835\n",
      "45 Train Loss 9991.567 Test RE 0.9977902516659194 c 184.99852 k 33.838737 m -2.564899\n",
      "46 Train Loss 9969.296 Test RE 0.9966463078430965 c 379.25314 k 69.22024 m -5.1378765\n",
      "47 Train Loss 9870.155 Test RE 0.9937190507341551 c 665.1591 k 121.19991 m -8.895419\n",
      "48 Train Loss 9843.146 Test RE 0.9914353264266851 c 804.97107 k 146.50903 m -10.716614\n",
      "49 Train Loss 9798.74 Test RE 0.9916641968853368 c 897.9265 k 163.3604 m -11.921343\n",
      "50 Train Loss 9785.313 Test RE 0.9915701663654968 c 898.3691 k 163.45268 m -11.906228\n",
      "51 Train Loss 9773.686 Test RE 0.9914772798167122 c 957.9779 k 174.16487 m -12.640373\n",
      "52 Train Loss 9750.253 Test RE 0.9898359335120245 c 1041.4142 k 188.74017 m -13.594074\n",
      "53 Train Loss 9651.824 Test RE 0.9888140148089137 c 1260.0424 k 228.04106 m -16.340601\n",
      "54 Train Loss 9629.321 Test RE 0.9875661460278016 c 1380.2346 k 249.35033 m -17.741514\n",
      "55 Train Loss 9590.453 Test RE 0.9874666755923707 c 1406.9846 k 253.47253 m -17.842121\n",
      "56 Train Loss 9542.022 Test RE 0.9877163222585447 c 1416.2761 k 254.47783 m -17.712149\n",
      "57 Train Loss 9530.548 Test RE 0.986976922178941 c 1478.5189 k 265.5292 m -18.455376\n",
      "58 Train Loss 9422.247 Test RE 0.9856018692119409 c 1589.4761 k 285.39597 m -19.757715\n",
      "59 Train Loss 9284.611 Test RE 0.9826240687870091 c 2063.8467 k 370.83725 m -25.728607\n",
      "60 Train Loss 9246.188 Test RE 0.9825571554955437 c 2142.7793 k 384.77106 m -26.643806\n",
      "61 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "62 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "63 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "64 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "65 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "66 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "67 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "68 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "69 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "70 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "71 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "72 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "73 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "74 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "75 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "76 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "77 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "78 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "79 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "80 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "81 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "82 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "83 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "84 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "85 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "86 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "87 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "88 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "89 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "90 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "91 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "92 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "93 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "94 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "95 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "96 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "97 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "98 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "99 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "100 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "101 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "102 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "103 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "104 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "105 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "106 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "107 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "108 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "109 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "110 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "111 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "112 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "113 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "114 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "115 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "116 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "117 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "118 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "119 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "120 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "121 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "122 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "123 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "124 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "125 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "126 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "127 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "128 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "129 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "130 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "131 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "132 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "133 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "134 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "135 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "136 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "137 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "138 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "139 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "140 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "141 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "142 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "143 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "144 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "145 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "146 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "147 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "148 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "149 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "150 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "151 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "152 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "153 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "154 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "155 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "156 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "157 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "158 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "159 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "160 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "161 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "162 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "163 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "164 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "165 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "166 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "167 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "168 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "169 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "170 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "171 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "172 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "173 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "174 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "175 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "176 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "177 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "178 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "179 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "180 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "181 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "182 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "183 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "184 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "185 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "186 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "187 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "188 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "189 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "190 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "191 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "192 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "193 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "194 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "195 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "196 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "197 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "198 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "199 Train Loss 9242.254 Test RE 0.9826977689405192 c 2131.721 k 382.73087 m -26.489187\n",
      "Training time: 22.48\n",
      "Training time: 22.48\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 10000.026 Test RE 0.9998849879435598 c 0.0029454017 k 0.00039679636 m -3.720341e-07\n",
      "1 Train Loss 10000.026 Test RE 0.9998833028125513 c 0.0033641912 k 0.0004515926 m -4.7149769e-07\n",
      "2 Train Loss 10000.026 Test RE 0.9998824268361998 c 0.003826286 k 0.0005205118 m -5.950999e-07\n",
      "3 Train Loss 10000.026 Test RE 0.9998814213512998 c 0.004363183 k 0.0005964448 m -7.5301034e-07\n",
      "4 Train Loss 10000.026 Test RE 0.9998806413664586 c 0.005024157 k 0.00069271435 m -9.659633e-07\n",
      "5 Train Loss 10000.025 Test RE 0.9998788955977904 c 0.0070733326 k 0.0009896846 m -1.7142384e-06\n",
      "6 Train Loss 10000.025 Test RE 0.9998778593021137 c 0.008772293 k 0.0012366679 m -2.4001347e-06\n",
      "7 Train Loss 10000.025 Test RE 0.9998766490604539 c 0.011278286 k 0.0016022961 m -3.4795312e-06\n",
      "8 Train Loss 10000.025 Test RE 0.9998751269468207 c 0.015065336 k 0.0021560339 m -5.212182e-06\n",
      "9 Train Loss 10000.025 Test RE 0.9998731607451042 c 0.020964669 k 0.003021417 m -8.06613e-06\n",
      "10 Train Loss 10000.006 Test RE 0.9998709958913059 c 0.30928153 k 0.045512553 m -0.00016385065\n",
      "11 Train Loss 10000.005 Test RE 0.999873782950868 c 0.32798856 k 0.048232205 m -0.00017966235\n",
      "12 Train Loss 9999.999 Test RE 0.9998653599799133 c 0.5025203 k 0.07498308 m -0.0013983565\n",
      "13 Train Loss 9999.999 Test RE 0.9998646923195542 c 0.51692516 k 0.07734435 m -0.0015639908\n",
      "14 Train Loss 9999.999 Test RE 0.9998637983985977 c 0.52905154 k 0.079381295 m -0.0017159043\n",
      "15 Train Loss 9999.999 Test RE 0.9998636781100083 c 0.54018235 k 0.081286505 m -0.0018648716\n",
      "16 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "17 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "18 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "19 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "20 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "21 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "22 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "23 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "24 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "25 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "26 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "27 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "28 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "29 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "30 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "31 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "32 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "33 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "34 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "35 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "36 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "37 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "38 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "39 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "40 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "41 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "42 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "43 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "44 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "45 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "46 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "47 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "48 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "49 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "50 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "51 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "52 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "53 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "54 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "55 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "56 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "57 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "58 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "59 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "60 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "61 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "62 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "63 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "64 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "65 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "66 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "67 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "68 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "69 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "70 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "71 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "72 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "73 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "74 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "75 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "76 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "77 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "78 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "79 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "80 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "81 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "82 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "83 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "84 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "85 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "86 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "87 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "88 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "89 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "90 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "91 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "92 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "93 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "94 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "95 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "96 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "97 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "98 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "99 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "100 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "101 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "102 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "103 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "104 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "105 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "106 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "107 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "108 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "109 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "110 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "111 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "112 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "113 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "114 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "115 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "116 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "117 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "118 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "119 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "120 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "121 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "122 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "123 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "124 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "125 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "126 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "127 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "128 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "129 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "130 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "131 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "132 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "133 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "134 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "135 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "136 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "137 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "138 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "139 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "140 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "141 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "142 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "143 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "144 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "145 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "146 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "147 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "148 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "149 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "150 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "151 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "152 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "153 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "154 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "155 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "156 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "157 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "158 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "159 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "160 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "161 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "162 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "163 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "164 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "165 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "166 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "167 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "168 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "169 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "170 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "171 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "172 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "173 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "174 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "175 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "176 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "177 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "178 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "179 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "180 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "181 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "182 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "183 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "184 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "185 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "186 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "187 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "188 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "189 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "190 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "191 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "192 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "193 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "194 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "195 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "196 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "197 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "198 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "199 Train Loss 9999.998 Test RE 0.9998629847332748 c 0.5611095 k 0.08495311 m -0.002165557\n",
      "Training time: 30.53\n",
      "Training time: 30.53\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 10000.026 Test RE 0.9998844689333022 c 0.0021138075 k 0.00037796583 m -3.7742137e-07\n",
      "1 Train Loss 10000.026 Test RE 0.9998829668980448 c 0.0024471167 k 0.0004399487 m -4.863561e-07\n",
      "2 Train Loss 10000.026 Test RE 0.9998817207178201 c 0.0028183274 k 0.0005092371 m -6.21196e-07\n",
      "3 Train Loss 10000.025 Test RE 0.9998795733681162 c 0.0037866505 k 0.00069055514 m -1.0277785e-06\n",
      "4 Train Loss 10000.025 Test RE 0.9998785043059017 c 0.004488444 k 0.0008223171 m -1.3593452e-06\n",
      "5 Train Loss 10000.025 Test RE 0.9998773356763623 c 0.005458147 k 0.0010047096 m -1.852475e-06\n",
      "6 Train Loss 10000.025 Test RE 0.9998759834403386 c 0.0068543516 k 0.0012677923 m -2.6125922e-06\n",
      "7 Train Loss 10000.025 Test RE 0.9998743456384023 c 0.008932483 k 0.0016601558 m -3.817936e-06\n",
      "8 Train Loss 10000.024 Test RE 0.999869484661587 c 0.017336423 k 0.0032528504 m -9.160563e-06\n",
      "9 Train Loss 10000.008 Test RE 0.9998714946035885 c 0.16163997 k 0.03047682 m -0.0001150659\n",
      "10 Train Loss 10000.008 Test RE 0.9998716186339893 c 0.16362615 k 0.03083956 m -0.00011742108\n",
      "11 Train Loss 10000.008 Test RE 0.9998718608756179 c 0.16652916 k 0.03136563 m -0.00012338658\n",
      "12 Train Loss 10000.007 Test RE 0.9998725312897679 c 0.18059298 k 0.03387446 m -0.00017118167\n",
      "13 Train Loss 9999.999 Test RE 0.9998657207233689 c 0.24449296 k 0.044254486 m -0.00089862815\n",
      "14 Train Loss 9999.999 Test RE 0.9998652882584875 c 0.24474591 k 0.044236384 m -0.0009372501\n",
      "15 Train Loss 9999.999 Test RE 0.9998650177448323 c 0.2447504 k 0.044194557 m -0.00096385187\n",
      "16 Train Loss 9999.999 Test RE 0.9998648255482064 c 0.24470481 k 0.044152968 m -0.000985408\n",
      "17 Train Loss 9999.999 Test RE 0.9998646825336616 c 0.24467707 k 0.044119604 m -0.0010044188\n",
      "18 Train Loss 9999.999 Test RE 0.9998645715435529 c 0.24469307 k 0.044096597 m -0.0010224086\n",
      "19 Train Loss 9999.999 Test RE 0.9998644808921818 c 0.2447649 k 0.04408412 m -0.001040657\n",
      "20 Train Loss 9999.999 Test RE 0.9998644022054429 c 0.24490494 k 0.04408237 m -0.0010605773\n",
      "21 Train Loss 9999.999 Test RE 0.9998643288505226 c 0.24513556 k 0.044092856 m -0.0010840443\n",
      "22 Train Loss 9999.999 Test RE 0.999864255222044 c 0.24549805 k 0.044119567 m -0.0011137492\n",
      "23 Train Loss 9999.999 Test RE 0.9998641767216242 c 0.24606256 k 0.04417021 m -0.0011536358\n",
      "24 Train Loss 9999.999 Test RE 0.9998640891221563 c 0.2469408 k 0.044257797 m -0.0012094723\n",
      "25 Train Loss 9999.999 Test RE 0.9998639888468647 c 0.2483037 k 0.044402868 m -0.0012896518\n",
      "26 Train Loss 9999.999 Test RE 0.9998638725180983 c 0.25040737 k 0.04463673 m -0.0014063669\n",
      "27 Train Loss 9999.999 Test RE 0.9998637378457104 c 0.25363183 k 0.045006387 m -0.001577316\n",
      "28 Train Loss 9999.999 Test RE 0.999863584284728 c 0.25854227 k 0.04558233 m -0.0018283846\n",
      "29 Train Loss 9999.999 Test RE 0.9998634152748223 c 0.2659902 k 0.04647149 m -0.0021980007\n",
      "30 Train Loss 9999.999 Test RE 0.9998632443572338 c 0.27729374 k 0.04784045 m -0.0027447606\n",
      "31 Train Loss 9999.999 Test RE 0.9998631105173527 c 0.29455924 k 0.0499574 m -0.0035606055\n",
      "32 Train Loss 9999.999 Test RE 0.9998631208414642 c 0.32101083 k 0.05323836 m -0.004781703\n",
      "33 Train Loss 9999.999 Test RE 0.9998634860558497 c 0.35882303 k 0.057988342 m -0.0064812014\n",
      "34 Train Loss 9999.999 Test RE 0.9998639310592434 c 0.40163302 k 0.06350643 m -0.00832518\n",
      "35 Train Loss 9999.999 Test RE 0.9998641055062658 c 0.42492417 k 0.066623434 m -0.009254797\n",
      "36 Train Loss 9999.998 Test RE 0.9998639866638962 c 0.46056923 k 0.07171914 m -0.010472522\n",
      "37 Train Loss 9999.998 Test RE 0.9998638160348949 c 0.47100464 k 0.073311195 m -0.010776894\n",
      "38 Train Loss 9999.998 Test RE 0.9998636540305845 c 0.47876182 k 0.07452943 m -0.010995245\n",
      "39 Train Loss 9999.998 Test RE 0.999863518212776 c 0.4852054 k 0.07557098 m -0.011172066\n",
      "40 Train Loss 9999.998 Test RE 0.9998634024248807 c 0.4910784 k 0.076547764 m -0.011330016\n",
      "41 Train Loss 9999.998 Test RE 0.9998632993649604 c 0.4969442 k 0.07754987 m -0.011485421\n",
      "42 Train Loss 9999.998 Test RE 0.9998632034611653 c 0.503337 k 0.07866838 m -0.011653146\n",
      "43 Train Loss 9999.998 Test RE 0.9998631112738021 c 0.5108702 k 0.080013156 m -0.011849857\n",
      "44 Train Loss 9999.998 Test RE 0.9998630215106454 c 0.52036786 k 0.08173607 m -0.012097745\n",
      "45 Train Loss 9999.998 Test RE 0.9998629375089748 c 0.5331087 k 0.0840757 m -0.012431117\n",
      "46 Train Loss 9999.998 Test RE 0.9998628739356274 c 0.5514408 k 0.087471955 m -0.012912691\n",
      "47 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "48 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "49 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "50 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "51 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "52 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "53 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "54 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "55 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "56 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "57 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "58 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "59 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "60 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "61 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "62 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "63 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "64 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "65 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "66 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "67 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "68 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "69 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "70 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "71 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "72 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "73 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "74 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "75 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "76 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "77 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "78 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "79 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "80 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "81 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "82 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "83 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "84 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "85 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "86 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "87 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "88 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "89 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "90 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "91 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "92 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "93 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "94 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "95 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "96 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "97 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "98 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "99 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "100 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "101 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "102 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "103 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "104 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "105 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "106 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "107 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "108 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "109 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "110 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "111 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "112 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "113 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "114 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "115 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "116 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "117 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "118 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "119 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "120 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "121 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "122 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "123 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "124 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "125 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "126 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "127 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "128 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "129 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "130 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "131 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "132 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "133 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "134 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "135 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "136 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "137 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "138 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "139 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "140 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "141 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "142 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "143 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "144 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "145 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "146 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "147 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "148 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "149 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "150 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "151 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "152 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "153 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "154 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "155 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "156 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "157 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "158 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "159 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "160 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "161 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "162 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "163 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "164 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "165 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "166 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "167 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "168 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "169 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "170 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "171 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "172 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "173 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "174 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "175 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "176 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "177 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "178 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "179 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "180 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "181 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "182 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "183 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "184 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "185 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "186 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "187 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "188 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "189 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "190 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "191 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "192 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "193 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "194 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "195 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "196 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "197 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "198 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "199 Train Loss 9999.998 Test RE 0.999862894409979 c 0.5810736 k 0.09299572 m -0.013693998\n",
      "Training time: 33.26\n",
      "Training time: 33.26\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 10000.026 Test RE 0.9998852519651779 c 0.0024936763 k 0.00059990125 m -6.1741696e-07\n",
      "1 Train Loss 10000.026 Test RE 0.9998833060640903 c 0.0029447856 k 0.0007043573 m -8.120587e-07\n",
      "2 Train Loss 10000.026 Test RE 0.9998817680139276 c 0.0034496952 k 0.0008230733 m -1.0567791e-06\n",
      "3 Train Loss 10000.025 Test RE 0.999879109088149 c 0.0047694 k 0.0011333607 m -1.8052452e-06\n",
      "4 Train Loss 10000.025 Test RE 0.9998777829980766 c 0.0057260813 k 0.001358891 m -2.421727e-06\n",
      "5 Train Loss 10000.025 Test RE 0.9998763668777251 c 0.00704975 k 0.001672481 m -3.3455967e-06\n",
      "6 Train Loss 10000.025 Test RE 0.9998746890622242 c 0.008960675 k 0.0021258253 m -4.7816593e-06\n",
      "7 Train Loss 10000.024 Test RE 0.9998700333929564 c 0.016264845 k 0.0038698863 m -1.0899503e-05\n",
      "8 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "9 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "10 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "11 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "12 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "13 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "14 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "15 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "16 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "17 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "18 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "19 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "20 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "21 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "22 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "23 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "24 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "25 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "26 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "27 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "28 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "29 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "30 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "31 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "32 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "33 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "34 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "35 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "36 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "37 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "38 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "39 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "40 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "41 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "42 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "43 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "44 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "45 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "46 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "47 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "48 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "49 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "50 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "51 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "52 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "53 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "54 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "55 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "56 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "57 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "58 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "59 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "60 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "61 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "62 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "63 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "64 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "65 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "66 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "67 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "68 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "69 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "70 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "71 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "72 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "73 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "74 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "75 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "76 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "77 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "78 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "79 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "80 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "81 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "82 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "83 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "84 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "85 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "86 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "87 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "88 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "89 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "90 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "91 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "92 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "93 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "94 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "95 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "96 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "97 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "98 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "99 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "100 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "101 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "102 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "103 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "104 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "105 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "106 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "107 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "108 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "109 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "110 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "111 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "112 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "113 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "114 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "115 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "116 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "117 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "118 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "119 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "120 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "121 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "122 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "123 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "124 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "125 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "126 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "127 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "128 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "129 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "130 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "131 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "132 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "133 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "134 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "135 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "136 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "137 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "138 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "139 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "140 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "141 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "142 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "143 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "144 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "145 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "146 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "147 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "148 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "149 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "150 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "151 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "152 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "153 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "154 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "155 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "156 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "157 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "158 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "159 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "160 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "161 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "162 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "163 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "164 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "165 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "166 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "167 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "168 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "169 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "170 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "171 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "172 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "173 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "174 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "175 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "176 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "177 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "178 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "179 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "180 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "181 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "182 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "183 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "184 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "185 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "186 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "187 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "188 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "189 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "190 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "191 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "192 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "193 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "194 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "195 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "196 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "197 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "198 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "199 Train Loss 10000.005 Test RE 0.999854136717928 c 0.17888981 k 0.043147698 m -0.00016596146\n",
      "Training time: 42.87\n",
      "Training time: 42.87\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 10000.026 Test RE 0.9998833552686587 c 0.0021771537 k 0.00041910997 m -5.5470963e-07\n",
      "1 Train Loss 10000.026 Test RE 0.9998816986978385 c 0.002660737 k 0.0005191278 m -7.845425e-07\n",
      "2 Train Loss 10000.025 Test RE 0.9998787340776042 c 0.003989285 k 0.0007962386 m -1.5535038e-06\n",
      "3 Train Loss 10000.025 Test RE 0.9998771756100485 c 0.0050016907 k 0.0010085278 m -2.236799e-06\n",
      "4 Train Loss 10000.025 Test RE 0.9998753848913023 c 0.006454802 k 0.0013141107 m -3.314422e-06\n",
      "5 Train Loss 10000.009 Test RE 0.9998766979232184 c 0.1097061 k 0.022886705 m -0.00011113867\n",
      "6 Train Loss 10000.008 Test RE 0.9998748820393821 c 0.11748814 k 0.024368184 m -0.00013355669\n",
      "7 Train Loss 10000.008 Test RE 0.9998739892110379 c 0.122586474 k 0.02532684 m -0.00015291313\n",
      "8 Train Loss 10000.008 Test RE 0.9998730659428248 c 0.12847732 k 0.026419852 m -0.00017953945\n",
      "9 Train Loss 10000.007 Test RE 0.9998709625473848 c 0.14262238 k 0.028963245 m -0.00026313055\n",
      "10 Train Loss 9999.999 Test RE 0.999862486515642 c 0.18645644 k 0.034965586 m -0.0009931396\n",
      "11 Train Loss 9999.999 Test RE 0.99986287705207 c 0.18644986 k 0.034891833 m -0.0010202657\n",
      "12 Train Loss 9999.999 Test RE 0.9998633028490674 c 0.18619636 k 0.034795213 m -0.0010401477\n",
      "13 Train Loss 9999.999 Test RE 0.9998636941756789 c 0.18588462 k 0.034699958 m -0.0010561344\n",
      "14 Train Loss 9999.999 Test RE 0.9998640267209005 c 0.1856145 k 0.034619145 m -0.0010699483\n",
      "15 Train Loss 9999.999 Test RE 0.9998642986379555 c 0.1854222 k 0.034555778 m -0.0010828393\n",
      "16 Train Loss 9999.999 Test RE 0.999864521079694 c 0.18531589 k 0.034508884 m -0.001095733\n",
      "17 Train Loss 9999.999 Test RE 0.9998647070298502 c 0.18529351 k 0.03447609 m -0.001109447\n",
      "18 Train Loss 9999.999 Test RE 0.9998648695977003 c 0.18535572 k 0.034455378 m -0.001124951\n",
      "19 Train Loss 9999.999 Test RE 0.9998650203250737 c 0.18551417 k 0.03444597 m -0.0011436271\n",
      "20 Train Loss 9999.999 Test RE 0.9998651695655166 c 0.18579958 k 0.03444918 m -0.0011675789\n",
      "21 Train Loss 9999.999 Test RE 0.9998653265235021 c 0.18627056 k 0.034469187 m -0.0012000116\n",
      "22 Train Loss 9999.999 Test RE 0.999865499315571 c 0.18702522 k 0.034514163 m -0.0012457496\n",
      "23 Train Loss 9999.999 Test RE 0.9998656956362775 c 0.18821919 k 0.034597915 m -0.0013120295\n",
      "24 Train Loss 9999.999 Test RE 0.9998659227222316 c 0.19009477 k 0.03474261 m -0.0014098005\n",
      "25 Train Loss 9999.999 Test RE 0.9998661878817862 c 0.1930324 k 0.034983598 m -0.0015559834\n",
      "26 Train Loss 9999.999 Test RE 0.9998664976264269 c 0.19765346 k 0.035379015 m -0.0017780175\n",
      "27 Train Loss 9999.999 Test RE 0.9998668566312808 c 0.20506506 k 0.03603268 m -0.0021246606\n",
      "28 Train Loss 9999.999 Test RE 0.9998672575342067 c 0.217604 k 0.03716365 m -0.0026988708\n",
      "29 Train Loss 9999.999 Test RE 0.9998675892425851 c 0.24241431 k 0.039440952 m -0.0038158377\n",
      "30 Train Loss 9999.999 Test RE 0.9998652761605005 c 0.32180822 k 0.046840746 m -0.007337576\n",
      "31 Train Loss 9999.999 Test RE 0.9998648567833703 c 0.3491068 k 0.04953069 m -0.008488389\n",
      "32 Train Loss 9999.999 Test RE 0.9998659518512877 c 0.36640468 k 0.051391367 m -0.009153727\n",
      "33 Train Loss 9999.999 Test RE 0.9998666491009086 c 0.3851674 k 0.0534384 m -0.009859016\n",
      "34 Train Loss 9999.998 Test RE 0.9998667753157986 c 0.41452527 k 0.056755118 m -0.01093122\n",
      "35 Train Loss 9999.998 Test RE 0.9998663562188924 c 0.42815647 k 0.05845036 m -0.011409037\n",
      "36 Train Loss 9999.998 Test RE 0.9998660258684888 c 0.4432435 k 0.06047467 m -0.011914857\n",
      "37 Train Loss 9999.998 Test RE 0.9998649544071804 c 0.4673791 k 0.06378141 m -0.012742858\n",
      "38 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "39 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "40 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "41 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "42 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "43 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "44 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "45 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "46 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "47 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "48 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "49 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "50 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "51 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "52 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "53 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "54 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "55 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "56 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "57 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "58 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "59 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "60 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "61 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "62 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "63 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "64 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "65 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "66 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "67 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "68 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "69 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "70 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "71 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "72 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "73 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "74 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "75 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "76 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "77 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "78 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "79 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "80 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "81 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "82 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "83 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "84 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "85 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "86 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "87 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "88 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "89 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "90 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "91 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "92 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "93 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "94 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "95 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "96 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "97 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "98 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "99 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "100 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "101 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "102 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "103 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "104 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "105 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "106 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "107 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "108 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "109 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "110 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "111 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "112 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "113 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "114 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "115 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "116 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "117 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "118 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "119 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "120 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "121 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "122 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "123 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "124 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "125 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "126 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "127 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "128 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "129 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "130 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "131 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "132 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "133 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "134 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "135 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "136 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "137 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "138 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "139 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "140 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "141 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "142 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "143 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "144 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "145 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "146 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "147 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "148 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "149 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "150 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "151 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "152 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "153 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "154 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "155 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "156 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "157 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "158 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "159 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "160 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "161 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "162 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "163 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "164 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "165 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "166 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "167 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "168 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "169 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "170 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "171 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "172 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "173 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "174 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "175 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "176 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "177 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "178 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "179 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "180 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "181 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "182 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "183 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "184 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "185 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "186 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "187 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "188 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "189 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "190 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "191 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "192 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "193 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "194 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "195 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "196 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "197 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "198 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "199 Train Loss 9999.998 Test RE 0.9998646285263428 c 0.49619797 k 0.06818303 m -0.013643458\n",
      "Training time: 29.29\n",
      "Training time: 29.29\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 10000.026 Test RE 0.9998843742386664 c 0.0023809602 k 0.00055672135 m -8.8617446e-07\n",
      "1 Train Loss 10000.025 Test RE 0.9998794502467343 c 0.0039587896 k 0.000948522 m -2.1168241e-06\n",
      "2 Train Loss 10000.024 Test RE 0.9998745756409362 c 0.0068061426 k 0.0016745077 m -5.110834e-06\n",
      "3 Train Loss 10000.006 Test RE 0.9998739728527252 c 0.07600335 k 0.019352814 m -0.00011719109\n",
      "4 Train Loss 10000.006 Test RE 0.9998729288367565 c 0.07781196 k 0.019721374 m -0.00012748897\n",
      "5 Train Loss 10000.006 Test RE 0.9998711336584344 c 0.081446454 k 0.020433757 m -0.000152691\n",
      "6 Train Loss 10000.001 Test RE 0.9998634448839828 c 0.1184233 k 0.02582259 m -0.00072310877\n",
      "7 Train Loss 10000.001 Test RE 0.9998638258212986 c 0.12075518 k 0.025970003 m -0.0007992727\n",
      "8 Train Loss 10000.001 Test RE 0.9998641117347198 c 0.122349896 k 0.02604618 m -0.00085737044\n",
      "9 Train Loss 10000.001 Test RE 0.9998643986191519 c 0.12361682 k 0.02609305 m -0.00090731616\n",
      "10 Train Loss 10000.001 Test RE 0.9998646475306119 c 0.12470362 k 0.026120426 m -0.0009534358\n",
      "11 Train Loss 10000.001 Test RE 0.9998648784710223 c 0.1257248 k 0.02613647 m -0.0009992915\n",
      "12 Train Loss 10000.001 Test RE 0.9998650973512461 c 0.12677401 k 0.0261443 m -0.0010485959\n",
      "13 Train Loss 10000.001 Test RE 0.999865318651636 c 0.12795919 k 0.026145576 m -0.0011061918\n",
      "14 Train Loss 10000.001 Test RE 0.99986555527418 c 0.12942345 k 0.02613993 m -0.0011791247\n",
      "15 Train Loss 10000.001 Test RE 0.9998658220274834 c 0.13138555 k 0.026125252 m -0.0012785543\n",
      "16 Train Loss 10000.001 Test RE 0.9998661336851737 c 0.13422765 k 0.026096746 m -0.0014242444\n",
      "17 Train Loss 10000.0 Test RE 0.9998668367387015 c 0.14765957 k 0.025941718 m -0.002117112\n",
      "18 Train Loss 10000.0 Test RE 0.9998645472571743 c 0.17617641 k 0.02570161 m -0.0035694726\n",
      "19 Train Loss 10000.0 Test RE 0.9998654437634782 c 0.17688319 k 0.025881553 m -0.0035689368\n",
      "20 Train Loss 10000.0 Test RE 0.9998662607960465 c 0.1811644 k 0.026159806 m -0.0037241813\n",
      "21 Train Loss 10000.0 Test RE 0.999866803830252 c 0.18247345 k 0.026364494 m -0.0037462846\n",
      "22 Train Loss 10000.0 Test RE 0.9998670653908481 c 0.1843821 k 0.026559126 m -0.0037991966\n",
      "23 Train Loss 10000.0 Test RE 0.9998671985315518 c 0.18616918 k 0.026770037 m -0.0038418681\n",
      "24 Train Loss 10000.0 Test RE 0.9998672040752887 c 0.18822376 k 0.02701264 m -0.0038903616\n",
      "25 Train Loss 10000.0 Test RE 0.999867098200281 c 0.19050716 k 0.027290775 m -0.0039421204\n",
      "26 Train Loss 10000.0 Test RE 0.9998668940267517 c 0.19302565 k 0.027602525 m -0.0039980966\n",
      "27 Train Loss 10000.0 Test RE 0.9998666102629212 c 0.1957491 k 0.027943006 m -0.004058079\n",
      "28 Train Loss 10000.0 Test RE 0.9998662689238307 c 0.19866255 k 0.028307216 m -0.00412261\n",
      "29 Train Loss 10000.0 Test RE 0.999865891661908 c 0.20178717 k 0.028693873 m -0.004193185\n",
      "30 Train Loss 10000.0 Test RE 0.9998654944532944 c 0.20520231 k 0.02910851 m -0.0042726765\n",
      "31 Train Loss 10000.0 Test RE 0.9998650846990823 c 0.20906626 k 0.029566158 m -0.0043658162\n",
      "32 Train Loss 10000.0 Test RE 0.9998646605325884 c 0.21364486 k 0.030094154 m -0.0044800625\n",
      "33 Train Loss 10000.0 Test RE 0.9998642127904863 c 0.21935523 k 0.03073613 m -0.0046269745\n",
      "34 Train Loss 10000.0 Test RE 0.9998637268940576 c 0.22683911 k 0.031559046 m -0.004824427\n",
      "35 Train Loss 10000.0 Test RE 0.9998631841886424 c 0.23708157 k 0.032665115 m -0.005100046\n",
      "36 Train Loss 9999.998 Test RE 0.9998618340743426 c 0.27290535 k 0.0364566 m -0.0060848985\n",
      "37 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "38 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "39 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "40 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "41 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "42 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "43 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "44 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "45 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "46 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "47 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "48 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "49 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "50 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "51 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "52 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "53 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "54 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "55 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "56 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "57 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "58 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "59 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "60 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "61 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "62 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "63 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "64 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "65 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "66 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "67 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "68 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "69 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "70 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "71 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "72 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "73 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "74 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "75 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "76 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "77 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "78 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "79 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "80 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "81 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "82 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "83 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "84 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "85 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "86 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "87 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "88 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "89 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "90 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "91 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "92 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "93 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "94 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "95 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "96 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "97 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "98 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "99 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "100 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "101 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "102 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "103 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "104 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "105 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "106 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "107 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "108 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "109 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "110 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "111 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "112 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "113 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "114 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "115 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "116 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "117 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "118 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "119 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "120 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "121 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "122 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "123 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "124 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "125 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "126 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "127 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "128 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "129 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "130 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "131 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "132 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "133 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "134 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "135 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "136 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "137 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "138 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "139 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "140 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "141 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "142 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "143 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "144 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "145 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "146 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "147 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "148 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "149 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "150 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "151 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "152 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "153 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "154 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "155 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "156 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "157 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "158 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "159 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "160 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "161 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "162 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "163 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "164 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "165 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "166 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "167 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "168 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "169 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "170 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "171 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "172 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "173 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "174 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "175 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "176 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "177 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "178 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "179 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "180 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "181 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "182 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "183 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "184 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "185 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "186 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "187 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "188 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "189 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "190 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "191 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "192 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "193 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "194 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "195 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "196 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "197 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "198 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "199 Train Loss 9999.998 Test RE 0.9998609654947416 c 0.30517963 k 0.039833702 m -0.0069829696\n",
      "Training time: 29.03\n",
      "Training time: 29.03\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 10000.026 Test RE 0.9998840889708255 c 0.0039694523 k 0.0006161805 m -8.059683e-07\n",
      "1 Train Loss 10000.026 Test RE 0.9998828602480428 c 0.0046522184 k 0.00074443186 m -1.088078e-06\n",
      "2 Train Loss 10000.025 Test RE 0.9998801109014078 c 0.0064126467 k 0.0010535417 m -1.9507236e-06\n",
      "3 Train Loss 10000.025 Test RE 0.9998784969117123 c 0.0076737623 k 0.0012704494 m -2.6607897e-06\n",
      "4 Train Loss 10000.025 Test RE 0.9998773189355352 c 0.009423531 k 0.0015877225 m -3.7362329e-06\n",
      "5 Train Loss 10000.025 Test RE 0.9998753997471033 c 0.011976239 k 0.0020392418 m -5.432105e-06\n",
      "6 Train Loss 10000.024 Test RE 0.9998704182152585 c 0.022300592 k 0.003902464 m -1.3099194e-05\n",
      "7 Train Loss 10000.007 Test RE 0.9998748729999024 c 0.22483326 k 0.04068251 m -0.00018944597\n",
      "8 Train Loss 10000.007 Test RE 0.9998744541627124 c 0.22803903 k 0.041244518 m -0.0001953676\n",
      "9 Train Loss 10000.007 Test RE 0.9998739473609436 c 0.23478888 k 0.042433802 m -0.0002109658\n",
      "10 Train Loss 10000.006 Test RE 0.9998721083230809 c 0.2645166 k 0.047680743 m -0.00029996442\n",
      "11 Train Loss 10000.005 Test RE 0.9998700604017797 c 0.30925333 k 0.055562068 m -0.00047044814\n",
      "12 Train Loss 9999.999 Test RE 0.9998641571012641 c 0.44736585 k 0.0791982 m -0.0015801387\n",
      "13 Train Loss 9999.999 Test RE 0.9998644523795412 c 0.4530243 k 0.080191016 m -0.0016430791\n",
      "14 Train Loss 9999.999 Test RE 0.9998639566468022 c 0.4572697 k 0.080943264 m -0.0016937656\n",
      "15 Train Loss 9999.999 Test RE 0.9998638482731558 c 0.46221554 k 0.08184651 m -0.0017548423\n",
      "16 Train Loss 9999.999 Test RE 0.999863510859259 c 0.46889332 k 0.08309122 m -0.0018424725\n",
      "17 Train Loss 9999.999 Test RE 0.9998632615788918 c 0.47908828 k 0.085027136 m -0.0019826521\n",
      "18 Train Loss 9999.999 Test RE 0.9998627422408632 c 0.49611232 k 0.08830627 m -0.0022253566\n",
      "19 Train Loss 9999.999 Test RE 0.9998622804941681 c 0.52287924 k 0.093516335 m -0.0026135994\n",
      "20 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "21 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "22 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "23 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "24 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "25 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "26 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "27 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "28 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "29 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "30 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "31 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "32 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "33 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "34 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "35 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "36 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "37 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "38 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "39 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "40 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "41 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "42 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "43 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "44 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "45 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "46 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "47 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "48 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "49 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "50 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "51 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "52 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "53 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "54 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "55 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "56 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "57 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "58 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "59 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "60 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "61 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "62 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "63 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "64 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "65 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "66 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "67 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "68 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "69 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "70 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "71 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "72 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "73 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "74 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "75 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "76 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "77 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "78 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "79 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "80 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "81 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "82 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "83 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "84 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "85 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "86 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "87 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "88 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "89 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "90 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "91 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "92 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "93 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "94 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "95 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "96 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "97 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "98 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "99 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "100 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "101 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "102 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "103 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "104 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "105 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "106 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "107 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "108 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "109 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "110 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "111 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "112 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "113 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "114 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "115 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "116 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "117 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "118 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "119 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "120 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "121 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "122 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "123 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "124 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "125 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "126 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "127 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "128 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "129 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "130 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "131 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "132 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "133 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "134 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "135 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "136 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "137 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "138 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "139 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "140 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "141 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "142 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "143 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "144 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "145 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "146 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "147 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "148 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "149 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "150 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "151 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "152 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "153 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "154 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "155 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "156 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "157 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "158 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "159 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "160 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "161 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "162 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "163 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "164 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "165 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "166 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "167 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "168 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "169 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "170 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "171 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "172 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "173 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "174 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "175 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "176 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "177 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "178 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "179 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "180 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "181 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "182 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "183 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "184 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "185 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "186 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "187 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "188 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "189 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "190 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "191 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "192 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "193 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "194 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "195 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "196 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "197 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "198 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "199 Train Loss 9999.997 Test RE 0.9998594954478657 c 0.67280847 k 0.12315655 m -0.0048198416\n",
      "Training time: 35.16\n",
      "Training time: 35.16\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 10000.026 Test RE 0.9998849734252999 c 0.002988516 k 0.00048480762 m -4.7790945e-07\n",
      "1 Train Loss 10000.026 Test RE 0.9998834792475937 c 0.003489112 k 0.00057509815 m -6.3513056e-07\n",
      "2 Train Loss 10000.026 Test RE 0.9998820677328313 c 0.004042733 k 0.0006714054 m -8.3042687e-07\n",
      "3 Train Loss 10000.025 Test RE 0.9998798642121571 c 0.005464387 k 0.0009249279 m -1.4204628e-06\n",
      "4 Train Loss 10000.025 Test RE 0.9998788719819539 c 0.0064808344 k 0.0011085664 m -1.900997e-06\n",
      "5 Train Loss 10000.025 Test RE 0.9998776104857212 c 0.007877402 k 0.0013579117 m -2.615708e-06\n",
      "6 Train Loss 10000.025 Test RE 0.9998763416936818 c 0.009890292 k 0.0017225329 m -3.7247728e-06\n",
      "7 Train Loss 10000.025 Test RE 0.9998745719113903 c 0.012903993 k 0.0022661479 m -5.5007677e-06\n",
      "8 Train Loss 10000.024 Test RE 0.9998692581077246 c 0.02565309 k 0.0045898343 m -1.3764925e-05\n",
      "9 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "10 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "11 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "12 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "13 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "14 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "15 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "16 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "17 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "18 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "19 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "20 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "21 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "22 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "23 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "24 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "25 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "26 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "27 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "28 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "29 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "30 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "31 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "32 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "33 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "34 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "35 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "36 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "37 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "38 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "39 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "40 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "41 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "42 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "43 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "44 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "45 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "46 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "47 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "48 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "49 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "50 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "51 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "52 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "53 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "54 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "55 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "56 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "57 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "58 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "59 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "60 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "61 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "62 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "63 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "64 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "65 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "66 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "67 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "68 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "69 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "70 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "71 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "72 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "73 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "74 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "75 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "76 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "77 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "78 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "79 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "80 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "81 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "82 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "83 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "84 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "85 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "86 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "87 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "88 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "89 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "90 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "91 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "92 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "93 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "94 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "95 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "96 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "97 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "98 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "99 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "100 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "101 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "102 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "103 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "104 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "105 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "106 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "107 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "108 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "109 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "110 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "111 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "112 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "113 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "114 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "115 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "116 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "117 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "118 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "119 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "120 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "121 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "122 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "123 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "124 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "125 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "126 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "127 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "128 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "129 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "130 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "131 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "132 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "133 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "134 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "135 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "136 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "137 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "138 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "139 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "140 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "141 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "142 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "143 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "144 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "145 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "146 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "147 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "148 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "149 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "150 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "151 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "152 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "153 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "154 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "155 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "156 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "157 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "158 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "159 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "160 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "161 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "162 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "163 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "164 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "165 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "166 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "167 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "168 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "169 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "170 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "171 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "172 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "173 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "174 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "175 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "176 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "177 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "178 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "179 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "180 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "181 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "182 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "183 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "184 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "185 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "186 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "187 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "188 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "189 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "190 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "191 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "192 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "193 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "194 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "195 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "196 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "197 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "198 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "199 Train Loss 10000.007 Test RE 0.9998701013978757 c 0.19058385 k 0.034883756 m -0.00012836244\n",
      "Training time: 32.43\n",
      "Training time: 32.43\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 200\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "m_full = []\n",
    "k_full = []\n",
    "c_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "\n",
    "for reps in range(max_reps):   \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []   \n",
    "    beta_val = []\n",
    "    m_val = []\n",
    "    k_val = []\n",
    "    c_val = []\n",
    "\n",
    "    torch.manual_seed(reps*36)\n",
    "    N_f = 10000 #Total number of collocation points\n",
    "\n",
    "    layers = np.array([1,50,50,50,1]) #9 hidden layers\n",
    "    PINN = Sequentialmodel(layers)\n",
    "\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.25, \n",
    "                                max_iter = 10, \n",
    "                                max_eval = 15, \n",
    "                                tolerance_grad = 1e-6, \n",
    "                                tolerance_change = 1e-6, \n",
    "                                history_size = 100, \n",
    "                                line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "    train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    beta_full.append(beta_val)\n",
    "    m_full.append(m_val)\n",
    "    k_full.append(k_val)\n",
    "    c_full.append(c_val)\n",
    "    \n",
    "\n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"beta\": beta_full,\"m\": m_full,\"k\": k_full,\"c\": c_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "pmHEeBpzfFQh",
    "outputId": "990b2054-f35e-4c8e-c378-69ff2eb19f4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f42ac265f10>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_pred = PINN.test()\n",
    "plt.plot(t,(x_true-x_mean)/x_std,'b')\n",
    "plt.plot(t,x_pred,'r--')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "stan_1D_FODE_tune.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
