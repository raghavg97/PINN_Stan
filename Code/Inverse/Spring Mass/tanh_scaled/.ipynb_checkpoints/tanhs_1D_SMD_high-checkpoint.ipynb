{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8byrnUmNKGR",
    "outputId": "510f67c6-b225-41ba-e78a-d0a6e54ded85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uoNYpDzlNKGV"
   },
   "outputs": [],
   "source": [
    "tstart= 0 \n",
    "tstop= 60 \n",
    "\n",
    "increment = 0.1 \n",
    "# Initial condition\n",
    "x_init= [0,0] \n",
    "\n",
    "t = np.arange(tstart,tstop+1,increment)\n",
    "\n",
    "c = 1 # Damping constant \n",
    "k = 0.01 # Stiffness of the spring\n",
    "m = 5 # Mass \n",
    "F0 = 100\n",
    "\n",
    "def mydiff(x, t):    \n",
    "    F =F0\n",
    "    \n",
    "    dx1dt = x[1] \n",
    "    dx2dt = (F -c*x[1] -k*x[0])/m\n",
    "    dxdt= [dx1dt, dx2dt] \n",
    "    \n",
    "    return dxdt \n",
    "\n",
    "x_full_sol = odeint(mydiff, x_init, t) \n",
    "\n",
    "x_sol = x_full_sol[:,0]\n",
    "v_sol = x_full_sol[:,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BR02v-fkNKGV"
   },
   "outputs": [],
   "source": [
    "level = \"high\"\n",
    "label = \"1D_SMD_tanhs_\" + level\n",
    "\n",
    "loss_thresh = 0.005\n",
    "\n",
    "t_test = t.reshape(-1,1)\n",
    "t_test_tensor = torch.from_numpy(t_test).float().to(device)\n",
    "\n",
    "# bc1_t = t[0].reshape(-1,1)\n",
    "# bc1_x = x_sol[0].reshape(-1,1)\n",
    "# t_bc1_train = torch.from_numpy(bc1_t).float().to(device)\n",
    "# x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "x_mean = np.mean(x_sol.reshape(-1,1))\n",
    "x_std  = np.mean(x_sol.reshape(-1,1))                \n",
    "bc1_t = t.reshape(-1,1)\n",
    "bc1_x = (x_sol-x_mean).reshape(-1,1)/x_std\n",
    "t_bc1_train = torch.from_numpy(bc1_t).float().to(device)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "\n",
    "\n",
    "bc2_t = t[0].reshape(-1,1)\n",
    "t_bc2_train = torch.from_numpy(bc2_t).float().to(device)\n",
    "bc2_val = v_sol[0].reshape(-1,1)\n",
    "bc2_val =torch.from_numpy(bc2_val).float().to(device)\n",
    "\n",
    "t_test = t.reshape(-1,1)\n",
    "t_test_tensor = torch.from_numpy(t_test).float().to(device)\n",
    "x_true = x_sol\n",
    "x_true_norm = np.linalg.norm(x_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(t[0]) \n",
    "ub = np.array(t[-1]) \n",
    "\n",
    "#torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SyyktBKBXRo1"
   },
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    \n",
    "    t01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=t01,random_state =seed)\n",
    "\n",
    "    t_coll = lb + (ub-lb)*sampling(N_f)\n",
    "    # t_coll = np.vstack((t_coll,)) # append training points to collocation points \n",
    "\n",
    "    return t_coll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "o1b21zLnNKGW"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "        \n",
    "        \n",
    "        self.k = Parameter(torch.tensor(0.0))\n",
    "        self.k.requiresGrad = True\n",
    "        self.c = Parameter(torch.tensor(0.0))\n",
    "        self.c.requiresGrad = True\n",
    "        self.m = Parameter(torch.tensor(0.0))\n",
    "        self.m.requiresGrad = True\n",
    "        \n",
    "    'forward pass'\n",
    "    def forward(self,t):\n",
    "        if torch.is_tensor(t) != True:         \n",
    "            t = torch.from_numpy(t)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        t = 2.0*(t - l_b)/(u_b - l_b) - 1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = t.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,t,x):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(t), x)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,t_bc2,bc2_val):\n",
    "        g = t_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        x = self.forward(g)    \n",
    "            \n",
    "        x_t = autograd.grad(x,g,torch.ones([t_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dx_dt = x_t[:,[0]]\n",
    "        \n",
    "        bc2 = dx_dt/x_std\n",
    "        \n",
    "        loss_bc2= self.loss_function(bc2,bc2_val)\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self,t_coll,f_hat):\n",
    "             \n",
    "        g = t_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        x = self.forward(g) \n",
    "\n",
    "        x_t = autograd.grad(x,g,torch.ones([t_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        x_tt = autograd.grad(x_t,g,torch.ones(t_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dx_dt = x_t[:,[0]]\n",
    "        \n",
    "        dx2_d2t = x_tt[:,[0]]\n",
    "        \n",
    "        f = self.m*dx2_d2t/x_std + self.c*dx_dt/x_std + self.k*x/x_std - F0\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,t_bc1,x_bc1,t_bc2,bc2_val,t_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(t_bc1,x_bc1)\n",
    "        loss_bc2 = self.loss_BC2(t_bc2,bc2_val)\n",
    "        loss_f = self.loss_PDE(t_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self):\n",
    "        x_pred = self.forward(t_test_tensor)\n",
    "        x_pred = x_pred.cpu().detach().numpy()\n",
    "        \n",
    "        x_pred = x_pred*x_std + x_mean\n",
    "\n",
    "        return x_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        x_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(x_pred.reshape(-1,1) - x_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(x_pred.reshape(-1,1) - x_true.reshape(-1,1),2)/x_true_norm\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fLY2mT5BOgjD"
   },
   "outputs": [],
   "source": [
    "def train_step(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8srA5uGuObil"
   },
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    m_val.append(PINN.m.cpu().detach().numpy())\n",
    "    k_val.append(PINN.k.cpu().detach().numpy())\n",
    "    c_val.append(PINN.c.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0SezTZ_racQB"
   },
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "\n",
    "    t_coll = colloc_pts(N_f,rep*11)\n",
    "    t_coll =  torch.from_numpy(t_coll).float().to(device)\n",
    "    \n",
    "    f_hat = torch.zeros(t_coll.shape[0],1).to(device)\n",
    "    for i in range(max_iter):      \n",
    "        train_step(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat)\n",
    "         \n",
    "        loss_np = PINN.loss(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test RE\",test_re_loss[-1],\"c\",PINN.c.cpu().detach().numpy(),\"k\",PINN.k.cpu().detach().numpy(),\"m\",PINN.m.cpu().detach().numpy())\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I9BYbcJ0NKGX",
    "outputId": "42720d97-b37a-4c42-b7e5-b77a0ef391f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 10000.002 Test RE 0.9998841351434371 c 0.0018465831 k 5.0723298e-05 m -4.7584173e-08\n",
      "1 Train Loss 10000.001 Test RE 0.9998814907101349 c 0.002492594 k 6.977424e-05 m -8.1778815e-08\n",
      "2 Train Loss 10000.001 Test RE 0.9998806628601482 c 0.0028288038 k 7.985038e-05 m -1.0300143e-07\n",
      "3 Train Loss 10000.001 Test RE 0.9998800319131486 c 0.0031735161 k 8.99505e-05 m -1.2709302e-07\n",
      "4 Train Loss 10000.001 Test RE 0.999879563105732 c 0.0035279214 k 0.000100534206 m -1.5438728e-07\n",
      "5 Train Loss 10000.001 Test RE 0.9998791971745585 c 0.0038951442 k 0.00011135861 m -1.8523826e-07\n",
      "6 Train Loss 10000.001 Test RE 0.9998789156588548 c 0.004281504 k 0.00012285999 m -2.2047718e-07\n",
      "7 Train Loss 10000.001 Test RE 0.9998786840325983 c 0.004698595 k 0.00013518127 m -2.6148308e-07\n",
      "8 Train Loss 10000.001 Test RE 0.9998784913148774 c 0.0051666303 k 0.00014907398 m -3.1085622e-07\n",
      "9 Train Loss 10000.001 Test RE 0.9998783153111742 c 0.0057194266 k 0.0001654073 m -3.730822e-07\n",
      "10 Train Loss 10000.001 Test RE 0.9998781488797599 c 0.00641137 k 0.00018589551 m -4.5585708e-07\n",
      "11 Train Loss 10000.001 Test RE 0.9998779756101064 c 0.00732684 k 0.00021292362 m -5.717592e-07\n",
      "12 Train Loss 10000.001 Test RE 0.999877791054333 c 0.008592897 k 0.00025033444 m -7.40884e-07\n",
      "13 Train Loss 10000.001 Test RE 0.999877580970309 c 0.010396736 k 0.00030353246 m -9.94467e-07\n",
      "14 Train Loss 10000.001 Test RE 0.9998773435658554 c 0.013011317 k 0.00038066818 m -1.3806305e-06\n",
      "15 Train Loss 10000.001 Test RE 0.9998770642757294 c 0.016831925 k 0.0004932301 m -1.9728434e-06\n",
      "16 Train Loss 10000.001 Test RE 0.9998767464593499 c 0.0224342 k 0.0006583391 m -2.8840657e-06\n",
      "17 Train Loss 10000.001 Test RE 0.9998763725042534 c 0.030665478 k 0.0009006789 m -4.2900215e-06\n",
      "18 Train Loss 10000.001 Test RE 0.9998759660673585 c 0.042815737 k 0.0012586911 m -6.4742835e-06\n",
      "19 Train Loss 10000.001 Test RE 0.9998754930996312 c 0.060931243 k 0.0017918149 m -9.918097e-06\n",
      "20 Train Loss 10000.001 Test RE 0.9998751384796021 c 0.088547096 k 0.0026070068 m -1.5526932e-05\n",
      "21 Train Loss 9999.998 Test RE 0.9998776855230107 c 0.27122325 k 0.008013397 m -5.9126436e-05\n",
      "22 Train Loss 9999.998 Test RE 0.999877609317377 c 0.2829994 k 0.00834945 m -6.3054606e-05\n",
      "23 Train Loss 9999.998 Test RE 0.999877541025864 c 0.28958663 k 0.008532861 m -6.606849e-05\n",
      "24 Train Loss 9999.998 Test RE 0.999877496952149 c 0.29418457 k 0.008656261 m -6.9089394e-05\n",
      "25 Train Loss 9999.998 Test RE 0.9998774711410783 c 0.29734322 k 0.008734747 m -7.2404815e-05\n",
      "26 Train Loss 9999.998 Test RE 0.9998774688632023 c 0.2992505 k 0.008772844 m -7.625725e-05\n",
      "27 Train Loss 9999.998 Test RE 0.9998774957671402 c 0.2998651 k 0.0087684775 m -8.0820646e-05\n",
      "28 Train Loss 9999.998 Test RE 0.9998775589627665 c 0.29916015 k 0.00872064 m -8.615045e-05\n",
      "29 Train Loss 9999.998 Test RE 0.9998776606246081 c 0.29729477 k 0.0086347535 m -9.21346e-05\n",
      "30 Train Loss 9999.998 Test RE 0.9998777949299054 c 0.29467964 k 0.008524245 m -9.856526e-05\n",
      "31 Train Loss 9999.998 Test RE 0.999877950389126 c 0.29180777 k 0.008404479 m -0.00010532585\n",
      "32 Train Loss 9999.998 Test RE 0.9998781172144392 c 0.28904223 k 0.008285677 m -0.0001125369\n",
      "33 Train Loss 9999.998 Test RE 0.9998782919320613 c 0.28657207 k 0.00817154 m -0.000120574885\n",
      "34 Train Loss 9999.998 Test RE 0.9998784763025239 c 0.28450215 k 0.008062083 m -0.00013004764\n",
      "35 Train Loss 9999.998 Test RE 0.9998786750273849 c 0.2829589 k 0.007956631 m -0.00014182668\n",
      "36 Train Loss 9999.998 Test RE 0.9998788946045762 c 0.28216425 k 0.007855365 m -0.00015717438\n",
      "37 Train Loss 9999.998 Test RE 0.9998791428560547 c 0.28250423 k 0.007760212 m -0.00017797016\n",
      "38 Train Loss 9999.998 Test RE 0.9998794291518556 c 0.28462732 k 0.0076758997 m -0.00020707546\n",
      "39 Train Loss 9999.998 Test RE 0.9998797643841487 c 0.28959048 k 0.007611347 m -0.00024891124\n",
      "40 Train Loss 9999.998 Test RE 0.9998801605650193 c 0.29909176 k 0.0075817388 m -0.00031040155\n",
      "41 Train Loss 9999.998 Test RE 0.9998806294463449 c 0.31585014 k 0.0076115965 m -0.00040257408\n",
      "42 Train Loss 9999.998 Test RE 0.9998811778711665 c 0.34425837 k 0.007739053 m -0.0005434976\n",
      "43 Train Loss 9999.998 Test RE 0.9998817948634171 c 0.3917188 k 0.008022831 m -0.00076462957\n",
      "44 Train Loss 9999.998 Test RE 0.99988240674721 c 0.47266352 k 0.008560765 m -0.0011304149\n",
      "45 Train Loss 9999.998 Test RE 0.9998825888107299 c 0.6338913 k 0.00962641 m -0.001858297\n",
      "46 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "47 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "48 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "49 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "50 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "51 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "52 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "53 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "54 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "55 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "56 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "57 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "58 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "59 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "60 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "61 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "62 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "63 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "64 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "65 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "66 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "67 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "68 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "69 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "70 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "71 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "72 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "73 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "74 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "75 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "76 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "77 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "78 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "79 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "80 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "81 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "82 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "83 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "84 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "85 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "86 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "87 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "88 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "89 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "90 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "91 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "92 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "93 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "94 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "95 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "96 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "97 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "98 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "99 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "100 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "101 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "102 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "103 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "104 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "105 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "106 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "107 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "108 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "109 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "110 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "111 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "112 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "113 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "114 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "115 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "116 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "117 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "118 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "119 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "120 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "121 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "122 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "123 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "124 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "125 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "126 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "127 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "128 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "129 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "130 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "131 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "132 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "133 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "134 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "135 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "136 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "137 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "138 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "139 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "140 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "141 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "142 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "143 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "144 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "145 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "146 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "147 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "148 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "149 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "150 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "151 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "152 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "153 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "154 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "155 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "156 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "157 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "158 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "159 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "160 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "161 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "162 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "163 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "164 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "165 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "166 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "167 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "168 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "169 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "170 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "171 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "172 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "173 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "174 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "175 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "176 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "177 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "178 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "179 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "180 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "181 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "182 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "183 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "184 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "185 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "186 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "187 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "188 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "189 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "190 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "191 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "192 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "193 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "194 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "195 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "196 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "197 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "198 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "199 Train Loss 9999.996 Test RE 0.9998747281997119 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "Training time: 32.93\n",
      "Training time: 32.93\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 10000.002 Test RE 0.9998833969694011 c 0.0019094152 k 8.793648e-05 m -1.5140024e-07\n",
      "1 Train Loss 10000.001 Test RE 0.9998810278033191 c 0.0026081647 k 0.00011976499 m -2.5651232e-07\n",
      "2 Train Loss 10000.001 Test RE 0.9998802645588954 c 0.0029741647 k 0.00013642447 m -3.2177e-07\n",
      "3 Train Loss 10000.001 Test RE 0.9998796797770256 c 0.0033550053 k 0.00015370721 m -3.9688956e-07\n",
      "4 Train Loss 10000.001 Test RE 0.9998792228091092 c 0.003757481 k 0.00017192937 m -4.839666e-07\n",
      "5 Train Loss 10000.001 Test RE 0.9998788535543914 c 0.0041940096 k 0.0001916175 m -5.8678e-07\n",
      "6 Train Loss 10000.001 Test RE 0.9998785420462831 c 0.004686107 k 0.00021373206 m -7.121278e-07\n",
      "7 Train Loss 10000.001 Test RE 0.9998782636064243 c 0.005269429 k 0.00023983375 m -8.719022e-07\n",
      "8 Train Loss 10000.001 Test RE 0.999877999246443 c 0.0060006604 k 0.00027241866 m -1.0862444e-06\n",
      "9 Train Loss 10000.001 Test RE 0.9998777333465114 c 0.006966353 k 0.0003152603 m -1.3880601e-06\n",
      "10 Train Loss 10000.001 Test RE 0.9998774551247186 c 0.008293874 k 0.00037390165 m -1.8294138e-06\n",
      "11 Train Loss 10000.001 Test RE 0.9998771576252147 c 0.010164665 k 0.00045617076 m -2.4906221e-06\n",
      "12 Train Loss 10000.001 Test RE 0.9998768415394258 c 0.012830941 k 0.00057291123 m -3.4940997e-06\n",
      "13 Train Loss 10000.001 Test RE 0.9998765147857233 c 0.016635837 k 0.00073870417 m -5.0269155e-06\n",
      "14 Train Loss 10000.001 Test RE 0.9998762065303084 c 0.022037366 k 0.0009728987 m -7.3840033e-06\n",
      "15 Train Loss 10000.001 Test RE 0.9998759499424478 c 0.029622415 k 0.0012989808 m -1.1073371e-05\n",
      "16 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "17 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "18 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "19 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "20 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "21 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "22 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "23 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "24 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "25 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "26 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "27 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "28 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "29 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "30 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "31 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "32 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "33 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "34 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "35 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "36 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "37 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "38 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "39 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "40 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "41 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "42 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "43 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "44 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "45 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "46 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "47 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "48 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "49 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "50 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "51 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "52 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "53 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "54 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "55 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "56 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "57 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "58 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "59 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "60 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "61 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "62 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "63 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "64 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "65 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "66 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "67 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "68 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "69 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "70 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "71 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "72 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "73 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "74 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "75 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "76 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "77 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "78 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "79 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "80 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "81 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "82 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "83 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "84 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "85 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "86 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "87 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "88 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "89 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "90 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "91 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "92 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "93 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "94 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "95 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "96 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "97 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "98 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "99 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "100 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "101 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "102 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "103 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "104 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "105 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "106 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "107 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "108 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "109 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "110 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "111 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "112 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "113 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "114 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "115 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "116 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "117 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "118 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "119 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "120 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "121 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "122 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "123 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "124 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "125 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "126 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "127 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "128 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "129 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "130 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "131 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "132 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "133 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "134 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "135 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "136 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "137 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "138 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "139 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "140 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "141 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "142 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "143 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "144 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "145 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "146 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "147 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "148 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "149 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "150 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "151 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "152 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "153 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "154 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "155 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "156 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "157 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "158 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "159 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "160 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "161 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "162 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "163 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "164 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "165 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "166 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "167 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "168 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "169 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "170 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "171 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "172 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "173 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "174 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "175 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "176 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "177 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "178 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "179 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "180 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "181 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "182 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "183 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "184 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "185 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "186 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "187 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "188 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "189 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "190 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "191 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "192 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "193 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "194 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "195 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "196 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "197 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "198 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "199 Train Loss 10000.0 Test RE 0.999876049198799 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "Training time: 29.79\n",
      "Training time: 29.79\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 10000.001 Test RE 0.999882966231496 c 0.0018588288 k 3.3221968e-05 m -3.0226463e-08\n",
      "1 Train Loss 10000.001 Test RE 0.9998816703285175 c 0.0021165868 k 3.6812336e-05 m -3.8685236e-08\n",
      "2 Train Loss 10000.001 Test RE 0.9998809384188508 c 0.0023804007 k 4.380872e-05 m -4.893828e-08\n",
      "3 Train Loss 10000.001 Test RE 0.9998801390508517 c 0.002649798 k 4.7412057e-05 m -5.994526e-08\n",
      "4 Train Loss 10000.001 Test RE 0.9998797885021151 c 0.0029243813 k 5.4691744e-05 m -7.286469e-08\n",
      "5 Train Loss 10000.001 Test RE 0.9998792943203991 c 0.003203545 k 5.884521e-05 m -8.661634e-08\n",
      "6 Train Loss 10000.001 Test RE 0.9998790954087919 c 0.0034873253 k 6.5622495e-05 m -1.02176955e-07\n",
      "7 Train Loss 10000.001 Test RE 0.9998788226866718 c 0.0037802174 k 7.087764e-05 m -1.1918543e-07\n",
      "8 Train Loss 10000.001 Test RE 0.9998786802880871 c 0.0040936423 k 7.760315e-05 m -1.3890022e-07\n",
      "9 Train Loss 10000.001 Test RE 0.9998785137192101 c 0.0044471193 k 8.450785e-05 m -1.6253162e-07\n",
      "10 Train Loss 10000.001 Test RE 0.9998783925015587 c 0.0048711523 k 9.322328e-05 m -1.9274182e-07\n",
      "11 Train Loss 10000.001 Test RE 0.999878254401611 c 0.005412899 k 0.000104069644 m -2.335496e-07\n",
      "12 Train Loss 10000.001 Test RE 0.9998781242254805 c 0.0061444137 k 0.00011891498 m -2.9169385e-07\n",
      "13 Train Loss 10000.001 Test RE 0.9998779707178412 c 0.007173498 k 0.00013964949 m -3.7766662e-07\n",
      "14 Train Loss 10000.001 Test RE 0.9998778035799778 c 0.00865962 k 0.00016970199 m -5.078962e-07\n",
      "15 Train Loss 10000.001 Test RE 0.9998776024900852 c 0.010836809 k 0.00021363968 m -7.075998e-07\n",
      "16 Train Loss 10000.001 Test RE 0.9998773701160093 c 0.014049548 k 0.00027854863 m -1.0156614e-06\n",
      "17 Train Loss 10000.001 Test RE 0.9998770896667362 c 0.018807324 k 0.00037462488 m -1.4920902e-06\n",
      "18 Train Loss 10000.001 Test RE 0.9998767589746117 c 0.025881313 k 0.00051755947 m -2.2314425e-06\n",
      "19 Train Loss 10000.001 Test RE 0.9998763608076682 c 0.03648115 k 0.0007317706 m -3.3875222e-06\n",
      "20 Train Loss 10000.001 Test RE 0.9998758885775839 c 0.05265356 k 0.0010588238 m -5.228415e-06\n",
      "21 Train Loss 10000.001 Test RE 0.9998753235841465 c 0.078338824 k 0.0015785911 m -8.281385e-06\n",
      "22 Train Loss 10000.001 Test RE 0.9998746849378802 c 0.12318186 k 0.0024869526 m -1.3852306e-05\n",
      "23 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "24 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "25 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "26 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "27 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "28 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "29 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "30 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "31 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "32 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "33 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "34 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "35 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "36 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "37 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "38 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "39 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "40 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "41 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "42 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "43 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "44 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "45 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "46 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "47 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "48 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "49 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "50 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "51 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "52 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "53 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "54 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "55 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "56 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "57 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "58 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "59 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "60 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "61 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "62 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "63 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "64 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "65 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "66 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "67 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "68 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "69 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "70 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "71 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "72 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "73 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "74 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "75 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "76 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "77 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "78 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "79 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "80 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "81 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "82 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "83 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "84 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "85 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "86 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "87 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "88 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "89 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "90 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "91 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "92 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "93 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "94 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "95 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "96 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "97 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "98 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "99 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "100 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "101 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "102 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "103 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "104 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "105 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "106 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "107 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "108 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "109 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "110 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "111 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "112 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "113 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "114 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "115 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "116 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "117 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "118 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "119 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "120 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "121 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "122 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "123 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "124 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "125 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "126 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "127 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "128 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "129 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "130 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "131 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "132 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "133 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "134 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "135 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "136 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "137 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "138 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "139 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "140 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "141 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "142 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "143 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "144 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "145 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "146 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "147 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "148 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "149 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "150 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "151 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "152 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "153 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "154 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "155 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "156 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "157 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "158 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "159 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "160 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "161 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "162 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "163 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "164 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "165 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "166 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "167 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "168 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "169 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "170 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "171 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "172 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "173 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "174 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "175 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "176 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "177 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "178 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "179 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "180 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "181 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "182 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "183 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "184 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "185 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "186 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "187 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "188 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "189 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "190 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "191 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "192 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "193 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "194 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "195 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "196 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "197 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "198 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "199 Train Loss 9999.998 Test RE 0.9998763888949246 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "Training time: 29.23\n",
      "Training time: 29.23\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 10000.002 Test RE 0.9998841387447447 c 0.0017812919 k 3.624816e-05 m -2.7908907e-08\n",
      "1 Train Loss 10000.001 Test RE 0.9998815872040778 c 0.0024545682 k 5.0764997e-05 m -4.9285294e-08\n",
      "2 Train Loss 10000.001 Test RE 0.9998805476040123 c 0.0028033191 k 5.4629334e-05 m -6.192843e-08\n",
      "3 Train Loss 10000.001 Test RE 0.9998801386742788 c 0.0031581037 k 6.540994e-05 m -7.747626e-08\n",
      "4 Train Loss 10000.001 Test RE 0.9998795136285372 c 0.0035157222 k 7.043241e-05 m -9.364552e-08\n",
      "5 Train Loss 10000.001 Test RE 0.999879270033887 c 0.0038748407 k 7.965824e-05 m -1.1216984e-07\n",
      "6 Train Loss 10000.001 Test RE 0.9998789497636001 c 0.004242061 k 8.652567e-05 m -1.323007e-07\n",
      "7 Train Loss 10000.001 Test RE 0.999878777154779 c 0.0046313326 k 9.52923e-05 m -1.5566789e-07\n",
      "8 Train Loss 10000.001 Test RE 0.9998785900805379 c 0.0050631664 k 0.00010416951 m -1.8339969e-07\n",
      "9 Train Loss 10000.001 Test RE 0.9998784525224023 c 0.0055685462 k 0.00011506124 m -2.182149e-07\n",
      "10 Train Loss 10000.001 Test RE 0.9998783062476123 c 0.0061960467 k 0.00012827401 m -2.6417212e-07\n",
      "11 Train Loss 10000.001 Test RE 0.999878169270909 c 0.0070214304 k 0.00014585583 m -3.2827256e-07\n",
      "12 Train Loss 10000.001 Test RE 0.9998780146967237 c 0.008159831 k 0.00016996484 m -4.215962e-07\n",
      "13 Train Loss 10000.001 Test RE 0.999877846821187 c 0.00978328 k 0.00020444453 m -5.617135e-07\n",
      "14 Train Loss 10000.001 Test RE 0.9998776488976844 c 0.012145724 k 0.00025454487 m -7.7583115e-07\n",
      "15 Train Loss 10000.001 Test RE 0.9998774195421267 c 0.015620924 k 0.0003283027 m -1.1060032e-06\n",
      "16 Train Loss 10000.001 Test RE 0.9998771449706932 c 0.02076127 k 0.00043736564 m -1.6172087e-06\n",
      "17 Train Loss 10000.001 Test RE 0.9998768197659567 c 0.0283956 k 0.00059940194 m -2.4111464e-06\n",
      "18 Train Loss 10000.001 Test RE 0.9998764298495993 c 0.039802857 k 0.0008415297 m -3.6508336e-06\n",
      "19 Train Loss 10000.001 Test RE 0.9998759664220972 c 0.05707471 k 0.0012082364 m -5.611535e-06\n",
      "20 Train Loss 10000.001 Test RE 0.9998754164901259 c 0.08400501 k 0.0017800634 m -8.804209e-06\n",
      "21 Train Loss 10000.001 Test RE 0.9998747921202724 c 0.12890157 k 0.0027333489 m -1.4361476e-05\n",
      "22 Train Loss 9999.999 Test RE 0.9998814234651875 c 0.54634106 k 0.011571601 m -7.0161455e-05\n",
      "23 Train Loss 9999.999 Test RE 0.9998796530572173 c 0.5368985 k 0.011352279 m -6.935394e-05\n",
      "24 Train Loss 9999.999 Test RE 0.999879117245564 c 0.55109054 k 0.011633714 m -7.192315e-05\n",
      "25 Train Loss 9999.999 Test RE 0.9998785343412724 c 0.5506494 k 0.011614121 m -7.272056e-05\n",
      "26 Train Loss 9999.999 Test RE 0.9998781880275833 c 0.5523275 k 0.011640711 m -7.393638e-05\n",
      "27 Train Loss 9999.999 Test RE 0.9998778959788982 c 0.55246145 k 0.011635694 m -7.517593e-05\n",
      "28 Train Loss 9999.999 Test RE 0.9998776590054019 c 0.5520217 k 0.01161876 m -7.66358e-05\n",
      "29 Train Loss 9999.999 Test RE 0.9998774584258355 c 0.5508018 k 0.0115851145 m -7.8397585e-05\n",
      "30 Train Loss 9999.999 Test RE 0.9998772896412849 c 0.5487481 k 0.011533142 m -8.0612605e-05\n",
      "31 Train Loss 9999.999 Test RE 0.9998771539495153 c 0.5457211 k 0.011459453 m -8.3464176e-05\n",
      "32 Train Loss 9999.999 Test RE 0.9998770594918382 c 0.54162997 k 0.011361716 m -8.716668e-05\n",
      "33 Train Loss 9999.999 Test RE 0.9998770184578973 c 0.5365296 k 0.011240727 m -9.195329e-05\n",
      "34 Train Loss 9999.999 Test RE 0.9998770430722242 c 0.5307249 k 0.011102505 m -9.809366e-05\n",
      "35 Train Loss 9999.999 Test RE 0.9998771413740734 c 0.5247624 k 0.010957864 m -0.00010596947\n",
      "36 Train Loss 9999.999 Test RE 0.9998773168217957 c 0.5192415 k 0.010818385 m -0.000116061245\n",
      "37 Train Loss 9999.998 Test RE 0.9998778836347294 c 0.5112576 k 0.010589993 m -0.00014339431\n",
      "38 Train Loss 9999.998 Test RE 0.9998782468833105 c 0.50949407 k 0.010516709 m -0.00016011827\n",
      "39 Train Loss 9999.998 Test RE 0.9998786454193394 c 0.50948477 k 0.010476103 m -0.00017889445\n",
      "40 Train Loss 9999.998 Test RE 0.9998790744134444 c 0.51138794 k 0.010469976 m -0.00020033779\n",
      "41 Train Loss 9999.998 Test RE 0.9998795355090302 c 0.51547927 k 0.010501408 m -0.00022559626\n",
      "42 Train Loss 9999.998 Test RE 0.9998800357229735 c 0.5222655 k 0.010576848 m -0.00025647596\n",
      "43 Train Loss 9999.998 Test RE 0.999880586719573 c 0.53261167 k 0.010708194 m -0.0002956908\n",
      "44 Train Loss 9999.998 Test RE 0.9998812044077174 c 0.54790926 k 0.010915366 m -0.0003472693\n",
      "45 Train Loss 9999.998 Test RE 0.9998819092206436 c 0.5703267 k 0.011230088 m -0.00041718583\n",
      "46 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "47 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "48 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "49 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "50 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "51 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "52 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "53 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "54 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "55 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "56 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "57 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "58 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "59 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "60 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "61 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "62 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "63 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "64 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "65 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "66 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "67 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "68 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "69 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "70 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "71 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "72 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "73 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "74 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "75 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "76 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "77 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "78 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "79 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "80 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "81 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "82 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "83 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "84 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "85 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "86 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "87 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "88 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "89 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "90 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "91 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "92 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "93 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "94 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "95 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "96 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "97 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "98 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "99 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "100 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "101 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "102 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "103 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "104 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "105 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "106 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "107 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "108 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "109 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "110 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "111 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "112 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "113 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "114 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "115 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "116 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "117 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "118 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "119 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "120 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "121 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "122 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "123 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "124 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "125 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "126 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "127 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "128 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "129 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "130 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "131 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "132 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "133 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "134 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "135 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "136 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "137 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "138 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "139 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "140 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "141 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "142 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "143 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "144 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "145 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "146 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "147 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "148 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "149 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "150 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "151 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "152 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "153 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "154 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "155 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "156 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "157 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "158 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "159 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "160 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "161 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "162 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "163 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "164 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "165 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "166 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "167 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "168 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "169 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "170 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "171 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "172 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "173 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "174 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "175 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "176 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "177 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "178 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "179 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "180 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "181 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "182 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "183 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "184 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "185 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "186 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "187 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "188 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "189 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "190 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "191 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "192 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "193 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "194 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "195 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "196 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "197 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "198 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "199 Train Loss 9999.998 Test RE 0.999882726838067 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "Training time: 30.77\n",
      "Training time: 30.77\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 10000.001 Test RE 0.9998833441154212 c 0.0015313121 k 3.6189547e-05 m -3.7155708e-08\n",
      "1 Train Loss 10000.001 Test RE 0.9998820523824269 c 0.0017426298 k 4.1612668e-05 m -4.703589e-08\n",
      "2 Train Loss 10000.001 Test RE 0.9998810931637419 c 0.00195827 k 4.716708e-05 m -5.825755e-08\n",
      "3 Train Loss 10000.001 Test RE 0.9998803766866957 c 0.0021780634 k 5.2839212e-05 m -7.087089e-08\n",
      "4 Train Loss 10000.001 Test RE 0.9998798381596706 c 0.002402285 k 5.8633483e-05 m -8.495161e-08\n",
      "5 Train Loss 10000.001 Test RE 0.9998794297845711 c 0.002631932 k 6.4571344e-05 m -1.0062737e-07\n",
      "6 Train Loss 10000.001 Test RE 0.9998791160399684 c 0.0028691972 k 7.070756e-05 m -1.1812856e-07\n",
      "7 Train Loss 10000.001 Test RE 0.9998788699374553 c 0.0031182775 k 7.714812e-05 m -1.3787741e-07\n",
      "8 Train Loss 10000.001 Test RE 0.999878670771595 c 0.0033867364 k 8.408776e-05 m -1.6064664e-07\n",
      "9 Train Loss 10000.001 Test RE 0.9998785020099394 c 0.0036876295 k 9.186111e-05 m -1.8782345e-07\n",
      "10 Train Loss 10000.001 Test RE 0.9998783503007981 c 0.0040427083 k 0.0001010287 m -2.2183436e-07\n",
      "11 Train Loss 10000.001 Test RE 0.9998782044494473 c 0.004486885 k 0.000112488706 m -2.667817e-07\n",
      "12 Train Loss 10000.001 Test RE 0.9998780548266802 c 0.005074274 k 0.00012763248 m -3.2936833e-07\n",
      "13 Train Loss 10000.001 Test RE 0.9998778930224193 c 0.0058862884 k 0.00014855224 m -4.2021765e-07\n",
      "14 Train Loss 10000.001 Test RE 0.999877711356477 c 0.007042992 k 0.00017833017 m -5.558109e-07\n",
      "15 Train Loss 10000.001 Test RE 0.9998775028762308 c 0.0087191155 k 0.00022145033 m -7.613542e-07\n",
      "16 Train Loss 10000.001 Test RE 0.9998772611152948 c 0.011167963 k 0.0002844087 m -1.0752127e-06\n",
      "17 Train Loss 10000.001 Test RE 0.9998769804216829 c 0.014756171 k 0.00037660397 m -1.5557305e-06\n",
      "18 Train Loss 10000.001 Test RE 0.9998766567754989 c 0.020019813 k 0.00051177567 m -2.2926304e-06\n",
      "19 Train Loss 10000.001 Test RE 0.9998762894485235 c 0.027758956 k 0.0007104245 m -3.4271393e-06\n",
      "20 Train Loss 10000.001 Test RE 0.9998758858946283 c 0.03921392 k 0.001004328 m -5.1914412e-06\n",
      "21 Train Loss 10000.001 Test RE 0.9998754743226229 c 0.056415103 k 0.0014454725 m -7.994831e-06\n",
      "22 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "23 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "24 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "25 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "26 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "27 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "28 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "29 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "30 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "31 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "32 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "33 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "34 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "35 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "36 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "37 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "38 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "39 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "40 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "41 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "42 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "43 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "44 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "45 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "46 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "47 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "48 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "49 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "50 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "51 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "52 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "53 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "54 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "55 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "56 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "57 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "58 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "59 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "60 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "61 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "62 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "63 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "64 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "65 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "66 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "67 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "68 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "69 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "70 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "71 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "72 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "73 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "74 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "75 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "76 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "77 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "78 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "79 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "80 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "81 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "82 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "83 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "84 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "85 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "86 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "87 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "88 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "89 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "90 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "91 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "92 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "93 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "94 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "95 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "96 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "97 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "98 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "99 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "100 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "101 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "102 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "103 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "104 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "105 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "106 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "107 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "108 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "109 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "110 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "111 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "112 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "113 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "114 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "115 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "116 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "117 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "118 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "119 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "120 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "121 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "122 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "123 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "124 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "125 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "126 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "127 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "128 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "129 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "130 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "131 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "132 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "133 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "134 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "135 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "136 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "137 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "138 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "139 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "140 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "141 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "142 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "143 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "144 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "145 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "146 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "147 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "148 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "149 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "150 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "151 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "152 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "153 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "154 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "155 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "156 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "157 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "158 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "159 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "160 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "161 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "162 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "163 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "164 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "165 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "166 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "167 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "168 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "169 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "170 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "171 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "172 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "173 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "174 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "175 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "176 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "177 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "178 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "179 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "180 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "181 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "182 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "183 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "184 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "185 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "186 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "187 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "188 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "189 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "190 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "191 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "192 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "193 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "194 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "195 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "196 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "197 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "198 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "199 Train Loss 10000.0 Test RE 0.9998751219547777 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "Training time: 41.49\n",
      "Training time: 41.49\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 10000.002 Test RE 0.999883729138155 c 0.0016939694 k 5.3910724e-05 m -5.904769e-08\n",
      "1 Train Loss 10000.001 Test RE 0.999881271521771 c 0.002277266 k 7.3177165e-05 m -9.859469e-08\n",
      "2 Train Loss 10000.001 Test RE 0.9998804907594224 c 0.0025821938 k 8.327509e-05 m -1.2299842e-07\n",
      "3 Train Loss 10000.001 Test RE 0.999879903203921 c 0.0028958975 k 9.3669485e-05 m -1.5073876e-07\n",
      "4 Train Loss 10000.001 Test RE 0.9998794560161465 c 0.00322001 k 0.00010440656 m -1.8215961e-07\n",
      "5 Train Loss 10000.001 Test RE 0.9998791099615608 c 0.003558427 k 0.0001156108 m -2.1788006e-07\n",
      "6 Train Loss 10000.001 Test RE 0.9998788351941714 c 0.0039187702 k 0.00012752981 m -2.5903682e-07\n",
      "7 Train Loss 10000.001 Test RE 0.9998786086073127 c 0.0043147164 k 0.00014061172 m -3.0769743e-07\n",
      "8 Train Loss 10000.001 Test RE 0.9998784117088262 c 0.0047696726 k 0.00015562373 m -3.6755011e-07\n",
      "9 Train Loss 10000.001 Test RE 0.9998782293900041 c 0.005322049 k 0.00017382592 m -4.4497568e-07\n",
      "10 Train Loss 10000.001 Test RE 0.9998780489538984 c 0.0060323556 k 0.00019719996 m -5.5061736e-07\n",
      "11 Train Loss 10000.001 Test RE 0.9998778596568717 c 0.006992726 k 0.00022876082 m -7.016501e-07\n",
      "12 Train Loss 10000.001 Test RE 0.999877652122844 c 0.008339414 k 0.00027295874 m -9.24999e-07\n",
      "13 Train Loss 10000.001 Test RE 0.9998774185891706 c 0.0102696745 k 0.0003362328 m -1.2619979e-06\n",
      "14 Train Loss 10000.001 Test RE 0.9998771525787079 c 0.013065199 k 0.0004277653 m -1.7753165e-06\n",
      "15 Train Loss 10000.001 Test RE 0.9998768501452885 c 0.017123608 k 0.0005605152 m -2.559248e-06\n",
      "16 Train Loss 10000.001 Test RE 0.9998765106256355 c 0.023003288 k 0.00075266825 m -3.7559353e-06\n",
      "17 Train Loss 10000.001 Test RE 0.9998761407774068 c 0.031487014 k 0.0010297329 m -5.5821943e-06\n",
      "18 Train Loss 10000.001 Test RE 0.9998757589931007 c 0.0436727 k 0.0014274649 m -8.378164e-06\n",
      "19 Train Loss 10000.001 Test RE 0.999875414033429 c 0.061073437 k 0.001995185 m -1.27063695e-05\n",
      "20 Train Loss 10000.0 Test RE 0.9998758304937111 c 0.118767425 k 0.0038758651 m -3.352382e-05\n",
      "21 Train Loss 10000.0 Test RE 0.999873338265141 c 0.18470317 k 0.005944895 m -6.254701e-05\n",
      "22 Train Loss 10000.0 Test RE 0.999874471470491 c 0.20041779 k 0.006463674 m -7.035524e-05\n",
      "23 Train Loss 10000.0 Test RE 0.9998753513156752 c 0.20872323 k 0.0067384355 m -7.521073e-05\n",
      "24 Train Loss 9999.998 Test RE 0.9998765633184177 c 0.21839443 k 0.007052445 m -8.243789e-05\n",
      "25 Train Loss 9999.998 Test RE 0.9998770017137975 c 0.22093263 k 0.007131406 m -8.530831e-05\n",
      "26 Train Loss 9999.998 Test RE 0.9998773718972849 c 0.22251666 k 0.007177401 m -8.797688e-05\n",
      "27 Train Loss 9999.998 Test RE 0.9998776947228826 c 0.22339104 k 0.0071982527 m -9.061649e-05\n",
      "28 Train Loss 9999.998 Test RE 0.9998779836884892 c 0.22366977 k 0.007197355 m -9.336274e-05\n",
      "29 Train Loss 9999.998 Test RE 0.9998782451331537 c 0.22342522 k 0.0071765953 m -9.633952e-05\n",
      "30 Train Loss 9999.998 Test RE 0.9998784796493875 c 0.2227124 k 0.0071372497 m -9.9652105e-05\n",
      "31 Train Loss 9999.998 Test RE 0.9998786840142989 c 0.2216163 k 0.007081575 m -0.00010339904\n",
      "32 Train Loss 9999.998 Test RE 0.9998788545426399 c 0.22026533 k 0.0070131468 m -0.000107702355\n",
      "33 Train Loss 9999.998 Test RE 0.9998789899848425 c 0.21882173 k 0.0069361622 m -0.00011277781\n",
      "34 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "35 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "36 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "37 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "38 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "39 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "40 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "41 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "42 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "43 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "44 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "45 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "46 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "47 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "48 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "49 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "50 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "51 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "52 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "53 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "54 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "55 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "56 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "57 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "58 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "59 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "60 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "61 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "62 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "63 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "64 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "65 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "66 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "67 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "68 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "69 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "70 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "71 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "72 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "73 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "74 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "75 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "76 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "77 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "78 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "79 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "80 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "81 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "82 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "83 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "84 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "85 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "86 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "87 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "88 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "89 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "90 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "91 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "92 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "93 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "94 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "95 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "96 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "97 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "98 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "99 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "100 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "101 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "102 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "103 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "104 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "105 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "106 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "107 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "108 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "109 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "110 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "111 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "112 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "113 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "114 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "115 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "116 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "117 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "118 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "119 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "120 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "121 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "122 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "123 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "124 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "125 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "126 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "127 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "128 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "129 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "130 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "131 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "132 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "133 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "134 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "135 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "136 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "137 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "138 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "139 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "140 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "141 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "142 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "143 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "144 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "145 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "146 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "147 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "148 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "149 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "150 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "151 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "152 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "153 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "154 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "155 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "156 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "157 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "158 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "159 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "160 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "161 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "162 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "163 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "164 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "165 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "166 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "167 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "168 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "169 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "170 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "171 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "172 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "173 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "174 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "175 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "176 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "177 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "178 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "179 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "180 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "181 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "182 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "183 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "184 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "185 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "186 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "187 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "188 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "189 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "190 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "191 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "192 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "193 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "194 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "195 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "196 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "197 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "198 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "199 Train Loss 9999.998 Test RE 0.999879092470108 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "Training time: 22.28\n",
      "Training time: 22.28\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 10000.001 Test RE 0.9998818723886104 c 0.0020889852 k 5.635547e-05 m -9.5533125e-08\n",
      "1 Train Loss 10000.001 Test RE 0.9998809459948421 c 0.0023705827 k 6.4586355e-05 m -1.2087571e-07\n",
      "2 Train Loss 10000.001 Test RE 0.9998802512737185 c 0.0026584791 k 7.301824e-05 m -1.497417e-07\n",
      "3 Train Loss 10000.001 Test RE 0.999879725077511 c 0.002953941 k 8.167371e-05 m -1.824324e-07\n",
      "4 Train Loss 10000.001 Test RE 0.9998793211276443 c 0.0032598304 k 9.063414e-05 m -2.1947837e-07\n",
      "5 Train Loss 10000.001 Test RE 0.9998790043019432 c 0.0035816808 k 0.000100054654 m -2.618426e-07\n",
      "6 Train Loss 10000.001 Test RE 0.9998787477609973 c 0.0039294474 k 0.000110222936 m -3.1128098e-07\n",
      "7 Train Loss 10000.001 Test RE 0.9998785302478518 c 0.004320354 k 0.00012163706 m -3.7095865e-07\n",
      "8 Train Loss 10000.001 Test RE 0.9998783346269263 c 0.0047829705 k 0.00013512501 m -4.464148e-07\n",
      "9 Train Loss 10000.001 Test RE 0.9998781466773353 c 0.0053630844 k 0.00015201245 m -5.4705123e-07\n",
      "10 Train Loss 10000.001 Test RE 0.9998779544284081 c 0.006131343 k 0.00017434188 m -6.882591e-07\n",
      "11 Train Loss 10000.001 Test RE 0.9998777477129819 c 0.0071937204 k 0.0002051731 m -8.945285e-07\n",
      "12 Train Loss 10000.001 Test RE 0.9998775179320014 c 0.008705523 k 0.00024898277 m -1.2039419e-06\n",
      "13 Train Loss 10000.001 Test RE 0.9998772582396157 c 0.010892307 k 0.000312268 m -1.6751846e-06\n",
      "14 Train Loss 10000.001 Test RE 0.9998769638520701 c 0.014081225 k 0.0004044427 m -2.3987313e-06\n",
      "15 Train Loss 10000.001 Test RE 0.9998766337595918 c 0.018752884 k 0.00053933606 m -3.5163368e-06\n",
      "16 Train Loss 10000.001 Test RE 0.9998762738767414 c 0.025634896 k 0.0007378794 m -5.2584396e-06\n",
      "17 Train Loss 10000.001 Test RE 0.9998759066355766 c 0.035883736 k 0.0010333372 m -8.024879e-06\n",
      "18 Train Loss 10000.0 Test RE 0.9998755290514657 c 0.07547001 k 0.0021711898 m -2.083035e-05\n",
      "19 Train Loss 10000.0 Test RE 0.9998737147467454 c 0.121775806 k 0.0032637483 m -9.7541255e-05\n",
      "20 Train Loss 10000.0 Test RE 0.9998754899694612 c 0.14992793 k 0.004044523 m -0.00012045853\n",
      "21 Train Loss 10000.0 Test RE 0.9998763561383146 c 0.15831932 k 0.004281771 m -0.00012677416\n",
      "22 Train Loss 10000.0 Test RE 0.9998768958280345 c 0.16294505 k 0.004407737 m -0.00013176817\n",
      "23 Train Loss 10000.0 Test RE 0.9998773110848309 c 0.16633414 k 0.0044935616 m -0.00013732404\n",
      "24 Train Loss 10000.0 Test RE 0.9998776617671074 c 0.16912374 k 0.0045569865 m -0.00014399433\n",
      "25 Train Loss 10000.0 Test RE 0.9998779820771214 c 0.17166336 k 0.0046065855 m -0.00015241417\n",
      "26 Train Loss 10000.0 Test RE 0.9998782953837887 c 0.17418869 k 0.00464654 m -0.00016347023\n",
      "27 Train Loss 10000.0 Test RE 0.9998786173708621 c 0.176885 k 0.0046783127 m -0.0001783729\n",
      "28 Train Loss 10000.0 Test RE 0.9998789562836435 c 0.17990392 k 0.0047013485 m -0.0001985959\n",
      "29 Train Loss 10000.0 Test RE 0.9998793121512268 c 0.18336698 k 0.004713864 m -0.0002256704\n",
      "30 Train Loss 10000.0 Test RE 0.9998796760209011 c 0.1873538 k 0.0047138426 m -0.00026078592\n",
      "31 Train Loss 10000.0 Test RE 0.9998800316044679 c 0.19189747 k 0.004700306 m -0.00030438014\n",
      "32 Train Loss 10000.0 Test RE 0.9998803604801131 c 0.19701418 k 0.004674232 m -0.0003561327\n",
      "33 Train Loss 10000.0 Test RE 0.9998806482314943 c 0.20277894 k 0.0046384525 m -0.0004156395\n",
      "34 Train Loss 10000.0 Test RE 0.9998808872384589 c 0.20943323 k 0.0045969132 m -0.00048349507\n",
      "35 Train Loss 10000.0 Test RE 0.9998810743596297 c 0.2175098 k 0.0045544547 m -0.00056227646\n",
      "36 Train Loss 9999.998 Test RE 0.9998812733496146 c 0.26409265 k 0.004514685 m -0.00093911315\n",
      "37 Train Loss 9999.998 Test RE 0.9998811724058548 c 0.2947584 k 0.004585448 m -0.001150945\n",
      "38 Train Loss 9999.998 Test RE 0.9998809589251545 c 0.33682585 k 0.004737362 m -0.0014211185\n",
      "39 Train Loss 9999.998 Test RE 0.9998806353100977 c 0.39282736 k 0.005006393 m -0.0017570244\n",
      "40 Train Loss 9999.998 Test RE 0.999880232843401 c 0.46800613 k 0.0054580546 m -0.0021781786\n",
      "41 Train Loss 9999.998 Test RE 0.9998798717374779 c 0.5748774 k 0.0062310114 m -0.0027371831\n",
      "42 Train Loss 9999.997 Test RE 0.9998818533339712 c 1.175042 k 0.011821536 m -0.0055388417\n",
      "43 Train Loss 9999.997 Test RE 0.9998835407340454 c 1.6282871 k 0.01639187 m -0.007563907\n",
      "44 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "45 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "46 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "47 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "48 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "49 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "50 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "51 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "52 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "53 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "54 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "55 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "56 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "57 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "58 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "59 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "60 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "61 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "62 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "63 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "64 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "65 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "66 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "67 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "68 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "69 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "70 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "71 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "72 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "73 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "74 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "75 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "76 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "77 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "78 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "79 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "80 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "81 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "82 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "83 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "84 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "85 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "86 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "87 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "88 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "89 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "90 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "91 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "92 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "93 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "94 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "95 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "96 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "97 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "98 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "99 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "100 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "101 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "102 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "103 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "104 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "105 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "106 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "107 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "108 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "109 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "110 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "111 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "112 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "113 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "114 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "115 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "116 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "117 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "118 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "119 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "120 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "121 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "122 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "123 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "124 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "125 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "126 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "127 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "128 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "129 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "130 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "131 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "132 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "133 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "134 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "135 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "136 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "137 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "138 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "139 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "140 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "141 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "142 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "143 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "144 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "145 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "146 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "147 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "148 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "149 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "150 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "151 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "152 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "153 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "154 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "155 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "156 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "157 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "158 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "159 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "160 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "161 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "162 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "163 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "164 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "165 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "166 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "167 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "168 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "169 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "170 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "171 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "172 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "173 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "174 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "175 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "176 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "177 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "178 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "179 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "180 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "181 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "182 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "183 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "184 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "185 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "186 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "187 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "188 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "189 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "190 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "191 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "192 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "193 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "194 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "195 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "196 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "197 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "198 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "199 Train Loss 9999.995 Test RE 0.9998807291354815 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "Training time: 35.48\n",
      "Training time: 35.48\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 10000.002 Test RE 0.999884065812244 c 0.0020324478 k 6.7768364e-05 m -1.3162388e-07\n",
      "1 Train Loss 10000.001 Test RE 0.9998813704620384 c 0.0028535628 k 9.820876e-05 m -2.4637777e-07\n",
      "2 Train Loss 10000.001 Test RE 0.9998806291933768 c 0.0032836956 k 0.00011672748 m -3.203304e-07\n",
      "3 Train Loss 10000.001 Test RE 0.9998798619147969 c 0.003729199 k 0.00013083636 m -4.049224e-07\n",
      "4 Train Loss 10000.001 Test RE 0.999879463983611 c 0.0041949768 k 0.00015039623 m -5.043685e-07\n",
      "5 Train Loss 10000.001 Test RE 0.9998789697682878 c 0.004690943 k 0.00016675665 m -6.1954916e-07\n",
      "6 Train Loss 10000.001 Test RE 0.999878719322904 c 0.0052346243 k 0.00018848976 m -7.5823186e-07\n",
      "7 Train Loss 10000.001 Test RE 0.9998783738558206 c 0.0058577647 k 0.0002100998 m -9.295183e-07\n",
      "8 Train Loss 10000.001 Test RE 0.9998781642371537 c 0.0066147777 k 0.00023893056 m -1.1543083e-06\n",
      "9 Train Loss 10000.001 Test RE 0.9998778678940736 c 0.0075929463 k 0.00027379522 m -1.4650984e-06\n",
      "10 Train Loss 10000.001 Test RE 0.9998776420021201 c 0.008927085 k 0.00032319105 m -1.9183924e-06\n",
      "11 Train Loss 10000.001 Test RE 0.9998773203312589 c 0.010812153 k 0.0003907871 m -2.600715e-06\n",
      "12 Train Loss 10000.001 Test RE 0.9998770556931262 c 0.013533202 k 0.0004900984 m -3.6508209e-06\n",
      "13 Train Loss 10000.001 Test RE 0.999876664660589 c 0.017489925 k 0.00063135894 m -5.281816e-06\n",
      "14 Train Loss 10000.001 Test RE 0.9998764294289659 c 0.023284998 k 0.0008415256 m -7.849763e-06\n",
      "15 Train Loss 10000.0 Test RE 0.9998762461480399 c 0.045065016 k 0.001624554 m -1.8984656e-05\n",
      "16 Train Loss 10000.0 Test RE 0.9998757154585322 c 0.06260705 k 0.0022327565 m -2.9100407e-05\n",
      "17 Train Loss 10000.0 Test RE 0.9998763753948514 c 0.079214185 k 0.0028183751 m -4.012628e-05\n",
      "18 Train Loss 10000.0 Test RE 0.9998767162858601 c 0.09059438 k 0.0032019124 m -4.9582854e-05\n",
      "19 Train Loss 10000.0 Test RE 0.9998770883440501 c 0.09778398 k 0.003436526 m -5.6859873e-05\n",
      "20 Train Loss 10000.0 Test RE 0.9998773575550368 c 0.10272017 k 0.0035892874 m -6.290319e-05\n",
      "21 Train Loss 10000.0 Test RE 0.9998775904373138 c 0.10650246 k 0.003698511 m -6.855814e-05\n",
      "22 Train Loss 10000.0 Test RE 0.9998778033869521 c 0.10967585 k 0.0037817114 m -7.438255e-05\n",
      "23 Train Loss 10000.0 Test RE 0.9998780106718157 c 0.112520084 k 0.0038467133 m -8.08192e-05\n",
      "24 Train Loss 10000.0 Test RE 0.9998782194427894 c 0.11518365 k 0.0038963263 m -8.826624e-05\n",
      "25 Train Loss 10000.0 Test RE 0.9998784315496415 c 0.117735036 k 0.003930552 m -9.7060445e-05\n",
      "26 Train Loss 10000.0 Test RE 0.9998786436620268 c 0.1201984 k 0.003948434 m -0.000107422835\n",
      "27 Train Loss 10000.0 Test RE 0.9998788487476729 c 0.12259343 k 0.003949958 m -0.000119421886\n",
      "28 Train Loss 10000.0 Test RE 0.9998790394820706 c 0.12498497 k 0.0039374162 m -0.00013305637\n",
      "29 Train Loss 10000.0 Test RE 0.9998792125470141 c 0.12753142 k 0.003915265 m -0.00014851874\n",
      "30 Train Loss 10000.0 Test RE 0.9998793704155482 c 0.13052498 k 0.0038887658 m -0.00016655902\n",
      "31 Train Loss 10000.0 Test RE 0.9998795204198528 c 0.13444684 k 0.0038631314 m -0.0001888449\n",
      "32 Train Loss 10000.0 Test RE 0.999879672185447 c 0.14006819 k 0.0038443357 m -0.00021833031\n",
      "33 Train Loss 10000.0 Test RE 0.9998798359965694 c 0.14860927 k 0.003841268 m -0.00025973815\n",
      "34 Train Loss 10000.0 Test RE 0.9998800222246059 c 0.16195723 k 0.0038684094 m -0.00032026018\n",
      "35 Train Loss 10000.0 Test RE 0.9998802415474705 c 0.18296733 k 0.0039487397 m -0.00041062315\n",
      "36 Train Loss 10000.0 Test RE 0.9998805057706217 c 0.21595214 k 0.004117414 m -0.0005469441\n",
      "37 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "38 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "39 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "40 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "41 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "42 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "43 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "44 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "45 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "46 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "47 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "48 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "49 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "50 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "51 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "52 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "53 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "54 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "55 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "56 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "57 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "58 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "59 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "60 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "61 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "62 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "63 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "64 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "65 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "66 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "67 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "68 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "69 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "70 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "71 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "72 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "73 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "74 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "75 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "76 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "77 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "78 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "79 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "80 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "81 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "82 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "83 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "84 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "85 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "86 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "87 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "88 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "89 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "90 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "91 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "92 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "93 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "94 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "95 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "96 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "97 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "98 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "99 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "100 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "101 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "102 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "103 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "104 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "105 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "106 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "107 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "108 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "109 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "110 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "111 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "112 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "113 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "114 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "115 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "116 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "117 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "118 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "119 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "120 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "121 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "122 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "123 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "124 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "125 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "126 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "127 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "128 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "129 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "130 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "131 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "132 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "133 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "134 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "135 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "136 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "137 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "138 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "139 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "140 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "141 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "142 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "143 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "144 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "145 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "146 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "147 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "148 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "149 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "150 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "151 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "152 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "153 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "154 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "155 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "156 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "157 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "158 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "159 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "160 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "161 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "162 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "163 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "164 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "165 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "166 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "167 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "168 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "169 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "170 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "171 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "172 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "173 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "174 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "175 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "176 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "177 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "178 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "179 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "180 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "181 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "182 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "183 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "184 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "185 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "186 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "187 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "188 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "189 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "190 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "191 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "192 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "193 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "194 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "195 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "196 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "197 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "198 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "199 Train Loss 9999.998 Test RE 0.9998812524957076 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "Training time: 36.51\n",
      "Training time: 36.51\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 10000.001 Test RE 0.999882076690183 c 0.0028199742 k 6.866212e-05 m -8.99025e-08\n",
      "1 Train Loss 10000.001 Test RE 0.9998810949298798 c 0.0032451109 k 8.037996e-05 m -1.1752725e-07\n",
      "2 Train Loss 10000.001 Test RE 0.9998803547328113 c 0.003682894 k 9.2231065e-05 m -1.4948556e-07\n",
      "3 Train Loss 10000.001 Test RE 0.9998798059707802 c 0.004134196 k 0.00010468708 m -1.8620162e-07\n",
      "4 Train Loss 10000.001 Test RE 0.9998793792651645 c 0.004602322 k 0.00011740624 m -2.2816295e-07\n",
      "5 Train Loss 10000.001 Test RE 0.999879054293783 c 0.00509448 k 0.00013097546 m -2.764767e-07\n",
      "6 Train Loss 10000.001 Test RE 0.9998787860594951 c 0.0056242933 k 0.00014538913 m -3.329534e-07\n",
      "7 Train Loss 10000.001 Test RE 0.999878568053132 c 0.0062158783 k 0.00016166263 m -4.0108617e-07\n",
      "8 Train Loss 10000.001 Test RE 0.999878366634053 c 0.0069099274 k 0.00018054637 m -4.868672e-07\n",
      "9 Train Loss 10000.001 Test RE 0.9998781832419986 c 0.0077724727 k 0.00020420972 m -6.007848e-07\n",
      "10 Train Loss 10000.001 Test RE 0.9998779872491709 c 0.008906564 k 0.00023504936 m -7.600179e-07\n",
      "11 Train Loss 10000.001 Test RE 0.9998777887983165 c 0.010468338 k 0.00027779283 m -9.924161e-07\n",
      "12 Train Loss 10000.001 Test RE 0.9998775513835622 c 0.012689238 k 0.00033812944 m -1.3414888e-06\n",
      "13 Train Loss 10000.001 Test RE 0.9998773017463051 c 0.015909739 k 0.000426123 m -1.8752751e-06\n",
      "14 Train Loss 10000.001 Test RE 0.9998769806156722 c 0.020629717 k 0.0005542047 m -2.698925e-06\n",
      "15 Train Loss 10000.001 Test RE 0.9998766596234987 c 0.027600108 k 0.00074458 m -3.9794304e-06\n",
      "16 Train Loss 10000.001 Test RE 0.9998761999160504 c 0.03797348 k 0.0010255764 m -5.9862623e-06\n",
      "17 Train Loss 10000.001 Test RE 0.9998758596422216 c 0.053712517 k 0.0014564843 m -9.1987185e-06\n",
      "18 Train Loss 10000.001 Test RE 0.9998750658105718 c 0.078544006 k 0.0021255137 m -1.4564124e-05\n",
      "19 Train Loss 10000.0 Test RE 0.9998747318815645 c 0.20429581 k 0.005542884 m -4.4371314e-05\n",
      "20 Train Loss 9999.998 Test RE 0.999878008018544 c 0.31159285 k 0.008460713 m -7.523075e-05\n",
      "21 Train Loss 9999.998 Test RE 0.999877905396408 c 0.31640118 k 0.008579754 m -7.787829e-05\n",
      "22 Train Loss 9999.998 Test RE 0.9998778712815511 c 0.32076257 k 0.008685573 m -8.105369e-05\n",
      "23 Train Loss 9999.998 Test RE 0.9998778302758213 c 0.3242025 k 0.0087631345 m -8.4862055e-05\n",
      "24 Train Loss 9999.998 Test RE 0.9998778086114207 c 0.32723743 k 0.008825153 m -8.977802e-05\n",
      "25 Train Loss 9999.998 Test RE 0.9998777982877592 c 0.32973325 k 0.008866146 m -9.618022e-05\n",
      "26 Train Loss 9999.998 Test RE 0.9998778100863661 c 0.331681 k 0.008883952 m -0.00010456254\n",
      "27 Train Loss 9999.998 Test RE 0.9998778489128414 c 0.33293936 k 0.008873058 m -0.00011532\n",
      "28 Train Loss 9999.998 Test RE 0.9998779238080084 c 0.33347064 k 0.008831644 m -0.0001286653\n",
      "29 Train Loss 9999.998 Test RE 0.9998780385314127 c 0.33335936 k 0.008762371 m -0.00014452124\n",
      "30 Train Loss 9999.998 Test RE 0.9998781929836673 c 0.33289504 k 0.008673904 m -0.00016269906\n",
      "31 Train Loss 9999.998 Test RE 0.9998783829334273 c 0.33248946 k 0.008576984 m -0.00018328485\n",
      "32 Train Loss 9999.998 Test RE 0.9998786062611263 c 0.33260798 k 0.008481044 m -0.00020701103\n",
      "33 Train Loss 9999.998 Test RE 0.9998788650681689 c 0.33375916 k 0.008393392 m -0.00023539046\n",
      "34 Train Loss 9999.998 Test RE 0.9998791661000659 c 0.33660722 k 0.008321758 m -0.00027083588\n",
      "35 Train Loss 9999.998 Test RE 0.9998795200526521 c 0.34213787 k 0.0082767755 m -0.00031712704\n",
      "36 Train Loss 9999.998 Test RE 0.9998799429826367 c 0.3519392 k 0.008275215 m -0.00038043107\n",
      "37 Train Loss 9999.998 Test RE 0.9998804570592221 c 0.3686571 k 0.008344531 m -0.0004711109\n",
      "38 Train Loss 9999.998 Test RE 0.9998810886350047 c 0.3967259 k 0.0085294545 m -0.00060676987\n",
      "39 Train Loss 9999.998 Test RE 0.999881850864622 c 0.44323063 k 0.008896477 m -0.00081651285\n",
      "40 Train Loss 9999.998 Test RE 0.9998826657633109 c 0.5170004 k 0.009510959 m -0.0011401104\n",
      "41 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "42 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "43 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "44 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "45 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "46 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "47 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "48 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "49 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "50 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "51 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "52 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "53 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "54 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "55 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "56 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "57 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "58 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "59 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "60 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "61 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "62 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "63 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "64 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "65 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "66 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "67 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "68 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "69 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "70 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "71 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "72 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "73 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "74 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "75 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "76 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "77 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "78 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "79 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "80 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "81 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "82 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "83 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "84 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "85 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "86 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "87 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "88 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "89 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "90 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "91 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "92 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "93 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "94 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "95 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "96 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "97 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "98 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "99 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "100 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "101 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "102 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "103 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "104 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "105 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "106 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "107 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "108 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "109 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "110 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "111 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "112 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "113 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "114 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "115 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "116 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "117 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "118 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "119 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "120 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "121 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "122 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "123 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "124 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "125 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "126 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "127 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "128 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "129 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "130 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "131 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "132 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "133 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "134 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "135 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "136 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "137 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "138 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "139 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "140 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "141 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "142 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "143 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "144 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "145 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "146 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "147 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "148 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "149 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "150 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "151 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "152 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "153 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "154 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "155 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "156 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "157 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "158 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "159 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "160 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "161 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "162 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "163 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "164 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "165 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "166 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "167 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "168 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "169 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "170 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "171 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "172 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "173 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "174 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "175 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "176 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "177 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "178 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "179 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "180 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "181 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "182 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "183 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "184 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "185 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "186 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "187 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "188 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "189 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "190 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "191 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "192 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "193 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "194 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "195 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "196 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "197 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "198 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "199 Train Loss 9999.998 Test RE 0.99988315227093 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "Training time: 32.33\n",
      "Training time: 32.33\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 10000.001 Test RE 0.9998818441719842 c 0.0024708128 k 5.948359e-05 m -7.022569e-08\n",
      "1 Train Loss 10000.001 Test RE 0.9998809258044955 c 0.0028020518 k 6.8207526e-05 m -8.904792e-08\n",
      "2 Train Loss 10000.001 Test RE 0.9998802437245929 c 0.0031408581 k 7.72222e-05 m -1.10495456e-07\n",
      "3 Train Loss 10000.001 Test RE 0.9998797279925898 c 0.0034879583 k 8.641573e-05 m -1.347182e-07\n",
      "4 Train Loss 10000.001 Test RE 0.9998793373412006 c 0.0038455343 k 9.593622e-05 m -1.6203364e-07\n",
      "5 Train Loss 10000.001 Test RE 0.9998790333669234 c 0.0042182035 k 0.000105828505 m -1.9296579e-07\n",
      "6 Train Loss 10000.001 Test RE 0.9998787929503747 c 0.004614635 k 0.00011637736 m -2.2851394e-07\n",
      "7 Train Loss 10000.001 Test RE 0.9998785933053361 c 0.0050502312 k 0.00012794383 m -2.7046786e-07\n",
      "8 Train Loss 10000.001 Test RE 0.9998784201201459 c 0.005551255 k 0.00014126065 m -3.2204514e-07\n",
      "9 Train Loss 10000.001 Test RE 0.9998782582713873 c 0.0061607 k 0.0001574347 m -3.8878335e-07\n",
      "10 Train Loss 10000.001 Test RE 0.9998780979006742 c 0.0069465395 k 0.00017829501 m -4.799544e-07\n",
      "11 Train Loss 10000.001 Test RE 0.9998779275334982 c 0.0080126235 k 0.00020656291 m -6.1052293e-07\n",
      "12 Train Loss 10000.001 Test RE 0.9998777398193661 c 0.009514022 k 0.0002463723 m -8.040931e-07\n",
      "13 Train Loss 10000.001 Test RE 0.999877524612682 c 0.011679066 k 0.00030373188 m -1.0972694e-06\n",
      "14 Train Loss 10000.001 Test RE 0.9998772759198189 c 0.014842477 k 0.0003875368 m -1.5464936e-06\n",
      "15 Train Loss 10000.001 Test RE 0.9998769833733352 c 0.019497568 k 0.0005107971 m -2.2390577e-06\n",
      "16 Train Loss 10000.001 Test RE 0.9998766425889445 c 0.026384534 k 0.0006931734 m -3.3121937e-06\n",
      "17 Train Loss 10000.001 Test RE 0.9998762423306679 c 0.036655176 k 0.0009650986 m -4.989057e-06\n",
      "18 Train Loss 10000.001 Test RE 0.9998757869591701 c 0.052232154 k 0.0013776941 m -7.657517e-06\n",
      "19 Train Loss 10000.001 Test RE 0.9998752734810133 c 0.076715514 k 0.0020261975 m -1.20718505e-05\n",
      "20 Train Loss 10000.001 Test RE 0.9998748257025226 c 0.118220404 k 0.0031266236 m -2.000794e-05\n",
      "21 Train Loss 9999.999 Test RE 0.9998823683935942 c 0.2674228 k 0.0071708416 m -7.0241185e-05\n",
      "22 Train Loss 9999.999 Test RE 0.9998822626347466 c 0.33706972 k 0.008974056 m -7.928967e-05\n",
      "23 Train Loss 9999.999 Test RE 0.9998799434539422 c 0.32595843 k 0.00865026 m -7.1865456e-05\n",
      "24 Train Loss 9999.999 Test RE 0.9998793415464038 c 0.32697225 k 0.008665253 m -7.067711e-05\n",
      "25 Train Loss 9999.998 Test RE 0.999878480693119 c 0.32516456 k 0.008597848 m -6.945373e-05\n",
      "26 Train Loss 9999.998 Test RE 0.9998781758147577 c 0.3237669 k 0.0085515445 m -6.946119e-05\n",
      "27 Train Loss 9999.998 Test RE 0.9998779251367192 c 0.32202724 k 0.008495791 m -6.985855e-05\n",
      "28 Train Loss 9999.998 Test RE 0.9998777174467167 c 0.31991425 k 0.008429182 m -7.0704526e-05\n",
      "29 Train Loss 9999.998 Test RE 0.9998775490549513 c 0.3173946 k 0.008350212 m -7.2120034e-05\n",
      "30 Train Loss 9999.998 Test RE 0.9998774229503743 c 0.31447566 k 0.008258417 m -7.428834e-05\n",
      "31 Train Loss 9999.998 Test RE 0.9998773466845386 c 0.31125376 k 0.008155655 m -7.745212e-05\n",
      "32 Train Loss 9999.998 Test RE 0.9998773290630425 c 0.30794483 k 0.008046823 m -8.19213e-05\n",
      "33 Train Loss 9999.998 Test RE 0.9998773769123778 c 0.3048593 k 0.007938967 m -8.810305e-05\n",
      "34 Train Loss 9999.998 Test RE 0.9998774944017431 c 0.30232587 k 0.007839209 m -9.648958e-05\n",
      "35 Train Loss 9999.998 Test RE 0.999877683020857 c 0.3006284 k 0.007753671 m -0.00010749318\n",
      "36 Train Loss 9999.998 Test RE 0.9998779388716816 c 0.30001608 k 0.007688449 m -0.00012126134\n",
      "37 Train Loss 9999.998 Test RE 0.9998782525927793 c 0.30068946 k 0.0076487525 m -0.00013780424\n",
      "38 Train Loss 9999.998 Test RE 0.9998786148369523 c 0.30280957 k 0.007637998 m -0.00015731926\n",
      "39 Train Loss 9999.998 Test RE 0.999879020042365 c 0.30656353 k 0.0076588914 m -0.00018039132\n",
      "40 Train Loss 9999.998 Test RE 0.9998794674543642 c 0.31224734 k 0.007715217 m -0.00020811634\n",
      "41 Train Loss 9999.998 Test RE 0.9998799614450685 c 0.32035893 k 0.00781369 m -0.0002422774\n",
      "42 Train Loss 9999.998 Test RE 0.9998805115212224 c 0.33171278 k 0.007966 m -0.00028562165\n",
      "43 Train Loss 9999.998 Test RE 0.9998811324181319 c 0.34759864 k 0.008191462 m -0.0003422894\n",
      "44 Train Loss 9999.998 Test RE 0.9998818444128582 c 0.37001687 k 0.008520872 m -0.00041847504\n",
      "45 Train Loss 9999.998 Test RE 0.9998826747694213 c 0.40206194 k 0.009002815 m -0.00052349956\n",
      "46 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "47 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "48 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "49 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "50 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "51 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "52 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "53 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "54 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "55 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "56 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "57 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "58 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "59 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "60 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "61 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "62 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "63 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "64 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "65 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "66 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "67 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "68 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "69 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "70 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "71 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "72 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "73 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "74 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "75 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "76 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "77 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "78 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "79 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "80 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "81 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "82 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "83 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "84 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "85 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "86 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "87 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "88 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "89 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "90 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "91 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "92 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "93 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "94 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "95 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "96 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "97 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "98 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "99 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "100 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "101 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "102 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "103 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "104 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "105 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "106 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "107 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "108 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "109 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "110 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "111 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "112 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "113 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "114 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "115 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "116 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "117 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "118 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "119 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "120 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "121 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "122 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "123 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "124 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "125 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "126 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "127 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "128 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "129 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "130 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "131 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "132 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "133 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "134 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "135 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "136 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "137 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "138 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "139 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "140 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "141 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "142 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "143 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "144 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "145 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "146 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "147 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "148 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "149 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "150 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "151 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "152 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "153 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "154 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "155 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "156 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "157 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "158 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "159 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "160 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "161 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "162 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "163 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "164 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "165 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "166 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "167 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "168 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "169 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "170 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "171 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "172 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "173 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "174 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "175 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "176 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "177 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "178 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "179 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "180 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "181 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "182 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "183 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "184 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "185 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "186 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "187 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "188 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "189 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "190 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "191 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "192 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "193 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "194 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "195 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "196 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "197 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "198 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "199 Train Loss 9999.998 Test RE 0.9998836605926718 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "Training time: 29.29\n",
      "Training time: 29.29\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 200\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "m_full = []\n",
    "k_full = []\n",
    "c_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "\n",
    "for reps in range(max_reps):   \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []   \n",
    "    beta_val = []\n",
    "    m_val = []\n",
    "    k_val = []\n",
    "    c_val = []\n",
    "\n",
    "    torch.manual_seed(reps*36)\n",
    "    N_f = 10000 #Total number of collocation points\n",
    "\n",
    "    layers = np.array([1,50,50,50,1]) #9 hidden layers\n",
    "    PINN = Sequentialmodel(layers)\n",
    "\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.25, \n",
    "                                max_iter = 10, \n",
    "                                max_eval = 15, \n",
    "                                tolerance_grad = 1e-6, \n",
    "                                tolerance_change = 1e-6, \n",
    "                                history_size = 100, \n",
    "                                line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "    train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    beta_full.append(beta_val)\n",
    "    m_full.append(m_val)\n",
    "    k_full.append(k_val)\n",
    "    c_full.append(c_val)\n",
    "    \n",
    "\n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"beta\": beta_full,\"m\": m_full,\"k\": k_full,\"c\": c_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "pmHEeBpzfFQh",
    "outputId": "990b2054-f35e-4c8e-c378-69ff2eb19f4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1708402c50>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_pred = PINN.test()\n",
    "plt.plot(t,x_true,'b')\n",
    "plt.plot(t,x_pred,'r--')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "stan_1D_FODE_tune.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
