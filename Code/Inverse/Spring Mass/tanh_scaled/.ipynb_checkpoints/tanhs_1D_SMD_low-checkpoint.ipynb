{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8byrnUmNKGR",
    "outputId": "510f67c6-b225-41ba-e78a-d0a6e54ded85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uoNYpDzlNKGV"
   },
   "outputs": [],
   "source": [
    "tstart= 0 \n",
    "tstop= 60 \n",
    "\n",
    "increment = 0.1 \n",
    "# Initial condition\n",
    "x_init= [0,0] \n",
    "\n",
    "t = np.arange(tstart,tstop+1,increment)\n",
    "\n",
    "c = 1 # Damping constant \n",
    "k = 1 # Stiffness of the spring\n",
    "m = 5 # Mass \n",
    "F0 = 100\n",
    "\n",
    "def mydiff(x, t):    \n",
    "    F =F0\n",
    "    \n",
    "    dx1dt = x[1] \n",
    "    dx2dt = (F -c*x[1] -k*x[0])/m\n",
    "    dxdt= [dx1dt, dx2dt] \n",
    "    \n",
    "    return dxdt \n",
    "\n",
    "x_full_sol = odeint(mydiff, x_init, t) \n",
    "\n",
    "x_sol = x_full_sol[:,0]\n",
    "v_sol = x_full_sol[:,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BR02v-fkNKGV"
   },
   "outputs": [],
   "source": [
    "level = \"low\"\n",
    "label = \"1D_SMD_tanhs_\" + level\n",
    "\n",
    "loss_thresh = 0.005\n",
    "\n",
    "t_test = t.reshape(-1,1)\n",
    "t_test_tensor = torch.from_numpy(t_test).float().to(device)\n",
    "\n",
    "# bc1_t = t[0].reshape(-1,1)\n",
    "# bc1_x = x_sol[0].reshape(-1,1)\n",
    "# t_bc1_train = torch.from_numpy(bc1_t).float().to(device)\n",
    "# x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "x_mean = np.mean(x_sol.reshape(-1,1))\n",
    "x_std  = np.mean(x_sol.reshape(-1,1))                \n",
    "bc1_t = t.reshape(-1,1)\n",
    "bc1_x = (x_sol-x_mean).reshape(-1,1)/x_std\n",
    "t_bc1_train = torch.from_numpy(bc1_t).float().to(device)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "\n",
    "\n",
    "bc2_t = t[0].reshape(-1,1)\n",
    "t_bc2_train = torch.from_numpy(bc2_t).float().to(device)\n",
    "bc2_val = v_sol[0].reshape(-1,1)\n",
    "bc2_val =torch.from_numpy(bc2_val).float().to(device)\n",
    "\n",
    "t_test = t.reshape(-1,1)\n",
    "t_test_tensor = torch.from_numpy(t_test).float().to(device)\n",
    "x_true = x_sol\n",
    "x_true_norm = np.linalg.norm(x_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(t[0]) \n",
    "ub = np.array(t[-1]) \n",
    "\n",
    "#torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SyyktBKBXRo1"
   },
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    \n",
    "    t01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=t01,random_state =seed)\n",
    "\n",
    "    t_coll = lb + (ub-lb)*sampling(N_f)\n",
    "    # t_coll = np.vstack((t_coll,)) # append training points to collocation points \n",
    "\n",
    "    return t_coll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "o1b21zLnNKGW"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "        \n",
    "        \n",
    "        self.k = Parameter(torch.tensor(0.0))\n",
    "        self.k.requiresGrad = True\n",
    "        self.c = Parameter(torch.tensor(0.0))\n",
    "        self.c.requiresGrad = True\n",
    "        self.m = Parameter(torch.tensor(0.0))\n",
    "        self.m.requiresGrad = True\n",
    "        \n",
    "    'forward pass'\n",
    "    def forward(self,t):\n",
    "        if torch.is_tensor(t) != True:         \n",
    "            t = torch.from_numpy(t)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        t = 2.0*(t - l_b)/(u_b - l_b) - 1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = t.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,t,x):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(t), x)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,t_bc2,bc2_val):\n",
    "        g = t_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        x = self.forward(g)    \n",
    "            \n",
    "        x_t = autograd.grad(x,g,torch.ones([t_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dx_dt = x_t[:,[0]]\n",
    "        \n",
    "        bc2 = dx_dt/x_std\n",
    "        \n",
    "        loss_bc2= self.loss_function(bc2,bc2_val)\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self,t_coll,f_hat):\n",
    "             \n",
    "        g = t_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        x = self.forward(g) \n",
    "\n",
    "        x_t = autograd.grad(x,g,torch.ones([t_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        x_tt = autograd.grad(x_t,g,torch.ones(t_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dx_dt = x_t[:,[0]]\n",
    "        \n",
    "        dx2_d2t = x_tt[:,[0]]\n",
    "        \n",
    "        f = self.m*dx2_d2t/x_std + self.c*dx_dt/x_std + self.k*x/x_std - F0\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,t_bc1,x_bc1,t_bc2,bc2_val,t_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(t_bc1,x_bc1)\n",
    "        loss_bc2 = self.loss_BC2(t_bc2,bc2_val)\n",
    "        loss_f = self.loss_PDE(t_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self):\n",
    "        x_pred = self.forward(t_test_tensor)\n",
    "        x_pred = x_pred.cpu().detach().numpy()\n",
    "\n",
    "        x_pred = x_pred*x_std + x_mean\n",
    "        \n",
    "        return x_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        x_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(x_pred.reshape(-1,1) - x_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(x_pred.reshape(-1,1) - x_true.reshape(-1,1),2)/x_true_norm\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fLY2mT5BOgjD"
   },
   "outputs": [],
   "source": [
    "def train_step(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8srA5uGuObil"
   },
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    m_val.append(PINN.m.cpu().detach().numpy())\n",
    "    k_val.append(PINN.k.cpu().detach().numpy())\n",
    "    c_val.append(PINN.c.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0SezTZ_racQB"
   },
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "\n",
    "    t_coll = colloc_pts(N_f,rep*11)\n",
    "    t_coll =  torch.from_numpy(t_coll).float().to(device)\n",
    "    \n",
    "    f_hat = torch.zeros(t_coll.shape[0],1).to(device)\n",
    "    for i in range(max_iter):      \n",
    "        train_step(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat)\n",
    "         \n",
    "        loss_np = PINN.loss(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test RE\",test_re_loss[-1],\"c\",PINN.c.cpu().detach().numpy(),\"k\",PINN.k.cpu().detach().numpy(),\"m\",PINN.m.cpu().detach().numpy())\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I9BYbcJ0NKGX",
    "outputId": "42720d97-b37a-4c42-b7e5-b77a0ef391f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 10000.051 Test RE 0.9999703108512078 c -0.00068137585 k 0.0030394667 m -1.5578928e-06\n",
      "1 Train Loss 10000.051 Test RE 0.9999468095382167 c -0.00018759503 k 0.0039372332 m -2.0509144e-06\n",
      "2 Train Loss 10000.05 Test RE 0.9999194696887005 c 0.00096892606 k 0.007287445 m -3.4351483e-06\n",
      "3 Train Loss 10000.05 Test RE 0.9998951708527134 c 0.0017687983 k 0.0101707205 m -4.517308e-06\n",
      "4 Train Loss 10000.05 Test RE 0.9998459615951635 c 0.0031559833 k 0.01565352 m -6.4942815e-06\n",
      "5 Train Loss 10000.047 Test RE 0.9985530434774538 c 0.036968354 k 0.15831509 m -5.6456553e-05\n",
      "6 Train Loss 9999.281 Test RE 0.9662511771356809 c 1.168381 k 5.2275634 m -0.0018128533\n",
      "7 Train Loss 5860.1455 Test RE 0.6133141312962833 c 27.145626 k 123.12568 m -0.04278683\n",
      "8 Train Loss 1707.3086 Test RE 0.6353605631050112 c 47.679863 k 224.68535 m -0.07719617\n",
      "9 Train Loss 921.66376 Test RE 0.7395040372141738 c 70.94851 k 335.84076 m -0.1148574\n",
      "10 Train Loss 664.2291 Test RE 0.7945928054015854 c 90.39501 k 429.1265 m -0.14494175\n",
      "11 Train Loss 529.5357 Test RE 0.8001395221997774 c 95.366196 k 452.9998 m -0.15105419\n",
      "12 Train Loss 410.37305 Test RE 0.8258966757683336 c 109.11817 k 518.8482 m -0.1705117\n",
      "13 Train Loss 335.21948 Test RE 0.8443812136274227 c 124.646095 k 593.56396 m -0.19191457\n",
      "14 Train Loss 253.41635 Test RE 0.8586324858916836 c 136.92493 k 652.5751 m -0.20891245\n",
      "15 Train Loss 210.12071 Test RE 0.8715122821608289 c 151.14177 k 721.00946 m -0.2295634\n",
      "16 Train Loss 183.1836 Test RE 0.8828626242961841 c 167.00479 k 797.25726 m -0.25201318\n",
      "17 Train Loss 167.74277 Test RE 0.8882661602281872 c 175.07344 k 835.9432 m -0.2633553\n",
      "18 Train Loss 153.6924 Test RE 0.8927835082411002 c 182.4765 k 871.41785 m -0.27299705\n",
      "19 Train Loss 137.82983 Test RE 0.896486903455037 c 189.558 k 905.36774 m -0.28147888\n",
      "20 Train Loss 131.67717 Test RE 0.8996084833452439 c 195.51738 k 933.9966 m -0.28919962\n",
      "21 Train Loss 125.9858 Test RE 0.9017686596557598 c 199.75232 k 954.2644 m -0.29390755\n",
      "22 Train Loss 121.892715 Test RE 0.9028442730380516 c 202.56052 k 967.7398 m -0.29646364\n",
      "23 Train Loss 113.61536 Test RE 0.9043341388338244 c 205.6372 k 982.6094 m -0.29811513\n",
      "24 Train Loss 106.42952 Test RE 0.9058031504252608 c 208.64955 k 997.1739 m -0.30097756\n",
      "25 Train Loss 100.12204 Test RE 0.9110746479104991 c 221.89365 k 1060.6881 m -0.3192403\n",
      "26 Train Loss 94.81589 Test RE 0.9137301368028269 c 228.52802 k 1092.5153 m -0.32730392\n",
      "27 Train Loss 92.242 Test RE 0.9137772442812312 c 228.42253 k 1092.0612 m -0.32657874\n",
      "28 Train Loss 87.664 Test RE 0.9165870753054023 c 236.60849 k 1131.4635 m -0.3362652\n",
      "29 Train Loss 82.42463 Test RE 0.920236807137584 c 247.50139 k 1183.7483 m -0.3496487\n",
      "30 Train Loss 79.76007 Test RE 0.9220120716431747 c 253.29138 k 1211.5269 m -0.35635087\n",
      "31 Train Loss 77.90353 Test RE 0.9217310651033978 c 252.10786 k 1205.845 m -0.35434413\n",
      "32 Train Loss 75.82351 Test RE 0.9224359411367055 c 254.59615 k 1217.7153 m -0.35696098\n",
      "33 Train Loss 72.245155 Test RE 0.9251276752494527 c 263.8014 k 1261.8413 m -0.36661413\n",
      "34 Train Loss 70.56699 Test RE 0.9258558188285653 c 265.9223 k 1271.9597 m -0.36872092\n",
      "35 Train Loss 68.796234 Test RE 0.928268042715905 c 275.17035 k 1316.2113 m -0.3800878\n",
      "36 Train Loss 67.780815 Test RE 0.9286030446799282 c 276.67743 k 1323.4491 m -0.3818427\n",
      "37 Train Loss 66.584785 Test RE 0.9284711412264309 c 276.02567 k 1320.2865 m -0.3807165\n",
      "38 Train Loss 65.29222 Test RE 0.9287626243837714 c 277.12592 k 1325.5134 m -0.38214833\n",
      "39 Train Loss 64.47374 Test RE 0.9293118270290786 c 279.34378 k 1336.1097 m -0.38499933\n",
      "40 Train Loss 63.560276 Test RE 0.9293755082828246 c 279.57614 k 1337.1321 m -0.38452125\n",
      "41 Train Loss 62.578537 Test RE 0.9300968381534961 c 282.3038 k 1350.1173 m -0.3870705\n",
      "42 Train Loss 61.433617 Test RE 0.9310196223461019 c 286.62878 k 1370.7866 m -0.3920278\n",
      "43 Train Loss 59.7895 Test RE 0.9314736688082081 c 288.31235 k 1378.8344 m -0.3932051\n",
      "44 Train Loss 59.27336 Test RE 0.9311017641765157 c 286.58603 k 1370.5425 m -0.39030123\n",
      "45 Train Loss 58.22992 Test RE 0.9320741744796313 c 290.55576 k 1389.5402 m -0.3932206\n",
      "46 Train Loss 56.870255 Test RE 0.9330547915282621 c 294.81656 k 1409.9559 m -0.39666095\n",
      "47 Train Loss 56.230927 Test RE 0.9328270561390413 c 293.97226 k 1405.9094 m -0.39546248\n",
      "48 Train Loss 55.65269 Test RE 0.9332012975875947 c 295.6756 k 1414.0702 m -0.39727074\n",
      "49 Train Loss 54.661358 Test RE 0.9352337209103725 c 304.9643 k 1458.5818 m -0.40716994\n",
      "50 Train Loss 54.133392 Test RE 0.9358239939309273 c 307.83853 k 1472.3451 m -0.40986088\n",
      "51 Train Loss 53.06629 Test RE 0.9354736887106464 c 305.8966 k 1463.0077 m -0.40679836\n",
      "52 Train Loss 52.593082 Test RE 0.9353479864850828 c 305.5564 k 1461.3801 m -0.40580952\n",
      "53 Train Loss 51.964607 Test RE 0.935442633994925 c 306.28378 k 1464.8972 m -0.4049373\n",
      "54 Train Loss 51.2956 Test RE 0.9357813230202808 c 307.70032 k 1471.7009 m -0.40463814\n",
      "55 Train Loss 50.993874 Test RE 0.9361053135359169 c 309.32266 k 1479.4648 m -0.40628594\n",
      "56 Train Loss 50.501183 Test RE 0.9362805729781468 c 310.3373 k 1484.302 m -0.4064402\n",
      "57 Train Loss 49.714935 Test RE 0.9360964258151093 c 309.37042 k 1479.6461 m -0.40170452\n",
      "58 Train Loss 48.902718 Test RE 0.9370516553749237 c 313.93588 k 1501.5177 m -0.40452674\n",
      "59 Train Loss 48.23001 Test RE 0.9377239011035237 c 317.47614 k 1518.4833 m -0.40722963\n",
      "60 Train Loss 47.81806 Test RE 0.9376912107114272 c 317.38617 k 1518.0537 m -0.40617552\n",
      "61 Train Loss 47.633873 Test RE 0.9381618869608005 c 319.7981 k 1529.6136 m -0.40883353\n",
      "62 Train Loss 47.237198 Test RE 0.9387643029966947 c 323.25464 k 1546.1763 m -0.41194043\n",
      "63 Train Loss 46.702255 Test RE 0.9395705412431081 c 327.30603 k 1565.59 m -0.41310546\n",
      "64 Train Loss 46.373096 Test RE 0.9404603768552575 c 331.9988 k 1588.0803 m -0.41543463\n",
      "65 Train Loss 44.90058 Test RE 0.9425922421093219 c 344.4458 k 1647.6737 m -0.42147386\n",
      "66 Train Loss 42.279133 Test RE 0.9436767896841531 c 350.71393 k 1677.6086 m -0.42206314\n",
      "67 Train Loss 41.697186 Test RE 0.9441885413033423 c 354.27713 k 1694.6527 m -0.4222592\n",
      "68 Train Loss 41.20347 Test RE 0.945378613304239 c 362.00052 k 1731.6499 m -0.42715353\n",
      "69 Train Loss 40.562325 Test RE 0.9452636165269742 c 361.19913 k 1727.8223 m -0.424949\n",
      "70 Train Loss 39.824036 Test RE 0.9455696774220148 c 363.50452 k 1738.8689 m -0.42247033\n",
      "71 Train Loss 39.542442 Test RE 0.946117584800436 c 367.18582 k 1756.4868 m -0.4246648\n",
      "72 Train Loss 38.97502 Test RE 0.9461576830900411 c 367.34714 k 1757.2289 m -0.42344347\n",
      "73 Train Loss 38.492725 Test RE 0.9460205627576816 c 366.44922 k 1752.9202 m -0.4212201\n",
      "74 Train Loss 37.985577 Test RE 0.9466702385864905 c 371.2741 k 1776.0294 m -0.42643526\n",
      "75 Train Loss 37.443893 Test RE 0.9469060070500644 c 372.3206 k 1781.0356 m -0.42588598\n",
      "76 Train Loss 36.722603 Test RE 0.9473524597040431 c 375.5494 k 1796.4542 m -0.42361543\n",
      "77 Train Loss 36.13347 Test RE 0.9479915550913771 c 380.3671 k 1819.4584 m -0.42149276\n",
      "78 Train Loss 35.670715 Test RE 0.9478882807242609 c 379.8173 k 1816.8191 m -0.41869923\n",
      "79 Train Loss 35.582886 Test RE 0.9479095027244707 c 379.93082 k 1817.3729 m -0.4180983\n",
      "80 Train Loss 35.496574 Test RE 0.9480019146813942 c 380.8168 k 1821.6155 m -0.41826978\n",
      "81 Train Loss 35.337936 Test RE 0.9475816794626767 c 377.70007 k 1806.7017 m -0.4165911\n",
      "82 Train Loss 35.187782 Test RE 0.9476924903417896 c 378.23828 k 1809.2971 m -0.4174089\n",
      "83 Train Loss 34.999546 Test RE 0.9483253959416925 c 383.18976 k 1833.0168 m -0.41958192\n",
      "84 Train Loss 34.81205 Test RE 0.9484677793469973 c 384.1809 k 1837.768 m -0.41901252\n",
      "85 Train Loss 34.519 Test RE 0.9484568011667353 c 383.77893 k 1835.8657 m -0.4196429\n",
      "86 Train Loss 34.411873 Test RE 0.9484457527965389 c 384.1571 k 1837.6699 m -0.41919926\n",
      "87 Train Loss 34.159897 Test RE 0.9488258498179378 c 386.80814 k 1850.3483 m -0.41737086\n",
      "88 Train Loss 33.716743 Test RE 0.9493055344838103 c 390.3309 k 1867.1934 m -0.41801926\n",
      "89 Train Loss 33.212055 Test RE 0.9491501533548193 c 389.4494 k 1862.9584 m -0.41488433\n",
      "90 Train Loss 32.987312 Test RE 0.9491705494743904 c 389.26926 k 1862.0968 m -0.41228843\n",
      "91 Train Loss 32.44432 Test RE 0.9493017719413833 c 390.34186 k 1867.199 m -0.40736815\n",
      "92 Train Loss 32.089977 Test RE 0.9503724754028395 c 399.0113 k 1908.6866 m -0.40810543\n",
      "93 Train Loss 31.3625 Test RE 0.9516028975902405 c 408.99994 k 1956.4772 m -0.4023863\n",
      "94 Train Loss 30.230368 Test RE 0.9519965743473195 c 412.80057 k 1974.6212 m -0.38990766\n",
      "95 Train Loss 29.920254 Test RE 0.9520880767381303 c 413.2773 k 1976.8936 m -0.3881035\n",
      "96 Train Loss 29.139647 Test RE 0.953303274072831 c 424.16754 k 2029.0549 m -0.3908486\n",
      "97 Train Loss 28.314062 Test RE 0.9536286316208711 c 427.9083 k 2046.9618 m -0.39021906\n",
      "98 Train Loss 26.980335 Test RE 0.9546904165524016 c 437.0103 k 2090.5212 m -0.38925812\n",
      "99 Train Loss 26.370087 Test RE 0.955156622046114 c 441.78143 k 2113.3423 m -0.3884489\n",
      "100 Train Loss 26.007336 Test RE 0.955662921995534 c 447.01282 k 2138.4062 m -0.3869738\n",
      "101 Train Loss 25.465683 Test RE 0.9563907754968903 c 454.5143 k 2174.3506 m -0.3778666\n",
      "102 Train Loss 25.12775 Test RE 0.9572358515172688 c 463.53232 k 2217.5483 m -0.37562057\n",
      "103 Train Loss 24.904694 Test RE 0.9578006698531265 c 469.66165 k 2246.9087 m -0.3793999\n",
      "104 Train Loss 24.662226 Test RE 0.9581350738438212 c 473.61664 k 2265.8325 m -0.3785143\n",
      "105 Train Loss 24.529037 Test RE 0.9584888746411537 c 477.48306 k 2284.337 m -0.37533864\n",
      "106 Train Loss 24.449507 Test RE 0.9585246905117971 c 477.79758 k 2285.8308 m -0.37608624\n",
      "107 Train Loss 24.402533 Test RE 0.95831320635304 c 475.44595 k 2274.5652 m -0.37664148\n",
      "108 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "109 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "110 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "111 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "112 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "113 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "114 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "115 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "116 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "117 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "118 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "119 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "120 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "121 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "122 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "123 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "124 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "125 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "126 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "127 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "128 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "129 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "130 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "131 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "132 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "133 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "134 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "135 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "136 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "137 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "138 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "139 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "140 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "141 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "142 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "143 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "144 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "145 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "146 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "147 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "148 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "149 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "150 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "151 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "152 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "153 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "154 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "155 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "156 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "157 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "158 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "159 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "160 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "161 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "162 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "163 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "164 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "165 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "166 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "167 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "168 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "169 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "170 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "171 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "172 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "173 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "174 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "175 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "176 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "177 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "178 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "179 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "180 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "181 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "182 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "183 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "184 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "185 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "186 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "187 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "188 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "189 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "190 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "191 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "192 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "193 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "194 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "195 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "196 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "197 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "198 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "199 Train Loss 24.396376 Test RE 0.9582007431176782 c 474.1722 k 2268.4656 m -0.37656203\n",
      "Training time: 41.41\n",
      "Training time: 41.41\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 10000.052 Test RE 0.9999558485680521 c -0.0017260106 k 0.0037455377 m -2.5148033e-06\n",
      "1 Train Loss 10000.051 Test RE 0.9999270488558895 c -0.0010520725 k 0.0066937227 m -4.457066e-06\n",
      "2 Train Loss 10000.051 Test RE 0.9999055590756802 c -0.0006017541 k 0.00907628 m -5.904795e-06\n",
      "3 Train Loss 10000.051 Test RE 0.999870608642095 c 5.058589e-05 k 0.012909884 m -8.13151e-06\n",
      "4 Train Loss 10000.051 Test RE 0.9998009892103502 c 0.0012774817 k 0.020625984 m -1.2478402e-05\n",
      "5 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "6 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "7 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "8 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "9 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "10 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "11 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "12 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "13 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "14 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "15 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "16 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "17 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "18 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "19 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "20 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "21 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "22 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "23 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "24 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "25 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "26 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "27 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "28 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "29 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "30 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "31 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "32 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "33 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "34 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "35 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "36 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "37 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "38 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "39 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "40 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "41 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "42 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "43 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "44 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "45 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "46 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "47 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "48 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "49 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "50 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "51 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "52 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "53 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "54 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "55 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "56 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "57 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "58 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "59 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "60 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "61 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "62 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "63 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "64 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "65 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "66 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "67 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "68 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "69 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "70 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "71 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "72 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "73 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "74 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "75 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "76 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "77 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "78 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "79 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "80 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "81 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "82 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "83 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "84 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "85 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "86 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "87 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "88 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "89 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "90 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "91 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "92 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "93 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "94 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "95 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "96 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "97 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "98 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "99 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "100 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "101 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "102 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "103 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "104 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "105 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "106 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "107 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "108 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "109 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "110 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "111 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "112 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "113 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "114 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "115 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "116 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "117 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "118 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "119 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "120 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "121 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "122 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "123 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "124 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "125 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "126 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "127 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "128 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "129 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "130 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "131 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "132 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "133 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "134 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "135 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "136 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "137 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "138 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "139 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "140 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "141 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "142 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "143 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "144 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "145 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "146 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "147 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "148 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "149 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "150 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "151 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "152 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "153 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "154 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "155 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "156 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "157 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "158 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "159 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "160 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "161 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "162 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "163 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "164 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "165 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "166 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "167 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "168 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "169 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "170 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "171 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "172 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "173 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "174 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "175 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "176 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "177 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "178 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "179 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "180 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "181 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "182 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "183 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "184 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "185 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "186 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "187 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "188 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "189 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "190 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "191 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "192 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "193 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "194 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "195 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "196 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "197 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "198 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "199 Train Loss 4917.8306 Test RE 0.39572878946231055 c 17.045687 k 111.69247 m -0.061843228\n",
      "Training time: 35.53\n",
      "Training time: 35.53\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 10000.051 Test RE 0.9999691199934656 c 0.0007603527 k -6.85759e-10 m 4.156675e-13\n",
      "1 Train Loss 10000.051 Test RE 0.9999750520752847 c 0.00081112806 k 7.649608e-05 m -1.0214315e-08\n",
      "2 Train Loss 10000.051 Test RE 0.9999764246922538 c 0.00094054744 k 0.00013253275 m -2.6607728e-08\n",
      "3 Train Loss 10000.051 Test RE 0.9999795457483199 c 0.0012180188 k 0.00026939285 m -6.439175e-08\n",
      "4 Train Loss 10000.051 Test RE 0.9999800892506762 c 0.0016502456 k 0.00045554966 m -1.2930334e-07\n",
      "5 Train Loss 10000.051 Test RE 0.999980773398884 c 0.0023558275 k 0.0007993221 m -2.524356e-07\n",
      "6 Train Loss 10000.051 Test RE 0.9999790441669619 c 0.003581842 k 0.0014328498 m -4.90875e-07\n",
      "7 Train Loss 10000.051 Test RE 0.9999740580145539 c 0.0058886465 k 0.0027211607 m -9.788024e-07\n",
      "8 Train Loss 10000.051 Test RE 0.9999594057788075 c 0.011207007 k 0.0058225314 m -2.1560381e-06\n",
      "9 Train Loss 10000.051 Test RE 0.9999034472792416 c 0.029990794 k 0.016962958 m -6.387825e-06\n",
      "10 Train Loss 7062.9194 Test RE 0.6301554241286633 c 168.43588 k 101.19167 m -0.039264694\n",
      "11 Train Loss 1135.8044 Test RE 0.6963607508893475 c 463.86337 k 281.20956 m -0.109352715\n",
      "12 Train Loss 316.31445 Test RE 0.840793498216272 c 967.5197 k 561.54663 m -0.21294355\n",
      "13 Train Loss 109.25665 Test RE 0.9027402516518226 c 1650.0713 k 952.59686 m -0.35945043\n",
      "14 Train Loss 45.319065 Test RE 0.9365401859685237 c 2559.9712 k 1477.0812 m -0.55622613\n",
      "15 Train Loss 21.089027 Test RE 0.9564233311600449 c 3748.01 k 2163.5134 m -0.8138342\n",
      "16 Train Loss 12.227417 Test RE 0.966881427320735 c 4944.525 k 2855.4763 m -1.0735528\n",
      "17 Train Loss 7.4459777 Test RE 0.9741882232188698 c 6351.2637 k 3669.3115 m -1.3790383\n",
      "18 Train Loss 4.880274 Test RE 0.9790440698433305 c 7823.96 k 4521.4272 m -1.698903\n",
      "19 Train Loss 3.3140993 Test RE 0.9827671204215784 c 9515.327 k 5500.131 m -2.0662925\n",
      "20 Train Loss 2.3783877 Test RE 0.9853046335295365 c 11174.61 k 6460.31 m -2.426732\n",
      "21 Train Loss 1.7861936 Test RE 0.9873634688570717 c 12990.265 k 7511.0044 m -2.821152\n",
      "22 Train Loss 1.4012195 Test RE 0.9887873146415526 c 14651.873 k 8472.572 m -3.182116\n",
      "23 Train Loss 1.1304948 Test RE 0.9900034542446635 c 16429.252 k 9501.146 m -3.5682354\n",
      "24 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "25 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "26 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "27 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "28 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "29 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "30 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "31 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "32 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "33 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "34 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "35 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "36 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "37 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "38 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "39 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "40 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "41 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "42 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "43 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "44 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "45 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "46 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "47 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "48 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "49 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "50 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "51 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "52 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "53 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "54 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "55 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "56 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "57 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "58 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "59 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "60 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "61 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "62 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "63 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "64 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "65 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "66 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "67 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "68 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "69 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "70 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "71 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "72 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "73 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "74 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "75 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "76 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "77 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "78 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "79 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "80 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "81 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "82 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "83 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "84 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "85 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "86 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "87 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "88 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "89 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "90 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "91 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "92 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "93 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "94 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "95 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "96 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "97 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "98 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "99 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "100 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "101 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "102 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "103 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "104 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "105 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "106 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "107 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "108 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "109 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "110 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "111 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "112 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "113 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "114 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "115 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "116 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "117 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "118 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "119 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "120 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "121 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "122 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "123 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "124 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "125 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "126 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "127 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "128 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "129 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "130 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "131 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "132 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "133 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "134 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "135 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "136 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "137 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "138 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "139 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "140 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "141 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "142 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "143 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "144 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "145 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "146 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "147 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "148 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "149 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "150 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "151 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "152 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "153 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "154 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "155 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "156 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "157 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "158 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "159 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "160 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "161 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "162 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "163 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "164 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "165 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "166 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "167 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "168 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "169 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "170 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "171 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "172 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "173 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "174 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "175 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "176 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "177 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "178 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "179 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "180 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "181 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "182 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "183 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "184 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "185 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "186 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "187 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "188 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "189 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "190 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "191 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "192 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "193 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "194 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "195 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "196 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "197 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "198 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "199 Train Loss 1.0809801 Test RE 0.9902183531043628 c 16793.6 k 9711.997 m -3.647387\n",
      "Training time: 39.34\n",
      "Training time: 39.34\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 10000.051 Test RE 0.9999864210954444 c 0.00030442173 k 0.00016209061 m -6.3667684e-08\n",
      "1 Train Loss 10000.051 Test RE 0.9999938889325456 c 0.0006629146 k 0.00049656176 m -1.5707971e-07\n",
      "2 Train Loss 10000.051 Test RE 0.9999747147505158 c 0.0011045441 k 0.0005597618 m -2.6503798e-07\n",
      "3 Train Loss 10000.05 Test RE 0.9999723802165322 c 0.0020611372 k 0.0013359556 m -6.1006995e-07\n",
      "4 Train Loss 10000.05 Test RE 0.9999688048413173 c 0.0026023004 k 0.0018773604 m -8.554764e-07\n",
      "5 Train Loss 10000.05 Test RE 0.999960666448742 c 0.0032742014 k 0.0026516137 m -1.202841e-06\n",
      "6 Train Loss 10000.05 Test RE 0.9999484992832243 c 0.0042458167 k 0.003984513 m -1.7620678e-06\n",
      "7 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "8 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "9 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "10 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "11 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "12 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "13 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "14 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "15 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "16 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "17 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "18 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "19 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "20 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "21 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "22 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "23 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "24 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "25 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "26 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "27 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "28 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "29 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "30 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "31 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "32 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "33 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "34 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "35 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "36 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "37 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "38 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "39 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "40 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "41 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "42 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "43 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "44 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "45 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "46 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "47 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "48 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "49 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "50 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "51 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "52 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "53 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "54 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "55 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "56 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "57 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "58 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "59 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "60 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "61 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "62 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "63 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "64 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "65 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "66 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "67 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "68 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "69 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "70 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "71 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "72 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "73 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "74 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "75 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "76 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "77 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "78 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "79 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "80 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "81 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "82 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "83 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "84 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "85 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "86 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "87 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "88 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "89 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "90 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "91 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "92 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "93 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "94 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "95 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "96 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "97 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "98 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "99 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "100 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "101 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "102 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "103 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "104 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "105 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "106 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "107 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "108 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "109 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "110 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "111 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "112 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "113 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "114 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "115 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "116 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "117 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "118 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "119 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "120 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "121 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "122 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "123 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "124 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "125 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "126 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "127 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "128 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "129 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "130 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "131 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "132 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "133 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "134 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "135 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "136 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "137 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "138 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "139 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "140 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "141 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "142 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "143 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "144 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "145 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "146 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "147 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "148 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "149 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "150 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "151 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "152 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "153 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "154 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "155 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "156 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "157 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "158 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "159 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "160 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "161 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "162 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "163 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "164 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "165 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "166 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "167 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "168 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "169 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "170 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "171 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "172 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "173 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "174 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "175 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "176 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "177 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "178 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "179 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "180 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "181 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "182 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "183 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "184 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "185 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "186 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "187 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "188 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "189 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "190 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "191 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "192 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "193 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "194 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "195 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "196 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "197 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "198 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "199 Train Loss 10000.05 Test RE 0.9999230717746755 c 0.006068611 k 0.006750594 m -2.8788938e-06\n",
      "Training time: 27.01\n",
      "Training time: 27.01\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 10000.05 Test RE 0.9999751777913166 c 0.00044432216 k -3.3456826e-10 m -1.573867e-13\n",
      "1 Train Loss 10000.05 Test RE 0.9999768089822385 c 0.000604225 k 8.1936894e-05 m -2.0347766e-08\n",
      "2 Train Loss 10000.05 Test RE 0.9999752363922927 c 0.00086153665 k 0.00018793382 m -6.6125864e-08\n",
      "3 Train Loss 10000.05 Test RE 0.9999750387991244 c 0.0012391836 k 0.0003986114 m -1.6388907e-07\n",
      "4 Train Loss 10000.05 Test RE 0.9999719429504526 c 0.0017937608 k 0.0007198268 m -3.4455906e-07\n",
      "5 Train Loss 10000.05 Test RE 0.9999681600327753 c 0.0026391048 k 0.0012929145 m -6.827857e-07\n",
      "6 Train Loss 10000.05 Test RE 0.9999596167729159 c 0.0040141037 k 0.0023125773 m -1.3200021e-06\n",
      "7 Train Loss 10000.05 Test RE 0.9999431011548308 c 0.006493645 k 0.0043161456 m -2.595481e-06\n",
      "8 Train Loss 10000.05 Test RE 0.9999048383374796 c 0.0118285045 k 0.008852769 m -5.510331e-06\n",
      "9 Train Loss 5864.11 Test RE 0.3606047144767129 c 106.96792 k 96.24437 m -0.06217754\n",
      "10 Train Loss 2996.4082 Test RE 0.6045738138677668 c 167.43031 k 170.03987 m -0.10824562\n",
      "11 Train Loss 993.7522 Test RE 0.7246677159238418 c 306.35767 k 318.5863 m -0.20254678\n",
      "12 Train Loss 472.43826 Test RE 0.7990259043601211 c 429.35065 k 453.96518 m -0.28744882\n",
      "13 Train Loss 161.49673 Test RE 0.8796592730687998 c 715.2114 k 773.0441 m -0.48626074\n",
      "14 Train Loss 68.54826 Test RE 0.9223368618674874 c 1101.4143 k 1198.965 m -0.7528556\n",
      "15 Train Loss 33.075356 Test RE 0.9451868132316308 c 1574.4325 k 1718.0647 m -1.0799398\n",
      "16 Train Loss 17.6611 Test RE 0.9608602083937142 c 2204.236 k 2407.9329 m -1.5155864\n",
      "17 Train Loss 10.261782 Test RE 0.9694234656642106 c 2832.0198 k 3094.888 m -1.9497106\n",
      "18 Train Loss 7.424633 Test RE 0.9741835020301389 c 3352.739 k 3664.4531 m -2.3098922\n",
      "19 Train Loss 6.1064973 Test RE 0.9763619504423692 c 3671.714 k 4013.2944 m -2.530637\n",
      "20 Train Loss 5.761066 Test RE 0.9769068445144062 c 3764.2366 k 4114.477 m -2.5946352\n",
      "21 Train Loss nan Test RE nan c nan k nan m nan\n",
      "22 Train Loss nan Test RE nan c nan k nan m nan\n",
      "23 Train Loss nan Test RE nan c nan k nan m nan\n",
      "24 Train Loss nan Test RE nan c nan k nan m nan\n",
      "25 Train Loss nan Test RE nan c nan k nan m nan\n",
      "26 Train Loss nan Test RE nan c nan k nan m nan\n",
      "27 Train Loss nan Test RE nan c nan k nan m nan\n",
      "28 Train Loss nan Test RE nan c nan k nan m nan\n",
      "29 Train Loss nan Test RE nan c nan k nan m nan\n",
      "30 Train Loss nan Test RE nan c nan k nan m nan\n",
      "31 Train Loss nan Test RE nan c nan k nan m nan\n",
      "32 Train Loss nan Test RE nan c nan k nan m nan\n",
      "33 Train Loss nan Test RE nan c nan k nan m nan\n",
      "34 Train Loss nan Test RE nan c nan k nan m nan\n",
      "35 Train Loss nan Test RE nan c nan k nan m nan\n",
      "36 Train Loss nan Test RE nan c nan k nan m nan\n",
      "37 Train Loss nan Test RE nan c nan k nan m nan\n",
      "38 Train Loss nan Test RE nan c nan k nan m nan\n",
      "39 Train Loss nan Test RE nan c nan k nan m nan\n",
      "40 Train Loss nan Test RE nan c nan k nan m nan\n",
      "41 Train Loss nan Test RE nan c nan k nan m nan\n",
      "42 Train Loss nan Test RE nan c nan k nan m nan\n",
      "43 Train Loss nan Test RE nan c nan k nan m nan\n",
      "44 Train Loss nan Test RE nan c nan k nan m nan\n",
      "45 Train Loss nan Test RE nan c nan k nan m nan\n",
      "46 Train Loss nan Test RE nan c nan k nan m nan\n",
      "47 Train Loss nan Test RE nan c nan k nan m nan\n",
      "48 Train Loss nan Test RE nan c nan k nan m nan\n",
      "49 Train Loss nan Test RE nan c nan k nan m nan\n",
      "50 Train Loss nan Test RE nan c nan k nan m nan\n",
      "51 Train Loss nan Test RE nan c nan k nan m nan\n",
      "52 Train Loss nan Test RE nan c nan k nan m nan\n",
      "53 Train Loss nan Test RE nan c nan k nan m nan\n",
      "54 Train Loss nan Test RE nan c nan k nan m nan\n",
      "55 Train Loss nan Test RE nan c nan k nan m nan\n",
      "56 Train Loss nan Test RE nan c nan k nan m nan\n",
      "57 Train Loss nan Test RE nan c nan k nan m nan\n",
      "58 Train Loss nan Test RE nan c nan k nan m nan\n",
      "59 Train Loss nan Test RE nan c nan k nan m nan\n",
      "60 Train Loss nan Test RE nan c nan k nan m nan\n",
      "61 Train Loss nan Test RE nan c nan k nan m nan\n",
      "62 Train Loss nan Test RE nan c nan k nan m nan\n",
      "63 Train Loss nan Test RE nan c nan k nan m nan\n",
      "64 Train Loss nan Test RE nan c nan k nan m nan\n",
      "65 Train Loss nan Test RE nan c nan k nan m nan\n",
      "66 Train Loss nan Test RE nan c nan k nan m nan\n",
      "67 Train Loss nan Test RE nan c nan k nan m nan\n",
      "68 Train Loss nan Test RE nan c nan k nan m nan\n",
      "69 Train Loss nan Test RE nan c nan k nan m nan\n",
      "70 Train Loss nan Test RE nan c nan k nan m nan\n",
      "71 Train Loss nan Test RE nan c nan k nan m nan\n",
      "72 Train Loss nan Test RE nan c nan k nan m nan\n",
      "73 Train Loss nan Test RE nan c nan k nan m nan\n",
      "74 Train Loss nan Test RE nan c nan k nan m nan\n",
      "75 Train Loss nan Test RE nan c nan k nan m nan\n",
      "76 Train Loss nan Test RE nan c nan k nan m nan\n",
      "77 Train Loss nan Test RE nan c nan k nan m nan\n",
      "78 Train Loss nan Test RE nan c nan k nan m nan\n",
      "79 Train Loss nan Test RE nan c nan k nan m nan\n",
      "80 Train Loss nan Test RE nan c nan k nan m nan\n",
      "81 Train Loss nan Test RE nan c nan k nan m nan\n",
      "82 Train Loss nan Test RE nan c nan k nan m nan\n",
      "83 Train Loss nan Test RE nan c nan k nan m nan\n",
      "84 Train Loss nan Test RE nan c nan k nan m nan\n",
      "85 Train Loss nan Test RE nan c nan k nan m nan\n",
      "86 Train Loss nan Test RE nan c nan k nan m nan\n",
      "87 Train Loss nan Test RE nan c nan k nan m nan\n",
      "88 Train Loss nan Test RE nan c nan k nan m nan\n",
      "89 Train Loss nan Test RE nan c nan k nan m nan\n",
      "90 Train Loss nan Test RE nan c nan k nan m nan\n",
      "91 Train Loss nan Test RE nan c nan k nan m nan\n",
      "92 Train Loss nan Test RE nan c nan k nan m nan\n",
      "93 Train Loss nan Test RE nan c nan k nan m nan\n",
      "94 Train Loss nan Test RE nan c nan k nan m nan\n",
      "95 Train Loss nan Test RE nan c nan k nan m nan\n",
      "96 Train Loss nan Test RE nan c nan k nan m nan\n",
      "97 Train Loss nan Test RE nan c nan k nan m nan\n",
      "98 Train Loss nan Test RE nan c nan k nan m nan\n",
      "99 Train Loss nan Test RE nan c nan k nan m nan\n",
      "100 Train Loss nan Test RE nan c nan k nan m nan\n",
      "101 Train Loss nan Test RE nan c nan k nan m nan\n",
      "102 Train Loss nan Test RE nan c nan k nan m nan\n",
      "103 Train Loss nan Test RE nan c nan k nan m nan\n",
      "104 Train Loss nan Test RE nan c nan k nan m nan\n",
      "105 Train Loss nan Test RE nan c nan k nan m nan\n",
      "106 Train Loss nan Test RE nan c nan k nan m nan\n",
      "107 Train Loss nan Test RE nan c nan k nan m nan\n",
      "108 Train Loss nan Test RE nan c nan k nan m nan\n",
      "109 Train Loss nan Test RE nan c nan k nan m nan\n",
      "110 Train Loss nan Test RE nan c nan k nan m nan\n",
      "111 Train Loss nan Test RE nan c nan k nan m nan\n",
      "112 Train Loss nan Test RE nan c nan k nan m nan\n",
      "113 Train Loss nan Test RE nan c nan k nan m nan\n",
      "114 Train Loss nan Test RE nan c nan k nan m nan\n",
      "115 Train Loss nan Test RE nan c nan k nan m nan\n",
      "116 Train Loss nan Test RE nan c nan k nan m nan\n",
      "117 Train Loss nan Test RE nan c nan k nan m nan\n",
      "118 Train Loss nan Test RE nan c nan k nan m nan\n",
      "119 Train Loss nan Test RE nan c nan k nan m nan\n",
      "120 Train Loss nan Test RE nan c nan k nan m nan\n",
      "121 Train Loss nan Test RE nan c nan k nan m nan\n",
      "122 Train Loss nan Test RE nan c nan k nan m nan\n",
      "123 Train Loss nan Test RE nan c nan k nan m nan\n",
      "124 Train Loss nan Test RE nan c nan k nan m nan\n",
      "125 Train Loss nan Test RE nan c nan k nan m nan\n",
      "126 Train Loss nan Test RE nan c nan k nan m nan\n",
      "127 Train Loss nan Test RE nan c nan k nan m nan\n",
      "128 Train Loss nan Test RE nan c nan k nan m nan\n",
      "129 Train Loss nan Test RE nan c nan k nan m nan\n",
      "130 Train Loss nan Test RE nan c nan k nan m nan\n",
      "131 Train Loss nan Test RE nan c nan k nan m nan\n",
      "132 Train Loss nan Test RE nan c nan k nan m nan\n",
      "133 Train Loss nan Test RE nan c nan k nan m nan\n",
      "134 Train Loss nan Test RE nan c nan k nan m nan\n",
      "135 Train Loss nan Test RE nan c nan k nan m nan\n",
      "136 Train Loss nan Test RE nan c nan k nan m nan\n",
      "137 Train Loss nan Test RE nan c nan k nan m nan\n",
      "138 Train Loss nan Test RE nan c nan k nan m nan\n",
      "139 Train Loss nan Test RE nan c nan k nan m nan\n",
      "140 Train Loss nan Test RE nan c nan k nan m nan\n",
      "141 Train Loss nan Test RE nan c nan k nan m nan\n",
      "142 Train Loss nan Test RE nan c nan k nan m nan\n",
      "143 Train Loss nan Test RE nan c nan k nan m nan\n",
      "144 Train Loss nan Test RE nan c nan k nan m nan\n",
      "145 Train Loss nan Test RE nan c nan k nan m nan\n",
      "146 Train Loss nan Test RE nan c nan k nan m nan\n",
      "147 Train Loss nan Test RE nan c nan k nan m nan\n",
      "148 Train Loss nan Test RE nan c nan k nan m nan\n",
      "149 Train Loss nan Test RE nan c nan k nan m nan\n",
      "150 Train Loss nan Test RE nan c nan k nan m nan\n",
      "151 Train Loss nan Test RE nan c nan k nan m nan\n",
      "152 Train Loss nan Test RE nan c nan k nan m nan\n",
      "153 Train Loss nan Test RE nan c nan k nan m nan\n",
      "154 Train Loss nan Test RE nan c nan k nan m nan\n",
      "155 Train Loss nan Test RE nan c nan k nan m nan\n",
      "156 Train Loss nan Test RE nan c nan k nan m nan\n",
      "157 Train Loss nan Test RE nan c nan k nan m nan\n",
      "158 Train Loss nan Test RE nan c nan k nan m nan\n",
      "159 Train Loss nan Test RE nan c nan k nan m nan\n",
      "160 Train Loss nan Test RE nan c nan k nan m nan\n",
      "161 Train Loss nan Test RE nan c nan k nan m nan\n",
      "162 Train Loss nan Test RE nan c nan k nan m nan\n",
      "163 Train Loss nan Test RE nan c nan k nan m nan\n",
      "164 Train Loss nan Test RE nan c nan k nan m nan\n",
      "165 Train Loss nan Test RE nan c nan k nan m nan\n",
      "166 Train Loss nan Test RE nan c nan k nan m nan\n",
      "167 Train Loss nan Test RE nan c nan k nan m nan\n",
      "168 Train Loss nan Test RE nan c nan k nan m nan\n",
      "169 Train Loss nan Test RE nan c nan k nan m nan\n",
      "170 Train Loss nan Test RE nan c nan k nan m nan\n",
      "171 Train Loss nan Test RE nan c nan k nan m nan\n",
      "172 Train Loss nan Test RE nan c nan k nan m nan\n",
      "173 Train Loss nan Test RE nan c nan k nan m nan\n",
      "174 Train Loss nan Test RE nan c nan k nan m nan\n",
      "175 Train Loss nan Test RE nan c nan k nan m nan\n",
      "176 Train Loss nan Test RE nan c nan k nan m nan\n",
      "177 Train Loss nan Test RE nan c nan k nan m nan\n",
      "178 Train Loss nan Test RE nan c nan k nan m nan\n",
      "179 Train Loss nan Test RE nan c nan k nan m nan\n",
      "180 Train Loss nan Test RE nan c nan k nan m nan\n",
      "181 Train Loss nan Test RE nan c nan k nan m nan\n",
      "182 Train Loss nan Test RE nan c nan k nan m nan\n",
      "183 Train Loss nan Test RE nan c nan k nan m nan\n",
      "184 Train Loss nan Test RE nan c nan k nan m nan\n",
      "185 Train Loss nan Test RE nan c nan k nan m nan\n",
      "186 Train Loss nan Test RE nan c nan k nan m nan\n",
      "187 Train Loss nan Test RE nan c nan k nan m nan\n",
      "188 Train Loss nan Test RE nan c nan k nan m nan\n",
      "189 Train Loss nan Test RE nan c nan k nan m nan\n",
      "190 Train Loss nan Test RE nan c nan k nan m nan\n",
      "191 Train Loss nan Test RE nan c nan k nan m nan\n",
      "192 Train Loss nan Test RE nan c nan k nan m nan\n",
      "193 Train Loss nan Test RE nan c nan k nan m nan\n",
      "194 Train Loss nan Test RE nan c nan k nan m nan\n",
      "195 Train Loss nan Test RE nan c nan k nan m nan\n",
      "196 Train Loss nan Test RE nan c nan k nan m nan\n",
      "197 Train Loss nan Test RE nan c nan k nan m nan\n",
      "198 Train Loss nan Test RE nan c nan k nan m nan\n",
      "199 Train Loss nan Test RE nan c nan k nan m nan\n",
      "Training time: 71.70\n",
      "Training time: 71.70\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 10000.052 Test RE 0.9999648981158274 c -0.002379665 k 0.0035942171 m -1.918405e-06\n",
      "1 Train Loss 10000.051 Test RE 0.9999338171423704 c -0.0015494177 k 0.0066002873 m -3.3306878e-06\n",
      "2 Train Loss 10000.051 Test RE 0.9999063440804202 c -0.0010277742 k 0.008844469 m -4.3067494e-06\n",
      "3 Train Loss 10000.051 Test RE 0.9998772796373753 c -0.00034673803 k 0.012397513 m -5.7059215e-06\n",
      "4 Train Loss 10000.051 Test RE 0.9998191162945045 c 0.0008029429 k 0.018918721 m -8.1864255e-06\n",
      "5 Train Loss 10000.034 Test RE 0.9958932410004018 c 0.069627434 k 0.4440734 m -0.00016437571\n",
      "6 Train Loss 9053.603 Test RE 0.840070703503873 c 13.351461 k 89.972115 m -0.032355845\n",
      "7 Train Loss 954.2454 Test RE 0.7215572115798745 c 42.180016 k 303.4305 m -0.10684264\n",
      "8 Train Loss 204.93707 Test RE 0.866233298114204 c 93.62284 k 683.5247 m -0.23957591\n",
      "9 Train Loss 80.58269 Test RE 0.915981539943897 c 151.334 k 1107.1917 m -0.38777405\n",
      "10 Train Loss 35.706577 Test RE 0.9432992836699583 c 226.5637 k 1658.4105 m -0.58066833\n",
      "11 Train Loss 18.834915 Test RE 0.9588781539737254 c 313.72913 k 2296.62 m -0.80394894\n",
      "12 Train Loss 11.067072 Test RE 0.9683100302676972 c 408.08752 k 2987.261 m -1.0455285\n",
      "13 Train Loss 6.673217 Test RE 0.9753673472848786 c 525.8072 k 3848.7393 m -1.3468301\n",
      "14 Train Loss 4.425957 Test RE 0.9800125412663773 c 648.74567 k 4748.3267 m -1.6613059\n",
      "15 Train Loss 3.0384035 Test RE 0.9833631733169175 c 780.1438 k 5709.773 m -1.9973046\n",
      "16 Train Loss 2.268681 Test RE 0.9858913433225012 c 918.24207 k 6720.2124 m -2.3502326\n",
      "17 Train Loss 1.741831 Test RE 0.987481087438757 c 1036.5746 k 7586.013 m -2.6526263\n",
      "18 Train Loss 1.3223743 Test RE 0.9891089694890719 c 1192.2606 k 8725.104 m -3.050399\n",
      "19 Train Loss 1.0612218 Test RE 0.9903338778121987 c 1342.7341 k 9826.045 m -3.434784\n",
      "20 Train Loss 0.8491157 Test RE 0.9913905673425846 c 1508.0724 k 11035.738 m -3.8570688\n",
      "21 Train Loss 0.70655566 Test RE 0.9921969839755689 c 1664.1182 k 12177.436 m -4.2555666\n",
      "22 Train Loss 0.58940184 Test RE 0.9929393375347744 c 1839.055 k 13457.34 m -4.7022567\n",
      "23 Train Loss 0.52669096 Test RE 0.99333539923001 c 1950.3816 k 14271.847 m -4.9865\n",
      "24 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "25 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "26 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "27 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "28 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "29 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "30 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "31 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "32 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "33 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "34 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "35 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "36 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "37 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "38 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "39 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "40 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "41 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "42 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "43 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "44 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "45 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "46 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "47 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "48 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "49 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "50 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "51 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "52 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "53 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "54 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "55 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "56 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "57 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "58 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "59 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "60 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "61 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "62 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "63 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "64 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "65 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "66 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "67 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "68 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "69 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "70 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "71 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "72 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "73 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "74 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "75 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "76 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "77 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "78 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "79 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "80 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "81 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "82 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "83 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "84 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "85 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "86 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "87 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "88 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "89 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "90 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "91 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "92 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "93 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "94 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "95 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "96 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "97 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "98 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "99 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "100 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "101 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "102 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "103 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "104 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "105 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "106 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "107 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "108 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "109 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "110 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "111 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "112 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "113 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "114 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "115 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "116 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "117 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "118 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "119 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "120 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "121 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "122 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "123 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "124 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "125 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "126 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "127 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "128 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "129 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "130 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "131 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "132 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "133 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "134 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "135 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "136 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "137 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "138 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "139 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "140 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "141 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "142 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "143 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "144 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "145 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "146 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "147 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "148 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "149 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "150 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "151 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "152 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "153 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "154 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "155 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "156 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "157 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "158 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "159 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "160 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "161 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "162 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "163 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "164 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "165 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "166 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "167 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "168 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "169 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "170 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "171 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "172 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "173 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "174 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "175 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "176 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "177 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "178 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "179 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "180 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "181 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "182 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "183 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "184 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "185 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "186 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "187 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "188 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "189 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "190 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "191 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "192 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "193 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "194 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "195 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "196 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "197 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "198 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "199 Train Loss 0.48968175 Test RE 0.9936492430130531 c 2044.447 k 14960.063 m -5.226663\n",
      "Training time: 36.62\n",
      "Training time: 36.62\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 10000.051 Test RE 0.9999541774150338 c 0.0052871886 k 0.002623205 m -1.8231195e-06\n",
      "1 Train Loss 10000.051 Test RE 0.9999487888971728 c 0.005752501 k 0.0034425694 m -2.3844968e-06\n",
      "2 Train Loss 10000.051 Test RE 0.999940316972075 c 0.006234019 k 0.0044752485 m -3.0643566e-06\n",
      "3 Train Loss 10000.051 Test RE 0.9999293537366118 c 0.0067674825 k 0.005868553 m -3.9259485e-06\n",
      "4 Train Loss 10000.051 Test RE 0.9999123705530746 c 0.0074373083 k 0.007905126 m -5.1238067e-06\n",
      "5 Train Loss 10000.051 Test RE 0.9998831977911518 c 0.0084642675 k 0.011395247 m -7.093393e-06\n",
      "6 Train Loss 10000.049 Test RE 0.9996048202388109 c 0.017370041 k 0.04435404 m -2.5126194e-05\n",
      "7 Train Loss 9947.113 Test RE 0.9876837668964982 c 4.894387 k 21.111057 m -0.010910493\n",
      "8 Train Loss 133.38817 Test RE 0.8938070846548852 c 199.88132 k 861.7517 m -0.4455285\n",
      "9 Train Loss 31.651974 Test RE 0.9456862826530443 c 407.05243 k 1750.1523 m -0.90552497\n",
      "10 Train Loss 14.019907 Test RE 0.9642378678863652 c 615.30115 k 2642.6091 m -1.3676989\n",
      "11 Train Loss 4.3840356 Test RE 0.9799634399815372 c 1108.0587 k 4753.779 m -2.4610872\n",
      "12 Train Loss 2.7180703 Test RE 0.9842404060580705 c 1408.0331 k 6039.06 m -3.126733\n",
      "13 Train Loss 1.9902782 Test RE 0.9865585820550953 c 1650.8673 k 7079.5674 m -3.6656034\n",
      "14 Train Loss 1.5587074 Test RE 0.9881614540804035 c 1874.975 k 8039.87 m -4.162932\n",
      "15 Train Loss 1.2224889 Test RE 0.9895743649408136 c 2128.142 k 9124.723 m -4.72476\n",
      "16 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "17 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "18 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "19 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "20 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "21 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "22 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "23 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "24 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "25 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "26 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "27 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "28 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "29 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "30 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "31 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "32 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "33 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "34 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "35 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "36 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "37 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "38 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "39 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "40 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "41 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "42 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "43 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "44 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "45 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "46 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "47 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "48 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "49 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "50 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "51 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "52 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "53 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "54 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "55 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "56 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "57 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "58 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "59 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "60 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "61 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "62 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "63 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "64 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "65 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "66 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "67 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "68 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "69 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "70 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "71 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "72 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "73 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "74 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "75 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "76 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "77 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "78 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "79 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "80 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "81 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "82 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "83 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "84 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "85 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "86 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "87 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "88 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "89 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "90 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "91 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "92 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "93 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "94 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "95 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "96 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "97 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "98 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "99 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "100 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "101 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "102 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "103 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "104 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "105 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "106 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "107 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "108 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "109 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "110 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "111 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "112 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "113 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "114 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "115 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "116 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "117 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "118 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "119 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "120 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "121 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "122 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "123 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "124 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "125 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "126 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "127 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "128 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "129 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "130 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "131 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "132 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "133 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "134 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "135 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "136 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "137 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "138 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "139 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "140 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "141 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "142 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "143 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "144 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "145 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "146 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "147 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "148 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "149 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "150 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "151 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "152 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "153 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "154 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "155 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "156 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "157 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "158 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "159 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "160 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "161 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "162 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "163 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "164 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "165 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "166 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "167 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "168 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "169 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "170 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "171 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "172 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "173 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "174 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "175 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "176 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "177 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "178 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "179 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "180 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "181 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "182 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "183 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "184 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "185 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "186 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "187 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "188 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "189 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "190 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "191 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "192 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "193 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "194 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "195 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "196 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "197 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "198 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "199 Train Loss 1.1313531 Test RE 0.9901043722740931 c 2244.7983 k 9624.617 m -4.983646\n",
      "Training time: 29.48\n",
      "Training time: 29.48\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 10000.052 Test RE 0.9999475915364259 c 0.008818127 k 0.004521011 m -3.417505e-06\n",
      "1 Train Loss 10000.051 Test RE 0.9999125834043057 c 0.010215069 k 0.008191489 m -5.6704766e-06\n",
      "2 Train Loss 10000.051 Test RE 0.9998833645757398 c 0.01108406 k 0.011635719 m -7.4607874e-06\n",
      "3 Train Loss 10000.051 Test RE 0.9998269104818902 c 0.012584386 k 0.018600566 m -1.0833881e-05\n",
      "4 Train Loss 9999.508 Test RE 0.9877670137803254 c 0.51422423 k 2.7093177 m -0.0012450751\n",
      "5 Train Loss nan Test RE nan c nan k nan m nan\n",
      "6 Train Loss nan Test RE nan c nan k nan m nan\n",
      "7 Train Loss nan Test RE nan c nan k nan m nan\n",
      "8 Train Loss nan Test RE nan c nan k nan m nan\n",
      "9 Train Loss nan Test RE nan c nan k nan m nan\n",
      "10 Train Loss nan Test RE nan c nan k nan m nan\n",
      "11 Train Loss nan Test RE nan c nan k nan m nan\n",
      "12 Train Loss nan Test RE nan c nan k nan m nan\n",
      "13 Train Loss nan Test RE nan c nan k nan m nan\n",
      "14 Train Loss nan Test RE nan c nan k nan m nan\n",
      "15 Train Loss nan Test RE nan c nan k nan m nan\n",
      "16 Train Loss nan Test RE nan c nan k nan m nan\n",
      "17 Train Loss nan Test RE nan c nan k nan m nan\n",
      "18 Train Loss nan Test RE nan c nan k nan m nan\n",
      "19 Train Loss nan Test RE nan c nan k nan m nan\n",
      "20 Train Loss nan Test RE nan c nan k nan m nan\n",
      "21 Train Loss nan Test RE nan c nan k nan m nan\n",
      "22 Train Loss nan Test RE nan c nan k nan m nan\n",
      "23 Train Loss nan Test RE nan c nan k nan m nan\n",
      "24 Train Loss nan Test RE nan c nan k nan m nan\n",
      "25 Train Loss nan Test RE nan c nan k nan m nan\n",
      "26 Train Loss nan Test RE nan c nan k nan m nan\n",
      "27 Train Loss nan Test RE nan c nan k nan m nan\n",
      "28 Train Loss nan Test RE nan c nan k nan m nan\n",
      "29 Train Loss nan Test RE nan c nan k nan m nan\n",
      "30 Train Loss nan Test RE nan c nan k nan m nan\n",
      "31 Train Loss nan Test RE nan c nan k nan m nan\n",
      "32 Train Loss nan Test RE nan c nan k nan m nan\n",
      "33 Train Loss nan Test RE nan c nan k nan m nan\n",
      "34 Train Loss nan Test RE nan c nan k nan m nan\n",
      "35 Train Loss nan Test RE nan c nan k nan m nan\n",
      "36 Train Loss nan Test RE nan c nan k nan m nan\n",
      "37 Train Loss nan Test RE nan c nan k nan m nan\n",
      "38 Train Loss nan Test RE nan c nan k nan m nan\n",
      "39 Train Loss nan Test RE nan c nan k nan m nan\n",
      "40 Train Loss nan Test RE nan c nan k nan m nan\n",
      "41 Train Loss nan Test RE nan c nan k nan m nan\n",
      "42 Train Loss nan Test RE nan c nan k nan m nan\n",
      "43 Train Loss nan Test RE nan c nan k nan m nan\n",
      "44 Train Loss nan Test RE nan c nan k nan m nan\n",
      "45 Train Loss nan Test RE nan c nan k nan m nan\n",
      "46 Train Loss nan Test RE nan c nan k nan m nan\n",
      "47 Train Loss nan Test RE nan c nan k nan m nan\n",
      "48 Train Loss nan Test RE nan c nan k nan m nan\n",
      "49 Train Loss nan Test RE nan c nan k nan m nan\n",
      "50 Train Loss nan Test RE nan c nan k nan m nan\n",
      "51 Train Loss nan Test RE nan c nan k nan m nan\n",
      "52 Train Loss nan Test RE nan c nan k nan m nan\n",
      "53 Train Loss nan Test RE nan c nan k nan m nan\n",
      "54 Train Loss nan Test RE nan c nan k nan m nan\n",
      "55 Train Loss nan Test RE nan c nan k nan m nan\n",
      "56 Train Loss nan Test RE nan c nan k nan m nan\n",
      "57 Train Loss nan Test RE nan c nan k nan m nan\n",
      "58 Train Loss nan Test RE nan c nan k nan m nan\n",
      "59 Train Loss nan Test RE nan c nan k nan m nan\n",
      "60 Train Loss nan Test RE nan c nan k nan m nan\n",
      "61 Train Loss nan Test RE nan c nan k nan m nan\n",
      "62 Train Loss nan Test RE nan c nan k nan m nan\n",
      "63 Train Loss nan Test RE nan c nan k nan m nan\n",
      "64 Train Loss nan Test RE nan c nan k nan m nan\n",
      "65 Train Loss nan Test RE nan c nan k nan m nan\n",
      "66 Train Loss nan Test RE nan c nan k nan m nan\n",
      "67 Train Loss nan Test RE nan c nan k nan m nan\n",
      "68 Train Loss nan Test RE nan c nan k nan m nan\n",
      "69 Train Loss nan Test RE nan c nan k nan m nan\n",
      "70 Train Loss nan Test RE nan c nan k nan m nan\n",
      "71 Train Loss nan Test RE nan c nan k nan m nan\n",
      "72 Train Loss nan Test RE nan c nan k nan m nan\n",
      "73 Train Loss nan Test RE nan c nan k nan m nan\n",
      "74 Train Loss nan Test RE nan c nan k nan m nan\n",
      "75 Train Loss nan Test RE nan c nan k nan m nan\n",
      "76 Train Loss nan Test RE nan c nan k nan m nan\n",
      "77 Train Loss nan Test RE nan c nan k nan m nan\n",
      "78 Train Loss nan Test RE nan c nan k nan m nan\n",
      "79 Train Loss nan Test RE nan c nan k nan m nan\n",
      "80 Train Loss nan Test RE nan c nan k nan m nan\n",
      "81 Train Loss nan Test RE nan c nan k nan m nan\n",
      "82 Train Loss nan Test RE nan c nan k nan m nan\n",
      "83 Train Loss nan Test RE nan c nan k nan m nan\n",
      "84 Train Loss nan Test RE nan c nan k nan m nan\n",
      "85 Train Loss nan Test RE nan c nan k nan m nan\n",
      "86 Train Loss nan Test RE nan c nan k nan m nan\n",
      "87 Train Loss nan Test RE nan c nan k nan m nan\n",
      "88 Train Loss nan Test RE nan c nan k nan m nan\n",
      "89 Train Loss nan Test RE nan c nan k nan m nan\n",
      "90 Train Loss nan Test RE nan c nan k nan m nan\n",
      "91 Train Loss nan Test RE nan c nan k nan m nan\n",
      "92 Train Loss nan Test RE nan c nan k nan m nan\n",
      "93 Train Loss nan Test RE nan c nan k nan m nan\n",
      "94 Train Loss nan Test RE nan c nan k nan m nan\n",
      "95 Train Loss nan Test RE nan c nan k nan m nan\n",
      "96 Train Loss nan Test RE nan c nan k nan m nan\n",
      "97 Train Loss nan Test RE nan c nan k nan m nan\n",
      "98 Train Loss nan Test RE nan c nan k nan m nan\n",
      "99 Train Loss nan Test RE nan c nan k nan m nan\n",
      "100 Train Loss nan Test RE nan c nan k nan m nan\n",
      "101 Train Loss nan Test RE nan c nan k nan m nan\n",
      "102 Train Loss nan Test RE nan c nan k nan m nan\n",
      "103 Train Loss nan Test RE nan c nan k nan m nan\n",
      "104 Train Loss nan Test RE nan c nan k nan m nan\n",
      "105 Train Loss nan Test RE nan c nan k nan m nan\n",
      "106 Train Loss nan Test RE nan c nan k nan m nan\n",
      "107 Train Loss nan Test RE nan c nan k nan m nan\n",
      "108 Train Loss nan Test RE nan c nan k nan m nan\n",
      "109 Train Loss nan Test RE nan c nan k nan m nan\n",
      "110 Train Loss nan Test RE nan c nan k nan m nan\n",
      "111 Train Loss nan Test RE nan c nan k nan m nan\n",
      "112 Train Loss nan Test RE nan c nan k nan m nan\n",
      "113 Train Loss nan Test RE nan c nan k nan m nan\n",
      "114 Train Loss nan Test RE nan c nan k nan m nan\n",
      "115 Train Loss nan Test RE nan c nan k nan m nan\n",
      "116 Train Loss nan Test RE nan c nan k nan m nan\n",
      "117 Train Loss nan Test RE nan c nan k nan m nan\n",
      "118 Train Loss nan Test RE nan c nan k nan m nan\n",
      "119 Train Loss nan Test RE nan c nan k nan m nan\n",
      "120 Train Loss nan Test RE nan c nan k nan m nan\n",
      "121 Train Loss nan Test RE nan c nan k nan m nan\n",
      "122 Train Loss nan Test RE nan c nan k nan m nan\n",
      "123 Train Loss nan Test RE nan c nan k nan m nan\n",
      "124 Train Loss nan Test RE nan c nan k nan m nan\n",
      "125 Train Loss nan Test RE nan c nan k nan m nan\n",
      "126 Train Loss nan Test RE nan c nan k nan m nan\n",
      "127 Train Loss nan Test RE nan c nan k nan m nan\n",
      "128 Train Loss nan Test RE nan c nan k nan m nan\n",
      "129 Train Loss nan Test RE nan c nan k nan m nan\n",
      "130 Train Loss nan Test RE nan c nan k nan m nan\n",
      "131 Train Loss nan Test RE nan c nan k nan m nan\n",
      "132 Train Loss nan Test RE nan c nan k nan m nan\n",
      "133 Train Loss nan Test RE nan c nan k nan m nan\n",
      "134 Train Loss nan Test RE nan c nan k nan m nan\n",
      "135 Train Loss nan Test RE nan c nan k nan m nan\n",
      "136 Train Loss nan Test RE nan c nan k nan m nan\n",
      "137 Train Loss nan Test RE nan c nan k nan m nan\n",
      "138 Train Loss nan Test RE nan c nan k nan m nan\n",
      "139 Train Loss nan Test RE nan c nan k nan m nan\n",
      "140 Train Loss nan Test RE nan c nan k nan m nan\n",
      "141 Train Loss nan Test RE nan c nan k nan m nan\n",
      "142 Train Loss nan Test RE nan c nan k nan m nan\n",
      "143 Train Loss nan Test RE nan c nan k nan m nan\n",
      "144 Train Loss nan Test RE nan c nan k nan m nan\n",
      "145 Train Loss nan Test RE nan c nan k nan m nan\n",
      "146 Train Loss nan Test RE nan c nan k nan m nan\n",
      "147 Train Loss nan Test RE nan c nan k nan m nan\n",
      "148 Train Loss nan Test RE nan c nan k nan m nan\n",
      "149 Train Loss nan Test RE nan c nan k nan m nan\n",
      "150 Train Loss nan Test RE nan c nan k nan m nan\n",
      "151 Train Loss nan Test RE nan c nan k nan m nan\n",
      "152 Train Loss nan Test RE nan c nan k nan m nan\n",
      "153 Train Loss nan Test RE nan c nan k nan m nan\n",
      "154 Train Loss nan Test RE nan c nan k nan m nan\n",
      "155 Train Loss nan Test RE nan c nan k nan m nan\n",
      "156 Train Loss nan Test RE nan c nan k nan m nan\n",
      "157 Train Loss nan Test RE nan c nan k nan m nan\n",
      "158 Train Loss nan Test RE nan c nan k nan m nan\n",
      "159 Train Loss nan Test RE nan c nan k nan m nan\n",
      "160 Train Loss nan Test RE nan c nan k nan m nan\n",
      "161 Train Loss nan Test RE nan c nan k nan m nan\n",
      "162 Train Loss nan Test RE nan c nan k nan m nan\n",
      "163 Train Loss nan Test RE nan c nan k nan m nan\n",
      "164 Train Loss nan Test RE nan c nan k nan m nan\n",
      "165 Train Loss nan Test RE nan c nan k nan m nan\n",
      "166 Train Loss nan Test RE nan c nan k nan m nan\n",
      "167 Train Loss nan Test RE nan c nan k nan m nan\n",
      "168 Train Loss nan Test RE nan c nan k nan m nan\n",
      "169 Train Loss nan Test RE nan c nan k nan m nan\n",
      "170 Train Loss nan Test RE nan c nan k nan m nan\n",
      "171 Train Loss nan Test RE nan c nan k nan m nan\n",
      "172 Train Loss nan Test RE nan c nan k nan m nan\n",
      "173 Train Loss nan Test RE nan c nan k nan m nan\n",
      "174 Train Loss nan Test RE nan c nan k nan m nan\n",
      "175 Train Loss nan Test RE nan c nan k nan m nan\n",
      "176 Train Loss nan Test RE nan c nan k nan m nan\n",
      "177 Train Loss nan Test RE nan c nan k nan m nan\n",
      "178 Train Loss nan Test RE nan c nan k nan m nan\n",
      "179 Train Loss nan Test RE nan c nan k nan m nan\n",
      "180 Train Loss nan Test RE nan c nan k nan m nan\n",
      "181 Train Loss nan Test RE nan c nan k nan m nan\n",
      "182 Train Loss nan Test RE nan c nan k nan m nan\n",
      "183 Train Loss nan Test RE nan c nan k nan m nan\n",
      "184 Train Loss nan Test RE nan c nan k nan m nan\n",
      "185 Train Loss nan Test RE nan c nan k nan m nan\n",
      "186 Train Loss nan Test RE nan c nan k nan m nan\n",
      "187 Train Loss nan Test RE nan c nan k nan m nan\n",
      "188 Train Loss nan Test RE nan c nan k nan m nan\n",
      "189 Train Loss nan Test RE nan c nan k nan m nan\n",
      "190 Train Loss nan Test RE nan c nan k nan m nan\n",
      "191 Train Loss nan Test RE nan c nan k nan m nan\n",
      "192 Train Loss nan Test RE nan c nan k nan m nan\n",
      "193 Train Loss nan Test RE nan c nan k nan m nan\n",
      "194 Train Loss nan Test RE nan c nan k nan m nan\n",
      "195 Train Loss nan Test RE nan c nan k nan m nan\n",
      "196 Train Loss nan Test RE nan c nan k nan m nan\n",
      "197 Train Loss nan Test RE nan c nan k nan m nan\n",
      "198 Train Loss nan Test RE nan c nan k nan m nan\n",
      "199 Train Loss nan Test RE nan c nan k nan m nan\n",
      "Training time: 65.01\n",
      "Training time: 65.01\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 10000.051 Test RE 0.9999822116283723 c 0.0012197602 k 0.00016411471 m -3.8845197e-08\n",
      "1 Train Loss 10000.051 Test RE 0.9999759909123244 c 0.0020254047 k 0.00022528411 m -1.2397047e-07\n",
      "2 Train Loss 10000.051 Test RE 0.9999837127887758 c 0.0029388354 k 0.00072235067 m -3.0354278e-07\n",
      "3 Train Loss 10000.05 Test RE 0.9999711083713515 c 0.0049548866 k 0.0016703983 m -8.7666876e-07\n",
      "4 Train Loss 10000.05 Test RE 0.999962360839428 c 0.0060776323 k 0.0025615953 m -1.3307811e-06\n",
      "5 Train Loss 10000.05 Test RE 0.9999489414684241 c 0.00776014 k 0.0043521672 m -2.1300325e-06\n",
      "6 Train Loss 10000.05 Test RE 0.999910206980755 c 0.011861812 k 0.009266673 m -4.223376e-06\n",
      "7 Train Loss 10000.049 Test RE 0.9993628651923327 c 0.066439144 k 0.077295 m -3.2795877e-05\n",
      "8 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "9 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "10 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "11 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "12 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "13 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "14 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "15 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "16 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "17 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "18 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "19 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "20 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "21 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "22 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "23 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "24 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "25 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "26 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "27 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "28 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "29 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "30 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "31 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "32 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "33 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "34 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "35 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "36 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "37 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "38 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "39 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "40 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "41 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "42 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "43 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "44 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "45 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "46 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "47 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "48 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "49 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "50 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "51 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "52 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "53 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "54 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "55 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "56 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "57 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "58 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "59 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "60 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "61 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "62 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "63 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "64 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "65 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "66 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "67 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "68 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "69 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "70 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "71 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "72 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "73 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "74 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "75 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "76 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "77 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "78 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "79 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "80 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "81 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "82 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "83 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "84 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "85 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "86 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "87 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "88 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "89 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "90 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "91 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "92 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "93 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "94 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "95 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "96 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "97 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "98 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "99 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "100 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "101 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "102 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "103 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "104 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "105 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "106 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "107 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "108 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "109 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "110 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "111 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "112 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "113 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "114 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "115 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "116 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "117 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "118 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "119 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "120 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "121 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "122 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "123 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "124 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "125 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "126 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "127 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "128 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "129 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "130 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "131 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "132 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "133 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "134 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "135 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "136 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "137 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "138 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "139 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "140 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "141 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "142 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "143 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "144 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "145 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "146 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "147 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "148 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "149 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "150 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "151 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "152 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "153 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "154 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "155 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "156 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "157 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "158 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "159 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "160 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "161 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "162 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "163 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "164 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "165 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "166 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "167 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "168 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "169 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "170 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "171 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "172 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "173 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "174 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "175 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "176 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "177 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "178 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "179 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "180 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "181 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "182 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "183 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "184 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "185 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "186 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "187 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "188 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "189 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "190 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "191 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "192 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "193 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "194 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "195 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "196 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "197 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "198 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "199 Train Loss 2046.7698 Test RE 0.580400607055552 c 159.00328 k 200.87712 m -0.08396851\n",
      "Training time: 22.80\n",
      "Training time: 22.80\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 10000.051 Test RE 0.9999727878859916 c 0.003808536 k 0.0011367574 m -4.4900122e-07\n",
      "1 Train Loss 10000.051 Test RE 0.9999619105751747 c 0.004491582 k 0.0015063263 m -6.599321e-07\n",
      "2 Train Loss 10000.051 Test RE 0.999965432754346 c 0.0051787714 k 0.0022805065 m -9.4874315e-07\n",
      "3 Train Loss 10000.051 Test RE 0.9999526862207155 c 0.0058780774 k 0.0030300056 m -1.2834273e-06\n",
      "4 Train Loss 10000.05 Test RE 0.9999321231311745 c 0.0075160675 k 0.0058743875 m -2.304182e-06\n",
      "5 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "6 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "7 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "8 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "9 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "10 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "11 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "12 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "13 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "14 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "15 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "16 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "17 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "18 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "19 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "20 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "21 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "22 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "23 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "24 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "25 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "26 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "27 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "28 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "29 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "30 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "31 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "32 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "33 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "34 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "35 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "36 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "37 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "38 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "39 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "40 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "41 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "42 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "43 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "44 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "45 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "46 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "47 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "48 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "49 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "50 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "51 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "52 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "53 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "54 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "55 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "56 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "57 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "58 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "59 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "60 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "61 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "62 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "63 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "64 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "65 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "66 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "67 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "68 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "69 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "70 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "71 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "72 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "73 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "74 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "75 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "76 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "77 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "78 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "79 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "80 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "81 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "82 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "83 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "84 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "85 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "86 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "87 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "88 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "89 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "90 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "91 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "92 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "93 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "94 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "95 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "96 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "97 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "98 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "99 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "100 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "101 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "102 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "103 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "104 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "105 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "106 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "107 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "108 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "109 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "110 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "111 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "112 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "113 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "114 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "115 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "116 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "117 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "118 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "119 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "120 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "121 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "122 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "123 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "124 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "125 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "126 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "127 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "128 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "129 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "130 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "131 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "132 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "133 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "134 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "135 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "136 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "137 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "138 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "139 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "140 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "141 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "142 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "143 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "144 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "145 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "146 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "147 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "148 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "149 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "150 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "151 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "152 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "153 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "154 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "155 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "156 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "157 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "158 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "159 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "160 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "161 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "162 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "163 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "164 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "165 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "166 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "167 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "168 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "169 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "170 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "171 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "172 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "173 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "174 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "175 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "176 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "177 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "178 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "179 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "180 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "181 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "182 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "183 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "184 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "185 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "186 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "187 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "188 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "189 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "190 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "191 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "192 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "193 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "194 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "195 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "196 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "197 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "198 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "199 Train Loss 10000.05 Test RE 0.9999067761573559 c 0.008954341 k 0.008963354 m -3.335747e-06\n",
      "Training time: 28.71\n",
      "Training time: 28.71\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 200\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "m_full = []\n",
    "k_full = []\n",
    "c_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "\n",
    "for reps in range(max_reps):   \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []   \n",
    "    beta_val = []\n",
    "    m_val = []\n",
    "    k_val = []\n",
    "    c_val = []\n",
    "\n",
    "    torch.manual_seed(reps*36)\n",
    "    N_f = 10000 #Total number of collocation points\n",
    "\n",
    "    layers = np.array([1,50,50,50,1]) #9 hidden layers\n",
    "    PINN = Sequentialmodel(layers)\n",
    "\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.25, \n",
    "                                max_iter = 10, \n",
    "                                max_eval = 15, \n",
    "                                tolerance_grad = 1e-6, \n",
    "                                tolerance_change = 1e-6, \n",
    "                                history_size = 100, \n",
    "                                line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "    train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    beta_full.append(beta_val)\n",
    "    m_full.append(m_val)\n",
    "    k_full.append(k_val)\n",
    "    c_full.append(c_val)\n",
    "    \n",
    "\n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"beta\": beta_full,\"m\": m_full,\"k\": k_full,\"c\": c_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "pmHEeBpzfFQh",
    "outputId": "990b2054-f35e-4c8e-c378-69ff2eb19f4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f113833e7d0>]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_pred = PINN.test()\n",
    "plt.plot(t,(x_true-x_mean)/x_std,'b')\n",
    "plt.plot(t,x_pred,'r--')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "stan_1D_FODE_tune.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
