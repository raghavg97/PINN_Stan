{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "j8byrnUmNKGR",
    "outputId": "510f67c6-b225-41ba-e78a-d0a6e54ded85"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:2\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "from scipy.integrate import odeint\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "#from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:2' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "uoNYpDzlNKGV"
   },
   "outputs": [],
   "source": [
    "tstart= 0 \n",
    "tstop= 60 \n",
    "\n",
    "increment = 0.1 \n",
    "# Initial condition\n",
    "x_init= [0,0] \n",
    "\n",
    "t = np.arange(tstart,tstop+1,increment)\n",
    "\n",
    "c = 1 # Damping constant \n",
    "k = 0.01 # Stiffness of the spring\n",
    "m = 5 # Mass \n",
    "F0 = 100\n",
    "\n",
    "def mydiff(x, t):    \n",
    "    F =F0\n",
    "    \n",
    "    dx1dt = x[1] \n",
    "    dx2dt = (F -c*x[1] -k*x[0])/m\n",
    "    dxdt= [dx1dt, dx2dt] \n",
    "    \n",
    "    return dxdt \n",
    "\n",
    "x_full_sol = odeint(mydiff, x_init, t) \n",
    "\n",
    "x_sol = x_full_sol[:,0]\n",
    "v_sol = x_full_sol[:,1] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "BR02v-fkNKGV"
   },
   "outputs": [],
   "source": [
    "level = \"high\"\n",
    "label = \"1D_SMD_tanhs_\" + level\n",
    "\n",
    "loss_thresh = 0.005\n",
    "\n",
    "t_test = t.reshape(-1,1)\n",
    "t_test_tensor = torch.from_numpy(t_test).float().to(device)\n",
    "\n",
    "# bc1_t = t[0].reshape(-1,1)\n",
    "# bc1_x = x_sol[0].reshape(-1,1)\n",
    "# t_bc1_train = torch.from_numpy(bc1_t).float().to(device)\n",
    "# x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "x_mean = np.mean(x_sol.reshape(-1,1))\n",
    "x_std  = np.mean(x_sol.reshape(-1,1))                \n",
    "bc1_t = t.reshape(-1,1)\n",
    "bc1_x = (x_sol-x_mean).reshape(-1,1)/x_std\n",
    "t_bc1_train = torch.from_numpy(bc1_t).float().to(device)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "\n",
    "\n",
    "bc2_t = t[0].reshape(-1,1)\n",
    "t_bc2_train = torch.from_numpy(bc2_t).float().to(device)\n",
    "bc2_val = v_sol[0].reshape(-1,1)\n",
    "bc2_val =torch.from_numpy(bc2_val).float().to(device)\n",
    "\n",
    "t_test = t.reshape(-1,1)\n",
    "t_test_tensor = torch.from_numpy(t_test).float().to(device)\n",
    "x_true = x_sol\n",
    "x_true_norm = np.linalg.norm(x_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(t[0]) \n",
    "ub = np.array(t[-1]) \n",
    "\n",
    "#torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "SyyktBKBXRo1"
   },
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    \n",
    "    t01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=t01,random_state =seed)\n",
    "\n",
    "    t_coll = lb + (ub-lb)*sampling(N_f)\n",
    "    # t_coll = np.vstack((t_coll,)) # append training points to collocation points \n",
    "\n",
    "    return t_coll"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "o1b21zLnNKGW"
   },
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "        \n",
    "        \n",
    "        self.k = Parameter(torch.tensor(0.0))\n",
    "        self.k.requiresGrad = True\n",
    "        self.c = Parameter(torch.tensor(0.0))\n",
    "        self.c.requiresGrad = True\n",
    "        self.m = Parameter(torch.tensor(0.0))\n",
    "        self.m.requiresGrad = True\n",
    "        \n",
    "    'forward pass'\n",
    "    def forward(self,t):\n",
    "        if torch.is_tensor(t) != True:         \n",
    "            t = torch.from_numpy(t)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        t = 2.0*(t - l_b)/(u_b - l_b) - 1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = t.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,t,x):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(t), x)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,t_bc2,bc2_val):\n",
    "        g = t_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        x = self.forward(g)    \n",
    "            \n",
    "        x_t = autograd.grad(x,g,torch.ones([t_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dx_dt = x_t[:,[0]]\n",
    "        \n",
    "        bc2 = dx_dt/x_std\n",
    "        \n",
    "        loss_bc2= self.loss_function(bc2,bc2_val)\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self,t_coll,f_hat):\n",
    "             \n",
    "        g = t_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        x = self.forward(g) \n",
    "\n",
    "        x_t = autograd.grad(x,g,torch.ones([t_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        x_tt = autograd.grad(x_t,g,torch.ones(t_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dx_dt = x_t[:,[0]]\n",
    "        \n",
    "        dx2_d2t = x_tt[:,[0]]\n",
    "        \n",
    "        f = self.m*dx2_d2t/x_std + self.c*dx_dt/x_std + self.k*x/x_std - F0\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,t_bc1,x_bc1,t_bc2,bc2_val,t_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.loss_BC1(t_bc1,x_bc1)\n",
    "        loss_bc2 = self.loss_BC2(t_bc2,bc2_val)\n",
    "        loss_f = self.loss_PDE(t_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self):\n",
    "        x_pred = self.forward(t_test_tensor)\n",
    "        x_pred = x_pred.cpu().detach().numpy()\n",
    "        \n",
    "        x_pred = x_pred*x_std + x_mean\n",
    "\n",
    "        return x_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        x_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(x_pred.reshape(-1,1) - x_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(x_pred.reshape(-1,1) - x_true.reshape(-1,1),2)/x_true_norm\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "fLY2mT5BOgjD"
   },
   "outputs": [],
   "source": [
    "def train_step(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "8srA5uGuObil"
   },
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    m_val.append(PINN.m.cpu().detach().numpy())\n",
    "    k_val.append(PINN.k.cpu().detach().numpy())\n",
    "    c_val.append(PINN.c.cpu().detach().numpy())\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "id": "0SezTZ_racQB"
   },
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "\n",
    "    t_coll = colloc_pts(N_f,rep*11)\n",
    "    t_coll =  torch.from_numpy(t_coll).float().to(device)\n",
    "    \n",
    "    f_hat = torch.zeros(t_coll.shape[0],1).to(device)\n",
    "    for i in range(max_iter):      \n",
    "        train_step(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat)\n",
    "         \n",
    "        loss_np = PINN.loss(t_bc1_train,x_bc1_train,t_bc2_train,bc2_val,t_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test RE\",test_re_loss[-1],\"c\",PINN.c.cpu().detach().numpy(),\"k\",PINN.k.cpu().detach().numpy(),\"m\",PINN.m.cpu().detach().numpy())\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I9BYbcJ0NKGX",
    "outputId": "42720d97-b37a-4c42-b7e5-b77a0ef391f3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 10000.002 Test RE 0.034208097760171724 c 0.0018465831 k 5.0723298e-05 m -4.7584173e-08\n",
      "1 Train Loss 10000.001 Test RE 0.026942650284667212 c 0.002492594 k 6.977424e-05 m -8.1778815e-08\n",
      "2 Train Loss 10000.001 Test RE 0.02526602612900026 c 0.0028288038 k 7.985038e-05 m -1.0300143e-07\n",
      "3 Train Loss 10000.001 Test RE 0.024270765922213482 c 0.0031735161 k 8.99505e-05 m -1.2709302e-07\n",
      "4 Train Loss 10000.001 Test RE 0.023681710814539233 c 0.0035279214 k 0.000100534206 m -1.5438728e-07\n",
      "5 Train Loss 10000.001 Test RE 0.023328738393035005 c 0.0038951442 k 0.00011135861 m -1.8523826e-07\n",
      "6 Train Loss 10000.001 Test RE 0.02311055301593934 c 0.004281504 k 0.00012285999 m -2.2047718e-07\n",
      "7 Train Loss 10000.001 Test RE 0.02296813024475859 c 0.004698595 k 0.00013518127 m -2.6148308e-07\n",
      "8 Train Loss 10000.001 Test RE 0.02286723133713616 c 0.0051666303 k 0.00014907398 m -3.1085622e-07\n",
      "9 Train Loss 10000.001 Test RE 0.022787534251322166 c 0.0057194266 k 0.0001654073 m -3.730822e-07\n",
      "10 Train Loss 10000.001 Test RE 0.02271617720550561 c 0.00641137 k 0.00018589551 m -4.5585708e-07\n",
      "11 Train Loss 10000.001 Test RE 0.02264394201432375 c 0.00732684 k 0.00021292362 m -5.717592e-07\n",
      "12 Train Loss 10000.001 Test RE 0.022562623627375742 c 0.008592897 k 0.00025033444 m -7.40884e-07\n",
      "13 Train Loss 10000.001 Test RE 0.02246356108741063 c 0.010396736 k 0.00030353246 m -9.94467e-07\n",
      "14 Train Loss 10000.001 Test RE 0.022336013258328402 c 0.013011317 k 0.00038066818 m -1.3806305e-06\n",
      "15 Train Loss 10000.001 Test RE 0.022165699918516153 c 0.016831925 k 0.0004932301 m -1.9728434e-06\n",
      "16 Train Loss 10000.001 Test RE 0.021932820393254993 c 0.0224342 k 0.0006583391 m -2.8840657e-06\n",
      "17 Train Loss 10000.001 Test RE 0.021608926020363788 c 0.030665478 k 0.0009006789 m -4.2900215e-06\n",
      "18 Train Loss 10000.001 Test RE 0.021151324483302397 c 0.042815737 k 0.0012586911 m -6.4742835e-06\n",
      "19 Train Loss 10000.001 Test RE 0.020494340074836317 c 0.060931243 k 0.0017918149 m -9.918097e-06\n",
      "20 Train Loss 10000.001 Test RE 0.019528693870496067 c 0.088547096 k 0.0026070068 m -1.5526932e-05\n",
      "21 Train Loss 9999.998 Test RE 0.01735261321435922 c 0.27122325 k 0.008013397 m -5.9126436e-05\n",
      "22 Train Loss 9999.998 Test RE 0.017471687416915325 c 0.2829994 k 0.00834945 m -6.3054606e-05\n",
      "23 Train Loss 9999.998 Test RE 0.0175295078027235 c 0.28958663 k 0.008532861 m -6.606849e-05\n",
      "24 Train Loss 9999.998 Test RE 0.017534823027329817 c 0.29418457 k 0.008656261 m -6.9089394e-05\n",
      "25 Train Loss 9999.998 Test RE 0.017471963710851975 c 0.29734322 k 0.008734747 m -7.2404815e-05\n",
      "26 Train Loss 9999.998 Test RE 0.017327200718749533 c 0.2992505 k 0.008772844 m -7.625725e-05\n",
      "27 Train Loss 9999.998 Test RE 0.017085792253344546 c 0.2998651 k 0.0087684775 m -8.0820646e-05\n",
      "28 Train Loss 9999.998 Test RE 0.016745598926990672 c 0.29916015 k 0.00872064 m -8.615045e-05\n",
      "29 Train Loss 9999.998 Test RE 0.016327162262762374 c 0.29729477 k 0.0086347535 m -9.21346e-05\n",
      "30 Train Loss 9999.998 Test RE 0.015871435178555313 c 0.29467964 k 0.008524245 m -9.856526e-05\n",
      "31 Train Loss 9999.998 Test RE 0.015418603457845766 c 0.29180777 k 0.008404479 m -0.00010532585\n",
      "32 Train Loss 9999.998 Test RE 0.014989336706047234 c 0.28904223 k 0.008285677 m -0.0001125369\n",
      "33 Train Loss 9999.998 Test RE 0.014584940765978422 c 0.28657207 k 0.00817154 m -0.000120574885\n",
      "34 Train Loss 9999.998 Test RE 0.014197730923725518 c 0.28450215 k 0.008062083 m -0.00013004764\n",
      "35 Train Loss 9999.998 Test RE 0.01381966139175637 c 0.2829589 k 0.007956631 m -0.00014182668\n",
      "36 Train Loss 9999.998 Test RE 0.013446235743065882 c 0.28216425 k 0.007855365 m -0.00015717438\n",
      "37 Train Loss 9999.998 Test RE 0.013078603854965426 c 0.28250423 k 0.007760212 m -0.00017797016\n",
      "38 Train Loss 9999.998 Test RE 0.012726965563536275 c 0.28462732 k 0.0076758997 m -0.00020707546\n",
      "39 Train Loss 9999.998 Test RE 0.01241584904014797 c 0.28959048 k 0.007611347 m -0.00024891124\n",
      "40 Train Loss 9999.998 Test RE 0.01219257949361363 c 0.29909176 k 0.0075817388 m -0.00031040155\n",
      "41 Train Loss 9999.998 Test RE 0.012139367985605711 c 0.31585014 k 0.0076115965 m -0.00040257408\n",
      "42 Train Loss 9999.998 Test RE 0.012386638126880779 c 0.34425837 k 0.007739053 m -0.0005434976\n",
      "43 Train Loss 9999.998 Test RE 0.013124964632261758 c 0.3917188 k 0.008022831 m -0.00076462957\n",
      "44 Train Loss 9999.998 Test RE 0.01462729197940666 c 0.47266352 k 0.008560765 m -0.0011304149\n",
      "45 Train Loss 9999.998 Test RE 0.01745784667555123 c 0.6338913 k 0.00962641 m -0.001858297\n",
      "46 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "47 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "48 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "49 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "50 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "51 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "52 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "53 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "54 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "55 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "56 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "57 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "58 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "59 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "60 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "61 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "62 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "63 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "64 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "65 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "66 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "67 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "68 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "69 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "70 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "71 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "72 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "73 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "74 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "75 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "76 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "77 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "78 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "79 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "80 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "81 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "82 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "83 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "84 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "85 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "86 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "87 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "88 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "89 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "90 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "91 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "92 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "93 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "94 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "95 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "96 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "97 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "98 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "99 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "100 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "101 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "102 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "103 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "104 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "105 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "106 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "107 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "108 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "109 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "110 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "111 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "112 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "113 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "114 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "115 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "116 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "117 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "118 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "119 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "120 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "121 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "122 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "123 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "124 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "125 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "126 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "127 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "128 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "129 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "130 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "131 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "132 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "133 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "134 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "135 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "136 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "137 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "138 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "139 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "140 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "141 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "142 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "143 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "144 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "145 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "146 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "147 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "148 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "149 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "150 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "151 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "152 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "153 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "154 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "155 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "156 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "157 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "158 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "159 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "160 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "161 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "162 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "163 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "164 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "165 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "166 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "167 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "168 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "169 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "170 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "171 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "172 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "173 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "174 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "175 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "176 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "177 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "178 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "179 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "180 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "181 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "182 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "183 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "184 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "185 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "186 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "187 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "188 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "189 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "190 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "191 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "192 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "193 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "194 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "195 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "196 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "197 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "198 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "199 Train Loss 9999.996 Test RE 0.029903384162043693 c 1.2960113 k 0.014193134 m -0.0048146476\n",
      "Training time: 37.13\n",
      "Training time: 37.13\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 10000.002 Test RE 0.03389335746430427 c 0.0019094152 k 8.793648e-05 m -1.5140024e-07\n",
      "1 Train Loss 10000.001 Test RE 0.028085298689071392 c 0.0026081647 k 0.00011976499 m -2.5651232e-07\n",
      "2 Train Loss 10000.001 Test RE 0.02672242091822189 c 0.0029741647 k 0.00013642447 m -3.2177e-07\n",
      "3 Train Loss 10000.001 Test RE 0.02587724504314664 c 0.0033550053 k 0.00015370721 m -3.9688956e-07\n",
      "4 Train Loss 10000.001 Test RE 0.02533469124648598 c 0.003757481 k 0.00017192937 m -4.839666e-07\n",
      "5 Train Loss 10000.001 Test RE 0.02496377660086723 c 0.0041940096 k 0.0001916175 m -5.8678e-07\n",
      "6 Train Loss 10000.001 Test RE 0.024685394359359986 c 0.004686107 k 0.00021373206 m -7.121278e-07\n",
      "7 Train Loss 10000.001 Test RE 0.02445024165529436 c 0.005269429 k 0.00023983375 m -8.719022e-07\n",
      "8 Train Loss 10000.001 Test RE 0.024224303365387598 c 0.0060006604 k 0.00027241866 m -1.0862444e-06\n",
      "9 Train Loss 10000.001 Test RE 0.023979722675486818 c 0.006966353 k 0.0003152603 m -1.3880601e-06\n",
      "10 Train Loss 10000.001 Test RE 0.023688391961525997 c 0.008293874 k 0.00037390165 m -1.8294138e-06\n",
      "11 Train Loss 10000.001 Test RE 0.02331730547819158 c 0.010164665 k 0.00045617076 m -2.4906221e-06\n",
      "12 Train Loss 10000.001 Test RE 0.022823940647564267 c 0.012830941 k 0.00057291123 m -3.4940997e-06\n",
      "13 Train Loss 10000.001 Test RE 0.02215157172188378 c 0.016635837 k 0.00073870417 m -5.0269155e-06\n",
      "14 Train Loss 10000.001 Test RE 0.021223234003963573 c 0.022037366 k 0.0009728987 m -7.3840033e-06\n",
      "15 Train Loss 10000.001 Test RE 0.019938002970366443 c 0.029622415 k 0.0012989808 m -1.1073371e-05\n",
      "16 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "17 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "18 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "19 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "20 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "21 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "22 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "23 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "24 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "25 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "26 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "27 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "28 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "29 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "30 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "31 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "32 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "33 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "34 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "35 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "36 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "37 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "38 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "39 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "40 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "41 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "42 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "43 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "44 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "45 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "46 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "47 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "48 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "49 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "50 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "51 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "52 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "53 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "54 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "55 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "56 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "57 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "58 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "59 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "60 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "61 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "62 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "63 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "64 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "65 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "66 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "67 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "68 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "69 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "70 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "71 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "72 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "73 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "74 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "75 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "76 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "77 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "78 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "79 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "80 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "81 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "82 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "83 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "84 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "85 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "86 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "87 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "88 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "89 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "90 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "91 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "92 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "93 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "94 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "95 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "96 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "97 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "98 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "99 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "100 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "101 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "102 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "103 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "104 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "105 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "106 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "107 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "108 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "109 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "110 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "111 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "112 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "113 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "114 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "115 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "116 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "117 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "118 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "119 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "120 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "121 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "122 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "123 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "124 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "125 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "126 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "127 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "128 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "129 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "130 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "131 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "132 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "133 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "134 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "135 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "136 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "137 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "138 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "139 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "140 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "141 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "142 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "143 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "144 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "145 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "146 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "147 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "148 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "149 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "150 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "151 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "152 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "153 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "154 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "155 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "156 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "157 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "158 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "159 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "160 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "161 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "162 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "163 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "164 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "165 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "166 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "167 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "168 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "169 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "170 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "171 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "172 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "173 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "174 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "175 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "176 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "177 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "178 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "179 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "180 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "181 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "182 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "183 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "184 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "185 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "186 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "187 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "188 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "189 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "190 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "191 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "192 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "193 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "194 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "195 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "196 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "197 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "198 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "199 Train Loss 10000.0 Test RE 0.01817508163386218 c 0.040137053 k 0.00174762 m -1.7317501e-05\n",
      "Training time: 25.54\n",
      "Training time: 25.54\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 10000.001 Test RE 0.030418488362750216 c 0.0018588288 k 3.3221968e-05 m -3.0226463e-08\n",
      "1 Train Loss 10000.001 Test RE 0.027471365392307236 c 0.0021165868 k 3.6812336e-05 m -3.8685236e-08\n",
      "2 Train Loss 10000.001 Test RE 0.025693242744374963 c 0.0023804007 k 4.380872e-05 m -4.893828e-08\n",
      "3 Train Loss 10000.001 Test RE 0.024642747328747126 c 0.002649798 k 4.7412057e-05 m -5.994526e-08\n",
      "4 Train Loss 10000.001 Test RE 0.02402927190670359 c 0.0029243813 k 5.4691744e-05 m -7.286469e-08\n",
      "5 Train Loss 10000.001 Test RE 0.023673567131122692 c 0.003203545 k 5.884521e-05 m -8.661634e-08\n",
      "6 Train Loss 10000.001 Test RE 0.023467287544220148 c 0.0034873253 k 6.5622495e-05 m -1.02176955e-07\n",
      "7 Train Loss 10000.001 Test RE 0.02334507621002979 c 0.0037802174 k 7.087764e-05 m -1.1918543e-07\n",
      "8 Train Loss 10000.001 Test RE 0.023268226915056006 c 0.0040936423 k 7.760315e-05 m -1.3890022e-07\n",
      "9 Train Loss 10000.001 Test RE 0.023215196153826435 c 0.0044471193 k 8.450785e-05 m -1.6253162e-07\n",
      "10 Train Loss 10000.001 Test RE 0.02317401720286211 c 0.0048711523 k 9.322328e-05 m -1.9274182e-07\n",
      "11 Train Loss 10000.001 Test RE 0.02313765270950877 c 0.005412899 k 0.000104069644 m -2.335496e-07\n",
      "12 Train Loss 10000.001 Test RE 0.023101429203491095 c 0.0061444137 k 0.00011891498 m -2.9169385e-07\n",
      "13 Train Loss 10000.001 Test RE 0.023061498079850105 c 0.007173498 k 0.00013964949 m -3.7766662e-07\n",
      "14 Train Loss 10000.001 Test RE 0.02301402696186367 c 0.00865962 k 0.00016970199 m -5.078962e-07\n",
      "15 Train Loss 10000.001 Test RE 0.022954491065970008 c 0.010836809 k 0.00021363968 m -7.075998e-07\n",
      "16 Train Loss 10000.001 Test RE 0.022876976201262122 c 0.014049548 k 0.00027854863 m -1.0156614e-06\n",
      "17 Train Loss 10000.001 Test RE 0.022773498113975273 c 0.018807324 k 0.00037462488 m -1.4920902e-06\n",
      "18 Train Loss 10000.001 Test RE 0.022632519503954878 c 0.025881313 k 0.00051755947 m -2.2314425e-06\n",
      "19 Train Loss 10000.001 Test RE 0.02243664339920962 c 0.03648115 k 0.0007317706 m -3.3875222e-06\n",
      "20 Train Loss 10000.001 Test RE 0.022157271659631744 c 0.05265356 k 0.0010588238 m -5.228415e-06\n",
      "21 Train Loss 10000.001 Test RE 0.0217392787695931 c 0.078338824 k 0.0015785911 m -8.281385e-06\n",
      "22 Train Loss 10000.001 Test RE 0.021034215892970016 c 0.12318186 k 0.0024869526 m -1.3852306e-05\n",
      "23 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "24 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "25 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "26 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "27 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "28 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "29 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "30 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "31 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "32 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "33 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "34 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "35 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "36 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "37 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "38 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "39 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "40 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "41 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "42 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "43 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "44 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "45 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "46 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "47 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "48 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "49 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "50 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "51 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "52 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "53 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "54 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "55 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "56 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "57 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "58 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "59 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "60 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "61 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "62 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "63 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "64 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "65 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "66 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "67 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "68 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "69 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "70 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "71 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "72 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "73 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "74 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "75 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "76 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "77 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "78 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "79 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "80 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "81 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "82 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "83 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "84 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "85 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "86 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "87 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "88 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "89 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "90 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "91 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "92 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "93 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "94 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "95 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "96 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "97 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "98 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "99 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "100 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "101 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "102 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "103 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "104 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "105 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "106 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "107 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "108 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "109 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "110 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "111 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "112 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "113 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "114 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "115 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "116 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "117 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "118 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "119 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "120 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "121 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "122 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "123 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "124 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "125 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "126 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "127 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "128 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "129 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "130 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "131 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "132 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "133 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "134 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "135 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "136 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "137 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "138 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "139 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "140 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "141 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "142 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "143 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "144 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "145 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "146 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "147 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "148 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "149 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "150 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "151 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "152 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "153 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "154 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "155 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "156 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "157 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "158 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "159 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "160 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "161 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "162 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "163 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "164 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "165 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "166 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "167 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "168 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "169 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "170 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "171 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "172 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "173 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "174 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "175 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "176 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "177 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "178 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "179 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "180 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "181 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "182 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "183 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "184 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "185 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "186 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "187 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "188 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "189 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "190 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "191 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "192 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "193 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "194 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "195 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "196 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "197 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "198 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "199 Train Loss 9999.998 Test RE 0.018311927601144702 c 0.37050775 k 0.007504285 m -5.5375916e-05\n",
      "Training time: 26.24\n",
      "Training time: 26.24\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 10000.002 Test RE 0.034035351144546644 c 0.0017812919 k 3.624816e-05 m -2.7908907e-08\n",
      "1 Train Loss 10000.001 Test RE 0.027192998741030858 c 0.0024545682 k 5.0764997e-05 m -4.9285294e-08\n",
      "2 Train Loss 10000.001 Test RE 0.02563195085459076 c 0.0028033191 k 5.4629334e-05 m -6.192843e-08\n",
      "3 Train Loss 10000.001 Test RE 0.024715720759028944 c 0.0031581037 k 6.540994e-05 m -7.747626e-08\n",
      "4 Train Loss 10000.001 Test RE 0.024189447500156357 c 0.0035157222 k 7.043241e-05 m -9.364552e-08\n",
      "5 Train Loss 10000.001 Test RE 0.023891225199636247 c 0.0038748407 k 7.965824e-05 m -1.1216984e-07\n",
      "6 Train Loss 10000.001 Test RE 0.023719581658390113 c 0.004242061 k 8.652567e-05 m -1.323007e-07\n",
      "7 Train Loss 10000.001 Test RE 0.023615599165032877 c 0.0046313326 k 9.52923e-05 m -1.5566789e-07\n",
      "8 Train Loss 10000.001 Test RE 0.023547781879987333 c 0.0050631664 k 0.00010416951 m -1.8339969e-07\n",
      "9 Train Loss 10000.001 Test RE 0.023499088585546397 c 0.0055685462 k 0.00011506124 m -2.182149e-07\n",
      "10 Train Loss 10000.001 Test RE 0.023460030299458905 c 0.0061960467 k 0.00012827401 m -2.6417212e-07\n",
      "11 Train Loss 10000.001 Test RE 0.023424930026737484 c 0.0070214304 k 0.00014585583 m -3.2827256e-07\n",
      "12 Train Loss 10000.001 Test RE 0.023389937666668737 c 0.008159831 k 0.00016996484 m -4.215962e-07\n",
      "13 Train Loss 10000.001 Test RE 0.023351953620903313 c 0.00978328 k 0.00020444453 m -5.617135e-07\n",
      "14 Train Loss 10000.001 Test RE 0.023308132982774413 c 0.012145724 k 0.00025454487 m -7.7583115e-07\n",
      "15 Train Loss 10000.001 Test RE 0.023255197621870772 c 0.015620924 k 0.0003283027 m -1.1060032e-06\n",
      "16 Train Loss 10000.001 Test RE 0.023189282348735955 c 0.02076127 k 0.00043736564 m -1.6172087e-06\n",
      "17 Train Loss 10000.001 Test RE 0.023105299388613033 c 0.0283956 k 0.00059940194 m -2.4111464e-06\n",
      "18 Train Loss 10000.001 Test RE 0.022996419323126238 c 0.039802857 k 0.0008415297 m -3.6508336e-06\n",
      "19 Train Loss 10000.001 Test RE 0.02285255915430743 c 0.05707471 k 0.0012082364 m -5.611535e-06\n",
      "20 Train Loss 10000.001 Test RE 0.02265662229945487 c 0.08400501 k 0.0017800634 m -8.804209e-06\n",
      "21 Train Loss 10000.001 Test RE 0.022367702334070445 c 0.12890157 k 0.0027333489 m -1.4361476e-05\n",
      "22 Train Loss 9999.999 Test RE 0.025721384177202655 c 0.54634106 k 0.011571601 m -7.0161455e-05\n",
      "23 Train Loss 9999.999 Test RE 0.02366801989956636 c 0.5368985 k 0.011352279 m -6.935394e-05\n",
      "24 Train Loss 9999.999 Test RE 0.023468826791698892 c 0.55109054 k 0.011633714 m -7.192315e-05\n",
      "25 Train Loss 9999.999 Test RE 0.023060827888720454 c 0.5506494 k 0.011614121 m -7.272056e-05\n",
      "26 Train Loss 9999.999 Test RE 0.022887885229729968 c 0.5523275 k 0.011640711 m -7.393638e-05\n",
      "27 Train Loss 9999.999 Test RE 0.02270776926720377 c 0.55246145 k 0.011635694 m -7.517593e-05\n",
      "28 Train Loss 9999.999 Test RE 0.022528715734646956 c 0.5520217 k 0.01161876 m -7.66358e-05\n",
      "29 Train Loss 9999.999 Test RE 0.022321450188011276 c 0.5508018 k 0.0115851145 m -7.8397585e-05\n",
      "30 Train Loss 9999.999 Test RE 0.022068568260859945 c 0.5487481 k 0.011533142 m -8.0612605e-05\n",
      "31 Train Loss 9999.999 Test RE 0.021751862681109933 c 0.5457211 k 0.011459453 m -8.3464176e-05\n",
      "32 Train Loss 9999.999 Test RE 0.021357731352822113 c 0.54162997 k 0.011361716 m -8.716668e-05\n",
      "33 Train Loss 9999.999 Test RE 0.020882464187758083 c 0.5365296 k 0.011240727 m -9.195329e-05\n",
      "34 Train Loss 9999.999 Test RE 0.02033766766265789 c 0.5307249 k 0.011102505 m -9.809366e-05\n",
      "35 Train Loss 9999.999 Test RE 0.019748294477617948 c 0.5247624 k 0.010957864 m -0.00010596947\n",
      "36 Train Loss 9999.999 Test RE 0.019142359226851133 c 0.5192415 k 0.010818385 m -0.000116061245\n",
      "37 Train Loss 9999.998 Test RE 0.018001977542199107 c 0.5112576 k 0.010589993 m -0.00014339431\n",
      "38 Train Loss 9999.998 Test RE 0.01753973417104151 c 0.50949407 k 0.010516709 m -0.00016011827\n",
      "39 Train Loss 9999.998 Test RE 0.0171762767703615 c 0.50948477 k 0.010476103 m -0.00017889445\n",
      "40 Train Loss 9999.998 Test RE 0.016916362198201914 c 0.51138794 k 0.010469976 m -0.00020033779\n",
      "41 Train Loss 9999.998 Test RE 0.016763593888622 c 0.51547927 k 0.010501408 m -0.00022559626\n",
      "42 Train Loss 9999.998 Test RE 0.01672711676452884 c 0.5222655 k 0.010576848 m -0.00025647596\n",
      "43 Train Loss 9999.998 Test RE 0.01682665293217401 c 0.53261167 k 0.010708194 m -0.0002956908\n",
      "44 Train Loss 9999.998 Test RE 0.017096601164778853 c 0.54790926 k 0.010915366 m -0.0003472693\n",
      "45 Train Loss 9999.998 Test RE 0.017590064726326293 c 0.5703267 k 0.011230088 m -0.00041718583\n",
      "46 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "47 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "48 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "49 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "50 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "51 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "52 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "53 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "54 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "55 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "56 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "57 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "58 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "59 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "60 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "61 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "62 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "63 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "64 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "65 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "66 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "67 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "68 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "69 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "70 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "71 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "72 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "73 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "74 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "75 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "76 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "77 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "78 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "79 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "80 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "81 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "82 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "83 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "84 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "85 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "86 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "87 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "88 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "89 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "90 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "91 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "92 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "93 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "94 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "95 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "96 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "97 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "98 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "99 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "100 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "101 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "102 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "103 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "104 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "105 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "106 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "107 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "108 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "109 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "110 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "111 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "112 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "113 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "114 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "115 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "116 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "117 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "118 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "119 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "120 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "121 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "122 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "123 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "124 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "125 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "126 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "127 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "128 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "129 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "130 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "131 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "132 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "133 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "134 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "135 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "136 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "137 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "138 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "139 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "140 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "141 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "142 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "143 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "144 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "145 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "146 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "147 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "148 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "149 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "150 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "151 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "152 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "153 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "154 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "155 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "156 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "157 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "158 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "159 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "160 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "161 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "162 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "163 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "164 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "165 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "166 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "167 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "168 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "169 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "170 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "171 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "172 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "173 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "174 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "175 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "176 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "177 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "178 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "179 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "180 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "181 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "182 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "183 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "184 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "185 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "186 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "187 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "188 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "189 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "190 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "191 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "192 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "193 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "194 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "195 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "196 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "197 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "198 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "199 Train Loss 9999.998 Test RE 0.018382040494727966 c 0.60319304 k 0.011701682 m -0.0005143351\n",
      "Training time: 22.38\n",
      "Training time: 22.38\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 10000.001 Test RE 0.03167275697293094 c 0.0015313121 k 3.6189547e-05 m -3.7155708e-08\n",
      "1 Train Loss 10000.001 Test RE 0.028183165462415217 c 0.0017426298 k 4.1612668e-05 m -4.703589e-08\n",
      "2 Train Loss 10000.001 Test RE 0.026045424528314945 c 0.00195827 k 4.716708e-05 m -5.825755e-08\n",
      "3 Train Loss 10000.001 Test RE 0.024767021532771043 c 0.0021780634 k 5.2839212e-05 m -7.087089e-08\n",
      "4 Train Loss 10000.001 Test RE 0.02401211324823107 c 0.002402285 k 5.8633483e-05 m -8.495161e-08\n",
      "5 Train Loss 10000.001 Test RE 0.02356607214279773 c 0.002631932 k 6.4571344e-05 m -1.0062737e-07\n",
      "6 Train Loss 10000.001 Test RE 0.023298335170394528 c 0.0028691972 k 7.070756e-05 m -1.1812856e-07\n",
      "7 Train Loss 10000.001 Test RE 0.02313208564677242 c 0.0031182775 k 7.714812e-05 m -1.3787741e-07\n",
      "8 Train Loss 10000.001 Test RE 0.023022617447489895 c 0.0033867364 k 8.408776e-05 m -1.6064664e-07\n",
      "9 Train Loss 10000.001 Test RE 0.022944045898876744 c 0.0036876295 k 9.186111e-05 m -1.8782345e-07\n",
      "10 Train Loss 10000.001 Test RE 0.022880926988909825 c 0.0040427083 k 0.0001010287 m -2.2183436e-07\n",
      "11 Train Loss 10000.001 Test RE 0.022823274288913915 c 0.004486885 k 0.000112488706 m -2.667817e-07\n",
      "12 Train Loss 10000.001 Test RE 0.022763635524561426 c 0.005074274 k 0.00012763248 m -3.2936833e-07\n",
      "13 Train Loss 10000.001 Test RE 0.022695094117148536 c 0.0058862884 k 0.00014855224 m -4.2021765e-07\n",
      "14 Train Loss 10000.001 Test RE 0.022609999960505927 c 0.007042992 k 0.00017833017 m -5.558109e-07\n",
      "15 Train Loss 10000.001 Test RE 0.022498630405035223 c 0.0087191155 k 0.00022145033 m -7.613542e-07\n",
      "16 Train Loss 10000.001 Test RE 0.022347795225318417 c 0.011167963 k 0.0002844087 m -1.0752127e-06\n",
      "17 Train Loss 10000.001 Test RE 0.02213904833580691 c 0.014756171 k 0.00037660397 m -1.5557305e-06\n",
      "18 Train Loss 10000.001 Test RE 0.02184568603741072 c 0.020019813 k 0.00051177567 m -2.2926304e-06\n",
      "19 Train Loss 10000.001 Test RE 0.021427939460171068 c 0.027758956 k 0.0007104245 m -3.4271393e-06\n",
      "20 Train Loss 10000.001 Test RE 0.020824017052004172 c 0.03921392 k 0.001004328 m -5.1914412e-06\n",
      "21 Train Loss 10000.001 Test RE 0.019932999443903508 c 0.056415103 k 0.0014454725 m -7.994831e-06\n",
      "22 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "23 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "24 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "25 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "26 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "27 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "28 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "29 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "30 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "31 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "32 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "33 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "34 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "35 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "36 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "37 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "38 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "39 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "40 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "41 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "42 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "43 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "44 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "45 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "46 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "47 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "48 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "49 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "50 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "51 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "52 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "53 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "54 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "55 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "56 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "57 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "58 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "59 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "60 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "61 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "62 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "63 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "64 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "65 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "66 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "67 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "68 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "69 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "70 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "71 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "72 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "73 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "74 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "75 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "76 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "77 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "78 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "79 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "80 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "81 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "82 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "83 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "84 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "85 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "86 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "87 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "88 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "89 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "90 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "91 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "92 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "93 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "94 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "95 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "96 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "97 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "98 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "99 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "100 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "101 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "102 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "103 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "104 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "105 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "106 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "107 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "108 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "109 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "110 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "111 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "112 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "113 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "114 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "115 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "116 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "117 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "118 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "119 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "120 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "121 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "122 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "123 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "124 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "125 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "126 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "127 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "128 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "129 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "130 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "131 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "132 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "133 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "134 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "135 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "136 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "137 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "138 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "139 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "140 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "141 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "142 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "143 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "144 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "145 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "146 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "147 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "148 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "149 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "150 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "151 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "152 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "153 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "154 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "155 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "156 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "157 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "158 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "159 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "160 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "161 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "162 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "163 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "164 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "165 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "166 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "167 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "168 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "169 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "170 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "171 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "172 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "173 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "174 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "175 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "176 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "177 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "178 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "179 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "180 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "181 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "182 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "183 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "184 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "185 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "186 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "187 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "188 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "189 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "190 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "191 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "192 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "193 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "194 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "195 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "196 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "197 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "198 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "199 Train Loss 10000.0 Test RE 0.016644623883324786 c 0.123381376 k 0.0031587824 m -2.1016085e-05\n",
      "Training time: 29.03\n",
      "Training time: 29.03\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 10000.002 Test RE 0.03343010946766469 c 0.0016939694 k 5.3910724e-05 m -5.904769e-08\n",
      "1 Train Loss 10000.001 Test RE 0.027064630032034537 c 0.002277266 k 7.3177165e-05 m -9.859469e-08\n",
      "2 Train Loss 10000.001 Test RE 0.025604133098862822 c 0.0025821938 k 8.327509e-05 m -1.2299842e-07\n",
      "3 Train Loss 10000.001 Test RE 0.024734267185804383 c 0.0028958975 k 9.3669485e-05 m -1.5073876e-07\n",
      "4 Train Loss 10000.001 Test RE 0.024213816687466187 c 0.00322001 k 0.00010440656 m -1.8215961e-07\n",
      "5 Train Loss 10000.001 Test RE 0.023895350967996363 c 0.003558427 k 0.0001156108 m -2.1788006e-07\n",
      "6 Train Loss 10000.001 Test RE 0.02369154792314404 c 0.0039187702 k 0.00012752981 m -2.5903682e-07\n",
      "7 Train Loss 10000.001 Test RE 0.023551387148005573 c 0.0043147164 k 0.00014061172 m -3.0769743e-07\n",
      "8 Train Loss 10000.001 Test RE 0.02344475130021832 c 0.0047696726 k 0.00015562373 m -3.6755011e-07\n",
      "9 Train Loss 10000.001 Test RE 0.023353039156961207 c 0.005322049 k 0.00017382592 m -4.4497568e-07\n",
      "10 Train Loss 10000.001 Test RE 0.023263330254801515 c 0.0060323556 k 0.00019719996 m -5.5061736e-07\n",
      "11 Train Loss 10000.001 Test RE 0.023164814852627513 c 0.006992726 k 0.00022876082 m -7.016501e-07\n",
      "12 Train Loss 10000.001 Test RE 0.02304645357810563 c 0.008339414 k 0.00027295874 m -9.24999e-07\n",
      "13 Train Loss 10000.001 Test RE 0.022895021186007704 c 0.0102696745 k 0.0003362328 m -1.2619979e-06\n",
      "14 Train Loss 10000.001 Test RE 0.022693323737638597 c 0.013065199 k 0.0004277653 m -1.7753165e-06\n",
      "15 Train Loss 10000.001 Test RE 0.02241809929342953 c 0.017123608 k 0.0005605152 m -2.559248e-06\n",
      "16 Train Loss 10000.001 Test RE 0.022037227201611078 c 0.023003288 k 0.00075266825 m -3.7559353e-06\n",
      "17 Train Loss 10000.001 Test RE 0.02150584352222289 c 0.031487014 k 0.0010297329 m -5.5821943e-06\n",
      "18 Train Loss 10000.001 Test RE 0.020761252002013456 c 0.0436727 k 0.0014274649 m -8.378164e-06\n",
      "19 Train Loss 10000.001 Test RE 0.019719675198799365 c 0.061073437 k 0.001995185 m -1.27063695e-05\n",
      "20 Train Loss 10000.0 Test RE 0.016543921004024572 c 0.118767425 k 0.0038758651 m -3.352382e-05\n",
      "21 Train Loss 10000.0 Test RE 0.0169138539139632 c 0.18470317 k 0.005944895 m -6.254701e-05\n",
      "22 Train Loss 10000.0 Test RE 0.016165857769546918 c 0.20041779 k 0.006463674 m -7.035524e-05\n",
      "23 Train Loss 10000.0 Test RE 0.015715544060307736 c 0.20872323 k 0.0067384355 m -7.521073e-05\n",
      "24 Train Loss 9999.998 Test RE 0.015361301197621507 c 0.21839443 k 0.007052445 m -8.243789e-05\n",
      "25 Train Loss 9999.998 Test RE 0.015258545093926256 c 0.22093263 k 0.007131406 m -8.530831e-05\n",
      "26 Train Loss 9999.998 Test RE 0.015157806014914257 c 0.22251666 k 0.007177401 m -8.797688e-05\n",
      "27 Train Loss 9999.998 Test RE 0.015044880117762163 c 0.22339104 k 0.0071982527 m -9.061649e-05\n",
      "28 Train Loss 9999.998 Test RE 0.014909403360493543 c 0.22366977 k 0.007197355 m -9.336274e-05\n",
      "29 Train Loss 9999.998 Test RE 0.014745729073210607 c 0.22342522 k 0.0071765953 m -9.633952e-05\n",
      "30 Train Loss 9999.998 Test RE 0.014551679656391769 c 0.2227124 k 0.0071372497 m -9.9652105e-05\n",
      "31 Train Loss 9999.998 Test RE 0.014330122969438868 c 0.2216163 k 0.007081575 m -0.00010339904\n",
      "32 Train Loss 9999.998 Test RE 0.014088053110406291 c 0.22026533 k 0.0070131468 m -0.000107702355\n",
      "33 Train Loss 9999.998 Test RE 0.013833366311668793 c 0.21882173 k 0.0069361622 m -0.00011277781\n",
      "34 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "35 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "36 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "37 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "38 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "39 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "40 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "41 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "42 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "43 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "44 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "45 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "46 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "47 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "48 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "49 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "50 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "51 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "52 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "53 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "54 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "55 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "56 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "57 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "58 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "59 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "60 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "61 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "62 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "63 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "64 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "65 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "66 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "67 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "68 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "69 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "70 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "71 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "72 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "73 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "74 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "75 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "76 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "77 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "78 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "79 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "80 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "81 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "82 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "83 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "84 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "85 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "86 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "87 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "88 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "89 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "90 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "91 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "92 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "93 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "94 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "95 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "96 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "97 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "98 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "99 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "100 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "101 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "102 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "103 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "104 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "105 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "106 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "107 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "108 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "109 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "110 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "111 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "112 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "113 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "114 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "115 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "116 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "117 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "118 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "119 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "120 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "121 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "122 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "123 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "124 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "125 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "126 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "127 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "128 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "129 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "130 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "131 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "132 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "133 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "134 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "135 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "136 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "137 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "138 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "139 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "140 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "141 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "142 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "143 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "144 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "145 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "146 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "147 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "148 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "149 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "150 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "151 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "152 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "153 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "154 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "155 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "156 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "157 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "158 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "159 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "160 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "161 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "162 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "163 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "164 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "165 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "166 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "167 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "168 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "169 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "170 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "171 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "172 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "173 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "174 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "175 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "176 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "177 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "178 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "179 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "180 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "181 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "182 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "183 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "184 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "185 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "186 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "187 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "188 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "189 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "190 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "191 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "192 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "193 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "194 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "195 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "196 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "197 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "198 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "199 Train Loss 9999.998 Test RE 0.013570991935532142 c 0.21745886 k 0.0068540233 m -0.00011904342\n",
      "Training time: 16.93\n",
      "Training time: 16.93\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 10000.001 Test RE 0.027985917402060093 c 0.0020889852 k 5.635547e-05 m -9.5533125e-08\n",
      "1 Train Loss 10000.001 Test RE 0.0259846108044693 c 0.0023705827 k 6.4586355e-05 m -1.2087571e-07\n",
      "2 Train Loss 10000.001 Test RE 0.02478003261821938 c 0.0026584791 k 7.301824e-05 m -1.497417e-07\n",
      "3 Train Loss 10000.001 Test RE 0.024057662337768404 c 0.002953941 k 8.167371e-05 m -1.824324e-07\n",
      "4 Train Loss 10000.001 Test RE 0.0236184450920312 c 0.0032598304 k 9.063414e-05 m -2.1947837e-07\n",
      "5 Train Loss 10000.001 Test RE 0.02334179182749051 c 0.0035816808 k 0.000100054654 m -2.618426e-07\n",
      "6 Train Loss 10000.001 Test RE 0.023156539832882045 c 0.0039294474 k 0.000110222936 m -3.1128098e-07\n",
      "7 Train Loss 10000.001 Test RE 0.023020657187817806 c 0.004320354 k 0.00012163706 m -3.7095865e-07\n",
      "8 Train Loss 10000.001 Test RE 0.02290866138175639 c 0.0047829705 k 0.00013512501 m -4.464148e-07\n",
      "9 Train Loss 10000.001 Test RE 0.022803538137866085 c 0.0053630844 k 0.00015201245 m -5.4705123e-07\n",
      "10 Train Loss 10000.001 Test RE 0.022691911953506932 c 0.006131343 k 0.00017434188 m -6.882591e-07\n",
      "11 Train Loss 10000.001 Test RE 0.022560739601223782 c 0.0071937204 k 0.0002051731 m -8.945285e-07\n",
      "12 Train Loss 10000.001 Test RE 0.022394828308577367 c 0.008705523 k 0.00024898277 m -1.2039419e-06\n",
      "13 Train Loss 10000.001 Test RE 0.022174351255308727 c 0.010892307 k 0.000312268 m -1.6751846e-06\n",
      "14 Train Loss 10000.001 Test RE 0.021871725385930043 c 0.014081225 k 0.0004044427 m -2.3987313e-06\n",
      "15 Train Loss 10000.001 Test RE 0.02144676747489072 c 0.018752884 k 0.00053933606 m -3.5163368e-06\n",
      "16 Train Loss 10000.001 Test RE 0.02083787769381153 c 0.025634896 k 0.0007378794 m -5.2584396e-06\n",
      "17 Train Loss 10000.001 Test RE 0.01994500730983791 c 0.035883736 k 0.0010333372 m -8.024879e-06\n",
      "18 Train Loss 10000.0 Test RE 0.01651962501050285 c 0.07547001 k 0.0021711898 m -2.083035e-05\n",
      "19 Train Loss 10000.0 Test RE 0.014766071515083322 c 0.121775806 k 0.0032637483 m -9.7541255e-05\n",
      "20 Train Loss 10000.0 Test RE 0.013132773214456911 c 0.14992793 k 0.004044523 m -0.00012045853\n",
      "21 Train Loss 10000.0 Test RE 0.012819521675776895 c 0.15831932 k 0.004281771 m -0.00012677416\n",
      "22 Train Loss 10000.0 Test RE 0.012742721979960075 c 0.16294505 k 0.004407737 m -0.00013176817\n",
      "23 Train Loss 10000.0 Test RE 0.012732541776306025 c 0.16633414 k 0.0044935616 m -0.00013732404\n",
      "24 Train Loss 10000.0 Test RE 0.01274458589223178 c 0.16912374 k 0.0045569865 m -0.00014399433\n",
      "25 Train Loss 10000.0 Test RE 0.012762846590452655 c 0.17166336 k 0.0046065855 m -0.00015241417\n",
      "26 Train Loss 10000.0 Test RE 0.01278009713351554 c 0.17418869 k 0.00464654 m -0.00016347023\n",
      "27 Train Loss 10000.0 Test RE 0.012792324161814658 c 0.176885 k 0.0046783127 m -0.0001783729\n",
      "28 Train Loss 10000.0 Test RE 0.012796515405509423 c 0.17990392 k 0.0047013485 m -0.0001985959\n",
      "29 Train Loss 10000.0 Test RE 0.012790828522057623 c 0.18336698 k 0.004713864 m -0.0002256704\n",
      "30 Train Loss 10000.0 Test RE 0.012775515902305211 c 0.1873538 k 0.0047138426 m -0.00026078592\n",
      "31 Train Loss 10000.0 Test RE 0.01275362154863666 c 0.19189747 k 0.004700306 m -0.00030438014\n",
      "32 Train Loss 10000.0 Test RE 0.012730230708489918 c 0.19701418 k 0.004674232 m -0.0003561327\n",
      "33 Train Loss 10000.0 Test RE 0.012710340573400605 c 0.20277894 k 0.0046384525 m -0.0004156395\n",
      "34 Train Loss 10000.0 Test RE 0.01269811603165962 c 0.20943323 k 0.0045969132 m -0.00048349507\n",
      "35 Train Loss 10000.0 Test RE 0.012698307798030552 c 0.2175098 k 0.0045544547 m -0.00056227646\n",
      "36 Train Loss 9999.998 Test RE 0.012930964754550679 c 0.26409265 k 0.004514685 m -0.00093911315\n",
      "37 Train Loss 9999.998 Test RE 0.01320451843259587 c 0.2947584 k 0.004585448 m -0.001150945\n",
      "38 Train Loss 9999.998 Test RE 0.013667296944145277 c 0.33682585 k 0.004737362 m -0.0014211185\n",
      "39 Train Loss 9999.998 Test RE 0.014391862621839628 c 0.39282736 k 0.005006393 m -0.0017570244\n",
      "40 Train Loss 9999.998 Test RE 0.015498893268996256 c 0.46800613 k 0.0054580546 m -0.0021781786\n",
      "41 Train Loss 9999.998 Test RE 0.01721350797251062 c 0.5748774 k 0.0062310114 m -0.0027371831\n",
      "42 Train Loss 9999.997 Test RE 0.0272624687152374 c 1.175042 k 0.011821536 m -0.0055388417\n",
      "43 Train Loss 9999.997 Test RE 0.034698047662454816 c 1.6282871 k 0.01639187 m -0.007563907\n",
      "44 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "45 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "46 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "47 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "48 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "49 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "50 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "51 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "52 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "53 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "54 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "55 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "56 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "57 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "58 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "59 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "60 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "61 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "62 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "63 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "64 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "65 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "66 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "67 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "68 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "69 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "70 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "71 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "72 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "73 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "74 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "75 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "76 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "77 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "78 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "79 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "80 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "81 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "82 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "83 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "84 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "85 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "86 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "87 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "88 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "89 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "90 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "91 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "92 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "93 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "94 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "95 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "96 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "97 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "98 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "99 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "100 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "101 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "102 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "103 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "104 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "105 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "106 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "107 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "108 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "109 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "110 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "111 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "112 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "113 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "114 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "115 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "116 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "117 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "118 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "119 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "120 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "121 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "122 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "123 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "124 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "125 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "126 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "127 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "128 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "129 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "130 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "131 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "132 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "133 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "134 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "135 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "136 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "137 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "138 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "139 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "140 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "141 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "142 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "143 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "144 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "145 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "146 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "147 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "148 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "149 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "150 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "151 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "152 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "153 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "154 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "155 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "156 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "157 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "158 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "159 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "160 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "161 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "162 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "163 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "164 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "165 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "166 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "167 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "168 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "169 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "170 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "171 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "172 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "173 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "174 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "175 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "176 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "177 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "178 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "179 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "180 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "181 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "182 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "183 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "184 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "185 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "186 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "187 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "188 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "189 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "190 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "191 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "192 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "193 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "194 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "195 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "196 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "197 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "198 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "199 Train Loss 9999.995 Test RE 0.036473532790326044 c 2.2894547 k 0.023041278 m -0.01052462\n",
      "Training time: 33.94\n",
      "Training time: 33.94\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 10000.002 Test RE 0.03451058817925686 c 0.0020324478 k 6.7768364e-05 m -1.3162388e-07\n",
      "1 Train Loss 10000.001 Test RE 0.027076855105016832 c 0.0028535628 k 9.820876e-05 m -2.4637777e-07\n",
      "2 Train Loss 10000.001 Test RE 0.025308852980405853 c 0.0032836956 k 0.00011672748 m -3.203304e-07\n",
      "3 Train Loss 10000.001 Test RE 0.024221266803314 c 0.003729199 k 0.00013083636 m -4.049224e-07\n",
      "4 Train Loss 10000.001 Test RE 0.02353981050185555 c 0.0041949768 k 0.00015039623 m -5.043685e-07\n",
      "5 Train Loss 10000.001 Test RE 0.02309482027520573 c 0.004690943 k 0.00016675665 m -6.1954916e-07\n",
      "6 Train Loss 10000.001 Test RE 0.022782383537320542 c 0.0052346243 k 0.00018848976 m -7.5823186e-07\n",
      "7 Train Loss 10000.001 Test RE 0.022539270301669658 c 0.0058577647 k 0.0002100998 m -9.295183e-07\n",
      "8 Train Loss 10000.001 Test RE 0.02232333125843734 c 0.0066147777 k 0.00023893056 m -1.1543083e-06\n",
      "9 Train Loss 10000.001 Test RE 0.02210378606475051 c 0.0075929463 k 0.00027379522 m -1.4650984e-06\n",
      "10 Train Loss 10000.001 Test RE 0.02185198108297977 c 0.008927085 k 0.00032319105 m -1.9183924e-06\n",
      "11 Train Loss 10000.001 Test RE 0.02153737095598192 c 0.010812153 k 0.0003907871 m -2.600715e-06\n",
      "12 Train Loss 10000.001 Test RE 0.02112009117472168 c 0.013533202 k 0.0004900984 m -3.6508209e-06\n",
      "13 Train Loss 10000.001 Test RE 0.020547523244969983 c 0.017489925 k 0.00063135894 m -5.281816e-06\n",
      "14 Train Loss 10000.001 Test RE 0.019739087223496644 c 0.023284998 k 0.0008415256 m -7.849763e-06\n",
      "15 Train Loss 10000.0 Test RE 0.016889984852248265 c 0.045065016 k 0.001624554 m -1.8984656e-05\n",
      "16 Train Loss 10000.0 Test RE 0.014820378232487368 c 0.06260705 k 0.0022327565 m -2.9100407e-05\n",
      "17 Train Loss 10000.0 Test RE 0.01322765715072306 c 0.079214185 k 0.0028183751 m -4.012628e-05\n",
      "18 Train Loss 10000.0 Test RE 0.012568251238780422 c 0.09059438 k 0.0032019124 m -4.9582854e-05\n",
      "19 Train Loss 10000.0 Test RE 0.01237102590943177 c 0.09778398 k 0.003436526 m -5.6859873e-05\n",
      "20 Train Loss 10000.0 Test RE 0.01233096902483939 c 0.10272017 k 0.0035892874 m -6.290319e-05\n",
      "21 Train Loss 10000.0 Test RE 0.012334735867749328 c 0.10650246 k 0.003698511 m -6.855814e-05\n",
      "22 Train Loss 10000.0 Test RE 0.012345163148839496 c 0.10967585 k 0.0037817114 m -7.438255e-05\n",
      "23 Train Loss 10000.0 Test RE 0.012346440076321791 c 0.112520084 k 0.0038467133 m -8.08192e-05\n",
      "24 Train Loss 10000.0 Test RE 0.012329578171756466 c 0.11518365 k 0.0038963263 m -8.826624e-05\n",
      "25 Train Loss 10000.0 Test RE 0.012288792860775926 c 0.117735036 k 0.003930552 m -9.7060445e-05\n",
      "26 Train Loss 10000.0 Test RE 0.01222218739388353 c 0.1201984 k 0.003948434 m -0.000107422835\n",
      "27 Train Loss 10000.0 Test RE 0.012133907168988513 c 0.12259343 k 0.003949958 m -0.000119421886\n",
      "28 Train Loss 10000.0 Test RE 0.012034153593492724 c 0.12498497 k 0.0039374162 m -0.00013305637\n",
      "29 Train Loss 10000.0 Test RE 0.011936056785296792 c 0.12753142 k 0.003915265 m -0.00014851874\n",
      "30 Train Loss 10000.0 Test RE 0.011852215285886928 c 0.13052498 k 0.0038887658 m -0.00016655902\n",
      "31 Train Loss 10000.0 Test RE 0.0117954167519889 c 0.13444684 k 0.0038631314 m -0.0001888449\n",
      "32 Train Loss 10000.0 Test RE 0.011783985539109398 c 0.14006819 k 0.0038443357 m -0.00021833031\n",
      "33 Train Loss 10000.0 Test RE 0.011849776612313678 c 0.14860927 k 0.003841268 m -0.00025973815\n",
      "34 Train Loss 10000.0 Test RE 0.012045815278205618 c 0.16195723 k 0.0038684094 m -0.00032026018\n",
      "35 Train Loss 10000.0 Test RE 0.012451861481486517 c 0.18296733 k 0.0039487397 m -0.00041062315\n",
      "36 Train Loss 10000.0 Test RE 0.013175879907928767 c 0.21595214 k 0.004117414 m -0.0005469441\n",
      "37 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "38 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "39 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "40 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "41 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "42 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "43 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "44 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "45 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "46 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "47 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "48 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "49 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "50 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "51 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "52 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "53 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "54 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "55 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "56 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "57 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "58 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "59 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "60 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "61 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "62 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "63 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "64 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "65 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "66 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "67 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "68 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "69 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "70 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "71 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "72 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "73 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "74 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "75 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "76 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "77 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "78 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "79 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "80 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "81 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "82 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "83 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "84 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "85 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "86 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "87 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "88 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "89 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "90 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "91 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "92 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "93 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "94 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "95 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "96 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "97 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "98 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "99 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "100 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "101 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "102 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "103 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "104 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "105 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "106 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "107 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "108 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "109 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "110 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "111 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "112 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "113 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "114 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "115 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "116 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "117 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "118 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "119 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "120 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "121 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "122 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "123 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "124 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "125 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "126 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "127 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "128 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "129 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "130 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "131 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "132 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "133 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "134 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "135 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "136 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "137 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "138 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "139 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "140 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "141 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "142 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "143 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "144 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "145 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "146 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "147 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "148 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "149 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "150 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "151 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "152 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "153 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "154 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "155 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "156 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "157 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "158 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "159 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "160 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "161 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "162 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "163 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "164 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "165 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "166 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "167 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "168 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "169 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "170 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "171 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "172 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "173 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "174 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "175 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "176 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "177 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "178 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "179 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "180 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "181 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "182 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "183 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "184 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "185 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "186 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "187 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "188 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "189 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "190 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "191 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "192 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "193 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "194 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "195 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "196 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "197 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "198 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "199 Train Loss 9999.998 Test RE 0.016209561876991786 c 0.35277584 k 0.004990504 m -0.0010894596\n",
      "Training time: 28.23\n",
      "Training time: 28.23\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 10000.001 Test RE 0.02822547762854288 c 0.0028199742 k 6.866212e-05 m -8.99025e-08\n",
      "1 Train Loss 10000.001 Test RE 0.025992788591587504 c 0.0032451109 k 8.037996e-05 m -1.1752725e-07\n",
      "2 Train Loss 10000.001 Test RE 0.02465864721147904 c 0.003682894 k 9.2231065e-05 m -1.4948556e-07\n",
      "3 Train Loss 10000.001 Test RE 0.023867149027324615 c 0.004134196 k 0.00010468708 m -1.8620162e-07\n",
      "4 Train Loss 10000.001 Test RE 0.02339375204026608 c 0.004602322 k 0.00011740624 m -2.2816295e-07\n",
      "5 Train Loss 10000.001 Test RE 0.02310291394090413 c 0.00509448 k 0.00013097546 m -2.764767e-07\n",
      "6 Train Loss 10000.001 Test RE 0.022915192486585637 c 0.0056242933 k 0.00014538913 m -3.329534e-07\n",
      "7 Train Loss 10000.001 Test RE 0.022784340682075705 c 0.0062158783 k 0.00016166263 m -4.0108617e-07\n",
      "8 Train Loss 10000.001 Test RE 0.022683081863714052 c 0.0069099274 k 0.00018054637 m -4.868672e-07\n",
      "9 Train Loss 10000.001 Test RE 0.022594525128867372 c 0.0077724727 k 0.00020420972 m -6.007848e-07\n",
      "10 Train Loss 10000.001 Test RE 0.02250681332990703 c 0.008906564 k 0.00023504936 m -7.600179e-07\n",
      "11 Train Loss 10000.001 Test RE 0.022409892557122783 c 0.010468338 k 0.00027779283 m -9.924161e-07\n",
      "12 Train Loss 10000.001 Test RE 0.022293482547475204 c 0.012689238 k 0.00033812944 m -1.3414888e-06\n",
      "13 Train Loss 10000.001 Test RE 0.0221449306222317 c 0.015909739 k 0.000426123 m -1.8752751e-06\n",
      "14 Train Loss 10000.001 Test RE 0.021947601471696422 c 0.020629717 k 0.0005542047 m -2.698925e-06\n",
      "15 Train Loss 10000.001 Test RE 0.021677421438906632 c 0.027600108 k 0.00074458 m -3.9794304e-06\n",
      "16 Train Loss 10000.001 Test RE 0.021299479876846716 c 0.03797348 k 0.0010255764 m -5.9862623e-06\n",
      "17 Train Loss 10000.001 Test RE 0.020754239855392393 c 0.053712517 k 0.0014564843 m -9.1987185e-06\n",
      "18 Train Loss 10000.001 Test RE 0.019951852633423185 c 0.078544006 k 0.0021255137 m -1.4564124e-05\n",
      "19 Train Loss 10000.0 Test RE 0.016766899950996342 c 0.20429581 k 0.005542884 m -4.4371314e-05\n",
      "20 Train Loss 9999.998 Test RE 0.01758472065395661 c 0.31159285 k 0.008460713 m -7.523075e-05\n",
      "21 Train Loss 9999.998 Test RE 0.017672060013608555 c 0.31640118 k 0.008579754 m -7.787829e-05\n",
      "22 Train Loss 9999.998 Test RE 0.017762625525645608 c 0.32076257 k 0.008685573 m -8.105369e-05\n",
      "23 Train Loss 9999.998 Test RE 0.017803173776135342 c 0.3242025 k 0.0087631345 m -8.4862055e-05\n",
      "24 Train Loss 9999.998 Test RE 0.017805585895810282 c 0.32723743 k 0.008825153 m -8.977802e-05\n",
      "25 Train Loss 9999.998 Test RE 0.017750094333598417 c 0.32973325 k 0.008866146 m -9.618022e-05\n",
      "26 Train Loss 9999.998 Test RE 0.017625310328484366 c 0.331681 k 0.008883952 m -0.00010456254\n",
      "27 Train Loss 9999.998 Test RE 0.017416611309045406 c 0.33293936 k 0.008873058 m -0.00011532\n",
      "28 Train Loss 9999.998 Test RE 0.017121518966713045 c 0.33347064 k 0.008831644 m -0.0001286653\n",
      "29 Train Loss 9999.998 Test RE 0.016751792822352115 c 0.33335936 k 0.008762371 m -0.00014452124\n",
      "30 Train Loss 9999.998 Test RE 0.01633336417509028 c 0.33289504 k 0.008673904 m -0.00016269906\n",
      "31 Train Loss 9999.998 Test RE 0.01589325313104834 c 0.33248946 k 0.008576984 m -0.00018328485\n",
      "32 Train Loss 9999.998 Test RE 0.015450020550776529 c 0.33260798 k 0.008481044 m -0.00020701103\n",
      "33 Train Loss 9999.998 Test RE 0.015014050853336647 c 0.33375916 k 0.008393392 m -0.00023539046\n",
      "34 Train Loss 9999.998 Test RE 0.014596088274889375 c 0.33660722 k 0.008321758 m -0.00027083588\n",
      "35 Train Loss 9999.998 Test RE 0.014213636334383155 c 0.34213787 k 0.0082767755 m -0.00031712704\n",
      "36 Train Loss 9999.998 Test RE 0.013898542050020038 c 0.3519392 k 0.008275215 m -0.00038043107\n",
      "37 Train Loss 9999.998 Test RE 0.013709032479200316 c 0.3686571 k 0.008344531 m -0.0004711109\n",
      "38 Train Loss 9999.998 Test RE 0.013749727762615073 c 0.3967259 k 0.0085294545 m -0.00060676987\n",
      "39 Train Loss 9999.998 Test RE 0.014196206113656917 c 0.44323063 k 0.008896477 m -0.00081651285\n",
      "40 Train Loss 9999.998 Test RE 0.01529724630883979 c 0.5170004 k 0.009510959 m -0.0011401104\n",
      "41 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "42 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "43 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "44 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "45 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "46 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "47 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "48 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "49 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "50 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "51 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "52 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "53 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "54 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "55 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "56 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "57 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "58 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "59 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "60 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "61 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "62 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "63 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "64 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "65 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "66 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "67 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "68 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "69 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "70 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "71 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "72 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "73 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "74 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "75 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "76 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "77 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "78 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "79 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "80 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "81 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "82 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "83 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "84 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "85 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "86 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "87 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "88 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "89 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "90 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "91 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "92 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "93 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "94 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "95 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "96 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "97 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "98 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "99 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "100 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "101 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "102 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "103 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "104 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "105 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "106 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "107 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "108 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "109 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "110 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "111 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "112 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "113 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "114 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "115 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "116 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "117 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "118 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "119 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "120 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "121 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "122 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "123 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "124 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "125 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "126 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "127 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "128 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "129 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "130 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "131 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "132 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "133 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "134 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "135 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "136 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "137 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "138 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "139 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "140 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "141 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "142 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "143 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "144 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "145 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "146 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "147 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "148 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "149 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "150 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "151 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "152 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "153 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "154 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "155 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "156 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "157 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "158 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "159 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "160 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "161 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "162 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "163 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "164 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "165 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "166 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "167 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "168 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "169 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "170 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "171 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "172 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "173 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "174 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "175 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "176 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "177 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "178 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "179 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "180 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "181 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "182 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "183 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "184 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "185 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "186 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "187 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "188 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "189 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "190 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "191 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "192 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "193 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "194 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "195 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "196 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "197 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "198 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "199 Train Loss 9999.998 Test RE 0.017181326935661805 c 0.6175945 k 0.010302607 m -0.0015890222\n",
      "Training time: 24.53\n",
      "Training time: 24.53\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 10000.001 Test RE 0.02767498021961621 c 0.0024708128 k 5.948359e-05 m -7.022569e-08\n",
      "1 Train Loss 10000.001 Test RE 0.025709623431254453 c 0.0028020518 k 6.8207526e-05 m -8.904792e-08\n",
      "2 Train Loss 10000.001 Test RE 0.02454200358805844 c 0.0031408581 k 7.72222e-05 m -1.10495456e-07\n",
      "3 Train Loss 10000.001 Test RE 0.023854237383956756 c 0.0034879583 k 8.641573e-05 m -1.347182e-07\n",
      "4 Train Loss 10000.001 Test RE 0.023447086277334907 c 0.0038455343 k 9.593622e-05 m -1.6203364e-07\n",
      "5 Train Loss 10000.001 Test RE 0.023200909054488253 c 0.0042182035 k 0.000105828505 m -1.9296579e-07\n",
      "6 Train Loss 10000.001 Test RE 0.02304579246400508 c 0.004614635 k 0.00011637736 m -2.2851394e-07\n",
      "7 Train Loss 10000.001 Test RE 0.022941373684473007 c 0.0050502312 k 0.00012794383 m -2.7046786e-07\n",
      "8 Train Loss 10000.001 Test RE 0.022864160484789915 c 0.005551255 k 0.00014126065 m -3.2204514e-07\n",
      "9 Train Loss 10000.001 Test RE 0.022800007323332185 c 0.0061607 k 0.0001574347 m -3.8878335e-07\n",
      "10 Train Loss 10000.001 Test RE 0.02273958199795145 c 0.0069465395 k 0.00017829501 m -4.799544e-07\n",
      "11 Train Loss 10000.001 Test RE 0.02267571785106844 c 0.0080126235 k 0.00020656291 m -6.1052293e-07\n",
      "12 Train Loss 10000.001 Test RE 0.022601557791942558 c 0.009514022 k 0.0002463723 m -8.040931e-07\n",
      "13 Train Loss 10000.001 Test RE 0.022509330268599467 c 0.011679066 k 0.00030373188 m -1.0972694e-06\n",
      "14 Train Loss 10000.001 Test RE 0.022389092558374266 c 0.014842477 k 0.0003875368 m -1.5464936e-06\n",
      "15 Train Loss 10000.001 Test RE 0.022227220496025008 c 0.019497568 k 0.0005107971 m -2.2390577e-06\n",
      "16 Train Loss 10000.001 Test RE 0.02200404063305752 c 0.026384534 k 0.0006931734 m -3.3121937e-06\n",
      "17 Train Loss 10000.001 Test RE 0.02168963097152014 c 0.036655176 k 0.0009650986 m -4.989057e-06\n",
      "18 Train Loss 10000.001 Test RE 0.021234601049761184 c 0.052232154 k 0.0013776941 m -7.657517e-06\n",
      "19 Train Loss 10000.001 Test RE 0.020546520368734892 c 0.076715514 k 0.0020261975 m -1.20718505e-05\n",
      "20 Train Loss 10000.001 Test RE 0.019411963435633096 c 0.118220404 k 0.0031266236 m -2.000794e-05\n",
      "21 Train Loss 9999.999 Test RE 0.024054625360267454 c 0.2674228 k 0.0071708416 m -7.0241185e-05\n",
      "22 Train Loss 9999.999 Test RE 0.022506552711892055 c 0.33706972 k 0.008974056 m -7.928967e-05\n",
      "23 Train Loss 9999.999 Test RE 0.0193998461032187 c 0.32595843 k 0.00865026 m -7.1865456e-05\n",
      "24 Train Loss 9999.999 Test RE 0.018854708779720467 c 0.32697225 k 0.008665253 m -7.067711e-05\n",
      "25 Train Loss 9999.998 Test RE 0.018103729475518922 c 0.32516456 k 0.008597848 m -6.945373e-05\n",
      "26 Train Loss 9999.998 Test RE 0.017816435363357787 c 0.3237669 k 0.0085515445 m -6.946119e-05\n",
      "27 Train Loss 9999.998 Test RE 0.017538431177664363 c 0.32202724 k 0.008495791 m -6.985855e-05\n",
      "28 Train Loss 9999.998 Test RE 0.01724741339281251 c 0.31991425 k 0.008429182 m -7.0704526e-05\n",
      "29 Train Loss 9999.998 Test RE 0.016927182751148045 c 0.3173946 k 0.008350212 m -7.2120034e-05\n",
      "30 Train Loss 9999.998 Test RE 0.016567897701748182 c 0.31447566 k 0.008258417 m -7.428834e-05\n",
      "31 Train Loss 9999.998 Test RE 0.016168308664322454 c 0.31125376 k 0.008155655 m -7.745212e-05\n",
      "32 Train Loss 9999.998 Test RE 0.015737027543460985 c 0.30794483 k 0.008046823 m -8.19213e-05\n",
      "33 Train Loss 9999.998 Test RE 0.015289490931384344 c 0.3048593 k 0.007938967 m -8.810305e-05\n",
      "34 Train Loss 9999.998 Test RE 0.014842744136764754 c 0.30232587 k 0.007839209 m -9.648958e-05\n",
      "35 Train Loss 9999.998 Test RE 0.014415732907033948 c 0.3006284 k 0.007753671 m -0.00010749318\n",
      "36 Train Loss 9999.998 Test RE 0.014034410007090747 c 0.30001608 k 0.007688449 m -0.00012126134\n",
      "37 Train Loss 9999.998 Test RE 0.01372472565259195 c 0.30068946 k 0.0076487525 m -0.00013780424\n",
      "38 Train Loss 9999.998 Test RE 0.013504177068214768 c 0.30280957 k 0.007637998 m -0.00015731926\n",
      "39 Train Loss 9999.998 Test RE 0.01338335651812882 c 0.30656353 k 0.0076588914 m -0.00018039132\n",
      "40 Train Loss 9999.998 Test RE 0.013371640360618027 c 0.31224734 k 0.007715217 m -0.00020811634\n",
      "41 Train Loss 9999.998 Test RE 0.013482669004598618 c 0.32035893 k 0.00781369 m -0.0002422774\n",
      "42 Train Loss 9999.998 Test RE 0.013738897315265048 c 0.33171278 k 0.007966 m -0.00028562165\n",
      "43 Train Loss 9999.998 Test RE 0.014175730714368947 c 0.34759864 k 0.008191462 m -0.0003422894\n",
      "44 Train Loss 9999.998 Test RE 0.014844804500260309 c 0.37001687 k 0.008520872 m -0.00041847504\n",
      "45 Train Loss 9999.998 Test RE 0.015817192070524854 c 0.40206194 k 0.009002815 m -0.00052349956\n",
      "46 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "47 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "48 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "49 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "50 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "51 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "52 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "53 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "54 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "55 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "56 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "57 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "58 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "59 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "60 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "61 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "62 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "63 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "64 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "65 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "66 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "67 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "68 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "69 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "70 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "71 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "72 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "73 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "74 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "75 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "76 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "77 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "78 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "79 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "80 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "81 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "82 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "83 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "84 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "85 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "86 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "87 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "88 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "89 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "90 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "91 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "92 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "93 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "94 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "95 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "96 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "97 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "98 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "99 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "100 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "101 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "102 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "103 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "104 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "105 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "106 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "107 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "108 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "109 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "110 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "111 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "112 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "113 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "114 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "115 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "116 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "117 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "118 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "119 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "120 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "121 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "122 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "123 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "124 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "125 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "126 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "127 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "128 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "129 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "130 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "131 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "132 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "133 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "134 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "135 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "136 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "137 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "138 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "139 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "140 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "141 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "142 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "143 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "144 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "145 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "146 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "147 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "148 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "149 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "150 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "151 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "152 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "153 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "154 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "155 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "156 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "157 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "158 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "159 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "160 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "161 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "162 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "163 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "164 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "165 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "166 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "167 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "168 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "169 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "170 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "171 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "172 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "173 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "174 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "175 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "176 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "177 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "178 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "179 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "180 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "181 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "182 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "183 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "184 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "185 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "186 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "187 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "188 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "189 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "190 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "191 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "192 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "193 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "194 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "195 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "196 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "197 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "198 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "199 Train Loss 9999.998 Test RE 0.01718716614002322 c 0.44857273 k 0.0097144535 m -0.0006716122\n",
      "Training time: 27.08\n",
      "Training time: 27.08\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 200\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "beta_full = []\n",
    "m_full = []\n",
    "k_full = []\n",
    "c_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "\n",
    "for reps in range(max_reps):   \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss = []   \n",
    "    beta_val = []\n",
    "    m_val = []\n",
    "    k_val = []\n",
    "    c_val = []\n",
    "\n",
    "    torch.manual_seed(reps*36)\n",
    "    N_f = 10000 #Total number of collocation points\n",
    "\n",
    "    layers = np.array([1,50,50,50,1]) #9 hidden layers\n",
    "    PINN = Sequentialmodel(layers)\n",
    "\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "\n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.25, \n",
    "                                max_iter = 10, \n",
    "                                max_eval = 15, \n",
    "                                tolerance_grad = 1e-6, \n",
    "                                tolerance_change = 1e-6, \n",
    "                                history_size = 100, \n",
    "                                line_search_fn = 'strong_wolfe')\n",
    "\n",
    "\n",
    "\n",
    "    train_model(max_iter,reps)\n",
    "\n",
    "\n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    beta_full.append(beta_val)\n",
    "    m_full.append(m_val)\n",
    "    k_full.append(k_val)\n",
    "    c_full.append(c_val)\n",
    "    \n",
    "\n",
    "\n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full,\"test_re_loss\": test_re_full,\"Time\": elapsed_time, \"beta\": beta_full,\"m\": m_full,\"k\": k_full,\"c\": c_full, \"label\": label,\"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 36
    },
    "id": "pmHEeBpzfFQh",
    "outputId": "990b2054-f35e-4c8e-c378-69ff2eb19f4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fd550634710>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "x_pred = PINN.test()\n",
    "plt.plot(t,x_true,'b')\n",
    "plt.plot(t,x_pred,'r--')"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "stan_1D_FODE_tune.ipynb",
   "provenance": []
  },
  "gpuClass": "standard",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
