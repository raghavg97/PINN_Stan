{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_1D_1(x): #True function for 1D_1 dy2/dx2 + dy/dx - 6y = 0; BC1: y(0)=2; BC2: dy/dx at (x=0) = -1;\n",
    "    y = np.exp(3.0*x) + np.exp(-4.0*x)\n",
    "    return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "level = \"high\"\n",
    "label = \"1D_SODE_stanALR_\" + level\n",
    "\n",
    "u_coeff = 12.0\n",
    "fo_val = -1.0\n",
    "\n",
    "loss_thresh = 0.005\n",
    "\n",
    "x = np.linspace(0,2,100).reshape(-1,1)\n",
    "\n",
    "bc1_x = x[0].reshape(-1,1)\n",
    "bc1_y = true_1D_1(x[0]).reshape(-1,1)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "y_bc1_train = torch.from_numpy(bc1_y).float().to(device)\n",
    "    \n",
    "\n",
    "bc2_x = x[0].reshape(-1,1)\n",
    "x_bc2_train = torch.from_numpy(bc2_x).float().to(device)\n",
    "bc2_val = torch.tensor(fo_val,device=device)\n",
    "bc2_val = bc2_val.view(1,1)\n",
    "\n",
    "x_test = x.reshape(-1,1)\n",
    "x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "y_true = true_1D_1(x_test)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(x[0]) \n",
    "ub = np.array(x[-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    #Collocation Points\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,y)\n",
    "    x01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    \n",
    "    x_coll_train = lb + (ub-lb)*sampling(N_f)\n",
    "    x_coll_train = np.vstack((x_coll_train, bc1_x.reshape(-1,1))) # append training points to collocation points \n",
    "\n",
    "    return x_coll_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "            \n",
    "        self.lambdas = torch.ones((2,),device = device)\n",
    "        \n",
    "        self.lambda_alpha = 0.1\n",
    "        \n",
    "        self.beta = Parameter(torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "\n",
    "    'forward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = 2.0*(x - l_b)/(u_b - l_b) - 1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z) + self.beta[:,i]*z*self.activation(z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "                        \n",
    "    def loss_BC1(self,x,y):\n",
    "                \n",
    "        loss_bc1 = self.loss_function(self.forward(x), y)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,x_bc2,bc2_val):\n",
    "        g = x_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        y = self.forward(g)    \n",
    "            \n",
    "        y_x = autograd.grad(y,g,torch.ones([x_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        bc2 = dy_dx\n",
    "        \n",
    "        loss_bc2= self.loss_function(bc2,bc2_val)\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self, x_coll,f_hat):\n",
    "             \n",
    "        g = x_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        y = self.forward(g) \n",
    "\n",
    "        y_x = autograd.grad(y,g,torch.ones([x_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        y_xx = autograd.grad(y_x,g,torch.ones(x_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        dy2_d2x = y_xx[:,[0]]\n",
    "        \n",
    "        f = dy2_d2x + dy_dx - u_coeff*y\n",
    "        \n",
    "        loss_f = self.loss_function(f,f_hat)\n",
    "                \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,x_bc1,y_bc1,x_bc2,bc2_val,x_coll,f_hat):\n",
    "\n",
    "        loss_bc1 = self.lambdas[0]*self.loss_BC1(x_bc1,y_bc1)\n",
    "        loss_bc2 = self.lambdas[1]*self.loss_BC2(x_bc2,bc2_val)\n",
    "        loss_f = self.loss_PDE(x_coll,f_hat)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def lambda_update(self,x_bc1,y_bc1,x_bc2,bc2_val,x_coll,f_hat):\n",
    "        loss_bc1 = self.lambdas[0]*self.loss_BC1(x_bc1,y_bc1)\n",
    "        loss_bc1.backward()\n",
    "        bc1_grads = []\n",
    "        for param in self.parameters():\n",
    "            bc1_grads.append(param.grad.view(-1))\n",
    "        bc1_grads = torch.cat(bc1_grads)\n",
    "        bc1_grads = torch.mean(torch.abs(bc1_grads))\n",
    "        \n",
    "        \n",
    "        loss_bc2 = self.lambdas[1]*self.loss_BC2(x_bc2,bc2_val)\n",
    "        loss_bc2.backward()\n",
    "        bc2_grads = []\n",
    "        for param in self.parameters():\n",
    "            bc2_grads.append(param.grad.view(-1))\n",
    "        bc2_grads = torch.cat(bc2_grads)\n",
    "        bc2_grads = torch.mean(torch.abs(bc2_grads))\n",
    "        \n",
    "    \n",
    "        loss_f = self.loss_PDE(x_coll,f_hat)\n",
    "        loss_f.backward()\n",
    "        f_grads = []\n",
    "        for param in self.parameters():\n",
    "            f_grads.append(param.grad.view(-1))   \n",
    "        f_grads = torch.cat(f_grads)\n",
    "        f_grads = torch.max(torch.abs(f_grads))\n",
    "    \n",
    "        self.lambdas[0] = (1.0-self.lambda_alpha)*self.lambdas[0] + self.lambda_alpha*f_grads/bc1_grads\n",
    "        self.lambdas[1] = (1.0-self.lambda_alpha)*self.lambdas[1] + self.lambda_alpha*f_grads/bc2_grads\n",
    "        \n",
    "        return None\n",
    "    \n",
    "    \n",
    "    def test(self):\n",
    "        y_pred = self.forward(x_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_coll,f_hat):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "\n",
    "    optimizer.step(closure)\n",
    "    PINN.lambda_update(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "    beta_val.append(PINN.beta.cpu().detach().numpy())\n",
    "\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        x_coll = torch.from_numpy(colloc_pts(N_f,i*11)).float().to(device)\n",
    "        f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "        train_step(x_coll,f_hat)\n",
    "        \n",
    "        loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat).cpu().detach().numpy()\n",
    "        print\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 18.833092 Test MSE 14252.737472413675 Test RE 0.9998036240324598\n",
      "1 Train Loss 13.357956 Test MSE 14249.872318799864 Test RE 0.9997031263228103\n",
      "2 Train Loss 12.173988 Test MSE 14252.047753303796 Test RE 0.9997794324689183\n",
      "3 Train Loss nan Test MSE nan Test RE nan\n",
      "4 Train Loss nan Test MSE nan Test RE nan\n",
      "5 Train Loss nan Test MSE nan Test RE nan\n",
      "6 Train Loss nan Test MSE nan Test RE nan\n",
      "7 Train Loss nan Test MSE nan Test RE nan\n",
      "8 Train Loss nan Test MSE nan Test RE nan\n",
      "9 Train Loss nan Test MSE nan Test RE nan\n",
      "10 Train Loss nan Test MSE nan Test RE nan\n",
      "11 Train Loss nan Test MSE nan Test RE nan\n",
      "12 Train Loss nan Test MSE nan Test RE nan\n",
      "13 Train Loss nan Test MSE nan Test RE nan\n",
      "14 Train Loss nan Test MSE nan Test RE nan\n",
      "15 Train Loss nan Test MSE nan Test RE nan\n",
      "16 Train Loss nan Test MSE nan Test RE nan\n",
      "17 Train Loss nan Test MSE nan Test RE nan\n",
      "18 Train Loss nan Test MSE nan Test RE nan\n",
      "19 Train Loss nan Test MSE nan Test RE nan\n",
      "20 Train Loss nan Test MSE nan Test RE nan\n",
      "21 Train Loss nan Test MSE nan Test RE nan\n",
      "22 Train Loss nan Test MSE nan Test RE nan\n",
      "23 Train Loss nan Test MSE nan Test RE nan\n",
      "24 Train Loss nan Test MSE nan Test RE nan\n",
      "25 Train Loss nan Test MSE nan Test RE nan\n",
      "26 Train Loss nan Test MSE nan Test RE nan\n",
      "27 Train Loss nan Test MSE nan Test RE nan\n",
      "28 Train Loss nan Test MSE nan Test RE nan\n",
      "29 Train Loss nan Test MSE nan Test RE nan\n",
      "30 Train Loss nan Test MSE nan Test RE nan\n",
      "31 Train Loss nan Test MSE nan Test RE nan\n",
      "32 Train Loss nan Test MSE nan Test RE nan\n",
      "33 Train Loss nan Test MSE nan Test RE nan\n",
      "34 Train Loss nan Test MSE nan Test RE nan\n",
      "35 Train Loss nan Test MSE nan Test RE nan\n",
      "36 Train Loss nan Test MSE nan Test RE nan\n",
      "37 Train Loss nan Test MSE nan Test RE nan\n",
      "38 Train Loss nan Test MSE nan Test RE nan\n",
      "39 Train Loss nan Test MSE nan Test RE nan\n",
      "40 Train Loss nan Test MSE nan Test RE nan\n",
      "41 Train Loss nan Test MSE nan Test RE nan\n",
      "42 Train Loss nan Test MSE nan Test RE nan\n",
      "43 Train Loss nan Test MSE nan Test RE nan\n",
      "44 Train Loss nan Test MSE nan Test RE nan\n",
      "45 Train Loss nan Test MSE nan Test RE nan\n",
      "46 Train Loss nan Test MSE nan Test RE nan\n",
      "47 Train Loss nan Test MSE nan Test RE nan\n",
      "48 Train Loss nan Test MSE nan Test RE nan\n",
      "49 Train Loss nan Test MSE nan Test RE nan\n",
      "50 Train Loss nan Test MSE nan Test RE nan\n",
      "51 Train Loss nan Test MSE nan Test RE nan\n",
      "52 Train Loss nan Test MSE nan Test RE nan\n",
      "53 Train Loss nan Test MSE nan Test RE nan\n",
      "54 Train Loss nan Test MSE nan Test RE nan\n",
      "55 Train Loss nan Test MSE nan Test RE nan\n",
      "56 Train Loss nan Test MSE nan Test RE nan\n",
      "57 Train Loss nan Test MSE nan Test RE nan\n",
      "58 Train Loss nan Test MSE nan Test RE nan\n",
      "59 Train Loss nan Test MSE nan Test RE nan\n",
      "60 Train Loss nan Test MSE nan Test RE nan\n",
      "61 Train Loss nan Test MSE nan Test RE nan\n",
      "62 Train Loss nan Test MSE nan Test RE nan\n",
      "63 Train Loss nan Test MSE nan Test RE nan\n",
      "64 Train Loss nan Test MSE nan Test RE nan\n",
      "65 Train Loss nan Test MSE nan Test RE nan\n",
      "66 Train Loss nan Test MSE nan Test RE nan\n",
      "67 Train Loss nan Test MSE nan Test RE nan\n",
      "68 Train Loss nan Test MSE nan Test RE nan\n",
      "69 Train Loss nan Test MSE nan Test RE nan\n",
      "70 Train Loss nan Test MSE nan Test RE nan\n",
      "71 Train Loss nan Test MSE nan Test RE nan\n",
      "72 Train Loss nan Test MSE nan Test RE nan\n",
      "73 Train Loss nan Test MSE nan Test RE nan\n",
      "74 Train Loss nan Test MSE nan Test RE nan\n",
      "75 Train Loss nan Test MSE nan Test RE nan\n",
      "76 Train Loss nan Test MSE nan Test RE nan\n",
      "77 Train Loss nan Test MSE nan Test RE nan\n",
      "78 Train Loss nan Test MSE nan Test RE nan\n",
      "79 Train Loss nan Test MSE nan Test RE nan\n",
      "80 Train Loss nan Test MSE nan Test RE nan\n",
      "81 Train Loss nan Test MSE nan Test RE nan\n",
      "82 Train Loss nan Test MSE nan Test RE nan\n",
      "83 Train Loss nan Test MSE nan Test RE nan\n",
      "84 Train Loss nan Test MSE nan Test RE nan\n",
      "85 Train Loss nan Test MSE nan Test RE nan\n",
      "86 Train Loss nan Test MSE nan Test RE nan\n",
      "87 Train Loss nan Test MSE nan Test RE nan\n",
      "88 Train Loss nan Test MSE nan Test RE nan\n",
      "89 Train Loss nan Test MSE nan Test RE nan\n",
      "90 Train Loss nan Test MSE nan Test RE nan\n",
      "91 Train Loss nan Test MSE nan Test RE nan\n",
      "92 Train Loss nan Test MSE nan Test RE nan\n",
      "93 Train Loss nan Test MSE nan Test RE nan\n",
      "94 Train Loss nan Test MSE nan Test RE nan\n",
      "95 Train Loss nan Test MSE nan Test RE nan\n",
      "96 Train Loss nan Test MSE nan Test RE nan\n",
      "97 Train Loss nan Test MSE nan Test RE nan\n",
      "98 Train Loss nan Test MSE nan Test RE nan\n",
      "99 Train Loss nan Test MSE nan Test RE nan\n",
      "100 Train Loss nan Test MSE nan Test RE nan\n",
      "101 Train Loss nan Test MSE nan Test RE nan\n",
      "102 Train Loss nan Test MSE nan Test RE nan\n",
      "103 Train Loss nan Test MSE nan Test RE nan\n",
      "104 Train Loss nan Test MSE nan Test RE nan\n",
      "105 Train Loss nan Test MSE nan Test RE nan\n",
      "106 Train Loss nan Test MSE nan Test RE nan\n",
      "107 Train Loss nan Test MSE nan Test RE nan\n",
      "108 Train Loss nan Test MSE nan Test RE nan\n",
      "109 Train Loss nan Test MSE nan Test RE nan\n",
      "110 Train Loss nan Test MSE nan Test RE nan\n",
      "111 Train Loss nan Test MSE nan Test RE nan\n",
      "112 Train Loss nan Test MSE nan Test RE nan\n",
      "113 Train Loss nan Test MSE nan Test RE nan\n",
      "114 Train Loss nan Test MSE nan Test RE nan\n",
      "115 Train Loss nan Test MSE nan Test RE nan\n",
      "116 Train Loss nan Test MSE nan Test RE nan\n",
      "117 Train Loss nan Test MSE nan Test RE nan\n",
      "118 Train Loss nan Test MSE nan Test RE nan\n",
      "119 Train Loss nan Test MSE nan Test RE nan\n",
      "120 Train Loss nan Test MSE nan Test RE nan\n",
      "121 Train Loss nan Test MSE nan Test RE nan\n",
      "122 Train Loss nan Test MSE nan Test RE nan\n",
      "123 Train Loss nan Test MSE nan Test RE nan\n",
      "124 Train Loss nan Test MSE nan Test RE nan\n",
      "125 Train Loss nan Test MSE nan Test RE nan\n",
      "126 Train Loss nan Test MSE nan Test RE nan\n",
      "127 Train Loss nan Test MSE nan Test RE nan\n",
      "128 Train Loss nan Test MSE nan Test RE nan\n",
      "129 Train Loss nan Test MSE nan Test RE nan\n",
      "130 Train Loss nan Test MSE nan Test RE nan\n",
      "131 Train Loss nan Test MSE nan Test RE nan\n",
      "132 Train Loss nan Test MSE nan Test RE nan\n",
      "133 Train Loss nan Test MSE nan Test RE nan\n",
      "134 Train Loss nan Test MSE nan Test RE nan\n",
      "135 Train Loss nan Test MSE nan Test RE nan\n",
      "136 Train Loss nan Test MSE nan Test RE nan\n",
      "137 Train Loss nan Test MSE nan Test RE nan\n",
      "138 Train Loss nan Test MSE nan Test RE nan\n",
      "139 Train Loss nan Test MSE nan Test RE nan\n",
      "140 Train Loss nan Test MSE nan Test RE nan\n",
      "141 Train Loss nan Test MSE nan Test RE nan\n",
      "142 Train Loss nan Test MSE nan Test RE nan\n",
      "143 Train Loss nan Test MSE nan Test RE nan\n",
      "144 Train Loss nan Test MSE nan Test RE nan\n",
      "145 Train Loss nan Test MSE nan Test RE nan\n",
      "146 Train Loss nan Test MSE nan Test RE nan\n",
      "147 Train Loss nan Test MSE nan Test RE nan\n",
      "148 Train Loss nan Test MSE nan Test RE nan\n",
      "149 Train Loss nan Test MSE nan Test RE nan\n",
      "150 Train Loss nan Test MSE nan Test RE nan\n",
      "151 Train Loss nan Test MSE nan Test RE nan\n",
      "152 Train Loss nan Test MSE nan Test RE nan\n",
      "153 Train Loss nan Test MSE nan Test RE nan\n",
      "154 Train Loss nan Test MSE nan Test RE nan\n",
      "155 Train Loss nan Test MSE nan Test RE nan\n",
      "156 Train Loss nan Test MSE nan Test RE nan\n",
      "157 Train Loss nan Test MSE nan Test RE nan\n",
      "158 Train Loss nan Test MSE nan Test RE nan\n",
      "159 Train Loss nan Test MSE nan Test RE nan\n",
      "160 Train Loss nan Test MSE nan Test RE nan\n",
      "161 Train Loss nan Test MSE nan Test RE nan\n",
      "162 Train Loss nan Test MSE nan Test RE nan\n",
      "163 Train Loss nan Test MSE nan Test RE nan\n",
      "164 Train Loss nan Test MSE nan Test RE nan\n",
      "165 Train Loss nan Test MSE nan Test RE nan\n",
      "166 Train Loss nan Test MSE nan Test RE nan\n",
      "167 Train Loss nan Test MSE nan Test RE nan\n",
      "168 Train Loss nan Test MSE nan Test RE nan\n",
      "169 Train Loss nan Test MSE nan Test RE nan\n",
      "170 Train Loss nan Test MSE nan Test RE nan\n",
      "171 Train Loss nan Test MSE nan Test RE nan\n",
      "172 Train Loss nan Test MSE nan Test RE nan\n",
      "173 Train Loss nan Test MSE nan Test RE nan\n",
      "174 Train Loss nan Test MSE nan Test RE nan\n",
      "175 Train Loss nan Test MSE nan Test RE nan\n",
      "176 Train Loss nan Test MSE nan Test RE nan\n",
      "177 Train Loss nan Test MSE nan Test RE nan\n",
      "178 Train Loss nan Test MSE nan Test RE nan\n",
      "179 Train Loss nan Test MSE nan Test RE nan\n",
      "180 Train Loss nan Test MSE nan Test RE nan\n",
      "181 Train Loss nan Test MSE nan Test RE nan\n",
      "182 Train Loss nan Test MSE nan Test RE nan\n",
      "183 Train Loss nan Test MSE nan Test RE nan\n",
      "184 Train Loss nan Test MSE nan Test RE nan\n",
      "185 Train Loss nan Test MSE nan Test RE nan\n",
      "186 Train Loss nan Test MSE nan Test RE nan\n",
      "187 Train Loss nan Test MSE nan Test RE nan\n",
      "188 Train Loss nan Test MSE nan Test RE nan\n",
      "189 Train Loss nan Test MSE nan Test RE nan\n",
      "190 Train Loss nan Test MSE nan Test RE nan\n",
      "191 Train Loss nan Test MSE nan Test RE nan\n",
      "192 Train Loss nan Test MSE nan Test RE nan\n",
      "193 Train Loss nan Test MSE nan Test RE nan\n",
      "194 Train Loss nan Test MSE nan Test RE nan\n",
      "195 Train Loss nan Test MSE nan Test RE nan\n",
      "196 Train Loss nan Test MSE nan Test RE nan\n",
      "197 Train Loss nan Test MSE nan Test RE nan\n",
      "198 Train Loss nan Test MSE nan Test RE nan\n",
      "199 Train Loss nan Test MSE nan Test RE nan\n",
      "Training time: 233.64\n",
      "Training time: 233.64\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 7.7000136 Test MSE 14265.201780687794 Test RE 1.0002407027782378\n",
      "1 Train Loss 7.826824 Test MSE 14260.80296961151 Test RE 1.0000864740057414\n",
      "2 Train Loss 7.313306 Test MSE 14260.17203277099 Test RE 1.0000643504836346\n",
      "3 Train Loss 6.949761 Test MSE 14258.658473742562 Test RE 1.0000112762083107\n",
      "4 Train Loss 6.382039 Test MSE 14258.688841919171 Test RE 1.00001234112276\n",
      "5 Train Loss 5.8725095 Test MSE 14258.794271672865 Test RE 1.0000160381969123\n",
      "6 Train Loss 5.4302073 Test MSE 14258.836466123335 Test RE 1.0000175178134625\n",
      "7 Train Loss 4.996927 Test MSE 14258.896303643038 Test RE 1.0000196161089754\n",
      "8 Train Loss 4.5826945 Test MSE 14258.948822239447 Test RE 1.0000214577514537\n",
      "9 Train Loss 4.2392364 Test MSE 14258.926897229845 Test RE 1.0000206889188854\n",
      "10 Train Loss 3.8968062 Test MSE 14258.853750300203 Test RE 1.000018123910406\n",
      "11 Train Loss 3.710157 Test MSE 14258.733547582791 Test RE 1.000013908804782\n",
      "12 Train Loss 3.3676217 Test MSE 14258.597707043164 Test RE 1.0000091453118638\n",
      "13 Train Loss 3.105538 Test MSE 14258.38158125751 Test RE 1.0000015664255795\n",
      "14 Train Loss 2.8939817 Test MSE 14258.325367702726 Test RE 0.999999595174567\n",
      "15 Train Loss 2.6802003 Test MSE 14258.236239177468 Test RE 0.9999964696802977\n",
      "16 Train Loss 2.5050788 Test MSE 14258.135715050586 Test RE 0.9999929445616401\n",
      "17 Train Loss 2.392028 Test MSE 14258.0546234318 Test RE 0.9999901008812471\n",
      "18 Train Loss 2.2951727 Test MSE 14258.000484573664 Test RE 0.9999882023623037\n",
      "19 Train Loss 2.2646184 Test MSE 14257.956531057236 Test RE 0.999986661016033\n",
      "20 Train Loss 2.2351577 Test MSE 14257.941325494174 Test RE 0.999986127792206\n",
      "21 Train Loss 2.2038426 Test MSE 14257.926347566978 Test RE 0.9999856025507629\n",
      "22 Train Loss 2.1081324 Test MSE 14257.905555622312 Test RE 0.9999848734246476\n",
      "23 Train Loss 2.108889 Test MSE 14257.85966924993 Test RE 0.999983264292261\n",
      "24 Train Loss 2.0102956 Test MSE 14257.853790819338 Test RE 0.9999830581486685\n",
      "25 Train Loss 2.0152376 Test MSE 14257.806482524113 Test RE 0.9999813991495771\n",
      "26 Train Loss 1.9291443 Test MSE 14257.802998451241 Test RE 0.9999812769706049\n",
      "27 Train Loss 1.9723456 Test MSE 14257.762435075076 Test RE 0.9999798544987994\n",
      "28 Train Loss 1.9066216 Test MSE 14257.762435075076 Test RE 0.9999798544987994\n",
      "29 Train Loss 2.1529107 Test MSE 14257.762435075076 Test RE 0.9999798544987994\n",
      "30 Train Loss 3.3648715 Test MSE 14257.769375941441 Test RE 0.9999800979004456\n",
      "31 Train Loss 4.8170915 Test MSE 14257.21661444834 Test RE 0.9999607135270936\n",
      "32 Train Loss 5.0097413 Test MSE 14257.077537863526 Test RE 0.999955836296709\n",
      "33 Train Loss 5.0585995 Test MSE 14256.932929449538 Test RE 0.999950765047225\n",
      "34 Train Loss 4.751759 Test MSE 14256.491709432214 Test RE 0.9999352918136187\n",
      "35 Train Loss 4.4417067 Test MSE 14256.219985575595 Test RE 0.9999257625558579\n",
      "36 Train Loss 4.201272 Test MSE 14255.865010486117 Test RE 0.9999133135705393\n",
      "37 Train Loss 3.8518384 Test MSE 14255.661358397987 Test RE 0.9999061714167183\n",
      "38 Train Loss 3.5868824 Test MSE 14255.501844562912 Test RE 0.9999005771720779\n",
      "39 Train Loss 3.356196 Test MSE 14255.47429305814 Test RE 0.9998996109213231\n",
      "40 Train Loss 3.1331818 Test MSE 14255.48738031303 Test RE 0.9999000699007229\n",
      "41 Train Loss 2.953371 Test MSE 14255.534377193244 Test RE 0.9999017181132263\n",
      "42 Train Loss 2.8418567 Test MSE 14255.587515966557 Test RE 0.9999035817228183\n",
      "43 Train Loss 2.751693 Test MSE 14255.621319298858 Test RE 0.9999047672247475\n",
      "44 Train Loss 2.7151468 Test MSE 14255.643457666523 Test RE 0.9999055436290263\n",
      "45 Train Loss 2.649247 Test MSE 14255.64990905249 Test RE 0.9999057698824357\n",
      "46 Train Loss 2.5768368 Test MSE 14255.659129034342 Test RE 0.999906093231831\n",
      "47 Train Loss 2.522916 Test MSE 14255.665949003935 Test RE 0.9999063324115235\n",
      "48 Train Loss 2.5149255 Test MSE 14255.669672216713 Test RE 0.9999064629864051\n",
      "49 Train Loss 2.3406892 Test MSE 14255.671696793965 Test RE 0.9999065339893063\n",
      "50 Train Loss 2.3817925 Test MSE 14255.688700154251 Test RE 0.9999071303051567\n",
      "51 Train Loss 2.199586 Test MSE 14255.688700154251 Test RE 0.9999071303051567\n",
      "52 Train Loss 3.3862844 Test MSE 14255.70331443051 Test RE 0.9999076428344211\n",
      "53 Train Loss 8.053117 Test MSE 14255.70331443051 Test RE 0.9999076428344211\n",
      "54 Train Loss 17.141373 Test MSE 14255.79303634216 Test RE 0.9999107894166013\n",
      "55 Train Loss 20.734726 Test MSE 14258.946580165872 Test RE 1.0000213791298878\n",
      "56 Train Loss 18.348265 Test MSE 14251.286476353713 Test RE 0.9997527303691827\n",
      "57 Train Loss 17.55948 Test MSE 14255.003984591574 Test RE 0.9998831167268201\n",
      "58 Train Loss 16.319773 Test MSE 14256.535478201673 Test RE 0.9999368267601034\n",
      "59 Train Loss 14.993482 Test MSE 14257.212508232733 Test RE 0.9999605695279281\n",
      "60 Train Loss 13.743078 Test MSE 14256.790532117051 Test RE 0.9999457713126743\n",
      "61 Train Loss 12.526651 Test MSE 14255.861736855004 Test RE 0.9999131987634926\n",
      "62 Train Loss 11.496366 Test MSE 14256.001380273114 Test RE 0.9999180960807339\n",
      "63 Train Loss 10.506453 Test MSE 14256.415845976811 Test RE 0.9999326313187057\n",
      "64 Train Loss 9.601271 Test MSE 14256.693105277489 Test RE 0.9999423546347603\n",
      "65 Train Loss 8.760657 Test MSE 14257.073953739426 Test RE 0.9999557106059416\n",
      "66 Train Loss 7.9986005 Test MSE 14257.413331251952 Test RE 0.9999676120824741\n",
      "67 Train Loss 7.3030195 Test MSE 14257.689810767572 Test RE 0.9999773077128907\n",
      "68 Train Loss 6.659513 Test MSE 14257.933878632843 Test RE 0.9999858666479484\n",
      "69 Train Loss 6.0842586 Test MSE 14258.087343630079 Test RE 0.9999912482977941\n",
      "70 Train Loss 5.55217 Test MSE 14258.12007785698 Test RE 0.9999923962049728\n",
      "71 Train Loss 5.0673437 Test MSE 14258.052050199882 Test RE 0.999990010644303\n",
      "72 Train Loss 4.6062016 Test MSE 14257.947821494039 Test RE 0.9999863555918883\n",
      "73 Train Loss 4.211259 Test MSE 14257.790654439998 Test RE 0.9999808440924539\n",
      "74 Train Loss 3.8375056 Test MSE 14257.638716294605 Test RE 0.9999755159303517\n",
      "75 Train Loss 3.5008059 Test MSE 14257.44239541328 Test RE 0.9999686313138209\n",
      "76 Train Loss 3.2287683 Test MSE 14257.22486825261 Test RE 0.9999610029762558\n",
      "77 Train Loss 3.0188258 Test MSE 14257.027069660317 Test RE 0.9999540664382345\n",
      "78 Train Loss 2.8212159 Test MSE 14256.858296915554 Test RE 0.9999481477606916\n",
      "79 Train Loss 2.7027214 Test MSE 14256.718313006018 Test RE 0.9999432386498716\n",
      "80 Train Loss 2.5958061 Test MSE 14256.63757814466 Test RE 0.999940407339353\n",
      "81 Train Loss 2.4929466 Test MSE 14256.574950385686 Test RE 0.9999382110256975\n",
      "82 Train Loss 2.3948922 Test MSE 14256.527621752432 Test RE 0.9999365512389563\n",
      "83 Train Loss 2.3568294 Test MSE 14256.493592167506 Test RE 0.99993535784015\n",
      "84 Train Loss 2.281602 Test MSE 14256.477832489405 Test RE 0.999934805156434\n",
      "85 Train Loss 2.2239954 Test MSE 14256.45451691868 Test RE 0.9999339874909321\n",
      "86 Train Loss 2.202743 Test MSE 14256.438889465759 Test RE 0.9999334394435343\n",
      "87 Train Loss 2.1580415 Test MSE 14256.431569694609 Test RE 0.9999331827425119\n",
      "88 Train Loss 2.1161554 Test MSE 14256.419816436968 Test RE 0.9999327705610105\n",
      "89 Train Loss 2.073144 Test MSE 14256.40906660843 Test RE 0.999932393569171\n",
      "90 Train Loss 2.0377858 Test MSE 14256.398427094982 Test RE 0.9999320204458942\n",
      "91 Train Loss 2.0491812 Test MSE 14256.391221901244 Test RE 0.999931767762691\n",
      "92 Train Loss 1.9637061 Test MSE 14256.389674143858 Test RE 0.9999317134834629\n",
      "93 Train Loss 2.0153995 Test MSE 14256.373910691991 Test RE 0.9999311606653878\n",
      "94 Train Loss 1.9516336 Test MSE 14256.373910691991 Test RE 0.9999311606653878\n",
      "95 Train Loss 2.073311 Test MSE 14256.373910691991 Test RE 0.9999311606653878\n",
      "96 Train Loss 2.3859396 Test MSE 14256.374181664773 Test RE 0.9999311701682996\n",
      "97 Train Loss 2.786111 Test MSE 14256.46620611326 Test RE 0.9999343974252725\n",
      "98 Train Loss 2.7933621 Test MSE 14256.618849546674 Test RE 0.999939750540443\n",
      "99 Train Loss 2.6646643 Test MSE 14256.61441755379 Test RE 0.9999395951134707\n",
      "100 Train Loss 2.490772 Test MSE 14256.475591165756 Test RE 0.9999347265543542\n",
      "101 Train Loss 2.3483462 Test MSE 14256.400491169412 Test RE 0.9999320928321301\n",
      "102 Train Loss 2.3916433 Test MSE 14256.412164812067 Test RE 0.9999325022218478\n",
      "103 Train Loss 2.1885078 Test MSE 14256.412164812067 Test RE 0.9999325022218478\n",
      "104 Train Loss 2.3482997 Test MSE 14256.39045473058 Test RE 0.9999317408583251\n",
      "105 Train Loss 2.266272 Test MSE 14256.389921101229 Test RE 0.9999317221441586\n",
      "106 Train Loss 2.3879898 Test MSE 14256.389921101229 Test RE 0.9999317221441586\n",
      "107 Train Loss 2.5353494 Test MSE 14256.389921101229 Test RE 0.9999317221441586\n",
      "108 Train Loss 3.34975 Test MSE 14256.389921101229 Test RE 0.9999317221441586\n",
      "109 Train Loss 6.3802853 Test MSE 14256.40130948053 Test RE 0.9999321215299609\n",
      "110 Train Loss 11.811949 Test MSE 14256.519208194346 Test RE 0.9999362561802398\n",
      "111 Train Loss 10.726799 Test MSE 14262.95779171575 Test RE 1.0001620282029726\n",
      "112 Train Loss 10.062457 Test MSE 14261.097328572636 Test RE 1.000096795405492\n",
      "113 Train Loss 9.252066 Test MSE 14256.64984590281 Test RE 0.9999408375608683\n",
      "114 Train Loss 8.596404 Test MSE 14253.971229613187 Test RE 0.9998468960084106\n",
      "115 Train Loss 7.8921022 Test MSE 14252.844421418711 Test RE 0.9998073751645651\n",
      "116 Train Loss 7.221287 Test MSE 14252.301498898938 Test RE 0.9997883325407649\n",
      "117 Train Loss 6.6853647 Test MSE 14252.279146137009 Test RE 0.999787548525659\n",
      "118 Train Loss 6.2226124 Test MSE 14252.872257274628 Test RE 0.9998083514777705\n",
      "119 Train Loss 5.726421 Test MSE 14253.596558737454 Test RE 0.9998337552503778\n",
      "120 Train Loss 5.213254 Test MSE 14254.187010973474 Test RE 0.9998544639873027\n",
      "121 Train Loss 4.7893095 Test MSE 14254.680566805466 Test RE 0.9998717739794395\n",
      "122 Train Loss 4.458847 Test MSE 14254.877216483583 Test RE 0.99987867079426\n",
      "123 Train Loss 4.1539555 Test MSE 14254.874812959995 Test RE 0.9998785864991785\n",
      "124 Train Loss 3.7811985 Test MSE 14254.716882824583 Test RE 0.9998730476431726\n",
      "125 Train Loss 3.549684 Test MSE 14254.526522462953 Test RE 0.9998663713678053\n",
      "126 Train Loss 3.3355732 Test MSE 14254.372757512874 Test RE 0.9998609785258527\n",
      "127 Train Loss 3.1314552 Test MSE 14254.247836772767 Test RE 0.9998565972861719\n",
      "128 Train Loss 2.9397573 Test MSE 14254.175684326356 Test RE 0.9998540667355713\n",
      "129 Train Loss 2.75508 Test MSE 14254.157091893687 Test RE 0.9998534146555813\n",
      "130 Train Loss 2.640083 Test MSE 14254.179609978688 Test RE 0.9998542044173032\n",
      "131 Train Loss 2.528696 Test MSE 14254.202031067614 Test RE 0.9998549907765263\n",
      "132 Train Loss 2.432672 Test MSE 14254.233488128551 Test RE 0.9998560940470549\n",
      "133 Train Loss 2.3341086 Test MSE 14254.26735528862 Test RE 0.9998572818438699\n",
      "134 Train Loss 2.3329194 Test MSE 14254.302859502237 Test RE 0.9998585270542872\n",
      "135 Train Loss 2.256226 Test MSE 14254.325875533194 Test RE 0.9998593342759996\n",
      "136 Train Loss 2.4656475 Test MSE 14254.308240349264 Test RE 0.999858715772225\n",
      "137 Train Loss 2.9018254 Test MSE 14254.194082492779 Test RE 0.9998547120017724\n",
      "138 Train Loss 4.266445 Test MSE 14253.62193341544 Test RE 0.9998346452169269\n",
      "139 Train Loss 4.6344233 Test MSE 14251.000410855047 Test RE 0.9997426963207113\n",
      "140 Train Loss 4.326495 Test MSE 14250.985423838012 Test RE 0.9997421706325378\n",
      "141 Train Loss 4.0809627 Test MSE 14252.774134405974 Test RE 0.9998049099176896\n",
      "142 Train Loss 3.7358558 Test MSE 14252.913011075996 Test RE 0.9998097808725305\n",
      "143 Train Loss 3.4250073 Test MSE 14252.97050135588 Test RE 0.9998117972773919\n",
      "144 Train Loss 3.2042186 Test MSE 14253.363633202773 Test RE 0.9998255858117902\n",
      "145 Train Loss 3.0609715 Test MSE 14253.54486144432 Test RE 0.9998319420678103\n",
      "146 Train Loss 2.9957085 Test MSE 14253.679339193139 Test RE 0.9998366586079043\n",
      "147 Train Loss 2.8981323 Test MSE 14253.746807683134 Test RE 0.9998390249228805\n",
      "148 Train Loss 2.8488462 Test MSE 14253.82355545211 Test RE 0.9998417166821858\n",
      "149 Train Loss 2.7784836 Test MSE 14253.863111914288 Test RE 0.9998431040353212\n",
      "150 Train Loss 2.6740685 Test MSE 14253.912507357854 Test RE 0.9998448364656961\n",
      "151 Train Loss 2.5467882 Test MSE 14253.970741658406 Test RE 0.9998468788945807\n",
      "152 Train Loss 2.377895 Test MSE 14254.017281464417 Test RE 0.9998485111640248\n",
      "153 Train Loss 2.2335396 Test MSE 14254.030636828415 Test RE 0.9998489795701386\n",
      "154 Train Loss 2.1038876 Test MSE 14254.011718545275 Test RE 0.9998483160584327\n",
      "155 Train Loss 2.0819411 Test MSE 14253.98210129349 Test RE 0.9998472773061401\n",
      "156 Train Loss 2.041953 Test MSE 14253.97421187549 Test RE 0.9998470006040177\n",
      "157 Train Loss 2.0857701 Test MSE 14253.965683206503 Test RE 0.9998467014816376\n",
      "158 Train Loss 1.9720112 Test MSE 14253.965683206503 Test RE 0.9998467014816376\n",
      "159 Train Loss 2.0910907 Test MSE 14253.944958878443 Test RE 0.999845974625721\n",
      "160 Train Loss 2.464522 Test MSE 14253.944958878443 Test RE 0.999845974625721\n",
      "161 Train Loss 2.559311 Test MSE 14254.060747341808 Test RE 0.9998500356205627\n",
      "162 Train Loss 2.6275113 Test MSE 14254.002352699981 Test RE 0.9998479875745514\n",
      "163 Train Loss 2.5708036 Test MSE 14253.905714875256 Test RE 0.9998445982353347\n",
      "164 Train Loss 2.4555314 Test MSE 14253.905714875256 Test RE 0.9998445982353347\n",
      "165 Train Loss 2.2589202 Test MSE 14253.901906277342 Test RE 0.9998444646576895\n",
      "166 Train Loss 2.226331 Test MSE 14253.865164913577 Test RE 0.9998431760395592\n",
      "167 Train Loss 2.2179909 Test MSE 14253.868343921778 Test RE 0.9998432875359654\n",
      "168 Train Loss 2.0337136 Test MSE 14253.867520985143 Test RE 0.9998432586733554\n",
      "169 Train Loss 2.097873 Test MSE 14253.920348902338 Test RE 0.9998451114893669\n",
      "170 Train Loss 2.1116757 Test MSE 14253.920348902338 Test RE 0.9998451114893669\n",
      "171 Train Loss 2.222711 Test MSE 14253.920348902338 Test RE 0.9998451114893669\n",
      "172 Train Loss 2.2686543 Test MSE 14253.893706455809 Test RE 0.9998441770681089\n",
      "173 Train Loss 2.390683 Test MSE 14253.887316459924 Test RE 0.9998439529538767\n",
      "174 Train Loss 2.4711761 Test MSE 14253.841991287227 Test RE 0.9998423632775006\n",
      "175 Train Loss 2.5260634 Test MSE 14253.790460542305 Test RE 0.9998405559518555\n",
      "176 Train Loss 2.5739017 Test MSE 14253.749068401863 Test RE 0.9998391042127242\n",
      "177 Train Loss 2.5924544 Test MSE 14253.708120952668 Test RE 0.9998376680680956\n",
      "178 Train Loss 2.6336932 Test MSE 14253.696660243504 Test RE 0.9998372661077448\n",
      "179 Train Loss 2.6230228 Test MSE 14253.65827213672 Test RE 0.9998359197241894\n",
      "180 Train Loss 2.5520525 Test MSE 14253.65827213672 Test RE 0.9998359197241894\n",
      "181 Train Loss 2.5446413 Test MSE 14253.65827213672 Test RE 0.9998359197241894\n",
      "182 Train Loss 2.5864105 Test MSE 14253.65827213672 Test RE 0.9998359197241894\n",
      "183 Train Loss 2.4000692 Test MSE 14253.65827213672 Test RE 0.9998359197241894\n",
      "184 Train Loss 3.4648542 Test MSE 14253.64975935985 Test RE 0.9998356211558781\n",
      "185 Train Loss 4.8441424 Test MSE 14252.017878988127 Test RE 0.999778384628547\n",
      "186 Train Loss 4.5417104 Test MSE 14251.010383147586 Test RE 0.999743046111063\n",
      "187 Train Loss 4.276244 Test MSE 14252.440444778453 Test RE 0.9997932060038554\n",
      "188 Train Loss 4.0557885 Test MSE 14252.489200641592 Test RE 0.9997949160877959\n",
      "189 Train Loss 3.722091 Test MSE 14252.560912221737 Test RE 0.9997974313250312\n",
      "190 Train Loss 3.4439692 Test MSE 14253.05888493512 Test RE 0.9998148972209514\n",
      "191 Train Loss 3.2793891 Test MSE 14253.444499709212 Test RE 0.9998284220646766\n",
      "192 Train Loss 3.1889837 Test MSE 14253.671211249755 Test RE 0.999836373537056\n",
      "193 Train Loss 3.087512 Test MSE 14253.821360920947 Test RE 0.9998416397139291\n",
      "194 Train Loss 3.0573115 Test MSE 14253.971251932113 Test RE 0.9998468967911927\n",
      "195 Train Loss 3.0288513 Test MSE 14254.022520876691 Test RE 0.9998486949233811\n",
      "196 Train Loss 2.9999394 Test MSE 14254.071196069819 Test RE 0.9998504020831214\n",
      "197 Train Loss 2.9755788 Test MSE 14254.118324161615 Test RE 0.9998520549794043\n",
      "198 Train Loss 2.9432724 Test MSE 14254.160839121523 Test RE 0.9998535460796384\n",
      "199 Train Loss 2.9348106 Test MSE 14254.20561553257 Test RE 0.9998551164919097\n",
      "Training time: 87.47\n",
      "Training time: 87.47\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 4.970844 Test MSE 14254.268250258165 Test RE 0.9998573132324278\n",
      "1 Train Loss 5.841858 Test MSE 14253.442391791084 Test RE 0.9998283481331952\n",
      "2 Train Loss 5.864032 Test MSE 14252.995476721513 Test RE 0.9998126732580956\n",
      "3 Train Loss 5.477447 Test MSE 14255.041860303245 Test RE 0.9998844450764832\n",
      "4 Train Loss 5.080209 Test MSE 14255.394280753491 Test RE 0.9998968048279268\n",
      "5 Train Loss 4.7562704 Test MSE 14255.366831637186 Test RE 0.9998958421643779\n",
      "6 Train Loss 4.505305 Test MSE 14255.411011163633 Test RE 0.9998973915769988\n",
      "7 Train Loss 4.249836 Test MSE 14255.45821766689 Test RE 0.9998990471455533\n",
      "8 Train Loss 4.0244355 Test MSE 14255.499903285283 Test RE 0.9999005090901292\n",
      "9 Train Loss 3.8050463 Test MSE 14255.530512549789 Test RE 0.9999015825776583\n",
      "10 Train Loss 3.6044812 Test MSE 14255.556960781852 Test RE 0.9999025101340479\n",
      "11 Train Loss 3.4126 Test MSE 14255.58265440947 Test RE 0.9999034112251537\n",
      "12 Train Loss 3.2344253 Test MSE 14255.607641001021 Test RE 0.999904287519297\n",
      "13 Train Loss 3.0640948 Test MSE 14255.630633901423 Test RE 0.9999050938928661\n",
      "14 Train Loss 2.904267 Test MSE 14255.651794317411 Test RE 0.9999058359996318\n",
      "15 Train Loss 2.7510011 Test MSE 14255.671159132056 Test RE 0.999906515133244\n",
      "16 Train Loss 2.6063106 Test MSE 14255.689129253971 Test RE 0.9999071453538813\n",
      "17 Train Loss 2.5331585 Test MSE 14255.704608883434 Test RE 0.9999076882314568\n",
      "18 Train Loss 2.430238 Test MSE 14255.713309783723 Test RE 0.9999079933758187\n",
      "19 Train Loss 2.3176992 Test MSE 14255.724774073064 Test RE 0.9999083954332952\n",
      "20 Train Loss 2.2302332 Test MSE 14255.737793153368 Test RE 0.9999088520177528\n",
      "21 Train Loss 2.182409 Test MSE 14255.747449217397 Test RE 0.999909190659723\n",
      "22 Train Loss 2.1226907 Test MSE 14255.752769244802 Test RE 0.9999093772351312\n",
      "23 Train Loss 2.1024444 Test MSE 14255.757921043409 Test RE 0.9999095579106574\n",
      "24 Train Loss 2.0723763 Test MSE 14255.759718671034 Test RE 0.9999096209541308\n",
      "25 Train Loss 2.0605686 Test MSE 14255.761736720595 Test RE 0.9999096917278791\n",
      "26 Train Loss 2.0827503 Test MSE 14255.762221327743 Test RE 0.9999097087232309\n",
      "27 Train Loss 2.1060696 Test MSE 14255.761735988215 Test RE 0.9999096917021943\n",
      "28 Train Loss 2.1389482 Test MSE 14255.761287990917 Test RE 0.9999096759907629\n",
      "29 Train Loss 2.4149837 Test MSE 14255.760677870212 Test RE 0.999909654593603\n",
      "30 Train Loss 4.314286 Test MSE 14255.758931849332 Test RE 0.9999095933599996\n",
      "31 Train Loss 5.696507 Test MSE 14256.130252880566 Test RE 0.9999226156426232\n",
      "32 Train Loss 5.54933 Test MSE 14256.215248892942 Test RE 0.999925596441298\n",
      "33 Train Loss 5.1707196 Test MSE 14255.358039030714 Test RE 0.9998955338000924\n",
      "34 Train Loss 4.700319 Test MSE 14255.328962540603 Test RE 0.9998945140618531\n",
      "35 Train Loss 4.330881 Test MSE 14255.39563677395 Test RE 0.9998968523846785\n",
      "36 Train Loss 3.9444256 Test MSE 14255.437337155181 Test RE 0.9998983148502073\n",
      "37 Train Loss 3.5823052 Test MSE 14255.503156161794 Test RE 0.9999006231707548\n",
      "38 Train Loss 3.2610204 Test MSE 14255.532317096046 Test RE 0.9999016458642753\n",
      "39 Train Loss 2.9765754 Test MSE 14255.532888262214 Test RE 0.9999016658954456\n",
      "40 Train Loss 2.7239435 Test MSE 14255.514996791051 Test RE 0.9999010384296899\n",
      "41 Train Loss 2.494969 Test MSE 14255.486467347506 Test RE 0.999900037882375\n",
      "42 Train Loss 2.3498666 Test MSE 14255.446341731511 Test RE 0.999898630647579\n",
      "43 Train Loss 2.2171087 Test MSE 14255.408595727036 Test RE 0.9998973068656969\n",
      "44 Train Loss 2.0903962 Test MSE 14255.362346202959 Test RE 0.9998956848563464\n",
      "45 Train Loss 2.0615466 Test MSE 14255.31164437372 Test RE 0.9998939066979202\n",
      "46 Train Loss 2.0566213 Test MSE 14255.295723922041 Test RE 0.9998933483528113\n",
      "47 Train Loss 2.0011215 Test MSE 14255.25474828476 Test RE 0.9998919112975094\n",
      "48 Train Loss 2.091143 Test MSE 14255.292120597362 Test RE 0.999893221980806\n",
      "49 Train Loss 2.6579647 Test MSE 14255.330770844052 Test RE 0.9998945774806854\n",
      "50 Train Loss 5.4857802 Test MSE 14255.070137500745 Test RE 0.9998854367928239\n",
      "51 Train Loss 7.356231 Test MSE 14250.385604747491 Test RE 0.9997211310085483\n",
      "52 Train Loss 6.7078967 Test MSE 14251.25046765795 Test RE 0.9997514673318173\n",
      "53 Train Loss 6.394675 Test MSE 14250.6672595762 Test RE 0.9997310105620011\n",
      "54 Train Loss 5.8180304 Test MSE 14250.219839304567 Test RE 0.9997153164396445\n",
      "55 Train Loss 5.4403515 Test MSE 14248.679260624232 Test RE 0.9996612758091024\n",
      "56 Train Loss 5.1923604 Test MSE 14247.448692778631 Test RE 0.999618107682386\n",
      "57 Train Loss 5.0222745 Test MSE 14246.31565692029 Test RE 0.9995783593139695\n",
      "58 Train Loss 5.0139875 Test MSE 14243.865448135151 Test RE 0.9994923974044749\n",
      "59 Train Loss 4.765643 Test MSE 14240.768261742702 Test RE 0.9993837266732127\n",
      "60 Train Loss 4.4790826 Test MSE 14240.171425975208 Test RE 0.9993627841863943\n",
      "61 Train Loss 4.1428857 Test MSE 14239.737743676904 Test RE 0.9993475663482098\n",
      "62 Train Loss 4.1121016 Test MSE 14238.598061036402 Test RE 0.9993075739732287\n",
      "63 Train Loss 5.2892146 Test MSE 14226.615360799535 Test RE 0.9988869945235831\n",
      "64 Train Loss 6.0444036 Test MSE 14212.365854125868 Test RE 0.9983866220717712\n",
      "65 Train Loss 6.2340636 Test MSE 14206.42646421329 Test RE 0.998177985904388\n",
      "66 Train Loss 6.4408875 Test MSE 14197.01969330559 Test RE 0.9978474599093127\n",
      "67 Train Loss 5.882853 Test MSE 14173.490521452513 Test RE 0.9970202348694645\n",
      "68 Train Loss 6.0335116 Test MSE 14155.448364265803 Test RE 0.9963854539028029\n",
      "69 Train Loss 5.8832784 Test MSE 14141.291640315356 Test RE 0.9958870916493824\n",
      "70 Train Loss 5.8682065 Test MSE 14108.820091663301 Test RE 0.9947430454716343\n",
      "71 Train Loss 6.105104 Test MSE 13997.761237659757 Test RE 0.990820205526055\n",
      "72 Train Loss 6.5546093 Test MSE 13977.225485359271 Test RE 0.9900931354548144\n",
      "73 Train Loss 6.8077497 Test MSE 13953.932548226065 Test RE 0.9892678002181021\n",
      "74 Train Loss 6.8410554 Test MSE 13908.159990060723 Test RE 0.9876439385415239\n",
      "75 Train Loss 6.602943 Test MSE 13885.148353202876 Test RE 0.9868265510409369\n",
      "76 Train Loss 7.287719 Test MSE 13820.16368522643 Test RE 0.9845145914132215\n",
      "77 Train Loss 7.600536 Test MSE 13777.649923856747 Test RE 0.9829991369654645\n",
      "78 Train Loss 8.874242 Test MSE 13616.203466777908 Test RE 0.9772227749045298\n",
      "79 Train Loss 8.310951 Test MSE 13525.932162354491 Test RE 0.9739780425725156\n",
      "80 Train Loss 9.324461 Test MSE 13336.442010710905 Test RE 0.9671315567441591\n",
      "81 Train Loss 9.064077 Test MSE 13315.025817490048 Test RE 0.9663547154357128\n",
      "82 Train Loss 9.6690445 Test MSE 13266.839971202284 Test RE 0.964604556439374\n",
      "83 Train Loss 9.68776 Test MSE 13190.841302363819 Test RE 0.9618377351475805\n",
      "84 Train Loss 10.941815 Test MSE 13081.29153215949 Test RE 0.9578353828374025\n",
      "85 Train Loss 11.065022 Test MSE 12988.95273762796 Test RE 0.9544487906872952\n",
      "86 Train Loss 13.553916 Test MSE 12888.959195845304 Test RE 0.950767850906114\n",
      "87 Train Loss 15.060359 Test MSE 12781.146971448425 Test RE 0.9467830585315807\n",
      "88 Train Loss 15.443764 Test MSE 12615.8662882262 Test RE 0.9406414286346928\n",
      "89 Train Loss 15.155394 Test MSE 12498.29433842742 Test RE 0.9362480752017491\n",
      "90 Train Loss 17.985632 Test MSE 12400.212506631497 Test RE 0.932567181085037\n",
      "91 Train Loss 18.11018 Test MSE 12254.281286055231 Test RE 0.9270635078899585\n",
      "92 Train Loss 17.736004 Test MSE 12044.524737196602 Test RE 0.9190949877751429\n",
      "93 Train Loss 16.834608 Test MSE 11876.959741959665 Test RE 0.9126793111641117\n",
      "94 Train Loss 16.454422 Test MSE 11842.29724873332 Test RE 0.9113465266458532\n",
      "95 Train Loss 16.097755 Test MSE 11408.398107496654 Test RE 0.8944949596205817\n",
      "96 Train Loss 16.24635 Test MSE 11139.788241035045 Test RE 0.8839018266559644\n",
      "97 Train Loss 15.059134 Test MSE 10940.801087979698 Test RE 0.8759717993424216\n",
      "98 Train Loss 14.808897 Test MSE 10791.049244142696 Test RE 0.8699562267849376\n",
      "99 Train Loss 15.708988 Test MSE 10438.635003293717 Test RE 0.8556327908767912\n",
      "100 Train Loss 14.896162 Test MSE 10006.708311869836 Test RE 0.8377437260750795\n",
      "101 Train Loss 14.191935 Test MSE 10043.9175662933 Test RE 0.8392998269756093\n",
      "102 Train Loss 17.961775 Test MSE 9481.14904285437 Test RE 0.8154475844924347\n",
      "103 Train Loss 16.762661 Test MSE 9004.22531159187 Test RE 0.7946735179407243\n",
      "104 Train Loss 17.225307 Test MSE 8954.135495561311 Test RE 0.7924600813837531\n",
      "105 Train Loss 18.742735 Test MSE 8698.23802973837 Test RE 0.7810542642112189\n",
      "106 Train Loss 21.976257 Test MSE 8051.854315086424 Test RE 0.7514732365880215\n",
      "107 Train Loss 20.474993 Test MSE 7470.4397936176365 Test RE 0.7238334496222082\n",
      "108 Train Loss 20.033962 Test MSE 7137.690652244987 Test RE 0.7075292920991909\n",
      "109 Train Loss 18.31973 Test MSE 6751.404783351653 Test RE 0.688117554595792\n",
      "110 Train Loss 19.002068 Test MSE 6491.580537776517 Test RE 0.6747467299887224\n",
      "111 Train Loss 15.57241 Test MSE 5020.507802700662 Test RE 0.5933888574349168\n",
      "112 Train Loss 14.469315 Test MSE 4380.354887473031 Test RE 0.5542685190540956\n",
      "113 Train Loss 13.021759 Test MSE 3739.2348692383885 Test RE 0.5121025474505081\n",
      "114 Train Loss 12.846525 Test MSE 3703.1786262921605 Test RE 0.5096275462512363\n",
      "115 Train Loss 12.030304 Test MSE 3468.9517964314514 Test RE 0.493247276880563\n",
      "116 Train Loss 11.194564 Test MSE 2882.5732102784427 Test RE 0.44963047261334455\n",
      "117 Train Loss 9.966133 Test MSE 2347.4484416389664 Test RE 0.40575473347561697\n",
      "118 Train Loss 8.26703 Test MSE 1781.1039922457308 Test RE 0.3534355270567488\n",
      "119 Train Loss 7.5632277 Test MSE 1530.2457193948096 Test RE 0.3276017027597053\n",
      "120 Train Loss 6.770402 Test MSE 1151.617570240847 Test RE 0.28419714116479994\n",
      "121 Train Loss 5.9974947 Test MSE 853.7154501273042 Test RE 0.2446933317323496\n",
      "122 Train Loss 5.0036154 Test MSE 505.5949464993582 Test RE 0.1883072000975173\n",
      "123 Train Loss 4.6212044 Test MSE 404.76337332883327 Test RE 0.16848691060899632\n",
      "124 Train Loss 4.1651707 Test MSE 387.5463776554253 Test RE 0.1648645967746622\n",
      "125 Train Loss 3.5589943 Test MSE 352.2487571056835 Test RE 0.15717747164947965\n",
      "126 Train Loss 2.8306143 Test MSE 198.97494821962556 Test RE 0.11813124033734106\n",
      "127 Train Loss 1.6758225 Test MSE 107.20926544978428 Test RE 0.0867125032863808\n",
      "128 Train Loss 1.5034307 Test MSE 86.25357023390265 Test RE 0.07777752229930184\n",
      "129 Train Loss 1.172055 Test MSE 86.0647395505357 Test RE 0.07769233840651027\n",
      "130 Train Loss 0.8626898 Test MSE 76.0219275591605 Test RE 0.07301885226673573\n",
      "131 Train Loss 0.76490986 Test MSE 46.198068555530064 Test RE 0.056921649038953026\n",
      "132 Train Loss 0.6401777 Test MSE 16.105587837073237 Test RE 0.03360886561128176\n",
      "133 Train Loss 0.6008282 Test MSE 10.313063732835396 Test RE 0.026894248572327254\n",
      "134 Train Loss 0.557136 Test MSE 6.004434792228037 Test RE 0.020521146941262332\n",
      "135 Train Loss 0.50139135 Test MSE 10.098159777492347 Test RE 0.026612561786915578\n",
      "136 Train Loss 0.44178066 Test MSE 6.362633944881284 Test RE 0.02112438299312963\n",
      "137 Train Loss 0.4282708 Test MSE 2.5496023345751464 Test RE 0.013372166880620682\n",
      "138 Train Loss 0.37811187 Test MSE 0.8695322129275221 Test RE 0.007809233372703873\n",
      "139 Train Loss 0.33194253 Test MSE 2.656205389697017 Test RE 0.01364886035929994\n",
      "140 Train Loss 0.3244188 Test MSE 3.3175827065228316 Test RE 0.015253743633450752\n",
      "141 Train Loss 0.31368226 Test MSE 3.783507706482194 Test RE 0.016289692086678166\n",
      "142 Train Loss 0.2727768 Test MSE 3.0586258758952596 Test RE 0.014646327565480005\n",
      "143 Train Loss 0.25584337 Test MSE 1.3437027781242403 Test RE 0.00970771843117695\n",
      "144 Train Loss 0.23519067 Test MSE 0.6614924837109737 Test RE 0.006811268816346432\n",
      "145 Train Loss 0.20797025 Test MSE 1.3812577417827654 Test RE 0.009842443407723422\n",
      "146 Train Loss 0.17477617 Test MSE 1.6880496519030996 Test RE 0.010880733469491529\n",
      "147 Train Loss 0.15839934 Test MSE 2.421056764828261 Test RE 0.013030709106326714\n",
      "148 Train Loss 0.14771399 Test MSE 2.4855374763282505 Test RE 0.013203094192613694\n",
      "149 Train Loss 0.14030224 Test MSE 1.600758352830083 Test RE 0.0105956706896089\n",
      "150 Train Loss 0.13774167 Test MSE 1.204660688957014 Test RE 0.009191744269042679\n",
      "151 Train Loss 0.13708502 Test MSE 1.1636332504082794 Test RE 0.009033865598181732\n",
      "152 Train Loss 0.1356329 Test MSE 1.3995000815746383 Test RE 0.009907225035929732\n",
      "153 Train Loss 0.13313553 Test MSE 1.740680732185278 Test RE 0.011049054729896\n",
      "154 Train Loss 0.12950699 Test MSE 1.1730536875689301 Test RE 0.00907035966305787\n",
      "155 Train Loss 0.0959136 Test MSE 1.2521168533129505 Test RE 0.009371044372925524\n",
      "156 Train Loss 0.060343675 Test MSE 1.0970304271607851 Test RE 0.008771520857428496\n",
      "157 Train Loss 0.053821348 Test MSE 0.3257552428173245 Test RE 0.004779817014053195\n",
      "158 Train Loss 0.05096704 Test MSE 0.4105192310507516 Test RE 0.005365773263286549\n",
      "159 Train Loss 0.04939609 Test MSE 0.6712581249599939 Test RE 0.006861362134713564\n",
      "160 Train Loss 0.04232755 Test MSE 0.15500785412843643 Test RE 0.003297178170006552\n",
      "161 Train Loss 0.036076818 Test MSE 0.044811259785067475 Test RE 0.0017727975403233197\n",
      "162 Train Loss 0.032638747 Test MSE 0.031362502786245486 Test RE 0.001483101657294658\n",
      "163 Train Loss 0.03154234 Test MSE 0.0029794524533079615 Test RE 0.0004571237580075428\n",
      "164 Train Loss 0.031542636 Test MSE 0.0029794524533079615 Test RE 0.0004571237580075428\n",
      "165 Train Loss 0.03144253 Test MSE 0.00596413776024444 Test RE 0.0006467544072942342\n",
      "166 Train Loss 0.031389005 Test MSE 0.008251582778477025 Test RE 0.0007607363949432015\n",
      "167 Train Loss 0.031343397 Test MSE 0.009629306983321664 Test RE 0.0008217942153049462\n",
      "168 Train Loss 0.031339407 Test MSE 0.009629306983321664 Test RE 0.0008217942153049462\n",
      "169 Train Loss 0.03131866 Test MSE 0.00968296317978335 Test RE 0.0008240806257969272\n",
      "170 Train Loss 0.031315926 Test MSE 0.00968296317978335 Test RE 0.0008240806257969272\n",
      "171 Train Loss 0.031313583 Test MSE 0.00968296317978335 Test RE 0.0008240806257969272\n",
      "172 Train Loss 0.031311575 Test MSE 0.00968296317978335 Test RE 0.0008240806257969272\n",
      "173 Train Loss 0.03130987 Test MSE 0.00968296317978335 Test RE 0.0008240806257969272\n",
      "174 Train Loss 0.031308424 Test MSE 0.00968296317978335 Test RE 0.0008240806257969272\n",
      "175 Train Loss 0.031307194 Test MSE 0.00968296317978335 Test RE 0.0008240806257969272\n",
      "176 Train Loss 0.031306155 Test MSE 0.00968296317978335 Test RE 0.0008240806257969272\n",
      "177 Train Loss 0.031305272 Test MSE 0.00968296317978335 Test RE 0.0008240806257969272\n",
      "178 Train Loss 0.031304516 Test MSE 0.00968296317978335 Test RE 0.0008240806257969272\n",
      "179 Train Loss 0.031303883 Test MSE 0.00968296317978335 Test RE 0.0008240806257969272\n",
      "180 Train Loss 0.031303342 Test MSE 0.00968296317978335 Test RE 0.0008240806257969272\n",
      "181 Train Loss 0.031302884 Test MSE 0.00968296317978335 Test RE 0.0008240806257969272\n",
      "182 Train Loss 0.031302497 Test MSE 0.00968296317978335 Test RE 0.0008240806257969272\n",
      "183 Train Loss 0.031302165 Test MSE 0.00968296317978335 Test RE 0.0008240806257969272\n",
      "184 Train Loss 0.031301886 Test MSE 0.00968296317978335 Test RE 0.0008240806257969272\n",
      "185 Train Loss 0.031301647 Test MSE 0.00968296317978335 Test RE 0.0008240806257969272\n",
      "186 Train Loss 0.031301446 Test MSE 0.00968296317978335 Test RE 0.0008240806257969272\n",
      "187 Train Loss 0.031301275 Test MSE 0.00968296317978335 Test RE 0.0008240806257969272\n",
      "188 Train Loss 0.03130113 Test MSE 0.00968296317978335 Test RE 0.0008240806257969272\n",
      "189 Train Loss 0.031301003 Test MSE 0.00968296317978335 Test RE 0.0008240806257969272\n",
      "190 Train Loss 0.0313009 Test MSE 0.00968296317978335 Test RE 0.0008240806257969272\n",
      "191 Train Loss 0.03130081 Test MSE 0.00968296317978335 Test RE 0.0008240806257969272\n",
      "192 Train Loss 0.031300735 Test MSE 0.00968296317978335 Test RE 0.0008240806257969272\n",
      "193 Train Loss 0.03130148 Test MSE 0.01458720866040661 Test RE 0.0010114668679497543\n",
      "194 Train Loss 0.031306714 Test MSE 0.01458720866040661 Test RE 0.0010114668679497543\n",
      "195 Train Loss 0.031311247 Test MSE 0.01458720866040661 Test RE 0.0010114668679497543\n",
      "196 Train Loss 0.03131517 Test MSE 0.01458720866040661 Test RE 0.0010114668679497543\n",
      "197 Train Loss 0.03131114 Test MSE 0.014587720015693164 Test RE 0.0010114845963037334\n",
      "198 Train Loss 0.03131406 Test MSE 0.014587720015693164 Test RE 0.0010114845963037334\n",
      "199 Train Loss 0.031316582 Test MSE 0.014587720015693164 Test RE 0.0010114845963037334\n",
      "Training time: 107.43\n",
      "Training time: 107.43\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 16.438276 Test MSE 14252.920166929816 Test RE 0.9998100318560218\n",
      "1 Train Loss 11.711773 Test MSE 14243.348497438625 Test RE 0.9994742600182084\n",
      "2 Train Loss 10.696401 Test MSE 14236.09782462508 Test RE 0.9992198330712394\n",
      "3 Train Loss 9.786688 Test MSE 14237.70512807175 Test RE 0.9992762391233726\n",
      "4 Train Loss 9.2588 Test MSE 14236.422828750416 Test RE 0.9992312388903904\n",
      "5 Train Loss 8.578733 Test MSE 14232.242504957621 Test RE 0.9990845230736781\n",
      "6 Train Loss 8.028586 Test MSE 14229.303918673077 Test RE 0.998981375321745\n",
      "7 Train Loss 7.6913366 Test MSE 14225.433746952134 Test RE 0.9988455115993708\n",
      "8 Train Loss 7.8562503 Test MSE 14216.564844378248 Test RE 0.9985340959737008\n",
      "9 Train Loss 8.364284 Test MSE 14197.501192013944 Test RE 0.9978643810026042\n",
      "10 Train Loss 7.8218813 Test MSE 14185.562258429725 Test RE 0.9974447317270391\n",
      "11 Train Loss 7.8595057 Test MSE 14180.725576152492 Test RE 0.9972746738025906\n",
      "12 Train Loss 7.428061 Test MSE 14178.640365984455 Test RE 0.997201348790735\n",
      "13 Train Loss 7.651467 Test MSE 14168.440778409633 Test RE 0.9968426094458489\n",
      "14 Train Loss 7.4086637 Test MSE 14152.928121937952 Test RE 0.9962967514996041\n",
      "15 Train Loss 7.199313 Test MSE 14135.403900874453 Test RE 0.9956797508241851\n",
      "16 Train Loss 6.9656057 Test MSE 14131.560656475946 Test RE 0.9955443850153438\n",
      "17 Train Loss 6.8440876 Test MSE 14108.140464252321 Test RE 0.994719086601302\n",
      "18 Train Loss 8.095348 Test MSE 14070.466021821028 Test RE 0.993390047554766\n",
      "19 Train Loss 8.178565 Test MSE 14016.547635698786 Test RE 0.9914848725822238\n",
      "20 Train Loss 8.194592 Test MSE 13909.116439006315 Test RE 0.9876778975542388\n",
      "21 Train Loss 7.932422 Test MSE 13878.538942789653 Test RE 0.9865916555299427\n",
      "22 Train Loss 8.601032 Test MSE 13867.146896375885 Test RE 0.9861866559353077\n",
      "23 Train Loss 8.4060955 Test MSE 13811.825012368368 Test RE 0.9842175332881903\n",
      "24 Train Loss 8.847603 Test MSE 13754.819762716694 Test RE 0.9821843632781907\n",
      "25 Train Loss 8.709687 Test MSE 13693.15557554375 Test RE 0.9799802764778096\n",
      "26 Train Loss 8.553057 Test MSE 13595.588169124927 Test RE 0.9764827239240111\n",
      "27 Train Loss 8.151553 Test MSE 13540.097010108633 Test RE 0.9744879017241671\n",
      "28 Train Loss 8.337578 Test MSE 13471.922372655055 Test RE 0.9720315235918324\n",
      "29 Train Loss 8.195013 Test MSE 13391.420290824952 Test RE 0.9691229628422513\n",
      "30 Train Loss 8.371328 Test MSE 13204.681338114047 Test RE 0.9623421904135877\n",
      "31 Train Loss 7.968542 Test MSE 13093.637767219741 Test RE 0.9582872828574597\n",
      "32 Train Loss 7.42906 Test MSE 13062.48516885463 Test RE 0.9571466175474397\n",
      "33 Train Loss 7.1966267 Test MSE 13069.125252962844 Test RE 0.9573898609458648\n",
      "34 Train Loss 8.450174 Test MSE 12987.113290693902 Test RE 0.9543812055594161\n",
      "35 Train Loss 7.992002 Test MSE 12828.404442764326 Test RE 0.9485317784005947\n",
      "36 Train Loss 7.977587 Test MSE 12813.446648839194 Test RE 0.9479786277301602\n",
      "37 Train Loss 7.88407 Test MSE 12719.367452088554 Test RE 0.944492080424844\n",
      "38 Train Loss 10.7609825 Test MSE 12297.703604298797 Test RE 0.928704552691136\n",
      "39 Train Loss 10.723658 Test MSE 11979.32102015458 Test RE 0.9166038252938551\n",
      "40 Train Loss 13.252931 Test MSE 11727.707056868556 Test RE 0.9069265551896589\n",
      "41 Train Loss 15.144705 Test MSE 11287.706795524107 Test RE 0.8897508748106505\n",
      "42 Train Loss 15.841553 Test MSE 11034.30459681244 Test RE 0.8797070002907418\n",
      "43 Train Loss 19.171938 Test MSE 10527.919139610427 Test RE 0.8592842153518473\n",
      "44 Train Loss 20.07454 Test MSE 10229.147119466952 Test RE 0.8470036389404568\n",
      "45 Train Loss 19.711231 Test MSE 10023.231959526664 Test RE 0.8384351058986395\n",
      "46 Train Loss 18.45567 Test MSE 9862.557152421821 Test RE 0.8316877987163442\n",
      "47 Train Loss 19.400068 Test MSE 9741.545280495306 Test RE 0.8265697179858114\n",
      "48 Train Loss 18.659542 Test MSE 9476.588696339295 Test RE 0.8152514494665541\n",
      "49 Train Loss 19.171717 Test MSE 8926.247226993208 Test RE 0.7912250333606344\n",
      "50 Train Loss 19.055134 Test MSE 8716.426419578627 Test RE 0.7818704467960115\n",
      "51 Train Loss 19.758215 Test MSE 8295.73223326402 Test RE 0.7627688105902451\n",
      "52 Train Loss 19.015865 Test MSE 7973.486806962667 Test RE 0.7478073058084117\n",
      "53 Train Loss 18.368109 Test MSE 7693.505339854868 Test RE 0.7345607071814356\n",
      "54 Train Loss 17.66037 Test MSE 6975.446823134835 Test RE 0.6994417949202776\n",
      "55 Train Loss 16.613548 Test MSE 6494.634768759565 Test RE 0.6749054424943\n",
      "56 Train Loss 15.671807 Test MSE 5487.141052131879 Test RE 0.6203526273243096\n",
      "57 Train Loss 14.437607 Test MSE 5132.94821192366 Test RE 0.5999968979245294\n",
      "58 Train Loss 13.7097645 Test MSE 5046.067014068195 Test RE 0.59489739977005\n",
      "59 Train Loss 12.797324 Test MSE 4967.714534131908 Test RE 0.5902607149036243\n",
      "60 Train Loss 13.511885 Test MSE 3557.401464801076 Test RE 0.4994959871359689\n",
      "61 Train Loss 12.21837 Test MSE 2509.6039597507006 Test RE 0.41953496254242206\n",
      "62 Train Loss 11.274043 Test MSE 1841.9432056207158 Test RE 0.3594211929208832\n",
      "63 Train Loss 10.166043 Test MSE 1293.1871417034226 Test RE 0.30115928169165673\n",
      "64 Train Loss 8.903615 Test MSE 855.0922231045793 Test RE 0.24489055872004117\n",
      "65 Train Loss 7.6740894 Test MSE 618.3090855109642 Test RE 0.20824202475972944\n",
      "66 Train Loss 6.5238476 Test MSE 567.7906211021094 Test RE 0.19955364779124068\n",
      "67 Train Loss 5.7951818 Test MSE 441.05928093292823 Test RE 0.1758790237418798\n",
      "68 Train Loss 3.5880077 Test MSE 110.70499136478193 Test RE 0.08811486190911003\n",
      "69 Train Loss 2.7752903 Test MSE 49.182676252975824 Test RE 0.058731574297286926\n",
      "70 Train Loss 1.9903657 Test MSE 6.058869875013685 Test RE 0.020613957505056842\n",
      "71 Train Loss 1.7732804 Test MSE 0.9198944303698058 Test RE 0.008032200826068304\n",
      "72 Train Loss 1.5778055 Test MSE 14.335607623036868 Test RE 0.03170834804079848\n",
      "73 Train Loss 1.3975512 Test MSE 0.714314941990946 Test RE 0.00707799792745025\n",
      "74 Train Loss 1.0287012 Test MSE 14.432022243521162 Test RE 0.03181479714979013\n",
      "75 Train Loss 0.55346245 Test MSE 24.433141586374198 Test RE 0.04139569892192104\n",
      "76 Train Loss 0.40518636 Test MSE 4.72201708576294 Test RE 0.018198238013249886\n",
      "77 Train Loss 0.38198602 Test MSE 0.7148748440920514 Test RE 0.007080771360711137\n",
      "78 Train Loss 0.35899282 Test MSE 1.017833380395683 Test RE 0.00844897273425639\n",
      "79 Train Loss 0.3412365 Test MSE 1.2521530222819575 Test RE 0.009371179719145388\n",
      "80 Train Loss 0.32468656 Test MSE 0.38282978649070665 Test RE 0.005181654170883884\n",
      "81 Train Loss 0.22048771 Test MSE 2.080618253330955 Test RE 0.012079856240387628\n",
      "82 Train Loss 0.12351626 Test MSE 2.747105026320453 Test RE 0.013880438796826822\n",
      "83 Train Loss 0.084679395 Test MSE 0.3268503100352592 Test RE 0.004787844251588641\n",
      "84 Train Loss 0.07940448 Test MSE 0.25807394137657513 Test RE 0.004254393348750208\n",
      "85 Train Loss 0.07609581 Test MSE 0.10583782254423477 Test RE 0.0027244949791975287\n",
      "86 Train Loss 0.05565548 Test MSE 0.009779163346033862 Test RE 0.0008281641258828395\n",
      "87 Train Loss 0.050359163 Test MSE 0.036152809277628296 Test RE 0.001592342874385042\n",
      "88 Train Loss 0.049761318 Test MSE 0.007163951526343224 Test RE 0.0007088296621918144\n",
      "89 Train Loss 0.049736876 Test MSE 0.006374574431824628 Test RE 0.0006686381650848467\n",
      "90 Train Loss 0.049726315 Test MSE 0.0063798347130258615 Test RE 0.0006689139873774575\n",
      "91 Train Loss 0.04972635 Test MSE 0.0063798347130258615 Test RE 0.0006689139873774575\n",
      "92 Train Loss 0.049721 Test MSE 0.006377146449212748 Test RE 0.0006687730427593472\n",
      "93 Train Loss 0.049682617 Test MSE 0.005495192918905666 Test RE 0.0006208076152297627\n",
      "94 Train Loss 0.049675558 Test MSE 0.006293749889820004 Test RE 0.0006643857422513576\n",
      "95 Train Loss 0.049634866 Test MSE 0.007426856829351533 Test RE 0.0007217189185898462\n",
      "96 Train Loss 0.049584344 Test MSE 0.008813341409155483 Test RE 0.000786205107378681\n",
      "97 Train Loss 0.04719435 Test MSE 0.06067843103047697 Test RE 0.0020629216564007785\n",
      "98 Train Loss 0.04195168 Test MSE 0.00306354943389492 Test RE 0.0004635301737476758\n",
      "99 Train Loss 0.040958278 Test MSE 0.003924582542556063 Test RE 0.0005246410851321207\n",
      "100 Train Loss 0.040193908 Test MSE 0.005663073494776242 Test RE 0.0006302192473371212\n",
      "101 Train Loss 0.038566638 Test MSE 0.0018792995081677926 Test RE 0.0003630475947265158\n",
      "102 Train Loss 0.038005363 Test MSE 0.00022022048602773819 Test RE 0.00012427804767156748\n",
      "103 Train Loss 0.03788195 Test MSE 1.955246341596692e-05 Test RE 3.703107372745037e-05\n",
      "104 Train Loss 0.037868477 Test MSE 1.7791870393634865e-05 Test RE 3.532452794203959e-05\n",
      "105 Train Loss 0.037840888 Test MSE 3.314090215608873e-07 Test RE 4.821117619980319e-06\n",
      "106 Train Loss 0.037816785 Test MSE 1.0429201685070769e-05 Test RE 2.7045256010005322e-05\n",
      "107 Train Loss 0.03781601 Test MSE 2.5646882478119183e-05 Test RE 4.241142401024941e-05\n",
      "108 Train Loss 0.037809733 Test MSE 4.1770831442172945e-05 Test RE 5.41255252261045e-05\n",
      "109 Train Loss 0.037803665 Test MSE 7.222308984160955e-05 Test RE 7.117108652283887e-05\n",
      "110 Train Loss 0.037772685 Test MSE 0.00014653398946600738 Test RE 0.00010137590647744215\n",
      "111 Train Loss 0.037772685 Test MSE 0.00014653398946600738 Test RE 0.00010137590647744215\n",
      "112 Train Loss 0.037771113 Test MSE 0.00014654238634610837 Test RE 0.00010137881102245776\n",
      "113 Train Loss 0.037771113 Test MSE 0.00014654238634610837 Test RE 0.00010137881102245776\n",
      "114 Train Loss 0.037771117 Test MSE 0.00014654238634610837 Test RE 0.00010137881102245776\n",
      "115 Train Loss 0.037771117 Test MSE 0.00014654238634610837 Test RE 0.00010137881102245776\n",
      "116 Train Loss 0.037771117 Test MSE 0.00014654238634610837 Test RE 0.00010137881102245776\n",
      "117 Train Loss 0.037771117 Test MSE 0.00014654238634610837 Test RE 0.00010137881102245776\n",
      "118 Train Loss 0.037771117 Test MSE 0.00014654238634610837 Test RE 0.00010137881102245776\n",
      "119 Train Loss 0.037766904 Test MSE 0.0003665008049556503 Test RE 0.00016032565769236767\n",
      "120 Train Loss 0.037763394 Test MSE 0.0003745419322117323 Test RE 0.00016207490923606684\n",
      "121 Train Loss 0.037748102 Test MSE 0.0019497468547915143 Test RE 0.00036978958732837927\n",
      "122 Train Loss 0.037734844 Test MSE 0.0019436750774335999 Test RE 0.0003692133507794882\n",
      "123 Train Loss 0.037734844 Test MSE 0.0019436750774335999 Test RE 0.0003692133507794882\n",
      "124 Train Loss 0.03773484 Test MSE 0.0019436750774335999 Test RE 0.0003692133507794882\n",
      "125 Train Loss 0.03773484 Test MSE 0.0019436750774335999 Test RE 0.0003692133507794882\n",
      "126 Train Loss 0.03773484 Test MSE 0.0019436750774335999 Test RE 0.0003692133507794882\n",
      "127 Train Loss 0.03773484 Test MSE 0.0019436750774335999 Test RE 0.0003692133507794882\n",
      "128 Train Loss 0.03773382 Test MSE 0.0017779153839835374 Test RE 0.0003531190176656883\n",
      "129 Train Loss 0.037733514 Test MSE 0.0015233134051006252 Test RE 0.00032685881034615673\n",
      "130 Train Loss 0.037725676 Test MSE 0.0014064030159514698 Test RE 0.00031406566370686484\n",
      "131 Train Loss 0.037723754 Test MSE 0.001375274246093798 Test RE 0.00031057051262803044\n",
      "132 Train Loss 0.037706546 Test MSE 0.0013602278092983153 Test RE 0.0003088669137286789\n",
      "133 Train Loss 0.037698403 Test MSE 0.0013447659533037325 Test RE 0.0003071064348063318\n",
      "134 Train Loss 0.037695594 Test MSE 0.0013445329234870905 Test RE 0.00030707982495032076\n",
      "135 Train Loss 0.0376956 Test MSE 0.0013445329234870905 Test RE 0.00030707982495032076\n",
      "136 Train Loss 0.037689243 Test MSE 0.0013376542684759851 Test RE 0.0003062933046988613\n",
      "137 Train Loss 0.037687562 Test MSE 0.0013738422192201213 Test RE 0.0003104087771936528\n",
      "138 Train Loss 0.037678167 Test MSE 0.001386354866412597 Test RE 0.0003118191399611016\n",
      "139 Train Loss 0.037643526 Test MSE 0.001348309612502153 Test RE 0.00030751080425721827\n",
      "140 Train Loss 0.036067106 Test MSE 0.00043726824268838706 Test RE 0.0001751215257308449\n",
      "141 Train Loss 0.031646654 Test MSE 0.010175825154915838 Test RE 0.0008447931462967392\n",
      "142 Train Loss 0.029741634 Test MSE 0.07373196548381858 Test RE 0.0022740157551801826\n",
      "143 Train Loss 0.02730659 Test MSE 0.015461011873755193 Test RE 0.0010413207454526337\n",
      "144 Train Loss 0.020115592 Test MSE 0.015875961442272612 Test RE 0.0010552019412002531\n",
      "145 Train Loss 0.016910354 Test MSE 4.437376036471436e-05 Test RE 5.5786444201918376e-05\n",
      "146 Train Loss 0.015231537 Test MSE 0.0010760070283143586 Test RE 0.0002747091453650033\n",
      "147 Train Loss 0.014873083 Test MSE 0.0025867896608782613 Test RE 0.0004259377397585098\n",
      "148 Train Loss 0.014873544 Test MSE 0.0025867896608782613 Test RE 0.0004259377397585098\n",
      "149 Train Loss 0.01486755 Test MSE 0.002404403125931995 Test RE 0.00041064752127602716\n",
      "150 Train Loss 0.014178301 Test MSE 0.0010506832110245216 Test RE 0.00027145725933542896\n",
      "151 Train Loss 0.01275799 Test MSE 2.7427955536988855e-05 Test RE 4.385935919355256e-05\n",
      "152 Train Loss 0.012552121 Test MSE 5.02570500226648e-05 Test RE 5.936959142827418e-05\n",
      "153 Train Loss 0.012496937 Test MSE 1.7029598802195236e-05 Test RE 3.455952558282649e-05\n",
      "154 Train Loss 0.012496933 Test MSE 1.7029598802195236e-05 Test RE 3.455952558282649e-05\n",
      "155 Train Loss 0.01249693 Test MSE 1.7029598802195236e-05 Test RE 3.455952558282649e-05\n",
      "156 Train Loss 0.012496926 Test MSE 1.7029598802195236e-05 Test RE 3.455952558282649e-05\n",
      "157 Train Loss 0.012496923 Test MSE 1.7029598802195236e-05 Test RE 3.455952558282649e-05\n",
      "158 Train Loss 0.012496891 Test MSE 1.7029598802195236e-05 Test RE 3.455952558282649e-05\n",
      "159 Train Loss 0.012495524 Test MSE 1.705215338352878e-05 Test RE 3.458240391717254e-05\n",
      "160 Train Loss 0.012492056 Test MSE 1.7092634771209196e-05 Test RE 3.46234284668381e-05\n",
      "161 Train Loss 0.012492051 Test MSE 1.7092634771209196e-05 Test RE 3.46234284668381e-05\n",
      "162 Train Loss 0.0124920495 Test MSE 1.7092634771209196e-05 Test RE 3.46234284668381e-05\n",
      "163 Train Loss 0.012492049 Test MSE 1.7092634771209196e-05 Test RE 3.46234284668381e-05\n",
      "164 Train Loss 0.012492049 Test MSE 1.7092634771209196e-05 Test RE 3.46234284668381e-05\n",
      "165 Train Loss 0.012492047 Test MSE 1.7092634771209196e-05 Test RE 3.46234284668381e-05\n",
      "166 Train Loss 0.012492047 Test MSE 1.7092634771209196e-05 Test RE 3.46234284668381e-05\n",
      "167 Train Loss 0.012492046 Test MSE 1.7092634771209196e-05 Test RE 3.46234284668381e-05\n",
      "168 Train Loss 0.012492046 Test MSE 1.7092634771209196e-05 Test RE 3.46234284668381e-05\n",
      "169 Train Loss 0.012492045 Test MSE 1.7092634771209196e-05 Test RE 3.46234284668381e-05\n",
      "170 Train Loss 0.012492045 Test MSE 1.7092634771209196e-05 Test RE 3.46234284668381e-05\n",
      "171 Train Loss 0.012492043 Test MSE 1.7092634771209196e-05 Test RE 3.46234284668381e-05\n",
      "172 Train Loss 0.012492043 Test MSE 1.7092634771209196e-05 Test RE 3.46234284668381e-05\n",
      "173 Train Loss 0.012492042 Test MSE 1.7092634771209196e-05 Test RE 3.46234284668381e-05\n",
      "174 Train Loss 0.012492043 Test MSE 1.7092634771209196e-05 Test RE 3.46234284668381e-05\n",
      "175 Train Loss 0.012492043 Test MSE 1.7092634771209196e-05 Test RE 3.46234284668381e-05\n",
      "176 Train Loss 0.012492042 Test MSE 1.7092634771209196e-05 Test RE 3.46234284668381e-05\n",
      "177 Train Loss 0.012492042 Test MSE 1.7092634771209196e-05 Test RE 3.46234284668381e-05\n",
      "178 Train Loss 0.012492041 Test MSE 1.7092634771209196e-05 Test RE 3.46234284668381e-05\n",
      "179 Train Loss 0.012492043 Test MSE 1.7092634771209196e-05 Test RE 3.46234284668381e-05\n",
      "180 Train Loss 0.012492042 Test MSE 1.7092634771209196e-05 Test RE 3.46234284668381e-05\n",
      "181 Train Loss 0.01249204 Test MSE 1.7092634771209196e-05 Test RE 3.46234284668381e-05\n",
      "182 Train Loss 0.01249204 Test MSE 1.7092634771209196e-05 Test RE 3.46234284668381e-05\n",
      "183 Train Loss 0.012492041 Test MSE 1.7092634771209196e-05 Test RE 3.46234284668381e-05\n",
      "184 Train Loss 0.012492041 Test MSE 1.7092634771209196e-05 Test RE 3.46234284668381e-05\n",
      "185 Train Loss 0.012492041 Test MSE 1.7092634771209196e-05 Test RE 3.46234284668381e-05\n",
      "186 Train Loss 0.01249204 Test MSE 1.7092634771209196e-05 Test RE 3.46234284668381e-05\n",
      "187 Train Loss 0.01249204 Test MSE 1.7092634771209196e-05 Test RE 3.46234284668381e-05\n",
      "188 Train Loss 0.01249204 Test MSE 1.7092634771209196e-05 Test RE 3.46234284668381e-05\n",
      "189 Train Loss 0.01249204 Test MSE 1.7092634771209196e-05 Test RE 3.46234284668381e-05\n",
      "190 Train Loss 0.012492039 Test MSE 1.7092634771209196e-05 Test RE 3.46234284668381e-05\n",
      "191 Train Loss 0.012492039 Test MSE 1.7092634771209196e-05 Test RE 3.46234284668381e-05\n",
      "192 Train Loss 0.01249204 Test MSE 1.7092634771209196e-05 Test RE 3.46234284668381e-05\n",
      "193 Train Loss 0.01249204 Test MSE 1.7092634771209196e-05 Test RE 3.46234284668381e-05\n",
      "194 Train Loss 0.012492039 Test MSE 1.7092634771209196e-05 Test RE 3.46234284668381e-05\n",
      "195 Train Loss 0.012492039 Test MSE 1.7092634771209196e-05 Test RE 3.46234284668381e-05\n",
      "196 Train Loss 0.01249204 Test MSE 1.7092634771209196e-05 Test RE 3.46234284668381e-05\n",
      "197 Train Loss 0.01249204 Test MSE 1.7092634771209196e-05 Test RE 3.46234284668381e-05\n",
      "198 Train Loss 0.01249204 Test MSE 1.7092634771209196e-05 Test RE 3.46234284668381e-05\n",
      "199 Train Loss 0.01249204 Test MSE 1.7092634771209196e-05 Test RE 3.46234284668381e-05\n",
      "Training time: 91.16\n",
      "Training time: 91.16\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 14.256481 Test MSE 14251.095914148587 Test RE 0.9997460462104555\n",
      "1 Train Loss 12.868033 Test MSE 14250.406190523523 Test RE 0.9997218530952495\n",
      "2 Train Loss 11.481113 Test MSE 14253.21578924956 Test RE 0.9998204004199681\n",
      "3 Train Loss 10.459161 Test MSE 14240.769459499978 Test RE 0.9993837687011106\n",
      "4 Train Loss 9.803958 Test MSE 14237.082686345108 Test RE 0.9992543957848481\n",
      "5 Train Loss 9.533794 Test MSE 14235.795508807101 Test RE 0.9992092233673499\n",
      "6 Train Loss 10.226025 Test MSE 14222.787189420562 Test RE 0.9987525926373891\n",
      "7 Train Loss 9.635748 Test MSE 14195.027143530258 Test RE 0.9977774335748096\n",
      "8 Train Loss 9.035556 Test MSE 14190.978300227252 Test RE 0.9976351255485211\n",
      "9 Train Loss 8.446571 Test MSE 14189.403876513707 Test RE 0.9975797825012058\n",
      "10 Train Loss 8.387789 Test MSE 14184.71389691623 Test RE 0.9974149054022601\n",
      "11 Train Loss 8.8243065 Test MSE 14166.314571907711 Test RE 0.9967678103627007\n",
      "12 Train Loss 8.717021 Test MSE 14157.15533809019 Test RE 0.9964455280373022\n",
      "13 Train Loss 8.072837 Test MSE 14113.311390528794 Test RE 0.9949013624955355\n",
      "14 Train Loss 7.7159657 Test MSE 14082.442357965107 Test RE 0.9938127287426398\n",
      "15 Train Loss 7.5105395 Test MSE 14045.8193781563 Test RE 0.9925196264306462\n",
      "16 Train Loss 7.1920776 Test MSE 13977.612155229004 Test RE 0.990106830466618\n",
      "17 Train Loss 6.8561788 Test MSE 13937.068431481044 Test RE 0.9886698264465004\n",
      "18 Train Loss 8.513964 Test MSE 13839.487255087573 Test RE 0.9852026328696466\n",
      "19 Train Loss 10.150652 Test MSE 13715.801295650657 Test RE 0.9807902866718016\n",
      "20 Train Loss 9.535077 Test MSE 13665.939938900054 Test RE 0.9790059191630407\n",
      "21 Train Loss 9.499529 Test MSE 13641.32659863398 Test RE 0.9781238918151139\n",
      "22 Train Loss 9.602339 Test MSE 13628.555357441777 Test RE 0.9776659165152342\n",
      "23 Train Loss 9.469034 Test MSE 13544.737431127454 Test RE 0.9746548741897371\n",
      "24 Train Loss 10.162834 Test MSE 13379.954569285195 Test RE 0.9687079929114063\n",
      "25 Train Loss 10.707827 Test MSE 13236.223453353894 Test RE 0.9634908816323916\n",
      "26 Train Loss nan Test MSE nan Test RE nan\n",
      "27 Train Loss nan Test MSE nan Test RE nan\n",
      "28 Train Loss nan Test MSE nan Test RE nan\n",
      "29 Train Loss nan Test MSE nan Test RE nan\n",
      "30 Train Loss nan Test MSE nan Test RE nan\n",
      "31 Train Loss nan Test MSE nan Test RE nan\n",
      "32 Train Loss nan Test MSE nan Test RE nan\n",
      "33 Train Loss nan Test MSE nan Test RE nan\n",
      "34 Train Loss nan Test MSE nan Test RE nan\n",
      "35 Train Loss nan Test MSE nan Test RE nan\n",
      "36 Train Loss nan Test MSE nan Test RE nan\n",
      "37 Train Loss nan Test MSE nan Test RE nan\n",
      "38 Train Loss nan Test MSE nan Test RE nan\n",
      "39 Train Loss nan Test MSE nan Test RE nan\n",
      "40 Train Loss nan Test MSE nan Test RE nan\n",
      "41 Train Loss nan Test MSE nan Test RE nan\n",
      "42 Train Loss nan Test MSE nan Test RE nan\n",
      "43 Train Loss nan Test MSE nan Test RE nan\n",
      "44 Train Loss nan Test MSE nan Test RE nan\n",
      "45 Train Loss nan Test MSE nan Test RE nan\n",
      "46 Train Loss nan Test MSE nan Test RE nan\n",
      "47 Train Loss nan Test MSE nan Test RE nan\n",
      "48 Train Loss nan Test MSE nan Test RE nan\n",
      "49 Train Loss nan Test MSE nan Test RE nan\n",
      "50 Train Loss nan Test MSE nan Test RE nan\n",
      "51 Train Loss nan Test MSE nan Test RE nan\n",
      "52 Train Loss nan Test MSE nan Test RE nan\n",
      "53 Train Loss nan Test MSE nan Test RE nan\n",
      "54 Train Loss nan Test MSE nan Test RE nan\n",
      "55 Train Loss nan Test MSE nan Test RE nan\n",
      "56 Train Loss nan Test MSE nan Test RE nan\n",
      "57 Train Loss nan Test MSE nan Test RE nan\n",
      "58 Train Loss nan Test MSE nan Test RE nan\n",
      "59 Train Loss nan Test MSE nan Test RE nan\n",
      "60 Train Loss nan Test MSE nan Test RE nan\n",
      "61 Train Loss nan Test MSE nan Test RE nan\n",
      "62 Train Loss nan Test MSE nan Test RE nan\n",
      "63 Train Loss nan Test MSE nan Test RE nan\n",
      "64 Train Loss nan Test MSE nan Test RE nan\n",
      "65 Train Loss nan Test MSE nan Test RE nan\n",
      "66 Train Loss nan Test MSE nan Test RE nan\n",
      "67 Train Loss nan Test MSE nan Test RE nan\n",
      "68 Train Loss nan Test MSE nan Test RE nan\n",
      "69 Train Loss nan Test MSE nan Test RE nan\n",
      "70 Train Loss nan Test MSE nan Test RE nan\n",
      "71 Train Loss nan Test MSE nan Test RE nan\n",
      "72 Train Loss nan Test MSE nan Test RE nan\n",
      "73 Train Loss nan Test MSE nan Test RE nan\n",
      "74 Train Loss nan Test MSE nan Test RE nan\n",
      "75 Train Loss nan Test MSE nan Test RE nan\n",
      "76 Train Loss nan Test MSE nan Test RE nan\n",
      "77 Train Loss nan Test MSE nan Test RE nan\n",
      "78 Train Loss nan Test MSE nan Test RE nan\n",
      "79 Train Loss nan Test MSE nan Test RE nan\n",
      "80 Train Loss nan Test MSE nan Test RE nan\n",
      "81 Train Loss nan Test MSE nan Test RE nan\n",
      "82 Train Loss nan Test MSE nan Test RE nan\n",
      "83 Train Loss nan Test MSE nan Test RE nan\n",
      "84 Train Loss nan Test MSE nan Test RE nan\n",
      "85 Train Loss nan Test MSE nan Test RE nan\n",
      "86 Train Loss nan Test MSE nan Test RE nan\n",
      "87 Train Loss nan Test MSE nan Test RE nan\n",
      "88 Train Loss nan Test MSE nan Test RE nan\n",
      "89 Train Loss nan Test MSE nan Test RE nan\n",
      "90 Train Loss nan Test MSE nan Test RE nan\n",
      "91 Train Loss nan Test MSE nan Test RE nan\n",
      "92 Train Loss nan Test MSE nan Test RE nan\n",
      "93 Train Loss nan Test MSE nan Test RE nan\n",
      "94 Train Loss nan Test MSE nan Test RE nan\n",
      "95 Train Loss nan Test MSE nan Test RE nan\n",
      "96 Train Loss nan Test MSE nan Test RE nan\n",
      "97 Train Loss nan Test MSE nan Test RE nan\n",
      "98 Train Loss nan Test MSE nan Test RE nan\n",
      "99 Train Loss nan Test MSE nan Test RE nan\n",
      "100 Train Loss nan Test MSE nan Test RE nan\n",
      "101 Train Loss nan Test MSE nan Test RE nan\n",
      "102 Train Loss nan Test MSE nan Test RE nan\n",
      "103 Train Loss nan Test MSE nan Test RE nan\n",
      "104 Train Loss nan Test MSE nan Test RE nan\n",
      "105 Train Loss nan Test MSE nan Test RE nan\n",
      "106 Train Loss nan Test MSE nan Test RE nan\n",
      "107 Train Loss nan Test MSE nan Test RE nan\n",
      "108 Train Loss nan Test MSE nan Test RE nan\n",
      "109 Train Loss nan Test MSE nan Test RE nan\n",
      "110 Train Loss nan Test MSE nan Test RE nan\n",
      "111 Train Loss nan Test MSE nan Test RE nan\n",
      "112 Train Loss nan Test MSE nan Test RE nan\n",
      "113 Train Loss nan Test MSE nan Test RE nan\n",
      "114 Train Loss nan Test MSE nan Test RE nan\n",
      "115 Train Loss nan Test MSE nan Test RE nan\n",
      "116 Train Loss nan Test MSE nan Test RE nan\n",
      "117 Train Loss nan Test MSE nan Test RE nan\n",
      "118 Train Loss nan Test MSE nan Test RE nan\n",
      "119 Train Loss nan Test MSE nan Test RE nan\n",
      "120 Train Loss nan Test MSE nan Test RE nan\n",
      "121 Train Loss nan Test MSE nan Test RE nan\n",
      "122 Train Loss nan Test MSE nan Test RE nan\n",
      "123 Train Loss nan Test MSE nan Test RE nan\n",
      "124 Train Loss nan Test MSE nan Test RE nan\n",
      "125 Train Loss nan Test MSE nan Test RE nan\n",
      "126 Train Loss nan Test MSE nan Test RE nan\n",
      "127 Train Loss nan Test MSE nan Test RE nan\n",
      "128 Train Loss nan Test MSE nan Test RE nan\n",
      "129 Train Loss nan Test MSE nan Test RE nan\n",
      "130 Train Loss nan Test MSE nan Test RE nan\n",
      "131 Train Loss nan Test MSE nan Test RE nan\n",
      "132 Train Loss nan Test MSE nan Test RE nan\n",
      "133 Train Loss nan Test MSE nan Test RE nan\n",
      "134 Train Loss nan Test MSE nan Test RE nan\n",
      "135 Train Loss nan Test MSE nan Test RE nan\n",
      "136 Train Loss nan Test MSE nan Test RE nan\n",
      "137 Train Loss nan Test MSE nan Test RE nan\n",
      "138 Train Loss nan Test MSE nan Test RE nan\n",
      "139 Train Loss nan Test MSE nan Test RE nan\n",
      "140 Train Loss nan Test MSE nan Test RE nan\n",
      "141 Train Loss nan Test MSE nan Test RE nan\n",
      "142 Train Loss nan Test MSE nan Test RE nan\n",
      "143 Train Loss nan Test MSE nan Test RE nan\n",
      "144 Train Loss nan Test MSE nan Test RE nan\n",
      "145 Train Loss nan Test MSE nan Test RE nan\n",
      "146 Train Loss nan Test MSE nan Test RE nan\n",
      "147 Train Loss nan Test MSE nan Test RE nan\n",
      "148 Train Loss nan Test MSE nan Test RE nan\n",
      "149 Train Loss nan Test MSE nan Test RE nan\n",
      "150 Train Loss nan Test MSE nan Test RE nan\n",
      "151 Train Loss nan Test MSE nan Test RE nan\n",
      "152 Train Loss nan Test MSE nan Test RE nan\n",
      "153 Train Loss nan Test MSE nan Test RE nan\n",
      "154 Train Loss nan Test MSE nan Test RE nan\n",
      "155 Train Loss nan Test MSE nan Test RE nan\n",
      "156 Train Loss nan Test MSE nan Test RE nan\n",
      "157 Train Loss nan Test MSE nan Test RE nan\n",
      "158 Train Loss nan Test MSE nan Test RE nan\n",
      "159 Train Loss nan Test MSE nan Test RE nan\n",
      "160 Train Loss nan Test MSE nan Test RE nan\n",
      "161 Train Loss nan Test MSE nan Test RE nan\n",
      "162 Train Loss nan Test MSE nan Test RE nan\n",
      "163 Train Loss nan Test MSE nan Test RE nan\n",
      "164 Train Loss nan Test MSE nan Test RE nan\n",
      "165 Train Loss nan Test MSE nan Test RE nan\n",
      "166 Train Loss nan Test MSE nan Test RE nan\n",
      "167 Train Loss nan Test MSE nan Test RE nan\n",
      "168 Train Loss nan Test MSE nan Test RE nan\n",
      "169 Train Loss nan Test MSE nan Test RE nan\n",
      "170 Train Loss nan Test MSE nan Test RE nan\n",
      "171 Train Loss nan Test MSE nan Test RE nan\n",
      "172 Train Loss nan Test MSE nan Test RE nan\n",
      "173 Train Loss nan Test MSE nan Test RE nan\n",
      "174 Train Loss nan Test MSE nan Test RE nan\n",
      "175 Train Loss nan Test MSE nan Test RE nan\n",
      "176 Train Loss nan Test MSE nan Test RE nan\n",
      "177 Train Loss nan Test MSE nan Test RE nan\n",
      "178 Train Loss nan Test MSE nan Test RE nan\n",
      "179 Train Loss nan Test MSE nan Test RE nan\n",
      "180 Train Loss nan Test MSE nan Test RE nan\n",
      "181 Train Loss nan Test MSE nan Test RE nan\n",
      "182 Train Loss nan Test MSE nan Test RE nan\n",
      "183 Train Loss nan Test MSE nan Test RE nan\n",
      "184 Train Loss nan Test MSE nan Test RE nan\n",
      "185 Train Loss nan Test MSE nan Test RE nan\n",
      "186 Train Loss nan Test MSE nan Test RE nan\n",
      "187 Train Loss nan Test MSE nan Test RE nan\n",
      "188 Train Loss nan Test MSE nan Test RE nan\n",
      "189 Train Loss nan Test MSE nan Test RE nan\n",
      "190 Train Loss nan Test MSE nan Test RE nan\n",
      "191 Train Loss nan Test MSE nan Test RE nan\n",
      "192 Train Loss nan Test MSE nan Test RE nan\n",
      "193 Train Loss nan Test MSE nan Test RE nan\n",
      "194 Train Loss nan Test MSE nan Test RE nan\n",
      "195 Train Loss nan Test MSE nan Test RE nan\n",
      "196 Train Loss nan Test MSE nan Test RE nan\n",
      "197 Train Loss nan Test MSE nan Test RE nan\n",
      "198 Train Loss nan Test MSE nan Test RE nan\n",
      "199 Train Loss nan Test MSE nan Test RE nan\n",
      "Training time: 219.68\n",
      "Training time: 219.68\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 14.856691 Test MSE 14208.769853745624 Test RE 0.9982603086263032\n",
      "1 Train Loss 12.765678 Test MSE 14209.227879900227 Test RE 0.998276398184037\n",
      "2 Train Loss 11.810183 Test MSE 14198.667828831229 Test RE 0.9979053784075606\n",
      "3 Train Loss 10.785902 Test MSE 14168.688546890357 Test RE 0.9968513254754725\n",
      "4 Train Loss 10.137968 Test MSE 14149.261337531118 Test RE 0.9961676813166322\n",
      "5 Train Loss 10.123982 Test MSE 14126.227393071098 Test RE 0.9953565076247047\n",
      "6 Train Loss 9.961647 Test MSE 14066.740912586858 Test RE 0.9932585404892273\n",
      "7 Train Loss 9.942232 Test MSE 13949.620229969398 Test RE 0.9891149269250183\n",
      "8 Train Loss 10.393783 Test MSE 13800.207112421313 Test RE 0.9838035059461215\n",
      "9 Train Loss 10.112382 Test MSE 13680.157988160026 Test RE 0.9795150658390606\n",
      "10 Train Loss 10.343956 Test MSE 13557.348826694524 Test RE 0.9751085152575075\n",
      "11 Train Loss 12.834196 Test MSE 13361.135260800706 Test RE 0.968026494727899\n",
      "12 Train Loss 13.126411 Test MSE 13254.85888929922 Test RE 0.9641688981877502\n",
      "13 Train Loss 12.166802 Test MSE 13203.1513022554 Test RE 0.9622864351599423\n",
      "14 Train Loss 12.151022 Test MSE 13072.82844358416 Test RE 0.9575254915244316\n",
      "15 Train Loss 13.16989 Test MSE 12400.392807195727 Test RE 0.9325739608792404\n",
      "16 Train Loss 13.492892 Test MSE 12182.168380904832 Test RE 0.9243317325735064\n",
      "17 Train Loss 13.989758 Test MSE 11800.256990563466 Test RE 0.9097274443836246\n",
      "18 Train Loss 14.270613 Test MSE 11448.810844666948 Test RE 0.8960778739214644\n",
      "19 Train Loss 13.711035 Test MSE 10898.208441948485 Test RE 0.8742650532210295\n",
      "20 Train Loss 13.589335 Test MSE 10587.04605615284 Test RE 0.8616937936211237\n",
      "21 Train Loss 18.316593 Test MSE 10003.622069195535 Test RE 0.8376145287536689\n",
      "22 Train Loss 17.81543 Test MSE 9268.193240485478 Test RE 0.8062377032859743\n",
      "23 Train Loss 16.559961 Test MSE 9075.296177862887 Test RE 0.7978035554925829\n",
      "24 Train Loss 16.220436 Test MSE 8737.172524977383 Test RE 0.7828003649505346\n",
      "25 Train Loss 15.857852 Test MSE 7900.328527521985 Test RE 0.7443687621709426\n",
      "26 Train Loss 15.207846 Test MSE 7513.767434910575 Test RE 0.7259294877660172\n",
      "27 Train Loss 18.095482 Test MSE 7103.564926047626 Test RE 0.7058358956711999\n",
      "28 Train Loss 18.598986 Test MSE 6212.763789915899 Test RE 0.6600973431535039\n",
      "29 Train Loss 18.382729 Test MSE 5123.530906678047 Test RE 0.5994462447983518\n",
      "30 Train Loss 17.636837 Test MSE 4768.379315787291 Test RE 0.578297029461009\n",
      "31 Train Loss 17.434904 Test MSE 4440.175138521486 Test RE 0.5580403650552329\n",
      "32 Train Loss 15.565632 Test MSE 4413.832245299304 Test RE 0.5563825175576615\n",
      "33 Train Loss 15.966782 Test MSE 4198.349874375877 Test RE 0.5426313459453961\n",
      "34 Train Loss 14.540689 Test MSE 3770.9206624167878 Test RE 0.5142677156997306\n",
      "35 Train Loss 13.378637 Test MSE 3625.198241764924 Test RE 0.5042332078455174\n",
      "36 Train Loss 12.527319 Test MSE 3381.8486680897513 Test RE 0.4870153487532152\n",
      "37 Train Loss 10.460581 Test MSE 2540.533514459221 Test RE 0.4221123200817218\n",
      "38 Train Loss 8.6497 Test MSE 1931.1580711117297 Test RE 0.36802258841223495\n",
      "39 Train Loss 6.932473 Test MSE 1110.3849179685394 Test RE 0.27906305280711385\n",
      "40 Train Loss 5.231922 Test MSE 673.0879616257258 Test RE 0.21727085596694584\n",
      "41 Train Loss 3.9735012 Test MSE 560.1522903474093 Test RE 0.19820683275571255\n",
      "42 Train Loss 2.7278173 Test MSE 263.452403981103 Test RE 0.13593041620503207\n",
      "43 Train Loss 2.5233443 Test MSE 251.47895693464892 Test RE 0.13280560020116036\n",
      "44 Train Loss 2.2252345 Test MSE 203.77363795292368 Test RE 0.11954724254900792\n",
      "45 Train Loss 2.1347756 Test MSE 195.52816734808948 Test RE 0.11710359521446087\n",
      "46 Train Loss 1.7777197 Test MSE 187.7888819671549 Test RE 0.11476263252294759\n",
      "47 Train Loss 1.5989301 Test MSE 137.78606790501928 Test RE 0.09830332713338387\n",
      "48 Train Loss 1.5517135 Test MSE 121.09850115397556 Test RE 0.09215840456500177\n",
      "49 Train Loss 1.1766652 Test MSE 83.12420419677186 Test RE 0.0763535640597359\n",
      "50 Train Loss 0.91691864 Test MSE 43.05777811309005 Test RE 0.05495299557107433\n",
      "51 Train Loss 0.7096283 Test MSE 16.63061666350011 Test RE 0.034152282982614274\n",
      "52 Train Loss 0.518435 Test MSE 9.760753315253254 Test RE 0.02616418626315306\n",
      "53 Train Loss 0.41650102 Test MSE 10.240725168661827 Test RE 0.026799760894260293\n",
      "54 Train Loss 0.37302998 Test MSE 8.00888884609224 Test RE 0.02370018286071081\n",
      "55 Train Loss 0.25606495 Test MSE 1.7164317541143541 Test RE 0.010971824038595254\n",
      "56 Train Loss 0.20637864 Test MSE 0.1256075827169597 Test RE 0.002968065580844829\n",
      "57 Train Loss 0.19700602 Test MSE 0.07720017112117362 Test RE 0.0023268837958810174\n",
      "58 Train Loss 0.19325338 Test MSE 0.2095045737778664 Test RE 0.0038332074017785305\n",
      "59 Train Loss 0.18973397 Test MSE 0.058749299921074 Test RE 0.002029863861105606\n",
      "60 Train Loss 0.18309127 Test MSE 0.008863569857981075 Test RE 0.0007884422701293326\n",
      "61 Train Loss 0.16619916 Test MSE 0.009686416544264784 Test RE 0.00082422756413682\n",
      "62 Train Loss 0.15122926 Test MSE 0.004413020397353613 Test RE 0.0005563313467438191\n",
      "63 Train Loss 0.14307286 Test MSE 0.13295719499260747 Test RE 0.003053665664197803\n",
      "64 Train Loss 0.13793491 Test MSE 0.24771988743355636 Test RE 0.0041681755403950695\n",
      "65 Train Loss 0.13592578 Test MSE 0.12734964058916745 Test RE 0.0029885768331295113\n",
      "66 Train Loss 0.13354628 Test MSE 0.18110826978711686 Test RE 0.003563975456128605\n",
      "67 Train Loss 0.11954707 Test MSE 1.1056808826543731 Test RE 0.008806036152328792\n",
      "68 Train Loss 0.10822004 Test MSE 0.3135241007009466 Test RE 0.00468922454059392\n",
      "69 Train Loss 0.10134093 Test MSE 0.012540296696832438 Test RE 0.0009378199554318291\n",
      "70 Train Loss 0.097661816 Test MSE 0.004956393273893628 Test RE 0.0005895877387535381\n",
      "71 Train Loss 0.08656707 Test MSE 0.10592100371829626 Test RE 0.002725565400786565\n",
      "72 Train Loss 0.07756299 Test MSE 0.051248808595183395 Test RE 0.0018958651812414997\n",
      "73 Train Loss 0.074214116 Test MSE 8.258997477908406e-05 Test RE 7.610781090904766e-05\n",
      "74 Train Loss 0.072328836 Test MSE 0.012404276619121997 Test RE 0.0009327199908628627\n",
      "75 Train Loss 0.06789707 Test MSE 3.1043615772044155e-05 Test RE 4.666074982596684e-05\n",
      "76 Train Loss 0.062129892 Test MSE 0.006455104203541384 Test RE 0.0006728483510940028\n",
      "77 Train Loss 0.05769931 Test MSE 0.0006347404476659318 Test RE 0.00021099086404917166\n",
      "78 Train Loss 0.05467225 Test MSE 5.686618292707066e-05 Test RE 6.315279884123622e-05\n",
      "79 Train Loss 0.05347307 Test MSE 0.0247004690271981 Test RE 0.001316188717735401\n",
      "80 Train Loss 0.05122701 Test MSE 0.03510714577536936 Test RE 0.0015691458951464805\n",
      "81 Train Loss 0.04728152 Test MSE 0.015084277413795723 Test RE 0.0010285557092707064\n",
      "82 Train Loss 0.044999994 Test MSE 0.007617211491733662 Test RE 0.0007309094395075547\n",
      "83 Train Loss 0.0447117 Test MSE 4.908456537634165e-05 Test RE 5.8672965421164354e-05\n",
      "84 Train Loss 0.044574324 Test MSE 0.001102676314327413 Test RE 0.0002780926987518534\n",
      "85 Train Loss 0.043308787 Test MSE 0.0004699060445595292 Test RE 0.000181539476822169\n",
      "86 Train Loss 0.03947449 Test MSE 0.021478943949969628 Test RE 0.001227360171546844\n",
      "87 Train Loss 0.034010507 Test MSE 0.016927965337202276 Test RE 0.0010896021318540193\n",
      "88 Train Loss 0.031411137 Test MSE 0.04226232529538217 Test RE 0.0017216396576514973\n",
      "89 Train Loss 0.031216133 Test MSE 0.03595223685806749 Test RE 0.001587919645938178\n",
      "90 Train Loss 0.03104873 Test MSE 0.03607703315031407 Test RE 0.0015906732272446202\n",
      "91 Train Loss 0.030911392 Test MSE 0.06374222391470287 Test RE 0.0021143611462251214\n",
      "92 Train Loss 0.03086072 Test MSE 0.07114344290645325 Test RE 0.002233741963519378\n",
      "93 Train Loss 0.030858137 Test MSE 0.07114344290645325 Test RE 0.002233741963519378\n",
      "94 Train Loss 0.030856086 Test MSE 0.07114344290645325 Test RE 0.002233741963519378\n",
      "95 Train Loss 0.030840714 Test MSE 0.07624236114119917 Test RE 0.0023124041093301\n",
      "96 Train Loss 0.030835554 Test MSE 0.07624236114119917 Test RE 0.0023124041093301\n",
      "97 Train Loss 0.030831464 Test MSE 0.07624236114119917 Test RE 0.0023124041093301\n",
      "98 Train Loss 0.030828254 Test MSE 0.07624236114119917 Test RE 0.0023124041093301\n",
      "99 Train Loss 0.030825756 Test MSE 0.07624236114119917 Test RE 0.0023124041093301\n",
      "100 Train Loss 0.030823829 Test MSE 0.07624236114119917 Test RE 0.0023124041093301\n",
      "101 Train Loss 0.030822355 Test MSE 0.07624236114119917 Test RE 0.0023124041093301\n",
      "102 Train Loss 0.030821238 Test MSE 0.07624236114119917 Test RE 0.0023124041093301\n",
      "103 Train Loss 0.030820401 Test MSE 0.07624236114119917 Test RE 0.0023124041093301\n",
      "104 Train Loss 0.03081978 Test MSE 0.07624236114119917 Test RE 0.0023124041093301\n",
      "105 Train Loss 0.030809673 Test MSE 0.08787776563109663 Test RE 0.0024825903720876986\n",
      "106 Train Loss 0.030803349 Test MSE 0.08791709294867282 Test RE 0.002483145818043769\n",
      "107 Train Loss 0.030787464 Test MSE 0.1265214063930065 Test RE 0.0029788426902999277\n",
      "108 Train Loss 0.030458668 Test MSE 0.14541758817444556 Test RE 0.0031935522967204656\n",
      "109 Train Loss 0.028721405 Test MSE 0.052720742724743384 Test RE 0.0019228983364539872\n",
      "110 Train Loss 0.026014492 Test MSE 0.0012183392601796967 Test RE 0.0002923140454799072\n",
      "111 Train Loss 0.025295354 Test MSE 0.01770267037102809 Test RE 0.001114255932747879\n",
      "112 Train Loss 0.024006646 Test MSE 0.053103683702977786 Test RE 0.0019298692573983156\n",
      "113 Train Loss 0.023426874 Test MSE 0.03370734391260744 Test RE 0.0015375449805121054\n",
      "114 Train Loss 0.023423258 Test MSE 0.032371016494349945 Test RE 0.0015067587850905294\n",
      "115 Train Loss 0.023393074 Test MSE 0.03167611019656166 Test RE 0.0014904983044380518\n",
      "116 Train Loss 0.023370765 Test MSE 0.03287636768150962 Test RE 0.0015184744127377933\n",
      "117 Train Loss 0.023332985 Test MSE 0.032823398766384466 Test RE 0.001517250671017477\n",
      "118 Train Loss 0.023311125 Test MSE 0.033046428531142945 Test RE 0.0015223966825151406\n",
      "119 Train Loss 0.023313528 Test MSE 0.033046428531142945 Test RE 0.0015223966825151406\n",
      "120 Train Loss 0.023313837 Test MSE 0.03381956475051174 Test RE 0.001540102304713861\n",
      "121 Train Loss 0.023264704 Test MSE 0.03684508686337657 Test RE 0.0016075161919434154\n",
      "122 Train Loss 0.023258755 Test MSE 0.03689702107446371 Test RE 0.0016086487131884435\n",
      "123 Train Loss 0.023259526 Test MSE 0.03689702107446371 Test RE 0.0016086487131884435\n",
      "124 Train Loss 0.02324879 Test MSE 0.036894691431884646 Test RE 0.0016085979281171246\n",
      "125 Train Loss 0.023249306 Test MSE 0.036894691431884646 Test RE 0.0016085979281171246\n",
      "126 Train Loss 0.023249744 Test MSE 0.036894691431884646 Test RE 0.0016085979281171246\n",
      "127 Train Loss 0.023250118 Test MSE 0.036894691431884646 Test RE 0.0016085979281171246\n",
      "128 Train Loss 0.023250436 Test MSE 0.036894691431884646 Test RE 0.0016085979281171246\n",
      "129 Train Loss 0.023250708 Test MSE 0.036894691431884646 Test RE 0.0016085979281171246\n",
      "130 Train Loss 0.02325094 Test MSE 0.036894691431884646 Test RE 0.0016085979281171246\n",
      "131 Train Loss 0.023251139 Test MSE 0.036894691431884646 Test RE 0.0016085979281171246\n",
      "132 Train Loss 0.023251306 Test MSE 0.036894691431884646 Test RE 0.0016085979281171246\n",
      "133 Train Loss 0.02325145 Test MSE 0.036894691431884646 Test RE 0.0016085979281171246\n",
      "134 Train Loss 0.023251576 Test MSE 0.036894691431884646 Test RE 0.0016085979281171246\n",
      "135 Train Loss 0.02325168 Test MSE 0.036894691431884646 Test RE 0.0016085979281171246\n",
      "136 Train Loss 0.02325177 Test MSE 0.036894691431884646 Test RE 0.0016085979281171246\n",
      "137 Train Loss 0.023251845 Test MSE 0.036894691431884646 Test RE 0.0016085979281171246\n",
      "138 Train Loss 0.023251912 Test MSE 0.036894691431884646 Test RE 0.0016085979281171246\n",
      "139 Train Loss 0.023251968 Test MSE 0.036894691431884646 Test RE 0.0016085979281171246\n",
      "140 Train Loss 0.023252014 Test MSE 0.036894691431884646 Test RE 0.0016085979281171246\n",
      "141 Train Loss 0.023252057 Test MSE 0.036894691431884646 Test RE 0.0016085979281171246\n",
      "142 Train Loss 0.02325209 Test MSE 0.036894691431884646 Test RE 0.0016085979281171246\n",
      "143 Train Loss 0.02325212 Test MSE 0.036894691431884646 Test RE 0.0016085979281171246\n",
      "144 Train Loss 0.023252146 Test MSE 0.036894691431884646 Test RE 0.0016085979281171246\n",
      "145 Train Loss 0.023252169 Test MSE 0.036894691431884646 Test RE 0.0016085979281171246\n",
      "146 Train Loss 0.02325219 Test MSE 0.036894691431884646 Test RE 0.0016085979281171246\n",
      "147 Train Loss 0.023252206 Test MSE 0.036894691431884646 Test RE 0.0016085979281171246\n",
      "148 Train Loss 0.02325222 Test MSE 0.036894691431884646 Test RE 0.0016085979281171246\n",
      "149 Train Loss 0.02325223 Test MSE 0.036894691431884646 Test RE 0.0016085979281171246\n",
      "150 Train Loss 0.023252241 Test MSE 0.036894691431884646 Test RE 0.0016085979281171246\n",
      "151 Train Loss 0.023252252 Test MSE 0.036894691431884646 Test RE 0.0016085979281171246\n",
      "152 Train Loss 0.02325226 Test MSE 0.036894691431884646 Test RE 0.0016085979281171246\n",
      "153 Train Loss 0.023252264 Test MSE 0.036894691431884646 Test RE 0.0016085979281171246\n",
      "154 Train Loss 0.023252271 Test MSE 0.036894691431884646 Test RE 0.0016085979281171246\n",
      "155 Train Loss 0.023252277 Test MSE 0.036894691431884646 Test RE 0.0016085979281171246\n",
      "156 Train Loss 0.023252279 Test MSE 0.036894691431884646 Test RE 0.0016085979281171246\n",
      "157 Train Loss 0.023252282 Test MSE 0.036894691431884646 Test RE 0.0016085979281171246\n",
      "158 Train Loss 0.023252288 Test MSE 0.036894691431884646 Test RE 0.0016085979281171246\n",
      "159 Train Loss 0.023252288 Test MSE 0.036894691431884646 Test RE 0.0016085979281171246\n",
      "160 Train Loss 0.023252293 Test MSE 0.036894691431884646 Test RE 0.0016085979281171246\n",
      "161 Train Loss 0.023252295 Test MSE 0.036894691431884646 Test RE 0.0016085979281171246\n",
      "162 Train Loss 0.023252297 Test MSE 0.036894691431884646 Test RE 0.0016085979281171246\n",
      "163 Train Loss 0.02325219 Test MSE 0.036894691431884646 Test RE 0.0016085979281171246\n",
      "164 Train Loss 0.023252191 Test MSE 0.036894691431884646 Test RE 0.0016085979281171246\n",
      "165 Train Loss 0.023252193 Test MSE 0.036894691431884646 Test RE 0.0016085979281171246\n",
      "166 Train Loss 0.023252193 Test MSE 0.036894691431884646 Test RE 0.0016085979281171246\n",
      "167 Train Loss 0.023252195 Test MSE 0.036894691431884646 Test RE 0.0016085979281171246\n",
      "168 Train Loss 0.023251513 Test MSE 0.03689478319510487 Test RE 0.0016085999285408766\n",
      "169 Train Loss 0.023251496 Test MSE 0.03689478319510487 Test RE 0.0016085999285408766\n",
      "170 Train Loss 0.023251481 Test MSE 0.03689478319510487 Test RE 0.0016085999285408766\n",
      "171 Train Loss 0.02325147 Test MSE 0.03689478319510487 Test RE 0.0016085999285408766\n",
      "172 Train Loss 0.02325146 Test MSE 0.03689478319510487 Test RE 0.0016085999285408766\n",
      "173 Train Loss 0.023251452 Test MSE 0.03689478319510487 Test RE 0.0016085999285408766\n",
      "174 Train Loss 0.023251446 Test MSE 0.03689478319510487 Test RE 0.0016085999285408766\n",
      "175 Train Loss 0.023251439 Test MSE 0.03689478319510487 Test RE 0.0016085999285408766\n",
      "176 Train Loss 0.023251435 Test MSE 0.03689478319510487 Test RE 0.0016085999285408766\n",
      "177 Train Loss 0.02325143 Test MSE 0.03689478319510487 Test RE 0.0016085999285408766\n",
      "178 Train Loss 0.023251425 Test MSE 0.03689478319510487 Test RE 0.0016085999285408766\n",
      "179 Train Loss 0.02325142 Test MSE 0.03689478319510487 Test RE 0.0016085999285408766\n",
      "180 Train Loss 0.023251418 Test MSE 0.03689478319510487 Test RE 0.0016085999285408766\n",
      "181 Train Loss 0.023251414 Test MSE 0.03689478319510487 Test RE 0.0016085999285408766\n",
      "182 Train Loss 0.023251414 Test MSE 0.03689478319510487 Test RE 0.0016085999285408766\n",
      "183 Train Loss 0.02325141 Test MSE 0.03689478319510487 Test RE 0.0016085999285408766\n",
      "184 Train Loss 0.023251409 Test MSE 0.03689478319510487 Test RE 0.0016085999285408766\n",
      "185 Train Loss 0.02325141 Test MSE 0.03689478319510487 Test RE 0.0016085999285408766\n",
      "186 Train Loss 0.023251407 Test MSE 0.03689478319510487 Test RE 0.0016085999285408766\n",
      "187 Train Loss 0.023251407 Test MSE 0.03689478319510487 Test RE 0.0016085999285408766\n",
      "188 Train Loss 0.023251407 Test MSE 0.03689478319510487 Test RE 0.0016085999285408766\n",
      "189 Train Loss 0.023251403 Test MSE 0.03689478319510487 Test RE 0.0016085999285408766\n",
      "190 Train Loss 0.023251405 Test MSE 0.03689478319510487 Test RE 0.0016085999285408766\n",
      "191 Train Loss 0.023251405 Test MSE 0.03689478319510487 Test RE 0.0016085999285408766\n",
      "192 Train Loss 0.023251405 Test MSE 0.03689478319510487 Test RE 0.0016085999285408766\n",
      "193 Train Loss 0.023251405 Test MSE 0.03689478319510487 Test RE 0.0016085999285408766\n",
      "194 Train Loss 0.023251403 Test MSE 0.03689478319510487 Test RE 0.0016085999285408766\n",
      "195 Train Loss 0.023251403 Test MSE 0.03689478319510487 Test RE 0.0016085999285408766\n",
      "196 Train Loss 0.023251401 Test MSE 0.03689478319510487 Test RE 0.0016085999285408766\n",
      "197 Train Loss 0.023251403 Test MSE 0.03689478319510487 Test RE 0.0016085999285408766\n",
      "198 Train Loss 0.023247747 Test MSE 0.0404227505226327 Test RE 0.0016837534337045773\n",
      "199 Train Loss 0.023247436 Test MSE 0.0404227505226327 Test RE 0.0016837534337045773\n",
      "Training time: 101.72\n",
      "Training time: 101.72\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 9.1441145 Test MSE 14249.022238684645 Test RE 0.9996733070934043\n",
      "1 Train Loss 8.161216 Test MSE 14250.781163151778 Test RE 0.9997350059085733\n",
      "2 Train Loss 7.6198335 Test MSE 14248.97660082538 Test RE 0.9996717061770234\n",
      "3 Train Loss 7.038863 Test MSE 14248.381708177098 Test RE 0.9996508378868781\n",
      "4 Train Loss 6.4864817 Test MSE 14248.535759103326 Test RE 0.9996562418943081\n",
      "5 Train Loss 5.9157863 Test MSE 14248.690124204548 Test RE 0.9996616568934922\n",
      "6 Train Loss 5.459126 Test MSE 14248.707795299857 Test RE 0.999662276778927\n",
      "7 Train Loss 5.0582213 Test MSE 14248.736629969082 Test RE 0.9996632882712293\n",
      "8 Train Loss 4.669947 Test MSE 14248.689006191647 Test RE 0.9996616176746367\n",
      "9 Train Loss 4.2418156 Test MSE 14248.591100695328 Test RE 0.9996581832346361\n",
      "10 Train Loss 3.9464998 Test MSE 14248.381233256217 Test RE 0.9996508212269137\n",
      "11 Train Loss 3.7419283 Test MSE 14248.087703112787 Test RE 0.9996405242972006\n",
      "12 Train Loss 3.4374619 Test MSE 14247.701187870638 Test RE 0.9996269653225593\n",
      "13 Train Loss 3.4362283 Test MSE 14246.899716114725 Test RE 0.9995988490659107\n",
      "14 Train Loss 3.7735837 Test MSE 14246.200922871307 Test RE 0.9995743342064923\n",
      "15 Train Loss 4.83855 Test MSE 14246.002475263213 Test RE 0.9995673722161484\n",
      "16 Train Loss 4.653696 Test MSE 14245.478764184545 Test RE 0.999548999015709\n",
      "17 Train Loss 4.570506 Test MSE 14244.208584206835 Test RE 0.9995044362653691\n",
      "18 Train Loss 4.8383627 Test MSE 14241.630149593339 Test RE 0.9994139688498925\n",
      "19 Train Loss 5.251546 Test MSE 14237.701115128473 Test RE 0.9992760982987411\n",
      "20 Train Loss 5.4820495 Test MSE 14231.362114460224 Test RE 0.9990536214739179\n",
      "21 Train Loss 5.136539 Test MSE 14229.191626807482 Test RE 0.9989774335370909\n",
      "22 Train Loss 4.950238 Test MSE 14225.661477458465 Test RE 0.9988535066696017\n",
      "23 Train Loss 4.687894 Test MSE 14225.693435462015 Test RE 0.9988546286331054\n",
      "24 Train Loss 5.300127 Test MSE 14221.430252864031 Test RE 0.9987049481000736\n",
      "25 Train Loss 5.14249 Test MSE 14199.276430471691 Test RE 0.9979267649330352\n",
      "26 Train Loss 5.7374034 Test MSE 14192.872676931773 Test RE 0.9977017112917426\n",
      "27 Train Loss 5.877609 Test MSE 14158.613299045204 Test RE 0.9964968357060272\n",
      "28 Train Loss 5.9838514 Test MSE 14132.218108137837 Test RE 0.9955675429210347\n",
      "29 Train Loss 6.2912145 Test MSE 14107.5364876077 Test RE 0.9946977941592569\n",
      "30 Train Loss 6.203346 Test MSE 14087.292892233438 Test RE 0.9939838676528057\n",
      "31 Train Loss 5.86556 Test MSE 14075.9239630866 Test RE 0.9935826971386842\n",
      "32 Train Loss 5.8906074 Test MSE 14067.277103540684 Test RE 0.9932774706441397\n",
      "33 Train Loss 5.9181404 Test MSE 14038.881025533261 Test RE 0.9922744537703593\n",
      "34 Train Loss 6.0846863 Test MSE 13978.171190875019 Test RE 0.9901266299671042\n",
      "35 Train Loss 6.517529 Test MSE 13898.608860559472 Test RE 0.9873047586924864\n",
      "36 Train Loss 6.2824492 Test MSE 13894.702901182998 Test RE 0.9871660166321186\n",
      "37 Train Loss 6.1801677 Test MSE 13809.550012741005 Test RE 0.9841364727936659\n",
      "38 Train Loss 5.9167585 Test MSE 13788.755115414848 Test RE 0.9833952202935413\n",
      "39 Train Loss 6.8052387 Test MSE 13673.697774952961 Test RE 0.9792837591756528\n",
      "40 Train Loss 6.875701 Test MSE 13603.10915692451 Test RE 0.9767527784138921\n",
      "41 Train Loss 7.4946384 Test MSE 13546.172163273255 Test RE 0.9747064931914873\n",
      "42 Train Loss 8.600271 Test MSE 13458.514433520919 Test RE 0.9715476956896537\n",
      "43 Train Loss 8.122218 Test MSE 13408.080167739095 Test RE 0.9697256043290737\n",
      "44 Train Loss 9.857691 Test MSE 13401.80471380697 Test RE 0.9694986448948851\n",
      "45 Train Loss 9.729411 Test MSE 13224.359110705882 Test RE 0.9630589703419694\n",
      "46 Train Loss 9.068362 Test MSE 13166.134850100607 Test RE 0.9609365516944526\n",
      "47 Train Loss 9.930144 Test MSE 13156.230059146335 Test RE 0.9605750307733674\n",
      "48 Train Loss 9.572506 Test MSE 13061.016848784944 Test RE 0.9570928208515388\n",
      "49 Train Loss 9.137853 Test MSE 13003.140579055156 Test RE 0.9549699209322979\n",
      "50 Train Loss 9.651446 Test MSE 12911.993775025703 Test RE 0.9516170568360336\n",
      "51 Train Loss 9.8707 Test MSE 12846.97542535096 Test RE 0.9492180990065138\n",
      "52 Train Loss 9.586344 Test MSE 12732.343741938726 Test RE 0.9449737426908724\n",
      "53 Train Loss 15.10343 Test MSE 12425.13818326204 Test RE 0.9335039875540082\n",
      "54 Train Loss 13.819362 Test MSE 12116.439286677409 Test RE 0.9218347361729652\n",
      "55 Train Loss 13.394363 Test MSE 11903.942887556364 Test RE 0.9137154764666107\n",
      "56 Train Loss 12.341782 Test MSE 11810.54746024838 Test RE 0.9101240239939199\n",
      "57 Train Loss 13.443239 Test MSE 11742.024406295408 Test RE 0.9074799806694916\n",
      "58 Train Loss 17.304762 Test MSE 11512.391352485514 Test RE 0.8985625951072194\n",
      "59 Train Loss 16.810335 Test MSE 11323.2208161504 Test RE 0.8911494678183047\n",
      "60 Train Loss 16.965881 Test MSE 11036.379954223252 Test RE 0.8797897250600593\n",
      "61 Train Loss 16.211153 Test MSE 10916.954761690973 Test RE 0.8750166542967707\n",
      "62 Train Loss 16.148 Test MSE 10765.185175829958 Test RE 0.8689130425052051\n",
      "63 Train Loss 14.937813 Test MSE 10331.095444820206 Test RE 0.85121398584149\n",
      "64 Train Loss 14.832918 Test MSE 9891.688748733148 Test RE 0.8329151948172537\n",
      "65 Train Loss 14.11051 Test MSE 9653.230515048546 Test RE 0.8228144353453924\n",
      "66 Train Loss 13.19983 Test MSE 9541.999744452807 Test RE 0.8180602000519714\n",
      "67 Train Loss 13.376777 Test MSE 9093.441130851186 Test RE 0.7986007129155542\n",
      "68 Train Loss 12.566025 Test MSE 8801.046523288069 Test RE 0.7856565256792809\n",
      "69 Train Loss 12.660054 Test MSE 8167.666474394738 Test RE 0.7568582710065528\n",
      "70 Train Loss 11.603417 Test MSE 7409.877596277133 Test RE 0.7208934519139598\n",
      "71 Train Loss 12.206789 Test MSE 7263.905738856352 Test RE 0.7137574653870784\n",
      "72 Train Loss 11.40045 Test MSE 7218.930426013152 Test RE 0.711544378456403\n",
      "73 Train Loss 11.237658 Test MSE 6564.762570566029 Test RE 0.67853940790645\n",
      "74 Train Loss 10.166495 Test MSE 6070.131236996567 Test RE 0.6524760937855969\n",
      "75 Train Loss 9.443774 Test MSE 5443.288671287489 Test RE 0.6178687737010253\n",
      "76 Train Loss 9.341137 Test MSE 5222.80426741359 Test RE 0.6052258079925141\n",
      "77 Train Loss 9.497394 Test MSE 4963.242407354237 Test RE 0.589994967436377\n",
      "78 Train Loss 9.330616 Test MSE 4069.91874432117 Test RE 0.5342671021997932\n",
      "79 Train Loss 9.062288 Test MSE 3777.570844881413 Test RE 0.5147209826815806\n",
      "80 Train Loss 8.413658 Test MSE 3657.3569021819917 Test RE 0.5064647632986293\n",
      "81 Train Loss 8.183129 Test MSE 3235.5705927290323 Test RE 0.47636627073191906\n",
      "82 Train Loss 7.3390512 Test MSE 2651.2039607306147 Test RE 0.43120832187517655\n",
      "83 Train Loss 6.8898478 Test MSE 2601.9250947660917 Test RE 0.4271820136974602\n",
      "84 Train Loss 6.834156 Test MSE 2663.957856195263 Test RE 0.4322442639828398\n",
      "85 Train Loss 6.403203 Test MSE 2336.3523830108775 Test RE 0.4047946247780013\n",
      "86 Train Loss 5.508167 Test MSE 1418.3870719438178 Test RE 0.31540091277078197\n",
      "87 Train Loss 4.033985 Test MSE 751.3341456477444 Test RE 0.22955255290751023\n",
      "88 Train Loss 3.3751187 Test MSE 593.2418966455845 Test RE 0.20397712728896344\n",
      "89 Train Loss 3.0174186 Test MSE 498.4430845311194 Test RE 0.1869706126976871\n",
      "90 Train Loss 2.832701 Test MSE 369.6207956239779 Test RE 0.16100663094716236\n",
      "91 Train Loss 2.3386962 Test MSE 129.44214710883725 Test RE 0.09528036623992032\n",
      "92 Train Loss 1.7981392 Test MSE 88.92039833087983 Test RE 0.07897075016358156\n",
      "93 Train Loss 1.3282356 Test MSE 68.9787879904209 Test RE 0.06955419736664697\n",
      "94 Train Loss 1.0575578 Test MSE 50.942094125568445 Test RE 0.05977284958982541\n",
      "95 Train Loss 0.9725138 Test MSE 50.85141719220478 Test RE 0.05971962805614332\n",
      "96 Train Loss 0.8449271 Test MSE 37.917514087053675 Test RE 0.05156861792246383\n",
      "97 Train Loss 0.81945735 Test MSE 32.515879926580844 Test RE 0.0477543919165667\n",
      "98 Train Loss 0.6095678 Test MSE 31.585820856221762 Test RE 0.04706647220916058\n",
      "99 Train Loss 0.38128868 Test MSE 44.16717727737039 Test RE 0.05565643515736933\n",
      "100 Train Loss 0.33007193 Test MSE 45.65253156890803 Test RE 0.05658456694557594\n",
      "101 Train Loss 0.3120827 Test MSE 37.11464938012623 Test RE 0.05101974043979341\n",
      "102 Train Loss 0.2910872 Test MSE 32.533921018575334 Test RE 0.04776763808902942\n",
      "103 Train Loss 0.26216856 Test MSE 24.23916584264422 Test RE 0.04123105038113391\n",
      "104 Train Loss 0.19969857 Test MSE 7.8290412385694825 Test RE 0.023432566278135572\n",
      "105 Train Loss 0.17261793 Test MSE 3.5221252928258933 Test RE 0.015716938880470283\n",
      "106 Train Loss 0.16807413 Test MSE 2.8868627803323843 Test RE 0.014229139410142694\n",
      "107 Train Loss 0.16309166 Test MSE 1.7181630536689494 Test RE 0.010977356075166568\n",
      "108 Train Loss 0.1566245 Test MSE 1.4502521074023234 Test RE 0.010085265050227199\n",
      "109 Train Loss 0.1408438 Test MSE 3.906996966179559 Test RE 0.016553395852295098\n",
      "110 Train Loss 0.13309963 Test MSE 3.3930933464669124 Test RE 0.015426360191822586\n",
      "111 Train Loss 0.12135484 Test MSE 0.7197285833317398 Test RE 0.007104768622873663\n",
      "112 Train Loss 0.11689162 Test MSE 0.08527918668936328 Test RE 0.002445609364394411\n",
      "113 Train Loss 0.11235861 Test MSE 0.13240873336011993 Test RE 0.003047360818397228\n",
      "114 Train Loss 0.107817434 Test MSE 0.09210599590313465 Test RE 0.0025416135381185236\n",
      "115 Train Loss 0.101165265 Test MSE 0.00018027741619399957 Test RE 0.00011244398440143663\n",
      "116 Train Loss 0.09788934 Test MSE 0.004554426010430041 Test RE 0.0005651742794470064\n",
      "117 Train Loss 0.095243715 Test MSE 0.0001085593477283841 Test RE 8.725677879575074e-05\n",
      "118 Train Loss 0.0832881 Test MSE 0.006525397316936961 Test RE 0.0006765019359719174\n",
      "119 Train Loss 0.07543108 Test MSE 0.4220233931929783 Test RE 0.005440437509938645\n",
      "120 Train Loss 0.06915508 Test MSE 0.5492444932981136 Test RE 0.006206523719158733\n",
      "121 Train Loss 0.058766063 Test MSE 1.551428647607421e-05 Test RE 3.298613765714943e-05\n",
      "122 Train Loss 0.057771683 Test MSE 0.0007020378801511133 Test RE 0.00022189414190713802\n",
      "123 Train Loss 0.057767928 Test MSE 0.0007020367722397065 Test RE 0.00022189396681747772\n",
      "124 Train Loss 0.05776796 Test MSE 0.0007020367722397065 Test RE 0.00022189396681747772\n",
      "125 Train Loss 0.057761755 Test MSE 0.0007374726558210118 Test RE 0.00022742516831088323\n",
      "126 Train Loss 0.05759356 Test MSE 0.0007951231305292336 Test RE 0.0002361471766112697\n",
      "127 Train Loss 0.057121523 Test MSE 0.004770262946379974 Test RE 0.0005784112391694224\n",
      "128 Train Loss 0.057062622 Test MSE 0.008605858910264896 Test RE 0.0007768956229158565\n",
      "129 Train Loss 0.05706244 Test MSE 0.008605858910264896 Test RE 0.0007768956229158565\n",
      "130 Train Loss 0.05706251 Test MSE 0.008605858910264896 Test RE 0.0007768956229158565\n",
      "131 Train Loss 0.05705492 Test MSE 0.008605327199386007 Test RE 0.000776871622392519\n",
      "132 Train Loss 0.05705496 Test MSE 0.008605327199386007 Test RE 0.000776871622392519\n",
      "133 Train Loss 0.057053726 Test MSE 0.008605373424330693 Test RE 0.000776873708937209\n",
      "134 Train Loss 0.057053752 Test MSE 0.008605373424330693 Test RE 0.000776873708937209\n",
      "135 Train Loss 0.057053786 Test MSE 0.008605373424330693 Test RE 0.000776873708937209\n",
      "136 Train Loss 0.057053808 Test MSE 0.008605373424330693 Test RE 0.000776873708937209\n",
      "137 Train Loss 0.057053827 Test MSE 0.008605373424330693 Test RE 0.000776873708937209\n",
      "138 Train Loss 0.057053845 Test MSE 0.008605373424330693 Test RE 0.000776873708937209\n",
      "139 Train Loss 0.057053868 Test MSE 0.008605373424330693 Test RE 0.000776873708937209\n",
      "140 Train Loss 0.057053875 Test MSE 0.008605373424330693 Test RE 0.000776873708937209\n",
      "141 Train Loss 0.05705389 Test MSE 0.008605373424330693 Test RE 0.000776873708937209\n",
      "142 Train Loss 0.0570539 Test MSE 0.008605373424330693 Test RE 0.000776873708937209\n",
      "143 Train Loss 0.05705391 Test MSE 0.008605373424330693 Test RE 0.000776873708937209\n",
      "144 Train Loss 0.057053916 Test MSE 0.008605373424330693 Test RE 0.000776873708937209\n",
      "145 Train Loss 0.057053927 Test MSE 0.008605373424330693 Test RE 0.000776873708937209\n",
      "146 Train Loss 0.057053935 Test MSE 0.008605373424330693 Test RE 0.000776873708937209\n",
      "147 Train Loss 0.057053942 Test MSE 0.008605373424330693 Test RE 0.000776873708937209\n",
      "148 Train Loss 0.057053942 Test MSE 0.008605373424330693 Test RE 0.000776873708937209\n",
      "149 Train Loss 0.05705395 Test MSE 0.008605373424330693 Test RE 0.000776873708937209\n",
      "150 Train Loss 0.057053953 Test MSE 0.008605373424330693 Test RE 0.000776873708937209\n",
      "151 Train Loss 0.057053953 Test MSE 0.008605373424330693 Test RE 0.000776873708937209\n",
      "152 Train Loss 0.057053957 Test MSE 0.008605373424330693 Test RE 0.000776873708937209\n",
      "153 Train Loss 0.05705396 Test MSE 0.008605373424330693 Test RE 0.000776873708937209\n",
      "154 Train Loss 0.05705396 Test MSE 0.008605373424330693 Test RE 0.000776873708937209\n",
      "155 Train Loss 0.057053965 Test MSE 0.008605373424330693 Test RE 0.000776873708937209\n",
      "156 Train Loss 0.057053626 Test MSE 0.008605364102423197 Test RE 0.0007768732881567005\n",
      "157 Train Loss 0.057053626 Test MSE 0.008605364102423197 Test RE 0.0007768732881567005\n",
      "158 Train Loss 0.05705363 Test MSE 0.008605364102423197 Test RE 0.0007768732881567005\n",
      "159 Train Loss 0.057053633 Test MSE 0.008605364102423197 Test RE 0.0007768732881567005\n",
      "160 Train Loss 0.057053633 Test MSE 0.008605364102423197 Test RE 0.0007768732881567005\n",
      "161 Train Loss 0.05705363 Test MSE 0.008605364102423197 Test RE 0.0007768732881567005\n",
      "162 Train Loss 0.057053637 Test MSE 0.008605364102423197 Test RE 0.0007768732881567005\n",
      "163 Train Loss 0.057053637 Test MSE 0.008605364102423197 Test RE 0.0007768732881567005\n",
      "164 Train Loss 0.05705364 Test MSE 0.008605364102423197 Test RE 0.0007768732881567005\n",
      "165 Train Loss 0.05705364 Test MSE 0.008605364102423197 Test RE 0.0007768732881567005\n",
      "166 Train Loss 0.057053633 Test MSE 0.008605364102423197 Test RE 0.0007768732881567005\n",
      "167 Train Loss 0.057053637 Test MSE 0.008605364102423197 Test RE 0.0007768732881567005\n",
      "168 Train Loss 0.057053637 Test MSE 0.008605364102423197 Test RE 0.0007768732881567005\n",
      "169 Train Loss 0.057053637 Test MSE 0.008605364102423197 Test RE 0.0007768732881567005\n",
      "170 Train Loss 0.057053637 Test MSE 0.008605364102423197 Test RE 0.0007768732881567005\n",
      "171 Train Loss 0.05705364 Test MSE 0.008605364102423197 Test RE 0.0007768732881567005\n",
      "172 Train Loss 0.057053644 Test MSE 0.008605364102423197 Test RE 0.0007768732881567005\n",
      "173 Train Loss 0.057053637 Test MSE 0.008605364102423197 Test RE 0.0007768732881567005\n",
      "174 Train Loss 0.057053637 Test MSE 0.008605364102423197 Test RE 0.0007768732881567005\n",
      "175 Train Loss 0.057053637 Test MSE 0.008605364102423197 Test RE 0.0007768732881567005\n",
      "176 Train Loss 0.057053637 Test MSE 0.008605364102423197 Test RE 0.0007768732881567005\n",
      "177 Train Loss 0.057053637 Test MSE 0.008605364102423197 Test RE 0.0007768732881567005\n",
      "178 Train Loss 0.05705364 Test MSE 0.008605364102423197 Test RE 0.0007768732881567005\n",
      "179 Train Loss 0.05705364 Test MSE 0.008605364102423197 Test RE 0.0007768732881567005\n",
      "180 Train Loss 0.05705364 Test MSE 0.008605364102423197 Test RE 0.0007768732881567005\n",
      "181 Train Loss 0.057053637 Test MSE 0.008605364102423197 Test RE 0.0007768732881567005\n",
      "182 Train Loss 0.057053637 Test MSE 0.008605364102423197 Test RE 0.0007768732881567005\n",
      "183 Train Loss 0.057053637 Test MSE 0.008605364102423197 Test RE 0.0007768732881567005\n",
      "184 Train Loss 0.05705364 Test MSE 0.008605364102423197 Test RE 0.0007768732881567005\n",
      "185 Train Loss 0.05705364 Test MSE 0.008605364102423197 Test RE 0.0007768732881567005\n",
      "186 Train Loss 0.057053637 Test MSE 0.008605364102423197 Test RE 0.0007768732881567005\n",
      "187 Train Loss 0.05705364 Test MSE 0.008605364102423197 Test RE 0.0007768732881567005\n",
      "188 Train Loss 0.057053637 Test MSE 0.008605364102423197 Test RE 0.0007768732881567005\n",
      "189 Train Loss 0.057053637 Test MSE 0.008605364102423197 Test RE 0.0007768732881567005\n",
      "190 Train Loss 0.057053637 Test MSE 0.008605364102423197 Test RE 0.0007768732881567005\n",
      "191 Train Loss 0.05705364 Test MSE 0.008605364102423197 Test RE 0.0007768732881567005\n",
      "192 Train Loss 0.057053637 Test MSE 0.008605364102423197 Test RE 0.0007768732881567005\n",
      "193 Train Loss 0.057053637 Test MSE 0.008605364102423197 Test RE 0.0007768732881567005\n",
      "194 Train Loss 0.05705364 Test MSE 0.008605364102423197 Test RE 0.0007768732881567005\n",
      "195 Train Loss 0.05705364 Test MSE 0.008605364102423197 Test RE 0.0007768732881567005\n",
      "196 Train Loss 0.057053637 Test MSE 0.008605364102423197 Test RE 0.0007768732881567005\n",
      "197 Train Loss 0.05705364 Test MSE 0.008605364102423197 Test RE 0.0007768732881567005\n",
      "198 Train Loss 0.057053637 Test MSE 0.008605364102423197 Test RE 0.0007768732881567005\n",
      "199 Train Loss 0.05705364 Test MSE 0.008605364102423197 Test RE 0.0007768732881567005\n",
      "Training time: 108.36\n",
      "Training time: 108.36\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 5.845148 Test MSE 14252.493419553866 Test RE 0.9997950640635869\n",
      "1 Train Loss 6.551464 Test MSE 14250.187529728275 Test RE 0.9997141831098799\n",
      "2 Train Loss 6.029585 Test MSE 14250.039102736186 Test RE 0.9997089766887282\n",
      "3 Train Loss 5.668682 Test MSE 14250.087039269492 Test RE 0.9997106581768285\n",
      "4 Train Loss 5.372315 Test MSE 14250.108228027599 Test RE 0.9997114014220935\n",
      "5 Train Loss 5.0925703 Test MSE 14250.16461894578 Test RE 0.9997133794627923\n",
      "6 Train Loss 5.298839 Test MSE 14250.16943256611 Test RE 0.9997135483113763\n",
      "7 Train Loss 5.059558 Test MSE 14249.71522924458 Test RE 0.9996976159803729\n",
      "8 Train Loss 4.6100206 Test MSE 14249.561830527322 Test RE 0.9996922350747833\n",
      "9 Train Loss 4.238263 Test MSE 14248.774320280389 Test RE 0.999664610409346\n",
      "10 Train Loss 3.9707842 Test MSE 14247.766855618705 Test RE 0.9996292689635191\n",
      "11 Train Loss 3.9987228 Test MSE 14246.609894311601 Test RE 0.9995886816954659\n",
      "12 Train Loss 4.0116735 Test MSE 14244.78407249471 Test RE 0.9995246268320719\n",
      "13 Train Loss 4.032338 Test MSE 14232.824808670059 Test RE 0.9991049613382751\n",
      "14 Train Loss 3.8010507 Test MSE 14231.238514727405 Test RE 0.9990492830617613\n",
      "15 Train Loss 3.789911 Test MSE 14227.266613160482 Test RE 0.9989098573076104\n",
      "16 Train Loss 4.2958603 Test MSE 14206.623507792512 Test RE 0.998184908260321\n",
      "17 Train Loss 4.2978926 Test MSE 14182.127258979557 Test RE 0.9973239600057426\n",
      "18 Train Loss 4.3025913 Test MSE 14170.292362940854 Test RE 0.9969077428676041\n",
      "19 Train Loss 5.110958 Test MSE 14137.507024949655 Test RE 0.9957538187526898\n",
      "20 Train Loss 5.2935185 Test MSE 14092.013916020098 Test RE 0.9941504088196982\n",
      "21 Train Loss 5.5783777 Test MSE 14045.033527302408 Test RE 0.9924918607558597\n",
      "22 Train Loss 6.8894925 Test MSE 14015.04214334113 Test RE 0.991431624342333\n",
      "23 Train Loss 6.630029 Test MSE 13927.774285840622 Test RE 0.9883401167287412\n",
      "24 Train Loss 7.902549 Test MSE 13914.818123648376 Test RE 0.9878803133916492\n",
      "25 Train Loss 9.241575 Test MSE 13801.481227128199 Test RE 0.9838489201045706\n",
      "26 Train Loss 13.2671385 Test MSE 13639.437567201441 Test RE 0.978056164870283\n",
      "27 Train Loss 13.072357 Test MSE 13567.760340149556 Test RE 0.9754828659934666\n",
      "28 Train Loss 12.799288 Test MSE 13519.705160338657 Test RE 0.9737538191562706\n",
      "29 Train Loss 13.581302 Test MSE 13394.577718926246 Test RE 0.9692372059794024\n",
      "30 Train Loss 14.073104 Test MSE 13305.917225659176 Test RE 0.9660241251443008\n",
      "31 Train Loss 13.377618 Test MSE 13209.245489386465 Test RE 0.9625084911270392\n",
      "32 Train Loss 13.050431 Test MSE 13180.463293131033 Test RE 0.9614592935914915\n",
      "33 Train Loss 16.33438 Test MSE 13072.51971076143 Test RE 0.9575141848169924\n",
      "34 Train Loss 16.092644 Test MSE 12963.70249624544 Test RE 0.9535206255481735\n",
      "35 Train Loss 15.544356 Test MSE 12855.646080076252 Test RE 0.9495383671587421\n",
      "36 Train Loss 15.932908 Test MSE 12340.371717377455 Test RE 0.9303142743102847\n",
      "37 Train Loss 15.328986 Test MSE 12282.575506743216 Test RE 0.928133151030887\n",
      "38 Train Loss 14.963247 Test MSE 12269.493251185499 Test RE 0.9276387388474823\n",
      "39 Train Loss 16.44389 Test MSE 12116.432465581856 Test RE 0.9218344766939404\n",
      "40 Train Loss 16.899294 Test MSE 11954.988094545079 Test RE 0.9156724290062328\n",
      "41 Train Loss 16.51072 Test MSE 11848.880677591105 Test RE 0.9115998115985559\n",
      "42 Train Loss 17.79972 Test MSE 11756.833986735232 Test RE 0.9080520780515177\n",
      "43 Train Loss 17.371313 Test MSE 11638.330807731962 Test RE 0.9034641256303294\n",
      "44 Train Loss 16.7566 Test MSE 11570.072725856178 Test RE 0.9008108493049023\n",
      "45 Train Loss 15.845028 Test MSE 11525.125302968914 Test RE 0.8990594114896162\n",
      "46 Train Loss 15.702171 Test MSE 11392.559434550009 Test RE 0.8938738148398971\n",
      "47 Train Loss 15.277684 Test MSE 11256.374197701558 Test RE 0.8885151242760778\n",
      "48 Train Loss 15.076029 Test MSE 10961.794131340834 Test RE 0.8768117972999571\n",
      "49 Train Loss 15.745066 Test MSE 10571.214128241589 Test RE 0.8610492617115613\n",
      "50 Train Loss 15.623658 Test MSE 10191.542062833807 Test RE 0.8454453004435558\n",
      "51 Train Loss 18.383177 Test MSE 10110.38263879322 Test RE 0.8420722581750578\n",
      "52 Train Loss 17.412767 Test MSE 10074.717260451167 Test RE 0.8405856992823011\n",
      "53 Train Loss 23.118814 Test MSE 9290.280874128892 Test RE 0.807197830324799\n",
      "54 Train Loss 23.910484 Test MSE 8548.707810844386 Test RE 0.7743116623525089\n",
      "55 Train Loss 24.85859 Test MSE 8202.401639227226 Test RE 0.7584659336902001\n",
      "56 Train Loss 23.843678 Test MSE 7963.929559367803 Test RE 0.7473589998993684\n",
      "57 Train Loss 21.931543 Test MSE 7790.117522464844 Test RE 0.739158488176419\n",
      "58 Train Loss 20.709532 Test MSE 7744.559805783085 Test RE 0.736993966876723\n",
      "59 Train Loss 19.399467 Test MSE 7336.415995186806 Test RE 0.7173110784059802\n",
      "60 Train Loss 19.344309 Test MSE 7062.910156210124 Test RE 0.7038131949995391\n",
      "61 Train Loss 19.928646 Test MSE 6941.6681198991255 Test RE 0.6977462111473001\n",
      "62 Train Loss 19.285683 Test MSE 6715.128619082799 Test RE 0.6862663926524186\n",
      "63 Train Loss 19.999489 Test MSE 6329.7127039944935 Test RE 0.6662812057552394\n",
      "64 Train Loss 20.215546 Test MSE 5918.38921443239 Test RE 0.6442691333876855\n",
      "65 Train Loss 19.556341 Test MSE 5783.953448991991 Test RE 0.6369098394484756\n",
      "66 Train Loss 19.945597 Test MSE 5327.406435027735 Test RE 0.6112564843226151\n",
      "67 Train Loss 19.359318 Test MSE 5015.062400186765 Test RE 0.5930669659096719\n",
      "68 Train Loss 18.450874 Test MSE 4799.785474074496 Test RE 0.5801983338152193\n",
      "69 Train Loss 18.121143 Test MSE 4374.702471674225 Test RE 0.553910789194334\n",
      "70 Train Loss 16.922169 Test MSE 3947.887781021475 Test RE 0.5261965101382068\n",
      "71 Train Loss 14.9132 Test MSE 3503.534659018903 Test RE 0.49569983326607336\n",
      "72 Train Loss 13.138493 Test MSE 2950.943151646429 Test RE 0.45493147525443434\n",
      "73 Train Loss 11.953422 Test MSE 2480.685870479803 Test RE 0.41711081472584155\n",
      "74 Train Loss 11.212954 Test MSE 2500.359018096362 Test RE 0.4187615028837669\n",
      "75 Train Loss 10.833967 Test MSE 2501.4556161321275 Test RE 0.4188533222383576\n",
      "76 Train Loss 10.29221 Test MSE 2468.9932010010416 Test RE 0.41612663135697087\n",
      "77 Train Loss 10.116051 Test MSE 2402.9041969623995 Test RE 0.41051950043348906\n",
      "78 Train Loss 9.459906 Test MSE 2244.4136585443102 Test RE 0.39675007291111963\n",
      "79 Train Loss 8.611563 Test MSE 1918.2206592478815 Test RE 0.36678776952919345\n",
      "80 Train Loss 7.6607227 Test MSE 1515.690694339516 Test RE 0.32603997853660305\n",
      "81 Train Loss 6.465924 Test MSE 1051.5970611823705 Test RE 0.2715752860341776\n",
      "82 Train Loss 6.1095095 Test MSE 816.706258322224 Test RE 0.23933075206540588\n",
      "83 Train Loss 5.577974 Test MSE 607.6983152628485 Test RE 0.20644747709753838\n",
      "84 Train Loss 4.164833 Test MSE 383.37109895174547 Test RE 0.16397409726825252\n",
      "85 Train Loss 2.8034601 Test MSE 400.0340500092975 Test RE 0.1674997037651293\n",
      "86 Train Loss 2.5352812 Test MSE 390.3885756649108 Test RE 0.1654680366137854\n",
      "87 Train Loss 2.3913352 Test MSE 384.3227403998191 Test RE 0.16417748744362032\n",
      "88 Train Loss 2.2113242 Test MSE 377.4389910922746 Test RE 0.16270052154087142\n",
      "89 Train Loss 1.9533974 Test MSE 281.6985965933584 Test RE 0.1405587561956193\n",
      "90 Train Loss 1.2451061 Test MSE 134.38592459063065 Test RE 0.09708283623487586\n",
      "91 Train Loss 1.058969 Test MSE 77.73033009677722 Test RE 0.07383475187707927\n",
      "92 Train Loss 0.923627 Test MSE 26.61793165378255 Test RE 0.0432068610801461\n",
      "93 Train Loss 0.74746704 Test MSE 19.43586303163952 Test RE 0.03692049190667488\n",
      "94 Train Loss 0.5805761 Test MSE 19.830624833895364 Test RE 0.03729355316169313\n",
      "95 Train Loss 0.5137661 Test MSE 9.647026155578656 Test RE 0.026011313998764413\n",
      "96 Train Loss 0.50091714 Test MSE 9.912640767317532 Test RE 0.026366971368556282\n",
      "97 Train Loss 0.4631971 Test MSE 7.418613479385021 Test RE 0.02281008669407656\n",
      "98 Train Loss 0.4286704 Test MSE 6.5698641230369095 Test RE 0.02146563584983877\n",
      "99 Train Loss 0.41324115 Test MSE 10.025242903726813 Test RE 0.02651630560959932\n",
      "100 Train Loss 0.3780426 Test MSE 5.866735529279221 Test RE 0.020284477209765353\n",
      "101 Train Loss 0.3349515 Test MSE 0.000729433301044208 Test RE 0.00022618216508740713\n",
      "102 Train Loss 0.30816832 Test MSE 0.3012902364177059 Test RE 0.0045968262941758235\n",
      "103 Train Loss 0.28708047 Test MSE 0.14306702232495216 Test RE 0.0031676364580867466\n",
      "104 Train Loss 0.2701708 Test MSE 1.7619034160482037 Test RE 0.011116206671601975\n",
      "105 Train Loss 0.26363924 Test MSE 0.7292585835339068 Test RE 0.007151651424300557\n",
      "106 Train Loss 0.26159936 Test MSE 0.3266150106150624 Test RE 0.004786120557637006\n",
      "107 Train Loss 0.25353634 Test MSE 0.07347202269644398 Test RE 0.002270003683176858\n",
      "108 Train Loss 0.22243734 Test MSE 0.30193936713568736 Test RE 0.0046017755678226455\n",
      "109 Train Loss 0.19905768 Test MSE 0.0573877379856485 Test RE 0.002006204116255839\n",
      "110 Train Loss 0.19632365 Test MSE 0.287360259569141 Test RE 0.0044893029878880905\n",
      "111 Train Loss 0.19391909 Test MSE 0.45465627787553964 Test RE 0.0056468618104228\n",
      "112 Train Loss 0.18818246 Test MSE 0.21377857765112065 Test RE 0.003872109723520091\n",
      "113 Train Loss 0.173755 Test MSE 0.13577598081997566 Test RE 0.0030858658202110936\n",
      "114 Train Loss 0.16760743 Test MSE 0.018618931823920486 Test RE 0.001142728199115681\n",
      "115 Train Loss 0.16636461 Test MSE 0.03571669197246167 Test RE 0.0015827093861953206\n",
      "116 Train Loss 0.16541658 Test MSE 0.11887653859719453 Test RE 0.0028874444594335883\n",
      "117 Train Loss 0.16261624 Test MSE 0.05272547281197934 Test RE 0.001922984595413987\n",
      "118 Train Loss 0.1566608 Test MSE 0.0009083726176505156 Test RE 0.00025240478325696457\n",
      "119 Train Loss 0.15085864 Test MSE 0.07549349584767363 Test RE 0.0023010196743572667\n",
      "120 Train Loss 0.14460456 Test MSE 0.00048813659635094916 Test RE 0.00018502748600887213\n",
      "121 Train Loss 0.13965236 Test MSE 1.5236856303010649e-05 Test RE 3.2689874226509406e-05\n",
      "122 Train Loss 0.13632448 Test MSE 0.014189023738421838 Test RE 0.0009975664196912462\n",
      "123 Train Loss 0.13463174 Test MSE 2.386296014471179e-05 Test RE 4.0909834417973347e-05\n",
      "124 Train Loss 0.13375399 Test MSE 0.003339462005138223 Test RE 0.0004839537028091343\n",
      "125 Train Loss 0.13306797 Test MSE 0.004036307162650263 Test RE 0.0005320563956516954\n",
      "126 Train Loss 0.13306847 Test MSE 0.004036307162650263 Test RE 0.0005320563956516954\n",
      "127 Train Loss 0.13306892 Test MSE 0.004036307162650263 Test RE 0.0005320563956516954\n",
      "128 Train Loss 0.13276672 Test MSE 0.0001321047902103221 Test RE 9.625534315048606e-05\n",
      "129 Train Loss 0.13276018 Test MSE 0.0007093326064923958 Test RE 0.00022304399009335916\n",
      "130 Train Loss 0.13050786 Test MSE 0.04380505974507853 Test RE 0.0017527811831885036\n",
      "131 Train Loss 0.12345429 Test MSE 0.024508764278732983 Test RE 0.0013110711810945927\n",
      "132 Train Loss 0.109589316 Test MSE 0.0031141662686912275 Test RE 0.0004673437745948411\n",
      "133 Train Loss 0.094611324 Test MSE 0.4116494084099925 Test RE 0.0053731542906907855\n",
      "134 Train Loss 0.08605322 Test MSE 0.35929525466822143 Test RE 0.005019856523851919\n",
      "135 Train Loss 0.07005192 Test MSE 0.12266123956017663 Test RE 0.002933048459014816\n",
      "136 Train Loss 0.06215615 Test MSE 0.37138654151891565 Test RE 0.005103623688096403\n",
      "137 Train Loss 0.058277704 Test MSE 0.19875733964781858 Test RE 0.0037335945274707904\n",
      "138 Train Loss 0.0554653 Test MSE 0.05893779358112369 Test RE 0.0020331176025106955\n",
      "139 Train Loss 0.05373245 Test MSE 0.06666893825961823 Test RE 0.002162356684466787\n",
      "140 Train Loss 0.05258143 Test MSE 0.026737292189373506 Test RE 0.0013693809275535291\n",
      "141 Train Loss 0.050589196 Test MSE 0.06502320265305439 Test RE 0.002135500817253291\n",
      "142 Train Loss 0.049343437 Test MSE 0.10926250383855911 Test RE 0.0027682234162766882\n",
      "143 Train Loss 0.04868601 Test MSE 0.038536877668745084 Test RE 0.001644007619134742\n",
      "144 Train Loss 0.04859851 Test MSE 0.033838778879974656 Test RE 0.0015405397367660742\n",
      "145 Train Loss 0.048470978 Test MSE 0.03264304005890467 Test RE 0.0015130764183473398\n",
      "146 Train Loss 0.04838354 Test MSE 0.03283579386047458 Test RE 0.0015175371235238747\n",
      "147 Train Loss 0.048061956 Test MSE 0.0224640516666994 Test RE 0.0012551903982895543\n",
      "148 Train Loss 0.047571152 Test MSE 0.013620435263063365 Test RE 0.0009773746192294486\n",
      "149 Train Loss 0.047113698 Test MSE 0.0231694685321891 Test RE 0.0012747458292942167\n",
      "150 Train Loss 0.046918407 Test MSE 0.023891559481862158 Test RE 0.0012944575503466122\n",
      "151 Train Loss 0.04688067 Test MSE 0.023842206894076953 Test RE 0.001293119884305167\n",
      "152 Train Loss 0.046796866 Test MSE 0.022861341052230352 Test RE 0.0012662411268415223\n",
      "153 Train Loss 0.046795737 Test MSE 0.022861341052230352 Test RE 0.0012662411268415223\n",
      "154 Train Loss 0.04679477 Test MSE 0.022861341052230352 Test RE 0.0012662411268415223\n",
      "155 Train Loss 0.046793938 Test MSE 0.022861341052230352 Test RE 0.0012662411268415223\n",
      "156 Train Loss 0.046793185 Test MSE 0.022834156491139658 Test RE 0.0012654880553529956\n",
      "157 Train Loss 0.046698883 Test MSE 0.024757196605373958 Test RE 0.0013176992432772541\n",
      "158 Train Loss 0.04669821 Test MSE 0.024757196605373958 Test RE 0.0013176992432772541\n",
      "159 Train Loss 0.046697613 Test MSE 0.024757196605373958 Test RE 0.0013176992432772541\n",
      "160 Train Loss 0.046697095 Test MSE 0.024757196605373958 Test RE 0.0013176992432772541\n",
      "161 Train Loss 0.046696648 Test MSE 0.024757196605373958 Test RE 0.0013176992432772541\n",
      "162 Train Loss 0.046696257 Test MSE 0.024757196605373958 Test RE 0.0013176992432772541\n",
      "163 Train Loss 0.046695914 Test MSE 0.024757196605373958 Test RE 0.0013176992432772541\n",
      "164 Train Loss 0.046695616 Test MSE 0.024757196605373958 Test RE 0.0013176992432772541\n",
      "165 Train Loss 0.046695355 Test MSE 0.024757196605373958 Test RE 0.0013176992432772541\n",
      "166 Train Loss 0.046694886 Test MSE 0.024757215596789936 Test RE 0.0013176997486852382\n",
      "167 Train Loss 0.046694685 Test MSE 0.024757215596789936 Test RE 0.0013176997486852382\n",
      "168 Train Loss 0.04669451 Test MSE 0.024757215596789936 Test RE 0.0013176997486852382\n",
      "169 Train Loss 0.046694368 Test MSE 0.024757215596789936 Test RE 0.0013176997486852382\n",
      "170 Train Loss 0.04669423 Test MSE 0.024757215596789936 Test RE 0.0013176997486852382\n",
      "171 Train Loss 0.04669413 Test MSE 0.024757215596789936 Test RE 0.0013176997486852382\n",
      "172 Train Loss 0.046694033 Test MSE 0.024757215596789936 Test RE 0.0013176997486852382\n",
      "173 Train Loss 0.046693947 Test MSE 0.024757215596789936 Test RE 0.0013176997486852382\n",
      "174 Train Loss 0.046693873 Test MSE 0.024757215596789936 Test RE 0.0013176997486852382\n",
      "175 Train Loss 0.04669382 Test MSE 0.024757215596789936 Test RE 0.0013176997486852382\n",
      "176 Train Loss 0.04669376 Test MSE 0.024757215596789936 Test RE 0.0013176997486852382\n",
      "177 Train Loss 0.046693712 Test MSE 0.024757215596789936 Test RE 0.0013176997486852382\n",
      "178 Train Loss 0.04669368 Test MSE 0.024757215596789936 Test RE 0.0013176997486852382\n",
      "179 Train Loss 0.04669364 Test MSE 0.024757215596789936 Test RE 0.0013176997486852382\n",
      "180 Train Loss 0.046693612 Test MSE 0.024757215596789936 Test RE 0.0013176997486852382\n",
      "181 Train Loss 0.046693586 Test MSE 0.024757215596789936 Test RE 0.0013176997486852382\n",
      "182 Train Loss 0.046693563 Test MSE 0.024757215596789936 Test RE 0.0013176997486852382\n",
      "183 Train Loss 0.04669354 Test MSE 0.024757215596789936 Test RE 0.0013176997486852382\n",
      "184 Train Loss 0.046693526 Test MSE 0.024757215596789936 Test RE 0.0013176997486852382\n",
      "185 Train Loss 0.04669351 Test MSE 0.024757215596789936 Test RE 0.0013176997486852382\n",
      "186 Train Loss 0.046693496 Test MSE 0.024757215596789936 Test RE 0.0013176997486852382\n",
      "187 Train Loss 0.046693485 Test MSE 0.024757215596789936 Test RE 0.0013176997486852382\n",
      "188 Train Loss 0.046693478 Test MSE 0.024757215596789936 Test RE 0.0013176997486852382\n",
      "189 Train Loss 0.046694044 Test MSE 0.026114280110589173 Test RE 0.0013533327536967177\n",
      "190 Train Loss 0.04669533 Test MSE 0.026114280110589173 Test RE 0.0013533327536967177\n",
      "191 Train Loss 0.046695262 Test MSE 0.027445079215932387 Test RE 0.0013873875983057042\n",
      "192 Train Loss 0.046653774 Test MSE 0.03168513585546797 Test RE 0.0014907106375032257\n",
      "193 Train Loss 0.046219844 Test MSE 0.05973433897449283 Test RE 0.002046810304894123\n",
      "194 Train Loss 0.04612958 Test MSE 0.05288555251201059 Test RE 0.0019259015678345516\n",
      "195 Train Loss 0.04609946 Test MSE 0.04909018022906508 Test RE 0.0018555081960685783\n",
      "196 Train Loss 0.04609554 Test MSE 0.046497307149328565 Test RE 0.0018058408153972735\n",
      "197 Train Loss 0.04606439 Test MSE 0.044218630099659706 Test RE 0.0017610358848107834\n",
      "198 Train Loss 0.046059407 Test MSE 0.0419661085254481 Test RE 0.0017155955593849425\n",
      "199 Train Loss 0.04604167 Test MSE 0.03946006895159091 Test RE 0.0016635830381065422\n",
      "Training time: 114.39\n",
      "Training time: 114.39\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 12.761958 Test MSE 14265.427098987622 Test RE 1.0002486021282821\n",
      "1 Train Loss 9.055853 Test MSE 14259.21026328813 Test RE 1.0000306255191296\n",
      "2 Train Loss 8.215074 Test MSE 14257.86307664099 Test RE 0.9999833837819286\n",
      "3 Train Loss 7.574825 Test MSE 14257.93680435588 Test RE 0.9999859692463182\n",
      "4 Train Loss 6.94596 Test MSE 14258.109075183638 Test RE 0.999992010368981\n",
      "5 Train Loss 6.5123568 Test MSE 14257.728804439863 Test RE 0.9999786751420214\n",
      "6 Train Loss 6.2066116 Test MSE 14257.194651749875 Test RE 0.9999599433261251\n",
      "7 Train Loss 5.873831 Test MSE 14256.58572712962 Test RE 0.9999385889592493\n",
      "8 Train Loss 5.4419503 Test MSE 14255.991099937644 Test RE 0.9999177355485256\n",
      "9 Train Loss 5.1451716 Test MSE 14255.437440549174 Test RE 0.9998983184763145\n",
      "10 Train Loss 4.741132 Test MSE 14255.745073954062 Test RE 0.9999091073583125\n",
      "11 Train Loss 4.3673224 Test MSE 14256.243613873032 Test RE 0.9999265911953414\n",
      "12 Train Loss 4.068162 Test MSE 14256.178110423449 Test RE 0.9999242940012818\n",
      "13 Train Loss 3.818654 Test MSE 14256.181235937798 Test RE 0.9999244036126215\n",
      "14 Train Loss 3.5775506 Test MSE 14256.205778132082 Test RE 0.9999252643034336\n",
      "15 Train Loss 3.3550568 Test MSE 14256.220513368657 Test RE 0.9999257810654583\n",
      "16 Train Loss 3.1474848 Test MSE 14256.229494172874 Test RE 0.9999260960204718\n",
      "17 Train Loss 2.955193 Test MSE 14256.239639418382 Test RE 0.9999264518120904\n",
      "18 Train Loss 2.7759895 Test MSE 14256.24867104755 Test RE 0.9999267685493125\n",
      "19 Train Loss 2.6677947 Test MSE 14256.255051312493 Test RE 0.9999269923037216\n",
      "20 Train Loss 2.5089617 Test MSE 14256.25427990363 Test RE 0.9999269652505945\n",
      "21 Train Loss 2.417275 Test MSE 14256.268159215488 Test RE 0.9999274519946765\n",
      "22 Train Loss 2.3173373 Test MSE 14256.260099573956 Test RE 0.9999271693450423\n",
      "23 Train Loss 2.244416 Test MSE 14256.268713586209 Test RE 0.9999274714363175\n",
      "24 Train Loss 2.2021208 Test MSE 14256.268403939153 Test RE 0.9999274605770738\n",
      "25 Train Loss 2.1276948 Test MSE 14256.264226731928 Test RE 0.9999273140834612\n",
      "26 Train Loss 2.0984278 Test MSE 14256.273096090485 Test RE 0.9999276251296217\n",
      "27 Train Loss 2.0096402 Test MSE 14256.27021396231 Test RE 0.9999275240541263\n",
      "28 Train Loss 2.0419137 Test MSE 14256.27089588595 Test RE 0.9999275479690144\n",
      "29 Train Loss 2.2696664 Test MSE 14256.266080715062 Test RE 0.9999273791021887\n",
      "30 Train Loss 3.05933 Test MSE 14256.297395646206 Test RE 0.9999284773083968\n",
      "31 Train Loss 3.1030746 Test MSE 14256.214692598476 Test RE 0.999925576932155\n",
      "32 Train Loss 3.0117362 Test MSE 14256.212019169026 Test RE 0.999925483175479\n",
      "33 Train Loss 3.0228586 Test MSE 14256.212019169026 Test RE 0.999925483175479\n",
      "34 Train Loss 2.9797924 Test MSE 14256.212019169026 Test RE 0.999925483175479\n",
      "35 Train Loss 2.9204853 Test MSE 14256.204136623917 Test RE 0.999925206736029\n",
      "36 Train Loss 2.8620908 Test MSE 14256.2045525072 Test RE 0.9999252213209835\n",
      "37 Train Loss 2.7467985 Test MSE 14256.213471225206 Test RE 0.9999255340988218\n",
      "38 Train Loss 2.6439555 Test MSE 14256.2365174769 Test RE 0.9999263423262751\n",
      "39 Train Loss 2.5341127 Test MSE 14256.259612420741 Test RE 0.9999271522606972\n",
      "40 Train Loss 2.4472795 Test MSE 14256.277490985496 Test RE 0.999927779257442\n",
      "41 Train Loss 2.4039247 Test MSE 14256.284051619821 Test RE 0.99992800933713\n",
      "42 Train Loss 2.3382816 Test MSE 14256.283915360393 Test RE 0.999928004558549\n",
      "43 Train Loss 2.3410661 Test MSE 14256.280719162001 Test RE 0.9999278924687338\n",
      "44 Train Loss 2.2429001 Test MSE 14256.27872777597 Test RE 0.9999278226313566\n",
      "45 Train Loss 2.261207 Test MSE 14256.265773056333 Test RE 0.9999273683126738\n",
      "46 Train Loss 2.1368766 Test MSE 14256.265773056333 Test RE 0.9999273683126738\n",
      "47 Train Loss 2.1911125 Test MSE 14256.241080289656 Test RE 0.9999265023431315\n",
      "48 Train Loss 2.1407738 Test MSE 14256.241080289656 Test RE 0.9999265023431315\n",
      "49 Train Loss 2.1642756 Test MSE 14256.241080289656 Test RE 0.9999265023431315\n",
      "50 Train Loss 1.9966499 Test MSE 14256.241080289656 Test RE 0.9999265023431315\n",
      "51 Train Loss 3.3635824 Test MSE 14256.241080289656 Test RE 0.9999265023431315\n",
      "52 Train Loss 8.146276 Test MSE 14256.241080289656 Test RE 0.9999265023431315\n",
      "53 Train Loss 14.86739 Test MSE 14250.401332996325 Test RE 0.9997216827079484\n",
      "54 Train Loss 14.687691 Test MSE 14241.733254357641 Test RE 0.9994175865591757\n",
      "55 Train Loss 13.413223 Test MSE 14248.201761733195 Test RE 0.9996445254443476\n",
      "56 Train Loss 12.715035 Test MSE 14251.505258080491 Test RE 0.9997604043005717\n",
      "57 Train Loss 11.815814 Test MSE 14254.112714897044 Test RE 0.9998518582490583\n",
      "58 Train Loss 10.77873 Test MSE 14255.365107658501 Test RE 0.9998957817029673\n",
      "59 Train Loss 9.886342 Test MSE 14255.794530480178 Test RE 0.9999108418165167\n",
      "60 Train Loss 8.990504 Test MSE 14255.912051593992 Test RE 0.9999149633122847\n",
      "61 Train Loss 8.366499 Test MSE 14255.559211215686 Test RE 0.9999025890581599\n",
      "62 Train Loss 7.724758 Test MSE 14255.24735572719 Test RE 0.9998916520331323\n",
      "63 Train Loss 7.1523466 Test MSE 14255.19478016706 Test RE 0.9998898081537931\n",
      "64 Train Loss 6.59212 Test MSE 14255.432978930767 Test RE 0.9998981620039117\n",
      "65 Train Loss 5.9983597 Test MSE 14255.661598530545 Test RE 0.9999061798382858\n",
      "66 Train Loss 5.490171 Test MSE 14255.282172992338 Test RE 0.999892873108805\n",
      "67 Train Loss 5.0812488 Test MSE 14254.58347284473 Test RE 0.9998683687233335\n",
      "68 Train Loss 4.6737895 Test MSE 14254.70688007819 Test RE 0.9998726968302326\n",
      "69 Train Loss 4.315761 Test MSE 14254.992245667067 Test RE 0.9998827050273618\n",
      "70 Train Loss 4.030765 Test MSE 14254.993282397654 Test RE 0.999882741386868\n",
      "71 Train Loss 3.7672927 Test MSE 14254.952526873332 Test RE 0.9998813120359834\n",
      "72 Train Loss 3.568155 Test MSE 14254.927935128602 Test RE 0.9998804495695343\n",
      "73 Train Loss 3.4089472 Test MSE 14254.930870190423 Test RE 0.9998805525062446\n",
      "74 Train Loss 3.2400954 Test MSE 14254.947284594895 Test RE 0.9998811281821016\n",
      "75 Train Loss 3.0967164 Test MSE 14254.972453057506 Test RE 0.9998820108742358\n",
      "76 Train Loss 2.955441 Test MSE 14254.994771721484 Test RE 0.999882793619413\n",
      "77 Train Loss 2.8900743 Test MSE 14255.011620711026 Test RE 0.9998833845354401\n",
      "78 Train Loss 2.796388 Test MSE 14255.017063871242 Test RE 0.9998835754340898\n",
      "79 Train Loss 2.7593496 Test MSE 14255.02314168681 Test RE 0.9998837885908785\n",
      "80 Train Loss 2.6798 Test MSE 14255.023081248803 Test RE 0.9998837864712404\n",
      "81 Train Loss 2.6340187 Test MSE 14255.024804176603 Test RE 0.9998838468965201\n",
      "82 Train Loss 2.58695 Test MSE 14255.02431072193 Test RE 0.9998838295904345\n",
      "83 Train Loss 2.5490072 Test MSE 14255.023393653457 Test RE 0.999883797427671\n",
      "84 Train Loss 2.4918962 Test MSE 14255.02224447943 Test RE 0.9998837571246675\n",
      "85 Train Loss 2.4900694 Test MSE 14255.022087564641 Test RE 0.999883751621465\n",
      "86 Train Loss 2.3072808 Test MSE 14255.019030339323 Test RE 0.9998836444006529\n",
      "87 Train Loss 3.4098952 Test MSE 14255.027694090071 Test RE 0.9998839482494706\n",
      "88 Train Loss 7.2714176 Test MSE 14255.070289075538 Test RE 0.9998854421087368\n",
      "89 Train Loss 14.86748 Test MSE 14254.93558556806 Test RE 0.999880717881091\n",
      "90 Train Loss 32.054127 Test MSE 14257.790946432879 Test RE 0.9999808543320239\n",
      "91 Train Loss 28.430475 Test MSE 14258.413005509316 Test RE 1.0000026683838745\n",
      "92 Train Loss 26.145771 Test MSE 14257.376956122107 Test RE 0.9999663364662708\n",
      "93 Train Loss 24.0404 Test MSE 14253.058171797176 Test RE 0.9998148722085679\n",
      "94 Train Loss 22.121998 Test MSE 14250.49346170994 Test RE 0.9997249142984693\n",
      "95 Train Loss 20.287058 Test MSE 14250.069120359041 Test RE 0.9997100296286888\n",
      "96 Train Loss 18.561865 Test MSE 14250.378437940348 Test RE 0.9997208796185283\n",
      "97 Train Loss 16.989367 Test MSE 14251.060051386885 Test RE 0.9997447882834513\n",
      "98 Train Loss 15.515963 Test MSE 14251.08528590598 Test RE 0.9997456734129676\n",
      "99 Train Loss 14.181877 Test MSE 14251.10708120048 Test RE 0.999746437907131\n",
      "100 Train Loss 12.948959 Test MSE 14251.548757648252 Test RE 0.9997619300731647\n",
      "101 Train Loss 11.834795 Test MSE 14252.069888556041 Test RE 0.9997802088612554\n",
      "102 Train Loss 10.811672 Test MSE 14252.380423442874 Test RE 0.9997911007859125\n",
      "103 Train Loss 9.870861 Test MSE 14252.706502939216 Test RE 0.9998025378057026\n",
      "104 Train Loss 9.009857 Test MSE 14252.95621912088 Test RE 0.9998112963449504\n",
      "105 Train Loss 8.229403 Test MSE 14253.12468783347 Test RE 0.9998172051691135\n",
      "106 Train Loss 7.5145288 Test MSE 14253.234044741224 Test RE 0.9998210407037962\n",
      "107 Train Loss 6.863802 Test MSE 14253.267304920142 Test RE 0.9998222072533383\n",
      "108 Train Loss 6.2665505 Test MSE 14253.181066342402 Test RE 0.9998191825652538\n",
      "109 Train Loss 5.7190843 Test MSE 14253.023915453205 Test RE 0.9998136707112354\n",
      "110 Train Loss 5.2195735 Test MSE 14252.799240542818 Test RE 0.9998057904910992\n",
      "111 Train Loss 4.79469 Test MSE 14252.538437031535 Test RE 0.9997966430229696\n",
      "112 Train Loss 4.409216 Test MSE 14252.295828860164 Test RE 0.999788133666247\n",
      "113 Train Loss 4.057301 Test MSE 14252.086304378437 Test RE 0.999780784644624\n",
      "114 Train Loss 3.7338808 Test MSE 14251.929202055093 Test RE 0.9997752742823659\n",
      "115 Train Loss 3.4696848 Test MSE 14251.83327869426 Test RE 0.9997719097566096\n",
      "116 Train Loss 3.2157066 Test MSE 14251.768952217379 Test RE 0.9997696534897859\n",
      "117 Train Loss 3.0338361 Test MSE 14251.716359884887 Test RE 0.9997678087971472\n",
      "118 Train Loss 2.843598 Test MSE 14251.67184857368 Test RE 0.999766247546218\n",
      "119 Train Loss 2.6862364 Test MSE 14251.622534120652 Test RE 0.999764517820442\n",
      "120 Train Loss 2.57639 Test MSE 14251.572174171426 Test RE 0.9997627514203455\n",
      "121 Train Loss 2.4484372 Test MSE 14251.52550636598 Test RE 0.9997611145212302\n",
      "122 Train Loss 2.3543794 Test MSE 14251.470219486893 Test RE 0.9997591752998661\n",
      "123 Train Loss 2.237211 Test MSE 14251.415136958216 Test RE 0.9997572432424782\n",
      "124 Train Loss 2.2229495 Test MSE 14251.354960703922 Test RE 0.9997551325149109\n",
      "125 Train Loss 2.1815517 Test MSE 14251.316487723063 Test RE 0.9997537830437152\n",
      "126 Train Loss 2.1060429 Test MSE 14251.273109069041 Test RE 0.9997522614998458\n",
      "127 Train Loss 2.091048 Test MSE 14251.216522356743 Test RE 0.9997502766684094\n",
      "128 Train Loss 2.05814 Test MSE 14251.166966026816 Test RE 0.9997485384309212\n",
      "129 Train Loss 2.0077772 Test MSE 14251.1150728078 Test RE 0.9997467182214878\n",
      "130 Train Loss 1.9569093 Test MSE 14251.053308878667 Test RE 0.9997445517821586\n",
      "131 Train Loss 1.993762 Test MSE 14250.997981651522 Test RE 0.99974261111341\n",
      "132 Train Loss 2.0652266 Test MSE 14250.965331186824 Test RE 0.9997414658574926\n",
      "133 Train Loss 2.169491 Test MSE 14250.960274469006 Test RE 0.9997412884866685\n",
      "134 Train Loss 2.2234044 Test MSE 14250.988064215608 Test RE 0.9997422632470703\n",
      "135 Train Loss 2.2390764 Test MSE 14250.999654774268 Test RE 0.9997426698002492\n",
      "136 Train Loss 2.281433 Test MSE 14250.996772156306 Test RE 0.9997425686888747\n",
      "137 Train Loss 2.3152542 Test MSE 14250.995579841494 Test RE 0.9997425268669613\n",
      "138 Train Loss 2.3157246 Test MSE 14250.994487098842 Test RE 0.999742488537663\n",
      "139 Train Loss 2.3352005 Test MSE 14250.99092588015 Test RE 0.9997423636235084\n",
      "140 Train Loss 2.3474097 Test MSE 14250.983812265691 Test RE 0.9997421141046273\n",
      "141 Train Loss 2.399556 Test MSE 14250.979657787777 Test RE 0.9997419683811148\n",
      "142 Train Loss 2.4117484 Test MSE 14250.978934581393 Test RE 0.9997419430137435\n",
      "143 Train Loss 2.4452345 Test MSE 14250.977838629857 Test RE 0.9997419045718674\n",
      "144 Train Loss 2.402556 Test MSE 14250.978370722805 Test RE 0.999741923235696\n",
      "145 Train Loss 2.25639 Test MSE 14250.978370722805 Test RE 0.999741923235696\n",
      "146 Train Loss 3.0944014 Test MSE 14250.960941089252 Test RE 0.9997413118692252\n",
      "147 Train Loss 5.758958 Test MSE 14250.87048504345 Test RE 0.9997381390028558\n",
      "148 Train Loss 11.622561 Test MSE 14251.34411427015 Test RE 0.9997547520675804\n",
      "149 Train Loss 16.750994 Test MSE 14252.042693541965 Test RE 0.9997792549980612\n",
      "150 Train Loss 15.2026005 Test MSE 14251.309438698743 Test RE 0.9997535357932379\n",
      "151 Train Loss 14.034038 Test MSE 14250.892786895522 Test RE 0.9997389212709773\n",
      "152 Train Loss 12.916059 Test MSE 14249.583641288742 Test RE 0.9996930001523608\n",
      "153 Train Loss 11.919706 Test MSE 14246.89246857591 Test RE 0.9995985948129671\n",
      "154 Train Loss 10.906875 Test MSE 14242.676000042264 Test RE 0.9994506647319005\n",
      "155 Train Loss 9.965709 Test MSE 14236.659621809311 Test RE 0.9992395489146351\n",
      "156 Train Loss 9.081352 Test MSE 14234.009983760188 Test RE 0.9991465584800612\n",
      "157 Train Loss 8.268563 Test MSE 14233.690113553695 Test RE 0.9991353318822197\n",
      "158 Train Loss 7.519049 Test MSE 14233.04589619312 Test RE 0.9991127211746512\n",
      "159 Train Loss 6.8950124 Test MSE 14233.004344194509 Test RE 0.9991112627671889\n",
      "160 Train Loss 6.296892 Test MSE 14231.702182424535 Test RE 0.9990655579315447\n",
      "161 Train Loss 5.7381625 Test MSE 14230.583497650414 Test RE 0.9990262913217747\n",
      "162 Train Loss 5.266004 Test MSE 14230.729091769128 Test RE 0.9990314018634632\n",
      "163 Train Loss 4.9323673 Test MSE 14231.071990203212 Test RE 0.9990434379382673\n",
      "164 Train Loss 4.7551837 Test MSE 14232.090350003034 Test RE 0.9990791825215312\n",
      "165 Train Loss 4.7648354 Test MSE 14235.065050919971 Test RE 0.9991835876513592\n",
      "166 Train Loss 4.462694 Test MSE 14239.357357615228 Test RE 0.999334218475147\n",
      "167 Train Loss 4.1559763 Test MSE 14234.748365111987 Test RE 0.999172473230015\n",
      "168 Train Loss 4.0222197 Test MSE 14234.808597888821 Test RE 0.9991745871721583\n",
      "169 Train Loss 4.0598536 Test MSE 14235.396334253503 Test RE 0.9991952142558991\n",
      "170 Train Loss 4.107021 Test MSE 14233.299867596383 Test RE 0.999121635110914\n",
      "171 Train Loss 3.9192188 Test MSE 14219.566491315767 Test RE 0.9986395043002921\n",
      "172 Train Loss 4.034546 Test MSE 14212.52966033308 Test RE 0.9983923755633555\n",
      "173 Train Loss 4.0238905 Test MSE 14191.895391050568 Test RE 0.9976673610723197\n",
      "174 Train Loss 3.9807456 Test MSE 14171.806331762938 Test RE 0.9969609967769534\n",
      "175 Train Loss 3.8465374 Test MSE 14151.697921640838 Test RE 0.996253450523857\n",
      "176 Train Loss 3.691061 Test MSE 14142.057147865744 Test RE 0.9959140463572869\n",
      "177 Train Loss 3.4536111 Test MSE 14143.565512531479 Test RE 0.995967156081303\n",
      "178 Train Loss 3.866671 Test MSE 14139.676617033827 Test RE 0.9958302217819016\n",
      "179 Train Loss 4.4228497 Test MSE 14122.574892848754 Test RE 0.9952278188107208\n",
      "180 Train Loss 5.5418763 Test MSE 14090.186777312865 Test RE 0.9940859570817625\n",
      "181 Train Loss 5.285922 Test MSE 14049.454623172516 Test RE 0.9926480567665908\n",
      "182 Train Loss 5.934001 Test MSE 14034.07230045984 Test RE 0.9921044977849894\n",
      "183 Train Loss 6.51913 Test MSE 14014.109336170377 Test RE 0.9913986301526653\n",
      "184 Train Loss 11.159995 Test MSE 13968.144948793612 Test RE 0.9897714679814222\n",
      "185 Train Loss 10.24058 Test MSE 13874.043740424477 Test RE 0.9864318660761053\n",
      "186 Train Loss 9.783764 Test MSE 13855.893529637908 Test RE 0.9857864231897298\n",
      "187 Train Loss 12.422167 Test MSE 13780.285536099052 Test RE 0.983093154475089\n",
      "188 Train Loss 13.147085 Test MSE 13663.891342202576 Test RE 0.9789325373294644\n",
      "189 Train Loss 15.83651 Test MSE 13505.409501142223 Test RE 0.9732388621223186\n",
      "190 Train Loss 14.607204 Test MSE 13350.468550146466 Test RE 0.9676400110949809\n",
      "191 Train Loss 13.957798 Test MSE 13307.99606985789 Test RE 0.9660995853768553\n",
      "192 Train Loss 13.854068 Test MSE 13104.508191600898 Test RE 0.958684988539508\n",
      "193 Train Loss 14.304962 Test MSE 13066.860276873176 Test RE 0.9573068959783378\n",
      "194 Train Loss 14.180833 Test MSE 12928.66666878235 Test RE 0.9522312567717174\n",
      "195 Train Loss 16.821182 Test MSE 12703.801559320944 Test RE 0.9439139713613067\n",
      "196 Train Loss 15.938847 Test MSE 12639.602019714215 Test RE 0.9415258831862606\n",
      "197 Train Loss 14.923595 Test MSE 12584.413496054889 Test RE 0.9394681337573892\n",
      "198 Train Loss 15.51668 Test MSE 12532.161954267538 Test RE 0.9375157297147745\n",
      "199 Train Loss 15.110193 Test MSE 12422.862750088701 Test RE 0.933418506685834\n",
      "Training time: 103.67\n",
      "Training time: 103.67\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 22.697374 Test MSE 14263.98161969372 Test RE 1.0001979245263868\n",
      "1 Train Loss 17.687243 Test MSE 14258.597825718281 Test RE 1.0000091494734302\n",
      "2 Train Loss 16.322681 Test MSE 14256.574799228534 Test RE 0.9999382057247114\n",
      "3 Train Loss 15.130561 Test MSE 14256.94112769509 Test RE 0.9999510525508827\n",
      "4 Train Loss 13.997632 Test MSE 14257.614599790271 Test RE 0.9999746702109259\n",
      "5 Train Loss 12.950508 Test MSE 14257.494649745871 Test RE 0.9999704637829313\n",
      "6 Train Loss 11.905543 Test MSE 14257.228253817651 Test RE 0.9999611217031764\n",
      "7 Train Loss 10.888195 Test MSE 14256.596547506935 Test RE 0.9999389684228523\n",
      "8 Train Loss 9.913387 Test MSE 14255.564752361264 Test RE 0.9999027833895501\n",
      "9 Train Loss 9.1176815 Test MSE 14254.536684452201 Test RE 0.9998667277679107\n",
      "10 Train Loss 8.370309 Test MSE 14254.063266113457 Test RE 0.9998501239600838\n",
      "11 Train Loss 7.6429133 Test MSE 14254.05462116842 Test RE 0.9998498207605501\n",
      "12 Train Loss 6.967295 Test MSE 14254.429451971997 Test RE 0.9998629669164318\n",
      "13 Train Loss 6.3473577 Test MSE 14255.058307894846 Test RE 0.9998850219140126\n",
      "14 Train Loss 5.8407536 Test MSE 14255.48214152809 Test RE 0.9998998861728092\n",
      "15 Train Loss 5.3328824 Test MSE 14255.475304826174 Test RE 0.9998996464047599\n",
      "16 Train Loss 4.8411965 Test MSE 14255.209616931477 Test RE 0.9998903284948298\n",
      "17 Train Loss 4.466635 Test MSE 14254.964966707694 Test RE 0.999881748317871\n",
      "18 Train Loss 4.098317 Test MSE 14254.86550791332 Test RE 0.999878260157539\n",
      "19 Train Loss 3.812355 Test MSE 14254.896236759643 Test RE 0.9998793378628607\n",
      "20 Train Loss 3.5389318 Test MSE 14254.851909320378 Test RE 0.9998777832347406\n",
      "21 Train Loss 3.2880607 Test MSE 14254.799012571322 Test RE 0.9998759280652312\n",
      "22 Train Loss 3.1013508 Test MSE 14254.732654682735 Test RE 0.9998736007882\n",
      "23 Train Loss 2.9100807 Test MSE 14254.6695036801 Test RE 0.9998713859767453\n",
      "24 Train Loss 2.7311718 Test MSE 14254.626985708235 Test RE 0.9998698947974811\n",
      "25 Train Loss 2.6176624 Test MSE 14254.609402668662 Test RE 0.999869278129035\n",
      "26 Train Loss 2.4862578 Test MSE 14254.581445878148 Test RE 0.999868297633924\n",
      "27 Train Loss 2.3631155 Test MSE 14254.56979679727 Test RE 0.9998678890793448\n",
      "28 Train Loss 2.3103123 Test MSE 14254.558460287864 Test RE 0.999867491487059\n",
      "29 Train Loss 2.250819 Test MSE 14254.530462576256 Test RE 0.999866509555016\n",
      "30 Train Loss 2.1749797 Test MSE 14254.506098419604 Test RE 0.9998656550577478\n",
      "31 Train Loss 2.1433825 Test MSE 14254.48746739519 Test RE 0.999865001631828\n",
      "32 Train Loss 2.1294086 Test MSE 14254.4676075503 Test RE 0.9998643051083366\n",
      "33 Train Loss 2.1284184 Test MSE 14254.451751322053 Test RE 0.9998637489991404\n",
      "34 Train Loss 2.1171465 Test MSE 14254.439774930983 Test RE 0.9998633289632806\n",
      "35 Train Loss 2.1330674 Test MSE 14254.430294752656 Test RE 0.9998629964744427\n",
      "36 Train Loss 2.1716096 Test MSE 14254.424237019555 Test RE 0.9998627840175542\n",
      "37 Train Loss 2.138947 Test MSE 14254.429500071856 Test RE 0.9998629686033904\n",
      "38 Train Loss 2.118643 Test MSE 14254.42583499081 Test RE 0.9998628400616254\n",
      "39 Train Loss 2.1155732 Test MSE 14254.41759984 Test RE 0.9998625512382612\n",
      "40 Train Loss 2.1322658 Test MSE 14254.409895023804 Test RE 0.9998622810147262\n",
      "41 Train Loss 2.164377 Test MSE 14254.406861588908 Test RE 0.9998621746259905\n",
      "42 Train Loss 2.1965914 Test MSE 14254.410316219451 Test RE 0.9998622957869138\n",
      "43 Train Loss 2.1863005 Test MSE 14254.419954141242 Test RE 0.9998626338083672\n",
      "44 Train Loss 2.0125644 Test MSE 14254.425879912493 Test RE 0.9998628416371198\n",
      "45 Train Loss 1.9616529 Test MSE 14254.37529301193 Test RE 0.9998610674510721\n",
      "46 Train Loss 1.9478455 Test MSE 14254.360858302789 Test RE 0.9998605611957276\n",
      "47 Train Loss 1.9721798 Test MSE 14254.35723162256 Test RE 0.9998604340004509\n",
      "48 Train Loss 1.9763533 Test MSE 14254.355299271067 Test RE 0.9998603662288269\n",
      "49 Train Loss 1.9589041 Test MSE 14254.351749858903 Test RE 0.9998602417434789\n",
      "50 Train Loss 1.8929276 Test MSE 14254.351749858903 Test RE 0.9998602417434789\n",
      "51 Train Loss 2.0711133 Test MSE 14254.347141274104 Test RE 0.9998600801106958\n",
      "52 Train Loss 2.7980747 Test MSE 14254.347141274104 Test RE 0.9998600801106958\n",
      "53 Train Loss 2.8009439 Test MSE 14254.605422048897 Test RE 0.9998691385215724\n",
      "54 Train Loss 2.6915514 Test MSE 14254.611165154012 Test RE 0.9998693399425459\n",
      "55 Train Loss 2.8626463 Test MSE 14254.609331272412 Test RE 0.9998692756250406\n",
      "56 Train Loss 3.0874705 Test MSE 14254.604587060867 Test RE 0.9998691092370452\n",
      "57 Train Loss 4.2613454 Test MSE 14254.604587060867 Test RE 0.9998691092370452\n",
      "58 Train Loss 6.8984213 Test MSE 14254.690999898461 Test RE 0.9998721398856848\n",
      "59 Train Loss 6.993937 Test MSE 14255.024661953314 Test RE 0.9998838419085676\n",
      "60 Train Loss 6.518285 Test MSE 14254.752558751372 Test RE 0.9998742988562138\n",
      "61 Train Loss 6.033546 Test MSE 14254.10972814973 Test RE 0.999851753496659\n",
      "62 Train Loss 5.536781 Test MSE 14253.737551642422 Test RE 0.9998387002871072\n",
      "63 Train Loss 5.0538287 Test MSE 14253.15978797896 Test RE 0.999818436257332\n",
      "64 Train Loss 4.6803026 Test MSE 14252.624485066981 Test RE 0.9997996610961192\n",
      "65 Train Loss 4.302631 Test MSE 14252.405235792958 Test RE 0.9997919710670609\n",
      "66 Train Loss 4.010669 Test MSE 14252.22189792304 Test RE 0.9997855405622873\n",
      "67 Train Loss 3.7265682 Test MSE 14252.016716886095 Test RE 0.9997783438678448\n",
      "68 Train Loss 3.5029087 Test MSE 14251.779623210023 Test RE 0.9997700277778006\n",
      "69 Train Loss 3.27123 Test MSE 14251.585183988382 Test RE 0.9997632077464009\n",
      "70 Train Loss 3.062718 Test MSE 14251.407106464876 Test RE 0.9997569615671169\n",
      "71 Train Loss 2.9220736 Test MSE 14251.267395932153 Test RE 0.9997520611064477\n",
      "72 Train Loss 2.7570338 Test MSE 14251.168148154062 Test RE 0.9997485798952436\n",
      "73 Train Loss 2.630755 Test MSE 14251.053819393734 Test RE 0.9997445696890683\n",
      "74 Train Loss 2.5479143 Test MSE 14250.959166858638 Test RE 0.9997412496358185\n",
      "75 Train Loss 2.488211 Test MSE 14250.894071452294 Test RE 0.9997389663285533\n",
      "76 Train Loss 2.4676306 Test MSE 14250.849655735243 Test RE 0.9997374083856824\n",
      "77 Train Loss 2.4346392 Test MSE 14250.829673641325 Test RE 0.9997367074852354\n",
      "78 Train Loss 2.3691537 Test MSE 14250.8014191087 Test RE 0.9997357164163617\n",
      "79 Train Loss 2.3658843 Test MSE 14250.756181190229 Test RE 0.9997341296284546\n",
      "80 Train Loss 2.3546383 Test MSE 14250.706702434049 Test RE 0.999732394083914\n",
      "81 Train Loss 2.2565608 Test MSE 14250.706702434049 Test RE 0.999732394083914\n",
      "82 Train Loss 2.5827694 Test MSE 14250.706702434049 Test RE 0.999732394083914\n",
      "83 Train Loss 3.7669835 Test MSE 14250.706702434049 Test RE 0.999732394083914\n",
      "84 Train Loss 8.401677 Test MSE 14250.706702434049 Test RE 0.999732394083914\n",
      "85 Train Loss 16.1661 Test MSE 14249.912095228618 Test RE 0.9997045215841694\n",
      "86 Train Loss 19.852768 Test MSE 14246.933079539762 Test RE 0.9996000194967556\n",
      "87 Train Loss 18.31552 Test MSE 14248.883279403723 Test RE 0.9996684325756913\n",
      "88 Train Loss 17.480127 Test MSE 14247.311720344407 Test RE 0.999613302595611\n",
      "89 Train Loss 16.093544 Test MSE 14247.710703303264 Test RE 0.9996272991266351\n",
      "90 Train Loss 14.747352 Test MSE 14248.599117533435 Test RE 0.9996584644588156\n",
      "91 Train Loss 13.523071 Test MSE 14248.884456164822 Test RE 0.9996684738550985\n",
      "92 Train Loss 12.34766 Test MSE 14248.29351008167 Test RE 0.9996477439412553\n",
      "93 Train Loss 11.30333 Test MSE 14246.334486571732 Test RE 0.9995790198954958\n",
      "94 Train Loss 10.301411 Test MSE 14243.171732340521 Test RE 0.9994680580807287\n",
      "95 Train Loss 9.521967 Test MSE 14240.068038093834 Test RE 0.9993591563442348\n",
      "96 Train Loss 8.669126 Test MSE 14239.987631053367 Test RE 0.9993563348820992\n",
      "97 Train Loss 7.9216065 Test MSE 14240.498607878642 Test RE 0.9993742647904565\n",
      "98 Train Loss 7.20963 Test MSE 14239.825663722855 Test RE 0.9993506514664162\n",
      "99 Train Loss 6.563339 Test MSE 14239.361508521552 Test RE 0.9993343641327888\n",
      "100 Train Loss 5.9606733 Test MSE 14238.934616915696 Test RE 0.999319384154625\n",
      "101 Train Loss 5.4184337 Test MSE 14238.380757730825 Test RE 0.9992999484448333\n",
      "102 Train Loss 4.93532 Test MSE 14237.691428860253 Test RE 0.9992757583822799\n",
      "103 Train Loss 4.5572324 Test MSE 14236.850866565084 Test RE 0.9992462604146217\n",
      "104 Train Loss 4.399903 Test MSE 14235.635802305662 Test RE 0.9992036184590487\n",
      "105 Train Loss 4.631438 Test MSE 14225.254815994422 Test RE 0.9988392297190011\n",
      "106 Train Loss 4.4311285 Test MSE 14220.386388938585 Test RE 0.9986682945715626\n",
      "107 Train Loss 5.033807 Test MSE 14223.14888367698 Test RE 0.9987652920040778\n",
      "108 Train Loss 5.1670275 Test MSE 14217.974962184508 Test RE 0.998583616227379\n",
      "109 Train Loss 6.154824 Test MSE 14198.171011803704 Test RE 0.9978879197020412\n",
      "110 Train Loss 6.0169296 Test MSE 14164.896949768747 Test RE 0.9967179358713261\n",
      "111 Train Loss 5.6030087 Test MSE 14149.59638821895 Test RE 0.9961794757376625\n",
      "112 Train Loss 5.7582707 Test MSE 14141.264881391911 Test RE 0.9958861494129967\n",
      "113 Train Loss 6.2255397 Test MSE 14110.493220576484 Test RE 0.994802025740729\n",
      "114 Train Loss 8.468992 Test MSE 14045.01907497039 Test RE 0.9924913501189303\n",
      "115 Train Loss 8.713837 Test MSE 13983.482781329243 Test RE 0.9903147321004627\n",
      "116 Train Loss 8.350355 Test MSE 13946.616112920043 Test RE 0.989008415888833\n",
      "117 Train Loss 10.01536 Test MSE 13889.560034469217 Test RE 0.9869833091230884\n",
      "118 Train Loss 10.141247 Test MSE 13802.106092303453 Test RE 0.983871191843358\n",
      "119 Train Loss 10.087705 Test MSE 13775.6567389573 Test RE 0.982928030139776\n",
      "120 Train Loss 11.346738 Test MSE 13639.37606522846 Test RE 0.9780539597774651\n",
      "121 Train Loss nan Test MSE nan Test RE nan\n",
      "122 Train Loss nan Test MSE nan Test RE nan\n",
      "123 Train Loss nan Test MSE nan Test RE nan\n",
      "124 Train Loss nan Test MSE nan Test RE nan\n",
      "125 Train Loss nan Test MSE nan Test RE nan\n",
      "126 Train Loss nan Test MSE nan Test RE nan\n",
      "127 Train Loss nan Test MSE nan Test RE nan\n",
      "128 Train Loss nan Test MSE nan Test RE nan\n",
      "129 Train Loss nan Test MSE nan Test RE nan\n",
      "130 Train Loss nan Test MSE nan Test RE nan\n",
      "131 Train Loss nan Test MSE nan Test RE nan\n",
      "132 Train Loss nan Test MSE nan Test RE nan\n",
      "133 Train Loss nan Test MSE nan Test RE nan\n",
      "134 Train Loss nan Test MSE nan Test RE nan\n",
      "135 Train Loss nan Test MSE nan Test RE nan\n",
      "136 Train Loss nan Test MSE nan Test RE nan\n",
      "137 Train Loss nan Test MSE nan Test RE nan\n",
      "138 Train Loss nan Test MSE nan Test RE nan\n",
      "139 Train Loss nan Test MSE nan Test RE nan\n",
      "140 Train Loss nan Test MSE nan Test RE nan\n",
      "141 Train Loss nan Test MSE nan Test RE nan\n",
      "142 Train Loss nan Test MSE nan Test RE nan\n",
      "143 Train Loss nan Test MSE nan Test RE nan\n",
      "144 Train Loss nan Test MSE nan Test RE nan\n",
      "145 Train Loss nan Test MSE nan Test RE nan\n",
      "146 Train Loss nan Test MSE nan Test RE nan\n",
      "147 Train Loss nan Test MSE nan Test RE nan\n",
      "148 Train Loss nan Test MSE nan Test RE nan\n",
      "149 Train Loss nan Test MSE nan Test RE nan\n",
      "150 Train Loss nan Test MSE nan Test RE nan\n",
      "151 Train Loss nan Test MSE nan Test RE nan\n",
      "152 Train Loss nan Test MSE nan Test RE nan\n",
      "153 Train Loss nan Test MSE nan Test RE nan\n",
      "154 Train Loss nan Test MSE nan Test RE nan\n",
      "155 Train Loss nan Test MSE nan Test RE nan\n",
      "156 Train Loss nan Test MSE nan Test RE nan\n",
      "157 Train Loss nan Test MSE nan Test RE nan\n",
      "158 Train Loss nan Test MSE nan Test RE nan\n",
      "159 Train Loss nan Test MSE nan Test RE nan\n",
      "160 Train Loss nan Test MSE nan Test RE nan\n",
      "161 Train Loss nan Test MSE nan Test RE nan\n",
      "162 Train Loss nan Test MSE nan Test RE nan\n",
      "163 Train Loss nan Test MSE nan Test RE nan\n",
      "164 Train Loss nan Test MSE nan Test RE nan\n",
      "165 Train Loss nan Test MSE nan Test RE nan\n",
      "166 Train Loss nan Test MSE nan Test RE nan\n",
      "167 Train Loss nan Test MSE nan Test RE nan\n",
      "168 Train Loss nan Test MSE nan Test RE nan\n",
      "169 Train Loss nan Test MSE nan Test RE nan\n",
      "170 Train Loss nan Test MSE nan Test RE nan\n",
      "171 Train Loss nan Test MSE nan Test RE nan\n",
      "172 Train Loss nan Test MSE nan Test RE nan\n",
      "173 Train Loss nan Test MSE nan Test RE nan\n",
      "174 Train Loss nan Test MSE nan Test RE nan\n",
      "175 Train Loss nan Test MSE nan Test RE nan\n",
      "176 Train Loss nan Test MSE nan Test RE nan\n",
      "177 Train Loss nan Test MSE nan Test RE nan\n",
      "178 Train Loss nan Test MSE nan Test RE nan\n",
      "179 Train Loss nan Test MSE nan Test RE nan\n",
      "180 Train Loss nan Test MSE nan Test RE nan\n",
      "181 Train Loss nan Test MSE nan Test RE nan\n",
      "182 Train Loss nan Test MSE nan Test RE nan\n",
      "183 Train Loss nan Test MSE nan Test RE nan\n",
      "184 Train Loss nan Test MSE nan Test RE nan\n",
      "185 Train Loss nan Test MSE nan Test RE nan\n",
      "186 Train Loss nan Test MSE nan Test RE nan\n",
      "187 Train Loss nan Test MSE nan Test RE nan\n",
      "188 Train Loss nan Test MSE nan Test RE nan\n",
      "189 Train Loss nan Test MSE nan Test RE nan\n",
      "190 Train Loss nan Test MSE nan Test RE nan\n",
      "191 Train Loss nan Test MSE nan Test RE nan\n",
      "192 Train Loss nan Test MSE nan Test RE nan\n",
      "193 Train Loss nan Test MSE nan Test RE nan\n",
      "194 Train Loss nan Test MSE nan Test RE nan\n",
      "195 Train Loss nan Test MSE nan Test RE nan\n",
      "196 Train Loss nan Test MSE nan Test RE nan\n",
      "197 Train Loss nan Test MSE nan Test RE nan\n",
      "198 Train Loss nan Test MSE nan Test RE nan\n",
      "199 Train Loss nan Test MSE nan Test RE nan\n",
      "Training time: 158.89\n",
      "Training time: 158.89\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 200\n",
    "\n",
    "N_f = 1000\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "beta_full = []\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss =[]\n",
    "    \n",
    "    'Generate Training data'\n",
    "    torch.manual_seed(reps*36)\n",
    "     #Total number of collocation points \n",
    "    \n",
    "    \n",
    "    layers = np.array([1,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "    PINN = Sequentialmodel(layers)\n",
    "    PINN.to(device)\n",
    "    beta_val = []\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "    \n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.25, \n",
    "                              max_iter = 10, \n",
    "                              max_eval = 15, \n",
    "                              tolerance_grad = 1e-5, \n",
    "                              tolerance_change = 1e-5, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "   \n",
    "\n",
    "    start_time = time.time()\n",
    "    train_model(max_iter,reps)\n",
    "\n",
    "    \n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    beta_full.append(beta_val)  \n",
    " \n",
    "    \n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full, \"test_re_loss\": test_re_full, \"Time\": elapsed_time, \"beta\": beta_full, \"label\": label, \"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f72b0422650>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "u_pred = PINN.test()\n",
    "plt.plot(u_pred,'r')\n",
    "plt.plot(y_true,'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n",
      "0.9998551164919097\n",
      "0.0010114845963037334\n",
      "3.46234284668381e-05\n",
      "nan\n",
      "0.0016837534337045773\n",
      "0.0007768732881567005\n",
      "0.0016635830381065422\n",
      "0.933418506685834\n",
      "nan\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "nan"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + test_re_full[i][-1]\n",
    "    print(test_re_full[i][-1])\n",
    "(a/10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan],\n",
       "        [nan]], device='cuda:0')"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "PINN.forward(x_test_tensor).detach()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
