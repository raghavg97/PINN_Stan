{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_1D_1(x): #True function for 1D_1 dy2/dx2 + dy/dx - 6y = 0; BC1: y(0)=2; BC2: dy/dx at (x=0) = -1;\n",
    "    y = np.exp(3.0*x) + np.exp(-4.0*x)\n",
    "    return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "level = \"high\"\n",
    "label = \"1D_SODE_stanAW_\" + level\n",
    "\n",
    "u_coeff = 12.0\n",
    "fo_val = -1.0\n",
    "\n",
    "loss_thresh = 0.005\n",
    "\n",
    "x = np.linspace(0,2,100).reshape(-1,1)\n",
    "\n",
    "bc1_x = x[0].reshape(-1,1)\n",
    "bc1_y = true_1D_1(x[0]).reshape(-1,1)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "y_bc1_train = torch.from_numpy(bc1_y).float().to(device)    \n",
    "\n",
    "bc2_x = x[0].reshape(-1,1)\n",
    "x_bc2_train = torch.from_numpy(bc2_x).float().to(device)\n",
    "bc2_val = torch.tensor(fo_val,device=device)\n",
    "bc2_val = bc2_val.view(1,1)\n",
    "\n",
    "\n",
    "x_test = x.reshape(-1,1)\n",
    "x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "y_true = true_1D_1(x_test)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(x[0]) \n",
    "ub = np.array(x[-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    #Collocation Points\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,y)\n",
    "    x01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    \n",
    "    x_coll_train = lb + (ub-lb)*sampling(N_f)\n",
    "    x_coll_train = np.vstack((x_coll_train, bc1_x.reshape(-1,1))) # append training points to collocation points \n",
    "\n",
    "    return x_coll_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        \n",
    "        \n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "            \n",
    "        self.m_lambda = nn.Sigmoid()    \n",
    "        self.lambdas_bc1 = Parameter(torch.ones(1,1))\n",
    "        self.lambdas_bc1.requiresGrad = True\n",
    "        \n",
    "        self.lambdas_bc2 = Parameter(torch.ones(1,1))\n",
    "        self.lambdas_bc2.requiresGrad = True\n",
    "        \n",
    "        self.lambdas_f = Parameter(torch.ones(N_f+1,1))\n",
    "        self.lambdas_f.requiresGrad = True\n",
    "        \n",
    "        self.beta = Parameter(torch.ones((50,len(layers)-2)))\n",
    "        self.beta.requiresGrad = True\n",
    "             \n",
    "\n",
    "    'forward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = 2.0*(x - l_b)/(u_b - l_b) - 1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z) + self.beta[:,i]*z*self.activation(z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "\n",
    "                        \n",
    "    def loss_BC1(self,x,y,lambda_ind):\n",
    "        \n",
    "        m = self.m_lambda(self.lambdas_bc1)\n",
    "        u_pred = self.forward(x)\n",
    "        \n",
    "        if(lambda_ind):\n",
    "            u_pred = u_pred.detach()\n",
    "        else:\n",
    "            m = m.detach()\n",
    "            \n",
    "        loss_bc1 = torch.sum(m*torch.square(u_pred - y))/2.0\n",
    "        \n",
    "        # loss_bc1 = self.loss_function(self.forward(x), y)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,x_bc2,bc2_val,lambda_ind):\n",
    "        g = x_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        y = self.forward(g)    \n",
    "            \n",
    "        y_x = autograd.grad(y,g,torch.ones([x_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        bc2 = dy_dx\n",
    "        \n",
    "        m = self.m_lambda(self.lambdas_bc2)\n",
    "        \n",
    "        if(lambda_ind):\n",
    "            bc2 = bc2.detach()\n",
    "        else:\n",
    "            m = m.detach()\n",
    "        \n",
    "        loss_bc2 = torch.sum(m*torch.square(bc2 - bc2_val))/2.0\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self, x_coll,f_hat,lambda_ind):\n",
    "             \n",
    "        g = x_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        y = self.forward(g) \n",
    "\n",
    "        y_x = autograd.grad(y,g,torch.ones([x_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        y_xx = autograd.grad(y_x,g,torch.ones(x_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        dy2_d2x = y_xx[:,[0]]\n",
    "        \n",
    "        f = dy2_d2x + dy_dx - u_coeff*y\n",
    "\n",
    "        m = self.m_lambda(self.lambdas_f)\n",
    "        \n",
    "        if(lambda_ind):\n",
    "            f = f.detach()\n",
    "        else:\n",
    "            m = m.detach()\n",
    "        \n",
    "        #loss_f  = torch.sum(m*(torch.square(f)))/2.0\n",
    "        loss_f = self.loss_function(m*(torch.square(f)),f_hat)/2.0\n",
    "        \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,x_bc1,y_bc1,x_bc2,bc2_val,x_coll,f_hat):\n",
    "\n",
    "        lambda_ind = False\n",
    "        \n",
    "        loss_bc1 = self.loss_BC1(x_bc1,y_bc1,lambda_ind)\n",
    "        loss_bc2 = self.loss_BC2(x_bc2,bc2_val,lambda_ind)\n",
    "        loss_f = self.loss_PDE(x_coll,f_hat,lambda_ind)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "    \n",
    "    def loss_lambdas(self,x_bc1,y_bc1,x_bc2,bc2_val,x_coll,f_hat):\n",
    "\n",
    "        lambda_ind = True        \n",
    "        loss_bc1 = self.loss_BC1(x_bc1,y_bc1,lambda_ind)\n",
    "        loss_bc2 = self.loss_BC2(x_bc2,bc2_val,lambda_ind)\n",
    "        loss_f = self.loss_PDE(x_coll,f_hat,lambda_ind)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return -1.0*loss_val\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self):\n",
    "        y_pred = self.forward(x_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_coll,f_hat):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    optimizer.step(closure)\n",
    "    \n",
    "    for i in range(20):\n",
    "        optimizer_lambda.zero_grad()\n",
    "        loss = PINN.loss_lambdas(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat)\n",
    "        loss.backward()\n",
    "        optimizer_lambda.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        x_coll = torch.from_numpy(colloc_pts(N_f,i*11)).float().to(device)\n",
    "        f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "        train_step(x_coll,f_hat)\n",
    "        \n",
    "        loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      "  (m_lambda): Sigmoid()\n",
      ")\n",
      "0\n",
      "0 Train Loss 3.0134203 Test MSE 14268.07260904286 Test RE 1.0003413454105319\n",
      "1 Train Loss 1.7086726 Test MSE 14258.61211731196 Test RE 1.0000096506349412\n",
      "2 Train Loss 1.2296883 Test MSE 14249.0213087375 Test RE 0.9996732744721012\n",
      "3 Train Loss 1.1087238 Test MSE 14254.259767614538 Test RE 0.9998570157273445\n",
      "4 Train Loss 1.1179152 Test MSE 14253.258237905631 Test RE 0.999821889241906\n",
      "5 Train Loss 1.130814 Test MSE 14251.395707675058 Test RE 0.9997565617461848\n",
      "6 Train Loss 1.1435962 Test MSE 14251.870424002187 Test RE 0.9997732126350015\n",
      "7 Train Loss 1.1567187 Test MSE 14251.756088762348 Test RE 0.9997692023003606\n",
      "8 Train Loss 1.1691998 Test MSE 14250.90280826428 Test RE 0.9997392727840642\n",
      "9 Train Loss 1.1805955 Test MSE 14250.80569458191 Test RE 0.9997358663848733\n",
      "10 Train Loss 1.1918099 Test MSE 14250.872704493711 Test RE 0.9997382168531525\n",
      "11 Train Loss 1.2021681 Test MSE 14250.890330250915 Test RE 0.9997388351008204\n",
      "12 Train Loss 1.2117311 Test MSE 14250.881982955176 Test RE 0.9997385423079932\n",
      "13 Train Loss 1.2208823 Test MSE 14250.754623182718 Test RE 0.999734074978957\n",
      "14 Train Loss 1.2291192 Test MSE 14250.755837293544 Test RE 0.9997341175657523\n",
      "15 Train Loss 1.2369784 Test MSE 14250.746919989866 Test RE 0.999733804777637\n",
      "16 Train Loss 1.244434 Test MSE 14250.718155796465 Test RE 0.9997327958287315\n",
      "17 Train Loss 1.2510002 Test MSE 14250.693456164461 Test RE 0.9997319294498376\n",
      "18 Train Loss 1.2574587 Test MSE 14250.642520425778 Test RE 0.9997301427953859\n",
      "19 Train Loss 1.2632916 Test MSE 14250.583516204813 Test RE 0.9997280731217121\n",
      "20 Train Loss 1.2689041 Test MSE 14250.519809329311 Test RE 0.9997258384896004\n",
      "21 Train Loss 1.2737418 Test MSE 14249.62234077499 Test RE 0.9996943576509454\n",
      "22 Train Loss 1.2783183 Test MSE 14244.481056523473 Test RE 0.9995139957991713\n",
      "23 Train Loss 1.2828244 Test MSE 14237.444546327637 Test RE 0.9992670945900943\n",
      "24 Train Loss 1.2863721 Test MSE 14233.311684221373 Test RE 0.9991220498511127\n",
      "25 Train Loss 1.2898942 Test MSE 14223.642871260166 Test RE 0.9987826360315143\n",
      "26 Train Loss 1.2910582 Test MSE 14200.700470569533 Test RE 0.9979768045285962\n",
      "27 Train Loss 1.2915311 Test MSE 14166.554445302821 Test RE 0.9967762492924402\n",
      "28 Train Loss 1.2940305 Test MSE 14150.970794341416 Test RE 0.9962278559842335\n",
      "29 Train Loss 1.2946668 Test MSE 14106.30502539301 Test RE 0.9946543790852568\n",
      "30 Train Loss 1.2873058 Test MSE 14007.929331458672 Test RE 0.991180010343559\n",
      "31 Train Loss 1.2846146 Test MSE 13940.362513630116 Test RE 0.9887866575874369\n",
      "32 Train Loss 1.2822781 Test MSE 13809.807548713703 Test RE 0.9841456493910898\n",
      "33 Train Loss 1.2621701 Test MSE 13618.299715958154 Test RE 0.9772979949762933\n",
      "34 Train Loss 1.2557589 Test MSE 13523.443950164747 Test RE 0.9738884526103034\n",
      "35 Train Loss 1.2527776 Test MSE 13481.723479820992 Test RE 0.972385046004127\n",
      "36 Train Loss 1.2427403 Test MSE 13363.710626739434 Test RE 0.9681197840439778\n",
      "37 Train Loss 1.2287376 Test MSE 13185.391940498632 Test RE 0.9616390388250996\n",
      "38 Train Loss 1.2235441 Test MSE 13101.094259610521 Test RE 0.9585601040917056\n",
      "39 Train Loss 1.2232636 Test MSE 13065.391138326637 Test RE 0.9572530783065378\n",
      "40 Train Loss 1.223836 Test MSE 13064.96160175113 Test RE 0.9572373428959293\n",
      "41 Train Loss 1.2175272 Test MSE 12977.708577338688 Test RE 0.9540355819012173\n",
      "42 Train Loss 1.2128245 Test MSE 12928.029376776374 Test RE 0.9522077873428173\n",
      "43 Train Loss 1.2083874 Test MSE 12845.435027881795 Test RE 0.9491611900088106\n",
      "44 Train Loss 1.2066 Test MSE 12782.714725514314 Test RE 0.9468411236441383\n",
      "45 Train Loss 1.202698 Test MSE 12730.884841467905 Test RE 0.9449196025356997\n",
      "46 Train Loss 1.1897432 Test MSE 12609.751709164013 Test RE 0.9404134489090982\n",
      "47 Train Loss 1.1821728 Test MSE 12545.455306054648 Test RE 0.9380128276326554\n",
      "48 Train Loss 1.1781712 Test MSE 12465.262148694665 Test RE 0.9350100348604415\n",
      "49 Train Loss 1.1723691 Test MSE 12351.781840975713 Test RE 0.9307442673659232\n",
      "50 Train Loss 1.171976 Test MSE 12319.089371196062 Test RE 0.9295117128162678\n",
      "51 Train Loss 1.1720618 Test MSE 12320.906194679586 Test RE 0.9295802526408069\n",
      "52 Train Loss 1.169209 Test MSE 12320.624348049978 Test RE 0.9295696202832773\n",
      "53 Train Loss 1.1601745 Test MSE 12264.410618801267 Test RE 0.9274465819677983\n",
      "54 Train Loss 1.1547936 Test MSE 12203.465226224223 Test RE 0.9251393373182343\n",
      "55 Train Loss 1.1499215 Test MSE 12128.939134789063 Test RE 0.9223101160923928\n",
      "56 Train Loss 1.1488322 Test MSE 12086.889681832155 Test RE 0.9207099634579439\n",
      "57 Train Loss 1.1487314 Test MSE 12094.865203430503 Test RE 0.9210136781211425\n",
      "58 Train Loss 1.1474 Test MSE 12060.700000919142 Test RE 0.9197119326604155\n",
      "59 Train Loss 1.1447691 Test MSE 11986.86459236903 Test RE 0.916892380001637\n",
      "60 Train Loss 1.1429875 Test MSE 11930.361139333401 Test RE 0.9147288124626429\n",
      "61 Train Loss 1.1315875 Test MSE 11827.854014753246 Test RE 0.9107906038070919\n",
      "62 Train Loss 1.1222525 Test MSE 11771.054750496256 Test RE 0.9086010902937711\n",
      "63 Train Loss 1.1173631 Test MSE 11692.86405733909 Test RE 0.905578314342922\n",
      "64 Train Loss 1.1033081 Test MSE 11561.543183167274 Test RE 0.9004787458954384\n",
      "65 Train Loss 1.1001887 Test MSE 11489.888629835155 Test RE 0.8976839768110861\n",
      "66 Train Loss 1.0905828 Test MSE 11276.382986515635 Test RE 0.8893044645113424\n",
      "67 Train Loss 1.0751921 Test MSE 11138.056045852376 Test RE 0.8838331022791054\n",
      "68 Train Loss 1.0725603 Test MSE 11169.93972003366 Test RE 0.8850972236182661\n",
      "69 Train Loss 1.0696155 Test MSE 11141.058962848705 Test RE 0.8839522388037975\n",
      "70 Train Loss 1.0625849 Test MSE 11067.556918447635 Test RE 0.8810315194192353\n",
      "71 Train Loss 1.0506346 Test MSE 10993.92922633706 Test RE 0.8780960674701412\n",
      "72 Train Loss 1.0431676 Test MSE 10910.983150464795 Test RE 0.8747773030241814\n",
      "73 Train Loss 1.0368328 Test MSE 10791.595452351183 Test RE 0.8699782436995356\n",
      "74 Train Loss 1.0330958 Test MSE 10721.738498557592 Test RE 0.867157868269447\n",
      "75 Train Loss 1.0293337 Test MSE 10635.188821862443 Test RE 0.8636507733312176\n",
      "76 Train Loss 1.0120838 Test MSE 10405.07120606674 Test RE 0.854256106690576\n",
      "77 Train Loss 0.99189824 Test MSE 10196.281154159966 Test RE 0.8456418446367244\n",
      "78 Train Loss 0.98692834 Test MSE 10129.885668497434 Test RE 0.8428840497989054\n",
      "79 Train Loss 0.9791941 Test MSE 10107.406370424376 Test RE 0.8419483055222458\n",
      "80 Train Loss 0.97612804 Test MSE 10087.42824946227 Test RE 0.8411158038646717\n",
      "81 Train Loss 0.97237325 Test MSE 9992.486186249538 Test RE 0.8371481889340834\n",
      "82 Train Loss 0.96403354 Test MSE 9840.94981121753 Test RE 0.8307762493461668\n",
      "83 Train Loss 0.9552938 Test MSE 9750.280703577002 Test RE 0.8269402350873634\n",
      "84 Train Loss 0.94257754 Test MSE 9645.2294728213 Test RE 0.8224733714006086\n",
      "85 Train Loss 0.92337155 Test MSE 9478.12503163798 Test RE 0.815317530672559\n",
      "86 Train Loss 0.9143627 Test MSE 9362.565829860274 Test RE 0.8103320305685683\n",
      "87 Train Loss 0.90523124 Test MSE 9273.952258775811 Test RE 0.8064881521329811\n",
      "88 Train Loss 0.9006752 Test MSE 9284.751399103488 Test RE 0.8069575768469713\n",
      "89 Train Loss 0.8933854 Test MSE 9247.843464379812 Test RE 0.8053521060542279\n",
      "90 Train Loss 0.88751066 Test MSE 9177.215151632605 Test RE 0.8022708644307672\n",
      "91 Train Loss 0.8847063 Test MSE 9135.93744844301 Test RE 0.8004645855319994\n",
      "92 Train Loss 0.8755627 Test MSE 8978.50600866126 Test RE 0.793537769777999\n",
      "93 Train Loss 0.85981345 Test MSE 8888.967390698188 Test RE 0.7895710571036155\n",
      "94 Train Loss 0.85651505 Test MSE 8841.688160909313 Test RE 0.7874684460248221\n",
      "95 Train Loss 0.8494637 Test MSE 8743.875250094321 Test RE 0.7831005701799164\n",
      "96 Train Loss 0.8472073 Test MSE 8691.024961855648 Test RE 0.7807303499967267\n",
      "97 Train Loss 0.84573984 Test MSE 8644.980619879872 Test RE 0.7786594806455285\n",
      "98 Train Loss 0.8239976 Test MSE 8398.888325162809 Test RE 0.7674966121835028\n",
      "99 Train Loss 0.8107696 Test MSE 8324.087932241755 Test RE 0.7640713111574879\n",
      "100 Train Loss 0.8076767 Test MSE 8310.041513142352 Test RE 0.7634263757979409\n",
      "101 Train Loss 0.79525495 Test MSE 8151.712916281877 Test RE 0.7561187400520246\n",
      "102 Train Loss 0.7736433 Test MSE 7996.579912101622 Test RE 0.7488894363311979\n",
      "103 Train Loss 0.7720651 Test MSE 7982.090294674104 Test RE 0.7482106435534838\n",
      "104 Train Loss 0.7681944 Test MSE 7908.851237602183 Test RE 0.7447701587140436\n",
      "105 Train Loss 0.76738584 Test MSE 7888.201547239811 Test RE 0.7437972408718795\n",
      "106 Train Loss 0.76348335 Test MSE 7820.291224811117 Test RE 0.7405886073627953\n",
      "107 Train Loss 0.74681103 Test MSE 7677.589952071777 Test RE 0.7338005289280978\n",
      "108 Train Loss 0.7372828 Test MSE 7630.888824388017 Test RE 0.7315653493518834\n",
      "109 Train Loss 0.7309864 Test MSE 7530.479979281853 Test RE 0.7267363659487083\n",
      "110 Train Loss 0.7260549 Test MSE 7419.500504361265 Test RE 0.7213613976099186\n",
      "111 Train Loss 0.7241647 Test MSE 7363.025118086633 Test RE 0.7186107418147996\n",
      "112 Train Loss 0.7232523 Test MSE 7370.525271399709 Test RE 0.7189766456947451\n",
      "113 Train Loss 0.7219349 Test MSE 7339.973089759217 Test RE 0.717484953097554\n",
      "114 Train Loss 0.7183966 Test MSE 7224.975709874142 Test RE 0.7118422472078318\n",
      "115 Train Loss 0.70747846 Test MSE 7090.9646644999 Test RE 0.7052096140509697\n",
      "116 Train Loss 0.70335245 Test MSE 7075.966368746857 Test RE 0.7044634150724999\n",
      "117 Train Loss 0.7004577 Test MSE 7022.528503518759 Test RE 0.7017983116266099\n",
      "118 Train Loss 0.6979684 Test MSE 6940.653711328148 Test RE 0.6976952273214465\n",
      "119 Train Loss 0.69121015 Test MSE 6845.726507189504 Test RE 0.6929076180012552\n",
      "120 Train Loss 0.6808146 Test MSE 6826.513378838086 Test RE 0.691934581970141\n",
      "121 Train Loss 0.67331165 Test MSE 6780.658386853223 Test RE 0.6896067379808727\n",
      "122 Train Loss 0.6675429 Test MSE 6720.38529687529 Test RE 0.6865349486096161\n",
      "123 Train Loss 0.6595397 Test MSE 6730.562745096064 Test RE 0.687054601145498\n",
      "124 Train Loss 0.6578658 Test MSE 6734.956232273032 Test RE 0.68727880774235\n",
      "125 Train Loss 0.65420586 Test MSE 6695.41642111358 Test RE 0.6852583880366824\n",
      "126 Train Loss 0.64187884 Test MSE 6596.07435244753 Test RE 0.6801556890354912\n",
      "127 Train Loss 0.6372048 Test MSE 6547.850585521553 Test RE 0.6776648257395718\n",
      "128 Train Loss 0.6332652 Test MSE 6498.430383569709 Test RE 0.6751026288655952\n",
      "129 Train Loss 0.6261005 Test MSE 6432.737648830677 Test RE 0.6716816496861626\n",
      "130 Train Loss 0.6205495 Test MSE 6326.186143996361 Test RE 0.6660955726841739\n",
      "131 Train Loss 0.61028147 Test MSE 6196.416845573316 Test RE 0.6592283513285192\n",
      "132 Train Loss 0.59393454 Test MSE 6093.278673726152 Test RE 0.6537189646198202\n",
      "133 Train Loss 0.58576506 Test MSE 6059.960575436229 Test RE 0.6519292443565662\n",
      "134 Train Loss 0.57722634 Test MSE 5956.736421791197 Test RE 0.6463529800915551\n",
      "135 Train Loss 0.57312727 Test MSE 5884.224482774137 Test RE 0.6424068750027848\n",
      "136 Train Loss 0.5670487 Test MSE 5815.154795077072 Test RE 0.6386254234545705\n",
      "137 Train Loss 0.55931014 Test MSE 5665.023853742094 Test RE 0.6303277615519643\n",
      "138 Train Loss 0.5529155 Test MSE 5520.161278286692 Test RE 0.6222163901722523\n",
      "139 Train Loss 0.54595166 Test MSE 5409.148843708221 Test RE 0.6159281167168271\n",
      "140 Train Loss 0.53839463 Test MSE 5310.317656763405 Test RE 0.610275329913857\n",
      "141 Train Loss 0.53656536 Test MSE 5272.24211290953 Test RE 0.6080835243805701\n",
      "142 Train Loss 0.5322942 Test MSE 5250.355720425145 Test RE 0.606820058560336\n",
      "143 Train Loss 0.5260533 Test MSE 5223.252272651567 Test RE 0.6052517651702969\n",
      "144 Train Loss 0.5183231 Test MSE 5157.706496299871 Test RE 0.6014421710397052\n",
      "145 Train Loss 0.50335014 Test MSE 5076.914655858619 Test RE 0.5967129940838863\n",
      "146 Train Loss 0.4857787 Test MSE 4923.250570631747 Test RE 0.587613187274125\n",
      "147 Train Loss 0.47889894 Test MSE 4884.9302806173055 Test RE 0.5853218662012536\n",
      "148 Train Loss 0.4734948 Test MSE 4817.989018373633 Test RE 0.5812975153408153\n",
      "149 Train Loss 0.46791074 Test MSE 4762.390807307955 Test RE 0.5779337797655957\n",
      "150 Train Loss 0.46388644 Test MSE 4748.057075093084 Test RE 0.5770633985874607\n",
      "151 Train Loss 0.4554447 Test MSE 4658.760570044617 Test RE 0.5716112390308768\n",
      "152 Train Loss 0.45031258 Test MSE 4591.804274252012 Test RE 0.567488738659602\n",
      "153 Train Loss 0.4496227 Test MSE 4576.755728212589 Test RE 0.5665580708604682\n",
      "154 Train Loss 0.4476432 Test MSE 4516.448795034516 Test RE 0.5628129855273069\n",
      "155 Train Loss 0.44750053 Test MSE 4506.483528864684 Test RE 0.5621917365522433\n",
      "156 Train Loss 0.44749442 Test MSE 4508.289040627593 Test RE 0.5623043456562713\n",
      "157 Train Loss 0.44663683 Test MSE 4495.823847616165 Test RE 0.5615264359115598\n",
      "158 Train Loss 0.44246835 Test MSE 4463.314155064563 Test RE 0.5594925291977465\n",
      "159 Train Loss 0.42943522 Test MSE 4393.580155121795 Test RE 0.5551046185310964\n",
      "160 Train Loss 0.41929778 Test MSE 4275.531962716864 Test RE 0.5475964733058328\n",
      "161 Train Loss 0.41303897 Test MSE 4164.172319834202 Test RE 0.5404181298697477\n",
      "162 Train Loss 0.40678915 Test MSE 4093.246594377315 Test RE 0.5357960632707971\n",
      "163 Train Loss 0.40191197 Test MSE 3991.9342085909248 Test RE 0.5291237448337853\n",
      "164 Train Loss 0.39429954 Test MSE 3798.40082798013 Test RE 0.5161381486764087\n",
      "165 Train Loss 0.39236906 Test MSE 3694.9768209483327 Test RE 0.5090628715063706\n",
      "166 Train Loss 0.3865102 Test MSE 3560.4502670201223 Test RE 0.49970998298988045\n",
      "167 Train Loss 0.3760317 Test MSE 3470.081835587191 Test RE 0.4933276099873574\n",
      "168 Train Loss 0.36310384 Test MSE 3421.29371988179 Test RE 0.4898473284874747\n",
      "169 Train Loss 0.35476822 Test MSE 3431.09895352735 Test RE 0.4905487636012401\n",
      "170 Train Loss 0.34966063 Test MSE 3436.0602842896024 Test RE 0.4909032995002279\n",
      "171 Train Loss 0.3431845 Test MSE 3357.5113821827736 Test RE 0.48525979517489987\n",
      "172 Train Loss 0.33424643 Test MSE 3234.410932650264 Test RE 0.47628089592112105\n",
      "173 Train Loss 0.31986314 Test MSE 3132.761747449397 Test RE 0.4687370121110085\n",
      "174 Train Loss 0.31321928 Test MSE 3033.747169043963 Test RE 0.46127004868146276\n",
      "175 Train Loss 0.30226386 Test MSE 2924.5019833235106 Test RE 0.45288874077869856\n",
      "176 Train Loss 0.2903511 Test MSE 2796.350939387217 Test RE 0.44285484539328496\n",
      "177 Train Loss 0.27609962 Test MSE 2567.9736903153475 Test RE 0.4243858046854011\n",
      "178 Train Loss 0.26205662 Test MSE 2391.2843878387716 Test RE 0.40952571569705654\n",
      "179 Train Loss 0.2552401 Test MSE 2346.4969805764217 Test RE 0.4056724954698547\n",
      "180 Train Loss 0.2520993 Test MSE 2308.5350048185205 Test RE 0.4023775999399071\n",
      "181 Train Loss 0.2482661 Test MSE 2243.3408981832868 Test RE 0.3966552444422182\n",
      "182 Train Loss 0.24525216 Test MSE 2201.2883827781206 Test RE 0.3929199087917152\n",
      "183 Train Loss 0.23954871 Test MSE 2159.9744424353676 Test RE 0.3892152695232771\n",
      "184 Train Loss 0.23801382 Test MSE 2153.290575641029 Test RE 0.38861260529983355\n",
      "185 Train Loss 0.2377669 Test MSE 2153.3270842704137 Test RE 0.388615899712024\n",
      "186 Train Loss 0.23434937 Test MSE 2159.4300185916486 Test RE 0.38916621537178553\n",
      "187 Train Loss 0.22684376 Test MSE 2161.518695709003 Test RE 0.3893543775498543\n",
      "188 Train Loss 0.22418766 Test MSE 2147.547017484768 Test RE 0.3880939782504072\n",
      "189 Train Loss 0.21899366 Test MSE 2119.8965463251907 Test RE 0.38558745644040915\n",
      "190 Train Loss 0.21375078 Test MSE 2087.7986280013947 Test RE 0.38265718091511414\n",
      "191 Train Loss 0.2090751 Test MSE 2071.9504275717713 Test RE 0.38120206448987326\n",
      "192 Train Loss 0.20310281 Test MSE 2041.6100582103309 Test RE 0.3784007269638882\n",
      "193 Train Loss 0.19774975 Test MSE 1996.8862936849164 Test RE 0.37423312984815804\n",
      "194 Train Loss 0.19185315 Test MSE 1911.9660670231567 Test RE 0.3661893031770638\n",
      "195 Train Loss 0.18680924 Test MSE 1852.0239908195458 Test RE 0.36040339039715874\n",
      "196 Train Loss 0.17909299 Test MSE 1792.6406274147273 Test RE 0.354578322587401\n",
      "197 Train Loss 0.16861932 Test MSE 1675.8410460917894 Test RE 0.3428324919093499\n",
      "198 Train Loss 0.1603141 Test MSE 1621.3320812348245 Test RE 0.33721085925297073\n",
      "199 Train Loss 0.15361206 Test MSE 1530.1272182776468 Test RE 0.327589017895154\n",
      "Training time: 232.60\n",
      "Training time: 232.60\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      "  (m_lambda): Sigmoid()\n",
      ")\n",
      "1\n",
      "0 Train Loss 6.488765 Test MSE 14274.81394347922 Test RE 1.0005776365824315\n",
      "1 Train Loss 1.666342 Test MSE 14270.937143291507 Test RE 1.0004417572991364\n",
      "2 Train Loss 1.1502548 Test MSE 14264.060033475267 Test RE 1.0002006737304963\n",
      "3 Train Loss 1.1166269 Test MSE 14260.594722712223 Test RE 1.0000791719741517\n",
      "4 Train Loss 1.1184167 Test MSE 14260.360356638239 Test RE 1.0000709540282395\n",
      "5 Train Loss 1.1325747 Test MSE 14257.257640965203 Test RE 0.9999621522677914\n",
      "6 Train Loss 1.1474051 Test MSE 14257.037081329445 Test RE 0.9999544175355374\n",
      "7 Train Loss 1.1614095 Test MSE 14256.893116991603 Test RE 0.9999493688659508\n",
      "8 Train Loss 1.174461 Test MSE 14256.924659665601 Test RE 0.9999504750347091\n",
      "9 Train Loss 1.1862556 Test MSE 14256.948670562024 Test RE 0.9999513170710436\n",
      "10 Train Loss 1.1975744 Test MSE 14256.972085887814 Test RE 0.9999521382206739\n",
      "11 Train Loss 1.2080864 Test MSE 14257.10998639198 Test RE 0.9999569742254626\n",
      "12 Train Loss 1.2177931 Test MSE 14257.10998639198 Test RE 0.9999569742254626\n",
      "13 Train Loss 1.2268639 Test MSE 14257.061430711152 Test RE 0.9999552714388152\n",
      "14 Train Loss 1.2350868 Test MSE 14257.043678668128 Test RE 0.9999546488962736\n",
      "15 Train Loss 1.242717 Test MSE 14257.043678668128 Test RE 0.9999546488962736\n",
      "16 Train Loss 1.2501291 Test MSE 14257.050033566382 Test RE 0.9999548717548652\n",
      "17 Train Loss 1.256493 Test MSE 14257.050033566382 Test RE 0.9999548717548652\n",
      "18 Train Loss 1.2629858 Test MSE 14257.050033566382 Test RE 0.9999548717548652\n",
      "19 Train Loss 1.2685385 Test MSE 14257.080639898497 Test RE 0.9999559450811987\n",
      "20 Train Loss 1.2739013 Test MSE 14257.137433451553 Test RE 0.9999579367580327\n",
      "21 Train Loss 1.2787366 Test MSE 14257.162189353632 Test RE 0.9999588049144257\n",
      "22 Train Loss 1.2835197 Test MSE 14257.184997777544 Test RE 0.9999596047746956\n",
      "23 Train Loss 1.2881417 Test MSE 14257.331044105256 Test RE 0.9999647264047424\n",
      "24 Train Loss 1.2920078 Test MSE 14257.355992790417 Test RE 0.9999656013158369\n",
      "25 Train Loss 1.2960181 Test MSE 14257.377013420988 Test RE 0.9999663384756496\n",
      "26 Train Loss 1.299406 Test MSE 14257.377013420988 Test RE 0.9999663384756496\n",
      "27 Train Loss 1.302701 Test MSE 14257.377013420988 Test RE 0.9999663384756496\n",
      "28 Train Loss 1.306009 Test MSE 14257.44055456695 Test RE 0.9999685667584614\n",
      "29 Train Loss 1.3090808 Test MSE 14257.44055456695 Test RE 0.9999685667584614\n",
      "30 Train Loss 1.3119743 Test MSE 14257.437672330663 Test RE 0.9999684656833131\n",
      "31 Train Loss 1.3145713 Test MSE 14257.437672330663 Test RE 0.9999684656833131\n",
      "32 Train Loss 1.3174587 Test MSE 14257.446217717996 Test RE 0.9999687653555533\n",
      "33 Train Loss 1.3196203 Test MSE 14257.446217717996 Test RE 0.9999687653555533\n",
      "34 Train Loss 1.3218707 Test MSE 14257.446854922671 Test RE 0.9999687877012371\n",
      "35 Train Loss 1.3240566 Test MSE 14257.458850791327 Test RE 0.9999692083758442\n",
      "36 Train Loss 1.3262638 Test MSE 14257.45632154377 Test RE 0.999969119679471\n",
      "37 Train Loss 1.3278135 Test MSE 14257.413070433962 Test RE 0.9999676029360203\n",
      "38 Train Loss 1.3295581 Test MSE 14257.414924544297 Test RE 0.9999676679565888\n",
      "39 Train Loss 1.331762 Test MSE 14257.413209083523 Test RE 0.9999676077982301\n",
      "40 Train Loss 1.3328866 Test MSE 14257.410026055135 Test RE 0.9999674961747044\n",
      "41 Train Loss 1.3348973 Test MSE 14257.410026055135 Test RE 0.9999674961747044\n",
      "42 Train Loss 1.3364841 Test MSE 14257.380618478717 Test RE 0.9999664648991898\n",
      "43 Train Loss 1.337646 Test MSE 14257.380618478717 Test RE 0.9999664648991898\n",
      "44 Train Loss 1.3391287 Test MSE 14257.380618478717 Test RE 0.9999664648991898\n",
      "45 Train Loss 1.3403789 Test MSE 14257.395178790564 Test RE 0.9999669755055993\n",
      "46 Train Loss 1.3417748 Test MSE 14257.395178790564 Test RE 0.9999669755055993\n",
      "47 Train Loss 1.3429474 Test MSE 14257.395178790564 Test RE 0.9999669755055993\n",
      "48 Train Loss 1.344095 Test MSE 14257.457130087021 Test RE 0.9999691480336967\n",
      "49 Train Loss 1.3451405 Test MSE 14257.457130087021 Test RE 0.9999691480336967\n",
      "50 Train Loss 1.346208 Test MSE 14257.43280488241 Test RE 0.9999682949901182\n",
      "51 Train Loss 1.3473915 Test MSE 14257.43280488241 Test RE 0.9999682949901182\n",
      "52 Train Loss 1.3482661 Test MSE 14257.43280488241 Test RE 0.9999682949901182\n",
      "53 Train Loss 1.3488505 Test MSE 14257.43280488241 Test RE 0.9999682949901182\n",
      "54 Train Loss 1.34987 Test MSE 14257.43280488241 Test RE 0.9999682949901182\n",
      "55 Train Loss 1.3506094 Test MSE 14257.43280488241 Test RE 0.9999682949901182\n",
      "56 Train Loss 1.3513681 Test MSE 14257.43280488241 Test RE 0.9999682949901182\n",
      "57 Train Loss 1.3521888 Test MSE 14257.442832397106 Test RE 0.9999686466381024\n",
      "58 Train Loss 1.3529897 Test MSE 14257.442832397106 Test RE 0.9999686466381024\n",
      "59 Train Loss 1.3535844 Test MSE 14257.442832397106 Test RE 0.9999686466381024\n",
      "60 Train Loss 1.3545717 Test MSE 14257.442832397106 Test RE 0.9999686466381024\n",
      "61 Train Loss 1.35522 Test MSE 14257.442832397106 Test RE 0.9999686466381024\n",
      "62 Train Loss 1.3559489 Test MSE 14257.442832397106 Test RE 0.9999686466381024\n",
      "63 Train Loss 1.3567613 Test MSE 14257.46536283777 Test RE 0.9999694367421068\n",
      "64 Train Loss 1.3570095 Test MSE 14257.46536283777 Test RE 0.9999694367421068\n",
      "65 Train Loss 1.3582324 Test MSE 14257.46536283777 Test RE 0.9999694367421068\n",
      "66 Train Loss 1.3582461 Test MSE 14257.46536283777 Test RE 0.9999694367421068\n",
      "67 Train Loss 1.35875 Test MSE 14257.46536283777 Test RE 0.9999694367421068\n",
      "68 Train Loss 1.3595585 Test MSE 14257.46536283777 Test RE 0.9999694367421068\n",
      "69 Train Loss 1.3601131 Test MSE 14257.610961152397 Test RE 0.9999745426108313\n",
      "70 Train Loss 1.360359 Test MSE 14257.610961152397 Test RE 0.9999745426108313\n",
      "71 Train Loss 1.3608059 Test MSE 14257.425659966006 Test RE 0.9999680444299063\n",
      "72 Train Loss 1.3612002 Test MSE 14257.40679333221 Test RE 0.9999673828084608\n",
      "73 Train Loss 1.3617095 Test MSE 14257.389927981207 Test RE 0.9999667913683015\n",
      "74 Train Loss 1.3623693 Test MSE 14257.37314141455 Test RE 0.9999662026906323\n",
      "75 Train Loss 1.3627151 Test MSE 14257.358841850353 Test RE 0.9999657012278331\n",
      "76 Train Loss 1.3632463 Test MSE 14257.330239184077 Test RE 0.9999646981774121\n",
      "77 Train Loss 1.3635421 Test MSE 14257.330239184077 Test RE 0.9999646981774121\n",
      "78 Train Loss 1.3642436 Test MSE 14257.319280507334 Test RE 0.9999643138736257\n",
      "79 Train Loss 1.3645626 Test MSE 14257.307468507737 Test RE 0.9999638996449652\n",
      "80 Train Loss 1.3646966 Test MSE 14257.307468507737 Test RE 0.9999638996449652\n",
      "81 Train Loss 1.3651217 Test MSE 14257.307468507737 Test RE 0.9999638996449652\n",
      "82 Train Loss 1.3652643 Test MSE 14257.307468507737 Test RE 0.9999638996449652\n",
      "83 Train Loss 1.3659613 Test MSE 14257.249143533627 Test RE 0.9999618542753076\n",
      "84 Train Loss 1.3662797 Test MSE 14257.249143533627 Test RE 0.9999618542753076\n",
      "85 Train Loss 1.3665836 Test MSE 14257.249143533627 Test RE 0.9999618542753076\n",
      "86 Train Loss 1.3665926 Test MSE 14257.276474911745 Test RE 0.9999628127464042\n",
      "87 Train Loss 1.3668272 Test MSE 14257.276474911745 Test RE 0.9999628127464042\n",
      "88 Train Loss 1.3673109 Test MSE 14257.276474911745 Test RE 0.9999628127464042\n",
      "89 Train Loss 1.367639 Test MSE 14257.276474911745 Test RE 0.9999628127464042\n",
      "90 Train Loss 1.3680012 Test MSE 14257.212804261626 Test RE 0.9999605799092428\n",
      "91 Train Loss 1.3681312 Test MSE 14257.212804261626 Test RE 0.9999605799092428\n",
      "92 Train Loss 1.368253 Test MSE 14257.212804261626 Test RE 0.9999605799092428\n",
      "93 Train Loss 1.3688543 Test MSE 14257.240060359461 Test RE 0.9999615357415905\n",
      "94 Train Loss 1.3687398 Test MSE 14257.240060359461 Test RE 0.9999615357415905\n",
      "95 Train Loss 1.3693625 Test MSE 14257.240060359461 Test RE 0.9999615357415905\n",
      "96 Train Loss 1.3695886 Test MSE 14257.240060359461 Test RE 0.9999615357415905\n",
      "97 Train Loss 1.3696426 Test MSE 14257.240060359461 Test RE 0.9999615357415905\n",
      "98 Train Loss 1.3698577 Test MSE 14257.240060359461 Test RE 0.9999615357415905\n",
      "99 Train Loss 1.3705267 Test MSE 14257.240060359461 Test RE 0.9999615357415905\n",
      "100 Train Loss 1.3702161 Test MSE 14257.240060359461 Test RE 0.9999615357415905\n",
      "101 Train Loss 1.3706058 Test MSE 14257.30908187828 Test RE 0.9999639562233995\n",
      "102 Train Loss 1.3710397 Test MSE 14257.30908187828 Test RE 0.9999639562233995\n",
      "103 Train Loss 1.3710185 Test MSE 14257.30908187828 Test RE 0.9999639562233995\n",
      "104 Train Loss 1.3711371 Test MSE 14257.30908187828 Test RE 0.9999639562233995\n",
      "105 Train Loss 1.3715013 Test MSE 14257.174662156765 Test RE 0.9999592423186726\n",
      "106 Train Loss 1.3715922 Test MSE 14257.153955448335 Test RE 0.9999585161624575\n",
      "107 Train Loss 1.3713968 Test MSE 14256.980957550453 Test RE 0.9999524493399142\n",
      "108 Train Loss 1.372113 Test MSE 14256.980957550453 Test RE 0.9999524493399142\n",
      "109 Train Loss 1.3719199 Test MSE 14257.105976813384 Test RE 0.9999568336146947\n",
      "110 Train Loss 1.3721135 Test MSE 14257.105976813384 Test RE 0.9999568336146947\n",
      "111 Train Loss 1.372649 Test MSE 14257.067014340219 Test RE 0.9999554672498105\n",
      "112 Train Loss 1.3726132 Test MSE 14257.067014340219 Test RE 0.9999554672498105\n",
      "113 Train Loss 1.3729489 Test MSE 14257.067014340219 Test RE 0.9999554672498105\n",
      "114 Train Loss 1.3730005 Test MSE 14257.067014340219 Test RE 0.9999554672498105\n",
      "115 Train Loss 1.3731415 Test MSE 14257.067014340219 Test RE 0.9999554672498105\n",
      "116 Train Loss 1.3730367 Test MSE 14257.067014340219 Test RE 0.9999554672498105\n",
      "117 Train Loss 1.3737824 Test MSE 14257.067014340219 Test RE 0.9999554672498105\n",
      "118 Train Loss 1.3733693 Test MSE 14257.00122926216 Test RE 0.9999531602457021\n",
      "119 Train Loss 1.3737547 Test MSE 14257.00122926216 Test RE 0.9999531602457021\n",
      "120 Train Loss 1.3738221 Test MSE 14257.00122926216 Test RE 0.9999531602457021\n",
      "121 Train Loss 1.3738557 Test MSE 14257.00122926216 Test RE 0.9999531602457021\n",
      "122 Train Loss 1.3739239 Test MSE 14257.00122926216 Test RE 0.9999531602457021\n",
      "123 Train Loss 1.3743399 Test MSE 14257.00122926216 Test RE 0.9999531602457021\n",
      "124 Train Loss 1.3746444 Test MSE 14257.070665503228 Test RE 0.9999555952915802\n",
      "125 Train Loss 1.3743273 Test MSE 14257.070665503228 Test RE 0.9999555952915802\n",
      "126 Train Loss 1.3746914 Test MSE 14257.070665503228 Test RE 0.9999555952915802\n",
      "127 Train Loss 1.3749667 Test MSE 14257.070665503228 Test RE 0.9999555952915802\n",
      "128 Train Loss 1.3746173 Test MSE 14257.02171701877 Test RE 0.9999538787274249\n",
      "129 Train Loss 1.3749635 Test MSE 14256.942292868187 Test RE 0.9999510934122436\n",
      "130 Train Loss 1.3752987 Test MSE 14256.853918006243 Test RE 0.9999479941966138\n",
      "131 Train Loss 1.3755305 Test MSE 14256.853918006243 Test RE 0.9999479941966138\n",
      "132 Train Loss 1.3756065 Test MSE 14256.853918006243 Test RE 0.9999479941966138\n",
      "133 Train Loss 1.3752049 Test MSE 14256.853918006243 Test RE 0.9999479941966138\n",
      "134 Train Loss 1.3758742 Test MSE 14256.853918006243 Test RE 0.9999479941966138\n",
      "135 Train Loss 1.3756974 Test MSE 14256.853918006243 Test RE 0.9999479941966138\n",
      "136 Train Loss 1.3757181 Test MSE 14256.853918006243 Test RE 0.9999479941966138\n",
      "137 Train Loss 1.3757514 Test MSE 14256.590733227653 Test RE 0.999938764519873\n",
      "138 Train Loss 1.3758347 Test MSE 14256.590733227653 Test RE 0.999938764519873\n",
      "139 Train Loss 1.3764244 Test MSE 14256.590733227653 Test RE 0.999938764519873\n",
      "140 Train Loss 1.3763679 Test MSE 14256.590733227653 Test RE 0.999938764519873\n",
      "141 Train Loss 1.3762711 Test MSE 14256.590733227653 Test RE 0.999938764519873\n",
      "142 Train Loss 1.3764434 Test MSE 14256.770936616591 Test RE 0.9999450841157732\n",
      "143 Train Loss 1.3768163 Test MSE 14256.770936616591 Test RE 0.9999450841157732\n",
      "144 Train Loss 1.3762666 Test MSE 14256.666994678306 Test RE 0.999941438955865\n",
      "145 Train Loss 1.3767258 Test MSE 14256.666994678306 Test RE 0.999941438955865\n",
      "146 Train Loss 1.3770531 Test MSE 14256.741681327741 Test RE 0.9999440581577419\n",
      "147 Train Loss 1.3772514 Test MSE 14256.741681327741 Test RE 0.9999440581577419\n",
      "148 Train Loss 1.377012 Test MSE 14256.794446078215 Test RE 0.9999459085717801\n",
      "149 Train Loss 1.377498 Test MSE 14256.875851564359 Test RE 0.9999487633848632\n",
      "150 Train Loss 1.3773693 Test MSE 14256.875851564359 Test RE 0.9999487633848632\n",
      "151 Train Loss 1.377518 Test MSE 14256.875851564359 Test RE 0.9999487633848632\n",
      "152 Train Loss 1.3771309 Test MSE 14256.872717850045 Test RE 0.9999486534886292\n",
      "153 Train Loss 1.3770189 Test MSE 14256.872717850045 Test RE 0.9999486534886292\n",
      "154 Train Loss 1.3774133 Test MSE 14256.872717850045 Test RE 0.9999486534886292\n",
      "155 Train Loss 1.3781961 Test MSE 14256.872717850045 Test RE 0.9999486534886292\n",
      "156 Train Loss 1.3773241 Test MSE 14256.872717850045 Test RE 0.9999486534886292\n",
      "157 Train Loss 1.3776722 Test MSE 14256.706978171635 Test RE 0.9999428411462873\n",
      "158 Train Loss 1.3778347 Test MSE 14256.706978171635 Test RE 0.9999428411462873\n",
      "159 Train Loss 1.3779528 Test MSE 14256.820332382058 Test RE 0.9999468163807539\n",
      "160 Train Loss 1.3776684 Test MSE 14256.787806267102 Test RE 0.9999456757195516\n",
      "161 Train Loss 1.378274 Test MSE 14256.519380345082 Test RE 0.9999362622174695\n",
      "162 Train Loss 1.378196 Test MSE 14256.519380345082 Test RE 0.9999362622174695\n",
      "163 Train Loss 1.3781885 Test MSE 14256.519380345082 Test RE 0.9999362622174695\n",
      "164 Train Loss 1.3781806 Test MSE 14256.519380345082 Test RE 0.9999362622174695\n",
      "165 Train Loss 1.3786693 Test MSE 14256.519380345082 Test RE 0.9999362622174695\n",
      "166 Train Loss 1.3785452 Test MSE 14256.519380345082 Test RE 0.9999362622174695\n",
      "167 Train Loss 1.3785188 Test MSE 14256.528694392542 Test RE 0.9999365888558317\n",
      "168 Train Loss 1.3782626 Test MSE 14256.525466110623 Test RE 0.9999364756418274\n",
      "169 Train Loss 1.3786058 Test MSE 14255.352111744527 Test RE 0.9998953259250049\n",
      "170 Train Loss 1.3785446 Test MSE 14255.287815412888 Test RE 0.9998930709938931\n",
      "171 Train Loss 1.3784081 Test MSE 14255.247215085323 Test RE 0.9998916471006809\n",
      "172 Train Loss 1.3784153 Test MSE 14255.130034241774 Test RE 0.9998875374424729\n",
      "173 Train Loss 1.3785841 Test MSE 14255.130034241774 Test RE 0.9998875374424729\n",
      "174 Train Loss 1.3788567 Test MSE 14255.125038327093 Test RE 0.9998873622299964\n",
      "175 Train Loss 1.3786203 Test MSE 14255.125038327093 Test RE 0.9998873622299964\n",
      "176 Train Loss 1.3790072 Test MSE 14255.106140219175 Test RE 0.9998866994513297\n",
      "177 Train Loss 1.379037 Test MSE 14255.106140219175 Test RE 0.9998866994513297\n",
      "178 Train Loss 1.3790915 Test MSE 14255.118871189428 Test RE 0.9998871459413402\n",
      "179 Train Loss 1.3790381 Test MSE 14255.118871189428 Test RE 0.9998871459413402\n",
      "180 Train Loss 1.3789427 Test MSE 14255.159420726372 Test RE 0.9998885680596787\n",
      "181 Train Loss 1.379185 Test MSE 14255.184797221082 Test RE 0.9998894580411557\n",
      "182 Train Loss 1.3788519 Test MSE 14255.184797221082 Test RE 0.9998894580411557\n",
      "183 Train Loss 1.379488 Test MSE 14255.261118824541 Test RE 0.9998921347186472\n",
      "184 Train Loss 1.3794603 Test MSE 14255.261118824541 Test RE 0.9998921347186472\n",
      "185 Train Loss 1.3793303 Test MSE 14255.261118824541 Test RE 0.9998921347186472\n",
      "186 Train Loss 1.3791171 Test MSE 14254.815208170614 Test RE 0.9998764960699169\n",
      "187 Train Loss 1.3793612 Test MSE 14254.836085602441 Test RE 0.9998772282732264\n",
      "188 Train Loss 1.379548 Test MSE 14254.829178345357 Test RE 0.9998769860252669\n",
      "189 Train Loss 1.3793002 Test MSE 14254.80465861144 Test RE 0.9998761260806165\n",
      "190 Train Loss 1.3793297 Test MSE 14254.74779394157 Test RE 0.9998741317466399\n",
      "191 Train Loss 1.3797169 Test MSE 14254.725161942413 Test RE 0.9998733380055012\n",
      "192 Train Loss 1.3795247 Test MSE 14254.693218082853 Test RE 0.9998722176811519\n",
      "193 Train Loss 1.3794079 Test MSE 14254.549406043194 Test RE 0.9998671739378916\n",
      "194 Train Loss 1.379671 Test MSE 14254.358232363873 Test RE 0.9998604690985475\n",
      "195 Train Loss 1.3797147 Test MSE 14254.287698996164 Test RE 0.9998579953423737\n",
      "196 Train Loss 1.3800474 Test MSE 14253.947999928498 Test RE 0.9998460812832641\n",
      "197 Train Loss 1.3800267 Test MSE 14253.947999928498 Test RE 0.9998460812832641\n",
      "198 Train Loss 1.3800032 Test MSE 14253.949964949517 Test RE 0.9998461502016611\n",
      "199 Train Loss 1.3800732 Test MSE 14253.99236149252 Test RE 0.9998476371575188\n",
      "Training time: 142.23\n",
      "Training time: 142.23\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      "  (m_lambda): Sigmoid()\n",
      ")\n",
      "2\n",
      "0 Train Loss 1.3530215 Test MSE 14247.698702033138 Test RE 0.9996268781186566\n",
      "1 Train Loss 1.0828344 Test MSE 14255.464545869496 Test RE 0.9998992690803014\n",
      "2 Train Loss 1.0846384 Test MSE 14255.52961548065 Test RE 0.9999015511168554\n",
      "3 Train Loss 1.1013985 Test MSE 14252.598564817514 Test RE 0.999798751962912\n",
      "4 Train Loss 1.1198407 Test MSE 14252.11918647184 Test RE 0.9997819379798439\n",
      "5 Train Loss 1.1372428 Test MSE 14252.302079753781 Test RE 0.9997883529140319\n",
      "6 Train Loss 1.1533142 Test MSE 14252.368135258586 Test RE 0.9997906697835266\n",
      "7 Train Loss 1.1682417 Test MSE 14252.740809962022 Test RE 0.9998037410939278\n",
      "8 Train Loss 1.1819891 Test MSE 14252.740809962022 Test RE 0.9998037410939278\n",
      "9 Train Loss 1.1944579 Test MSE 14252.74785903537 Test RE 0.9998039883337089\n",
      "10 Train Loss 1.2062023 Test MSE 14252.742204676073 Test RE 0.9998037900122485\n",
      "11 Train Loss 1.216871 Test MSE 14252.742204676073 Test RE 0.9998037900122485\n",
      "12 Train Loss 1.2267652 Test MSE 14252.757253033698 Test RE 0.9998043178194873\n",
      "13 Train Loss 1.235813 Test MSE 14252.757978567894 Test RE 0.9998043432669216\n",
      "14 Train Loss 1.2442081 Test MSE 14252.785080130625 Test RE 0.9998052938284653\n",
      "15 Train Loss 1.2518736 Test MSE 14252.785080130625 Test RE 0.9998052938284653\n",
      "16 Train Loss 1.2589735 Test MSE 14252.785080130625 Test RE 0.9998052938284653\n",
      "17 Train Loss 1.2657769 Test MSE 14252.787777479612 Test RE 0.9998053884353668\n",
      "18 Train Loss 1.2717725 Test MSE 14252.734794005159 Test RE 0.9998035300897115\n",
      "19 Train Loss 1.2773399 Test MSE 14252.734794005159 Test RE 0.9998035300897115\n",
      "20 Train Loss 1.28273 Test MSE 14252.711316820165 Test RE 0.9998027066483705\n",
      "21 Train Loss 1.2874987 Test MSE 14252.548970967922 Test RE 0.9997970124937255\n",
      "22 Train Loss 1.2920002 Test MSE 14251.803952669401 Test RE 0.9997708811397858\n",
      "23 Train Loss 1.2959855 Test MSE 14250.307322733486 Test RE 0.9997183851077274\n",
      "24 Train Loss 1.2999206 Test MSE 14250.220100907427 Test RE 0.9997153256159375\n",
      "25 Train Loss 1.3038249 Test MSE 14249.821163051714 Test RE 0.9997013318994604\n",
      "26 Train Loss 1.3072932 Test MSE 14249.271005653613 Test RE 0.9996820334680632\n",
      "27 Train Loss 1.3098223 Test MSE 14246.18275569307 Test RE 0.9995736968628605\n",
      "28 Train Loss 1.3128251 Test MSE 14243.981558199048 Test RE 0.999496471119228\n",
      "29 Train Loss 1.3150219 Test MSE 14236.197561019908 Test RE 0.999223333272294\n",
      "30 Train Loss 1.3174663 Test MSE 14230.9910939154 Test RE 0.999040598411477\n",
      "31 Train Loss 1.319826 Test MSE 14230.291545315624 Test RE 0.9990160433388201\n",
      "32 Train Loss 1.3224453 Test MSE 14230.42161670587 Test RE 0.9990206090607621\n",
      "33 Train Loss 1.323735 Test MSE 14213.452370319683 Test RE 0.9984247839975311\n",
      "34 Train Loss 1.3242507 Test MSE 14200.647414031422 Test RE 0.9979749402105671\n",
      "35 Train Loss 1.3254519 Test MSE 14186.571108128635 Test RE 0.9974801992646858\n",
      "36 Train Loss 1.3263994 Test MSE 14182.276778634627 Test RE 0.9973292172967939\n",
      "37 Train Loss 1.3272024 Test MSE 14171.789588023217 Test RE 0.9969604078308448\n",
      "38 Train Loss 1.3275892 Test MSE 14156.517305846352 Test RE 0.9964230739664025\n",
      "39 Train Loss 1.3274562 Test MSE 14131.154207657148 Test RE 0.9955300680988064\n",
      "40 Train Loss 1.3253139 Test MSE 14095.519896489644 Test RE 0.9942740694680432\n",
      "41 Train Loss 1.3248086 Test MSE 14078.971935247224 Test RE 0.9936902655116117\n",
      "42 Train Loss 1.3234718 Test MSE 14040.58988209843 Test RE 0.992334843309765\n",
      "43 Train Loss 1.3192384 Test MSE 13978.03125992147 Test RE 0.9901216740358768\n",
      "44 Train Loss 1.3178556 Test MSE 13943.726747909646 Test RE 0.9889059625657867\n",
      "45 Train Loss 1.3159207 Test MSE 13920.989212601662 Test RE 0.9880993468519649\n",
      "46 Train Loss 1.3140368 Test MSE 13879.154718560938 Test RE 0.9866135422898366\n",
      "47 Train Loss 1.3121674 Test MSE 13848.146395603682 Test RE 0.9855107972498341\n",
      "48 Train Loss 1.3103896 Test MSE 13806.267173399503 Test RE 0.9840194901998721\n",
      "49 Train Loss 1.3055667 Test MSE 13711.299975089227 Test RE 0.9806293331237673\n",
      "50 Train Loss 1.3024356 Test MSE 13659.4959835398 Test RE 0.9787750746572011\n",
      "51 Train Loss 1.2910546 Test MSE 13466.477147470863 Test RE 0.9718350607065793\n",
      "52 Train Loss 1.2803805 Test MSE 13405.919048543205 Test RE 0.9696474508184677\n",
      "53 Train Loss 1.2775697 Test MSE 13387.68128056652 Test RE 0.968987659282606\n",
      "54 Train Loss 1.2773829 Test MSE 13386.354337029297 Test RE 0.9689396366359788\n",
      "55 Train Loss 1.2745719 Test MSE 13375.676661654736 Test RE 0.9685531203898009\n",
      "56 Train Loss 1.2675718 Test MSE 13272.456042323136 Test RE 0.9648087013080398\n",
      "57 Train Loss 1.264699 Test MSE 13246.675270101445 Test RE 0.9638712107721649\n",
      "58 Train Loss 1.2636491 Test MSE 13246.703110854382 Test RE 0.9638722236635354\n",
      "59 Train Loss 1.2631699 Test MSE 13246.70127574415 Test RE 0.9638721568993067\n",
      "60 Train Loss 1.2636403 Test MSE 13246.546169349347 Test RE 0.9638665138652476\n",
      "61 Train Loss 1.2629764 Test MSE 13227.470488275707 Test RE 0.9631722561286982\n",
      "62 Train Loss 1.2573909 Test MSE 13173.468618033829 Test RE 0.9612041436892448\n",
      "63 Train Loss 1.2517222 Test MSE 13098.034370142257 Test RE 0.958448156982862\n",
      "64 Train Loss 1.2406152 Test MSE 12982.43136019697 Test RE 0.9542091600416456\n",
      "65 Train Loss 1.2321028 Test MSE 12853.763864810653 Test RE 0.9494688529148997\n",
      "66 Train Loss 1.2117239 Test MSE 12591.382832959212 Test RE 0.9397282397877282\n",
      "67 Train Loss 1.1943554 Test MSE 12371.048096850225 Test RE 0.9314698699688684\n",
      "68 Train Loss 1.1856431 Test MSE 12297.847800017167 Test RE 0.9287099973998346\n",
      "69 Train Loss 1.1723372 Test MSE 12230.077768045456 Test RE 0.9261475305183902\n",
      "70 Train Loss 1.1422461 Test MSE 11897.612314155107 Test RE 0.9134724850399002\n",
      "71 Train Loss 1.1274579 Test MSE 11663.401731063388 Test RE 0.9044367090264025\n",
      "72 Train Loss 1.1242522 Test MSE 11664.223750420373 Test RE 0.904468580147711\n",
      "73 Train Loss 1.1236308 Test MSE 11661.84864624632 Test RE 0.9043764901589899\n",
      "74 Train Loss 1.1217973 Test MSE 11627.958445440452 Test RE 0.9030614413346824\n",
      "75 Train Loss 1.1203038 Test MSE 11608.82864279113 Test RE 0.9023182972023643\n",
      "76 Train Loss 1.1161978 Test MSE 11525.34558136267 Test RE 0.8990680032581027\n",
      "77 Train Loss 1.1121974 Test MSE 11442.310706383909 Test RE 0.8958234607273885\n",
      "78 Train Loss 1.1084878 Test MSE 11393.65548685909 Test RE 0.8939168125921182\n",
      "79 Train Loss 1.088561 Test MSE 11272.612953121059 Test RE 0.8891557914923559\n",
      "80 Train Loss 1.0767702 Test MSE 11124.22869310081 Test RE 0.8832843140816546\n",
      "81 Train Loss 1.071855 Test MSE 11034.481606443593 Test RE 0.8797140562853353\n",
      "82 Train Loss 1.070807 Test MSE 10974.792772094304 Test RE 0.8773315107994631\n",
      "83 Train Loss 1.048679 Test MSE 10687.338239432644 Test RE 0.8657656303741764\n",
      "84 Train Loss 1.0382236 Test MSE 10600.354414723108 Test RE 0.8622352160505743\n",
      "85 Train Loss 1.0136194 Test MSE 10352.837593152673 Test RE 0.8521092197400906\n",
      "86 Train Loss 0.95296746 Test MSE 9734.916013603612 Test RE 0.8262884236093163\n",
      "87 Train Loss 0.9332366 Test MSE 9491.809527541724 Test RE 0.8159058952007521\n",
      "88 Train Loss 0.8998902 Test MSE 9310.743791688345 Test RE 0.808086314612283\n",
      "89 Train Loss 0.8916348 Test MSE 9249.818726274842 Test RE 0.8054381096937777\n",
      "90 Train Loss 0.88772005 Test MSE 9195.520185565481 Test RE 0.8030705776014886\n",
      "91 Train Loss 0.8702488 Test MSE 8920.247914047186 Test RE 0.790959098286629\n",
      "92 Train Loss 0.84549445 Test MSE 8661.103963680685 Test RE 0.7793852628519633\n",
      "93 Train Loss 0.82610446 Test MSE 8287.930211226516 Test RE 0.7624100394292527\n",
      "94 Train Loss 0.8093807 Test MSE 8077.842936780846 Test RE 0.7526850084011496\n",
      "95 Train Loss 0.8066928 Test MSE 8089.6805018629875 Test RE 0.7532363125047623\n",
      "96 Train Loss 0.7958601 Test MSE 8149.750723337955 Test RE 0.7560277321764644\n",
      "97 Train Loss 0.7921392 Test MSE 8079.728517282722 Test RE 0.7527728514908351\n",
      "98 Train Loss 0.79070646 Test MSE 8008.658150056427 Test RE 0.7494547937677951\n",
      "99 Train Loss 0.77387834 Test MSE 7808.24245119269 Test RE 0.7400178725992007\n",
      "100 Train Loss 0.7553636 Test MSE 7600.839491556013 Test RE 0.730123529440138\n",
      "101 Train Loss 0.7493215 Test MSE 7465.0916879133465 Test RE 0.7235743061851976\n",
      "102 Train Loss 0.7367379 Test MSE 7301.19840226236 Test RE 0.7155873239643236\n",
      "103 Train Loss 0.70390475 Test MSE 6922.967719218526 Test RE 0.6968057359576125\n",
      "104 Train Loss 0.65670365 Test MSE 6465.950987344321 Test RE 0.6734134215447853\n",
      "105 Train Loss 0.6379868 Test MSE 6236.375987978368 Test RE 0.6613505348455968\n",
      "106 Train Loss 0.6253188 Test MSE 6160.633727851568 Test RE 0.6573221367515705\n",
      "107 Train Loss 0.6128283 Test MSE 5986.156859035404 Test RE 0.6479471890397555\n",
      "108 Train Loss 0.60195327 Test MSE 5869.604933397725 Test RE 0.6416083381853404\n",
      "109 Train Loss 0.5996516 Test MSE 5888.063685398039 Test RE 0.6426164122158045\n",
      "110 Train Loss 0.5916486 Test MSE 5837.964922486064 Test RE 0.6398767117463726\n",
      "111 Train Loss 0.57529444 Test MSE 5563.98042844261 Test RE 0.6246810916761709\n",
      "112 Train Loss 0.5606139 Test MSE 5251.239544974756 Test RE 0.6068711312804331\n",
      "113 Train Loss 0.5549915 Test MSE 5143.336204808734 Test RE 0.6006037239619532\n",
      "114 Train Loss 0.53367716 Test MSE 4848.468341849365 Test RE 0.5831333043227025\n",
      "115 Train Loss 0.523774 Test MSE 4737.372646921201 Test RE 0.576413757606838\n",
      "116 Train Loss 0.5103623 Test MSE 4724.618322316603 Test RE 0.5756373015791261\n",
      "117 Train Loss 0.5041697 Test MSE 4694.89784326569 Test RE 0.5738239057445325\n",
      "118 Train Loss 0.49967623 Test MSE 4595.171256070875 Test RE 0.5676967586464954\n",
      "119 Train Loss 0.48186392 Test MSE 4268.842950685214 Test RE 0.5471679519915859\n",
      "120 Train Loss 0.44649476 Test MSE 3965.7259756857065 Test RE 0.5273839573604702\n",
      "121 Train Loss 0.4110158 Test MSE 3652.130108564745 Test RE 0.5061027350190795\n",
      "122 Train Loss 0.3954326 Test MSE 3470.173968758661 Test RE 0.4933341590455273\n",
      "123 Train Loss 0.3889675 Test MSE 3440.3106249709626 Test RE 0.4912068246494701\n",
      "124 Train Loss 0.38017097 Test MSE 3379.2411429986028 Test RE 0.4868275595074352\n",
      "125 Train Loss 0.36918175 Test MSE 3299.280682942571 Test RE 0.48103335983030926\n",
      "126 Train Loss 0.3655193 Test MSE 3305.0162561458646 Test RE 0.48145130002253983\n",
      "127 Train Loss 0.36103696 Test MSE 3315.4606327853166 Test RE 0.4822114312751853\n",
      "128 Train Loss 0.34946287 Test MSE 3308.5396429421426 Test RE 0.48170786264702187\n",
      "129 Train Loss 0.34363768 Test MSE 3263.978532802409 Test RE 0.4784529213140518\n",
      "130 Train Loss 0.33967853 Test MSE 3208.4228956604557 Test RE 0.47436361186707254\n",
      "131 Train Loss 0.33732978 Test MSE 3186.3568990086355 Test RE 0.4727295745771816\n",
      "132 Train Loss 0.32960743 Test MSE 3048.7609327356026 Test RE 0.46241003364936195\n",
      "133 Train Loss 0.32767487 Test MSE 2948.3251383085008 Test RE 0.45472962776243214\n",
      "134 Train Loss 0.327539 Test MSE 2939.5562704473573 Test RE 0.454052898922577\n",
      "135 Train Loss 0.3273811 Test MSE 2934.903010113786 Test RE 0.4536933781390592\n",
      "136 Train Loss 0.32750404 Test MSE 2927.8121935917843 Test RE 0.45314497807806253\n",
      "137 Train Loss 0.32744458 Test MSE 2927.012019707183 Test RE 0.4530830513677442\n",
      "138 Train Loss 0.3275977 Test MSE 2927.1245630274957 Test RE 0.4530917617830909\n",
      "139 Train Loss 0.3276266 Test MSE 2927.1245630274957 Test RE 0.4530917617830909\n",
      "140 Train Loss 0.3275395 Test MSE 2927.1648303962165 Test RE 0.45309487828016803\n",
      "141 Train Loss 0.32755 Test MSE 2927.1648303962165 Test RE 0.45309487828016803\n",
      "142 Train Loss 0.3276378 Test MSE 2927.1648303962165 Test RE 0.45309487828016803\n",
      "143 Train Loss 0.32764202 Test MSE 2927.1648303962165 Test RE 0.45309487828016803\n",
      "144 Train Loss 0.32767186 Test MSE 2927.1648303962165 Test RE 0.45309487828016803\n",
      "145 Train Loss 0.32766965 Test MSE 2927.1648303962165 Test RE 0.45309487828016803\n",
      "146 Train Loss 0.32770103 Test MSE 2927.1648303962165 Test RE 0.45309487828016803\n",
      "147 Train Loss 0.3278406 Test MSE 2927.1648303962165 Test RE 0.45309487828016803\n",
      "148 Train Loss 0.32773158 Test MSE 2927.1648303962165 Test RE 0.45309487828016803\n",
      "149 Train Loss 0.32782406 Test MSE 2927.1648303962165 Test RE 0.45309487828016803\n",
      "150 Train Loss 0.3278697 Test MSE 2927.1648303962165 Test RE 0.45309487828016803\n",
      "151 Train Loss 0.32786888 Test MSE 2927.1648303962165 Test RE 0.45309487828016803\n",
      "152 Train Loss 0.32800218 Test MSE 2927.1648303962165 Test RE 0.45309487828016803\n",
      "153 Train Loss 0.32787517 Test MSE 2927.1648303962165 Test RE 0.45309487828016803\n",
      "154 Train Loss 0.32801658 Test MSE 2927.1648303962165 Test RE 0.45309487828016803\n",
      "155 Train Loss 0.32800716 Test MSE 2927.1648303962165 Test RE 0.45309487828016803\n",
      "156 Train Loss 0.32803646 Test MSE 2927.1648303962165 Test RE 0.45309487828016803\n",
      "157 Train Loss 0.32801437 Test MSE 2927.1648303962165 Test RE 0.45309487828016803\n",
      "158 Train Loss 0.32801765 Test MSE 2927.1648303962165 Test RE 0.45309487828016803\n",
      "159 Train Loss 0.32802635 Test MSE 2927.1648303962165 Test RE 0.45309487828016803\n",
      "160 Train Loss 0.32806584 Test MSE 2927.1648303962165 Test RE 0.45309487828016803\n",
      "161 Train Loss 0.32810372 Test MSE 2927.1648303962165 Test RE 0.45309487828016803\n",
      "162 Train Loss 0.32805145 Test MSE 2927.1648303962165 Test RE 0.45309487828016803\n",
      "163 Train Loss 0.328203 Test MSE 2927.1648303962165 Test RE 0.45309487828016803\n",
      "164 Train Loss 0.3281367 Test MSE 2927.1648303962165 Test RE 0.45309487828016803\n",
      "165 Train Loss 0.32815123 Test MSE 2927.1648303962165 Test RE 0.45309487828016803\n",
      "166 Train Loss 0.32820225 Test MSE 2927.1648303962165 Test RE 0.45309487828016803\n",
      "167 Train Loss 0.3282187 Test MSE 2927.1648303962165 Test RE 0.45309487828016803\n",
      "168 Train Loss 0.3282296 Test MSE 2927.1648303962165 Test RE 0.45309487828016803\n",
      "169 Train Loss 0.3282285 Test MSE 2927.1648303962165 Test RE 0.45309487828016803\n",
      "170 Train Loss 0.3283141 Test MSE 2927.1648303962165 Test RE 0.45309487828016803\n",
      "171 Train Loss 0.32829505 Test MSE 2927.1648303962165 Test RE 0.45309487828016803\n",
      "172 Train Loss 0.32828835 Test MSE 2927.1648303962165 Test RE 0.45309487828016803\n",
      "173 Train Loss 0.32836613 Test MSE 2927.1648303962165 Test RE 0.45309487828016803\n",
      "174 Train Loss 0.32831392 Test MSE 2927.1648303962165 Test RE 0.45309487828016803\n",
      "175 Train Loss 0.32832527 Test MSE 2927.1648303962165 Test RE 0.45309487828016803\n",
      "176 Train Loss 0.3283797 Test MSE 2927.1648303962165 Test RE 0.45309487828016803\n",
      "177 Train Loss 0.32835892 Test MSE 2927.1648303962165 Test RE 0.45309487828016803\n",
      "178 Train Loss 0.32847565 Test MSE 2927.1648303962165 Test RE 0.45309487828016803\n",
      "179 Train Loss 0.32837245 Test MSE 2927.1648303962165 Test RE 0.45309487828016803\n",
      "180 Train Loss 0.32847083 Test MSE 2927.1648303962165 Test RE 0.45309487828016803\n",
      "181 Train Loss 0.32850295 Test MSE 2927.1648303962165 Test RE 0.45309487828016803\n",
      "182 Train Loss 0.32849488 Test MSE 2927.1648303962165 Test RE 0.45309487828016803\n",
      "183 Train Loss 0.32847077 Test MSE 2927.1648303962165 Test RE 0.45309487828016803\n",
      "184 Train Loss 0.32853085 Test MSE 2927.1648303962165 Test RE 0.45309487828016803\n",
      "185 Train Loss 0.32851264 Test MSE 2927.1648303962165 Test RE 0.45309487828016803\n",
      "186 Train Loss 0.32855737 Test MSE 2927.1648303962165 Test RE 0.45309487828016803\n",
      "187 Train Loss 0.32860932 Test MSE 2927.1648303962165 Test RE 0.45309487828016803\n",
      "188 Train Loss 0.3286122 Test MSE 2927.1648303962165 Test RE 0.45309487828016803\n",
      "189 Train Loss 0.32860452 Test MSE 2927.16483022632 Test RE 0.4530948782670189\n",
      "190 Train Loss 0.3286685 Test MSE 2927.16483022632 Test RE 0.4530948782670189\n",
      "191 Train Loss 0.32870054 Test MSE 2927.16483022632 Test RE 0.4530948782670189\n",
      "192 Train Loss 0.32868397 Test MSE 2927.16483022632 Test RE 0.4530948782670189\n",
      "193 Train Loss 0.328716 Test MSE 2927.16483022632 Test RE 0.4530948782670189\n",
      "194 Train Loss 0.32872397 Test MSE 2927.16483022632 Test RE 0.4530948782670189\n",
      "195 Train Loss 0.32871774 Test MSE 2927.16483022632 Test RE 0.4530948782670189\n",
      "196 Train Loss 0.32879072 Test MSE 2927.16483022632 Test RE 0.4530948782670189\n",
      "197 Train Loss 0.32874668 Test MSE 2927.16483022632 Test RE 0.4530948782670189\n",
      "198 Train Loss 0.32876894 Test MSE 2927.16483022632 Test RE 0.4530948782670189\n",
      "199 Train Loss 0.32882676 Test MSE 2927.16483022632 Test RE 0.4530948782670189\n",
      "Training time: 204.03\n",
      "Training time: 204.03\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      "  (m_lambda): Sigmoid()\n",
      ")\n",
      "3\n",
      "0 Train Loss 2.9766521 Test MSE 14262.275943340444 Test RE 1.000138121275164\n",
      "1 Train Loss 1.8697712 Test MSE 14254.53149599389 Test RE 0.9998665457989199\n",
      "2 Train Loss 1.7206707 Test MSE 14248.069873963119 Test RE 0.999639898853756\n",
      "3 Train Loss 1.4686996 Test MSE 14248.363421840208 Test RE 0.9996501964119989\n",
      "4 Train Loss 1.2015096 Test MSE 14241.898364578605 Test RE 0.9994233798701914\n",
      "5 Train Loss 1.1385103 Test MSE 14247.054803530946 Test RE 0.9996042897155946\n",
      "6 Train Loss 1.1443796 Test MSE 14249.082911372947 Test RE 0.9996754354079438\n",
      "7 Train Loss 1.1555843 Test MSE 14246.450465536509 Test RE 0.9995830886580291\n",
      "8 Train Loss 1.1663911 Test MSE 14237.75667139902 Test RE 0.9992780479112054\n",
      "9 Train Loss 1.1762472 Test MSE 14212.672379841619 Test RE 0.9983973883835522\n",
      "10 Train Loss 1.1856122 Test MSE 14205.329224842706 Test RE 0.9981394378101016\n",
      "11 Train Loss 1.1942375 Test MSE 14195.891656303264 Test RE 0.997807816686942\n",
      "12 Train Loss 1.2005998 Test MSE 14165.55895696228 Test RE 0.9967412267832377\n",
      "13 Train Loss 1.2059035 Test MSE 14122.16374651488 Test RE 0.9952133318191367\n",
      "14 Train Loss 1.2084198 Test MSE 14061.355589070366 Test RE 0.993068392299854\n",
      "15 Train Loss 1.2106159 Test MSE 13993.656674297748 Test RE 0.990674925776611\n",
      "16 Train Loss 1.2137817 Test MSE 13940.93611673428 Test RE 0.9888070001452925\n",
      "17 Train Loss 1.2128519 Test MSE 13844.88980041049 Test RE 0.9853949117608801\n",
      "18 Train Loss 1.2134166 Test MSE 13781.85982256036 Test RE 0.9831493081026591\n",
      "19 Train Loss 1.2124234 Test MSE 13709.989519993187 Test RE 0.9805824702655476\n",
      "20 Train Loss 1.21375 Test MSE 13648.493077910212 Test RE 0.978380787072082\n",
      "21 Train Loss 1.2142696 Test MSE 13568.63278823473 Test RE 0.9755142287407533\n",
      "22 Train Loss 1.213535 Test MSE 13448.064980263389 Test RE 0.9711704581561413\n",
      "23 Train Loss 1.2134858 Test MSE 13387.243528502966 Test RE 0.9689718171137489\n",
      "24 Train Loss 1.2119707 Test MSE 13350.146797103484 Test RE 0.9676283507179387\n",
      "25 Train Loss 1.2088411 Test MSE 13325.103651924914 Test RE 0.9667203520066462\n",
      "26 Train Loss 1.2020963 Test MSE 13239.016259060665 Test RE 0.9635925231964317\n",
      "27 Train Loss 1.1893293 Test MSE 13037.875889425075 Test RE 0.9562445765964581\n",
      "28 Train Loss 1.1802294 Test MSE 12892.20210651484 Test RE 0.9508874517693743\n",
      "29 Train Loss 1.1672746 Test MSE 12702.807175803977 Test RE 0.9438770284486581\n",
      "30 Train Loss 1.162031 Test MSE 12569.463987164247 Test RE 0.9389099527691316\n",
      "31 Train Loss 1.1562066 Test MSE 12532.123715198064 Test RE 0.9375142994046418\n",
      "32 Train Loss 1.1538687 Test MSE 12497.51296797591 Test RE 0.9362188084876006\n",
      "33 Train Loss 1.1547648 Test MSE 12464.350662249173 Test RE 0.9349758492756085\n",
      "34 Train Loss 1.1551343 Test MSE 12444.73556897263 Test RE 0.9342398759414645\n",
      "35 Train Loss 1.1534795 Test MSE 12391.601735806764 Test RE 0.9322433351511715\n",
      "36 Train Loss 1.1507075 Test MSE 12260.208803694251 Test RE 0.927287695698651\n",
      "37 Train Loss 1.13829 Test MSE 12067.811392184212 Test RE 0.9199830391291021\n",
      "38 Train Loss 1.0870435 Test MSE 11640.971299365468 Test RE 0.9035666082891264\n",
      "39 Train Loss 1.0704076 Test MSE 11460.044462416265 Test RE 0.8965173836840384\n",
      "40 Train Loss 1.0683988 Test MSE 11426.099377637365 Test RE 0.8951886398217793\n",
      "41 Train Loss 1.0558796 Test MSE 11237.311421225995 Test RE 0.8877624510807861\n",
      "42 Train Loss 1.0329198 Test MSE 10937.38221658569 Test RE 0.8758349232169046\n",
      "43 Train Loss 1.0042024 Test MSE 10631.879817588053 Test RE 0.8635164058800638\n",
      "44 Train Loss 1.003547 Test MSE 10619.43752371093 Test RE 0.8630109792608861\n",
      "45 Train Loss 1.00039 Test MSE 10594.470732907534 Test RE 0.8619958928349786\n",
      "46 Train Loss 0.9768293 Test MSE 10386.844216030824 Test RE 0.8535075610009867\n",
      "47 Train Loss 0.94914526 Test MSE 10133.825899960995 Test RE 0.8430479625747295\n",
      "48 Train Loss 0.9401845 Test MSE 9989.776828501268 Test RE 0.8370346892676398\n",
      "49 Train Loss 0.9323313 Test MSE 9840.507294320989 Test RE 0.8307575704245406\n",
      "50 Train Loss 0.9129267 Test MSE 9660.890445189745 Test RE 0.8231408261462031\n",
      "51 Train Loss 0.89553255 Test MSE 9463.401911636975 Test RE 0.8146840360138409\n",
      "52 Train Loss 0.8919375 Test MSE 9355.986993587974 Test RE 0.8100472807105619\n",
      "53 Train Loss 0.88505405 Test MSE 9263.537643128273 Test RE 0.8060351832527611\n",
      "54 Train Loss 0.84989667 Test MSE 8899.339779625177 Test RE 0.7900315914944706\n",
      "55 Train Loss 0.83868533 Test MSE 8785.06962921187 Test RE 0.7849430847872524\n",
      "56 Train Loss 0.83248913 Test MSE 8685.19669971452 Test RE 0.7804685244765414\n",
      "57 Train Loss 0.81990683 Test MSE 8424.9989983076 Test RE 0.7686886926987909\n",
      "58 Train Loss 0.8152032 Test MSE 8374.749158617278 Test RE 0.7663928911393929\n",
      "59 Train Loss 0.8078989 Test MSE 8317.811422386563 Test RE 0.7637831951797686\n",
      "60 Train Loss 0.80117625 Test MSE 8166.265517138254 Test RE 0.7567933582458414\n",
      "61 Train Loss 0.7951228 Test MSE 8150.164054933038 Test RE 0.7560469036945284\n",
      "62 Train Loss 0.79067546 Test MSE 8079.705179186916 Test RE 0.7527717643071767\n",
      "63 Train Loss 0.7687298 Test MSE 7741.8752440994795 Test RE 0.7368662206070589\n",
      "64 Train Loss 0.7378134 Test MSE 7541.085283969944 Test RE 0.7272479235753226\n",
      "65 Train Loss 0.719867 Test MSE 7293.325551567679 Test RE 0.7152014125446271\n",
      "66 Train Loss 0.7132993 Test MSE 7131.188541648835 Test RE 0.7072069552184529\n",
      "67 Train Loss 0.70874524 Test MSE 7039.749078231054 Test RE 0.7026582561904622\n",
      "68 Train Loss 0.69722164 Test MSE 6782.829870649092 Test RE 0.68971715129923\n",
      "69 Train Loss 0.66072434 Test MSE 6578.823060341814 Test RE 0.6792656712881341\n",
      "70 Train Loss 0.6445309 Test MSE 6515.478849538368 Test RE 0.6759876060604026\n",
      "71 Train Loss 0.6422175 Test MSE 6450.914804544915 Test RE 0.6726299744709773\n",
      "72 Train Loss 0.64141244 Test MSE 6452.149520034194 Test RE 0.6726943426216131\n",
      "73 Train Loss 0.6404518 Test MSE 6482.573248397771 Test RE 0.6742784504444922\n",
      "74 Train Loss 0.64037025 Test MSE 6483.470980512055 Test RE 0.674325137187578\n",
      "75 Train Loss 0.64053476 Test MSE 6483.798433933105 Test RE 0.6743421656655146\n",
      "76 Train Loss 0.6413376 Test MSE 6485.307708529081 Test RE 0.6744206465350899\n",
      "77 Train Loss 0.6405434 Test MSE 6478.669294379564 Test RE 0.6740753869078248\n",
      "78 Train Loss 0.6379073 Test MSE 6469.888231273169 Test RE 0.6736184176398973\n",
      "79 Train Loss 0.6360706 Test MSE 6430.471628225904 Test RE 0.6715633347007676\n",
      "80 Train Loss 0.6311172 Test MSE 6397.579098790342 Test RE 0.6698435746330959\n",
      "81 Train Loss 0.62602234 Test MSE 6311.271382766492 Test RE 0.6653099082528169\n",
      "82 Train Loss 0.62136495 Test MSE 6223.84281591443 Test RE 0.6606856463959634\n",
      "83 Train Loss 0.6145274 Test MSE 6116.924437955628 Test RE 0.6549861573720934\n",
      "84 Train Loss 0.6058643 Test MSE 6034.448464076244 Test RE 0.650555503284046\n",
      "85 Train Loss 0.599384 Test MSE 5919.465162949799 Test RE 0.6443276939935444\n",
      "86 Train Loss 0.584222 Test MSE 5654.558517102303 Test RE 0.6297452713652184\n",
      "87 Train Loss 0.5751426 Test MSE 5532.405440064666 Test RE 0.6229060709456556\n",
      "88 Train Loss 0.56508577 Test MSE 5474.8494647620455 Test RE 0.6196574207095884\n",
      "89 Train Loss 0.5525969 Test MSE 5282.005475822878 Test RE 0.6086463014662313\n",
      "90 Train Loss 0.5422419 Test MSE 5143.320472141168 Test RE 0.6006028053844408\n",
      "91 Train Loss 0.53423125 Test MSE 5045.4896295675335 Test RE 0.594863363919622\n",
      "92 Train Loss 0.5314338 Test MSE 5009.312583647656 Test RE 0.5927268899599876\n",
      "93 Train Loss 0.529274 Test MSE 4992.2889906358605 Test RE 0.5917188745389786\n",
      "94 Train Loss 0.52604026 Test MSE 4935.804296378096 Test RE 0.5883618835085109\n",
      "95 Train Loss 0.5231327 Test MSE 4885.2707589136535 Test RE 0.5853422642323296\n",
      "96 Train Loss 0.5227934 Test MSE 4878.586680356915 Test RE 0.584941691463899\n",
      "97 Train Loss 0.5226 Test MSE 4878.634995063402 Test RE 0.5849445879191651\n",
      "98 Train Loss 0.52334404 Test MSE 4879.325864138169 Test RE 0.5849860037886335\n",
      "99 Train Loss 0.5228971 Test MSE 4879.316873050324 Test RE 0.5849854648142838\n",
      "100 Train Loss 0.5229951 Test MSE 4879.316873050324 Test RE 0.5849854648142838\n",
      "101 Train Loss 0.52303374 Test MSE 4879.316873050324 Test RE 0.5849854648142838\n",
      "102 Train Loss 0.52328974 Test MSE 4880.545207679748 Test RE 0.5850590932243172\n",
      "103 Train Loss 0.52381575 Test MSE 4880.54525640093 Test RE 0.5850590961445618\n",
      "104 Train Loss 0.5235736 Test MSE 4879.980357073549 Test RE 0.5850252362952512\n",
      "105 Train Loss 0.52380264 Test MSE 4880.48002396448 Test RE 0.5850551862373864\n",
      "106 Train Loss 0.5241631 Test MSE 4880.957899740591 Test RE 0.5850838285898029\n",
      "107 Train Loss 0.5240759 Test MSE 4881.744979379015 Test RE 0.5851310005808753\n",
      "108 Train Loss 0.5238198 Test MSE 4881.744979379015 Test RE 0.5851310005808753\n",
      "109 Train Loss 0.5241411 Test MSE 4881.744979379015 Test RE 0.5851310005808753\n",
      "110 Train Loss 0.52359796 Test MSE 4882.870124586411 Test RE 0.58519842722991\n",
      "111 Train Loss 0.52440566 Test MSE 4888.6255645160245 Test RE 0.5855432124080868\n",
      "112 Train Loss 0.5226586 Test MSE 4913.285127638687 Test RE 0.5870181746986461\n",
      "113 Train Loss 0.5209683 Test MSE 4941.58715252949 Test RE 0.5887064490482102\n",
      "114 Train Loss 0.5207853 Test MSE 4943.023862478887 Test RE 0.5887920226623906\n",
      "115 Train Loss 0.5191435 Test MSE 4896.0426339784435 Test RE 0.5859872399322866\n",
      "116 Train Loss 0.515289 Test MSE 4841.5942172301975 Test RE 0.5827197765294174\n",
      "117 Train Loss 0.5111542 Test MSE 4807.465609439147 Test RE 0.5806623358710404\n",
      "118 Train Loss 0.5057126 Test MSE 4784.23439550978 Test RE 0.5792576636464224\n",
      "119 Train Loss 0.4951666 Test MSE 4696.284690530011 Test RE 0.5739086517155766\n",
      "120 Train Loss 0.48768082 Test MSE 4620.519340886201 Test RE 0.5692603821341405\n",
      "121 Train Loss 0.48038357 Test MSE 4629.140983022411 Test RE 0.5697912393356019\n",
      "122 Train Loss 0.47793525 Test MSE 4629.453197492175 Test RE 0.5698104539232363\n",
      "123 Train Loss 0.47434357 Test MSE 4590.054297138579 Test RE 0.5673805908725276\n",
      "124 Train Loss 0.47334045 Test MSE 4565.011661598194 Test RE 0.565830703048859\n",
      "125 Train Loss 0.47168422 Test MSE 4553.587441899079 Test RE 0.5651222466352194\n",
      "126 Train Loss 0.46885002 Test MSE 4547.418266935695 Test RE 0.5647393046495769\n",
      "127 Train Loss 0.46626464 Test MSE 4558.562766610324 Test RE 0.5654308932918799\n",
      "128 Train Loss 0.45813513 Test MSE 4518.652815471059 Test RE 0.5629502947364495\n",
      "129 Train Loss 0.44515863 Test MSE 4398.101611599101 Test RE 0.5553901756261603\n",
      "130 Train Loss 0.44242758 Test MSE 4321.242453810139 Test RE 0.5505159177991666\n",
      "131 Train Loss 0.43248188 Test MSE 4171.802624813719 Test RE 0.5409130262848865\n",
      "132 Train Loss 0.4176889 Test MSE 4062.046569968771 Test RE 0.5337501533631978\n",
      "133 Train Loss 0.40640554 Test MSE 3919.1562816383316 Test RE 0.5242782663991399\n",
      "134 Train Loss 0.396357 Test MSE 3764.120239463728 Test RE 0.5138037950247406\n",
      "135 Train Loss 0.3922432 Test MSE 3613.0200472040283 Test RE 0.5033855554580872\n",
      "136 Train Loss 0.3902492 Test MSE 3551.047858108085 Test RE 0.49904973171924577\n",
      "137 Train Loss 0.3831579 Test MSE 3520.7506243387734 Test RE 0.4969162466791267\n",
      "138 Train Loss 0.35603008 Test MSE 3323.04239042733 Test RE 0.48276247423190244\n",
      "139 Train Loss 0.34478515 Test MSE 3232.424405838631 Test RE 0.47613461115537203\n",
      "140 Train Loss 0.33365065 Test MSE 3216.5001967574526 Test RE 0.47496034885433835\n",
      "141 Train Loss 0.32814163 Test MSE 3204.9647094996435 Test RE 0.47410789745467385\n",
      "142 Train Loss 0.3247936 Test MSE 3174.074664749883 Test RE 0.4718175955463148\n",
      "143 Train Loss 0.32255894 Test MSE 3167.4702003366906 Test RE 0.471326472069887\n",
      "144 Train Loss 0.32069358 Test MSE 3144.334406725216 Test RE 0.4696019890451149\n",
      "145 Train Loss 0.3188432 Test MSE 3125.5943611136695 Test RE 0.4682004977938773\n",
      "146 Train Loss 0.31853262 Test MSE 3115.4163027992736 Test RE 0.46743756166261324\n",
      "147 Train Loss 0.31623945 Test MSE 3156.483822574377 Test RE 0.47050836356816084\n",
      "148 Train Loss 0.31394768 Test MSE 3182.327710846462 Test RE 0.4724305938004483\n",
      "149 Train Loss 0.310955 Test MSE 3084.2914168040925 Test RE 0.4650967089110446\n",
      "150 Train Loss 0.3063285 Test MSE 3008.7138910677045 Test RE 0.45936299776071954\n",
      "151 Train Loss 0.3032293 Test MSE 2991.2842532918003 Test RE 0.4580305081619811\n",
      "152 Train Loss 0.29828337 Test MSE 2900.3651924304313 Test RE 0.45101595538842465\n",
      "153 Train Loss 0.29132152 Test MSE 2800.4982568062765 Test RE 0.4431831266360851\n",
      "154 Train Loss 0.2886236 Test MSE 2765.9231687638717 Test RE 0.4404388498397006\n",
      "155 Train Loss 0.285841 Test MSE 2695.8412585867477 Test RE 0.4348232137969478\n",
      "156 Train Loss 0.28388178 Test MSE 2682.537847113898 Test RE 0.43374900620731355\n",
      "157 Train Loss 0.2813549 Test MSE 2686.238117026589 Test RE 0.4340480579622808\n",
      "158 Train Loss 0.27846342 Test MSE 2624.9729663469207 Test RE 0.4290698330410548\n",
      "159 Train Loss 0.27611947 Test MSE 2620.288666643354 Test RE 0.42868682163709537\n",
      "160 Train Loss 0.2740584 Test MSE 2668.2360854102094 Test RE 0.4325912098203438\n",
      "161 Train Loss 0.26833335 Test MSE 2598.421595453915 Test RE 0.4268943159389065\n",
      "162 Train Loss 0.2631412 Test MSE 2514.9633002104015 Test RE 0.41998268888011586\n",
      "163 Train Loss 0.26129806 Test MSE 2478.065657696521 Test RE 0.41689047085804753\n",
      "164 Train Loss 0.25979295 Test MSE 2465.913926455537 Test RE 0.41586705836258847\n",
      "165 Train Loss 0.25871035 Test MSE 2471.8557627839978 Test RE 0.41636779101753313\n",
      "166 Train Loss 0.25165522 Test MSE 2390.3589808558845 Test RE 0.40944646652215205\n",
      "167 Train Loss 0.24755985 Test MSE 2301.806695734914 Test RE 0.40179080000375356\n",
      "168 Train Loss 0.24484727 Test MSE 2201.8566944624135 Test RE 0.39297062603672334\n",
      "169 Train Loss 0.24177097 Test MSE 2137.8098070636765 Test RE 0.3872131486382259\n",
      "170 Train Loss 0.2379968 Test MSE 2058.5238193202485 Test RE 0.37996492833876977\n",
      "171 Train Loss 0.23585922 Test MSE 2005.9016552998812 Test RE 0.3750769554559366\n",
      "172 Train Loss 0.23514615 Test MSE 1980.3368043550654 Test RE 0.3726791473378102\n",
      "173 Train Loss 0.23417094 Test MSE 1913.4607950048899 Test RE 0.36633241410158607\n",
      "174 Train Loss 0.23344186 Test MSE 1860.7820361887445 Test RE 0.3612545420139093\n",
      "175 Train Loss 0.2317645 Test MSE 1804.6699991383462 Test RE 0.3557660181886843\n",
      "176 Train Loss 0.23163603 Test MSE 1760.5104443274422 Test RE 0.35138633371343303\n",
      "177 Train Loss 0.22995712 Test MSE 1754.2060343065855 Test RE 0.35075661000770253\n",
      "178 Train Loss 0.22623038 Test MSE 1710.0942100669736 Test RE 0.34631841250810197\n",
      "179 Train Loss 0.22002323 Test MSE 1587.9089759957742 Test RE 0.33371702674835607\n",
      "180 Train Loss 0.20586018 Test MSE 1492.6557155495796 Test RE 0.32355296789417204\n",
      "181 Train Loss 0.18388975 Test MSE 1492.8721970361046 Test RE 0.3235764296634543\n",
      "182 Train Loss 0.16995218 Test MSE 1457.6229609837208 Test RE 0.31973351650431586\n",
      "183 Train Loss 0.15715516 Test MSE 1362.0706840092391 Test RE 0.30907607367891105\n",
      "184 Train Loss 0.1508928 Test MSE 1320.2578408538593 Test RE 0.3042950876586562\n",
      "185 Train Loss 0.14841494 Test MSE 1290.604862105193 Test RE 0.30085844894476893\n",
      "186 Train Loss 0.14287172 Test MSE 1271.6203625939681 Test RE 0.2986374720150442\n",
      "187 Train Loss 0.13501012 Test MSE 1260.301909375395 Test RE 0.2973054434250591\n",
      "188 Train Loss 0.12944463 Test MSE 1210.1063639180184 Test RE 0.29132471898211326\n",
      "189 Train Loss 0.12511419 Test MSE 1125.8558493635564 Test RE 0.2810004128763418\n",
      "190 Train Loss 0.12424099 Test MSE 1068.3794839542495 Test RE 0.27373374152416086\n",
      "191 Train Loss 0.12415859 Test MSE 1058.561376922799 Test RE 0.2724730704806349\n",
      "192 Train Loss 0.12396197 Test MSE 1053.5864008515334 Test RE 0.27183203847594534\n",
      "193 Train Loss 0.123394385 Test MSE 1035.2154156544887 Test RE 0.269451700671021\n",
      "194 Train Loss 0.121100634 Test MSE 1039.8519041825762 Test RE 0.2700544322292137\n",
      "195 Train Loss 0.12029123 Test MSE 1048.5725526706553 Test RE 0.2711844646628663\n",
      "196 Train Loss 0.118956946 Test MSE 1013.5426438410378 Test RE 0.26661622606585056\n",
      "197 Train Loss 0.11798607 Test MSE 999.1559012769291 Test RE 0.2647172195898221\n",
      "198 Train Loss 0.117284514 Test MSE 1014.818937181966 Test RE 0.266784040148303\n",
      "199 Train Loss 0.11743246 Test MSE 1021.9467853844487 Test RE 0.26771931473589095\n",
      "Training time: 232.73\n",
      "Training time: 232.73\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      "  (m_lambda): Sigmoid()\n",
      ")\n",
      "4\n",
      "0 Train Loss 1.8874285 Test MSE 14250.217633083006 Test RE 0.9997152390515902\n",
      "1 Train Loss 1.5983621 Test MSE 14250.61215373387 Test RE 0.9997290776324163\n",
      "2 Train Loss 1.1251459 Test MSE 14255.490844371112 Test RE 0.9999001913876848\n",
      "3 Train Loss 1.1012102 Test MSE 14250.449519103884 Test RE 0.9997233729289702\n",
      "4 Train Loss 1.1110238 Test MSE 14237.871791582163 Test RE 0.9992820877624969\n",
      "5 Train Loss 1.1238499 Test MSE 14224.525449642495 Test RE 0.9988136228292447\n",
      "6 Train Loss 1.1356786 Test MSE 14190.936252490095 Test RE 0.9976336475556118\n",
      "7 Train Loss 1.1482714 Test MSE 14178.52670668691 Test RE 0.9971973518830969\n",
      "8 Train Loss 1.1581576 Test MSE 14150.944770622358 Test RE 0.9962269399493503\n",
      "9 Train Loss 1.1689978 Test MSE 14132.924022751047 Test RE 0.9955924072733843\n",
      "10 Train Loss 1.1767011 Test MSE 14094.759761577436 Test RE 0.9942472597927821\n",
      "11 Train Loss 1.181372 Test MSE 14019.807921537664 Test RE 0.9916001768719109\n",
      "12 Train Loss 1.1883373 Test MSE 13978.81814610568 Test RE 0.9901495428425374\n",
      "13 Train Loss 1.1932515 Test MSE 13919.127402725662 Test RE 0.9880332698437936\n",
      "14 Train Loss 1.1886749 Test MSE 13781.008920336846 Test RE 0.9831189574512956\n",
      "15 Train Loss 1.1877378 Test MSE 13666.293687332536 Test RE 0.9790185900649214\n",
      "16 Train Loss 1.192631 Test MSE 13628.98305167913 Test RE 0.9776812570552648\n",
      "17 Train Loss 1.1849737 Test MSE 13509.248856336455 Test RE 0.973377189810735\n",
      "18 Train Loss 1.1851047 Test MSE 13419.757748641458 Test RE 0.9701477969360074\n",
      "19 Train Loss 1.1811575 Test MSE 13289.6628043914 Test RE 0.9654339004658856\n",
      "20 Train Loss 1.1772281 Test MSE 13210.762018873082 Test RE 0.9625637414672611\n",
      "21 Train Loss 1.1716771 Test MSE 13027.438729956772 Test RE 0.955861750612026\n",
      "22 Train Loss 1.1581893 Test MSE 12859.678975893232 Test RE 0.9496872935139958\n",
      "23 Train Loss 1.1562593 Test MSE 12757.911984278831 Test RE 0.9459220834098065\n",
      "24 Train Loss 1.1472125 Test MSE 12584.16645619142 Test RE 0.939458912540263\n",
      "25 Train Loss 1.1375248 Test MSE 12506.801632650493 Test RE 0.9365666619905\n",
      "26 Train Loss 1.1354607 Test MSE 12464.161866453018 Test RE 0.934968768273835\n",
      "27 Train Loss 1.1295894 Test MSE 12371.363884805707 Test RE 0.9314817584154426\n",
      "28 Train Loss 1.118544 Test MSE 12199.496205432673 Test RE 0.9249888802224515\n",
      "29 Train Loss 1.1076425 Test MSE 11862.413036221456 Test RE 0.9121202225860667\n",
      "30 Train Loss 1.0992157 Test MSE 11707.86624063408 Test RE 0.906159065855948\n",
      "31 Train Loss 1.0810568 Test MSE 11548.9286853084 Test RE 0.8999873674036422\n",
      "32 Train Loss 1.0518655 Test MSE 11338.589312242691 Test RE 0.8917540212123035\n",
      "33 Train Loss 1.0310619 Test MSE 10942.89982111979 Test RE 0.876055812518143\n",
      "34 Train Loss 1.0290561 Test MSE 10796.624522506236 Test RE 0.8701809325434331\n",
      "35 Train Loss 1.029518 Test MSE 10721.351085325226 Test RE 0.8671422014320413\n",
      "36 Train Loss 1.0158831 Test MSE 10690.204522137985 Test RE 0.8658817192747509\n",
      "37 Train Loss 0.9835051 Test MSE 10376.137049072204 Test RE 0.8530675330146259\n",
      "38 Train Loss 0.9590215 Test MSE 10163.221496637978 Test RE 0.8442698087307422\n",
      "39 Train Loss 0.9500366 Test MSE 10092.643084651061 Test RE 0.8413331889822081\n",
      "40 Train Loss 0.9153126 Test MSE 9680.41361237311 Test RE 0.8239721266014823\n",
      "41 Train Loss 0.8792646 Test MSE 9205.169691132625 Test RE 0.803491826316354\n",
      "42 Train Loss 0.8025836 Test MSE 8498.450868965601 Test RE 0.7720322594641706\n",
      "43 Train Loss 0.7686249 Test MSE 8064.490583799677 Test RE 0.752062671934537\n",
      "44 Train Loss 0.703046 Test MSE 7349.795237723318 Test RE 0.7179648518148255\n",
      "45 Train Loss 0.69949764 Test MSE 7327.867852523611 Test RE 0.7168930633572834\n",
      "46 Train Loss 0.69154394 Test MSE 7236.896036714144 Test RE 0.7124292315455726\n",
      "47 Train Loss 0.6806878 Test MSE 7131.756608138529 Test RE 0.7072351225127196\n",
      "48 Train Loss 0.6712444 Test MSE 6964.083311271321 Test RE 0.6988718418412666\n",
      "49 Train Loss 0.6554325 Test MSE 6622.126032969741 Test RE 0.6814975278638805\n",
      "50 Train Loss 0.62482387 Test MSE 6247.648645091624 Test RE 0.6619479821400215\n",
      "51 Train Loss 0.57157516 Test MSE 5683.395400543946 Test RE 0.6313490037359784\n",
      "52 Train Loss 0.5531367 Test MSE 5463.896756825987 Test RE 0.6190372826958735\n",
      "53 Train Loss 0.5512436 Test MSE 5369.713190922639 Test RE 0.6136787831194582\n",
      "54 Train Loss 0.55078334 Test MSE 5391.981877794476 Test RE 0.6149499573078752\n",
      "55 Train Loss 0.5486193 Test MSE 5371.338924768642 Test RE 0.6137716747535175\n",
      "56 Train Loss 0.5365447 Test MSE 5136.535517881507 Test RE 0.6002065237017179\n",
      "57 Train Loss 0.50807095 Test MSE 4864.969783284685 Test RE 0.5841247892490508\n",
      "58 Train Loss 0.50280994 Test MSE 4697.3134804751435 Test RE 0.5739715098211955\n",
      "59 Train Loss 0.50241363 Test MSE 4684.975474207223 Test RE 0.5732172147945588\n",
      "60 Train Loss 0.49809054 Test MSE 4554.454814739641 Test RE 0.5651760666567609\n",
      "61 Train Loss 0.47197375 Test MSE 4100.116568342528 Test RE 0.5362455062419178\n",
      "62 Train Loss 0.43686852 Test MSE 3762.574125850384 Test RE 0.5136982616486516\n",
      "63 Train Loss 0.4185773 Test MSE 3486.429699920667 Test RE 0.4944882997016719\n",
      "64 Train Loss 0.4010187 Test MSE 3437.4699669662496 Test RE 0.4910039884953963\n",
      "65 Train Loss 0.39248428 Test MSE 3383.66466054966 Test RE 0.48714609048516677\n",
      "66 Train Loss 0.38907492 Test MSE 3263.861906410269 Test RE 0.4784443733508949\n",
      "67 Train Loss 0.3834285 Test MSE 3227.6693692190934 Test RE 0.4757842749270716\n",
      "68 Train Loss 0.37151992 Test MSE 3290.886176342581 Test RE 0.4804210127894262\n",
      "69 Train Loss 0.35872263 Test MSE 3281.0620587825088 Test RE 0.4797033884319435\n",
      "70 Train Loss 0.3488572 Test MSE 3238.5654620302253 Test RE 0.4765866839019431\n",
      "71 Train Loss 0.33543733 Test MSE 3027.3275465828283 Test RE 0.4607817502874674\n",
      "72 Train Loss 0.33089104 Test MSE 2933.828689584665 Test RE 0.4536103333636751\n",
      "73 Train Loss 0.32675347 Test MSE 2955.7273324674875 Test RE 0.4553001019774284\n",
      "74 Train Loss 0.3186916 Test MSE 2931.7025198456067 Test RE 0.4534459360068076\n",
      "75 Train Loss 0.3153734 Test MSE 2872.902054277715 Test RE 0.4488755742008956\n",
      "76 Train Loss 0.3146568 Test MSE 2821.011059264083 Test RE 0.4448032564688398\n",
      "77 Train Loss 0.31372976 Test MSE 2799.95994789508 Test RE 0.4431405304853331\n",
      "78 Train Loss 0.31178933 Test MSE 2851.3772763792995 Test RE 0.44719084693976324\n",
      "79 Train Loss 0.31047606 Test MSE 2829.9592643603764 Test RE 0.4455081525862765\n",
      "80 Train Loss 0.30863956 Test MSE 2813.3522897963385 Test RE 0.4441990474016481\n",
      "81 Train Loss 0.3012973 Test MSE 2799.3652746183957 Test RE 0.4430934694862648\n",
      "82 Train Loss 0.28599346 Test MSE 2778.1190177542912 Test RE 0.4414088004122623\n",
      "83 Train Loss 0.27199176 Test MSE 2716.707857387111 Test RE 0.4365027992849944\n",
      "84 Train Loss 0.26212686 Test MSE 2617.051216390807 Test RE 0.42842191166255444\n",
      "85 Train Loss 0.25637668 Test MSE 2536.9799710781185 Test RE 0.42181700428979885\n",
      "86 Train Loss 0.24731709 Test MSE 2434.9662988490472 Test RE 0.41324921866819486\n",
      "87 Train Loss 0.24007367 Test MSE 2376.3933290846708 Test RE 0.408248620629851\n",
      "88 Train Loss 0.2356234 Test MSE 2301.884482673927 Test RE 0.4017975889779253\n",
      "89 Train Loss 0.23327972 Test MSE 2240.2618092612192 Test RE 0.39638293715642925\n",
      "90 Train Loss 0.22766186 Test MSE 2250.4929811703432 Test RE 0.397287037342722\n",
      "91 Train Loss 0.22571044 Test MSE 2255.43861053031 Test RE 0.39772333200546434\n",
      "92 Train Loss 0.2225784 Test MSE 2230.9309403866932 Test RE 0.39555659255806774\n",
      "93 Train Loss 0.21817839 Test MSE 2118.317204038172 Test RE 0.38544379659023115\n",
      "94 Train Loss 0.21251474 Test MSE 2002.9040632408412 Test RE 0.37479659573357077\n",
      "95 Train Loss 0.21006323 Test MSE 1914.6656468523552 Test RE 0.36644773050326296\n",
      "96 Train Loss 0.20928657 Test MSE 1862.3792446458185 Test RE 0.3614095507775206\n",
      "97 Train Loss 0.20864564 Test MSE 1807.016583617004 Test RE 0.3559972415736364\n",
      "98 Train Loss 0.2087411 Test MSE 1801.2915221466724 Test RE 0.35543285191219326\n",
      "99 Train Loss 0.20867087 Test MSE 1801.2915221466724 Test RE 0.35543285191219326\n",
      "100 Train Loss 0.20877032 Test MSE 1801.2915221466724 Test RE 0.35543285191219326\n",
      "101 Train Loss 0.20875974 Test MSE 1801.2915221466724 Test RE 0.35543285191219326\n",
      "102 Train Loss 0.20883249 Test MSE 1801.2915221466724 Test RE 0.35543285191219326\n",
      "103 Train Loss 0.20885971 Test MSE 1801.2860892063711 Test RE 0.3554323158948696\n",
      "104 Train Loss 0.20882651 Test MSE 1801.2856825015197 Test RE 0.35543227576907943\n",
      "105 Train Loss 0.20884117 Test MSE 1785.7564311487292 Test RE 0.35389683218149964\n",
      "106 Train Loss 0.20566678 Test MSE 1692.765120421421 Test RE 0.3445592509583914\n",
      "107 Train Loss 0.19215159 Test MSE 1552.7556023022041 Test RE 0.3300024135273733\n",
      "108 Train Loss 0.17837922 Test MSE 1472.5525769697967 Test RE 0.32136677073993647\n",
      "109 Train Loss 0.15820393 Test MSE 1190.47335290968 Test RE 0.2889517992866569\n",
      "110 Train Loss 0.13764015 Test MSE 1025.4402989285218 Test RE 0.2681765220566553\n",
      "111 Train Loss 0.13142961 Test MSE 920.8871068490798 Test RE 0.25413750389547335\n",
      "112 Train Loss 0.126759 Test MSE 789.4006344241537 Test RE 0.23529586726553697\n",
      "113 Train Loss 0.12089755 Test MSE 716.3287772079516 Test RE 0.22414123627558472\n",
      "114 Train Loss 0.1119999 Test MSE 694.40139884245 Test RE 0.22068400514483616\n",
      "115 Train Loss 0.08990915 Test MSE 608.7862914938312 Test RE 0.20663219828335985\n",
      "116 Train Loss 0.0817952 Test MSE 504.84886656957735 Test RE 0.1881682112773904\n",
      "117 Train Loss 0.07391526 Test MSE 425.48573676044344 Test RE 0.17274602661560567\n",
      "118 Train Loss 0.07077867 Test MSE 383.75506948347385 Test RE 0.16405619195987522\n",
      "119 Train Loss 0.06545067 Test MSE 341.7889756350115 Test RE 0.15482624785983728\n",
      "120 Train Loss 0.058454007 Test MSE 311.48377881746353 Test RE 0.14780301088716596\n",
      "121 Train Loss 0.05218931 Test MSE 220.50462032320212 Test RE 0.12435819523977854\n",
      "122 Train Loss 0.04639046 Test MSE 164.45043339050596 Test RE 0.10739475582694902\n",
      "123 Train Loss 0.04309071 Test MSE 152.11215006375033 Test RE 0.10328744066042961\n",
      "124 Train Loss 0.040502805 Test MSE 141.86203541474856 Test RE 0.09974672770852828\n",
      "125 Train Loss 0.034527875 Test MSE 100.67444207438983 Test RE 0.08402822303130844\n",
      "126 Train Loss 0.022366753 Test MSE 86.78850670831218 Test RE 0.07801833391760502\n",
      "127 Train Loss 0.017287064 Test MSE 81.64013512759642 Test RE 0.07566890009072517\n",
      "128 Train Loss 0.015008012 Test MSE 62.14167108944893 Test RE 0.06601718892726641\n",
      "129 Train Loss 0.01410527 Test MSE 49.48902054345198 Test RE 0.05891420112756713\n",
      "130 Train Loss 0.012290506 Test MSE 40.10378331078593 Test RE 0.053034470388125636\n",
      "131 Train Loss 0.0092830695 Test MSE 28.72898137746269 Test RE 0.044887526823225006\n",
      "132 Train Loss 0.008598471 Test MSE 20.980622733860447 Test RE 0.03835966016346364\n",
      "133 Train Loss 0.0074171005 Test MSE 8.370993902012456 Test RE 0.024230036956120644\n",
      "134 Train Loss 0.006635996 Test MSE 1.8538415637729773 Test RE 0.011402547005406676\n",
      "135 Train Loss 0.00543112 Test MSE 0.3076785188477509 Test RE 0.004645304121040584\n",
      "136 Train Loss 0.0043588276 Test MSE 0.027302576730246723 Test RE 0.0013837810595047659\n",
      "137 Train Loss 0.0038802335 Test MSE 0.25066617160763344 Test RE 0.004192889603614337\n",
      "138 Train Loss 0.0037603176 Test MSE 0.05429565957356876 Test RE 0.0019514081764582906\n",
      "139 Train Loss 0.0037495967 Test MSE 0.04000168804070912 Test RE 0.0016749610915344235\n",
      "140 Train Loss 0.0037438695 Test MSE 0.02332463571280949 Test RE 0.0012790072272887104\n",
      "141 Train Loss 0.0037356769 Test MSE 0.01736820622179446 Test RE 0.0011036796813348555\n",
      "142 Train Loss 0.0037344245 Test MSE 0.011036645322488903 Test RE 0.0008798003022090445\n",
      "143 Train Loss 0.0037243727 Test MSE 0.007997653039536019 Test RE 0.0007489396844902771\n",
      "144 Train Loss 0.0037211012 Test MSE 0.006229698446609875 Test RE 0.0006609963725606917\n",
      "145 Train Loss 0.003716927 Test MSE 0.005014946065741408 Test RE 0.0005930600871800418\n",
      "146 Train Loss 0.0037153498 Test MSE 0.004610112542959566 Test RE 0.0005686189480391654\n",
      "147 Train Loss 0.0037060615 Test MSE 0.006990918061119899 Test RE 0.000700217031054272\n",
      "148 Train Loss 0.0037001995 Test MSE 0.010999609611850434 Test RE 0.0008783228871971825\n",
      "149 Train Loss 0.00369246 Test MSE 0.017845455774360818 Test RE 0.001118740565642235\n",
      "150 Train Loss 0.0036699981 Test MSE 0.04782838608863482 Test RE 0.0018315063423990081\n",
      "151 Train Loss 0.0036488175 Test MSE 0.1093524709080283 Test RE 0.002769362863451317\n",
      "152 Train Loss 0.003630826 Test MSE 0.21926122898492495 Test RE 0.003921448228585431\n",
      "153 Train Loss 0.00341532 Test MSE 0.9292972809625425 Test RE 0.00807314767968489\n",
      "154 Train Loss 0.0033319753 Test MSE 0.8154613259297784 Test RE 0.007562532399233335\n",
      "155 Train Loss 0.0033058596 Test MSE 0.730894915188752 Test RE 0.0071596704703844015\n",
      "156 Train Loss 0.003305331 Test MSE 0.7041386411708873 Test RE 0.007027399649974912\n",
      "157 Train Loss 0.0033105433 Test MSE 0.7041386411708873 Test RE 0.007027399649974912\n",
      "158 Train Loss 0.003304729 Test MSE 0.6665317908454849 Test RE 0.0068371640103660185\n",
      "159 Train Loss 0.003299037 Test MSE 0.6665317908454849 Test RE 0.0068371640103660185\n",
      "160 Train Loss 0.0033000703 Test MSE 0.6357088332874583 Test RE 0.0066772046444038835\n",
      "161 Train Loss 0.003304841 Test MSE 0.6357088332874583 Test RE 0.0066772046444038835\n",
      "162 Train Loss 0.0032980174 Test MSE 0.6357088332874583 Test RE 0.0066772046444038835\n",
      "163 Train Loss 0.0033011534 Test MSE 0.6357088332874583 Test RE 0.0066772046444038835\n",
      "164 Train Loss 0.0033016412 Test MSE 0.6357088332874583 Test RE 0.0066772046444038835\n",
      "165 Train Loss 0.003302859 Test MSE 0.5416614204333178 Test RE 0.006163530024268957\n",
      "166 Train Loss 0.0032991918 Test MSE 0.5416614204333178 Test RE 0.006163530024268957\n",
      "167 Train Loss 0.003301575 Test MSE 0.5417394341252514 Test RE 0.00616397386464803\n",
      "168 Train Loss 0.0032995401 Test MSE 0.5417394341252514 Test RE 0.00616397386464803\n",
      "169 Train Loss 0.0033019474 Test MSE 0.5417394341252514 Test RE 0.00616397386464803\n",
      "170 Train Loss 0.003300247 Test MSE 0.5417394341252514 Test RE 0.00616397386464803\n",
      "171 Train Loss 0.0032986398 Test MSE 0.5417396746835769 Test RE 0.006163975233198086\n",
      "172 Train Loss 0.0033015155 Test MSE 0.5417396746835769 Test RE 0.006163975233198086\n",
      "173 Train Loss 0.0032990354 Test MSE 0.5417396746835769 Test RE 0.006163975233198086\n",
      "174 Train Loss 0.0032992708 Test MSE 0.5417396746835769 Test RE 0.006163975233198086\n",
      "175 Train Loss 0.003294094 Test MSE 0.5417396746835769 Test RE 0.006163975233198086\n",
      "176 Train Loss 0.0032978074 Test MSE 0.5417396746835769 Test RE 0.006163975233198086\n",
      "177 Train Loss 0.0032972104 Test MSE 0.5417396746835769 Test RE 0.006163975233198086\n",
      "178 Train Loss 0.0033028647 Test MSE 0.5563720027763209 Test RE 0.006246664736085777\n",
      "179 Train Loss 0.0032983073 Test MSE 0.5563720027763209 Test RE 0.006246664736085777\n",
      "180 Train Loss 0.0033036734 Test MSE 0.5562214130192553 Test RE 0.006245819305886866\n",
      "181 Train Loss 0.0032958083 Test MSE 0.5562210414181427 Test RE 0.006245817219529047\n",
      "182 Train Loss 0.0032985888 Test MSE 0.5562210414181427 Test RE 0.006245817219529047\n",
      "183 Train Loss 0.00329438 Test MSE 0.5562210414181427 Test RE 0.006245817219529047\n",
      "184 Train Loss 0.003293626 Test MSE 0.5562210414181427 Test RE 0.006245817219529047\n",
      "185 Train Loss 0.003297531 Test MSE 0.5562210414181427 Test RE 0.006245817219529047\n",
      "186 Train Loss 0.0033009634 Test MSE 0.5562210414181427 Test RE 0.006245817219529047\n",
      "187 Train Loss 0.0032994703 Test MSE 0.5562201292943743 Test RE 0.00624581209839891\n",
      "188 Train Loss 0.0033010405 Test MSE 0.5562201292943743 Test RE 0.00624581209839891\n",
      "189 Train Loss 0.003304559 Test MSE 0.5562201292943743 Test RE 0.00624581209839891\n",
      "190 Train Loss 0.0032969648 Test MSE 0.5562201292943743 Test RE 0.00624581209839891\n",
      "191 Train Loss 0.0032977012 Test MSE 0.5562201292943743 Test RE 0.00624581209839891\n",
      "192 Train Loss 0.0033004754 Test MSE 0.5562201292943743 Test RE 0.00624581209839891\n",
      "193 Train Loss 0.003297296 Test MSE 0.5562201292943743 Test RE 0.00624581209839891\n",
      "194 Train Loss 0.003295102 Test MSE 0.5562201292943743 Test RE 0.00624581209839891\n",
      "195 Train Loss 0.003299505 Test MSE 0.5562201292943743 Test RE 0.00624581209839891\n",
      "196 Train Loss 0.0033006778 Test MSE 0.5562201292943743 Test RE 0.00624581209839891\n",
      "197 Train Loss 0.003302273 Test MSE 0.5562201292943743 Test RE 0.00624581209839891\n",
      "198 Train Loss 0.00329938 Test MSE 0.5562201292943743 Test RE 0.00624581209839891\n",
      "199 Train Loss 0.0033014233 Test MSE 0.5562201292943743 Test RE 0.00624581209839891\n",
      "Training time: 214.33\n",
      "Training time: 214.33\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      "  (m_lambda): Sigmoid()\n",
      ")\n",
      "5\n",
      "0 Train Loss 3.2496953 Test MSE 14220.542994698651 Test RE 0.998673793605571\n",
      "1 Train Loss 1.6115742 Test MSE 14211.082132809888 Test RE 0.9983415317911022\n",
      "2 Train Loss 1.1401196 Test MSE 14208.649695961662 Test RE 0.9982560876768011\n",
      "3 Train Loss 1.0984813 Test MSE 14206.084954864704 Test RE 0.998165988194645\n",
      "4 Train Loss 1.1062247 Test MSE 14200.488964746284 Test RE 0.9979693725469128\n",
      "5 Train Loss 1.1185315 Test MSE 14171.76955795305 Test RE 0.9969597032905377\n",
      "6 Train Loss 1.1324598 Test MSE 14166.407091245397 Test RE 0.9967710652723177\n",
      "7 Train Loss 1.146045 Test MSE 14165.103298162394 Test RE 0.9967251957333882\n",
      "8 Train Loss 1.1572571 Test MSE 14149.618523361603 Test RE 0.9961802549318381\n",
      "9 Train Loss 1.1639607 Test MSE 14084.65715845835 Test RE 0.99389087606712\n",
      "10 Train Loss 1.17349 Test MSE 14027.754851300779 Test RE 0.9918811743230694\n",
      "11 Train Loss 1.1777927 Test MSE 13980.539094654829 Test RE 0.990210490197291\n",
      "12 Train Loss 1.1805745 Test MSE 13916.321575884242 Test RE 0.9879336806270769\n",
      "13 Train Loss 1.1833606 Test MSE 13816.440348338076 Test RE 0.9843819617820591\n",
      "14 Train Loss 1.1827108 Test MSE 13693.175272656998 Test RE 0.9799809813107987\n",
      "15 Train Loss 1.1854911 Test MSE 13595.241031326517 Test RE 0.9764702575186397\n",
      "16 Train Loss 1.1743033 Test MSE 13434.018063412772 Test RE 0.9706631169544765\n",
      "17 Train Loss 1.1747807 Test MSE 13379.94910692334 Test RE 0.9687077951740334\n",
      "18 Train Loss 1.1787189 Test MSE 13335.240473380096 Test RE 0.9670879892453136\n",
      "19 Train Loss 1.1743463 Test MSE 13245.298256264217 Test RE 0.9638211114656562\n",
      "20 Train Loss 1.1763242 Test MSE 13228.061985798178 Test RE 0.9631937911468572\n",
      "21 Train Loss 1.1736308 Test MSE 13137.527630740247 Test RE 0.9598920283161793\n",
      "22 Train Loss 1.1466742 Test MSE 12777.622937654995 Test RE 0.9466525254300467\n",
      "23 Train Loss 1.1332096 Test MSE 12601.190330981166 Test RE 0.9400941483221914\n",
      "24 Train Loss 1.1218271 Test MSE 12409.924523033395 Test RE 0.9329323093050226\n",
      "25 Train Loss 1.1180375 Test MSE 12321.2191843709 Test RE 0.9295920596939569\n",
      "26 Train Loss 1.1182419 Test MSE 12266.817475294787 Test RE 0.9275375819102759\n",
      "27 Train Loss 1.112433 Test MSE 12178.227777217637 Test RE 0.9241822222595811\n",
      "28 Train Loss 1.1109705 Test MSE 12061.540142217238 Test RE 0.9197439653998563\n",
      "29 Train Loss 1.1097255 Test MSE 11964.72210949956 Test RE 0.9160451334983002\n",
      "30 Train Loss 1.1045729 Test MSE 11881.09047377763 Test RE 0.912838009425731\n",
      "31 Train Loss 1.100091 Test MSE 11816.763784789771 Test RE 0.9103635083220962\n",
      "32 Train Loss 1.0799758 Test MSE 11572.816795891375 Test RE 0.9009176654663344\n",
      "33 Train Loss 1.0377502 Test MSE 11113.932605995591 Test RE 0.8828754552835494\n",
      "34 Train Loss 1.0037568 Test MSE 10807.701384010277 Test RE 0.8706272017757998\n",
      "35 Train Loss 0.9878783 Test MSE 10651.54689338838 Test RE 0.8643147123108629\n",
      "36 Train Loss 0.98862106 Test MSE 10637.98787324243 Test RE 0.8637644170047164\n",
      "37 Train Loss 0.9868525 Test MSE 10579.67493723693 Test RE 0.8613937687917204\n",
      "38 Train Loss 0.98223275 Test MSE 10508.29623819344 Test RE 0.8584830354782905\n",
      "39 Train Loss 0.9797355 Test MSE 10460.10645409877 Test RE 0.8565123234881289\n",
      "40 Train Loss 0.9761293 Test MSE 10351.56343484645 Test RE 0.8520567821625527\n",
      "41 Train Loss 0.9567624 Test MSE 10176.069763554628 Test RE 0.8448032998939468\n",
      "42 Train Loss 0.95447147 Test MSE 10147.14415705148 Test RE 0.8436017634292033\n",
      "43 Train Loss 0.951134 Test MSE 9976.091369856525 Test RE 0.8364611464466021\n",
      "44 Train Loss 0.93714976 Test MSE 9550.867263359585 Test RE 0.818440229391611\n",
      "45 Train Loss 0.89960563 Test MSE 9048.575851157422 Test RE 0.7966282061890653\n",
      "46 Train Loss 0.89277935 Test MSE 9069.787973928329 Test RE 0.7975614073797804\n",
      "47 Train Loss 0.88068414 Test MSE 8994.244494092454 Test RE 0.7942329641877623\n",
      "48 Train Loss 0.84896827 Test MSE 8828.927164647946 Test RE 0.7868999736833914\n",
      "49 Train Loss 0.81219065 Test MSE 8578.284290744032 Test RE 0.7756499721687954\n",
      "50 Train Loss 0.79044616 Test MSE 8253.032796318917 Test RE 0.7608032325984426\n",
      "51 Train Loss 0.7790974 Test MSE 8025.156468335105 Test RE 0.7502263576186352\n",
      "52 Train Loss 0.7653319 Test MSE 7914.9651035895595 Test RE 0.745057972022764\n",
      "53 Train Loss 0.7485875 Test MSE 7567.1859475073 Test RE 0.7285053856331889\n",
      "54 Train Loss 0.7334197 Test MSE 7247.21932406484 Test RE 0.7129371834841118\n",
      "55 Train Loss 0.7170183 Test MSE 6851.473655322165 Test RE 0.6931984130887311\n",
      "56 Train Loss 0.6748816 Test MSE 6429.719790546135 Test RE 0.6715240746427502\n",
      "57 Train Loss 0.62795407 Test MSE 5958.135515091118 Test RE 0.6464288819752386\n",
      "58 Train Loss 0.61519915 Test MSE 5734.70544987339 Test RE 0.6341925294312605\n",
      "59 Train Loss 0.60745454 Test MSE 5703.305697899902 Test RE 0.6324539203854318\n",
      "60 Train Loss 0.597774 Test MSE 5766.448429558059 Test RE 0.6359453116690367\n",
      "61 Train Loss 0.5923294 Test MSE 5695.073537506372 Test RE 0.6319973130684626\n",
      "62 Train Loss 0.5761058 Test MSE 5613.104861021458 Test RE 0.6274326890046779\n",
      "63 Train Loss 0.5434271 Test MSE 5390.100745617779 Test RE 0.6148426773604779\n",
      "64 Train Loss 0.5252101 Test MSE 5152.082920516869 Test RE 0.6011141979306609\n",
      "65 Train Loss 0.5110727 Test MSE 4973.32200390942 Test RE 0.5905937589617168\n",
      "66 Train Loss 0.49372733 Test MSE 4963.256386387742 Test RE 0.5899957982998462\n",
      "67 Train Loss 0.47629303 Test MSE 4835.875628791414 Test RE 0.5823755387643943\n",
      "68 Train Loss 0.4627718 Test MSE 4676.341718605064 Test RE 0.5726887915875343\n",
      "69 Train Loss 0.4586183 Test MSE 4554.5148363483695 Test RE 0.5651797907759313\n",
      "70 Train Loss 0.4430397 Test MSE 4333.778124705265 Test RE 0.5513138467254443\n",
      "71 Train Loss 0.42674145 Test MSE 4258.081982881507 Test RE 0.5464778619433018\n",
      "72 Train Loss 0.42102325 Test MSE 4153.709597485633 Test RE 0.5397387870696545\n",
      "73 Train Loss 0.41783464 Test MSE 4010.617051049055 Test RE 0.5303604881761045\n",
      "74 Train Loss 0.4156451 Test MSE 3926.3065473739057 Test RE 0.524756305596869\n",
      "75 Train Loss 0.41113424 Test MSE 3813.638702143379 Test RE 0.5171723965788193\n",
      "76 Train Loss 0.4019649 Test MSE 3626.37158663227 Test RE 0.5043148022221029\n",
      "77 Train Loss 0.3962834 Test MSE 3505.513313323039 Test RE 0.4958397890909105\n",
      "78 Train Loss 0.39395127 Test MSE 3403.028767690441 Test RE 0.4885380268114038\n",
      "79 Train Loss 0.38042587 Test MSE 3116.2886525583003 Test RE 0.46750300083088536\n",
      "80 Train Loss 0.32072872 Test MSE 2676.0999777653765 Test RE 0.4332282126530932\n",
      "81 Train Loss 0.27403992 Test MSE 2175.584248268315 Test RE 0.39061913739733406\n",
      "82 Train Loss 0.23820809 Test MSE 1873.131119839517 Test RE 0.36245129297733897\n",
      "83 Train Loss 0.22636764 Test MSE 1804.7073567068792 Test RE 0.35576970043658374\n",
      "84 Train Loss 0.20633292 Test MSE 1743.4857679019167 Test RE 0.34968319939628995\n",
      "85 Train Loss 0.19212362 Test MSE 1725.724041182417 Test RE 0.34789744433438863\n",
      "86 Train Loss 0.18902335 Test MSE 1751.4874693091156 Test RE 0.3504847136764941\n",
      "87 Train Loss 0.18495315 Test MSE 1697.9650679336867 Test RE 0.34508806507137607\n",
      "88 Train Loss 0.1782815 Test MSE 1612.8192619006963 Test RE 0.3363244297768065\n",
      "89 Train Loss 0.17031687 Test MSE 1475.6577546273277 Test RE 0.3217054260166535\n",
      "90 Train Loss 0.16623658 Test MSE 1374.8840451118374 Test RE 0.31052645104671833\n",
      "91 Train Loss 0.16396958 Test MSE 1333.6575872626368 Test RE 0.30583538650332837\n",
      "92 Train Loss 0.15032789 Test MSE 1252.2906310869012 Test RE 0.29635900600524134\n",
      "93 Train Loss 0.14026819 Test MSE 1264.2495677586048 Test RE 0.2977707060286039\n",
      "94 Train Loss 0.12872358 Test MSE 1160.6634842644758 Test RE 0.28531113693553345\n",
      "95 Train Loss 0.124638595 Test MSE 1080.2287358629717 Test RE 0.27524752769352495\n",
      "96 Train Loss 0.12287114 Test MSE 1036.175082622589 Test RE 0.2695765655021831\n",
      "97 Train Loss 0.12184815 Test MSE 1033.8420564266007 Test RE 0.26927290851449787\n",
      "98 Train Loss 0.11654169 Test MSE 988.4740056314818 Test RE 0.26329838194286154\n",
      "99 Train Loss 0.105546646 Test MSE 864.4198991194608 Test RE 0.24622261629165088\n",
      "100 Train Loss 0.09891647 Test MSE 759.7518576428295 Test RE 0.23083488857404233\n",
      "101 Train Loss 0.092909634 Test MSE 645.3783896291187 Test RE 0.21275156981894403\n",
      "102 Train Loss 0.0806427 Test MSE 595.730974813555 Test RE 0.20440459505017178\n",
      "103 Train Loss nan Test MSE nan Test RE nan\n",
      "104 Train Loss nan Test MSE nan Test RE nan\n",
      "105 Train Loss nan Test MSE nan Test RE nan\n",
      "106 Train Loss nan Test MSE nan Test RE nan\n",
      "107 Train Loss nan Test MSE nan Test RE nan\n",
      "108 Train Loss nan Test MSE nan Test RE nan\n",
      "109 Train Loss nan Test MSE nan Test RE nan\n",
      "110 Train Loss nan Test MSE nan Test RE nan\n",
      "111 Train Loss nan Test MSE nan Test RE nan\n",
      "112 Train Loss nan Test MSE nan Test RE nan\n",
      "113 Train Loss nan Test MSE nan Test RE nan\n",
      "114 Train Loss nan Test MSE nan Test RE nan\n",
      "115 Train Loss nan Test MSE nan Test RE nan\n",
      "116 Train Loss nan Test MSE nan Test RE nan\n",
      "117 Train Loss nan Test MSE nan Test RE nan\n",
      "118 Train Loss nan Test MSE nan Test RE nan\n",
      "119 Train Loss nan Test MSE nan Test RE nan\n",
      "120 Train Loss nan Test MSE nan Test RE nan\n",
      "121 Train Loss nan Test MSE nan Test RE nan\n",
      "122 Train Loss nan Test MSE nan Test RE nan\n",
      "123 Train Loss nan Test MSE nan Test RE nan\n",
      "124 Train Loss nan Test MSE nan Test RE nan\n",
      "125 Train Loss nan Test MSE nan Test RE nan\n",
      "126 Train Loss nan Test MSE nan Test RE nan\n",
      "127 Train Loss nan Test MSE nan Test RE nan\n",
      "128 Train Loss nan Test MSE nan Test RE nan\n",
      "129 Train Loss nan Test MSE nan Test RE nan\n",
      "130 Train Loss nan Test MSE nan Test RE nan\n",
      "131 Train Loss nan Test MSE nan Test RE nan\n",
      "132 Train Loss nan Test MSE nan Test RE nan\n",
      "133 Train Loss nan Test MSE nan Test RE nan\n",
      "134 Train Loss nan Test MSE nan Test RE nan\n",
      "135 Train Loss nan Test MSE nan Test RE nan\n",
      "136 Train Loss nan Test MSE nan Test RE nan\n",
      "137 Train Loss nan Test MSE nan Test RE nan\n",
      "138 Train Loss nan Test MSE nan Test RE nan\n",
      "139 Train Loss nan Test MSE nan Test RE nan\n",
      "140 Train Loss nan Test MSE nan Test RE nan\n",
      "141 Train Loss nan Test MSE nan Test RE nan\n",
      "142 Train Loss nan Test MSE nan Test RE nan\n",
      "143 Train Loss nan Test MSE nan Test RE nan\n",
      "144 Train Loss nan Test MSE nan Test RE nan\n",
      "145 Train Loss nan Test MSE nan Test RE nan\n",
      "146 Train Loss nan Test MSE nan Test RE nan\n",
      "147 Train Loss nan Test MSE nan Test RE nan\n",
      "148 Train Loss nan Test MSE nan Test RE nan\n",
      "149 Train Loss nan Test MSE nan Test RE nan\n",
      "150 Train Loss nan Test MSE nan Test RE nan\n",
      "151 Train Loss nan Test MSE nan Test RE nan\n",
      "152 Train Loss nan Test MSE nan Test RE nan\n",
      "153 Train Loss nan Test MSE nan Test RE nan\n",
      "154 Train Loss nan Test MSE nan Test RE nan\n",
      "155 Train Loss nan Test MSE nan Test RE nan\n",
      "156 Train Loss nan Test MSE nan Test RE nan\n",
      "157 Train Loss nan Test MSE nan Test RE nan\n",
      "158 Train Loss nan Test MSE nan Test RE nan\n",
      "159 Train Loss nan Test MSE nan Test RE nan\n",
      "160 Train Loss nan Test MSE nan Test RE nan\n",
      "161 Train Loss nan Test MSE nan Test RE nan\n",
      "162 Train Loss nan Test MSE nan Test RE nan\n",
      "163 Train Loss nan Test MSE nan Test RE nan\n",
      "164 Train Loss nan Test MSE nan Test RE nan\n",
      "165 Train Loss nan Test MSE nan Test RE nan\n",
      "166 Train Loss nan Test MSE nan Test RE nan\n",
      "167 Train Loss nan Test MSE nan Test RE nan\n",
      "168 Train Loss nan Test MSE nan Test RE nan\n",
      "169 Train Loss nan Test MSE nan Test RE nan\n",
      "170 Train Loss nan Test MSE nan Test RE nan\n",
      "171 Train Loss nan Test MSE nan Test RE nan\n",
      "172 Train Loss nan Test MSE nan Test RE nan\n",
      "173 Train Loss nan Test MSE nan Test RE nan\n",
      "174 Train Loss nan Test MSE nan Test RE nan\n",
      "175 Train Loss nan Test MSE nan Test RE nan\n",
      "176 Train Loss nan Test MSE nan Test RE nan\n",
      "177 Train Loss nan Test MSE nan Test RE nan\n",
      "178 Train Loss nan Test MSE nan Test RE nan\n",
      "179 Train Loss nan Test MSE nan Test RE nan\n",
      "180 Train Loss nan Test MSE nan Test RE nan\n",
      "181 Train Loss nan Test MSE nan Test RE nan\n",
      "182 Train Loss nan Test MSE nan Test RE nan\n",
      "183 Train Loss nan Test MSE nan Test RE nan\n",
      "184 Train Loss nan Test MSE nan Test RE nan\n",
      "185 Train Loss nan Test MSE nan Test RE nan\n",
      "186 Train Loss nan Test MSE nan Test RE nan\n",
      "187 Train Loss nan Test MSE nan Test RE nan\n",
      "188 Train Loss nan Test MSE nan Test RE nan\n",
      "189 Train Loss nan Test MSE nan Test RE nan\n",
      "190 Train Loss nan Test MSE nan Test RE nan\n",
      "191 Train Loss nan Test MSE nan Test RE nan\n",
      "192 Train Loss nan Test MSE nan Test RE nan\n",
      "193 Train Loss nan Test MSE nan Test RE nan\n",
      "194 Train Loss nan Test MSE nan Test RE nan\n",
      "195 Train Loss nan Test MSE nan Test RE nan\n",
      "196 Train Loss nan Test MSE nan Test RE nan\n",
      "197 Train Loss nan Test MSE nan Test RE nan\n",
      "198 Train Loss nan Test MSE nan Test RE nan\n",
      "199 Train Loss nan Test MSE nan Test RE nan\n",
      "Training time: 282.89\n",
      "Training time: 282.89\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      "  (m_lambda): Sigmoid()\n",
      ")\n",
      "6\n",
      "0 Train Loss 2.7467287 Test MSE 14232.204251155763 Test RE 0.9990831803897315\n",
      "1 Train Loss 1.3744749 Test MSE 14244.750449944728 Test RE 0.9995234472218235\n",
      "2 Train Loss 1.1004161 Test MSE 14251.902104046861 Test RE 0.999774323816955\n",
      "3 Train Loss 1.0952556 Test MSE 14249.684882881817 Test RE 0.9996965514958492\n",
      "4 Train Loss 1.1094393 Test MSE 14245.333964840589 Test RE 0.999543919003675\n",
      "5 Train Loss 1.1250336 Test MSE 14245.086223675631 Test RE 0.9995352274120224\n",
      "6 Train Loss 1.1399012 Test MSE 14244.924311980605 Test RE 0.9995295469654376\n",
      "7 Train Loss 1.1537662 Test MSE 14244.761263755565 Test RE 0.9995238266126552\n",
      "8 Train Loss 1.1663922 Test MSE 14242.116438355337 Test RE 0.999431031490471\n",
      "9 Train Loss 1.1777627 Test MSE 14225.356461781741 Test RE 0.9988427982884172\n",
      "10 Train Loss 1.1892394 Test MSE 14225.405762851718 Test RE 0.9988445291405108\n",
      "11 Train Loss 1.1998024 Test MSE 14225.426692413332 Test RE 0.9988452639304625\n",
      "12 Train Loss 1.2087461 Test MSE 14211.619893308396 Test RE 0.9983604206961126\n",
      "13 Train Loss 1.2156962 Test MSE 14187.655591815459 Test RE 0.9975183244156871\n",
      "14 Train Loss 1.22255 Test MSE 14163.147459776841 Test RE 0.9966563822325972\n",
      "15 Train Loss 1.2290883 Test MSE 14132.573060108583 Test RE 0.995580045432664\n",
      "16 Train Loss 1.2316766 Test MSE 14073.80328187284 Test RE 0.9935078476462244\n",
      "17 Train Loss 1.2357769 Test MSE 14070.856822406995 Test RE 0.9934038429300037\n",
      "18 Train Loss 1.2405366 Test MSE 14047.401658041912 Test RE 0.9925755291719903\n",
      "19 Train Loss 1.2447933 Test MSE 14015.332094816835 Test RE 0.9914418799510138\n",
      "20 Train Loss 1.2476647 Test MSE 13961.077001375477 Test RE 0.989521021773296\n",
      "21 Train Loss 1.2434384 Test MSE 13885.081966219921 Test RE 0.9868241919550249\n",
      "22 Train Loss 1.2437627 Test MSE 13835.219490594374 Test RE 0.9850507147769466\n",
      "23 Train Loss 1.2364298 Test MSE 13666.03785937762 Test RE 0.979009426589381\n",
      "24 Train Loss 1.2305273 Test MSE 13574.206987231593 Test RE 0.9757145861445333\n",
      "25 Train Loss 1.225481 Test MSE 13486.064687001974 Test RE 0.9725415907211855\n",
      "26 Train Loss 1.223593 Test MSE 13437.25125001997 Test RE 0.9707799154495625\n",
      "27 Train Loss 1.2237208 Test MSE 13390.274614224136 Test RE 0.9690815062519061\n",
      "28 Train Loss 1.2226075 Test MSE 13347.976167796905 Test RE 0.9675496831245994\n",
      "29 Train Loss 1.2218884 Test MSE 13328.17884632989 Test RE 0.9668318964116598\n",
      "30 Train Loss 1.2169665 Test MSE 13255.674445369874 Test RE 0.9641985598276605\n",
      "31 Train Loss 1.2032838 Test MSE 13056.411226876868 Test RE 0.9569240592468665\n",
      "32 Train Loss 1.2004166 Test MSE 12980.369449843904 Test RE 0.9541333817910784\n",
      "33 Train Loss 1.1989183 Test MSE 12940.782673201642 Test RE 0.9526773405275752\n",
      "34 Train Loss 1.1973141 Test MSE 12881.58053291817 Test RE 0.950495664466661\n",
      "35 Train Loss 1.1963121 Test MSE 12842.133728282712 Test RE 0.9490392141161429\n",
      "36 Train Loss 1.1940461 Test MSE 12812.116946748898 Test RE 0.9479294387062535\n",
      "37 Train Loss 1.1840538 Test MSE 12714.895219718579 Test RE 0.9443260203030043\n",
      "38 Train Loss 1.1756811 Test MSE 12523.220030094613 Test RE 0.9371812028182748\n",
      "39 Train Loss 1.1565313 Test MSE 12204.988426385695 Test RE 0.9251970720842386\n",
      "40 Train Loss 1.143595 Test MSE 11992.217060914822 Test RE 0.9170970661336573\n",
      "41 Train Loss 1.1005126 Test MSE 11287.99676022846 Test RE 0.8897623029376018\n",
      "42 Train Loss 1.0734344 Test MSE 11177.666562826267 Test RE 0.8854033051619852\n",
      "43 Train Loss 1.0614461 Test MSE 11146.127160804985 Test RE 0.8841532760460714\n",
      "44 Train Loss 1.0496275 Test MSE 11127.715436078315 Test RE 0.8834227301540177\n",
      "45 Train Loss 1.0490679 Test MSE 11116.693046646216 Test RE 0.8829850912724548\n",
      "46 Train Loss 1.0497749 Test MSE 11121.623292296246 Test RE 0.8831808712053427\n",
      "47 Train Loss 1.0495962 Test MSE 11102.802266918838 Test RE 0.8824332551612615\n",
      "48 Train Loss 1.0483108 Test MSE 11051.015670595072 Test RE 0.8803728912260849\n",
      "49 Train Loss 1.044674 Test MSE 10987.936380506633 Test RE 0.877856707566618\n",
      "50 Train Loss 1.0390388 Test MSE 10902.339929692931 Test RE 0.8744307535336105\n",
      "51 Train Loss 1.0245584 Test MSE 10753.550802863492 Test RE 0.8684433807624996\n",
      "52 Train Loss 1.0101581 Test MSE 10612.69475738113 Test RE 0.8627369531952978\n",
      "53 Train Loss 1.0070218 Test MSE 10575.572934567486 Test RE 0.8612267607254533\n",
      "54 Train Loss 1.0021663 Test MSE 10580.760181482945 Test RE 0.8614379477794871\n",
      "55 Train Loss 1.0001316 Test MSE 10540.66569490914 Test RE 0.8598042421658072\n",
      "56 Train Loss 0.9971018 Test MSE 10497.605933615943 Test RE 0.8580462481911584\n",
      "57 Train Loss 0.993158 Test MSE 10441.199983736971 Test RE 0.8557379074256429\n",
      "58 Train Loss 0.98963 Test MSE 10382.451316094779 Test RE 0.8533270552696077\n",
      "59 Train Loss 0.9864554 Test MSE 10351.944588763312 Test RE 0.852072468768372\n",
      "60 Train Loss 0.97577745 Test MSE 10231.22527227295 Test RE 0.8470896731707205\n",
      "61 Train Loss 0.96863514 Test MSE 10149.194033242547 Test RE 0.8436869692672467\n",
      "62 Train Loss 0.95323974 Test MSE 9972.909172984897 Test RE 0.836327727643378\n",
      "63 Train Loss 0.91039866 Test MSE 9447.961533313657 Test RE 0.8140191501203823\n",
      "64 Train Loss 0.88850456 Test MSE 9179.984469028994 Test RE 0.8023919019700281\n",
      "65 Train Loss 0.8846024 Test MSE 9061.628863911144 Test RE 0.7972025866749776\n",
      "66 Train Loss 0.8740866 Test MSE 8942.646829503221 Test RE 0.7919515324872257\n",
      "67 Train Loss 0.86615336 Test MSE 8856.217276537423 Test RE 0.7881151847219758\n",
      "68 Train Loss 0.86499494 Test MSE 8775.764990572405 Test RE 0.7845272914594451\n",
      "69 Train Loss 0.86056083 Test MSE 8696.909318253125 Test RE 0.780994606415961\n",
      "70 Train Loss 0.8564155 Test MSE 8656.230195554677 Test RE 0.7791659445297014\n",
      "71 Train Loss 0.855542 Test MSE 8583.002629989254 Test RE 0.775863259375694\n",
      "72 Train Loss 0.852067 Test MSE 8425.4086962012 Test RE 0.7687073826897167\n",
      "73 Train Loss 0.8298347 Test MSE 8367.276708799514 Test RE 0.7660509041534734\n",
      "74 Train Loss 0.8228451 Test MSE 8342.563040998462 Test RE 0.7649187599513341\n",
      "75 Train Loss 0.8175381 Test MSE 8142.484004130759 Test RE 0.7556906012248393\n",
      "76 Train Loss 0.80712867 Test MSE 8007.498024053708 Test RE 0.7494005091755795\n",
      "77 Train Loss 0.7929269 Test MSE 7912.451920608969 Test RE 0.7449396761331586\n",
      "78 Train Loss 0.78281623 Test MSE 7783.545653141456 Test RE 0.7388466393557424\n",
      "79 Train Loss 0.7668528 Test MSE 7688.551162664878 Test RE 0.7343241615641772\n",
      "80 Train Loss 0.7561672 Test MSE 7711.515594734734 Test RE 0.7354199963800245\n",
      "81 Train Loss 0.750418 Test MSE 7671.898866453047 Test RE 0.7335285102446581\n",
      "82 Train Loss 0.74744356 Test MSE 7662.29510797558 Test RE 0.7330692473694006\n",
      "83 Train Loss 0.74510187 Test MSE 7661.365773532825 Test RE 0.7330247902485707\n",
      "84 Train Loss 0.7440126 Test MSE 7686.682865079439 Test RE 0.734234936729531\n",
      "85 Train Loss 0.74283963 Test MSE 7684.856591161188 Test RE 0.7341477083506227\n",
      "86 Train Loss 0.7409791 Test MSE 7644.350421650165 Test RE 0.7322103396632008\n",
      "87 Train Loss 0.74034536 Test MSE 7627.280514589947 Test RE 0.7313923664528006\n",
      "88 Train Loss 0.73807466 Test MSE 7591.753607716836 Test RE 0.7296870117863019\n",
      "89 Train Loss 0.73571575 Test MSE 7510.4135588962345 Test RE 0.7257674552525661\n",
      "90 Train Loss 0.7320615 Test MSE 7492.8050569174475 Test RE 0.7249161587828226\n",
      "91 Train Loss 0.7270334 Test MSE 7436.435712328808 Test RE 0.7221841916886979\n",
      "92 Train Loss 0.7244949 Test MSE 7352.469013316949 Test RE 0.7180954338431771\n",
      "93 Train Loss 0.7221986 Test MSE 7331.993747798671 Test RE 0.7170948553065041\n",
      "94 Train Loss 0.7219214 Test MSE 7298.757608997614 Test RE 0.7154677034156717\n",
      "95 Train Loss 0.7202037 Test MSE 7258.369239068212 Test RE 0.7134854029816533\n",
      "96 Train Loss 0.7151515 Test MSE 7267.73534327731 Test RE 0.7139455906648532\n",
      "97 Train Loss 0.7126777 Test MSE 7254.188238021568 Test RE 0.7132798807200648\n",
      "98 Train Loss 0.7116855 Test MSE 7255.014413044742 Test RE 0.7133204970664442\n",
      "99 Train Loss 0.70991415 Test MSE 7266.022770858057 Test RE 0.7138614684791533\n",
      "100 Train Loss 0.7068948 Test MSE 7218.4995540339905 Test RE 0.7115231433778398\n",
      "101 Train Loss 0.7035749 Test MSE 7167.254160976905 Test RE 0.7089930312235632\n",
      "102 Train Loss 0.7019279 Test MSE 7122.098033945649 Test RE 0.7067560541975532\n",
      "103 Train Loss 0.7028964 Test MSE 7116.18890777294 Test RE 0.7064627995065234\n",
      "104 Train Loss 0.70209545 Test MSE 7109.232147595451 Test RE 0.706117397348995\n",
      "105 Train Loss 0.7005224 Test MSE 7124.565732834175 Test RE 0.7068784837172379\n",
      "106 Train Loss 0.6984066 Test MSE 7098.3590918342925 Test RE 0.7055772130135576\n",
      "107 Train Loss 0.69680035 Test MSE 7053.5050777123915 Test RE 0.7033444347223875\n",
      "108 Train Loss 0.6943176 Test MSE 7046.307129628338 Test RE 0.7029854692926067\n",
      "109 Train Loss 0.68575466 Test MSE 6979.392513954142 Test RE 0.699639588049429\n",
      "110 Train Loss 0.67952454 Test MSE 6912.062026962118 Test RE 0.6962566835637984\n",
      "111 Train Loss 0.6732503 Test MSE 6878.284456984934 Test RE 0.6945533813730854\n",
      "112 Train Loss 0.667773 Test MSE 6780.441683605636 Test RE 0.6895957183124893\n",
      "113 Train Loss 0.6540025 Test MSE 6693.289290080242 Test RE 0.6851495262361447\n",
      "114 Train Loss 0.64338917 Test MSE 6616.703324563817 Test RE 0.6812184393039924\n",
      "115 Train Loss 0.63892066 Test MSE 6503.234598132388 Test RE 0.6753521305506495\n",
      "116 Train Loss 0.63401854 Test MSE 6422.036596993433 Test RE 0.6711227358712719\n",
      "117 Train Loss 0.62130195 Test MSE 6269.825817985386 Test RE 0.663121794184648\n",
      "118 Train Loss 0.6167398 Test MSE 6217.480738610949 Test RE 0.6603478801635739\n",
      "119 Train Loss 0.61420983 Test MSE 6258.325679589663 Test RE 0.6625133648004347\n",
      "120 Train Loss 0.6114058 Test MSE 6219.761914984904 Test RE 0.6604690089535732\n",
      "121 Train Loss 0.6071764 Test MSE 6155.821623231767 Test RE 0.6570653676470187\n",
      "122 Train Loss 0.6026687 Test MSE 6143.562321936262 Test RE 0.6564107696694702\n",
      "123 Train Loss 0.5968752 Test MSE 6055.783336902872 Test RE 0.6517045124142523\n",
      "124 Train Loss 0.5882523 Test MSE 5982.793746433869 Test RE 0.6477651502493101\n",
      "125 Train Loss 0.583571 Test MSE 5924.950178079951 Test RE 0.6446261440005285\n",
      "126 Train Loss 0.5790162 Test MSE 5861.622793344239 Test RE 0.6411719246598548\n",
      "127 Train Loss 0.57355815 Test MSE 5878.053786984972 Test RE 0.642069945537618\n",
      "128 Train Loss 0.56987375 Test MSE 5824.621664544178 Test RE 0.6391450420335052\n",
      "129 Train Loss 0.5674734 Test MSE 5757.777954819104 Test RE 0.6354670257630313\n",
      "130 Train Loss 0.56753415 Test MSE 5729.301234007451 Test RE 0.633893636908375\n",
      "131 Train Loss 0.5664236 Test MSE 5731.986315630188 Test RE 0.634042159102616\n",
      "132 Train Loss 0.5655507 Test MSE 5750.87017320456 Test RE 0.6350857167870265\n",
      "133 Train Loss 0.56456083 Test MSE 5732.99610320546 Test RE 0.6340980053367162\n",
      "134 Train Loss 0.5613091 Test MSE 5607.903945705653 Test RE 0.6271419425629378\n",
      "135 Train Loss 0.55847937 Test MSE 5513.669771348959 Test RE 0.6218504307397826\n",
      "136 Train Loss 0.5546711 Test MSE 5474.079287377782 Test RE 0.6196138338566634\n",
      "137 Train Loss 0.5512105 Test MSE 5408.293188360221 Test RE 0.6158793989752839\n",
      "138 Train Loss 0.5499735 Test MSE 5341.668573393727 Test RE 0.612074142763085\n",
      "139 Train Loss 0.54478604 Test MSE 5198.261267740763 Test RE 0.603802095010643\n",
      "140 Train Loss 0.54082155 Test MSE 5114.6211011485 Test RE 0.5989248003532035\n",
      "141 Train Loss 0.53422874 Test MSE 5136.5873984615655 Test RE 0.6002095548288141\n",
      "142 Train Loss 0.5135529 Test MSE 5142.604758450916 Test RE 0.600561015784924\n",
      "143 Train Loss 0.5061376 Test MSE 5070.449535414616 Test RE 0.5963329354663418\n",
      "144 Train Loss 0.5042704 Test MSE 5057.676245582314 Test RE 0.5955813318244405\n",
      "145 Train Loss 0.50412107 Test MSE 5061.6088590583495 Test RE 0.5958128349832167\n",
      "146 Train Loss 0.5046729 Test MSE 5065.215392718487 Test RE 0.5960250635915181\n",
      "147 Train Loss 0.5048117 Test MSE 5070.635628897346 Test RE 0.5963438785448445\n",
      "148 Train Loss 0.5047873 Test MSE 5071.972714020529 Test RE 0.5964224988627979\n",
      "149 Train Loss 0.504785 Test MSE 5071.972714020529 Test RE 0.5964224988627979\n",
      "150 Train Loss 0.5046003 Test MSE 5071.972714020529 Test RE 0.5964224988627979\n",
      "151 Train Loss 0.50497913 Test MSE 5072.940897459716 Test RE 0.5964794213724713\n",
      "152 Train Loss 0.5050575 Test MSE 5072.940897459716 Test RE 0.5964794213724713\n",
      "153 Train Loss 0.5050426 Test MSE 5072.9340576244 Test RE 0.5964790192563747\n",
      "154 Train Loss 0.50514966 Test MSE 5072.9340576244 Test RE 0.5964790192563747\n",
      "155 Train Loss 0.5053048 Test MSE 5072.9340576244 Test RE 0.5964790192563747\n",
      "156 Train Loss 0.50507724 Test MSE 5072.933650152025 Test RE 0.5964789953009353\n",
      "157 Train Loss 0.50534886 Test MSE 5072.933650152025 Test RE 0.5964789953009353\n",
      "158 Train Loss 0.50524175 Test MSE 5072.933650152025 Test RE 0.5964789953009353\n",
      "159 Train Loss 0.50516325 Test MSE 5072.933650152025 Test RE 0.5964789953009353\n",
      "160 Train Loss 0.5054072 Test MSE 5072.933650152025 Test RE 0.5964789953009353\n",
      "161 Train Loss 0.50548625 Test MSE 5072.933650152025 Test RE 0.5964789953009353\n",
      "162 Train Loss 0.5055393 Test MSE 5072.933650152025 Test RE 0.5964789953009353\n",
      "163 Train Loss 0.50552636 Test MSE 5072.933650152025 Test RE 0.5964789953009353\n",
      "164 Train Loss 0.50552416 Test MSE 5072.933650152025 Test RE 0.5964789953009353\n",
      "165 Train Loss 0.50567865 Test MSE 5072.933650152025 Test RE 0.5964789953009353\n",
      "166 Train Loss 0.5055912 Test MSE 5072.933650152025 Test RE 0.5964789953009353\n",
      "167 Train Loss 0.5054848 Test MSE 5072.933650152025 Test RE 0.5964789953009353\n",
      "168 Train Loss 0.50581825 Test MSE 5072.933650152025 Test RE 0.5964789953009353\n",
      "169 Train Loss 0.5058271 Test MSE 5072.912606031696 Test RE 0.5964777581086483\n",
      "170 Train Loss 0.5059025 Test MSE 5072.912606031696 Test RE 0.5964777581086483\n",
      "171 Train Loss 0.505715 Test MSE 5072.912606031696 Test RE 0.5964777581086483\n",
      "172 Train Loss 0.5060584 Test MSE 5072.912606031696 Test RE 0.5964777581086483\n",
      "173 Train Loss 0.50600517 Test MSE 5072.912606031696 Test RE 0.5964777581086483\n",
      "174 Train Loss 0.5059702 Test MSE 5072.912606031696 Test RE 0.5964777581086483\n",
      "175 Train Loss 0.5055863 Test MSE 5072.912606031696 Test RE 0.5964777581086483\n",
      "176 Train Loss 0.50602114 Test MSE 5072.912908997853 Test RE 0.5964777759201687\n",
      "177 Train Loss 0.50598776 Test MSE 5072.912896988719 Test RE 0.5964777752141461\n",
      "178 Train Loss 0.5060676 Test MSE 5072.912896988719 Test RE 0.5964777752141461\n",
      "179 Train Loss 0.50594604 Test MSE 5072.912896988719 Test RE 0.5964777752141461\n",
      "180 Train Loss 0.50603294 Test MSE 5072.912896988719 Test RE 0.5964777752141461\n",
      "181 Train Loss 0.50617695 Test MSE 5072.91289907876 Test RE 0.5964777753370206\n",
      "182 Train Loss 0.50620615 Test MSE 5072.91289907876 Test RE 0.5964777753370206\n",
      "183 Train Loss 0.50616246 Test MSE 5072.912899077611 Test RE 0.596477775336953\n",
      "184 Train Loss 0.5062795 Test MSE 5072.912899077611 Test RE 0.596477775336953\n",
      "185 Train Loss 0.50636697 Test MSE 5072.912899077611 Test RE 0.596477775336953\n",
      "186 Train Loss 0.50638574 Test MSE 5072.9129123647135 Test RE 0.5964777761181079\n",
      "187 Train Loss 0.5062923 Test MSE 5072.9129123647135 Test RE 0.5964777761181079\n",
      "188 Train Loss 0.50652766 Test MSE 5072.9129123647135 Test RE 0.5964777761181079\n",
      "189 Train Loss 0.50615364 Test MSE 5072.9129123647135 Test RE 0.5964777761181079\n",
      "190 Train Loss 0.5065062 Test MSE 5072.912906794853 Test RE 0.5964777757906532\n",
      "191 Train Loss 0.5065083 Test MSE 5072.912906794853 Test RE 0.5964777757906532\n",
      "192 Train Loss 0.50636077 Test MSE 5072.912906794853 Test RE 0.5964777757906532\n",
      "193 Train Loss 0.5067557 Test MSE 5072.912906794853 Test RE 0.5964777757906532\n",
      "194 Train Loss 0.50668967 Test MSE 5072.912906794853 Test RE 0.5964777757906532\n",
      "195 Train Loss 0.5067576 Test MSE 5072.912906794853 Test RE 0.5964777757906532\n",
      "196 Train Loss 0.506695 Test MSE 5072.912906794853 Test RE 0.5964777757906532\n",
      "197 Train Loss 0.50685287 Test MSE 5072.912906794853 Test RE 0.5964777757906532\n",
      "198 Train Loss 0.50681657 Test MSE 5072.912906794853 Test RE 0.5964777757906532\n",
      "199 Train Loss 0.50677484 Test MSE 5072.912906794853 Test RE 0.5964777757906532\n",
      "Training time: 219.64\n",
      "Training time: 219.64\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      "  (m_lambda): Sigmoid()\n",
      ")\n",
      "7\n",
      "0 Train Loss 1.198032 Test MSE 14260.136690018955 Test RE 1.000063111191143\n",
      "1 Train Loss 1.0642282 Test MSE 14250.679008571105 Test RE 0.9997314226771644\n",
      "2 Train Loss 1.08322 Test MSE 14248.385327710987 Test RE 0.9996509648581346\n",
      "3 Train Loss 1.1027125 Test MSE 14246.67659965358 Test RE 0.9995910218250578\n",
      "4 Train Loss 1.121294 Test MSE 14244.84283167564 Test RE 0.9995266883300745\n",
      "5 Train Loss 1.138708 Test MSE 14242.465078564239 Test RE 0.9994432642127884\n",
      "6 Train Loss 1.1538166 Test MSE 14222.913831270696 Test RE 0.9987570391496274\n",
      "7 Train Loss 1.1682976 Test MSE 14218.403596562839 Test RE 0.9985986684427179\n",
      "8 Train Loss 1.1807152 Test MSE 14186.353203304405 Test RE 0.9974725386192598\n",
      "9 Train Loss 1.1906682 Test MSE 14165.494009721575 Test RE 0.996738941816341\n",
      "10 Train Loss 1.2015775 Test MSE 14148.044622557327 Test RE 0.9961248494626227\n",
      "11 Train Loss 1.2068022 Test MSE 14078.950343618502 Test RE 0.9936895035454825\n",
      "12 Train Loss 1.2139522 Test MSE 14066.76365678576 Test RE 0.9932593434776669\n",
      "13 Train Loss 1.2203708 Test MSE 14027.659317035235 Test RE 0.9918777967761335\n",
      "14 Train Loss 1.2222909 Test MSE 13942.795238561193 Test RE 0.9888729301293797\n",
      "15 Train Loss 1.219699 Test MSE 13845.277322722739 Test RE 0.9854087024030636\n",
      "16 Train Loss 1.2169707 Test MSE 13738.78044357435 Test RE 0.9816115399362906\n",
      "17 Train Loss 1.2164959 Test MSE 13614.769183843013 Test RE 0.9771713049457682\n",
      "18 Train Loss 1.216932 Test MSE 13540.709209657101 Test RE 0.9745099316368665\n",
      "19 Train Loss 1.2149557 Test MSE 13495.309556264314 Test RE 0.9728748784418911\n",
      "20 Train Loss 1.2120286 Test MSE 13404.451505799381 Test RE 0.96959437582893\n",
      "21 Train Loss 1.2093484 Test MSE 13342.587912787782 Test RE 0.9673543752142971\n",
      "22 Train Loss 1.208476 Test MSE 13295.126062088953 Test RE 0.9656323205579326\n",
      "23 Train Loss 1.2009909 Test MSE 13183.233232419072 Test RE 0.9615603159875309\n",
      "24 Train Loss 1.1996471 Test MSE 13136.078889475431 Test RE 0.9598391007954741\n",
      "25 Train Loss 1.1935939 Test MSE 12986.711243316873 Test RE 0.9543664328604541\n",
      "26 Train Loss 1.1845261 Test MSE 12787.888650592195 Test RE 0.9470327257225041\n",
      "27 Train Loss 1.1781093 Test MSE 12676.154710662673 Test RE 0.9428863081002985\n",
      "28 Train Loss 1.1599535 Test MSE 12503.345502234104 Test RE 0.9364372476016529\n",
      "29 Train Loss 1.1568234 Test MSE 12473.10829609279 Test RE 0.9353042554070065\n",
      "30 Train Loss 1.1479714 Test MSE 12345.459380495711 Test RE 0.9305060285778987\n",
      "31 Train Loss 1.1384163 Test MSE 12223.657570754354 Test RE 0.9259044073538782\n",
      "32 Train Loss 1.1309779 Test MSE 12098.11595396622 Test RE 0.9211374407472654\n",
      "33 Train Loss 1.1291251 Test MSE 12022.474092730694 Test RE 0.9182532807924996\n",
      "34 Train Loss 1.1291851 Test MSE 11978.207791472105 Test RE 0.9165612346756487\n",
      "35 Train Loss 1.1283913 Test MSE 11899.295977163263 Test RE 0.9135371167233153\n",
      "36 Train Loss 1.126966 Test MSE 11830.55225107709 Test RE 0.9108944852105675\n",
      "37 Train Loss 1.1172668 Test MSE 11700.42180548364 Test RE 0.9058709298737615\n",
      "38 Train Loss 1.095109 Test MSE 11435.322848452446 Test RE 0.8955498777395899\n",
      "39 Train Loss 1.0860475 Test MSE 11262.309980779235 Test RE 0.8887493621330196\n",
      "40 Train Loss 1.0838796 Test MSE 11231.970002146661 Test RE 0.8875514364197965\n",
      "41 Train Loss 1.0819523 Test MSE 11247.204554275342 Test RE 0.8881531504002504\n",
      "42 Train Loss 1.0771655 Test MSE 11198.801744763488 Test RE 0.8862399879477644\n",
      "43 Train Loss 1.0643818 Test MSE 11157.547365364999 Test RE 0.8846061071540994\n",
      "44 Train Loss 1.0596286 Test MSE 11125.20261713633 Test RE 0.8833229789194389\n",
      "45 Train Loss 1.055309 Test MSE 11073.521337010105 Test RE 0.8812688858500672\n",
      "46 Train Loss 1.0459718 Test MSE 10961.352650552515 Test RE 0.8767941405433879\n",
      "47 Train Loss 1.0375923 Test MSE 10860.278163331837 Test RE 0.872742324914076\n",
      "48 Train Loss 1.0338641 Test MSE 10782.853655457819 Test RE 0.8696258067535879\n",
      "49 Train Loss 1.0246243 Test MSE 10717.893952852639 Test RE 0.8670023838315111\n",
      "50 Train Loss 1.0178449 Test MSE 10693.387609365665 Test RE 0.866010621013634\n",
      "51 Train Loss 1.0139624 Test MSE 10648.162589390979 Test RE 0.8641773925323574\n",
      "52 Train Loss 1.0042511 Test MSE 10544.028014567939 Test RE 0.8599413637805536\n",
      "53 Train Loss 0.99571574 Test MSE 10420.377436028555 Test RE 0.8548841963538029\n",
      "54 Train Loss 0.99204415 Test MSE 10345.78958162207 Test RE 0.8518191206229632\n",
      "55 Train Loss 0.98759526 Test MSE 10349.06646268069 Test RE 0.85195401071172\n",
      "56 Train Loss 0.98121196 Test MSE 10251.903387475959 Test RE 0.8479452586895765\n",
      "57 Train Loss 0.9703271 Test MSE 10062.325207900025 Test RE 0.8400685737392605\n",
      "58 Train Loss 0.96273714 Test MSE 9945.105067422255 Test RE 0.8351610883999471\n",
      "59 Train Loss 0.95583224 Test MSE 9802.33481551833 Test RE 0.8291447018851361\n",
      "60 Train Loss 0.9381327 Test MSE 9639.236877527626 Test RE 0.8222178297431316\n",
      "61 Train Loss 0.91371757 Test MSE 9541.566121982278 Test RE 0.8180416120555022\n",
      "62 Train Loss 0.9083418 Test MSE 9464.909058410492 Test RE 0.8147489069526803\n",
      "63 Train Loss 0.9052648 Test MSE 9428.476647704932 Test RE 0.8131793257767435\n",
      "64 Train Loss 0.89648587 Test MSE 9382.720082412394 Test RE 0.811203739088286\n",
      "65 Train Loss 0.8894548 Test MSE 9268.559309513093 Test RE 0.806253625253189\n",
      "66 Train Loss 0.88625073 Test MSE 9191.701919565116 Test RE 0.8029038303486851\n",
      "67 Train Loss 0.8855478 Test MSE 9173.632717869896 Test RE 0.802114261217168\n",
      "68 Train Loss 0.88391894 Test MSE 9142.78522465009 Test RE 0.8007645205754843\n",
      "69 Train Loss 0.8823803 Test MSE 9133.561201404736 Test RE 0.80036047879358\n",
      "70 Train Loss 0.8808494 Test MSE 9077.871611302446 Test RE 0.7979167498260157\n",
      "71 Train Loss 0.87968355 Test MSE 9069.490677953483 Test RE 0.7975483357545188\n",
      "72 Train Loss 0.8786975 Test MSE 9062.409842377143 Test RE 0.7972369394744835\n",
      "73 Train Loss 0.8740088 Test MSE 9010.66270158357 Test RE 0.7949575351200332\n",
      "74 Train Loss 0.87133676 Test MSE 8977.039127406759 Test RE 0.7934729442254105\n",
      "75 Train Loss 0.87038463 Test MSE 8935.454430056112 Test RE 0.7916329927061005\n",
      "76 Train Loss 0.8700236 Test MSE 8903.609929622364 Test RE 0.7902211082955856\n",
      "77 Train Loss 0.8690518 Test MSE 8906.060608026712 Test RE 0.7903298532080331\n",
      "78 Train Loss 0.8674536 Test MSE 8825.84752729776 Test RE 0.7867627215650747\n",
      "79 Train Loss 0.86709267 Test MSE 8779.61616464281 Test RE 0.7846994143321757\n",
      "80 Train Loss 0.8673291 Test MSE 8778.6634748968 Test RE 0.7846568387028462\n",
      "81 Train Loss 0.8673444 Test MSE 8779.021647515132 Test RE 0.7846728456797971\n",
      "82 Train Loss 0.8678726 Test MSE 8779.83280899005 Test RE 0.7847090958285245\n",
      "83 Train Loss 0.86790246 Test MSE 8782.98965328899 Test RE 0.7848501567038159\n",
      "84 Train Loss 0.8682264 Test MSE 8785.428545905994 Test RE 0.7849591191678256\n",
      "85 Train Loss 0.86754435 Test MSE 8795.94219993771 Test RE 0.7854286649038235\n",
      "86 Train Loss 0.8671982 Test MSE 8796.881405258831 Test RE 0.7854705966874754\n",
      "87 Train Loss 0.86703956 Test MSE 8777.636792033214 Test RE 0.7846109537405317\n",
      "88 Train Loss 0.86621374 Test MSE 8795.94295216338 Test RE 0.7854286984886047\n",
      "89 Train Loss 0.86232936 Test MSE 8837.496263445279 Test RE 0.7872817521339591\n",
      "90 Train Loss 0.85708654 Test MSE 8732.969266109449 Test RE 0.7826120483992928\n",
      "91 Train Loss 0.8494667 Test MSE 8590.98650556648 Test RE 0.7762240279880125\n",
      "92 Train Loss 0.8377395 Test MSE 8605.732967218333 Test RE 0.7768899381282829\n",
      "93 Train Loss 0.8291863 Test MSE 8528.506473399942 Test RE 0.7733962384435813\n",
      "94 Train Loss 0.8214209 Test MSE 8432.392520889225 Test RE 0.7690259076277854\n",
      "95 Train Loss 0.80318207 Test MSE 8235.671174288624 Test RE 0.7600025734638093\n",
      "96 Train Loss 0.7851966 Test MSE 7976.230628744186 Test RE 0.7479359617863026\n",
      "97 Train Loss 0.77401936 Test MSE 7936.259916249592 Test RE 0.746059569154231\n",
      "98 Train Loss 0.7622336 Test MSE 7804.094598703461 Test RE 0.7398212923353803\n",
      "99 Train Loss 0.75644314 Test MSE 7645.278706292084 Test RE 0.7322547959571993\n",
      "100 Train Loss 0.7531014 Test MSE 7637.569190309441 Test RE 0.7318854991546504\n",
      "101 Train Loss 0.7502176 Test MSE 7539.5221635387 Test RE 0.7271725474882531\n",
      "102 Train Loss 0.74559814 Test MSE 7465.936577945034 Test RE 0.7236152516582957\n",
      "103 Train Loss 0.7363712 Test MSE 7386.320696870526 Test RE 0.7197466358716815\n",
      "104 Train Loss 0.73189974 Test MSE 7368.1338120698665 Test RE 0.7188599957385277\n",
      "105 Train Loss 0.7305676 Test MSE 7351.529250597216 Test RE 0.7180495403561118\n",
      "106 Train Loss 0.72838295 Test MSE 7310.521706845521 Test RE 0.7160440647431854\n",
      "107 Train Loss 0.72265846 Test MSE 7260.979692751295 Test RE 0.7136136930358052\n",
      "108 Train Loss 0.7168651 Test MSE 7192.078008345725 Test RE 0.7102197717122927\n",
      "109 Train Loss 0.70607316 Test MSE 7048.799633629267 Test RE 0.703109792511721\n",
      "110 Train Loss 0.6994317 Test MSE 6972.249053299224 Test RE 0.6992814532011287\n",
      "111 Train Loss 0.69405043 Test MSE 6923.181365608723 Test RE 0.6968164877686608\n",
      "112 Train Loss 0.68712974 Test MSE 6889.235293782428 Test RE 0.695106056669457\n",
      "113 Train Loss 0.67893904 Test MSE 6838.847724193203 Test RE 0.6925594037480378\n",
      "114 Train Loss 0.6696893 Test MSE 6727.0119553478135 Test RE 0.6868733452557699\n",
      "115 Train Loss 0.6638111 Test MSE 6713.882522781168 Test RE 0.6862027160098862\n",
      "116 Train Loss 0.6557927 Test MSE 6600.422496761733 Test RE 0.6803798320417148\n",
      "117 Train Loss 0.6471327 Test MSE 6436.940453930447 Test RE 0.6719010342061671\n",
      "118 Train Loss 0.64248997 Test MSE 6388.386449411885 Test RE 0.6693621541938176\n",
      "119 Train Loss 0.63653016 Test MSE 6390.478372723824 Test RE 0.6694717389609175\n",
      "120 Train Loss 0.63150406 Test MSE 6364.78543823543 Test RE 0.6681245771189377\n",
      "121 Train Loss 0.628451 Test MSE 6369.081512567538 Test RE 0.6683500229481749\n",
      "122 Train Loss 0.6281359 Test MSE 6374.816283221993 Test RE 0.6686508490328094\n",
      "123 Train Loss 0.6280845 Test MSE 6368.737392981973 Test RE 0.6683319673275062\n",
      "124 Train Loss 0.6273267 Test MSE 6309.203728311293 Test RE 0.6652009172429592\n",
      "125 Train Loss 0.62533414 Test MSE 6240.9119855711715 Test RE 0.661591006166626\n",
      "126 Train Loss 0.6161759 Test MSE 6134.220007432338 Test RE 0.6559114885813713\n",
      "127 Train Loss 0.5949463 Test MSE 5962.533812097339 Test RE 0.6466674349329992\n",
      "128 Train Loss 0.5647392 Test MSE 5578.234461732479 Test RE 0.6254807466787042\n",
      "129 Train Loss 0.5316386 Test MSE 5374.162710917495 Test RE 0.6139329876264711\n",
      "130 Train Loss 0.52391374 Test MSE 5398.044913142094 Test RE 0.6152956016256962\n",
      "131 Train Loss 0.52136725 Test MSE 5387.223997442282 Test RE 0.6146785817622548\n",
      "132 Train Loss 0.520171 Test MSE 5335.847906064023 Test RE 0.6117405718025718\n",
      "133 Train Loss 0.5187109 Test MSE 5328.921148611674 Test RE 0.6113433758249035\n",
      "134 Train Loss 0.517607 Test MSE 5327.602614628802 Test RE 0.6112677388562012\n",
      "135 Train Loss 0.515644 Test MSE 5297.0288970296415 Test RE 0.6095112624239367\n",
      "136 Train Loss 0.5148165 Test MSE 5295.0342011935745 Test RE 0.609396490154473\n",
      "137 Train Loss 0.51378465 Test MSE 5257.13010244192 Test RE 0.6072114135412364\n",
      "138 Train Loss 0.51104957 Test MSE 5208.074376986476 Test RE 0.6043717453142354\n",
      "139 Train Loss 0.50683314 Test MSE 5162.906943141907 Test RE 0.6017453077165204\n",
      "140 Train Loss 0.49601927 Test MSE 5014.164808772062 Test RE 0.593013890235267\n",
      "141 Train Loss 0.48615938 Test MSE 4964.2688658458865 Test RE 0.5900559733259009\n",
      "142 Train Loss 0.46406367 Test MSE 4733.405302979141 Test RE 0.5761723463034666\n",
      "143 Train Loss 0.44228545 Test MSE 4462.111971942947 Test RE 0.5594171751279176\n",
      "144 Train Loss 0.42434618 Test MSE 4318.325548101289 Test RE 0.5503300830401132\n",
      "145 Train Loss 0.4025913 Test MSE 4084.0046065329498 Test RE 0.5351908445425739\n",
      "146 Train Loss 0.38973993 Test MSE 3935.2646640879993 Test RE 0.5253545968816311\n",
      "147 Train Loss 0.3682941 Test MSE 3722.5330086395534 Test RE 0.5109575755591054\n",
      "148 Train Loss 0.3385085 Test MSE 3334.001220696198 Test RE 0.48355785340086915\n",
      "149 Train Loss 0.31506956 Test MSE 3150.4477610357703 Test RE 0.4700582778685109\n",
      "150 Train Loss 0.29009864 Test MSE 2950.5763305381533 Test RE 0.45490319892959435\n",
      "151 Train Loss 0.28241083 Test MSE 2890.46565350353 Test RE 0.4502455926775826\n",
      "152 Train Loss 0.27842745 Test MSE 2862.9537387838463 Test RE 0.4480977146887186\n",
      "153 Train Loss 0.27665508 Test MSE 2855.199758001666 Test RE 0.44749049271179986\n",
      "154 Train Loss 0.27309582 Test MSE 2808.3266796983207 Test RE 0.44380212429634525\n",
      "155 Train Loss 0.2673328 Test MSE 2741.6909706746765 Test RE 0.43850526748471447\n",
      "156 Train Loss 0.26235336 Test MSE 2682.9932220213605 Test RE 0.43378582023251744\n",
      "157 Train Loss 0.2565261 Test MSE 2589.1359002722647 Test RE 0.4261308604859394\n",
      "158 Train Loss 0.25477612 Test MSE 2558.261020095692 Test RE 0.42358248176127994\n",
      "159 Train Loss 0.25201535 Test MSE 2530.489556768708 Test RE 0.4212770866675806\n",
      "160 Train Loss 0.24966414 Test MSE 2519.8216131218533 Test RE 0.42038814666047725\n",
      "161 Train Loss 0.24735814 Test MSE 2506.9869177065034 Test RE 0.41931615769531844\n",
      "162 Train Loss 0.24615161 Test MSE 2467.3584742934204 Test RE 0.41598884929373703\n",
      "163 Train Loss 0.24337983 Test MSE 2386.4879174908224 Test RE 0.40911479344436863\n",
      "164 Train Loss 0.24065968 Test MSE 2342.366329704942 Test RE 0.40531527584405175\n",
      "165 Train Loss 0.2361556 Test MSE 2296.9378327665086 Test RE 0.40136563399475583\n",
      "166 Train Loss 0.23241332 Test MSE 2275.23802950579 Test RE 0.3994652291809477\n",
      "167 Train Loss 0.2281175 Test MSE 2272.9038187292645 Test RE 0.399260267039732\n",
      "168 Train Loss 0.22555046 Test MSE 2247.127482431655 Test RE 0.3969898648369182\n",
      "169 Train Loss 0.22187343 Test MSE 2216.762436516185 Test RE 0.3942985142036622\n",
      "170 Train Loss 0.21867165 Test MSE 2205.3327110383207 Test RE 0.39328069020479695\n",
      "171 Train Loss 0.21316916 Test MSE 2141.4183909340295 Test RE 0.3875398152024398\n",
      "172 Train Loss 0.21085235 Test MSE 2094.1236631332517 Test RE 0.3832363770472454\n",
      "173 Train Loss 0.2066663 Test MSE 2056.893014440658 Test RE 0.3798143905051463\n",
      "174 Train Loss 0.20342878 Test MSE 2012.5547451316518 Test RE 0.3756984602340521\n",
      "175 Train Loss 0.20128772 Test MSE 1989.2938369187964 Test RE 0.37352100747505657\n",
      "176 Train Loss 0.19989167 Test MSE 1987.2351699608873 Test RE 0.37332768399821864\n",
      "177 Train Loss 0.19676538 Test MSE 1962.4633000378992 Test RE 0.3709935299157049\n",
      "178 Train Loss 0.19417988 Test MSE 1895.7400659765322 Test RE 0.36463214998764754\n",
      "179 Train Loss 0.19268021 Test MSE 1867.8443980623838 Test RE 0.3619394406329075\n",
      "180 Train Loss 0.18932119 Test MSE 1827.0118203353795 Test RE 0.35796143657147983\n",
      "181 Train Loss 0.18561599 Test MSE 1812.1773724992056 Test RE 0.35650523821460106\n",
      "182 Train Loss 0.18129015 Test MSE 1777.5951466228744 Test RE 0.35308721441010005\n",
      "183 Train Loss 0.17476784 Test MSE 1705.2198466158663 Test RE 0.3458244963177089\n",
      "184 Train Loss 0.16848968 Test MSE 1663.4648415257816 Test RE 0.34156422491854954\n",
      "185 Train Loss 0.16406041 Test MSE 1614.158791470517 Test RE 0.33646406818016183\n",
      "186 Train Loss 0.16048989 Test MSE 1534.0855660409673 Test RE 0.32801247082610435\n",
      "187 Train Loss 0.1530985 Test MSE 1415.944085439078 Test RE 0.31512917727105294\n",
      "188 Train Loss 0.14599146 Test MSE 1368.8098366734607 Test RE 0.3098397421161598\n",
      "189 Train Loss 0.14148912 Test MSE 1345.2217661733594 Test RE 0.30715847776780975\n",
      "190 Train Loss 0.13810836 Test MSE 1317.0209857178756 Test RE 0.30392184103584796\n",
      "191 Train Loss 0.12878674 Test MSE 1254.1622003950295 Test RE 0.2965803800716727\n",
      "192 Train Loss 0.11597654 Test MSE 1076.629705675429 Test RE 0.2747886199507928\n",
      "193 Train Loss 0.10506609 Test MSE 929.2280495679628 Test RE 0.25528583579042713\n",
      "194 Train Loss 0.10138398 Test MSE 863.2656874760147 Test RE 0.2460581777236312\n",
      "195 Train Loss 0.100232035 Test MSE 832.8779665458477 Test RE 0.24168864691158368\n",
      "196 Train Loss 0.09367292 Test MSE 741.1897831648082 Test RE 0.22799760021140175\n",
      "197 Train Loss 0.08524416 Test MSE 721.8461646743967 Test RE 0.2250027833065596\n",
      "198 Train Loss 0.07928461 Test MSE 726.3073704495825 Test RE 0.22569700156615904\n",
      "199 Train Loss 0.07392997 Test MSE 667.4885182834921 Test RE 0.2163652264479961\n",
      "Training time: 237.36\n",
      "Training time: 237.36\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      "  (m_lambda): Sigmoid()\n",
      ")\n",
      "8\n",
      "0 Train Loss 9.802455 Test MSE 14285.262354186885 Test RE 1.0009437546210744\n",
      "1 Train Loss 2.695847 Test MSE 14271.076518350048 Test RE 1.0004466426225962\n",
      "2 Train Loss 1.4195406 Test MSE 14262.064038203805 Test RE 1.000130691353274\n",
      "3 Train Loss 1.1238598 Test MSE 14256.113690675762 Test RE 0.9999220348073031\n",
      "4 Train Loss 1.1099731 Test MSE 14255.924309711032 Test RE 0.9999153932066319\n",
      "5 Train Loss 1.1167731 Test MSE 14256.947558966285 Test RE 0.9999512780885914\n",
      "6 Train Loss 1.1282678 Test MSE 14256.948651175224 Test RE 0.99995131639117\n",
      "7 Train Loss 1.1394613 Test MSE 14253.564501921914 Test RE 0.9998326309199619\n",
      "8 Train Loss 1.1506903 Test MSE 14253.56263618934 Test RE 0.9998325654829842\n",
      "9 Train Loss 1.1613369 Test MSE 14253.56891082189 Test RE 0.9998327855536229\n",
      "10 Train Loss 1.1714659 Test MSE 14253.56959848866 Test RE 0.9998328096722062\n",
      "11 Train Loss 1.1811838 Test MSE 14253.569182702346 Test RE 0.9998327950893047\n",
      "12 Train Loss 1.1903431 Test MSE 14253.58608915829 Test RE 0.9998333880504034\n",
      "13 Train Loss 1.1990001 Test MSE 14253.601841956794 Test RE 0.9998339405488921\n",
      "14 Train Loss 1.2070097 Test MSE 14253.640240134258 Test RE 0.9998352872883217\n",
      "15 Train Loss 1.2147336 Test MSE 14253.553732896173 Test RE 0.9998322532170593\n",
      "16 Train Loss 1.2220507 Test MSE 14253.544205661286 Test RE 0.9998319190674678\n",
      "17 Train Loss 1.2288384 Test MSE 14253.491812619832 Test RE 0.9998300814794769\n",
      "18 Train Loss 1.2350361 Test MSE 14253.758202031073 Test RE 0.9998394245550477\n",
      "19 Train Loss 1.2410606 Test MSE 14253.758202031073 Test RE 0.9998394245550477\n",
      "20 Train Loss 1.2470019 Test MSE 14253.273851469572 Test RE 0.9998224368633151\n",
      "21 Train Loss 1.2520307 Test MSE 14253.158907138268 Test RE 0.9998184053631024\n",
      "22 Train Loss 1.2572007 Test MSE 14253.158907138268 Test RE 0.9998184053631024\n",
      "23 Train Loss 1.2619243 Test MSE 14253.117411305228 Test RE 0.9998169499549334\n",
      "24 Train Loss 1.266258 Test MSE 14252.968223011712 Test RE 0.9998117173671911\n",
      "25 Train Loss 1.2707402 Test MSE 14252.968223011712 Test RE 0.9998117173671911\n",
      "26 Train Loss 1.2748568 Test MSE 14252.94448002278 Test RE 0.9998108846098295\n",
      "27 Train Loss 1.2784342 Test MSE 14252.91628543585 Test RE 0.9998098957170122\n",
      "28 Train Loss 1.282056 Test MSE 14252.91628543585 Test RE 0.9998098957170122\n",
      "29 Train Loss 1.2856536 Test MSE 14252.787833887269 Test RE 0.9998053904138104\n",
      "30 Train Loss 1.2887465 Test MSE 14252.787833887269 Test RE 0.9998053904138104\n",
      "31 Train Loss 1.2917343 Test MSE 14253.018746828173 Test RE 0.9998134894282154\n",
      "32 Train Loss 1.294844 Test MSE 14253.066271778043 Test RE 0.9998151563047979\n",
      "33 Train Loss 1.2977206 Test MSE 14253.081692345128 Test RE 0.9998156971607871\n",
      "34 Train Loss 1.300155 Test MSE 14253.073586511882 Test RE 0.9998154128594464\n",
      "35 Train Loss 1.3028677 Test MSE 14253.054295389155 Test RE 0.9998147362486002\n",
      "36 Train Loss 1.3050473 Test MSE 14253.003617689132 Test RE 0.9998129587925101\n",
      "37 Train Loss 1.3076346 Test MSE 14253.003617689132 Test RE 0.9998129587925101\n",
      "38 Train Loss 1.3095602 Test MSE 14252.772197180793 Test RE 0.9998048419713624\n",
      "39 Train Loss 1.3117993 Test MSE 14252.772197180793 Test RE 0.9998048419713624\n",
      "40 Train Loss 1.3142897 Test MSE 14253.072899509969 Test RE 0.9998153887637623\n",
      "41 Train Loss 1.3161235 Test MSE 14253.072899509969 Test RE 0.9998153887637623\n",
      "42 Train Loss 1.3181183 Test MSE 14253.072899509969 Test RE 0.9998153887637623\n",
      "43 Train Loss 1.3197249 Test MSE 14253.072899509969 Test RE 0.9998153887637623\n",
      "44 Train Loss 1.3214283 Test MSE 14253.072899509969 Test RE 0.9998153887637623\n",
      "45 Train Loss 1.3230759 Test MSE 14253.072899509969 Test RE 0.9998153887637623\n",
      "46 Train Loss 1.3250937 Test MSE 14252.879143995726 Test RE 0.9998085930220822\n",
      "47 Train Loss 1.3262247 Test MSE 14252.890206063872 Test RE 0.9998089810120641\n",
      "48 Train Loss 1.3280032 Test MSE 14252.928216341834 Test RE 0.9998103141799828\n",
      "49 Train Loss 1.3291626 Test MSE 14253.043762479803 Test RE 0.9998143668204161\n",
      "50 Train Loss 1.3306409 Test MSE 14253.043762479803 Test RE 0.9998143668204161\n",
      "51 Train Loss 1.3318366 Test MSE 14252.948893539058 Test RE 0.9998110394087654\n",
      "52 Train Loss 1.3331504 Test MSE 14252.948893539058 Test RE 0.9998110394087654\n",
      "53 Train Loss 1.3344204 Test MSE 14252.948893539058 Test RE 0.9998110394087654\n",
      "54 Train Loss 1.3355241 Test MSE 14252.952279900393 Test RE 0.9998111581814233\n",
      "55 Train Loss 1.3364035 Test MSE 14252.983330995803 Test RE 0.9998122472615711\n",
      "56 Train Loss 1.337446 Test MSE 14252.841135670491 Test RE 0.9998072599203583\n",
      "57 Train Loss 1.3384805 Test MSE 14252.750776057866 Test RE 0.9998040906455791\n",
      "58 Train Loss 1.3395308 Test MSE 14252.154817608242 Test RE 0.9997831877358934\n",
      "59 Train Loss 1.3401754 Test MSE 14252.216692188073 Test RE 0.9997853579725456\n",
      "60 Train Loss 1.3410107 Test MSE 14251.591443126681 Test RE 0.9997634272888385\n",
      "61 Train Loss 1.3420441 Test MSE 14251.564698866918 Test RE 0.9997624892200814\n",
      "62 Train Loss 1.3433651 Test MSE 14251.418245958554 Test RE 0.999757352292892\n",
      "63 Train Loss 1.3440115 Test MSE 14251.508716001747 Test RE 0.9997605255892523\n",
      "64 Train Loss 1.3449283 Test MSE 14251.509588002138 Test RE 0.9997605561751914\n",
      "65 Train Loss 1.3454704 Test MSE 14251.488933982062 Test RE 0.9997598317227413\n",
      "66 Train Loss 1.3464582 Test MSE 14251.488933982062 Test RE 0.9997598317227413\n",
      "67 Train Loss 1.347032 Test MSE 14251.488933982062 Test RE 0.9997598317227413\n",
      "68 Train Loss 1.3475144 Test MSE 14251.455246478976 Test RE 0.9997586501117364\n",
      "69 Train Loss 1.3484551 Test MSE 14251.396809184409 Test RE 0.9997566003824417\n",
      "70 Train Loss 1.3489798 Test MSE 14251.396809184409 Test RE 0.9997566003824417\n",
      "71 Train Loss 1.3497695 Test MSE 14251.375145532846 Test RE 0.9997558405135263\n",
      "72 Train Loss 1.3503528 Test MSE 14251.375145532846 Test RE 0.9997558405135263\n",
      "73 Train Loss 1.3514181 Test MSE 14251.248616434774 Test RE 0.9997514023984577\n",
      "74 Train Loss 1.3518598 Test MSE 14251.248616434774 Test RE 0.9997514023984577\n",
      "75 Train Loss 1.3521861 Test MSE 14251.248616434774 Test RE 0.9997514023984577\n",
      "76 Train Loss 1.3526309 Test MSE 14251.248616434774 Test RE 0.9997514023984577\n",
      "77 Train Loss 1.353707 Test MSE 14251.147725170413 Test RE 0.9997478635379595\n",
      "78 Train Loss 1.3543311 Test MSE 14251.08196999258 Test RE 0.9997455571035709\n",
      "79 Train Loss 1.3544577 Test MSE 14251.08196999258 Test RE 0.9997455571035709\n",
      "80 Train Loss 1.3548946 Test MSE 14251.08196999258 Test RE 0.9997455571035709\n",
      "81 Train Loss 1.3558685 Test MSE 14251.08196999258 Test RE 0.9997455571035709\n",
      "82 Train Loss 1.3558854 Test MSE 14251.08196999258 Test RE 0.9997455571035709\n",
      "83 Train Loss 1.3562515 Test MSE 14251.011562928987 Test RE 0.9997430874933299\n",
      "84 Train Loss 1.357114 Test MSE 14250.990391591129 Test RE 0.9997423448826581\n",
      "85 Train Loss 1.3573217 Test MSE 14250.964102287433 Test RE 0.9997414227522827\n",
      "86 Train Loss 1.3579885 Test MSE 14250.955654835912 Test RE 0.9997411264471205\n",
      "87 Train Loss 1.3581898 Test MSE 14250.955654835912 Test RE 0.9997411264471205\n",
      "88 Train Loss 1.3585538 Test MSE 14250.952360236137 Test RE 0.9997410108847978\n",
      "89 Train Loss 1.358923 Test MSE 14250.961608514164 Test RE 0.9997413352800062\n",
      "90 Train Loss 1.3595841 Test MSE 14250.961608514164 Test RE 0.9997413352800062\n",
      "91 Train Loss 1.3601807 Test MSE 14250.961608514164 Test RE 0.9997413352800062\n",
      "92 Train Loss 1.3602417 Test MSE 14250.961608514164 Test RE 0.9997413352800062\n",
      "93 Train Loss 1.3604251 Test MSE 14250.964608925111 Test RE 0.9997414405232443\n",
      "94 Train Loss 1.3605583 Test MSE 14250.964608925111 Test RE 0.9997414405232443\n",
      "95 Train Loss 1.3612602 Test MSE 14250.990774690237 Test RE 0.9997423583203349\n",
      "96 Train Loss 1.3617716 Test MSE 14250.990774690237 Test RE 0.9997423583203349\n",
      "97 Train Loss 1.3619075 Test MSE 14250.990774690237 Test RE 0.9997423583203349\n",
      "98 Train Loss 1.3622804 Test MSE 14250.990774690237 Test RE 0.9997423583203349\n",
      "99 Train Loss 1.3628263 Test MSE 14250.990774690237 Test RE 0.9997423583203349\n",
      "100 Train Loss 1.3629762 Test MSE 14250.990774690237 Test RE 0.9997423583203349\n",
      "101 Train Loss 1.36297 Test MSE 14250.990774690237 Test RE 0.9997423583203349\n",
      "102 Train Loss 1.3635391 Test MSE 14250.990774690237 Test RE 0.9997423583203349\n",
      "103 Train Loss 1.363753 Test MSE 14250.990774690237 Test RE 0.9997423583203349\n",
      "104 Train Loss 1.364415 Test MSE 14250.990774690237 Test RE 0.9997423583203349\n",
      "105 Train Loss 1.3651582 Test MSE 14250.990774690237 Test RE 0.9997423583203349\n",
      "106 Train Loss 1.3648837 Test MSE 14250.990774690237 Test RE 0.9997423583203349\n",
      "107 Train Loss 1.3651534 Test MSE 14251.059602391171 Test RE 0.9997447725344079\n",
      "108 Train Loss 1.3651689 Test MSE 14251.059602391171 Test RE 0.9997447725344079\n",
      "109 Train Loss 1.3655722 Test MSE 14251.267909576683 Test RE 0.9997520791229915\n",
      "110 Train Loss 1.3660016 Test MSE 14251.267909576683 Test RE 0.9997520791229915\n",
      "111 Train Loss 1.3660576 Test MSE 14251.267909576683 Test RE 0.9997520791229915\n",
      "112 Train Loss 1.3667496 Test MSE 14251.267909576683 Test RE 0.9997520791229915\n",
      "113 Train Loss 1.3667294 Test MSE 14251.132273839357 Test RE 0.9997473215661936\n",
      "114 Train Loss 1.3671608 Test MSE 14251.132273839357 Test RE 0.9997473215661936\n",
      "115 Train Loss 1.367151 Test MSE 14251.130178783216 Test RE 0.9997472480798627\n",
      "116 Train Loss 1.3674104 Test MSE 14251.130178783216 Test RE 0.9997472480798627\n",
      "117 Train Loss 1.3676537 Test MSE 14251.130178783216 Test RE 0.9997472480798627\n",
      "118 Train Loss 1.3679546 Test MSE 14251.271621858152 Test RE 0.9997522093345943\n",
      "119 Train Loss 1.3677734 Test MSE 14251.271621858152 Test RE 0.9997522093345943\n",
      "120 Train Loss 1.3684509 Test MSE 14251.186351263927 Test RE 0.9997492183877249\n",
      "121 Train Loss 1.368072 Test MSE 14251.088257420033 Test RE 0.9997457776421581\n",
      "122 Train Loss 1.3686675 Test MSE 14250.908145011985 Test RE 0.9997394599776702\n",
      "123 Train Loss 1.3689167 Test MSE 14250.82252892045 Test RE 0.9997364568738396\n",
      "124 Train Loss 1.3692937 Test MSE 14250.694318129772 Test RE 0.999731959684655\n",
      "125 Train Loss 1.3694742 Test MSE 14250.694318129772 Test RE 0.999731959684655\n",
      "126 Train Loss 1.3692954 Test MSE 14250.657459773398 Test RE 0.9997306668178005\n",
      "127 Train Loss 1.3702374 Test MSE 14250.60269440493 Test RE 0.9997287458302572\n",
      "128 Train Loss 1.3700049 Test MSE 14250.559272484928 Test RE 0.9997272227306511\n",
      "129 Train Loss 1.3698477 Test MSE 14250.559272484928 Test RE 0.9997272227306511\n",
      "130 Train Loss 1.3705564 Test MSE 14250.206713500214 Test RE 0.9997148560232149\n",
      "131 Train Loss 1.3702164 Test MSE 14250.206713500214 Test RE 0.9997148560232149\n",
      "132 Train Loss 1.3706582 Test MSE 14250.206713500214 Test RE 0.9997148560232149\n",
      "133 Train Loss 1.3701414 Test MSE 14250.280666462468 Test RE 0.9997174500830972\n",
      "134 Train Loss 1.3707172 Test MSE 14250.329601453794 Test RE 0.9997191665799118\n",
      "135 Train Loss 1.371065 Test MSE 14250.496978403966 Test RE 0.9997250376530193\n",
      "136 Train Loss 1.3712618 Test MSE 14250.496978403966 Test RE 0.9997250376530193\n",
      "137 Train Loss 1.3712039 Test MSE 14250.496978403966 Test RE 0.9997250376530193\n",
      "138 Train Loss 1.3714647 Test MSE 14250.717468001752 Test RE 0.9997327717032476\n",
      "139 Train Loss 1.3717067 Test MSE 14250.67879371033 Test RE 0.9997314151405737\n",
      "140 Train Loss 1.3717456 Test MSE 14249.911132640345 Test RE 0.9997044878189134\n",
      "141 Train Loss 1.37174 Test MSE 14249.911132640345 Test RE 0.9997044878189134\n",
      "142 Train Loss 1.3719071 Test MSE 14249.779320641754 Test RE 0.9996998641637185\n",
      "143 Train Loss 1.3724478 Test MSE 14249.62687673156 Test RE 0.9996945167628756\n",
      "144 Train Loss 1.3726131 Test MSE 14249.62687673156 Test RE 0.9996945167628756\n",
      "145 Train Loss 1.3722738 Test MSE 14249.651758076654 Test RE 0.999695389548304\n",
      "146 Train Loss 1.372356 Test MSE 14249.684613892025 Test RE 0.999696542060266\n",
      "147 Train Loss 1.372636 Test MSE 14249.684613892025 Test RE 0.999696542060266\n",
      "148 Train Loss 1.3729959 Test MSE 14249.649727806858 Test RE 0.9996953183307241\n",
      "149 Train Loss 1.3728347 Test MSE 14249.649727806858 Test RE 0.9996953183307241\n",
      "150 Train Loss 1.3732139 Test MSE 14249.649727806858 Test RE 0.9996953183307241\n",
      "151 Train Loss 1.3731419 Test MSE 14249.611524039588 Test RE 0.9996939782222439\n",
      "152 Train Loss 1.3732332 Test MSE 14249.565933635033 Test RE 0.9996923790035817\n",
      "153 Train Loss 1.3733351 Test MSE 14249.565933635033 Test RE 0.9996923790035817\n",
      "154 Train Loss 1.3734752 Test MSE 14249.537707464875 Test RE 0.9996913888856618\n",
      "155 Train Loss 1.3735181 Test MSE 14249.352003420088 Test RE 0.9996848747326024\n",
      "156 Train Loss 1.3735332 Test MSE 14249.352003420088 Test RE 0.9996848747326024\n",
      "157 Train Loss 1.3737999 Test MSE 14249.341349673168 Test RE 0.9996845010176558\n",
      "158 Train Loss 1.3736231 Test MSE 14249.341349673168 Test RE 0.9996845010176558\n",
      "159 Train Loss 1.3742396 Test MSE 14249.341349673168 Test RE 0.9996845010176558\n",
      "160 Train Loss 1.3740475 Test MSE 14249.22129508222 Test RE 0.9996802897013674\n",
      "161 Train Loss 1.3739252 Test MSE 14248.007697675446 Test RE 0.9996377177174243\n",
      "162 Train Loss 1.3746467 Test MSE 14247.828140391202 Test RE 0.9996314188438153\n",
      "163 Train Loss 1.3744968 Test MSE 14247.731344890562 Test RE 0.9996280232389896\n",
      "164 Train Loss 1.3743135 Test MSE 14247.601790186602 Test RE 0.9996234784169442\n",
      "165 Train Loss 1.3747199 Test MSE 14247.508937033599 Test RE 0.9996202210847847\n",
      "166 Train Loss 1.3747888 Test MSE 14247.508937033599 Test RE 0.9996202210847847\n",
      "167 Train Loss 1.3747838 Test MSE 14247.508937033599 Test RE 0.9996202210847847\n",
      "168 Train Loss 1.3748009 Test MSE 14247.09823583924 Test RE 0.9996058133669453\n",
      "169 Train Loss 1.3750087 Test MSE 14247.09823583924 Test RE 0.9996058133669453\n",
      "170 Train Loss 1.3748765 Test MSE 14247.433604037255 Test RE 0.9996175783601394\n",
      "171 Train Loss 1.3748273 Test MSE 14245.617997127702 Test RE 0.9995538837167153\n",
      "172 Train Loss 1.3752532 Test MSE 14245.497028949168 Test RE 0.9995496397987127\n",
      "173 Train Loss 1.375153 Test MSE 14241.29291219833 Test RE 0.9994021358730802\n",
      "174 Train Loss 1.374865 Test MSE 14239.49284012371 Test RE 0.9993389726217451\n",
      "175 Train Loss 1.3750979 Test MSE 14238.960089928876 Test RE 0.9993202780299443\n",
      "176 Train Loss 1.3750321 Test MSE 14239.091279793276 Test RE 0.9993248816101558\n",
      "177 Train Loss 1.3753949 Test MSE 14239.177046528459 Test RE 0.9993278912370249\n",
      "178 Train Loss 1.37534 Test MSE 14238.690781929965 Test RE 0.9993108276833189\n",
      "179 Train Loss 1.3754536 Test MSE 14238.690781929965 Test RE 0.9993108276833189\n",
      "180 Train Loss 1.3757184 Test MSE 14238.620500924131 Test RE 0.999308361422275\n",
      "181 Train Loss 1.3755292 Test MSE 14238.446308564788 Test RE 0.9993022487369353\n",
      "182 Train Loss 1.3755965 Test MSE 14237.73644802993 Test RE 0.9992773382215955\n",
      "183 Train Loss 1.3756273 Test MSE 14237.845935244413 Test RE 0.9992811804012255\n",
      "184 Train Loss 1.3758048 Test MSE 14237.859096352384 Test RE 0.9992816422563591\n",
      "185 Train Loss 1.3755244 Test MSE 14235.388643904413 Test RE 0.9991949443596182\n",
      "186 Train Loss 1.374882 Test MSE 14228.597642483532 Test RE 0.9989565826291938\n",
      "187 Train Loss 1.3751458 Test MSE 14228.218607461498 Test RE 0.9989432769591093\n",
      "188 Train Loss 1.3750914 Test MSE 14224.44961853686 Test RE 0.9988109604823507\n",
      "189 Train Loss 1.3740216 Test MSE 14206.750553375152 Test RE 0.9981893714848044\n",
      "190 Train Loss 1.3716955 Test MSE 14190.445909320033 Test RE 0.9976164116572048\n",
      "191 Train Loss 1.3703685 Test MSE 14167.46548001907 Test RE 0.996808299540499\n",
      "192 Train Loss 1.3684704 Test MSE 14147.937830525689 Test RE 0.9961210899891397\n",
      "193 Train Loss 1.366852 Test MSE 14129.492349168226 Test RE 0.995471527985435\n",
      "194 Train Loss 1.3631498 Test MSE 14087.819792263033 Test RE 0.9940024562218149\n",
      "195 Train Loss 1.3596408 Test MSE 14057.544188758524 Test RE 0.9929337951169805\n",
      "196 Train Loss 1.3533709 Test MSE 13988.141534040858 Test RE 0.9904796855426421\n",
      "197 Train Loss 1.3461267 Test MSE 13908.790944832428 Test RE 0.9876663409149816\n",
      "198 Train Loss 1.3416401 Test MSE 13836.011266988724 Test RE 0.9850789011293652\n",
      "199 Train Loss 1.3350662 Test MSE 13778.613206202714 Test RE 0.9830335001975653\n",
      "Training time: 154.34\n",
      "Training time: 154.34\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      "  (m_lambda): Sigmoid()\n",
      ")\n",
      "9\n",
      "0 Train Loss 1.9898322 Test MSE 14267.951146572022 Test RE 1.0003370875057183\n",
      "1 Train Loss 1.2026265 Test MSE 14259.1764512441 Test RE 1.0000294398609142\n",
      "2 Train Loss 1.0864964 Test MSE 14255.339064347841 Test RE 0.9998948683412903\n",
      "3 Train Loss 1.0930961 Test MSE 14253.929608518649 Test RE 0.999845436248464\n",
      "4 Train Loss 1.1088002 Test MSE 14255.029747218108 Test RE 0.9998840202552828\n",
      "5 Train Loss 1.1237818 Test MSE 14253.530356421517 Test RE 0.9998314333315996\n",
      "6 Train Loss 1.1380634 Test MSE 14253.70549602806 Test RE 0.9998375760043816\n",
      "7 Train Loss 1.1521897 Test MSE 14253.943428816905 Test RE 0.999845920962475\n",
      "8 Train Loss 1.1652528 Test MSE 14253.967465702886 Test RE 0.9998467639983827\n",
      "9 Train Loss 1.1775937 Test MSE 14254.059129878593 Test RE 0.9998499788921423\n",
      "10 Train Loss 1.1892294 Test MSE 14254.059129878593 Test RE 0.9998499788921423\n",
      "11 Train Loss 1.1998196 Test MSE 14254.140949641187 Test RE 0.9998528485087814\n",
      "12 Train Loss 1.2098554 Test MSE 14254.140949641187 Test RE 0.9998528485087814\n",
      "13 Train Loss 1.2192076 Test MSE 14254.140949641187 Test RE 0.9998528485087814\n",
      "14 Train Loss 1.2278165 Test MSE 14254.140949641187 Test RE 0.9998528485087814\n",
      "15 Train Loss 1.2361151 Test MSE 14254.140949641187 Test RE 0.9998528485087814\n",
      "16 Train Loss 1.2434565 Test MSE 14254.140949641187 Test RE 0.9998528485087814\n",
      "17 Train Loss 1.2505174 Test MSE 14254.140949641187 Test RE 0.9998528485087814\n",
      "18 Train Loss 1.2572292 Test MSE 14254.156402809454 Test RE 0.9998533904877814\n",
      "19 Train Loss 1.2632579 Test MSE 14254.138519129265 Test RE 0.999852763264978\n",
      "20 Train Loss 1.2687489 Test MSE 14254.138519129265 Test RE 0.999852763264978\n",
      "21 Train Loss 1.2742392 Test MSE 14254.138519129265 Test RE 0.999852763264978\n",
      "22 Train Loss 1.2793614 Test MSE 14254.138519129265 Test RE 0.999852763264978\n",
      "23 Train Loss 1.2841294 Test MSE 14254.134876145192 Test RE 0.9998526354968944\n",
      "24 Train Loss 1.288446 Test MSE 14254.132009877874 Test RE 0.9998525349700963\n",
      "25 Train Loss 1.292688 Test MSE 14254.13146405224 Test RE 0.9998525158266948\n",
      "26 Train Loss 1.2963395 Test MSE 14254.13146405224 Test RE 0.9998525158266948\n",
      "27 Train Loss 1.2999696 Test MSE 14254.13146405224 Test RE 0.9998525158266948\n",
      "28 Train Loss 1.3035308 Test MSE 14254.13146405224 Test RE 0.9998525158266948\n",
      "29 Train Loss 1.3067801 Test MSE 14254.13146405224 Test RE 0.9998525158266948\n",
      "30 Train Loss 1.3098006 Test MSE 14254.121932695703 Test RE 0.9998521815393189\n",
      "31 Train Loss 1.3125395 Test MSE 14254.11441849444 Test RE 0.9998519179983042\n",
      "32 Train Loss 1.31549 Test MSE 14254.107645411146 Test RE 0.99985168045001\n",
      "33 Train Loss 1.3179338 Test MSE 14254.087235874464 Test RE 0.9998509646382118\n",
      "34 Train Loss 1.3203557 Test MSE 14254.08016360824 Test RE 0.9998507165966163\n",
      "35 Train Loss 1.3226525 Test MSE 14254.062466937741 Test RE 0.9998500959310255\n",
      "36 Train Loss 1.3250186 Test MSE 14254.046870024775 Test RE 0.9998495489087561\n",
      "37 Train Loss 1.3269112 Test MSE 14254.046870024775 Test RE 0.9998495489087561\n",
      "38 Train Loss 1.3289493 Test MSE 14254.046870024775 Test RE 0.9998495489087561\n",
      "39 Train Loss 1.3308058 Test MSE 14254.046870024775 Test RE 0.9998495489087561\n",
      "40 Train Loss 1.3324176 Test MSE 14254.046870024775 Test RE 0.9998495489087561\n",
      "41 Train Loss 1.3342093 Test MSE 14254.046870024775 Test RE 0.9998495489087561\n",
      "42 Train Loss 1.3358297 Test MSE 14254.034676359859 Test RE 0.9998491212465951\n",
      "43 Train Loss 1.3374492 Test MSE 14254.021566469415 Test RE 0.9998486614499218\n",
      "44 Train Loss 1.3388728 Test MSE 14254.004540140122 Test RE 0.999848064293619\n",
      "45 Train Loss 1.340252 Test MSE 14253.980190638616 Test RE 0.9998472102945805\n",
      "46 Train Loss 1.3415294 Test MSE 14253.980190638616 Test RE 0.9998472102945805\n",
      "47 Train Loss 1.3428605 Test MSE 14253.96383698894 Test RE 0.9998466367300225\n",
      "48 Train Loss 1.344023 Test MSE 14253.96383698894 Test RE 0.9998466367300225\n",
      "49 Train Loss 1.3453345 Test MSE 14253.944518531127 Test RE 0.999845959181593\n",
      "50 Train Loss 1.3463607 Test MSE 14253.944518531127 Test RE 0.999845959181593\n",
      "51 Train Loss 1.3474516 Test MSE 14253.926587706259 Test RE 0.9998453303006404\n",
      "52 Train Loss 1.348575 Test MSE 14253.908929879368 Test RE 0.9998447109940543\n",
      "53 Train Loss 1.3493134 Test MSE 14253.908929879368 Test RE 0.9998447109940543\n",
      "54 Train Loss 1.3504168 Test MSE 14253.908929879368 Test RE 0.9998447109940543\n",
      "55 Train Loss 1.3513769 Test MSE 14253.908929879368 Test RE 0.9998447109940543\n",
      "56 Train Loss 1.3522309 Test MSE 14253.908929879368 Test RE 0.9998447109940543\n",
      "57 Train Loss 1.3530847 Test MSE 14253.908929879368 Test RE 0.9998447109940543\n",
      "58 Train Loss 1.3539207 Test MSE 14253.908929879368 Test RE 0.9998447109940543\n",
      "59 Train Loss 1.3547139 Test MSE 14253.908929879368 Test RE 0.9998447109940543\n",
      "60 Train Loss 1.3553832 Test MSE 14253.831488228228 Test RE 0.9998419949064582\n",
      "61 Train Loss 1.3561425 Test MSE 14253.804071356082 Test RE 0.9998410333210297\n",
      "62 Train Loss 1.3568466 Test MSE 14253.784630145137 Test RE 0.999840351463507\n",
      "63 Train Loss 1.3576177 Test MSE 14253.76565267137 Test RE 0.9998396858700928\n",
      "64 Train Loss 1.3582054 Test MSE 14253.76565267137 Test RE 0.9998396858700928\n",
      "65 Train Loss 1.3588715 Test MSE 14253.76565267137 Test RE 0.9998396858700928\n",
      "66 Train Loss 1.359386 Test MSE 14253.76565267137 Test RE 0.9998396858700928\n",
      "67 Train Loss 1.3600024 Test MSE 14253.757141156622 Test RE 0.9998393873471725\n",
      "68 Train Loss 1.3606553 Test MSE 14253.709684261528 Test RE 0.9998377228978643\n",
      "69 Train Loss 1.3611004 Test MSE 14253.709684261528 Test RE 0.9998377228978643\n",
      "70 Train Loss 1.361689 Test MSE 14253.718320928725 Test RE 0.9998380258107393\n",
      "71 Train Loss 1.3621764 Test MSE 14253.714993031395 Test RE 0.9998379090917745\n",
      "72 Train Loss 1.3627043 Test MSE 14253.698070654478 Test RE 0.9998373155749621\n",
      "73 Train Loss 1.3631312 Test MSE 14253.676995484917 Test RE 0.9998365764071812\n",
      "74 Train Loss 1.3636235 Test MSE 14253.636119024139 Test RE 0.9998351427486976\n",
      "75 Train Loss 1.3640666 Test MSE 14253.395256578367 Test RE 0.9998266949479668\n",
      "76 Train Loss 1.3645165 Test MSE 14252.918898169823 Test RE 0.9998099873557027\n",
      "77 Train Loss 1.3646473 Test MSE 14250.502142096304 Test RE 0.9997252187790552\n",
      "78 Train Loss 1.3649634 Test MSE 14248.382453880857 Test RE 0.9996508640457568\n",
      "79 Train Loss 1.365396 Test MSE 14248.190414900067 Test RE 0.9996441274010691\n",
      "80 Train Loss 1.3657298 Test MSE 14248.190414900067 Test RE 0.9996441274010691\n",
      "81 Train Loss 1.3661174 Test MSE 14247.820722361284 Test RE 0.9996311586183037\n",
      "82 Train Loss 1.3663913 Test MSE 14247.820722361284 Test RE 0.9996311586183037\n",
      "83 Train Loss 1.3668764 Test MSE 14247.745927328604 Test RE 0.9996285347944657\n",
      "84 Train Loss 1.367216 Test MSE 14247.632116459565 Test RE 0.9996245422745246\n",
      "85 Train Loss 1.3675286 Test MSE 14247.279371848528 Test RE 0.9996121677848554\n",
      "86 Train Loss 1.3677181 Test MSE 14247.129683802794 Test RE 0.9996069165933469\n",
      "87 Train Loss 1.368114 Test MSE 14247.026219026478 Test RE 0.999603286939652\n",
      "88 Train Loss 1.3683927 Test MSE 14247.026219026478 Test RE 0.999603286939652\n",
      "89 Train Loss 1.3687764 Test MSE 14247.026219026478 Test RE 0.999603286939652\n",
      "90 Train Loss 1.369044 Test MSE 14246.611026396851 Test RE 0.9995887214108645\n",
      "91 Train Loss 1.3692966 Test MSE 14246.611026396851 Test RE 0.9995887214108645\n",
      "92 Train Loss 1.3695095 Test MSE 14246.609348392321 Test RE 0.9995886625437258\n",
      "93 Train Loss 1.3698233 Test MSE 14246.609348392321 Test RE 0.9995886625437258\n",
      "94 Train Loss 1.3701141 Test MSE 14246.609348392321 Test RE 0.9995886625437258\n",
      "95 Train Loss 1.3703877 Test MSE 14246.555320763335 Test RE 0.9995867671644367\n",
      "96 Train Loss 1.3706223 Test MSE 14246.555320763335 Test RE 0.9995867671644367\n",
      "97 Train Loss 1.3708463 Test MSE 14246.555514296992 Test RE 0.9995867739539263\n",
      "98 Train Loss 1.3710903 Test MSE 14246.568577099242 Test RE 0.9995872322191311\n",
      "99 Train Loss 1.3713206 Test MSE 14246.56519975027 Test RE 0.999587113736035\n",
      "100 Train Loss 1.371487 Test MSE 14246.594434792023 Test RE 0.9995881393501252\n",
      "101 Train Loss 1.3717732 Test MSE 14246.608268224341 Test RE 0.9995886246496678\n",
      "102 Train Loss 1.3720635 Test MSE 14246.608268224341 Test RE 0.9995886246496678\n",
      "103 Train Loss 1.3721212 Test MSE 14246.609504381002 Test RE 0.9995886680160635\n",
      "104 Train Loss 1.3724134 Test MSE 14246.609504381002 Test RE 0.9995886680160635\n",
      "105 Train Loss 1.3725586 Test MSE 14246.609504381002 Test RE 0.9995886680160635\n",
      "106 Train Loss 1.3727535 Test MSE 14246.627139107355 Test RE 0.9995892866708797\n",
      "107 Train Loss 1.3729424 Test MSE 14246.627139107355 Test RE 0.9995892866708797\n",
      "108 Train Loss 1.3732047 Test MSE 14246.627139107355 Test RE 0.9995892866708797\n",
      "109 Train Loss 1.3733618 Test MSE 14246.469027859615 Test RE 0.999583739858069\n",
      "110 Train Loss 1.3734982 Test MSE 14246.469027859615 Test RE 0.999583739858069\n",
      "111 Train Loss 1.3737748 Test MSE 14246.532292293701 Test RE 0.9995859592862407\n",
      "112 Train Loss 1.3738563 Test MSE 14246.509446315733 Test RE 0.9995851578095204\n",
      "113 Train Loss 1.3740499 Test MSE 14246.390122486753 Test RE 0.9995809717112129\n",
      "114 Train Loss 1.3742404 Test MSE 14246.321789123998 Test RE 0.9995785744438885\n",
      "115 Train Loss 1.37437 Test MSE 14246.245901375169 Test RE 0.9995759121472644\n",
      "116 Train Loss 1.374451 Test MSE 14246.163799205297 Test RE 0.9995730318281212\n",
      "117 Train Loss 1.3746029 Test MSE 14245.446901899844 Test RE 0.9995478811897266\n",
      "118 Train Loss 1.3747073 Test MSE 14245.505137696719 Test RE 0.9995499242778897\n",
      "119 Train Loss 1.3746651 Test MSE 14242.431238047871 Test RE 0.999442076858772\n",
      "120 Train Loss 1.3743563 Test MSE 14236.4977492814 Test RE 0.9992338681617109\n",
      "121 Train Loss 1.3742098 Test MSE 14229.591142932088 Test RE 0.9989914576925686\n",
      "122 Train Loss 1.3742766 Test MSE 14228.886736075056 Test RE 0.9989667308703144\n",
      "123 Train Loss 1.3744221 Test MSE 14228.083270218493 Test RE 0.9989385260287843\n",
      "124 Train Loss 1.374087 Test MSE 14217.607615296338 Test RE 0.998570716044533\n",
      "125 Train Loss 1.3729666 Test MSE 14207.205683642664 Test RE 0.9982053604527064\n",
      "126 Train Loss 1.3700577 Test MSE 14182.363925953157 Test RE 0.9973322814886306\n",
      "127 Train Loss 1.368143 Test MSE 14159.284875944084 Test RE 0.9965204685401545\n",
      "128 Train Loss 1.3656809 Test MSE 14126.464870802283 Test RE 0.9953648741193513\n",
      "129 Train Loss 1.3644321 Test MSE 14094.871006736568 Test RE 0.9942511834132717\n",
      "130 Train Loss 1.3516806 Test MSE 13977.550381990988 Test RE 0.9901046426046356\n",
      "131 Train Loss 1.3461826 Test MSE 13909.965941630937 Test RE 0.9877080584260406\n",
      "132 Train Loss 1.3392729 Test MSE 13816.676737275842 Test RE 0.9843903827647843\n",
      "133 Train Loss 1.3338653 Test MSE 13775.250003412124 Test RE 0.9829135192265452\n",
      "134 Train Loss 1.3317316 Test MSE 13770.757301714506 Test RE 0.9827532209615101\n",
      "135 Train Loss 1.3296465 Test MSE 13758.522825473403 Test RE 0.9823165658674593\n",
      "136 Train Loss 1.327433 Test MSE 13720.896527735253 Test RE 0.9809724448265917\n",
      "137 Train Loss 1.3198366 Test MSE 13637.776395787549 Test RE 0.9779966034548425\n",
      "138 Train Loss 1.3089013 Test MSE 13486.882222503325 Test RE 0.972571068380016\n",
      "139 Train Loss 1.3029943 Test MSE 13453.719885188128 Test RE 0.9713746250531309\n",
      "140 Train Loss 1.3008897 Test MSE 13430.32886370033 Test RE 0.9705298278686293\n",
      "141 Train Loss 1.2972994 Test MSE 13372.252566667683 Test RE 0.9684291404880293\n",
      "142 Train Loss 1.2842575 Test MSE 13243.314910856594 Test RE 0.9637489476795437\n",
      "143 Train Loss 1.2805483 Test MSE 13190.579044626109 Test RE 0.961828173564794\n",
      "144 Train Loss 1.2742995 Test MSE 13128.145604231004 Test RE 0.9595492187282424\n",
      "145 Train Loss 1.2621229 Test MSE 12956.005793994149 Test RE 0.9532375253313092\n",
      "146 Train Loss 1.25088 Test MSE 12883.611228824375 Test RE 0.950570581189272\n",
      "147 Train Loss 1.2384907 Test MSE 12728.801972069065 Test RE 0.9448423013639267\n",
      "148 Train Loss 1.2343429 Test MSE 12647.70543722403 Test RE 0.9418276472311938\n",
      "149 Train Loss 1.220763 Test MSE 12464.017271907742 Test RE 0.9349633450541499\n",
      "150 Train Loss 1.2088152 Test MSE 12374.693190225926 Test RE 0.9316070873072673\n",
      "151 Train Loss 1.2025703 Test MSE 12282.661515779502 Test RE 0.928136400662777\n",
      "152 Train Loss 1.1960902 Test MSE 12188.727282110462 Test RE 0.9245805300172992\n",
      "153 Train Loss 1.1859518 Test MSE 12033.599194431064 Test RE 0.9186780394096395\n",
      "154 Train Loss 1.1720471 Test MSE 11937.562791192699 Test RE 0.9150048545935724\n",
      "155 Train Loss 1.162985 Test MSE 11793.202576839065 Test RE 0.9094554777137305\n",
      "156 Train Loss 1.158566 Test MSE 11665.142925480563 Test RE 0.9045042168347952\n",
      "157 Train Loss 1.1533371 Test MSE 11619.14383426588 Test RE 0.9027190921324095\n",
      "158 Train Loss 1.142062 Test MSE 11588.451502999764 Test RE 0.9015260233939173\n",
      "159 Train Loss 1.1242703 Test MSE 11510.332593630666 Test RE 0.8984822466220329\n",
      "160 Train Loss 1.1037935 Test MSE 11379.03549952469 Test RE 0.8933431052496229\n",
      "161 Train Loss 1.0946666 Test MSE 11264.252272039092 Test RE 0.888825995421499\n",
      "162 Train Loss 1.0861745 Test MSE 11163.82802156975 Test RE 0.8848550474009869\n",
      "163 Train Loss 1.0733199 Test MSE 11000.871876062527 Test RE 0.8783732818830625\n",
      "164 Train Loss 1.062548 Test MSE 10855.721800248668 Test RE 0.8725592288634797\n",
      "165 Train Loss 1.0588996 Test MSE 10801.743009939315 Test RE 0.8703871767446558\n",
      "166 Train Loss 1.0563577 Test MSE 10764.053587390308 Test RE 0.8688673731656613\n",
      "167 Train Loss 1.0519819 Test MSE 10686.0545448325 Test RE 0.8657136337022864\n",
      "168 Train Loss 1.0504261 Test MSE 10651.81444166635 Test RE 0.8643255672816534\n",
      "169 Train Loss 1.0477352 Test MSE 10609.203782631688 Test RE 0.8625950457577483\n",
      "170 Train Loss 1.0390074 Test MSE 10536.880192314788 Test RE 0.8596498361936455\n",
      "171 Train Loss 1.02155 Test MSE 10425.542915311906 Test RE 0.8550960571774381\n",
      "172 Train Loss 1.014813 Test MSE 10324.62172262563 Test RE 0.8509472480869958\n",
      "173 Train Loss 1.0016081 Test MSE 10131.132331029226 Test RE 0.8429359141372724\n",
      "174 Train Loss 0.99829143 Test MSE 10070.431214597806 Test RE 0.8404068767893653\n",
      "175 Train Loss 0.99725515 Test MSE 10042.714720515507 Test RE 0.8392495687733762\n",
      "176 Train Loss 0.99289775 Test MSE 9999.30744607003 Test RE 0.8374338751484468\n",
      "177 Train Loss 0.9905247 Test MSE 10031.53299862284 Test RE 0.8387822215899393\n",
      "178 Train Loss 0.98756635 Test MSE 10043.32608373914 Test RE 0.839275113585094\n",
      "179 Train Loss 0.98473513 Test MSE 9980.460142874581 Test RE 0.8366442797380723\n",
      "180 Train Loss 0.9821237 Test MSE 9877.804287920413 Test RE 0.8323304291781402\n",
      "181 Train Loss 0.9743425 Test MSE 9766.111025036187 Test RE 0.8276112629791359\n",
      "182 Train Loss 0.9707875 Test MSE 9711.736642291637 Test RE 0.8253041182109058\n",
      "183 Train Loss 0.96652126 Test MSE 9642.921230037802 Test RE 0.8223749506307441\n",
      "184 Train Loss 0.9502041 Test MSE 9493.45488865475 Test RE 0.8159766088858353\n",
      "185 Train Loss 0.9327196 Test MSE 9416.771312010394 Test RE 0.8126743930360026\n",
      "186 Train Loss 0.91283226 Test MSE 9255.18646093799 Test RE 0.8056717764903182\n",
      "187 Train Loss 0.88896 Test MSE 9027.39029283209 Test RE 0.7956950812541383\n",
      "188 Train Loss 0.8708181 Test MSE 8894.829777922805 Test RE 0.7898313802740601\n",
      "189 Train Loss 0.86401784 Test MSE 8778.976187650644 Test RE 0.7846708140657493\n",
      "190 Train Loss 0.85687035 Test MSE 8602.75736813268 Test RE 0.7767556141338705\n",
      "191 Train Loss 0.8524173 Test MSE 8496.45066546845 Test RE 0.7719414009934819\n",
      "192 Train Loss 0.83424246 Test MSE 8308.055836624584 Test RE 0.7633351603453664\n",
      "193 Train Loss 0.8145248 Test MSE 8055.148459499557 Test RE 0.7516269408237775\n",
      "194 Train Loss 0.807232 Test MSE 7977.656326272568 Test RE 0.748002803058587\n",
      "195 Train Loss 0.80503714 Test MSE 7965.666969615323 Test RE 0.7474405173440271\n",
      "196 Train Loss 0.7989658 Test MSE 7976.194937088717 Test RE 0.7479342883703893\n",
      "197 Train Loss 0.7947965 Test MSE 7952.338565172307 Test RE 0.746814935060553\n",
      "198 Train Loss 0.7929826 Test MSE 7932.125175552943 Test RE 0.7458651977005868\n",
      "199 Train Loss 0.7925062 Test MSE 7916.058755823045 Test RE 0.7451094445282632\n",
      "Training time: 183.67\n",
      "Training time: 183.67\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 200\n",
    "\n",
    "N_f = 1000\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "beta_full = []\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss =[]\n",
    "    beta_val = []\n",
    "    'Generate Training data'\n",
    "    torch.manual_seed(reps*36)\n",
    "     #Total number of collocation points \n",
    "    \n",
    "    \n",
    "    layers = np.array([1,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "    PINN = Sequentialmodel(layers)\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "    \n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.25, \n",
    "                              max_iter = 10, \n",
    "                              max_eval = 15, \n",
    "                              tolerance_grad = 1e-5, \n",
    "                              tolerance_change = 1e-5, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "    \n",
    "    optimizer_lambda = torch.optim.Adam(PINN.parameters(), lr=5e-3)\n",
    "   \n",
    "\n",
    "    start_time = time.time()\n",
    "    train_model(max_iter,reps)\n",
    "\n",
    "    \n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    "    beta_full.append(beta_val)\n",
    " \n",
    "    \n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full, \"test_re_loss\": test_re_full, \"Time\": elapsed_time, \"beta\": beta_full, \"label\": label, \"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f72a0159dd0>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "u_pred = PINN.test()\n",
    "plt.plot(u_pred,'r')\n",
    "plt.plot(y_true,'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + test_re_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
