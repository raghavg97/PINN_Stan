{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:3\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:3' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_1D_1(x): #True function for 1D_1 dy2/dx2 + dy/dx - 6y = 0; BC1: y(0)=2; BC2: dy/dx at (x=0) = -1;\n",
    "    y = np.exp(3.0*x) + np.exp(-4.0*x)\n",
    "    return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "level = \"high\"\n",
    "label = \"1D_SODE_tanhAW_\" + level\n",
    "\n",
    "u_coeff = 12.0\n",
    "fo_val = -1.0\n",
    "\n",
    "loss_thresh = 0.005\n",
    "\n",
    "x = np.linspace(0,2,100).reshape(-1,1)\n",
    "\n",
    "bc1_x = x[0].reshape(-1,1)\n",
    "bc1_y = true_1D_1(x[0]).reshape(-1,1)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "y_bc1_train = torch.from_numpy(bc1_y).float().to(device)    \n",
    "\n",
    "bc2_x = x[0].reshape(-1,1)\n",
    "x_bc2_train = torch.from_numpy(bc2_x).float().to(device)\n",
    "bc2_val = torch.tensor(fo_val,device=device)\n",
    "bc2_val = bc2_val.view(1,1)\n",
    "\n",
    "\n",
    "x_test = x.reshape(-1,1)\n",
    "x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "y_true = true_1D_1(x_test)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(x[0]) \n",
    "ub = np.array(x[-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    #Collocation Points\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,y)\n",
    "    x01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    \n",
    "    x_coll_train = lb + (ub-lb)*sampling(N_f)\n",
    "    x_coll_train = np.vstack((x_coll_train, bc1_x.reshape(-1,1))) # append training points to collocation points \n",
    "\n",
    "    return x_coll_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        \n",
    "        \n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "            \n",
    "        self.m_lambda = nn.Sigmoid()    \n",
    "        self.lambdas_bc1 = Parameter(torch.ones(1,1))\n",
    "        self.lambdas_bc1.requiresGrad = True\n",
    "        \n",
    "        self.lambdas_bc2 = Parameter(torch.ones(1,1))\n",
    "        self.lambdas_bc2.requiresGrad = True\n",
    "        \n",
    "        self.lambdas_f = Parameter(torch.ones(N_f+1,1))\n",
    "        self.lambdas_f.requiresGrad = True\n",
    "             \n",
    "\n",
    "    'forward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = 2.0*(x - l_b)/(u_b - l_b) - 1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "\n",
    "                        \n",
    "    def loss_BC1(self,x,y,lambda_ind):\n",
    "        \n",
    "        m = self.m_lambda(self.lambdas_bc1)\n",
    "        u_pred = self.forward(x)\n",
    "        \n",
    "        if(lambda_ind):\n",
    "            u_pred = u_pred.detach()\n",
    "        else:\n",
    "            m = m.detach()\n",
    "            \n",
    "        loss_bc1 = torch.sum(m*torch.square(u_pred - y))/2.0\n",
    "        \n",
    "        # loss_bc1 = self.loss_function(self.forward(x), y)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,x_bc2,bc2_val,lambda_ind):\n",
    "        g = x_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        y = self.forward(g)    \n",
    "            \n",
    "        y_x = autograd.grad(y,g,torch.ones([x_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        bc2 = dy_dx\n",
    "        \n",
    "        m = self.m_lambda(self.lambdas_bc2)\n",
    "        \n",
    "        if(lambda_ind):\n",
    "            bc2 = bc2.detach()\n",
    "        else:\n",
    "            m = m.detach()\n",
    "        \n",
    "        loss_bc2 = torch.sum(m*torch.square(bc2 - bc2_val))/2.0\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self, x_coll,f_hat,lambda_ind):\n",
    "             \n",
    "        g = x_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        y = self.forward(g) \n",
    "\n",
    "        y_x = autograd.grad(y,g,torch.ones([x_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        y_xx = autograd.grad(y_x,g,torch.ones(x_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        dy2_d2x = y_xx[:,[0]]\n",
    "        \n",
    "        f = dy2_d2x + dy_dx - u_coeff*y\n",
    "\n",
    "        m = self.m_lambda(self.lambdas_f)\n",
    "        \n",
    "        if(lambda_ind):\n",
    "            f = f.detach()\n",
    "        else:\n",
    "            m = m.detach()\n",
    "        \n",
    "        #loss_f  = torch.sum(m*(torch.square(f)))/2.0\n",
    "        loss_f = (N_f+1)*self.loss_function(m*(torch.square(f)),f_hat)/2.0\n",
    "        \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,x_bc1,y_bc1,x_bc2,bc2_val,x_coll,f_hat):\n",
    "\n",
    "        lambda_ind = False\n",
    "        \n",
    "        loss_bc1 = self.loss_BC1(x_bc1,y_bc1,lambda_ind)\n",
    "        loss_bc2 = self.loss_BC2(x_bc2,bc2_val,lambda_ind)\n",
    "        loss_f = self.loss_PDE(x_coll,f_hat,lambda_ind)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "    \n",
    "    def loss_lambdas(self,x_bc1,y_bc1,x_bc2,bc2_val,x_coll,f_hat):\n",
    "\n",
    "        lambda_ind = True        \n",
    "        loss_bc1 = self.loss_BC1(x_bc1,y_bc1,lambda_ind)\n",
    "        loss_bc2 = self.loss_BC2(x_bc2,bc2_val,lambda_ind)\n",
    "        loss_f = self.loss_PDE(x_coll,f_hat,lambda_ind)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return -1.0*loss_val\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self):\n",
    "        y_pred = self.forward(x_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_coll,f_hat):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    optimizer.step(closure)\n",
    "    \n",
    "    for i in range(20):\n",
    "        optimizer_lambda.zero_grad()\n",
    "        loss = PINN.loss_lambdas(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat)\n",
    "        loss.backward()\n",
    "        optimizer_lambda.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        x_coll = torch.from_numpy(colloc_pts(N_f,i*11)).float().to(device)\n",
    "        f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "        train_step(x_coll,f_hat)\n",
    "        \n",
    "        loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (m_lambda): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 90.755714 Test MSE 14266.794916561574 Test RE 1.0002965545944957\n",
      "1 Train Loss 5.2070913 Test MSE 14262.175170297109 Test RE 1.000134587928149\n",
      "2 Train Loss 2.060594 Test MSE 14260.172424102791 Test RE 1.0000643642056635\n",
      "3 Train Loss 1.9802067 Test MSE 14259.28541854329 Test RE 1.0000332609195624\n",
      "4 Train Loss 2.0123975 Test MSE 14258.947430739268 Test RE 1.0000214089564798\n",
      "5 Train Loss 2.0426307 Test MSE 14258.589567763225 Test RE 1.0000088598926484\n",
      "6 Train Loss 2.066949 Test MSE 14258.60064276727 Test RE 1.000009248258551\n",
      "7 Train Loss 2.0934448 Test MSE 14258.641965110945 Test RE 1.0000106973031289\n",
      "8 Train Loss 2.1177852 Test MSE 14258.65962925493 Test RE 1.00001131672844\n",
      "9 Train Loss 2.1396706 Test MSE 14258.70916635583 Test RE 1.000013053834864\n",
      "10 Train Loss 2.1561651 Test MSE 14259.259764246084 Test RE 1.000032361324622\n",
      "11 Train Loss 2.1692357 Test MSE 14259.384341205317 Test RE 1.0000367297392756\n",
      "12 Train Loss 2.171325 Test MSE 14259.41332916842 Test RE 1.0000377462282206\n",
      "13 Train Loss 2.1694386 Test MSE 14259.889806135754 Test RE 1.0000544541726886\n",
      "14 Train Loss 2.1758232 Test MSE 14260.193085535902 Test RE 1.0000650886974463\n",
      "15 Train Loss 2.1836998 Test MSE 14260.122717064838 Test RE 1.0000626212295218\n",
      "16 Train Loss 2.1946018 Test MSE 14259.997556309567 Test RE 1.0000582324570644\n",
      "17 Train Loss 2.2039955 Test MSE 14260.016856574486 Test RE 1.0000589092237402\n",
      "18 Train Loss 2.2121234 Test MSE 14260.2116799021 Test RE 1.000065740707216\n",
      "19 Train Loss 2.2125757 Test MSE 14260.263602861167 Test RE 1.0000675613788306\n",
      "20 Train Loss 2.1819959 Test MSE 14260.101335464256 Test RE 1.000061871483255\n",
      "21 Train Loss 2.1530402 Test MSE 14258.964681482033 Test RE 1.0000220138786506\n",
      "22 Train Loss 2.1396132 Test MSE 14257.86740576534 Test RE 0.9999835355947237\n",
      "23 Train Loss 2.1301963 Test MSE 14256.83409640293 Test RE 0.9999472990720277\n",
      "24 Train Loss 2.1257472 Test MSE 14256.293982548337 Test RE 0.9999283576120314\n",
      "25 Train Loss 2.1201968 Test MSE 14256.060938345712 Test RE 0.9999201847846692\n",
      "26 Train Loss 2.0499597 Test MSE 14256.468766893195 Test RE 0.9999344872305505\n",
      "27 Train Loss 1.7882348 Test MSE 14257.669050642171 Test RE 0.9999765796965661\n",
      "28 Train Loss 1.406914 Test MSE 14256.269563654145 Test RE 0.9999275012479846\n",
      "29 Train Loss 1.3818438 Test MSE 14256.268610006931 Test RE 0.9999274678038187\n",
      "30 Train Loss 1.3779936 Test MSE 14256.49929238104 Test RE 0.9999355577435993\n",
      "31 Train Loss 1.3721453 Test MSE 14256.754023097312 Test RE 0.9999444909732038\n",
      "32 Train Loss 1.3731072 Test MSE 14256.861734242111 Test RE 0.9999482683043642\n",
      "33 Train Loss 1.3747432 Test MSE 14256.872445613193 Test RE 0.9999486439415537\n",
      "34 Train Loss 1.3763831 Test MSE 14256.886554996116 Test RE 0.9999491387434426\n",
      "35 Train Loss 1.3779719 Test MSE 14256.900117935547 Test RE 0.9999496143818764\n",
      "36 Train Loss 1.3794532 Test MSE 14256.900117935547 Test RE 0.9999496143818764\n",
      "37 Train Loss 1.3809011 Test MSE 14256.900117935547 Test RE 0.9999496143818764\n",
      "38 Train Loss 1.3823119 Test MSE 14256.923948993872 Test RE 0.9999504501122042\n",
      "39 Train Loss 1.383669 Test MSE 14256.923948993872 Test RE 0.9999504501122042\n",
      "40 Train Loss 1.3849697 Test MSE 14256.923948993872 Test RE 0.9999504501122042\n",
      "41 Train Loss 1.3862326 Test MSE 14256.928338693655 Test RE 0.9999506040543151\n",
      "42 Train Loss 1.3874593 Test MSE 14256.928338693655 Test RE 0.9999506040543151\n",
      "43 Train Loss 1.3886176 Test MSE 14256.937253002026 Test RE 0.9999509166695729\n",
      "44 Train Loss 1.3897759 Test MSE 14256.937253002026 Test RE 0.9999509166695729\n",
      "45 Train Loss 1.3908863 Test MSE 14256.942098439162 Test RE 0.9999510865938284\n",
      "46 Train Loss 1.3919677 Test MSE 14256.949683816092 Test RE 0.9999513526047598\n",
      "47 Train Loss 1.3929696 Test MSE 14256.949683816092 Test RE 0.9999513526047598\n",
      "48 Train Loss 1.3939648 Test MSE 14256.949683816092 Test RE 0.9999513526047598\n",
      "49 Train Loss 1.3949304 Test MSE 14256.96648630346 Test RE 0.9999519418494925\n",
      "50 Train Loss 1.3958597 Test MSE 14256.96648630346 Test RE 0.9999519418494925\n",
      "51 Train Loss 1.3967471 Test MSE 14256.767682229885 Test RE 0.9999449699872583\n",
      "52 Train Loss 1.3976011 Test MSE 14256.767682229885 Test RE 0.9999449699872583\n",
      "53 Train Loss 1.3984598 Test MSE 14256.816226599578 Test RE 0.9999466723947769\n",
      "54 Train Loss 1.3992748 Test MSE 14256.816226599578 Test RE 0.9999466723947769\n",
      "55 Train Loss 1.4000876 Test MSE 14256.816226599578 Test RE 0.9999466723947769\n",
      "56 Train Loss 1.4008293 Test MSE 14256.816226599578 Test RE 0.9999466723947769\n",
      "57 Train Loss 1.4015764 Test MSE 14256.816179253912 Test RE 0.9999466707344083\n",
      "58 Train Loss 1.4022944 Test MSE 14256.816179253912 Test RE 0.9999466707344083\n",
      "59 Train Loss 1.4030145 Test MSE 14256.816179253912 Test RE 0.9999466707344083\n",
      "60 Train Loss 1.4036617 Test MSE 14256.816179253912 Test RE 0.9999466707344083\n",
      "61 Train Loss 1.4043467 Test MSE 14256.816179253912 Test RE 0.9999466707344083\n",
      "62 Train Loss 1.405031 Test MSE 14256.816179253912 Test RE 0.9999466707344083\n",
      "63 Train Loss 1.4056301 Test MSE 14256.816179253912 Test RE 0.9999466707344083\n",
      "64 Train Loss 1.4062607 Test MSE 14256.816179253912 Test RE 0.9999466707344083\n",
      "65 Train Loss 1.4068418 Test MSE 14256.807508638534 Test RE 0.9999463666639132\n",
      "66 Train Loss 1.4073788 Test MSE 14256.807508638534 Test RE 0.9999463666639132\n",
      "67 Train Loss 1.4079636 Test MSE 14256.807508638534 Test RE 0.9999463666639132\n",
      "68 Train Loss 1.4085097 Test MSE 14256.800374011476 Test RE 0.9999461164590447\n",
      "69 Train Loss 1.4090334 Test MSE 14256.800374011476 Test RE 0.9999461164590447\n",
      "70 Train Loss 1.409524 Test MSE 14256.800374011476 Test RE 0.9999461164590447\n",
      "71 Train Loss 1.4100358 Test MSE 14256.800374011476 Test RE 0.9999461164590447\n",
      "72 Train Loss 1.4104966 Test MSE 14256.800374011476 Test RE 0.9999461164590447\n",
      "73 Train Loss 1.4109799 Test MSE 14256.800374011476 Test RE 0.9999461164590447\n",
      "74 Train Loss 1.4114404 Test MSE 14256.800374011476 Test RE 0.9999461164590447\n",
      "75 Train Loss 1.4118923 Test MSE 14256.823123034474 Test RE 0.9999469142463295\n",
      "76 Train Loss 1.4123156 Test MSE 14256.823123034474 Test RE 0.9999469142463295\n",
      "77 Train Loss 1.4127496 Test MSE 14256.823123034474 Test RE 0.9999469142463295\n",
      "78 Train Loss 1.4132001 Test MSE 14256.817543207262 Test RE 0.9999467185669872\n",
      "79 Train Loss 1.4136078 Test MSE 14256.817543207262 Test RE 0.9999467185669872\n",
      "80 Train Loss 1.4139538 Test MSE 14256.817543207262 Test RE 0.9999467185669872\n",
      "81 Train Loss 1.4143593 Test MSE 14256.817543207262 Test RE 0.9999467185669872\n",
      "82 Train Loss 1.4147155 Test MSE 14256.817543207262 Test RE 0.9999467185669872\n",
      "83 Train Loss 1.4150949 Test MSE 14256.8067086732 Test RE 0.9999463386098615\n",
      "84 Train Loss 1.4154367 Test MSE 14256.8067086732 Test RE 0.9999463386098615\n",
      "85 Train Loss 1.4158293 Test MSE 14256.8067086732 Test RE 0.9999463386098615\n",
      "86 Train Loss 1.4161321 Test MSE 14256.806707925187 Test RE 0.9999463385836291\n",
      "87 Train Loss 1.416431 Test MSE 14256.806707925187 Test RE 0.9999463385836291\n",
      "88 Train Loss 1.4167587 Test MSE 14256.806636475312 Test RE 0.9999463360779473\n",
      "89 Train Loss 1.4171054 Test MSE 14256.806636475312 Test RE 0.9999463360779473\n",
      "90 Train Loss 1.4174387 Test MSE 14256.806636475312 Test RE 0.9999463360779473\n",
      "91 Train Loss 1.4177262 Test MSE 14256.806636475312 Test RE 0.9999463360779473\n",
      "92 Train Loss 1.4179839 Test MSE 14256.806636475312 Test RE 0.9999463360779473\n",
      "93 Train Loss 1.4182749 Test MSE 14256.789773619736 Test RE 0.9999457447128519\n",
      "94 Train Loss 1.4185352 Test MSE 14256.789773619736 Test RE 0.9999457447128519\n",
      "95 Train Loss 1.4188598 Test MSE 14256.789773619736 Test RE 0.9999457447128519\n",
      "96 Train Loss 1.4191413 Test MSE 14256.789773619736 Test RE 0.9999457447128519\n",
      "97 Train Loss 1.4193797 Test MSE 14256.789773619736 Test RE 0.9999457447128519\n",
      "98 Train Loss 1.4196309 Test MSE 14256.776414784577 Test RE 0.9999452762303355\n",
      "99 Train Loss 1.4199187 Test MSE 14256.776414784577 Test RE 0.9999452762303355\n",
      "100 Train Loss 1.4201069 Test MSE 14256.776414784577 Test RE 0.9999452762303355\n",
      "101 Train Loss 1.4203688 Test MSE 14256.776414784577 Test RE 0.9999452762303355\n",
      "102 Train Loss 1.4206321 Test MSE 14256.776414784577 Test RE 0.9999452762303355\n",
      "103 Train Loss 1.4208287 Test MSE 14256.776414784577 Test RE 0.9999452762303355\n",
      "104 Train Loss 1.4210317 Test MSE 14256.797744129624 Test RE 0.9999460242314694\n",
      "105 Train Loss 1.4212644 Test MSE 14256.797744129624 Test RE 0.9999460242314694\n",
      "106 Train Loss 1.4214921 Test MSE 14256.81044205274 Test RE 0.9999464695360579\n",
      "107 Train Loss 1.4216522 Test MSE 14256.822955707728 Test RE 0.9999469083783372\n",
      "108 Train Loss 1.4219061 Test MSE 14256.822955707728 Test RE 0.9999469083783372\n",
      "109 Train Loss 1.4220686 Test MSE 14256.822063577443 Test RE 0.9999468770921616\n",
      "110 Train Loss 1.4222753 Test MSE 14256.822063577443 Test RE 0.9999468770921616\n",
      "111 Train Loss 1.4225023 Test MSE 14256.822063577443 Test RE 0.9999468770921616\n",
      "112 Train Loss 1.422658 Test MSE 14256.822063577443 Test RE 0.9999468770921616\n",
      "113 Train Loss 1.4228587 Test MSE 14256.824935910852 Test RE 0.9999469778222073\n",
      "114 Train Loss 1.4230419 Test MSE 14256.824935910852 Test RE 0.9999469778222073\n",
      "115 Train Loss 1.4232103 Test MSE 14256.842939363136 Test RE 0.9999476091862067\n",
      "116 Train Loss 1.4233747 Test MSE 14256.842939363136 Test RE 0.9999476091862067\n",
      "117 Train Loss 1.4235317 Test MSE 14256.873457965923 Test RE 0.9999486794437557\n",
      "118 Train Loss 1.4236959 Test MSE 14256.873120988479 Test RE 0.9999486676262928\n",
      "119 Train Loss 1.4238627 Test MSE 14256.873120988479 Test RE 0.9999486676262928\n",
      "120 Train Loss 1.4240551 Test MSE 14256.873120988479 Test RE 0.9999486676262928\n",
      "121 Train Loss 1.4242084 Test MSE 14256.878181662525 Test RE 0.9999488450990727\n",
      "122 Train Loss 1.4243371 Test MSE 14256.878181662525 Test RE 0.9999488450990727\n",
      "123 Train Loss 1.424528 Test MSE 14256.878181662525 Test RE 0.9999488450990727\n",
      "124 Train Loss 1.4246724 Test MSE 14256.878181662525 Test RE 0.9999488450990727\n",
      "125 Train Loss 1.4248167 Test MSE 14256.878181662525 Test RE 0.9999488450990727\n",
      "126 Train Loss 1.4249349 Test MSE 14256.891838573136 Test RE 0.9999493240331193\n",
      "127 Train Loss 1.4251125 Test MSE 14256.891838573136 Test RE 0.9999493240331193\n",
      "128 Train Loss 1.425238 Test MSE 14256.891838573136 Test RE 0.9999493240331193\n",
      "129 Train Loss 1.4253615 Test MSE 14256.897852244472 Test RE 0.9999495349264201\n",
      "130 Train Loss 1.4254875 Test MSE 14256.90398773758 Test RE 0.9999497500918397\n",
      "131 Train Loss 1.4256388 Test MSE 14256.905607587461 Test RE 0.9999498068982978\n",
      "132 Train Loss 1.4257529 Test MSE 14256.905607587461 Test RE 0.9999498068982978\n",
      "133 Train Loss 1.4258476 Test MSE 14256.905607587461 Test RE 0.9999498068982978\n",
      "134 Train Loss 1.4260018 Test MSE 14256.906536328997 Test RE 0.9999498394683005\n",
      "135 Train Loss 1.4261341 Test MSE 14256.906536328997 Test RE 0.9999498394683005\n",
      "136 Train Loss 1.4262419 Test MSE 14256.906536328997 Test RE 0.9999498394683005\n",
      "137 Train Loss 1.4263449 Test MSE 14256.906536328997 Test RE 0.9999498394683005\n",
      "138 Train Loss 1.4264759 Test MSE 14256.906536328997 Test RE 0.9999498394683005\n",
      "139 Train Loss 1.4265825 Test MSE 14256.905641010138 Test RE 0.9999498080703966\n",
      "140 Train Loss 1.4267191 Test MSE 14256.905641010138 Test RE 0.9999498080703966\n",
      "141 Train Loss 1.4268358 Test MSE 14256.904596710721 Test RE 0.9999497714478977\n",
      "142 Train Loss 1.4269303 Test MSE 14256.904596710721 Test RE 0.9999497714478977\n",
      "143 Train Loss 1.4270579 Test MSE 14256.904596710721 Test RE 0.9999497714478977\n",
      "144 Train Loss 1.4271084 Test MSE 14256.907471304135 Test RE 0.9999498722569079\n",
      "145 Train Loss 1.4272572 Test MSE 14256.907471304135 Test RE 0.9999498722569079\n",
      "146 Train Loss 1.4273554 Test MSE 14256.953276204351 Test RE 0.9999514785858898\n",
      "147 Train Loss 1.4274565 Test MSE 14256.953276204351 Test RE 0.9999514785858898\n",
      "148 Train Loss 1.4275424 Test MSE 14256.999099572928 Test RE 0.9999530855599555\n",
      "149 Train Loss 1.4276178 Test MSE 14256.999099572928 Test RE 0.9999530855599555\n",
      "150 Train Loss 1.4277594 Test MSE 14256.981612818423 Test RE 0.9999524723194217\n",
      "151 Train Loss 1.4278512 Test MSE 14256.981612818423 Test RE 0.9999524723194217\n",
      "152 Train Loss 1.4279407 Test MSE 14256.981612818423 Test RE 0.9999524723194217\n",
      "153 Train Loss 1.4279714 Test MSE 14256.97308528568 Test RE 0.9999521732684389\n",
      "154 Train Loss 1.4280878 Test MSE 14256.97308528568 Test RE 0.9999521732684389\n",
      "155 Train Loss 1.4282159 Test MSE 14256.95141533115 Test RE 0.9999514133271025\n",
      "156 Train Loss 1.4282529 Test MSE 14256.95141533115 Test RE 0.9999514133271025\n",
      "157 Train Loss 1.4283173 Test MSE 14256.95567897191 Test RE 0.9999515628483135\n",
      "158 Train Loss 1.4283997 Test MSE 14256.95567897191 Test RE 0.9999515628483135\n",
      "159 Train Loss 1.4285212 Test MSE 14256.95567897191 Test RE 0.9999515628483135\n",
      "160 Train Loss 1.4285969 Test MSE 14256.95567897191 Test RE 0.9999515628483135\n",
      "161 Train Loss 1.428721 Test MSE 14256.979564772351 Test RE 0.9999524004967574\n",
      "162 Train Loss 1.4288102 Test MSE 14256.979564772351 Test RE 0.9999524004967574\n",
      "163 Train Loss 1.428838 Test MSE 14256.953849002817 Test RE 0.9999514986733047\n",
      "164 Train Loss 1.428943 Test MSE 14256.953849002817 Test RE 0.9999514986733047\n",
      "165 Train Loss 1.4289904 Test MSE 14256.953849002817 Test RE 0.9999514986733047\n",
      "166 Train Loss 1.4290403 Test MSE 14256.953849002817 Test RE 0.9999514986733047\n",
      "167 Train Loss 1.4291284 Test MSE 14256.953849002817 Test RE 0.9999514986733047\n",
      "168 Train Loss 1.4291956 Test MSE 14256.968583468664 Test RE 0.9999520153947431\n",
      "169 Train Loss 1.4292865 Test MSE 14256.968583468664 Test RE 0.9999520153947431\n",
      "170 Train Loss 1.4293678 Test MSE 14256.968583468664 Test RE 0.9999520153947431\n",
      "171 Train Loss 1.4294255 Test MSE 14256.968583468664 Test RE 0.9999520153947431\n",
      "172 Train Loss 1.4294748 Test MSE 14256.970008295812 Test RE 0.9999520653618429\n",
      "173 Train Loss 1.4295446 Test MSE 14256.970008295812 Test RE 0.9999520653618429\n",
      "174 Train Loss 1.4295952 Test MSE 14256.981289858388 Test RE 0.9999524609935778\n",
      "175 Train Loss 1.4296534 Test MSE 14256.981289858388 Test RE 0.9999524609935778\n",
      "176 Train Loss 1.4297442 Test MSE 14256.981289858388 Test RE 0.9999524609935778\n",
      "177 Train Loss 1.429843 Test MSE 14256.981289858388 Test RE 0.9999524609935778\n",
      "178 Train Loss 1.4298679 Test MSE 14256.981289858388 Test RE 0.9999524609935778\n",
      "179 Train Loss 1.4299451 Test MSE 14256.981289858388 Test RE 0.9999524609935778\n",
      "180 Train Loss 1.4299972 Test MSE 14256.981289858388 Test RE 0.9999524609935778\n",
      "181 Train Loss 1.4300598 Test MSE 14256.981289858388 Test RE 0.9999524609935778\n",
      "182 Train Loss 1.4301007 Test MSE 14256.981289858388 Test RE 0.9999524609935778\n",
      "183 Train Loss 1.4301913 Test MSE 14256.970073635517 Test RE 0.9999520676532336\n",
      "184 Train Loss 1.4302391 Test MSE 14256.970073635517 Test RE 0.9999520676532336\n",
      "185 Train Loss 1.4302603 Test MSE 14257.002953008734 Test RE 0.9999532206955035\n",
      "186 Train Loss 1.4303364 Test MSE 14257.002953008734 Test RE 0.9999532206955035\n",
      "187 Train Loss 1.4303874 Test MSE 14257.047153944843 Test RE 0.9999547707700287\n",
      "188 Train Loss 1.4303943 Test MSE 14257.047153944843 Test RE 0.9999547707700287\n",
      "189 Train Loss 1.4304506 Test MSE 14257.047153944843 Test RE 0.9999547707700287\n",
      "190 Train Loss 1.4305648 Test MSE 14257.047153944843 Test RE 0.9999547707700287\n",
      "191 Train Loss 1.4306232 Test MSE 14257.047153944843 Test RE 0.9999547707700287\n",
      "192 Train Loss 1.4306664 Test MSE 14257.047153944843 Test RE 0.9999547707700287\n",
      "193 Train Loss 1.4306753 Test MSE 14257.047457352288 Test RE 0.9999547814101601\n",
      "194 Train Loss 1.4307262 Test MSE 14257.047457352288 Test RE 0.9999547814101601\n",
      "195 Train Loss 1.4307564 Test MSE 14257.047457352288 Test RE 0.9999547814101601\n",
      "196 Train Loss 1.4308436 Test MSE 14257.047457352288 Test RE 0.9999547814101601\n",
      "197 Train Loss 1.4308913 Test MSE 14257.047457352288 Test RE 0.9999547814101601\n",
      "198 Train Loss 1.4309794 Test MSE 14257.047457352288 Test RE 0.9999547814101601\n",
      "199 Train Loss 1.4310184 Test MSE 14257.047457352288 Test RE 0.9999547814101601\n",
      "Training time: 84.48\n",
      "Training time: 84.48\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (m_lambda): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 73.053406 Test MSE 14265.983829441637 Test RE 1.0002681200656514\n",
      "1 Train Loss 4.5736637 Test MSE 14261.617044835162 Test RE 1.000115018471408\n",
      "2 Train Loss 2.0547266 Test MSE 14259.737874644761 Test RE 1.0000491266361131\n",
      "3 Train Loss 2.0025117 Test MSE 14258.957848788541 Test RE 1.0000217742804445\n",
      "4 Train Loss 2.0355887 Test MSE 14258.662726682061 Test RE 1.000011425345312\n",
      "5 Train Loss 2.0432677 Test MSE 14258.28679989694 Test RE 0.9999982427080167\n",
      "6 Train Loss 2.0678625 Test MSE 14258.053397558271 Test RE 0.9999900578928645\n",
      "7 Train Loss 2.0878496 Test MSE 14258.315726536102 Test RE 0.9999992570856228\n",
      "8 Train Loss 2.103795 Test MSE 14259.37527171893 Test RE 1.0000364117093827\n",
      "9 Train Loss 2.1241455 Test MSE 14259.323452753537 Test RE 1.0000345946276976\n",
      "10 Train Loss 2.1343896 Test MSE 14259.373927628827 Test RE 1.000036364577617\n",
      "11 Train Loss 2.1401818 Test MSE 14259.333432502714 Test RE 1.0000349445774241\n",
      "12 Train Loss 2.1507595 Test MSE 14259.984254603334 Test RE 1.000057766030479\n",
      "13 Train Loss 2.1657255 Test MSE 14260.077315484778 Test RE 1.0000610292215073\n",
      "14 Train Loss 2.1774201 Test MSE 14259.941948626163 Test RE 1.0000562825627684\n",
      "15 Train Loss 2.1841133 Test MSE 14259.777437229746 Test RE 1.0000505139162115\n",
      "16 Train Loss 2.1929126 Test MSE 14260.161266382083 Test RE 1.0000639729607081\n",
      "17 Train Loss 2.1926308 Test MSE 14260.446636411287 Test RE 1.0000739793993478\n",
      "18 Train Loss 2.1882777 Test MSE 14260.059867071384 Test RE 1.000060417391599\n",
      "19 Train Loss 2.162378 Test MSE 14258.693997886747 Test RE 1.000012521925876\n",
      "20 Train Loss 2.1550803 Test MSE 14257.767815090197 Test RE 0.9999800431646668\n",
      "21 Train Loss 2.0751815 Test MSE 14259.14529498902 Test RE 1.0000283473297487\n",
      "22 Train Loss 2.0215125 Test MSE 14258.555446624401 Test RE 1.00000766336962\n",
      "23 Train Loss 1.9229745 Test MSE 14259.585063810026 Test RE 1.0000437682363805\n",
      "24 Train Loss 1.785534 Test MSE 14260.143356240029 Test RE 1.0000633449420937\n",
      "25 Train Loss 1.6663246 Test MSE 14260.479863093298 Test RE 1.0000751444792921\n",
      "26 Train Loss 1.625256 Test MSE 14260.75369589983 Test RE 1.000084746262386\n",
      "27 Train Loss 1.476284 Test MSE 14260.679748274526 Test RE 1.0000821533423503\n",
      "28 Train Loss 1.443651 Test MSE 14260.023134059093 Test RE 1.000059129344578\n",
      "29 Train Loss 1.4001625 Test MSE 14260.658259079557 Test RE 1.0000813998379998\n",
      "30 Train Loss 1.3887179 Test MSE 14261.27917733425 Test RE 1.0001031716954716\n",
      "31 Train Loss 1.3871526 Test MSE 14261.126013015919 Test RE 1.0000978011908364\n",
      "32 Train Loss 1.3786012 Test MSE 14259.849535852727 Test RE 1.0000530420823817\n",
      "33 Train Loss 1.3746051 Test MSE 14258.963311612717 Test RE 1.000021965842221\n",
      "34 Train Loss 1.3757086 Test MSE 14259.003903790275 Test RE 1.0000233892640864\n",
      "35 Train Loss 1.3765836 Test MSE 14259.377411597705 Test RE 1.0000364867462117\n",
      "36 Train Loss 1.3780205 Test MSE 14259.2831750847 Test RE 1.0000331822503572\n",
      "37 Train Loss 1.3795377 Test MSE 14259.268774791904 Test RE 1.0000326772889898\n",
      "38 Train Loss 1.3809913 Test MSE 14259.243578761323 Test RE 1.0000317937631424\n",
      "39 Train Loss 1.3824141 Test MSE 14259.243578761323 Test RE 1.0000317937631424\n",
      "40 Train Loss 1.383766 Test MSE 14259.243578761323 Test RE 1.0000317937631424\n",
      "41 Train Loss 1.3850965 Test MSE 14259.243578761323 Test RE 1.0000317937631424\n",
      "42 Train Loss 1.3863815 Test MSE 14259.240076204873 Test RE 1.0000316709421806\n",
      "43 Train Loss 1.3875844 Test MSE 14259.240076204873 Test RE 1.0000316709421806\n",
      "44 Train Loss 1.3887886 Test MSE 14259.215885533746 Test RE 1.0000308226695027\n",
      "45 Train Loss 1.3899311 Test MSE 14259.215885533746 Test RE 1.0000308226695027\n",
      "46 Train Loss 1.3910314 Test MSE 14259.215885533746 Test RE 1.0000308226695027\n",
      "47 Train Loss 1.3921086 Test MSE 14259.229760898734 Test RE 1.0000313092246331\n",
      "48 Train Loss 1.3931412 Test MSE 14259.229760898734 Test RE 1.0000313092246331\n",
      "49 Train Loss 1.3941331 Test MSE 14259.237920973623 Test RE 1.0000315953666388\n",
      "50 Train Loss 1.3951072 Test MSE 14259.237920973623 Test RE 1.0000315953666388\n",
      "51 Train Loss 1.3960519 Test MSE 14259.233442676601 Test RE 1.0000314383302358\n",
      "52 Train Loss 1.3969421 Test MSE 14259.233442676601 Test RE 1.0000314383302358\n",
      "53 Train Loss 1.3978307 Test MSE 14259.233442676601 Test RE 1.0000314383302358\n",
      "54 Train Loss 1.3986781 Test MSE 14259.233442676601 Test RE 1.0000314383302358\n",
      "55 Train Loss 1.3995074 Test MSE 14259.214888819393 Test RE 1.0000307877185943\n",
      "56 Train Loss 1.4002749 Test MSE 14259.214888819393 Test RE 1.0000307877185943\n",
      "57 Train Loss 1.4010772 Test MSE 14259.214888819393 Test RE 1.0000307877185943\n",
      "58 Train Loss 1.4018097 Test MSE 14259.214888819393 Test RE 1.0000307877185943\n",
      "59 Train Loss 1.4025508 Test MSE 14259.21701654979 Test RE 1.000030862329849\n",
      "60 Train Loss 1.4032321 Test MSE 14259.21701654979 Test RE 1.000030862329849\n",
      "61 Train Loss 1.4039334 Test MSE 14259.21701654979 Test RE 1.000030862329849\n",
      "62 Train Loss 1.4046253 Test MSE 14259.21701654979 Test RE 1.000030862329849\n",
      "63 Train Loss 1.405252 Test MSE 14259.21701654979 Test RE 1.000030862329849\n",
      "64 Train Loss 1.4059002 Test MSE 14259.207517042201 Test RE 1.0000305292189098\n",
      "65 Train Loss 1.4065033 Test MSE 14259.203145132897 Test RE 1.0000303759129456\n",
      "66 Train Loss 1.407056 Test MSE 14259.203807937281 Test RE 1.0000303991549357\n",
      "67 Train Loss 1.4076494 Test MSE 14259.203807937281 Test RE 1.0000303991549357\n",
      "68 Train Loss 1.4082153 Test MSE 14259.203807937281 Test RE 1.0000303991549357\n",
      "69 Train Loss 1.4087659 Test MSE 14259.216531264188 Test RE 1.0000308453127653\n",
      "70 Train Loss 1.4092634 Test MSE 14259.216531264188 Test RE 1.0000308453127653\n",
      "71 Train Loss 1.4097784 Test MSE 14259.224887309163 Test RE 1.0000311383268126\n",
      "72 Train Loss 1.4102777 Test MSE 14259.224887309163 Test RE 1.0000311383268126\n",
      "73 Train Loss 1.4107555 Test MSE 14259.224887309163 Test RE 1.0000311383268126\n",
      "74 Train Loss 1.4112341 Test MSE 14259.224887309163 Test RE 1.0000311383268126\n",
      "75 Train Loss 1.4116999 Test MSE 14259.23506347314 Test RE 1.0000314951652491\n",
      "76 Train Loss 1.4121475 Test MSE 14259.23506347314 Test RE 1.0000314951652491\n",
      "77 Train Loss 1.4126012 Test MSE 14259.23506347314 Test RE 1.0000314951652491\n",
      "78 Train Loss 1.4130296 Test MSE 14259.212598640404 Test RE 1.0000307074108912\n",
      "79 Train Loss 1.4134504 Test MSE 14259.212598640404 Test RE 1.0000307074108912\n",
      "80 Train Loss 1.4138204 Test MSE 14259.144634036491 Test RE 1.0000283241526486\n",
      "81 Train Loss 1.4142222 Test MSE 14259.144634036491 Test RE 1.0000283241526486\n",
      "82 Train Loss 1.4146045 Test MSE 14259.144634036491 Test RE 1.0000283241526486\n",
      "83 Train Loss 1.4149889 Test MSE 14259.07993130497 Test RE 1.0000260552704952\n",
      "84 Train Loss 1.4153274 Test MSE 14259.07993130497 Test RE 1.0000260552704952\n",
      "85 Train Loss 1.4157212 Test MSE 14259.125381030077 Test RE 1.0000276490224522\n",
      "86 Train Loss 1.416052 Test MSE 14259.125381030077 Test RE 1.0000276490224522\n",
      "87 Train Loss 1.416362 Test MSE 14259.125381030077 Test RE 1.0000276490224522\n",
      "88 Train Loss 1.4166887 Test MSE 14259.125381030077 Test RE 1.0000276490224522\n",
      "89 Train Loss 1.4170489 Test MSE 14259.091867203706 Test RE 1.0000264738180904\n",
      "90 Train Loss 1.417383 Test MSE 14259.091867203706 Test RE 1.0000264738180904\n",
      "91 Train Loss 1.4176639 Test MSE 14259.046373442605 Test RE 1.0000248785200794\n",
      "92 Train Loss 1.417949 Test MSE 14259.046373442605 Test RE 1.0000248785200794\n",
      "93 Train Loss 1.4182374 Test MSE 14259.046373442605 Test RE 1.0000248785200794\n",
      "94 Train Loss 1.418513 Test MSE 14258.991596005806 Test RE 1.0000229576745072\n",
      "95 Train Loss 1.4188346 Test MSE 14258.991596005806 Test RE 1.0000229576745072\n",
      "96 Train Loss 1.4191409 Test MSE 14258.991596005806 Test RE 1.0000229576745072\n",
      "97 Train Loss 1.4193789 Test MSE 14259.050094042184 Test RE 1.0000250089878475\n",
      "98 Train Loss 1.4196396 Test MSE 14259.050094042184 Test RE 1.0000250089878475\n",
      "99 Train Loss 1.4199113 Test MSE 14259.002188716584 Test RE 1.0000233291226535\n",
      "100 Train Loss 1.4201156 Test MSE 14259.002188716584 Test RE 1.0000233291226535\n",
      "101 Train Loss 1.4203573 Test MSE 14259.002188716584 Test RE 1.0000233291226535\n",
      "102 Train Loss 1.420634 Test MSE 14258.954107170819 Test RE 1.000021643075223\n",
      "103 Train Loss 1.4208533 Test MSE 14258.954107170819 Test RE 1.000021643075223\n",
      "104 Train Loss 1.4210634 Test MSE 14258.954107170819 Test RE 1.000021643075223\n",
      "105 Train Loss 1.4212962 Test MSE 14258.982582944393 Test RE 1.0000226416188553\n",
      "106 Train Loss 1.4215478 Test MSE 14258.982582944393 Test RE 1.0000226416188553\n",
      "107 Train Loss 1.4217173 Test MSE 14258.982582944393 Test RE 1.0000226416188553\n",
      "108 Train Loss 1.4219675 Test MSE 14258.982582944393 Test RE 1.0000226416188553\n",
      "109 Train Loss 1.4221461 Test MSE 14258.982582944393 Test RE 1.0000226416188553\n",
      "110 Train Loss 1.4223323 Test MSE 14259.005331994318 Test RE 1.0000234393460354\n",
      "111 Train Loss 1.4225563 Test MSE 14259.005331994318 Test RE 1.0000234393460354\n",
      "112 Train Loss 1.4227391 Test MSE 14258.986659415676 Test RE 1.000022784566056\n",
      "113 Train Loss 1.4229432 Test MSE 14258.944892929354 Test RE 1.0000213199644845\n",
      "114 Train Loss 1.423122 Test MSE 14258.944892929354 Test RE 1.0000213199644845\n",
      "115 Train Loss 1.4233063 Test MSE 14258.830780841503 Test RE 1.0000173184500005\n",
      "116 Train Loss 1.4234519 Test MSE 14258.830780841503 Test RE 1.0000173184500005\n",
      "117 Train Loss 1.4236419 Test MSE 14258.830780841503 Test RE 1.0000173184500005\n",
      "118 Train Loss 1.4238029 Test MSE 14258.865253542175 Test RE 1.0000185272895397\n",
      "119 Train Loss 1.4239633 Test MSE 14258.865253542175 Test RE 1.0000185272895397\n",
      "120 Train Loss 1.4241582 Test MSE 14258.865253542175 Test RE 1.0000185272895397\n",
      "121 Train Loss 1.4243181 Test MSE 14258.892488732447 Test RE 1.0000194823333448\n",
      "122 Train Loss 1.4244546 Test MSE 14258.892488732447 Test RE 1.0000194823333448\n",
      "123 Train Loss 1.4246219 Test MSE 14258.834937287287 Test RE 1.0000174642023958\n",
      "124 Train Loss 1.4247901 Test MSE 14258.834937287287 Test RE 1.0000174642023958\n",
      "125 Train Loss 1.4249293 Test MSE 14258.782029243064 Test RE 1.00001560889593\n",
      "126 Train Loss 1.4250715 Test MSE 14258.782029243064 Test RE 1.00001560889593\n",
      "127 Train Loss 1.425225 Test MSE 14258.782029243064 Test RE 1.00001560889593\n",
      "128 Train Loss 1.4253513 Test MSE 14258.65900245985 Test RE 1.0000112947487378\n",
      "129 Train Loss 1.4254912 Test MSE 14258.65900245985 Test RE 1.0000112947487378\n",
      "130 Train Loss 1.425617 Test MSE 14258.723190971104 Test RE 1.0000135456323591\n",
      "131 Train Loss 1.4257854 Test MSE 14258.714607232965 Test RE 1.0000132446286998\n",
      "132 Train Loss 1.4259018 Test MSE 14258.69067685529 Test RE 1.000012405468042\n",
      "133 Train Loss 1.4259756 Test MSE 14258.66784393231 Test RE 1.000011604790901\n",
      "134 Train Loss 1.4261096 Test MSE 14258.63367698746 Test RE 1.000010406664884\n",
      "135 Train Loss 1.4262618 Test MSE 14258.574990339543 Test RE 1.000008348707566\n",
      "136 Train Loss 1.4263698 Test MSE 14258.545000041979 Test RE 1.00000729703993\n",
      "137 Train Loss 1.4264481 Test MSE 14258.521576308302 Test RE 1.000006475640754\n",
      "138 Train Loss 1.4265769 Test MSE 14258.498720737542 Test RE 1.0000056741646752\n",
      "139 Train Loss 1.4266686 Test MSE 14258.410978998812 Test RE 1.0000025973200055\n",
      "140 Train Loss 1.4267904 Test MSE 14258.3938830159 Test RE 1.0000019978130739\n",
      "141 Train Loss 1.4268909 Test MSE 14258.386728414307 Test RE 1.0000017469216744\n",
      "142 Train Loss 1.4270194 Test MSE 14258.384145712038 Test RE 1.0000016563536829\n",
      "143 Train Loss 1.4271212 Test MSE 14258.370484833895 Test RE 1.0000011773055697\n",
      "144 Train Loss 1.4271805 Test MSE 14258.331953163906 Test RE 0.9999998261083406\n",
      "145 Train Loss 1.4273218 Test MSE 14258.332345895784 Test RE 0.9999998398803513\n",
      "146 Train Loss 1.4273908 Test MSE 14258.329064376827 Test RE 0.9999997248066338\n",
      "147 Train Loss 1.4274946 Test MSE 14258.332302851728 Test RE 0.9999998383709163\n",
      "148 Train Loss 1.4275603 Test MSE 14258.293144528338 Test RE 0.9999984651968624\n",
      "149 Train Loss 1.4276557 Test MSE 14258.280714378292 Test RE 0.9999980293054989\n",
      "150 Train Loss 1.4277688 Test MSE 14258.242850264633 Test RE 0.9999967015134271\n",
      "151 Train Loss 1.4278643 Test MSE 14258.196934355607 Test RE 0.9999950913643011\n",
      "152 Train Loss 1.4279505 Test MSE 14258.196934355607 Test RE 0.9999950913643011\n",
      "153 Train Loss 1.4280064 Test MSE 14258.178823629885 Test RE 0.9999944562684799\n",
      "154 Train Loss 1.4280903 Test MSE 14258.178823629885 Test RE 0.9999944562684799\n",
      "155 Train Loss 1.4282298 Test MSE 14258.178823629885 Test RE 0.9999944562684799\n",
      "156 Train Loss 1.428265 Test MSE 14258.178823629885 Test RE 0.9999944562684799\n",
      "157 Train Loss 1.4283355 Test MSE 14258.178823629885 Test RE 0.9999944562684799\n",
      "158 Train Loss 1.428423 Test MSE 14258.193339972251 Test RE 0.9999949653187022\n",
      "159 Train Loss 1.4285636 Test MSE 14258.193339972251 Test RE 0.9999949653187022\n",
      "160 Train Loss 1.4285996 Test MSE 14258.146761080821 Test RE 0.9999933319176868\n",
      "161 Train Loss 1.4287119 Test MSE 14258.146761080821 Test RE 0.9999933319176868\n",
      "162 Train Loss 1.4288139 Test MSE 14258.101627986032 Test RE 0.9999917492144677\n",
      "163 Train Loss 1.4288563 Test MSE 14258.101627986032 Test RE 0.9999917492144677\n",
      "164 Train Loss 1.4289407 Test MSE 14258.113898905061 Test RE 0.9999921795247385\n",
      "165 Train Loss 1.4290099 Test MSE 14258.113898905061 Test RE 0.9999921795247385\n",
      "166 Train Loss 1.4290537 Test MSE 14258.138626328868 Test RE 0.9999930466527427\n",
      "167 Train Loss 1.4291488 Test MSE 14258.138626328868 Test RE 0.9999930466527427\n",
      "168 Train Loss 1.4292356 Test MSE 14258.138626328868 Test RE 0.9999930466527427\n",
      "169 Train Loss 1.4293113 Test MSE 14258.12285031074 Test RE 0.9999924934279071\n",
      "170 Train Loss 1.4293756 Test MSE 14258.12285031074 Test RE 0.9999924934279071\n",
      "171 Train Loss 1.4294432 Test MSE 14258.12285031074 Test RE 0.9999924934279071\n",
      "172 Train Loss 1.4294885 Test MSE 14258.048885406371 Test RE 0.9999898996627308\n",
      "173 Train Loss 1.4295509 Test MSE 14258.048885406371 Test RE 0.9999898996627308\n",
      "174 Train Loss 1.4296144 Test MSE 14258.048885406371 Test RE 0.9999898996627308\n",
      "175 Train Loss 1.4296801 Test MSE 14258.101483321318 Test RE 0.9999917441414389\n",
      "176 Train Loss 1.429765 Test MSE 14258.101483321318 Test RE 0.9999917441414389\n",
      "177 Train Loss 1.4298762 Test MSE 14258.101483321318 Test RE 0.9999917441414389\n",
      "178 Train Loss 1.4298996 Test MSE 14258.101483321318 Test RE 0.9999917441414389\n",
      "179 Train Loss 1.4299562 Test MSE 14258.101483321318 Test RE 0.9999917441414389\n",
      "180 Train Loss 1.4300243 Test MSE 14258.101483321318 Test RE 0.9999917441414389\n",
      "181 Train Loss 1.4300759 Test MSE 14258.101483321318 Test RE 0.9999917441414389\n",
      "182 Train Loss 1.430096 Test MSE 14258.101483321318 Test RE 0.9999917441414389\n",
      "183 Train Loss 1.4302078 Test MSE 14258.064502144929 Test RE 0.9999904473034585\n",
      "184 Train Loss 1.4302425 Test MSE 14258.064502144929 Test RE 0.9999904473034585\n",
      "185 Train Loss 1.4302846 Test MSE 14258.064502144929 Test RE 0.9999904473034585\n",
      "186 Train Loss 1.4303583 Test MSE 14258.064502144929 Test RE 0.9999904473034585\n",
      "187 Train Loss 1.4304227 Test MSE 14258.064502144929 Test RE 0.9999904473034585\n",
      "188 Train Loss 1.4304326 Test MSE 14258.069774627522 Test RE 0.9999906321964273\n",
      "189 Train Loss 1.4304724 Test MSE 14258.069774627522 Test RE 0.9999906321964273\n",
      "190 Train Loss 1.4305946 Test MSE 14258.069936219359 Test RE 0.9999906378630539\n",
      "191 Train Loss 1.4306461 Test MSE 14258.070147704908 Test RE 0.9999906452793303\n",
      "192 Train Loss 1.4306841 Test MSE 14258.070147704908 Test RE 0.9999906452793303\n",
      "193 Train Loss 1.4307171 Test MSE 14258.070147704908 Test RE 0.9999906452793303\n",
      "194 Train Loss 1.4307383 Test MSE 14258.039400358344 Test RE 0.9999895670452075\n",
      "195 Train Loss 1.4307985 Test MSE 14258.039400358344 Test RE 0.9999895670452075\n",
      "196 Train Loss 1.4308536 Test MSE 14258.042529623146 Test RE 0.9999896767809113\n",
      "197 Train Loss 1.4309214 Test MSE 14258.042529623146 Test RE 0.9999896767809113\n",
      "198 Train Loss 1.4309906 Test MSE 14258.05637382817 Test RE 0.9999901622633587\n",
      "199 Train Loss 1.4310391 Test MSE 14258.05637382817 Test RE 0.9999901622633587\n",
      "Training time: 85.96\n",
      "Training time: 85.96\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (m_lambda): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 4.279728 Test MSE 14254.339377190641 Test RE 0.9998598078078647\n",
      "1 Train Loss 2.0708175 Test MSE 14256.299080005898 Test RE 0.9999285363784036\n",
      "2 Train Loss 2.0045376 Test MSE 14257.385801088705 Test RE 0.9999666466449065\n",
      "3 Train Loss 2.034921 Test MSE 14258.101579731989 Test RE 0.9999917475223193\n",
      "4 Train Loss 2.0369768 Test MSE 14258.272322207991 Test RE 0.9999977350149333\n",
      "5 Train Loss 2.0380821 Test MSE 14257.579546806863 Test RE 0.9999734409689366\n",
      "6 Train Loss 2.0522387 Test MSE 14257.987782436609 Test RE 0.9999877569285263\n",
      "7 Train Loss 2.0712547 Test MSE 14258.127115707455 Test RE 0.9999926430045528\n",
      "8 Train Loss 2.0885048 Test MSE 14258.367908479255 Test RE 1.0000010869601201\n",
      "9 Train Loss 2.1040845 Test MSE 14258.432409716794 Test RE 1.0000033488331168\n",
      "10 Train Loss 2.1106703 Test MSE 14259.46394737904 Test RE 1.0000395211987325\n",
      "11 Train Loss 2.1151042 Test MSE 14259.462748957618 Test RE 1.0000394791751044\n",
      "12 Train Loss 2.09137 Test MSE 14258.281486690534 Test RE 0.9999980563883827\n",
      "13 Train Loss 2.0187035 Test MSE 14257.706676319702 Test RE 0.9999778991538829\n",
      "14 Train Loss 1.8677986 Test MSE 14256.405034348934 Test RE 0.9999322521595371\n",
      "15 Train Loss 1.5954765 Test MSE 14257.213483940404 Test RE 0.999960603744616\n",
      "16 Train Loss 1.4645681 Test MSE 14258.697423418724 Test RE 1.000012642048191\n",
      "17 Train Loss 1.4187574 Test MSE 14258.642186106219 Test RE 1.0000107050527323\n",
      "18 Train Loss 1.3353571 Test MSE 14257.295778335278 Test RE 0.9999634896884272\n",
      "19 Train Loss 1.3277727 Test MSE 14257.481072982888 Test RE 0.9999699876696468\n",
      "20 Train Loss 1.3296137 Test MSE 14257.713822890115 Test RE 0.9999781497696284\n",
      "21 Train Loss 1.3298941 Test MSE 14257.871108875379 Test RE 0.9999836654545662\n",
      "22 Train Loss 1.3326591 Test MSE 14257.756287478585 Test RE 0.999979638915417\n",
      "23 Train Loss 1.3360758 Test MSE 14257.80101576593 Test RE 0.9999812074420719\n",
      "24 Train Loss 1.3391006 Test MSE 14257.927418020494 Test RE 0.9999856400891138\n",
      "25 Train Loss 1.342274 Test MSE 14258.149962841524 Test RE 0.9999934441952226\n",
      "26 Train Loss 1.3453782 Test MSE 14258.193207590517 Test RE 0.9999949606764214\n",
      "27 Train Loss 1.3483281 Test MSE 14258.193207590517 Test RE 0.9999949606764214\n",
      "28 Train Loss 1.351178 Test MSE 14258.229967235215 Test RE 0.9999962497400228\n",
      "29 Train Loss 1.3539082 Test MSE 14258.229967235215 Test RE 0.9999962497400228\n",
      "30 Train Loss 1.356506 Test MSE 14258.229967235215 Test RE 0.9999962497400228\n",
      "31 Train Loss 1.3589883 Test MSE 14258.229967235215 Test RE 0.9999962497400228\n",
      "32 Train Loss 1.3614054 Test MSE 14258.229967235215 Test RE 0.9999962497400228\n",
      "33 Train Loss 1.3636912 Test MSE 14258.228606172383 Test RE 0.9999962020111756\n",
      "34 Train Loss 1.3658721 Test MSE 14258.228606172383 Test RE 0.9999962020111756\n",
      "35 Train Loss 1.3679444 Test MSE 14258.228606172383 Test RE 0.9999962020111756\n",
      "36 Train Loss 1.3699749 Test MSE 14258.211549808702 Test RE 0.9999956038897512\n",
      "37 Train Loss 1.3718716 Test MSE 14258.211549808702 Test RE 0.9999956038897512\n",
      "38 Train Loss 1.373719 Test MSE 14258.211549808702 Test RE 0.9999956038897512\n",
      "39 Train Loss 1.3755182 Test MSE 14258.211549808702 Test RE 0.9999956038897512\n",
      "40 Train Loss 1.3771782 Test MSE 14258.211549808702 Test RE 0.9999956038897512\n",
      "41 Train Loss 1.3788562 Test MSE 14258.211549808702 Test RE 0.9999956038897512\n",
      "42 Train Loss 1.380434 Test MSE 14258.211549808702 Test RE 0.9999956038897512\n",
      "43 Train Loss 1.3819052 Test MSE 14258.211549808702 Test RE 0.9999956038897512\n",
      "44 Train Loss 1.3833853 Test MSE 14258.211549808702 Test RE 0.9999956038897512\n",
      "45 Train Loss 1.3847781 Test MSE 14258.221723188011 Test RE 0.9999959606432168\n",
      "46 Train Loss 1.3861063 Test MSE 14258.221723188011 Test RE 0.9999959606432168\n",
      "47 Train Loss 1.3874178 Test MSE 14258.215501196635 Test RE 0.9999957424544751\n",
      "48 Train Loss 1.3886598 Test MSE 14258.215501196635 Test RE 0.9999957424544751\n",
      "49 Train Loss 1.3898596 Test MSE 14258.221277317629 Test RE 0.9999959450077262\n",
      "50 Train Loss 1.3910089 Test MSE 14258.221277317629 Test RE 0.9999959450077262\n",
      "51 Train Loss 1.3921566 Test MSE 14258.221277317629 Test RE 0.9999959450077262\n",
      "52 Train Loss 1.3931862 Test MSE 14258.212914530988 Test RE 0.9999956517469545\n",
      "53 Train Loss 1.394219 Test MSE 14258.212914530988 Test RE 0.9999956517469545\n",
      "54 Train Loss 1.3952304 Test MSE 14258.212914530988 Test RE 0.9999956517469545\n",
      "55 Train Loss 1.3961991 Test MSE 14258.231929091164 Test RE 0.9999963185370798\n",
      "56 Train Loss 1.3971049 Test MSE 14258.231929091164 Test RE 0.9999963185370798\n",
      "57 Train Loss 1.398029 Test MSE 14258.209211286414 Test RE 0.9999955218839506\n",
      "58 Train Loss 1.3989123 Test MSE 14258.209211286414 Test RE 0.9999955218839506\n",
      "59 Train Loss 1.3997412 Test MSE 14258.19607681429 Test RE 0.9999950612925772\n",
      "60 Train Loss 1.4005688 Test MSE 14258.19607681429 Test RE 0.9999950612925772\n",
      "61 Train Loss 1.4013724 Test MSE 14258.19607681429 Test RE 0.9999950612925772\n",
      "62 Train Loss 1.4021392 Test MSE 14258.19607681429 Test RE 0.9999950612925772\n",
      "63 Train Loss 1.4028922 Test MSE 14258.19607681429 Test RE 0.9999950612925772\n",
      "64 Train Loss 1.4035983 Test MSE 14258.187871956181 Test RE 0.9999947735696869\n",
      "65 Train Loss 1.4043399 Test MSE 14258.187871956181 Test RE 0.9999947735696869\n",
      "66 Train Loss 1.4049742 Test MSE 14258.187871956181 Test RE 0.9999947735696869\n",
      "67 Train Loss 1.4056177 Test MSE 14258.187871956181 Test RE 0.9999947735696869\n",
      "68 Train Loss 1.406269 Test MSE 14258.187871956181 Test RE 0.9999947735696869\n",
      "69 Train Loss 1.4068989 Test MSE 14258.184181638479 Test RE 0.9999946441598808\n",
      "70 Train Loss 1.4074596 Test MSE 14258.184181638479 Test RE 0.9999946441598808\n",
      "71 Train Loss 1.4080573 Test MSE 14258.184181638479 Test RE 0.9999946441598808\n",
      "72 Train Loss 1.4085952 Test MSE 14258.184181638479 Test RE 0.9999946441598808\n",
      "73 Train Loss 1.4091613 Test MSE 14258.184181638479 Test RE 0.9999946441598808\n",
      "74 Train Loss 1.4097104 Test MSE 14258.18231554001 Test RE 0.9999945787206737\n",
      "75 Train Loss 1.4102288 Test MSE 14258.18231554001 Test RE 0.9999945787206737\n",
      "76 Train Loss 1.410735 Test MSE 14258.18231554001 Test RE 0.9999945787206737\n",
      "77 Train Loss 1.4112439 Test MSE 14258.181349253806 Test RE 0.9999945448355345\n",
      "78 Train Loss 1.4117128 Test MSE 14258.181349253806 Test RE 0.9999945448355345\n",
      "79 Train Loss 1.4121711 Test MSE 14258.181349253806 Test RE 0.9999945448355345\n",
      "80 Train Loss 1.4125882 Test MSE 14258.189389269966 Test RE 0.9999948267779146\n",
      "81 Train Loss 1.4130315 Test MSE 14258.189389269966 Test RE 0.9999948267779146\n",
      "82 Train Loss 1.4134563 Test MSE 14258.189389269966 Test RE 0.9999948267779146\n",
      "83 Train Loss 1.4139127 Test MSE 14258.200266528134 Test RE 0.9999952082148396\n",
      "84 Train Loss 1.414288 Test MSE 14258.200266528134 Test RE 0.9999952082148396\n",
      "85 Train Loss 1.414684 Test MSE 14258.200266528134 Test RE 0.9999952082148396\n",
      "86 Train Loss 1.4150453 Test MSE 14258.200266528134 Test RE 0.9999952082148396\n",
      "87 Train Loss 1.4154184 Test MSE 14258.192544251977 Test RE 0.9999949374148766\n",
      "88 Train Loss 1.415806 Test MSE 14258.192544251977 Test RE 0.9999949374148766\n",
      "89 Train Loss 1.416146 Test MSE 14258.192544251977 Test RE 0.9999949374148766\n",
      "90 Train Loss 1.4165144 Test MSE 14258.192544251977 Test RE 0.9999949374148766\n",
      "91 Train Loss 1.4168384 Test MSE 14258.192544251977 Test RE 0.9999949374148766\n",
      "92 Train Loss 1.4171617 Test MSE 14258.192544251977 Test RE 0.9999949374148766\n",
      "93 Train Loss 1.4174869 Test MSE 14258.193294355231 Test RE 0.9999949637190327\n",
      "94 Train Loss 1.4177829 Test MSE 14258.193294355231 Test RE 0.9999949637190327\n",
      "95 Train Loss 1.4181006 Test MSE 14258.193294355231 Test RE 0.9999949637190327\n",
      "96 Train Loss 1.4184217 Test MSE 14258.193294355231 Test RE 0.9999949637190327\n",
      "97 Train Loss 1.4186879 Test MSE 14258.193294355231 Test RE 0.9999949637190327\n",
      "98 Train Loss 1.4189906 Test MSE 14258.193294355231 Test RE 0.9999949637190327\n",
      "99 Train Loss 1.4192877 Test MSE 14258.204739363358 Test RE 0.999995365065401\n",
      "100 Train Loss 1.4195288 Test MSE 14258.204739363358 Test RE 0.999995365065401\n",
      "101 Train Loss 1.4197813 Test MSE 14258.204739363358 Test RE 0.999995365065401\n",
      "102 Train Loss 1.4200798 Test MSE 14258.204739363358 Test RE 0.999995365065401\n",
      "103 Train Loss 1.4203218 Test MSE 14258.204739363358 Test RE 0.999995365065401\n",
      "104 Train Loss 1.420563 Test MSE 14258.202020173745 Test RE 0.9999952697105793\n",
      "105 Train Loss 1.4208335 Test MSE 14258.202020173745 Test RE 0.9999952697105793\n",
      "106 Train Loss 1.4210666 Test MSE 14258.202020173745 Test RE 0.9999952697105793\n",
      "107 Train Loss 1.421241 Test MSE 14258.202020173745 Test RE 0.9999952697105793\n",
      "108 Train Loss 1.4215138 Test MSE 14258.202020173745 Test RE 0.9999952697105793\n",
      "109 Train Loss 1.4217163 Test MSE 14258.202020173745 Test RE 0.9999952697105793\n",
      "110 Train Loss 1.4219116 Test MSE 14258.202020173745 Test RE 0.9999952697105793\n",
      "111 Train Loss 1.4221704 Test MSE 14258.202020173745 Test RE 0.9999952697105793\n",
      "112 Train Loss 1.4223859 Test MSE 14258.195641312324 Test RE 0.9999950460206656\n",
      "113 Train Loss 1.4225825 Test MSE 14258.195641312324 Test RE 0.9999950460206656\n",
      "114 Train Loss 1.422786 Test MSE 14258.195641312324 Test RE 0.9999950460206656\n",
      "115 Train Loss 1.4229679 Test MSE 14258.188853338366 Test RE 0.9999948079841949\n",
      "116 Train Loss 1.4231212 Test MSE 14258.188853338366 Test RE 0.9999948079841949\n",
      "117 Train Loss 1.4233664 Test MSE 14258.179450195556 Test RE 0.9999944782405076\n",
      "118 Train Loss 1.4235135 Test MSE 14258.179450195556 Test RE 0.9999944782405076\n",
      "119 Train Loss 1.4237018 Test MSE 14258.18171696257 Test RE 0.9999945577301229\n",
      "120 Train Loss 1.4238871 Test MSE 14258.18171696257 Test RE 0.9999945577301229\n",
      "121 Train Loss 1.4240538 Test MSE 14258.18171696257 Test RE 0.9999945577301229\n",
      "122 Train Loss 1.4242193 Test MSE 14258.18171696257 Test RE 0.9999945577301229\n",
      "123 Train Loss 1.424407 Test MSE 14258.190196083631 Test RE 0.9999948550707595\n",
      "124 Train Loss 1.4245958 Test MSE 14258.190196083631 Test RE 0.9999948550707595\n",
      "125 Train Loss 1.4247068 Test MSE 14258.190196083631 Test RE 0.9999948550707595\n",
      "126 Train Loss 1.4248571 Test MSE 14258.190196083631 Test RE 0.9999948550707595\n",
      "127 Train Loss 1.4250458 Test MSE 14258.190196083631 Test RE 0.9999948550707595\n",
      "128 Train Loss 1.4251693 Test MSE 14258.190196083631 Test RE 0.9999948550707595\n",
      "129 Train Loss 1.4253209 Test MSE 14258.200215773499 Test RE 0.9999952064350077\n",
      "130 Train Loss 1.4254755 Test MSE 14258.200215773499 Test RE 0.9999952064350077\n",
      "131 Train Loss 1.4256313 Test MSE 14258.186243341828 Test RE 0.9999947164584364\n",
      "132 Train Loss 1.4257829 Test MSE 14258.186243341828 Test RE 0.9999947164584364\n",
      "133 Train Loss 1.4258513 Test MSE 14258.186243341828 Test RE 0.9999947164584364\n",
      "134 Train Loss 1.4260318 Test MSE 14258.186243341828 Test RE 0.9999947164584364\n",
      "135 Train Loss 1.4261678 Test MSE 14258.186243341828 Test RE 0.9999947164584364\n",
      "136 Train Loss 1.4262952 Test MSE 14258.163300193666 Test RE 0.9999939119018194\n",
      "137 Train Loss 1.4263803 Test MSE 14258.163300193666 Test RE 0.9999939119018194\n",
      "138 Train Loss 1.4265318 Test MSE 14258.163300193666 Test RE 0.9999939119018194\n",
      "139 Train Loss 1.4266742 Test MSE 14258.163300193666 Test RE 0.9999939119018194\n",
      "140 Train Loss 1.4268124 Test MSE 14258.170121778836 Test RE 0.9999941511171614\n",
      "141 Train Loss 1.4269053 Test MSE 14258.170121778836 Test RE 0.9999941511171614\n",
      "142 Train Loss 1.4270175 Test MSE 14258.169555472312 Test RE 0.9999941312582591\n",
      "143 Train Loss 1.427178 Test MSE 14258.169555472312 Test RE 0.9999941312582591\n",
      "144 Train Loss 1.4272393 Test MSE 14258.169555472312 Test RE 0.9999941312582591\n",
      "145 Train Loss 1.4273666 Test MSE 14258.169555472312 Test RE 0.9999941312582591\n",
      "146 Train Loss 1.4275073 Test MSE 14258.169555472312 Test RE 0.9999941312582591\n",
      "147 Train Loss 1.4276007 Test MSE 14258.169555472312 Test RE 0.9999941312582591\n",
      "148 Train Loss 1.4277099 Test MSE 14258.169555472312 Test RE 0.9999941312582591\n",
      "149 Train Loss 1.4278225 Test MSE 14258.169555472312 Test RE 0.9999941312582591\n",
      "150 Train Loss 1.4279077 Test MSE 14258.17953232125 Test RE 0.9999944811204415\n",
      "151 Train Loss 1.4280192 Test MSE 14258.17953232125 Test RE 0.9999944811204415\n",
      "152 Train Loss 1.4280931 Test MSE 14258.188764141873 Test RE 0.999994804856307\n",
      "153 Train Loss 1.4281448 Test MSE 14258.188764141873 Test RE 0.999994804856307\n",
      "154 Train Loss 1.4282556 Test MSE 14258.188764141873 Test RE 0.999994804856307\n",
      "155 Train Loss 1.4284259 Test MSE 14258.188764141873 Test RE 0.999994804856307\n",
      "156 Train Loss 1.4284326 Test MSE 14258.188764141873 Test RE 0.999994804856307\n",
      "157 Train Loss 1.4285324 Test MSE 14258.188764141873 Test RE 0.999994804856307\n",
      "158 Train Loss 1.4286406 Test MSE 14258.188764141873 Test RE 0.999994804856307\n",
      "159 Train Loss 1.4287539 Test MSE 14258.188764141873 Test RE 0.999994804856307\n",
      "160 Train Loss 1.4287841 Test MSE 14258.188764141873 Test RE 0.999994804856307\n",
      "161 Train Loss 1.4289261 Test MSE 14258.188764141873 Test RE 0.999994804856307\n",
      "162 Train Loss 1.4290254 Test MSE 14258.188764141873 Test RE 0.999994804856307\n",
      "163 Train Loss 1.429073 Test MSE 14258.214889275001 Test RE 0.9999957209960029\n",
      "164 Train Loss 1.4291478 Test MSE 14258.214889275001 Test RE 0.9999957209960029\n",
      "165 Train Loss 1.429249 Test MSE 14258.214889275001 Test RE 0.9999957209960029\n",
      "166 Train Loss 1.4293076 Test MSE 14258.20742335167 Test RE 0.9999954591857969\n",
      "167 Train Loss 1.4293729 Test MSE 14258.20742335167 Test RE 0.9999954591857969\n",
      "168 Train Loss 1.4294312 Test MSE 14258.203659333338 Test RE 0.9999953271915923\n",
      "169 Train Loss 1.4295373 Test MSE 14258.20407820716 Test RE 0.9999953418803945\n",
      "170 Train Loss 1.4296031 Test MSE 14258.20407820716 Test RE 0.9999953418803945\n",
      "171 Train Loss 1.4296664 Test MSE 14258.20407820716 Test RE 0.9999953418803945\n",
      "172 Train Loss 1.4297363 Test MSE 14258.20407820716 Test RE 0.9999953418803945\n",
      "173 Train Loss 1.4298074 Test MSE 14258.20670027976 Test RE 0.9999954338295715\n",
      "174 Train Loss 1.4298909 Test MSE 14258.20670027976 Test RE 0.9999954338295715\n",
      "175 Train Loss 1.4299409 Test MSE 14258.20670027976 Test RE 0.9999954338295715\n",
      "176 Train Loss 1.4300405 Test MSE 14258.208789466773 Test RE 0.999995507091849\n",
      "177 Train Loss 1.4301038 Test MSE 14258.208789466773 Test RE 0.999995507091849\n",
      "178 Train Loss 1.430176 Test MSE 14258.208789466773 Test RE 0.999995507091849\n",
      "179 Train Loss 1.4302379 Test MSE 14258.208789466773 Test RE 0.999995507091849\n",
      "180 Train Loss 1.430287 Test MSE 14258.208789466773 Test RE 0.999995507091849\n",
      "181 Train Loss 1.430363 Test MSE 14258.183588949229 Test RE 0.9999946233758148\n",
      "182 Train Loss 1.430374 Test MSE 14258.183588949229 Test RE 0.9999946233758148\n",
      "183 Train Loss 1.4304717 Test MSE 14258.183588949229 Test RE 0.9999946233758148\n",
      "184 Train Loss 1.4305414 Test MSE 14258.183588949229 Test RE 0.9999946233758148\n",
      "185 Train Loss 1.430584 Test MSE 14258.128618398123 Test RE 0.9999926957000984\n",
      "186 Train Loss 1.4306177 Test MSE 14258.128618398123 Test RE 0.9999926957000984\n",
      "187 Train Loss 1.4306879 Test MSE 14258.128618398123 Test RE 0.9999926957000984\n",
      "188 Train Loss 1.4307405 Test MSE 14258.128618398123 Test RE 0.9999926957000984\n",
      "189 Train Loss 1.4307561 Test MSE 14258.128618398123 Test RE 0.9999926957000984\n",
      "190 Train Loss 1.4308491 Test MSE 14258.12978598604 Test RE 0.9999927366444397\n",
      "191 Train Loss 1.4309092 Test MSE 14258.12978598604 Test RE 0.9999927366444397\n",
      "192 Train Loss 1.4309669 Test MSE 14258.12978598604 Test RE 0.9999927366444397\n",
      "193 Train Loss 1.43102 Test MSE 14258.12978598604 Test RE 0.9999927366444397\n",
      "194 Train Loss 1.431058 Test MSE 14258.12978598604 Test RE 0.9999927366444397\n",
      "195 Train Loss 1.4311032 Test MSE 14258.12978598604 Test RE 0.9999927366444397\n",
      "196 Train Loss 1.431197 Test MSE 14258.12978598604 Test RE 0.9999927366444397\n",
      "197 Train Loss 1.4312375 Test MSE 14258.12978598604 Test RE 0.9999927366444397\n",
      "198 Train Loss 1.4312917 Test MSE 14258.11894668836 Test RE 0.9999923565377505\n",
      "199 Train Loss 1.4313427 Test MSE 14258.11894668836 Test RE 0.9999923565377505\n",
      "Training time: 81.96\n",
      "Training time: 81.96\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (m_lambda): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 4.330419 Test MSE 14261.57436851115 Test RE 1.000113522103066\n",
      "1 Train Loss 2.013393 Test MSE 14259.584088182279 Test RE 1.0000437340253407\n",
      "2 Train Loss 1.9746706 Test MSE 14258.867584780255 Test RE 1.0000186090380214\n",
      "3 Train Loss 2.0119143 Test MSE 14258.809869648667 Test RE 1.00001658516538\n",
      "4 Train Loss 2.0464702 Test MSE 14258.760463438386 Test RE 1.00001485265499\n",
      "5 Train Loss 2.078351 Test MSE 14258.71581685716 Test RE 1.000013287046275\n",
      "6 Train Loss 2.107656 Test MSE 14258.67613639483 Test RE 1.0000118955809534\n",
      "7 Train Loss 2.1345668 Test MSE 14258.642106845766 Test RE 1.0000107022733196\n",
      "8 Train Loss 2.1525798 Test MSE 14257.399399283262 Test RE 0.9999671235113518\n",
      "9 Train Loss 2.1457694 Test MSE 14257.903764004983 Test RE 0.999984810596686\n",
      "10 Train Loss 2.1659868 Test MSE 14257.86494383021 Test RE 0.999983449260119\n",
      "11 Train Loss 2.1844168 Test MSE 14257.85111533862 Test RE 0.9999829643254484\n",
      "12 Train Loss 2.200543 Test MSE 14257.757177598867 Test RE 0.9999796701300809\n",
      "13 Train Loss 2.2036862 Test MSE 14259.226405100046 Test RE 1.0000311915498354\n",
      "14 Train Loss 2.2141616 Test MSE 14259.317512115253 Test RE 1.0000343863133117\n",
      "15 Train Loss 2.227265 Test MSE 14259.31350723999 Test RE 1.0000342458783558\n",
      "16 Train Loss 2.2398095 Test MSE 14259.307204957704 Test RE 1.000034024882485\n",
      "17 Train Loss 2.2502494 Test MSE 14259.305565986373 Test RE 1.0000339674102978\n",
      "18 Train Loss 2.260937 Test MSE 14259.305565986373 Test RE 1.0000339674102978\n",
      "19 Train Loss 2.2702103 Test MSE 14259.315155302425 Test RE 1.0000343036693156\n",
      "20 Train Loss 2.278911 Test MSE 14259.348069054988 Test RE 1.0000354578223123\n",
      "21 Train Loss 2.2829218 Test MSE 14259.187105478604 Test RE 1.0000298134640535\n",
      "22 Train Loss 2.2876215 Test MSE 14259.383959004836 Test RE 1.0000367163370658\n",
      "23 Train Loss 2.2877607 Test MSE 14259.366407807598 Test RE 1.000036100888071\n",
      "24 Train Loss 2.2853758 Test MSE 14258.972540846653 Test RE 1.000022289478483\n",
      "25 Train Loss 2.2751193 Test MSE 14259.324514004393 Test RE 1.0000346318415092\n",
      "26 Train Loss 2.2739656 Test MSE 14259.83839871266 Test RE 1.0000526515546644\n",
      "27 Train Loss 2.272042 Test MSE 14259.64376613488 Test RE 1.0000458266705312\n",
      "28 Train Loss 2.2706435 Test MSE 14259.447076949476 Test RE 1.0000389296231527\n",
      "29 Train Loss 2.2510133 Test MSE 14259.583803932406 Test RE 1.0000437240579287\n",
      "30 Train Loss 2.1856353 Test MSE 14258.407790195972 Test RE 1.0000024854978922\n",
      "31 Train Loss 2.1473784 Test MSE 14258.266838974936 Test RE 0.9999975427328305\n",
      "32 Train Loss 2.0565255 Test MSE 14258.087484019408 Test RE 0.9999912532208985\n",
      "33 Train Loss 2.0180721 Test MSE 14258.902379497747 Test RE 1.0000198291680058\n",
      "34 Train Loss 1.8391528 Test MSE 14259.02067447664 Test RE 1.000023977351228\n",
      "35 Train Loss 1.6801125 Test MSE 14261.043284458667 Test RE 1.0000949004111885\n",
      "36 Train Loss 1.6195431 Test MSE 14260.811602845022 Test RE 1.0000867767228732\n",
      "37 Train Loss 1.479594 Test MSE 14260.675328962652 Test RE 1.0000819983821607\n",
      "38 Train Loss 1.4587083 Test MSE 14260.74979141603 Test RE 1.0000846093546516\n",
      "39 Train Loss 1.4282253 Test MSE 14260.755658863827 Test RE 1.0000848150922081\n",
      "40 Train Loss 1.4104644 Test MSE 14260.391668304419 Test RE 1.0000720519622994\n",
      "41 Train Loss 1.4105275 Test MSE 14260.410354922209 Test RE 1.0000727072022992\n",
      "42 Train Loss 1.4060085 Test MSE 14260.411296526192 Test RE 1.0000727402193164\n",
      "43 Train Loss 1.401498 Test MSE 14260.384893013594 Test RE 1.0000718143888876\n",
      "44 Train Loss 1.3970044 Test MSE 14260.2251253741 Test RE 1.0000662121712116\n",
      "45 Train Loss 1.3964986 Test MSE 14259.85883883501 Test RE 1.0000533682945973\n",
      "46 Train Loss 1.3966228 Test MSE 14259.664313335337 Test RE 1.00004654717011\n",
      "47 Train Loss 1.3974986 Test MSE 14259.642241688725 Test RE 1.000045773214916\n",
      "48 Train Loss 1.3972931 Test MSE 14259.315718554873 Test RE 1.0000343234203264\n",
      "49 Train Loss 1.3978295 Test MSE 14259.274642244847 Test RE 1.0000328830374108\n",
      "50 Train Loss 1.3986517 Test MSE 14259.26631462872 Test RE 1.0000325910207624\n",
      "51 Train Loss 1.3994765 Test MSE 14259.265123454108 Test RE 1.0000325492509603\n",
      "52 Train Loss 1.3999745 Test MSE 14259.130450066756 Test RE 1.0000278267744618\n",
      "53 Train Loss 1.4005436 Test MSE 14258.912879127995 Test RE 1.0000201973533063\n",
      "54 Train Loss 1.4012177 Test MSE 14258.842441846755 Test RE 1.0000177273616782\n",
      "55 Train Loss 1.4019125 Test MSE 14258.814110846875 Test RE 1.0000167338898625\n",
      "56 Train Loss 1.4025476 Test MSE 14258.768173124598 Test RE 1.000015123008072\n",
      "57 Train Loss 1.4032333 Test MSE 14258.75864377443 Test RE 1.0000147888454098\n",
      "58 Train Loss 1.4038827 Test MSE 14258.708678771187 Test RE 1.0000130367368572\n",
      "59 Train Loss 1.404482 Test MSE 14258.675676723169 Test RE 1.0000118794617456\n",
      "60 Train Loss 1.4050802 Test MSE 14258.594078278085 Test RE 1.0000090180623764\n",
      "61 Train Loss 1.4056833 Test MSE 14258.583279593917 Test RE 1.0000086393860659\n",
      "62 Train Loss 1.4062926 Test MSE 14258.56016488573 Test RE 1.0000078288245675\n",
      "63 Train Loss 1.4068608 Test MSE 14258.56016488573 Test RE 1.0000078288245675\n",
      "64 Train Loss 1.4074053 Test MSE 14258.561213246257 Test RE 1.000007865587351\n",
      "65 Train Loss 1.4079771 Test MSE 14258.545126707628 Test RE 1.0000073014817072\n",
      "66 Train Loss 1.40845 Test MSE 14258.545126707628 Test RE 1.0000073014817072\n",
      "67 Train Loss 1.4089702 Test MSE 14258.545126707628 Test RE 1.0000073014817072\n",
      "68 Train Loss 1.4094752 Test MSE 14258.545126707628 Test RE 1.0000073014817072\n",
      "69 Train Loss 1.409961 Test MSE 14258.541529809925 Test RE 1.0000071753494773\n",
      "70 Train Loss 1.4104161 Test MSE 14258.541529809925 Test RE 1.0000071753494773\n",
      "71 Train Loss 1.4108905 Test MSE 14258.541529809925 Test RE 1.0000071753494773\n",
      "72 Train Loss 1.4113116 Test MSE 14258.541529809925 Test RE 1.0000071753494773\n",
      "73 Train Loss 1.4117571 Test MSE 14258.565286006731 Test RE 1.0000080084065375\n",
      "74 Train Loss 1.412189 Test MSE 14258.565286006731 Test RE 1.0000080084065375\n",
      "75 Train Loss 1.4126298 Test MSE 14258.565286006731 Test RE 1.0000080084065375\n",
      "76 Train Loss 1.4130127 Test MSE 14258.565286006731 Test RE 1.0000080084065375\n",
      "77 Train Loss 1.4134504 Test MSE 14258.565286006731 Test RE 1.0000080084065375\n",
      "78 Train Loss 1.4138516 Test MSE 14258.553328224378 Test RE 1.0000075890838218\n",
      "79 Train Loss 1.4142088 Test MSE 14258.553328224378 Test RE 1.0000075890838218\n",
      "80 Train Loss 1.414522 Test MSE 14258.553328224378 Test RE 1.0000075890838218\n",
      "81 Train Loss 1.4149102 Test MSE 14258.553328224378 Test RE 1.0000075890838218\n",
      "82 Train Loss 1.4152458 Test MSE 14258.553328224378 Test RE 1.0000075890838218\n",
      "83 Train Loss 1.4156368 Test MSE 14258.55325996831 Test RE 1.0000075866902907\n",
      "84 Train Loss 1.4159523 Test MSE 14258.55325996831 Test RE 1.0000075866902907\n",
      "85 Train Loss 1.4163033 Test MSE 14258.54192410737 Test RE 1.0000071891762867\n",
      "86 Train Loss 1.4165851 Test MSE 14258.54192410737 Test RE 1.0000071891762867\n",
      "87 Train Loss 1.4168863 Test MSE 14258.54192410737 Test RE 1.0000071891762867\n",
      "88 Train Loss 1.4172158 Test MSE 14258.54192410737 Test RE 1.0000071891762867\n",
      "89 Train Loss 1.4175078 Test MSE 14258.54192410737 Test RE 1.0000071891762867\n",
      "90 Train Loss 1.4178275 Test MSE 14258.54192410737 Test RE 1.0000071891762867\n",
      "91 Train Loss 1.4181075 Test MSE 14258.54192410737 Test RE 1.0000071891762867\n",
      "92 Train Loss 1.4183683 Test MSE 14258.54192410737 Test RE 1.0000071891762867\n",
      "93 Train Loss 1.418636 Test MSE 14258.54192410737 Test RE 1.0000071891762867\n",
      "94 Train Loss 1.4188888 Test MSE 14258.53493593955 Test RE 1.0000069441225163\n",
      "95 Train Loss 1.419185 Test MSE 14258.53493593955 Test RE 1.0000069441225163\n",
      "96 Train Loss 1.4194542 Test MSE 14258.53493593955 Test RE 1.0000069441225163\n",
      "97 Train Loss 1.4196833 Test MSE 14258.53493593955 Test RE 1.0000069441225163\n",
      "98 Train Loss 1.4199355 Test MSE 14258.508167291728 Test RE 1.0000060054269775\n",
      "99 Train Loss 1.4202063 Test MSE 14258.508167291728 Test RE 1.0000060054269775\n",
      "100 Train Loss 1.4203933 Test MSE 14258.508167291728 Test RE 1.0000060054269775\n",
      "101 Train Loss 1.4206173 Test MSE 14258.508167291728 Test RE 1.0000060054269775\n",
      "102 Train Loss 1.4208872 Test MSE 14258.508167291728 Test RE 1.0000060054269775\n",
      "103 Train Loss 1.4211086 Test MSE 14258.508167291728 Test RE 1.0000060054269775\n",
      "104 Train Loss 1.4212928 Test MSE 14258.508167291728 Test RE 1.0000060054269775\n",
      "105 Train Loss 1.4215398 Test MSE 14258.508167291728 Test RE 1.0000060054269775\n",
      "106 Train Loss 1.4217503 Test MSE 14258.493435092301 Test RE 1.0000054888129133\n",
      "107 Train Loss 1.4219052 Test MSE 14258.493435092301 Test RE 1.0000054888129133\n",
      "108 Train Loss 1.4221402 Test MSE 14258.493435092301 Test RE 1.0000054888129133\n",
      "109 Train Loss 1.4223171 Test MSE 14258.493435092301 Test RE 1.0000054888129133\n",
      "110 Train Loss 1.4225135 Test MSE 14258.493435092301 Test RE 1.0000054888129133\n",
      "111 Train Loss 1.4227314 Test MSE 14258.493435092301 Test RE 1.0000054888129133\n",
      "112 Train Loss 1.4229083 Test MSE 14258.465537793458 Test RE 1.0000045105375528\n",
      "113 Train Loss 1.4230963 Test MSE 14258.465537793458 Test RE 1.0000045105375528\n",
      "114 Train Loss 1.4232649 Test MSE 14258.465537793458 Test RE 1.0000045105375528\n",
      "115 Train Loss 1.4234402 Test MSE 14258.458838684583 Test RE 1.0000042756195715\n",
      "116 Train Loss 1.4235768 Test MSE 14258.458838684583 Test RE 1.0000042756195715\n",
      "117 Train Loss 1.4237846 Test MSE 14258.458838684583 Test RE 1.0000042756195715\n",
      "118 Train Loss 1.4239279 Test MSE 14258.458838684583 Test RE 1.0000042756195715\n",
      "119 Train Loss 1.4240848 Test MSE 14258.458838684583 Test RE 1.0000042756195715\n",
      "120 Train Loss 1.4242758 Test MSE 14258.458838684583 Test RE 1.0000042756195715\n",
      "121 Train Loss 1.4244442 Test MSE 14258.458838684583 Test RE 1.0000042756195715\n",
      "122 Train Loss 1.4245559 Test MSE 14258.458838684583 Test RE 1.0000042756195715\n",
      "123 Train Loss 1.4247295 Test MSE 14258.459255339794 Test RE 1.000004290230442\n",
      "124 Train Loss 1.4248966 Test MSE 14258.459255339794 Test RE 1.000004290230442\n",
      "125 Train Loss 1.4250228 Test MSE 14258.459255339794 Test RE 1.000004290230442\n",
      "126 Train Loss 1.425152 Test MSE 14258.459255339794 Test RE 1.000004290230442\n",
      "127 Train Loss 1.4253258 Test MSE 14258.459255339794 Test RE 1.000004290230442\n",
      "128 Train Loss 1.4254394 Test MSE 14258.459255339794 Test RE 1.000004290230442\n",
      "129 Train Loss 1.4255847 Test MSE 14258.459255339794 Test RE 1.000004290230442\n",
      "130 Train Loss 1.4257196 Test MSE 14258.459255339794 Test RE 1.000004290230442\n",
      "131 Train Loss 1.425863 Test MSE 14258.459094215092 Test RE 1.0000042845802737\n",
      "132 Train Loss 1.425994 Test MSE 14258.458679091395 Test RE 1.0000042700231089\n",
      "133 Train Loss 1.4260577 Test MSE 14258.458679091395 Test RE 1.0000042700231089\n",
      "134 Train Loss 1.4262092 Test MSE 14258.458679091395 Test RE 1.0000042700231089\n",
      "135 Train Loss 1.4263572 Test MSE 14258.458679091395 Test RE 1.0000042700231089\n",
      "136 Train Loss 1.4264787 Test MSE 14258.458679091395 Test RE 1.0000042700231089\n",
      "137 Train Loss 1.4265492 Test MSE 14258.458679091395 Test RE 1.0000042700231089\n",
      "138 Train Loss 1.4266872 Test MSE 14258.458679091395 Test RE 1.0000042700231089\n",
      "139 Train Loss 1.4268086 Test MSE 14258.458679091395 Test RE 1.0000042700231089\n",
      "140 Train Loss 1.4269612 Test MSE 14258.458684484225 Test RE 1.0000042702122196\n",
      "141 Train Loss 1.4270518 Test MSE 14258.452584737108 Test RE 1.0000040563120374\n",
      "142 Train Loss 1.4271429 Test MSE 14258.452584737108 Test RE 1.0000040563120374\n",
      "143 Train Loss 1.4272975 Test MSE 14258.452584737108 Test RE 1.0000040563120374\n",
      "144 Train Loss 1.4273423 Test MSE 14258.452584737108 Test RE 1.0000040563120374\n",
      "145 Train Loss 1.4274575 Test MSE 14258.452584737108 Test RE 1.0000040563120374\n",
      "146 Train Loss 1.4275895 Test MSE 14258.452584737108 Test RE 1.0000040563120374\n",
      "147 Train Loss 1.4276776 Test MSE 14258.452584737108 Test RE 1.0000040563120374\n",
      "148 Train Loss 1.4277855 Test MSE 14258.452584737108 Test RE 1.0000040563120374\n",
      "149 Train Loss 1.4278772 Test MSE 14258.452584737108 Test RE 1.0000040563120374\n",
      "150 Train Loss 1.4279965 Test MSE 14258.452584737108 Test RE 1.0000040563120374\n",
      "151 Train Loss 1.4280922 Test MSE 14258.452584737108 Test RE 1.0000040563120374\n",
      "152 Train Loss 1.4281479 Test MSE 14258.435993091363 Test RE 1.000003474491613\n",
      "153 Train Loss 1.4281926 Test MSE 14258.435993091363 Test RE 1.000003474491613\n",
      "154 Train Loss 1.4283148 Test MSE 14258.435993091363 Test RE 1.000003474491613\n",
      "155 Train Loss 1.4284573 Test MSE 14258.435993091363 Test RE 1.000003474491613\n",
      "156 Train Loss 1.428467 Test MSE 14258.435993091363 Test RE 1.000003474491613\n",
      "157 Train Loss 1.4285462 Test MSE 14258.435993091363 Test RE 1.000003474491613\n",
      "158 Train Loss 1.4286575 Test MSE 14258.435993091363 Test RE 1.000003474491613\n",
      "159 Train Loss 1.4287575 Test MSE 14258.39350863069 Test RE 1.0000019846844572\n",
      "160 Train Loss 1.4288167 Test MSE 14258.39350863069 Test RE 1.0000019846844572\n",
      "161 Train Loss 1.4289271 Test MSE 14258.39350863069 Test RE 1.0000019846844572\n",
      "162 Train Loss 1.4290389 Test MSE 14258.39350863069 Test RE 1.0000019846844572\n",
      "163 Train Loss 1.4290694 Test MSE 14258.39350863069 Test RE 1.0000019846844572\n",
      "164 Train Loss 1.4291605 Test MSE 14258.39350863069 Test RE 1.0000019846844572\n",
      "165 Train Loss 1.429253 Test MSE 14258.39350863069 Test RE 1.0000019846844572\n",
      "166 Train Loss 1.4292802 Test MSE 14258.39350863069 Test RE 1.0000019846844572\n",
      "167 Train Loss 1.4293807 Test MSE 14258.39350863069 Test RE 1.0000019846844572\n",
      "168 Train Loss 1.4294275 Test MSE 14258.39350863069 Test RE 1.0000019846844572\n",
      "169 Train Loss 1.4295336 Test MSE 14258.39350863069 Test RE 1.0000019846844572\n",
      "170 Train Loss 1.4296017 Test MSE 14258.39350863069 Test RE 1.0000019846844572\n",
      "171 Train Loss 1.4296609 Test MSE 14258.39350863069 Test RE 1.0000019846844572\n",
      "172 Train Loss 1.4297283 Test MSE 14258.378085117078 Test RE 1.0000014438258962\n",
      "173 Train Loss 1.4297944 Test MSE 14258.378085117078 Test RE 1.0000014438258962\n",
      "174 Train Loss 1.4298605 Test MSE 14258.347075452552 Test RE 1.0000003564046633\n",
      "175 Train Loss 1.4299115 Test MSE 14258.347075452552 Test RE 1.0000003564046633\n",
      "176 Train Loss 1.4300065 Test MSE 14258.347075452552 Test RE 1.0000003564046633\n",
      "177 Train Loss 1.4300756 Test MSE 14258.347075452552 Test RE 1.0000003564046633\n",
      "178 Train Loss 1.4301369 Test MSE 14258.347075452552 Test RE 1.0000003564046633\n",
      "179 Train Loss 1.4302092 Test MSE 14258.347075452552 Test RE 1.0000003564046633\n",
      "180 Train Loss 1.4302589 Test MSE 14258.347075452552 Test RE 1.0000003564046633\n",
      "181 Train Loss 1.430339 Test MSE 14258.347075452552 Test RE 1.0000003564046633\n",
      "182 Train Loss 1.4303452 Test MSE 14258.347075452552 Test RE 1.0000003564046633\n",
      "183 Train Loss 1.4304544 Test MSE 14258.347958488512 Test RE 1.0000003873702532\n",
      "184 Train Loss 1.4305043 Test MSE 14258.347958488512 Test RE 1.0000003873702532\n",
      "185 Train Loss 1.4305534 Test MSE 14258.347958488512 Test RE 1.0000003873702532\n",
      "186 Train Loss 1.4305968 Test MSE 14258.347958488512 Test RE 1.0000003873702532\n",
      "187 Train Loss 1.4306511 Test MSE 14258.347958488512 Test RE 1.0000003873702532\n",
      "188 Train Loss 1.4306928 Test MSE 14258.347958488512 Test RE 1.0000003873702532\n",
      "189 Train Loss 1.430719 Test MSE 14258.347958488512 Test RE 1.0000003873702532\n",
      "190 Train Loss 1.4308355 Test MSE 14258.347958488512 Test RE 1.0000003873702532\n",
      "191 Train Loss 1.4308895 Test MSE 14258.347958488512 Test RE 1.0000003873702532\n",
      "192 Train Loss 1.4309297 Test MSE 14258.354744145727 Test RE 1.000000625324154\n",
      "193 Train Loss 1.4309671 Test MSE 14258.354744145727 Test RE 1.000000625324154\n",
      "194 Train Loss 1.431014 Test MSE 14258.354744145727 Test RE 1.000000625324154\n",
      "195 Train Loss 1.4310561 Test MSE 14258.359901505914 Test RE 1.0000008061782233\n",
      "196 Train Loss 1.4311455 Test MSE 14258.359901505914 Test RE 1.0000008061782233\n",
      "197 Train Loss 1.4311957 Test MSE 14258.359901505914 Test RE 1.0000008061782233\n",
      "198 Train Loss 1.4312485 Test MSE 14258.359901505914 Test RE 1.0000008061782233\n",
      "199 Train Loss 1.4313012 Test MSE 14258.360262744436 Test RE 1.0000008188458376\n",
      "Training time: 89.12\n",
      "Training time: 89.12\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (m_lambda): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 18.419518 Test MSE 14253.029456072472 Test RE 0.9998138650414483\n",
      "1 Train Loss 2.5757625 Test MSE 14256.087153485214 Test RE 0.9999211041492537\n",
      "2 Train Loss 1.9928493 Test MSE 14257.435701649827 Test RE 0.9999683965748658\n",
      "3 Train Loss 1.9827318 Test MSE 14258.578241845255 Test RE 1.0000084627278198\n",
      "4 Train Loss 2.015317 Test MSE 14259.002795891696 Test RE 1.0000233504140894\n",
      "5 Train Loss 2.0289476 Test MSE 14258.246264547526 Test RE 0.9999968212431677\n",
      "6 Train Loss 2.0524104 Test MSE 14258.288386507644 Test RE 0.9999982983461132\n",
      "7 Train Loss 2.078461 Test MSE 14258.30736258583 Test RE 0.9999989637850167\n",
      "8 Train Loss 2.1001017 Test MSE 14258.880157547195 Test RE 1.0000190499214952\n",
      "9 Train Loss 2.1112108 Test MSE 14259.56346006216 Test RE 1.0000430106862304\n",
      "10 Train Loss 2.1312973 Test MSE 14259.547024446947 Test RE 1.0000424343597918\n",
      "11 Train Loss 2.1494832 Test MSE 14259.545629177526 Test RE 1.0000423854336702\n",
      "12 Train Loss 2.1664805 Test MSE 14259.545629177526 Test RE 1.0000423854336702\n",
      "13 Train Loss 2.1822555 Test MSE 14259.544813699136 Test RE 1.0000423568383352\n",
      "14 Train Loss 2.1963868 Test MSE 14259.540946334957 Test RE 1.0000422212264242\n",
      "15 Train Loss 2.2096334 Test MSE 14259.5404162016 Test RE 1.0000422026369145\n",
      "16 Train Loss 2.222166 Test MSE 14259.539764163792 Test RE 1.000042179772737\n",
      "17 Train Loss 2.229653 Test MSE 14259.302090380434 Test RE 1.0000338455346502\n",
      "18 Train Loss 2.2013242 Test MSE 14260.147944149641 Test RE 1.0000635058170493\n",
      "19 Train Loss 2.1905167 Test MSE 14260.94949630668 Test RE 1.000091611829671\n",
      "20 Train Loss 2.1854343 Test MSE 14261.226182504806 Test RE 1.0001013135049968\n",
      "21 Train Loss 2.1834543 Test MSE 14261.941814838254 Test RE 1.000126405872552\n",
      "22 Train Loss 2.186895 Test MSE 14262.531869092696 Test RE 1.0001470945958955\n",
      "23 Train Loss 2.1752706 Test MSE 14262.202591115383 Test RE 1.0001355493696988\n",
      "24 Train Loss 2.1749105 Test MSE 14261.758354049383 Test RE 1.0001199732081982\n",
      "25 Train Loss 2.1787677 Test MSE 14261.570731186366 Test RE 1.0001133945667282\n",
      "26 Train Loss 2.1694295 Test MSE 14262.325566831754 Test RE 1.0001398611903924\n",
      "27 Train Loss 2.166231 Test MSE 14262.95210736596 Test RE 1.0001618289009953\n",
      "28 Train Loss 2.1699731 Test MSE 14263.157584503075 Test RE 1.0001690332177295\n",
      "29 Train Loss 2.1732702 Test MSE 14263.232200605322 Test RE 1.0001716493501251\n",
      "30 Train Loss 2.1770163 Test MSE 14263.5193010468 Test RE 1.0001817153811996\n",
      "31 Train Loss 2.1680624 Test MSE 14264.194808273656 Test RE 1.000205398946368\n",
      "32 Train Loss 2.1404283 Test MSE 14265.338089765233 Test RE 1.0002454815946826\n",
      "33 Train Loss 2.120035 Test MSE 14265.411937942692 Test RE 1.000248070604833\n",
      "34 Train Loss 2.102366 Test MSE 14264.761862723448 Test RE 1.0002252796789692\n",
      "35 Train Loss 2.0940204 Test MSE 14264.196418870542 Test RE 1.0002054554138966\n",
      "36 Train Loss 2.0899973 Test MSE 14263.930642803132 Test RE 1.0001961372615449\n",
      "37 Train Loss 2.0883431 Test MSE 14263.957585127 Test RE 1.0001970818677761\n",
      "38 Train Loss 2.087487 Test MSE 14264.48096438938 Test RE 1.000215431530061\n",
      "39 Train Loss 2.0697637 Test MSE 14265.334903988298 Test RE 1.0002453699058063\n",
      "40 Train Loss 2.020404 Test MSE 14265.716083733338 Test RE 1.0002587334451378\n",
      "41 Train Loss 1.9425683 Test MSE 14264.353080084338 Test RE 1.0002109479409853\n",
      "42 Train Loss 1.8364011 Test MSE 14262.991508440284 Test RE 1.0001632103619664\n",
      "43 Train Loss 1.7517084 Test MSE 14263.529503141806 Test RE 1.0001820730750777\n",
      "44 Train Loss 1.6799937 Test MSE 14264.95218042894 Test RE 1.0002319520632368\n",
      "45 Train Loss 1.65141 Test MSE 14265.79126354312 Test RE 1.000261369105409\n",
      "46 Train Loss 1.6133807 Test MSE 14265.41797913698 Test RE 1.0002482824000591\n",
      "47 Train Loss 1.595268 Test MSE 14264.690731497458 Test RE 1.0002227858644988\n",
      "48 Train Loss 1.585 Test MSE 14263.913898039329 Test RE 1.0001955501844828\n",
      "49 Train Loss 1.5775192 Test MSE 14263.194519489818 Test RE 1.0001703282046863\n",
      "50 Train Loss 1.5673405 Test MSE 14263.268751695372 Test RE 1.000172930873839\n",
      "51 Train Loss 1.5576564 Test MSE 14263.72305453584 Test RE 1.00018885912259\n",
      "52 Train Loss 1.543416 Test MSE 14264.453076097527 Test RE 1.0002144537758206\n",
      "53 Train Loss 1.5342151 Test MSE 14265.092152641164 Test RE 1.0002368593469715\n",
      "54 Train Loss 1.5191101 Test MSE 14264.961739951726 Test RE 1.000232287211172\n",
      "55 Train Loss 1.5054352 Test MSE 14265.1208851706 Test RE 1.0002378666773213\n",
      "56 Train Loss 1.4927566 Test MSE 14265.66372462615 Test RE 1.0002568978309287\n",
      "57 Train Loss 1.4869251 Test MSE 14266.186598071656 Test RE 1.0002752286632106\n",
      "58 Train Loss 1.4789746 Test MSE 14266.731293898334 Test RE 1.0002943241845772\n",
      "59 Train Loss 1.4657286 Test MSE 14266.91550003378 Test RE 1.0003007818570882\n",
      "60 Train Loss 1.4519331 Test MSE 14266.750093254079 Test RE 1.0002949832312176\n",
      "61 Train Loss 1.4427342 Test MSE 14266.698593678371 Test RE 1.0002931778160609\n",
      "62 Train Loss 1.4327741 Test MSE 14266.369395876663 Test RE 1.0002816370866958\n",
      "63 Train Loss 1.4181793 Test MSE 14266.314646532712 Test RE 1.000279717719041\n",
      "64 Train Loss 1.4149194 Test MSE 14266.35583783365 Test RE 1.0002811617777962\n",
      "65 Train Loss 1.4152147 Test MSE 14266.371673829577 Test RE 1.0002817169456397\n",
      "66 Train Loss 1.415861 Test MSE 14266.367518171457 Test RE 1.0002815712593627\n",
      "67 Train Loss 1.416278 Test MSE 14266.36628340379 Test RE 1.0002815279717014\n",
      "68 Train Loss 1.4167154 Test MSE 14266.366147180952 Test RE 1.000281523196092\n",
      "69 Train Loss 1.4167341 Test MSE 14266.34378878809 Test RE 1.0002807393701407\n",
      "70 Train Loss 1.4171498 Test MSE 14266.365673267064 Test RE 1.0002815065819344\n",
      "71 Train Loss 1.4175235 Test MSE 14266.371020863147 Test RE 1.0002816940543813\n",
      "72 Train Loss 1.4177384 Test MSE 14266.373930058951 Test RE 1.0002817960430075\n",
      "73 Train Loss 1.4182651 Test MSE 14266.383968171729 Test RE 1.0002821479523212\n",
      "74 Train Loss 1.4186138 Test MSE 14266.383968171729 Test RE 1.0002821479523212\n",
      "75 Train Loss 1.419073 Test MSE 14266.383968171729 Test RE 1.0002821479523212\n",
      "76 Train Loss 1.4194126 Test MSE 14266.38397489884 Test RE 1.0002821481881556\n",
      "77 Train Loss 1.4198703 Test MSE 14266.387691115751 Test RE 1.0002822784687226\n",
      "78 Train Loss 1.4203243 Test MSE 14266.388732332363 Test RE 1.0002823149709728\n",
      "79 Train Loss 1.4205341 Test MSE 14266.388732332363 Test RE 1.0002823149709728\n",
      "80 Train Loss 1.4208025 Test MSE 14266.388732332363 Test RE 1.0002823149709728\n",
      "81 Train Loss 1.4212588 Test MSE 14266.388732332363 Test RE 1.0002823149709728\n",
      "82 Train Loss 1.421549 Test MSE 14266.388732332363 Test RE 1.0002823149709728\n",
      "83 Train Loss 1.421965 Test MSE 14266.388715548841 Test RE 1.000282314382588\n",
      "84 Train Loss 1.4223351 Test MSE 14266.388715548841 Test RE 1.000282314382588\n",
      "85 Train Loss 1.4225678 Test MSE 14266.388715548841 Test RE 1.000282314382588\n",
      "86 Train Loss 1.4229313 Test MSE 14266.388715548841 Test RE 1.000282314382588\n",
      "87 Train Loss 1.4232173 Test MSE 14266.388715548841 Test RE 1.000282314382588\n",
      "88 Train Loss 1.4234136 Test MSE 14266.388715548841 Test RE 1.000282314382588\n",
      "89 Train Loss 1.4237827 Test MSE 14266.38870669442 Test RE 1.000282314072176\n",
      "90 Train Loss 1.4240004 Test MSE 14266.38870669442 Test RE 1.000282314072176\n",
      "91 Train Loss 1.4244324 Test MSE 14266.38870669442 Test RE 1.000282314072176\n",
      "92 Train Loss 1.4245511 Test MSE 14266.38870669442 Test RE 1.000282314072176\n",
      "93 Train Loss 1.4248695 Test MSE 14266.38870669442 Test RE 1.000282314072176\n",
      "94 Train Loss 1.4250073 Test MSE 14266.38870669442 Test RE 1.000282314072176\n",
      "95 Train Loss 1.4254477 Test MSE 14266.38870669442 Test RE 1.000282314072176\n",
      "96 Train Loss 1.4255859 Test MSE 14266.38870669442 Test RE 1.000282314072176\n",
      "97 Train Loss 1.4260014 Test MSE 14266.38870669442 Test RE 1.000282314072176\n",
      "98 Train Loss 1.425981 Test MSE 14266.38870669442 Test RE 1.000282314072176\n",
      "99 Train Loss 1.4262707 Test MSE 14266.38870669442 Test RE 1.000282314072176\n",
      "100 Train Loss 1.4265829 Test MSE 14266.38870669442 Test RE 1.000282314072176\n",
      "101 Train Loss 1.4267577 Test MSE 14266.38870669442 Test RE 1.000282314072176\n",
      "102 Train Loss 1.4268619 Test MSE 14266.399347226403 Test RE 1.000282687100494\n",
      "103 Train Loss 1.4273714 Test MSE 14266.399347226403 Test RE 1.000282687100494\n",
      "104 Train Loss 1.4273009 Test MSE 14266.399347226403 Test RE 1.000282687100494\n",
      "105 Train Loss 1.4278061 Test MSE 14266.399347226403 Test RE 1.000282687100494\n",
      "106 Train Loss 1.4277585 Test MSE 14266.399347226403 Test RE 1.000282687100494\n",
      "107 Train Loss 1.428089 Test MSE 14266.395692499116 Test RE 1.00028255897564\n",
      "108 Train Loss 1.4282447 Test MSE 14266.395692499116 Test RE 1.00028255897564\n",
      "109 Train Loss 1.4283725 Test MSE 14266.395692499116 Test RE 1.00028255897564\n",
      "110 Train Loss 1.4285316 Test MSE 14266.395692499116 Test RE 1.00028255897564\n",
      "111 Train Loss 1.4288051 Test MSE 14266.395692499116 Test RE 1.00028255897564\n",
      "112 Train Loss 1.4291334 Test MSE 14266.395692499116 Test RE 1.00028255897564\n",
      "113 Train Loss 1.4290262 Test MSE 14266.395352735202 Test RE 1.0002825470644352\n",
      "114 Train Loss 1.4293852 Test MSE 14266.395264499968 Test RE 1.0002825439711462\n",
      "115 Train Loss 1.4296769 Test MSE 14266.395264499968 Test RE 1.0002825439711462\n",
      "116 Train Loss 1.4296274 Test MSE 14266.395264499968 Test RE 1.0002825439711462\n",
      "117 Train Loss 1.4297682 Test MSE 14266.395264499968 Test RE 1.0002825439711462\n",
      "118 Train Loss 1.4300894 Test MSE 14266.395264499968 Test RE 1.0002825439711462\n",
      "119 Train Loss 1.430225 Test MSE 14266.395264499968 Test RE 1.0002825439711462\n",
      "120 Train Loss 1.4302274 Test MSE 14266.395264499968 Test RE 1.0002825439711462\n",
      "121 Train Loss 1.4306446 Test MSE 14266.395264499968 Test RE 1.0002825439711462\n",
      "122 Train Loss 1.4306265 Test MSE 14266.395264499968 Test RE 1.0002825439711462\n",
      "123 Train Loss 1.4308571 Test MSE 14266.395264499968 Test RE 1.0002825439711462\n",
      "124 Train Loss 1.4310207 Test MSE 14266.395264499968 Test RE 1.0002825439711462\n",
      "125 Train Loss 1.4311612 Test MSE 14266.395716654402 Test RE 1.000282559822459\n",
      "126 Train Loss 1.4312087 Test MSE 14266.395716654402 Test RE 1.000282559822459\n",
      "127 Train Loss 1.4313844 Test MSE 14266.395716654402 Test RE 1.000282559822459\n",
      "128 Train Loss 1.4314774 Test MSE 14266.395716654402 Test RE 1.000282559822459\n",
      "129 Train Loss 1.4316328 Test MSE 14266.395716654402 Test RE 1.000282559822459\n",
      "130 Train Loss 1.4318436 Test MSE 14266.395716654402 Test RE 1.000282559822459\n",
      "131 Train Loss 1.4319478 Test MSE 14266.395716654402 Test RE 1.000282559822459\n",
      "132 Train Loss 1.4320408 Test MSE 14266.395716654402 Test RE 1.000282559822459\n",
      "133 Train Loss 1.4322529 Test MSE 14266.395716654402 Test RE 1.000282559822459\n",
      "134 Train Loss 1.4323338 Test MSE 14266.395660217135 Test RE 1.0002825578439207\n",
      "135 Train Loss 1.4324936 Test MSE 14266.395660217135 Test RE 1.0002825578439207\n",
      "136 Train Loss 1.4326279 Test MSE 14266.395660217135 Test RE 1.0002825578439207\n",
      "137 Train Loss 1.432703 Test MSE 14266.395660217135 Test RE 1.0002825578439207\n",
      "138 Train Loss 1.4328163 Test MSE 14266.395660217135 Test RE 1.0002825578439207\n",
      "139 Train Loss 1.4328197 Test MSE 14266.395660217135 Test RE 1.0002825578439207\n",
      "140 Train Loss 1.432968 Test MSE 14266.395691771266 Test RE 1.0002825589501234\n",
      "141 Train Loss 1.4330599 Test MSE 14266.395691771266 Test RE 1.0002825589501234\n",
      "142 Train Loss 1.4332662 Test MSE 14266.395691771266 Test RE 1.0002825589501234\n",
      "143 Train Loss 1.4331995 Test MSE 14266.395691771266 Test RE 1.0002825589501234\n",
      "144 Train Loss 1.4334011 Test MSE 14266.395691771266 Test RE 1.0002825589501234\n",
      "145 Train Loss 1.4335129 Test MSE 14266.395691771266 Test RE 1.0002825589501234\n",
      "146 Train Loss 1.4336032 Test MSE 14266.395691771266 Test RE 1.0002825589501234\n",
      "147 Train Loss 1.4338164 Test MSE 14266.395691771266 Test RE 1.0002825589501234\n",
      "148 Train Loss 1.4339311 Test MSE 14266.395691771266 Test RE 1.0002825589501234\n",
      "149 Train Loss 1.4339136 Test MSE 14266.395691771266 Test RE 1.0002825589501234\n",
      "150 Train Loss 1.4340732 Test MSE 14266.395691771266 Test RE 1.0002825589501234\n",
      "151 Train Loss 1.4342927 Test MSE 14266.395691771266 Test RE 1.0002825589501234\n",
      "152 Train Loss 1.4342974 Test MSE 14266.395691771266 Test RE 1.0002825589501234\n",
      "153 Train Loss 1.4343399 Test MSE 14266.395528088931 Test RE 1.000282553211863\n",
      "154 Train Loss 1.4345584 Test MSE 14266.395528088931 Test RE 1.000282553211863\n",
      "155 Train Loss 1.4346176 Test MSE 14266.395528088931 Test RE 1.000282553211863\n",
      "156 Train Loss 1.4348534 Test MSE 14266.395528088931 Test RE 1.000282553211863\n",
      "157 Train Loss 1.4348271 Test MSE 14266.395528088931 Test RE 1.000282553211863\n",
      "158 Train Loss 1.4347633 Test MSE 14266.395528088931 Test RE 1.000282553211863\n",
      "159 Train Loss 1.4347701 Test MSE 14266.395528088931 Test RE 1.000282553211863\n",
      "160 Train Loss 1.4349713 Test MSE 14266.395519862323 Test RE 1.0002825529234605\n",
      "161 Train Loss 1.4352506 Test MSE 14266.395519862323 Test RE 1.0002825529234605\n",
      "162 Train Loss 1.4351827 Test MSE 14266.395519862323 Test RE 1.0002825529234605\n",
      "163 Train Loss 1.435326 Test MSE 14266.395519862323 Test RE 1.0002825529234605\n",
      "164 Train Loss 1.4354274 Test MSE 14266.395150280932 Test RE 1.0002825399669355\n",
      "165 Train Loss 1.4352994 Test MSE 14266.395150280932 Test RE 1.0002825399669355\n",
      "166 Train Loss 1.4354235 Test MSE 14266.395213454498 Test RE 1.0002825421816302\n",
      "167 Train Loss 1.4354461 Test MSE 14266.395213454498 Test RE 1.0002825421816302\n",
      "168 Train Loss 1.4355967 Test MSE 14266.395213454498 Test RE 1.0002825421816302\n",
      "169 Train Loss 1.4358304 Test MSE 14266.395213454498 Test RE 1.0002825421816302\n",
      "170 Train Loss 1.4358501 Test MSE 14266.395213454498 Test RE 1.0002825421816302\n",
      "171 Train Loss 1.4358815 Test MSE 14266.395213454498 Test RE 1.0002825421816302\n",
      "172 Train Loss 1.4358435 Test MSE 14266.395213454498 Test RE 1.0002825421816302\n",
      "173 Train Loss 1.4358732 Test MSE 14266.395213454498 Test RE 1.0002825421816302\n",
      "174 Train Loss 1.436078 Test MSE 14266.395213454498 Test RE 1.0002825421816302\n",
      "175 Train Loss 1.4360706 Test MSE 14266.395213454498 Test RE 1.0002825421816302\n",
      "176 Train Loss 1.4361707 Test MSE 14266.394106689311 Test RE 1.0002825033814322\n",
      "177 Train Loss 1.4363891 Test MSE 14266.394106689311 Test RE 1.0002825033814322\n",
      "178 Train Loss 1.4363304 Test MSE 14266.390090908433 Test RE 1.0002823625989892\n",
      "179 Train Loss 1.4362533 Test MSE 14266.388316369477 Test RE 1.0002823003884345\n",
      "180 Train Loss 1.4360839 Test MSE 14266.349536975953 Test RE 1.00028094088642\n",
      "181 Train Loss 1.4361392 Test MSE 14266.349536975953 Test RE 1.00028094088642\n",
      "182 Train Loss 1.4363452 Test MSE 14266.34985285053 Test RE 1.0002809519601465\n",
      "183 Train Loss 1.436356 Test MSE 14266.35051585289 Test RE 1.0002809752032547\n",
      "184 Train Loss 1.4363714 Test MSE 14266.35084738549 Test RE 1.0002809868259102\n",
      "185 Train Loss 1.436604 Test MSE 14266.350781601814 Test RE 1.0002809845197087\n",
      "186 Train Loss 1.4365474 Test MSE 14266.350781601814 Test RE 1.0002809845197087\n",
      "187 Train Loss 1.436591 Test MSE 14266.350878822612 Test RE 1.0002809879280123\n",
      "188 Train Loss 1.4367095 Test MSE 14266.350878822612 Test RE 1.0002809879280123\n",
      "189 Train Loss 1.4366072 Test MSE 14266.350878822612 Test RE 1.0002809879280123\n",
      "190 Train Loss 1.4367656 Test MSE 14266.350839688443 Test RE 1.000280986556072\n",
      "191 Train Loss 1.4368025 Test MSE 14266.35103157842 Test RE 1.0002809932832266\n",
      "192 Train Loss 1.4366657 Test MSE 14266.35103157842 Test RE 1.0002809932832266\n",
      "193 Train Loss 1.4368794 Test MSE 14266.35103157842 Test RE 1.0002809932832266\n",
      "194 Train Loss 1.4368612 Test MSE 14266.350380364112 Test RE 1.0002809704533768\n",
      "195 Train Loss 1.4371328 Test MSE 14266.348157100689 Test RE 1.0002808925116424\n",
      "196 Train Loss 1.4368857 Test MSE 14266.32327112752 Test RE 1.0002800200748412\n",
      "197 Train Loss 1.4369696 Test MSE 14266.318340848173 Test RE 1.000279847232117\n",
      "198 Train Loss 1.4369907 Test MSE 14266.300925752017 Test RE 1.000279236704067\n",
      "199 Train Loss 1.4371048 Test MSE 14266.299317278865 Test RE 1.000279180315156\n",
      "Training time: 102.55\n",
      "Training time: 102.55\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (m_lambda): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 1.8415558 Test MSE 14259.712779229803 Test RE 1.000048246652937\n",
      "1 Train Loss 1.8831044 Test MSE 14259.785175505158 Test RE 1.0000507852621452\n",
      "2 Train Loss 1.9180372 Test MSE 14259.426492163355 Test RE 1.0000382078001115\n",
      "3 Train Loss 1.9550029 Test MSE 14259.19858273314 Test RE 1.0000302159271925\n",
      "4 Train Loss 1.9871956 Test MSE 14258.552856940436 Test RE 1.0000075725573379\n",
      "5 Train Loss 2.0015562 Test MSE 14258.972905216766 Test RE 1.000022302255639\n",
      "6 Train Loss 2.009576 Test MSE 14260.381056022761 Test RE 1.000071679846003\n",
      "7 Train Loss 2.0344725 Test MSE 14260.249707183631 Test RE 1.0000670741290814\n",
      "8 Train Loss 2.0545053 Test MSE 14260.267098186687 Test RE 1.0000676839418334\n",
      "9 Train Loss 2.0632718 Test MSE 14261.24608839535 Test RE 1.0001020114779504\n",
      "10 Train Loss 2.060511 Test MSE 14262.189884636262 Test RE 1.0001351038493025\n",
      "11 Train Loss 2.0740058 Test MSE 14261.815557501535 Test RE 1.0001219789306428\n",
      "12 Train Loss 2.0745609 Test MSE 14261.717265297424 Test RE 1.0001185325123845\n",
      "13 Train Loss 2.06202 Test MSE 14262.671002509178 Test RE 1.0001519728862356\n",
      "14 Train Loss 2.0519469 Test MSE 14262.132458281489 Test RE 1.0001330903376477\n",
      "15 Train Loss 2.0629647 Test MSE 14261.776674741956 Test RE 1.0001206155863755\n",
      "16 Train Loss 2.0686913 Test MSE 14261.806027349427 Test RE 1.0001216447755648\n",
      "17 Train Loss 2.0685198 Test MSE 14262.55026945311 Test RE 1.0001477397499694\n",
      "18 Train Loss 2.0672765 Test MSE 14263.286614921964 Test RE 1.0001735571787351\n",
      "19 Train Loss 2.0732067 Test MSE 14263.488763176587 Test RE 1.0001806446974815\n",
      "20 Train Loss 2.0755255 Test MSE 14263.898210608806 Test RE 1.000195000177569\n",
      "21 Train Loss 2.0292497 Test MSE 14264.95766310965 Test RE 1.0002321442809126\n",
      "22 Train Loss 1.8737961 Test MSE 14268.237864095221 Test RE 1.000347138448852\n",
      "23 Train Loss 1.7389114 Test MSE 14267.901036176605 Test RE 1.000335330864891\n",
      "24 Train Loss 1.6465071 Test MSE 14267.316345003852 Test RE 1.0003148340437928\n",
      "25 Train Loss 1.5653443 Test MSE 14266.423689555568 Test RE 1.0002835404762984\n",
      "26 Train Loss 1.4298251 Test MSE 14266.45896302863 Test RE 1.0002847770669634\n",
      "27 Train Loss 1.3962994 Test MSE 14266.960732271182 Test RE 1.0003023675469516\n",
      "28 Train Loss 1.3890126 Test MSE 14266.880632569948 Test RE 1.0002995595195536\n",
      "29 Train Loss 1.3772235 Test MSE 14265.76445906882 Test RE 1.0002604293926283\n",
      "30 Train Loss 1.3688538 Test MSE 14264.533835238113 Test RE 1.0002172851618165\n",
      "31 Train Loss 1.3705614 Test MSE 14264.500987406112 Test RE 1.0002161335297814\n",
      "32 Train Loss 1.3725237 Test MSE 14264.493245053207 Test RE 1.000215862085749\n",
      "33 Train Loss 1.3744061 Test MSE 14264.488028266103 Test RE 1.0002156791870787\n",
      "34 Train Loss 1.3762335 Test MSE 14264.481957620976 Test RE 1.000215466352413\n",
      "35 Train Loss 1.3779618 Test MSE 14264.474099187133 Test RE 1.0002151908384413\n",
      "36 Train Loss 1.3796709 Test MSE 14264.461400565586 Test RE 1.0002147456290031\n",
      "37 Train Loss 1.3812782 Test MSE 14264.461400565586 Test RE 1.0002147456290031\n",
      "38 Train Loss 1.3828218 Test MSE 14264.461366188314 Test RE 1.0002147444237472\n",
      "39 Train Loss 1.3843422 Test MSE 14264.441183786892 Test RE 1.0002140368350727\n",
      "40 Train Loss 1.385806 Test MSE 14264.441183786892 Test RE 1.0002140368350727\n",
      "41 Train Loss 1.3872087 Test MSE 14264.441183786892 Test RE 1.0002140368350727\n",
      "42 Train Loss 1.3885387 Test MSE 14264.43466581342 Test RE 1.0002138083168566\n",
      "43 Train Loss 1.3898457 Test MSE 14264.43466581342 Test RE 1.0002138083168566\n",
      "44 Train Loss 1.3910838 Test MSE 14264.43466581342 Test RE 1.0002138083168566\n",
      "45 Train Loss 1.3922937 Test MSE 14264.427273311918 Test RE 1.000213549137884\n",
      "46 Train Loss 1.3934947 Test MSE 14264.427273311918 Test RE 1.000213549137884\n",
      "47 Train Loss 1.3946148 Test MSE 14264.427273311918 Test RE 1.000213549137884\n",
      "48 Train Loss 1.3957014 Test MSE 14264.427273311918 Test RE 1.000213549137884\n",
      "49 Train Loss 1.3967592 Test MSE 14264.425574592911 Test RE 1.0002134895812758\n",
      "50 Train Loss 1.3977727 Test MSE 14264.425574592911 Test RE 1.0002134895812758\n",
      "51 Train Loss 1.3987702 Test MSE 14264.425574592911 Test RE 1.0002134895812758\n",
      "52 Train Loss 1.3997096 Test MSE 14264.425574592911 Test RE 1.0002134895812758\n",
      "53 Train Loss 1.4006411 Test MSE 14264.425574592911 Test RE 1.0002134895812758\n",
      "54 Train Loss 1.4015225 Test MSE 14264.425574592911 Test RE 1.0002134895812758\n",
      "55 Train Loss 1.4024018 Test MSE 14264.425580042298 Test RE 1.00021348977233\n",
      "56 Train Loss 1.4032235 Test MSE 14264.425580042298 Test RE 1.00021348977233\n",
      "57 Train Loss 1.4040327 Test MSE 14264.425580042298 Test RE 1.00021348977233\n",
      "58 Train Loss 1.4048088 Test MSE 14264.425580042298 Test RE 1.00021348977233\n",
      "59 Train Loss 1.4055926 Test MSE 14264.425580042298 Test RE 1.00021348977233\n",
      "60 Train Loss 1.4063121 Test MSE 14264.425580042298 Test RE 1.00021348977233\n",
      "61 Train Loss 1.407014 Test MSE 14264.425580042298 Test RE 1.00021348977233\n",
      "62 Train Loss 1.4077128 Test MSE 14264.425580042298 Test RE 1.00021348977233\n",
      "63 Train Loss 1.408375 Test MSE 14264.425580042298 Test RE 1.00021348977233\n",
      "64 Train Loss 1.4090497 Test MSE 14264.425580042298 Test RE 1.00021348977233\n",
      "65 Train Loss 1.4096507 Test MSE 14264.425580042298 Test RE 1.00021348977233\n",
      "66 Train Loss 1.4102627 Test MSE 14264.425580042298 Test RE 1.00021348977233\n",
      "67 Train Loss 1.4108773 Test MSE 14264.425580042298 Test RE 1.00021348977233\n",
      "68 Train Loss 1.4114449 Test MSE 14264.425580042298 Test RE 1.00021348977233\n",
      "69 Train Loss 1.4120041 Test MSE 14264.425580042298 Test RE 1.00021348977233\n",
      "70 Train Loss 1.4125644 Test MSE 14264.425580042298 Test RE 1.00021348977233\n",
      "71 Train Loss 1.413091 Test MSE 14264.425580042298 Test RE 1.00021348977233\n",
      "72 Train Loss 1.4136208 Test MSE 14264.425580042298 Test RE 1.00021348977233\n",
      "73 Train Loss 1.4141153 Test MSE 14264.425580042298 Test RE 1.00021348977233\n",
      "74 Train Loss 1.4146008 Test MSE 14264.425580042298 Test RE 1.00021348977233\n",
      "75 Train Loss 1.41507 Test MSE 14264.425580042298 Test RE 1.00021348977233\n",
      "76 Train Loss 1.4155568 Test MSE 14264.42554391286 Test RE 1.000213488505642\n",
      "77 Train Loss 1.416008 Test MSE 14264.42554391286 Test RE 1.000213488505642\n",
      "78 Train Loss 1.4164335 Test MSE 14264.42554391286 Test RE 1.000213488505642\n",
      "79 Train Loss 1.4168389 Test MSE 14264.42554391286 Test RE 1.000213488505642\n",
      "80 Train Loss 1.4172632 Test MSE 14264.42554391286 Test RE 1.000213488505642\n",
      "81 Train Loss 1.4176788 Test MSE 14264.42554391286 Test RE 1.000213488505642\n",
      "82 Train Loss 1.4180595 Test MSE 14264.42554391286 Test RE 1.000213488505642\n",
      "83 Train Loss 1.4184808 Test MSE 14264.42554391286 Test RE 1.000213488505642\n",
      "84 Train Loss 1.4188427 Test MSE 14264.42554391286 Test RE 1.000213488505642\n",
      "85 Train Loss 1.4191965 Test MSE 14264.42554391286 Test RE 1.000213488505642\n",
      "86 Train Loss 1.4195434 Test MSE 14264.42554391286 Test RE 1.000213488505642\n",
      "87 Train Loss 1.4199097 Test MSE 14264.42554391286 Test RE 1.000213488505642\n",
      "88 Train Loss 1.4202635 Test MSE 14264.42554391286 Test RE 1.000213488505642\n",
      "89 Train Loss 1.4206024 Test MSE 14264.42554391286 Test RE 1.000213488505642\n",
      "90 Train Loss 1.4209043 Test MSE 14264.42554391286 Test RE 1.000213488505642\n",
      "91 Train Loss 1.4212619 Test MSE 14264.42554391286 Test RE 1.000213488505642\n",
      "92 Train Loss 1.4215229 Test MSE 14264.42554391286 Test RE 1.000213488505642\n",
      "93 Train Loss 1.4218298 Test MSE 14264.42554391286 Test RE 1.000213488505642\n",
      "94 Train Loss 1.4221475 Test MSE 14264.42554391286 Test RE 1.000213488505642\n",
      "95 Train Loss 1.4224106 Test MSE 14264.42554391286 Test RE 1.000213488505642\n",
      "96 Train Loss 1.4226979 Test MSE 14264.42554391286 Test RE 1.000213488505642\n",
      "97 Train Loss 1.4229712 Test MSE 14264.42554391286 Test RE 1.000213488505642\n",
      "98 Train Loss 1.4232534 Test MSE 14264.440367161382 Test RE 1.0002140082044275\n",
      "99 Train Loss 1.4235033 Test MSE 14264.440188907636 Test RE 1.0002140019549046\n",
      "100 Train Loss 1.4237407 Test MSE 14264.440188907636 Test RE 1.0002140019549046\n",
      "101 Train Loss 1.4240054 Test MSE 14264.435896161182 Test RE 1.0002138514524883\n",
      "102 Train Loss 1.4242727 Test MSE 14264.435896161182 Test RE 1.0002138514524883\n",
      "103 Train Loss 1.4244885 Test MSE 14264.456255106848 Test RE 1.000214565230879\n",
      "104 Train Loss 1.4247453 Test MSE 14264.456255106848 Test RE 1.000214565230879\n",
      "105 Train Loss 1.4249687 Test MSE 14264.456255106848 Test RE 1.000214565230879\n",
      "106 Train Loss 1.4251865 Test MSE 14264.455621813482 Test RE 1.0002145430278153\n",
      "107 Train Loss 1.4253844 Test MSE 14264.455621813482 Test RE 1.0002145430278153\n",
      "108 Train Loss 1.4255972 Test MSE 14264.456214837857 Test RE 1.000214563819061\n",
      "109 Train Loss 1.4258262 Test MSE 14264.456214837857 Test RE 1.000214563819061\n",
      "110 Train Loss 1.4260266 Test MSE 14264.454734833122 Test RE 1.0002145119305657\n",
      "111 Train Loss 1.42622 Test MSE 14264.45205610801 Test RE 1.0002144180153092\n",
      "112 Train Loss 1.4263304 Test MSE 14264.313677154736 Test RE 1.000209566480902\n",
      "113 Train Loss 1.426515 Test MSE 14264.30674499762 Test RE 1.0002093234404426\n",
      "114 Train Loss 1.4267026 Test MSE 14264.303672172695 Test RE 1.0002092157076112\n",
      "115 Train Loss 1.4268905 Test MSE 14264.298677002165 Test RE 1.000209040577575\n",
      "116 Train Loss 1.4270525 Test MSE 14264.289404339408 Test RE 1.0002087154791306\n",
      "117 Train Loss 1.427235 Test MSE 14264.287604624753 Test RE 1.0002086523813378\n",
      "118 Train Loss 1.4273835 Test MSE 14264.280844272322 Test RE 1.0002084153641106\n",
      "119 Train Loss 1.427562 Test MSE 14264.280844272322 Test RE 1.0002084153641106\n",
      "120 Train Loss 1.4277354 Test MSE 14264.277127973959 Test RE 1.000208285071067\n",
      "121 Train Loss 1.4279214 Test MSE 14264.277127973959 Test RE 1.000208285071067\n",
      "122 Train Loss 1.4280552 Test MSE 14264.282237600508 Test RE 1.0002084642140512\n",
      "123 Train Loss 1.428197 Test MSE 14264.282237600508 Test RE 1.0002084642140512\n",
      "124 Train Loss 1.4283772 Test MSE 14264.282237600508 Test RE 1.0002084642140512\n",
      "125 Train Loss 1.4285526 Test MSE 14264.282237600508 Test RE 1.0002084642140512\n",
      "126 Train Loss 1.4286808 Test MSE 14264.297755862728 Test RE 1.0002090082825414\n",
      "127 Train Loss 1.4288319 Test MSE 14264.297755862728 Test RE 1.0002090082825414\n",
      "128 Train Loss 1.4289589 Test MSE 14264.31152913757 Test RE 1.000209491171727\n",
      "129 Train Loss 1.4291043 Test MSE 14264.31152913757 Test RE 1.000209491171727\n",
      "130 Train Loss 1.4292241 Test MSE 14264.336961788367 Test RE 1.0002103828364708\n",
      "131 Train Loss 1.4293768 Test MSE 14264.336961788367 Test RE 1.0002103828364708\n",
      "132 Train Loss 1.4294941 Test MSE 14264.336961788367 Test RE 1.0002103828364708\n",
      "133 Train Loss 1.4296223 Test MSE 14264.328079399234 Test RE 1.0002100714213924\n",
      "134 Train Loss 1.4297707 Test MSE 14264.328079399234 Test RE 1.0002100714213924\n",
      "135 Train Loss 1.4298838 Test MSE 14264.328079399234 Test RE 1.0002100714213924\n",
      "136 Train Loss 1.4299974 Test MSE 14264.343114957963 Test RE 1.0002105985655112\n",
      "137 Train Loss 1.4301273 Test MSE 14264.343114957963 Test RE 1.0002105985655112\n",
      "138 Train Loss 1.4302492 Test MSE 14264.350688530889 Test RE 1.000210864093578\n",
      "139 Train Loss 1.4303641 Test MSE 14264.355154075984 Test RE 1.0002110206547306\n",
      "140 Train Loss 1.430468 Test MSE 14264.364854646583 Test RE 1.0002113607547867\n",
      "141 Train Loss 1.4304106 Test MSE 14264.417702748484 Test RE 1.0002132135965887\n",
      "142 Train Loss 1.429687 Test MSE 14263.635848767084 Test RE 1.0001858016329237\n",
      "143 Train Loss 1.429249 Test MSE 14263.026350210677 Test RE 1.0001644319652896\n",
      "144 Train Loss 1.4285728 Test MSE 14261.745676873461 Test RE 1.000119528708321\n",
      "145 Train Loss 1.4286302 Test MSE 14261.468705725205 Test RE 1.0001098172183873\n",
      "146 Train Loss 1.4287475 Test MSE 14261.439754274812 Test RE 1.000108802082953\n",
      "147 Train Loss 1.4288473 Test MSE 14261.439754274812 Test RE 1.000108802082953\n",
      "148 Train Loss 1.4289567 Test MSE 14261.439754274812 Test RE 1.000108802082953\n",
      "149 Train Loss 1.4290526 Test MSE 14261.400100371242 Test RE 1.0001074116817648\n",
      "150 Train Loss 1.4291524 Test MSE 14261.400100371242 Test RE 1.0001074116817648\n",
      "151 Train Loss 1.4292408 Test MSE 14261.400100371242 Test RE 1.0001074116817648\n",
      "152 Train Loss 1.4293376 Test MSE 14261.401576833112 Test RE 1.0001074634515912\n",
      "153 Train Loss 1.4294336 Test MSE 14261.401576833112 Test RE 1.0001074634515912\n",
      "154 Train Loss 1.4295144 Test MSE 14261.401576833112 Test RE 1.0001074634515912\n",
      "155 Train Loss 1.4296186 Test MSE 14261.401576833112 Test RE 1.0001074634515912\n",
      "156 Train Loss 1.4296963 Test MSE 14261.421590733193 Test RE 1.0001081652074357\n",
      "157 Train Loss 1.4297861 Test MSE 14261.421590733193 Test RE 1.0001081652074357\n",
      "158 Train Loss 1.4298681 Test MSE 14261.421590733193 Test RE 1.0001081652074357\n",
      "159 Train Loss 1.4299467 Test MSE 14261.421590733193 Test RE 1.0001081652074357\n",
      "160 Train Loss 1.4300362 Test MSE 14261.421590733193 Test RE 1.0001081652074357\n",
      "161 Train Loss 1.4301225 Test MSE 14261.421590733193 Test RE 1.0001081652074357\n",
      "162 Train Loss 1.4301976 Test MSE 14261.421590733193 Test RE 1.0001081652074357\n",
      "163 Train Loss 1.4302855 Test MSE 14261.421590733193 Test RE 1.0001081652074357\n",
      "164 Train Loss 1.4303569 Test MSE 14261.421590733193 Test RE 1.0001081652074357\n",
      "165 Train Loss 1.4304345 Test MSE 14261.421590733193 Test RE 1.0001081652074357\n",
      "166 Train Loss 1.4305092 Test MSE 14261.421590733193 Test RE 1.0001081652074357\n",
      "167 Train Loss 1.4305753 Test MSE 14261.437075550759 Test RE 1.0001087081578066\n",
      "168 Train Loss 1.4306505 Test MSE 14261.410708985133 Test RE 1.0001077836561616\n",
      "169 Train Loss 1.4307199 Test MSE 14261.410708985133 Test RE 1.0001077836561616\n",
      "170 Train Loss 1.4307925 Test MSE 14261.410708985133 Test RE 1.0001077836561616\n",
      "171 Train Loss 1.4308586 Test MSE 14261.410708985133 Test RE 1.0001077836561616\n",
      "172 Train Loss 1.430927 Test MSE 14261.410708985133 Test RE 1.0001077836561616\n",
      "173 Train Loss 1.4310024 Test MSE 14261.410708985133 Test RE 1.0001077836561616\n",
      "174 Train Loss 1.4310725 Test MSE 14261.409935672034 Test RE 1.000107756541156\n",
      "175 Train Loss 1.4311339 Test MSE 14261.409935672034 Test RE 1.000107756541156\n",
      "176 Train Loss 1.4311942 Test MSE 14261.409935672034 Test RE 1.000107756541156\n",
      "177 Train Loss 1.4312562 Test MSE 14261.418586161313 Test RE 1.0001080598568926\n",
      "178 Train Loss 1.4313066 Test MSE 14261.418586161313 Test RE 1.0001080598568926\n",
      "179 Train Loss 1.4313782 Test MSE 14261.44586169117 Test RE 1.0001090162296418\n",
      "180 Train Loss 1.4314382 Test MSE 14261.44586169117 Test RE 1.0001090162296418\n",
      "181 Train Loss 1.431504 Test MSE 14261.44586169117 Test RE 1.0001090162296418\n",
      "182 Train Loss 1.4315535 Test MSE 14261.44586169117 Test RE 1.0001090162296418\n",
      "183 Train Loss 1.4316037 Test MSE 14261.445656275462 Test RE 1.0001090090270726\n",
      "184 Train Loss 1.4316679 Test MSE 14261.445656275462 Test RE 1.0001090090270726\n",
      "185 Train Loss 1.4317261 Test MSE 14261.445638749616 Test RE 1.0001090084125572\n",
      "186 Train Loss 1.4317816 Test MSE 14261.445638749616 Test RE 1.0001090084125572\n",
      "187 Train Loss 1.431834 Test MSE 14261.445638749616 Test RE 1.0001090084125572\n",
      "188 Train Loss 1.431894 Test MSE 14261.445503982086 Test RE 1.000109003687152\n",
      "189 Train Loss 1.4319482 Test MSE 14261.445506168186 Test RE 1.0001090037638038\n",
      "190 Train Loss 1.4319984 Test MSE 14261.445506168186 Test RE 1.0001090037638038\n",
      "191 Train Loss 1.4320502 Test MSE 14261.445506168186 Test RE 1.0001090037638038\n",
      "192 Train Loss 1.4320942 Test MSE 14261.445506168186 Test RE 1.0001090037638038\n",
      "193 Train Loss 1.4321499 Test MSE 14261.445506168186 Test RE 1.0001090037638038\n",
      "194 Train Loss 1.4321926 Test MSE 14261.445506168186 Test RE 1.0001090037638038\n",
      "195 Train Loss 1.4322453 Test MSE 14261.445506168186 Test RE 1.0001090037638038\n",
      "196 Train Loss 1.4323065 Test MSE 14261.445506168186 Test RE 1.0001090037638038\n",
      "197 Train Loss 1.4323491 Test MSE 14261.445506168186 Test RE 1.0001090037638038\n",
      "198 Train Loss 1.432391 Test MSE 14261.445506168186 Test RE 1.0001090037638038\n",
      "199 Train Loss 1.4324322 Test MSE 14261.445506168186 Test RE 1.0001090037638038\n",
      "Training time: 91.74\n",
      "Training time: 91.74\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (m_lambda): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 1422.3391 Test MSE 14241.428883262719 Test RE 0.9994069068390538\n",
      "1 Train Loss 50.23759 Test MSE 14251.087504902016 Test RE 0.9997457512467453\n",
      "2 Train Loss 3.746359 Test MSE 14255.173331434135 Test RE 0.9998890559235416\n",
      "3 Train Loss 2.082832 Test MSE 14256.940882128054 Test RE 0.9999510439391118\n",
      "4 Train Loss 2.0212681 Test MSE 14257.856616139177 Test RE 0.9999831572264078\n",
      "5 Train Loss 2.0459208 Test MSE 14258.926508616514 Test RE 1.0000206752915843\n",
      "6 Train Loss 2.0553603 Test MSE 14259.372560273554 Test RE 1.000036316630034\n",
      "7 Train Loss 2.0639908 Test MSE 14258.296235068801 Test RE 0.9999985735736334\n",
      "8 Train Loss 2.08736 Test MSE 14258.265205920925 Test RE 0.9999974854660512\n",
      "9 Train Loss 2.106826 Test MSE 14258.28577269957 Test RE 0.9999982066870138\n",
      "10 Train Loss 2.10596 Test MSE 14259.864700018701 Test RE 1.000053573818928\n",
      "11 Train Loss 2.1213758 Test MSE 14260.132266637771 Test RE 1.0000629560853127\n",
      "12 Train Loss 2.1334796 Test MSE 14260.455876179896 Test RE 1.0000743033881716\n",
      "13 Train Loss 2.1460774 Test MSE 14260.293891931371 Test RE 1.0000686234594953\n",
      "14 Train Loss 2.1598594 Test MSE 14260.184565092817 Test RE 1.0000647899287376\n",
      "15 Train Loss 2.1717184 Test MSE 14260.07468156564 Test RE 1.0000609368629618\n",
      "16 Train Loss 2.1813169 Test MSE 14260.316470631615 Test RE 1.0000694151767244\n",
      "17 Train Loss 2.1915543 Test MSE 14260.578861474 Test RE 1.0000786158087633\n",
      "18 Train Loss 2.2003162 Test MSE 14260.628872803021 Test RE 1.0000803694268086\n",
      "19 Train Loss 2.2083473 Test MSE 14260.54420057545 Test RE 1.0000774004427941\n",
      "20 Train Loss 2.214432 Test MSE 14260.481234484056 Test RE 1.0000751925665163\n",
      "21 Train Loss 2.218614 Test MSE 14260.461082059035 Test RE 1.000074485930214\n",
      "22 Train Loss 2.212186 Test MSE 14259.439249548801 Test RE 1.000038655148749\n",
      "23 Train Loss 2.2026656 Test MSE 14257.86762692164 Test RE 0.9999835433501844\n",
      "24 Train Loss 2.161171 Test MSE 14259.544765487206 Test RE 1.0000423551477493\n",
      "25 Train Loss 2.065468 Test MSE 14259.26763724877 Test RE 1.000032637399836\n",
      "26 Train Loss 2.056056 Test MSE 14259.782464318685 Test RE 1.0000506901932418\n",
      "27 Train Loss 1.8493341 Test MSE 14258.853154818053 Test RE 1.000018103028891\n",
      "28 Train Loss 1.7166115 Test MSE 14258.2133431225 Test RE 0.9999956667765256\n",
      "29 Train Loss 1.5611892 Test MSE 14258.141465540923 Test RE 0.9999931462166568\n",
      "30 Train Loss 1.465555 Test MSE 14257.829282171975 Test RE 0.9999821986840175\n",
      "31 Train Loss 1.3812368 Test MSE 14258.261193621915 Test RE 0.9999973447655822\n",
      "32 Train Loss 1.3780246 Test MSE 14258.54671158576 Test RE 1.000007357058544\n",
      "33 Train Loss 1.3769346 Test MSE 14258.896675375132 Test RE 1.0000196291443242\n",
      "34 Train Loss 1.3771756 Test MSE 14258.580898585778 Test RE 1.0000085558914873\n",
      "35 Train Loss 1.3780345 Test MSE 14258.837030807288 Test RE 1.0000175376150024\n",
      "36 Train Loss 1.3777984 Test MSE 14259.810992313243 Test RE 1.00005169053902\n",
      "37 Train Loss 1.3783745 Test MSE 14260.00321356723 Test RE 1.0000584308296978\n",
      "38 Train Loss 1.3791293 Test MSE 14260.231861123822 Test RE 1.0000664483594501\n",
      "39 Train Loss 1.380495 Test MSE 14260.244877806217 Test RE 1.0000669047876654\n",
      "40 Train Loss 1.3818353 Test MSE 14260.24879437616 Test RE 1.0000670421216225\n",
      "41 Train Loss 1.383151 Test MSE 14260.248566118138 Test RE 1.0000670341177889\n",
      "42 Train Loss 1.3844203 Test MSE 14260.248566118138 Test RE 1.0000670341177889\n",
      "43 Train Loss 1.3856474 Test MSE 14260.248083541253 Test RE 1.0000670171963009\n",
      "44 Train Loss 1.3868399 Test MSE 14260.248083541253 Test RE 1.0000670171963009\n",
      "45 Train Loss 1.3879861 Test MSE 14260.223936276676 Test RE 1.00006617047565\n",
      "46 Train Loss 1.3891011 Test MSE 14260.223936276676 Test RE 1.00006617047565\n",
      "47 Train Loss 1.3901781 Test MSE 14260.223936276676 Test RE 1.00006617047565\n",
      "48 Train Loss 1.3912216 Test MSE 14260.223936276676 Test RE 1.00006617047565\n",
      "49 Train Loss 1.3922296 Test MSE 14260.223936276676 Test RE 1.00006617047565\n",
      "50 Train Loss 1.3932111 Test MSE 14260.237170388522 Test RE 1.000066634528127\n",
      "51 Train Loss 1.3941627 Test MSE 14260.237170388522 Test RE 1.000066634528127\n",
      "52 Train Loss 1.3950717 Test MSE 14260.237170388522 Test RE 1.000066634528127\n",
      "53 Train Loss 1.395967 Test MSE 14260.23441546253 Test RE 1.0000665379270093\n",
      "54 Train Loss 1.3968265 Test MSE 14260.23441546253 Test RE 1.0000665379270093\n",
      "55 Train Loss 1.3976687 Test MSE 14260.23441546253 Test RE 1.0000665379270093\n",
      "56 Train Loss 1.3984722 Test MSE 14260.23441546253 Test RE 1.0000665379270093\n",
      "57 Train Loss 1.3992691 Test MSE 14260.23441546253 Test RE 1.0000665379270093\n",
      "58 Train Loss 1.400027 Test MSE 14260.23441546253 Test RE 1.0000665379270093\n",
      "59 Train Loss 1.4007714 Test MSE 14260.23441546253 Test RE 1.0000665379270093\n",
      "60 Train Loss 1.4014882 Test MSE 14260.23441546253 Test RE 1.0000665379270093\n",
      "61 Train Loss 1.402196 Test MSE 14260.23441546253 Test RE 1.0000665379270093\n",
      "62 Train Loss 1.4028801 Test MSE 14260.23441546253 Test RE 1.0000665379270093\n",
      "63 Train Loss 1.403538 Test MSE 14260.23441546253 Test RE 1.0000665379270093\n",
      "64 Train Loss 1.4041822 Test MSE 14260.23441546253 Test RE 1.0000665379270093\n",
      "65 Train Loss 1.4048177 Test MSE 14260.23441546253 Test RE 1.0000665379270093\n",
      "66 Train Loss 1.4054084 Test MSE 14260.23441546253 Test RE 1.0000665379270093\n",
      "67 Train Loss 1.4060037 Test MSE 14260.23441546253 Test RE 1.0000665379270093\n",
      "68 Train Loss 1.4065753 Test MSE 14260.23441546253 Test RE 1.0000665379270093\n",
      "69 Train Loss 1.40714 Test MSE 14260.23441546253 Test RE 1.0000665379270093\n",
      "70 Train Loss 1.4076699 Test MSE 14260.23441546253 Test RE 1.0000665379270093\n",
      "71 Train Loss 1.4082049 Test MSE 14260.23441546253 Test RE 1.0000665379270093\n",
      "72 Train Loss 1.4087203 Test MSE 14260.23441546253 Test RE 1.0000665379270093\n",
      "73 Train Loss 1.409211 Test MSE 14260.23441546253 Test RE 1.0000665379270093\n",
      "74 Train Loss 1.4097041 Test MSE 14260.23441546253 Test RE 1.0000665379270093\n",
      "75 Train Loss 1.4101865 Test MSE 14260.23441546253 Test RE 1.0000665379270093\n",
      "76 Train Loss 1.4106443 Test MSE 14260.23441546253 Test RE 1.0000665379270093\n",
      "77 Train Loss 1.4110993 Test MSE 14260.234338180746 Test RE 1.0000665352171336\n",
      "78 Train Loss 1.4115437 Test MSE 14260.234338180746 Test RE 1.0000665352171336\n",
      "79 Train Loss 1.411975 Test MSE 14260.231693718184 Test RE 1.0000664424893926\n",
      "80 Train Loss 1.4123824 Test MSE 14260.231693718184 Test RE 1.0000664424893926\n",
      "81 Train Loss 1.4127944 Test MSE 14260.231693718184 Test RE 1.0000664424893926\n",
      "82 Train Loss 1.4131836 Test MSE 14260.231693718184 Test RE 1.0000664424893926\n",
      "83 Train Loss 1.4135838 Test MSE 14260.231693718184 Test RE 1.0000664424893926\n",
      "84 Train Loss 1.4139521 Test MSE 14260.231693718184 Test RE 1.0000664424893926\n",
      "85 Train Loss 1.4143356 Test MSE 14260.231693718184 Test RE 1.0000664424893926\n",
      "86 Train Loss 1.4146894 Test MSE 14260.231693718184 Test RE 1.0000664424893926\n",
      "87 Train Loss 1.41503 Test MSE 14260.232728413735 Test RE 1.0000664787708604\n",
      "88 Train Loss 1.4153776 Test MSE 14260.232728413735 Test RE 1.0000664787708604\n",
      "89 Train Loss 1.4157288 Test MSE 14260.232728413735 Test RE 1.0000664787708604\n",
      "90 Train Loss 1.4160609 Test MSE 14260.232728413735 Test RE 1.0000664787708604\n",
      "91 Train Loss 1.4163722 Test MSE 14260.232728413735 Test RE 1.0000664787708604\n",
      "92 Train Loss 1.4166805 Test MSE 14260.232728413735 Test RE 1.0000664787708604\n",
      "93 Train Loss 1.4169927 Test MSE 14260.232728413735 Test RE 1.0000664787708604\n",
      "94 Train Loss 1.4172839 Test MSE 14260.232728413735 Test RE 1.0000664787708604\n",
      "95 Train Loss 1.4175923 Test MSE 14260.23662832495 Test RE 1.00006661552074\n",
      "96 Train Loss 1.4178818 Test MSE 14260.23662832495 Test RE 1.00006661552074\n",
      "97 Train Loss 1.4181523 Test MSE 14260.23662832495 Test RE 1.00006661552074\n",
      "98 Train Loss 1.4184327 Test MSE 14260.23662832495 Test RE 1.00006661552074\n",
      "99 Train Loss 1.4187106 Test MSE 14260.236622467944 Test RE 1.0000666153153648\n",
      "100 Train Loss 1.4189565 Test MSE 14260.236622467944 Test RE 1.0000666153153648\n",
      "101 Train Loss 1.4192108 Test MSE 14260.236622467944 Test RE 1.0000666153153648\n",
      "102 Train Loss 1.419474 Test MSE 14260.236622467944 Test RE 1.0000666153153648\n",
      "103 Train Loss 1.4197199 Test MSE 14260.236602541048 Test RE 1.0000666146166306\n",
      "104 Train Loss 1.4199501 Test MSE 14260.236602541048 Test RE 1.0000666146166306\n",
      "105 Train Loss 1.4201865 Test MSE 14260.237578200831 Test RE 1.0000666488280114\n",
      "106 Train Loss 1.4204156 Test MSE 14260.237578200831 Test RE 1.0000666488280114\n",
      "107 Train Loss 1.4206396 Test MSE 14260.237578200831 Test RE 1.0000666488280114\n",
      "108 Train Loss 1.4208789 Test MSE 14260.237578200831 Test RE 1.0000666488280114\n",
      "109 Train Loss 1.4210826 Test MSE 14260.237578200831 Test RE 1.0000666488280114\n",
      "110 Train Loss 1.4212918 Test MSE 14260.237578200831 Test RE 1.0000666488280114\n",
      "111 Train Loss 1.4215182 Test MSE 14260.237578200831 Test RE 1.0000666488280114\n",
      "112 Train Loss 1.4217192 Test MSE 14260.237578200831 Test RE 1.0000666488280114\n",
      "113 Train Loss 1.421914 Test MSE 14260.237578200831 Test RE 1.0000666488280114\n",
      "114 Train Loss 1.4221182 Test MSE 14260.237578200831 Test RE 1.0000666488280114\n",
      "115 Train Loss 1.4223061 Test MSE 14260.237578200831 Test RE 1.0000666488280114\n",
      "116 Train Loss 1.4224792 Test MSE 14260.237578200831 Test RE 1.0000666488280114\n",
      "117 Train Loss 1.4226736 Test MSE 14260.237578200831 Test RE 1.0000666488280114\n",
      "118 Train Loss 1.4228499 Test MSE 14260.236517921856 Test RE 1.00006661164947\n",
      "119 Train Loss 1.4230223 Test MSE 14260.236517921856 Test RE 1.00006661164947\n",
      "120 Train Loss 1.4232091 Test MSE 14260.236517921856 Test RE 1.00006661164947\n",
      "121 Train Loss 1.423378 Test MSE 14260.234120522207 Test RE 1.0000665275849652\n",
      "122 Train Loss 1.4235369 Test MSE 14260.234120522207 Test RE 1.0000665275849652\n",
      "123 Train Loss 1.4237053 Test MSE 14260.234120522207 Test RE 1.0000665275849652\n",
      "124 Train Loss 1.4238724 Test MSE 14260.234120522207 Test RE 1.0000665275849652\n",
      "125 Train Loss 1.4240327 Test MSE 14260.234120522207 Test RE 1.0000665275849652\n",
      "126 Train Loss 1.4241939 Test MSE 14260.234120522207 Test RE 1.0000665275849652\n",
      "127 Train Loss 1.4243411 Test MSE 14260.234120522207 Test RE 1.0000665275849652\n",
      "128 Train Loss 1.424491 Test MSE 14260.234120522207 Test RE 1.0000665275849652\n",
      "129 Train Loss 1.4246421 Test MSE 14260.234120522207 Test RE 1.0000665275849652\n",
      "130 Train Loss 1.4247744 Test MSE 14260.234120522207 Test RE 1.0000665275849652\n",
      "131 Train Loss 1.4249289 Test MSE 14260.234120522207 Test RE 1.0000665275849652\n",
      "132 Train Loss 1.4250665 Test MSE 14260.234076298872 Test RE 1.0000665260342796\n",
      "133 Train Loss 1.4251891 Test MSE 14260.22958773619 Test RE 1.0000663686433975\n",
      "134 Train Loss 1.4253256 Test MSE 14260.22958773619 Test RE 1.0000663686433975\n",
      "135 Train Loss 1.4254775 Test MSE 14260.22958773619 Test RE 1.0000663686433975\n",
      "136 Train Loss 1.4255968 Test MSE 14260.22958773619 Test RE 1.0000663686433975\n",
      "137 Train Loss 1.425726 Test MSE 14260.22958773619 Test RE 1.0000663686433975\n",
      "138 Train Loss 1.4258534 Test MSE 14260.22958773619 Test RE 1.0000663686433975\n",
      "139 Train Loss 1.4259679 Test MSE 14260.22958773619 Test RE 1.0000663686433975\n",
      "140 Train Loss 1.4260949 Test MSE 14260.22958773619 Test RE 1.0000663686433975\n",
      "141 Train Loss 1.4262067 Test MSE 14260.22958773619 Test RE 1.0000663686433975\n",
      "142 Train Loss 1.4263369 Test MSE 14260.22958773619 Test RE 1.0000663686433975\n",
      "143 Train Loss 1.426453 Test MSE 14260.22958773619 Test RE 1.0000663686433975\n",
      "144 Train Loss 1.4265506 Test MSE 14260.229265135575 Test RE 1.0000663573314468\n",
      "145 Train Loss 1.4266798 Test MSE 14260.229265135575 Test RE 1.0000663573314468\n",
      "146 Train Loss 1.4267795 Test MSE 14260.229265135575 Test RE 1.0000663573314468\n",
      "147 Train Loss 1.4268987 Test MSE 14260.229265135575 Test RE 1.0000663573314468\n",
      "148 Train Loss 1.4269813 Test MSE 14260.229265135575 Test RE 1.0000663573314468\n",
      "149 Train Loss 1.4270934 Test MSE 14260.229265135575 Test RE 1.0000663573314468\n",
      "150 Train Loss 1.4272058 Test MSE 14260.229265135575 Test RE 1.0000663573314468\n",
      "151 Train Loss 1.4273095 Test MSE 14260.229265135575 Test RE 1.0000663573314468\n",
      "152 Train Loss 1.4274001 Test MSE 14260.229265135575 Test RE 1.0000663573314468\n",
      "153 Train Loss 1.4274906 Test MSE 14260.22615695314 Test RE 1.0000662483434075\n",
      "154 Train Loss 1.4275858 Test MSE 14260.22615695314 Test RE 1.0000662483434075\n",
      "155 Train Loss 1.4276892 Test MSE 14260.22615695314 Test RE 1.0000662483434075\n",
      "156 Train Loss 1.4277685 Test MSE 14260.22615695314 Test RE 1.0000662483434075\n",
      "157 Train Loss 1.4278556 Test MSE 14260.22615695314 Test RE 1.0000662483434075\n",
      "158 Train Loss 1.4279394 Test MSE 14260.22615695314 Test RE 1.0000662483434075\n",
      "159 Train Loss 1.4280547 Test MSE 14260.22615695314 Test RE 1.0000662483434075\n",
      "160 Train Loss 1.4281322 Test MSE 14260.22615695314 Test RE 1.0000662483434075\n",
      "161 Train Loss 1.4282328 Test MSE 14260.218024035365 Test RE 1.0000659631635782\n",
      "162 Train Loss 1.4283293 Test MSE 14260.218024035365 Test RE 1.0000659631635782\n",
      "163 Train Loss 1.428386 Test MSE 14260.218024035365 Test RE 1.0000659631635782\n",
      "164 Train Loss 1.4284796 Test MSE 14260.218024035365 Test RE 1.0000659631635782\n",
      "165 Train Loss 1.4285604 Test MSE 14260.218024035365 Test RE 1.0000659631635782\n",
      "166 Train Loss 1.4286202 Test MSE 14260.216975132882 Test RE 1.000065926383927\n",
      "167 Train Loss 1.4287066 Test MSE 14260.216975132882 Test RE 1.000065926383927\n",
      "168 Train Loss 1.428782 Test MSE 14260.216975132882 Test RE 1.000065926383927\n",
      "169 Train Loss 1.428866 Test MSE 14260.227409797877 Test RE 1.0000662922742574\n",
      "170 Train Loss 1.4289414 Test MSE 14260.227409797877 Test RE 1.0000662922742574\n",
      "171 Train Loss 1.4290073 Test MSE 14260.227409797877 Test RE 1.0000662922742574\n",
      "172 Train Loss 1.4290845 Test MSE 14260.227409797877 Test RE 1.0000662922742574\n",
      "173 Train Loss 1.4291481 Test MSE 14260.229855236614 Test RE 1.0000663780232666\n",
      "174 Train Loss 1.4292125 Test MSE 14260.229855236614 Test RE 1.0000663780232666\n",
      "175 Train Loss 1.4292868 Test MSE 14260.229855236614 Test RE 1.0000663780232666\n",
      "176 Train Loss 1.4293565 Test MSE 14260.229855236614 Test RE 1.0000663780232666\n",
      "177 Train Loss 1.4294521 Test MSE 14260.229855236614 Test RE 1.0000663780232666\n",
      "178 Train Loss 1.4295053 Test MSE 14260.229855236614 Test RE 1.0000663780232666\n",
      "179 Train Loss 1.4295516 Test MSE 14260.229855236614 Test RE 1.0000663780232666\n",
      "180 Train Loss 1.4296328 Test MSE 14260.229855236614 Test RE 1.0000663780232666\n",
      "181 Train Loss 1.4296823 Test MSE 14260.229855236614 Test RE 1.0000663780232666\n",
      "182 Train Loss 1.4297286 Test MSE 14260.227061716821 Test RE 1.0000662800688374\n",
      "183 Train Loss 1.4298276 Test MSE 14260.227061716821 Test RE 1.0000662800688374\n",
      "184 Train Loss 1.42987 Test MSE 14260.227061716821 Test RE 1.0000662800688374\n",
      "185 Train Loss 1.429921 Test MSE 14260.227061716821 Test RE 1.0000662800688374\n",
      "186 Train Loss 1.4299829 Test MSE 14260.227061716821 Test RE 1.0000662800688374\n",
      "187 Train Loss 1.4300455 Test MSE 14260.227061716821 Test RE 1.0000662800688374\n",
      "188 Train Loss 1.4300863 Test MSE 14260.227061716821 Test RE 1.0000662800688374\n",
      "189 Train Loss 1.4301541 Test MSE 14260.227061716821 Test RE 1.0000662800688374\n",
      "190 Train Loss 1.4302305 Test MSE 14260.227061716821 Test RE 1.0000662800688374\n",
      "191 Train Loss 1.4302759 Test MSE 14260.222384713672 Test RE 1.0000661160702733\n",
      "192 Train Loss 1.4303217 Test MSE 14260.222384713672 Test RE 1.0000661160702733\n",
      "193 Train Loss 1.4303697 Test MSE 14260.222384713672 Test RE 1.0000661160702733\n",
      "194 Train Loss 1.4304137 Test MSE 14260.222384713672 Test RE 1.0000661160702733\n",
      "195 Train Loss 1.4304762 Test MSE 14260.222384713672 Test RE 1.0000661160702733\n",
      "196 Train Loss 1.4305208 Test MSE 14260.222384713672 Test RE 1.0000661160702733\n",
      "197 Train Loss 1.430598 Test MSE 14260.222384713672 Test RE 1.0000661160702733\n",
      "198 Train Loss 1.4306359 Test MSE 14260.222384713672 Test RE 1.0000661160702733\n",
      "199 Train Loss 1.4306878 Test MSE 14260.222384713672 Test RE 1.0000661160702733\n",
      "Training time: 91.49\n",
      "Training time: 91.49\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (m_lambda): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 8.392431 Test MSE 14262.47042295113 Test RE 1.000144940166138\n",
      "1 Train Loss 2.1656194 Test MSE 14259.964497093833 Test RE 1.0000570732296563\n",
      "2 Train Loss 1.9795223 Test MSE 14258.909985989152 Test RE 1.0000200959010621\n",
      "3 Train Loss 2.011498 Test MSE 14258.524226868327 Test RE 1.0000065685878752\n",
      "4 Train Loss 2.016235 Test MSE 14258.307674464468 Test RE 0.9999989747217397\n",
      "5 Train Loss 2.034254 Test MSE 14257.982605021416 Test RE 0.9999875753688153\n",
      "6 Train Loss 2.0618978 Test MSE 14258.20206200199 Test RE 0.9999952711773857\n",
      "7 Train Loss 2.0709794 Test MSE 14259.364879000823 Test RE 1.0000360472790266\n",
      "8 Train Loss 2.0929713 Test MSE 14259.543189407237 Test RE 1.000042299881372\n",
      "9 Train Loss 2.1140332 Test MSE 14259.605246221716 Test RE 1.00004447594589\n",
      "10 Train Loss 2.1341105 Test MSE 14259.608777992853 Test RE 1.000044599789711\n",
      "11 Train Loss 2.1521924 Test MSE 14259.629622760329 Test RE 1.000045330724544\n",
      "12 Train Loss 2.161637 Test MSE 14259.52648270063 Test RE 1.000041714048503\n",
      "13 Train Loss 2.1719744 Test MSE 14259.095545765846 Test RE 1.0000266028115536\n",
      "14 Train Loss 2.1804318 Test MSE 14259.404687183025 Test RE 1.000037443189354\n",
      "15 Train Loss 2.185719 Test MSE 14259.96953479489 Test RE 1.0000572498776459\n",
      "16 Train Loss 2.1910486 Test MSE 14259.72182983633 Test RE 1.0000485640170342\n",
      "17 Train Loss 2.1361117 Test MSE 14260.090125150362 Test RE 1.0000614783931419\n",
      "18 Train Loss 2.1356401 Test MSE 14260.291721171825 Test RE 1.0000685473422495\n",
      "19 Train Loss 1.9786294 Test MSE 14259.46424525619 Test RE 1.0000395316440382\n",
      "20 Train Loss 1.7218267 Test MSE 14255.634897673717 Test RE 0.9999052434256166\n",
      "21 Train Loss 1.6385926 Test MSE 14255.198807153676 Test RE 0.9998899493845042\n",
      "22 Train Loss 1.5405402 Test MSE 14255.324130744892 Test RE 0.9998943446064007\n",
      "23 Train Loss 1.5370305 Test MSE 14255.097303524935 Test RE 0.9998863895380143\n",
      "24 Train Loss 1.431895 Test MSE 14255.369182616007 Test RE 0.9998959246152146\n",
      "25 Train Loss 1.3707616 Test MSE 14256.708285852003 Test RE 0.9999428870056009\n",
      "26 Train Loss 1.3669516 Test MSE 14257.610034351226 Test RE 0.9999745101096781\n",
      "27 Train Loss 1.3638859 Test MSE 14258.384524405668 Test RE 1.0000016696333873\n",
      "28 Train Loss 1.3636513 Test MSE 14258.452569155801 Test RE 1.0000040557656467\n",
      "29 Train Loss 1.3648851 Test MSE 14258.614548864474 Test RE 1.000009735901861\n",
      "30 Train Loss 1.3647074 Test MSE 14258.530687130904 Test RE 1.0000067951297034\n",
      "31 Train Loss 1.3660185 Test MSE 14258.244248115247 Test RE 0.9999967505322995\n",
      "32 Train Loss 1.3678455 Test MSE 14258.059246901163 Test RE 0.9999902630149795\n",
      "33 Train Loss 1.3694474 Test MSE 14258.1392004131 Test RE 0.9999930667844112\n",
      "34 Train Loss 1.3711647 Test MSE 14258.30259740563 Test RE 0.9999987966832868\n",
      "35 Train Loss 1.3729705 Test MSE 14258.379223354745 Test RE 1.00000148374065\n",
      "36 Train Loss 1.3746872 Test MSE 14258.38781942161 Test RE 1.0000017851801821\n",
      "37 Train Loss 1.3763369 Test MSE 14258.397490306636 Test RE 1.0000021243104107\n",
      "38 Train Loss 1.37796 Test MSE 14258.397490306636 Test RE 1.0000021243104107\n",
      "39 Train Loss 1.3795116 Test MSE 14258.397490306636 Test RE 1.0000021243104107\n",
      "40 Train Loss 1.3809823 Test MSE 14258.397490306636 Test RE 1.0000021243104107\n",
      "41 Train Loss 1.3824464 Test MSE 14258.409031218727 Test RE 1.0000025290169805\n",
      "42 Train Loss 1.3838308 Test MSE 14258.409031218727 Test RE 1.0000025290169805\n",
      "43 Train Loss 1.3851359 Test MSE 14258.40179575155 Test RE 1.0000022752899909\n",
      "44 Train Loss 1.3864417 Test MSE 14258.40179575155 Test RE 1.0000022752899909\n",
      "45 Train Loss 1.3876905 Test MSE 14258.40179575155 Test RE 1.0000022752899909\n",
      "46 Train Loss 1.3888927 Test MSE 14258.40179575155 Test RE 1.0000022752899909\n",
      "47 Train Loss 1.3900406 Test MSE 14258.40179575155 Test RE 1.0000022752899909\n",
      "48 Train Loss 1.3911536 Test MSE 14258.425152712538 Test RE 1.0000030943510927\n",
      "49 Train Loss 1.3922235 Test MSE 14258.425152712538 Test RE 1.0000030943510927\n",
      "50 Train Loss 1.3932616 Test MSE 14258.425152712538 Test RE 1.0000030943510927\n",
      "51 Train Loss 1.3942904 Test MSE 14258.425152712538 Test RE 1.0000030943510927\n",
      "52 Train Loss 1.3952128 Test MSE 14258.408395010236 Test RE 1.000002506706984\n",
      "53 Train Loss 1.3961548 Test MSE 14258.39768654458 Test RE 1.0000021311919118\n",
      "54 Train Loss 1.3970658 Test MSE 14258.377187654036 Test RE 1.0000014123544223\n",
      "55 Train Loss 1.3979537 Test MSE 14258.377187654036 Test RE 1.0000014123544223\n",
      "56 Train Loss 1.3987608 Test MSE 14258.257369375793 Test RE 0.9999972106596007\n",
      "57 Train Loss 1.3996016 Test MSE 14258.249995102604 Test RE 0.9999969520637083\n",
      "58 Train Loss 1.4003886 Test MSE 14258.249995102604 Test RE 0.9999969520637083\n",
      "59 Train Loss 1.4011668 Test MSE 14258.249995102604 Test RE 0.9999969520637083\n",
      "60 Train Loss 1.4018959 Test MSE 14258.249995102604 Test RE 0.9999969520637083\n",
      "61 Train Loss 1.4026369 Test MSE 14258.232374996214 Test RE 0.9999963341737804\n",
      "62 Train Loss 1.4033629 Test MSE 14258.232374996214 Test RE 0.9999963341737804\n",
      "63 Train Loss 1.4040458 Test MSE 14258.20693839794 Test RE 0.9999954421797485\n",
      "64 Train Loss 1.4047002 Test MSE 14258.20693839794 Test RE 0.9999954421797485\n",
      "65 Train Loss 1.4053742 Test MSE 14258.20693839794 Test RE 0.9999954421797485\n",
      "66 Train Loss 1.4059436 Test MSE 14258.116579963575 Test RE 0.9999922735426934\n",
      "67 Train Loss 1.4065484 Test MSE 14258.116579963575 Test RE 0.9999922735426934\n",
      "68 Train Loss 1.4071482 Test MSE 14258.022642113208 Test RE 0.9999889793741137\n",
      "69 Train Loss 1.4077168 Test MSE 14258.022642113208 Test RE 0.9999889793741137\n",
      "70 Train Loss 1.4082726 Test MSE 14258.022642113208 Test RE 0.9999889793741137\n",
      "71 Train Loss 1.4088082 Test MSE 14258.069423233734 Test RE 0.9999906198739141\n",
      "72 Train Loss 1.4093145 Test MSE 14258.069423233734 Test RE 0.9999906198739141\n",
      "73 Train Loss 1.409828 Test MSE 14258.069423233734 Test RE 0.9999906198739141\n",
      "74 Train Loss 1.410322 Test MSE 14258.071047548552 Test RE 0.999990676834625\n",
      "75 Train Loss 1.4108176 Test MSE 14258.071047548552 Test RE 0.999990676834625\n",
      "76 Train Loss 1.4112853 Test MSE 14258.029891816966 Test RE 0.999989233603719\n",
      "77 Train Loss 1.411751 Test MSE 14258.029891816966 Test RE 0.999989233603719\n",
      "78 Train Loss 1.4122345 Test MSE 14258.029891816966 Test RE 0.999989233603719\n",
      "79 Train Loss 1.4126579 Test MSE 14258.029891816966 Test RE 0.999989233603719\n",
      "80 Train Loss 1.4130356 Test MSE 14258.029891816966 Test RE 0.999989233603719\n",
      "81 Train Loss 1.4134603 Test MSE 14258.029891816966 Test RE 0.999989233603719\n",
      "82 Train Loss 1.4138294 Test MSE 14258.06119333629 Test RE 0.999990331271678\n",
      "83 Train Loss 1.4142671 Test MSE 14258.06119333629 Test RE 0.999990331271678\n",
      "84 Train Loss 1.4146315 Test MSE 14258.06119333629 Test RE 0.999990331271678\n",
      "85 Train Loss 1.4150223 Test MSE 14258.06119333629 Test RE 0.999990331271678\n",
      "86 Train Loss 1.4153628 Test MSE 14258.047633417302 Test RE 0.9999898557585307\n",
      "87 Train Loss 1.4156796 Test MSE 14258.047633417302 Test RE 0.9999898557585307\n",
      "88 Train Loss 1.4160446 Test MSE 14258.043544243108 Test RE 0.999989712361163\n",
      "89 Train Loss 1.4163841 Test MSE 14258.043544243108 Test RE 0.999989712361163\n",
      "90 Train Loss 1.4167309 Test MSE 14258.043544243108 Test RE 0.999989712361163\n",
      "91 Train Loss 1.4170462 Test MSE 14258.035438796289 Test RE 0.9999894281228464\n",
      "92 Train Loss 1.417327 Test MSE 14258.035438796289 Test RE 0.9999894281228464\n",
      "93 Train Loss 1.4176371 Test MSE 14258.035438796289 Test RE 0.9999894281228464\n",
      "94 Train Loss 1.4179189 Test MSE 14258.035438796289 Test RE 0.9999894281228464\n",
      "95 Train Loss 1.4182378 Test MSE 14258.035438796289 Test RE 0.9999894281228464\n",
      "96 Train Loss 1.4185495 Test MSE 14258.035438796289 Test RE 0.9999894281228464\n",
      "97 Train Loss 1.4188077 Test MSE 14258.035438796289 Test RE 0.9999894281228464\n",
      "98 Train Loss 1.4190781 Test MSE 14258.035438796289 Test RE 0.9999894281228464\n",
      "99 Train Loss 1.4193668 Test MSE 14258.035438796289 Test RE 0.9999894281228464\n",
      "100 Train Loss 1.4195759 Test MSE 14258.03679389184 Test RE 0.9999894756427588\n",
      "101 Train Loss 1.4198408 Test MSE 14258.03679389184 Test RE 0.9999894756427588\n",
      "102 Train Loss 1.4201293 Test MSE 14258.03679389184 Test RE 0.9999894756427588\n",
      "103 Train Loss 1.4203565 Test MSE 14258.03679389184 Test RE 0.9999894756427588\n",
      "104 Train Loss 1.4205709 Test MSE 14258.03679389184 Test RE 0.9999894756427588\n",
      "105 Train Loss 1.4208192 Test MSE 14258.03679389184 Test RE 0.9999894756427588\n",
      "106 Train Loss 1.4210587 Test MSE 14258.03679389184 Test RE 0.9999894756427588\n",
      "107 Train Loss 1.4212347 Test MSE 14258.009561554603 Test RE 0.9999885206703448\n",
      "108 Train Loss 1.4214859 Test MSE 14258.009561554603 Test RE 0.9999885206703448\n",
      "109 Train Loss 1.4216791 Test MSE 14258.009561554603 Test RE 0.9999885206703448\n",
      "110 Train Loss 1.4218936 Test MSE 14258.009561554603 Test RE 0.9999885206703448\n",
      "111 Train Loss 1.4221183 Test MSE 14257.98233550998 Test RE 0.9999875659176857\n",
      "112 Train Loss 1.4223157 Test MSE 14257.98233550998 Test RE 0.9999875659176857\n",
      "113 Train Loss 1.422522 Test MSE 14257.98233550998 Test RE 0.9999875659176857\n",
      "114 Train Loss 1.4227072 Test MSE 14257.98233550998 Test RE 0.9999875659176857\n",
      "115 Train Loss 1.4228784 Test MSE 14258.008015368767 Test RE 0.9999884664493076\n",
      "116 Train Loss 1.4230498 Test MSE 14258.008015368767 Test RE 0.9999884664493076\n",
      "117 Train Loss 1.4232522 Test MSE 14257.993690790154 Test RE 0.9999879641204769\n",
      "118 Train Loss 1.4234178 Test MSE 14257.993690790154 Test RE 0.9999879641204769\n",
      "119 Train Loss 1.423584 Test MSE 14257.993690790154 Test RE 0.9999879641204769\n",
      "120 Train Loss 1.4237751 Test MSE 14257.993690790154 Test RE 0.9999879641204769\n",
      "121 Train Loss 1.4239559 Test MSE 14257.993690790154 Test RE 0.9999879641204769\n",
      "122 Train Loss 1.4240781 Test MSE 14257.993690790154 Test RE 0.9999879641204769\n",
      "123 Train Loss 1.4242626 Test MSE 14257.9679263374 Test RE 0.9999870606218901\n",
      "124 Train Loss 1.4244217 Test MSE 14257.9679263374 Test RE 0.9999870606218901\n",
      "125 Train Loss 1.4245825 Test MSE 14257.987352283986 Test RE 0.9999877418440919\n",
      "126 Train Loss 1.4247086 Test MSE 14257.987352283986 Test RE 0.9999877418440919\n",
      "127 Train Loss 1.4249011 Test MSE 14257.987352283986 Test RE 0.9999877418440919\n",
      "128 Train Loss 1.4250174 Test MSE 14257.987352283986 Test RE 0.9999877418440919\n",
      "129 Train Loss 1.4251685 Test MSE 14257.987352283986 Test RE 0.9999877418440919\n",
      "130 Train Loss 1.42532 Test MSE 14257.987352283986 Test RE 0.9999877418440919\n",
      "131 Train Loss 1.4254596 Test MSE 14257.997994742145 Test RE 0.9999881150498416\n",
      "132 Train Loss 1.4255949 Test MSE 14257.997994742145 Test RE 0.9999881150498416\n",
      "133 Train Loss 1.4256842 Test MSE 14257.997994742145 Test RE 0.9999881150498416\n",
      "134 Train Loss 1.4258367 Test MSE 14257.997994742145 Test RE 0.9999881150498416\n",
      "135 Train Loss 1.4259886 Test MSE 14257.997994742145 Test RE 0.9999881150498416\n",
      "136 Train Loss 1.4260956 Test MSE 14257.997994742145 Test RE 0.9999881150498416\n",
      "137 Train Loss 1.4262028 Test MSE 14257.997994742145 Test RE 0.9999881150498416\n",
      "138 Train Loss 1.4263351 Test MSE 14257.997994742145 Test RE 0.9999881150498416\n",
      "139 Train Loss 1.4264548 Test MSE 14257.997994742145 Test RE 0.9999881150498416\n",
      "140 Train Loss 1.426603 Test MSE 14257.997994742145 Test RE 0.9999881150498416\n",
      "141 Train Loss 1.4267074 Test MSE 14257.997994742145 Test RE 0.9999881150498416\n",
      "142 Train Loss 1.4268105 Test MSE 14257.983614382818 Test RE 0.999987610764736\n",
      "143 Train Loss 1.426955 Test MSE 14257.983614382818 Test RE 0.999987610764736\n",
      "144 Train Loss 1.4270133 Test MSE 14257.983614382818 Test RE 0.999987610764736\n",
      "145 Train Loss 1.4271505 Test MSE 14257.983614382818 Test RE 0.999987610764736\n",
      "146 Train Loss 1.4272647 Test MSE 14257.983614382818 Test RE 0.999987610764736\n",
      "147 Train Loss 1.4273641 Test MSE 14257.996531907025 Test RE 0.9999880637516941\n",
      "148 Train Loss 1.4274665 Test MSE 14257.996531907025 Test RE 0.9999880637516941\n",
      "149 Train Loss 1.4275676 Test MSE 14257.996531907025 Test RE 0.9999880637516941\n",
      "150 Train Loss 1.4276834 Test MSE 14257.996531907025 Test RE 0.9999880637516941\n",
      "151 Train Loss 1.4277892 Test MSE 14257.996531907025 Test RE 0.9999880637516941\n",
      "152 Train Loss 1.4278567 Test MSE 14257.996531907025 Test RE 0.9999880637516941\n",
      "153 Train Loss 1.427899 Test MSE 14257.996531907025 Test RE 0.9999880637516941\n",
      "154 Train Loss 1.4280324 Test MSE 14257.996531907025 Test RE 0.9999880637516941\n",
      "155 Train Loss 1.428155 Test MSE 14257.996531907025 Test RE 0.9999880637516941\n",
      "156 Train Loss 1.4281782 Test MSE 14257.996531907025 Test RE 0.9999880637516941\n",
      "157 Train Loss 1.4282733 Test MSE 14257.996531907025 Test RE 0.9999880637516941\n",
      "158 Train Loss 1.4283652 Test MSE 14257.972785794349 Test RE 0.9999872310316457\n",
      "159 Train Loss 1.4284645 Test MSE 14257.972785794349 Test RE 0.9999872310316457\n",
      "160 Train Loss 1.4285474 Test MSE 14257.972785794349 Test RE 0.9999872310316457\n",
      "161 Train Loss 1.4286704 Test MSE 14257.989515149551 Test RE 0.9999878176906654\n",
      "162 Train Loss 1.4287659 Test MSE 14257.989515149551 Test RE 0.9999878176906654\n",
      "163 Train Loss 1.4287865 Test MSE 14257.989515149551 Test RE 0.9999878176906654\n",
      "164 Train Loss 1.4289006 Test MSE 14257.989515149551 Test RE 0.9999878176906654\n",
      "165 Train Loss 1.4289738 Test MSE 14257.989515149551 Test RE 0.9999878176906654\n",
      "166 Train Loss 1.429025 Test MSE 14257.989515149551 Test RE 0.9999878176906654\n",
      "167 Train Loss 1.4291143 Test MSE 14257.959817701387 Test RE 0.9999867762709825\n",
      "168 Train Loss 1.429169 Test MSE 14257.959817701387 Test RE 0.9999867762709825\n",
      "169 Train Loss 1.4292694 Test MSE 14257.959817701387 Test RE 0.9999867762709825\n",
      "170 Train Loss 1.42935 Test MSE 14257.959817701387 Test RE 0.9999867762709825\n",
      "171 Train Loss 1.4293978 Test MSE 14257.959817701387 Test RE 0.9999867762709825\n",
      "172 Train Loss 1.4294759 Test MSE 14257.971297294967 Test RE 0.9999871788334673\n",
      "173 Train Loss 1.4295381 Test MSE 14257.971297294967 Test RE 0.9999871788334673\n",
      "174 Train Loss 1.4296033 Test MSE 14257.951920961808 Test RE 0.9999864993507093\n",
      "175 Train Loss 1.4296578 Test MSE 14257.951920961808 Test RE 0.9999864993507093\n",
      "176 Train Loss 1.4297585 Test MSE 14257.951920961808 Test RE 0.9999864993507093\n",
      "177 Train Loss 1.4298398 Test MSE 14257.951920961808 Test RE 0.9999864993507093\n",
      "178 Train Loss 1.4298984 Test MSE 14257.951920961808 Test RE 0.9999864993507093\n",
      "179 Train Loss 1.4299595 Test MSE 14257.951920961808 Test RE 0.9999864993507093\n",
      "180 Train Loss 1.4300156 Test MSE 14257.951920961808 Test RE 0.9999864993507093\n",
      "181 Train Loss 1.4300998 Test MSE 14257.951920961808 Test RE 0.9999864993507093\n",
      "182 Train Loss 1.4301071 Test MSE 14257.951920961808 Test RE 0.9999864993507093\n",
      "183 Train Loss 1.4302093 Test MSE 14257.951920961808 Test RE 0.9999864993507093\n",
      "184 Train Loss 1.4302787 Test MSE 14257.951920961808 Test RE 0.9999864993507093\n",
      "185 Train Loss 1.43031 Test MSE 14257.951920961808 Test RE 0.9999864993507093\n",
      "186 Train Loss 1.4303554 Test MSE 14257.951920961808 Test RE 0.9999864993507093\n",
      "187 Train Loss 1.4304097 Test MSE 14257.951920961808 Test RE 0.9999864993507093\n",
      "188 Train Loss 1.4304508 Test MSE 14257.951920961808 Test RE 0.9999864993507093\n",
      "189 Train Loss 1.4304838 Test MSE 14257.951920961808 Test RE 0.9999864993507093\n",
      "190 Train Loss 1.4305923 Test MSE 14257.951920961808 Test RE 0.9999864993507093\n",
      "191 Train Loss 1.4306751 Test MSE 14257.951920961808 Test RE 0.9999864993507093\n",
      "192 Train Loss 1.4307002 Test MSE 14257.951920961808 Test RE 0.9999864993507093\n",
      "193 Train Loss 1.4307184 Test MSE 14257.951920961808 Test RE 0.9999864993507093\n",
      "194 Train Loss 1.4307799 Test MSE 14257.956323596472 Test RE 0.9999866537408667\n",
      "195 Train Loss 1.430827 Test MSE 14257.956323596472 Test RE 0.9999866537408667\n",
      "196 Train Loss 1.4309189 Test MSE 14257.956323596472 Test RE 0.9999866537408667\n",
      "197 Train Loss 1.4309646 Test MSE 14257.956323596472 Test RE 0.9999866537408667\n",
      "198 Train Loss 1.4310216 Test MSE 14257.956323596472 Test RE 0.9999866537408667\n",
      "199 Train Loss 1.4310793 Test MSE 14257.956323596472 Test RE 0.9999866537408667\n",
      "Training time: 84.88\n",
      "Training time: 84.88\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (m_lambda): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 771.19775 Test MSE 14272.471604149856 Test RE 1.0004955413442405\n",
      "1 Train Loss 27.523556 Test MSE 14264.432097962686 Test RE 1.0002137182887576\n",
      "2 Train Loss 2.8627572 Test MSE 14261.090834879926 Test RE 1.000096567711863\n",
      "3 Train Loss 2.0158894 Test MSE 14259.66537129352 Test RE 1.0000465842680175\n",
      "4 Train Loss 2.022628 Test MSE 14259.079443652776 Test RE 1.0000260381703419\n",
      "5 Train Loss 2.0538127 Test MSE 14258.703968494425 Test RE 1.0000128715627639\n",
      "6 Train Loss 2.0722704 Test MSE 14258.092675630607 Test RE 0.9999914352777599\n",
      "7 Train Loss 2.097083 Test MSE 14258.271414326418 Test RE 0.9999977031779929\n",
      "8 Train Loss 2.121224 Test MSE 14258.320346836617 Test RE 0.9999994191067476\n",
      "9 Train Loss 2.1431575 Test MSE 14258.365636736471 Test RE 1.0000010072965373\n",
      "10 Train Loss 2.1633942 Test MSE 14258.380915308722 Test RE 1.0000015430726539\n",
      "11 Train Loss 2.1803496 Test MSE 14259.056510506449 Test RE 1.0000252339896518\n",
      "12 Train Loss 2.1905758 Test MSE 14259.28407512769 Test RE 1.0000332138113002\n",
      "13 Train Loss 2.2054203 Test MSE 14259.183820049238 Test RE 1.0000296982566619\n",
      "14 Train Loss 2.2076764 Test MSE 14258.685628685116 Test RE 1.0000122284450121\n",
      "15 Train Loss 2.1974347 Test MSE 14259.579574247322 Test RE 1.0000435757411756\n",
      "16 Train Loss 2.2003603 Test MSE 14259.660733834808 Test RE 1.0000464216528717\n",
      "17 Train Loss 2.2069502 Test MSE 14259.587649282985 Test RE 1.0000438588977145\n",
      "18 Train Loss 2.2075477 Test MSE 14259.830634520747 Test RE 1.0000523793004654\n",
      "19 Train Loss 2.209651 Test MSE 14260.352383376712 Test RE 1.0000706744480545\n",
      "20 Train Loss 2.2114747 Test MSE 14260.407832445859 Test RE 1.0000726187525353\n",
      "21 Train Loss 2.1964772 Test MSE 14260.354874669434 Test RE 1.000070761804545\n",
      "22 Train Loss 2.078743 Test MSE 14260.170166228834 Test RE 1.0000642850334311\n",
      "23 Train Loss 1.874587 Test MSE 14257.990137450412 Test RE 0.999987839513282\n",
      "24 Train Loss 1.7317886 Test MSE 14258.424093369253 Test RE 1.0000030572030005\n",
      "25 Train Loss 1.7264731 Test MSE 14258.347378737044 Test RE 1.0000003670399984\n",
      "26 Train Loss 1.5298932 Test MSE 14261.188368546162 Test RE 1.0000999876086714\n",
      "27 Train Loss 1.5053921 Test MSE 14260.874999574746 Test RE 1.0000889996735156\n",
      "28 Train Loss 1.429278 Test MSE 14260.049763956038 Test RE 1.0000600631249397\n",
      "29 Train Loss 1.3962404 Test MSE 14259.87573336301 Test RE 1.0000539607066543\n",
      "30 Train Loss 1.3841009 Test MSE 14259.676952738546 Test RE 1.0000469903779043\n",
      "31 Train Loss 1.3800212 Test MSE 14259.3541566823 Test RE 1.0000356712907852\n",
      "32 Train Loss 1.3798281 Test MSE 14259.089326203843 Test RE 1.0000263847146864\n",
      "33 Train Loss 1.3781561 Test MSE 14258.656253460153 Test RE 1.0000111983500906\n",
      "34 Train Loss 1.3772329 Test MSE 14258.677027245205 Test RE 1.0000119268202128\n",
      "35 Train Loss 1.3772479 Test MSE 14258.52424731075 Test RE 1.0000065693047295\n",
      "36 Train Loss 1.3784082 Test MSE 14258.187328570215 Test RE 0.999994754514561\n",
      "37 Train Loss 1.3799069 Test MSE 14258.153118937387 Test RE 0.9999935548713975\n",
      "38 Train Loss 1.3813411 Test MSE 14258.111725212784 Test RE 0.9999921032988301\n",
      "39 Train Loss 1.3827394 Test MSE 14258.069815469898 Test RE 0.9999906336286687\n",
      "40 Train Loss 1.3840683 Test MSE 14258.01878792221 Test RE 0.9999888442169154\n",
      "41 Train Loss 1.3853695 Test MSE 14257.966526824099 Test RE 0.9999870115442364\n",
      "42 Train Loss 1.3866208 Test MSE 14257.93756141107 Test RE 0.9999859957944985\n",
      "43 Train Loss 1.3878295 Test MSE 14257.93756141107 Test RE 0.9999859957944985\n",
      "44 Train Loss 1.3889976 Test MSE 14257.89613675553 Test RE 0.9999845431262802\n",
      "45 Train Loss 1.3901265 Test MSE 14257.89613675553 Test RE 0.9999845431262802\n",
      "46 Train Loss 1.3912169 Test MSE 14257.89613675553 Test RE 0.9999845431262802\n",
      "47 Train Loss 1.3922714 Test MSE 14257.89613675553 Test RE 0.9999845431262802\n",
      "48 Train Loss 1.3932904 Test MSE 14257.90250241857 Test RE 0.999984766355706\n",
      "49 Train Loss 1.394271 Test MSE 14257.90250241857 Test RE 0.999984766355706\n",
      "50 Train Loss 1.3952271 Test MSE 14257.90250241857 Test RE 0.999984766355706\n",
      "51 Train Loss 1.3961502 Test MSE 14257.88444081534 Test RE 0.9999841329759431\n",
      "52 Train Loss 1.3970327 Test MSE 14257.88444081534 Test RE 0.9999841329759431\n",
      "53 Train Loss 1.397901 Test MSE 14257.88444081534 Test RE 0.9999841329759431\n",
      "54 Train Loss 1.3987377 Test MSE 14257.88444081534 Test RE 0.9999841329759431\n",
      "55 Train Loss 1.3995485 Test MSE 14257.899602178883 Test RE 0.9999846646508564\n",
      "56 Train Loss 1.4003214 Test MSE 14257.899602178883 Test RE 0.9999846646508564\n",
      "57 Train Loss 1.4010934 Test MSE 14257.899602178883 Test RE 0.9999846646508564\n",
      "58 Train Loss 1.4018235 Test MSE 14257.872297421807 Test RE 0.9999837071342453\n",
      "59 Train Loss 1.4025402 Test MSE 14257.872297421807 Test RE 0.9999837071342453\n",
      "60 Train Loss 1.4032215 Test MSE 14257.833546505699 Test RE 0.999982348224926\n",
      "61 Train Loss 1.4039025 Test MSE 14257.833546505699 Test RE 0.999982348224926\n",
      "62 Train Loss 1.4045591 Test MSE 14257.833546505699 Test RE 0.999982348224926\n",
      "63 Train Loss 1.4051882 Test MSE 14257.893131824967 Test RE 0.9999844377501347\n",
      "64 Train Loss 1.4058043 Test MSE 14257.893131824967 Test RE 0.9999844377501347\n",
      "65 Train Loss 1.4064139 Test MSE 14257.895270319172 Test RE 0.9999845127423103\n",
      "66 Train Loss 1.4069754 Test MSE 14257.895270319172 Test RE 0.9999845127423103\n",
      "67 Train Loss 1.4075432 Test MSE 14257.89347525593 Test RE 0.9999844497934853\n",
      "68 Train Loss 1.4080906 Test MSE 14257.89347525593 Test RE 0.9999844497934853\n",
      "69 Train Loss 1.4086305 Test MSE 14257.89347525593 Test RE 0.9999844497934853\n",
      "70 Train Loss 1.4091318 Test MSE 14257.894176713227 Test RE 0.9999844743920134\n",
      "71 Train Loss 1.4096401 Test MSE 14257.894176713227 Test RE 0.9999844743920134\n",
      "72 Train Loss 1.4101362 Test MSE 14257.894176713227 Test RE 0.9999844743920134\n",
      "73 Train Loss 1.4106011 Test MSE 14257.894176713227 Test RE 0.9999844743920134\n",
      "74 Train Loss 1.4110711 Test MSE 14257.894176713227 Test RE 0.9999844743920134\n",
      "75 Train Loss 1.411532 Test MSE 14257.894176713227 Test RE 0.9999844743920134\n",
      "76 Train Loss 1.4119649 Test MSE 14257.894176713227 Test RE 0.9999844743920134\n",
      "77 Train Loss 1.4123982 Test MSE 14257.894176713227 Test RE 0.9999844743920134\n",
      "78 Train Loss 1.4128175 Test MSE 14257.894176713227 Test RE 0.9999844743920134\n",
      "79 Train Loss 1.4132334 Test MSE 14257.894176713227 Test RE 0.9999844743920134\n",
      "80 Train Loss 1.4136153 Test MSE 14257.894176713227 Test RE 0.9999844743920134\n",
      "81 Train Loss 1.4140035 Test MSE 14257.894176713227 Test RE 0.9999844743920134\n",
      "82 Train Loss 1.4143742 Test MSE 14257.894176713227 Test RE 0.9999844743920134\n",
      "83 Train Loss 1.4147542 Test MSE 14257.894176713227 Test RE 0.9999844743920134\n",
      "84 Train Loss 1.4150962 Test MSE 14257.894176713227 Test RE 0.9999844743920134\n",
      "85 Train Loss 1.4154624 Test MSE 14257.894176713227 Test RE 0.9999844743920134\n",
      "86 Train Loss 1.4157962 Test MSE 14257.894176713227 Test RE 0.9999844743920134\n",
      "87 Train Loss 1.4161175 Test MSE 14257.894176713227 Test RE 0.9999844743920134\n",
      "88 Train Loss 1.4164455 Test MSE 14257.894176713227 Test RE 0.9999844743920134\n",
      "89 Train Loss 1.416775 Test MSE 14257.901252866546 Test RE 0.9999847225367426\n",
      "90 Train Loss 1.4170871 Test MSE 14257.901252866546 Test RE 0.9999847225367426\n",
      "91 Train Loss 1.4173832 Test MSE 14257.901252866546 Test RE 0.9999847225367426\n",
      "92 Train Loss 1.4176706 Test MSE 14257.901252866546 Test RE 0.9999847225367426\n",
      "93 Train Loss 1.4179665 Test MSE 14257.913283106624 Test RE 0.999985144409974\n",
      "94 Train Loss 1.4182398 Test MSE 14257.913283106624 Test RE 0.999985144409974\n",
      "95 Train Loss 1.4185305 Test MSE 14257.913283106624 Test RE 0.999985144409974\n",
      "96 Train Loss 1.4188055 Test MSE 14257.913283106624 Test RE 0.999985144409974\n",
      "97 Train Loss 1.4190543 Test MSE 14257.913283106624 Test RE 0.999985144409974\n",
      "98 Train Loss 1.4193234 Test MSE 14257.913283106624 Test RE 0.999985144409974\n",
      "99 Train Loss 1.4195818 Test MSE 14257.913283106624 Test RE 0.999985144409974\n",
      "100 Train Loss 1.4198097 Test MSE 14257.913283106624 Test RE 0.999985144409974\n",
      "101 Train Loss 1.420047 Test MSE 14257.913283106624 Test RE 0.999985144409974\n",
      "102 Train Loss 1.4202963 Test MSE 14257.897040684638 Test RE 0.9999845748250353\n",
      "103 Train Loss 1.4205279 Test MSE 14257.897040684638 Test RE 0.9999845748250353\n",
      "104 Train Loss 1.4207408 Test MSE 14257.896749685866 Test RE 0.9999845646203649\n",
      "105 Train Loss 1.4209595 Test MSE 14257.896749685866 Test RE 0.9999845646203649\n",
      "106 Train Loss 1.4211783 Test MSE 14257.896749685866 Test RE 0.9999845646203649\n",
      "107 Train Loss 1.4213823 Test MSE 14257.890450786474 Test RE 0.9999843437321451\n",
      "108 Train Loss 1.4216112 Test MSE 14257.890450786474 Test RE 0.9999843437321451\n",
      "109 Train Loss 1.4217992 Test MSE 14257.890450786474 Test RE 0.9999843437321451\n",
      "110 Train Loss 1.421994 Test MSE 14257.890450786474 Test RE 0.9999843437321451\n",
      "111 Train Loss 1.4222031 Test MSE 14257.890450786474 Test RE 0.9999843437321451\n",
      "112 Train Loss 1.4223951 Test MSE 14257.890450786474 Test RE 0.9999843437321451\n",
      "113 Train Loss 1.4225792 Test MSE 14257.890450786474 Test RE 0.9999843437321451\n",
      "114 Train Loss 1.422767 Test MSE 14257.891468117637 Test RE 0.9999843794076618\n",
      "115 Train Loss 1.4229447 Test MSE 14257.891468117637 Test RE 0.9999843794076618\n",
      "116 Train Loss 1.4231033 Test MSE 14257.891468117637 Test RE 0.9999843794076618\n",
      "117 Train Loss 1.4232872 Test MSE 14257.891468117637 Test RE 0.9999843794076618\n",
      "118 Train Loss 1.423447 Test MSE 14257.891468117637 Test RE 0.9999843794076618\n",
      "119 Train Loss 1.4236082 Test MSE 14257.891468117637 Test RE 0.9999843794076618\n",
      "120 Train Loss 1.4237809 Test MSE 14257.891468117637 Test RE 0.9999843794076618\n",
      "121 Train Loss 1.4239438 Test MSE 14257.891468117637 Test RE 0.9999843794076618\n",
      "122 Train Loss 1.4240891 Test MSE 14257.891512704125 Test RE 0.9999843809712097\n",
      "123 Train Loss 1.4242419 Test MSE 14257.891512704125 Test RE 0.9999843809712097\n",
      "124 Train Loss 1.4244033 Test MSE 14257.891512704125 Test RE 0.9999843809712097\n",
      "125 Train Loss 1.4245554 Test MSE 14257.891512704125 Test RE 0.9999843809712097\n",
      "126 Train Loss 1.4247068 Test MSE 14257.891512704125 Test RE 0.9999843809712097\n",
      "127 Train Loss 1.4248389 Test MSE 14257.891512704125 Test RE 0.9999843809712097\n",
      "128 Train Loss 1.4249808 Test MSE 14257.891512704125 Test RE 0.9999843809712097\n",
      "129 Train Loss 1.4251231 Test MSE 14257.891512704125 Test RE 0.9999843809712097\n",
      "130 Train Loss 1.4252402 Test MSE 14257.891512704125 Test RE 0.9999843809712097\n",
      "131 Train Loss 1.4253854 Test MSE 14257.896301884966 Test RE 0.9999845489169971\n",
      "132 Train Loss 1.4255164 Test MSE 14257.896301884966 Test RE 0.9999845489169971\n",
      "133 Train Loss 1.4256239 Test MSE 14257.896301884966 Test RE 0.9999845489169971\n",
      "134 Train Loss 1.4257504 Test MSE 14257.896301884966 Test RE 0.9999845489169971\n",
      "135 Train Loss 1.4258996 Test MSE 14257.896301884966 Test RE 0.9999845489169971\n",
      "136 Train Loss 1.4260079 Test MSE 14257.896301884966 Test RE 0.9999845489169971\n",
      "137 Train Loss 1.4261239 Test MSE 14257.896301884966 Test RE 0.9999845489169971\n",
      "138 Train Loss 1.4262451 Test MSE 14257.896301884966 Test RE 0.9999845489169971\n",
      "139 Train Loss 1.4263482 Test MSE 14257.896301884966 Test RE 0.9999845489169971\n",
      "140 Train Loss 1.4264677 Test MSE 14257.894641271378 Test RE 0.9999844906830213\n",
      "141 Train Loss 1.4265689 Test MSE 14257.894641271378 Test RE 0.9999844906830213\n",
      "142 Train Loss 1.426695 Test MSE 14257.894641271378 Test RE 0.9999844906830213\n",
      "143 Train Loss 1.4268049 Test MSE 14257.894641271378 Test RE 0.9999844906830213\n",
      "144 Train Loss 1.4268909 Test MSE 14257.894641271378 Test RE 0.9999844906830213\n",
      "145 Train Loss 1.4270177 Test MSE 14257.894641271378 Test RE 0.9999844906830213\n",
      "146 Train Loss 1.4271035 Test MSE 14257.894641271378 Test RE 0.9999844906830213\n",
      "147 Train Loss 1.4272196 Test MSE 14257.89284057057 Test RE 0.9999844275364984\n",
      "148 Train Loss 1.4272872 Test MSE 14257.89284057057 Test RE 0.9999844275364984\n",
      "149 Train Loss 1.4273993 Test MSE 14257.89284057057 Test RE 0.9999844275364984\n",
      "150 Train Loss 1.4274999 Test MSE 14257.89284057057 Test RE 0.9999844275364984\n",
      "151 Train Loss 1.4275987 Test MSE 14257.89284057057 Test RE 0.9999844275364984\n",
      "152 Train Loss 1.4276773 Test MSE 14257.89284057057 Test RE 0.9999844275364984\n",
      "153 Train Loss 1.427767 Test MSE 14257.89284057057 Test RE 0.9999844275364984\n",
      "154 Train Loss 1.4278467 Test MSE 14257.89284057057 Test RE 0.9999844275364984\n",
      "155 Train Loss 1.4279493 Test MSE 14257.89284057057 Test RE 0.9999844275364984\n",
      "156 Train Loss 1.4280157 Test MSE 14257.89284057057 Test RE 0.9999844275364984\n",
      "157 Train Loss 1.4280965 Test MSE 14257.881889706621 Test RE 0.9999840435142717\n",
      "158 Train Loss 1.4281741 Test MSE 14257.881889706621 Test RE 0.9999840435142717\n",
      "159 Train Loss 1.428291 Test MSE 14257.881889706621 Test RE 0.9999840435142717\n",
      "160 Train Loss 1.4283521 Test MSE 14257.894858006695 Test RE 0.9999844982834406\n",
      "161 Train Loss 1.4284478 Test MSE 14257.894858006695 Test RE 0.9999844982834406\n",
      "162 Train Loss 1.4285402 Test MSE 14257.894858006695 Test RE 0.9999844982834406\n",
      "163 Train Loss 1.428589 Test MSE 14257.891410085076 Test RE 0.9999843773725903\n",
      "164 Train Loss 1.4286753 Test MSE 14257.891410085076 Test RE 0.9999843773725903\n",
      "165 Train Loss 1.4287517 Test MSE 14257.891410085076 Test RE 0.9999843773725903\n",
      "166 Train Loss 1.4288056 Test MSE 14257.891410085076 Test RE 0.9999843773725903\n",
      "167 Train Loss 1.4288886 Test MSE 14257.891410085076 Test RE 0.9999843773725903\n",
      "168 Train Loss 1.4289593 Test MSE 14257.914665522556 Test RE 0.9999851928881537\n",
      "169 Train Loss 1.4290373 Test MSE 14257.914665522556 Test RE 0.9999851928881537\n",
      "170 Train Loss 1.429106 Test MSE 14257.914665522556 Test RE 0.9999851928881537\n",
      "171 Train Loss 1.4291654 Test MSE 14257.914665522556 Test RE 0.9999851928881537\n",
      "172 Train Loss 1.4292384 Test MSE 14257.914665522556 Test RE 0.9999851928881537\n",
      "173 Train Loss 1.4292984 Test MSE 14257.914667632715 Test RE 0.9999851929621519\n",
      "174 Train Loss 1.4293528 Test MSE 14257.914667632715 Test RE 0.9999851929621519\n",
      "175 Train Loss 1.4294292 Test MSE 14257.914667632715 Test RE 0.9999851929621519\n",
      "176 Train Loss 1.4294947 Test MSE 14257.914667632715 Test RE 0.9999851929621519\n",
      "177 Train Loss 1.4295814 Test MSE 14257.905332482544 Test RE 0.9999848655996418\n",
      "178 Train Loss 1.4296328 Test MSE 14257.905332482544 Test RE 0.9999848655996418\n",
      "179 Train Loss 1.4296718 Test MSE 14257.905332482544 Test RE 0.9999848655996418\n",
      "180 Train Loss 1.4297498 Test MSE 14257.905332482544 Test RE 0.9999848655996418\n",
      "181 Train Loss 1.429792 Test MSE 14257.905332482544 Test RE 0.9999848655996418\n",
      "182 Train Loss 1.4298279 Test MSE 14257.905332482544 Test RE 0.9999848655996418\n",
      "183 Train Loss 1.429933 Test MSE 14257.905332482544 Test RE 0.9999848655996418\n",
      "184 Train Loss 1.4299623 Test MSE 14257.905332482544 Test RE 0.9999848655996418\n",
      "185 Train Loss 1.4300108 Test MSE 14257.905332316268 Test RE 0.9999848655938106\n",
      "186 Train Loss 1.4300716 Test MSE 14257.905332316268 Test RE 0.9999848655938106\n",
      "187 Train Loss 1.4301279 Test MSE 14257.905332316268 Test RE 0.9999848655938106\n",
      "188 Train Loss 1.430164 Test MSE 14257.905332316268 Test RE 0.9999848655938106\n",
      "189 Train Loss 1.4302241 Test MSE 14257.905332316268 Test RE 0.9999848655938106\n",
      "190 Train Loss 1.4302998 Test MSE 14257.905332316268 Test RE 0.9999848655938106\n",
      "191 Train Loss 1.4303429 Test MSE 14257.919960677056 Test RE 0.9999853785771456\n",
      "192 Train Loss 1.430384 Test MSE 14257.919960677056 Test RE 0.9999853785771456\n",
      "193 Train Loss 1.4304255 Test MSE 14257.919960677056 Test RE 0.9999853785771456\n",
      "194 Train Loss 1.4304644 Test MSE 14257.919960677056 Test RE 0.9999853785771456\n",
      "195 Train Loss 1.4305303 Test MSE 14257.912052090536 Test RE 0.9999851012410401\n",
      "196 Train Loss 1.4305646 Test MSE 14257.912052090536 Test RE 0.9999851012410401\n",
      "197 Train Loss 1.4306408 Test MSE 14257.912052090536 Test RE 0.9999851012410401\n",
      "198 Train Loss 1.4306756 Test MSE 14257.912052090536 Test RE 0.9999851012410401\n",
      "199 Train Loss 1.4307215 Test MSE 14257.91469654379 Test RE 0.9999851939759976\n",
      "Training time: 90.68\n",
      "Training time: 90.68\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (m_lambda): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 18.260998 Test MSE 14253.448073757781 Test RE 0.9998285474180647\n",
      "1 Train Loss 2.5633202 Test MSE 14256.61266175841 Test RE 0.9999395335389158\n",
      "2 Train Loss 1.9638062 Test MSE 14258.067776744101 Test RE 0.9999905621355839\n",
      "3 Train Loss 1.9559073 Test MSE 14259.067445161289 Test RE 1.0000256174276676\n",
      "4 Train Loss 1.9885992 Test MSE 14259.388182616145 Test RE 1.0000368644418536\n",
      "5 Train Loss 2.0150619 Test MSE 14259.461466567067 Test RE 1.0000394342070285\n",
      "6 Train Loss 2.0409396 Test MSE 14258.999259492834 Test RE 1.0000232264053597\n",
      "7 Train Loss 2.0670202 Test MSE 14258.976675154116 Test RE 1.0000224344538415\n",
      "8 Train Loss 2.0896864 Test MSE 14258.98856135998 Test RE 1.000022851260402\n",
      "9 Train Loss 2.1063304 Test MSE 14259.7145607772 Test RE 1.000048309123806\n",
      "10 Train Loss 2.1260655 Test MSE 14259.747096152909 Test RE 1.0000494499926769\n",
      "11 Train Loss 2.1366591 Test MSE 14259.419732590055 Test RE 1.0000379707698082\n",
      "12 Train Loss 2.1457462 Test MSE 14259.664210273308 Test RE 1.0000465435561812\n",
      "13 Train Loss 2.1550179 Test MSE 14260.16401283675 Test RE 1.0000640692650271\n",
      "14 Train Loss 2.1627908 Test MSE 14260.575665220189 Test RE 1.000078503733886\n",
      "15 Train Loss 2.153332 Test MSE 14260.28993019 Test RE 1.0000684845418284\n",
      "16 Train Loss 2.1483033 Test MSE 14260.29140957693 Test RE 1.0000685364162367\n",
      "17 Train Loss 2.1332767 Test MSE 14259.591579161848 Test RE 1.000043996701522\n",
      "18 Train Loss 2.042397 Test MSE 14259.425358546729 Test RE 1.0000381680488648\n",
      "19 Train Loss 1.5666386 Test MSE 14259.870873323793 Test RE 1.0000537902878526\n",
      "20 Train Loss 1.3688473 Test MSE 14259.161845126295 Test RE 1.0000289276801582\n",
      "21 Train Loss 1.3593713 Test MSE 14259.288413361748 Test RE 1.000033365935972\n",
      "22 Train Loss 1.348264 Test MSE 14259.004758246641 Test RE 1.0000234192267803\n",
      "23 Train Loss 1.3494589 Test MSE 14258.873980345643 Test RE 1.000018833308415\n",
      "24 Train Loss 1.3512706 Test MSE 14258.72563000202 Test RE 1.0000136311611896\n",
      "25 Train Loss 1.353998 Test MSE 14258.710450489312 Test RE 1.0000130988652451\n",
      "26 Train Loss 1.3557104 Test MSE 14258.622826836381 Test RE 1.000010026184317\n",
      "27 Train Loss 1.3544949 Test MSE 14258.294796355078 Test RE 0.9999985231218942\n",
      "28 Train Loss 1.3555771 Test MSE 14258.030017985715 Test RE 0.9999892380281521\n",
      "29 Train Loss 1.3576329 Test MSE 14257.869312055504 Test RE 0.9999836024440869\n",
      "30 Train Loss 1.360018 Test MSE 14257.876500318824 Test RE 0.9999838545204803\n",
      "31 Train Loss 1.3622671 Test MSE 14257.869902639351 Test RE 0.9999836231545502\n",
      "32 Train Loss 1.364475 Test MSE 14257.837293277023 Test RE 0.9999824796160288\n",
      "33 Train Loss 1.3665315 Test MSE 14257.832553440678 Test RE 0.999982313400298\n",
      "34 Train Loss 1.3685508 Test MSE 14257.832553440678 Test RE 0.999982313400298\n",
      "35 Train Loss 1.3704885 Test MSE 14257.84580528257 Test RE 0.9999827781134435\n",
      "36 Train Loss 1.3723202 Test MSE 14257.85288094912 Test RE 0.9999830262415236\n",
      "37 Train Loss 1.3740928 Test MSE 14257.85288094912 Test RE 0.9999830262415236\n",
      "38 Train Loss 1.3758068 Test MSE 14257.857582353878 Test RE 0.9999831911094254\n",
      "39 Train Loss 1.3774575 Test MSE 14257.857582353878 Test RE 0.9999831911094254\n",
      "40 Train Loss 1.3790143 Test MSE 14257.864044566255 Test RE 0.9999834177249243\n",
      "41 Train Loss 1.3805536 Test MSE 14257.864044566255 Test RE 0.9999834177249243\n",
      "42 Train Loss 1.3820212 Test MSE 14257.864044566255 Test RE 0.9999834177249243\n",
      "43 Train Loss 1.3834109 Test MSE 14257.881395261618 Test RE 0.9999840261751912\n",
      "44 Train Loss 1.3847836 Test MSE 14257.881395261618 Test RE 0.9999840261751912\n",
      "45 Train Loss 1.3860964 Test MSE 14257.890399431451 Test RE 0.99998434193124\n",
      "46 Train Loss 1.3873394 Test MSE 14257.901720092721 Test RE 0.9999847389213079\n",
      "47 Train Loss 1.3885525 Test MSE 14257.901720092721 Test RE 0.9999847389213079\n",
      "48 Train Loss 1.3897156 Test MSE 14257.903839369776 Test RE 0.9999848132395587\n",
      "49 Train Loss 1.3908381 Test MSE 14257.903839369776 Test RE 0.9999848132395587\n",
      "50 Train Loss 1.3919275 Test MSE 14257.911087097367 Test RE 0.9999850674009235\n",
      "51 Train Loss 1.3929871 Test MSE 14257.911087097367 Test RE 0.9999850674009235\n",
      "52 Train Loss 1.3939836 Test MSE 14257.912577301222 Test RE 0.9999851196589845\n",
      "53 Train Loss 1.394972 Test MSE 14257.912577301222 Test RE 0.9999851196589845\n",
      "54 Train Loss 1.3959119 Test MSE 14257.912577301222 Test RE 0.9999851196589845\n",
      "55 Train Loss 1.3968436 Test MSE 14257.912577301222 Test RE 0.9999851196589845\n",
      "56 Train Loss 1.397704 Test MSE 14257.902901194315 Test RE 0.9999847803398689\n",
      "57 Train Loss 1.3985764 Test MSE 14257.902901194315 Test RE 0.9999847803398689\n",
      "58 Train Loss 1.3993924 Test MSE 14257.877926802805 Test RE 0.9999839045440905\n",
      "59 Train Loss 1.4002063 Test MSE 14257.877926802805 Test RE 0.9999839045440905\n",
      "60 Train Loss 1.400958 Test MSE 14257.877926802805 Test RE 0.9999839045440905\n",
      "61 Train Loss 1.401732 Test MSE 14257.877926802805 Test RE 0.9999839045440905\n",
      "62 Train Loss 1.402487 Test MSE 14257.877926802805 Test RE 0.9999839045440905\n",
      "63 Train Loss 1.4031837 Test MSE 14257.877926802805 Test RE 0.9999839045440905\n",
      "64 Train Loss 1.4038819 Test MSE 14257.85817984662 Test RE 0.9999832120621768\n",
      "65 Train Loss 1.4045663 Test MSE 14257.85817984662 Test RE 0.9999832120621768\n",
      "66 Train Loss 1.4051708 Test MSE 14257.85817984662 Test RE 0.9999832120621768\n",
      "67 Train Loss 1.4058003 Test MSE 14257.87487601616 Test RE 0.9999837975598072\n",
      "68 Train Loss 1.4064196 Test MSE 14257.87487601616 Test RE 0.9999837975598072\n",
      "69 Train Loss 1.4070199 Test MSE 14257.87487601616 Test RE 0.9999837975598072\n",
      "70 Train Loss 1.4075692 Test MSE 14257.87487601616 Test RE 0.9999837975598072\n",
      "71 Train Loss 1.4081293 Test MSE 14257.881545317769 Test RE 0.9999840314373248\n",
      "72 Train Loss 1.4086717 Test MSE 14257.881545317769 Test RE 0.9999840314373248\n",
      "73 Train Loss 1.4091899 Test MSE 14257.881545317769 Test RE 0.9999840314373248\n",
      "74 Train Loss 1.4097054 Test MSE 14257.868309206235 Test RE 0.9999835672763904\n",
      "75 Train Loss 1.4102077 Test MSE 14257.868309206235 Test RE 0.9999835672763904\n",
      "76 Train Loss 1.4106833 Test MSE 14257.868309206235 Test RE 0.9999835672763904\n",
      "77 Train Loss 1.4111717 Test MSE 14257.868309206235 Test RE 0.9999835672763904\n",
      "78 Train Loss 1.4116493 Test MSE 14257.868309206235 Test RE 0.9999835672763904\n",
      "79 Train Loss 1.4121015 Test MSE 14257.868309206235 Test RE 0.9999835672763904\n",
      "80 Train Loss 1.4124857 Test MSE 14257.868309206235 Test RE 0.9999835672763904\n",
      "81 Train Loss 1.4129249 Test MSE 14257.868309206235 Test RE 0.9999835672763904\n",
      "82 Train Loss 1.4133185 Test MSE 14257.867072224886 Test RE 0.9999835238982001\n",
      "83 Train Loss 1.4137535 Test MSE 14257.865461584874 Test RE 0.999983467416627\n",
      "84 Train Loss 1.414117 Test MSE 14257.865461584874 Test RE 0.999983467416627\n",
      "85 Train Loss 1.4145312 Test MSE 14257.865461584874 Test RE 0.999983467416627\n",
      "86 Train Loss 1.4148793 Test MSE 14257.865461584874 Test RE 0.999983467416627\n",
      "87 Train Loss 1.415223 Test MSE 14257.865461584874 Test RE 0.999983467416627\n",
      "88 Train Loss 1.4155806 Test MSE 14257.865461584874 Test RE 0.999983467416627\n",
      "89 Train Loss 1.4159417 Test MSE 14257.865461584874 Test RE 0.999983467416627\n",
      "90 Train Loss 1.4162928 Test MSE 14257.865461584874 Test RE 0.999983467416627\n",
      "91 Train Loss 1.4166149 Test MSE 14257.865461584874 Test RE 0.999983467416627\n",
      "92 Train Loss 1.4169118 Test MSE 14257.865461584874 Test RE 0.999983467416627\n",
      "93 Train Loss 1.4172196 Test MSE 14257.877243692528 Test RE 0.9999838805889377\n",
      "94 Train Loss 1.417514 Test MSE 14257.877243692528 Test RE 0.9999838805889377\n",
      "95 Train Loss 1.4178458 Test MSE 14257.877243692528 Test RE 0.9999838805889377\n",
      "96 Train Loss 1.4181607 Test MSE 14257.877243692528 Test RE 0.9999838805889377\n",
      "97 Train Loss 1.4184198 Test MSE 14257.877243692528 Test RE 0.9999838805889377\n",
      "98 Train Loss 1.418701 Test MSE 14257.877243692528 Test RE 0.9999838805889377\n",
      "99 Train Loss 1.4189882 Test MSE 14257.877243692528 Test RE 0.9999838805889377\n",
      "100 Train Loss 1.4192107 Test MSE 14257.871103731723 Test RE 0.9999836652741895\n",
      "101 Train Loss 1.419469 Test MSE 14257.871103731723 Test RE 0.9999836652741895\n",
      "102 Train Loss 1.4197608 Test MSE 14257.871103731723 Test RE 0.9999836652741895\n",
      "103 Train Loss 1.4199971 Test MSE 14257.871103731723 Test RE 0.9999836652741895\n",
      "104 Train Loss 1.4202188 Test MSE 14257.871103731723 Test RE 0.9999836652741895\n",
      "105 Train Loss 1.4204648 Test MSE 14257.871103731723 Test RE 0.9999836652741895\n",
      "106 Train Loss 1.4207194 Test MSE 14257.871103731723 Test RE 0.9999836652741895\n",
      "107 Train Loss 1.4209033 Test MSE 14257.871103731723 Test RE 0.9999836652741895\n",
      "108 Train Loss 1.4211564 Test MSE 14257.871103731723 Test RE 0.9999836652741895\n",
      "109 Train Loss 1.4213651 Test MSE 14257.871103731723 Test RE 0.9999836652741895\n",
      "110 Train Loss 1.4215662 Test MSE 14257.871103731723 Test RE 0.9999836652741895\n",
      "111 Train Loss 1.4217935 Test MSE 14257.871103731723 Test RE 0.9999836652741895\n",
      "112 Train Loss 1.4220006 Test MSE 14257.876417259062 Test RE 0.9999838516077594\n",
      "113 Train Loss 1.4222099 Test MSE 14257.876417259062 Test RE 0.9999838516077594\n",
      "114 Train Loss 1.4224031 Test MSE 14257.876417259062 Test RE 0.9999838516077594\n",
      "115 Train Loss 1.42259 Test MSE 14257.876417259062 Test RE 0.9999838516077594\n",
      "116 Train Loss 1.4227496 Test MSE 14257.876417259062 Test RE 0.9999838516077594\n",
      "117 Train Loss 1.4229552 Test MSE 14257.876417259062 Test RE 0.9999838516077594\n",
      "118 Train Loss 1.4231223 Test MSE 14257.868718806358 Test RE 0.9999835816401572\n",
      "119 Train Loss 1.4232874 Test MSE 14257.868718806358 Test RE 0.9999835816401572\n",
      "120 Train Loss 1.4234899 Test MSE 14257.868718806358 Test RE 0.9999835816401572\n",
      "121 Train Loss 1.4236674 Test MSE 14257.868718806358 Test RE 0.9999835816401572\n",
      "122 Train Loss 1.4237983 Test MSE 14257.868718806358 Test RE 0.9999835816401572\n",
      "123 Train Loss 1.4239725 Test MSE 14257.868718806358 Test RE 0.9999835816401572\n",
      "124 Train Loss 1.4241437 Test MSE 14257.868718806358 Test RE 0.9999835816401572\n",
      "125 Train Loss 1.4243225 Test MSE 14257.868718806358 Test RE 0.9999835816401572\n",
      "126 Train Loss 1.4244573 Test MSE 14257.871531765692 Test RE 0.9999836802843887\n",
      "127 Train Loss 1.4246242 Test MSE 14257.871531765692 Test RE 0.9999836802843887\n",
      "128 Train Loss 1.4247621 Test MSE 14257.871531765692 Test RE 0.9999836802843887\n",
      "129 Train Loss 1.4249164 Test MSE 14257.871531765692 Test RE 0.9999836802843887\n",
      "130 Train Loss 1.4250442 Test MSE 14257.871531765692 Test RE 0.9999836802843887\n",
      "131 Train Loss 1.4251993 Test MSE 14257.871531765692 Test RE 0.9999836802843887\n",
      "132 Train Loss 1.4253404 Test MSE 14257.871531765692 Test RE 0.9999836802843887\n",
      "133 Train Loss 1.4254264 Test MSE 14257.871531765692 Test RE 0.9999836802843887\n",
      "134 Train Loss 1.4255697 Test MSE 14257.871531765692 Test RE 0.9999836802843887\n",
      "135 Train Loss 1.4257501 Test MSE 14257.871531765692 Test RE 0.9999836802843887\n",
      "136 Train Loss 1.4258603 Test MSE 14257.871531765692 Test RE 0.9999836802843887\n",
      "137 Train Loss 1.4259566 Test MSE 14257.871531765692 Test RE 0.9999836802843887\n",
      "138 Train Loss 1.4260949 Test MSE 14257.87150561141 Test RE 0.9999836793672164\n",
      "139 Train Loss 1.4262058 Test MSE 14257.87150561141 Test RE 0.9999836793672164\n",
      "140 Train Loss 1.4263476 Test MSE 14257.87150561141 Test RE 0.9999836793672164\n",
      "141 Train Loss 1.4264482 Test MSE 14257.87150561141 Test RE 0.9999836793672164\n",
      "142 Train Loss 1.4265809 Test MSE 14257.87150561141 Test RE 0.9999836793672164\n",
      "143 Train Loss 1.4267169 Test MSE 14257.87150561141 Test RE 0.9999836793672164\n",
      "144 Train Loss 1.4267832 Test MSE 14257.87150561141 Test RE 0.9999836793672164\n",
      "145 Train Loss 1.4269288 Test MSE 14257.871326896675 Test RE 0.999983673100088\n",
      "146 Train Loss 1.4270228 Test MSE 14257.871326896675 Test RE 0.999983673100088\n",
      "147 Train Loss 1.427143 Test MSE 14257.871326896675 Test RE 0.999983673100088\n",
      "148 Train Loss 1.4272227 Test MSE 14257.871326896675 Test RE 0.999983673100088\n",
      "149 Train Loss 1.4273328 Test MSE 14257.871326896675 Test RE 0.999983673100088\n",
      "150 Train Loss 1.4274484 Test MSE 14257.871326896675 Test RE 0.999983673100088\n",
      "151 Train Loss 1.427577 Test MSE 14257.871326896675 Test RE 0.999983673100088\n",
      "152 Train Loss 1.4276292 Test MSE 14257.871326896675 Test RE 0.999983673100088\n",
      "153 Train Loss 1.42769 Test MSE 14257.871326896675 Test RE 0.999983673100088\n",
      "154 Train Loss 1.4278013 Test MSE 14257.871326896675 Test RE 0.999983673100088\n",
      "155 Train Loss 1.4279294 Test MSE 14257.871326896675 Test RE 0.999983673100088\n",
      "156 Train Loss 1.427967 Test MSE 14257.871326896675 Test RE 0.999983673100088\n",
      "157 Train Loss 1.4280413 Test MSE 14257.871326896675 Test RE 0.999983673100088\n",
      "158 Train Loss 1.4281361 Test MSE 14257.871326896675 Test RE 0.999983673100088\n",
      "159 Train Loss 1.4282753 Test MSE 14257.871326896675 Test RE 0.999983673100088\n",
      "160 Train Loss 1.4283321 Test MSE 14257.871326896675 Test RE 0.999983673100088\n",
      "161 Train Loss 1.4284464 Test MSE 14257.871326896675 Test RE 0.999983673100088\n",
      "162 Train Loss 1.4285679 Test MSE 14257.871326896675 Test RE 0.999983673100088\n",
      "163 Train Loss 1.4285771 Test MSE 14257.871326896675 Test RE 0.999983673100088\n",
      "164 Train Loss 1.4286869 Test MSE 14257.871326896675 Test RE 0.999983673100088\n",
      "165 Train Loss 1.4287692 Test MSE 14257.871326896675 Test RE 0.999983673100088\n",
      "166 Train Loss 1.4288088 Test MSE 14257.871326896675 Test RE 0.999983673100088\n",
      "167 Train Loss 1.4289111 Test MSE 14257.871326896675 Test RE 0.999983673100088\n",
      "168 Train Loss 1.4289784 Test MSE 14257.871326896675 Test RE 0.999983673100088\n",
      "169 Train Loss 1.42907 Test MSE 14257.871326896675 Test RE 0.999983673100088\n",
      "170 Train Loss 1.4291524 Test MSE 14257.871326896675 Test RE 0.999983673100088\n",
      "171 Train Loss 1.4292002 Test MSE 14257.871326896675 Test RE 0.999983673100088\n",
      "172 Train Loss 1.4292741 Test MSE 14257.871326896675 Test RE 0.999983673100088\n",
      "173 Train Loss 1.4293381 Test MSE 14257.871326896675 Test RE 0.999983673100088\n",
      "174 Train Loss 1.429389 Test MSE 14257.871326896675 Test RE 0.999983673100088\n",
      "175 Train Loss 1.4294701 Test MSE 14257.87758394685 Test RE 0.9999838925208974\n",
      "176 Train Loss 1.4295558 Test MSE 14257.87758394685 Test RE 0.9999838925208974\n",
      "177 Train Loss 1.429648 Test MSE 14257.87758394685 Test RE 0.9999838925208974\n",
      "178 Train Loss 1.4297014 Test MSE 14257.87758394685 Test RE 0.9999838925208974\n",
      "179 Train Loss 1.4297627 Test MSE 14257.87758394685 Test RE 0.9999838925208974\n",
      "180 Train Loss 1.4298172 Test MSE 14257.868165030899 Test RE 0.9999835622204815\n",
      "181 Train Loss 1.4298996 Test MSE 14257.868165030899 Test RE 0.9999835622204815\n",
      "182 Train Loss 1.4298995 Test MSE 14257.868165030899 Test RE 0.9999835622204815\n",
      "183 Train Loss 1.4300225 Test MSE 14257.868165030899 Test RE 0.9999835622204815\n",
      "184 Train Loss 1.4300656 Test MSE 14257.868165030899 Test RE 0.9999835622204815\n",
      "185 Train Loss 1.4301112 Test MSE 14257.868165030899 Test RE 0.9999835622204815\n",
      "186 Train Loss 1.4301627 Test MSE 14257.868165030899 Test RE 0.9999835622204815\n",
      "187 Train Loss 1.4302173 Test MSE 14257.868165030899 Test RE 0.9999835622204815\n",
      "188 Train Loss 1.4302566 Test MSE 14257.868165030899 Test RE 0.9999835622204815\n",
      "189 Train Loss 1.4302912 Test MSE 14257.868165030899 Test RE 0.9999835622204815\n",
      "190 Train Loss 1.4304155 Test MSE 14257.868165030899 Test RE 0.9999835622204815\n",
      "191 Train Loss 1.430478 Test MSE 14257.868165030899 Test RE 0.9999835622204815\n",
      "192 Train Loss 1.4305077 Test MSE 14257.868165030899 Test RE 0.9999835622204815\n",
      "193 Train Loss 1.4305229 Test MSE 14257.868165030899 Test RE 0.9999835622204815\n",
      "194 Train Loss 1.4305744 Test MSE 14257.868912508406 Test RE 0.9999835884328577\n",
      "195 Train Loss 1.4306382 Test MSE 14257.868912508406 Test RE 0.9999835884328577\n",
      "196 Train Loss 1.4307071 Test MSE 14257.868912508406 Test RE 0.9999835884328577\n",
      "197 Train Loss 1.43077 Test MSE 14257.868912508406 Test RE 0.9999835884328577\n",
      "198 Train Loss 1.4308223 Test MSE 14257.868912508406 Test RE 0.9999835884328577\n",
      "199 Train Loss 1.4308952 Test MSE 14257.87710725059 Test RE 0.9999838758042233\n",
      "Training time: 90.40\n",
      "Training time: 90.40\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 200\n",
    "\n",
    "N_f = 1000\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss =[]\n",
    "    \n",
    "    'Generate Training data'\n",
    "    torch.manual_seed(reps*36)\n",
    "     #Total number of collocation points \n",
    "    \n",
    "    \n",
    "    layers = np.array([1,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "    PINN = Sequentialmodel(layers)\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "    \n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.25, \n",
    "                              max_iter = 10, \n",
    "                              max_eval = 15, \n",
    "                              tolerance_grad = 1e-5, \n",
    "                              tolerance_change = 1e-5, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "    \n",
    "    optimizer_lambda = torch.optim.Adam(PINN.parameters(), lr=5e-3)\n",
    "   \n",
    "\n",
    "    start_time = time.time()\n",
    "    train_model(max_iter,reps)\n",
    "\n",
    "    \n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    " \n",
    "    \n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full, \"test_re_loss\": test_re_full, \"Time\": elapsed_time, \"label\": label, \"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7f1d204a1350>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "u_pred = PINN.test()\n",
    "plt.plot(u_pred,'r')\n",
    "plt.plot(y_true,'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.000034814272743\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + test_re_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
