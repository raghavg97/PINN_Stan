{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.autograd as autograd         # computation graph\n",
    "from torch import Tensor                  # tensor node in the computation graph\n",
    "import torch.nn as nn                     # neural networks\n",
    "import torch.optim as optim               # optimizers e.g. gradient descent, ADAM, etc.\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import matplotlib.ticker\n",
    "from torch.nn.parameter import Parameter\n",
    "\n",
    "import numpy as np\n",
    "import time\n",
    "from pyDOE import lhs         #Latin Hypercube Sampling\n",
    "import scipy.io\n",
    "\n",
    "from smt.sampling_methods import LHS\n",
    "from scipy.io import savemat\n",
    "\n",
    "#Set default dtype to float32\n",
    "torch.set_default_dtype(torch.float)\n",
    "\n",
    "#PyTorch random number generator\n",
    "torch.manual_seed(1234)\n",
    "\n",
    "# Random number generators in other libraries\n",
    "np.random.seed(1234)\n",
    "\n",
    "# Device configuration\n",
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "print(device)\n",
    "\n",
    "if device == 'cuda': \n",
    "    print(torch.cuda.get_device_name())\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_1D_1(x): #True function for 1D_1 dy2/dx2 + dy/dx - 6y = 0; BC1: y(0)=2; BC2: dy/dx at (x=0) = -1;\n",
    "    y = np.exp(2.0*x) + np.exp(-3.0*x)\n",
    "    return y\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "level = \"medium\"\n",
    "label = \"1D_SODE_tanhAW_\" + level\n",
    "\n",
    "u_coeff = 6.0\n",
    "fo_val = -1.0\n",
    "\n",
    "loss_thresh = 0.005\n",
    "\n",
    "x = np.linspace(0,2,100).reshape(-1,1)\n",
    "\n",
    "bc1_x = x[0].reshape(-1,1)\n",
    "bc1_y = true_1D_1(x[0]).reshape(-1,1)\n",
    "x_bc1_train = torch.from_numpy(bc1_x).float().to(device)\n",
    "y_bc1_train = torch.from_numpy(bc1_y).float().to(device)    \n",
    "\n",
    "bc2_x = x[0].reshape(-1,1)\n",
    "x_bc2_train = torch.from_numpy(bc2_x).float().to(device)\n",
    "bc2_val = torch.tensor(fo_val,device=device)\n",
    "bc2_val = bc2_val.view(1,1)\n",
    "\n",
    "\n",
    "x_test = x.reshape(-1,1)\n",
    "x_test_tensor = torch.from_numpy(x_test).float().to(device)\n",
    "y_true = true_1D_1(x_test)\n",
    "y_true_norm = np.linalg.norm(y_true,2)\n",
    "\n",
    "# Domain bounds\n",
    "lb = np.array(x[0]) \n",
    "ub = np.array(x[-1]) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def colloc_pts(N_f,seed):\n",
    "    #Collocation Points\n",
    "    # Latin Hypercube sampling for collocation points \n",
    "    # N_f sets of tuples(x,y)\n",
    "    x01 = np.array([[0.0, 1.0]])\n",
    "    sampling = LHS(xlimits=x01,random_state =seed)\n",
    "    \n",
    "    x_coll_train = lb + (ub-lb)*sampling(N_f)\n",
    "    x_coll_train = np.vstack((x_coll_train, bc1_x.reshape(-1,1))) # append training points to collocation points \n",
    "\n",
    "    return x_coll_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Sequentialmodel(nn.Module):\n",
    "    \n",
    "    def __init__(self,layers):\n",
    "        super().__init__() #call __init__ from parent class \n",
    "              \n",
    "    \n",
    "        self.activation = nn.Tanh()\n",
    "        self.m_lambda = nn.Sigmoid()\n",
    "        \n",
    "        self.loss_function = nn.MSELoss(reduction ='mean')\n",
    "        \n",
    "        'Initialise neural network as a list using nn.Modulelist'  \n",
    "        self.linears = nn.ModuleList([nn.Linear(layers[i], layers[i+1]) for i in range(len(layers)-1)])\n",
    "        \n",
    "        for i in range(len(layers)-1):\n",
    "            nn.init.xavier_normal_(self.linears[i].weight.data, gain=1.0)\n",
    "            # set biases to zero\n",
    "            nn.init.zeros_(self.linears[i].bias.data) \n",
    "            \n",
    "        self.lambdas_bc1 = Parameter(torch.ones(1,1))\n",
    "        self.lambdas_bc1.requiresGrad = True\n",
    "        \n",
    "        self.lambdas_bc2 = Parameter(torch.ones(1,1))\n",
    "        self.lambdas_bc2.requiresGrad = True\n",
    "        \n",
    "        self.lambdas_f = Parameter(torch.ones(N_f+1,1))\n",
    "        self.lambdas_f.requiresGrad = True\n",
    "             \n",
    "\n",
    "    'forward pass'\n",
    "    def forward(self,x):\n",
    "        if torch.is_tensor(x) != True:         \n",
    "            x = torch.from_numpy(x)                \n",
    "        \n",
    "        u_b = torch.from_numpy(ub).float().to(device)\n",
    "        l_b = torch.from_numpy(lb).float().to(device)\n",
    "                      \n",
    "        #preprocessing input \n",
    "        x = 2.0*(x - l_b)/(u_b - l_b) - 1.0 #feature scaling\n",
    "        \n",
    "        #convert to float\n",
    "        a = x.float()\n",
    "        \n",
    "        for i in range(len(layers)-2):\n",
    "            z = self.linears[i](a)\n",
    "            a = self.activation(z)\n",
    "            \n",
    "        a = self.linears[-1](a) \n",
    "         \n",
    "        return a\n",
    "\n",
    "                        \n",
    "    def loss_BC1(self,x,y,lambda_ind):\n",
    "        \n",
    "        m = self.m_lambda(self.lambdas_bc1)\n",
    "        u_pred = self.forward(x)\n",
    "        \n",
    "        if(lambda_ind):\n",
    "            u_pred = u_pred.detach()\n",
    "        else:\n",
    "            m = m.detach()\n",
    "            \n",
    "        loss_bc1 = torch.sum(m*torch.square(u_pred - y))/2.0\n",
    "        \n",
    "        # loss_bc1 = self.loss_function(self.forward(x), y)\n",
    "                \n",
    "        return loss_bc1\n",
    "    \n",
    "    def loss_BC2(self,x_bc2,bc2_val,lambda_ind):\n",
    "        g = x_bc2.clone()             \n",
    "        g.requires_grad = True\n",
    "        y = self.forward(g)    \n",
    "            \n",
    "        y_x = autograd.grad(y,g,torch.ones([x_bc2.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        \n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        bc2 = dy_dx\n",
    "        \n",
    "        m = self.m_lambda(self.lambdas_bc2)\n",
    "        \n",
    "        if(lambda_ind):\n",
    "            bc2 = bc2.detach()\n",
    "        else:\n",
    "            m = m.detach()\n",
    "        \n",
    "        loss_bc2 = torch.sum(m*torch.square(bc2 - bc2_val))/2.0\n",
    "\n",
    "        return loss_bc2\n",
    "    \n",
    "    def loss_PDE(self, x_coll,f_hat,lambda_ind):\n",
    "             \n",
    "        g = x_coll.clone()             \n",
    "        g.requires_grad = True\n",
    "  \n",
    "        y = self.forward(g) \n",
    "\n",
    "        y_x = autograd.grad(y,g,torch.ones([x_coll.shape[0], 1]).to(device), retain_graph=True, create_graph=True,allow_unused = True)[0]\n",
    "        y_xx = autograd.grad(y_x,g,torch.ones(x_coll.shape).to(device), create_graph=True,allow_unused = True)[0]\n",
    "\n",
    "        dy_dx = y_x[:,[0]]\n",
    "        \n",
    "        dy2_d2x = y_xx[:,[0]]\n",
    "        \n",
    "        f = dy2_d2x + dy_dx - u_coeff*y\n",
    "\n",
    "        m = self.m_lambda(self.lambdas_f)\n",
    "        \n",
    "        if(lambda_ind):\n",
    "            f = f.detach()\n",
    "        else:\n",
    "            m = m.detach()\n",
    "        \n",
    "        #loss_f  = torch.sum(m*(torch.square(f)))/2.0\n",
    "        loss_f = (N_f+1)*self.loss_function(m*(torch.square(f)),f_hat)/2.0\n",
    "        \n",
    "        return loss_f\n",
    "    \n",
    "    \n",
    "    def loss(self,x_bc1,y_bc1,x_bc2,bc2_val,x_coll,f_hat):\n",
    "\n",
    "        lambda_ind = False\n",
    "        \n",
    "        loss_bc1 = self.loss_BC1(x_bc1,y_bc1,lambda_ind)\n",
    "        loss_bc2 = self.loss_BC2(x_bc2,bc2_val,lambda_ind)\n",
    "        loss_f = self.loss_PDE(x_coll,f_hat,lambda_ind)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return loss_val\n",
    "    \n",
    "    def loss_lambdas(self,x_bc1,y_bc1,x_bc2,bc2_val,x_coll,f_hat):\n",
    "\n",
    "        lambda_ind = True        \n",
    "        loss_bc1 = self.loss_BC1(x_bc1,y_bc1,lambda_ind)\n",
    "        loss_bc2 = self.loss_BC2(x_bc2,bc2_val,lambda_ind)\n",
    "        loss_f = self.loss_PDE(x_coll,f_hat,lambda_ind)\n",
    "        \n",
    "        loss_val = loss_bc1 + loss_bc2 + loss_f\n",
    "        \n",
    "        return -1.0*loss_val\n",
    "          \n",
    "    'test neural network'\n",
    "    \n",
    "    def test(self):\n",
    "        y_pred = self.forward(x_test_tensor)\n",
    "        y_pred = y_pred.cpu().detach().numpy()\n",
    "\n",
    "        return y_pred\n",
    "    \n",
    "    def test_loss(self):\n",
    "        y_pred = self.test()\n",
    "        \n",
    "        test_mse = np.mean(np.square(y_pred.reshape(-1,1) - y_true.reshape(-1,1)))\n",
    "        test_re = np.linalg.norm(y_pred.reshape(-1,1) - y_true.reshape(-1,1),2)/y_true_norm\n",
    "        \n",
    "        return test_mse, test_re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_step(x_coll,f_hat):\n",
    "    def closure():\n",
    "        optimizer.zero_grad()\n",
    "        loss = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat)\n",
    "        loss.backward()\n",
    "        \n",
    "        return loss\n",
    "    \n",
    "    optimizer.step(closure)\n",
    "    \n",
    "    for i in range(20):\n",
    "        optimizer_lambda.zero_grad()\n",
    "        loss = PINN.loss_lambdas(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat)\n",
    "        loss.backward()\n",
    "        optimizer_lambda.step()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_update(loss_np):\n",
    "    train_loss.append(loss_np)\n",
    "\n",
    "    \n",
    "    test_mse, test_re = PINN.test_loss()\n",
    "    test_mse_loss.append(test_mse)\n",
    "    test_re_loss.append(test_re)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(max_iter,rep):\n",
    "    print(rep) \n",
    "    torch.manual_seed(rep*123)\n",
    "    start_time = time.time()\n",
    "    thresh_flag = 0\n",
    "\n",
    "    for i in range(max_iter):\n",
    "        x_coll = torch.from_numpy(colloc_pts(N_f,i*11)).float().to(device)\n",
    "        f_hat = torch.zeros(x_coll.shape[0],1).to(device)\n",
    "        train_step(x_coll,f_hat)\n",
    "        \n",
    "        loss_np = PINN.loss(x_bc1_train,y_bc1_train,x_bc2_train,bc2_val,x_coll,f_hat).cpu().detach().numpy()\n",
    "        if(thresh_flag == 0):\n",
    "            if(loss_np < loss_thresh):\n",
    "                time_threshold[rep] = time.time() - start_time\n",
    "                epoch_threshold[rep] = i+1            \n",
    "                thresh_flag = 1       \n",
    "        data_update(loss_np)\n",
    "        print(i,\"Train Loss\",train_loss[-1],\"Test MSE\",test_mse_loss[-1],\"Test RE\",test_re_loss[-1])\n",
    "    \n",
    "    elapsed_time[rep] = time.time() - start_time\n",
    "    print('Training time: %.2f' % (elapsed_time[rep]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (m_lambda): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "0\n",
      "0 Train Loss 9.827199 Test MSE 386.2930285138861 Test RE 1.0018922512834345\n",
      "1 Train Loss 2.1615813 Test MSE 385.5258656355007 Test RE 1.0008968973997407\n",
      "2 Train Loss 1.9304618 Test MSE 385.195267053433 Test RE 1.0004676576355334\n",
      "3 Train Loss 1.9613669 Test MSE 385.04857710375984 Test RE 1.0002771405943076\n",
      "4 Train Loss 1.9891676 Test MSE 384.82988911184196 Test RE 0.999993047001203\n",
      "5 Train Loss 2.006002 Test MSE 384.9702332659057 Test RE 1.000175374815474\n",
      "6 Train Loss 2.032779 Test MSE 385.10294665593494 Test RE 1.0003477585679255\n",
      "7 Train Loss 2.0428379 Test MSE 385.31609928275225 Test RE 1.0006245641045857\n",
      "8 Train Loss 2.0432968 Test MSE 385.57795834916595 Test RE 1.0009645163054555\n",
      "9 Train Loss 2.0248163 Test MSE 385.7031578580408 Test RE 1.0011270127497118\n",
      "10 Train Loss 2.013387 Test MSE 385.67989509865606 Test RE 1.00109682200911\n",
      "11 Train Loss 2.0144207 Test MSE 385.53262955470495 Test RE 1.0009056775577734\n",
      "12 Train Loss 1.950366 Test MSE 385.8428120582068 Test RE 1.0013082388247854\n",
      "13 Train Loss 1.8868605 Test MSE 386.05906479886835 Test RE 1.0015888003802602\n",
      "14 Train Loss 1.4214824 Test MSE 385.39322673972487 Test RE 1.0007247049584793\n",
      "15 Train Loss 1.1650469 Test MSE 385.134193021111 Test RE 1.0003883407000924\n",
      "16 Train Loss 1.1302838 Test MSE 384.9626493838585 Test RE 1.0001655230804942\n",
      "17 Train Loss 1.1331987 Test MSE 384.93878787594093 Test RE 1.0001345254922653\n",
      "18 Train Loss 1.1363096 Test MSE 384.8739631996499 Test RE 1.0000503093406825\n",
      "19 Train Loss 1.1388104 Test MSE 384.7269368251084 Test RE 0.9998592756124476\n",
      "20 Train Loss 1.1421534 Test MSE 384.7116653835936 Test RE 0.9998394310908949\n",
      "21 Train Loss 1.1453435 Test MSE 384.7085253362272 Test RE 0.9998353506978872\n",
      "22 Train Loss 1.1483922 Test MSE 384.70903819599977 Test RE 0.9998360171442042\n",
      "23 Train Loss 1.1513662 Test MSE 384.7108472732362 Test RE 0.9998383679837646\n",
      "24 Train Loss 1.154244 Test MSE 384.7108472732362 Test RE 0.9998383679837646\n",
      "25 Train Loss 1.1569685 Test MSE 384.72359948674034 Test RE 0.9998549389314924\n",
      "26 Train Loss 1.1596175 Test MSE 384.72359948674034 Test RE 0.9998549389314924\n",
      "27 Train Loss 1.1621763 Test MSE 384.72359948674034 Test RE 0.9998549389314924\n",
      "28 Train Loss 1.1645749 Test MSE 384.7285088769519 Test RE 0.999861318397682\n",
      "29 Train Loss 1.1669513 Test MSE 384.7347630902187 Test RE 0.9998694453240269\n",
      "30 Train Loss 1.1692762 Test MSE 384.7347630902187 Test RE 0.9998694453240269\n",
      "31 Train Loss 1.1714464 Test MSE 384.7531913648017 Test RE 0.9998933912340602\n",
      "32 Train Loss 1.1735985 Test MSE 384.7531913648017 Test RE 0.9998933912340602\n",
      "33 Train Loss 1.17551 Test MSE 384.7401655251759 Test RE 0.999876465369003\n",
      "34 Train Loss 1.1771382 Test MSE 384.76355105839764 Test RE 0.999906852486192\n",
      "35 Train Loss 1.1776944 Test MSE 384.6470191969802 Test RE 0.9997554220422902\n",
      "36 Train Loss 1.1785666 Test MSE 384.41470710495264 Test RE 0.9994534694195768\n",
      "37 Train Loss 1.1794899 Test MSE 384.27919061879584 Test RE 0.9992772868248105\n",
      "38 Train Loss 1.1811041 Test MSE 384.27849905662845 Test RE 0.9992763876573639\n",
      "39 Train Loss 1.1826742 Test MSE 384.25691438365664 Test RE 0.999248322911382\n",
      "40 Train Loss 1.1841735 Test MSE 384.24808493786054 Test RE 0.9992368424939103\n",
      "41 Train Loss 1.1856856 Test MSE 384.24203424714904 Test RE 0.9992289750547216\n",
      "42 Train Loss 1.1871077 Test MSE 384.24203424714904 Test RE 0.9992289750547216\n",
      "43 Train Loss 1.1884362 Test MSE 384.2311408086441 Test RE 0.9992148106536773\n",
      "44 Train Loss 1.1897596 Test MSE 384.2311408086441 Test RE 0.9992148106536773\n",
      "45 Train Loss 1.1910665 Test MSE 384.22977316304497 Test RE 0.9992130323322332\n",
      "46 Train Loss 1.1923164 Test MSE 384.22977316304497 Test RE 0.9992130323322332\n",
      "47 Train Loss 1.1934743 Test MSE 384.2158073882365 Test RE 0.9991948727413096\n",
      "48 Train Loss 1.1946031 Test MSE 384.21707039749015 Test RE 0.9991965150362243\n",
      "49 Train Loss 1.1956737 Test MSE 384.21809909633805 Test RE 0.9991978526545849\n",
      "50 Train Loss 1.1967233 Test MSE 384.2183257054605 Test RE 0.9991981473144615\n",
      "51 Train Loss 1.1978191 Test MSE 384.2191225049473 Test RE 0.9991991833922653\n",
      "52 Train Loss 1.1987401 Test MSE 384.22626124426387 Test RE 0.9992084658419739\n",
      "53 Train Loss 1.1996889 Test MSE 384.22769485004176 Test RE 0.9992103299385207\n",
      "54 Train Loss 1.200606 Test MSE 384.2343529814638 Test RE 0.9992189873634842\n",
      "55 Train Loss 1.2014796 Test MSE 384.23364607131873 Test RE 0.9992180681869988\n",
      "56 Train Loss 1.202299 Test MSE 384.23291843983264 Test RE 0.9992171220662212\n",
      "57 Train Loss 1.2031703 Test MSE 384.23291843983264 Test RE 0.9992171220662212\n",
      "58 Train Loss 1.2039589 Test MSE 384.22798961110874 Test RE 0.999210713211578\n",
      "59 Train Loss 1.2047476 Test MSE 384.22798961110874 Test RE 0.999210713211578\n",
      "60 Train Loss 1.2054938 Test MSE 384.22831356944897 Test RE 0.9992111344492087\n",
      "61 Train Loss 1.2061965 Test MSE 384.2245095699436 Test RE 0.9992061881608496\n",
      "62 Train Loss 1.2069608 Test MSE 384.22534403509553 Test RE 0.9992072732065329\n",
      "63 Train Loss 1.2076365 Test MSE 384.2248858319788 Test RE 0.9992066774103021\n",
      "64 Train Loss 1.2083185 Test MSE 384.2248858319788 Test RE 0.9992066774103021\n",
      "65 Train Loss 1.2089995 Test MSE 384.2248858319788 Test RE 0.9992066774103021\n",
      "66 Train Loss 1.2095702 Test MSE 384.2248177880966 Test RE 0.9992065889335965\n",
      "67 Train Loss 1.2101952 Test MSE 384.2248177880966 Test RE 0.9992065889335965\n",
      "68 Train Loss 1.210787 Test MSE 384.22651735673725 Test RE 0.9992087988615183\n",
      "69 Train Loss 1.2113864 Test MSE 384.22651735673725 Test RE 0.9992087988615183\n",
      "70 Train Loss 1.2119074 Test MSE 384.22651735673725 Test RE 0.9992087988615183\n",
      "71 Train Loss 1.2124699 Test MSE 384.2293874406808 Test RE 0.9992125307848043\n",
      "72 Train Loss 1.2129631 Test MSE 384.2293874406808 Test RE 0.9992125307848043\n",
      "73 Train Loss 1.2135129 Test MSE 384.2293874406808 Test RE 0.9992125307848043\n",
      "74 Train Loss 1.2140136 Test MSE 384.2382275310363 Test RE 0.9992240253223632\n",
      "75 Train Loss 1.2144887 Test MSE 384.2382275310363 Test RE 0.9992240253223632\n",
      "76 Train Loss 1.2149482 Test MSE 384.2382275310363 Test RE 0.9992240253223632\n",
      "77 Train Loss 1.2154293 Test MSE 384.2382275310363 Test RE 0.9992240253223632\n",
      "78 Train Loss 1.2159421 Test MSE 384.2382275310363 Test RE 0.9992240253223632\n",
      "79 Train Loss 1.2163397 Test MSE 384.22914173016545 Test RE 0.9992122112919788\n",
      "80 Train Loss 1.2167176 Test MSE 384.22914173016545 Test RE 0.9992122112919788\n",
      "81 Train Loss 1.217146 Test MSE 384.22914173016545 Test RE 0.9992122112919788\n",
      "82 Train Loss 1.2175112 Test MSE 384.23136457804225 Test RE 0.9992151016161223\n",
      "83 Train Loss 1.2179282 Test MSE 384.23136457804225 Test RE 0.9992151016161223\n",
      "84 Train Loss 1.21831 Test MSE 384.23136457804225 Test RE 0.9992151016161223\n",
      "85 Train Loss 1.2187235 Test MSE 384.23136457804225 Test RE 0.9992151016161223\n",
      "86 Train Loss 1.2190515 Test MSE 384.23136457804225 Test RE 0.9992151016161223\n",
      "87 Train Loss 1.2193193 Test MSE 384.2394086161197 Test RE 0.9992255610462967\n",
      "88 Train Loss 1.219723 Test MSE 384.2394086161197 Test RE 0.9992255610462967\n",
      "89 Train Loss 1.2200394 Test MSE 384.23992123712634 Test RE 0.9992262275888617\n",
      "90 Train Loss 1.2204199 Test MSE 384.23992123712634 Test RE 0.9992262275888617\n",
      "91 Train Loss 1.2207371 Test MSE 384.23992123712634 Test RE 0.9992262275888617\n",
      "92 Train Loss 1.2209744 Test MSE 384.23992123712634 Test RE 0.9992262275888617\n",
      "93 Train Loss 1.2212875 Test MSE 384.2410087287102 Test RE 0.9992276416133864\n",
      "94 Train Loss 1.2215602 Test MSE 384.24421307997756 Test RE 0.9992318080995394\n",
      "95 Train Loss 1.2219111 Test MSE 384.24421307997756 Test RE 0.9992318080995394\n",
      "96 Train Loss 1.2222056 Test MSE 384.23208745118825 Test RE 0.9992160415516546\n",
      "97 Train Loss 1.2224447 Test MSE 384.2312667918054 Test RE 0.9992149744668316\n",
      "98 Train Loss 1.2226808 Test MSE 384.22574821211924 Test RE 0.9992077987529626\n",
      "99 Train Loss 1.222994 Test MSE 384.2218645937449 Test RE 0.9992027489201611\n",
      "100 Train Loss 1.2231628 Test MSE 384.2218645937449 Test RE 0.9992027489201611\n",
      "101 Train Loss 1.223432 Test MSE 384.2218645937449 Test RE 0.9992027489201611\n",
      "102 Train Loss 1.2237438 Test MSE 384.2218645937449 Test RE 0.9992027489201611\n",
      "103 Train Loss 1.2239571 Test MSE 384.2194058009662 Test RE 0.9991995517616175\n",
      "104 Train Loss 1.2241601 Test MSE 384.2194058009662 Test RE 0.9991995517616175\n",
      "105 Train Loss 1.2244272 Test MSE 384.2194058009662 Test RE 0.9991995517616175\n",
      "106 Train Loss 1.2246222 Test MSE 384.2194058009662 Test RE 0.9991995517616175\n",
      "107 Train Loss 1.2247876 Test MSE 384.2194058009662 Test RE 0.9991995517616175\n",
      "108 Train Loss 1.2250559 Test MSE 384.21393922718295 Test RE 0.999192443560652\n",
      "109 Train Loss 1.225234 Test MSE 384.21393922718295 Test RE 0.999192443560652\n",
      "110 Train Loss 1.2254503 Test MSE 384.2120018772715 Test RE 0.9991899244070834\n",
      "111 Train Loss 1.2256824 Test MSE 384.2120018772715 Test RE 0.9991899244070834\n",
      "112 Train Loss 1.2258878 Test MSE 384.2120018772715 Test RE 0.9991899244070834\n",
      "113 Train Loss 1.2260859 Test MSE 384.2143628802506 Test RE 0.9991929944397118\n",
      "114 Train Loss 1.2262733 Test MSE 384.2143628802506 Test RE 0.9991929944397118\n",
      "115 Train Loss 1.226424 Test MSE 384.2143628802506 Test RE 0.9991929944397118\n",
      "116 Train Loss 1.226585 Test MSE 384.2143628802506 Test RE 0.9991929944397118\n",
      "117 Train Loss 1.2268115 Test MSE 384.2143628802506 Test RE 0.9991929944397118\n",
      "118 Train Loss 1.226935 Test MSE 384.2143628802506 Test RE 0.9991929944397118\n",
      "119 Train Loss 1.2270924 Test MSE 384.20957039206615 Test RE 0.9991867627159459\n",
      "120 Train Loss 1.2272725 Test MSE 384.20957039206615 Test RE 0.9991867627159459\n",
      "121 Train Loss 1.2274535 Test MSE 384.2087952134412 Test RE 0.9991857547392179\n",
      "122 Train Loss 1.2275509 Test MSE 384.2087952134412 Test RE 0.9991857547392179\n",
      "123 Train Loss 1.2277817 Test MSE 384.20687052647406 Test RE 0.9991832520346569\n",
      "124 Train Loss 1.2279403 Test MSE 384.20419131065296 Test RE 0.999179768192423\n",
      "125 Train Loss 1.2280654 Test MSE 384.20419131065296 Test RE 0.999179768192423\n",
      "126 Train Loss 1.2281791 Test MSE 384.20419131065296 Test RE 0.999179768192423\n",
      "127 Train Loss 1.2284203 Test MSE 384.1983183707897 Test RE 0.9991721314392993\n",
      "128 Train Loss 1.2284958 Test MSE 384.1983183707897 Test RE 0.9991721314392993\n",
      "129 Train Loss 1.2286521 Test MSE 384.1983183707897 Test RE 0.9991721314392993\n",
      "130 Train Loss 1.2287987 Test MSE 384.1983183707897 Test RE 0.9991721314392993\n",
      "131 Train Loss 1.2289333 Test MSE 384.1956732700035 Test RE 0.9991686919193782\n",
      "132 Train Loss 1.2290655 Test MSE 384.19184217215593 Test RE 0.9991637101837237\n",
      "133 Train Loss 1.2291172 Test MSE 384.18936937247736 Test RE 0.9991604946864042\n",
      "134 Train Loss 1.2292793 Test MSE 384.18685758668335 Test RE 0.9991572284830181\n",
      "135 Train Loss 1.2294241 Test MSE 384.1813474718909 Test RE 0.9991500633618585\n",
      "136 Train Loss 1.22952 Test MSE 384.17603431655624 Test RE 0.9991431543098201\n",
      "137 Train Loss 1.2296219 Test MSE 384.1741608896978 Test RE 0.9991407181559018\n",
      "138 Train Loss 1.2297387 Test MSE 384.1741608896978 Test RE 0.9991407181559018\n",
      "139 Train Loss 1.2299066 Test MSE 384.1741608896978 Test RE 0.9991407181559018\n",
      "140 Train Loss 1.2300359 Test MSE 384.1741608896978 Test RE 0.9991407181559018\n",
      "141 Train Loss 1.2301241 Test MSE 384.1741608896978 Test RE 0.9991407181559018\n",
      "142 Train Loss 1.2302215 Test MSE 384.1741608896978 Test RE 0.9991407181559018\n",
      "143 Train Loss 1.2303964 Test MSE 384.1741608896978 Test RE 0.9991407181559018\n",
      "144 Train Loss 1.2304107 Test MSE 384.17298922161893 Test RE 0.9991391945472139\n",
      "145 Train Loss 1.230566 Test MSE 384.17298922161893 Test RE 0.9991391945472139\n",
      "146 Train Loss 1.2306564 Test MSE 384.17298922161893 Test RE 0.9991391945472139\n",
      "147 Train Loss 1.2307981 Test MSE 384.17298922161893 Test RE 0.9991391945472139\n",
      "148 Train Loss 1.230885 Test MSE 384.17298922161893 Test RE 0.9991391945472139\n",
      "149 Train Loss 1.2310051 Test MSE 384.17298922161893 Test RE 0.9991391945472139\n",
      "150 Train Loss 1.2310935 Test MSE 384.17275587965065 Test RE 0.9991388911147132\n",
      "151 Train Loss 1.2312155 Test MSE 384.17275587965065 Test RE 0.9991388911147132\n",
      "152 Train Loss 1.2312411 Test MSE 384.17275587965065 Test RE 0.9991388911147132\n",
      "153 Train Loss 1.2312448 Test MSE 384.17275587965065 Test RE 0.9991388911147132\n",
      "154 Train Loss 1.2314281 Test MSE 384.17275587965065 Test RE 0.9991388911147132\n",
      "155 Train Loss 1.2315795 Test MSE 384.17275587965065 Test RE 0.9991388911147132\n",
      "156 Train Loss 1.231544 Test MSE 384.17586682173555 Test RE 0.9991429365043003\n",
      "157 Train Loss 1.2316009 Test MSE 384.17586682173555 Test RE 0.9991429365043003\n",
      "158 Train Loss 1.2317358 Test MSE 384.17586682173555 Test RE 0.9991429365043003\n",
      "159 Train Loss 1.2318474 Test MSE 384.18711453999543 Test RE 0.9991575626130269\n",
      "160 Train Loss 1.2319034 Test MSE 384.18512312958507 Test RE 0.9991549730736138\n",
      "161 Train Loss 1.2320399 Test MSE 384.18512312958507 Test RE 0.9991549730736138\n",
      "162 Train Loss 1.2321651 Test MSE 384.18512312958507 Test RE 0.9991549730736138\n",
      "163 Train Loss 1.2321376 Test MSE 384.18512312958507 Test RE 0.9991549730736138\n",
      "164 Train Loss 1.2322568 Test MSE 384.18512312958507 Test RE 0.9991549730736138\n",
      "165 Train Loss 1.2323728 Test MSE 384.18512312958507 Test RE 0.9991549730736138\n",
      "166 Train Loss 1.2323948 Test MSE 384.18512312958507 Test RE 0.9991549730736138\n",
      "167 Train Loss 1.2324725 Test MSE 384.18420700255507 Test RE 0.9991537817814883\n",
      "168 Train Loss 1.2324938 Test MSE 384.18420700255507 Test RE 0.9991537817814883\n",
      "169 Train Loss 1.2326255 Test MSE 384.18420700255507 Test RE 0.9991537817814883\n",
      "170 Train Loss 1.2327098 Test MSE 384.1820844025403 Test RE 0.9991510216384077\n",
      "171 Train Loss 1.2327296 Test MSE 384.1820844025403 Test RE 0.9991510216384077\n",
      "172 Train Loss 1.2328198 Test MSE 384.1820844025403 Test RE 0.9991510216384077\n",
      "173 Train Loss 1.232893 Test MSE 384.1820844025403 Test RE 0.9991510216384077\n",
      "174 Train Loss 1.2329357 Test MSE 384.1813894160179 Test RE 0.9991501179044285\n",
      "175 Train Loss 1.2329948 Test MSE 384.1813894160179 Test RE 0.9991501179044285\n",
      "176 Train Loss 1.2331129 Test MSE 384.1813894160179 Test RE 0.9991501179044285\n",
      "177 Train Loss 1.2332238 Test MSE 384.1813894160179 Test RE 0.9991501179044285\n",
      "178 Train Loss 1.2332681 Test MSE 384.179344391819 Test RE 0.9991474586281789\n",
      "179 Train Loss 1.2333071 Test MSE 384.179344391819 Test RE 0.9991474586281789\n",
      "180 Train Loss 1.2333508 Test MSE 384.179344391819 Test RE 0.9991474586281789\n",
      "181 Train Loss 1.2334586 Test MSE 384.179344391819 Test RE 0.9991474586281789\n",
      "182 Train Loss 1.2334161 Test MSE 384.179344391819 Test RE 0.9991474586281789\n",
      "183 Train Loss 1.2335579 Test MSE 384.179344391819 Test RE 0.9991474586281789\n",
      "184 Train Loss 1.2336714 Test MSE 384.179344391819 Test RE 0.9991474586281789\n",
      "185 Train Loss 1.2336357 Test MSE 384.179344391819 Test RE 0.9991474586281789\n",
      "186 Train Loss 1.2336915 Test MSE 384.179344391819 Test RE 0.9991474586281789\n",
      "187 Train Loss 1.2337275 Test MSE 384.179344391819 Test RE 0.9991474586281789\n",
      "188 Train Loss 1.233767 Test MSE 384.17751505424314 Test RE 0.9991450798170947\n",
      "189 Train Loss 1.2337694 Test MSE 384.17751505424314 Test RE 0.9991450798170947\n",
      "190 Train Loss 1.2339035 Test MSE 384.1724883833657 Test RE 0.9991385432686154\n",
      "191 Train Loss 1.234009 Test MSE 384.1724883833657 Test RE 0.9991385432686154\n",
      "192 Train Loss 1.2340006 Test MSE 384.1675094758019 Test RE 0.9991320687881166\n",
      "193 Train Loss 1.2340237 Test MSE 384.1675094758019 Test RE 0.9991320687881166\n",
      "194 Train Loss 1.2340798 Test MSE 384.1666273642523 Test RE 0.9991309217019868\n",
      "195 Train Loss 1.2341256 Test MSE 384.1666273642523 Test RE 0.9991309217019868\n",
      "196 Train Loss 1.2342607 Test MSE 384.1666273642523 Test RE 0.9991309217019868\n",
      "197 Train Loss 1.234304 Test MSE 384.1666273642523 Test RE 0.9991309217019868\n",
      "198 Train Loss 1.2343379 Test MSE 384.1666273642523 Test RE 0.9991309217019868\n",
      "199 Train Loss 1.2344242 Test MSE 384.1666273642523 Test RE 0.9991309217019868\n",
      "Training time: 79.56\n",
      "Training time: 79.56\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (m_lambda): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "1\n",
      "0 Train Loss 8.5583935 Test MSE 386.1156201054655 Test RE 1.0016621610295282\n",
      "1 Train Loss 2.1381059 Test MSE 385.3798759228255 Test RE 1.0007073712236256\n",
      "2 Train Loss 1.9552761 Test MSE 385.0749344594183 Test RE 1.0003113755074537\n",
      "3 Train Loss 1.9874924 Test MSE 384.9517753459929 Test RE 1.0001513971456915\n",
      "4 Train Loss 1.9865488 Test MSE 384.772210659145 Test RE 0.9999181045205706\n",
      "5 Train Loss 2.0054095 Test MSE 384.8367121744982 Test RE 1.0000019119375887\n",
      "6 Train Loss 2.0246685 Test MSE 385.3096804176606 Test RE 1.0006162295179362\n",
      "7 Train Loss 2.019462 Test MSE 385.4836449020661 Test RE 1.0008420894582837\n",
      "8 Train Loss 2.0375054 Test MSE 385.4524781165659 Test RE 1.0008016290371249\n",
      "9 Train Loss 2.0314178 Test MSE 385.3365678053425 Test RE 1.0006511410300518\n",
      "10 Train Loss 2.0230293 Test MSE 385.4792976074602 Test RE 1.0008364459404049\n",
      "11 Train Loss 2.0215414 Test MSE 385.58669215159995 Test RE 1.0009758527636672\n",
      "12 Train Loss 2.0027757 Test MSE 385.45814277406794 Test RE 1.0008089829626399\n",
      "13 Train Loss 1.7943153 Test MSE 385.99002208270247 Test RE 1.001499234417896\n",
      "14 Train Loss 1.4360372 Test MSE 385.9268392619358 Test RE 1.0014172632104839\n",
      "15 Train Loss 1.2940025 Test MSE 385.52560942675603 Test RE 1.0008965648168937\n",
      "16 Train Loss 1.2138188 Test MSE 385.42519748629473 Test RE 1.000766212242745\n",
      "17 Train Loss 1.1702696 Test MSE 385.4498877696467 Test RE 1.0007982662000448\n",
      "18 Train Loss 1.1460333 Test MSE 385.2703066992866 Test RE 1.000565103125105\n",
      "19 Train Loss 1.1421338 Test MSE 385.0365003933644 Test RE 1.000261454064371\n",
      "20 Train Loss 1.1412449 Test MSE 384.8337457401077 Test RE 0.9999980577759601\n",
      "21 Train Loss 1.1430731 Test MSE 384.7636756004135 Test RE 0.9999070143133775\n",
      "22 Train Loss 1.1460909 Test MSE 384.7393078905723 Test RE 0.9998753509426825\n",
      "23 Train Loss 1.1490883 Test MSE 384.73126514663693 Test RE 0.9998648999915094\n",
      "24 Train Loss 1.1518649 Test MSE 384.71047391785464 Test RE 0.9998378828205199\n",
      "25 Train Loss 1.154601 Test MSE 384.7005919401179 Test RE 0.9998250414244338\n",
      "26 Train Loss 1.1572292 Test MSE 384.6938550259154 Test RE 0.9998162868693953\n",
      "27 Train Loss 1.159776 Test MSE 384.68834129079914 Test RE 0.9998091217667403\n",
      "28 Train Loss 1.1621798 Test MSE 384.6808878274032 Test RE 0.9997994359042006\n",
      "29 Train Loss 1.1645805 Test MSE 384.6808878274032 Test RE 0.9997994359042006\n",
      "30 Train Loss 1.166833 Test MSE 384.6709273786523 Test RE 0.9997864920384907\n",
      "31 Train Loss 1.1690042 Test MSE 384.6709273786523 Test RE 0.9997864920384907\n",
      "32 Train Loss 1.1711217 Test MSE 384.6721161418027 Test RE 0.9997880368763471\n",
      "33 Train Loss 1.1731261 Test MSE 384.6721161418027 Test RE 0.9997880368763471\n",
      "34 Train Loss 1.175072 Test MSE 384.68226901317945 Test RE 0.999801230778295\n",
      "35 Train Loss 1.1769482 Test MSE 384.68226901317945 Test RE 0.999801230778295\n",
      "36 Train Loss 1.1787262 Test MSE 384.68226901317945 Test RE 0.999801230778295\n",
      "37 Train Loss 1.1804698 Test MSE 384.68226901317945 Test RE 0.999801230778295\n",
      "38 Train Loss 1.1821331 Test MSE 384.68226901317945 Test RE 0.999801230778295\n",
      "39 Train Loss 1.1837288 Test MSE 384.67894960960837 Test RE 0.9997969171522165\n",
      "40 Train Loss 1.18522 Test MSE 384.6707806756583 Test RE 0.9997863013928306\n",
      "41 Train Loss 1.1867489 Test MSE 384.6672598244746 Test RE 0.9997817259129473\n",
      "42 Train Loss 1.1878377 Test MSE 384.53314534121387 Test RE 0.9996074234536391\n",
      "43 Train Loss 1.1887542 Test MSE 384.38508473307456 Test RE 0.999414960548595\n",
      "44 Train Loss 1.1898752 Test MSE 384.2650287632574 Test RE 0.9992588734533486\n",
      "45 Train Loss 1.1905262 Test MSE 384.03139381222843 Test RE 0.9989550502093018\n",
      "46 Train Loss 1.1912827 Test MSE 383.9954509845986 Test RE 0.9989083012847058\n",
      "47 Train Loss 1.1924278 Test MSE 383.99499736690234 Test RE 0.9989077112743097\n",
      "48 Train Loss 1.1935551 Test MSE 383.99733753739997 Test RE 0.9989107550780184\n",
      "49 Train Loss 1.1946658 Test MSE 384.00226752763297 Test RE 0.9989171673678686\n",
      "50 Train Loss 1.1957116 Test MSE 384.00226752763297 Test RE 0.9989171673678686\n",
      "51 Train Loss 1.1967736 Test MSE 384.00226752763297 Test RE 0.9989171673678686\n",
      "52 Train Loss 1.1977378 Test MSE 383.99774997986117 Test RE 0.9989112915310865\n",
      "53 Train Loss 1.1987076 Test MSE 383.99774997986117 Test RE 0.9989112915310865\n",
      "54 Train Loss 1.1996404 Test MSE 383.99774997986117 Test RE 0.9989112915310865\n",
      "55 Train Loss 1.200543 Test MSE 383.9954792003797 Test RE 0.9989083379843298\n",
      "56 Train Loss 1.2013865 Test MSE 383.9954792003797 Test RE 0.9989083379843298\n",
      "57 Train Loss 1.2022505 Test MSE 383.9954792003797 Test RE 0.9989083379843298\n",
      "58 Train Loss 1.203056 Test MSE 383.9954792003797 Test RE 0.9989083379843298\n",
      "59 Train Loss 1.20384 Test MSE 383.9954792003797 Test RE 0.9989083379843298\n",
      "60 Train Loss 1.204589 Test MSE 383.9828519711721 Test RE 0.9988919139000365\n",
      "61 Train Loss 1.2053363 Test MSE 383.9828519711721 Test RE 0.9988919139000365\n",
      "62 Train Loss 1.2060982 Test MSE 383.9828519711721 Test RE 0.9988919139000365\n",
      "63 Train Loss 1.2067852 Test MSE 383.98267587085945 Test RE 0.9988916848465805\n",
      "64 Train Loss 1.2074451 Test MSE 383.9815763379783 Test RE 0.9988902546843527\n",
      "65 Train Loss 1.208128 Test MSE 383.97537791891375 Test RE 0.9988821923634413\n",
      "66 Train Loss 1.2086948 Test MSE 383.97334375078754 Test RE 0.9988795464945582\n",
      "67 Train Loss 1.2092906 Test MSE 383.9708112626091 Test RE 0.9988762524445215\n",
      "68 Train Loss 1.2099211 Test MSE 383.97050928062777 Test RE 0.998875859650746\n",
      "69 Train Loss 1.2104878 Test MSE 383.9674119279364 Test RE 0.9988718308555695\n",
      "70 Train Loss 1.2110548 Test MSE 383.96589062596985 Test RE 0.998869852058491\n",
      "71 Train Loss 1.2115617 Test MSE 383.9583496443722 Test RE 0.9988600432495044\n",
      "72 Train Loss 1.2117382 Test MSE 383.8826835046052 Test RE 0.9987616164179447\n",
      "73 Train Loss 1.212244 Test MSE 383.8599842234435 Test RE 0.9987320872064568\n",
      "74 Train Loss 1.212739 Test MSE 383.8572590996129 Test RE 0.9987285420681856\n",
      "75 Train Loss 1.2132419 Test MSE 383.8536900277788 Test RE 0.99872389901128\n",
      "76 Train Loss 1.2136807 Test MSE 383.8468880949952 Test RE 0.9987150502192775\n",
      "77 Train Loss 1.2141734 Test MSE 383.8463759331429 Test RE 0.9987143839328796\n",
      "78 Train Loss 1.2146548 Test MSE 383.8396811424107 Test RE 0.9987056744420845\n",
      "79 Train Loss 1.2149581 Test MSE 383.83818519910676 Test RE 0.9987037283059822\n",
      "80 Train Loss 1.2153119 Test MSE 383.84139587067426 Test RE 0.9987079052001567\n",
      "81 Train Loss 1.2157279 Test MSE 383.8449601181731 Test RE 0.9987125420552851\n",
      "82 Train Loss 1.2161012 Test MSE 383.8588028072411 Test RE 0.9987305502929646\n",
      "83 Train Loss 1.2165179 Test MSE 383.8637523355268 Test RE 0.9987369891568616\n",
      "84 Train Loss 1.2168815 Test MSE 383.864169560193 Test RE 0.9987375319244337\n",
      "85 Train Loss 1.2172692 Test MSE 383.8626985431989 Test RE 0.9987356182770996\n",
      "86 Train Loss 1.2175903 Test MSE 383.8506946736592 Test RE 0.9987200022953451\n",
      "87 Train Loss 1.2178956 Test MSE 383.84841629944407 Test RE 0.9987170383026918\n",
      "88 Train Loss 1.2182696 Test MSE 383.846360088992 Test RE 0.9987143633207515\n",
      "89 Train Loss 1.2186043 Test MSE 383.8445457727718 Test RE 0.9987120030198624\n",
      "90 Train Loss 1.2189571 Test MSE 383.8354826087444 Test RE 0.998700212381921\n",
      "91 Train Loss 1.2192849 Test MSE 383.8354826087444 Test RE 0.998700212381921\n",
      "92 Train Loss 1.2195449 Test MSE 383.8354826087444 Test RE 0.998700212381921\n",
      "93 Train Loss 1.2198457 Test MSE 383.81970617614735 Test RE 0.9986796878483608\n",
      "94 Train Loss 1.2201308 Test MSE 383.821227490607 Test RE 0.9986816670384852\n",
      "95 Train Loss 1.2204592 Test MSE 383.82774133184853 Test RE 0.9986901413303744\n",
      "96 Train Loss 1.2207692 Test MSE 383.82827722489174 Test RE 0.9986908385062875\n",
      "97 Train Loss 1.2210081 Test MSE 383.82877973039837 Test RE 0.9986914922458782\n",
      "98 Train Loss 1.2212732 Test MSE 383.82877973039837 Test RE 0.9986914922458782\n",
      "99 Train Loss 1.2215804 Test MSE 383.82877973039837 Test RE 0.9986914922458782\n",
      "100 Train Loss 1.2217604 Test MSE 383.8188871725131 Test RE 0.9986786223445353\n",
      "101 Train Loss 1.2220242 Test MSE 383.8188871725131 Test RE 0.9986786223445353\n",
      "102 Train Loss 1.222326 Test MSE 383.8100270211132 Test RE 0.9986670954302083\n",
      "103 Train Loss 1.2225568 Test MSE 383.80970902373633 Test RE 0.9986666817182673\n",
      "104 Train Loss 1.2227418 Test MSE 383.8066603668646 Test RE 0.9986627154323499\n",
      "105 Train Loss 1.223007 Test MSE 383.80584871048114 Test RE 0.9986616594691788\n",
      "106 Train Loss 1.2231994 Test MSE 383.7946202396023 Test RE 0.9986470511386725\n",
      "107 Train Loss 1.2233715 Test MSE 383.7946202396023 Test RE 0.9986470511386725\n",
      "108 Train Loss 1.2236469 Test MSE 383.79575131091804 Test RE 0.9986485226811883\n",
      "109 Train Loss 1.2238098 Test MSE 383.79533064220254 Test RE 0.9986479753844749\n",
      "110 Train Loss 1.2239904 Test MSE 383.791752522466 Test RE 0.9986433201814622\n",
      "111 Train Loss 1.224203 Test MSE 383.79165191067 Test RE 0.9986431892832548\n",
      "112 Train Loss 1.2244115 Test MSE 383.7934883940502 Test RE 0.9986455785866921\n",
      "113 Train Loss 1.2244235 Test MSE 383.7472388890209 Test RE 0.9985854052686954\n",
      "114 Train Loss 1.2244912 Test MSE 383.65828297543914 Test RE 0.9984696582137285\n",
      "115 Train Loss 1.2246126 Test MSE 383.6332218271849 Test RE 0.9984370468954182\n",
      "116 Train Loss 1.2247691 Test MSE 383.6305847210912 Test RE 0.9984336152466513\n",
      "117 Train Loss 1.2249839 Test MSE 383.62806839411815 Test RE 0.9984303407559605\n",
      "118 Train Loss 1.2251337 Test MSE 383.6282691197276 Test RE 0.9984306019601434\n",
      "119 Train Loss 1.2252847 Test MSE 383.627778721572 Test RE 0.9984299638050314\n",
      "120 Train Loss 1.2253575 Test MSE 383.55465368017576 Test RE 0.9983348016243717\n",
      "121 Train Loss 1.2255673 Test MSE 383.5507537033902 Test RE 0.9983297260863612\n",
      "122 Train Loss 1.2256638 Test MSE 383.5429130967466 Test RE 0.998319522025216\n",
      "123 Train Loss 1.2258812 Test MSE 383.53824635496574 Test RE 0.9983134485019717\n",
      "124 Train Loss 1.2260386 Test MSE 383.53095073102037 Test RE 0.9983039535504183\n",
      "125 Train Loss 1.2261659 Test MSE 383.53095073102037 Test RE 0.9983039535504183\n",
      "126 Train Loss 1.2262789 Test MSE 383.53095073102037 Test RE 0.9983039535504183\n",
      "127 Train Loss 1.2265095 Test MSE 383.5247193945678 Test RE 0.998295843653326\n",
      "128 Train Loss 1.2265956 Test MSE 383.5247193945678 Test RE 0.998295843653326\n",
      "129 Train Loss 1.2267555 Test MSE 383.52520999096174 Test RE 0.9982964821521757\n",
      "130 Train Loss 1.2269124 Test MSE 383.52520999096174 Test RE 0.9982964821521757\n",
      "131 Train Loss 1.227036 Test MSE 383.53625678370946 Test RE 0.9983108591665708\n",
      "132 Train Loss 1.2271911 Test MSE 383.53625678370946 Test RE 0.9983108591665708\n",
      "133 Train Loss 1.2272539 Test MSE 383.53361697656874 Test RE 0.9983074235687326\n",
      "134 Train Loss 1.227399 Test MSE 383.53361697656874 Test RE 0.9983074235687326\n",
      "135 Train Loss 1.2275769 Test MSE 383.53361697656874 Test RE 0.9983074235687326\n",
      "136 Train Loss 1.2276429 Test MSE 383.53638063406675 Test RE 0.9983110203523052\n",
      "137 Train Loss 1.2277827 Test MSE 383.53638063406675 Test RE 0.9983110203523052\n",
      "138 Train Loss 1.2278994 Test MSE 383.5391690005688 Test RE 0.9983146492804709\n",
      "139 Train Loss 1.2280234 Test MSE 383.5391690005688 Test RE 0.9983146492804709\n",
      "140 Train Loss 1.2281712 Test MSE 383.5391690005688 Test RE 0.9983146492804709\n",
      "141 Train Loss 1.22825 Test MSE 383.5424979194709 Test RE 0.9983189816949462\n",
      "142 Train Loss 1.228372 Test MSE 383.5436547150771 Test RE 0.9983204871997428\n",
      "143 Train Loss 1.2285078 Test MSE 383.5440837234266 Test RE 0.9983210455295438\n",
      "144 Train Loss 1.2285321 Test MSE 383.5425844901754 Test RE 0.9983190943619558\n",
      "145 Train Loss 1.2286917 Test MSE 383.5403662742481 Test RE 0.9983162074719376\n",
      "146 Train Loss 1.2287536 Test MSE 383.5333585598757 Test RE 0.9983070872496524\n",
      "147 Train Loss 1.2289069 Test MSE 383.5307642189828 Test RE 0.9983037108115639\n",
      "148 Train Loss 1.2289094 Test MSE 383.5080491857526 Test RE 0.9982741475537361\n",
      "149 Train Loss 1.2290395 Test MSE 383.506834349368 Test RE 0.9982725664386602\n",
      "150 Train Loss 1.2291236 Test MSE 383.49480808706977 Test RE 0.9982569140685138\n",
      "151 Train Loss 1.2292577 Test MSE 383.4941118192826 Test RE 0.9982560078574867\n",
      "152 Train Loss 1.2292871 Test MSE 383.4927917741617 Test RE 0.998254289781425\n",
      "153 Train Loss 1.2292492 Test MSE 383.4868230868001 Test RE 0.9982465213290906\n",
      "154 Train Loss 1.229455 Test MSE 383.49225389289995 Test RE 0.9982535897129013\n",
      "155 Train Loss 1.2295805 Test MSE 383.49225389289995 Test RE 0.9982535897129013\n",
      "156 Train Loss 1.2293335 Test MSE 383.3390723328297 Test RE 0.9980541993553989\n",
      "157 Train Loss 1.2293994 Test MSE 383.3362905109748 Test RE 0.9980505780000226\n",
      "158 Train Loss 1.229303 Test MSE 383.345115601295 Test RE 0.998062066392878\n",
      "159 Train Loss 1.2291509 Test MSE 383.31961961907336 Test RE 0.9980288756793574\n",
      "160 Train Loss 1.2288752 Test MSE 383.15943142498764 Test RE 0.9978203171539005\n",
      "161 Train Loss 1.2282226 Test MSE 382.96188141758387 Test RE 0.9975630550206778\n",
      "162 Train Loss 1.2260169 Test MSE 381.4484478115693 Test RE 0.995589960469019\n",
      "163 Train Loss 1.2234738 Test MSE 380.1116292668692 Test RE 0.99384386442268\n",
      "164 Train Loss 1.2056096 Test MSE 375.16266833650064 Test RE 0.9873528647381457\n",
      "165 Train Loss 1.1988482 Test MSE 371.8801197842003 Test RE 0.9830238700531367\n",
      "166 Train Loss 1.1683354 Test MSE 359.63572725224475 Test RE 0.9667050719095969\n",
      "167 Train Loss 1.1624064 Test MSE 356.1075563329282 Test RE 0.961951502068085\n",
      "168 Train Loss 1.1438128 Test MSE 353.0353006416162 Test RE 0.9577929791938151\n",
      "169 Train Loss 1.1352861 Test MSE 352.028873296611 Test RE 0.9564267744163353\n",
      "170 Train Loss 1.1292096 Test MSE 351.0820085269251 Test RE 0.955139640508565\n",
      "171 Train Loss 1.1213918 Test MSE 347.6078198213174 Test RE 0.9504020216296981\n",
      "172 Train Loss 1.1150765 Test MSE 344.91368990705007 Test RE 0.9467118182110905\n",
      "173 Train Loss 1.1095581 Test MSE 344.05143591860434 Test RE 0.945527729441724\n",
      "174 Train Loss 1.1070225 Test MSE 342.4956856738221 Test RE 0.9433875382805956\n",
      "175 Train Loss 1.1010644 Test MSE 338.7322099201997 Test RE 0.9381900663542162\n",
      "176 Train Loss 1.0867175 Test MSE 337.36206876587056 Test RE 0.9362906967289856\n",
      "177 Train Loss 1.0770206 Test MSE 334.8610326828055 Test RE 0.9328136415961569\n",
      "178 Train Loss 1.0735719 Test MSE 332.70448089279904 Test RE 0.9298050648320156\n",
      "179 Train Loss 1.0642722 Test MSE 329.8898914325002 Test RE 0.9258637625040804\n",
      "180 Train Loss 1.0443672 Test MSE 323.56375168049317 Test RE 0.9169433679722748\n",
      "181 Train Loss 1.0418491 Test MSE 321.49811841466294 Test RE 0.9140117952314708\n",
      "182 Train Loss 1.0404251 Test MSE 320.45873437081684 Test RE 0.9125331266238388\n",
      "183 Train Loss 1.0377128 Test MSE 318.1876469043645 Test RE 0.9092938211575049\n",
      "184 Train Loss 1.0136158 Test MSE 311.45436320253464 Test RE 0.8996214298460469\n",
      "185 Train Loss 0.999365 Test MSE 307.6215983802506 Test RE 0.8940689132291492\n",
      "186 Train Loss 0.9883046 Test MSE 301.91451470856214 Test RE 0.8857365755289567\n",
      "187 Train Loss 0.9690492 Test MSE 297.9203540393774 Test RE 0.8798581683010502\n",
      "188 Train Loss 0.9554414 Test MSE 295.8460171124284 Test RE 0.876789713380187\n",
      "189 Train Loss 0.951599 Test MSE 294.8915176583701 Test RE 0.8753741604026966\n",
      "190 Train Loss 0.95171654 Test MSE 294.81500986383486 Test RE 0.875260597809525\n",
      "191 Train Loss 0.95179814 Test MSE 294.7780638389628 Test RE 0.8752057525473721\n",
      "192 Train Loss 0.95019084 Test MSE 293.5598658391777 Test RE 0.8733954453604685\n",
      "193 Train Loss 0.9467026 Test MSE 292.0442194477135 Test RE 0.8711378620319397\n",
      "194 Train Loss 0.9420844 Test MSE 292.15118373883433 Test RE 0.8712973791652003\n",
      "195 Train Loss 0.93646777 Test MSE 290.164132099602 Test RE 0.8683292809757388\n",
      "196 Train Loss 0.92690736 Test MSE 285.835128352692 Test RE 0.86182757014523\n",
      "197 Train Loss 0.9173889 Test MSE 283.2444693613774 Test RE 0.8579131048022662\n",
      "198 Train Loss 0.9110502 Test MSE 280.29304507446454 Test RE 0.8534316470723226\n",
      "199 Train Loss 0.9105303 Test MSE 279.48954169655417 Test RE 0.8522075222314163\n",
      "Training time: 99.48\n",
      "Training time: 99.48\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (m_lambda): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "2\n",
      "0 Train Loss 2.1304157 Test MSE 384.08167104655587 Test RE 0.9990204394642029\n",
      "1 Train Loss 1.967845 Test MSE 384.40809434481736 Test RE 0.9994448730074725\n",
      "2 Train Loss 1.9788507 Test MSE 384.90892115759584 Test RE 1.000095725406969\n",
      "3 Train Loss 1.9324977 Test MSE 384.9013987760924 Test RE 1.0000859527855386\n",
      "4 Train Loss 1.9452994 Test MSE 384.71580614191845 Test RE 0.9998448118509293\n",
      "5 Train Loss 1.971054 Test MSE 384.90602261935055 Test RE 1.0000919598133349\n",
      "6 Train Loss 1.9655256 Test MSE 385.3518744793907 Test RE 1.0006710151998417\n",
      "7 Train Loss 1.9628761 Test MSE 385.1027807892126 Test RE 1.0003475431392679\n",
      "8 Train Loss 1.9715023 Test MSE 385.06812115187853 Test RE 1.000302525984187\n",
      "9 Train Loss 1.9339164 Test MSE 384.8767409462598 Test RE 1.000053918160205\n",
      "10 Train Loss 1.7277058 Test MSE 384.49662240608524 Test RE 0.999559950999755\n",
      "11 Train Loss 1.3607068 Test MSE 384.03236504856477 Test RE 0.9989563134144005\n",
      "12 Train Loss 1.160512 Test MSE 384.05178713190907 Test RE 0.9989815737430097\n",
      "13 Train Loss 1.1126362 Test MSE 384.2355535604764 Test RE 0.999220548442548\n",
      "14 Train Loss 1.1076779 Test MSE 384.2122874143968 Test RE 0.9991902956939827\n",
      "15 Train Loss 1.1108037 Test MSE 384.09555597182816 Test RE 0.9990384970803691\n",
      "16 Train Loss 1.1142486 Test MSE 384.09722741143815 Test RE 0.9990406707981762\n",
      "17 Train Loss 1.1177297 Test MSE 384.1746274337648 Test RE 0.9991413248378184\n",
      "18 Train Loss 1.1219144 Test MSE 384.181293288132 Test RE 0.999149992903326\n",
      "19 Train Loss 1.1259606 Test MSE 384.1843461188918 Test RE 0.9991539626824713\n",
      "20 Train Loss 1.1298524 Test MSE 384.1849810739435 Test RE 0.99915478835067\n",
      "21 Train Loss 1.1336012 Test MSE 384.1851728235104 Test RE 0.9991550376934129\n",
      "22 Train Loss 1.1371934 Test MSE 384.1851728235104 Test RE 0.9991550376934129\n",
      "23 Train Loss 1.1406844 Test MSE 384.18455917610163 Test RE 0.9991542397329656\n",
      "24 Train Loss 1.1439401 Test MSE 384.18395228514294 Test RE 0.9991534505576861\n",
      "25 Train Loss 1.1471317 Test MSE 384.18352881398596 Test RE 0.9991528998933732\n",
      "26 Train Loss 1.150188 Test MSE 384.18352881398596 Test RE 0.9991528998933732\n",
      "27 Train Loss 1.15308 Test MSE 384.18298020933054 Test RE 0.999152186510202\n",
      "28 Train Loss 1.1559153 Test MSE 384.18173530196503 Test RE 0.9991505676814304\n",
      "29 Train Loss 1.1585764 Test MSE 384.18173530196503 Test RE 0.9991505676814304\n",
      "30 Train Loss 1.1611512 Test MSE 384.1802605801887 Test RE 0.9991486500076074\n",
      "31 Train Loss 1.1636232 Test MSE 384.1802605801887 Test RE 0.9991486500076074\n",
      "32 Train Loss 1.1659958 Test MSE 384.1738813713118 Test RE 0.9991403546771981\n",
      "33 Train Loss 1.1682395 Test MSE 384.1738813713118 Test RE 0.9991403546771981\n",
      "34 Train Loss 1.1703297 Test MSE 384.1886042413703 Test RE 0.9991594997484687\n",
      "35 Train Loss 1.1723859 Test MSE 384.187282555088 Test RE 0.9991577810918951\n",
      "36 Train Loss 1.174396 Test MSE 384.187282555088 Test RE 0.9991577810918951\n",
      "37 Train Loss 1.1763207 Test MSE 384.187282555088 Test RE 0.9991577810918951\n",
      "38 Train Loss 1.1781363 Test MSE 384.1871363495052 Test RE 0.9991575909730832\n",
      "39 Train Loss 1.1799235 Test MSE 384.18765651065894 Test RE 0.9991582673658358\n",
      "40 Train Loss 1.1815745 Test MSE 384.1889153104366 Test RE 0.9991599042471269\n",
      "41 Train Loss 1.183245 Test MSE 384.1889153104366 Test RE 0.9991599042471269\n",
      "42 Train Loss 1.1847876 Test MSE 384.1889153104366 Test RE 0.9991599042471269\n",
      "43 Train Loss 1.1863006 Test MSE 384.20377948335937 Test RE 0.999179232683416\n",
      "44 Train Loss 1.1877491 Test MSE 384.20377948335937 Test RE 0.999179232683416\n",
      "45 Train Loss 1.1891445 Test MSE 384.20080734087065 Test RE 0.9991753679260227\n",
      "46 Train Loss 1.190495 Test MSE 384.20080734087065 Test RE 0.9991753679260227\n",
      "47 Train Loss 1.1917919 Test MSE 384.2000628514741 Test RE 0.9991743998439508\n",
      "48 Train Loss 1.1930315 Test MSE 384.2000628514741 Test RE 0.9991743998439508\n",
      "49 Train Loss 1.19424 Test MSE 384.2000628514741 Test RE 0.9991743998439508\n",
      "50 Train Loss 1.1953672 Test MSE 384.2000628514741 Test RE 0.9991743998439508\n",
      "51 Train Loss 1.1965053 Test MSE 384.1972601655771 Test RE 0.9991707554183289\n",
      "52 Train Loss 1.1975541 Test MSE 384.1972601655771 Test RE 0.9991707554183289\n",
      "53 Train Loss 1.1986061 Test MSE 384.1972601655771 Test RE 0.9991707554183289\n",
      "54 Train Loss 1.1996007 Test MSE 384.1972601655771 Test RE 0.9991707554183289\n",
      "55 Train Loss 1.2005553 Test MSE 384.1972601655771 Test RE 0.9991707554183289\n",
      "56 Train Loss 1.2014811 Test MSE 384.1927970689812 Test RE 0.9991649518780504\n",
      "57 Train Loss 1.2024051 Test MSE 384.1927970689812 Test RE 0.9991649518780504\n",
      "58 Train Loss nan Test MSE nan Test RE nan\n",
      "59 Train Loss nan Test MSE nan Test RE nan\n",
      "60 Train Loss nan Test MSE nan Test RE nan\n",
      "61 Train Loss nan Test MSE nan Test RE nan\n",
      "62 Train Loss nan Test MSE nan Test RE nan\n",
      "63 Train Loss nan Test MSE nan Test RE nan\n",
      "64 Train Loss nan Test MSE nan Test RE nan\n",
      "65 Train Loss nan Test MSE nan Test RE nan\n",
      "66 Train Loss nan Test MSE nan Test RE nan\n",
      "67 Train Loss nan Test MSE nan Test RE nan\n",
      "68 Train Loss nan Test MSE nan Test RE nan\n",
      "69 Train Loss nan Test MSE nan Test RE nan\n",
      "70 Train Loss nan Test MSE nan Test RE nan\n",
      "71 Train Loss nan Test MSE nan Test RE nan\n",
      "72 Train Loss nan Test MSE nan Test RE nan\n",
      "73 Train Loss nan Test MSE nan Test RE nan\n",
      "74 Train Loss nan Test MSE nan Test RE nan\n",
      "75 Train Loss nan Test MSE nan Test RE nan\n",
      "76 Train Loss nan Test MSE nan Test RE nan\n",
      "77 Train Loss nan Test MSE nan Test RE nan\n",
      "78 Train Loss nan Test MSE nan Test RE nan\n",
      "79 Train Loss nan Test MSE nan Test RE nan\n",
      "80 Train Loss nan Test MSE nan Test RE nan\n",
      "81 Train Loss nan Test MSE nan Test RE nan\n",
      "82 Train Loss nan Test MSE nan Test RE nan\n",
      "83 Train Loss nan Test MSE nan Test RE nan\n",
      "84 Train Loss nan Test MSE nan Test RE nan\n",
      "85 Train Loss nan Test MSE nan Test RE nan\n",
      "86 Train Loss nan Test MSE nan Test RE nan\n",
      "87 Train Loss nan Test MSE nan Test RE nan\n",
      "88 Train Loss nan Test MSE nan Test RE nan\n",
      "89 Train Loss nan Test MSE nan Test RE nan\n",
      "90 Train Loss nan Test MSE nan Test RE nan\n",
      "91 Train Loss nan Test MSE nan Test RE nan\n",
      "92 Train Loss nan Test MSE nan Test RE nan\n",
      "93 Train Loss nan Test MSE nan Test RE nan\n",
      "94 Train Loss nan Test MSE nan Test RE nan\n",
      "95 Train Loss nan Test MSE nan Test RE nan\n",
      "96 Train Loss nan Test MSE nan Test RE nan\n",
      "97 Train Loss nan Test MSE nan Test RE nan\n",
      "98 Train Loss nan Test MSE nan Test RE nan\n",
      "99 Train Loss nan Test MSE nan Test RE nan\n",
      "100 Train Loss nan Test MSE nan Test RE nan\n",
      "101 Train Loss nan Test MSE nan Test RE nan\n",
      "102 Train Loss nan Test MSE nan Test RE nan\n",
      "103 Train Loss nan Test MSE nan Test RE nan\n",
      "104 Train Loss nan Test MSE nan Test RE nan\n",
      "105 Train Loss nan Test MSE nan Test RE nan\n",
      "106 Train Loss nan Test MSE nan Test RE nan\n",
      "107 Train Loss nan Test MSE nan Test RE nan\n",
      "108 Train Loss nan Test MSE nan Test RE nan\n",
      "109 Train Loss nan Test MSE nan Test RE nan\n",
      "110 Train Loss nan Test MSE nan Test RE nan\n",
      "111 Train Loss nan Test MSE nan Test RE nan\n",
      "112 Train Loss nan Test MSE nan Test RE nan\n",
      "113 Train Loss nan Test MSE nan Test RE nan\n",
      "114 Train Loss nan Test MSE nan Test RE nan\n",
      "115 Train Loss nan Test MSE nan Test RE nan\n",
      "116 Train Loss nan Test MSE nan Test RE nan\n",
      "117 Train Loss nan Test MSE nan Test RE nan\n",
      "118 Train Loss nan Test MSE nan Test RE nan\n",
      "119 Train Loss nan Test MSE nan Test RE nan\n",
      "120 Train Loss nan Test MSE nan Test RE nan\n",
      "121 Train Loss nan Test MSE nan Test RE nan\n",
      "122 Train Loss nan Test MSE nan Test RE nan\n",
      "123 Train Loss nan Test MSE nan Test RE nan\n",
      "124 Train Loss nan Test MSE nan Test RE nan\n",
      "125 Train Loss nan Test MSE nan Test RE nan\n",
      "126 Train Loss nan Test MSE nan Test RE nan\n",
      "127 Train Loss nan Test MSE nan Test RE nan\n",
      "128 Train Loss nan Test MSE nan Test RE nan\n",
      "129 Train Loss nan Test MSE nan Test RE nan\n",
      "130 Train Loss nan Test MSE nan Test RE nan\n",
      "131 Train Loss nan Test MSE nan Test RE nan\n",
      "132 Train Loss nan Test MSE nan Test RE nan\n",
      "133 Train Loss nan Test MSE nan Test RE nan\n",
      "134 Train Loss nan Test MSE nan Test RE nan\n",
      "135 Train Loss nan Test MSE nan Test RE nan\n",
      "136 Train Loss nan Test MSE nan Test RE nan\n",
      "137 Train Loss nan Test MSE nan Test RE nan\n",
      "138 Train Loss nan Test MSE nan Test RE nan\n",
      "139 Train Loss nan Test MSE nan Test RE nan\n",
      "140 Train Loss nan Test MSE nan Test RE nan\n",
      "141 Train Loss nan Test MSE nan Test RE nan\n",
      "142 Train Loss nan Test MSE nan Test RE nan\n",
      "143 Train Loss nan Test MSE nan Test RE nan\n",
      "144 Train Loss nan Test MSE nan Test RE nan\n",
      "145 Train Loss nan Test MSE nan Test RE nan\n",
      "146 Train Loss nan Test MSE nan Test RE nan\n",
      "147 Train Loss nan Test MSE nan Test RE nan\n",
      "148 Train Loss nan Test MSE nan Test RE nan\n",
      "149 Train Loss nan Test MSE nan Test RE nan\n",
      "150 Train Loss nan Test MSE nan Test RE nan\n",
      "151 Train Loss nan Test MSE nan Test RE nan\n",
      "152 Train Loss nan Test MSE nan Test RE nan\n",
      "153 Train Loss nan Test MSE nan Test RE nan\n",
      "154 Train Loss nan Test MSE nan Test RE nan\n",
      "155 Train Loss nan Test MSE nan Test RE nan\n",
      "156 Train Loss nan Test MSE nan Test RE nan\n",
      "157 Train Loss nan Test MSE nan Test RE nan\n",
      "158 Train Loss nan Test MSE nan Test RE nan\n",
      "159 Train Loss nan Test MSE nan Test RE nan\n",
      "160 Train Loss nan Test MSE nan Test RE nan\n",
      "161 Train Loss nan Test MSE nan Test RE nan\n",
      "162 Train Loss nan Test MSE nan Test RE nan\n",
      "163 Train Loss nan Test MSE nan Test RE nan\n",
      "164 Train Loss nan Test MSE nan Test RE nan\n",
      "165 Train Loss nan Test MSE nan Test RE nan\n",
      "166 Train Loss nan Test MSE nan Test RE nan\n",
      "167 Train Loss nan Test MSE nan Test RE nan\n",
      "168 Train Loss nan Test MSE nan Test RE nan\n",
      "169 Train Loss nan Test MSE nan Test RE nan\n",
      "170 Train Loss nan Test MSE nan Test RE nan\n",
      "171 Train Loss nan Test MSE nan Test RE nan\n",
      "172 Train Loss nan Test MSE nan Test RE nan\n",
      "173 Train Loss nan Test MSE nan Test RE nan\n",
      "174 Train Loss nan Test MSE nan Test RE nan\n",
      "175 Train Loss nan Test MSE nan Test RE nan\n",
      "176 Train Loss nan Test MSE nan Test RE nan\n",
      "177 Train Loss nan Test MSE nan Test RE nan\n",
      "178 Train Loss nan Test MSE nan Test RE nan\n",
      "179 Train Loss nan Test MSE nan Test RE nan\n",
      "180 Train Loss nan Test MSE nan Test RE nan\n",
      "181 Train Loss nan Test MSE nan Test RE nan\n",
      "182 Train Loss nan Test MSE nan Test RE nan\n",
      "183 Train Loss nan Test MSE nan Test RE nan\n",
      "184 Train Loss nan Test MSE nan Test RE nan\n",
      "185 Train Loss nan Test MSE nan Test RE nan\n",
      "186 Train Loss nan Test MSE nan Test RE nan\n",
      "187 Train Loss nan Test MSE nan Test RE nan\n",
      "188 Train Loss nan Test MSE nan Test RE nan\n",
      "189 Train Loss nan Test MSE nan Test RE nan\n",
      "190 Train Loss nan Test MSE nan Test RE nan\n",
      "191 Train Loss nan Test MSE nan Test RE nan\n",
      "192 Train Loss nan Test MSE nan Test RE nan\n",
      "193 Train Loss nan Test MSE nan Test RE nan\n",
      "194 Train Loss nan Test MSE nan Test RE nan\n",
      "195 Train Loss nan Test MSE nan Test RE nan\n",
      "196 Train Loss nan Test MSE nan Test RE nan\n",
      "197 Train Loss nan Test MSE nan Test RE nan\n",
      "198 Train Loss nan Test MSE nan Test RE nan\n",
      "199 Train Loss nan Test MSE nan Test RE nan\n",
      "Training time: 164.26\n",
      "Training time: 164.26\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (m_lambda): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "3\n",
      "0 Train Loss 2.0950718 Test MSE 385.3747883645989 Test RE 1.000700765827055\n",
      "1 Train Loss 1.9237908 Test MSE 385.0244082254124 Test RE 1.0002457472100181\n",
      "2 Train Loss 1.9579809 Test MSE 384.8584656574445 Test RE 1.000030174855919\n",
      "3 Train Loss 1.9498699 Test MSE 384.67352096585074 Test RE 0.999789862489559\n",
      "4 Train Loss 1.9759277 Test MSE 384.7522039997034 Test RE 0.9998921082551375\n",
      "5 Train Loss 2.004572 Test MSE 384.8293081025586 Test RE 0.9999922921151099\n",
      "6 Train Loss 2.0061066 Test MSE 385.45636289180305 Test RE 1.000806672304208\n",
      "7 Train Loss 2.0282283 Test MSE 385.4677061195045 Test RE 1.0008213980884257\n",
      "8 Train Loss 2.0516055 Test MSE 385.4668397557962 Test RE 1.0008202733822424\n",
      "9 Train Loss 2.0528812 Test MSE 385.4526836773628 Test RE 1.000801895899533\n",
      "10 Train Loss 2.0453284 Test MSE 385.68318539934114 Test RE 1.001101092263505\n",
      "11 Train Loss 2.0479286 Test MSE 385.79661444219477 Test RE 1.001248292861296\n",
      "12 Train Loss 2.052066 Test MSE 385.78571130233564 Test RE 1.0012341444384045\n",
      "13 Train Loss 2.0323827 Test MSE 386.0359484035105 Test RE 1.0015588134282536\n",
      "14 Train Loss 2.0423954 Test MSE 386.15002544978836 Test RE 1.0017067872529672\n",
      "15 Train Loss 2.0442462 Test MSE 386.268192097362 Test RE 1.0018600428155016\n",
      "16 Train Loss 1.7923568 Test MSE 385.55646087234913 Test RE 1.0009366120762884\n",
      "17 Train Loss 1.4154992 Test MSE 385.1424510692486 Test RE 1.0003990658070026\n",
      "18 Train Loss 1.246042 Test MSE 384.49140425986275 Test RE 0.9995531682773913\n",
      "19 Train Loss 1.215244 Test MSE 383.872116846424 Test RE 0.9987478704928437\n",
      "20 Train Loss 1.1737546 Test MSE 383.84684667231346 Test RE 0.9987149963313108\n",
      "21 Train Loss 1.1501527 Test MSE 383.9986438308264 Test RE 0.998912454138553\n",
      "22 Train Loss 1.1490687 Test MSE 383.90155165191834 Test RE 0.998786161090791\n",
      "23 Train Loss 1.1513486 Test MSE 383.93826164294717 Test RE 0.9988339136387798\n",
      "24 Train Loss 1.1535889 Test MSE 383.88259142726815 Test RE 0.9987614966374498\n",
      "25 Train Loss 1.1559451 Test MSE 383.855538202858 Test RE 0.9987263033315292\n",
      "26 Train Loss 1.1583346 Test MSE 383.85189079266587 Test RE 0.9987215583522481\n",
      "27 Train Loss 1.1606097 Test MSE 383.85189079266587 Test RE 0.9987215583522481\n",
      "28 Train Loss 1.162857 Test MSE 383.8500665786107 Test RE 0.9987191851920373\n",
      "29 Train Loss 1.1649911 Test MSE 383.8500665786107 Test RE 0.9987191851920373\n",
      "30 Train Loss 1.1670682 Test MSE 383.8473706285581 Test RE 0.9987156779609567\n",
      "31 Train Loss 1.1690665 Test MSE 383.8473706285581 Test RE 0.9987156779609567\n",
      "32 Train Loss 1.171044 Test MSE 383.84809723724277 Test RE 0.9987166232261806\n",
      "33 Train Loss 1.1728909 Test MSE 383.84809723724277 Test RE 0.9987166232261806\n",
      "34 Train Loss 1.1747016 Test MSE 383.84809723724277 Test RE 0.9987166232261806\n",
      "35 Train Loss 1.1764171 Test MSE 383.84809723724277 Test RE 0.9987166232261806\n",
      "36 Train Loss 1.1780986 Test MSE 383.84809723724277 Test RE 0.9987166232261806\n",
      "37 Train Loss 1.1797091 Test MSE 383.8446639229749 Test RE 0.998712156725338\n",
      "38 Train Loss 1.1812415 Test MSE 383.8446639229749 Test RE 0.998712156725338\n",
      "39 Train Loss 1.1827557 Test MSE 383.8446639229749 Test RE 0.998712156725338\n",
      "40 Train Loss 1.1842058 Test MSE 383.8446639229749 Test RE 0.998712156725338\n",
      "41 Train Loss 1.1856347 Test MSE 383.8433410205504 Test RE 0.9987104357168205\n",
      "42 Train Loss 1.1869783 Test MSE 383.8425859290008 Test RE 0.9987094533913501\n",
      "43 Train Loss 1.1882584 Test MSE 383.84128733337275 Test RE 0.9987077639998203\n",
      "44 Train Loss 1.1895584 Test MSE 383.8386256585556 Test RE 0.9987043013187575\n",
      "45 Train Loss 1.1907665 Test MSE 383.8368071582035 Test RE 0.9987019355507586\n",
      "46 Train Loss 1.1919298 Test MSE 383.83474839761146 Test RE 0.9986992572107624\n",
      "47 Train Loss 1.1930823 Test MSE 383.8339138269184 Test RE 0.9986981714758365\n",
      "48 Train Loss 1.1941689 Test MSE 383.83416318526156 Test RE 0.9986984958787513\n",
      "49 Train Loss 1.1952347 Test MSE 383.83470845153505 Test RE 0.9986992052429325\n",
      "50 Train Loss 1.196239 Test MSE 383.83707046119525 Test RE 0.9987022780936797\n",
      "51 Train Loss 1.1972648 Test MSE 383.84004350861943 Test RE 0.9987061458590865\n",
      "52 Train Loss 1.1982043 Test MSE 383.84175664130555 Test RE 0.9987083745403591\n",
      "53 Train Loss 1.1991404 Test MSE 383.84478628137526 Test RE 0.9987123159053706\n",
      "54 Train Loss 1.2000369 Test MSE 383.847789317606 Test RE 0.9987162226451648\n",
      "55 Train Loss 1.2009034 Test MSE 383.847789317606 Test RE 0.9987162226451648\n",
      "56 Train Loss 1.201725 Test MSE 383.847789317606 Test RE 0.9987162226451648\n",
      "57 Train Loss 1.2025939 Test MSE 383.847789317606 Test RE 0.9987162226451648\n",
      "58 Train Loss 1.2033663 Test MSE 383.84962611121745 Test RE 0.9987186121774662\n",
      "59 Train Loss 1.2041416 Test MSE 383.8554746004956 Test RE 0.9987262205903046\n",
      "60 Train Loss 1.2048955 Test MSE 383.8554746004956 Test RE 0.9987262205903046\n",
      "61 Train Loss 1.2056143 Test MSE 383.8643536654246 Test RE 0.9987377714268375\n",
      "62 Train Loss 1.2063302 Test MSE 383.8643536654246 Test RE 0.9987377714268375\n",
      "63 Train Loss 1.2070268 Test MSE 383.8643536654246 Test RE 0.9987377714268375\n",
      "64 Train Loss 1.2076607 Test MSE 383.86568278539426 Test RE 0.9987395004766367\n",
      "65 Train Loss 1.2083404 Test MSE 383.86582136106483 Test RE 0.9987396807493074\n",
      "66 Train Loss 1.2089379 Test MSE 383.86582136106483 Test RE 0.9987396807493074\n",
      "67 Train Loss 1.209523 Test MSE 383.86582136106483 Test RE 0.9987396807493074\n",
      "68 Train Loss 1.210151 Test MSE 383.86633322543526 Test RE 0.9987403466318359\n",
      "69 Train Loss 1.2107233 Test MSE 383.86633322543526 Test RE 0.9987403466318359\n",
      "70 Train Loss 1.2112602 Test MSE 383.8686514089466 Test RE 0.9987433623429152\n",
      "71 Train Loss 1.2117962 Test MSE 383.8686514089466 Test RE 0.9987433623429152\n",
      "72 Train Loss 1.2123152 Test MSE 383.8686514089466 Test RE 0.9987433623429152\n",
      "73 Train Loss 1.2128221 Test MSE 383.8652116059677 Test RE 0.9987388875205032\n",
      "74 Train Loss 1.2133536 Test MSE 383.8652116059677 Test RE 0.9987388875205032\n",
      "75 Train Loss 1.2138208 Test MSE 383.8658271701585 Test RE 0.998739688306339\n",
      "76 Train Loss 1.2142801 Test MSE 383.8677454361321 Test RE 0.9987421837693777\n",
      "77 Train Loss 1.2147688 Test MSE 383.8677454361321 Test RE 0.9987421837693777\n",
      "78 Train Loss 1.2152194 Test MSE 383.8677454361321 Test RE 0.9987421837693777\n",
      "79 Train Loss 1.2156317 Test MSE 383.8677454361321 Test RE 0.9987421837693777\n",
      "80 Train Loss 1.2160444 Test MSE 383.8677454361321 Test RE 0.9987421837693777\n",
      "81 Train Loss 1.2164714 Test MSE 383.8677454361321 Test RE 0.9987421837693777\n",
      "82 Train Loss 1.2168801 Test MSE 383.8677454361321 Test RE 0.9987421837693777\n",
      "83 Train Loss 1.2172891 Test MSE 383.8677454361321 Test RE 0.9987421837693777\n",
      "84 Train Loss 1.2176498 Test MSE 383.87055101025567 Test RE 0.9987458335161205\n",
      "85 Train Loss 1.2180423 Test MSE 383.87055101025567 Test RE 0.9987458335161205\n",
      "86 Train Loss 1.2183816 Test MSE 383.87055101025567 Test RE 0.9987458335161205\n",
      "87 Train Loss 1.2187289 Test MSE 383.87055101025567 Test RE 0.9987458335161205\n",
      "88 Train Loss 1.219104 Test MSE 383.87055101025567 Test RE 0.9987458335161205\n",
      "89 Train Loss 1.2194139 Test MSE 383.8724454878598 Test RE 0.9987482980178575\n",
      "90 Train Loss 1.2197556 Test MSE 383.8724454878598 Test RE 0.9987482980178575\n",
      "91 Train Loss 1.2200849 Test MSE 383.87202459916824 Test RE 0.9987477504896555\n",
      "92 Train Loss 1.220382 Test MSE 383.87202459916824 Test RE 0.9987477504896555\n",
      "93 Train Loss 1.2206789 Test MSE 383.8721502268093 Test RE 0.9987479139169251\n",
      "94 Train Loss 1.2209704 Test MSE 383.8721502268093 Test RE 0.9987479139169251\n",
      "95 Train Loss 1.2212784 Test MSE 383.87519210222507 Test RE 0.9987518710426316\n",
      "96 Train Loss 1.221576 Test MSE 383.87519210222507 Test RE 0.9987518710426316\n",
      "97 Train Loss 1.2218332 Test MSE 383.87519210222507 Test RE 0.9987518710426316\n",
      "98 Train Loss 1.2221087 Test MSE 383.87519210222507 Test RE 0.9987518710426316\n",
      "99 Train Loss 1.222406 Test MSE 383.87519210222507 Test RE 0.9987518710426316\n",
      "100 Train Loss 1.2226503 Test MSE 383.87519210222507 Test RE 0.9987518710426316\n",
      "101 Train Loss 1.2228912 Test MSE 383.87519210222507 Test RE 0.9987518710426316\n",
      "102 Train Loss 1.223162 Test MSE 383.87519210222507 Test RE 0.9987518710426316\n",
      "103 Train Loss 1.223404 Test MSE 383.87519210222507 Test RE 0.9987518710426316\n",
      "104 Train Loss 1.2236234 Test MSE 383.87841879731275 Test RE 0.998756068580074\n",
      "105 Train Loss 1.2238867 Test MSE 383.87841879731275 Test RE 0.998756068580074\n",
      "106 Train Loss 1.2241044 Test MSE 383.87826969226177 Test RE 0.9987558746129457\n",
      "107 Train Loss 1.2243001 Test MSE 383.87826969226177 Test RE 0.9987558746129457\n",
      "108 Train Loss 1.2245505 Test MSE 383.87826969226177 Test RE 0.9987558746129457\n",
      "109 Train Loss 1.2247285 Test MSE 383.87826969226177 Test RE 0.9987558746129457\n",
      "110 Train Loss 1.2249594 Test MSE 383.87826969226177 Test RE 0.9987558746129457\n",
      "111 Train Loss 1.2251856 Test MSE 383.87785507455555 Test RE 0.9987553352466748\n",
      "112 Train Loss 1.2253944 Test MSE 383.87785507455555 Test RE 0.9987553352466748\n",
      "113 Train Loss 1.2255782 Test MSE 383.87785507455555 Test RE 0.9987553352466748\n",
      "114 Train Loss 1.2257732 Test MSE 383.87785507455555 Test RE 0.9987553352466748\n",
      "115 Train Loss 1.2259779 Test MSE 383.87785507455555 Test RE 0.9987553352466748\n",
      "116 Train Loss 1.226115 Test MSE 383.87785507455555 Test RE 0.9987553352466748\n",
      "117 Train Loss 1.2263337 Test MSE 383.87785507455555 Test RE 0.9987553352466748\n",
      "118 Train Loss 1.2265027 Test MSE 383.87812268735865 Test RE 0.9987556833778116\n",
      "119 Train Loss 1.2266551 Test MSE 383.87812268735865 Test RE 0.9987556833778116\n",
      "120 Train Loss 1.2268429 Test MSE 383.87821251054066 Test RE 0.9987558002266277\n",
      "121 Train Loss 1.2270188 Test MSE 383.87821251054066 Test RE 0.9987558002266277\n",
      "122 Train Loss 1.227172 Test MSE 383.8782160838242 Test RE 0.9987558048750258\n",
      "123 Train Loss 1.2273482 Test MSE 383.8782160838242 Test RE 0.9987558048750258\n",
      "124 Train Loss 1.2275281 Test MSE 383.87857193575206 Test RE 0.9987562677940993\n",
      "125 Train Loss 1.2276498 Test MSE 383.87857193575206 Test RE 0.9987562677940993\n",
      "126 Train Loss 1.2278003 Test MSE 383.87857193575206 Test RE 0.9987562677940993\n",
      "127 Train Loss 1.2279705 Test MSE 383.87857193575206 Test RE 0.9987562677940993\n",
      "128 Train Loss 1.2280815 Test MSE 383.87857193575206 Test RE 0.9987562677940993\n",
      "129 Train Loss 1.2282394 Test MSE 383.87805885130797 Test RE 0.9987556003350213\n",
      "130 Train Loss 1.2283906 Test MSE 383.87805885130797 Test RE 0.9987556003350213\n",
      "131 Train Loss 1.2285409 Test MSE 383.87805885130797 Test RE 0.9987556003350213\n",
      "132 Train Loss 1.2286849 Test MSE 383.87805885130797 Test RE 0.9987556003350213\n",
      "133 Train Loss 1.2287663 Test MSE 383.87805885130797 Test RE 0.9987556003350213\n",
      "134 Train Loss 1.2289339 Test MSE 383.87805885130797 Test RE 0.9987556003350213\n",
      "135 Train Loss 1.2290635 Test MSE 383.87805885130797 Test RE 0.9987556003350213\n",
      "136 Train Loss 1.2291865 Test MSE 383.87805885130797 Test RE 0.9987556003350213\n",
      "137 Train Loss 1.2292874 Test MSE 383.87764535901465 Test RE 0.9987550624326256\n",
      "138 Train Loss 1.2294412 Test MSE 383.87764535901465 Test RE 0.9987550624326256\n",
      "139 Train Loss 1.22956 Test MSE 383.87764535901465 Test RE 0.9987550624326256\n",
      "140 Train Loss 1.2296897 Test MSE 383.87764535901465 Test RE 0.9987550624326256\n",
      "141 Train Loss 1.2297851 Test MSE 383.87764535901465 Test RE 0.9987550624326256\n",
      "142 Train Loss 1.2299078 Test MSE 383.87764535901465 Test RE 0.9987550624326256\n",
      "143 Train Loss 1.2300439 Test MSE 383.87764535901465 Test RE 0.9987550624326256\n",
      "144 Train Loss 1.2301134 Test MSE 383.87764407986765 Test RE 0.998755060768613\n",
      "145 Train Loss 1.2302314 Test MSE 383.87764407986765 Test RE 0.998755060768613\n",
      "146 Train Loss 1.2303544 Test MSE 383.8776428614646 Test RE 0.998755059183621\n",
      "147 Train Loss 1.2304623 Test MSE 383.8776428614646 Test RE 0.998755059183621\n",
      "148 Train Loss 1.2305683 Test MSE 383.8776428614646 Test RE 0.998755059183621\n",
      "149 Train Loss 1.2306782 Test MSE 383.8776428614646 Test RE 0.998755059183621\n",
      "150 Train Loss 1.2307762 Test MSE 383.8776428614646 Test RE 0.998755059183621\n",
      "151 Train Loss 1.2308689 Test MSE 383.8776428614646 Test RE 0.998755059183621\n",
      "152 Train Loss 1.2309524 Test MSE 383.8776428614646 Test RE 0.998755059183621\n",
      "153 Train Loss 1.2310195 Test MSE 383.8776428614646 Test RE 0.998755059183621\n",
      "154 Train Loss 1.2311263 Test MSE 383.8776428614646 Test RE 0.998755059183621\n",
      "155 Train Loss 1.2312615 Test MSE 383.8776428614646 Test RE 0.998755059183621\n",
      "156 Train Loss 1.2313018 Test MSE 383.8776428614646 Test RE 0.998755059183621\n",
      "157 Train Loss 1.2313958 Test MSE 383.8776428614646 Test RE 0.998755059183621\n",
      "158 Train Loss 1.2314883 Test MSE 383.8776428614646 Test RE 0.998755059183621\n",
      "159 Train Loss 1.2315766 Test MSE 383.8776428614646 Test RE 0.998755059183621\n",
      "160 Train Loss 1.2316431 Test MSE 383.8776428614646 Test RE 0.998755059183621\n",
      "161 Train Loss 1.2317642 Test MSE 383.8769713147544 Test RE 0.9987541855836859\n",
      "162 Train Loss 1.2318482 Test MSE 383.8769713147544 Test RE 0.9987541855836859\n",
      "163 Train Loss 1.2319294 Test MSE 383.8769713147544 Test RE 0.9987541855836859\n",
      "164 Train Loss 1.2319844 Test MSE 383.8769713147544 Test RE 0.9987541855836859\n",
      "165 Train Loss 1.2321061 Test MSE 383.8768590296185 Test RE 0.9987540395144161\n",
      "166 Train Loss 1.2321495 Test MSE 383.8768590296185 Test RE 0.9987540395144161\n",
      "167 Train Loss 1.2322247 Test MSE 383.8767038419271 Test RE 0.9987538376341182\n",
      "168 Train Loss 1.2322832 Test MSE 383.8767038419271 Test RE 0.9987538376341182\n",
      "169 Train Loss 1.2323862 Test MSE 383.8767038419271 Test RE 0.9987538376341182\n",
      "170 Train Loss 1.2324514 Test MSE 383.8767038419271 Test RE 0.9987538376341182\n",
      "171 Train Loss 1.2325175 Test MSE 383.8767038419271 Test RE 0.9987538376341182\n",
      "172 Train Loss 1.2325903 Test MSE 383.8767038419271 Test RE 0.9987538376341182\n",
      "173 Train Loss 1.232665 Test MSE 383.8767038419271 Test RE 0.9987538376341182\n",
      "174 Train Loss 1.2327422 Test MSE 383.8767038419271 Test RE 0.9987538376341182\n",
      "175 Train Loss 1.2327895 Test MSE 383.8767038419271 Test RE 0.9987538376341182\n",
      "176 Train Loss 1.2328845 Test MSE 383.8767038419271 Test RE 0.9987538376341182\n",
      "177 Train Loss 1.2329416 Test MSE 383.87670837296287 Test RE 0.9987538435284449\n",
      "178 Train Loss 1.233001 Test MSE 383.87670837296287 Test RE 0.9987538435284449\n",
      "179 Train Loss 1.2330627 Test MSE 383.87670837296287 Test RE 0.9987538435284449\n",
      "180 Train Loss 1.2331377 Test MSE 383.87670837296287 Test RE 0.9987538435284449\n",
      "181 Train Loss 1.2331983 Test MSE 383.87670837296287 Test RE 0.9987538435284449\n",
      "182 Train Loss 1.2332237 Test MSE 383.87670837296287 Test RE 0.9987538435284449\n",
      "183 Train Loss 1.233313 Test MSE 383.87670837296287 Test RE 0.9987538435284449\n",
      "184 Train Loss 1.2333696 Test MSE 383.87670837296287 Test RE 0.9987538435284449\n",
      "185 Train Loss 1.2334284 Test MSE 383.87670837296287 Test RE 0.9987538435284449\n",
      "186 Train Loss 1.2334728 Test MSE 383.87670837296287 Test RE 0.9987538435284449\n",
      "187 Train Loss 1.233537 Test MSE 383.87670837296287 Test RE 0.9987538435284449\n",
      "188 Train Loss 1.2335972 Test MSE 383.877641501432 Test RE 0.9987550574143859\n",
      "189 Train Loss 1.2336223 Test MSE 383.877641501432 Test RE 0.9987550574143859\n",
      "190 Train Loss 1.2336942 Test MSE 383.877641501432 Test RE 0.9987550574143859\n",
      "191 Train Loss 1.2337542 Test MSE 383.877641501432 Test RE 0.9987550574143859\n",
      "192 Train Loss 1.2337985 Test MSE 383.88392072365275 Test RE 0.9987632258756687\n",
      "193 Train Loss 1.2338523 Test MSE 383.88392072365275 Test RE 0.9987632258756687\n",
      "194 Train Loss 1.2338933 Test MSE 383.88392072365275 Test RE 0.9987632258756687\n",
      "195 Train Loss 1.2339532 Test MSE 383.8820426973419 Test RE 0.9987607828117512\n",
      "196 Train Loss 1.2340231 Test MSE 383.8820426973419 Test RE 0.9987607828117512\n",
      "197 Train Loss 1.2340598 Test MSE 383.8820426973419 Test RE 0.9987607828117512\n",
      "198 Train Loss 1.2341076 Test MSE 383.8820426973419 Test RE 0.9987607828117512\n",
      "199 Train Loss 1.2341596 Test MSE 383.8820426973419 Test RE 0.9987607828117512\n",
      "Training time: 84.85\n",
      "Training time: 84.85\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (m_lambda): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "4\n",
      "0 Train Loss 3.523262 Test MSE 383.9486856141239 Test RE 0.9988474727787465\n",
      "1 Train Loss 2.0027742 Test MSE 384.4758020828489 Test RE 0.9995328877672094\n",
      "2 Train Loss 1.9347959 Test MSE 384.76951784476427 Test RE 0.999914605569129\n",
      "3 Train Loss 1.9409369 Test MSE 385.28607931335534 Test RE 1.0005855840256974\n",
      "4 Train Loss 1.9516424 Test MSE 384.96950539224855 Test RE 1.0001744292856782\n",
      "5 Train Loss 1.9780804 Test MSE 384.96262938366687 Test RE 1.0001654970993992\n",
      "6 Train Loss 1.9808534 Test MSE 385.6076621348712 Test RE 1.001003071236105\n",
      "7 Train Loss 2.004058 Test MSE 385.60483511214625 Test RE 1.000999401880054\n",
      "8 Train Loss 2.0056927 Test MSE 385.44895381321885 Test RE 1.0007970537174764\n",
      "9 Train Loss 2.0050015 Test MSE 385.55495031619625 Test RE 1.0009346513097181\n",
      "10 Train Loss 2.0068846 Test MSE 386.02345289996975 Test RE 1.0015426036904993\n",
      "11 Train Loss 2.0008316 Test MSE 386.20429006460074 Test RE 1.0017771683482\n",
      "12 Train Loss 2.0060308 Test MSE 386.19558382228547 Test RE 1.0017658767026225\n",
      "13 Train Loss 1.9156144 Test MSE 385.64937452661144 Test RE 1.0010572105948758\n",
      "14 Train Loss 1.8486686 Test MSE 385.4434507037819 Test RE 1.0007899094311652\n",
      "15 Train Loss 1.7912427 Test MSE 386.31032423652755 Test RE 1.0019146801848844\n",
      "16 Train Loss 1.6833191 Test MSE 386.7250223002679 Test RE 1.0024523057744696\n",
      "17 Train Loss 1.5982383 Test MSE 388.2066140191775 Test RE 1.0043707299094216\n",
      "18 Train Loss 1.5918312 Test MSE 388.16010340154367 Test RE 1.0043105618177872\n",
      "19 Train Loss 1.5646081 Test MSE 388.17810451201944 Test RE 1.0043338492391625\n",
      "20 Train Loss 1.4928641 Test MSE 388.185965993585 Test RE 1.0043440191997735\n",
      "21 Train Loss 1.4481739 Test MSE 388.27676030305395 Test RE 1.004461467276448\n",
      "22 Train Loss 1.3444376 Test MSE 387.8842602736185 Test RE 1.0039536454325548\n",
      "23 Train Loss 1.3093778 Test MSE 387.42645679797334 Test RE 1.0033610083871103\n",
      "24 Train Loss 1.2979194 Test MSE 387.0974838167055 Test RE 1.0029349291202052\n",
      "25 Train Loss 1.2599716 Test MSE 386.8975191411662 Test RE 1.002675850370382\n",
      "26 Train Loss 1.241412 Test MSE 386.7922034563542 Test RE 1.0025393740741564\n",
      "27 Train Loss 1.2254181 Test MSE 386.8395922558921 Test RE 1.002600786483606\n",
      "28 Train Loss 1.219415 Test MSE 387.0168335974276 Test RE 1.0028304449283767\n",
      "29 Train Loss 1.215296 Test MSE 386.8582227392968 Test RE 1.0026249291939124\n",
      "30 Train Loss 1.1826266 Test MSE 386.1984479629339 Test RE 1.0017695913917186\n",
      "31 Train Loss 1.1795037 Test MSE 385.96762033787286 Test RE 1.0014701719362993\n",
      "32 Train Loss 1.1774775 Test MSE 385.30222000485895 Test RE 1.0006065424445996\n",
      "33 Train Loss 1.1762024 Test MSE 384.50078615998774 Test RE 0.9995653631544229\n",
      "34 Train Loss 1.1778848 Test MSE 384.43541466691573 Test RE 0.999480388220493\n",
      "35 Train Loss 1.1796825 Test MSE 384.41726306999294 Test RE 0.9994567920862616\n",
      "36 Train Loss 1.1813067 Test MSE 384.42085949423694 Test RE 0.999461467295437\n",
      "37 Train Loss 1.1828313 Test MSE 384.4325591512451 Test RE 0.9994766762352\n",
      "38 Train Loss 1.1844097 Test MSE 384.4325591512451 Test RE 0.9994766762352\n",
      "39 Train Loss 1.1859032 Test MSE 384.4299503118033 Test RE 0.9994732849009648\n",
      "40 Train Loss 1.1873891 Test MSE 384.4299503118033 Test RE 0.9994732849009648\n",
      "41 Train Loss 1.1888615 Test MSE 384.4299503118033 Test RE 0.9994732849009648\n",
      "42 Train Loss 1.1901352 Test MSE 384.4310979445984 Test RE 0.999474776755934\n",
      "43 Train Loss 1.1914243 Test MSE 384.4310979445984 Test RE 0.999474776755934\n",
      "44 Train Loss 1.1927453 Test MSE 384.4310979445984 Test RE 0.999474776755934\n",
      "45 Train Loss 1.1940277 Test MSE 384.4310979445984 Test RE 0.999474776755934\n",
      "46 Train Loss 1.1952505 Test MSE 384.4310979445984 Test RE 0.999474776755934\n",
      "47 Train Loss 1.1963546 Test MSE 384.4365505306669 Test RE 0.9994818647659169\n",
      "48 Train Loss 1.1974449 Test MSE 384.4365505306669 Test RE 0.9994818647659169\n",
      "49 Train Loss 1.1985503 Test MSE 384.4365505306669 Test RE 0.9994818647659169\n",
      "50 Train Loss 1.1995338 Test MSE 384.4377756586134 Test RE 0.9994834573462623\n",
      "51 Train Loss 1.2005745 Test MSE 384.4377756586134 Test RE 0.9994834573462623\n",
      "52 Train Loss 1.201558 Test MSE 384.4377756586134 Test RE 0.9994834573462623\n",
      "53 Train Loss 1.202456 Test MSE 384.4286101684328 Test RE 0.9994715427907794\n",
      "54 Train Loss 1.2033888 Test MSE 384.4286101684328 Test RE 0.9994715427907794\n",
      "55 Train Loss 1.2042449 Test MSE 384.4286101684328 Test RE 0.9994715427907794\n",
      "56 Train Loss 1.2050227 Test MSE 384.4286101684328 Test RE 0.9994715427907794\n",
      "57 Train Loss 1.2059875 Test MSE 384.4286101684328 Test RE 0.9994715427907794\n",
      "58 Train Loss 1.2067181 Test MSE 384.4286101684328 Test RE 0.9994715427907794\n",
      "59 Train Loss 1.2075167 Test MSE 384.4286101684328 Test RE 0.9994715427907794\n",
      "60 Train Loss 1.208326 Test MSE 384.4286101684328 Test RE 0.9994715427907794\n",
      "61 Train Loss 1.2089925 Test MSE 384.4286101684328 Test RE 0.9994715427907794\n",
      "62 Train Loss 1.2097971 Test MSE 384.42748216065394 Test RE 0.999470076442448\n",
      "63 Train Loss 1.2104706 Test MSE 384.4246479170678 Test RE 0.9994663920715247\n",
      "64 Train Loss 1.2111338 Test MSE 384.420735528852 Test RE 0.9994613061457072\n",
      "65 Train Loss 1.211842 Test MSE 384.4178412802275 Test RE 0.9994575437381571\n",
      "66 Train Loss 1.2123673 Test MSE 384.4149536515256 Test RE 0.9994537899221274\n",
      "67 Train Loss 1.2130351 Test MSE 384.4118076684021 Test RE 0.9994497002383639\n",
      "68 Train Loss 1.2135619 Test MSE 384.40831431069216 Test RE 0.9994451589584413\n",
      "69 Train Loss 1.2141739 Test MSE 384.4030798220405 Test RE 0.9994383542120854\n",
      "70 Train Loss 1.2147087 Test MSE 384.3933199798497 Test RE 0.9994256664592495\n",
      "71 Train Loss 1.2153338 Test MSE 384.3933199798497 Test RE 0.9994256664592495\n",
      "72 Train Loss 1.215782 Test MSE 384.38965794865913 Test RE 0.9994209057929088\n",
      "73 Train Loss 1.2163162 Test MSE 384.3776373619205 Test RE 0.9994052787860919\n",
      "74 Train Loss 1.2168101 Test MSE 384.3776373619205 Test RE 0.9994052787860919\n",
      "75 Train Loss 1.2172686 Test MSE 384.3496825157571 Test RE 0.9993689359735871\n",
      "76 Train Loss 1.2177663 Test MSE 384.3496825157571 Test RE 0.9993689359735871\n",
      "77 Train Loss 1.2182393 Test MSE 384.3496825157571 Test RE 0.9993689359735871\n",
      "78 Train Loss 1.218829 Test MSE 384.3549650132112 Test RE 0.9993758036073603\n",
      "79 Train Loss 1.2191546 Test MSE 384.3549650132112 Test RE 0.9993758036073603\n",
      "80 Train Loss 1.2196572 Test MSE 384.3549650132112 Test RE 0.9993758036073603\n",
      "81 Train Loss 1.2200603 Test MSE 384.3549650132112 Test RE 0.9993758036073603\n",
      "82 Train Loss 1.2204082 Test MSE 384.3549650132112 Test RE 0.9993758036073603\n",
      "83 Train Loss 1.2208177 Test MSE 384.3549650132112 Test RE 0.9993758036073603\n",
      "84 Train Loss 1.2211478 Test MSE 384.3571815417287 Test RE 0.9993786852427968\n",
      "85 Train Loss 1.2216144 Test MSE 384.3586565900312 Test RE 0.9993806028997184\n",
      "86 Train Loss 1.2219509 Test MSE 384.3589082328466 Test RE 0.9993809300510693\n",
      "87 Train Loss 1.2221627 Test MSE 384.3584619808776 Test RE 0.9993803498956106\n",
      "88 Train Loss 1.2225761 Test MSE 384.3577800581563 Test RE 0.99937946335279\n",
      "89 Train Loss 1.2228998 Test MSE 384.3577800581563 Test RE 0.99937946335279\n",
      "90 Train Loss 1.2233361 Test MSE 384.3528609077083 Test RE 0.9993730681216804\n",
      "91 Train Loss 1.223708 Test MSE 384.3528609077083 Test RE 0.9993730681216804\n",
      "92 Train Loss 1.2239146 Test MSE 384.3528609077083 Test RE 0.9993730681216804\n",
      "93 Train Loss 1.2242422 Test MSE 384.3517826264661 Test RE 0.9993716662770493\n",
      "94 Train Loss 1.2245353 Test MSE 384.34755984408497 Test RE 0.999366176331579\n",
      "95 Train Loss 1.2248111 Test MSE 384.3444214301674 Test RE 0.9993620961298981\n",
      "96 Train Loss 1.2251887 Test MSE 384.341518303343 Test RE 0.9993583218063677\n",
      "97 Train Loss 1.2254325 Test MSE 384.3361919527422 Test RE 0.9993513970389588\n",
      "98 Train Loss 1.2256122 Test MSE 384.3211224951161 Test RE 0.9993318050366069\n",
      "99 Train Loss 1.2259256 Test MSE 384.3051128674656 Test RE 0.9993109902858929\n",
      "100 Train Loss 1.22615 Test MSE 384.3051128674656 Test RE 0.9993109902858929\n",
      "101 Train Loss 1.2263726 Test MSE 384.32036489807 Test RE 0.9993308200644675\n",
      "102 Train Loss 1.2267133 Test MSE 384.32036489807 Test RE 0.9993308200644675\n",
      "103 Train Loss 1.2269887 Test MSE 384.32036489807 Test RE 0.9993308200644675\n",
      "104 Train Loss 1.2271425 Test MSE 384.3229466289862 Test RE 0.9993341766377541\n",
      "105 Train Loss 1.2274525 Test MSE 384.33741014017477 Test RE 0.9993529808038676\n",
      "106 Train Loss 1.2276965 Test MSE 384.3516923306361 Test RE 0.9993715488857641\n",
      "107 Train Loss 1.2277881 Test MSE 384.35381448611895 Test RE 0.9993743078418541\n",
      "108 Train Loss 1.228084 Test MSE 384.367284968287 Test RE 0.9993918202683425\n",
      "109 Train Loss 1.228162 Test MSE 384.367284968287 Test RE 0.9993918202683425\n",
      "110 Train Loss 1.2284336 Test MSE 384.3621302107881 Test RE 0.9993851188138305\n",
      "111 Train Loss 1.2286805 Test MSE 384.3621302107881 Test RE 0.9993851188138305\n",
      "112 Train Loss 1.2289174 Test MSE 384.3597453042619 Test RE 0.9993820182953327\n",
      "113 Train Loss 1.2291131 Test MSE 384.35708195050984 Test RE 0.9993785557677244\n",
      "114 Train Loss 1.2293137 Test MSE 384.34383793379277 Test RE 0.9993613375337683\n",
      "115 Train Loss 1.2294819 Test MSE 384.34383793379277 Test RE 0.9993613375337683\n",
      "116 Train Loss 1.229567 Test MSE 384.33709594764844 Test RE 0.9993525723225518\n",
      "117 Train Loss 1.2299229 Test MSE 384.33709594764844 Test RE 0.9993525723225518\n",
      "118 Train Loss 1.2300079 Test MSE 384.338020445406 Test RE 0.9993537742604683\n",
      "119 Train Loss 1.230143 Test MSE 384.338020445406 Test RE 0.9993537742604683\n",
      "120 Train Loss 1.2303417 Test MSE 384.33686327958554 Test RE 0.999352269830983\n",
      "121 Train Loss 1.2305733 Test MSE 384.3336337710768 Test RE 0.9993480711400076\n",
      "122 Train Loss 1.2306166 Test MSE 384.3336337710768 Test RE 0.9993480711400076\n",
      "123 Train Loss 1.2309176 Test MSE 384.3319558519004 Test RE 0.9993458896667053\n",
      "124 Train Loss 1.2310425 Test MSE 384.329411234478 Test RE 0.9993425813844612\n",
      "125 Train Loss 1.2310958 Test MSE 384.3272960925676 Test RE 0.9993398314592365\n",
      "126 Train Loss 1.2312523 Test MSE 384.3264841263744 Test RE 0.999338775808759\n",
      "127 Train Loss 1.2314816 Test MSE 384.3256083934388 Test RE 0.9993376372528509\n",
      "128 Train Loss 1.2314994 Test MSE 384.31998808366666 Test RE 0.9993303301576433\n",
      "129 Train Loss 1.2316976 Test MSE 384.31998808366666 Test RE 0.9993303301576433\n",
      "130 Train Loss 1.2319437 Test MSE 384.31998808366666 Test RE 0.9993303301576433\n",
      "131 Train Loss 1.2320893 Test MSE 384.32395947865984 Test RE 0.9993354934660058\n",
      "132 Train Loss 1.2321268 Test MSE 384.33005387837613 Test RE 0.9993434168931739\n",
      "133 Train Loss 1.2322426 Test MSE 384.3334223570873 Test RE 0.9993477962796089\n",
      "134 Train Loss 1.2324005 Test MSE 384.33740219386857 Test RE 0.9993529704728867\n",
      "135 Train Loss 1.232505 Test MSE 384.3429087582256 Test RE 0.9993601295233787\n",
      "136 Train Loss 1.2326511 Test MSE 384.36473597877045 Test RE 0.9993885064541916\n",
      "137 Train Loss 1.2327455 Test MSE 384.3918016803341 Test RE 0.9994236926619371\n",
      "138 Train Loss 1.2328284 Test MSE 384.4041011794017 Test RE 0.9994396819629483\n",
      "139 Train Loss 1.2330122 Test MSE 384.41125928762864 Test RE 0.9994489873580827\n",
      "140 Train Loss 1.2331536 Test MSE 384.4360252819487 Test RE 0.99948118197867\n",
      "141 Train Loss 1.2333121 Test MSE 384.4360252819487 Test RE 0.99948118197867\n",
      "142 Train Loss 1.2332996 Test MSE 384.44696702548214 Test RE 0.999495405397167\n",
      "143 Train Loss 1.2334203 Test MSE 384.43156394664345 Test RE 0.9994753825304817\n",
      "144 Train Loss 1.2334617 Test MSE 384.43156394664345 Test RE 0.9994753825304817\n",
      "145 Train Loss 1.2336745 Test MSE 384.43280640169723 Test RE 0.9994769976453074\n",
      "146 Train Loss 1.233792 Test MSE 384.43280640169723 Test RE 0.9994769976453074\n",
      "147 Train Loss 1.233901 Test MSE 384.43280640169723 Test RE 0.9994769976453074\n",
      "148 Train Loss 1.2340338 Test MSE 384.43280640169723 Test RE 0.9994769976453074\n",
      "149 Train Loss 1.2341152 Test MSE 384.4142805993797 Test RE 0.9994529149759189\n",
      "150 Train Loss 1.2342104 Test MSE 384.4142805993797 Test RE 0.9994529149759189\n",
      "151 Train Loss 1.2342769 Test MSE 384.4133905338096 Test RE 0.9994517579180412\n",
      "152 Train Loss 1.2343875 Test MSE 384.42157186084546 Test RE 0.9994623933412387\n",
      "153 Train Loss 1.23405 Test MSE 384.32872684183513 Test RE 0.9993416915969572\n",
      "154 Train Loss 1.233433 Test MSE 383.7827843423748 Test RE 0.9986316523102748\n",
      "155 Train Loss 1.2326484 Test MSE 383.8713459420616 Test RE 0.9987468676335017\n",
      "156 Train Loss 1.2315617 Test MSE 383.66603292410133 Test RE 0.9984797427731635\n",
      "157 Train Loss 1.2306689 Test MSE 383.25810442112726 Test RE 0.9979487905440992\n",
      "158 Train Loss 1.2306306 Test MSE 383.21217878440336 Test RE 0.9978889968878568\n",
      "159 Train Loss 1.2301801 Test MSE 383.1141427993265 Test RE 0.9977613452971085\n",
      "160 Train Loss 1.2300055 Test MSE 383.08876274027114 Test RE 0.9977282955366751\n",
      "161 Train Loss 1.2301341 Test MSE 383.0940810658218 Test RE 0.9977352211189248\n",
      "162 Train Loss 1.2299175 Test MSE 383.05003623203413 Test RE 0.9976778640004922\n",
      "163 Train Loss 1.2299643 Test MSE 383.0551947781738 Test RE 0.9976845818564902\n",
      "164 Train Loss 1.2300518 Test MSE 383.0545848917234 Test RE 0.9976837876177503\n",
      "165 Train Loss 1.2301265 Test MSE 383.05623091251834 Test RE 0.9976859311849621\n",
      "166 Train Loss 1.2301917 Test MSE 383.05577894697956 Test RE 0.9976853426032396\n",
      "167 Train Loss 1.2302502 Test MSE 383.05321229007257 Test RE 0.9976820001131822\n",
      "168 Train Loss 1.2301596 Test MSE 383.1010985053258 Test RE 0.9977443592325607\n",
      "169 Train Loss 1.230259 Test MSE 383.1122893079963 Test RE 0.9977589317288876\n",
      "170 Train Loss 1.2303106 Test MSE 383.11621238631494 Test RE 0.9977640402525776\n",
      "171 Train Loss 1.2303864 Test MSE 383.12291778409724 Test RE 0.997772771774884\n",
      "172 Train Loss 1.2304475 Test MSE 383.12291778409724 Test RE 0.997772771774884\n",
      "173 Train Loss 1.2305084 Test MSE 383.14562093756365 Test RE 0.9978023344187943\n",
      "174 Train Loss 1.2306324 Test MSE 383.14562093756365 Test RE 0.9978023344187943\n",
      "175 Train Loss 1.2306259 Test MSE 383.14186943354923 Test RE 0.9977974495024314\n",
      "176 Train Loss 1.2307388 Test MSE 383.14186943354923 Test RE 0.9977974495024314\n",
      "177 Train Loss 1.2308525 Test MSE 383.14186943354923 Test RE 0.9977974495024314\n",
      "178 Train Loss 1.2309052 Test MSE 383.14186943354923 Test RE 0.9977974495024314\n",
      "179 Train Loss 1.2309111 Test MSE 383.1360359660208 Test RE 0.9977898535678164\n",
      "180 Train Loss 1.2309825 Test MSE 383.1360359660208 Test RE 0.9977898535678164\n",
      "181 Train Loss 1.2310827 Test MSE 383.13729976826806 Test RE 0.9977914992078775\n",
      "182 Train Loss 1.231076 Test MSE 383.13729976826806 Test RE 0.9977914992078775\n",
      "183 Train Loss 1.2311559 Test MSE 383.13729976826806 Test RE 0.9977914992078775\n",
      "184 Train Loss 1.2312856 Test MSE 383.14987346070507 Test RE 0.9978078716943262\n",
      "185 Train Loss 1.2313123 Test MSE 383.14987346070507 Test RE 0.9978078716943262\n",
      "186 Train Loss 1.2313448 Test MSE 383.14987346070507 Test RE 0.9978078716943262\n",
      "187 Train Loss 1.2314008 Test MSE 383.14987346070507 Test RE 0.9978078716943262\n",
      "188 Train Loss 1.2314454 Test MSE 383.14987346070507 Test RE 0.9978078716943262\n",
      "189 Train Loss 1.2314547 Test MSE 383.14987346070507 Test RE 0.9978078716943262\n",
      "190 Train Loss 1.2315538 Test MSE 383.14987346070507 Test RE 0.9978078716943262\n",
      "191 Train Loss 1.231624 Test MSE 383.14987346070507 Test RE 0.9978078716943262\n",
      "192 Train Loss 1.2316588 Test MSE 383.14987346070507 Test RE 0.9978078716943262\n",
      "193 Train Loss 1.23173 Test MSE 383.14987346070507 Test RE 0.9978078716943262\n",
      "194 Train Loss 1.2317725 Test MSE 383.14987346070507 Test RE 0.9978078716943262\n",
      "195 Train Loss 1.2318311 Test MSE 383.14987346070507 Test RE 0.9978078716943262\n",
      "196 Train Loss 1.2319297 Test MSE 383.14987390783944 Test RE 0.9978078722765452\n",
      "197 Train Loss 1.2319618 Test MSE 383.14987390783944 Test RE 0.9978078722765452\n",
      "198 Train Loss 1.2319765 Test MSE 383.14987390783944 Test RE 0.9978078722765452\n",
      "199 Train Loss 1.2320486 Test MSE 383.14987390783944 Test RE 0.9978078722765452\n",
      "Training time: 88.57\n",
      "Training time: 88.57\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (m_lambda): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "5\n",
      "0 Train Loss 1.7992976 Test MSE 385.2656208562803 Test RE 1.0005590184292885\n",
      "1 Train Loss 1.842666 Test MSE 385.27491248868193 Test RE 1.0005710838330502\n",
      "2 Train Loss 1.8830287 Test MSE 385.2804324557376 Test RE 1.0005782515715373\n",
      "3 Train Loss 1.9201399 Test MSE 385.28448495169835 Test RE 1.0005835137498351\n",
      "4 Train Loss 1.9543129 Test MSE 385.2879848989767 Test RE 1.000588058419721\n",
      "5 Train Loss 1.9856008 Test MSE 385.2905037898677 Test RE 1.0005913291783815\n",
      "6 Train Loss 2.01411 Test MSE 385.2935812504227 Test RE 1.000595325220547\n",
      "7 Train Loss 2.040361 Test MSE 385.2964541986062 Test RE 1.000599055691678\n",
      "8 Train Loss 2.0621402 Test MSE 385.41809578369356 Test RE 1.0007569923256276\n",
      "9 Train Loss 2.0675144 Test MSE 385.69711129360206 Test RE 1.0011191655197953\n",
      "10 Train Loss 2.0845757 Test MSE 385.6009469139654 Test RE 1.0009943551411256\n",
      "11 Train Loss 2.0769882 Test MSE 385.44873096797835 Test RE 1.0007967644147162\n",
      "12 Train Loss 2.056593 Test MSE 385.842069615306 Test RE 1.0013072754602652\n",
      "13 Train Loss 2.0559816 Test MSE 386.0449313004566 Test RE 1.0015704662902547\n",
      "14 Train Loss 2.0451667 Test MSE 386.1743114412976 Test RE 1.0017382867459552\n",
      "15 Train Loss 1.8806316 Test MSE 387.2861064986464 Test RE 1.0031792516028823\n",
      "16 Train Loss 1.8675866 Test MSE 387.3391486078809 Test RE 1.0032479461942472\n",
      "17 Train Loss 1.6123174 Test MSE 386.0990885573169 Test RE 1.0016407177070763\n",
      "18 Train Loss 1.5198107 Test MSE 386.3353870140208 Test RE 1.0019471804260829\n",
      "19 Train Loss 1.3400037 Test MSE 386.28815588277763 Test RE 1.0018859324187572\n",
      "20 Train Loss 1.2592325 Test MSE 386.1527457370194 Test RE 1.0017103155777167\n",
      "21 Train Loss 1.2394464 Test MSE 386.08230306800505 Test RE 1.001618944523976\n",
      "22 Train Loss 1.1895434 Test MSE 386.1545772266289 Test RE 1.0017126910882879\n",
      "23 Train Loss 1.182699 Test MSE 386.2665636199135 Test RE 1.0018579309302373\n",
      "24 Train Loss 1.1790959 Test MSE 386.44110824739886 Test RE 1.0020842631907878\n",
      "25 Train Loss 1.1759986 Test MSE 386.53750392184907 Test RE 1.002209237686539\n",
      "26 Train Loss 1.1753874 Test MSE 386.48904820934945 Test RE 1.0021464180581388\n",
      "27 Train Loss 1.1748809 Test MSE 386.406845418205 Test RE 1.0020398385652907\n",
      "28 Train Loss 1.1742109 Test MSE 386.07982705509096 Test RE 1.0016157327406354\n",
      "29 Train Loss 1.1738994 Test MSE 385.6313737838383 Test RE 1.0010338474250913\n",
      "30 Train Loss 1.1741723 Test MSE 385.0064669173013 Test RE 1.0002224422898407\n",
      "31 Train Loss 1.1749276 Test MSE 384.579794282184 Test RE 0.9996680543954877\n",
      "32 Train Loss 1.1754545 Test MSE 384.24289288204426 Test RE 0.9992300915025729\n",
      "33 Train Loss 1.1759583 Test MSE 384.077653496754 Test RE 0.999015214501432\n",
      "34 Train Loss 1.1771637 Test MSE 384.0549757484701 Test RE 0.9989857207912536\n",
      "35 Train Loss 1.1785897 Test MSE 384.050957645298 Test RE 0.9989804949271895\n",
      "36 Train Loss 1.1802144 Test MSE 384.05375542906717 Test RE 0.998984133671305\n",
      "37 Train Loss 1.1817609 Test MSE 384.0554239053699 Test RE 0.9989863036533974\n",
      "38 Train Loss 1.1832588 Test MSE 384.0539487975532 Test RE 0.9989843851621736\n",
      "39 Train Loss 1.1847259 Test MSE 384.0531188926177 Test RE 0.9989833058053251\n",
      "40 Train Loss 1.1861448 Test MSE 384.05319492033755 Test RE 0.9989834046854219\n",
      "41 Train Loss 1.1874979 Test MSE 384.0529426673386 Test RE 0.9989830766102952\n",
      "42 Train Loss 1.1888158 Test MSE 384.0524352807541 Test RE 0.9989824167132832\n",
      "43 Train Loss 1.1900915 Test MSE 384.0524736314705 Test RE 0.998982466591486\n",
      "44 Train Loss 1.1913137 Test MSE 384.05225461975556 Test RE 0.9989821817490367\n",
      "45 Train Loss 1.1925042 Test MSE 384.05191163542406 Test RE 0.9989817356700096\n",
      "46 Train Loss 1.1936519 Test MSE 384.0490401854092 Test RE 0.9989780011078387\n",
      "47 Train Loss 1.1947585 Test MSE 384.0490181354406 Test RE 0.9989779724299469\n",
      "48 Train Loss 1.1955873 Test MSE 384.0329702837291 Test RE 0.9989571005918698\n",
      "49 Train Loss 1.1966383 Test MSE 384.0239886325572 Test RE 0.9989454188648196\n",
      "50 Train Loss 1.197611 Test MSE 384.02120436335844 Test RE 0.998941797557187\n",
      "51 Train Loss 1.1985964 Test MSE 384.0181378912429 Test RE 0.9989378091924201\n",
      "52 Train Loss 1.19954 Test MSE 384.0181378912429 Test RE 0.9989378091924201\n",
      "53 Train Loss 1.2004468 Test MSE 384.01652758145684 Test RE 0.9989357147587759\n",
      "54 Train Loss 1.2013301 Test MSE 384.01652758145684 Test RE 0.9989357147587759\n",
      "55 Train Loss 1.2021793 Test MSE 384.00780539941604 Test RE 0.9989243702618593\n",
      "56 Train Loss 1.2030032 Test MSE 384.00780539941604 Test RE 0.9989243702618593\n",
      "57 Train Loss 1.2038084 Test MSE 384.00780539941604 Test RE 0.9989243702618593\n",
      "58 Train Loss 1.2045946 Test MSE 384.00620822247384 Test RE 0.99892229288139\n",
      "59 Train Loss 1.2053641 Test MSE 384.00620822247384 Test RE 0.99892229288139\n",
      "60 Train Loss 1.2060568 Test MSE 383.9969910079531 Test RE 0.9989103043560529\n",
      "61 Train Loss 1.206769 Test MSE 383.99080545418167 Test RE 0.9989022589306619\n",
      "62 Train Loss 1.2074537 Test MSE 383.99080545418167 Test RE 0.9989022589306619\n",
      "63 Train Loss 1.2081267 Test MSE 383.9866047045748 Test RE 0.9988967950631833\n",
      "64 Train Loss 1.2087699 Test MSE 383.9866047045748 Test RE 0.9988967950631833\n",
      "65 Train Loss 1.209406 Test MSE 383.9845451233447 Test RE 0.9988941161782646\n",
      "66 Train Loss 1.2100157 Test MSE 383.9845451233447 Test RE 0.9988941161782646\n",
      "67 Train Loss 1.2105987 Test MSE 383.9822835686017 Test RE 0.9988911745793368\n",
      "68 Train Loss 1.2111899 Test MSE 383.9822835686017 Test RE 0.9988911745793368\n",
      "69 Train Loss 1.2117364 Test MSE 383.9822835686017 Test RE 0.9988911745793368\n",
      "70 Train Loss 1.2122912 Test MSE 383.9822835686017 Test RE 0.9988911745793368\n",
      "71 Train Loss 1.2128097 Test MSE 383.9822835686017 Test RE 0.9988911745793368\n",
      "72 Train Loss 1.213344 Test MSE 383.9822835686017 Test RE 0.9988911745793368\n",
      "73 Train Loss 1.2138435 Test MSE 383.9822835686017 Test RE 0.9988911745793368\n",
      "74 Train Loss 1.2143278 Test MSE 383.9822835686017 Test RE 0.9988911745793368\n",
      "75 Train Loss 1.2147897 Test MSE 383.9822835686017 Test RE 0.9988911745793368\n",
      "76 Train Loss 1.215266 Test MSE 383.9822835686017 Test RE 0.9988911745793368\n",
      "77 Train Loss 1.2157212 Test MSE 383.9822835686017 Test RE 0.9988911745793368\n",
      "78 Train Loss 1.2161533 Test MSE 383.9822835686017 Test RE 0.9988911745793368\n",
      "79 Train Loss 1.2165706 Test MSE 383.979412626334 Test RE 0.9988874403390123\n",
      "80 Train Loss 1.2169873 Test MSE 383.979412626334 Test RE 0.9988874403390123\n",
      "81 Train Loss 1.2174014 Test MSE 383.9744191054303 Test RE 0.9988809452231723\n",
      "82 Train Loss 1.217779 Test MSE 383.9744191054303 Test RE 0.9988809452231723\n",
      "83 Train Loss 1.2181748 Test MSE 383.9744191054303 Test RE 0.9988809452231723\n",
      "84 Train Loss 1.2185415 Test MSE 383.970976519578 Test RE 0.9988764673973762\n",
      "85 Train Loss 1.2189022 Test MSE 383.970976519578 Test RE 0.9988764673973762\n",
      "86 Train Loss 1.219256 Test MSE 383.970976519578 Test RE 0.9988764673973762\n",
      "87 Train Loss 1.2196107 Test MSE 383.96788151744704 Test RE 0.9988724416620809\n",
      "88 Train Loss 1.2199522 Test MSE 383.96788151744704 Test RE 0.9988724416620809\n",
      "89 Train Loss 1.2202725 Test MSE 383.96707022996395 Test RE 0.9988713864004612\n",
      "90 Train Loss 1.220594 Test MSE 383.96707022996395 Test RE 0.9988713864004612\n",
      "91 Train Loss 1.2209076 Test MSE 383.9708132897754 Test RE 0.9988762550812951\n",
      "92 Train Loss 1.2212095 Test MSE 383.9708132897754 Test RE 0.9988762550812951\n",
      "93 Train Loss 1.2215099 Test MSE 383.9708132897754 Test RE 0.9988762550812951\n",
      "94 Train Loss 1.2218064 Test MSE 383.97440531485074 Test RE 0.9988809272855876\n",
      "95 Train Loss 1.2220885 Test MSE 383.97440531485074 Test RE 0.9988809272855876\n",
      "96 Train Loss 1.2223654 Test MSE 383.97440531485074 Test RE 0.9988809272855876\n",
      "97 Train Loss 1.2226278 Test MSE 383.9686998396031 Test RE 0.9988735060727437\n",
      "98 Train Loss 1.2228941 Test MSE 383.9686998396031 Test RE 0.9988735060727437\n",
      "99 Train Loss 1.223155 Test MSE 383.9686998396031 Test RE 0.9988735060727437\n",
      "100 Train Loss 1.2234174 Test MSE 383.9686998396031 Test RE 0.9988735060727437\n",
      "101 Train Loss 1.2236505 Test MSE 383.9675463955467 Test RE 0.9988720057609027\n",
      "102 Train Loss 1.223901 Test MSE 383.9662578166813 Test RE 0.9988703296733691\n",
      "103 Train Loss 1.2241172 Test MSE 383.9662578166813 Test RE 0.9988703296733691\n",
      "104 Train Loss 1.224365 Test MSE 383.9635821883279 Test RE 0.9988668494060576\n",
      "105 Train Loss 1.2245823 Test MSE 383.9635821883279 Test RE 0.9988668494060576\n",
      "106 Train Loss 1.2248234 Test MSE 383.9635821883279 Test RE 0.9988668494060576\n",
      "107 Train Loss 1.2250192 Test MSE 383.9657289872422 Test RE 0.998869641810559\n",
      "108 Train Loss 1.2252402 Test MSE 383.9657289872422 Test RE 0.998869641810559\n",
      "109 Train Loss 1.2254256 Test MSE 383.954421262079 Test RE 0.9988549334320627\n",
      "110 Train Loss 1.2256281 Test MSE 383.95755476017655 Test RE 0.9988590093112222\n",
      "111 Train Loss 1.2258238 Test MSE 383.95755476017655 Test RE 0.9988590093112222\n",
      "112 Train Loss 1.2260234 Test MSE 383.9563482467705 Test RE 0.9988574399479982\n",
      "113 Train Loss 1.2261262 Test MSE 383.93110727477824 Test RE 0.9988246073784222\n",
      "114 Train Loss 1.226325 Test MSE 383.927179791731 Test RE 0.9988194985494283\n",
      "115 Train Loss 1.2264949 Test MSE 383.92286655292025 Test RE 0.9988138879032525\n",
      "116 Train Loss 1.226674 Test MSE 383.92010441724415 Test RE 0.9988102949101332\n",
      "117 Train Loss 1.2268512 Test MSE 383.92010441724415 Test RE 0.9988102949101332\n",
      "118 Train Loss 1.227022 Test MSE 383.92010441724415 Test RE 0.9988102949101332\n",
      "119 Train Loss 1.2271917 Test MSE 383.92010441724415 Test RE 0.9988102949101332\n",
      "120 Train Loss 1.2273644 Test MSE 383.9171457294877 Test RE 0.9988064462272298\n",
      "121 Train Loss 1.2275172 Test MSE 383.9171457294877 Test RE 0.9988064462272298\n",
      "122 Train Loss 1.2276837 Test MSE 383.91494569017516 Test RE 0.9988035843902164\n",
      "123 Train Loss 1.2278416 Test MSE 383.91494569017516 Test RE 0.9988035843902164\n",
      "124 Train Loss 1.227992 Test MSE 383.9041491054421 Test RE 0.9987895399469511\n",
      "125 Train Loss 1.2281505 Test MSE 383.9041491054421 Test RE 0.9987895399469511\n",
      "126 Train Loss 1.2282761 Test MSE 383.90712909076626 Test RE 0.9987934163994485\n",
      "127 Train Loss 1.2284402 Test MSE 383.90712909076626 Test RE 0.9987934163994485\n",
      "128 Train Loss 1.228565 Test MSE 383.90039322469556 Test RE 0.998784654165452\n",
      "129 Train Loss 1.2287017 Test MSE 383.8990653750218 Test RE 0.998782926846273\n",
      "130 Train Loss 1.2288283 Test MSE 383.89531045386434 Test RE 0.9987780422807031\n",
      "131 Train Loss 1.228975 Test MSE 383.88269325050317 Test RE 0.9987616290960741\n",
      "132 Train Loss 1.2290974 Test MSE 383.88269325050317 Test RE 0.9987616290960741\n",
      "133 Train Loss 1.2292057 Test MSE 383.8686747430391 Test RE 0.9987433926980505\n",
      "134 Train Loss 1.2293439 Test MSE 383.8537881809562 Test RE 0.9987240267004495\n",
      "135 Train Loss 1.2294601 Test MSE 383.84755292505344 Test RE 0.9987159151155657\n",
      "136 Train Loss 1.2295616 Test MSE 383.84755292505344 Test RE 0.9987159151155657\n",
      "137 Train Loss 1.229694 Test MSE 383.8324145792037 Test RE 0.9986962210262287\n",
      "138 Train Loss 1.2298241 Test MSE 383.8324145792037 Test RE 0.9986962210262287\n",
      "139 Train Loss 1.2299404 Test MSE 383.8324145792037 Test RE 0.9986962210262287\n",
      "140 Train Loss 1.2300407 Test MSE 383.82745964226 Test RE 0.9986897748630293\n",
      "141 Train Loss 1.2301625 Test MSE 383.82745964226 Test RE 0.9986897748630293\n",
      "142 Train Loss 1.2302585 Test MSE 383.81630925888334 Test RE 0.9986752685341299\n",
      "143 Train Loss 1.230374 Test MSE 383.81630925888334 Test RE 0.9986752685341299\n",
      "144 Train Loss 1.230472 Test MSE 383.81630925888334 Test RE 0.9986752685341299\n",
      "145 Train Loss 1.2305797 Test MSE 383.81221783934393 Test RE 0.9986699456618371\n",
      "146 Train Loss 1.2306818 Test MSE 383.81221783934393 Test RE 0.9986699456618371\n",
      "147 Train Loss 1.2307938 Test MSE 383.81221783934393 Test RE 0.9986699456618371\n",
      "148 Train Loss 1.2308822 Test MSE 383.7985383568384 Test RE 0.9986521486648376\n",
      "149 Train Loss 1.2309866 Test MSE 383.7985383568384 Test RE 0.9986521486648376\n",
      "150 Train Loss 1.2310033 Test MSE 383.78319037515126 Test RE 0.9986321805740371\n",
      "151 Train Loss 1.231079 Test MSE 383.77788063869076 Test RE 0.9986252723871062\n",
      "152 Train Loss 1.2311703 Test MSE 383.7680928529287 Test RE 0.998612537947425\n",
      "153 Train Loss 1.2312517 Test MSE 383.7622229440187 Test RE 0.9986049007999417\n",
      "154 Train Loss 1.2313361 Test MSE 383.75477277188855 Test RE 0.9985952075392912\n",
      "155 Train Loss 1.2313659 Test MSE 383.6973748633356 Test RE 0.9985205251850169\n",
      "156 Train Loss 1.2314125 Test MSE 383.6808019353015 Test RE 0.9984989605502487\n",
      "157 Train Loss 1.231518 Test MSE 383.6745532945752 Test RE 0.9984908297203476\n",
      "158 Train Loss 1.2314185 Test MSE 383.5727736379628 Test RE 0.9983583831018016\n",
      "159 Train Loss 1.2311606 Test MSE 383.50799412035735 Test RE 0.9982740758859283\n",
      "160 Train Loss 1.2312446 Test MSE 383.50831746198367 Test RE 0.9982744967161327\n",
      "161 Train Loss 1.231328 Test MSE 383.50957126373766 Test RE 0.9982761285416174\n",
      "162 Train Loss 1.2313921 Test MSE 383.51024211861017 Test RE 0.9982770016594767\n",
      "163 Train Loss 1.2314733 Test MSE 383.50969018755785 Test RE 0.9982762833210681\n",
      "164 Train Loss 1.2315334 Test MSE 383.5069864615808 Test RE 0.9982727644135337\n",
      "165 Train Loss 1.2316116 Test MSE 383.50468499143926 Test RE 0.9982697690334122\n",
      "166 Train Loss 1.2316725 Test MSE 383.4981479938984 Test RE 0.9982612610345676\n",
      "167 Train Loss 1.2317303 Test MSE 383.4937305063883 Test RE 0.9982555115682733\n",
      "168 Train Loss 1.2318064 Test MSE 383.48521275257553 Test RE 0.9982444254132358\n",
      "169 Train Loss 1.231868 Test MSE 383.48101256577496 Test RE 0.9982389586776216\n",
      "170 Train Loss 1.231936 Test MSE 383.4766683775545 Test RE 0.9982333044860648\n",
      "171 Train Loss 1.2319925 Test MSE 383.47217859526796 Test RE 0.9982274607624481\n",
      "172 Train Loss 1.2320487 Test MSE 383.46560928648364 Test RE 0.9982189103474793\n",
      "173 Train Loss 1.2321312 Test MSE 383.459669821593 Test RE 0.9982111796553574\n",
      "174 Train Loss 1.2321922 Test MSE 383.45597638933066 Test RE 0.9982063723254824\n",
      "175 Train Loss 1.2322468 Test MSE 383.45376861022754 Test RE 0.998203498693987\n",
      "176 Train Loss 1.2323096 Test MSE 383.4453495887079 Test RE 0.9981925404722229\n",
      "177 Train Loss 1.2323707 Test MSE 383.442879790489 Test RE 0.998189325753188\n",
      "178 Train Loss 1.2324102 Test MSE 383.43871304924784 Test RE 0.9981839022492598\n",
      "179 Train Loss 1.2324804 Test MSE 383.43871304924784 Test RE 0.9981839022492598\n",
      "180 Train Loss 1.232549 Test MSE 383.43871304924784 Test RE 0.9981839022492598\n",
      "181 Train Loss 1.2326064 Test MSE 383.439323729262 Test RE 0.9981846971229854\n",
      "182 Train Loss 1.2326565 Test MSE 383.439323729262 Test RE 0.9981846971229854\n",
      "183 Train Loss 1.2327164 Test MSE 383.439323729262 Test RE 0.9981846971229854\n",
      "184 Train Loss 1.2327694 Test MSE 383.439323729262 Test RE 0.9981846971229854\n",
      "185 Train Loss 1.2328253 Test MSE 383.439323729262 Test RE 0.9981846971229854\n",
      "186 Train Loss 1.232877 Test MSE 383.4404238900981 Test RE 0.9981861291134667\n",
      "187 Train Loss 1.2329329 Test MSE 383.4404238900981 Test RE 0.9981861291134667\n",
      "188 Train Loss 1.2329935 Test MSE 383.4404238900981 Test RE 0.9981861291134667\n",
      "189 Train Loss 1.2330376 Test MSE 383.4413449856274 Test RE 0.9981873280275241\n",
      "190 Train Loss 1.2330884 Test MSE 383.4413449856274 Test RE 0.9981873280275241\n",
      "191 Train Loss 1.2331401 Test MSE 383.4413449856274 Test RE 0.9981873280275241\n",
      "192 Train Loss 1.2331799 Test MSE 383.44385221093495 Test RE 0.9981905914685589\n",
      "193 Train Loss 1.2332367 Test MSE 383.44385221093495 Test RE 0.9981905914685589\n",
      "194 Train Loss 1.2332546 Test MSE 383.42257480720275 Test RE 0.9981628961505349\n",
      "195 Train Loss 1.2332946 Test MSE 383.4216375993326 Test RE 0.9981616762345008\n",
      "196 Train Loss 1.2333463 Test MSE 383.42068375991147 Test RE 0.9981604346684883\n",
      "197 Train Loss 1.2333828 Test MSE 383.4180052775469 Test RE 0.9981569482110493\n",
      "198 Train Loss 1.2334285 Test MSE 383.4161808552079 Test RE 0.9981545734373356\n",
      "199 Train Loss 1.233456 Test MSE 383.4158028424352 Test RE 0.9981540813933247\n",
      "Training time: 83.98\n",
      "Training time: 83.98\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (m_lambda): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "6\n",
      "0 Train Loss 131.36101 Test MSE 381.8842183151927 Test RE 0.996158484083955\n",
      "1 Train Loss 6.5258484 Test MSE 383.5386418503286 Test RE 0.9983139632201794\n",
      "2 Train Loss 2.1686754 Test MSE 384.2659106903955 Test RE 0.9992600201528228\n",
      "3 Train Loss 1.9759763 Test MSE 384.65807004139293 Test RE 0.999769783342023\n",
      "4 Train Loss 1.9946071 Test MSE 385.14627233144574 Test RE 1.0004040286170481\n",
      "5 Train Loss 1.952298 Test MSE 384.99403476138883 Test RE 1.000206293182224\n",
      "6 Train Loss 1.9685053 Test MSE 385.1761011208111 Test RE 1.000442767487755\n",
      "7 Train Loss 1.9679195 Test MSE 385.7028611444315 Test RE 1.0011266276762945\n",
      "8 Train Loss 1.9871151 Test MSE 385.8251918317932 Test RE 1.001285375266415\n",
      "9 Train Loss 2.001886 Test MSE 385.8766714419884 Test RE 1.0013521724359822\n",
      "10 Train Loss 2.0009813 Test MSE 385.9553525030379 Test RE 1.0014542561345052\n",
      "11 Train Loss 2.002818 Test MSE 386.15368466165086 Test RE 1.0017115333988087\n",
      "12 Train Loss 2.0108383 Test MSE 386.2313644770343 Test RE 1.0018122819572255\n",
      "13 Train Loss 1.9964607 Test MSE 386.3416936107389 Test RE 1.0019553583606247\n",
      "14 Train Loss 1.999864 Test MSE 386.26386724318706 Test RE 1.0018544341337543\n",
      "15 Train Loss 1.9921315 Test MSE 385.8238187932316 Test RE 1.0012835936245166\n",
      "16 Train Loss 1.9724581 Test MSE 385.6263124253685 Test RE 1.0010272781881058\n",
      "17 Train Loss 1.6862521 Test MSE 384.5908298294203 Test RE 0.9996823970690497\n",
      "18 Train Loss 1.1797407 Test MSE 384.06425032619137 Test RE 0.9989977830158817\n",
      "19 Train Loss 1.1638751 Test MSE 384.13425559252516 Test RE 0.9990888249694673\n",
      "20 Train Loss 1.1506535 Test MSE 384.22853094746563 Test RE 0.9992114171021214\n",
      "21 Train Loss 1.1449176 Test MSE 384.23379637005166 Test RE 0.9992182636165199\n",
      "22 Train Loss 1.1450306 Test MSE 384.3577710511628 Test RE 0.9993794516431213\n",
      "23 Train Loss 1.1474196 Test MSE 384.4157878448197 Test RE 0.9994548743455988\n",
      "24 Train Loss 1.1497406 Test MSE 384.3029112073605 Test RE 0.9993081277865177\n",
      "25 Train Loss 1.1522638 Test MSE 384.2944907565806 Test RE 0.9992971798197547\n",
      "26 Train Loss 1.1547147 Test MSE 384.2899773078776 Test RE 0.9992913115469696\n",
      "27 Train Loss 1.1571076 Test MSE 384.28643979523514 Test RE 0.9992867121376795\n",
      "28 Train Loss 1.1593692 Test MSE 384.2811405782766 Test RE 0.9992798221530239\n",
      "29 Train Loss 1.1616155 Test MSE 384.27952162006704 Test RE 0.9992777171915712\n",
      "30 Train Loss 1.1637673 Test MSE 384.2752557946969 Test RE 0.9992721707653006\n",
      "31 Train Loss 1.1658238 Test MSE 384.2726295317227 Test RE 0.9992687560827943\n",
      "32 Train Loss 1.167853 Test MSE 384.2686695894852 Test RE 0.9992636073206342\n",
      "33 Train Loss 1.1697562 Test MSE 384.2686695894852 Test RE 0.9992636073206342\n",
      "34 Train Loss 1.1716387 Test MSE 384.258439566891 Test RE 0.9992503060055303\n",
      "35 Train Loss 1.1734573 Test MSE 384.25657020650283 Test RE 0.9992478754002145\n",
      "36 Train Loss 1.1751795 Test MSE 384.25572454231246 Test RE 0.9992467758372198\n",
      "37 Train Loss 1.1765342 Test MSE 384.18610265386917 Test RE 0.9991562468032261\n",
      "38 Train Loss 1.1780899 Test MSE 384.18559687037947 Test RE 0.9991555891051779\n",
      "39 Train Loss 1.1794747 Test MSE 384.12703347176085 Test RE 0.999079432974405\n",
      "40 Train Loss 1.1809199 Test MSE 384.1039103152882 Test RE 0.999049361909733\n",
      "41 Train Loss 1.1823792 Test MSE 384.09150074191086 Test RE 0.9990332232085001\n",
      "42 Train Loss 1.1837964 Test MSE 384.0870330185972 Test RE 0.9990274128521223\n",
      "43 Train Loss 1.1851461 Test MSE 384.0833362809356 Test RE 0.9990226051512223\n",
      "44 Train Loss 1.1864592 Test MSE 384.0792950262948 Test RE 0.9990173493707994\n",
      "45 Train Loss 1.1877396 Test MSE 384.07441232697255 Test RE 0.9990109992278717\n",
      "46 Train Loss 1.188961 Test MSE 384.07183495141527 Test RE 0.9990076472330919\n",
      "47 Train Loss 1.1901531 Test MSE 384.0702782154065 Test RE 0.999005622621272\n",
      "48 Train Loss 1.1912866 Test MSE 384.06734279561005 Test RE 0.9990018049521487\n",
      "49 Train Loss 1.1924087 Test MSE 384.06734279561005 Test RE 0.9990018049521487\n",
      "50 Train Loss 1.1934742 Test MSE 384.06782857425236 Test RE 0.9990024367340437\n",
      "51 Train Loss 1.1945426 Test MSE 384.06782857425236 Test RE 0.9990024367340437\n",
      "52 Train Loss 1.1955334 Test MSE 384.06782857425236 Test RE 0.9990024367340437\n",
      "53 Train Loss 1.196511 Test MSE 384.06782857425236 Test RE 0.9990024367340437\n",
      "54 Train Loss 1.1974562 Test MSE 384.06782857425236 Test RE 0.9990024367340437\n",
      "55 Train Loss 1.1983777 Test MSE 384.06218618743236 Test RE 0.9989950984744115\n",
      "56 Train Loss 1.1992425 Test MSE 384.06218618743236 Test RE 0.9989950984744115\n",
      "57 Train Loss 1.2001094 Test MSE 384.05665886180674 Test RE 0.9989879098064384\n",
      "58 Train Loss 1.2009325 Test MSE 384.05665886180674 Test RE 0.9989879098064384\n",
      "59 Train Loss 1.2017542 Test MSE 384.05665886180674 Test RE 0.9989879098064384\n",
      "60 Train Loss 1.2025255 Test MSE 384.05665886180674 Test RE 0.9989879098064384\n",
      "61 Train Loss 1.2032883 Test MSE 384.05665886180674 Test RE 0.9989879098064384\n",
      "62 Train Loss 1.2040493 Test MSE 384.05665886180674 Test RE 0.9989879098064384\n",
      "63 Train Loss 1.2047713 Test MSE 384.0536903362902 Test RE 0.9989840490130333\n",
      "64 Train Loss 1.2054862 Test MSE 384.0536903362902 Test RE 0.9989840490130333\n",
      "65 Train Loss 1.2061696 Test MSE 384.0536903362902 Test RE 0.9989840490130333\n",
      "66 Train Loss 1.2067751 Test MSE 384.0536903362902 Test RE 0.9989840490130333\n",
      "67 Train Loss 1.207418 Test MSE 384.0536903362902 Test RE 0.9989840490130333\n",
      "68 Train Loss 1.2080488 Test MSE 384.04710808495673 Test RE 0.9989754882408955\n",
      "69 Train Loss 1.2086527 Test MSE 384.04710808495673 Test RE 0.9989754882408955\n",
      "70 Train Loss 1.209219 Test MSE 384.0488361675683 Test RE 0.9989777357649522\n",
      "71 Train Loss 1.2097867 Test MSE 384.0488361675683 Test RE 0.9989777357649522\n",
      "72 Train Loss 1.2103448 Test MSE 384.0488361675683 Test RE 0.9989777357649522\n",
      "73 Train Loss 1.21086 Test MSE 384.0488361675683 Test RE 0.9989777357649522\n",
      "74 Train Loss 1.2114 Test MSE 384.0459323959375 Test RE 0.9989739591506015\n",
      "75 Train Loss 1.2118942 Test MSE 384.0459323959375 Test RE 0.9989739591506015\n",
      "76 Train Loss 1.2123905 Test MSE 384.0459323959375 Test RE 0.9989739591506015\n",
      "77 Train Loss 1.2128713 Test MSE 384.0459323959375 Test RE 0.9989739591506015\n",
      "78 Train Loss 1.2133858 Test MSE 384.0459323959375 Test RE 0.9989739591506015\n",
      "79 Train Loss 1.2138426 Test MSE 384.0459323959375 Test RE 0.9989739591506015\n",
      "80 Train Loss 1.2142389 Test MSE 384.04187869927773 Test RE 0.9989686869321689\n",
      "81 Train Loss 1.2146759 Test MSE 384.04187869927773 Test RE 0.9989686869321689\n",
      "82 Train Loss 1.2150937 Test MSE 384.04187869927773 Test RE 0.9989686869321689\n",
      "83 Train Loss 1.2155132 Test MSE 384.04187869927773 Test RE 0.9989686869321689\n",
      "84 Train Loss 1.2159009 Test MSE 384.0388673231574 Test RE 0.9989647703328175\n",
      "85 Train Loss 1.216322 Test MSE 384.0388673231574 Test RE 0.9989647703328175\n",
      "86 Train Loss 1.216677 Test MSE 384.0388673231574 Test RE 0.9989647703328175\n",
      "87 Train Loss 1.2170112 Test MSE 384.0388673231574 Test RE 0.9989647703328175\n",
      "88 Train Loss 1.2173867 Test MSE 384.0364523651083 Test RE 0.9989616294244218\n",
      "89 Train Loss 1.2177746 Test MSE 384.0364523651083 Test RE 0.9989616294244218\n",
      "90 Train Loss 1.2181442 Test MSE 384.0364523651083 Test RE 0.9989616294244218\n",
      "91 Train Loss 1.2184793 Test MSE 384.0364523651083 Test RE 0.9989616294244218\n",
      "92 Train Loss 1.2187523 Test MSE 384.0364523651083 Test RE 0.9989616294244218\n",
      "93 Train Loss 1.219075 Test MSE 384.0364523651083 Test RE 0.9989616294244218\n",
      "94 Train Loss 1.2193816 Test MSE 384.0364523651083 Test RE 0.9989616294244218\n",
      "95 Train Loss 1.2197328 Test MSE 384.033867463915 Test RE 0.998958267476048\n",
      "96 Train Loss 1.2200583 Test MSE 384.033867463915 Test RE 0.998958267476048\n",
      "97 Train Loss 1.2203202 Test MSE 384.033867463915 Test RE 0.998958267476048\n",
      "98 Train Loss 1.2206023 Test MSE 384.033867463915 Test RE 0.998958267476048\n",
      "99 Train Loss 1.2209073 Test MSE 384.033867463915 Test RE 0.998958267476048\n",
      "100 Train Loss 1.2211237 Test MSE 384.033867463915 Test RE 0.998958267476048\n",
      "101 Train Loss 1.2214055 Test MSE 384.033867463915 Test RE 0.998958267476048\n",
      "102 Train Loss 1.2217003 Test MSE 384.033867463915 Test RE 0.998958267476048\n",
      "103 Train Loss 1.2219322 Test MSE 384.0350000141345 Test RE 0.9989597404835956\n",
      "104 Train Loss 1.2221577 Test MSE 384.03569718014165 Test RE 0.9989606472244623\n",
      "105 Train Loss 1.2224171 Test MSE 384.0365539879139 Test RE 0.998961761595829\n",
      "106 Train Loss 1.222644 Test MSE 384.0365539879139 Test RE 0.998961761595829\n",
      "107 Train Loss 1.2228429 Test MSE 384.0365539879139 Test RE 0.998961761595829\n",
      "108 Train Loss 1.2231296 Test MSE 384.0365539879139 Test RE 0.998961761595829\n",
      "109 Train Loss 1.2233114 Test MSE 384.03798197164275 Test RE 0.9989636188406672\n",
      "110 Train Loss 1.223534 Test MSE 384.03798197164275 Test RE 0.9989636188406672\n",
      "111 Train Loss 1.223762 Test MSE 384.03798197164275 Test RE 0.9989636188406672\n",
      "112 Train Loss 1.2239777 Test MSE 384.03798197164275 Test RE 0.9989636188406672\n",
      "113 Train Loss 1.224182 Test MSE 384.03940009743104 Test RE 0.9989654632607873\n",
      "114 Train Loss 1.2244012 Test MSE 384.03940009743104 Test RE 0.9989654632607873\n",
      "115 Train Loss 1.2245742 Test MSE 384.0419058320206 Test RE 0.9989687222209767\n",
      "116 Train Loss 1.2247543 Test MSE 384.0419058320206 Test RE 0.9989687222209767\n",
      "117 Train Loss 1.2249496 Test MSE 384.0419058320206 Test RE 0.9989687222209767\n",
      "118 Train Loss 1.2251241 Test MSE 384.0419058320206 Test RE 0.9989687222209767\n",
      "119 Train Loss 1.2252913 Test MSE 384.0428205130931 Test RE 0.9989699118531812\n",
      "120 Train Loss 1.22549 Test MSE 384.0428205130931 Test RE 0.9989699118531812\n",
      "121 Train Loss 1.2256613 Test MSE 384.0428205130931 Test RE 0.9989699118531812\n",
      "122 Train Loss 1.2258105 Test MSE 384.0441660009736 Test RE 0.9989716617892046\n",
      "123 Train Loss 1.2260048 Test MSE 384.0441660009736 Test RE 0.9989716617892046\n",
      "124 Train Loss 1.226157 Test MSE 384.04457356095014 Test RE 0.9989721918594346\n",
      "125 Train Loss 1.2263316 Test MSE 384.04457356095014 Test RE 0.9989721918594346\n",
      "126 Train Loss 1.2264727 Test MSE 384.04277347975244 Test RE 0.9989698506817556\n",
      "127 Train Loss 1.22667 Test MSE 384.0375189438182 Test RE 0.9989630166240736\n",
      "128 Train Loss 1.2267927 Test MSE 384.0375189438182 Test RE 0.9989630166240736\n",
      "129 Train Loss 1.2269484 Test MSE 384.0375189438182 Test RE 0.9989630166240736\n",
      "130 Train Loss 1.2270766 Test MSE 384.03853704035674 Test RE 0.9989643407656812\n",
      "131 Train Loss 1.2272362 Test MSE 384.03853704035674 Test RE 0.9989643407656812\n",
      "132 Train Loss 1.2273772 Test MSE 384.03853704035674 Test RE 0.9989643407656812\n",
      "133 Train Loss 1.2274611 Test MSE 384.03853704035674 Test RE 0.9989643407656812\n",
      "134 Train Loss 1.2276217 Test MSE 384.03853704035674 Test RE 0.9989643407656812\n",
      "135 Train Loss 1.2277892 Test MSE 384.02896249476055 Test RE 0.9989518879929484\n",
      "136 Train Loss 1.227884 Test MSE 384.02896249476055 Test RE 0.9989518879929484\n",
      "137 Train Loss 1.2280129 Test MSE 384.02896249476055 Test RE 0.9989518879929484\n",
      "138 Train Loss 1.2281516 Test MSE 384.02896249476055 Test RE 0.9989518879929484\n",
      "139 Train Loss 1.2282639 Test MSE 384.02896249476055 Test RE 0.9989518879929484\n",
      "140 Train Loss 1.2284044 Test MSE 384.02896249476055 Test RE 0.9989518879929484\n",
      "141 Train Loss 1.2285024 Test MSE 384.02896249476055 Test RE 0.9989518879929484\n",
      "142 Train Loss 1.2286438 Test MSE 384.02891727057926 Test RE 0.9989518291734489\n",
      "143 Train Loss 1.2287848 Test MSE 384.02891727057926 Test RE 0.9989518291734489\n",
      "144 Train Loss 1.228835 Test MSE 384.02891727057926 Test RE 0.9989518291734489\n",
      "145 Train Loss 1.2289914 Test MSE 384.02891727057926 Test RE 0.9989518291734489\n",
      "146 Train Loss 1.2290833 Test MSE 384.02891727057926 Test RE 0.9989518291734489\n",
      "147 Train Loss 1.2292265 Test MSE 384.02786796697444 Test RE 0.9989504644266495\n",
      "148 Train Loss 1.229271 Test MSE 384.02781394629733 Test RE 0.9989503941661466\n",
      "149 Train Loss 1.2293922 Test MSE 384.02781394629733 Test RE 0.9989503941661466\n",
      "150 Train Loss 1.2295126 Test MSE 384.02781394629733 Test RE 0.9989503941661466\n",
      "151 Train Loss 1.2296488 Test MSE 384.02781394629733 Test RE 0.9989503941661466\n",
      "152 Train Loss 1.2297179 Test MSE 384.0284972830198 Test RE 0.9989512829288347\n",
      "153 Train Loss 1.229741 Test MSE 384.0284972830198 Test RE 0.9989512829288347\n",
      "154 Train Loss 1.229868 Test MSE 384.0312350011195 Test RE 0.998954843656949\n",
      "155 Train Loss 1.2299937 Test MSE 384.0351292058593 Test RE 0.9989599085116674\n",
      "156 Train Loss 1.2300342 Test MSE 384.0402453983323 Test RE 0.9989665626609289\n",
      "157 Train Loss 1.2300842 Test MSE 384.0429137639735 Test RE 0.9989700331349931\n",
      "158 Train Loss 1.230187 Test MSE 384.0477471839981 Test RE 0.9989763194462319\n",
      "159 Train Loss 1.2303417 Test MSE 384.0477471839981 Test RE 0.9989763194462319\n",
      "160 Train Loss 1.2303959 Test MSE 384.0477471839981 Test RE 0.9989763194462319\n",
      "161 Train Loss 1.2305216 Test MSE 384.0549579167543 Test RE 0.9989856975997432\n",
      "162 Train Loss 1.230647 Test MSE 384.0549579167543 Test RE 0.9989856975997432\n",
      "163 Train Loss 1.2306426 Test MSE 384.05415511914237 Test RE 0.9989846534994985\n",
      "164 Train Loss 1.2307519 Test MSE 384.05415511914237 Test RE 0.9989846534994985\n",
      "165 Train Loss 1.2308486 Test MSE 384.05415511914237 Test RE 0.9989846534994985\n",
      "166 Train Loss 1.2308846 Test MSE 384.05415511914237 Test RE 0.9989846534994985\n",
      "167 Train Loss 1.2309703 Test MSE 384.05415511914237 Test RE 0.9989846534994985\n",
      "168 Train Loss 1.2310356 Test MSE 384.05415511914237 Test RE 0.9989846534994985\n",
      "169 Train Loss 1.2311312 Test MSE 384.0546569518771 Test RE 0.9989853061718235\n",
      "170 Train Loss 1.2312402 Test MSE 384.0546569518771 Test RE 0.9989853061718235\n",
      "171 Train Loss 1.2312601 Test MSE 384.0546569518771 Test RE 0.9989853061718235\n",
      "172 Train Loss 1.231345 Test MSE 384.0546569518771 Test RE 0.9989853061718235\n",
      "173 Train Loss 1.2314178 Test MSE 384.0546569518771 Test RE 0.9989853061718235\n",
      "174 Train Loss 1.2314514 Test MSE 384.0527694345934 Test RE 0.9989828513072432\n",
      "175 Train Loss 1.2315485 Test MSE 384.0527694345934 Test RE 0.9989828513072432\n",
      "176 Train Loss 1.231639 Test MSE 384.0527694345934 Test RE 0.9989828513072432\n",
      "177 Train Loss 1.231748 Test MSE 384.0527694345934 Test RE 0.9989828513072432\n",
      "178 Train Loss 1.2317963 Test MSE 384.0527694345934 Test RE 0.9989828513072432\n",
      "179 Train Loss 1.2318492 Test MSE 384.0527694345934 Test RE 0.9989828513072432\n",
      "180 Train Loss 1.2319214 Test MSE 384.0527694345934 Test RE 0.9989828513072432\n",
      "181 Train Loss 1.2319802 Test MSE 384.0555037484408 Test RE 0.9989864074953497\n",
      "182 Train Loss 1.2319673 Test MSE 384.0555037484408 Test RE 0.9989864074953497\n",
      "183 Train Loss 1.2321194 Test MSE 384.0555037484408 Test RE 0.9989864074953497\n",
      "184 Train Loss 1.23216 Test MSE 384.0555037484408 Test RE 0.9989864074953497\n",
      "185 Train Loss 1.232195 Test MSE 384.0555037484408 Test RE 0.9989864074953497\n",
      "186 Train Loss 1.2322711 Test MSE 384.0555037484408 Test RE 0.9989864074953497\n",
      "187 Train Loss 1.2323184 Test MSE 384.0555037484408 Test RE 0.9989864074953497\n",
      "188 Train Loss 1.2323354 Test MSE 384.0564184193903 Test RE 0.9989875970933282\n",
      "189 Train Loss 1.2323804 Test MSE 384.0564184193903 Test RE 0.9989875970933282\n",
      "190 Train Loss 1.2325075 Test MSE 384.0564184193903 Test RE 0.9989875970933282\n",
      "191 Train Loss 1.2325844 Test MSE 384.0545212495949 Test RE 0.9989851296805409\n",
      "192 Train Loss 1.2325999 Test MSE 384.0545212495949 Test RE 0.9989851296805409\n",
      "193 Train Loss 1.2326149 Test MSE 384.05567173978847 Test RE 0.9989866259807676\n",
      "194 Train Loss 1.2326742 Test MSE 384.05567173978847 Test RE 0.9989866259807676\n",
      "195 Train Loss 1.2327254 Test MSE 384.05567173978847 Test RE 0.9989866259807676\n",
      "196 Train Loss 1.2328018 Test MSE 384.05567173978847 Test RE 0.9989866259807676\n",
      "197 Train Loss 1.2328781 Test MSE 384.05567173978847 Test RE 0.9989866259807676\n",
      "198 Train Loss 1.2329255 Test MSE 384.05567173978847 Test RE 0.9989866259807676\n",
      "199 Train Loss 1.233009 Test MSE 384.05567173978847 Test RE 0.9989866259807676\n",
      "Training time: 81.33\n",
      "Training time: 81.33\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (m_lambda): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "7\n",
      "0 Train Loss 2.4600606 Test MSE 385.52067613503544 Test RE 1.0008901609225587\n",
      "1 Train Loss 1.9352642 Test MSE 385.09014209985276 Test RE 1.000331127800019\n",
      "2 Train Loss 1.9606549 Test MSE 384.9303375599707 Test RE 1.0001235477739865\n",
      "3 Train Loss 1.9342237 Test MSE 384.7367738505751 Test RE 0.9998720581568252\n",
      "4 Train Loss 1.9635227 Test MSE 384.8526411437169 Test RE 1.0000226075135852\n",
      "5 Train Loss 1.9892204 Test MSE 384.8423257647145 Test RE 1.0000092053951777\n",
      "6 Train Loss 1.9845858 Test MSE 385.4051556532385 Test RE 1.0007401923428636\n",
      "7 Train Loss 1.9954458 Test MSE 385.6249450610407 Test RE 1.001025503451579\n",
      "8 Train Loss 1.9933922 Test MSE 385.92172172458913 Test RE 1.0014106236007025\n",
      "9 Train Loss 1.9944596 Test MSE 386.1163420163364 Test RE 1.001663097420634\n",
      "10 Train Loss 2.0059812 Test MSE 386.12466253540384 Test RE 1.001673889908668\n",
      "11 Train Loss 2.0157316 Test MSE 385.8891181106954 Test RE 1.0013683218940461\n",
      "12 Train Loss 1.9775298 Test MSE 386.4361310507946 Test RE 1.0020778099611172\n",
      "13 Train Loss 1.9667511 Test MSE 386.65321510478987 Test RE 1.0023592336608154\n",
      "14 Train Loss 1.9734305 Test MSE 386.66265666314814 Test RE 1.0023714717271992\n",
      "15 Train Loss 1.8177537 Test MSE 384.9962024691286 Test RE 1.0002091090075038\n",
      "16 Train Loss 1.6178367 Test MSE 384.4028159586032 Test RE 0.9994380111928708\n",
      "17 Train Loss 1.5924534 Test MSE 384.4290048067637 Test RE 0.999472055798487\n",
      "18 Train Loss 1.4475621 Test MSE 385.2871513045421 Test RE 1.0005869760019963\n",
      "19 Train Loss 1.3867209 Test MSE 385.8018917704383 Test RE 1.001255140896799\n",
      "20 Train Loss 1.3576468 Test MSE 385.9544635070194 Test RE 1.0014531027765414\n",
      "21 Train Loss 1.3149374 Test MSE 385.79779034381204 Test RE 1.0012498187541443\n",
      "22 Train Loss 1.3083856 Test MSE 385.8177273085196 Test RE 1.0012756893331272\n",
      "23 Train Loss 1.2731099 Test MSE 385.71956925384916 Test RE 1.00114831114533\n",
      "24 Train Loss 1.193735 Test MSE 385.29784783256395 Test RE 1.0006008652950962\n",
      "25 Train Loss 1.1750395 Test MSE 384.81817458404834 Test RE 0.9999778265936609\n",
      "26 Train Loss 1.1698878 Test MSE 384.7791494283182 Test RE 0.9999271204648715\n",
      "27 Train Loss 1.1651225 Test MSE 384.84581950140654 Test RE 1.0000137446060313\n",
      "28 Train Loss 1.1659535 Test MSE 384.6896256216972 Test RE 0.9998107907602314\n",
      "29 Train Loss 1.1680787 Test MSE 384.6857729093137 Test RE 0.9998057841356661\n",
      "30 Train Loss 1.1701391 Test MSE 384.6826418660162 Test RE 0.9998017153062609\n",
      "31 Train Loss 1.1721416 Test MSE 384.67980966932146 Test RE 0.9997980348174409\n",
      "32 Train Loss 1.1740932 Test MSE 384.67980966932146 Test RE 0.9997980348174409\n",
      "33 Train Loss 1.1759104 Test MSE 384.6777539129903 Test RE 0.9997953633178791\n",
      "34 Train Loss 1.1777238 Test MSE 384.6777539129903 Test RE 0.9997953633178791\n",
      "35 Train Loss 1.1794463 Test MSE 384.6777539129903 Test RE 0.9997953633178791\n",
      "36 Train Loss 1.1811165 Test MSE 384.6758466187589 Test RE 0.9997928847416248\n",
      "37 Train Loss 1.1826984 Test MSE 384.6758466187589 Test RE 0.9997928847416248\n",
      "38 Train Loss 1.1842573 Test MSE 384.6758466187589 Test RE 0.9997928847416248\n",
      "39 Train Loss 1.1857665 Test MSE 384.6758466187589 Test RE 0.9997928847416248\n",
      "40 Train Loss 1.1871927 Test MSE 384.6752341118757 Test RE 0.9997920887724046\n",
      "41 Train Loss 1.1886513 Test MSE 384.6752341118757 Test RE 0.9997920887724046\n",
      "42 Train Loss 1.1899573 Test MSE 384.6752341118757 Test RE 0.9997920887724046\n",
      "43 Train Loss 1.19125 Test MSE 384.67691439720727 Test RE 0.9997942723470123\n",
      "44 Train Loss 1.1925569 Test MSE 384.67691439720727 Test RE 0.9997942723470123\n",
      "45 Train Loss 1.1937747 Test MSE 384.67691439720727 Test RE 0.9997942723470123\n",
      "46 Train Loss 1.1949679 Test MSE 384.67691439720727 Test RE 0.9997942723470123\n",
      "47 Train Loss 1.196088 Test MSE 384.67691439720727 Test RE 0.9997942723470123\n",
      "48 Train Loss 1.1971978 Test MSE 384.67691439720727 Test RE 0.9997942723470123\n",
      "49 Train Loss 1.1982584 Test MSE 384.6778180353108 Test RE 0.9997954466463213\n",
      "50 Train Loss 1.1992515 Test MSE 384.6778180353108 Test RE 0.9997954466463213\n",
      "51 Train Loss 1.2002808 Test MSE 384.6778180353108 Test RE 0.9997954466463213\n",
      "52 Train Loss 1.2012187 Test MSE 384.6779902717124 Test RE 0.9997956704714845\n",
      "53 Train Loss 1.2021623 Test MSE 384.6779902717124 Test RE 0.9997956704714845\n",
      "54 Train Loss 1.203056 Test MSE 384.6779902717124 Test RE 0.9997956704714845\n",
      "55 Train Loss 1.203923 Test MSE 384.6779902717124 Test RE 0.9997956704714845\n",
      "56 Train Loss 1.2047294 Test MSE 384.6779902717124 Test RE 0.9997956704714845\n",
      "57 Train Loss 1.2056142 Test MSE 384.6775863606293 Test RE 0.999795145579668\n",
      "58 Train Loss 1.206381 Test MSE 384.6775863606293 Test RE 0.999795145579668\n",
      "59 Train Loss 1.2071441 Test MSE 384.6790139988836 Test RE 0.9997970008274425\n",
      "60 Train Loss 1.2079042 Test MSE 384.6790139988836 Test RE 0.9997970008274425\n",
      "61 Train Loss 1.2085941 Test MSE 384.6790139988836 Test RE 0.9997970008274425\n",
      "62 Train Loss 1.2093498 Test MSE 384.6790139988836 Test RE 0.9997970008274425\n",
      "63 Train Loss 1.2100239 Test MSE 384.6748974054671 Test RE 0.9997916512130722\n",
      "64 Train Loss 1.2106675 Test MSE 384.6748974054671 Test RE 0.9997916512130722\n",
      "65 Train Loss 1.2113277 Test MSE 384.66178766959524 Test RE 0.9997746145982106\n",
      "66 Train Loss 1.2117188 Test MSE 384.6053798664795 Test RE 0.9997013071381765\n",
      "67 Train Loss 1.2123212 Test MSE 384.6002808025638 Test RE 0.9996946801407077\n",
      "68 Train Loss 1.2129129 Test MSE 384.5987459836409 Test RE 0.999692685404979\n",
      "69 Train Loss 1.2134588 Test MSE 384.5930233244178 Test RE 0.9996852478848265\n",
      "70 Train Loss 1.2140255 Test MSE 384.58994690533103 Test RE 0.9996812495583604\n",
      "71 Train Loss 1.2145574 Test MSE 384.5866578325498 Test RE 0.9996769748347955\n",
      "72 Train Loss 1.2150573 Test MSE 384.58440657078756 Test RE 0.9996740489170656\n",
      "73 Train Loss 1.2155461 Test MSE 384.5795339604401 Test RE 0.9996677160581922\n",
      "74 Train Loss 1.216053 Test MSE 384.5784762885662 Test RE 0.9996663414125593\n",
      "75 Train Loss 1.2164725 Test MSE 384.57252181468454 Test RE 0.999658602406702\n",
      "76 Train Loss 1.2169416 Test MSE 384.57174080754527 Test RE 0.9996575873304065\n",
      "77 Train Loss 1.2172841 Test MSE 384.5272547582935 Test RE 0.9995997670356714\n",
      "78 Train Loss 1.2177618 Test MSE 384.52197012659025 Test RE 0.9995928981665553\n",
      "79 Train Loss 1.2181576 Test MSE 384.5204152080694 Test RE 0.9995908771024774\n",
      "80 Train Loss 1.2185876 Test MSE 384.5201416740694 Test RE 0.9995905215659064\n",
      "81 Train Loss 1.2189778 Test MSE 384.5173446319402 Test RE 0.999586885993173\n",
      "82 Train Loss 1.2193892 Test MSE 384.5173446319402 Test RE 0.999586885993173\n",
      "83 Train Loss 1.2197837 Test MSE 384.52101429019586 Test RE 0.9995916557825828\n",
      "84 Train Loss 1.2201521 Test MSE 384.52101429019586 Test RE 0.9995916557825828\n",
      "85 Train Loss 1.2205393 Test MSE 384.5256410075533 Test RE 0.9995976695165277\n",
      "86 Train Loss 1.220891 Test MSE 384.5288784715612 Test RE 0.999601877499456\n",
      "87 Train Loss 1.2212055 Test MSE 384.53271016179065 Test RE 0.9996068578213777\n",
      "88 Train Loss 1.2215729 Test MSE 384.53271016179065 Test RE 0.9996068578213777\n",
      "89 Train Loss 1.2218695 Test MSE 384.54786326678317 Test RE 0.9996265531529406\n",
      "90 Train Loss 1.2219536 Test MSE 384.4763498249004 Test RE 0.9995335997574004\n",
      "91 Train Loss 1.2222688 Test MSE 384.4603916513434 Test RE 0.9995128560937325\n",
      "92 Train Loss 1.2225184 Test MSE 384.4480060448807 Test RE 0.9994967560313276\n",
      "93 Train Loss 1.2226182 Test MSE 384.35512029570515 Test RE 0.9993760054852671\n",
      "94 Train Loss 1.2226464 Test MSE 384.2682808993346 Test RE 0.9992631019398275\n",
      "95 Train Loss 1.2229512 Test MSE 384.2659980435382 Test RE 0.9992601337310713\n",
      "96 Train Loss 1.2232548 Test MSE 384.26519142624517 Test RE 0.9992590849511488\n",
      "97 Train Loss 1.2234969 Test MSE 384.25946585793446 Test RE 0.9992516404211854\n",
      "98 Train Loss 1.2237359 Test MSE 384.2537019863277 Test RE 0.9992441460322784\n",
      "99 Train Loss 1.2240319 Test MSE 384.24860221512864 Test RE 0.9992375150832481\n",
      "100 Train Loss 1.2242521 Test MSE 384.23185486801276 Test RE 0.9992157391291309\n",
      "101 Train Loss 1.2244823 Test MSE 384.2187440032285 Test RE 0.9991986912268844\n",
      "102 Train Loss 1.224763 Test MSE 384.21554447562477 Test RE 0.9991945308748721\n",
      "103 Train Loss 1.225009 Test MSE 384.2146332727843 Test RE 0.999193346032864\n",
      "104 Train Loss 1.2252015 Test MSE 384.21135763853664 Test RE 0.999189086696237\n",
      "105 Train Loss 1.2254614 Test MSE 384.2054216339638 Test RE 0.9991813680099213\n",
      "106 Train Loss 1.2256696 Test MSE 384.19891749664987 Test RE 0.9991729105026118\n",
      "107 Train Loss 1.225854 Test MSE 384.19490272511115 Test RE 0.9991676899498456\n",
      "108 Train Loss 1.2261138 Test MSE 384.1886357833793 Test RE 0.9991595407641275\n",
      "109 Train Loss 1.2262645 Test MSE 384.1886357833793 Test RE 0.9991595407641275\n",
      "110 Train Loss 1.2264925 Test MSE 384.1854959695091 Test RE 0.9991554578981833\n",
      "111 Train Loss 1.2266963 Test MSE 384.1736895872138 Test RE 0.9991401052858876\n",
      "112 Train Loss 1.2269188 Test MSE 384.17173437921883 Test RE 0.999137562778084\n",
      "113 Train Loss 1.2271117 Test MSE 384.16837961315787 Test RE 0.9991332003018742\n",
      "114 Train Loss 1.2272904 Test MSE 384.1682025821694 Test RE 0.999132970093497\n",
      "115 Train Loss 1.2272699 Test MSE 384.2222640050993 Test RE 0.9992032682722027\n",
      "116 Train Loss 1.2273455 Test MSE 384.20192282549675 Test RE 0.9991768184226525\n",
      "117 Train Loss 1.2275449 Test MSE 384.18687344869437 Test RE 0.9991572491092348\n",
      "118 Train Loss 1.2277026 Test MSE 384.18413254362184 Test RE 0.9991536849582368\n",
      "119 Train Loss 1.2275021 Test MSE 384.0113985541746 Test RE 0.9989290437104298\n",
      "120 Train Loss 1.2274826 Test MSE 383.91749417631735 Test RE 0.998806899490212\n",
      "121 Train Loss 1.2276064 Test MSE 383.8923090431025 Test RE 0.9987741378972128\n",
      "122 Train Loss 1.2277446 Test MSE 383.8778010335077 Test RE 0.9987552649459449\n",
      "123 Train Loss 1.2279043 Test MSE 383.8707564935048 Test RE 0.9987461008269509\n",
      "124 Train Loss 1.2280908 Test MSE 383.86242830215764 Test RE 0.9987352667198867\n",
      "125 Train Loss 1.2282096 Test MSE 383.8558838334251 Test RE 0.998726752967169\n",
      "126 Train Loss 1.2283343 Test MSE 383.8520783909447 Test RE 0.9987218024026582\n",
      "127 Train Loss 1.2284948 Test MSE 383.8445983069649 Test RE 0.9987120713633206\n",
      "128 Train Loss 1.228369 Test MSE 383.80828954617704 Test RE 0.9986648349876851\n",
      "129 Train Loss 1.228344 Test MSE 383.83185118900104 Test RE 0.9986954880815222\n",
      "130 Train Loss 1.2284869 Test MSE 383.83258084188515 Test RE 0.998696437326217\n",
      "131 Train Loss 1.2286216 Test MSE 383.833900980648 Test RE 0.9986981547634691\n",
      "132 Train Loss 1.228757 Test MSE 383.8315909390453 Test RE 0.9986951495081343\n",
      "133 Train Loss 1.2284374 Test MSE 383.7245923002268 Test RE 0.9985559394062569\n",
      "134 Train Loss 1.2280108 Test MSE 383.43379619496534 Test RE 0.9981775023470575\n",
      "135 Train Loss 1.2277824 Test MSE 383.2438390620328 Test RE 0.9979302179031495\n",
      "136 Train Loss 1.2279227 Test MSE 383.22773855807174 Test RE 0.997909255598551\n",
      "137 Train Loss 1.2280109 Test MSE 383.22773855807174 Test RE 0.997909255598551\n",
      "138 Train Loss 1.2281746 Test MSE 383.2236876073303 Test RE 0.9979039813301266\n",
      "139 Train Loss 1.2283019 Test MSE 383.2236876073303 Test RE 0.9979039813301266\n",
      "140 Train Loss 1.2284412 Test MSE 383.2236876073303 Test RE 0.9979039813301266\n",
      "141 Train Loss 1.2285461 Test MSE 383.2221110626377 Test RE 0.9979019286884259\n",
      "142 Train Loss 1.2286292 Test MSE 383.2221110626377 Test RE 0.9979019286884259\n",
      "143 Train Loss 1.2287383 Test MSE 383.2221110626377 Test RE 0.9979019286884259\n",
      "144 Train Loss 1.2287799 Test MSE 383.1959628007164 Test RE 0.9978678833588762\n",
      "145 Train Loss 1.2289518 Test MSE 383.1959628007164 Test RE 0.9978678833588762\n",
      "146 Train Loss 1.2290757 Test MSE 383.2225383727966 Test RE 0.9979024850413876\n",
      "147 Train Loss 1.2291788 Test MSE 383.2195522051055 Test RE 0.997898597078564\n",
      "148 Train Loss 1.2292899 Test MSE 383.2085045734334 Test RE 0.9978842130322143\n",
      "149 Train Loss 1.2293705 Test MSE 383.20317413463346 Test RE 0.9978772727123086\n",
      "150 Train Loss 1.2294667 Test MSE 383.1787265738637 Test RE 0.9978454409634836\n",
      "151 Train Loss 1.2295402 Test MSE 383.1594152818904 Test RE 0.9978202961340468\n",
      "152 Train Loss 1.2296066 Test MSE 383.1534815573198 Test RE 0.9978125698281085\n",
      "153 Train Loss 1.229567 Test MSE 383.10328127361254 Test RE 0.9977472016175765\n",
      "154 Train Loss 1.2297378 Test MSE 383.102234174837 Test RE 0.9977458380941637\n",
      "155 Train Loss 1.2298397 Test MSE 383.09330043123924 Test RE 0.9977342045709628\n",
      "156 Train Loss 1.2298288 Test MSE 383.09330043123924 Test RE 0.9977342045709628\n",
      "157 Train Loss 1.229947 Test MSE 383.09511130102584 Test RE 0.9977365626970239\n",
      "158 Train Loss 1.2300402 Test MSE 383.0941961314407 Test RE 0.9977353709581129\n",
      "159 Train Loss 1.2301003 Test MSE 383.0941961314407 Test RE 0.9977353709581129\n",
      "160 Train Loss 1.2302147 Test MSE 383.0941961314407 Test RE 0.9977353709581129\n",
      "161 Train Loss 1.2303255 Test MSE 383.0941961314407 Test RE 0.9977353709581129\n",
      "162 Train Loss 1.2303991 Test MSE 383.09038228460673 Test RE 0.9977304045302803\n",
      "163 Train Loss 1.2304767 Test MSE 383.09038228460673 Test RE 0.9977304045302803\n",
      "164 Train Loss 1.2305605 Test MSE 383.09038228460673 Test RE 0.9977304045302803\n",
      "165 Train Loss 1.2306334 Test MSE 383.09683085893425 Test RE 0.9977388019110218\n",
      "166 Train Loss 1.2306701 Test MSE 383.09683085893425 Test RE 0.9977388019110218\n",
      "167 Train Loss 1.230758 Test MSE 383.09630622555943 Test RE 0.9977381187322382\n",
      "168 Train Loss 1.2307746 Test MSE 383.09630622555943 Test RE 0.9977381187322382\n",
      "169 Train Loss 1.2309109 Test MSE 383.09846659659274 Test RE 0.9977409319693493\n",
      "170 Train Loss 1.230979 Test MSE 383.09846659659274 Test RE 0.9977409319693493\n",
      "171 Train Loss 1.2310565 Test MSE 383.10633078341033 Test RE 0.9977511726535359\n",
      "172 Train Loss 1.2311056 Test MSE 383.10633078341033 Test RE 0.9977511726535359\n",
      "173 Train Loss 1.2311808 Test MSE 383.10633078341033 Test RE 0.9977511726535359\n",
      "174 Train Loss 1.2312887 Test MSE 383.10633078341033 Test RE 0.9977511726535359\n",
      "175 Train Loss 1.2312845 Test MSE 383.10399248729385 Test RE 0.9977481277531194\n",
      "176 Train Loss 1.231383 Test MSE 383.10399248729385 Test RE 0.9977481277531194\n",
      "177 Train Loss 1.2315049 Test MSE 383.10399248729385 Test RE 0.9977481277531194\n",
      "178 Train Loss 1.2315255 Test MSE 383.09901091821405 Test RE 0.9977416407842051\n",
      "179 Train Loss 1.2315934 Test MSE 383.09901091821405 Test RE 0.9977416407842051\n",
      "180 Train Loss 1.2316455 Test MSE 383.09901091821405 Test RE 0.9977416407842051\n",
      "181 Train Loss 1.2317281 Test MSE 383.0996263190323 Test RE 0.9977424421576857\n",
      "182 Train Loss 1.2317549 Test MSE 383.10086234450483 Test RE 0.9977440517051691\n",
      "183 Train Loss 1.2318237 Test MSE 383.1019580976222 Test RE 0.9977454785883749\n",
      "184 Train Loss 1.2319026 Test MSE 383.1028884311143 Test RE 0.9977466900615247\n",
      "185 Train Loss 1.2319585 Test MSE 383.1084371868916 Test RE 0.9977539155782615\n",
      "186 Train Loss 1.2319748 Test MSE 383.1092638394527 Test RE 0.9977549920298673\n",
      "187 Train Loss 1.2320352 Test MSE 383.1092638394527 Test RE 0.9977549920298673\n",
      "188 Train Loss 1.2320676 Test MSE 383.1092638394527 Test RE 0.9977549920298673\n",
      "189 Train Loss 1.2321018 Test MSE 383.1086152018927 Test RE 0.9977541473861864\n",
      "190 Train Loss 1.2322067 Test MSE 383.1086152018927 Test RE 0.9977541473861864\n",
      "191 Train Loss 1.2322164 Test MSE 383.0846618464698 Test RE 0.9977229552746649\n",
      "192 Train Loss 1.2321855 Test MSE 382.9912065274493 Test RE 0.9976012482294621\n",
      "193 Train Loss 1.2322496 Test MSE 382.9852479397414 Test RE 0.9975934878454711\n",
      "194 Train Loss 1.2322584 Test MSE 382.96741650557897 Test RE 0.9975702640662862\n",
      "195 Train Loss 1.2321643 Test MSE 382.9090733374693 Test RE 0.997494273754903\n",
      "196 Train Loss 1.2321683 Test MSE 382.857419551543 Test RE 0.9974269913411116\n",
      "197 Train Loss 1.2322108 Test MSE 382.853701534601 Test RE 0.9974221482072629\n",
      "198 Train Loss 1.232188 Test MSE 382.8453419314413 Test RE 0.9974112588011738\n",
      "199 Train Loss 1.2322559 Test MSE 382.8441080529612 Test RE 0.9974096515133933\n",
      "Training time: 94.34\n",
      "Training time: 94.34\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (m_lambda): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "8\n",
      "0 Train Loss 74.63312 Test MSE 387.25232720756316 Test RE 1.0031355017467478\n",
      "1 Train Loss 4.283011 Test MSE 385.9020164586379 Test RE 1.0013850571276932\n",
      "2 Train Loss 2.00584 Test MSE 385.3395366029844 Test RE 1.0006549957447015\n",
      "3 Train Loss 1.9724709 Test MSE 385.1106326675666 Test RE 1.0003577411533768\n",
      "4 Train Loss 2.0038893 Test MSE 384.93759633305535 Test RE 1.0001329775784955\n",
      "5 Train Loss 2.0134468 Test MSE 384.85058132915873 Test RE 1.0000199313413811\n",
      "6 Train Loss 2.040343 Test MSE 384.9280841990969 Test RE 1.0001206204357773\n",
      "7 Train Loss 2.057019 Test MSE 385.3420128066443 Test RE 1.0006582108593558\n",
      "8 Train Loss 2.0630298 Test MSE 385.40165645237846 Test RE 1.0007356493329445\n",
      "9 Train Loss 2.0776284 Test MSE 385.31774590110996 Test RE 1.000626702147895\n",
      "10 Train Loss 2.0727563 Test MSE 385.4260794137233 Test RE 1.000767357215457\n",
      "11 Train Loss 2.0667539 Test MSE 384.9566406509027 Test RE 1.0001577174518175\n",
      "12 Train Loss 2.0217242 Test MSE 385.3044398433624 Test RE 1.0006094248335597\n",
      "13 Train Loss 1.9245981 Test MSE 384.89951448262013 Test RE 1.000083504810882\n",
      "14 Train Loss 1.9005857 Test MSE 384.6344870947984 Test RE 0.99973913550095\n",
      "15 Train Loss 1.4475347 Test MSE 385.0542055375523 Test RE 1.00028445132524\n",
      "16 Train Loss 1.2406987 Test MSE 385.7480727379537 Test RE 1.0011853013450538\n",
      "17 Train Loss 1.1763328 Test MSE 385.82371298279736 Test RE 1.0012834563257624\n",
      "18 Train Loss 1.1501868 Test MSE 385.60996345237885 Test RE 1.0010060582390647\n",
      "19 Train Loss 1.1468525 Test MSE 385.4641961374205 Test RE 1.0008168414509722\n",
      "20 Train Loss 1.1476449 Test MSE 385.35506199788927 Test RE 1.000675153820914\n",
      "21 Train Loss 1.1499231 Test MSE 385.2301533006799 Test RE 1.0005129616343447\n",
      "22 Train Loss 1.1516727 Test MSE 384.8506340279574 Test RE 1.0000199998093138\n",
      "23 Train Loss 1.1537629 Test MSE 384.66390852865567 Test RE 0.9997773707572508\n",
      "24 Train Loss 1.1559068 Test MSE 384.4465603134819 Test RE 0.9994948767067131\n",
      "25 Train Loss 1.1567585 Test MSE 384.1933654084239 Test RE 0.9991656909140845\n",
      "26 Train Loss 1.158544 Test MSE 384.1456749400411 Test RE 0.999103675060987\n",
      "27 Train Loss 1.1609042 Test MSE 384.1521586410735 Test RE 0.9991121065787922\n",
      "28 Train Loss 1.1631628 Test MSE 384.1557064765991 Test RE 0.9991167202158129\n",
      "29 Train Loss 1.1654053 Test MSE 384.15716067415383 Test RE 0.9991186112611523\n",
      "30 Train Loss 1.1675507 Test MSE 384.1582411673332 Test RE 0.9991200163372466\n",
      "31 Train Loss 1.1695846 Test MSE 384.16049925850234 Test RE 0.9991229527583989\n",
      "32 Train Loss 1.1716092 Test MSE 384.16347042812174 Test RE 0.9991268164534287\n",
      "33 Train Loss 1.173481 Test MSE 384.16347042812174 Test RE 0.9991268164534287\n",
      "34 Train Loss 1.1753376 Test MSE 384.16347042812174 Test RE 0.9991268164534287\n",
      "35 Train Loss 1.1771468 Test MSE 384.1646372616948 Test RE 0.9991283337942454\n",
      "36 Train Loss 1.1788304 Test MSE 384.1646372616948 Test RE 0.9991283337942454\n",
      "37 Train Loss 1.1804652 Test MSE 384.17268018091426 Test RE 0.9991387926778067\n",
      "38 Train Loss 1.1820097 Test MSE 384.1828358744778 Test RE 0.9991519988229478\n",
      "39 Train Loss 1.1835475 Test MSE 384.1847383306078 Test RE 0.999154472697773\n",
      "40 Train Loss 1.1849823 Test MSE 384.1879013563713 Test RE 0.9991585857513624\n",
      "41 Train Loss 1.1864146 Test MSE 384.1910297682948 Test RE 0.9991626537780479\n",
      "42 Train Loss 1.1877848 Test MSE 384.19289780853 Test RE 0.9991650828740253\n",
      "43 Train Loss 1.1890919 Test MSE 384.1948502112965 Test RE 0.9991676216640508\n",
      "44 Train Loss 1.1903416 Test MSE 384.2031827113444 Test RE 0.9991784566857975\n",
      "45 Train Loss 1.1915708 Test MSE 384.20474054374483 Test RE 0.9991804823730719\n",
      "46 Train Loss 1.1927403 Test MSE 384.2153768272033 Test RE 0.9991943128808235\n",
      "47 Train Loss 1.193884 Test MSE 384.2153768272033 Test RE 0.9991943128808235\n",
      "48 Train Loss 1.1949788 Test MSE 384.2153768272033 Test RE 0.9991943128808235\n",
      "49 Train Loss 1.196058 Test MSE 384.2153768272033 Test RE 0.9991943128808235\n",
      "50 Train Loss 1.1970718 Test MSE 384.21490088233315 Test RE 0.9991936940071611\n",
      "51 Train Loss 1.1980891 Test MSE 384.21490088233315 Test RE 0.9991936940071611\n",
      "52 Train Loss 1.1990274 Test MSE 384.2135531296062 Test RE 0.9991919415149965\n",
      "53 Train Loss 1.1999546 Test MSE 384.2135531296062 Test RE 0.9991919415149965\n",
      "54 Train Loss 1.2008578 Test MSE 384.21233414014785 Test RE 0.9991903564519503\n",
      "55 Train Loss 1.201738 Test MSE 384.21233414014785 Test RE 0.9991903564519503\n",
      "56 Train Loss 1.2025472 Test MSE 384.21194905274564 Test RE 0.9991898557187868\n",
      "57 Train Loss 1.2033705 Test MSE 384.21104333367373 Test RE 0.9991886780018464\n",
      "58 Train Loss 1.2041405 Test MSE 384.21000864262555 Test RE 0.9991873325794615\n",
      "59 Train Loss 1.204916 Test MSE 384.2092245084685 Test RE 0.9991863129582921\n",
      "60 Train Loss 1.2056382 Test MSE 384.2075855476152 Test RE 0.9991841817900459\n",
      "61 Train Loss 1.2063605 Test MSE 384.2052468549646 Test RE 0.9991811407409533\n",
      "62 Train Loss 1.2070712 Test MSE 384.20329617930247 Test RE 0.9991786042310757\n",
      "63 Train Loss 1.2077374 Test MSE 384.1845792476295 Test RE 0.999154265833119\n",
      "64 Train Loss 1.2083886 Test MSE 384.18380850709696 Test RE 0.9991532635947175\n",
      "65 Train Loss 1.2090259 Test MSE 384.16796201442594 Test RE 0.9991326572628147\n",
      "66 Train Loss 1.2095767 Test MSE 384.167991183831 Test RE 0.9991326951942766\n",
      "67 Train Loss 1.210177 Test MSE 384.1711660717235 Test RE 0.9991368237627883\n",
      "68 Train Loss 1.2107772 Test MSE 384.1711660717235 Test RE 0.9991368237627883\n",
      "69 Train Loss 1.2113416 Test MSE 384.1711660717235 Test RE 0.9991368237627883\n",
      "70 Train Loss 1.2118781 Test MSE 384.17202380223785 Test RE 0.999137939137573\n",
      "71 Train Loss 1.2123983 Test MSE 384.17332718079206 Test RE 0.9991396340213453\n",
      "72 Train Loss 1.212922 Test MSE 384.17332718079206 Test RE 0.9991396340213453\n",
      "73 Train Loss 1.2134007 Test MSE 384.17332718079206 Test RE 0.9991396340213453\n",
      "74 Train Loss 1.2139008 Test MSE 384.17332718079206 Test RE 0.9991396340213453\n",
      "75 Train Loss 1.2143625 Test MSE 384.17162328739073 Test RE 0.9991374183166146\n",
      "76 Train Loss 1.2148272 Test MSE 384.1690598431516 Test RE 0.999134084862012\n",
      "77 Train Loss 1.2152739 Test MSE 384.1680840554543 Test RE 0.9991328159631485\n",
      "78 Train Loss 1.215747 Test MSE 384.1663778011747 Test RE 0.99913059717324\n",
      "79 Train Loss 1.2161728 Test MSE 384.16378510431065 Test RE 0.9991272256559932\n",
      "80 Train Loss 1.2165178 Test MSE 384.1588368597041 Test RE 0.9991207909762843\n",
      "81 Train Loss 1.2169237 Test MSE 384.15880411361525 Test RE 0.9991207483932489\n",
      "82 Train Loss 1.2172502 Test MSE 384.1564338964588 Test RE 0.9991176661564669\n",
      "83 Train Loss 1.2176504 Test MSE 384.15531917282254 Test RE 0.999116216563557\n",
      "84 Train Loss 1.2179872 Test MSE 384.15367385459353 Test RE 0.9991140769785998\n",
      "85 Train Loss 1.2183832 Test MSE 384.1528191068161 Test RE 0.9991129654559595\n",
      "86 Train Loss 1.2187097 Test MSE 384.1528191068161 Test RE 0.9991129654559595\n",
      "87 Train Loss 1.218993 Test MSE 384.14573291606786 Test RE 0.999103750454338\n",
      "88 Train Loss 1.2193246 Test MSE 384.14903470235555 Test RE 0.9991080441633292\n",
      "89 Train Loss 1.219684 Test MSE 384.1496652847633 Test RE 0.9991088641832184\n",
      "90 Train Loss 1.220022 Test MSE 384.1508287442352 Test RE 0.9991103771635701\n",
      "91 Train Loss 1.2203156 Test MSE 384.1491895158477 Test RE 0.999108245485441\n",
      "92 Train Loss 1.2205607 Test MSE 384.13848592201646 Test RE 0.9990943262534885\n",
      "93 Train Loss 1.2208303 Test MSE 384.13158556681367 Test RE 0.9990853527492117\n",
      "94 Train Loss 1.2211044 Test MSE 384.12618244690157 Test RE 0.9990783262547126\n",
      "95 Train Loss 1.2214099 Test MSE 384.11277858509413 Test RE 0.9990608949713787\n",
      "96 Train Loss 1.2216835 Test MSE 384.1022103877103 Test RE 0.9990471511623344\n",
      "97 Train Loss 1.2219101 Test MSE 384.09868875097607 Test RE 0.9990425712756325\n",
      "98 Train Loss 1.2221657 Test MSE 384.09405790378827 Test RE 0.9990365488293684\n",
      "99 Train Loss 1.2224393 Test MSE 384.09305345806627 Test RE 0.9990352425365179\n",
      "100 Train Loss 1.2226202 Test MSE 384.0907678848934 Test RE 0.9990322701167808\n",
      "101 Train Loss 1.2227197 Test MSE 384.0581095597716 Test RE 0.9989897965441445\n",
      "102 Train Loss 1.2228526 Test MSE 384.00869936181715 Test RE 0.9989255329990437\n",
      "103 Train Loss 1.223072 Test MSE 384.00593488261745 Test RE 0.9989219373593713\n",
      "104 Train Loss 1.2232664 Test MSE 384.00052369126314 Test RE 0.9989148992171155\n",
      "105 Train Loss 1.2234923 Test MSE 383.99897903064544 Test RE 0.9989128901234271\n",
      "106 Train Loss 1.2237023 Test MSE 383.997570669445 Test RE 0.9989110583067768\n",
      "107 Train Loss 1.2238866 Test MSE 383.9945649951132 Test RE 0.9989071488976655\n",
      "108 Train Loss 1.2241476 Test MSE 383.9945649951132 Test RE 0.9989071488976655\n",
      "109 Train Loss 1.224324 Test MSE 383.98896129647125 Test RE 0.9988998602592761\n",
      "110 Train Loss 1.2245227 Test MSE 383.98896129647125 Test RE 0.9988998602592761\n",
      "111 Train Loss 1.2247314 Test MSE 383.98896129647125 Test RE 0.9988998602592761\n",
      "112 Train Loss 1.2249254 Test MSE 383.99011997838164 Test RE 0.9989013673420859\n",
      "113 Train Loss 1.225127 Test MSE 383.99011997838164 Test RE 0.9989013673420859\n",
      "114 Train Loss 1.2253171 Test MSE 383.9900258136253 Test RE 0.9989012448632708\n",
      "115 Train Loss 1.225483 Test MSE 383.9900244953766 Test RE 0.9989012431486426\n",
      "116 Train Loss 1.2256454 Test MSE 383.9900244953766 Test RE 0.9989012431486426\n",
      "117 Train Loss 1.2258345 Test MSE 383.9857618113116 Test RE 0.9988956987178539\n",
      "118 Train Loss 1.2259798 Test MSE 383.9857618113116 Test RE 0.9988956987178539\n",
      "119 Train Loss 1.226147 Test MSE 383.9885214009833 Test RE 0.9988992880922198\n",
      "120 Train Loss 1.2263224 Test MSE 383.9885214009833 Test RE 0.9988992880922198\n",
      "121 Train Loss 1.2264754 Test MSE 383.9885214009833 Test RE 0.9988992880922198\n",
      "122 Train Loss 1.2266222 Test MSE 383.9885214009833 Test RE 0.9988992880922198\n",
      "123 Train Loss 1.226788 Test MSE 383.9885214009833 Test RE 0.9988992880922198\n",
      "124 Train Loss 1.2269478 Test MSE 383.9885214009833 Test RE 0.9988992880922198\n",
      "125 Train Loss 1.2271166 Test MSE 383.9885214009833 Test RE 0.9988992880922198\n",
      "126 Train Loss 1.2272393 Test MSE 383.9885214009833 Test RE 0.9988992880922198\n",
      "127 Train Loss 1.2274212 Test MSE 383.99271000437614 Test RE 0.9989047361477009\n",
      "128 Train Loss 1.227538 Test MSE 383.99271000437614 Test RE 0.9989047361477009\n",
      "129 Train Loss 1.2276782 Test MSE 383.99271000437614 Test RE 0.9989047361477009\n",
      "130 Train Loss 1.2278042 Test MSE 383.99271000437614 Test RE 0.9989047361477009\n",
      "131 Train Loss 1.2279555 Test MSE 383.99271000437614 Test RE 0.9989047361477009\n",
      "132 Train Loss 1.2280862 Test MSE 383.99271000437614 Test RE 0.9989047361477009\n",
      "133 Train Loss 1.228129 Test MSE 383.99271000437614 Test RE 0.9989047361477009\n",
      "134 Train Loss 1.2282846 Test MSE 383.99221856779616 Test RE 0.9989040969448316\n",
      "135 Train Loss 1.2284777 Test MSE 383.99221856779616 Test RE 0.9989040969448316\n",
      "136 Train Loss 1.2285541 Test MSE 383.99221856779616 Test RE 0.9989040969448316\n",
      "137 Train Loss 1.228647 Test MSE 383.99221856779616 Test RE 0.9989040969448316\n",
      "138 Train Loss 1.2288008 Test MSE 383.99221856779616 Test RE 0.9989040969448316\n",
      "139 Train Loss 1.2288849 Test MSE 383.99221856779616 Test RE 0.9989040969448316\n",
      "140 Train Loss 1.2290182 Test MSE 383.9923924949646 Test RE 0.9989043231688707\n",
      "141 Train Loss 1.2291023 Test MSE 383.9923924949646 Test RE 0.9989043231688707\n",
      "142 Train Loss 1.2292651 Test MSE 383.9923924949646 Test RE 0.9989043231688707\n",
      "143 Train Loss 1.2293811 Test MSE 383.9923924949646 Test RE 0.9989043231688707\n",
      "144 Train Loss 1.2294272 Test MSE 383.9923924949646 Test RE 0.9989043231688707\n",
      "145 Train Loss 1.229579 Test MSE 383.98916596075446 Test RE 0.9989001264636694\n",
      "146 Train Loss 1.2296559 Test MSE 383.98916596075446 Test RE 0.9989001264636694\n",
      "147 Train Loss 1.2297978 Test MSE 383.98916596075446 Test RE 0.9989001264636694\n",
      "148 Train Loss 1.2298402 Test MSE 383.98916596075446 Test RE 0.9989001264636694\n",
      "149 Train Loss 1.2299623 Test MSE 383.9897102671205 Test RE 0.9989008344361243\n",
      "150 Train Loss 1.2300693 Test MSE 383.9897102671205 Test RE 0.9989008344361243\n",
      "151 Train Loss 1.230192 Test MSE 383.98806554478296 Test RE 0.9988986951649008\n",
      "152 Train Loss 1.2302591 Test MSE 383.98806554478296 Test RE 0.9988986951649008\n",
      "153 Train Loss 1.2302862 Test MSE 383.98806554478296 Test RE 0.9988986951649008\n",
      "154 Train Loss 1.2303811 Test MSE 383.98806554478296 Test RE 0.9988986951649008\n",
      "155 Train Loss 1.2305175 Test MSE 383.98806554478296 Test RE 0.9988986951649008\n",
      "156 Train Loss 1.230548 Test MSE 383.98510294171973 Test RE 0.9988948417299797\n",
      "157 Train Loss 1.2305932 Test MSE 383.98510294171973 Test RE 0.9988948417299797\n",
      "158 Train Loss 1.2307014 Test MSE 383.98510294171973 Test RE 0.9988948417299797\n",
      "159 Train Loss 1.2308705 Test MSE 383.98510294171973 Test RE 0.9988948417299797\n",
      "160 Train Loss 1.2308795 Test MSE 383.98508479215934 Test RE 0.9988948181229406\n",
      "161 Train Loss 1.2310101 Test MSE 383.98508479215934 Test RE 0.9988948181229406\n",
      "162 Train Loss 1.2311305 Test MSE 383.98508479215934 Test RE 0.9988948181229406\n",
      "163 Train Loss 1.2311394 Test MSE 383.98508479215934 Test RE 0.9988948181229406\n",
      "164 Train Loss 1.2312168 Test MSE 383.98508479215934 Test RE 0.9988948181229406\n",
      "165 Train Loss 1.2313142 Test MSE 383.98508479215934 Test RE 0.9988948181229406\n",
      "166 Train Loss 1.2313417 Test MSE 383.98508479215934 Test RE 0.9988948181229406\n",
      "167 Train Loss 1.2314284 Test MSE 383.98508479215934 Test RE 0.9988948181229406\n",
      "168 Train Loss 1.2315111 Test MSE 383.98508479215934 Test RE 0.9988948181229406\n",
      "169 Train Loss 1.231604 Test MSE 383.98508479215934 Test RE 0.9988948181229406\n",
      "170 Train Loss 1.2316773 Test MSE 383.98507647336766 Test RE 0.9988948073027316\n",
      "171 Train Loss 1.2317097 Test MSE 383.98507647336766 Test RE 0.9988948073027316\n",
      "172 Train Loss 1.231768 Test MSE 383.98507647336766 Test RE 0.9988948073027316\n",
      "173 Train Loss 1.2318485 Test MSE 383.98507647336766 Test RE 0.9988948073027316\n",
      "174 Train Loss 1.2318685 Test MSE 383.98507654461866 Test RE 0.9988948073954074\n",
      "175 Train Loss 1.2319607 Test MSE 383.98507654461866 Test RE 0.9988948073954074\n",
      "176 Train Loss 1.2320592 Test MSE 383.98507654461866 Test RE 0.9988948073954074\n",
      "177 Train Loss 1.232164 Test MSE 383.98507654461866 Test RE 0.9988948073954074\n",
      "178 Train Loss 1.2321963 Test MSE 383.98507654461866 Test RE 0.9988948073954074\n",
      "179 Train Loss 1.232248 Test MSE 383.98507654461866 Test RE 0.9988948073954074\n",
      "180 Train Loss 1.2323219 Test MSE 383.98507654461866 Test RE 0.9988948073954074\n",
      "181 Train Loss 1.2323763 Test MSE 383.98507654461866 Test RE 0.9988948073954074\n",
      "182 Train Loss 1.2323304 Test MSE 383.98507654461866 Test RE 0.9988948073954074\n",
      "183 Train Loss 1.2324942 Test MSE 383.98507654461866 Test RE 0.9988948073954074\n",
      "184 Train Loss 1.2325195 Test MSE 383.98507654461866 Test RE 0.9988948073954074\n",
      "185 Train Loss 1.2325534 Test MSE 383.98507654461866 Test RE 0.9988948073954074\n",
      "186 Train Loss 1.2326382 Test MSE 383.98507654461866 Test RE 0.9988948073954074\n",
      "187 Train Loss 1.2326901 Test MSE 383.98507654461866 Test RE 0.9988948073954074\n",
      "188 Train Loss 1.2326941 Test MSE 383.98507654461866 Test RE 0.9988948073954074\n",
      "189 Train Loss 1.2327132 Test MSE 383.98507654461866 Test RE 0.9988948073954074\n",
      "190 Train Loss 1.2328641 Test MSE 383.98507654461866 Test RE 0.9988948073954074\n",
      "191 Train Loss 1.2329278 Test MSE 383.98507654461866 Test RE 0.9988948073954074\n",
      "192 Train Loss 1.23295 Test MSE 383.98507654461866 Test RE 0.9988948073954074\n",
      "193 Train Loss 1.2329534 Test MSE 383.98507654461866 Test RE 0.9988948073954074\n",
      "194 Train Loss 1.2330055 Test MSE 383.98507654461866 Test RE 0.9988948073954074\n",
      "195 Train Loss 1.233068 Test MSE 383.98507654461866 Test RE 0.9988948073954074\n",
      "196 Train Loss 1.2331194 Test MSE 383.98507654461866 Test RE 0.9988948073954074\n",
      "197 Train Loss 1.2332042 Test MSE 383.98507654461866 Test RE 0.9988948073954074\n",
      "198 Train Loss 1.2332544 Test MSE 383.98507654461866 Test RE 0.9988948073954074\n",
      "199 Train Loss 1.2333484 Test MSE 383.98507654461866 Test RE 0.9988948073954074\n",
      "Training time: 91.26\n",
      "Training time: 91.26\n",
      "Sequentialmodel(\n",
      "  (activation): Tanh()\n",
      "  (m_lambda): Sigmoid()\n",
      "  (loss_function): MSELoss()\n",
      "  (linears): ModuleList(\n",
      "    (0): Linear(in_features=1, out_features=50, bias=True)\n",
      "    (1): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (2): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (3): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (4): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (5): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (6): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (7): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (8): Linear(in_features=50, out_features=50, bias=True)\n",
      "    (9): Linear(in_features=50, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "9\n",
      "0 Train Loss 3.5672512 Test MSE 384.061095852783 Test RE 0.9989936804231387\n",
      "1 Train Loss 1.972203 Test MSE 384.6346027563985 Test RE 0.9997392858143298\n",
      "2 Train Loss 1.8997234 Test MSE 384.96110680639214 Test RE 1.0001635192051226\n",
      "3 Train Loss 1.9293673 Test MSE 385.331116371029 Test RE 1.0006440627980397\n",
      "4 Train Loss 1.9349331 Test MSE 385.21634232250443 Test RE 1.0004950266590968\n",
      "5 Train Loss 1.9548327 Test MSE 385.2615452475631 Test RE 1.000553726109365\n",
      "6 Train Loss 1.9681551 Test MSE 385.6695967238099 Test RE 1.0010834563407356\n",
      "7 Train Loss 1.9883598 Test MSE 385.61557119412953 Test RE 1.0010133367893999\n",
      "8 Train Loss 1.9957439 Test MSE 385.3832147288201 Test RE 1.0007117061159514\n",
      "9 Train Loss 1.9837918 Test MSE 385.8301743761086 Test RE 1.0012918405423399\n",
      "10 Train Loss 1.9998118 Test MSE 386.0202981672908 Test RE 1.0015385111857393\n",
      "11 Train Loss 1.990105 Test MSE 386.13561567892896 Test RE 1.0016880969784834\n",
      "12 Train Loss 1.9430152 Test MSE 386.1714721678378 Test RE 1.0017346041937047\n",
      "13 Train Loss 1.9450322 Test MSE 386.26762740224177 Test RE 1.0018593104931208\n",
      "14 Train Loss 1.9388472 Test MSE 386.0080220162943 Test RE 1.0015225856815586\n",
      "15 Train Loss 1.876812 Test MSE 384.8132192747013 Test RE 0.9999713882082748\n",
      "16 Train Loss 1.355227 Test MSE 383.75378856196915 Test RE 0.9985939269976425\n",
      "17 Train Loss 1.1643105 Test MSE 383.86182170448876 Test RE 0.9987344775950865\n",
      "18 Train Loss 1.1349318 Test MSE 383.9309646490091 Test RE 0.998824421852766\n",
      "19 Train Loss 1.1363589 Test MSE 384.07897337254195 Test RE 0.9990169310486571\n",
      "20 Train Loss 1.1393625 Test MSE 384.1026019837946 Test RE 0.9990476604315369\n",
      "21 Train Loss 1.1423926 Test MSE 384.1047598490184 Test RE 0.99905046672231\n",
      "22 Train Loss 1.1452216 Test MSE 384.03425174270075 Test RE 0.9989587672735999\n",
      "23 Train Loss 1.1481202 Test MSE 384.0247080439593 Test RE 0.9989463545516634\n",
      "24 Train Loss 1.1507896 Test MSE 384.022262529329 Test RE 0.998943173842502\n",
      "25 Train Loss 1.1534859 Test MSE 384.01912673033354 Test RE 0.9989390953141178\n",
      "26 Train Loss 1.1555859 Test MSE 383.99128751732184 Test RE 0.9989028859428213\n",
      "27 Train Loss 1.1578887 Test MSE 384.00239111810004 Test RE 0.9989173281177388\n",
      "28 Train Loss 1.160252 Test MSE 384.005400711279 Test RE 0.9989212425841016\n",
      "29 Train Loss 1.1625762 Test MSE 384.0066955943597 Test RE 0.9989229267858384\n",
      "30 Train Loss 1.1647981 Test MSE 384.0079528011341 Test RE 0.9989245619808197\n",
      "31 Train Loss 1.1669204 Test MSE 384.0079528011341 Test RE 0.9989245619808197\n",
      "32 Train Loss 1.1690254 Test MSE 384.0079528011341 Test RE 0.9989245619808197\n",
      "33 Train Loss 1.1709504 Test MSE 384.0098234088601 Test RE 0.9989269949951803\n",
      "34 Train Loss 1.1728889 Test MSE 384.0098234088601 Test RE 0.9989269949951803\n",
      "35 Train Loss 1.1747258 Test MSE 384.0149804508435 Test RE 0.9989337024943667\n",
      "36 Train Loss 1.1764811 Test MSE 384.0149804508435 Test RE 0.9989337024943667\n",
      "37 Train Loss 1.1781816 Test MSE 384.0149804508435 Test RE 0.9989337024943667\n",
      "38 Train Loss 1.1798131 Test MSE 384.0112690633224 Test RE 0.9989288752881063\n",
      "39 Train Loss 1.1814082 Test MSE 384.0112690633224 Test RE 0.9989288752881063\n",
      "40 Train Loss 1.1829015 Test MSE 384.0112690633224 Test RE 0.9989288752881063\n",
      "41 Train Loss 1.1844399 Test MSE 384.0112690633224 Test RE 0.9989288752881063\n",
      "42 Train Loss 1.18583 Test MSE 384.0072344888322 Test RE 0.9989236277031153\n",
      "43 Train Loss 1.1871684 Test MSE 384.0072344888322 Test RE 0.9989236277031153\n",
      "44 Train Loss 1.18853 Test MSE 384.0091547411029 Test RE 0.9989261252901321\n",
      "45 Train Loss 1.1898005 Test MSE 384.0091547411029 Test RE 0.9989261252901321\n",
      "46 Train Loss 1.1910414 Test MSE 384.0091547411029 Test RE 0.9989261252901321\n",
      "47 Train Loss 1.1922132 Test MSE 384.0091547411029 Test RE 0.9989261252901321\n",
      "48 Train Loss 1.19336 Test MSE 384.0091547411029 Test RE 0.9989261252901321\n",
      "49 Train Loss 1.1944654 Test MSE 384.0091547411029 Test RE 0.9989261252901321\n",
      "50 Train Loss 1.1955202 Test MSE 384.0091547411029 Test RE 0.9989261252901321\n",
      "51 Train Loss 1.1965848 Test MSE 384.0091547411029 Test RE 0.9989261252901321\n",
      "52 Train Loss 1.1975354 Test MSE 384.0089943545372 Test RE 0.9989259166826739\n",
      "53 Train Loss 1.198533 Test MSE 384.0089943545372 Test RE 0.9989259166826739\n",
      "54 Train Loss 1.1994442 Test MSE 384.0089943545372 Test RE 0.9989259166826739\n",
      "55 Train Loss 1.2003586 Test MSE 384.00831073249816 Test RE 0.9989250275263183\n",
      "56 Train Loss 1.2012016 Test MSE 384.00831073249816 Test RE 0.9989250275263183\n",
      "57 Train Loss 1.2020812 Test MSE 384.00831073249816 Test RE 0.9989250275263183\n",
      "58 Train Loss 1.2028937 Test MSE 384.00831073249816 Test RE 0.9989250275263183\n",
      "59 Train Loss 1.203673 Test MSE 384.00831073249816 Test RE 0.9989250275263183\n",
      "60 Train Loss 1.2044567 Test MSE 384.00831073249816 Test RE 0.9989250275263183\n",
      "61 Train Loss 1.205189 Test MSE 384.0088181605173 Test RE 0.9989256875151341\n",
      "62 Train Loss 1.2059544 Test MSE 384.0088181605173 Test RE 0.9989256875151341\n",
      "63 Train Loss 1.2066506 Test MSE 384.0088181605173 Test RE 0.9989256875151341\n",
      "64 Train Loss 1.207325 Test MSE 384.0088181605173 Test RE 0.9989256875151341\n",
      "65 Train Loss 1.2080388 Test MSE 384.0088181605173 Test RE 0.9989256875151341\n",
      "66 Train Loss 1.208633 Test MSE 384.0088181605173 Test RE 0.9989256875151341\n",
      "67 Train Loss 1.2092345 Test MSE 384.0088181605173 Test RE 0.9989256875151341\n",
      "68 Train Loss 1.2098693 Test MSE 384.0115839834793 Test RE 0.9989292848890724\n",
      "69 Train Loss 1.2104433 Test MSE 384.0115839834793 Test RE 0.9989292848890724\n",
      "70 Train Loss 1.2110002 Test MSE 384.0115839834793 Test RE 0.9989292848890724\n",
      "71 Train Loss 1.2115722 Test MSE 384.0115839834793 Test RE 0.9989292848890724\n",
      "72 Train Loss 1.2120713 Test MSE 384.0115839834793 Test RE 0.9989292848890724\n",
      "73 Train Loss 1.2126107 Test MSE 384.0115839834793 Test RE 0.9989292848890724\n",
      "74 Train Loss 1.2131248 Test MSE 384.0115839834793 Test RE 0.9989292848890724\n",
      "75 Train Loss 1.213627 Test MSE 384.0115839834793 Test RE 0.9989292848890724\n",
      "76 Train Loss 1.2140847 Test MSE 384.0115839834793 Test RE 0.9989292848890724\n",
      "77 Train Loss 1.2145821 Test MSE 384.00929988309696 Test RE 0.9989263140696368\n",
      "78 Train Loss 1.2150862 Test MSE 384.00929988309696 Test RE 0.9989263140696368\n",
      "79 Train Loss 1.215481 Test MSE 384.00929988309696 Test RE 0.9989263140696368\n",
      "80 Train Loss 1.2158749 Test MSE 384.00929988309696 Test RE 0.9989263140696368\n",
      "81 Train Loss 1.2163123 Test MSE 384.00929988309696 Test RE 0.9989263140696368\n",
      "82 Train Loss 1.2167118 Test MSE 384.00929988309696 Test RE 0.9989263140696368\n",
      "83 Train Loss 1.2171323 Test MSE 384.00929988309696 Test RE 0.9989263140696368\n",
      "84 Train Loss 1.2175206 Test MSE 384.00929988309696 Test RE 0.9989263140696368\n",
      "85 Train Loss 1.2179236 Test MSE 384.00929988309696 Test RE 0.9989263140696368\n",
      "86 Train Loss 1.2182621 Test MSE 384.00929988309696 Test RE 0.9989263140696368\n",
      "87 Train Loss 1.2185718 Test MSE 384.00896489045516 Test RE 0.9989258783600876\n",
      "88 Train Loss 1.2189758 Test MSE 384.00896489045516 Test RE 0.9989258783600876\n",
      "89 Train Loss 1.2192898 Test MSE 384.00896489045516 Test RE 0.9989258783600876\n",
      "90 Train Loss 1.2196734 Test MSE 384.00896489045516 Test RE 0.9989258783600876\n",
      "91 Train Loss 1.2200145 Test MSE 384.00896489045516 Test RE 0.9989258783600876\n",
      "92 Train Loss 1.2202778 Test MSE 384.00904703241076 Test RE 0.9989259851983752\n",
      "93 Train Loss 1.2205712 Test MSE 384.00904703241076 Test RE 0.9989259851983752\n",
      "94 Train Loss 1.2208594 Test MSE 384.00904703241076 Test RE 0.9989259851983752\n",
      "95 Train Loss 1.221213 Test MSE 384.00904703241076 Test RE 0.9989259851983752\n",
      "96 Train Loss 1.2215164 Test MSE 384.00904703241076 Test RE 0.9989259851983752\n",
      "97 Train Loss 1.2217721 Test MSE 384.00904703241076 Test RE 0.9989259851983752\n",
      "98 Train Loss 1.2220227 Test MSE 384.00904703241076 Test RE 0.9989259851983752\n",
      "99 Train Loss 1.2223603 Test MSE 384.00965080537566 Test RE 0.9989267704979293\n",
      "100 Train Loss 1.2225567 Test MSE 384.00965080537566 Test RE 0.9989267704979293\n",
      "101 Train Loss 1.2228202 Test MSE 384.0125628838109 Test RE 0.9989305580949153\n",
      "102 Train Loss 1.223132 Test MSE 384.0125628838109 Test RE 0.9989305580949153\n",
      "103 Train Loss 1.2233565 Test MSE 384.0125628838109 Test RE 0.9989305580949153\n",
      "104 Train Loss 1.223551 Test MSE 384.0125628838109 Test RE 0.9989305580949153\n",
      "105 Train Loss 1.223823 Test MSE 384.0125628838109 Test RE 0.9989305580949153\n",
      "106 Train Loss 1.2240525 Test MSE 384.01318828527025 Test RE 0.9989313715219159\n",
      "107 Train Loss 1.2242349 Test MSE 384.01318828527025 Test RE 0.9989313715219159\n",
      "108 Train Loss 1.2244726 Test MSE 384.01318828527025 Test RE 0.9989313715219159\n",
      "109 Train Loss 1.2246577 Test MSE 384.01318828527025 Test RE 0.9989313715219159\n",
      "110 Train Loss 1.2249206 Test MSE 384.01318828527025 Test RE 0.9989313715219159\n",
      "111 Train Loss 1.2251322 Test MSE 384.0127787870646 Test RE 0.9989308389090711\n",
      "112 Train Loss 1.2253344 Test MSE 384.0127787870646 Test RE 0.9989308389090711\n",
      "113 Train Loss 1.225547 Test MSE 384.0127787870646 Test RE 0.9989308389090711\n",
      "114 Train Loss 1.2257278 Test MSE 384.0127787870646 Test RE 0.9989308389090711\n",
      "115 Train Loss 1.2259191 Test MSE 384.0127787870646 Test RE 0.9989308389090711\n",
      "116 Train Loss 1.2260542 Test MSE 384.0127787870646 Test RE 0.9989308389090711\n",
      "117 Train Loss 1.2262862 Test MSE 384.0127787870646 Test RE 0.9989308389090711\n",
      "118 Train Loss 1.2264403 Test MSE 384.00652972516184 Test RE 0.9989227110466803\n",
      "119 Train Loss 1.2265897 Test MSE 384.00652972516184 Test RE 0.9989227110466803\n",
      "120 Train Loss 1.2267969 Test MSE 384.00652972516184 Test RE 0.9989227110466803\n",
      "121 Train Loss 1.2269949 Test MSE 384.00652972516184 Test RE 0.9989227110466803\n",
      "122 Train Loss 1.2270918 Test MSE 384.00652972516184 Test RE 0.9989227110466803\n",
      "123 Train Loss 1.2272983 Test MSE 384.00652972516184 Test RE 0.9989227110466803\n",
      "124 Train Loss 1.2274848 Test MSE 384.00693455279537 Test RE 0.9989232375890428\n",
      "125 Train Loss 1.2276077 Test MSE 384.00693455279537 Test RE 0.9989232375890428\n",
      "126 Train Loss 1.2277296 Test MSE 384.00693455279537 Test RE 0.9989232375890428\n",
      "127 Train Loss 1.2279512 Test MSE 384.00693455279537 Test RE 0.9989232375890428\n",
      "128 Train Loss 1.2280457 Test MSE 384.00708498438894 Test RE 0.9989234332490505\n",
      "129 Train Loss 1.2282025 Test MSE 384.00708498438894 Test RE 0.9989234332490505\n",
      "130 Train Loss 1.2283814 Test MSE 384.00708498438894 Test RE 0.9989234332490505\n",
      "131 Train Loss 1.2285067 Test MSE 384.00708498438894 Test RE 0.9989234332490505\n",
      "132 Train Loss 1.2286555 Test MSE 384.00708498438894 Test RE 0.9989234332490505\n",
      "133 Train Loss 1.2287054 Test MSE 384.00708498438894 Test RE 0.9989234332490505\n",
      "134 Train Loss 1.2288845 Test MSE 384.00708498438894 Test RE 0.9989234332490505\n",
      "135 Train Loss 1.2290413 Test MSE 384.00708498438894 Test RE 0.9989234332490505\n",
      "136 Train Loss 1.22915 Test MSE 384.00708498438894 Test RE 0.9989234332490505\n",
      "137 Train Loss 1.2292426 Test MSE 384.00708498438894 Test RE 0.9989234332490505\n",
      "138 Train Loss 1.2294009 Test MSE 384.00708498438894 Test RE 0.9989234332490505\n",
      "139 Train Loss 1.2295212 Test MSE 384.00708498438894 Test RE 0.9989234332490505\n",
      "140 Train Loss 1.2296689 Test MSE 384.0068373324718 Test RE 0.9989231111386613\n",
      "141 Train Loss 1.2297757 Test MSE 384.0068373324718 Test RE 0.9989231111386613\n",
      "142 Train Loss 1.2298656 Test MSE 384.0068373324718 Test RE 0.9989231111386613\n",
      "143 Train Loss 1.2300514 Test MSE 384.0068373324718 Test RE 0.9989231111386613\n",
      "144 Train Loss 1.230084 Test MSE 384.0068373324718 Test RE 0.9989231111386613\n",
      "145 Train Loss 1.2301911 Test MSE 384.0067405620597 Test RE 0.9989229852734453\n",
      "146 Train Loss 1.2303454 Test MSE 384.0067405620597 Test RE 0.9989229852734453\n",
      "147 Train Loss 1.2304423 Test MSE 384.0067405620597 Test RE 0.9989229852734453\n",
      "148 Train Loss 1.2305535 Test MSE 384.0067405620597 Test RE 0.9989229852734453\n",
      "149 Train Loss 1.2306474 Test MSE 384.0067405620597 Test RE 0.9989229852734453\n",
      "150 Train Loss 1.2307829 Test MSE 384.0067405620597 Test RE 0.9989229852734453\n",
      "151 Train Loss 1.2308657 Test MSE 384.0067405620597 Test RE 0.9989229852734453\n",
      "152 Train Loss 1.2309241 Test MSE 384.0067405620597 Test RE 0.9989229852734453\n",
      "153 Train Loss 1.2309415 Test MSE 384.0067405620597 Test RE 0.9989229852734453\n",
      "154 Train Loss 1.2311196 Test MSE 384.0067405620597 Test RE 0.9989229852734453\n",
      "155 Train Loss 1.2312615 Test MSE 384.0067405620597 Test RE 0.9989229852734453\n",
      "156 Train Loss 1.2312498 Test MSE 384.0067405620597 Test RE 0.9989229852734453\n",
      "157 Train Loss 1.2313513 Test MSE 384.0067405620597 Test RE 0.9989229852734453\n",
      "158 Train Loss 1.2314482 Test MSE 384.0067692098622 Test RE 0.9989230225344421\n",
      "159 Train Loss 1.2315398 Test MSE 384.0067692098622 Test RE 0.9989230225344421\n",
      "160 Train Loss 1.2316308 Test MSE 384.0067692098622 Test RE 0.9989230225344421\n",
      "161 Train Loss 1.2317646 Test MSE 384.0067692098622 Test RE 0.9989230225344421\n",
      "162 Train Loss 1.2318666 Test MSE 384.00699694671874 Test RE 0.998923318742183\n",
      "163 Train Loss 1.2318832 Test MSE 384.00699694671874 Test RE 0.998923318742183\n",
      "164 Train Loss 1.2319819 Test MSE 384.0071530912322 Test RE 0.9989235218327268\n",
      "165 Train Loss 1.2321063 Test MSE 384.0071530912322 Test RE 0.9989235218327268\n",
      "166 Train Loss 1.2321125 Test MSE 384.0071530912322 Test RE 0.9989235218327268\n",
      "167 Train Loss 1.2322205 Test MSE 384.0071530912322 Test RE 0.9989235218327268\n",
      "168 Train Loss 1.2322254 Test MSE 384.0071530912322 Test RE 0.9989235218327268\n",
      "169 Train Loss 1.2323798 Test MSE 384.0071530912322 Test RE 0.9989235218327268\n",
      "170 Train Loss 1.232448 Test MSE 384.007172072534 Test RE 0.998923546520898\n",
      "171 Train Loss 1.2324991 Test MSE 384.007172072534 Test RE 0.998923546520898\n",
      "172 Train Loss 1.232602 Test MSE 384.007172072534 Test RE 0.998923546520898\n",
      "173 Train Loss 1.2326584 Test MSE 384.007172072534 Test RE 0.998923546520898\n",
      "174 Train Loss 1.2327188 Test MSE 384.007172072534 Test RE 0.998923546520898\n",
      "175 Train Loss 1.232753 Test MSE 384.007172072534 Test RE 0.998923546520898\n",
      "176 Train Loss 1.2328713 Test MSE 384.007172072534 Test RE 0.998923546520898\n",
      "177 Train Loss 1.2329688 Test MSE 384.0071872632391 Test RE 0.9989235662788007\n",
      "178 Train Loss 1.233018 Test MSE 384.0071872632391 Test RE 0.9989235662788007\n",
      "179 Train Loss 1.2330828 Test MSE 384.00672017259893 Test RE 0.9989229587537266\n",
      "180 Train Loss 1.2331463 Test MSE 384.00672017259893 Test RE 0.9989229587537266\n",
      "181 Train Loss 1.2332381 Test MSE 384.00672017259893 Test RE 0.9989229587537266\n",
      "182 Train Loss 1.2332132 Test MSE 384.00718049968526 Test RE 0.9989235574817348\n",
      "183 Train Loss 1.2333422 Test MSE 384.00718049968526 Test RE 0.9989235574817348\n",
      "184 Train Loss 1.233421 Test MSE 384.00718049968526 Test RE 0.9989235574817348\n",
      "185 Train Loss 1.2334527 Test MSE 384.00718049968526 Test RE 0.9989235574817348\n",
      "186 Train Loss 1.2334822 Test MSE 384.00718049968526 Test RE 0.9989235574817348\n",
      "187 Train Loss 1.233544 Test MSE 384.00718049968526 Test RE 0.9989235574817348\n",
      "188 Train Loss 1.2335824 Test MSE 384.00695806224775 Test RE 0.9989232681667948\n",
      "189 Train Loss 1.2336003 Test MSE 384.00695806224775 Test RE 0.9989232681667948\n",
      "190 Train Loss 1.2337093 Test MSE 384.00695806224775 Test RE 0.9989232681667948\n",
      "191 Train Loss 1.2337977 Test MSE 384.00695806224775 Test RE 0.9989232681667948\n",
      "192 Train Loss 1.2338182 Test MSE 384.00695806224775 Test RE 0.9989232681667948\n",
      "193 Train Loss 1.2338456 Test MSE 384.00422833144347 Test RE 0.9989197177149023\n",
      "194 Train Loss 1.2338997 Test MSE 384.004683676407 Test RE 0.9989203099647971\n",
      "195 Train Loss 1.2339406 Test MSE 384.004683676407 Test RE 0.9989203099647971\n",
      "196 Train Loss 1.2340434 Test MSE 384.00446335371095 Test RE 0.9989200233994807\n",
      "197 Train Loss 1.234084 Test MSE 384.00414718317677 Test RE 0.9989196121683976\n",
      "198 Train Loss 1.2341304 Test MSE 384.00327002132326 Test RE 0.9989184712761895\n",
      "199 Train Loss 1.2341999 Test MSE 384.00173632635915 Test RE 0.9989164764523453\n",
      "Training time: 88.75\n",
      "Training time: 88.75\n"
     ]
    }
   ],
   "source": [
    "max_reps = 10\n",
    "max_iter = 200\n",
    "\n",
    "N_f = 1000\n",
    "\n",
    "train_loss_full = []\n",
    "test_mse_full = []\n",
    "test_re_full = []\n",
    "elapsed_time= np.zeros((max_reps,1))\n",
    "\n",
    "time_threshold = np.empty((max_reps,1))\n",
    "time_threshold[:] = np.nan\n",
    "epoch_threshold = max_iter*np.ones((max_reps,1))\n",
    "\n",
    "for reps in range(max_reps):\n",
    "    \n",
    "    train_loss = []\n",
    "    test_mse_loss = []\n",
    "    test_re_loss =[]\n",
    "    \n",
    "    'Generate Training data'\n",
    "    torch.manual_seed(reps*36)\n",
    "     #Total number of collocation points \n",
    "    \n",
    "    \n",
    "    layers = np.array([1,50,50,50,50,50,50,50,50,50,1]) #9 hidden layers\n",
    "    PINN = Sequentialmodel(layers)\n",
    "    PINN.to(device)\n",
    "\n",
    "    'Neural Network Summary'\n",
    "    print(PINN)\n",
    "\n",
    "    params = list(PINN.parameters())\n",
    "    \n",
    "    optimizer = torch.optim.LBFGS(PINN.parameters(), lr=0.25, \n",
    "                              max_iter = 10, \n",
    "                              max_eval = 15, \n",
    "                              tolerance_grad = 1e-5, \n",
    "                              tolerance_change = 1e-5, \n",
    "                              history_size = 100, \n",
    "                              line_search_fn = 'strong_wolfe')\n",
    "    \n",
    "    optimizer_lambda = torch.optim.Adam(PINN.parameters(), lr=5e-3)\n",
    "   \n",
    "\n",
    "    start_time = time.time()\n",
    "    train_model(max_iter,reps)\n",
    "\n",
    "    \n",
    "    torch.save(PINN.state_dict(),label+'_'+str(reps)+'.pt')\n",
    "    train_loss_full.append(train_loss)\n",
    "    test_mse_full.append(test_mse_loss)\n",
    "    test_re_full.append(test_re_loss)\n",
    " \n",
    "    \n",
    "    print('Training time: %.2f' % (elapsed_time[reps]))\n",
    "\n",
    "mdic = {\"train_loss\": train_loss_full,\"test_mse_loss\": test_mse_full, \"test_re_loss\": test_re_full, \"Time\": elapsed_time, \"label\": label, \"Thresh Time\": time_threshold,\"Thresh epoch\": epoch_threshold}\n",
    "savemat(label+'.mat', mdic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x7fa4085d3d10>]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAh8AAAGdCAYAAACyzRGfAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMiwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy8qNh9FAAAACXBIWXMAAA9hAAAPYQGoP6dpAAA1lklEQVR4nO3deXgUVb7G8bdJoAmQRNY0kYBhzIgCboAoMgIiEQQUcZTFBcSZEVmGiMoijqKjCaAiKALqKKKIMCruG8EFF64CwSiLAygoKMQIYjpsCSR1/zh2ZwWydLp6+X6e5zxVXV3d+aUuc/N66tQ5DsuyLAEAAPhJLbsLAAAA4YXwAQAA/IrwAQAA/IrwAQAA/IrwAQAA/IrwAQAA/IrwAQAA/IrwAQAA/CrS7gJKKyws1K5duxQdHS2Hw2F3OQAAoAIsy1Jubq7i4+NVq9bx+zYCLnzs2rVLCQkJdpcBAACqYOfOnWrRosVxzwm48BEdHS3JFB8TE2NzNQAAoCLcbrcSEhK8f8ePJ+DCh+dWS0xMDOEDAIAgU5EhEww4BQAAfkX4AAAAfkX4AAAAfkX4AAAAfkX4AAAAfkX4AAAAfkX4AAAAfkX4AAAAfkX4AAAAfkX4AAAAfkX4AAAAfkX4AAAAfkX4AAAgTFiWdO210qOPSrm59tVB+AAAIEx88420eLE0YYIJInYhfAAAECaWLDHbvn2lmBj76iB8AAAQBiyrKHwMGmRvLYQPAADCwJo10g8/SPXrm54POxE+AAAIA55ej8svNwHEToQPAABCXGGh9N//mn27b7lIhA8AAELe559LP/8sxcZKvXvbXQ3hAwCAkLd0qdkOGCA5nbaWIonwAQBASDt6VHrpJbM/eLC9tXgQPgAACGEffyxlZ0uNG0s9e9pdjUH4AAAghHluuVx1lVS7tr21eBA+AAAIUfn50iuvmP1AueUiET4AAAhZK1ZI+/ZJLpd00UV2V1OE8AEAQIjyTCx29dVSRIS9tRRH+AAAIAQdOCC9+qrZHzLE3lpKI3wAABCC3nhD2r9fat1aOv98u6spifABAEAIeuEFsx06VHI47K2lNMIHAAAhZs8e6f33zf6119pbS3kIHwAAhJj//tfMbNqhg9Smjd3VlEX4AAAgxCxaZLaB2OshET4AAAgp27ZJ//d/Uq1agTWxWHGEDwAAQsjixWbbs6fUvLm9tRwL4QMAgBBhWUVPuQTqLReJ8AEAQMhYt0763/+kunWlK6+0u5pjI3wAABAiPL0el18uxcTYW8vxVCp8TJ06VQ6Ho0RzuVze9y3L0tSpUxUfH6+oqCh1795dGzdu9HnRAACgpIKCorVcAvmWi1SFno+2bdtq9+7d3rZ+/XrvezNmzNDMmTM1Z84crVmzRi6XS7169VJubq5PiwYAACV9+KG0e7fUqJHUu7fd1RxfpcNHZGSkXC6XtzVt2lSS6fWYNWuWpkyZooEDB6pdu3ZauHChDh48qMWeobcAAKBGLFxotoMHS3Xq2FvLiVQ6fGzdulXx8fFKTEzU4MGDtW3bNknS9u3blZWVpeTkZO+5TqdT3bp106pVq475fXl5eXK73SUaAACoOLdbWrbM7A8bZm8tFVGp8NG5c2c999xzev/99/XUU08pKytLXbp00d69e5WVlSVJiouLK/GZuLg473vlSUtLU2xsrLclJCRU4dcAACB8vfSSdOiQmUq9Uye7qzmxSoWPPn366KqrrlL79u11ySWX6O2335YkLfT09UhylFo6z7KsMseKmzx5snJycrxt586dlSkJAICw5/kzPGxY4K1gW55qPWpbv359tW/fXlu3bvU+9VK6lyM7O7tMb0hxTqdTMTExJRoAAKiYbdukTz81oeO66+yupmKqFT7y8vL07bffqnnz5kpMTJTL5VJ6err3/fz8fK1cuVJdunSpdqEAAKCs554z20sukVq0sLeWioqszMm33367+vfvr5YtWyo7O1v333+/3G63hg0bJofDoZSUFKWmpiopKUlJSUlKTU1VvXr1NHTo0JqqHwCAsFVYWBQ+gmGgqUelwsdPP/2kIUOGaM+ePWratKnOP/98ffHFF2rVqpUkacKECTp06JBGjRqlffv2qXPnzlq+fLmio6NrpHgAAMLZZ59J27dL0dGBPZ16aQ7Lsiy7iyjO7XYrNjZWOTk5jP8AAOA4brpJeuYZacQI6emn7a2lMn+/WdsFAIAgdPCgecRWCq5bLhLhAwCAoPTqq1JurpSYKHXtanc1lUP4AAAgCHnm9rjhBqlWkP01D7JyAQDAjz9KK1aY/RtusLeWqiB8AAAQZBYskCxLuvhiqXVru6upPMIHAABBpKDAhA/JPO0SjAgfAAAEkQ8+kHbskE46Kbjm9iiO8AEAQBDxzOdx7bVSVJS9tVQV4QMAgCCxd6/02mtmP1hvuUiEDwAAgsYLL0j5+dI555gWrAgfAAAEAcsquuUSzL0eEuEDAICgkJEhffON5HRKwb5YPOEDAIAg4On1uOoqqWFDe2upLsIHAAAB7uBBafFisx/st1wkwgcAAAHv5Zclt9ssIte9u93VVB/hAwCAAPfEE2b7t78F3yJy5QmBXwEAgNC1YYO0apUUGSmNGGF3Nb5B+AAAIIB5ej2uuEJyueytxVcIHwAABKiDB6Xnnzf7N99sby2+RPgAACBALV0q5eRIrVtLPXvaXY3vED4AAAhQnlsuf/97aAw09QihXwUAgNDx9dfSl1+agaY33mh3Nb5F+AAAIAB5ej2uvFKKi7O3Fl8jfAAAEGD275cWLTL7oTTQ1IPwAQBAgFmyRMrNlU49VerRw+5qfI/wAQBAgPHccvnHP0JroKlHCP5KAAAEr9WrpbVrpTp1pOHD7a6mZhA+AAAIII8/braDBklNm9pbS00hfAAAECB+/dVMLCZJo0fbW0tNInwAABAgnn5aysuTOnSQzjvP7mpqDuEDAIAAUFAgzZ9v9seMkRwOe+upSYQPAAACwNtvSz/+KDVqZMZ7hDLCBwAAAcAz0PSmm6SoKHtrqWmEDwAAbLZli7R8ubnVcsstdldT8wgfAADYbO5cs73sMikx0d5a/IHwAQCAjQ4ckJ591uyPGWNrKX5D+AAAwEaLFkk5OWYdl+Rku6vxD8IHAAA2sSxp9myzP2pUaK7jUp4w+TUBAAg86enSt99KDRpII0bYXY3/ED4AALDJrFlmO2KEFBtrayl+RfgAAMAGmzdL775rHq8dO9buavyL8AEAgA0efdRs+/Uzg03DCeEDAAA/27ev6PHalBQ7K7EH4QMAAD97+mnp4EGpfXupRw+7q/E/wgcAAH509Kj02GNmf9y40F699lgIHwAA+NHrr0s7dkhNmkhDh9pdjT0IHwAA+JHn8dqRI0N/9dpjIXwAAOAnq1dLn30m1a4dHqvXHgvhAwAAP3noIbMdOlSKj7e3FjsRPgAA8INt26RXXjH7t91mby12q1b4SEtLk8PhUEqxh5Qty9LUqVMVHx+vqKgode/eXRs3bqxunQAABLVHHpEKC6Xevc0jtuGsyuFjzZo1evLJJ3XmmWeWOD5jxgzNnDlTc+bM0Zo1a+RyudSrVy/l5uZWu1gAAILR3r3SM8+Y/dtvt7eWQFCl8LF//35de+21euqpp9SwYUPvccuyNGvWLE2ZMkUDBw5Uu3bttHDhQh08eFCLFy/2WdEAAAST+fPNpGJnny1dfLHd1divSuFj9OjR6tu3ry655JISx7dv366srCwlJyd7jzmdTnXr1k2rVq0q97vy8vLkdrtLNAAAQsXhw0WTit1xR3hOKlZaZGU/sGTJEq1bt05r1qwp815WVpYkKS4ursTxuLg4/fjjj+V+X1pamu69997KlgEAQFBYtEj65RcpIUG6+mq7qwkMler52Llzp8aNG6dFixapbt26xzzPUSrWWZZV5pjH5MmTlZOT4207d+6sTEkAAASswkLp4YfNfkqKmd8Dlez5yMjIUHZ2tjp06OA9VlBQoE8++URz5szR5s2bJZkekObNm3vPyc7OLtMb4uF0OuV0OqtSOwAAAe3tt6X//U+KjZX+/ne7qwkcler56Nmzp9avX6/MzExv69ixo6699lplZmaqdevWcrlcSk9P934mPz9fK1euVJcuXXxePAAAgWz6dLO9+WYpOtreWgJJpXo+oqOj1a5duxLH6tevr8aNG3uPp6SkKDU1VUlJSUpKSlJqaqrq1aunoeG6eg4AICx9+qn0+eeS02luuaBIpQecnsiECRN06NAhjRo1Svv27VPnzp21fPlyRRP5AABhJDXVbIcPl4qNRIAkh2VZlt1FFOd2uxUbG6ucnBzFxMTYXQ4AAJX21VfSuedKtWpJW7ZIf/qT3RXVvMr8/WZtFwAAfGzaNLO95prwCB6VRfgAAMCHtm6VXn7Z7E+aZG8tgYrwAQCAD82YYeb3uOwy6ayz7K4mMBE+AADwkZ9/lhYuNPuTJ9tbSyAjfAAA4CMzZ0pHjkhdu5qG8hE+AADwgb17pSeeMPv0ehwf4QMAAB945BHpwAHp7LOlPn3sriawET4AAKimffukRx81+//6l3SMtVTxB8IHAADVNHu2lJsrtWsnDRhgdzWBj/ABAEA15ORIs2aZ/bvvNrOa4vi4RAAAVMNjj5kAcsYZ0lVX2V1NcCB8AABQRbm55vFaSbrrLno9KorLBABAFT3+uBls+uc/m3VcUDGEDwAAqmD/funhh83+XXdJERH21hNMCB8AAFTB/PnSnj1m1dohQ+yuJrgQPgAAqKT9+6Xp083+lClSZKS99QQbwgcAAJX06KOm1+PUU6Xrr7e7muBD+AAAoBJycqSHHjL7U6fS61EVhA8AACrhkUfMEy5nnCENHmx3NcGJ8AEAQAXt3Vs0r8fUqTzhUlWEDwAAKuihh8zEYmeeyWym1UH4AACgArKzi1au/fe/mc20Orh0AABUwPTp0sGDUqdOUv/+dlcT3AgfAACcwK5d0ty5Zv+++ySHw956gh3hAwCAE7j3XunwYenCC6VLL7W7muBH+AAA4Dg2b5aeftrsT5tGr4cvED4AADiOf/1LKiiQ+vWTuna1u5rQQPgAAOAY1q6VXnrJ9HakptpdTeggfAAAcAyTJpntdddJ7dvbW0soIXwAAFCOFSukDz6Qatc2T7jAdwgfAACUUlhY1Otxyy3SKafYWk7IIXwAAFDKyy9LGRlSgwbSlCl2VxN6CB8AABSTn18UOG6/XWrWzN56QhHhAwCAYubNk777ToqLk8aPt7ua0ET4AADgD/v2FQ0uve8+KTra3npCFeEDAIA/PPCA9NtvUtu20ogRdlcTuggfAABI2rZNeuwxs//gg1JkpL31hDLCBwAAku680ww2veQSqXdvu6sJbYQPAEDY++ILaelSM436Qw+xeFxNI3wAAMKaZUm33Wb2hw+XzjrL1nLCAuEDABDWXn5ZWrVKqldP+ve/7a4mPBA+AABh69AhM5GYJN1xh3TyyfbWEy4IHwCAsPXQQ9KOHVKLFtKECXZXEz4IHwCAsLRzp5SWZvYffNDcdoF/ED4AAGFp0iRz26VrV2nQILurCS+EDwBA2Pn8c2nxYvNI7ezZPFrrb4QPAEBYKSyUxo0z+zfdJJ17rr31hCPCBwAgrCxcKGVkSDEx0v33211NeCJ8AADCxu+/m7EeknT33VJcnK3lhC3CBwAgbNx9t5SdLbVpI40da3c14YvwAQAIC199JT3+uNmfM0eqU8feesJZpcLHvHnzdOaZZyomJkYxMTG64IIL9O6773rftyxLU6dOVXx8vKKiotS9e3dt3LjR50UDAFAZhYXS6NFmO2iQ1LOn3RWFt0qFjxYtWmjatGlau3at1q5dq4svvlhXXHGFN2DMmDFDM2fO1Jw5c7RmzRq5XC716tVLubm5NVI8AAAVsXCh9H//J9WvLz38sN3VwGFZllWdL2jUqJEefPBBjRgxQvHx8UpJSdHEiRMlSXl5eYqLi9P06dN18803V+j73G63YmNjlZOTo5iYmOqUBgCAfvtNOu00ac8eM5OpZy0X+FZl/n5XecxHQUGBlixZogMHDuiCCy7Q9u3blZWVpeTkZO85TqdT3bp106pVq475PXl5eXK73SUaAAC+ctddJniccUbR/B6wV6XDx/r169WgQQM5nU6NHDlSr776qs444wxlZWVJkuJKPbcUFxfnfa88aWlpio2N9baEhITKlgQAQLnWrpXmzzf7jz8u1a5tbz0wKh0+TjvtNGVmZuqLL77QLbfcomHDhmnTpk3e9x2l5qi1LKvMseImT56snJwcb9u5c2dlSwIAoIyjR6Wbb5YsSxo6VOre3e6K4BFZ2Q/UqVNHp556qiSpY8eOWrNmjWbPnu0d55GVlaXmzZt7z8/Ozi7TG1Kc0+mU0+msbBkAABzXY49J69ZJJ53EINNAU+15PizLUl5enhITE+VyuZSenu59Lz8/XytXrlSXLl2q+2MAAKiwHTukf/3L7E+fLrlc9taDkirV83HnnXeqT58+SkhIUG5urpYsWaKPP/5Y7733nhwOh1JSUpSamqqkpCQlJSUpNTVV9erV09ChQ2uqfgAASrAsacwY6cAB6cILpb/9ze6KUFqlwscvv/yi66+/Xrt371ZsbKzOPPNMvffee+rVq5ckacKECTp06JBGjRqlffv2qXPnzlq+fLmio6NrpHgAAEpbtkx6800zuPTJJ6VazOUdcKo9z4evMc8HAKCqcnKk00+Xdu82j9j++992VxQ+/DLPBwAAgWbKFBM8Tj1VuvNOu6vBsRA+AAAh4fPPpblzzf78+VJUlL314NgIHwCAoHf4sHTTTWaw6Y03snBcoCN8AACC3r33Sps3m0dqmdMj8BE+AABBLSPDLBgnmdstDRvaWw9OjPABAAha+fnSiBFSQYE0aJB0xRV2V4SKIHwAAILW9OnSN99IjRub6dQRHAgfAICgtHFj0Twejz0mNW1qbz2oOMIHACDoHDkiDRtmtv37S4MH210RKoPwAQAIOmlpZqBpw4ZmkKnDYXdFqAzCBwAgqKxbV3S75fHHpfh4e+tB5RE+AABB4/Bh6YYbpKNHpb/+ldstwYrwAQAIGvfcYwaaNmtmplLndktwInwAAILC558XTSb2xBM83RLMCB8AgIC3f795usWyzG2XAQPsrgjVQfgAAAS8W2+Vvv9eatFCmj3b7mpQXYQPAEBAe/VV6T//MeM7nntOOukkuytCdRE+AAABa9cu6W9/M/t33CH16GFvPfANwgcAICAVFkrDh0u//Sadc07R3B4IfoQPAEBAevRRKT1dqltXeuEFqU4duyuCrxA+AAAB55tvpIkTzf7MmdLpp9tbD3yL8AEACCgHDpiZS/PzpX79pJEj7a4Ivkb4AAAElHHjpG+/lVwu6emnmcU0FBE+AAAB48UXiwLHCy+YadQReggfAICA8N130j/+Yfbvuku6+GJ760HNIXwAAGyXlycNGmSmUf/LX6S777a7ItQkwgcAwHYTJ0rr1kmNGkmLF0uRkXZXhJpE+AAA2OrVV4vWa3n2WbN+C0Ib4QMAYJutW80sppJZPK5/f1vLgZ8QPgAAtjh0SPrrXyW3W7rwQmn6dLsrgr8QPgAAthg92sxk2qyZtHSpVLu23RXBXwgfAAC/e+YZacECqVYtM7fHySfbXRH8ifABAPCrzEzT6yGZlWqZzyP8ED4AAH6zd680cKB0+LBZt2XSJLsrgh0IHwAAvygokIYMkbZvl1q3lhYuNLddEH74PzsAwC/uvFNKT5fq1ZNee81MKIbwRPgAANS4pUulGTPM/oIFUvv29tYDexE+AAA16uuvpREjzP7EidI119hbD+xH+AAA1Ji9e6Urr5QOHpSSk6UHHrC7IgQCwgcAoEbk55sZTD0DTF98UYqIsLsqBALCBwDA5yxLGjtW+vhjqUED6fXXGWCKIoQPAIDPPfaY9OSTksNhejzatbO7IgQSwgcAwKfee8+sUCuZJ1z69bO3HgQewgcAwGe+/VYaNEgqLJRuvFG67Ta7K0IgInwAAHzi119NL4fbLXXtKs2bZ267AKURPgAA1XbokHT55dK2bdIpp0ivvCI5nXZXhUBF+AAAVEthoXT99dIXX0gnnSS9+67UrJndVSGQET4AANUycaLp6ahTx6zZ0qaN3RUh0BE+AABVNm+e9NBDZn/BAqlbN3vrQXAgfAAAquStt6QxY8z+/fdLQ4faWw+CR6XCR1pamjp16qTo6Gg1a9ZMAwYM0ObNm0ucY1mWpk6dqvj4eEVFRal79+7auHGjT4sGANhr1SqzQFxhoVk07s477a4IwaRS4WPlypUaPXq0vvjiC6Wnp+vo0aNKTk7WgQMHvOfMmDFDM2fO1Jw5c7RmzRq5XC716tVLubm5Pi8eAOB/mzaZR2oPHZIuu0yaP59HalE5DsuyrKp++Ndff1WzZs20cuVKXXTRRbIsS/Hx8UpJSdHEiRMlSXl5eYqLi9P06dN18803n/A73W63YmNjlZOTo5iYmKqWBgCoATt3Sl26SD/9JJ1/vrRihVS/vt1VIRBU5u93tcZ85OTkSJIa/bFa0Pbt25WVlaXk5GTvOU6nU926ddOqVavK/Y68vDy53e4SDQAQeH77Terd2wSPNm3MmA+CB6qiyuHDsiyNHz9eXbt2Vbs/VgzKysqSJMXFxZU4Ny4uzvteaWlpaYqNjfW2hISEqpYEAKghBw5I/fubWy7x8dL770uNG9tdFYJVlcPHmDFj9M033+jFF18s856j1M0/y7LKHPOYPHmycnJyvG3nzp1VLQkAUAPy8qSBA80g09hYs3Bcy5Z2V4VgFlmVD40dO1ZvvPGGPvnkE7Vo0cJ73OVySTI9IM2bN/cez87OLtMb4uF0OuVkDl4ACEhHj5pHaJcvl+rVk955R2rf3u6qEOwq1fNhWZbGjBmjZcuW6cMPP1RiYmKJ9xMTE+VyuZSenu49lp+fr5UrV6pLly6+qRgA4BeFhdLf/y4tW2ZmL339dTPYFKiuSvV8jB49WosXL9brr7+u6Oho7ziO2NhYRUVFyeFwKCUlRampqUpKSlJSUpJSU1NVr149DWX2GQAIGpYl3Xqr9OyzUkSEtHSpdMkldleFUFGp8DFv3jxJUvfu3UscX7BggYYPHy5JmjBhgg4dOqRRo0Zp37596ty5s5YvX67o6GifFAwAqFmWJd11l/Too+b1ggXSgAG2loQQU615PmoC83wAgL3uvVeaOtXsz5kjjR5tazkIEn6b5wMAEFpSU4uCx8yZBA/UDMIHAECS9OCD0pQpZn/aNDPmA6gJhA8AgGbPliZMMPv//rf0xwoZQI0gfABAmHvkESklxezffbcZbArUJMIHAISxGTOk8ePN/p13Fo33AGoS4QMAwtT99xfdXpk61bw+xkoYgE9VaXp1AEDwsizzOO2995rX999fNNAU8AfCBwCEEcsyt1emTTOvp08vGmgK+AvhAwDCRGGhNHasNHeuef3ww0XjPQB/InwAQBg4elS68UZp0SIzrmPePOnmm+2uCuGK8AEAIe7wYWnwYLMqbWSk9Nxz0pAhdleFcEb4AIAQlpsrDRworVghOZ3Syy9L/frZXRXCHeEDAEJUdrZ02WVSRobUoIH0xhtSjx52VwUQPgAgJG3bJl16qfTdd1KTJtI770idOtldFWAQPgAgxGRmSr17S7/8Ip1yivT++9Kf/2x3VUARZjgFgBDy0UfSRReZ4HHWWdKqVQQPBB7CBwCEiOeeM7dacnOl7t2llSul5s3trgooi/ABAEHOsqT77pOGDZOOHJGuuUZ6910pNtbuyoDyET4AIIjl55vJw+65x7yeOFF68UWpbl176wKOhwGnABCk9u2T/vpX6cMPpYgI6fHHmbUUwYHwAQBBaOtWM1nYli1mDo///lfq08fuqoCK4bYLAASZDz+UOnc2wSMhQfr0U4IHggvhAwCCyPz5UnKyueVy/vnS6tXS2WfbXRVQOYQPAAgCR45IY8ZIt9wiFRRI111n5vRwueyuDKg8xnwAQID75Rfp6qvN7RVJSk2VJk2SHA576wKqivABAAFszRqzKu1PP0nR0dKiRdLll9tdFVA93HYBgAD17LPSX/5igsdpp5nxHQQPhALCBwAEmLw8adQoM3lYXp4JHF9+KbVpY3dlgG8QPgAggPzwg9S1qzRvnhnTMXWq9OqrTJWO0MKYDwAIEO+8Y55i2bdPatRIeuEFqXdvu6sCfI+eDwCw2dGj0l13SX37muDRqZO0bh3BA6GLng8AsNFPP0lDhkiffWZejx4tPfyw5HTaWxdQkwgfAGCTN9+Uhg+XfvvNPEb75JPS4MF2VwXUPG67AICf5eVJt95qnmL57TepQwdzm4XggXBB+AAAP/r2W7Mmy6xZ5nVKivT559Kpp9pZFeBfhA8A8APLkubOlc49V8rMlBo3lt54Q3rkEcZ3IPww5gMAatgvv0g33SS9/bZ5feml0oIFUvPm9tYF2IWeDwCoQa+9JrVvb4KH0ynNnm3m8yB4IJzR8wEANeD336V//lN6/nnzun17M2lY+/a2lgUEBHo+AMDH3n9fatfOBI9ataRJk8zqtAQPwKDnAwB8JCdHuuMO6amnzOukJGnhQumCC+ytCwg09HwAgA+89ZbUtm1R8Bg71jzVQvAAyqLnAwCqYc8eadw4afFi8/rUU6X//Efq1s3euoBARs8HAFSBZZkBpGecYYJHrVrS7bdLX39N8ABOhJ4PAKik77+XbrlFSk83r9u2lZ55RjrvPHvrAoIFPR8AUEH5+VJamnmSJT3dzNtx//1mXRaCB1Bx9HwAQAV89JE0Zoy0aZN53bOnNH8+a7IAVUHPBwAcx65d0tCh0sUXm+DRpIn03HOm54PgAVQN4QMAynHkiFn0rU0b6cUXzYDSUaOkLVuk66+XHA67KwSCF7ddAKCU996Tbr1V+t//zOvOnYtWpAVQffR8AMAftmyR+vWT+vQxwaNpUzNnx6pVBA/AlyodPj755BP1799f8fHxcjgceu2110q8b1mWpk6dqvj4eEVFRal79+7auHGjr+oFAJ/bt0+67TbzFMvbb0uRkeb11q3STTeZWy4AfKfS/5M6cOCAzjrrLM2ZM6fc92fMmKGZM2dqzpw5WrNmjVwul3r16qXc3NxqFwsAvpSXZ8Z1/OlP0syZZpzHZZdJGzZIDz0kxcbaXSEQmio95qNPnz7q06dPue9ZlqVZs2ZpypQpGjhwoCRp4cKFiouL0+LFi3XzzTdXr1oA8AHLkl5+2aw2u22bOdaunfTgg1Lv3vbWBoQDn3Ymbt++XVlZWUpOTvYeczqd6tatm1atWlXuZ/Ly8uR2u0s0AKgpH35oBpBec40JHi6XWQwuM5PgAfiLT8NHVlaWJCkuLq7E8bi4OO97paWlpSk2NtbbEhISfFkSAEiSMjKk5GQzOdiaNVL9+tLdd5txHX/7mxQRYXeFQPiokWFUjlIPwFuWVeaYx+TJk5WTk+NtO3furImSAISpTZtML0fHjmZisNq1zUyl338v3Xuv1KCB3RUC4cen83y4XC5JpgekefPm3uPZ2dllekM8nE6nnE6nL8sAAG3dasLF4sVmjIfDIV17rTnWurXd1QHhzac9H4mJiXK5XEr3LPUoKT8/XytXrlSXLl18+aMAoFzbtpnHY08/3Sx5b1nSlVeape6ff57gAQSCSvd87N+/X99995339fbt25WZmalGjRqpZcuWSklJUWpqqpKSkpSUlKTU1FTVq1dPQ4cO9WnhAFDcli1Saqq0aJFUUGCO9e0r3XcfE4QBgabS4WPt2rXq0aOH9/X48eMlScOGDdOzzz6rCRMm6NChQxo1apT27dunzp07a/ny5YqOjvZd1QDwh2+/lR54wKy/Ulhojl16qTR1qnT++baWBuAYHJZlWXYXUZzb7VZsbKxycnIUExNjdzkAAtTq1dK0adJrr5lbK5LUv790113SeefZWhoQlirz95uF5QAEDcuSPvhASksz83V4XHmlCR3cXgGCA+EDQMA7ckR66SUz5flXX5ljkZHm6ZWJE83gUgDBg/ABIGC53Wb20dmzJc8UQFFRZlKw226TWrWytz4AVUP4ABBwvv9eeuwx6ZlnJM+alM2aSWPHSrfcIjVubG99AKqH8AEgIFiWGccxe7b01ltFg0jbtDG9HNddJ9Wta2+NAHyD8AHAVm63mfxr7lwzFbpHnz7SuHFSr15SrRpZCAKAXQgfAGyxYYMJHM8/L+3fb47Vry8NH25ur5x2mq3lAahBhA8AfnPokHlq5cknpc8/Lzrepo00apR0ww1SbKx99QHwD8IHgBq3fr15auX556XffzfHIiKkK66QRo+WevQwC78BCA+EDwA14vffpSVLpKefltauLTp+yinS3/8u3XijVGzxawBhhPABwGcKCswTKwsXSq+8Ih0+bI5HRkqXXy794x8MIAVA+ADgAxs3Ss89Z1aU3bWr6HjbtmZ5++uuk5o2ta8+AIGF8AGgSn76SVq6VHrhhaIpzyWpYUNpyBBp2DCpUyfGcgAoi/ABoML27pWWLZMWL5ZWriyaCCwyUurXzzytctllktNpb50AAhvhA8Bx/f67WbZ+6VJpxQrp6NGi9/7yF2noUOmvf5WaNLGrQgDBhvABoIy9e6U33jCDRpcvN6vKepx1lrmtMngwC7sBqBrCBwBJUlaW6eF45RXpo4/MkysebdtKgwZJ11zDzKMAqo/wAYSx//1Pev11Ezq+/LJoDIcktW8vXXWVuaXStq1tJQIIQYQPIIwcOWKmNX/rLenNN6UtW0q+36mTNHCgCR1JSfbUCCD0ET6AEJedLb3/vvTOO9J77xVNby5JtWtLF18sDRgg9e8vnXyyXVUCCCeEDyDEHD0qrVkjvfuuaRkZJW+nNGliHoft109KTmYhNwD+R/gAQsAPP5inUpYvlz74oGTvhiSdfbbUp4/p3TjvPLOoGwDYhfABBKE9e8waKh98YNr335d8/6STzBoqffpIvXuzgBuAwEL4AILAvn3Sp59KH39sHoPNzCz5fkSEdMEF5jZKcrLUsSO9GwACF+EDCEC//ip99pn0ySdmGvPMzJLjNiTzKGzPnqZddJEUE2NLqQBQaYQPwGaWJW3bZh6B/fxz08Px7bdlzzvtNKlHD6l7d9Pi4vxdKQD4BuED8LNDh6R166QvvjBhY9Uq6Zdfyp7Xtq1ZO+Wii0zYYNwGgFBB+ABqUGGh9N130urVpn3xhbmFUnytFEmqU0fq0EG68EITOC68UGrc2JaSAaDGET4AH7EsaccOM69GRoYJG2vXln3sVTK3TC64wLQLLzTBo25dv5cMALYgfABVUFhoHm/96quilpFhHoEtrW5d6dxzzdTl559vAkfLlpLD4f+6ASAQED6AEzh4UNqwQfr665ItN7fsuZGR5imUDh3M467nnSe1a2emMQcAGIQP4A9Hj5rejA0bpPXrTduwwYzZKCwse77TaYLGOeeY1rGjec3tEwA4PsIHwk5+vrR1q1lOftMm0zZulDZvNu+Vp2lT6ayzitrZZ0tt2tCjAQBVQfhASLIsM1HX5s1m2fgtW8z+t9+a3o2CgvI/FxVlHnFt397cLmnf3jSXy7/1A0AoI3wgaFmWlJVlJuj6/nvTm/Hdd0XbnJxjfzY62vRcnH66CRtnnGG2rVpJtWr573cAgHBE+EBAc7vNiq3btxdtt283gWPbNjMY9FgcDhMmTjtN+vOfTfMEjvh4njYBALsQPmCbo0dNz8WOHdLOnUXtxx9N0Pjxx/LnyCiuVi3z2Grr1lJSknTqqWablGSOMfgTAAIP4QM+Z1nmlsfu3dKuXSXbTz9JP/9strt3l/8USWmNGkmJiWXbn/5kejbq1Kn53wkA4DuED1RIQYH0229mEOevv0rZ2WY9Es/2l19ML4an5eVV7HsjI6WTTza9FwkJprVqVbI1aFCzvxsAwL8IH2GmsNCMo/j9d9P27TOhonjbu9fM1OnZ7tljjlekl6K4k04ywSI+3iyKFh9vXrdoYdrJJ0vNmkkRETXwiwIAAhbhI8AdOWJWQT14sKgdOFDU9u837cABEypyc01zu03LySnZ3G5zW6SqGjY0c140bWrWJ/G0Zs1MwHC5zDYujvEWAIDyhU34KCiQOnc24wNKt8hI02rXNtuICNNq1SradziKmmS2llW2FRSYHoKCgqJ29GhRO3LETGSVn1+0f/iwuU1RvB06ZNqx5qOorrp1Tc9Ew4ZmTIWnNWwoNWliVlT1bBs3NuGicWMm1QIAVF/YhI8jR8zCX8HK4ZDq1TOtfn2zbdCgqNWvL8XEmPkroqOL9mNjTYuJMduTTjKNXgkAgF3CJnxERkrvvFPU6+BpeXkleyU8+4WFJXswPD0bUtF+6Z6QWrWKeks8+57elNq1zfHatU1vS/Gt02nCgNNZtB8VZZpnv25d5qUAAISGsAofffrYXQUAAGAiaQAA4FeEDwAA4FeEDwAA4FeEDwAA4FeEDwAA4Fc19rTL3Llz9eCDD2r37t1q27atZs2apb/85S819eNOzLKkG24wz616Jsoo3aKiirblNc9zr8wHDgBAldVI+Fi6dKlSUlI0d+5cXXjhhXriiSfUp08fbdq0SS1btqyJH3lieXnSokW++a7ISBNEik/C4Zmoo/T2WK1OnWNvy2vlTRBSet8zmQgAAAHMYVnVWemjfJ07d9a5556refPmeY+dfvrpGjBggNLS0o77WbfbrdjYWOXk5CgmJsZ3ReXlSY8/bhZBKb1QimfxlOKLqHjmN/e0I0d8V0tNcjhKzmxWuhWfR96zrUiLiCi775l7/lj7lWnFZ2crb3770rO3Fd8v73VFWvGZ4Y73uvhscgCAclXm77fPez7y8/OVkZGhSZMmlTienJysVatW+frHVZzTKY0fX/XPFxSYAHP4cFEg8bz2HPO8Lr6tSCu+4EteXtnFX4qfc+RIyf3S2dGyij4P3yodSI61rcr+idrxzpWOfV7x94rvH+ucE332WN93on2PY51T+vvL21b3nNL7lT3neJ873jkV+Z4THavKd/vie6vDF9/l+/82rt53V+QzFf3eqnxXVX9+ecciI81/kNvE5+Fjz549KigoUFxcXInjcXFxysrKKnN+Xl6e8vLyvK/dbrevS/KNiIiisSGBwrOSnSeQHKt55o4vPn988eOlV77zrIbn2XqOHet18e3xmmfe+vLeK37cs1/6WPE570u/9syBX945nuOl58mvrMJCs62p1f4AwF+cztAKHx6OUqnXsqwyxyQpLS1N9957b02VEdocjqJbIVFRdlcTPDwBpHggKR5mPK+LHy9+rHiIKX1Oea+Lf7Yqx6rTiv++J3r/RPuVOfdE31F8W5HvP97nj/dfehX5fHmfK+/1sT5X2e+p7ueq8j0V/VxV1GTvREX5qsemqt9T1V6lmuqxqsjPsnl8oM/DR5MmTRQREVGmlyM7O7tMb4gkTZ48WeOL3Q5xu91KSEjwdVlAkeK3KCLDZnkjAAgYPp/no06dOurQoYPS09NLHE9PT1eXLl3KnO90OhUTE1OiAQCA0FUj/9k3fvx4XX/99erYsaMuuOACPfnkk9qxY4dGjhxZEz8OAAAEkRoJH4MGDdLevXt13333affu3WrXrp3eeecdtWrVqiZ+HAAACCI1Ms9HddTYPB8AAKDGVObvN2u7AAAAvyJ8AAAAvyJ8AAAAvyJ8AAAAvyJ8AAAAvyJ8AAAAvyJ8AAAAvyJ8AAAAvyJ8AAAAvwq4JT09E6663W6bKwEAABXl+btdkYnTAy585ObmSpISEhJsrgQAAFRWbm6uYmNjj3tOwK3tUlhYqF27dik6OloOh8On3+12u5WQkKCdO3eybkwN41r7D9faf7jW/sO19h9fXWvLspSbm6v4+HjVqnX8UR0B1/NRq1YttWjRokZ/RkxMDP+Y/YRr7T9ca//hWvsP19p/fHGtT9Tj4cGAUwAA4FeEDwAA4FdhFT6cTqfuueceOZ1Ou0sJeVxr/+Fa+w/X2n+41v5jx7UOuAGnAAAgtIVVzwcAALAf4QMAAPgV4QMAAPgV4QMAAPhV2ISPuXPnKjExUXXr1lWHDh306aef2l1S0EtLS1OnTp0UHR2tZs2aacCAAdq8eXOJcyzL0tSpUxUfH6+oqCh1795dGzdutKni0JGWliaHw6GUlBTvMa617/z888+67rrr1LhxY9WrV09nn322MjIyvO9zrX3n6NGjuuuuu5SYmKioqCi1bt1a9913nwoLC73ncL2r5pNPPlH//v0VHx8vh8Oh1157rcT7FbmueXl5Gjt2rJo0aaL69evr8ssv108//VT94qwwsGTJEqt27drWU089ZW3atMkaN26cVb9+fevHH3+0u7Sgdumll1oLFiywNmzYYGVmZlp9+/a1WrZsae3fv997zrRp06zo6GjrlVdesdavX28NGjTIat68ueV2u22sPLitXr3aOuWUU6wzzzzTGjdunPc419o3fvvtN6tVq1bW8OHDrS+//NLavn27tWLFCuu7777znsO19p3777/faty4sfXWW29Z27dvt1566SWrQYMG1qxZs7zncL2r5p133rGmTJlivfLKK5Yk69VXXy3xfkWu68iRI62TTz7ZSk9Pt9atW2f16NHDOuuss6yjR49Wq7awCB/nnXeeNXLkyBLH2rRpY02aNMmmikJTdna2JclauXKlZVmWVVhYaLlcLmvatGnecw4fPmzFxsZa8+fPt6vMoJabm2slJSVZ6enpVrdu3bzhg2vtOxMnTrS6du16zPe51r7Vt29fa8SIESWODRw40Lruuussy+J6+0rp8FGR6/r7779btWvXtpYsWeI95+eff7Zq1aplvffee9WqJ+Rvu+Tn5ysjI0PJyckljicnJ2vVqlU2VRWacnJyJEmNGjWSJG3fvl1ZWVklrr3T6VS3bt249lU0evRo9e3bV5dcckmJ41xr33njjTfUsWNHXX311WrWrJnOOeccPfXUU973uda+1bVrV33wwQfasmWLJOnrr7/WZ599pssuu0wS17umVOS6ZmRk6MiRIyXOiY+PV7t27ap97QNuYTlf27NnjwoKChQXF1fieFxcnLKysmyqKvRYlqXx48era9euateunSR5r2951/7HH3/0e43BbsmSJVq3bp3WrFlT5j2ute9s27ZN8+bN0/jx43XnnXdq9erV+uc//ymn06kbbriBa+1jEydOVE5Ojtq0aaOIiAgVFBTogQce0JAhQyTxb7umVOS6ZmVlqU6dOmrYsGGZc6r79zPkw4eHw+Eo8dqyrDLHUHVjxozRN998o88++6zMe1z76tu5c6fGjRun5cuXq27dusc8j2tdfYWFherYsaNSU1MlSeecc442btyoefPm6YYbbvCex7X2jaVLl2rRokVavHix2rZtq8zMTKWkpCg+Pl7Dhg3znsf1rhlVua6+uPYhf9ulSZMmioiIKJPSsrOzyyQ+VM3YsWP1xhtv6KOPPlKLFi28x10ulyRx7X0gIyND2dnZ6tChgyIjIxUZGamVK1fq0UcfVWRkpPd6cq2rr3nz5jrjjDNKHDv99NO1Y8cOSfy79rU77rhDkyZN0uDBg9W+fXtdf/31uvXWW5WWliaJ611TKnJdXS6X8vPztW/fvmOeU1UhHz7q1KmjDh06KD09vcTx9PR0denSxaaqQoNlWRozZoyWLVumDz/8UImJiSXeT0xMlMvlKnHt8/PztXLlSq59JfXs2VPr169XZmamt3Xs2FHXXnutMjMz1bp1a661j1x44YVlHhnfsmWLWrVqJYl/17528OBB1apV8k9RRESE91FbrnfNqMh17dChg2rXrl3inN27d2vDhg3Vv/bVGq4aJDyP2j799NPWpk2brJSUFKt+/frWDz/8YHdpQe2WW26xYmNjrY8//tjavXu3tx08eNB7zrRp06zY2Fhr2bJl1vr1660hQ4bwiJyPFH/axbK41r6yevVqKzIy0nrggQesrVu3Wi+88IJVr149a9GiRd5zuNa+M2zYMOvkk0/2Pmq7bNkyq0mTJtaECRO853C9qyY3N9f66quvrK+++sqSZM2cOdP66quvvNNMVOS6jhw50mrRooW1YsUKa926ddbFF1/Mo7aV8fjjj1utWrWy6tSpY5177rnex0FRdZLKbQsWLPCeU1hYaN1zzz2Wy+WynE6nddFFF1nr16+3r+gQUjp8cK19580337TatWtnOZ1Oq02bNtaTTz5Z4n2ute+43W5r3LhxVsuWLa26detarVu3tqZMmWLl5eV5z+F6V81HH31U7v+PHjZsmGVZFbuuhw4dssaMGWM1atTIioqKsvr162ft2LGj2rU5LMuyqtd3AgAAUHEhP+YDAAAEFsIHAADwK8IHAADwK8IHAADwK8IHAADwK8IHAADwK8IHAADwK8IHAADwK8IHAADwK8IHAADwK8IHAADwK8IHAADwq/8HJ0V0Ib/NGbEAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "u_pred = PINN.test()\n",
    "plt.plot(u_pred,'r')\n",
    "plt.plot(y_true,'b')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    }
   ],
   "source": [
    "a = 0\n",
    "for i in range(10):\n",
    "    a = a + test_re_full[i][-1]\n",
    "print(a/10)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
